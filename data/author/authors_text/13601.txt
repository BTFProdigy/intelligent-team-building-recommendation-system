Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1416?1425,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Reducing Grounded Learning Tasks to Grammatical Inference
Benjamin B?rschinger
Department of Computing
Macquarie University
Sydney, Australia
benjamin.borschinger@mq.edu.au
Bevan K. Jones
School of Informatics
University of Edinburgh
Edinburgh, UK
b.k.jones@sms.ed.ac.uk
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mark.johnson@mq.edu.au
Abstract
It is often assumed that ?grounded? learning
tasks are beyond the scope of grammatical in-
ference techniques. In this paper, we show
that the grounded task of learning a seman-
tic parser from ambiguous training data as dis-
cussed in Kim and Mooney (2010) can be re-
duced to a Probabilistic Context-Free Gram-
mar learning task in a way that gives state
of the art results. We further show that ad-
ditionally letting our model learn the lan-
guage?s canonical word order improves its
performance and leads to the highest seman-
tic parsing f-scores previously reported in the
literature.1
1 Introduction
One of the most fundamental ideas about language
is that we use it to express our thoughts. Learning a
natural language, then, amounts to (at least) learning
a mapping between the things we utter and the things
we think, and can therefore be seen as the task of
learning a semantic parser, i.e. something that maps
natural language expressions such as sentences into
meaning representations such as logical forms. Ob-
viously, this learning can neither take place in a fully
supervised nor in a fully unsupervised fashion: the
learner does not ?hear? the meanings of the sentences
she observes, but she is also not treating them as
merely meaningless strings. Rather, it seems plau-
sible to assume that she uses extra-linguistic context
1The source code used for our experiments and the evalua-
tion is available as supplementary material for this article.
to assign certain meanings to the linguistic input she
is confronted with.
In this sense, learning a semantic parser seems
to go beyond the well-studied task of unsupervised
grammar induction. It involves not only learning
a grammar for the form-side of language, i.e. lan-
guage expressions such as sentences, but also the
?grounding? of this structure in meaning represen-
tations. It requires going beyond the mere linguistic
input to incorporate, for example, perceptual infor-
mation that provides a clue to the meaning of the ob-
served forms. Essentially, it seems as if ?grounded?
learning tasks like this require dealing with two
different kinds of information, the purely formal
(phonemic) and meaningful (semantic) aspects of
language. Grammatical inference seems to be lim-
ited to dealing with one level of formal information
(Chang and Maia, 2001). For this reason, probably,
approaches to the task of learning a semantic parser
employ a variety of sophisticated and task-specific
techniques that go beyond (but often elaborate on)
the techniques used for grammatical inference (Lu
et al, 2008; Chen and Mooney, 2008; Liang et al,
2009; Kim and Mooney, 2010; Chen et al, 2010).
In this paper, we show that one can reduce the
task of learning a semantic parser to a Probabilistic
Context Free Grammar (PCFG) learning task, and
more generally, that grounded learning tasks are not
in principle beyond the scope of grammatical infer-
ence techniques. In particular, we show how to for-
mulate the task of learning a semantic parser as dis-
cussed by Chen, Kim and Mooney (2008, 2010) as
the task of learning a PCFG from strings. Our model
does not only constitute a proof of concept that this
1416
reduction is possible for certain cases, it also yields
highly competitive results.2
By reducing the problem to the well understood
PCFG formalism, it also becomes easy to consider
extensions, leading to our second contribution. We
demonstrate that a slight modification to our model
so that it also learns the language?s canonical word
order improves its performance even beyond the best
results previously reported in the literature. This
language-independent and linguistically well moti-
vated elaboration allows the model to learn a global
fact about the language?s syntax, its canonical word
order.
Our contribution is two-fold. We provide an illus-
tration of how to reduce grounded learning tasks to
grammatical inference. Secondly, we show that ex-
tending the model so that it can learn linguistically
well motivated generalizations such as the canonical
word order can lead to better results.
The structure of the paper is as follows. First we
give a short overview of the previous work by Chen,
Kim and Mooney and describe their dataset. Then,
we show how to reduce the parsing task addressed
by them to a PCFG-learning task. Finally, we ex-
plain how to let our model additionally learn the lan-
guage?s canonical word order.
2 Previous Work by Chen, Kim and
Mooney
In a series of recent papers, Chen, Kim and Mooney
approach the task of learning a semantic parser from
ambiguous training data (Chen and Mooney, 2008;
Kim and Mooney, 2010; Chen et al, 2010). This
goes beyond previous work on semantic parsing
such as Lu et al (2008) or Zettlemoyer and Collins
(2005) which rely on unambiguous training data
where every sentence is paired only with its mean-
ing. In contrast, Chen, Kim and Mooney allow
their training examples to exhibit the kind of uncer-
tainty about sentence meanings human learners are
likely to have to deal with by allowing for sentences
to be associated with a set of candidate-meanings,
2It has been pointed out to us by one reviewer that the task
we address falls short of what is often called ?grounded learn-
ing?. We acknowledge that semantic parsing constitutes a very
limited kind of grounded learning but want to point out that the
task has been introduced as an instance of grounded learning in
the previous literature such as Chen and Mooney (2008).
and the correct meaning might not even be in this
set. They create the training data by first collect-
ing humanly generated written language comments
on four different RoboCup games. The comments
are recorded with a time-stamp and then associated
with all game events automatically extracted from
the games which occured up to five seconds before
the comment was made. This leads to an ambigu-
ous pairing of comments with candidate meanings
that can be considered similar to the "linguistic in-
put in the context of a rich, relevant, perceptual en-
vironment" to which real language learners prob-
ably have access (Chen and Mooney, 2008). For
evaluation purposes, they manually create a gold-
standard which contains unambiguous natural lan-
guage comment / event pairs. Due to the fact
that some comments refer to events not detected
by their extraction-algorithm, not every natural lan-
guage sentence has a gold matching meaning repre-
sentation. In addition to the inherent ambiguity of
the training examples, the learner therefore has to
somehow deal with those examples which only have
?wrong? meanings associated with them.
Datasets exist for both Korean and English, each
comprising training and gold data for four games.3
Some details about this data are given in Table 1,
such as the number of examples, their average am-
biguity and the number of misleading examples.
For the following short discussion of previous ap-
proaches, we mainly focus on Kim and Mooney
(2010). This is the most recent publication and re-
ports the highest scores.
2.1 The parsing task
Learning a semantic parser from the ambiguous data
is, in fact, just one of three tasks discussed by Kim
and Mooney (2010), henceforth KM. In addition to
parsing, they discuss matching and natural language
generation. We are ignoring the generation task as
we are currently only interested in the parsing prob-
lem, and we treat the matching task, picking the cor-
rect meaning from the set of candidates, merely as
a byproduct of parsing, rather than as a completely
separate task: parsing implicitly requires the model
to disambiguate the data it is learning from.
3The datasets are freely available at http://www.cs.
utexas.edu/~ml/clamp/sportscasting/. We re-
trieved the data used here on March 29th, 2011.
1417
Number of comments Ambiguity
# Training # Training with
Gold Match
# Training with
correct MR
# Gold Noise Avg. # of MRs
English dataset
total 1872 1492 1360 1539 0.2735 2.20
Korean dataset
total 1914 1763 1733 1763 0.0946 2.39
Table 1: Statistics for the Korean and the English datasets. The numbers are basically identical to those reported in
Chen et al (2010) except for minimal differences in the number of training examples (we give one more for every
English training set, and one more for the 2004 Korean training set). In addition, our calculation of the average
sentential ambiguity (Avg. # of MRs) differs because we assume that mutiple occurences of the same event in a
context do not add to the overall ambiguity, and our calculation of the noise (fraction of training examples without
the correct meaning in their context) takes into account that there are training examples which do not have their gold
meaning associated with them in the training data and is therefore slightly higher than the one reported in Chen et al
(2010).
KM?s model builds on previous work by Lu et al
(2008) and is a generative model which defines a
joint probability distribution over natural language
sentences (NLs), meaning representations (MRs)
and hybrid trees. The NLs are the natural language
comments to the games, the MRs are simple log-
ical formulae describing game events and playing
the role of sentence meanings, and a hybrid tree is
a tree structure that represents the correspondence
between a sentence and its meaning. More specif-
ically, if some NL W has as its meaning an MR
m, and m has been generated by a meaning gram-
mar (MG) G, the hybrid tree corresponding to the
pair ?W,m? has as its internal nodes those rules of
G used in the derivation of m, and as its leaves the
words making up W.4 An example hybrid tree for
the pair ?THE PINK GOALIE PASSES THE BALL TO
PINK11,pass(pink1,pink11)? is given in Figure 1.
Their model is trained by a variant of the Inside-
Outside algorithm which deals with the hybrid tree
structure and takes into account the ambiguity of the
training examples.
In addition to learning directly from the ambigu-
ous training data, they also train a semantic parser
in a supervised fashion on data that has been pre-
viously disambiguated by their matching model.
This slightly improves their system?s performance.
Consequently, there are two scores for each of the
4We use SMALL CAPS for words, sans serif for MRs and
MR constituents (concepts), and italics for non-terminals and
Grammars.
S
S? pass PLAYER PLAYER
PLAYER
PLAYER? pink11
PINK11
PASSES THE BALL TOPLAYER
PLAYER? pink1
THE PINK GOALIE
Figure 1: A hybrid tree for the sentence-meaning
pair ?THE PINK GOALIE PASSES THE BALL TO
PINK11,pass(pink1,pink11)? . The internal nodes cor-
respond to the rules used to derive pass(pink1,pink11)
from a given Meaning Grammar, and the leaves corre-
spond to the words or substrings that make up the sen-
tence.
two languages (English and Korean) with which
we compare our own model: those of the parsers
trained directly from the ambiguous data, and those
of the ?supervised? parsers which constitute the cur-
rent state of the art. The details of their evaluation
method are disccused in Section 3.3, and their scores
are given in Table 2, together with our own scores.
3 Learning a Semantic Parser as a
PCFG-learning problem
Given that one can effectively represent both a sen-
tence?s form and its meaning in a hybrid tree, it is in-
teresting to ask whether one can do with a structure
that can be learned by grammatical inference tech-
1418
niques from strings which incorporate the contextual
information. In this section, we show how to reduce
hybrid trees to such ?standard? trees. In effect, we
show via construction that ?grounded? learning tasks
such as learning a semantic parser from semantically
enriched and ambiguous data can be reduced to ?un-
grounded? tasks such as grammatical inference.
Instead of taking the internal nodes of the trees
generated by our model as corresponding to MG
production rules, we take them to correspond to MR
constituents. The MR pass(pink1,pink11), for exam-
ple, has 4 constituents: the whole MR, the predicate
pass, and the two arguments pink1 and pink11. Fig-
ure 2 gives the tree we assume instead of Figure 1
for the sentence-meaning pair ?THE PINK GOALIE
PASSES THE BALL TO PINK11,pass(pink1,pink11)?.
Its root is assumed to correspond to the whole
MR and is labeled Spass(pink1,pink11). The remain-
ing three MR constituents correspond to the root?s
daughters which we label Phrasepink1, Phrasepass
and Phrasepink11. Generally speaking, we assume a
special non-terminal Sm for every MR m generated
by the MG, and a special non-terminal Phrasecon for
each of the terminals of the MG (which loosely cor-
respond to concepts). This is only possible for MGs
which create a finite set of MRs, but the MG used by
Kim and Mooney (2010) obeys this restriction.5
The tree?s terminals are the words that make up
the sentence, and we assume them to be dominated
by concept-specific pre-terminals Wordcon which
correspond to concept-specific probability distribu-
tions over the language?s vocabulary. Since each
Phrasecon may span multiple words, we give trees
rooted in Phrasecon a left-recursive structure that
corresponds to a unigram Markov-process. This
process generates an arbitrary sequence of words
semantically related to con, dominated by the cor-
responding pre-terminal Wordcon in our model, and
words not directly semantically related to con, dom-
inated by a special word pre-terminal Word?. The
sole further restriction is that every Phrasecon must
contain at least one Wordcon.
Trees like the one in Figure 2 can be generated by
a Context-Free Grammar (CFG) which, in turn, can
be trained on strings to yield a PCFG which embod-
5This grammar is given in the Appendix to Chen et al
(2010) and generates a total of 2048 MRs.
ies a semantic parser as will be discussed in Section
3.3. We now describe how to set up such a CFG in a
systematic way and how to train it on the data used
by KM.
3.1 Setting up the PCFG
The training data expresses information of two dif-
ferent kinds ? form and meaning. Every training ex-
ample consists of a natural language string (the for-
mal information) and a set of candidate meanings
for the string (the semantic information, its context),
allowing for the possibility that none of the mean-
ings in the context is the correct one. In order to
learn from data like this within a grammatical in-
ference framework, we have to encode the semantic
information as part of the string. Assigning a spe-
cific MR m to a string corresponds, in our frame-
work, to analyzing it as a tree with Sm as its root.
A sentence?s context constrains which of the many
possible meanings might be expressed by the string.
Thus the role played by the context is adequately
modelled if we ensure that if a string W is associated
with a context {m1,...,mn}, the model only considers
the possibilities that this string might be analyzed as
Sm1,...,Smn.There are 959 different contexts, i.e. 959 dif-
ferent sets of MRs, in the English data set (984
for the Korean data), and we therefore introduce
959 new terminal symbols which play the role of
context-identifiers, for example C1 to C959.6 For-
mally speaking, a context-identifier is a terminal
like any other word of the language and we can
therefore prefix every comment in the training data
with the context-identifier standing for the set of
MRs associated with this comment, an idea taken
from previous work such as Johnson et al (2010).
Thus having incorporated the contextual informa-
tion into the string, we go on to show how our model
makes use of this information, considering the MR
pass(pink1,pink11) as an example. A formal de-
scription of the model is given Figure 3.
Assume that pass(pink1,pink11) is associated
with only one training example and therefore occurs
only in one specific context. If the context-identifier
introduced for this context is C1, we require the
6If we were to consider every possible context, we would
have to consider 22048 contexts because the MG generates 2048
MRs.
1419
Root
Spass(pink1,pink11)
Phrasepink11
PINK11
Phrasepass
Wordpass
TO
PhXpass
Word?
BALL
PhXpass
Word?
THE
PhXpass
Wordpass
PASSES
Phrasepink1
THE PINK GOALIE
C76
Figure 2: The tree-structure we propose instead of the Hybrid Tree structure used by (Kim and Mooney, 2010). The
non-terminal nodes do not correspond to MG productions, but to MR constituents. The internal structure of the
Phrasecon constituents, shown in full detail for Phrasepass, corresponds to a Markov process that generates the wordsthat make up the sentence. The terminal C76 is a context-identifier that restricts the range of Sm non-terminals thatmight dominate the sentence and is only used during training, as described in Section 3.1. The grammar that generates
this trees is described in Figure 3.
right-hand side of all rules with Spass(pink1,pink11) on
their left-hand side to begin with C1. More gener-
ally, if an MR m occurs in the contexts associated
with the context-identifiers CK,...,CL, we require the
right-hand side of all rules with Sm on their left-hand
side to begin with exactly one of CK,...,CL.
In this sense, the context-identifiers can be seen
as providing the model with a top-down constraint
? if it encounters a context-identifier, it can only
try analyses leading to MRs which are licensed by
this context-identifier. On the other hand, the words
have to be generated by concept-specific word-
distributions, and the concepts that are present re-
strict the range of possible Sm non-terminals which
might dominate the whole string. In this sense, the
words the model observes provide it with a bottom-
up constraint ? if it sees words which are semanti-
cally related to certain concepts con1,...,conn, it has
to arrive at an MR which licenses the presence of the
corresponding Phraseconx non-terminals. Of course,the model has to also learn which words are seman-
tically related to which concepts. To enable it to do
this, our grammar allows every Wordx non-terminal
to be rewritten as every word of the language.
Since there are sentences in the training data with-
out the correct meaning in their context, we want
to give our model the possibility of not assigning to
a sentence any of the MRs licensed by its context-
identifier. To do this, we employ another trick of
previous work by Johnson et. al and assume a spe-
cial null meaning ? to be present in every context.
S? may only span words generated by Word?, the
language-specific distribution for words not directly
related to any concept; this also has to be learned by
the model.
As a last complication, we deal with the fact that
syntactic constituents are linearized with respect to
each other. For example, if an MR has 3 proper con-
stituents (i.e. excluding the MR itself), our grammar
allows the corresponding 3 syntactic constituents ?
which we might label Phrasepredicate, Phrasearg1
and Phrasearg2 ? to occur in any of the 6 possible
orders. Therefore, we have an Sm rule for every con-
text in which m occurs and for every possible order
of the proper constituents of m.
A formally explicit description of the rule
1420
schemata used to generate the CFG is given in Fig-
ure 3.7 Instantiating all those schemata leads to a
grammar with 33,101 rules for the English data and
30,731 rules for the Korean data. The difference in
size is due to differences in the size of the vocabu-
lary and the different number of contexts in the data
sets.
These CFGs can now be trained on the training
data using the Inside-Outside algorithm (Lari and
Young, 1990). After training, the resulting PCFG
embodies a semantic parser in the sense that, with
a slight modification we describe in section 3.3, it
can be used to parse a string into its meaning rep-
resentation by determining the most likely syntactic
analysis and reading off the meaning assigned by our
model at the Sm-node.
3.2 Possible objections to our reduction
Before we go on to discuss the details of training
and evaluation of our model, we want to address an
objection that might seem tempting. Isn?t our reduc-
tion impractical and unrealistic as even a highly ab-
stract model of language learning ? after all, setting
up the huge CFG requires knowledge about the vo-
cabulary, the MG and all the complicated rules dis-
cussed which, presumably, is more knowledge than
we want to provide a language learner with, lest we
trivialize the task. To this we reply firstly, that it is
true that our reduction only works for offline or batch
grounded learning tasks where all the data is avail-
able to the model before the actual learning begins
so that it ?knows? the words, the meanings and the
contexts present in the data. This offline constraint
is, however, true of all models which are trained by
iterating multiple times over training data such as
KM?s model. Secondly, the intimidating CFG can in
principle be reduced to a hand-full of intuitive prin-
ciples and is easy to generate automatically.
First of all, the many specific Sm-rewrite rules re-
duce to the heuristic that every semantic constituent
should correspond to a syntactic constituent, and the
fact that natural language expressions are linearly or-
dered. Note that our model does not contain knowl-
edge about the specific word order of the language.
7In our description, we use context-identifiers such as C1with a systematic ambiguity, letting them stand for the terminal
symbol representing a context and, in contexts such as m?C1,for the represented context itself.
It simply allows for the constituents of an MR to oc-
cur in every possible order which is a very unbiased
and empiricist assumption. Of course, this leads to
some limited kind of ?implicit learning? of word or-
der in the sense that for every meaning and for every
context, our model might (and in most cases will) as-
sign different probabilities to the different rules for
every word order; so it can learn that certain specific
MRs such as pass(pink1,pink11) are more often lin-
earized in one way than in any other. It cannot, how-
ever, generalize this to other (or even unseen) MRs,
i.e. it does not learn a global fact about the language.
In a way, it lacks the knowledge that there is such a
thing as word order, a point which we will elaborate
on in Section 4.
The many re-write rules for the pre-terminal
Wordxs are nothing but an explicit version of the
assumption that every word the model encounters
might, in principle, be semantically related to every
concept it knows. Again, this seems to us to be a
reasonable assumption.
Finally, the complicated looking set of rules for
the internal structure of Phrasexs corresponds to
a simple unigram Markov-process for generating
strings. All in all, we do not see that we make any
more assumptions than other approaches; our for-
mulation may make explicit how rich those assump-
tions are but we have not qualitatively changed them.
3.3 Training and Evaluation
The CFG described in the previous section is trained
on the same training data used by KM, except that
we reduce it to strings (without changing the infor-
mation present in the original data) by prefixing ev-
ery sentence with a context-identifier. For training
we run the Inside-Outside algorithm8 with uniform
initialization weights until convergence. For En-
glish, this results in an average number of 76 itera-
tions for each fold, for Korean the average number of
iterations is 50. To deal with the fact that the model
might not observe certain meanings during training,
we apply a simple smoothing technique by using a
Dirichlet prior of ?=0.1 on the rule probabilities. In
effect, this provides our system with a small number
of pseudo-observations for each rule which prevents
8We use Mark Johnson?s freely available implementa-
tion, available at http://web.science.mq.edu.au/
~mjohnson/Software.htm.
1421
Root? Sm m ?M ? {?}
Sm ? c Phrasep(m) c ? C,m ? c,m ? Pred0(M)
Sm ? c {Phrasep(m), Phrasea1(m)} c ? C,m ? c,m ? Pred1(M)
Sm ? c {Phrasep(m), Phrasea1(m), Phrasea2(m)} c ? C,m ? c,m ? Pred2(M)
S? ? c Phrase? c ? C
Phrase? ?Word?
Phrase? ? Phrase?Word?
Phrasex ?Wordx x ? T
Phrasex ? PhXxWordx x ? T
Phrasex ? PhxWord? x ? T
PhXx ?Wordr x ? T, r ? {x, ?}
PhXx ? PhXxWordr x ? T, r ? {x, ?}
Phx ? PhXxWordx x ? T
Phx ? PhxWord? x ? T
Phx ?Wordx x ? T
Wordx ? v x ? T ? {?}, v ? V
Figure 3: The rule-schemata used to generate the NoWo-PCFG. Root is the unique start-symbol, M is the set of all
MRs present in the corpus, C is set the of all context-identifiers present in the corpus, T is the set of terminals of the
MG, V is the vocabulary of the corpus. Pred0(M) is the subset of all MRs in M of the form predicate, Pred1(M)
is the subset of all MRs in M of the form predicate(arg1) and Pred2(M) is the subset of all MRs in M of the form
predicate(arg1,arg2). p(m) is the predicate of the MR m, a1(m) is the first argument of the MR m, a2(m) is the
second argument of the MR m. The rules expanding Phrasex ensure that it contains at least one Wordx. A set on theright-hand side of a rule is shorthand for all possible orderings of the elements of the set.
the automatic assignment of zero probability to rules
not used during training.9
For parsing, the resulting PCFG is slightly mod-
ified by removing the context-identifiers. This is
done because the task of a semantic parser is to es-
tablish a mapping between NLs and MRs, irrespec-
tive of contexts which were only used for learning
the parser and should not play a role in its final per-
formance. To do this, we add up the probability of
all rules which differ only in the context-identifier
which can be thought of as marginalizing out the dif-
ferent contexts, giving our first model which we call
NoWo-PCFG.10
Note that the context-deletion (and the simple
smoothing) enables NoWo-PCFG to parse sentences
into meanings not present in the data it was trained
on which, in fact, happens. For example, there are
81 meanings in the training data for the first English
9We experimented with ?=0.1, ?=0.5 and ?=1.0 and found
that overall, 0.1 yields the best results. We also tried jittering
the initial rule weights during training and found that our re-
sults are very robust and seem to be independent of a specific
initialization.
10NoWo because this model, unlike the one described in Sec-
tion 4, does not make explicit use of word order generalisa-
tions.
match that are not present in any of the other games?
training data. The PCFG trained on games 2, 3 and 4
is still able to correctly assign 12 of those 81 mean-
ings which it has not seen during the training phase
which shows the effectiveness of the bottom-up con-
straint.
For evaluation, we employ 4-fold cross validation
as described in detail in Chen and Mooney (2008)
and used by KM: the model is trained on all possible
combinations of 3 of the 4 games and is then used
to produce an MR for all sentences of the held-out
game for which there is a matching gold-standard
meaning. For an NL W, our model produces an MR
m by finding the most probable parse of W with the
CKY algorithm and reading off m at the Sm-node.11
An MR is considered correct if and only if it matches
the gold-standard MR exactly; the final evaluation
result is averaged over all 4 folds. Our evaluation
results for NoWo-PCFG are given in Table 2. All
scores are reported in F-measure which is the har-
monic mean of Precision and Recall. In this specific
case, precision is the fraction of correct parses out
11For parsing, we use Mark Johnson?s freely available CKY
implementation which can be downloaded at http://web.
science.mq.edu.au/~mjohnson/Software.htm.
1422
English Korean
KM 0.742 0.764
KM ?supervised? 0.810 0.808
Chen et al (2010) 0.801 0.812
NoWo-PCFG 0.742 0.718
WO-PCFG 0.860 0.829
Table 2: A summary of results for the parsing task, in F-
measure. We also show the results of Chen et al (2010),
as given in Kim and Mooney (2010), which to our knowl-
edge are the highest previously reported scores for Ko-
rean. WO-PCFG, described in Section 4 performs better
than all previously reported models, but only slightly so
for Korean.
of the total number of parses the model returns. Re-
call is the fraction of correct parses out of the total
number of test sentences.12
NoWo-PCFG performs a little worse than KM?s
model. Its scores are virtually identical for English
(0.742) and worse for Korean (0.718 vs 0.764). We
are not sure as to why our model performs worse on
the Korean data, but it might have to do with the fact
that the Korean average ambiguity is higher than for
the English data.
This shows that it is not only possible to re-
duce the task of learning a semantic parser to stan-
dard grammatical inference, but that this way of ap-
proaching the problem yields comparable results.
The remainder of the paper focuses on our second
main point: that letting the model learn additional
kinds of information, such as the language?s canoni-
cal word order, can further improve its performance.
In order to do this we propose a model that learns
the word order as well as the mapping from NLs
to MRs, and compare its performance to that of the
other models.
4 Extending NoWo-PCFG to WO-PCFG
We already pointed out that our model considers ev-
ery possible linear order of syntactic constituents.
Our NoWo-PCFG model considers each of the pos-
sible word orders for every meaning and context in
isolation: it is unable to infer from the fact that most
meanings it has observed are most likely to be ex-
pressed with a certain word order that new meanings
12Because our model parses every sentence, for it Recall and
Precision are identical and F-measure is identical to Accuracy.
it will encounter are also more likely to be expressed
with this word order. It seems, however, to be at
least a soft fact about languages that they do have
a canonical word order that is more likely to be re-
alized in its sentences than any other possible word
order. In order to test whether trying to learn this
order helps our model, we modify the CFG used for
NoWo-PCFG so it can learn word order generaliza-
tions, and train it in the same way to yield another
semantic parser, WO-PCFG.
4.1 Setting up WO-PCFG
For every possible ordering of the constituents cor-
responding to an MR, our grammar contains a rule.
In NoWo-PCFG, these different rules all share the
same parent which prevents the model from learn-
ing the probability of the different word orders cor-
responding to the many rules. A straight-forward
way to overcome this is to annotate every Sm node
with the word order of its daughter. We split every
Sm non-terminal in multiple Swo_m non-terminals,
where wo ? {v,sv,vs,svo,sov,osv,ovs,vso,vos} indi-
cates the linear order of the constituents the non-
terminal rewrites as.13
This in itself does not yet alow our model to use
word order as a means of generalization. To model
that whenever it encounters a specific example that
is indicative of a certain word order, this word or-
der becomes slightly more probable for every other
example as well, we have to make a further slight
change to the CFG which we now describe. A for-
mally explicit description of the necessary changes
which we go on to describe is given in Figure 4.
We introduce six new non-terminals, correspond-
ing to the six possible word orders SVO, SOV, VSO,
VOS, OSV and OVS and require every Swo_m non-
terminal to be dominated by the non-terminal com-
patible with its daughters linear order. As an exam-
ple, consider the two syntactic non-terminals cor-
responding to the MR kick(pink1), Svs_kick(pink11)
and Ssv_kick(pink11). Whenever an example is suc-
cessfully analyzed as Svs_kick(pink11), this should
strengthen our model?s expectation of encountering
13We assume, somewhat simplifying, that an MR?s predicate
corresponds to a V(erb), its first argument corresponds to the
S(ubject) and its second argument corresponds to the O(bject).
These are purely formal categories that are not constrained to
correspond to specific linguistic categories.
1423
Root? wo wo ?WO
wo? Sx_m wo ?WO,x ?WOS, x ? wo,m ?M
Sv_m ? c Phrasep(m) c ? C,m ? c,m ? Pred0(M)
Sx_m ? c {Phrasep(m), Phrasea1(m)} c ? C,m ? c,m ? Pred1(M), x ? {sv, vs}
Sx_m ? c {Phrasep(m), Phrasea1(m), Phrasea2(m)} c ? C,m ? c,m ? Pred2(M), x ?WOS
Sv_? ? c Phrase? c ? C
Figure 4: In order to turn NoWo-PCFG described in Figure 3 into the WO-PCFG described in the
text, substitute the first five rule-schemata with the schemata given here. WO is the set of word
order non-terminals {SV O, SOV,OSV,OV S, V SO, V OS}, WOS is the set of word order annotations
{v, sv, vs, svo, svo, ovs, osv, vso, vos}. We take x ? wo to mean that x is compatible with wo, where v is com-
patible with all word orders, sv is compatible with SVO,SOV and OSV, and so on. For rule-schemata 4 and 5, the
choice of x determines the order of the elements of the set on the right-hand side. All other symbols have the same
meaning as explained in Figure 3.
more examples where the verb precedes the sub-
ject, i.e. of the language being pre-dominantly VSO,
VOS or OVS. Therefore, we allow VSO, VOS and
OVS to be rewritten as Svs_kick(pink11). More gener-
ally, every word order non-terminal can rewrite as
any of the Swo_m non-terminals that are compatible
with it. Adding this additional layer of word order
abstraction leads to a grammar with 36,019 rules for
English and a grammar with 33,715 rules for Ko-
rean.
4.2 Evaluation of WO-PCFG
Training and evaluating WO-PCFG in exactly the
same way as the previous grammar gives an F-
measure of 0.860 for English and an F-measure of
0.829 for Korean. Those scores are, to our knowl-
edge, the highest scores previously reported for this
parsing task and establish our second main point:
letting the model learn the language?s word order in
addition to learning the mapping from sentences to
MR increases semantic parsing accuracy.14
An intuitive explanation for the increase in perfor-
mance is that by allowing the model to learn word
order, we are providing it with a new dimension
along which it can generalize.
In this sense, we can look at our refinement as
providing the model with abstract linguistic knowl-
edge, namely that languages tend to have a canon-
14Liang et al (2009)?s model can be seen as capturing some-
thing similar to our word order generalization with the help of
a Field Choice Model which primarily captures discourse co-
herence and salience properties. It differs, however, in that it
can only learn one generalization for each predicate type and
no language wide generalization.
ical word order. The usefulness of this kind of in-
formation is impressive ? for English, it improves
the accuracy of semantic parsing by almost 12% in
F-measure and for Korean by 11.1%. In addition,
our model correctly learns that English?s predomi-
nant word order is SVO and that Korean is predomi-
nantly SOV, assigning by far the highest probability
to the corresponding Root rewrite rule (0.91 for En-
glish and 0.98 for Korean). This kind of information
is useful in its own right and could, for example, be
exploited by coupling word order with other linguis-
tic properties, perhaps following Greenberg (1966)?s
implicational universals.
In this sense, the reduction of grounded learning
problems to grammatical inference does not only
make possible the application of a wide variety of
tools and insights developed over years of research,
it might also make it easier to bring abstract (and not
so abstract) linguistic knowledge to bear on those
tasks.
The overall slightly worse performance of our
system on Korean data might stem from the fact that
Korean, unlike English, has a rich morphology, and
that our model does not learn anything about mor-
phology at all. We plan on further investigating ef-
fects like this in the future, as well as applying more
advanced grammatical inference algorithms.
5 Conclusion and Future Work
We have shown that certain grounded learning tasks
such as learning a semantic parser from semantically
enriched training data can be reduced to a gram-
matical inference problem over strings. This allows
1424
for the application of techniques and insights devel-
oped for grammatical inference to grounded learn-
ing tasks. In addition, we have shown that letting
the model learn the language?s canonical word or-
der improves parsing performance, beyond the top
scores previously reported, thus illustrating the use-
fullnes of linguistic knowledge for tasks like this.
In future research, we plan to address the limi-
tation of our model to a finite set of meaning rep-
resentations, in particular through the use of non-
parametric Bayesian models such as the Infinite
PCFG model of Liang et al (2007) and the Infi-
nite Tree model of Finkel et al (2007); both allow
for a potentially infinite set of non-terminals, hence
directly addressing this problem. In addition, we
are thinking about using an extension of the PCFG
formalism that allows for some kind of ?feature-
passing? which could lead to much smaller and more
general grammars.
References
N. C. Chang and T. V. Maia. 2001. Grounded learning
of grammatical constructions. In 2001 AAAI Spring
Symposium on Learning Grounded Representations.
David L. Chen and Raymond J. Mooney. 2008. Learning
to sportscast: A test of grounded language acquisition.
In Proceedings of the 25th International Conference
on Machine Learning (ICML).
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Jenny R. Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 272?279.
Joseph H. Greenberg. 1966. Some universals of gram-
mar with particular reference to the order of meaning-
ful elements. In Joseph H. Greenberg, editor, Univer-
sals of Language, chapter 5, pages 73?113. The MIT
Press, Cambridge, Massachusetts.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems
23, pages 1018?1026.
Joohyun Kim and Raymond J. Mooney. 2010. Genera-
tive alignment and semantic parsing for learning from
ambiguous supervision. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010).
K. Lari and S.J. Young. 1990. The estimation of Stochas-
tic Context-Free Grammars using the Inside-Outside
algorithm. Computer Speech and Language, 4(35-56).
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688?697.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the 47th Annual Meeting of the
ACL and the 4th IJCNLP of the AFNLP.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natu-
ral language to meaning representations. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 783?792.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of UAI 2005.
1425
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 501?509,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Words and Their Meanings from Unsegmented Child-directed
Speech
Bevan K. Jones & Mark Johnson
Dept of Cognitive and Linguistic Sciences
Brown University
Providence, RI 02912, USA
{Bevan Jones,Mark Johnson}@Brown.edu
Michael C. Frank
Dept of Brain and Cognitive Science
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
mcfrank@mit.edu
Abstract
Most work on language acquisition treats
word segmentation?the identification of lin-
guistic segments from continuous speech?
and word learning?the mapping of those seg-
ments to meanings?as separate problems.
These two abilities develop in parallel, how-
ever, raising the question of whether they
might interact. To explore the question, we
present a new Bayesian segmentation model
that incorporates aspects of word learning and
compare it to a model that ignores word mean-
ings. The model that learns word meanings
proposes more adult-like segmentations for
the meaning-bearing words. This result sug-
gests that the non-linguistic context may sup-
ply important information for learning word
segmentations as well as word meanings.
1 Introduction
Acquiring a language entails mastering many learn-
ing tasks simultaneously, including identifying
where words begin and end in continuous speech
and learning meanings for those words. It is com-
mon to treat these tasks as separate, sequential pro-
cesses, where segmentation is a prerequisite to word
learning but otherwise there are few if any depen-
dencies. The earliest evidence of segmentation,
however, is for words bordering a child?s own name
(Bortfeld et al, 2005). In addition, infants begin
learning their first words before they achieve adult-
level competence in segmentation. These two pieces
of evidence raise the question of whether the tasks of
meaning learning and segmentation might mutually
inform one another.
To explore this question we present a joint model
that simultaneously identifies word boundaries and
attempts to associate meanings with words. In do-
ing so we make two contributions. First, by model-
ing the two levels of structure in parallel we simu-
late a more realistic situation. Second, a joint model
allows us to explore possible synergies and interac-
tions. We find evidence that our joint model per-
forms better on a segmentation task than an alterna-
tive model that does not learn word meanings.
The picture in Figure 1 depicts a language learn-
ing situation from our corpus (originally from Fer-
nald and Morikawa, 1993; recoded in Frank et al,
2009) where a mother talks while playing with var-
ious toys. Setting down the dog and picking up the
hand puppet of a pig, she asks, ?Is that the pig??
Starting out, a young learner not only does not know
that the word ?pig? refers to the puppet but does not
even know that ?pig? is a word at all. Our model
simulates the learning task, taking as input the un-
segmented phonemic representation of the speech
along with the set of objects in the non-linguistic
context as shown in Figure 1 (a), and infers both a
segmentation and a word-object mapping as in Fig-
ure 1 (b).
One can formulate the word learning task as
that of finding a reasonably small set of reusable
word-meaning pairs consistent with the underlying
communicative intent. Infant directed speech often
refers to objects in the immediate environment, and
early word learning seems to involve associating fre-
quently co-occurring word-object pairs (Akhtar and
Montague, 1999; Markman, 1990). Several compu-
tational models are based on this idea that a word
501
Figure 1: (a) The input to our system for the utterance
?Is that the pig?? consists of an unsegmented sequence
of phonemes and the set of objects representing the non-
linguistic context. These objects were manually iden-
tified by inspecting the associated video, a frame from
which is shown above. (b) The gold-standard segmenta-
tion and word-object assignments of the same utterance,
against which the output of our system is evaluated (all
words except ?pIg? are mapped to a special ?null? object,
as explained in the text).
that frequently occurs in the presence of an object
and not so frequently in its absence is likely to re-
fer to that object (Frank et al, 2009a; Siskind, 1996;
Yu and Ballard, 2007). Importantly, all these models
assume words are pre-segmented in the input.
While the word segmentation task relates less
clearly to the communicative content, it can be for-
mulated according to a similar objective, that of at-
tempting to explain the sound sequences in the input
in terms of some reasonably small set of reusable
units, or words. Computational models have suc-
cessfully addressed the problem in much this way
(Johnson and Goldwater, 2009; Goldwater et al,
2009; Brent, 1999), and the general approach is con-
sistent with experimental observations that humans
are sensitive to statistics of sound sequences (Saffran
et al, 1996; Frank et al, 2007).
The two tasks can be integrated in a relatively
seamless way, since, as we have just formulated
them, they have a common objective, that of finding
a minimal, consistent set of reusable units. However,
the two deal with different types of information with
different dependencies. The basic idea is that learn-
ing a vocabulary that both meets the constraints of
the word-learning task and is consistent with the ob-
jective of the segmentation task can yield a better
segmentation. That is, we hope to find a synergy in
the joint inference of meaning and segmentation.
Note that to the best of our knowledge there is
very little computational work that combines word
form and word meaning learning (Frank et al 2006
takes a first step but their model is applicable only
to small artificial languages). Frank et al (2009a)
and Regier (2003) review pure word learning mod-
els and, in addition to the papers we have already
cited, Brent (1999) presents a fairly comprehensive
review of previous pure segmentation models. How-
ever, none of the models reviewed make any attempt
to jointly address the two problems. Similarly, in the
behavioral literature on development, we are aware
of only one segmentation study (Graf-Estes et al,
2007) that involves non-linguistic context, though
this study treats the two tasks sequentially rather
than jointly.
We now describe our model and inference proce-
dure and follow with evaluation and discussion.
2 Model Definition
Cross-situational meaning learning in our joint word
learning and segmenting model is inspired by the
model of Frank et al (2009a). Our model can
be viewed as a variant of the Latent Dirichlet Al-
location (LDA) topic model of Blei et al (2003),
where topics are drawn from the objects in the non-
linguistic context. The model associates each utter-
ance with a single referent object, the topic, and ev-
ery word in the utterance is either generated from a
distribution over words associated with that object
or else from a distribution associated with a special
?null? object shared by all utterances. Note that in
this paper we use ?topic? to denote the referent ob-
ject of an utterance, otherwise we depart from topic
modeling convention and use the term ?object? in-
stead.
Segmentation is based on the unigram model pro-
posed by Brent (1999) and reformulated by Goldwa-
ter et al (2009) in terms of a Dirichlet process. Since
both LDA and the unigram segmenter are based on
unigram distributions it is relatively straightforward
502
Figure 2: Topical Unigram Model: Oj is the set of objects
in the non-linguistic context of the jth utterance, zj is the
utterance topic, wji is the ith word of the utterance, xji is
the category of the word (referring or non-referring), and
the other variables are distribution parameters.
to integrate the two to simultaneously infer word
boundaries and word-object associations.
Figure 2 illustrates a slightly simplified form of
the model, and the the relevant distributions are as
follows:
z|O ? Uniform(O)
Gz|z, ?0, ?1, P0 ?
{
DP(?1, P0) if z 6= 0
DP(?0, P0) otherwise
pi ? Beta(1, 1)
x|pi ? Bernoulli(pi)
w|G, z, x ?
{
Gz if x = 1
G0 if x = 0
Note that Uniform(O) denotes a discrete uniform
distribution over the elements of the set O. P0 is
described later.
Briefly, each utterance has a single topic zj , drawn
from the objects in the non-linguistic context Oj ,
and then for each word wji we first flip a coin xji
to determine if it refers to the topic or not. Then, de-
pending on xji the word is either drawn from a dis-
tribution specific to the topic (xji = 1) or from a dis-
tribution associated with the ?null? object (xji = 0).
In slightly greater detail but still glossing over the
details of how the multinomial parameters are gen-
erated, the generative story proceeds as follows:
1. For each utterance, indexed by j
2. (a) Pick a single topic zj uniformly from the set
of objects in the environment Oj
(b) For each word wji of the utterance
(c) i. Determine if it refers to zj or not by set-
ting xji to 1 (referring) with probability pi,
and to 0 (non-referring) otherwise.
ii. if xji is 1, draw wji from the topic specific
distribution over words Gzj .
iii. otherwise, draw wji from G0, the distribu-
tion over words associated with the ?null?
object.
This generative story is a simplification since it
does not describe how we model utterance bound-
aries. It is important for segmentation purposes
to explicitly model utterance boundaries since, un-
like utterance-internal word boundaries, we as-
sume utterance boundaries are observed. Thus,
the story is complicated by the fact that there is
a chance each time we generate a word that we
also generate an utterance boundary. The choice of
whether to terminate the utterance or not is captured
by a Bernoulli(?) random variable $ji indicating
whether the ith word was the last word of the jth
utterance.
? ? Beta(1, 1)
$|? ? Bernoulli(?)
The Gz multinomial parameters are generated
from a Dirichlet process with base distribution over
words, P0, which describes how new word types
are generated from their constituent phonemes.
Phonemes are generated sequentially, i.i.d. uni-
formly from m phonemic types. In addition, there
is a probability p# of generating a word boundary.
P0(w) = (1? p#)|w|?1p#
1
m|w|
The concentration parameters ?0 and ?1 also play
a critical role in the generation of words and word
types. Any given word has a certain probability
of either being produced from the set of previously
seen word types, or from an entirely new one. The
503
greater the concentration parameter, the more likely
the model is to appeal to the base distribution P0 to
introduce a new word type.
Like Frank et al (2009a), we distinguish between
two coarse grammatical categories, referring and
non-referring. Referring words are generated by the
topic, while non-referring words are drawn from G0,
a distribution associated with the ?null? object. The
distinction ensures sparse word-object maps that
obey the principle of mutual exclusion. Otherwise
all words in the utterance would be associated with
the topic object, resulting in a very large set of words
for each object that is very likely to overlap with the
words for other objects. As a further bias toward
a small lexicon, we employ different concentration
parameters (?0 and ?1) for the non-referring and re-
ferring words, using a much smaller value for the
referring words. Intuitively, there should be a rela-
tively small prior probability of introducing a new
word-object pair, corresponding to a small ?1 value.
On the other hand, most other words don?t refer to
the topic object (or any other object for that matter),
corresponding to a much larger ?0 value.
Note that this topical unigram model is a straight-
forward generalization of the unigram segmentation
model (Goldwater et al, 2009) to the case of multi-
ple topics. In fact, if all words were assumed to refer
to the same object (or to no object at all) the models
would be identical.
Unlike LDA, each ?document? has only one topic,
which is necessitated by the fact that in our model
documents correspond single utterances. The ut-
terances in our corpus of child directed speech are
often only four or five words long, whereas the
general LDA model assumes documents are much
larger. Thus, there may not be enough words to in-
fer a useful utterance specific distribution over top-
ics. Consequently, rather than inferring a separate
topic distribution for each utterance, we simply as-
sume a uniform distribution over objects in the non-
linguistic context. In effect, we rely entirely on the
non-linguistic context and word-object associations
to infer topics. Though necessitated by data sparsity
issues, we also note that it is very rare in our cor-
pus for utterances to refer to more than one object in
the non-linguistic context, so the choice of a single
topic may also be a more accurate model. In fact,
even with multi-sentence documents, LDA may per-
form better if only one topic is assumed per sentence
(Gruber et al, 2007).
3 Inference
We use a collapsed Gibbs sampling procedure, in-
tegrating over all possible Gz , pi, and ? values and
then iteratively sample values for each variable con-
ditioned on the current state of all other variables.
We visit each utterance once per iteration, sample a
topic, and then visit each possible word boundary lo-
cation to sample the boundary and word categories
simultaneously according to their joint probability.
A single topic is sampled for each utterance, con-
ditioned on the words and their current determina-
tions as referring or non-referring. Since zj is drawn
from a uniform distribution, this probability is sim-
ply proportionate to the conditional probability of
the words given zj and the xji variables.
P (zj |wj,xj,h?j) ?
?(?Wjw n
(h?)
w,zj + ?1P0(w))
?(?Wjw n
(h)
w,zj + ?1P0(w))
?
Wj
?
w
?(n(h)w,zj + ?1P0(w))
?(n(h?)w,zj + ?1P0(w))
Here, P (zj |wj,xj,h?j) is the probability of topic
zj given the current hypothesis h for all variables ex-
cluding those for the current utterance. Also, n(h
?j)
w,zj
is the count of occurrences of word type w that refer
to topic zj among the current variable assignments,
and Wj is the set of word types appearing in utter-
ance j. The vectors of word and category variables
in utterance j are represented as wj and xj, respec-
tively. Note that only referring words have any bear-
ing on the appropriate selection of zj and so all fac-
tors involving only non-referring words are absorbed
by the constant of proportionality.
The word categories can be sampled conditioned
on the current word boundary states according to the
following conditional probability, where n(h
?ji)
xji is
the number of words categorized according to label
504
xji over the entire corpus excluding word wji.
P (xji|wji, zj ,h?ji) ? P (wji|zj , xji,h?ji)
?P (xji|h?ji)
=
n(h
?ji)
wji,xjizj + ?xjiP0(wji)
n(h
?ji)
?,xjizj + ?xji
? n
(h?ji)
xji + 1
n(h
?ji)
? + 2
(1)
In practice, however, we actually sample the word
category variables jointly with the boundary states,
using a scheme similar to that outlined in Gold-
water et al (2009). We visit each possible word
boundary location (any point between two consec-
utive phonemes) and compute probabilities for the
hypotheses for which the phonemic environment
makes up either one word or two. As illustrated be-
low there are two sets of cases: those where we treat
the segment as a single word, and those where we
treat it as two words.
x1 x2 x3
. . .#w1#. . . vs. . . .#w2#w3#. . .
? ?
The probabilities of the hypotheses can be derived
by application of equation 1. Since the x variables
can each describe two possible events, there are a to-
tal of six different cases to consider for each bound-
ary assignment: two cases without and four with a
word boundary.
The probability of each of the two cases without
a word boundary can be computed as follows:
P (w1, x1|z,h?) =
n(h
?)
w1,x1z + ?x1P0(w1)
n(h
?)
?,x1z + ?x1
?n
(h?)
x1 + 1
n(h
?)
? + 2
?
n(h
?)
$1 + 1
n(h
?)
? + 2
Here h? signifies the current hypothesis for all
variables excluding those for the current segment
and n(h
?)
$1 is the count for h
? of either utterance fi-
nal words if w1 is utterance final or non-utterance
final words if w1 is also not utterance final.
In the four cases with a word boundary, we have
two words and two categories to sample.
P (w2, x2, w3, x3|z,h?) =
n(h
?)
w2,x2z + ?x2P0(w2)
n(h
?)
?,x2z + ?x2
?n
(h?)
x2 + 1
n(h
?)
? + 2
?
n(h
?)
$2=0 + 1
n(h
?)
? + 2
?n
(h?)
w3,x3z + ?x2(x3)?w2(w3) + ?x3P0(w3)
n(h
?)
?,x3z + ?x2(x3) + ?x3
?n
(h?)
x3 + ?x2(x3) + 1
n(h
?)
? + 3
?
n(h
?)
$3 + ?$2($3) + 1
n(h
?)
? + 3
Here ?x(y) is 1 if x = y and 0 otherwise.
4 Results & Model Comparisons
4.1 Corpus
Our training corpus (Fernald and Morikawa, 1993;
Frank et al, 2009b) consists of about 22,000 words
and 5,600 utterances. Video recordings consisting
of mother-child play over pairs of toys were ortho-
graphically transcribed, and each utterance was an-
notated with the set of objects present in the non-
linguistic context. The object referred to by the ut-
terance, if any, was noted, as described in Frank et al
(2009b). We used the VoxForge dictionary to map
orthographic words to phoneme sequences in a pro-
cess similar to that described in Brent (1999).
Figure 1 (a) presents an example of the coding
of phonemic transcription and non-linguistic context
for a single utterance. The input to the system con-
sists solely of the phonemic transcription and the ob-
jects in the non-linguistic context.
4.2 Evaluation
We ran the sampler ten times for 100,000 iterations
with parameter settings of ?1 = 0.01, ?0 = 20, and
p# = 0.5, keeping only the final sample for evalu-
ation. We defined the word-object pairs for a sam-
ple as the words in the referring category that were
paired at least once with a particular topic. These
pairs were then compared against a gold standard
set of word-object pairs, while segmentation perfor-
mance was evaluated by comparing the final bound-
ary assignments against the gold standard segmenta-
tion.
505
4.2.1 Word Learning
To explore the contribution of word boundaries
to the joint word learning and segmenting task, we
compare our full joint model against a variant that
only infers topics, using the gold standard segmen-
tation as input. In this way we also reproduce the
usual assumption of a sequential relationship be-
tween segmentation and word learning and test the
necessity of the simplifying assumption. The re-
sults are shown in Table 2. We compare them with
three different metric types: topic accuracy; preci-
sion, recall, and F-score of the word-object pairs;
and Kullback-Liebler (KL) divergence.
First, treating utterances with no referring words
as though they have no topic, we compute the ac-
curacy of the inferred topics. Note that we don?t
report accuracy for the the variant with no non-
linguistic context, since in this case the objects are
interchangeable, and we have a problem identifying
which cluster corresponds to which topic. Table 2
shows that the joint segmentation and word learning
model gets the topic right for 81% of the utterances.
The variant that assumes pre-segmented input does
comparably well with an accuracy of 79%. Surpris-
ingly, it seems that knowing the gold segmentation
doesn?t add very much, at least for the topic infer-
ence task.
To evaluate how well we discovered the word-
object map, we manually compiled a list of all the
nouns in the corpus that named one of the 30 ob-
jects. We used this set of nouns, cross-referenced
with their topic objects, as a gold standard set of
word-object pairs. By counting the co-occurrences,
we also compute a gold standard probability distri-
bution for the words given the topic, P (w|z, x = 1).
Precision, recall and F-score are computed as per
Frank et al (2009a). In particular, precision is the
fraction of gold pairs among the sampled set and re-
call is the fraction of sampled pairs among the gold
standard pairs.
p = |Sampled ? Gold||Sampled| , r =
|Sampled ? Gold|
|Gold|
KL divergence is a way of measuring the differ-
ence between distributions. Small numbers gener-
ally indicate a close match and is zero only when
the two are equal. Using the empirical distribution
Object Words
BOX thebox box
BRUSH brush
BUNNY rabbit Rosie
BUS bus
CAR car thecar
CHEESE cheese
DOG thedoggy doggy
DOLL doll thedoll yeah benice
DOUGH dough
ERNIE Ernie
Table 1: Subset of an inferred word-object mapping. For
clarity, the proposed words have been converted to stan-
dard English orthography.
p r f KL acc
Joint 0.21 0.45 0.28 2.78 0.81
Gold Seg 0.21 0.60 0.31 1.82 0.79
Table 2: Word Learning Performance. Comparing
precision, recall, and F-score of word-object pairs,
DKL(P (w, z)||Q(w, z)), and accuracy of utterance top-
ics for the full joint model and a variant that only infers
meanings given a gold standard segmentation.
over gold topics P (z), we use the standard formula
for KL divergence to compare the gold standard dis-
tribution P against the inferred distribution Q. I.e.,
we compute DKL(P (w, z)||Q(w, z)).
The model learns fairly meaningful word-object
associations; results are shown in Table 2. As in the
case of topic accuracy, the joint and word learning
only variants perform similarly, this time with some-
what better performance for the easier task with an
F-score and KL divergence of 0.31 and 1.82 vs. 0.28
and 2.78 for the joint task.
Table 1 illustrates the sort of word-object pairs
the model discovers. As can be seen, many of the
errors are due to the segmentation, usually under-
segmentation errors where it segments two words as
one. This is a general problem with the unigram seg-
menter on which our model is based (Goldwater et
al., 2009). Yet, even though these segmentation er-
rors are also counted as word learning errors, they
are often still meaningful in the sense that the true
referring word is a subsequence.
So, word segmentation has an impact on word
learning. Yet, the joint model still tends to uncover
reasonable meanings. The next question is whether
these meanings have an impact on the segmentation.
506
NoCon Random Joint
Referring Nouns 0.36 0.35 0.50
Neighbors 0.33 0.33 0.37
Utt Final Nouns 0.36 0.36 0.52
Entire Corpus 0.53 0.53 0.54
Table 3: Segmentation performance. F-score for three
subsets and the full corpus for three variants: the model
without non-linguistic context, the model with random
topics, and the full joint model.
4.2.2 Word Segmentation
To measure the impact of word learning on seg-
mentation, we again compare the model on the full
joint task against two other variants: one where top-
ics are randomly selected, and one that ignores the
non-linguistic context. For the random topics vari-
ant, we choose each topic during initialization ac-
cording to the empirical distribution over gold topics
and treat these topic assignments as observed vari-
ables for subsequent iterations. The variant that ig-
nores non-linguistic context draws topics uniformly
from the entire set of objects ever discussed in the
corpus, another test of the contribution of the non-
linguistic context to segmentation. We report token
F-score, computed as per Goldwater et al (2009),
where any segment proposed by the model is a true
positive only if it matches the gold segmentation and
is a false positive otherwise. Any segment in the
gold data not found by the model is a false negative.
Table 3 shows the segmentation performance for
various subsets as well as for the entire corpus. Be-
cause we are primarily interested in synergies be-
tween word learning and segmentation, we focus on
the words most directly impacted by the meanings:
gold standard referring nouns and their neighboring
words.
The model behaves the same with randomized
topics as without context; it learns none of the gold
standard pairs (no matter how we identify clusters
with topics for the contextless case). On all subsets,
the full joint model outperforms the other two vari-
ants. In particular, the greatest gain is for the refer-
ring nouns, with a 21% reduction in error. Also, sim-
ilar to the findings of Bortfeld et al (2005) regarding
6 month olds? abilities to segment words adjoining a
familiar name, we also find that neighboring words
benefit from sharing a word boundary with a learned
word.
The model performs exceptionally well on utter-
ance final referring nouns, with a 24% reduction
in error. This may explain certain psycholinguistic
observations. Frank et al (2006) performed an ar-
tificial language experiment with humans subjects
demonstrating that adults were able to learn words
at the same time as they learned to segment the lan-
guage. However, subjects did much better on a word
learning task when the meaning bearing words were
consistently placed at the end of utterances. There
are several possible reasons why this might have
been the case. For instance, it is common in English
for the object noun to occur at the end of the sen-
tence, and since the subjects were all English speak-
ers, they may have found it easier to learn an artifi-
cial language with a similar pattern. However, our
model predicts another simple possibility: the seg-
mentation task is easier at the end because one of
the two word boundaries is already known (the ut-
terance boundary itself).
4.3 Discussion
The word learning model generally prefers a very
sparse word-to-object map. This is enforced by us-
ing a concentration parameter ?1 that is quite small
relative to the ?0 parameter, and it biases the model
so that the distributions over referring words are
very different from that over non-referring words. A
small concentration parameter biases the estimator
to prefer a small set of word types. In contrast, the
relatively large concentration parameter for the non-
referring words tends to result in most of the words
receiving highest probability as non-referring words.
The model thus categorizes words accordingly. It is
in part due to this tendency towards sparse word-
object maps that the model enforces mutual exclu-
sivity, a phenomenon well documented among natu-
ral word learners (Markman, 1990).
Aside from contributing to mutual exclusivity
and specialization among the topical word distri-
butions, the small concentration parameter also has
important implications for the segmentation task.
A very small value for ?1 discourages the learner
from acquiring more word types for each mean-
ing than absolutely necessary, thereby forcing the
segmenter to use fewer types to explain the se-
quence of phonemes. A model without any notion
507
of meaning cannot maintain separate distributions
for different topics, and must in some sense treat all
words as non-referring. A segmenting model with-
out meanings cannot share the word learner?s reluc-
tance to propose new meaning-bearing word types
and might propose three separate types for ?your
book?, ?a book?, and ?the book?. However, with
a small enough prior on new referring word types,
the word learner that discovers a common refer-
ent for all three sequences and, preferring fewer re-
ferring word types, is more likely to discover the
common subsequence ?book?. With a single word-
object pair (?book?, BOOK), the word learner could
explain reference for all three sequences instead of
using the three separate pairs (?yourbook?, BOOK),
(?abook?, BOOK), and (?thebook?, BOOK).
While relying on non-linguistic context helps seg-
ment the meaning-bearing words, the overall im-
provement is small in our current corpus. One rea-
son for this small improvement was that only 9%
of the tokens in the corpus were referring words.
In corpora containing a larger variety of objects ?
and in cases where sub- and super-ordinate labels
like ?eyes? and ?ears? are coded ? this percentage is
likely to be much higher, leading to a greater boost
in overall segmentation performance.
We should acknowledge that the decisions en-
tailed in enriching the annotations are neither triv-
ial nor without theoretic implication, however. It is
not immediately obvious how to represent the non-
linguistic correlates of verbs, for instance. Devel-
opmentally, verbs are typically acquired much later
than nouns, and it has been argued that this may be
due to the difficulty of producing a cognitive rep-
resentation of the associated meaning (Gentner and
Boroditsky, 2001). Even among concrete nouns, not
all are equal. Children tend to have a bias toward
whole objects when mapping novel words to their
non-linguistic counterparts (Markman, 1990). De-
cisions about more sophisticated encoding of non-
linguistic information may thus require more knowl-
edge about children?s representations of the world
around them
5 Conclusion and Future Work
We find (1) that it is possible to jointly infer both
meanings and a segmentation in a fully unsupervised
way and (2) that doing so improves the segmenta-
tion performance of our model. In particular, we
found that although the word learning side suffered
from segmentation errors, and performed worse than
a model that learned from a gold standard segmen-
tation, the loss was only slight. On the other hand,
segmentation performance for the meaning bearing
words improved a great deal. The first result sug-
gests that is not necessary to assume fully segmented
input in order to learn word meanings, and that the
segmentation and word learning tasks can be effec-
tively modeled in parallel, allowing us to explore po-
tential developmental interactions. The second re-
sult suggests that synergies do actually exist and ar-
gue not only that we can model the two as parallel
processes, but that doing so could prove fruitful.
Our model is relatively simple both in terms of
word learning and in terms of word segmentation.
For instance, social cues and shared attention, or dis-
course effects, might all play a role (Frank et al,
2009b). Shared features or other relationships can
also potentially impact how quickly one might gen-
eralize a label to multiple instances (Tenenbaum and
Xu, 2000). There are many ways to elaborate on the
word learning task, with additional potential syner-
gistic implications.
We might also elaborate the linguistic structures
we incorporate into the word learning model. For
instance, Johnson (2008) explores synergies in syl-
lable and morphological structures in word segmen-
tation. Aspects of linguistic structure, such as mor-
phology, may contribute to word meaning learning
beyond its contribution to word segmentation per-
formance.
Acknowledgments
This research was funded by NSF awards 0544127
and 0631667 to Mark Johnson and by NSF DDRIG
0746251 to Michael C. Frank. We would also like
to thank Anne Fernald for providing the corpus and
Maeve Cullinane for help in coding it.
References
Nameera Akhtar and Lisa Montague. 1999. Early lexi-
cal acquisition: The role of cross-situational learning.
First Language, 19(57 Pt 3):347?358.
508
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Heather Bortfeld, James L. Morgan, Roberta Michnick
Golinkoff, and Karen Rathbun. 2005. Mommy
and me: Familiar names help launch babies into
speechstream segmentation. Psychological Science,
16(4):298?304.
Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Anne Fernald and Hiromi Morikawa. 1993. Common
themes and cultural variations in japanese and ameri-
can mothers? speech to infants. In Child Development,
number 3, pages 637?656, June.
Michael C. Frank, Vikash Mansinghka, Edward Gibson,
and Joshua B. Tenenbaum. 2006. Word segmentation
as word learning: Integrating stress and meaning with
distributional cues. In Proceedings of the 31st Annual
Boston University Conference on Language Develop-
ment.
Michael C. Frank, Sharon Goldwater, Vikash Mans-
inghka, Tom Griffiths, and Joshua Tenenbaum. 2007.
Modeling human performance in statistical word seg-
mentation. Proceedings of the 29th Annual Meeting of
the Cognitive Science Society, pages 281?286.
Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009a. Using speakers? referential inten-
tions to model early cross-situational word learning.
Psychological Science, 5:578?585.
Michael C. Frank, Noah D. Goodman, Joshua B. Tenen-
baum, and Anne Fernald. 2009b. Continuity of dis-
course provides information for word learning.
Dedre Gentner and Lera Boroditsky. 2001. Individua-
tion, relativity, and early word learning. Language,
culture, & cognition, 3:215?56.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54.
Katharine Graf-Estes, Julia L. Evans, Martha W. Alibali,
and Jenny R. Saffran. 2007. Can infants map meaning
to newly segmented words? statistical segmentation
and word learning. Psychological Science, 18(3):254?
260.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Artificial Intelligence
and Statistics (AISTATS), March.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 317?325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio. Association for Computational
Linguistics.
Ellen M. Markman. 1990. Constraints children place on
word learning. Cognitive Science, 14:57?77.
Terry Regier. 2003. Emergent constraints on word-
learning: A computational review. Trends in Cognitive
Sciences, 7:263?268.
Jenny R. Saffran, Elissa L. Newport, and Richard N.
Aslin. 1996. Word segmentation: The role of dis-
tributional cues. Journal of memory and Language,
35:606?621.
Jeffrey M. Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39?91.
Joshua B. Tenenbaum and Fei Xu. 2000. Word learn-
ing as bayesian inference. In Proceedings of the 22nd
Annual Conference of the Cognitive Science Society,
pages 517?522.
Chen Yu and Dana H. Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149?2165.
509
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488?496,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Semantic Parsing with Bayesian Tree Transducers
Bevan Keeley Jones??
b.k.jones@sms.ed.ac.uk
Mark Johnson?
Mark.Johnson@mq.edu.au
Sharon Goldwater?
sgwater@inf.ed.ac.uk
? School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
? Department of Computing
Macquarie University
Sydney, NSW 2109, Australia
Abstract
Many semantic parsing models use tree trans-
formations to map between natural language
and meaning representation. However, while
tree transformations are central to several
state-of-the-art approaches, little use has been
made of the rich literature on tree automata.
This paper makes the connection concrete
with a tree transducer based semantic parsing
model and suggests that other models can be
interpreted in a similar framework, increasing
the generality of their contributions. In par-
ticular, this paper further introduces a varia-
tional Bayesian inference algorithm that is ap-
plicable to a wide class of tree transducers,
producing state-of-the-art semantic parsing re-
sults while remaining applicable to any do-
main employing probabilistic tree transducers.
1 Introduction
Semantic parsing is the task of mapping natural lan-
guage sentences to a formal representation of mean-
ing. Typically, a system is trained on pairs of natural
language sentences (NLs) and their meaning repre-
sentation expressions (MRs), as in figure 1(a), and
the system must generalize to novel sentences.
Most semantic parsing models rely on an assump-
tion of structural similarity between MR and NL.
Since strict isomorphism is overly restrictive, this
assumption is often relaxed by applying transforma-
tions. Several approaches assume a tree structure to
the NL, MR, or both (Ge and Mooney, 2005; Kate
and Mooney, 2006; Wong and Mooney, 2006; Lu
et al, 2008; Bo?rschinger et al, 2011), and often in-
Figure 1: (a) An example sentence/meaning pair, (b) a
tree transformation based mapping, and (c) a tree trans-
ducer that performs the mapping.
volve tree transformations either between two trees
or a tree and a string.
The tree transducer, a formalism from automata
theory which has seen interest in machine transla-
tion (Yamada and Knight, 2001; Graehl et al, 2008)
and has potential applications in many other areas,
is well suited to formalizing such tree transforma-
tion based models. Yet, while many semantic pars-
ing systems resemble the formalism, each was pro-
posed as an independent model requiring custom al-
gorithms, leaving it unclear how developments in
one line of inquiry relate to others. We argue for a
unifying theory of tree transformation based seman-
tic parsing by presenting a tree transducer model and
drawing connections to other similar systems.
We make a further contribution by bringing to
tree transducers the benefits of the Bayesian frame-
work for principled handling of data sparsity and
488
prior knowledge. Graehl et al (2008) present an EM
training procedure for top down tree transducers, but
while there are Bayesian approaches to string trans-
ducers (Chiang et al, 2010) and PCFGs (Kurihara
and Sato, 2006), there has yet to be a proposal for
Bayesian inference in tree transducers. Our vari-
ational algorithm produces better semantic parses
than EM while remaining general to a broad class
of transducers appropriate for other domains.
In short, our contributions are three-fold: we
present a new state-of-the-art semantic parsing
model, propose a broader theory for tree transforma-
tion based semantic parsing, and present a general
inference algorithm for the tree transducer frame-
work. We recommend the last of these as just one
benefit of working within a general theory: contri-
butions are more broadly applicable.
2 Meaning representations and regular
tree grammars
In semantic parsing, an MR is typically an expres-
sion from a machine interpretable language (e.g., a
database query language or a logical language like
Prolog). In this paper we assume MRs can be rep-
resented as trees, either by pre-parsing or because
they are already trees (often the case for functional
languages like LISP).1 More specifically, we assume
the MR language is a regular tree language.
A regular tree grammar (RTG) closely resembles
a context free grammar (CFG), and is a way of de-
scribing a language of trees. Formally, define T? as
the set of trees with symbols from alphabet ?, and
T?(A) as the set of all trees in T??A where symbols
from A only occur at the leaves. Then an RTG is a
tuple (Q,?, qstart,R), where Q is a set of states, ?
is an alphabet, qstart ? Q is the initial state, and R
is a set of grammar rules of the form q ? t, where q
is a state from Q and t is a tree from T?(Q).
A rule typically consists of a parent state (left) and
its child states and output symbol (right). We indi-
cate states using all capital letters:
NUM ? population(PLACE).
Intuitively, an RTG is a CFG where the yield of
every parse is itself a tree. In fact, for any CFG G, it
1See Liang et al (2011) for work in representing lambda
calculus expressions with trees.
is straightforward to produce a corresponding RTG
that generates the set of parses of G. Consequently,
while we assume we have an RTG for the MR lan-
guage, there is no loss of generality if the MR lan-
guage is actually context free.
3 Weighted root-to-frontier, linear,
non-deleting tree-to-string transducers
Tree transducers (Rounds, 1970; Thatcher, 1970) are
generalizations of finite state machines that operate
on trees. Mirroring the branching nature of its in-
put, the transducer may simultaneously transition to
several successor states, assigning a separate state to
each subtree.
There are many classes of transducer with dif-
ferent formal properties (Knight and Greahl, 2005;
Maletti et al, 2009). Figure 1(c) is an example of
a root-to-frontier, linear, non-deleting tree-to-string
transducer. It is defined using rules where the left
hand side identifies a state of the transducer and a
fragment of the input tree, and the right hand side
describes a portion of the output string. Variables
xi stand for entire sub-trees, and state-variable pairs
qj .xi stand for strings produced by applying the
transducer starting at state qj to subtree xi. Fig-
ure 1(b) illustrates an application of the transducer,
taking the tree on the left as input and outputting the
string on the right.
Formally, a weighted root-to-frontier, tree-to-
string transducer is a 5-tuple (Q,?,?, qstart,R). Q
is a finite set of states, ? and ? are the input and out-
put alphabets, qstart is the start state, and R is the
set of rules. Denote a pair of symbols, a and b by
a.b, the cross product of two sets A and B by A.B,
and let X be the set of variables {x0, x1, ...}. Then,
each rule r ? R is of the form [q.t ? u].v, where
v ? ??0 is the rule weight, q ? Q, t ? T?(X ), and
u is a string in (? ? Q.X )? such that every x ? X
in u also occurs in t.
We say q.t is the left hand side of rule r and u its
right hand side. The transducer is linear iff no vari-
able appears more than once on the right hand side.
It is non-deleting iff all variables on the left hand
side also occur on the right hand side. In this paper
we assume that every tree t on the left hand side is ei-
ther a single variable x0 or of the form ?(x0, ...xn),
where ? ? ? (i.e., it is a tree of depth ? 1).
489
A weighted tree transducer may define a probabil-
ity distribution, either a joint distribution over input
and output pairs or a conditional distribution of the
output given the input. Here, we will use joint dis-
tributions, which can be defined by ensuring that the
weights of all rules with the same state on the left-
hand side sum to one. In this case, it can be help-
ful to view the transducer as simultaneously gener-
ating both the input and output, rather than the usual
view of mapping input trees into output strings. A
joint distribution allows us to model with a single
machine both the input and output languages, which
is important during decoding when we want to infer
the input given the output.
4 A generative model of semantic parsing
Like the hybrid tree semantic parser (Lu et al, 2008)
and the synchronous grammar based WASP (Wong
and Mooney, 2006), our model simultaneously gen-
erates the input MR tree and the output NL string.
The MR tree is built up according to the provided
MR grammar, one grammar rule at a time. Coupled
with the application of the MR rule, similar CFG-
like productions are applied to the NL side, repeated
until both the MR and NL are fully generated. In
each step, we select an MR rule and then build the
NL by first choosing a pattern with which to expand
it and then filling out that pattern with words drawn
from a unigram distribution.
This kind of coupled generative process can
be naturally formalized with tree transducer rules,
where the input tree fragment on the left side of each
rule describes the derivation of the MR and the right
describes the corresponding NL derivation.
For a simple example of a tree-to-string trans-
ducer rule consider
q.population(x1) ? ?population of? q.x1 (1)
which simultaneously generates tree fragment
population(x1) on the left and sub-string ?popula-
tion of q.x1? on the right. Variable x1 stands for
an MR subtree under population, and, on the right,
state-variable pair q.x1 stands for the NL substring
generated while processing subtree x1 starting from
q. While this rule can serve as a single step of
an MR-to-NL map such as the example transducer
shown in Figure 1(c), such rules do not model the
NUM ? population(PLACE) (m)
PLACE ? cityid(CITY, STATE) (r)
CITY ? portland (u)
STATE ? maine (v)
qMRm,1.x1 ? qNLr .x1 (2)
qMRr,1 .x1 ? qNLu .x1
qMRr,2 .x1 ? qNLv .x1
qNLm .population(w1, x1, w2) ?
qWm .w1 qMRm,1.x1 qEND.w2 (3)
qNLr .cityid(w1, x1, w2, x2, w3) ?
qEND.w1 qMRr,2 .x2 qWr .w2 qMRr,1 .x1 qEND.w3 (4)
qWm .w1 ? ?population? qWm .w1 (5)
qWm .w1 ? ?of? qWm .w1
qWm .w1 ? ... qWm .w1
qWm .w1 ? ?of? qEND.w1 (6)
qWm .w1 ? ... qEND.w1
qEND.W ? ? (7)
Figure 2: Examples of transducer rules (bottom) that gen-
erate MR and NL associated with MR rules m-v (top).
Transducer rule 2 selects MR rule r from the MR gram-
mar. Rule 3 simultaneously writes the MR associated
with rule m and chooses an NL pattern (as does 4 for
r). Rules 5-7 generate the words associated with m ac-
cording to a unigram distribution specific to m.
grammaticality of the MR and lack flexibility since
sub-strings corresponding to a given tree fragment
must be completely pre-specified. Instead, we break
transductions down into a three stage process of
choosing the (i) MR grammar rule, (ii) NL expan-
sion pattern, and (iii) individual words according to
a unigram distribution. Such a decomposition in-
corporates independence assumptions that improve
generalizability. See Figure 2 for example rules
from our transducer and Figure 3 for a derivation.
To ensure that only grammatical MRs are gener-
ated, each state of our transducer encodes the iden-
tity of exactly one MR grammar rule. Transitions
between qMR and qNL states implicitly select the em-
bedded rule. For instance, rule 2 in Figure 2 selects
490
MR grammar rule r to expand the ith child of the
parent produced by rule m. Aside from ensuring
the grammaticality of the generated MR, rules of
this type also model the probability of the MR, con-
ditioning the probability of a rule both on the par-
ent rule and the index of the child being expanded.
Thus, parent state qMRm,1 encodes not only the identity
of rule m, but also the child index, 1 in this case.
Once the MR rule is selected, qNL states are ap-
plied to select among rules such as 3 and 4 to gen-
erate the MR entity and choose the NL expansion
pattern. These rules determine the word order of the
language by deciding (i) whether or not to generate
words in a given location and (ii) where to insert the
result of processing each MR subtree. Decision (i) is
made by either transitioning to state qWr to generate
words or to qEND to generate the empty string. De-
cision (ii) is made with the order of xi?s on the right
hand side. Rule 4 illustrates the case where port-
land and maine in cityid(portland, maine) would be
realized in reverse order as ?maine ... portland?.
The particular set of patterns that appear on the
right of rules such as 3 embodies the binary word at-
tachment decisions and the particular permutation of
xi in the NL. We allow words to be generated at the
beginning and end of each pattern and between the
xis. Thus, rule 4 is just one of 16 such possible pat-
terns (3 binary decisions and 2 permutations), while
rule 3 is one of 4. We instantiate all such rules and
allow the system to learn weights for them according
to the language of the training data.
Finally, the NL is filled out with words chosen ac-
cording to a unigram distribution, implemented in a
PCFG-like fashion, using a different rule for each
word which recursively chooses the next word un-
til a string termination rule is reached.2 Generating
word sequence ?population of? entails first choosing
rule 5 in Figure 2. State qWr is then recursively ap-
plied to choose rule 6, generating ?of? at the same
time as deciding to terminate the string by transi-
tioning to a new state qEND which deterministically
concludes by writing the empty string ?.
On the MR side, rules 5-7 do very little: the tree
on the left side of rules 5 and 6 consists entirely of a
2There are roughly 25,000 rules in the transducers in our
experiments, and the majority of these implement the unigram
word distributions since every entity in the MR may potentially
produce any of the words it is paired with in training.
subtree variable w1, indicating that nothing is gener-
ated in the MR. Rule 7 subsequently generates these
subtrees as W symbols, marking corresponding lo-
cations where words might be produced in the NL,
which are later removed during post processing.3
Figure 3(b) illustrates the coupled generative pro-
cess. At each step of the derivation, an MR rule is
chosen to expand a node of the MR tree, and then a
corresponding part of the NL is expanded. Step 1.1
of the example chooses MR rule m, NUM ?
population(PLACE). Transducer rule 3 then gener-
ates population in the MR (shown in the left column)
at the same time as choosing an NL expansion pat-
tern (Step 1.2) which is subsequently filled out with
specific words ?population? (1.3) and ?of? (1.4).
This coupled derivation can be represented by a
tree, shown in Figure 3(c), which explicitly repre-
sents the dependency structure of the coupled MR
and NL (a simplified version is shown in (d) for clar-
ity). In our transducer, which defines a joint distri-
bution over both the MR and NL, the probability of
a rule is conditioned on the parent state. Since each
state encodes an MR rule, MR rule specific distribu-
tions are learned for both the words and their order.
5 Relation to existing models
The tree transducer model can be viewed either as
a generative procedure for building up two separate
structures or as a transformative machine that takes
one as input and produces another as output. Dif-
ferent semantic parsing approaches have taken one
or the other view, and both can be captured in this
single framework.
WASP (Wong and Mooney, 2006) is an exam-
ple of the former perspective, coupling the genera-
tion of the MR and NL with a synchronous gram-
mar, a formalism closely related to tree transducers.
The most significant difference from our approach
is that they use machine translation techniques for
automatically extracting rules from parallel corpora;
similar techniques can be applied to tree transduc-
ers (Galley et al, 2004). In fact, synchronous gram-
mars and tree transducers can be seen as instances of
the same more general class of automata (Shieber,
3The addition of W symbols is a convenience; it is easier to
design transducer rules where every substring on the right side
corresponds to a subtree on the left.
491
Figure 3: Coupled derivation of an (MR, NL) pair. At each step an MR grammar rule is chosen to expand the MR and
the corresponding portion of the NL is then generated. Symbols W stand for locations in the tree corresponding to
substrings of the output and are removed in a post-processing step. (a) The (MR, NL) pair. (b) Step by step derivation.
(c) The same derivation shown in tree form. (d) The underlying dependency structure of the derivation.
2004). Rather than argue for one or the other, we
suggest that other approaches could also be inter-
preted in terms of general model classes, grounding
them in a broader base of theory.
The hybrid tree model (Lu et al, 2008) takes
a transformative perspective that is in some ways
more similar to our model. In fact, there is a one-
to-one relationship between the multinomial param-
eters of the two models. However, they represent the
MR and NL with a single tree and apply tree walk-
ing algorithms to extract them. Furthermore, they
implement a custom training procedure for search-
ing over the potential MR transformations. The tree
transducer, on the other hand, naturally captures the
same probabilistic dependencies while maintaining
the separation between MR and NL, and further al-
lows us to build upon a larger body of theory.
KRISP (Kate and Mooney, 2006) uses string clas-
sifiers to label substrings of the NL with entities
from the MR. To focus search, they impose an or-
dering constraint based on the structure of the MR
tree, which they relax by allowing the re-ordering
of sibling nodes and devise a procedure for recover-
ing the MR from the permuted tree. This procedure
corresponds to backward-application in tree trans-
ducers, identifying the most likely input tree given a
492
particular output string.
SCISSOR (Ge and Mooney, 2005) takes syntactic
parses rather than NL strings and attempts to trans-
late them into MR expressions. While few seman-
tic parsers attempt to exploit syntactic information,
there are techniques from machine translation for
using tree transducers to map between parsed par-
allel corpora, and these techniques could likely be
applied to semantic parsing.
Bo?rschinger et al (2011) argue for the PCFG as
an alternative model class, permitting conventional
grammar induction techniques, and tree transducers
are similar enough that many techniques are applica-
ble to both. However, the PCFG is less amenable to
conceptualizing correspondences between parallel
structures, and their model is more restrictive, only
applicable to domains with finite MR languages,
since their non-terminals encode entire MRs. The
tree transducer framework, on the other hand, allows
us to condition on individual MR rules.
6 Variational Bayes for tree transducers
As seen in the example in Figure 3(c), tree trans-
ducers not only operate on trees, their derivations
are themselves trees, making them amenable to dy-
namic programming and an EM training procedure
resembling inside-outside (Graehl et al, 2008). EM
assigns zero probability to events not seen in the
training data, however, limiting the ability to gen-
eralize to novel items. The Bayesian framework of-
fers an elegant solution to this problem, introducing
a prior over rule weights which simultaneously en-
sures that all rules receive non-zero probability and
allows the incorporation of prior knowledge and in-
tuitions. Unfortunately, the introduction of a prior
makes exact inference intractable, so we use an ap-
proximate method, variational Bayesian inference
(Bishop, 2006), deriving an algorithm similar to that
for PCFGs (Kurihara and Sato, 2006).
The tree transducer defines a joint distribution
over the input y, output w, and their derivation x
as the product of the weights of the rules appearing
in x. That is,
p(y, x, w|?) =
?
r?R
?(r)cr(x)
where ? is the set of multinomial parameters, r is a
transducer rule, ?(r) is its weight, and cr(x) is the
number of times r appears in x. In EM, we are in-
terested in the point estimate for ? that maximizes
p(Y,W|?), where Y and W are the N input-output
pairs in the training data. In the Bayesian setting,
however, we place a symmetric Dirichlet prior over
? and estimate a posterior distribution over both X
and ?.
p(?,X|Y,W) = p(Y,X ,W, ?)p(Y,W)
= p(?)
?N
i=1 p(yi, xi, wi|?)
?
p(?)?Ni=1
?
x?Xi p(yi, x, wi|?)d?
Since the integral in the denominator is in-
tractable, we look for an appropriate approximation
q(?,X ) ? p(?,X|Y,W). In particular, we assume
the rule weights and the derivations are independent,
i.e., q(?,X ) = q(?)q(X ). The basic idea is then to
define a lower bound F ? ln p(Y,W) in terms of q
and then apply the calculus of variations to find a q
that maximizes F .
ln p(Y,W|?) = lnEq[
p(Y,X ,W|?)
q(?,X ) ]
? Eq[ln
p(Y,X ,W|?)
q(?,X ) ] = F ,
Applying our independence assumption, we arrive at
the following expression for F , where ?t is the par-
ticular parameter vector corresponding to the rules
with parent state t:
F =
?
t?Q
(
Eq(?t)[ln p(?t|?t)]? Eq(?t)[ln q(?t)]
)
+
N
?
i=1
(
Eq[ln p(wi, xi, yi|?)]? Eq(xi)[ln q(xi)]
)
.
We find the q(?t) and q(xi) that maximize F by
taking derivatives of the Lagrangian, setting them to
zero, and solving, which yields:
q(?t) = Dirichlet(?t|??t)
q(xi) =
?
r?R ??(r)cr(xi)
?
x?Xi
?
r?R ??(r)cr(x)
where
??(r) = ?(r) +
?
i
Eq(xi)[cr(xi)]
??(r) = exp
?
??(??(r))??(
?
r:s(r)=t
??(r))
?
? .
493
The parameters of q(?t) are defined with respect
to q(xi) and the parameters of q(xi) with respect
to the parameters of q(?t). q(xi) can be computed
efficiently using inside-outside. Thus, we can per-
form an EM-like alternation between calculating ??
and ??.4
It is also possible to estimate the hyper-
parameters ? from data, a practice known as em-
pirical Bayes, by optimizing F . We explore learn-
ing separate hyper-parameters ?t for each ?t, us-
ing a fixed point update described by Minka (2000),
where kt is the number of rules with parent state t:
??t =
(
1
?t
+ 1kt?2t
(
?2F
??2t
)?1( ?F
??t
)
)?1
7 Training and decoding
We implement our VB training algorithm inside the
tree transducer package Tiburon (May and Knight,
2006), and experiment with both manually set and
automatically estimated priors. For our manually
set priors, we explore different hyper-parameter set-
tings for three different priors, one for each of the
main decision types: MR rule, NL pattern, and word
generation. For the automatic priors, we estimate
separate hyper-parameters for each multinomial (of
which there are hundreds). As is standard, we ini-
tialize the word distributions using a variant of IBM
model 1, and make use of NP lists (a manually cre-
ated list of the constants in the MR language paired
with the words that refer to them in the corpus).
At test time, since finding the most probable MR
for a sentence involves summing over all possible
derivations, we instead find the MR associated with
the most probable derivation.
8 Experimental setup and evaluation
We evaluate the system on GeoQuery (Wong and
Mooney, 2006), a parallel corpus of 880 English
questions and database queries about United States
geography, 250 of which were translated into Span-
ish, Japanese, and Turkish. We present here ad-
ditional translations of the full 880 sentences into
4Because of the resemblance to EM, this procedure has been
called VBEM. Unlike EM, however, this procedure alternates
between two estimation steps and has no maximization step.
German, Greek, and Thai. For evaluation, follow-
ing from Kwiatkowski et al (2010), we reserve 280
sentences for test and train on the remaining 600.
During development, we use cross-validation on the
600 sentence training set. At test, we run once on the
remaining 280 and perform 10 fold cross-validation
on the 250 sentence sets.
To judge correctness, we follow standard prac-
tice and submit each parse as a GeoQuery database
query, and say the parse is correct only if the answer
matches the gold standard. We report raw accuracy
(the percentage of sentences with correct answers),
as well as F1: the harmonic mean of precision (the
proportion of correct answers out of sentences with
a parse) and recall (the proportion of correct answers
out of all sentences).5
We run three other state-of-the-art systems for
comparison. WASP (Wong and Mooney, 2006) and
the hybrid tree (Lu et al, 2008) are chosen to rep-
resent tree transformation based approaches, and,
while this comparison is our primary focus, we also
report UBL-S (Kwiatkowski et al, 2010) as a non-
tree based top-performing system.6 The hybrid tree
is notable as the only other system based on a gen-
erative model, and uni-hybrid, a version that uses a
unigram distribution over words, is very similar to
our own model. We also report the best performing
version, re-hybrid, which incorporates a discrimina-
tive re-ranking step.
We report transducer performance under three dif-
ferent training conditions: tsEM using EM, tsVB-
auto using VB with empirical Bayes, and tsVB-hand
using hyper-parameters manually tuned on the Ger-
man training data (? of 0.3, 0.8, and 0.25 for MR
rule, NL pattern, and word choices, respectively).
Table 1 shows results for 10 fold cross-validation
on the training set. The results highlight the benefit
of the Dirichlet prior, whether manually or automat-
ically set. VB improves over EM considerably, most
likely because (1) the handling of unknown words
and MR entities allows it to return an analysis for all
sentences, and (2) the sparse Dirichlet prior favors
fewer rules, reasonable in this setting where only a
few words are likely to share the same meaning.
5Note that accuracy and f-score reduce to the same formula
if there are no parse failures.
6UBL-S is based on CCG, which can be viewed as a map-
ping between graphs more general than trees.
494
DEV geo600 - 10 fold cross-val
German Greek
Acc F1 Acc F1
UBL-S 76.7 76.9 76.2 76.5
WASP 66.3 75.0 71.2 79.7
uni-hybrid 61.7 66.1 71.0 75.4
re-hybrid 62.3 69.5 70.2 76.8
tsEM 61.7 67.9 67.3 73.2
tsVB-auto 74.0 74.0 ?79.8 ?79.8
tsVB-hand ?78.0 ?78.0 79.0 79.0
English Thai
UBL-S 85.3 85.4 74.0 74.1
WASP 73.5 79.4 69.8 73.9
uni-hybrid 76.3 79.0 71.3 73.7
re-hybrid 77.0 82.2 71.7 76.0
tsEM 73.5 78.1 69.8 72.9
tsVB-auto 81.2 81.2 74.7 74.7
tsVB-hand ?83.7 ?83.7 ?76.7 ?76.7
Table 1: Accuracy and F1 score comparisons on the
geo600 training set. Highest scores are in bold, while
the highest among the tree based models are marked with
a bullet. The dotted line separates the tree based from
non-tree based models.
On the test set (Table 2), we only run the model
variants that perform best on the training set. Test set
accuracy is consistently higher for the VB trained
tree transducer than the other tree transformation
based models (and often highest overall), while f-
score remains competitive.7
9 Conclusion
We have argued that tree transformation based se-
mantic parsing can benefit from the literature on for-
mal language theory and tree automata, and have
taken a step in this direction by presenting a tree
transducer based semantic parser. Drawing this con-
nection facilitates a greater flow of ideas in the
research community, allowing semantic parsing to
leverage ideas from other work with tree automata,
while making clearer how seemingly isolated ef-
forts might relate to one another. We demonstrate
this by both building on previous work in train-
ing tree transducers using EM (Graehl et al, 2008),
7Numbers differ slightly here from previously published re-
sults due to the fact that we have standardized the inputs to the
different systems.
TEST geo880 - 600 train/280 test
German Greek
Acc F1 Acc F1
UBL-S 75.0 75.0 73.6 73.7
WASP 65.7 ? 74.9 70.7 ? 78.6
re-hybrid 62.1 68.5 69.3 74.6
tsVB-hand ? 74.6 74.6 ?75.4 75.4
English Thai
UBL-S 82.1 82.1 66.4 66.4
WASP 71.1 77.7 71.4 75.0
re-hybrid 76.8 ? 81.0 73.6 76.7
tsVB-hand ? 79.3 79.3 ? 78.2 ? 78.2
geo250 - 10 fold cross-val
English Spanish
UBL-S 80.4 80.6 79.7 80.1
WASP 70.0 80.8 72.4 81.0
re-hybrid 74.8 82.6 78.8 ? 86.2
tsVB-hand ? 83.2 ? 83.2 ? 80.0 80.0
Japanese Turkish
UBL-S 80.5 80.6 74.2 74.9
WASP 74.4 ? 82.9 62.4 75.9
re-hybrid 76.8 82.4 66.8 ? 77.5
tsVB-hand ? 78.0 78.0 ? 75.6 75.6
Table 2: Accuracy and F1 score comparisons on the
geo880 and geo250 test sets. Highest scores are in
bold, while the highest among the tree based models are
marked with a bullet. The dotted line separates the tree
based from non-tree based models.7
and describing a general purpose variational infer-
ence algorithm for adapting tree transducers to the
Bayesian framework. The new VB algorithm re-
sults in an overall performance improvement for the
transducer over EM training, and the general effec-
tiveness of the approach is further demonstrated by
the Bayesian transducer achieving highest accuracy
among other tree transformation based approaches.
Acknowledgments
We thank Joel Lang, Michael Auli, Stella Frank,
Prachya Boonkwan, Christos Christodoulopoulos,
Ioannis Konstas, and Tom Kwiatkowski for provid-
ing the new translations of GeoQuery. This research
was supported in part under the Australian Re-
search Council?s Discovery Projects funding scheme
(project number DP110102506).
495
References
Christopher M. Bishop. Pattern Recognition and Ma-
chine Learning. Springer, 2006.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. Reducing grounded learning tasks to grammati-
cal inference. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, 2011.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. Bayesian inference for finite-
state transducers. In Proc. of the annual meeting of
the North American Association for Computational Lin-
guistics, 2010.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. What?s in a translation rule? In Proc. of the
annual meeting of the North American Association for
Computational Linguistics, 2004.
Ruifang Ge and Raymond J. Mooney. A statistical se-
mantic parser that integrates syntax and semantics. In
Proceedings of the Conference on Computational Natu-
ral Language Learning, 2005.
Jonathon Graehl, Kevin Knight, and Jon May. Training
tree transducers. Computational Linguistics, 34:391?
427, 2008.
Rohit J. Kate and Raymond J. Mooney. Using string-
kernels for learning semantic parsers. In Proc. of the
International Conference on Computational Linguistics
and the annual meeting of the Association for Compu-
tational Linguistics, 2006.
Kevin Knight and Jonathon Greahl. An overview of prob-
abilistic tree transducers for natural language process-
ing. In Proc. of the 6th International Conference on
Intelligent Text Processing and Computational Linguis-
tics, 2005.
Kenichi Kurihara and Taisuke Sato. Variational Bayesian
grammar induction for natural language. In Proc. of
the 8th International Colloquium on Grammatical In-
ference, 2006.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. Inducing probabilistic CCG
grammars from logical form with higher-order unifica-
tion. In Proc. of the Conference on Empirical Methods
in Natural Language Processing, 2010.
Percy Liang, Michael I. Jordan, and Dan Klein. Learning
dependency-based compositional semantics. In Proc.
of the annual meeting of the Association for Computa-
tional Linguistics, 2011.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. A generative model for parsing natural language
to meaning representations. In Proc. of the Conference
on Empirical Methods in Natural Language Processing,
2008.
Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. The power of extended top-down tree
transducers. SIAM J. Comput., 39:410?430, June 2009.
Jon May and Kevin Knight. Tiburon: A weighted tree au-
tomata toolkit. In Proc. of the International Conference
on Implementation and Application of Automata, 2006.
Tom Minka. Estimating a Dirichlet distribution. Techni-
cal report, M.I.T., 2000.
W.C. Rounds. Mappings and grammars on trees. Mathe-
matical Systems Theory 4, pages 257?287, 1970.
Stuart M. Shieber. Synchronous grammars as tree trans-
ducers. In Proc. of the Seventh International Workshop
on Tree Adjoining Grammar and Related Formalisms,
2004.
J.W. Thatcher. Generalized sequential machine maps. J.
Comput. System Sci. 4, pages 339?367, 1970.
Yuk Wah Wong and Raymond J. Mooney. Learning for
semantic parsing with statistical machine translation. In
Proc. of Human Language Technology Conference and
the annual meeting of the North American Chapter of
the Association for Computational Linguistics, 2006.
Kenji Yamada and Kevin Knight. A syntax-based statis-
tical translation model. In Proc. of the annual meeting
of the Association for Computational Linguistics, 2001.
496
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924?932,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing Graphs with Hyperedge Replacement Grammars
David Chiang
Information Sciences Institute
University of Southern California
Jacob Andreas
Columbia University
University of Cambridge
Daniel Bauer
Department of Computer Science
Columbia University
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Bevan Jones
University of Edinburgh
Macquarie University
Kevin Knight
Information Sciences Institute
University of Southern California
Abstract
Hyperedge replacement grammar (HRG)
is a formalism for generating and trans-
forming graphs that has potential appli-
cations in natural language understand-
ing and generation. A recognition al-
gorithm due to Lautemann is known to
be polynomial-time for graphs that are
connected and of bounded degree. We
present a more precise characterization of
the algorithm?s complexity, an optimiza-
tion analogous to binarization of context-
free grammars, and some important im-
plementation details, resulting in an algo-
rithm that is practical for natural-language
applications. The algorithm is part of Boli-
nas, a new software toolkit for HRG pro-
cessing.
1 Introduction
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al, 1997), and its synchronous
counterpart can be used for transforming graphs
to/from other graphs or trees. As such, it has great
potential for applications in natural language un-
derstanding and generation, and semantics-based
machine translation (Jones et al, 2012). Fig-
ure 1 shows some examples of graphs for natural-
language semantics.
A polynomial-time recognition algorithm for
HRGs was described by Lautemann (1990), build-
ing on the work of Rozenberg and Welzl (1986)
on boundary node label controlled grammars, and
others have presented polynomial-time algorithms
as well (Mazanek and Minas, 2008; Moot, 2008).
Although Lautemann?s algorithm is correct and
tractable, its presentation is prefaced with the re-
mark: ?As we are only interested in distinguish-
ing polynomial time from non-polynomial time,
the analysis will be rather crude, and implemen-
tation details will be explicated as little as possi-
ble.? Indeed, the key step of the algorithm, which
matches a rule against the input graph, is described
at a very high level, so that it is not obvious (for a
non-expert in graph algorithms) how to implement
it. More importantly, this step as described leads
to a time complexity that is polynomial, but poten-
tially of very high degree.
In this paper, we describe in detail a more effi-
cient version of this algorithm and its implementa-
tion. We give a more precise complexity analysis
in terms of the grammar and the size and maxi-
mum degree of the input graph, and we show how
to optimize it by a process analogous to binariza-
tion of CFGs, following Gildea (2011). The re-
sulting algorithm is practical and is implemented
as part of the open-source Bolinas toolkit for hy-
peredge replacement grammars.
2 Hyperedge replacement grammars
We give a short example of how HRG works, fol-
lowed by formal definitions.
2.1 Example
Consider a weighted graph language involving just
two types of semantic frames (want and believe),
two types of entities (boy and girl), and two roles
(arg0 and arg1). Figure 1 shows a few graphs from
this language.
Figure 2 shows how to derive one of these
graphs using an HRG. The derivation starts with
a single edge labeled with the nonterminal sym-
bol S . The first rewriting step replaces this edge
with a subgraph, which we might read as ?The
924
boy?girl?
want? arg0
arg1
boy?
believe? arg1
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 1: Sample members of a graph language,
representing the meanings of (clockwise from up-
per left): ?The girl wants the boy,? ?The boy is
believed,? and ?The boy wants the girl to believe
that he wants her.?
boy wants something (X) involving himself.? The
second rewriting step replaces the X edge with an-
other subgraph, which we might read as ?The boy
wants the girl to believe something (Y) involving
both of them.? The derivation continues with a
third rewriting step, after which there are no more
nonterminal-labeled edges.
2.2 Definitions
The graphs we use in this paper have edge labels,
but no node labels; while node labels are intu-
itive for many graphs in NLP, using both node and
edge labels complicates the definition of hyper-
edge grammar and algorithms. All of our graphs
are directed (ordered), as the purpose of most
graph structures in NLP is to model dependencies
between entities.
Definition 1. An edge-labeled, ordered hyper-
graph is a tuple H = ?V, E, ??, where
? V is a finite set of nodes
? E ? V+ is a finite set of hyperedges, each of
which connects one or more distinct nodes
? ? : E ? C assigns a label (drawn from the
finite set C) to each edge.
For brevity we use the terms graph and hyper-
graph interchangeably, and similarly for edge and
hyperedge. In the definition of HRGs, we will use
the notion of hypergraph fragments, which are the
elementary structures that the grammar assembles
into hypergraphs.
Definition 2. A hypergraph fragment is a tuple
?V, E, ?, X?, where ?V, E, ?? is a hypergraph and
X ? V+ is a list of distinct nodes called the ex-
ternal nodes.
The function of graph fragments in HRG is
analogous to the right-hand sides of CFG rules
and to elementary trees in tree adjoining gram-
mars (Joshi and Schabes, 1997). The external
nodes indicate how to integrate a graph into an-
other graph during a derivation, and are analogous
to foot nodes. In diagrams, we draw them with a
black circle ( ).
Definition 3. A hyperedge replacement grammar
(HRG) is a tuple G = ?N,T, P, S ? where
? N and T are finite disjoint sets of nonterminal
and terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
A ? R, where A ? N and R is a graph frag-
ment over N ? T .
We now describe the HRG rewriting mecha-
nism.
Definition 4. Given a HRG G, we define the re-
lation H ?G H? (or, H? is derived from H in one
step) as follows. Let e = (v1 ? ? ? vk) be an edge in
H with label A. Let (A? R) be a production ofG,
where R has external nodes XR = (u1 ? ? ? uk). Then
we write H ?G H? if H? is the graph formed by
removing e from H, making an isomorphic copy
of R, and identifying vi with (the copy of) ui for
i = 1, . . . , k.
Let H ??G H? (or, H? is derived from H) be thereflexive, transitive closure of?G. The graph lan-
guage of a grammar G is the (possibly infinite) set
of graphs H that have no edges with nonterminal
labels such that
S ??G H.
When a HRG rule (A ? R) is applied to an
edge e, the mapping of external nodes in R to the
925
1
X ?
believe? arg1
girl?
arg0
1
Y
1 2
Y
?
12
want?
arg0
arg1
S
1
boy?
X
want? arg1
arg0
2 believe? arg1
want? arg1
girl?
arg0
boy?
arg0
Y
3
want?
believe? arg1
want? arg1
girl?
arg0
boy?
arg0
arg1arg0
Figure 2: Derivation of a hyperedge replacement grammar for a graph representing the meaning of ?The
boy wants the girl to believe that he wants her.?
nodes of e is implied by the ordering of nodes
in e and XR. When writing grammar rules, we
make this ordering explicit by writing the left hand
side of a rule as an edge and indexing the external
nodes of R on both sides, as shown in Figure 2.
HRG derivations are context-free in the sense
that the applicability of each production depends
on the nonterminal label of the replaced edge only.
This allows us to represent a derivation as a deriva-
tion tree, and sets of derivations of a graph as a
derivation forest (which can in turn represented as
hypergraphs). Thus we can apply many of the
methods developed for other context free gram-
mars. For example, it is easy to define weighted
and synchronous versions of HRGs.
Definition 5. If K is a semiring, a K-weighted
HRG is a tuple G = ?N,T, P, S , ??, where
?N, T, P, S ? is a HRG and ? : P ? K assigns a
weight in K to each production. The weight of a
derivation ofG is the product of the weights of the
productions used in the derivation.
We defer a definition of synchronous HRGs un-
til Section 4, where they are discussed in detail.
3 Parsing
Lautemann?s recognition algorithm for HRGs is a
generalization of the CKY algorithm for CFGs.
Its key step is the matching of a rule against the
input graph, analogous to the concatenation of
two spans in CKY. The original description leaves
open how this matching is done, and because it
tries to match the whole rule at once, it has asymp-
totic complexity exponential in the number of non-
terminal edges. In this section, we present a re-
finement that makes the rule-matching procedure
explicit, and because it matches rules little by lit-
tle, similarly to binarization of CFG rules, it does
so more efficiently than the original.
Let H be the input graph. Let n be the number of
nodes in H, and d be the maximum degree of any
node. Let G be a HRG. For simplicity, we assume
that the right-hand sides of rules are connected.
This restriction entails that each graph generated
by G is connected; therefore, we assume that H is
connected as well. Finally, let m be an arbitrary
node of H called the marker node, whose usage
will become clear below.1
3.1 Representing subgraphs
Just as CKY deals with substrings (i, j] of the in-
put, the HRG parsing algorithm deals with edge-
induced subgraphs I of the input. An edge-
induced subgraph of H = ?V, E, ?? is, for some
1To handle the more general case where H is not con-
nected, we would need a marker for each component.
926
subset E? ? E, the smallest subgraph containing
all edges in E?. From now on, we will assume that
all subgraphs are edge-induced subgraphs.
In CKY, the two endpoints i and j com-
pletely specify the recognized part of the input,
wi+1 ? ? ?w j. Likewise, we do not need to store all
of I explicitly.
Definition 6. Let I be a subgraph of H. A bound-
ary node of I is a node in I which is either a node
with an edge in H\I or an external node. A bound-
ary edge of I is an edge in I which has a boundary
node as an endpoint. The boundary representation
of I is the tuple ?bn(I), be(I, v),m ? I?, where
? bn(I) is the set of boundary nodes of I
? be(I, v) be the set of boundary edges of v in I
? (m ? I) is a flag indicating whether the
marker node is in I.
The boundary representation of I suffices to
specify I compactly.
Proposition 1. If I and I? are two subgraphs of H
with the same boundary representation, then I =
I?.
Proof. Case 1: bn(I) is empty. If m ? I and m ? I?,
then all edges of H must belong to both I and I?,
that is, I = I? = H. Otherwise, if m < I and m < I?,
then no edges can belong to either I or I?, that is,
I = I? = ?.
Case 2: bn(I) is nonempty. Suppose I , I?;
without loss of generality, suppose that there is an
edge e that is in I \ I?. Let ? be the shortest path
(ignoring edge direction) that begins with e and
ends with a boundary node. All the edges along ?
must be in I \ I?, or else there would be a boundary
node in the middle of ?, and ? would not be the
shortest path from e to a boundary node. Then, in
particular, the last edge of ?must be in I \ I?. Since
it has a boundary node as an endpoint, it must be a
boundary edge of I, but cannot be a boundary edge
of I?, which is a contradiction. 
If two subgraphs are disjoint, we can use their
boundary representations to compute the boundary
representation of their union.
Proposition 2. Let I and J be two subgraphs
whose edges are disjoint. A node v is a boundary
node of I ? J iff one of the following holds:
(i) v is a boundary node of one subgraph but not
the other
(ii) v is a boundary node of both subgraphs, and
has an edge which is not a boundary edge of
either.
An edge is a boundary edge of I ? J iff it has a
boundary node of I ? J as an endpoint and is a
boundary edge of I or J.
Proof. (?) v has an edge in either I or J and an
edge e outside both I and J. Therefore it must be a
boundary node of either I or J. Moreover, e is not
a boundary edge of either, satisfying condition (ii).
(?) Case (i): without loss of generality, assume
v is a boundary node of I. It has an edge e in I, and
therefore in I ? J, and an edge e? outside I, which
must also be outside J. For e < J (because I and
J are disjoint), and if e? ? J, then v would be a
boundary node of J. Therefore, e? < I ? J, so v is
a boundary node of I ? J. Case (ii): v has an edge
in I and therefore I ? J, and an edge not in either
I or J. 
This result leads to Algorithm 1, which runs in
time linear in the number of boundary nodes.
Algorithm 1 Compute the union of two disjoint
subgraphs I and J.
for all v ? bn(I) do
E ? be(I, v) ? be(J, v)
if v < bn(J) or v has an edge not in E then
add v to bn(I ? J)
be(I ? J, v)? E
for all v ? bn(J) do
if v < bn(I) then
add v to bn(I ? J)
be(I ? J, v)? be(I, v) ? be(J, v)
(m ? I ? J)? (m ? I) ? (m ? J)
In practice, for small subgraphs, it may be more
efficient simply to use an explicit set of edges in-
stead of the boundary representation. For the Geo-
Query corpus (Tang and Mooney, 2001), whose
graphs are only 7.4 nodes on average, we gener-
ally find this to be the case.
3.2 Treewidth
Lautemann?s algorithm tries to match a rule
against the input graph all at once. But we can op-
timize the algorithm by matching a rule incremen-
tally. This is analogous to the rank-minimization
problem for linear context-free rewriting systems.
Gildea has shown that this problem is related to
927
the notion of treewidth (Gildea, 2011), which we
review briefly here.
Definition 7. A tree decomposition of a graph
H = ?V, E? is a tree T , each of whose nodes ?
is associated with sets V? ? V and E? ? E, with
the following properties:
1. Vertex cover: For each v ? V , there is a node
? ? T such that v ? V?.
2. Edge cover: For each e = (v1 ? ? ? vk) ? E,
there is exactly one node ? ? T such that e ?
E?. We say that ? introduces e. Moreover,
v1, . . . , vk ? V?.
3. Running intersection: For each v ? V , the set
{? ? T | v ? V?} is connected.
The width of T is max |V?| ? 1. The treewidth of H
is the minimal width of any tree decomposition
of H.
A tree decomposition of a graph fragment
?V, E, X? is a tree decomposition of ?V, E? that has
the additional property that all the external nodes
belong to V? for some ?. (Without loss of general-
ity, we assume that ? is the root.)
For example, Figure 3b shows a graph, and Fig-
ure 3c shows a tree decomposition. This decom-
position has width three, because its largest node
has 4 elements. In general, a tree has width one,
and it can be shown that a graph has treewidth at
most two iff it does not have the following graph
as a minor (Bodlaender, 1997):
K4 =
Finding a tree decomposition with minimal
width is in general NP-hard (Arnborg et al, 1987).
However, we find that for the graphs we are inter-
ested in in NLP applications, even a na??ve algo-
rithm gives tree decompositions of low width in
practice: simply perform a depth-first traversal of
the edges of the graph, forming a tree T . Then,
augment the V? as necessary to satisfy the running
intersection property.
As a test, we extracted rules from the Geo-
Query corpus (Tang and Mooney, 2001) using the
SynSem algorithm (Jones et al, 2012), and com-
puted tree decompositions exactly using a branch-
and-bound method (Gogate and Dechter, 2004)
and this approximate method. Table 1 shows that,
in practice, treewidths are not very high even when
computed only approximately.
method mean max
exact 1.491 2
approximate 1.494 3
Table 1: Mean and maximum treewidths of rules
extracted from the GeoQuery corpus, using exact
and approximate methods.
(a) 0
a
believe? arg1
b
girl?
arg0
1
Y
(b) 0
1
0
b 1
0
a
b 1
arg1
a
b 1
Y
?
0
b
arg0
b
girl?
?
0believe?
?
Figure 3: (a) A rule right-hand side, and (b) a nice
tree decomposition.
Any tree decomposition can be converted into
one which is nice in the following sense (simpli-
fied from Cygan et al (2011)). Each tree node ?
must be one of:
? A leaf node, such that V? = ?.
? A unary node, which introduces exactly one
edge e.
? A binary node, which introduces no edges.
The example decomposition in Figure 3c is nice.
This canonical form simplifies the operation of the
parser described in the following section.
Let G be a HRG. For each production (A ?
R) ? G, find a nice tree decomposition of R and
call it TR. The treewidth of G is the maximum
928
treewidth of any right-hand side in G.
The basic idea of the recognition algorithm is
to recognize the right-hand side of each rule incre-
mentally by working bottom-up on its tree decom-
position. The properties of tree decomposition al-
low us to limit the number of boundary nodes of
the partially-recognized rule.
More formally, let RD? be the subgraph of R in-
duced by the union of E?? for all ?? equal to or
dominated by ?. Then we can show the following.
Proposition 3. Let R be a graph fragment, and as-
sume a tree decomposition of R. All the boundary
nodes of RD? belong to V? ? Vparent(?).
Proof. Let v be a boundary node of RD?. Node v
must have an edge in RD? and therefore in R?? for
some ?? dominated by or equal to ?.
Case 1: v is an external node. Since the root
node contains all the external nodes, by the run-
ning intersection property, both V? and Vparent(?)
must contain v as well.
Case 2: v has an edge not in RD?. Therefore
there must be a tree node not dominated by or
equal to ? that contains this edge, and therefore
v. So by the running intersection property, ? and
its parent must contain v as well. 
This result, in turn, will allow us to bound the
complexity of the parsing algorithm in terms of the
treewidth of G.
3.3 Inference rules
We present the parsing algorithm as a deductive
system (Shieber et al, 1995). The items have
one of two forms. A passive item has the form
[A, I, X], where X ? V+ is an explicit ordering
of the boundary nodes of I. This means that we
have recognized that A ??G I. Thus, the goalitem is [S ,H, ?]. An active item has the form
[A? R, ?, I, ?], where
? (A? R) is a production of G
? ? is a node of TR
? I is a subgraph of H
? ? is a bijection between the boundary nodes
of RD? and those of I.
The parser must ensure that ? is a bijection when
it creates a new item. Below, we use the notation
{e 7? e?} or {e 7? X} for the mapping that sends
each node of e to the corresponding node of e?
or X.
Passive items are generated by the following
rule:
? Root [B? Q, ?, J, ?]
[B, J, X]
where ? is the root of TQ, and X j = ?(XQ, j).
If we assume that the TR are nice, then the in-
ference rules that generate active items follow the
different types of nodes in a nice tree decomposi-
tion:
? Leaf
[A? R, ?, ?, ?]
where ? is a leaf node of TR.
? (Unary) Nonterminal
[A? R, ?1, I, ?] [B, J, X]
[A? R, ?, I ? J, ? ? {e 7? X}]
where ?1 is the only child of ?, and e is intro-
duced by ? and is labeled with nonterminal B.
? (Unary) Terminal
[A? R, ?1, I, ?]
[A? R, ?, I ? {e?}, ? ? {e 7? e?}]
where ?1 is the only child of ?, e is introduced
by ?, and e and e? are both labeled with ter-
minal a.
? Binary
[A? R, ?1, I, ?1] [A? R, ?2, J, ?2]
[A? R, ?, I ? J, ?1 ? ?2]
where ?1 and ?2 are the two children of ?.
In the Nonterminal, Terminal, and Binary rules,
we form unions of subgraphs and unions of map-
pings. When forming the union of two subgraphs,
we require that the subgraphs be disjoint (however,
see Section 3.4 below for a relaxation of this con-
dition). When forming the union of two mappings,
we require that the result be a bijection. If either
of these conditions is not met, the inference rule
cannot apply.
For efficiency, it is important to index the items
for fast access. For the Nonterminal inference
rule, passive items [B, J, X] should be indexed by
key ?B, |bn(J)|?, so that when the next item on the
agenda is an active item [A ? R, ?1, I, ?], we
know that all possible matching passive items are
929
S ?
X
X
X
X ?
a
a a
a
a
(a) (b)
a
a a
a aa
(c)
Figure 4: Illustration of unsoundness in the recog-
nition algorithm without the disjointness check.
Using grammar (a), the recognition algorithm
would incorrectly accept the graph (b) by assem-
bling together the three overlapping fragments (c).
under key ??(e), |e|?. Similarly, active items should
be indexed by key ??(e), |e|? so that they can be
found when the next item on the agenda is a pas-
sive item. For the Binary inference rule, active
items should be indexed by their tree node (?1
or ?2).
This procedure can easily be extended to pro-
duce a packed forest of all possible derivations
of the input graph, representable as a hypergraph
just as for other context-free rewriting formalisms.
The Viterbi algorithm can then be applied to
this representation to find the highest-probability
derivation, or the Inside/Outside algorithm to set
weights by Expectation-Maximization.
3.4 The disjointness check
A successful proof using the inference rules above
builds an HRG derivation (comprising all the
rewrites used by the Nonterminal rule) which de-
rives a graph H?, as well as a graph isomorphism
? : H? ? H (the union of the mappings from all
the items).
During inference, whenever we form the union
of two subgraphs, we require that the subgraphs
be disjoint. This is a rather expensive operation:
it can be done using only their boundary represen-
tations, but the best algorithm we are aware of is
still quadratic in the number of boundary nodes.
Is it possible to drop the disjointness check? If
we did so, it would become possible for the algo-
rithm to recognize the same part of H twice. For
example, Figure 4 shows an example of a grammar
and an input that would be incorrectly recognized.
However, we can replace the disjointness check
with a weaker and faster check such that any
derivation that merges two non-disjoint subgraphs
will ultimately fail, and therefore the derived
graph H? is isomorphic to the input graph H? as
desired. This weaker check is to require, when
merging two subgraphs I and J, that:
1. I and J have no boundary edges in common,
and
2. If m belongs to both I and J, it must be a
boundary node of both.
Condition (1) is enough to guarantee that ? is lo-
cally one-to-one in the sense that for all v ? H?, ?
restricted to v and its neighbors is one-to-one. This
is easy to show by induction: if ?I : I? ? H and
?J : J? ? H are locally one-to-one, then ?I ? ?J
must also be, provided condition (1) is met. Intu-
itively, the consequence of this is that we can de-
tect any place where ? changes (say) from being
one-to-one to two-to-one. So if ? is two-to-one,
then it must be two-to-one everywhere (as in the
example of Figure 4).
But condition (2) guarantees that ? maps only
one node to the marker m. We can show this again
by induction: if ?I and ?J each map only one node
to m, then ?I??J must map only one node to m, by
a combination of condition (2) and the fact that the
inference rules guarantee that ?I , ?J , and ?I ? ?J
are one-to-one on boundary nodes.
Then we can show that, since m is recognized
exactly once, the whole graph is also recognized
exactly once.
Proposition 4. If H and H? are connected graphs,
? : H? ? H is locally one-to-one, and ??1 is de-
fined for some node of H, then ? is a bijection.
Proof. Suppose that ? is not a bijection. Then
there must be two nodes v?1, v?2 ? H? such that
?(v?1) = ?(v?2) = v ? H. We also know that thereis a node, namely, m, such that m? = ??1(m) is de-
fined.2 Choose a path ? (ignoring edge direction)
from v to m. Because ? is a local isomorphism,
we can construct a path from v?1 to m? that mapsto ?. Similarly, we can construct a path from v?2to m? that maps to ?. Let u? be the first node that
these two paths have in common. But u? must have
two edges that map to the same edge, which is a
contradiction. 
2If H were not connected, we would choose the marker in
the same connected component as v.
930
3.5 Complexity
The key to the efficiency of the algorithm is that
the treewidth of G leads to a bound on the number
of boundary nodes we must keep track of at any
time.
Let k be the treewidth of G. The time complex-
ity of the algorithm is the number of ways of in-
stantiating the inference rules. Each inference rule
mentions only boundary nodes of RD? or RD?i , all
of which belong to V? (by Proposition 3), so there
are at most |V?| ? k + 1 of them. In the Nonter-
minal and Binary inference rules, each boundary
edge could belong to I or J or neither. Therefore,
the number of possible instantiations of any infer-
ence rule is in O((3dn)k+1).
The space complexity of the algorithm is the
number of possible items. For each active item
[A? R, ?, I, ?], every boundary node of RD? must
belong to V??Vparent(?) (by Proposition 3). There-
fore the number of boundary nodes is at most k+1
(but typically less), and the number of possible
items is in O((2dn)k+1).
4 Synchronous Parsing
As mentioned in Section 2.2, because HRGs have
context-free derivation trees, it is easy to define
synchronous HRGs, which define mappings be-
tween languages of graphs.
Definition 8. A synchronous hyperedge re-
placement grammar (SHRG) is a tuple G =
?N, T, T ?, P, S ?, where
? N is a finite set of nonterminal symbols
? T and T ? are finite sets of terminal symbols
? S ? N is the start symbol
? P is a finite set of productions of the form
(A? ?R,R?,??), where R is a graph fragment
over N ? T and R? is a graph fragment over
N ? T ?. The relation ? is a bijection linking
nonterminal mentions in R and R?, such that
if e ? e?, then they have the same label. We
call R the source side and R? the target side.
Some NLP applications (for example, word
alignment) require synchronous parsing: given a
pair of graphs, finding the derivation or forest of
derivations that simultaneously generate both the
source and target. The algorithm to do this is a
straightforward generalization of the HRG parsing
algorithm. For each rule (A? ?R,R?,??), we con-
struct a nice tree decomposition of R?R? such that:
? All the external nodes of both R and R? be-
long to V? for some ?. (Without loss of gen-
erality, assume that ? is the root.)
? If e ? e?, then e and e? are introduced by the
same tree node.
In the synchronous parsing algorithm, passive
items have the form [A, I, X, I?, X?] and active
items have the form [A? R : R?, ?, I, ?, I?, ??].
For brevity we omit a re-presentation of all the in-
ference rules, as they are very similar to their non-
synchronous counterparts. The main difference is
that in the Nonterminal rule, two linked edges are
rewritten simultaneously:
[A? R : R?, ?1, I, ?, I?, ??] [B, J, X, J?, X?]
[A? R : R?, ?, I ? J, ? ? {e j 7? X j},
I? ? J?, ?? ? {e?j 7? X?j}]
where ?1 is the only child of ?, e and e? are both
introduced by ? and e ? e?, and both are labeled
with nonterminal B.
The complexity of the parsing algorithm is
again in O((3dn)k+1), where k is now the max-
imum treewidth of the dependency graph as de-
fined in this section. In general, this treewidth will
be greater than the treewidth of either the source or
target side on its own, so that synchronous parsing
is generally slower than standard parsing.
5 Conclusion
Although Lautemann?s polynomial-time extension
of CKY to HRGs has been known for some time,
the desire to use graph grammars for large-scale
NLP applications introduces some practical con-
siderations not accounted for in Lautemann?s orig-
inal presentation. We have provided a detailed de-
scription of our refinement of his algorithm and its
implementation. It runs in O((3dn)k+1) time and
requires O((2dn)k+1) space, where n is the num-
ber of nodes in the input graph, d is its maximum
degree, and k is the maximum treewidth of the
rule right-hand sides in the grammar. We have
also described how to extend this algorithm to
synchronous parsing. The parsing algorithms de-
scribed in this paper are implemented in the Boli-
nas toolkit.3
3The Bolinas toolkit can be downloaded from
?http://www.isi.edu/licensed-sw/bolinas/?.
931
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful comments. This research was sup-
ported in part by ARO grant W911NF-10-1-0533.
References
Stefan Arnborg, Derek G. Corneil, and Andrzej
Proskurowski. 1987. Complexity of finding embed-
dings in a k-tree. SIAM Journal on Algebraic and
Discrete Methods, 8(2).
Hans L. Bodlaender. 1997. Treewidth: Algorithmic
techniques and results. In Proc. 22nd International
Symposium on Mathematical Foundations of Com-
puter Science (MFCS ?97), pages 29?36, Berlin.
Springer-Verlag.
Marek Cygan, Jesper Nederlof, Marcin Pilipczuk,
Micha? Pilipczuk, Johan M. M. van Rooij, and
Jakub Onufry Wojtaszczyk. 2011. Solving connec-
tivity problems parameterized by treewidth in single
exponential time. Computing Research Repository,
abs/1103.0534.
Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95?162. World Scientific.
Daniel Gildea. 2011. Grammar factorization by
tree decomposition. Computational Linguistics,
37(1):231?248.
Vibhav Gogate and Rina Dechter. 2004. A complete
anytime algorithm for treewidth. In Proceedings of
the Conference on Uncertainty in Artificial Intelli-
gence.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. COLING.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages and Automata, volume 3, pages 69?124.
Springer.
Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399?421.
Steffen Mazanek and Mark Minas. 2008. Parsing of
hyperedge replacement grammars with graph parser
combinators. In Proc. 7th International Work-
shop on Graph Transformation and Visual Modeling
Techniques.
Richard Moot. 2008. Lambek grammars, tree ad-
joining grammars and hyperedge replacement gram-
mars. In Proc. TAG+9, pages 65?72.
Grzegorz Rozenberg and Emo Welzl. 1986. Bound-
ary NLC graph grammars?basic definitions, nor-
mal forms, and complexity. Information and Con-
trol, 69:136?167.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Lappoon Tang and Raymond Mooney. 2001. Using
multiple clause constructors in inductive logic pro-
gramming for semantic parsing. In Proc. European
Conference on Machine Learning.
932
