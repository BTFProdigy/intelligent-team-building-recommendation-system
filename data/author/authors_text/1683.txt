Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 315?322, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Enhanced Answer Type Inference from Questions using Sequential Models
Vijay Krishnan and Sujatha Das and Soumen Chakrabarti?
Computer Science and Engineering Department, IIT Bombay, India
Abstract
Question classification is an important
step in factual question answering (QA)
and other dialog systems. Several at-
tempts have been made to apply statistical
machine learning approaches, including
Support Vector Machines (SVMs) with
sophisticated features and kernels. Curi-
ously, the payoff beyond a simple bag-of-
words representation has been small. We
show that most questions reveal their class
through a short contiguous token subse-
quence, which we call its informer span.
Perfect knowledge of informer spans can
enhance accuracy from 79.4% to 88%
using linear SVMs on standard bench-
marks. In contrast, standard heuristics
based on shallow pattern-matching give
only a 3% improvement, showing that the
notion of an informer is non-trivial. Us-
ing a novel multi-resolution encoding of
the question?s parse tree, we induce a Con-
ditional Random Field (CRF) to identify
informer spans with about 85% accuracy.
Then we build a meta-classifier using a
linear SVM on the CRF output, enhancing
accuracy to 86.2%, which is better than all
published numbers.
1 Introduction
An important step in factual question answering
(QA) and other dialog systems is to classify the
question (e.g., Who painted Olympia?) to the antic-
ipated type of the answer (e.g., person). This step
is called ?question classification? or ?answer type
identification?.
The answer type is picked from a hand-built tax-
onomy having dozens to hundreds of answer types
(Harabagiu et al, 2000; Hovy et al, 2001; Kwok et
al., 2001; Zheng, 2002; Dumais et al, 2002). QA
? soumen@cse.iitb.ac.in
systems can use the answer type to short-list answer
tokens from passages retrieved by an information re-
trieval (IR) subsystem, or use the type together with
other question words to inject IR queries.
Early successful QA systems used manually-
constructed sets of rules to map a question to a
type, exploiting clues such as the wh-word (who,
where, when, how many) and the head of noun
phrases associated with the main verb (what is the
tallest mountain in . . .).
With the increasing popularity of statistical NLP,
Li and Roth (2002), Hacioglu and Ward (2003) and
Zhang and Lee (2003) used supervised learning for
question classification on a data set from UIUC that
is now standard1. It has 6 coarse and 50 fine answer
types in a two-level taxonomy, together with 5500
training and 500 test questions. Webclopedia (Hovy
et al, 2001) has also published its taxonomy with
over 140 types.
The promise of a machine learning approach is
that the QA system builder can now focus on de-
signing features and providing labeled data, rather
than coding and maintaining complex heuristic rule-
bases. The data sets and learning systems quoted
above have made question classification a well-
defined and non-trivial subtask of QA for which al-
gorithms can be evaluated precisely, isolating more
complex factors at work in a complete QA system.
Prior work: Compared to human performance,
the accuracy of question classifiers is not high. In all
studies, surprisingly slim gains have resulted from
sophisticated design of features and kernels.
Li and Roth (2002) used a Sparse Network of
Winnows (SNoW) (Khardon et al, 1999). Their fea-
tures included tokens, parts of speech (POS), chunks
(non-overlapping phrases) and named entity (NE)
tags. They achieved 78.8% accuracy for 50 classes,
which improved to 84.2% on using an (unpublished,
to our knowledge) hand-built dictionary of ?seman-
tically related words?.
1http://l2r.cs.uiuc.edu/?cogcomp/Data/
QA/QC/
315
Hacioglu and Ward (2003) used linear support
vector machines (SVMs) with question word 2-
grams and error-correcting output codes (ECOC)?
but no NE tagger or related word dictionary?to get
80.2?82% accuracy.
Zhang and Lee (2003) used linear SVMs with
all possible question word q-grams, and obtained
79.2% accuracy. They went on to design an inge-
nious kernel on question parse trees, which yielded
visible gains for the 6 coarse labels, but only ?slight?
gains for the 50 fine classes, because ?the syntactic
tree does not normally contain the information re-
quired to distinguish between the various fine cate-
gories within a coarse category?.
Algorithm 6-class 50-class
Li and Roth, SNoW (1) 78.8(2)
Hacioglu et al, SVM+ECOC ? 80.2?82
Zhang & Lee, LinearSVMq 87.4 79.2
Zhang & Lee, TreeSVM 90 ?
SVM, ?perfect? informer 94.2 88
SVM, CRF-informer 93.4 86.2
Table 1: Summary of % accuracy for UIUC data.
(1) SNoW accuracy without the related word dictio-
nary was not reported. With the related-word dic-
tionary, it achieved 91%. (2) SNoW with a related-
word dictionary achieved 84.2% but the other algo-
rithms did not use it. Our results are summarized in
the last two rows, see text for details.
Our contributions: We introduce the notion of
the answer type informer span of the question (in
?2): a short (typically 1?3 word) subsequence of
question tokens that are adequate clues for question
classification; e.g.: How much does an adult ele-
phant weigh?
We show (in ?3.2) that a simple linear SVM us-
ing features derived from human-annotated informer
spans beats all known learning approaches. This
confirms our suspicion that the earlier approaches
suffered from a feature localization problem.
Of course, informers are useful only if we can find
ways to automatically identify informer spans. Sur-
prisingly, syntactic pattern-matching and heuristics
widely used in QA systems are not very good at cap-
turing informer spans (?3.3). Therefore, the notion
of an informer is non-trivial.
Using a parse of the question sentence, we derive
a novel set of multi-resolution features suitable for
training a conditional random field (CRF) (Lafferty
et al, 2001; Sha and Pereira, 2003). Our feature de-
sign paradigm may be of independent interest (?4).
Our informer tagger is about 85?87% accurate.
We use a meta-learning framework (Chan and
Stolfo, 1993) in which a linear SVM predicts the an-
swer type based on features derived from the origi-
nal question as well as the output of the CRF. This
meta-classifier beats all published numbers on stan-
dard question classification benchmarks (?4.4). Ta-
ble 1 (last two rows) summarizes our main results.
2 Informer overview
Our key insight is that a human can classify a ques-
tion based on very few tokens gleaned from skeletal
syntactic information. This is certainly true of the
most trivial classes (Who wrote Hamlet? or How
many dogs pull a sled at Iditarod?) but is also true of
more subtle clues (How much does a rhino weigh?).
In fact, informal experiments revealed the surpris-
ing property that only one contiguous span of tokens
is adequate for a human to classify a question. E.g.,
in the above question, a human does not even need
the how much clue once the word weigh is avail-
able. In fact, ?How much does a rhino cost?? has an
identical syntax but a completely different answer
type, not revealed by how much alone. The only
exceptions to the single-span hypothesis are multi-
function questions like ?What is the name and age
of . . .?, which should be assigned to multiple answer
types. In this paper we consider questions where one
type suffices.
Consider another question with multiple clues:
Who is the CEO of IBM? In isolation, the clue who
merely tells us that the answer might be a person or
country or organization, while CEO is perfectly pre-
cise, rendering who unnecessary. All of the above
applies a forteriori to what and which clues, which
are essentially uninformative on their own, as in
?What is the distance between Pisa and Rome??
Conventional QA systems use mild analysis on
the wh-clues, and need much more sophistication on
the rest of the question (e.g. inferring author from
wrote, and even verb subcategorization). We submit
that a single, minimal, suitably-chosen contiguous
316
span of question token/s, defined as the informer
span of the question, is adequate for question clas-
sification.
The informer span is very sensitive to the struc-
ture of clauses, phrases and possessives in the ques-
tion, as is clear from these examples (informers ital-
icized): ?What is Bill Clinton?s wife?s profession?,
and ?What country?s president was shot at Ford?s
Theater?. The choice of informer spans also de-
pends on the target classification system. Initially
we wished to handle definition questions separately,
and marked no informer tokens in ?What is digi-
talis?. However, what is is an excellent informer
for the UIUC class DESC:def (description, defi-
nition).
3 The meta-learning approach
We propose a meta-learning approach (?3.1) in
which the SVM can use features from the original
question as well as its informer span. We show
(?3.2) that human-annotated informer spans lead to
large improvements in accuracy. However, we show
(?3.3) that simple heuristic extraction rules com-
monly used in QA systems (e.g. head of noun phrase
following wh-word) cannot provide informers that
are nearly as useful. This naturally leads us to de-
signing an informer tagger in ?4.
Figure 1 shows our meta-learning (Chan and
Stolfo, 1993) framework. The combiner is a linear
multi-class one-vs-one SVM2, as in the Zhang and
Lee (2003) baseline. We did not use ECOC (Ha-
cioglu and Ward, 2003) because the reported gain is
less than 1%.
The word feature extractor selects unigrams and
q-grams from the question. In our experience, q =
1 or q = 2 were best; if unspecified, all possible
qgrams were used. Through tuning, we also found
that the SVM ?C? parameter (used to trade between
training data fit and model complexity) must be set
to 300 to achieve their published baseline numbers.
3.1 Adding informer features
We propose two very simple ways to derive features
from informers for use with SVMs. Initially, assume
that perfect informers are known for all questions;
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
question CRF Informer
span tagger
Word and qgram
feature extractor
Informer
feature extractor
Combined feature vector
class
SV
M
 
M
et
a 
Le
ar
n
er
Figure 1: The meta-learning approach.
later (?4) we study how to predict informers.
Informer q-grams: This comprises of all word q-
grams within the informer span, for all possible q.
E.g., such features enable effective exploitation of
informers like length or height to classify to the
NUMBER:distance class in the UIUC data.
Informer q-gram hypernyms: For each word or
compound within the informer span that is a Word-
Net noun, we add all hypernyms of all senses. The
intuition is that the informer (e.g. author, crick-
eter, CEO) is often narrower than a broad ques-
tion class (HUMAN:individual). Following hy-
pernym links up to person via WordNet produces a
more reliably correlated feature.
Given informers, other question words might
seem useless to the classifier. However, retaining
regular features from other question words is an ex-
cellent idea for the following reasons.
First, we kept word sense disambiguation (WSD)
outside the scope of this work because WSD en-
tails computation costs, and is unlikely to be reliable
on short single-sentence questions. Questions like
How long . . . or Which bank . . . can thus become
ambiguous and corrupt the informer hypernym fea-
tures. Additional question words can often help nail
the correct class despite the feature corruption.
Second, while our CRF-based approach to in-
former span tagging is better than obvious alterna-
tives, it still has a 15% error rate. For the questions
where the CRF prediction is wrong, features from
non-informer words give the SVM an opportunity to
still pick the correct question class.
Word features: Based on the above discussion,
one boolean SVM feature is created for every word
q-gram over all question tokens. In experiments, we
found bigrams (q = 2) to be most effective, closely
followed by unigrams (q = 1). As with informers,
we can also use hypernyms of regular words as SVM
317
features (marked ?Question bigrams + hypernyms?
in Table 2).
3.2 Benefits from ?perfect? informers
We first wished to test the hypothesis that identi-
fying informer spans to an SVM learner can im-
prove classification accuracy. Over and above the
class labels, we had two volunteers tag the 6000
UIUC questions with informer spans (which we call
?perfect??agreement was near-perfect).
Features Coarse Fine
Question trigrams 91.2 77.6
All question qgrams 87.2 71.8
All question unigrams 88.4 78.2
Question bigrams 91.6 79.4
+informer q-grams 94.0 82.4
+informer hypernyms 94.2 88.0
Question unigrams + all informer 93.4 88.0
Only informer 92.2 85.0
Question bigrams + hypernyms 91.6 79.4
Table 2: Percent accuracy with linear SVMs, ?per-
fect? informer spans, and various feature encodings.
Observe in Table 2 that the unigram baseline is
already quite competitive with the best prior num-
bers, and exploiting perfect informer spans beats all
known numbers. It is clear that both informer q-
grams and informer hypernyms are very valuable
features for question classification. The fact that no
improvement was obtained with over Question bi-
grams using Question hypernyms highlights the im-
portance of choosing a few relevant tokens as in-
formers and designing suitable features on them.
Table 3 (columns b and e) shows the benefits from
perfect informers broken down into broad question
types. Questions with what as the trigger are the
biggest beneficiaries, and they also form by far the
most frequent category.
The remaining question, one that we address in
the rest of the paper, is whether we can effectively
and accurately automate the process of providing in-
former spans to the question classifier.
3.3 Informers provided by heuristics
In ?4 we will propose a non-trivial solution to the
informer-tagging problem. Before that, we must jus-
tify that such machinery is indeed required.
Some leading QA systems extract words very
similar in function to informers from the parse tree
of the question. Some (Singhal et al, 2000) pick
the head of the first noun phrase detected by a shal-
low parser, while others use the head of the noun
phrase adjoining the main verb (Ramakrishnan et al,
2004). Yet others (Harabagiu et al, 2000; Hovy
et al, 2001) use hundreds of (unpublished to our
knowledge) hand-built pattern-matching rules on the
output of a full-scale parser.
A natural baseline is to use these extracted words,
which we call ?heuristic informers?, with an SVM
just like we used ?perfect? informers. All that re-
mains is to make the heuristics precise.
How: For questions starting with how, we use the
bigram starting with how unless the next word
is a verb.
Wh: If the wh-word is not how, what or which, use
the wh-word in the question as a separate fea-
ture.
WhNP: For questions having what and which, use
the WHNP if it encloses a noun. WHNP is the
Noun Phrase corresponding to the Wh-word,
given by a sentence parser (see ?4.2).
NP1: Otherwise, for what and which questions, the
first (leftmost) noun phrase is added to yet an-
other feature subspace.
Table 3 (columns c and f) shows that these
already-messy heuristic informers do not capture the
same signal quality as ?perfect? informers. Our find-
ings corroborate Li and Roth (2002), who report lit-
tle benefit from adding head chunk features for the
fine classification task.
Moreover, observe that using heuristic informer
features without any word features leads to rather
poor performance (column c), unlike using perfect
informers (column b) or even CRF-predicted in-
former (column d, see ?4). These clearly establish
that the notion of an informer is nontrivial.
4 Using CRFs to label informers
Given informers are useful but nontrivial to recog-
nize, the next natural question is, how can we learn
to identify them automatically? From earlier sec-
tions, it is clear (and we give evidence later, see Ta-
ble 5) that sequence and syntax information will be
318
6 coarse classes
B Only Informers B+ B+ B+
Type #Quest. (Bigrams) Perf.Inf H.Inf CRF.Inf Perf.Inf H.Inf CRF.Inf
what 349 88.8 89.4 69.6 79.3 91.7 87.4 91.4
which 11 72.7 100.0 45.4 81.8 100.0 63.6 81.8
when 28 100.0 100.0 100.0 100.0 100.0 100.0 100.0
where 27 100.0 96.3 100.0 96.3 100.0 100.0 100.0
who 47 100.0 100.0 100.0 100.0 100.0 100.0 100.0
how * 32 100.0 96.9 100.0 100.0 100.0 100.0 100.0
rest 6 100.0 100.0 100.0 66.7 100.0 66.7 66.7
Total 500 91.6 92.2 77.2 84.6 94.2 90.0 93.4
50 fine classes
what 349 73.6 82.2 61.9 78.0 85.1 79.1 83.1
which 11 81.8 90.9 45.4 73.1 90.9 54.5 81.8
when 28 100.0 100.0 100.0 100.0 100.0 100.0 100.0
where 27 92.6 85.2 92.6 88.9 88.9 92.5 88.9
who 47 97.9 93.6 93.6 93.6 100.0 100.0 97.9
how * 32 87.5 84.3 81.2 78.1 87.5 90.6 90.6
rest 6 66.7 66.7 66.7 66.7 100.0 66.7 66.7
Total 500 79.4 85.0 69.6 78.0 88.0 82.6 86.2
a b c d e f g
Table 3: Summary of % accuracy broken down by question type (referred from ?3.2, ?3.3 and ?4.4). a:
question bigrams, b: perfect informers only, c: heuristic informers only, d: CRF informers only, e?g:
bigrams plus perfect, heuristic and CRF informers.
important.
We will model informer span identification as a
sequence tagging problem. An automaton makes
probabilistic transitions between hidden states y,
one of which is an ?informer generating state?, and
emits tokens x. We observe the tokens and have to
guess which were produced from the ?informer gen-
erating state?.
Hidden Markov models are extremely popular for
such applications, but recent work has shown that
conditional random fields (CRFs) (Lafferty et al,
2001; Sha and Pereira, 2003) have a consistent ad-
vantage over traditional HMMs in the face of many
redundant features. We refer the reader to the above
references for a detailed treatment of CRFs. Here
we will regard a CRF as largely a black box3.
To train a CRF, we need a set of state nodes, a
transition graph on these nodes, and tokenized text
where each token is assigned a state. Once the CRF
is trained, it can be applied to a token sequence, pro-
3We used http://crf.sourceforge.net/
ducing a predicted state sequence.
4.1 State transition models
We started with the common 2-state ?in/out? model
used in information extraction, shown in the left half
of Figure 2. State ?1? is the informer-generating
state. Either state can be initial and final (double
circle) states.
0 1 0 1 2
What kind of an animal is Winnie the Pooh
What, kind,
of, an, is,
Winnie, the,
Pooh 
animal
What, kind,
of, an 
is, Winnie,
the, Pooh 
animal
start start
Figure 2: 2- and 3-state transition models.
The 2-state model can be myopic. Consider the
question pair
319
A: What country is the largest producer of wheat?
B: Name the largest producer of wheat
The i?1 context of producer is identical in A and
B. In B, for want of a better informer, we would want
producer to be flagged as the informer, although it
might refer to a country, person, animal, company,
etc. But in A, country is far more precise.
Any 2-state model that depends on positions i?1
to define features will fail to distinguish between A
and B, and might select both country and producer
in A. As we have seen with heuristic informers, pol-
luting the informer pool can significantly hurt SVM
accuracy.
Therefore we also use the 3-state ?begin/in/out?
(BIO) model. The initial state cannot be ?2? in the
3-state model; all states can be final. The 3-state
model allows at most one informer span. Once the
3-state model chooses country as the informer, it is
unlikely to stretch state 1 up to producer.
There is no natural significance to using four or
more states. Besides, longer range syntax dependen-
cies are already largely captured by the parser.
What is the capital city of Japan
WP VBZ DT NN NN IN NNP
NP NP
PP
NP
VP
SQ
SBARQ
WHNP
0
1
2
3
4
5
6

Le
v
e
l
Figure 3: Stanford Parser output example.
4.2 Features from a parse of the question
Sentences with similar parse trees are likely to have
the informer in similar positions. This was the in-
tuition behind Zhang et al?s tree kernel, and is also
our starting point. We used the Stanford Lexicalized
Parser (Klein and Manning, 2003) to parse the ques-
tion. (We assume familiarity with parse tree notation
for lack of space.) Figure 3 shows a sample parse
tree organized in levels. Our first step was to trans-
i 1 2 3 4 5 6 7
yi 0 0 0 1 1 2 2
xi What is the capital city of Japan
` ? Features for xis
1 WP,1 VBZ,1 DT,1 NN,1 NN,1 IN,1 NNP,1
2 WHNP,1 VP,1 NP,1 NP,1 NP,1 Null,1 NP,2
3 Null,1 Null,1 Null,1 Null,1 Null,1 PP,1 PP,1
4 Null,1 Null,1 NP,1 NP,1 NP,1 NP,1 NP,1
5 Null,1 SQ,1 SQ,1 SQ,1 SQ,1 SQ,1 SQ,1
6 SBARQ SBARQSBARQSBARQSBARQSBARQSBARQ
Table 4: A multi-resolution tabular view of the ques-
tion parse showing tag and num attributes. capital
city is the informer span with y = 1.
late the parse tree into an equivalent multi-resolution
tabular format shown in Table 4.
Cells and attributes: A labeled question com-
prises the token sequence xi; i = 1, . . . and the label
sequence yi, i = 1, . . . Each xi leads to a column
vector of observations. Therefore we use matrix no-
tation to write down x: A table cell is addressed as
x[i, `] where i is the token position (column index)
and ` is the level or row index, 1?6 in this example.
(Although the parse tree can be arbitrarily deep, we
found that using features from up to level ` = 2 was
adequate.)
Intuitively, much of the information required for
spotting an informer can be obtained from the part
of speech of the tokens and phrase/clause attachment
information. Conversely, specific word information
is generally sparse and misleading; the same word
may or may not be an informer depending on its po-
sition. E.g., ?What birds eat snakes?? and ?What
snakes eat birds?? have the same words but different
informers. Accordingly, we observe two properties
at each cell:
tag: The syntactic class assigned to the cell by
the parser, e.g. x[4, 2].tag = NP. It is well-known
that POS and chunk information are major clues to
informer-tagging, specifically, informers are often
nouns or noun phrases.
num: Many heuristics exploit the fact that the first
NP is known to have a higher chance of containing
informers than subsequent NPs. To capture this po-
sitional information, we define num of a cell at [i, `]
as one plus the number of distinct contiguous chunks
to the left of [i, `] with tags equal to x[4, 2].tag.
E.g., at level 2 in the table above, the capital city
320
forms the first NP, while Japan forms the second NP.
Therefore x[7, 2].num = 2.
In conditional models, it is notationally conve-
nient to express features as functions on (xi, yi). To
one unfamiliar with CRFs, it may seem strange that
yi is passed as an argument to features. At training
time, yi is indeed known, and at testing time, the
CRF algorithm efficiently finds the most probable
sequence of yis using a Viterbi search. True labels
are not revealed to the CRF at testing time.
Cell features IsTag and IsNum: E.g., the ob-
servation ?y4 = 1 and x[4, 2].tag = NP? is cap-
tured by the statement that ?position 4 fires the fea-
ture IsTag1,NP,2? (which has a boolean value).
There is an IsTagy,t,` feature for each (y, t, `)
triplet. Similarly, for every possible state y, ev-
ery possible num value n (up to some maximum
horizon), and every level `, we define boolean fea-
tures IsNumy,n,`. E.g., position 7 fires the feature
IsNum2,2,2 in the 3-state model, capturing the state-
ment ?x[7, 2].num = 2 and y7 = 2?.
Adjacent cell features IsPrevTag and
IsNextTag: Context can be exploited by a
CRF by coupling the state at position i with
observations at positions adjacent to position i
(extending to larger windows did not help). To
capture this, we use more boolean features: posi-
tion 4 fires the feature IsPrevTag1,DT,1 because
x[3, 1].tag = DT and y4 = 1. Position 4 also fires
IsPrevTag1,NP,2 because x[3, 2].tag = NP and
y4 = 1. Similarly we define a IsNextTagy,t,`
feature for each possible (y, t, `) triple.
State transition features IsEdge: Position i
fires feature IsEdgeu,v if yi?1 = u and yi = v.
There is one such feature for each state-pair (u, v)
allowed by the transition graph. In addition we have
sentinel features IsBeginu and IsEndu marking
the beginning and end of the token sequence.
4.3 Informer-tagging accuracy
We study the accuracy of our CRF-based informer
tagger wrt human informer annotations. In the next
section we will see the effect of CRF tagging on
question classification.
There are at least two useful measures of
informer-tagging accuracy. Each question has a
known set Ik of informer tokens, and gets a set
of tokens Ic flagged as informers by the CRF. For
each question, we can grant ourself a reward of 1 if
Ic = Ik, and 0 otherwise. In ?3.1, informers were
regarded as a separate (high-value) bag of words.
Therefore, overlap between Ic and Ik would be a
reasonable predictor of question classification accu-
racy. We use the Jaccard similarity |Ik?Ic|/|Ik?Ic|.
Table 5 shows the effect of using diverse feature sets.
Fraction Jaccard
Features used Ic = Ik overlap
IsTag 0.368 0.396
+IsNum 0.474 0.542
+IsPrevTag+IsNextTag 0.692 0.751
+IsEdge+IsBegin+IsEnd 0.848 0.867
Table 5: Effect of feature choices.
? IsTag features are not adequate.
? IsNum features improve accuracy 10?20%.
? IsPrevTag and IsNextTag (?+Prev
+Next?) add over 20% of accuracy.
? IsEdge transition features help exploit
Markovian dependencies and adds another
10?15% accuracy, showing that sequential
models are indeed required.
Type #Quest. Heuristic 2-state 3-state
Informers CRF CRF
what 349 57.3 68.2 83.4
which 11 77.3 83.3 77.2
when 28 75.0 98.8 100.0
where 27 84.3 100.0 96.3
who 47 55.0 47.2 96.8
how * 32 90.6 88.5 93.8
rest 6 66.7 66.7 77.8
Total 500 62.4 71.2 86.7
Table 6: Effect of number of CRF states, and com-
parison with the heuristic baseline (Jaccard accuracy
expressed as %).
Table 6 shows that the 3-state CRF performs
much better than the 2-state CRF, especially on diffi-
cult questions with what and which. It also compares
the Jaccard accuracy of informers found by the CRF
vs. informers found by the heuristics described in
?3.3. Again we see a clear superiority of the CRF
321
approach.
Unlike the heuristic approach, the CRF approach
is relatively robust to the parser emitting a somewhat
incorrect parse tree, which is not uncommon. The
heuristic approach picks the ?easy? informer, who,
over the better one, CEO, in ?Who is the CEO of
IBM?. Its bias toward the NP-head can also be a
problem, as in ?What country?s president . . .?.
4.4 Question classification accuracy
We have already seen in ?3.2 that perfect knowledge
of informers can be a big help. Because the CRF
can make mistakes, the margin may decrease. In this
section we study this issue.
We used questions with human-tagged informers
(?3.2) to train a CRF. The CRF was applied back
on the training questions to get informer predictions,
which were used to train the 1-vs-1 SVM meta-
learner (?3). Using CRF-tagged and not human-
tagged informers may seem odd, but this lets the
SVM learn and work around systematic errors in
CRF outputs.
Results are shown in columns d and g of Table 3.
Despite the CRF tagger having about 15% error, we
obtained 86.2% SVM accuracy which is rather close
to the the SVM accuracy of 88% with perfect in-
formers.
The CRF-generated tags, being on the training
data, might be more accurate that would be for un-
seen test cases, potentially misleading the SVM.
This turns out not to be a problem: clearly we are
very close to the upper bound of 88%. In fact, anec-
dotal evidence suggests that using CRF-assigned
tags actually helped the SVM.
5 Conclusion
We presented a new approach to inferring the type
of the answer sought by a well-formed natural lan-
guage question. We introduced the notion of a span
of informer tokens and extract it using a sequential
graphical model with a novel feature representation
derived from the parse tree of the question. Our ap-
proach beats the accuracy of recent algorithms, even
ones that used max-margin methods with sophisti-
cated kernels defined on parse trees.
An intriguing feature of our approach is that
when an informer (actor) is narrower than the ques-
tion class (person), we can exploit direct hyper-
nymy connections like actor to Tom Hanks, if avail-
able. Existing knowledge bases like WordNet and
Wikipedia, combined with intense recent work (Et-
zioni et al, 2004) on bootstrapping is-a hierarchies,
can thus lead to potentially large benefits.
Acknowledgments: Thanks to Sunita Sarawagi
for help with CRFs, and the reviewers for improv-
ing the presentation.
References
P. K Chan and S. J Stolfo. 1993. Experiments in mul-
tistrategy learning by meta-learning. In CIKM, pages
314?323, Washington, DC.
S Dumais, M Banko, E Brill, J Lin, and A Ng. 2002.
Web question answering: Is more always better? In
SIGIR, pages 291?298.
O Etzioni, M Cafarella, et al 2004. Web-scale informa-
tion extraction in KnowItAll. In WWW Conference,
New York. ACM.
K Hacioglu and W Ward. 2003. Question classifica-
tion with support vector machines and error correcting
codes. In HLT, pages 28?30.
S Harabagiu, D Moldovan, M Pasca, R Mihalcea, M Sur-
deanu, R Bunescu, R Girju, V Rus, and P Morarescu.
2000. FALCON: Boosting knowledge for answer en-
gines. In TREC 9, pages 479?488. NIST.
E Hovy, L Gerber, U Hermjakob, M Junk, and C.-Y
Lin. 2001. Question answering in Webclopedia. In
TREC 9. NIST.
R Khardon, D Roth, and L. G Valiant. 1999. Relational
learning for NLP using linear threshold elements. In
IJCAI.
D Klein and C. D Manning. 2003. Accurate unlexical-
ized parsing. In ACL, volume 41, pages 423?430.
C Kwok, O Etzioni, and D. S Weld. 2001. Scaling ques-
tion answering to the Web. In WWW Conference, vol-
ume 10, pages 150?161, Hong Kong.
J Lafferty, A McCallum, and F Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
X Li and D Roth. 2002. Learning question classifiers. In
COLING, pages 556?562.
G Ramakrishnan, S Chakrabarti, D. A Paranjpe, and
P Bhattacharyya. 2004. Is question answering an ac-
quired skill? In WWW Conference, pages 111?120,
New York.
F Sha and F Pereira. 2003. Shallow parsing with condi-
tional random fields. In HLT-NAACL, pages 134?141.
A Singhal, S Abney, M Bacchiani, M Collins, D Hindle,
and F Pereira. 2000. AT&T at TREC-8. In TREC 8,
pages 317?330. NIST.
D Zhang and W Lee. 2003. Question classification using
support vector machines. In SIGIR, pages 26?32.
Z Zheng. 2002. AnswerBus question answering system.
In HLT.
322
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1435?1446,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Citation-Enhanced Keyphrase Extraction from Research Papers:
A Supervised Approach
Cornelia Caragea
1
, Florin Bulgarov
1
, Andreea Godea
1
, Sujatha Das Gollapalli
2
1
Computer Science and Engineering, University of North Texas, TX, USA
2
Institute for Infocomm Research, A*STAR, Singapore
ccaragea@unt.edu, FlorinBulgarov@my.unt.edu,
AndreeaGodea@my.unt.edu, gsdas@cse.psu.edu
Abstract
Given the large amounts of online textual
documents available these days, e.g., news
articles, weblogs, and scientific papers, ef-
fective methods for extracting keyphrases,
which provide a high-level topic descrip-
tion of a document, are greatly needed. In
this paper, we propose a supervised model
for keyphrase extraction from research pa-
pers, which are embedded in citation net-
works. To this end, we design novel fea-
tures based on citation network informa-
tion and use them in conjunction with tra-
ditional features for keyphrase extraction
to obtain remarkable improvements in per-
formance over strong baselines.
1 Introduction
Keyphrase extraction is the problem of automat-
ically extracting important phrases or concepts
(i.e., the essence) of a document. Keyphrases
provide a high-level topic description of a docu-
ment and are shown to be rich sources of informa-
tion for many applications such as document clas-
sification, clustering, recommendation, indexing,
searching, and summarization (Jones and Stave-
ley, 1999; Zha, 2002; Hammouda et al., 2005;
Pudota et al., 2010; Turney, 2003). Despite the
fact that keyphrase extraction has been widely re-
searched in the natural language processing com-
munity, its performance is still far from being sat-
isfactory (Hasan and Ng, 2014).
Many previous approaches to keyphrase extrac-
tion generally used only the textual content of
a target document to extract keyphrases (Hulth,
2003; Mihalcea and Tarau, 2004; Liu et al., 2010).
Recently, Wan and Xiao (2008) proposed a model
that incorporates a local neighborhood of a doc-
ument. However, their neighborhood is limited
to textually-similar documents, where the cosine
similarity between the tf-idf vectors of documents
is used to compute their similarity. We posit
that, in addition to a document?s textual content
and textually-similar neighbors, other informative
neighborhoods exist that have the potential to im-
prove keyphrase extraction. For example, in a
scholarly domain, research papers are not isolated.
Rather, they are highly inter-connected in giant ci-
tation networks, in which papers cite or are cited
by other papers. In a citation network, information
flows from one paper to another via the citation re-
lation (Shi et al., 2010). This information flow and
the influence of one paper on another are specifi-
cally captured by means of citation contexts, i.e.,
short text segments surrounding a citation?s men-
tion. These contexts are not arbitrary, but they
serve as brief summaries of a cited paper. Figure
1 illustrates this idea using a small citation net-
work of a paper by Rendle et al. (2010) that cites
(Zimdars et al., 2001), (Hu et al., 2008), (Pan and
Scholz, 2009) and (Shani et al., 2005) and is cited
by (Cheng et al., 2013). The citation mentions
and citation contexts are shown with a dashed line.
Note the high overlap between the words in con-
texts and those in the title and abstract (shown in
bold) and the author-annotated keywords.
One question that can be raised is the following:
Can we effectively exploit information available
in large inter-linked document networks in order
to improve the performance of keyphrase extrac-
tion? The research that we describe in this paper
addresses specifically this question using citation
networks of research papers as a case study. Ex-
tracting keyphrases that can accurately ?represent?
research papers is crucial to dealing with the large
numbers of research papers published during these
?big data? times. The importance of keyphrase ex-
traction from research papers is also emphasized
by the recent SemEval 2010 Shared Task on this
topic (Kim et al., 2010; Kim et al., 2013).
Our contributions. We present a supervised
1435
Figure 1: A small citation network corresponding to a paper by Rendle et al. (2010).
approach to keyphrase extraction from research
papers that, in addition to the information con-
tained in a paper itself, effectively incorporates,
in the learned models, information from the pa-
per?s local neighborhood available in citation net-
works. To this end, we design novel features for
keyphrase extraction based on citation context in-
formation and use them in conjunction with tradi-
tional features in a supervised probabilistic frame-
work. We show empirically that the proposed
models substantially outperform strong baselines
on two datasets of research papers compiled from
two machine learning conferences: the World
Wide Web and Knowledge Discovery from Data.
The rest of the paper is organized as follows:
We summarize closely related work in Section 2.
The supervised classification for keyphrase extrac-
tion is discussed in Section 3. Experiments and re-
sults are presented in Section 4, followed by con-
clusions and future directions of our work.
2 Related Work
Many approaches to keyphrase extraction have
been proposed in the literature along two lines of
research: supervised and unsupervised, using dif-
ferent types of documents including scientific ab-
stracts, newswire documents, meeting transcripts,
and webpages (Frank et al., 1999; Hulth, 2003;
Nguyen and Kan, 2007; Liu et al., 2009; Marujo
et al., 2013; Mihalcea and Tarau, 2004).
In the supervised line of research, keyphrase
extraction is formulated as a binary classification
problem, where candidate phrases are classified as
either positive (i.e., keyphrases) or negative (i.e.,
non-keyphrases) (Frank et al., 1999; Turney, 2000;
Hulth, 2003). Different feature sets and classifica-
tion algorithms gave rise to different models. For
example, Hulth (2003) used four different features
in conjunction with a bagging technique. These
features are: term frequency, collection frequency,
the relative position of the first occurrence and the
part-of-speech tag of a term. Frank et al. (1999)
developed a system called KEA that used only
two features: tf-idf (term frequency-inverse doc-
ument frequency) of a phrase and the distance of
a phrase from the beginning of a document (i.e.,
its relative position) and used them as input to
Na??ve Bayes. Nguyen and Kan (2007) extended
KEA to include features such as the distribution
of keyphrases among different sections of a re-
search paper, and the acronym status of a term. In
contrast to these works, we propose novel features
extracted from the local neighborhoods of docu-
ments available in interlinked document networks.
Medelyan et al. (2009) extended KEA as well to
integrate information from Wikipedia. In contrast,
we used only information intrinsic to our data. En-
hancing our models with Wikipedia information
would be an interesting future direction to pursue.
In the unsupervised line of research, keyphrase
extraction is formulated as a ranking problem,
where keyphrases are ranked using their tf (Barker
and Cornacchia, 2000), tf-idf (Zhang et al., 2007;
Lee and Kim, 2008; Liu et al., 2009; Tonella et al.,
2003), and term informativeness (Wu and Giles,
2013; Rennie and Jaakkola, 2005; Kireyev, 2009)
(among others). The ranking based on tf-idf has
1436
been shown to work well in practice (Liu et al.,
2009; Hasan and Ng, 2010) despite its simplicity.
Frantzi et al. (1998) combined linguistics and sta-
tistical information to extract technical terms from
documents in digital libraries. Graph-based al-
gorithms and centrality measures are also widely
used in unsupervised models. A word graph is
built for each document such that nodes corre-
spond to words and edges correspond to word as-
sociation patterns. Nodes are then ranked using
graph centrality measures such as PageRank and
its variants (Mihalcea and Tarau, 2004; Wan and
Xiao, 2008; Liu et al., 2010; Zhao et al., 2011),
HITS scores (Litvak and Last, 2008), as well as
node degree and betweenness (Boudin, 2013; Xie,
2005). Wan and Xiao (2008) were the first to
consider modeling a local neighborhood of a tar-
get document in addition to the document itself,
and applied this approach to news articles on the
Web. Their local neighborhood consists of textu-
ally similar documents, and did not capture infor-
mation contained in document networks.
Using terms from citation contexts of scientific
papers is not a new idea. It was used before in
various applications. For example, Ritchie et al.
(2006) used a combination of terms from citation
contexts and existing index terms of a paper to
improve indexing of cited papers. Citation con-
texts were also used to improve the performance of
citation recommendation systems (Kataria et al.,
2010; He et al., 2010) and to study author influ-
ence (Kataria et al., 2011). This idea of using
terms from citation contexts resembles the anal-
ysis of hyperlinks and the graph structure of the
Web, which are instrumental in Web search (Man-
ning et al., 2008). Many current Web search en-
gines build on the intuition that the anchor text
pointing to a page is a good descriptor of its con-
tent, and thus use anchor text terms as additional
index terms for a target webpage. The use of links
and anchor text was thoroughly researched for IR
tasks (Koolen and Kamps, 2010), broadening a
user?s search (Chakrabarti et al., 1998), query re-
finement (Kraft and Zien, 2004), and enriching
document representations (Metzler et al., 2009).
Moreover, citation contexts were used for scien-
tific paper summarization (Abu-Jbara and Radev,
2011; Qazvinian et al., 2010; Qazvinian and
Radev, 2008; Mei and Zhai, 2008; Lehnert et al.,
1990; Nakov et al., 2004). Among these, proba-
bly the most similar to our work is the work by
Qazvinian et al. (2010), where a set of important
keyphrases is extracted first from the citation con-
texts in which the paper to be summarized is cited
by other papers and then the ?best? subset of sen-
tences that contain such keyphrases is returned as
the summary. However, keyphrases in (Qazvinian
et al., 2010) are extracted using frequent n-grams
in a language model framework, whereas in our
work, we propose a supervised approach to a dif-
ferent task: keyphrase extraction. Mei and Zhai
(2008) used information from citation contexts to
determine what sentences of a paper are of high
impact (as measured by the influence of a target
paper on further studies of similar or related top-
ics). These sentences constitute the impact-based
summary of the paper.
Despite the use of citation contexts and anchor
text in many IR and NLP tasks, to our knowl-
edge, we are the first to propose the incorporation
of information available in citation networks for
keyphrase extraction. In our recent work (Gol-
lapalli and Caragea, 2014), we designed a fully
unsupervised graph-based algorithm that incorpo-
rates evidence from multiple sources (citation con-
texts as well as document content) in a flexible
manner to score keywords. In the current work,
we present a supervised approach to keyphrase ex-
traction from research papers that are embedded in
large citation networks, and propose novel features
that show improvement over strong supervised and
unsupervised baselines. To our knowledge, fea-
tures extracted from citation contexts have not
been used before for keyphrase extraction in a su-
pervised learning framework.
3 Problem Characterization
In citation networks, in addition to the informa-
tion contained in a paper itself, citing and cited
papers capture different aspects (e.g., topicality,
domain of study, algorithms used) about the tar-
get paper (Teufel et al., 2006), with citation con-
texts playing an instrumental role. A citation con-
text is defined as a window of n words surround-
ing a citation mention. We conjecture that cita-
tion contexts, which act as brief summaries about a
cited paper, provide additional clues in extracting
keyphrases for a target paper. These clues give rise
to the unique design of our model, called citation-
enhanced keyphrase extraction (CeKE).
3.1 Citation-enhanced Keyphrase Extraction
Our proposed citation-enhanced keyphrase extrac-
tion (CeKE) model is a supervised binary classifi-
1437
Feature Name Description
Existing features for keyphrase extraction
tf-idf term frequency * inverse document
frequency, computed from a target
paper; used in KEA
relativePos the position of the first occurrence of a
phrase divided by the total number of
tokens; used in KEA and Hulth?s methods
POS the part-of-speech tag of the phrase;
used in Hulth?s methods
Novel features - Citation Network Based
inCited if the phrase occurs in cited contexts
inCiting if the phrase occurs in citing contexts
citation tf-idf the tf-idf value of the phrase, computed
from the aggregated citation contexts
Novel features - Extensions of Existing Features
first position the distance of the first occurrence of
a phrase from the beginning of a paper
tf-idf-Over tf-idf larger than a threshold ?
firstPosUnder the distance of the first occurrence of a
phrase from the beginning of a paper is
below some value ?
Table 1: The list of features used in our model.
cation model, built on a combination of novel fea-
tures that capture information from citation con-
texts and existing features from previous works.
The features are described in ?3.1.1. CeKE classi-
fies candidate phrases as keyphrases (i.e., positive)
or non-keyphrases (i.e., negative) using Na??ve
Bayes classifiers. Positive examples for train-
ing correspond to manually annotated keyphrases
from the training research papers, whereas nega-
tive examples correspond to the remaining candi-
date phrases from these papers. The generation of
candidate phrases is explained in ?3.2.
Note that Na??ve Bayes classifies a phrase as a
keyphrase if the probability of the phrase belong-
ing to the positive class is greater than 0.5. How-
ever, the default threshold of 0.5 can be varied to
allow only high-confidence (e.g., 0.9 confidence)
phrases to be classified as keyphrases.
3.1.1 Features
We consider the following features in our model,
which are shown in Table 1. They are divided
into three categories: (1) Existing features for
keyphrase extraction include: tf-idf, i.e., the term
frequency - inverse document frequency of a can-
didate phrase, computed for each target paper;
This feature was used in KEA (Frank et al., 1999);
relative position, i.e., the position of the first oc-
currence of a phrase normalized by the length (in
the number of tokens) of the target paper; POS,
i.e., a phrase?s part-of-speech tag. If a phrase is
composed by more than one term, then the POS
will contain the tags of all terms. The relative posi-
tion was used in both KEA and Hulth (2003), and
POS was used in Hulth; (2) Novel features - Cita-
tion Network Based include: inCited and inCiting,
i.e., boolean features that are true if the candidate
phrase occurs in cited and citing contexts, respec-
tively. We differentiate between cited and citing
contexts for a paper: let d be a target paper and C
be a citation network such that d ? C. A cited con-
text for d is a context in which d is cited by some
paper d
i
in C. A citing context for d is a context
in which d is citing some paper d
j
in C. If a paper
is cited in multiple contexts by another paper, the
contexts are aggregated into a single one; citation
tf-idf, i.e., the tf-idf score of each phrase computed
from the citation contexts; (3) Novel features - Ex-
tend Other Existing Features include: first position
of a candidate phrase, i.e., the distance of the first
occurrence of a phrase from the beginning of a pa-
per; this is similar to relative position except that
it does not consider the length of a paper; tf-idf-
Over, i.e., a boolean feature, which is true if the
tf-idf of a candidate phrase is greater than a thresh-
old ?, and firstPosUnder, also a boolean feature,
which is true if the distance of the first occurrence
of a phrase from the beginning of a target paper is
below some value ?. This feature is similar to the
feature is-in-title, used previously in the literature
(Litvak and Last, 2008; Jiang et al., 2009). Both
tf-idf and citation tf-idf features showed better re-
sults when each tf was divided by the maximum tf
values from the target paper or citation contexts.
The tf-idf features have high values for phrases
that are frequent in a paper or citation contexts,
but are less frequent in collection and have low
values for phrases with high collection frequency.
We computed the idf component from each col-
lection used in experiments. Phrases that occur in
cited and citing contexts as well as early in a paper
are likely to be keyphrases since: (1) they capture
some aspect about the target paper and (2) authors
start to describe their problem upfront.
3.2 Generating Candidate Phrases
We generate candidate phrases from the textual
content of a target paper by applying parts-of-
1438
Dataset Num. (#) Average Average Average #uni- #bi- #tri-
Papers Cited Ctx. Citing Ctx. Keyphrases grams grams grams
WWW 425 15.45 18.78 4.87 680 1036 247
KDD 365 12.69 19.74 4.03 363 853 189
Table 2: A summary of our datasets.
speech filters. Consistent with previous works
(Hulth, 2003; Mihalcea and Tarau, 2004; Liu
et al., 2010; Wan and Xiao, 2008), only nouns
and adjectives are retained to form candidate
phrases. The generation process consists of two
steps. First, using the NLP Stanford part of speech
tagger, we preprocess each document and keep
only the nouns and adjectives corresponding to
{NN,NNS,NNP,NNPS, JJ}. We apply the
Porter stemmer on every word. The position of
each word is kept consistent with the initial state
of the document before any word removal is made.
Second, words extracted in the first step that
have contiguous positions in a document are con-
catenated into n-grams. We used unigrams, bi-
grams, and trigrams (n = 1, 2, 3) as candidate
phrases for classification. Similar to Wan and Xiao
(2008), we eliminated phrases that end with an ad-
jective and the unigrams that are adjectives.
4 Experiments and Results
In this section, we first describe our datasets and
then present experimental design and results.
4.1 Datasets
In order to test the performance of our proposed
approach, we built our own datasets since citation-
enhanced evaluation benchmarks are not available
for keyphrase extraction tasks. In particular, we
compiled two datasets consisting of research pa-
pers from two top-tier machine learning confer-
ences: World Wide Web (WWW) and Knowledge
Discovery and Data Mining (KDD). Our choice
for WWW and KDD was motivated by the avail-
ability of author-input keywords for each paper,
which we used as gold-standard for evaluation.
Using the CiteSeer
x
digital library
1
, we re-
trieved the papers published in WWW and KDD
(available in CiteSeer
x
), and their citation network
information, i.e., their cited and citing contexts.
Since our goal is to study the impact of citation
network information on extracting keyphrases, a
paper was considered for analysis if it had at least
1
http://citeseerx.ist.psu.edu/
one cited and one citing context. For each paper,
we used: the title and abstract (referred to as the
target paper) and its citation contexts. The rea-
son for not considering the entire text of a paper
is that scientific papers contain details, e.g., dis-
cussion of results, experimental design, notation,
that do not provide additional benefits for extract-
ing keyphrases. Hence, similar to (Hulth, 2003;
Mihalcea and Tarau, 2004; Liu et al., 2009), we
did not use the entire text of a paper. However, ex-
tracting keyphrases from sections such as ?intro-
duction? or ?conclusion? needs further attention.
From the pdf of each paper, we extracted the
author-input keyphrases. An analysis of these
keyphrases revealed that generally authors de-
scribe their work using, almost half of the time,
bigrams, followed by unigrams and only rarely us-
ing trigrams (or higher n-grams). A summary of
our datasets that contains the number of papers,
the average number of cited and citing contexts
per paper, the average number of keyphrases per
paper, and the number of unigrams, bigrams and
trigrams, in each collection, is shown in Table 2.
Consistent with previous works (Frank et al.,
1999; Hulth, 2003), the positive and negative ex-
amples in our datasets correspond to candidate
phrases that consist of up to three tokens. The
positive examples are candidate phrases that have
a match in the author-input keyphrases, whereas
negative examples correspond to the remaining
candidate phrases.
Context lengths. In CiteSeer
x
, citation con-
texts have about 50 words on each side of a citation
mention. A previous study by Ritchie et al. (2008)
shows that a fixed window length of about 100
words around a citation mention is generally effec-
tive for information retrieval tasks. For this reason,
we used the contexts provided by CiteSeer
x
di-
rectly. However, in future, it would be interesting
to incorporate in our models more sophisticated
approaches to identifying the text that is relevant
to a target citation (Abu-Jbara and Radev, 2012;
Teufel, 1999) and study the influence of context
lengths on the quality of extracted keyphrase.
1439
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280
Hulth - n-gram with tags 0.165 0.107 0.129 0.206 0.151 0.172
KEA 0.210 0.146 0.168 0.178 0.124 0.145
Table 3: The comparison of CeKE with supervised approaches on WWW and KDD collections.
4.2 Experimental Design
Our experiments are designed around the follow-
ing research questions:
1. How does the performance of citation-
enhanced keyphrase extraction (CeKE) com-
pare with the performance of existing super-
vised models that use only information intrin-
sic to the data and what are the most informa-
tive features for classification? We compared
CeKE?s performance with that of classifiers
trained on KEA features only and Hulth?s
features only and present a ranking of fea-
tures based on information gain.
2. How do supervised models that integrate ci-
tation network information compare with re-
cent unsupervised models? Since recent un-
supervised approaches are becoming compet-
itive with supervised approaches (Hasan and
Ng, 2014), we also compared CeKE with
unsupervised ranking of candidate phrases
by TF-IDF, TextRank (Mihalcea and Ta-
rau, 2004) and ExpandRank (Wan and Xiao,
2008). For unsupervised, we considered top
5 and top 10 ranked phrases when computing
?@5? and ?@10? measures.
3. How well does our proposed model perform
in the absence of either cited or citing con-
texts? Since newly published scientific pa-
pers are not cited by many other papers, e.g.,
due to their recency, no cited contexts are
available. We studied the quality of predicted
keyphrases when either cited or citing con-
texts are missing. For this, we compared
the performance of models trained using both
cited and citing contexts with that of models
that use either cited or citing contexts.
Evaluation metrics. To evaluate the perfor-
mance of CeKE, we used the following metrics:
precision, recall and F1-score for the positive class
since correct identification of keyphrases is of
most interest. These metrics were widely used in
previous works (Hulth, 2003; Mihalcea and Tarau,
2004; Wan and Xiao, 2008; Hasan and Ng, 2010).
The reported values are averaged in 10-fold cross-
validation experiments, where folds were created
at document level and candidate phrases were ex-
tracted from the documents in each fold to form
the training and test sets. In all experiments, we
used Na??ve Bayes and their Weka implementa-
tion
2
. However, any probabilistic classifier that re-
turns a posterior probability of the class given an
example, can be used with our features.
The ? parameter was set to the (title and ab-
stract) tf-idf averaged over the entire collection,
while ? was set to 20. These values were esti-
mated on a validation set sampled from training.
4.3 Results and Discussion
The impact of citation network information on the
keyphrase extraction task. Table 3 shows the re-
sults of the comparison of CeKE with two su-
pervised approaches, KEA and Hulth?s approach.
The features used in KEA are the tf-idf and the
relative position of a candidate phrase, whereas
those used in Hulth?s approach are tf, cf (i.e., col-
lection frequency), relative position and POS tags.
CeKE is trained using all features from Table 1.
Among the three methods for candidate phrase
formation proposed in Hulth (2003), i.e., n-grams,
NP-chunks, and POS Tag Patterns, our Hulth?s im-
plementation is based on n-grams since this gives
the best results among all methods (see (Hulth,
2003) for more details). In addition, the n-grams
method is the most similar to our candidate phrase
generation and that used in Frank et al. (1999).
As can be seen from Table 3, CeKE outperforms
KEA and Hulth?s approach in terms of all perfor-
mance measures on both WWW and KDD, with
a substantial improvement in recall over both ap-
proaches. For example, on WWW, CeKE achieves
a recall of 0.386 compared to 0.146 and 0.107 re-
call achieved by KEA and Hulth?s, respectively.
2
http://www.cs.waikato.ac.nz/ml/weka/
1440
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
Citation - Enhanced (CeKE) 0.227 0.386 0.284 0.213 0.413 0.280
TF-IDF - Top 5 0.089 0.100 0.094 0.083 0.102 0.092
TF-IDF - Top 10 0.075 0.169 0.104 0.080 0.203 0.115
TextRank - Top 5 0.058 0.071 0.062 0.051 0.065 0.056
TextRank - Top 10 0.062 0.133 0.081 0.053 0.127 0.072
ExpandRank - 1 neigh. - Top 5 0.088 0.109 0.095 0.077 0.103 0.086
ExpandRank - 1 neigh. - Top 10 0.078 0.165 0.101 0.071 0.177 0.098
ExpandRank - 5 neigh. - Top 5 0.093 0.113 0.100 0.080 0.108 0.090
ExpandRank - 5 neigh. - Top 10 0.080 0.172 0.104 0.068 0.172 0.095
ExpandRank - 10 neigh. - Top 5 0.094 0.113 0.100 0.077 0.103 0.086
ExpandRank - 10 neigh. - Top 10 0.076 0.162 0.099 0.065 0.164 0.091
Table 5: The comparison of CeKE with unsupervised approaches on WWW and KDD collections.
Rank Feature IG Score
1 abstract tf-idf 0.0234
2 first position 0.0188
3 citation tf-idf 0.0177
4 relativePos 0.0154
5 firstPosUnder 0.0148
6 inCiting 0.0129
7 inCited 0.0098
8 POS 0.0085
9 tf-idf-Over 0.0078
Table 4: Feature ranking by Info Gain on WWW.
Although there are only small variations from
KEA to Hulth?s approach, KEA performs better
on WWW, but worse on KDD compared with
Hulth?s approach. In contrast, CeKE shows con-
sistent improvement over the two approaches on
both datasets, hence, effectively making use of the
information available in the citation network.
In order to understand the importance of our
features, we ranked them based on Information
Gain (IG), which determines how informative a
feature is with respect to the class variable. Table
4 shows the features ranked in decreasing order of
their IG scores for WWW. As can be seen from
the table, tf-idf and citation tf-idf are both highly
ranked, first and third, respectively, illustrating
that they contain significant information in pre-
dicting keyphrases. The first position of a phrase
is also of great impact. This is consistent with the
fact that almost half of the identified keywords and
about 20% of the annotated keyphrases appear in
title. Similar ranking is obtained on KDD.
The comparison of CeKE with unsupervised
state-of-the-art models. Table 5 shows the re-
sults of the comparison of CeKE with three unsu-
pervised ranking approaches: TF-IDF (Tonella et
al., 2003), TextRank (Mihalcea and Tarau, 2004),
and ExpandRank (Wan and Xiao, 2008). TF-IDF
and TextRank use information only from the target
paper, whereas ExpandRank uses a small textual
neighborhood in addition to the target paper. Note
that, for all unsupervised methods, we used Porter
stemmer and the same candidate phrase generation
as in CeKE, as explained in ?3.2.
For TF-IDF, we first tokenized the target paper
and computed the score for each word, and then
formed phrases and summed up the score of every
word within a phrase. For TextRank, we built an
undirected graph for each paper, where the nodes
correspond to words in the target paper and edges
are drawn between two words that occur next to
each other in the text, i.e., the window size is 2.
For ExpandRank, we built an undirected graph
for each paper and its local textual neighborhood.
Again, nodes correspond to words in the target pa-
per and its textually similar papers and edges are
drawn between two words that occur within a win-
dow of 10 words from each other in the text, i.e.,
the window size is 10. We performed experiments
with 1, 5, and 10 textually-similar neighbors. For
TextRank and ExpandRank, we summed up the
scores of words within a phrase as in TF-IDF.
1441
WWW KDD
Method Precision Recall F1-score Precision Recall F1-score
CeKE - Both contexts 0.227 0.386 0.284 0.213 0.413 0.280
CeKE - Only cited contexts 0.222 0.286 0.247 0.192 0.300 0.233
CeKE - Only citing contexts 0.203 0.342 0.253 0.195 0.351 0.250
Table 6: Results of CeKE using both contexts and using with only cited or citing contexts.
For each unsupervised method, we computed
results for top 5 and top 10 ranked phrases. As
can be seen from Table 5, CeKE substantially out-
performs all the other methods for our domain of
study, i.e., papers from WWW and KDD, illustrat-
ing again that the citation network of a paper con-
tains important information that can show remark-
able benefits for keyphrase extraction. Among all
unsupervised methods, ExpandRank with fewer
textual similar neighbor (one or five) performs the
best. This is generally consistent with the results
shown in (Wan and Xiao, 2008) for news articles.
The effect of cited and citing contexts informa-
tion on models? performance. Table 6 shows the
precision, recall and F-score values for some vari-
ations of our method when: (1) all the citation con-
texts for a paper are used, (2) only cited contexts
are used, (3) only citing contexts are used. The
motivation behind this experiment was to deter-
mine how well the proposed model would perform
on newly published research papers that have not
accumulated citations yet. As shown in the table,
there is no substantial difference in terms of preci-
sion between CeKE models that use only cited or
only citing contexts, although the recall is substan-
tially higher for the case when only citing contexts
are used, for both WWW and KDD. The CeKE
that uses both citing and cited contexts achieves
a substantially higher recall and only a slightly
higher precision compared with the cases when
only one context type is available. The fact that
the citing context information provides a slight im-
provement in performance over cited contexts is
consistent with the intuition that when citing a pa-
per y, an author generally summarizes the main
ideas from y using important words from a target
paper x, making the citing contexts to have higher
overlap with words from x. In turn, a paper z that
cites x may use paraphrasing to summarize ideas
from x with words more similar to those from z.
Note that the results of all above experiments
are statistically significant at p-values ? 0.05, us-
ing a paired t-test on F1-scores.
4.4 Anecdotal Evidence
In order to check the transferability of our pro-
posed approach to other research fields, e.g., nat-
ural language processing, it would be interesting
to use our trained classifiers on WWW and KDD
collections and evaluate them on new collections
such as NLP related collections. Since NLP col-
lections annotated with keyphrases are not avail-
able, we show anecdotal evidence for only one pa-
per. We selected for this task an award winning pa-
per published in the EMNLP conference. The pa-
per?s title is ?Unsupervised semantic parsing? and
has won the Best Paper Award in the year 2009
(Poon and Domingos, 2009). In order for our al-
gorithm to work, we gathered from the Web (using
Google Scholar) all the cited and citing contexts
that were available (49 cited contexts and 30 cit-
ing contexts). We manually annotated the target
paper with keyphrases. The title, abstract and all
the contexts were POS tagged using the NLP Stan-
ford tool. We then trained a classifier on the fea-
tures shown in Table 1, on both WWW and KDD
datasets combined. The trained classifier was used
to make predictions, which were compared against
the manually annotated keyphrases. The results
are shown in Figure 2, which displays the title and
abstract of the paper and the predicted keyphrases.
Candidate phrases that are predicted as keyphrases
are marked in red bold, those predicted as non-
keyphrases are shown in black, while the filtered
out words are shown in light gray.
We tuned our classifier trained on WWW and
KDD to return as keyphrases only those that had
an extremely high probability to be keyphrases.
Specifically, we used a threshold of 0.985. The
probability of each returned keyphrase (which is
above 0.985) is shown in the upper right corner
of a keyphrase. Human annotated keyphrases are
marked in italic, under the figure. There is a clear
match between the predictions and the human an-
notations. It is also possible to extract more or
less keyphrases simply by adjusting the threshold
1442
Unsupervised Semantic Parsing
0.997
We present the first unsupervised approach to the problem of learning a semantic parser
1.000
, using
Markov logic
0.991
. Our USP system
0.985
transforms dependency trees into quasi-logical forms, recur-
sively induces lambda forms from these, and clusters them to abstract away syntactic variations of the
same meaning. The MAP semantic parse
1.000
of a sentence is obtained by recursively assigning its
parts to lambda-form clusters and composing them. We evaluate our approach by using it to extract a
knowledge base from biomedical abstracts and answer questions. USP
1.000
substantially outperforms
TextRunner, DIRT and an informed baseline on both precision and recall on this task.
Human annotated labels: unsupervised semantic parsing, Markov logic, USP system
Figure 2: The title and abstract of an EMNLP paper by Poon and Domingos (2009) and human annotated
keyphrases for the paper. Black words represent candidate phrases. Red bold words represent predicted
keyphrases. The numbers above predicted keyphrases are probabilities for the positive class assignment.
on the probability output by Na??ve Bayes. For ex-
ample, if we decrease the threshold to 0.920 the
following phrases would be added to the returned
set of keyphrases: dependency trees, quasi-logical
forms and unsupervised approach.
Another interesting aspect is the frequency of
occurrence of the predicted keyphrases in the cited
and citing contexts. Table 7 shows the term-
frequency of every predicted keyphrase within the
citation network. For example, the phrase seman-
tic parser appears in 29 cited contexts and 26 cit-
ing contexts. The reason for the higher cited con-
text frequency is not necessarily due to impor-
tance, but could be due to the larger number of
cited vs. citing contexts for this paper (49 vs. 30).
The high rate of keyphrases within the citation net-
work validates our assumption of the importance
of citation networks for keyphrase extraction.
Finally, we performed the same experiment
with Hulth?s and KEA methods. While the clas-
sifier trained on Hulth?s features did not identify
any keyphrases, KEA managed to identify several
good ones (e.g., USP, semantic parser), but left
out some important ones (e.g., Markov logic, un-
supervised). Moreover, the keyphrases predicted
by KEA have a lower confidence. For this reason,
lowering the probability threshold would result in
selecting other bad keyphrases.
4.5 Error analysis
We performed an error analysis and found that
candidate phrases are predicted as keyphrases
(FPs), although they do not appear in gold stan-
dard, i.e., the set of author-input keyphrases, in
cases when: 1) a more general terms is used to
describe an important concept of a document, e.g.,
Keyphrase #cited c. #citing c.
semantic parser 29 26
USP 31 10
Markov logic 15 10
unsupervised semantic parsing 12 1
USP system 3 2
Table 7: Frequency of the predicted keyphrases in
cited / citing contexts.
co-authorship prediction represented as link pre-
diction or Twitter platform represented as social
media; 2) an important concept is omitted (either
intentionally or forgetfully) from the set of author-
input keyphrases.
Hence, while we believe that authors are the
best keyphrase annotators for their own work,
there are cases when important keyphrases are
overlooked or expressed in different ways, possi-
bly due to the human subjective nature in choosing
important keyphrases that describe a document.
To this end, a limitation of our model is the use of
a single gold standard keyphrase annotation. In fu-
ture, we plan to acquire several human keyphrase
annotation sets for our datasets and test the perfor-
mance of the proposed approach on these annota-
tion sets, independently and in combination.
Keyphrases that appear in gold standard are
predicted as non-keyphrases (FNs) when: 1) a
keyphrase is infrequent in abstract; 2) its distance
from the beginning of a document is large; 3) does
not occur or occurs only rarely in a document?s
citation contexts, either citing or cited contexts.
Examples of FNs are model/algorithm/approach
names, e.g., random walks, that appear in sen-
tences such as: ?In this paper, we model the prob-
lem [? ? ?] by using random walks.? Although such
1443
a sentence may appear further away from the be-
ginning of an abstract, it contains significant in-
formation from the point of view of keyphrase
extraction. The design of patters such as <
by using $model > or < uses $model > could
lead to improved classification performance.
Further investigation of FPs and FNs will be
considered in future work. We believe that a bet-
ter understanding of errors has the potential to ad-
vance state-of-the-art for keyphrase extraction.
5 Conclusion and Future Directions
In this paper, we presented a supervised classifi-
cation model for keyphrase extraction from scien-
tific research papers that are embedded in citation
networks. More precisely, we designed novel fea-
tures that take into account citation network in-
formation for building supervised models for the
classification of candidate phrases as keyphrases
or non-keyphrases. The results of our experi-
ments show that the proposed supervised model
trained on a combination of citation-based features
and existing features for keyphrase extraction per-
forms substantially better compared with state-of-
the-art supervised and unsupervised models.
Although we illustrated the benefits of leverag-
ing inter-linked document networks for keyphrase
extraction from scientific documents, the proposed
model can be extended to other types of docu-
ments such as webpages, emails, and weblogs. For
example, the anchor text on hyperlinks in weblogs
can be seen as the ?citation context?.
Another aspect of future work would be the
use of external sources to better identify candi-
date phrases. For example, the use of Wikipedia
was studied before to check if the concept behind
a phrase has its own Wikipedia page (Medelyan
et al., 2009). Furthermore, since citations occur
in all sciences, extensions of the proposed model
to other domains, e.g., Biology and Chemistry,
and other applications, e.g., document summariza-
tion, similar to Mihalcea and Tarau (2004) and
Qazvinian et al. (2010), are of particular interest.
Acknowledgments
We are grateful to Dr. C. Lee Giles for the
CiteSeerX data, which allowed the generation of
citation graphs. We also thank Kishore Nep-
palli and Juan Fern?andez-Ram??zer for their help
with various dataset construction tasks. We very
much appreciate the constructive feedback from
our anonymous reviewers. This research was
supported in part by NSF awards #1353418 and
#1423337 to Cornelia Caragea. Any opinions,
findings, and conclusions expressed here are those
of the authors and do not necessarily reflect the
views of NSF.
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Co-
herent citation-based summarization of scientific pa-
pers. In Proc. of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, HLT ?11, pages 500?509.
Amjad Abu-Jbara and Dragomir Radev. 2012. Ref-
erence scope identification in citing sentences. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 80?90.
Ken Barker and Nadia Cornacchia. 2000. Using Noun
Phrase Heads to Extract Document Keyphrases. In
Proceedings of the 13th Biennial Conference of the
Canadian Society on Computational Studies of Intel-
ligence: Advances in Artificial Intelligence, AI ?00,
pages 40?52, London, UK, UK. Springer-Verlag.
Florian Boudin. 2013. A comparison of centrality
measures for graph-based keyphrase extraction. In
Proc. of IJCNLP, pages 834?838, Nagoya, Japan.
Soumen Chakrabarti, Byron Dom, Prabhakar Ragha-
van, Sridhar Rajagopalan, David Gibson, and Jon
Kleinberg. 1998. Automatic resource compilation
by analyzing hyperlink structure and associated text.
Comput. Netw. ISDN Syst., 30(1-7):65?74, April.
Chen Cheng, Haiqin Yang, Michael R. Lyu, and Irwin
King. 2013. Where you like to go next: Succes-
sive point-of-interest recommendation. In Proc. of
IJCAI?13, pages 2605?2611, Beijing, China.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence - Volume 2, IJCAI?99, pages
668?673, Stockholm, Sweden.
Katerina T. Frantzi, Sophia Ananiadou, and Jun-ichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Proc.
of ECDL ?98, pages 585?604.
Sujatha Das Gollapalli and Cornelia Caragea. 2014.
Extracting keyphrases from research papers using
citation networks. In Proceedings of the 28th
AAAI Conference on Artificial Intelligence (AAAI-
14), Qu?ebec City, Qu?ebec, Canada.
Khaled M. Hammouda, Diego N. Matute, and Mo-
hamed S. Kamel. 2005. Corephrase: Keyphrase ex-
traction for document clustering. In Proc. of the 4th
1444
International Conference on Machine Learning and
Data Mining in Pattern Recognition, MLDM?05,
pages 265?274, Leipzig, Germany.
Kazi Saidul Hasan and Vincent Ng. 2010. Conun-
drums in Unsupervised Keyphrase Extraction: Mak-
ing Sense of the State-of-the-Art. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 365?373.
Kazi Saidul Hasan and Vincent Ng. 2014. Automatic
keyphrase extraction: A survey of the state of the art.
In Proc. of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and Lee
Giles. 2010. Context-aware citation recommenda-
tion. In Proc. of WWW ?10, pages 421?430, Raleigh,
North Carolina, USA.
Yifan Hu, Yehuda Koren, and Chris Volinsky.
2008. Collaborative filtering for implicit feedback
datasets. In Proc. of the 8th IEEE Intl. Conference
on Data Mining, ICDM ?08, pages 263?272.
Anette Hulth. 2003. Improved Automatic Keyword
Extraction Given More Linguistic Knowledge. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, EMNLP
?03, pages 216?223.
Xin Jiang, Yunhua Hu, and Hang Li. 2009. A Ranking
Approach to Keyphrase Extraction. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 756?757. ACM.
Steve Jones and Mark S. Staveley. 1999. Phrasier:
A system for interactive document retrieval using
keyphrases. In Proceedings of SIGIR ?99, pages
160?167, Berkeley, California, USA.
Saurabh Kataria, Prasenjit Mitra, and Sumit Bhatia.
2010. Utilizing context in generative bayesian mod-
els for linked corpus. In In Proc. of AAAI ?10, pages
1340?1345, Atlanta, Georgia, USA.
Saurabh Kataria, Prasenjit Mitra, Cornelia Caragea,
and C. Lee Giles. 2011. Context sensitive topic
models for author influence in document networks.
In Proceedings of IJCAI?11, pages 2274?2280,
Barcelona, Catalonia, Spain.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010. SemEval-2010 Task 5:
Automatic Keyphrase Extraction from Scientific Ar-
ticles. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages
21?26, Los Angeles, California.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2013. Automatic keyphrase
extraction from scientific articles. Language Re-
sources and Evaluation, Springer, 47(3):723?742.
Kirill Kireyev. 2009. Semantic-based estimation of
term informativeness. In Proc. of NAACL ?09, pages
530?538, Boulder, Colorado.
Marijn Koolen and Jaap Kamps. 2010. The impor-
tance of anchor text for ad hoc search revisited. In
Proceedings of SIGIR ?10, pages 122?129, Geneva,
Switzerland.
Reiner Kraft and Jason Zien. 2004. Mining anchor text
for query refinement. In Proceedings of the 13th In-
ternational Conference on World Wide Web, WWW
?04, pages 666?674, New York, NY, USA. ACM.
Sungjick Lee and Han-joon Kim. 2008. News Key-
word Extraction for Topic Tracking. In Proceedings
of the 2008 Fourth International Conference on Net-
worked Computing and Advanced Information Man-
agement - Volume 02, NCM ?08, pages 554?559,
Washington, DC, USA. IEEE Computer Society.
Wendy Lehnert, Claire Cardie, and Ellen Rilofl. 1990.
Analyzing research papers using citation sentences.
In Proceedings of the 12th Annual Conference of the
Cognitive Science Society, pages 511?518.
Marina Litvak and Mark Last. 2008. Graph-
Based Keyword Extraction for Single-Document
Summarization. In Proceedings of the Workshop
on Multi-source Multilingual Information Extrac-
tion and Summarization, MMIES ?08, pages 17?24,
Manchester, United Kingdom.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009. Unsupervised Approaches for Automatic
Keyword Extraction Using Meeting Transcripts. In
Proceedings of NAACL ?09, pages 620?628, Boul-
der, Colorado.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic Keyphrase Ex-
traction via Topic Decomposition. In Proceedings
of EMNLP ?10, pages 366?376, Cambridge, Mas-
sachusetts.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Lu??s Marujo, Ricardo Ribeiro, David Martins
de Matos, Jo?ao Paulo Neto, Anatole Gershman, and
Jaime G. Carbonell. 2013. Key phrase extraction of
lightly filtered broadcast news. CoRR.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 3 - Volume 3, EMNLP
?09, pages 1318?1327, Singapore.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generat-
ing impact-based summaries for scientific literature.
In Proceedings of ACL-08: HLT, pages 816?824,
Columbus, Ohio.
1445
Donald Metzler, Jasmine Novak, Hang Cui, and Srihari
Reddy. 2009. Building enriched document repre-
sentations using aggregated anchor text. In Proc. of
SIGIR ?09, pages 219?226, Boston, MA, USA.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Proceedings of
EMNLP 2004, pages 404?411, Barcelona, Spain.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A.
Hearst. 2004. Citances: Citation sentences for se-
mantic analysis of bioscience text. In SIGIR Work-
shop on Search and Discovery in Bioinformatics.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase Extraction in Scientific Publications. In
Proc. of the Intl. Conf. on Asian digital libraries,
ICADL?07, pages 317?326, Hanoi, Vietnam.
Rong Pan and Martin Scholz. 2009. Mind the gaps:
Weighting the unknown in large-scale one-class col-
laborative filtering. In Proceedings of KDD ?09,
pages 667?676, Paris, France.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proc. of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?09, pages 1?10, Singapore.
Nirmala Pudota, Antonina Dattolo, Andrea Baruzzo,
Felice Ferrara, and Carlo Tasso. 2010. Auto-
matic keyphrase extraction and ontology mining for
content-based tag recommendation. International
Journal of Intelligent Systems.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proc. of the 22nd Intl. Conference
on Computational Linguistics, COLING ?08, pages
689?696, Manchester, United Kingdom.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
?
Ozg?ur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, COLING ?10, pages 895?903.
Steffen Rendle, Christoph Freudenthaler, and Lars
Schmidt-Thieme. 2010. Factorizing personalized
markov chains for next-basket recommendation. In
WWW ?10, pages 811?820, Raleigh, North Carolina.
Jason D. M. Rennie and Tommi Jaakkola. 2005. Using
Term Informativeness for Named Entity Detection.
In Proc. of SIGIR ?05, pages 353?360.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006. How to find better index terms through cita-
tions. In Proc. of the Workshop on How Can Compu-
tational Linguistics Improve Information Retrieval?,
CLIIR ?06, pages 25?32, Sydney, Australia.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proc. of CIKM ?08, pages 213?222,
Napa Valley, California, USA.
Guy Shani, David Heckerman, and Ronen I. Brafman.
2005. An mdp-based recommender system. J.
Mach. Learn. Res., 6:1265?1295, December.
Xiaolin Shi, Jure Leskovec, and Daniel A. McFarland.
2010. Citing for high impact. In Proceedings of the
10th Annual Joint Conference on Digital Libraries,
JCDL ?10, pages 49?58, Gold Coast, Queensland,
Australia.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proceed-
ings of EMNLP-06.
S. Teufel. 1999. Argumentative Zoning: Information
Extraction from Scientific Text. Ph.D. thesis, Uni-
versity of Edinburgh.
Paolo Tonella, Filippo Ricca, Emanuele Pianta, and
Christian Girardi. 2003. Using Keyword Extrac-
tion for Web Site Clustering. In Web Site Evolution,
2003. Theme: Architecture. Proceedings. Fifth IEEE
International Workshop on, pages 41?48.
Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. Inf. Retr., 2.
Peter D. Turney. 2003. Coherent Keyphrase Extraction
via Web Mining. In Proceedings of the 18th inter-
national joint conference on Artificial intelligence,
IJCAI?03, pages 434?439, Acapulco, Mexico.
Xiaojun Wan and Jianguo Xiao. 2008. Single Doc-
ument Keyphrase Extraction Using Neighborhood
Knowledge. In Proceedings of AAAI ?08, pages
855?860, Chicago, Illinois.
Zhaohui Wu and Lee C. Giles. 2013. Measuring
term informativeness in context. In Proceedings of
NAACL ?13, pages 259?269, Atlanta, Georgia.
Zhuli Xie. 2005. Centrality Measures in Text Mining:
Prediction of Noun Phrases that Appear in Abstracts.
In Proceedings of the ACL Student Research Work-
shop, pages 103?108, Ann Arbor, Michigan.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In SIGIR.
Yongzheng Zhang, Evangelos Milios, and Nur Zincir-
Heywood. 2007. A Comparative Study on Key
Phrase Extraction Methods in Automatic Web Site
Summarization. Journal of Digital Information
Management, 5(5):323.
Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song,
Palakorn Achananuparp, Ee-Peng Lim, and Xiaom-
ing Li. 2011. Topical Keyphrase Extraction from
Twitter. In Proceedings of HLT ?11, pages 379?388,
Portland, Oregon.
Andrew Zimdars, David Maxwell Chickering, and
Christopher Meek. 2001. Using temporal data for
making recommendations. In Proceedings of the
17th Conference in Uncertainty in Artificial Intelli-
gence, UAI ?01, pages 580?588.
1446
