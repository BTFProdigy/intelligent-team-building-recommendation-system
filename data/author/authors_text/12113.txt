Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 66?74,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Comparing learners for Boolean partitions:
implications for morphological paradigms ?
Katya Pertsova
University of North Carolina,
Chapel Hill
pertsova@email.unc.edu
Abstract
In this paper, I show that a problem of
learning a morphological paradigm is sim-
ilar to a problem of learning a partition
of the space of Boolean functions. I de-
scribe several learners that solve this prob-
lem in different ways, and compare their
basic properties.
1 Introduction
Lately, there has been a lot of work on acquir-
ing paradigms as part of the word-segmentation
problem (Zeman, 2007; Goldsmith, 2001; Snover
et al, 2002). However, the problem of learning
the distribution of affixes within paradigms as a
function of their semantic (or syntactic) features is
much less explored to my knowledge. This prob-
lem can be described as follows: suppose that the
segmentation has already been established. Can
we now predict what affixes should appear in
what contexts, where by a ?context? I mean some-
thing quite general: some specification of seman-
tic (and/or syntactic) features of the utterance. For
example, one might say that the nominal suffix -
z in English (as in apple-z) occurs in contexts that
involve plural or possesive nouns whose stems end
in a voiced segment.
In this paper, I show that the problem of learn-
ing the distribution of morphemes in contexts
specified over some finite number of features
is roughly equivalent to the problem of learn-
ing Boolean partitions of DNF formulas. Given
this insight, one can easily extend standard DNF-
learners to morphological paradigm learners. I
show how this can be done on an example of
the classical k-DNF learner (Valiant, 1984). This
insight also allows us to bridge the paradigm-
learning problem with other similar problems in
?This paper ows a great deal to the input from Ed Stabler.
As usual, all the errors and shortcomings are entirely mine.
the domain of cognitive science for which DNF?s
have been used, e.g., concept learning. I also de-
scribe two other learners proposed specifically for
learning morphological paradigms. The first of
these learners, proposed by me, was designed to
capture certain empirical facts about syncretism
and free variation in typological data (Pertsova,
2007). The second learner, proposed by David
Adger, was designed as a possible explanation of
another empirical fact - uneven frequencies of free
variants in paradigms (Adger, 2006).
In the last section, I compare the learners on
some simple examples and comment on their mer-
its and the key differences among the algorithms.
I also draw connections to other work, and discuss
directions for further empirical tests of these pro-
posals.
2 The problem
Consider a problem of learning the distribution
of inflectional morphemes as a function of some
set of features. Using featural representations, we
can represent morpheme distributions in terms of
a formula. The DNF formulas are commonly used
for such algebraic representation. For instance,
given the nominal suffix -z mentioned in the in-
troduction, we can assign to it the following rep-
resentation: [(noun; +voiced]stem; +plural) ?
(noun; +voiced]stem; +possesive)]. Presum-
ably, features like [plural] or [+voiced] or ]stem
(end of the stem) are accessible to the learners?
cognitive system, and can be exploited during
the learning process for the purpose of ?ground-
ing? the distribution of morphemes.1 This way
of looking at things is similar to how some re-
searchers conceive of concept-learning or word-
1Assuming an a priori given universal feature set, the
problem of feature discovery is a subproblem of learning
morpheme distributions. This is because learning what fea-
ture condition the distribution is the same as learning what
features (from the universal set) are relevant and should be
paid attention to.
66
learning (Siskind, 1996; Feldman, 2000; Nosofsky
et al, 1994).
However, one prominent distinction that sets
inflectional morphemes apart from words is that
they occur in paradigms, semantic spaces defin-
ing a relatively small set of possible distinctions.
In the absence of free variation, one can say that
the affixes define a partition of this semantic space
into disjoint blocks, in which each block is asso-
ciated with a unique form. Consider for instance
a present tense paradigm of the verb ?to be? in
standard English represented below as a partition
of the set of environments over the following fea-
tures: class (with values masc, fem, both (masc
& fem),inanim,), number (with values +sg and
?sg), and person (with values 1st, 2nd, 3rd).2
am 1st. person; fem; +sg.
1st. person; masc; +sg.
are 2nd. person; fem; +sg.
2nd. person; masc; +sg.
2nd. person; fem; ?sg.
2nd. person; masc; ?sg.
2nd. person; both; ?sg.
1st. person; fem; ?sg.
1st. person; masc; ?sg.
1st. person; both; ?sg.
3rd. person; masc; ?sg
3rd. person; fem; ?sg
3rd. person; both; ?sg
3rd. person; inanim; ?sg
is 3rd person; masc; +sg
3rd person; fem; +sg
3rd person; inanim; +sg
Each block in the above partition can be rep-
resented as a mapping between the phonological
form of the morpheme (a morph) and a DNF for-
mula. A single morph will be typically mapped to
a DNF containing a single conjunction of features
(called a monomial). When a morph is mapped
to a disjunction of monomials (as the morph [-z]
discussed above), we think of such a morph as
a homonym (having more than one ?meaning?).
Thus, one way of defining the learning problem is
in terms of learning a partition of a set of DNF?s.
2These particular features and their values are chosen just
for illustration. There might be a much better way to repre-
sent the distinctions encoded by the pronouns. Also notice
that the feature values are not fully independent: some com-
binations are logically ruled out (e.g. speakers and listeners
are usually animate entities).
Alternatively, we could say that the learner has
to learn a partition of Boolean functions associated
with each morph (a Boolean function for a morph
m maps the contexts in which m occurs to true,
and all other contexts to false).
However, when paradigms contain free varia-
tion, the divisions created by the morphs no longer
define a partition since a single context may be as-
sociated with more than one morph. (Free vari-
ation is attested in world?s languages, although
it is rather marginal (Kroch, 1994).) In case a
paradigm contains free variation, it is still possible
to represent it as a partition by doing the follow-
ing:
(1) Take a singleton partition of morph-
meaning pairs (m, r) and merge any cells
that have the same meaning r. Then merge
those blocks that are associated with the
same set of morphs.
Below is an example of how we can use this trick
to partition a paradigm with free-variation. The
data comes from the past tense forms of ?to be? in
Buckie English.
was 1st. person; fem; +sg.
1st. person; masc; +sg.
3rd person; masc; +sg
3rd person; fem; +sg
3rd person; inanim; +sg
was/were 2nd. person; fem; +sg.
2nd. person; masc; +sg.
2nd. person; fem; ?sg.
2nd. person; masc; ?sg.
2nd. person; both; ?sg.
1st. person; fem; ?sg.
1st. person; masc; ?sg.
1st. person; both; ?sg.
were 3rd. person; masc; ?sg
3rd. person; fem; ?sg
3rd. person; both; ?sg
3rd. person; inanim; ?sg
In general, then, the problem of learning the
distribution of morphs within a single inflectional
paradigm is equivalent to learning a Boolean par-
tition.
In what follows, I consider and compare several
learners for learning Boolean partitions. Some of
these learners are extensions of learners proposed
in the literature for learning DNFs. Other learners
67
were explicitly proposed for learning morphologi-
cal paradigms.
We should keep in mind that all these learners
are idealizations and are not realistic if only be-
cause they are batch-learners. However, because
they are relatively simple to state and to under-
stand, they allow a deeper understanding of what
properties of the data drive generalization.
2.1 Some definitions
Assume a finite set of morphs, ?, and a finite set
of features F . It would be convenient to think of
morphs as chunks of phonological material cor-
responding to the pronounced morphemes.3 Ev-
ery feature f ? F is associated with some set
of values Vf that includes a value [?], unspec-
ified. Let S be the space of all possible com-
plete assignments over F (an assignment is a set
{fi ? Vf |?fi ? F}). We will call those assign-
ments that do not include any unspecified features
environments. Let the set S? ? S correspond to
the set of environments.
It should be easy to see that the set S forms a
Boolean lattice with the following relation among
the assignments, ?R: for any two assignments a1
and a2, a1 ?R a2 iff the value of every feature fi
in a1 is identical to the value of fi in a2, unless fi
is unspecified in a2. The top element of the lattice
is an assignment in which all features are unspec-
ified, and the bottom is the contradiction. Every
element of the lattice is a monomial corresponding
to the conjunction of the specified feature values.
An example lattice for two binary features is given
in Figure 1.
Figure 1: A lattice for 2 binary features
A language L consists of pairs from ? ? S?.
That is, the learner is exposed to morphs in differ-
ent environments.
3However, we could also conceive of morphs as functions
specifying what transformations apply to the stem without
much change to the formalism.
One way of stating the learning problem is to
say that the learner has to learn a grammar for the
target language L (we would then have to spec-
ify what this grammar should look like). Another
way is to say that the learner has to learn the lan-
guage mapping itself. We can do the latter by us-
ing Boolean functions to represent the mapping of
each morph to a set of environments. Depending
on how we state the learning problem, we might
get different results. For instance, it?s known that
some subsets of DNF?s are not learnable, while
the Boolean functions corresponding to them are
learnable (Valiant, 1984). Since I will use Boolean
functions for some of the learners below, I intro-
duce the following notation. Let B be the set of
Boolean functions mapping elements of S? to true
or false. For convenience, we say that bm corre-
sponds to a Boolean function that maps a set of en-
vironments to true when they are associated with
m in L, and to false otherwise.
3 Learning Algorithms
3.1 Learner 1: an extension of the Valiant
k-DNF learner
An observation that a morphological paradigm can
be represented as a partition of environments in
which each block corresponds to a mapping be-
tween a morph and a DNF, allows us to easily con-
vert standard DNF learning algorithms that rely
on positive and negative examples into paradigm-
learning algorithms that rely on positive examples
only. We can do that by iteratively applying any
DNF learning algorithm treating instances of in-
put pairs like (m, e) as positive examples for m
and as negative examples for all other morphs.
Below, I show how this can be done by ex-
tending a k-DNF4 learner of (Valiant, 1984) to a
paradigm-learner. To handle cases of free varia-
tion we need to keep track of what morphs occur
in exactly the same environments. We can do this
by defining the partition ? on the input following
the recipe in (1) (substituting environments for the
variable r).
The original learner learns from negative exam-
ples alone. It initializes the hypothesis to the dis-
junction of all possible conjunctions of length at
most k, and subtracts from this hypothesis mono-
mials that are consistent with the negative ex-
amples. We will do the same thing for each
4k-DNF formula is a formula with at most k feature val-
ues in each conjunct.
68
morph using positive examples only (as described
above), and forgoing subtraction in a cases of free-
variation. The modified learner is given below.
The following additional notation is used: Lex is
the lexicon or a hypothesis. The formula D is a
disjunction of all possible conjunctions of length
at most k. We say that two assignments are con-
sistentwith each other if they agree on all specified
features. Following standard notation, we assume
that the learner is exposed to some text T that con-
sists of an infinite sequence of (possibly) repeating
elements from L. tj is a finite subsequence of the
first j elements from T . L(tj) is the set of ele-
ments in tj .
Learner 1 (input: tj)
1. set Lex := {?m,D?| ??m, e? ?
L(tj)}
2. For each ?m, e? ? L(tj), for each
m? s.t. ?? block bl ? ? of L(tj),
?m, e? ? bl and ?m?, e? ? bl:
replace ?m?, f? in Lex by ?m?, f ??
where f ? is the result of removing
every monomial consistent with e.
This learner initially assumes that every morph
can be used everywhere. Then, when it hears one
morph in a given environment, it assumes that no
other morph can be heard in exactly that environ-
ment unless it already knows that this environment
permits free variation (this is established in the
partition ?).
4 Learner 2:
The next learner is an elaboration on the previous
learner. It differs from it in only one respect: in-
stead of initializing lexical representations of ev-
ery morph to be a disjunction of all possible mono-
mials of length at most k, we initialize it to be the
disjunction of all and only those monomials that
are consistent with some environment paired with
the morph in the language. This learner is simi-
lar to the DNF learners that do something on both
positive and negative examples (see (Kushilevitz
and Roth, 1996; Blum, 1992)).
So, for every morph m used in the language, we
define a disjunction of monomials Dm that can be
derived as follows. (i) Let Em be the enumeration
of all environments in which m occurs in L (ii)
let Mi correspond to a set of all subsets of feature
values in ei, ei ? E (iii) let Dm be
?
M , where a
set s ?M iff s ?Mi, for some i.
Learner 2 can now be stated as a learner that
is identical to Learner 1 except for the initial set-
ting of Lex. Now, Lex will be set to Lex :=
{?m,Dm?| ??m, e? ? L(ti)}.
Because this learner does not require enumer-
ation of all possible monomials, but just those
that are consistent with the positive data, it can
handle ?polynomially explainable? subclass of
DNF?s (for more on this see (Kushilevitz and Roth,
1996)).
5 Learner 3: a learner biased towards
monomial and elsewhere distributions
Next, I present a batch version of a learner I pro-
posed based on certain typological observations
and linguists? insights about blocking. The typo-
logical observations come from a sample of verbal
agreement paradigms (Pertsova, 2007) and per-
sonal pronoun paradigms (Cysouw, 2003) show-
ing that majority of paradigms have either ?mono-
mial? or ?elsewhere? distribution (defined below).
Roughly speaking, a morph has a monomial dis-
tribution if it can be described with a single mono-
mial. A morph has an elsewhere distribution if
this distribution can be viewed as a complement
of distributions of other monomial or elsewhere-
morphs. To define these terms more precisely I
need to introduce some additional notation. Let
?
ex be the intersection of all environments in
which morph x occurs (i.e., these are the invariant
features of x). This set corresponds to a least up-
per bound of the environments associated with x in
the lattice ?S,?R?, call it lubx. Then, let the min-
imal monomial function for a morph x, denoted
mmx, be a Boolean function that maps an envi-
ronment to true if it is consistent with lubx and
to false otherwise. As usual, an extension of a
Boolean function, ext(b) is the set of all assign-
ments that b maps to true.
(2) Monomial distribution
A morph x has a monomial distribution iff
bx ? mmx.
The above definition states that a morph has a
monomial distribution if its invariant features pick
out just those environments that are associated
with this morph in the language. More concretely,
if a monomial morph always co-occurs with the
feature +singular, it will appear in all singular en-
69
vironments in the language.
(3) Elsewhere distribution
A morph x has an elsewhere distribution
iff bx ? mmx ? (mmx1 ?mmx2 ? . . . ?
(mmxn)) for all xi 6= x in ?.
The definition above amounts to saying that a
morph has an elsewhere distribution if the envi-
ronments in which it occurs are in the extension
of its minimal monomial function minus the min-
imal monomial functions of all other morphs. An
example of a lexical item with an elsewhere distri-
bution is the present tense form are of the verb ?to
be?, shown below.
Table 1: The present tense of ?to be? in English
sg. pl
1p. am are
2p. are are
3p. is are
Elsewhere morphemes are often described in
linguistic accounts by appealing to the notion of
blocking. For instance, the lexical representation
of are is said to be unspecified for both person
and number, and is said to be ?blocked? by two
other forms: am and is. My hypothesis is that
the reason why such non-monotonic analyses ap-
pear so natural to linguists is the same reason for
why monomial and elsewhere distributions are ty-
pologically common: namely, the learners (and,
apparently, the analysts) are prone to generalize
the distribution of morphs to minimal monomi-
als first, and later correct any overgeneralizations
that might arise by using default reasoning, i.e. by
positing exceptions that override the general rule.
Of course, the above strategy alone is not sufficient
to capture distributions that are neither monomial,
nor elsewhere (I call such distributions ?overlap-
ping?, cf. the suffixes -en and -t in the German
paradigm in Table 2), which might also explain
why such paradigms are typologically rare.
Table 2: Present tense of some regular verbs in
German
sg. pl
1p. -e -en
2p. -st -t
3p. -t -en
The original learner I proposed is an incre-
mental learner that calculates grammars similar
to those proposed by linguists, namely grammars
consisting of a lexicon and a filtering ?blocking?
component. The version presented here is a sim-
pler batch learner that learns a partition of Boolean
functions instead.5 Nevertheless, the main proper-
ties of the original learner are preserved: specifi-
cally, a bias towards monomial and elsewhere dis-
tributions.
To determine what kind of distribution a morph
has, I define a relation C. A morph m stands in a
relation C to another morph m? if ??m, e? ? L,
such that lubm? is consistent with e. In other
words, mCm? if m occurs in any environment
consistent with the invariant features of m?. Let
C+ be a transitive closure of C.
Learner 3 (input: tj)
1. Let S(tj) be the set of pairs in tj containing
monomial- or elsewhere-distribution morphs.
That is, ?m, e? ? S(tj) iff ??m? such that
mC+m? and m?C+m.
2. Let O(tj) = tj ? S(tj) (the set of all other
pairs).
3. A pair ?m, e? ? S is a least element of S
iff ???m?, e?? ? (S ? {?m, e?}) such that
m?C+m.
4. Given a hypothesis Lex, and for any expres-
sion ?m, e? ? Lex: let rem((m, e), Lex) =
(m, (mmm ? {b|?m?, b? ? Lex}))6
1. set S := S(tj) and Lex := ?
2. While S 6= ?: remove a least x
from S and set Lex := Lex ?
rem(x, Lex)
3. Set Lex := Lex ?O(tj).
This learner initially assumes that the lexicon is
empty. Then it proceeds adding Boolean functions
corresponding to minimal monomials for morphs
that are in the set S(tj) (i.e., morphs that have ei-
ther monomial or elsewhere distributions). This
5I thank Ed Stabler for relating this batch learner to me
(p.c.).
6For any two Boolean functions b, b?: b?b? is the function
that maps e to 1 iff e ? ext(b) and e 6? ext(b?). Similarly,
b + b? is the function that maps e to 1 iff e ? ext(b) and
e ? ext(b?).
70
is done in a particular order, namely in the or-
der in which the morphs can be said to block
each other. The remaining text is learned by rote-
memorization. Although this learner is more com-
plex than the previous two learners, it generalizes
fast when applied to paradigms with monomial
and elsewhere distributions.
5.1 Learner 4: a learner biased towards
shorter formulas
Next, I discuss a learner for morphological
paradigms, proposed by another linguist, David
Adger. Adger describes his learner informally
showing how it would work on a few examples.
Below, I formalize his proposal in terms of learn-
ing Boolean partitions. The general strategy of
this learner is to consider simplest monomials first
(those with the fewer number of specified features)
and see how much data they can unambiguously
and non-redundantly account for. If a monomial
is consistent with several morphs in the text - it is
discarded unless the morphs in question are in free
variation. This simple strategy is reiterated for the
next set of most simple monomials, etc.
Learner 4 (input tj)
1. Let Mi be the set of all monomials over F
with i specified features.
2. Let Bi be the set of Boolean functions from
environments to truth values corresponding
to Mi in the following way: for each mono-
mial mn ? Mi the corresponding Boolean
function b is such that b(e) = 1 if e is an
environment consistent with mn; otherwise
b(e) = 0.
3. Uniqueness check:
For a Boolean function b, morph m, and text
tj let unique(b,m, tj) = 1 iff ext(bm) ?
ext(b) and ???m?, e? ? L(tj), s.t. e ?
ext(b) and e 6? ext(bm).
1. set Lex := ?? ? and i := 0;
2. while Lex does not correspond to
L(tj) AND i ? |F | do:
for each b ? Bi, for each m, s.t.
??m, e? ? L(tj):
? if unique(b,m, tj) = 1 then
replace ?m, f? with ?m, f + b?
in Lex
i? i + 1
This learner considers all monomials in the or-
der of their simplicity (determined by the num-
ber of specified features), and if the monomial in
question is consistent with environments associ-
ated with a unique morph then these environments
are added to the extension of the Boolean function
for that morph. As a result, this learner will con-
verge faster on paradigms in which morphs can be
described with disjunctions of shorter monomials
since such monomials are considered first.
6 Comparison
6.1 Basic properties
First, consider some of the basic properties of the
learners presented here. For this purpose, we will
assume that we can apply these learners in an iter-
ative fashion to larger and larger batches of data.
We say that a learner is consistent if and only if,
given a text tj , it always converges on the gram-
mar generating all the data seen in tj (Osherson
et al, 1986). A learner is monotonic if and only
if for every text t and every point j < k, the hy-
pothesis the learner converges on at tj is a subset
of the hypothesis at tk (or for learners that learn
by elimination: the hypothesis at tj is a superset
of the hypothesis at tk). And, finally, a learner is
generalizing if and only if for some tj it converges
on a hypothesis that makes a prediction beyond the
elements of tj .
The table below classifies the four learners ac-
cording to the above properties.
Learner consist. monoton. generalizing
Learner 1 yes yes yes
Learner 2 yes yes yes
Learner 3 yes no yes
Learner 4 yes yes yes
All learners considered here are generalizing
and consistent, but they differ with respect to
monotonicity. Learner 3 is non-monotonic while
the remaining learners are monotonic. While
monotonicity is a nice computational property,
some aspects of human language acquisition are
suggestive of a non-monotonic learning strategy,
e.g. the presence of overgeneralization errors and
their subsequent corrections by children(Marcus et
al., 1992). Thus, the fact that Learner 3 is non-
monotonic might speak in its favor.
71
6.2 Illustration
To demonstrate how the learners work, consider
this simple example. Suppose we are learning the
following distribution of morphs A and B over 2
binary features.
(4) Example 1
+f1 ?f1
+f2 A B
?f2 B B
Suppose further that the text t3 is:
A +f1;+f2
B ?f1;+f2
B +f1;?f2
Learner 1 generalizes right away by assuming
that every morph can appear in every environment
which leads to massive overgeneralizations. These
overgeneralizations are eventually eliminated as
more data is discovered. For instance, after pro-
cessing the first pair in the text above, the learner
?learns? that B does not occur in any environ-
ment consistent with (+f1;+f2) since it has just
seen A in that environment. After processing t3,
Learner 1 has the following hypothesis:
A (+f1;+f2) ? (?f1;?f2)
B (?f1) ? (?f2)
That is, after seeing t3, Learner 2 correctly pre-
dicts the distribution of morphs in environments
that it has seen, but it still predicts that both A
and B should occur in the not-yet-observed en-
vironment, (?f1;?f2). This learner can some-
times converge before seeing all data-points, es-
pecially if the input includes a lot of free varia-
tion. If fact, if in the above example A and B were
in free variation in all environments, Learner 1
would have converged right away on its initial set-
ting of the lexicon. However, in paradigms with no
free variation convergence is typically slow since
the learner follows a very conservative strategy of
learning by elimination.
Unlike Learner 1, Learner 2 will converge after
seeing t3. This is because this learner?s initial hy-
pothesis is more restricted. Namely, the initial hy-
pothesis for A includes disjunction of only those
monomials that are consistent with (+f1;+f2).
Hence, A is never overgeneralized to (?f1;?f2).
Like Learner 1, Learner 2 also learns by elimina-
tion, however, on top of that it also restricts its ini-
tial hypothesis which leads to faster convergence.
Let?s now consider the behavior of learner 3 on
example 1. Recall that this learner first computes
minimal monomials of all morphs, and checks
in they have monomial or elsewhere distributions
(this is done via the relation C+). In this case, A
has a monomial distribution, and B has an else-
where distribution. Therefore, the learner first
computes the Boolean function forAwhose exten-
sion is simply (+f1;+f2); and then the Boolean
function for B, whose extension includes environ-
ments consistent with (*;*) minus those consistent
with (+f1;+f2), which yields the following hy-
pothesis:
ext(bA) [+f1;+f2]
ext(bB) [?f1;+f2][+f1;?f2][?f1;?f2]
That is, Learner 3 generalizes and converges on
the right language after seeing text t3.
Learner 4 also converges at this point. This
learner first considers how much data can be un-
ambiguously accounted for with the most minimal
monomial (*;*). Since both A and B occur in en-
vironments consistent with this monomial, noth-
ing is added to the lexicon. On the next round,
it considers all monomials with one specified fea-
ture. 2 such monomials, (?f1) and (?f2), are
consistent only with B, and so we predict B to ap-
pear in the not-yet-seen environment (?f1;?f2).
Thus, the hypothesis that Learner 4 arrives at is the
same as the hypothesis Learners 3 arrives at after
seeing t3.
6.3 Differences
While the last three learners perform similarly on
the simple example above, there are significant
differences between them. These differences be-
come apparent when we consider larger paradigms
with homonymy and free variation.
First, let?s look at an example that involves a
more elaborate homonymy than example 1. Con-
sider, for instance, the following text.
(5) Example 2
A [+f1;+f2;+f3]
A [+f1;?f2;?f3]
A [+f1;+f2;?f3]
A [?f1;+f2;+f3]
B [?f1;?f2;?f3]
72
Given this text, all three learners will differ in
their predictions with respect to the environ-
ment (?f1;+f2;?f3). Learner 2 will pre-
dict both A and B to occur in this environment
since not enough monomials will be removed
from representations of A or B to rule out ei-
ther morph from occurring in (?f1;+f2;?f3).
Learner 3 will predict A to appear in all envi-
ronments that haven?t been seen yet, including
(?f1;+f2;?f3). This is because in the cur-
rent text the minimal monomial for A is (?; ?; ?)
and A has an elsewhere distribution. On the
other hand, Learner 4 predicts B to occur in
(?f1;+f2;?f3). This is because the exten-
sion of the Boolean function for B includes
any environments consistent with (?f1;?f3) or
(?f1;?f2) since these are the simplest monomi-
als that uniquely pick out B.
Thus, the three learners follow very different
generalization routes. Overall, Learner 2 is more
cautious and slower to generalize. It predicts free
variation in all environments for which not enough
data has been seen to converge on a single morph.
Learner 3 is unique in preferring monomial and
elsewhere distributions. For instance, in the above
example it treats A as a ?default? morph. Learner
4 is unique in its preference for morphs describ-
able with disjunction of simpler monomials. Be-
cause of this preference, it will sometimes gener-
alize even after seeing just one instance of a morph
(since several simple monomials can be consistent
with this instance alone).
One way to test what the human learners do
in a situation like the one above is to use artifi-
cial grammar learning experiments. Such experi-
ments have been used for learning individual con-
cepts over features like shape, color, texture, etc.
Some work on concept learning suggests that it is
subjectively easier to learn concepts describable
with shorter formulas (Feldman, 2000; Feldman,
2004). Other recent work challenges this idea (La-
fond et al, 2007), showing that people don?t al-
ways converge on the most minimal representa-
tion, but instead go for the more simple and gen-
eral representation and learn exceptions to it (this
approach is more in line with Learner 3).
Some initial results from my pilot experiments
on learning partitions of concept spaces (using ab-
stract shapes, rather than language stimuli) also
suggest that people find paradigms with else-
where distributions easier to learn than the ones
with overlapping distributions (like the German
paradigms in 2). However, I also found a bias to-
ward paradigms with the fewer number of relevant
features. This bias is consistent with Learner 4
since this learner tries to assume the smallest num-
ber of relevant features possible. Thus, both learn-
ers have their merits.
Another area in which the considered learn-
ers make somewhat different predictions has to
do with free variation. While I can?t discuss
this at length due to space constraints, let me
comment that any batch learner can easily de-
tect free-variation before generalizing, which is
exactly what most of the above learners do (ex-
cept Learner 3, but it can also be changed to do
the same thing). However, since free variation
is rather marginal in morphological paradigms,
it is possible that it would be rather problem-
atic. In fact, free variation is more problematic if
we switch from the batch learners to incremental
learners.
7 Directions for further research
There are of course many other learners one could
consider for learning paradigms, including ap-
proaches quite different in spirit from the ones
considered here. In particular, some recently pop-
ular approaches conceive of learning as matching
probabilities of the observed data (e.g., Bayesian
learning). Comparing such approaches with the
algorithmic ones is difficult since the criteria for
success are defined so differently, but it would
still be interesting to see whether the kinds of
prior assumptions needed for a Bayesian model
to match human performance would have some-
thing in common with properties that the learn-
ers considered here relied on. These properties
include the disjoint nature of paradigm cells, the
prevalence of monomial and elsewhere morphs,
and the economy considerations. Other empirical
work that might help to differentiate Boolean par-
tition learners (besides typological and experimen-
tal work already mentioned) includes finding rele-
vant language acquisition data, and examining (or
modeling) language change (assuming that learn-
ing biases influence language change).
References
David Adger. 2006. Combinatorial variation. Journal
of Linguistics, 42:503?530.
73
Avrim Blum. 1992. Learning Boolean functions in an
infinite attribute space. Machine Learning, 9:373?
386.
Michael Cysouw. 2003. The Paradigmatic Structure
of Person Marking. Oxford University Press, NY.
Jacob Feldman. 2000. Minimization of complexity in
human concept learning. Nature, 407:630?633.
Jacob Feldman. 2004. How surprising is a simple pat-
tern? Quantifying ?Eureka!?. Cognition, 93:199?
224.
John Goldsmith. 2001. Unsupervised learning of a
morphology of a natural language. Computational
Linguistics, 27:153?198.
Anthony Kroch. 1994. Morphosyntactic variation. In
Katharine Beals et al, editor, Papers from the 30th
regional meeting of the Chicago Linguistics Soci-
ety: Parasession on variation and linguistic theory.
Chicago Linguistics Society, Chicago.
Eyal Kushilevitz and Dan Roth. 1996. On learning vi-
sual concepts and DNF formulae. Machine Learn-
ing, 24:65?85.
Daniel Lafond, Yves Lacouture, and Guy Mineau.
2007. Complexity minimization in rule-based cat-
egory learning: revising the catalog of boolean con-
cepts and evidence for non-minimal rules. Journal
of Mathematical Psychology, 51:57?74.
Gary Marcus, Steven Pinker, Michael Ullman,
Michelle Hollander, T. John Rosen, and Fei Xu.
1992. Overregularization in language acquisition.
Monographs of the Society for Research in Child
Development, 57(4). Includes commentary by
Harold Clahsen.
Robert M. Nosofsky, Thomas J. Palmeri, and S.C.
McKinley. 1994. Rule-plus-exception model
of classification learning. Psychological Review,
101:53?79.
Daniel Osherson, Scott Weinstein, and Michael Stob.
1986. Systems that Learn. MIT Press, Cambridge,
Massachusetts.
Katya Pertsova. 2007. Learning Form-Meaning Map-
pings in the Presence of Homonymy. Ph.D. thesis,
University of California, Los Angeles.
Jeffrey Mark Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):1?38, Oct-
Nov.
Matthew G. Snover, Gaja E. Jarosz, and Michael R.
Brent. 2002. Unsupervised learning of morphology
using a novel directed search algorithm: taking the
first step. In Proceedings of the ACL-02 workshop
on Morphological and phonological learning, pages
11?20, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Leslie G. Valiant. 1984. A theory of the learnable.
CACM, 17(11):1134?1142.
Daniel Zeman. 2007. Unsupervised acquiring of mor-
phological paradigms from tokenized text. In Work-
ing Notes for the Cross Language Evaluation Forum,
Budapest. Madarsko. Workshop.
74
Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 72?81,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Linguistic categorization and complexity
Katya Pertsova
UNC-Chapel Hill
Linguistics Dept, CB 3155
Chapel Hill, NC 27599, USA
pertsova@unc.edu
Abstract
This paper presents a memoryless categoriza-
tion learner that predicts differences in cate-
gory complexity found in several psycholin-
guistic and psychological experiments. In par-
ticular, this learner predicts the order of diffi-
culty of learning simple Boolean categories,
including the advantage of conjunctive cate-
gories over the disjunctive ones (an advantage
that is not typically modeled by the statistical
approaches). It also models the effect of la-
beling (positive and negative labels vs. posi-
tive labels of two different kinds) on category
complexity. This effect has implications for
the differences between learning a single cat-
egory (e.g., a phonological class of segments)
vs. a set of non-overlapping categories (e.g.,
affixes in a morphological paradigm).
1 Introduction
Learning a linguistic structure typically involves cat-
egorization. By ?categorization? I mean the task
of dividing the data into subsets, as in learning
what sounds are ?legal? and what are ?illegal,? what
morpheme should be used in a particular morpho-
syntactic context, what part of speech a given words
is, and so on. While there is an extensive literature
on categorization models within the fields of psy-
chology and formal learning, relatively few connec-
tions have been made between this work and learn-
ing of linguistic patterns.
One classical finding from the psychological liter-
ature is that the subjective complexity of categories
corresponding to Boolean connectives follows the
order shown in figure 1 (Bruner et al, 1956; Neisser
and Weene, 1962; Gottwald, 1971). In psycholog-
ical experiments subjective complexity is measured
in terms of the rate and accuracy of learning an arti-
ficial category defined by some (usually visual) fea-
tures such as color, size, shape, and so on. This
finding appears to be consistent with the complex-
ity of isomorphic phonological and morphological
linguistic patterns as suggested by typological stud-
ies not discussed here for reasons of space (Mielke,
2004; Cysouw, 2003; Clements, 2003; Moreton and
Pertsova, 2012). Morphological patterns isomorphic
to those in figure 1 appear in figure 2.
The first goal of this paper is to derive the above
complexity ranking from a learning bias. While the
difficulty of the XOR category is notorious and it
is predicted by many models, the relative difference
between AND and OR is not. This is because these
two categories are complements of each other (so
long as all features are binary), and in this sense
have the same structure. A memorizing learner can
predict the order AND > OR simply because AND
has fewer positive examples, but it will also incor-
rectly predict XOR > OR and AND > AFF. Many
popular statistical classification models do not pre-
dict the order AND > OR (such as models based
on linear classifiers, decision tree classifiers, naive
Bayes classifiers, and so on). This is because the
same classifier would be found for both of these cat-
egories given that AND and OR differ only with re-
spect to what subset of the stimuli is assigned a pos-
itive label. Models proposed by psychologists, such
as SUSTAIN (Love et al, 2004), RULEX (Nosof-
sky et al, 1994b), and Configural Cue (Gluck and
72
AFF (affirmation) AND OR XOR/?
? N
? M
? N
? M
? N
? M
? N
? M
circle circle AND black triangle OR white (black AND triangle)
OR (white AND circle)
Figure 1: Boolean categories over two features, shape and color: AFF > AND > OR > XOR
affirmation AND OR XOR/?
sg pl
?part m. - -im
+part m. - -im
sg pl
?part -s -
+part - -
sg pl
?poss - -s
+poss -s -s
sg pl
acc. - -s
nom. -s -
Hebrew, verb English, verb English nouns Old French,
agreement in pres. agreement in pres. o-stem nouns
Figure 2: Patterns of syncretism isomorphic to the structure of Boolean connectives
Bower, 1988) also do not predict the order AND >
OR fore similar reasons. Feldman (2000) speculates
that this order is due to a general advantage of the
UP-versions of a category over the DOWN-versions
(for a category that divides the set of instances into
two uneven sets, the UP-version is the version in
which the smaller subset is positively labeled, and
the DOWN-version is the version in which the larger
subset is positively labeled). However, he offers no
explanation for this observation. On the other hand,
it is known that the choice of representations can af-
fect learnability. For instance, k-DNF formulas are
not PAC-learnable while k-CNF formulas describing
the same class of patterns are PAC-learnable (Kearns
and Vazirani, 1994). Interestingly, this result also
shows that conjunctive representations have an ad-
vantage over the disjunctive ones because a very
simple strategy for learning conjunctions (Valiant,
1984) can be extended to the problem of learning
k-CNFs. The learner proposed here includes in its
core a similar intersective strategy which is respon-
sible for deriving the order AND > OR.
The second goal of the paper is to provide a uni-
fied account of learning one vs. several categories
that partition the feature space (the second problem
is the problem of learning paradigms). The most
straight-forward way of doing this ? treating cate-
gory labels as another feature with n values for n
labels ? is not satisfactory for several reasons dis-
cussed in section 2. In fact, there is empirical ev-
idence that the same pattern is learned differently
depending on whether it is presented as learning a
distinction between positive and negative instances
of a category or whether it is presented as learning
two different (non-overlapping) categories. This ev-
idence will be discussed in section 3.
I should stress that the learner proposed here is not
designed to be a model of ?performance.? It makes
a number of simplifying assumptions and does not
include parameters that are fitted to match the be-
havioral data. The main goal of the model is to pre-
dict the differences in subjective complexity of cate-
gories as a function of their logical structure and the
presence/absence of negative examples.
2 Learning one versus many categories
Compare the task of learning a phonological in-
ventory with the task of learning an inventory of
morph-meaning pairs (as in learning an inflectional
paradigm). The first task can be viewed as divid-
ing the set of sounds into attested and non-attested
(?accidental gaps?). At first glance, the second task
can be analogously viewed as dividing the set of
stimuli defined by morpho-syntactic features plus
an n-ry feature (for n distinct morphs) into pos-
sible vs. impossible combinations of morphs and
meanings. However, treating morphs as feature val-
ues leads to the possibility of paradigms in which
73
Neutral (AND/ORn)
f1 f1
f2 A B
f2 B B
Biased
ANDb ORb
f1 f1
f2 A ?A
f2 ?A ?A
f1 f1
f2 ?A A
f2 A A
Table 1: Three AND/OR conditions in Gottwald?s study
different morphs are used with exactly the same
set of features as well as paradigms with ?acciden-
tal gaps,? combinations of morpho-syntactic feature
values that are impossible in a language. In fact,
however, morphs tend to partition the space of pos-
sible instances so that no instance is associated with
more than one morph. That is, true free variation
is really rare (Kroch, 1994). Secondly, system-wide
rather than lexical ?accidental gaps? are also rare in
morphology (Sims, 1996). Therefore, I construe the
classification problem in both cases as learning a set
of non-overlapping Boolean formulas correspond-
ing to categories. This set can consist of just one
formula, corresponding to learning a single category
boundary, or it can consist of multiple formulas that
partition the feature space, corresponding to learn-
ing non-overlapping categories each associated with
a different label.
3 Effects of labeling on category
complexity
A study by Gottwald (1971) found interesting dif-
ferences in the subjective complexity of learning
patterns in figure 1 depending on whether the data
was presented to subjects as learning a single cat-
egory (stimuli were labeled A vs. ?A) or whether
it was presented as learning two distinct categories
(the same stimuli were labeled A vs. B). Follow-
ing this study, I refer to learning a single category as
?biased labeling? (abbreviated b) and learning sev-
eral categories as ?neutral labeling? (abbreviated n).
Observe that since the AND/OR category divides the
stimuli into unequal sets, it has two different biased
versions: one biased towards AND and one biased
towards OR (as demonstrated in table 1). The order
of category complexity found by Gottwald was
AFFn, AFFb> ANDb> AND/ORn> ORb, XORb
> XORn
These results show that for the XOR category the
neutral labeling was harder than biased labeling. On
the other hand, for the AND/OR category the neutral
labeling was of intermediate difficulty, and, interest-
ingly, easier than ORb. This is interesting because
it goes against an expectation that learning two cat-
egories should be harder than learning one category.
Pertsova (2012) partially replicated the above find-
ing with morphological stimuli (where null vs. overt
marking was the analog of biased vs. neutral label-
ing). Certain results from this study will be high-
lighted later.
4 The learning algorithm
This proposal is intended to explain the complex-
ity differences found in learning categories in the
lab and in the real world (as evinced by typologi-
cal facts). I focus on two factors that affect category
complexity, the logical structure of a category and
the learning mode. The learning mode refers to bi-
ased vs. neutral labeling, or, to put it differently,
to the difference between learning a single category
and learning a partition of a feature space into sev-
eral categories. The effect of the learning mode on
category complexity is derived from the following
two assumptions: (i) the algorithm only responds to
negative instances when they contradict the current
grammar, and (ii) a collection of instances can only
be referred to if it is associated with a positive label.
The first assumption is motivated by observations of
Bruner et. al (1956) that subjects seemed to rely less
on negative evidence than on positive evidence even
in cases when such evidence was very informative.
The second assumption corresponds to a common
sentiment that having a linguistic label for a cate-
gory aids in learning (Xu, 2002).
4.1 Some definitions
For a finite nonempty set of features F , we define
the set of instances over these features, I(F ), as fol-
lows. LetRf be a set of feature values for a feature f
(e.g., Rheight = {high,mid, low}). Each instance i
is a conjunction of feature values given by the func-
tions f ? Rf for all features f ? F . A category
is a set of instances that can be described by some
74
non-contradictory Boolean formula ?.1 Namely, ?
describes a set of instances X if and only if it is log-
ically equivalent to the disjunction of all instances
in X . For instance, in the world with three binary
features p, q, w, the formula p ? q describes the set
of instances {{pqw}, {pqw?}} (where each instance
is represented as a set). We will say that a formula
? subsumes a formula ? if and only if the set of in-
stances that ? describes is a superset of the set of
instances that ? describes. An empty conjunction ?
describes the set of all instances.
The goal of the learner is to learn a set of Boolean
formulas describing the distribution of positive la-
bels (in the neutral mode all labels are positive, in
the biased mode there is one positive label and one
negative label). A formula describing the distribu-
tion of a label l is encoded as a set of entries of the
form eli (an i-th entry for label l). The distribution
of l is given by el1 ? . . . ? eln , the disjunction of n
formulas corresponding to entries for l. Each entry
eli consists of two components: a maximal conjunc-
tion ?max and an (optional) list of other formulas
EX (for exceptions). A particular entry e with two
components, e[?max] and e[EX] = {?1 . . . ?n}, de-
fines the formula e[?max] ? ?(?1 ? ?2 ? . . . ? ?n).
e[?max] can intuitively be thought of as a rule of
thumb for a particular label and EX as a list of ex-
ceptions to that rule. In the neutral mode exceptions
are pointers to other entries or, more precisely, for-
mulas encoded by those entries. In the biased mode
they are formulas corresponding to instances (i.e.,
conjunctions of feature values for all features). The
algorithm knows which mode it is in because the bi-
ased mode contains negative labels while the neutral
mode does not. Finally, an instance i is consistent
with an entry e if and only if the conjunction en-
coded by i logically implies the formula encoded by
e. For example, an instance {pqw} is consistent with
an entry encoding the formula {p}.
Note that while this grammar can describe arbi-
trarily complex patterns/partitions, each entry in the
neutral learning mode can only describe what lin-
guistics often refer to as ?elsewhere? patterns (more
precisely Type II patterns in the sense of Pertsova
(2011)). And the e[?max] component of each entry
1The set of Boolean formulas is obtained by closing the set
of feature values under the operations of conjunction, negation,
and disjunction.
by definition can only describe conjunctions. There
are additional restrictions on the above grammar: (i)
the exceptions cannot have a wider distribution than
?the rule of thumb? (i.e., an entry el cannot corre-
spond to a formula that does not pick out any in-
stances), (ii) no loops in the statement of exceptions
is possible: that is, if an entry A is listed as an ex-
ception to the entry B, then B cannot also be an ex-
ception for A (a more complicated example of a loop
involves a longer chain of entries).
When learning a single category, there is only
one entry in the grammar. In this case arbitrarily
complex categories are encoded as a complement of
some conjunction with respect to a number of other
conjunctions (corresponding to instances).
4.2 General description
The general organization of the algorithm is as fol-
lows. Initially, each positive label is assumed to cor-
respond to a single grammatical entry, and the ?max
component of this entry is computed incrementally
through an intersective generalization strategy that
extracts features invariant across all instances used
with the same label. When the grammar overgener-
alizes by predicting two different labels for at least
one instance, exceptions are introduced. The pro-
cess of exception listing can also lead to overgener-
alizations if exceptions are pointers to other entries
in the grammar. When these overgeneralizations are
detected the algorithm creates another entry for the
same label. This latter process can be viewed as
positing homophonous entries when learning form-
meaning mappings, or as creating multiple ?clus-
ters? for a single category as in the prototype model
SUSTAIN (Love et al, 2004), and it corresponds to
explicitly positing a disjunctive rule. Note that if
exceptions are not formulas for other labels, but in-
dividual instances, then exception listing does not
lead to overgeneralization and no sub-entries are in-
troduced. Thus, when learning a single category the
learner generalizes by using an intersective strategy,
and then lists exceptions one-by-one as they are dis-
covered in form of negative evidence.
The problem of learning Boolean formulas is
known to be hard (Dalmau, 1999). However, it is
plausible that human learners employ an algorithm
that is not generally efficient, but can easily han-
dle certain restricted types of formulas under certain
75
simple distributions of data. (Subclasses of Boolean
formulas are efficiently learnable in various learning
frameworks (Kearns et al, 1994).) If the learning al-
gorithm can easily learn certain patterns (providing
an explanation for what patterns and distributions
count as simple), we do not need to require that it
be in general efficient.
4.3 Detailed description
First I describe how the grammar is updated in re-
sponse to the data. The update routine uses a strat-
egy that in word-learning literature is called cross-
situational inference. This strategy incrementally fil-
ters out features that change from one instance to
the next and keeps only those features that remain
invariant across the instances that have the same la-
bel. Obviously, this strategy leads to overgeneral-
izations, but not if the category being learned is an
affirmation or conjunction. This is because affirma-
tions and conjunctions are defined by a single set of
feature values which are shared by all instances of a
category (for proof see Pertsova (2007) p. 122). Af-
ter the entry for a given label has been updated, the
algorithm checks whether this entry subsumes or is
subsumed by any other entry. If so, this means that
there is at least one instance for which several labels
are predicted to occur (there is competition among
the entries). The algorithm tries to resolve competi-
tion by listing more specific entries as exceptions to
the more general ones.2 However there are cases in
which this strategy will either not resolve the com-
petition, or not resolve it correctly. In particular,
the intermediate entries that are in competition may
be such that neither subsumes the other. Or after
updating the entries using the intersective strategy
one entry may be subsumed by another based on the
instances that have been seen so far, but not if we
take the whole set of instances into account. These
cases are detected when the predictions of the cur-
rent grammar go against an observed stimulus (step
11 in the function ?Update? below). Finally, excep-
tion listing fails if it would lead to a ?loop? (see sec-
2This idea is familiar in linguistics from at least the times of
Pa?nini. In Distributed Morphology, it is referred to as the Subset
Principle for vocabulary insertion (Halle and Marantz, 1993).
Similar principles are assumed in rule-ordering systems and in
OT (i.e., more specific rules/constraints are typically ordered
before the more general ones).
tion 4.1). The XOR pattern is an example of a simple
pattern that will lead to a loop at some point during
learning. In general this happens whenever the dis-
tribution of the two labels are intertwined in such a
way that neither can be stated as a complement of
the invariant features of the other.
The following function is used to add an excep-
tion:
AddException(expEntry, ruleEntry):
1. if adding expEntry to ruleEntry[EX] leads
to a loop then FAIL
2. else add expEntry to ruleEntry[EX]
The routine below is called within the main func-
tion (presented later); it is used to update the gram-
mar in response to an observed instance x with the
label li (the index of the label is decided in the main
function).
Update
Input: G (current grammar); x (an observed in-
stance), li (a label for this instance)
Output: newG
1: newG? G
2: if ?eli ? newG then
3: eli [?max]? eli [?max] ? x
4: else
5: add the entry eli to newG with values
eli [?max] = x; eli [EX] = {}.
6: for all el?j ? newG (el?j 6= eli) do
7: if el?j subsumes eli then
8: AddException(eli , el?j )
9: else if eli subsumes el?j then
10: AddException(el?j , eli)
11: if ?el?j ? newG (l
? 6= l) such that x is consistent
with el?j then
12: AddException(eli , el?j )
Before turning to the main function of the algo-
rithm, it is important to note that because a grammar
may contain several different entries for a single la-
bel, this creates ambiguity for the learner. Namely,
in case a grammar contains more than one entry for
some label, say two A labels, the learner has to de-
cide after observing a datum (x,A), which entry to
update, eA1 or eA2 . I assume that in such cases the
learner selects the entry that is most similar to the
76
current instance, where similarity is calculated as the
number of features shared between x and eAi [?max]
(although other metrics of similarity could be ex-
plored).
Finally, I would like to note that the value of an
entry el(x) can change even if the algorithm has not
updated this entry. This is because the value of some
other entry that is listed as an exception in el(x)
may change. This is one of the factors contributing
to the difference between the neutral and the biased
learning modes: if exceptions themselves are entries
for other labels, the process of exception listing be-
comes generalizing.
Main
Input: an instance-label pair (x, l), previous hy-
pothesis G (initially set to an empty set)
Output: newG (new hypothesis)
1: set E to the list of existing entries for the label l
in G
2: k ? |E|
3: if E 6= {} then
4: set elcurr to eli ? E that is most similar to x
5: E ? E ? elcurr
6: else
7: curr ? k + 1
8: if l is positive and (??elcurr ? G or x is not
consistent with elcurr ) then
9: if update(G, x, lcurr) fails then
10: goto step 3
11: else
12: newG? update(G, x, lcurr)
13: else if l is negative and there is an entry e in G
consistent with x (positive label was expected)
then
14: add x to e[EX] and minimize e[EX] to get
newG
Notice that the loop triggered when update fails
is guaranteed to terminate because when the list of
all entries for a label l is exhausted, a new entry is
introduced and this entry is guaranteed not to cause
update to fail.
This learner will succeed (in the limit) on most
presentations of the data, but it may fail to converge
on certain patterns if the crucial piece of evidence
needed to resolve competition is seen very early on
and then never again (it is likely that a human learner
would also not converge in such a case).
This algorithm can be additionally augmented by
a procedure similar to the selective attention mech-
anism incorporated into several psychological mod-
els of categorization to capture the fact that certain
hard problems become easy if a subject can ignore
irrelevant features from the outset (Nosofsky et al,
1994a). One (not very efficient, but easy) way to
incorporate selective attention into the above algo-
rithm is as follows. Initially set the number of rel-
evant features k to 1. Generate all subsets of F of
length k, select one such subset Fk and apply the
above learning algorithm assuming that the feature
space is Fk. When processing a particular instance,
ignore all of its features except those that are in Fk.
If we discover two instances that have the same as-
signment of features in Fk but that appear with two
different labels, this means that the selected set of
features is not sufficient (recall that free variation is
ruled out). Therefore, when this happens we can
start over with a new Fk. If all sets of length k
have been exhausted, increase k to k + 1 and re-
peat. As a result of this change, patterns definable
by smaller number of features would generally be
easier to learn than those definable by larger number
of features.
5 Predictions of the model for learning
Boolean connectives
We can evaluate predictions of this algorithm with
respect to category complexity in terms of the pro-
portion of errors it predicts during learning, and in
terms of the computational load, roughly measured
as the number of required runs through the main
loop of the algorithm. Recall that a single data-point
may require several such runs if the update routine
fails and a new sub-category has to be created.
Below, I discuss how the predictions of this al-
gorithm compare to the subjective complexity rank-
ing found in Gottwald?s experiment. First, consider
the relative complexity order in the neutral learning
mode: AFF > AND/OR > XOR.
In terms of errors, the AFF pattern is predicted
to be learned without errors by the above algorithm
(since the intersective strategy does not overgener-
alize when learning conjunctive patterns). When
learning an AND/OR pattern certain orders of data
presentation will lead to an intermediate overgener-
77
alization of the label associated with the disjunctive
category to the rest of the instances. This will hap-
pen if the OR part of the pattern is processed before
the AND part. When learning an XOR pattern, the
learner is guaranteed to overgeneralize one of the
labels on any presentation of the data. Let?s walk
through the learning of the XOR pattern, repeated
below for convenience.
f1 f1
f2 A B
f2 B A
Suppose for simplicity that the space of features
includes only f1 and f2, and that the first two ex-
amples that the learner observes are (A, {f1, f2})
and (A, {f1, f2}). After intersecting {f1, f2}
and {f1, f2} the learner will overgeneralize A
to the whole paradigm. If the next example is
(B, {f1, f2}), the learner will partially correct this
overgeneralization by assuming thatA occurs every-
where except where B does (i.e., except {f1, f2}).
But it will continue to incorrectly predict A in the
remaining fourth cell that has not been seen yet.
When B is observed in that cell, the learner will at-
tempt to update the entry for B through the inter-
section but this attempt will fail (because the en-
try for B will subsume the entry for A, but we
can?t list A as an exception for B since B is al-
ready listed as an exception for A). Therefore, a
new sub-entry for B, {f1, f2}, will be introduced
and listed as another exception for A. Thus, the fi-
nal grammar will contain entries corresponding to
these formulas: B : (f1 ? f2) ? (f1 ? f2) and
A : ?((f1 ? f2) ? (f1 ? f2)).
Overall the error pattern predicted by the learner
is consistent with the order AFF > AND/OR >
XOR.
I now turn to a different measure of complexity
based on the number of computational steps needed
to learn a pattern (where a single step is equated to a
single run of the main function). Note that the speed
of learning a particular pattern depends not only on
the learning algorithm but also on the distribution of
the data. Here I will consider two possible proba-
bility distributions which are often used in catego-
rization experiments. In both distributions the stim-
uli is organized in blocks. In the first one (which
I call ?instance balanced?) each block contains all
possible instances repeated once; in the second dis-
tribution (?label balanced?) each block contains all
possible instances with the minimum number of rep-
etitions to insure equal numbers of each label. The
distributions differ only for those patterns that have
an unequal number of positive/negative labels (e.g.,
AND/OR). Let us now look at the minimum and
maximum number of runs through the main loop of
the algorithm required for convergence for each type
of pattern. The minimum is computed by finding the
shortest sequence of data that leads to convergence
and counting the number of runs on this data. The
maximum is computed analogously by finding the
longest sequence of data. The table below summa-
rizes min. and max. number of runs for the feature
space with 3 binary features (8 possible instances)
and for two distributions.
Min Max Max
(instance) (label)
AFF 4 7 7
AND/OR 4 8 11
XOR 7 9 9
Table 2: Complexity in the neutral mode
The difference between AFF and AND/OR in
the number of runs to convergence is more obvi-
ous for the label balanced distribution. On the other
hand, the difference between AND/OR and XOR is
clearer for the instance balanced distribution. This
difference is not expected to be large for the label
balanced distribution, which is not consistent with
Gottwald?s experiment in which the stimuli were
label balanced, and neutral XOR was significantly
more difficult to learn than any other condition.
We now turn to the biased learning mode. Here,
the observed order of difficulty was: AFFb>ANDb
> ORb, XORb. In terms of errors, both AFFb and
ANDb are predicted to be learned with no errors
since both are conjunctive categories. ORb is pre-
dicted to involve a temporary overgeneralization of
the positive label to the negative contexts. The same
is true for XORb except that the proportion of errors
will be higher than for ORb (since the latter category
has fewer negative instances).
The minimum and maximum number of runs re-
quired to converge on the biased categories for two
types of distributions (instance balanced and label
78
balanced) is given below. Notice that the minimum
numbers are lower than in the previous table because
in the biased mode some categories can be learned
from positive examples alone.
Min Max Max
(instance) (label)
AFFb 2 7 7
ANDb 2 8 8
ORb 4 16 22
XORb 6 16 16
Table 3: Complexity in the biased mode
The difference between affirmation and conjunc-
tion is not very large which is not surprising (both
are conjunctive categories). Again we see that the
two types of distributions give us slightly different
predictions. While ANDb seems to be learned faster
than ORb in both distributions, it is not clear whether
and to what extent ORb and XORb are on average
different from each other in the label balanced dis-
tribution. Recall that Gottwald found no significant
difference between ORb and XORb (in fact numer-
ically ORb was harder than XORb). Interestingly,
in a morphological analogue of Gottwald?s study in
which the number of instances rather than labels was
balanced, I found the opposite difference: ORb was
easier to learn than XORb (the number of people to
reach learning criterion was 8 vs. 4 correspondingly)
although the difference in error rates on the testing
trials was not significant (Pertsova, 2012). More
testing is needed to confirm whether the relative dif-
ficulty of these two categories is reliably affected by
the type of distribution as predicted by the learner.3
Finally, we look at the effect of labeling within
each condition. In the AFF condition, Gottwald
found no significant difference between neutral la-
beling and biased labeling. This could be due to
the fact that subjects were already almost at ceiling
3Another possible reason for the fact that Gottwald did not
find a difference between ORb and XORb is this: if selective at-
tention is used during learning, it will take longer for the learner
to realize that ORb requires the use of two features compared to
XORb especially when the number of positive and negative ex-
amples are balanced. In particular, a one feature analysis of
ORb can explain 5/6 of the data with label balanced stimuli,
while a one feature analysis of XORb can only explain 1/2 of
the data, so it will be quickly abandoned.
in learning this pattern (median number of trials to
convergence for both conditions was ? 5). In the
AND/OR condition, Gottwald observed the interest-
ing order ANDb > AND/OR > ORb. This order
is also predicted by the current algorithm. Namely,
the neutral category AND/OR is predicted to be
harder than ANDb because (1) ANDb requires less
computational resources (2) on some distributions
of data overgeneralization will occur when learn-
ing an AND/OR pattern but not an ANDb category.
The AND/OR > ORb order is also predicted and is
particularly pronounced for label balanced distribu-
tion. Since two labels are available when learning
the AND/OR pattern, the AND portion of the pat-
tern can be learned quickly and subsequently listed
as an exception for the OR portion (which becomes
the?elsewhere? case). On the other hand, when
learning the ORb category, the conjunctive part of
the pattern is initially ignored because it is not as-
sociated with a label. The learner only starts paying
attention to negative instances when it overgeneral-
izes. For a similar reason, the biased XOR category
is predicted to be harder to learn than the neutral
XOR category. This latter prediction is not consis-
tent with Gottwald?s finding, who found XORn not
just harder than other categories but virtually impos-
sible to learn: 6 out of 8 subjects in this condition
failed to learn it after more than 256 trials. In con-
trast to this result (and in line with the predictions
of the present learner), Pertsova (2012) found that
the neutral XOR condition was learned by 8 out of
12 subjects on less than 64 trials compared to only 4
out of 12 subjects in the biased XOR condition.
To conclude this section, almost all complexity
rankings discussed in this paper are predicted by the
proposed algorithm. This includes the difficult to
model AND > OR ranking which obtains in the bi-
ased learning mode. The only exception is the neu-
tral XOR pattern, which was really difficult to learn
in Gottwald?s non-linguistic experiment (but not in
Pertsova?s morphological experiment), and which is
not predicted to be more difficult than biased XOR.
Further empirical testing is needed to clarify the ef-
fect of labeling within the XOR condition.
79
Type I Type II Type III Type IV Type V Type VI
Figure 3: Shepard et. al. hierarchy
6 Other predictions
Another well-studied hierarchy of category com-
plexity is the hierarchy of symmetric patterns (4 pos-
itive and 4 negative instances) in the space of three
binary features originally established by Shepard et.
al (1961). These patterns are shown in figure 3 us-
ing cubes to represent the three dimensional feature
space.
Most studies find the following order of complex-
ity for the Shepad patterns: I > II > III, IV, V > VI
(Shepard et al, 1961; Nosofsky et al, 1994a; Love,
2002; Smith et al, 2004). However, a few studies
find different rankings for some of these patterns. In
particular, Love (2002) finds IV > II with a switch
to unsupervised training procedure. Nosofsky and
Palmeri (1996) find the numerical order I> IV> III
> V > II > VI with intergral stimulus dimensions
(feature values that are difficult to pay selective at-
tention to independent of other features, e.g., hue,
brightness, saturation). More recently Moreton and
Persova (2012) also found the order IV > III > V,
VI (as well as I > II, III, > VI) in an unsupervised
phonotactics learning experiment.
So, one might wonder what predictions does the
present learner make with respect to these patterns.
We already know that it predicts Type I (affirmation)
to be easier than all other types. For the rest of the
patterns the predictions in terms of speed of acquisi-
tion are II > III > IV, V > VI in the neutral learning
mode (similar to the typical findings). In the biased
learning mode, patterns II through VI are predicted
to be learned roughly at the same speed (since all re-
quire listing four exceptions). If selective attention
is used, Type II will be the second easiest to learn
after Type I because it can be stated using only two
features. However, based on the error rates, the order
of difficulty is predicted to be I > IV > III > V >
II > VI (similar to the order found by Nosofsky and
Palmeri (1996)). No errors are ever made with Type
I. The proportion of errors in other patterns depends
on how closely the positive examples cluster to each
other. For instance, when learning a Type VI pattern
(in the biased mode) the learner?s grammar will be
correct on 6 out of 8 instances after seeing any two
positive examples (the same is not true for any other
pattern, although it is almost true for III). After see-
ing the next instance (depending on what it is and
on the previous input) the accuracy of the grammar
will either stay the same, go up to 7/8, or go down to
1/2. But the latter event has the lowest probability.
Note that this learner predicts non-monotonic behav-
ior: it is possible that a later grammar is less accurate
than the previous grammar. So, for a non-monotonic
learner the predictions based on the speed of acqui-
sition and accuracy do not necessarily coincide.
There are many differences across the categoriza-
tion experiments that may be responsible for the dif-
ferent rankings. More work is needed to control for
such differences and to pin down the sources for dif-
ferent complexity results found with the patterns in
figure 3.
7 Summary
The current proposal presents a unified account for
learning a single category and a set of categories par-
titioning the stimuli space. It is consistent with many
predictions about subjective complexity rankings of
simple categories, including the ranking AND >
OR, not predicted by most categorization models,
and the difference between the biased and the neu-
tral learning modes not previously modeled to my
knowledge.
References
Jerome S. Bruner, Jacqueline J. Goodnow, and George A.
Austin. 1956. A study of thinking. John Wiley and
Sons, New York.
80
George N. Clements. 2003. Feature economy in sound
systems. Phonology, 20(3):287?333.
Michael Cysouw. 2003. The paradigmatic structure of
person marking. Oxford studies in typology and lin-
guistic theory. Oxford University Press, Oxford.
V??ctor Dalmau. 1999. Boolean formulas are hard to
learn for most gate bases. In Osamu Watanabe and
Takashi Yokomori, editors, Algorithmic Learning The-
ory, volume 1720 of Lecture Notes in Computer Sci-
ence, pages 301?312. Springer Berlin / Heidelberg.
Jacob Feldman. 2000. Minimization of Boolean com-
plexity in human concept learning. Nature, 407:630?
633.
Mark A. Gluck and Gordon H. Bower. 1988. evaluating
an adaptive network model of human learning. Jour-
nal of memory and language, 27:166?195.
Richard L. Gottwald. 1971. Effects of response labels in
concept attainment. Journal of Experimental Psychol-
ogy, 91(1):30?33.
Morris Halle and Alec Marantz. 1993. Distributed mor-
phology and the pieces of inflection. In K. Hale and
S. J. Keyser, editors, The View from Building 20, pages
111?176. MIT Press, Cambridge, Mass.
Michael Kearns and Umesh Vazirani. 1994. An intro-
duction to computational learning theory. MIT Press,
Cambridge, MA.
Michael Kearns, Ming Li, and Leslie Valiant. 1994.
Learning boolean formulas. J. ACM, 41(6):1298?
1328, November.
Anthony Kroch. 1994. Morphosyntactic variation. In
Katharine Beals et al, editor, Papers from the 30th
regional meeting of the Chicago Linguistics Soci-
ety: Parasession on variation and linguistic theory.
Chicago Linguistics Society, Chicago.
Bradley C. Love, Douglas L. Medin, and Todd M.
Gureckis. 2004. SUSTAIN: a network model of cat-
egory learning. Psychological Review, 111(2):309?
332.
Bradley C. Love. 2002. Comparing supervised and unsu-
pervised category learning. Psychonomic Bulletin and
Review, 9(4):829?835.
Jeff Mielke. 2004. The emergence of distinctive features.
Ph.D. thesis, Ohio State University.
Elliott Moreton and Katya Pertsova. 2012. Is phonolog-
ical learning special? Handout from a talk at the 48th
Meeting of the Chicago Society of Linguistics, April.
Ulrich Neisser and Paul Weene. 1962. Hierarchies in
concept attainment. Journal of Experimental Psychol-
ogy, 64(6):640?645.
Robert M. Nosofsky and Thomas J. Palmeri. 1996.
Learning to classify integral-dimension stimuli. Psy-
chonomic Bulletin and Review, 3(2):222?226.
Robert M. Nosofsky, Mark A. Gluck, Thomas J. Palmeri,
Stephen C. McKinley, and Paul Gauthier. 1994a.
Comparing models of rule-based classification learn-
ing: a replication and extension of Shepard, Hov-
land, and Jenkins (1961). Memory and Cognition,
22(3):352?369.
Robert M. Nosofsky, Thomas J. Palmeri, and Stephen C.
McKinley. 1994b. Rule-plus-exception model of clas-
sification learning. Psychological Review, 101(1):53?
79.
Katya Pertsova. 2007. Learning Form-Meaning Map-
pings in the Presence of Homonymy. Ph.D. thesis,
UCLA.
Katya Pertsova. 2011. Grounding systematic syncretism
in learning. Linguistic Inquiry, 42(2):225?266.
Katya Pertsova. 2012. Logical complexity in morpho-
logical learning. In Proceedings of the 38th Annual
Meeting of the Berkeley Linguistics Society.
Roger N. Shepard, C. L. Hovland, and H. M. Jenkins.
1961. Learning and memorization of classifications.
Psychological Monographs, 75(13, Whole No. 517).
Andrea Sims. 1996. Minding the Gaps: inflectional
defectiveness in a paradigmatic theory. Ph.D. thesis,
The Ohio State University.
J. David Smith, John Paul Minda, and David A. Wash-
burn. 2004. Category learning in rhesus monkeys:
a study of the Shepard, Hovland, and Jenkins (1961)
tasks. Journal of Experimental Psychology: General,
133(3):398?404.
Leslie G. Valiant. 1984. A theory of the learnable. In
Proceedings of the sixteenth annual ACM symposium
on Theory of computing, STOC ?84, pages 436?445,
New York, NY, USA. ACM.
Fei Xu. 2002. The role of language in acquiring object
kind concepts in infancy. Cognition, 85(3):223 ? 250.
81
