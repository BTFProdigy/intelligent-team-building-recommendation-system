Towards Light Semantic Processing for Question Answering
Benjamin Van Durme?, Yifen Huang?, Anna Kups?c??+, Eric Nyberg?
?Language Technologies Institute, Carnegie Mellon University
+Polish Academy of Sciences
{vandurme,hyifen,aniak,ehn}@cs.cmu.edu
Abstract
The paper1 presents a lightweight knowledge-
based reasoning framework for the JAVELIN
open-domain Question Answering (QA) sys-
tem. We propose a constrained representation
of text meaning, along with a flexible unifica-
tion strategy that matches questions with re-
trieved passages based on semantic similarities
and weighted relations between words.
1 Introduction
Modern Question Answering (QA) systems aim at pro-
viding answers to natural language questions in an open-
domain context. This task is usually achieved by com-
bining information retrieval (IR) with information extrac-
tion (IE) techniques, modified to be applicable to unre-
stricted texts. Although semantics-poor techniques, such
as surface pattern matching (Soubbotin, 2002; Ravichan-
dran and Hovy, 2002) or statistical methods (Ittycheriah
et al, 2002), have been successful in answering fac-
toid questions, more complex tasks require a consider-
ation of text meaning. This requirement has motivated
work on QA systems to incorporate knowledge process-
ing components such as semantic representation, ontolo-
gies, reasoning and inference engines, e.g., (Moldovan et
al., 2003), (Hovy et al, 2002), (Chu-Carroll et al, 2003).
Since world knowledge databases for open-domain tasks
are unavailable, alternative approaches for meaning rep-
resentation must be adopted. In this paper, we present
our preliminary approach to semantics-based answer de-
tection in the JAVELIN QA system (Nyberg et al, 2003).
In contrast to other QA systems, we are trying to realize
a formal model for a lightweight semantics-based open-
domain question answering. We propose a constrained
semantic representation as well as an explicit unification
1The authors appear in alphabetical order.
framework based on semantic similarities and weighted
relations between words. We obtain a lightweight roboust
mechanism to match questions with answer candidates.
The organization of the paper is as follows: Section 2
briefly presents system components; Section 3 discusses
syntactic processing strategies; Sections 4 and 5 describe
our preliminary semantic representation and the unifica-
tion framework which assigns confidence values to an-
swer candidates. The final section contains a summary
and future plans.
2 System Components
The JAVELIN system consists of four basic components:
a question analysis module, a retrieval engine, a passage
analysis module (supporting both statistical and NLP
techniques), and an answer selection module. JAVELIN
also includes a planner module, which supports feedback
loops and finer control over specific components (Nyberg
et al, 2003). In this paper we are concerned with the two
components which support linguistic analysis: the ques-
tion analysis and passage understanding modules (Ques-
tion Analyzer and Information Extractor, respectively).
The relevant aspects of syntactic processing in both mod-
ules are presented in Section 3, whereas the semantic rep-
resentation is introduced in Section 4.
3 Parsing
The system employs two different parsing techniques:
a chart parser with hand-written grammars for ques-
tion analysis, and a lexicalized, broad coverage skipping
parser for passage analysis. For question analysis, pars-
ing serves two goals: to identify the finest answer focus
(Moldovan et al, 2000; Hermjakob, 2001), and to pro-
duce a grammatical analysis (f-structure) for questions.
Due to the lack of publicly available parsers which have
suitable coverage of question forms, we have manually
developed a set of grammars to achieve these goals. On
the other hand, the limited coverage and ambiguity in
these grammars made adopting the same approach for
passage analysis inefficient. In effect, we use two dis-
tinct parsers which provide two syntactic representations,
including grammatical functions. These syntactic struc-
tures are then transformed into a common semantic rep-
resentation discussed in Section 4.
( (Brill-pos VBN)
(adjunct (
(object (
(Brill-pos WRB)
(atype temporal)
(cat n)
(ortho When)
(q-focus +)
(q-token +)
(root when)
(tokens 1)))
(time +)))
(cat v)
(finite +)
(form finite)
(modified +)
(ortho founded)
(passive +)
(punctuation (
(Brill-pos ".")
(cat punct)
(ortho ?)
(root ?)
(tokens 6)))
(qa (
(gap (
(atype temporal)
(path (*MULT* adjunct
object))))
(qtype entity)))
(root found)
(subject (
(BBN-name person)
(Brill-pos NNP)
(cat n)
(definite +)
(gen-pn +)
(human +)
(number sg)
(ortho "Wendy?s")
(person third)
(proper-noun +)
(root wendy)
(tokens 3)))
(tense past)
(tokens 5))
Figure 1: When was Wendy?s founded: KANTOO f-
structure
3.1 Questions
The question analysis consists of two steps: lexical pro-
cessing and syntactic parsing. For the lexical process-
ing step, we have integrated several external resources:
the Brill part-of-speech tagger (Brill, 1995), BBN Identi-
Finder (BBN, 2000) (to tag named entities such as proper
names, time expressions, numbers, etc.), WordNet (Fell-
baum, 1998) (for semantic categorization), and the KAN-
TOO Lexifier (Nyberg and Mitamura, 2000) (to access a
syntactic lexicon for verb valence information).
The hand-written grammars employed in the project
are based on the Lexical Functional Grammar (LFG) for-
malism (Bresnan, 1982), and are used with the KANTOO
parser (Nyberg and Mitamura, 2000). The parser out-
puts a functional structure (f-structure) which specifies
the grammatical functions of question components, e.g.,
subject, object, adjunct, etc. As illustrated in Fig. 1, the
resulting f-structure provides a deep, detailed syntactic
analysis of the question.
3.2 Passages
Passages selected by the retrieval engine are processed
by the Link Grammar parser (Grinberg et al, 1995). The
parser uses a lexicalized grammar which specifies links,
i.e., grammatical functions, and provides a constituent
structure as output. The parser covers a wide range of
syntactic constructions and is robust: it can skip over un-
recognized fragments of text, and is able to handle un-
known words.
An example of the passage analysis produced by the
Link Parser is presented in Fig. 2. Links are treated as
predicates which relate various arguments. For exam-
ple, O in Fig. 2 indicates that Wendy?s is an object of the
verb founded. In parallel to the Link parser, passages are
tagged with the BBN IdentiFinder (BBN, 2000), in or-
der to group together multi-word proper names such as
R. David Thomas.
4 Semantic Representation
At the core of our linguistic analysis is the semantic rep-
resentation, which bridges the distinct representations of
the functional structure obtained for questions and pas-
sages. Although our semantic representation is quite sim-
ple, it aims at providing the means of understanding and
processing broad-coverage linguistic data. The represen-
tation uses the following main constructs:2
? formula is a conjunction of literals and represents
the meaning of the entire sentence (or question);
? literal is a predicate relation over two terms; in par-
ticular, we distinguish two types of literals: extrin-
sic literal, a literal which relates a label to a label,
and intrinsic literal, a literal which relates a label to
a word;
2The use of terminology common in the field of formal logic
is aimed at providing an intuitive understanding to the reader,
but is not meant to give the impression that our work is built on
a firm logic-theoretic framework.
+------------------------Xp------------------------+
| +-------MVp-------+ |
+--------Wd-------+ +------O------+ | |
| +-G-+---G--+---S---+ +--YS-+ +-IN+ |
| | | | | | | | | |
LEFT-WALL R. David Thomas founded.v Wendy ?s.p in 1969 .
Constituent tree:
(S (NP R. David Thomas)
(VP founded
(NP (NP Wendy ?s))
(PP in
(NP 1969)))
.)
Figure 2: R. David Thomas founded Wendy?s in 1969.: Link Grammar parser output
? predicate is used to capture relations between
terms;
? term is either a label, a variable which refers to a
specific entity or an event, or a word, which is either
a single word (e.g., John) or a sequence of words
separated by whitespace (e.g., for proper names such
as John Smith).
The BNF syntax corresponding to this representation
is given in (1).
(1) <formula> := <literal>+
<literal> := <pred>(<term>,<term>)
<term> := <label>|<word>
<word> := |[a-nA-Z0-9\s]+|
<label> := [a-z]+[0-9]+
<pred> := [A-Z_-]+
With the exception of the unary ANS predicate which
indicates the sought answer, all predicates are binary re-
lations (see examples in Fig. 3). Currently, most pred-
icate names are based on grammatical functions (e.g.,
SUBJECT, OBJECT, DET) which link events and entities
with their arguments. Unlike in (Moldovan et al, 2003),
names of predicates belong to a fixed vocabulary, which
provides a more sound basis for a formal interpretation.
Names of labels and terms are restricted only by the syn-
tax in (1). Examples of semantic representations for the
question When was Wendy?s founded? and the passage R.
David Thomas founded Wendy?s in 1969. are shown in
Fig. 4.
Note that our semantic representation reflects the
?canonical? structure of an active sentence. This design
decision was made in order to eliminate structural differ-
ences between semantically equivalent structures. Hence,
at the semantic level, all passive sentences correspond to
their equivalents in the active form. Semantic representa-
tion of questions is not always derived directly from the
f-structure. For some types of questions, e.g., definition
When was Wendy?s R. David Thomas founded
founded? Wendy?s in 1969.
ROOT(x6,|Wendy?s|) ROOT(x6,|Wendy?s|)
ROOT(x2,|found|) ROOT(x2,|ound|)
ADJUNCT(x2,x1) ADJUNCT(x2,x1)
OBJECT(x2,x6) OBJECT(x2,x6)
SUBJECT(x2,x7) SUBJECT(x2,x7)
ROOT(x7,|R. David Thomas|)
TYPE(x2,|event|) TYPE(x2,|event|)
TENSE(x2,|past|)
ROOT(x1,x9) ROOT(x1,|1969|)
TYPE(x1,|time|) TYPE(x1,|time|)
ANS(x9)
Figure 4: An example of question and passage semantic
representation
questions such as What is the definition of hazmat?, spe-
cialized (dedicated) grammars are used, which allows us
to more easily arrive at an appropriate representation of
meaning. Also, in the preliminary implementation of the
unification algorithm (see Section 5), we have adopted
some simplifying assumptions, and we do not incorpo-
rate sets in the current representation.
The present formalism can quite successfully handle
questions (or sentences) which refer to specific events or
relations. However, it is more difficult to represent ques-
tions like What is the relationship between Jesse Ventura
and Target Stores?, which seek a relation between enti-
ties or a common event they participated in. In the next
section, we discuss the unification scheme which allows
us to select answer candidates based on the proposed rep-
resentation.
5 Fuzzy Unification
A unification algorithm is required to match question rep-
resentations with the representations of extracted pas-
sages which might contain answers. Using a precursor
predicate example comments
ROOT ROOT(x13,|John|) the root form of entity/event x13
OBJECT OBJECT(x2,x3) x3 is the object of verb
or preposition x2
SUBJECT SUBJECT(x2,x3) x3 is the subject of verb x2
DET DET(x2,x1) x1 is a determiner/quantifier of x2
TYPE TYPE(x3,|event|) x3 is of the type event
TENSE TENSE(x1,|present|) x1 is a verb in present tense
EQUIV EQUIV(x1,x3) semantic equivalence:
apposition: ?John, a student of CMU?
equality operator in copular sentences:
?John is a student of CMU?
ATTRIBUTE ATTRIBUTE(x1,x3) x3 is an adjective modifier of x1:
adjective-noun: ?stupid John?
copular constructions: ?John is stupid?
PREDICATE PREDICATE(x2,x3) copular constructions: ?Y is x3?
ROOT(x2,|be|) SUBJECT(x2,Y)
PREDICATE(x2,x3)
POSSESSOR POSSESSOR(x2,x4) x4 is the possessor of x2
?x4?s x2? or ?x2 of x4?
AND AND(x3,x1) ?John and Mary laughed.?
AND(x3,x2) ROOT(x1,|John|) ROOT(x2,|Mary|)
ROOT(x4,|laugh|) TYPE(x4,|event|)
AND(x3,x1)
AND(x3,x2)
SUBJECT(x4,x3)
ANS ANS(x1) only for questions: x1 indicates the answer
Figure 3: Examples of predicates
to the representation presented above, we constructed
an initial prototype using a traditional theorem prover
(Kalman, 2001). Answer extraction was performed by at-
tempting a unification between logical forms of the ques-
tion and retrieved passages. Early tests showed that a uni-
fication strategy based on a strict boolean logic was not
as flexible as we desired, given the lack of traditional do-
main constraints that one normally possesses when con-
sidering this type of approach. Unless a retrieved pas-
sage exactly matched the question, as in Fig. 4, the sys-
tem would fail due to lack of information. For instance,
knowing that Benjamin killed Jefferson. would not an-
swer the question Who murdered Jefferson?, using a strict
unification strategy.
This has led to more recent experimentation with prob-
abilistic models that perform what we informally refer to
as fuzzy unification.3 The basic idea of our unification
strategy is to treat relationships between question terms
as a set of weighted constraints. The confidence score
assigned to each extracted answer candidate is related to
the number of constraints the retrieved passage satisfies,
along with a measure of similarity between the relevant
terms.
5.1 Definitions
In this section, we present definitions which are necessary
for discussion of the similarity measure employed by our
fuzzy unification framework.
Given a user query Q, where Q is a formula, we re-
trieve a set of passages P. Our task to is find the best
passage Pbest ? P from which an answer candidate can
be extracted. An answer candidate exists within a pas-
sage P if the result of a fuzzy unification between Q and
P results in the single term of ANS(x0) being ground in a
term from P .
(2) Pbest = argmaxP? Psim(Q,P )
The restriction that an answer candidate must be found
within a passage P must be made explicit, as our no-
tion of fuzzy unification is such that a passage can unify
against a query with a non-zero level of confidence even
if one or more constraints from the query are left unsat-
isfied. Since the final goal is to find and return the best
possible answer, we are not concerned with those pas-
sages which seem highly related yet do not offer answer
candidates.
In Section 4, we introduced extrinsic literals where
predicates serve as relations over two labels. Extrinsic lit-
erals can be thought of as relations defined over distinct
3Fuzzy unification in a formal setting generally refers to a
unification framework that is employed in the realm of fuzzy
logics. Our current representation is of an ad-hoc nature, but
our usage of this term does foreshadow future progression to-
wards a representation scheme dependent on such a formal,
non-boolean model.
entities in our formula. For example, SUBJECT(x1, x2)
is an extrinsic literal, while ROOT(x1, |Benjamin|) is not.
The latter has been defined as an intrinsic literal in Sec-
tion 4 and it relates a label and a word.
This terminology is motivated by the intuitive distinc-
tion between intrinsic and extrinsic properties of an entity
in the world. We use this distinction as a simplifying as-
sumption in our measurements of similarity, which we
will now explain in more detail.
5.2 Similarity Measure
Given a set of extrinsic literals PE and QE from a pas-
sage and the question, respectively, we measure the sim-
ilarity between QE and a given ordering of PE as the
geometric mean of the similarity between each pair of
extrinsic literals from the sets QE and PE .
Let O be the set of all possible orderings of PE , O
an element of O, QEj literal j of QE , and Oj literal j of
ordering O. Then:
(3) sim(Q,P )= sim(QE , PE)
= maxO? O(
?n
j=0 sim(QEj , Oj))
1
n
The similarity of two extrinsic literals, lE and lE? , is
computed by the square root of the similarity scores of
each pair of labels, multiplied by the weight of the given
literal, dependent on the equivilance of the predicates
p, p? of the respective literals lE , lE? . If the predicates are
not equivilant, we rely on the engineers tactic of assign-
ing an epsilon value of similarity, where  is lower than
any possible similarity score4. Note that the similarity
score is still dependent on the weight of the literal, mean-
ing that failing to satisfy a heavier constraint imposes a
greater penalty than if we fail to satisfy a constraint of
lesser importance.
Let tj and t?j be the respective j-th term of lE , lE
?
.
Then:
(4) sim(lE , lE?) = weight(lE)?
{(sim(t0,t?0)?sim(t1,t?1))
1
2 ,p=p?
,otherwise
The weight of a literal is meant to capture the relative
importance of a particular constraint in a query. In stan-
dard boolean unification the importance of a literal is uni-
form, as any local failure dooms the entire attempt.5 In a
non-boolean framework the importance of one literal vs.
another becomes an issue. As an example, given a ques-
tion concerning a murder we might be more interested in
the suspect?s name than in the fact that he was tall. This
4The use of a constant value of  is ad hoc, and we are in-
vestigating more principled methods for assigning this penalty.
5That is to say, classic unification is usually an all or nothing
affair.
idea is similar to that commonly seen in information re-
trieval systems which place higher relative importance on
terms in a query that are judged a priori to posses higher
information value. While our prototype currently sets all
literals with a weight of 1.0, we are investigating methods
to train these weights to be specific to question type.
Per our definition, all terms within an extrinsic literal
will be labels. Thus, in equation (10), t0 is a label, as is
t1, and so on. Given a pair of labels, b and b?, we let I, I ?
be the respective sets of intrinsic literals from the formula
containing b, b? such that for all intrinsic literals lI ? I ,
the first term of lI is b, and likewise for b?, I ?.
Much like similarity between two formulae, the sim-
ilarity between two labels relies on finding the maximal
score over all possible orderings of a set of literals.
Now let O be the set of all possible orderings of I ?, O
an element of O, Ij the j-th literal of I , and Oj the j-th
literal of O. Then:
(5) sim(b, b?) = maxO? O(
?n
j=0 sim(Ij , Oj))
1
n
We measure the similarity between a pair of intrinsic
literals as the similarity between the two words multi-
plied by the weight of the first literal, dependent on the
predicates p, p? of the respective literals being equivilant.
(6)
sim(lI , lI
?
) = weight(lI) ?
{sim(t1,t?1),p=p?
,otherwise
The similarity between two words is currently measured
using a WordNet distance metric, applying weights intro-
duced in (Moldovan et al, 2003). We will soon be inte-
grating metrics which rely on other dimensions of simi-
larity.
5.3 Example
We now walk through a simple example in order to
present the current framework used to measure the level
of constraint satisfaction (confidence score) achieved by
a given passage. While a complete traversal of even a
small passage would exceed the space available here, we
will present a single instance of each type of usage of the
sim() function.
If we limit our focus to only a few key relationships,
we get the following analysis of a given question and pas-
sage.
(7) Who killed Jefferson?
ANS(x0), ROOT(x1,x0), ROOT(x2,|kill|),
ROOT(x3,|Jefferson|), TYPE(x2,|event|),
TYPE(x1,|person|), TYPE(x3,|person|), SUB-
JECT(x2,x1), OBJECT(x2,x3)
(8) Benjamin murdered Jefferson.
ROOT(y1,|Benjamin|), ROOT(y2,|murder|),
ROOT(y3,|Jefferson|), TYPE(y2,|event|),
TYPE(y1,|person|), TYPE(y3,|person|), SUB-
JECT(y2,y1), OBJECT(y2,y3)
Computing the similarity between two formulae,
(loosely referred to here by their original text), gives the
following:
(9) sim[|Who killed Jefferson?|,
|Benjamin murdered Jefferson.|] =
(sim[ SUBJECT(x2,x1), SUBJECT(y2,y1)]?
sim[ OBJECT(x2,x3), OBJECT(y2,y3)]) 12
The similarity between the given extrinsic literals shar-
ing the predicate SUBJECT:
(10) sim[SUBJECT(x2,x1), SUBJECT(y2,y1)] =
(sim[x2, y2] ? sim[x1, y1]) 12 ?
weight[SUBJECT(x2,x1)]
In order to find the result of this extrinsic similarity
evaluation, we need to determine the similarity between
the paired terms, (x1,y1) and (x2,y2). The similarity be-
tween x1 and y1 is measured as:
(11) sim[x2, y2] =
(sim[ROOT(x2,|kill|), ROOT(y2,|murder|)]?
sim[TYPE(x2,|event|), TYPE(y2,|event|)]) 12
The result of this function depends on the combined
similarity of the intrinsic literals that relate the given
terms to values. The similarity between one of these in-
trinsic literal pairs is measured by:
(12) sim[ROOT(x2,|kill|), ROOT(y2,|murder|)] =
sim[|kill|, |murder|]?weight[ROOT(x2,|kill|)]
Finally, the similarity between a pair of words is com-
puted as:
(13) sim[|kill|, |murder|] = 0.8
As stated earlier, our similarity metrics at the word
level are currently based on recent work on WordNet dis-
tance functions. We are actively developing methods to
complement this approach.
6 Summary and Future Work
The paper presents a lightweight semantic processing
technique for open-domain question answering. We pro-
pose a uniform semantic representation for questions and
passages, derived from their functional structure. We also
describe the unification framework which allows for flex-
ible matching of query terms with retrieved passages.
One characteristics of the current representation is that
it is built from grammatical functions and does not uti-
lize a canonical set of semantic roles and concepts. Our
overall approach in JAVELIN was to start with the sim-
plest form of meaning-based matching that could extend
simple keyword-based approaches. Since it was possi-
ble to extract grammatical functions from unrestricted
text fairly quickly (using KANTOO for questions and the
Link Grammar parser for answer passages), this frame-
work provides a reasonable first step. We intend to extend
our representation and unification algorithm by incorpo-
rating the Lexical Conceptual Structure Database (Dorr,
2001), which will allow us to use semantic roles instead
of grammatical relations as predicates in the represen-
tation. We also plan to enrich the representation with
temporal expressions, incorporating the ideas presented
in (Han, 2003).
Another limitation of the current implementation is
the limited scope of the similarity function. At present,
the similarity function is based on relationships found
in WordNet, and only relates words which belong to the
same syntactic category. We plan to extend our similar-
ity measure by using name lists, gazetteers and statistical
cooccurrence in text corpora. A complete approach to
word similarity will also require a suitable algorithm for
reference resolution. Unrestricted text makes heavy use
of various forms of co-reference, such as anaphora, def-
inite description, etc. We intend to adapt the anaphora
resolution algorithms used in KANTOO for this purpose,
but a general solution to resolving definite reference (e.g.,
the use of ?the organization? to refer to ?Microsoft?) is a
topic for ongoing research.
Acknowledgments
Research presented in this paper has been supported in
part by an ARDA grant under Phase I of the AQUAINT
program. The authors wish to thank all members of the
JAVELIN project for their support in preparing the work
presented in this paper. We are also grateful to two anony-
mous reviewers, Laurie Hiyakumoto and Kathryn Baker
for their comments and suggestions for improving this
paper. Needless to say, all remaining errors and omis-
sions are entirely our responsibility.
References
BBN Technologies, 2000. IdentiFinder User Manual.
Joan Bresnan, editor. 1982. The Mental Representation
of Grammatical Relations. MIT Press Series on Cog-
nitive Theory and Mental Representation. The MIT
Press, Cambridge, MA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21:543?565.
Jenifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003. A multi-
strategy and multi-source approach to question an-
swering. In TREC 2002 Proceedings.
Bonnie J. Dorr. 2001. LCS Database Docu-
mentation. HTML Manual. available from
http://www.umiacs.umd.edu/?bonnie/LCS Data-
base Documentation.html.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for link grammars.
In Proceedings of the Fourth International Workshop
on Parsing Technologies, Prague, September.
Benjamin Han. 2003. Text temporal analysis. Research
status summary. Draft of January 2003.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proceedings of the
Workshop on Open-Domain Question Answering at
ACL-2001.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin. 2002.
The use of external knowledge in factoid qa. In Pro-
ceedings of the TREC-10 Conference.
Abraham Ittycheriah and Salim Roukos. 2003. IBM?s
statistical question answering system ? TREC-11. In
TREC 2002 Proceedings.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2002. IBM?s statistical question answering system ?
TREC-10. In TREC 2001 Proceedings.
John A. Kalman. 2001. Automated Reasoning with OT-
TER. Rinton Press.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL-2000).
Dan Moldovan, Sanda Harabagiu, Roxana Girju, Paul
Morarescu, Finley Lacatusu, Adrian Novischi, Adriana
Badulescu, and Orest Bolohan. 2003. LCC tools for
question answering. In TREC 2002 Proceedings.
Eric Nyberg and Teruko Mitamura. 2000. The KAN-
TOO machine translation environment. In Proceed-
ings of AMTA 2000.
Eric Nyberg, Teruko Mitamura, Jaime Carbonell, Jaime
Callan, Kevyn Collins-Thompson, Krzysztof Czuba,
Michael Duggan, Laurie Hiyakumoto, Ng Hu, Yifen
Huang, Jeongwoo Ko, Lucian V. Lita, Stephen
Murtagh, Vasco Pedro, and David Svoboda. 2003. The
JAVELIN question answering system at TREC 2002.
In TREC 2002 Proceedings.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a Question Answering system.
In Proceedings of the ACL Conference.
Martin M. Soubbotin. 2002. Patterns of potential answer
expressions as clues to the right answer. In TREC 2001
Proceedings.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 921?928
Manchester, August 2008
Class-Driven Attribute Extraction
Benjamin Van Durme, Ting Qian and Lenhart Schubert
Department of Computer Science
University of Rochester
Rochester, NY 14627, USA
Abstract
We report on the large-scale acquisition
of class attributes with and without the
use of lists of representative instances, as
well as the discovery of unary attributes,
such as typically expressed in English
through prenominal adjectival modifica-
tion. Our method employs a system based
on compositional language processing, as
applied to the British National Corpus. Ex-
perimental results suggest that document-
based, open class attribute extraction can
produce results of comparable quality as
those obtained using web query logs, indi-
cating the utility of exploiting explicit oc-
currences of class labels in text.
1 Introduction
Recent work on the task of acquiring attributes
for concept classes has focused on the use of pre-
compiled lists of class representative instances,
where attributes recognized as applying to multi-
ple instances of the same class are inferred as be-
ing likely to apply to most, or all, members of
that class. For example, the class US President
might be represented as a list containing the en-
tries Bill Clinton, George Bush, Jimmy Carter, etc.
Phrases such as Bill Clinton?s chief of staff ..., or
search queries such as chief of staff bush, provide
evidence that the class US President has as an at-
tribute chief of staff.
Usually the focus of such systems has been on
on binary attributes, such as the example chief of
staff, while less attention has been paid to unary
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
class attributes such as illegal for the class Drug,
or warm-blooded for the class Animal.
1
These
attributes are most typically expressed in English
through prenominal adjectival modification, with
the nominal serving as a class designator. When
attribute extraction is based entirely on instances
and not the class labels themselves, this form of
modification goes undiscovered.
In what follows we explore both the impact of
gazetteers in attribute extraction as well as the
acquisition and filtering of unary class attributes,
through a process based on logical form genera-
tion from syntactic parses derived from the British
National Corpus.
2 Extraction Framework
Extraction was performed using a modified ver-
sion of the KNEXT system, a knowledge acquisi-
tion framework constructed for large scale genera-
tion of abstracted logical forms through composi-
tional linguistic analysis. The following provides
an overview of KNEXT and its target knowledge
representation, Episodic Logic.
2.1 Episodic Logic
Automatically acquiring general world knowledge
from text is not a task that provides an immedi-
ate solution to any real world problem.
2
Rather,
the motivation for acquiring large stores of back-
ground knowledge is to enable research within
other areas of artificial intelligence, e.g., the con-
struction of systems that can engage in dialogues
about everyday topics in unrestricted English, use
1
Almuhareb and Poesio (2004) treat unary attributes as
values of binary attributes; e.g., illegal might be the value of
a legality attribute. But for many unary attributes, this is a
stretch.
2
Unless one regularly needs reminding of facts such as, A
WOMAN MAY BOIL A GOAT.
921
(Some e0:
[e0 at-about Now0]
[(Many.det x :
[x ((attr athletic.a) (plur youngster.n))]
[x want.v
(Ka
(become.v
(plur
((attr professional.a) athlete.n))))])
** e0])
Figure 1: Example EL formula; square brackets indicate
a sentential infix syntax of form [subject pred object ...], Ka
reifies action predicates, and attr ?raises? adjectival predicates
to predicate modifiers; e0 is the situation characterized by the
sentence.
common sense in answering questions or solv-
ing problems, pursue intrinsic goals independently,
and show awareness of their own characteristics,
biography, and cognitive capacities and limita-
tions. An important challenge in the pursuit of
these long-range goals is the design and implemen-
tation of a knowledge representation that is as ex-
pressively rich as natural language and facilitates
language understanding and commonsense reason-
ing.
Episodic Logic (EL) (Schubert and Hwang,
2000), is a superset of FOL augmented with cer-
tain semantic features common to all human lan-
guages: generalized quantification, intensionality,
uncertainty, modification and reification of predi-
cates and propositions, and event characterization
by sentences. An implementation of EL exists as
the EPILOG system (Schaeffer et al, 1993), which
supports both forward and backward inference,
along with various specialized routines for dealing
with, e.g., color, time, class subsumption, etc. EPI-
LOG is under current development as a platform for
studying a notion of explicit self-awareness as de-
fined by Schubert (2005).
As an indication of EL?s NL-like syntax, figure
1 contains the output of EPILOG?s parser/logical-
form generator for the sentence, Many athletic
youngsters want to become professional athletes.
2.2 KNEXT
If ?deep? language understanding and common-
sense reasoning involve items as complex and
structured as seen in figure 1, then automated
knowledge acquisition cannot simply be a mat-
ter of accumulating rough associations between
word strings, along the lines ?(Youngster) (want
become) (professional athlete)?. Rather, acquired
knowledge needs to conform with a systematic,
highly expressive KR syntax such as EL.
The KNEXT project is aimed at extracting such
structured knowledge from text. One of the major
obstacles is that the bulk of commonsense knowl-
edge on which people rely is not explicitly written
down ? precisely because it is common. Even it
were written down, most of it could not be reliably
interpreted, because reliable interpretation of lan-
guage is itself dependent on commonsense knowl-
edge (among other things).
In view of these difficulties, KNEXT has initially
focused on attempting to abstract world knowl-
edge ?factoids? from texts, based on the logical
forms derived from parsed sentences. The idea is
that nominal pre- and post-modifiers, along with
subject-verb-object relations, captured in logical
forms similar to that in figure 1, give a glimpse
of the common properties and relationships in the
world ? even if the source sentences describe in-
vented situations. For example, the following were
extracted by KNEXT, then automatically verbal-
ized back into English for ease of readability:
? SOME NUMBER OF YOUNGSTERS MAY WANT
TO BECOME ATHLETES.
? YOUNGSTERS CAN BE ATHLETIC.
? ATHLETES CAN BE PROFESSIONAL.
2.3 Attribute Extraction via KNEXT
In order to study the contribution of lists of in-
stances (i.e., generalized gazetteers) to the task of
attribute extraction, the version of KNEXT as pre-
sented by Schubert (2002) was modified to provide
output of a form similar to that of the extraction
work of Pas?ca and Van Durme (2007).
KNEXT?s abstracted, propositional output was
automatically verbalized into English, with any re-
sultant statements of the form, A(N) X MAY HAVE
A(N) Y, taken to suggest that the class X has as an
attribute the property Y.
KNEXT was designed from the beginning to
make use of gazetteers if available, where a
phrase such as Bill Clinton vetoed the bill sup-
ports the (verbalized) proposition A PRESIDENT
MAY VETO A BILL. just as would The president
vetoed the bill. We instrumented the system to
record which propositions did or did not require
gazetteers in their construction, allowing for a nu-
merical breakdown of the respective contributions
of known instances of a class, versus the class label
itself.
922
Pas?ca and Van Durme (2007) described the re-
sults of an informal survey asking participants to
enumerate what they felt to be important attributes
for a small set of example classes. Some of these
resultant attributes were not of the form targeted
by the authors? system. For example, nonprofit
was given as an important potential attribute for
the class Company, as well as legal for the class
Drug. These attributes correspond to unary predi-
cates as compared to the targeted binary predicates
underlying such attributes as cost(X,Y) for the class
Drug.
We extracted such unary attributes by focusing
on verbalizations of the form, A(N) X CAN BE Y
as in AN ANIMAL CAN BE WARM-BLOODED.
3 Experimental Setting
3.1 Corpus Processing
Initial reports on the use of KNEXT were focused
on the processing of manually created parse trees,
on a corpus of limited size (the Brown corpus of
Kucera and Francis (1967)). Since that time the
system has been modified into a fully automatic
extraction system, making use of syntactic parse
trees generated by parsers trained on the Penn
Treebank.
For our studies here, the parser employed was
that of Collins (1997) applied to the sentences
of the British National Corpus (BNC Consortium,
2001). Our choice of the BNC was motivated by
its breadth of genre, its substantial size (100 mil-
lion words) and its familiarity (and accessibility)
to the community.
3.2 Gazetteers
KNEXT?s gazetteers were used as-is, and were
defined based on a variety of sources: miscella-
neous publicly available lists, as well as manual
enumeration. The classes covered can be seen in
the Results section in table 2, where the minimum,
maximum and mean size were 2, 249, and 41, re-
spectively.
3.3 Filtering out Non-predicative Adjectives
Beyond the pre-existing KNEXT framework, ad-
ditional processing was introduced for the extrac-
tion of unary attributes in order to filter out vacu-
ous or unsupported propositions derived from non-
compositional phrases.
This filtering was performed through the cre-
ation of three lists: a whitelist of accepted pred-
icative adjectives; a graylist containing such adjec-
tives that are meaningful as unary predicates only
when applied to plural nouns; and a blacklist de-
rived from Wikipedia topic titles, representing lex-
icalized, non-compositional phrases.
Whitelist The creation of the whitelist began with
calculating part-of-speech (POS) tagged bigram
counts using the Brown corpus. The advantage
of using a POS-tagged bigram model lies in the
saliency of phrase structures, which enabled fre-
quency calculations for both attributive and pred-
icative uses of a given adjective. Attributive counts
were based on instances when an adjective appears
in the pre-nominal position and modifies another
noun. Predicative counts were derived by sum-
ming over occurrences of a given adjective after
all possible copulas. These counts were used to
compute a p/a ratio - the quotient of predicative
count over attributive count - for each word classi-
fied by WordNet (Fellbaum, 1998) as a having an
adjectival use. After manual inspection, two cut-
off points were chosen at ratios of .06 and 0, as
seen in table 1.
Words not appearing in the Brown corpus (i.e.
having 0 count for both uses), were sampled and
inspected, with the decision made to place the
majority within the whitelist, excluding just those
with suffixes including -al, -c, -an, -st, -ion, -th, -o,
-ese, -er, -on, -i, -x, -v, and -ing.
This process resulted in a combined whitelist of
14,249 (usually) predicative adjectives.
p/a ratio (r) Cut-off decision
r ? .06 keep the adjective*
0 < r < .06 remove the adjective*
otherwise keep the adjective*
Table 1: Cut-off decision given the p/a ratio of an
adjective. *Note: except for hand-selected cases.
Graylist We manually constructed a short list (cur-
rently 33 words) containing adjectives that are gen-
erally inappropriate as whitelist entries, but could
be acceptable when applied to plurals. For exam-
ple, the verbalized proposition OBJECTS CAN BE
SIMILAR was deemed acceptable, while a state-
ment such as AN OBJECT CAN BE SIMILAR is
erroneous because of a missing argument.
Blacklist From an exhaustive set of Wikipedia
topic titles was derived a blacklist consisting of en-
tries that had to satisfy four criteria: 1) no more
than three words in length; 2) has no closed-class
923
words, such as prepositions or adverbs; 3) must
begin with an adjective and end with a noun (de-
termined by WordNet); and 4) does not contain
any numerical characters or miscellaneous sym-
bols that are usually not meaningful in English.
Therefore, each title in the resultant list is liter-
ally a short noun phrase with adjectives as pre-
modifiers. It was observed that in these encyclope-
dia titles, the role of adjectives is predominantly to
restrict the scope of the object that is being named
(e.g. CRIMINAL LAW), rather than to describe
its attributions or features (e.g. DARK EYES).
More often than not, only cases similar to the sec-
ond example can be safely verbalized as X CAN
BE Y from a noun phrase Y X, with Y being the
pre-nominal adjective.
We further refined this list by examining trigram
frequencies as reported in the web-derived n-gram
collection of Brants and Franz (2006). For each ti-
tle of the form (Adj N) ..., we gathered trigram fre-
quencies for adverbial modifications such as (very
Adj N) ..., and (truly Adj N) .... Intuitively, high rel-
ative frequency of such modification with respect
to the non-modified bigram supports removal of
the given title from the blacklist.
Trigram counts were collected using the modi-
fiers: absolutely, almost, entirely, highly, nearly,
perfectly, truly and very. These counts were
summed for a given title then divided by the afore-
mentioned bigram score. Upon sampled inspec-
tion, all three-word titles were kept on the black-
list, along with any two-word title with a resultant
ratio less than 0.028. For example, the titles Hardy
Fish, Young Galaxy, and Sad Book were removed,
while Common Cause, Bouncy Ball, and Heavy Oil
were retained.
4 Results
From the parsed BNC, 6,205,877 propositions
were extracted, giving an average of 1.396 propo-
sitions per input sentence.
3
These results were
then used to explore the necessity of gazetteers,
and the potential for extracting unary attributes.
Quality judgements were performed using a 5
point scale as seen in figure 2.
3
These approximately six million verbalized propo-
sitions, along with their underlying logical form and
respective source sentence(s), may be browsed in-
teractively through an online browser available at:
http://www.cs.rochester.edu/u/vandurme/epik
THE STATEMENT ABOVE IS A REASONABLY
CLEAR, ENTIRELY PLAUSIBLE GENERAL
CLAIM AND SEEMS NEITHER TOO SPECIFIC
NOR TOO GENERAL OR VAGUE TO BE USEFUL:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 2: Instructions for scaled judging.
4.1 Necessity of Gazetteers
From the total set of extracted propositions,
638,809 could be verbalized as statements of the
form X MAY HAVE Y. There were 71,531 unique
classes (X) for which at least a single candidate at-
tribute (Y) was extracted, with 9,743 of those hav-
ing at least a single such attribute that was sup-
ported by a minimum of two distinct sentences.
Table 2 gives the number of attributes extracted
for the given classes when using only gazetteers,
when using only the given names as class labels,
and when using both together. While instance-
based extraction generated more unique attributes,
there were still a significant number of results de-
rived based exclusively on class labels. Further,
as can be seen for cases such as Artist, class-
driven extraction provided a large number of at-
tribute candidates not observed when relying only
on gazetteers (701 total candidate attributes were
gathered based on the union of 441 and 303 can-
didates respectively extracted with, and without a
gazetteer for Artist).
We note that this volume measure is potentially
biased against class-driven extraction, as no ef-
fort was made to pick an optimal label for a given
gazetteer, (the original hand-specified class labels
were retained). For example, one might expect the
label Drink to generate more, yet still appropriate,
propositions than Beverage, Actor and/or Actress
as compared to Show Biz Star, or the semantically
similar Book versus Literary Work. This is sug-
gested by the entries in the table based on using
supertypes of the given class, as well as in figure
3, which favorably compares top attributes discov-
ered for select classes against those reported else-
where in the literature.
Table 3 gives the assessed quality for the top ten
attributes extracted for five of the classes in table
2. As can be seen, class-driven extraction can pro-
duce attributes of quality assessed at par with at-
tributes extracted using only gazetteers.
924
BasicFood Religion
K (Food): quality, part, taste, value, portion.. K: basis, influence, name, truths, symbols, principles,
D: species, pounds, cup, kinds, lbs, bowl.. strength, practice, origin, adherent, god, defence..
Q: nutritional value, health benefits, glycemic index, D: teachings, practice, beliefs, religion spread,
varieties, nutrition facts, calories.. principles, emergence, doctrines..
Q: basic beliefs, teachings, holy book, practices, rise,
branches, spread, sects..
HeavenlyBody Painter
K
G
(Planet): surface, orbit, bars, history, atmosphere.. K
G
(Artist) : works, life, career, painting, impression,
K (Planet): surface, history, future, orbit, mass, field.. drawings, paintings, studio, exhibition..
K (Star): surface, mass, field, regions.. K (Artist): works, impression, career, life, studio..
D: observations, spectrum, planet, spectra, conjunction, K (Painter) : works, life, wife, eye..
transit, temple, surface.. Q?: paintings, works, portrait, death, style, artwork,
Q: atmosphere, surface, gravity, diameter, mass, bibliography, bio, autobiography, childhood..
rotation, revolution, moons, radius..
Figure 3: Qualitative comparison of top extracted attributes; K
G
is KNEXT using gazetteers, K (class) is KNEXT for a class
label similar to the heading, D and Q are document- and query-based results as reported in (Pas?ca et al, 2007), Q? is query-based
results reported in (Pas?ca and Van Durme, 2007).
The noticeable drop in quality for the class
Planet when only using gazetteers (3.2 mean
judged acceptability) highlights the recurring
problem of word sense ambiguity in extraction.
The names of Roman deities, such as Mars or Mer-
cury, are used to refer to a number of conceptu-
ally distinct items, such as planets within our so-
lar system. Two of the attributes judged as poor
quality for this class were bars and customers, re-
spectively derived from the noun phrases: (NP
(NNP Mars) (NNS bars)), and (NP (NNP Mer-
cury) (NNS customers)). Note that in both cases
the underlying extraction is correctly performed;
the error comes from abstracting to the wrong
class. These NPs may arguably support the ver-
balized propositions, e.g.: A CANDY-COMPANY
MAY HAVE BARS, and A CAR-COMPANY
MAY HAVE CUSTOMERS.
These examples point to additional areas for
improvement beyond sense disambiguation: non-
compositional phrase filtering for all NPs, rather
than just in the cases of adjectival modification
(Mars bar is a Wikipedia topic); and relative dis-
counting of patterns used in the extraction pro-
cess
4
. This later technique is commonly used in
specialized extraction systems, such as constructed
by Snow et al (2005) who fit a logistic regression
model for hypernym (X is-a Y) classification based
on WordNet, and Girju et al (2003) who trained a
classifier to look specifically for part-whole rela-
tions.
4
For example, (NP (NNP X) (NNS Y)) may be more se-
mantically ambiguous than, e.g., the possessive construction
(NP (NP (NNP X) (POS ?s)) (NP (NNS Y))).
4.2 Unary Attributes
Table 4 shows how filtering non-compositional
phrases from CAN BE propositions affects extrac-
tion volume. Table 5 shows the difference between
such post-filtered propositions and those that were
deleted. As our filter lists were not built fully au-
tomatically, evaluation was performed exclusively
by an author with negligible direct involvement in
the lists? creation (so-as to minimize judgement
bias).
As examples, the top ten unary attributes for
select classes are given in table 6, which the au-
thors believe to be high quality on average, with
some bad entries present. Attributes such as
pre-raphaelite for Painter are considered obscure,
while those such as favourite for Animal are con-
sidered unlikely to be useful as a unary predicate.
The importance of class-driven extraction can be
seen in results such as those given for the class Ap-
ple. Even if it were the case that gazetteer-based
extraction could deliver perfect results for those
classes whose instances occasionally appear ex-
plicitly in text, there are a number of classes for
which such instances are entirely lacking. For ex-
ample, there are many instances of the class Com-
pany which have been individually named and ap-
pear in text with some frequency, e.g., Microsoft,
Walmart, or Boeing. However, despite the many
real-world instantiations of the class Apple, this
does not translate into a list of individually named
members in text.
5
If our goal is to acquire at-
tributes for as many classes as possible, our results
5
Instances of Apple are referred to directly as such; ?an
apple.?
925
Class Both Gaz. Class Lbl.
Continent 777 698 96
Country 7,285 5,993 1,696
US State 1,289 1,286 609*
US City 2,216 2,120 813*
World City 4,780 4,747 813*
Beverage 53 53 0
Tycoon 19 10 10
TV Network 71 71 0
Artist 706 441 303
Medicine 29 2 27
Weekday 1,234 1,232 2
Month 2,282 1,875 474
Dictator 533 509 28
Conqueror 103 84 19
Philosopher 672 649 37
Conductor 118 74 45
Singer 220 179 49
Band 349 58 303
King 811 208 664
Queen 541 17 532
Religious Leader 127 127 0
Adventurer 32 27 5
Planet 289 163 141
Criminal/Outlaw 30 30 6/4*
Service Agency 85 83 2
Architect 72 67 63
Show Biz Star 82 82 0
Film Maker 42 33 9
Composer 722 651 98
Humanitarian 5 5 0
Pope 235 123 113
River 402 168 253
Company 3,968 1,553 2,941
Deity 1,037 1,027 19
Scientist 798 750 60
Religious Holiday 594 593 65*
Civic Holiday 3 3 65*
Military Commander 71 71 26*
Intl Political Entity 673 673 0
Sports Celebrity 45 45 0
Activist Organization 63 63 0
Martial Art 3 3 0
Government Agency 295 294 2
Criminal Organization 0 0 0
US President 596 596 1,421*
Political Leader 568 568 170*
Supreme Court Justice 0 0 18*
Emperor 436 211 259
Fictitious Character 227 227 180*
Literary Work 9 9 0
Engineer/Inventor 10 10 73/13*
Famous Lawyer 0 0 72*
Writer 1,116 957 236
TOTAL 35,723 29,518 8,506
Table 2: Extraction volume with and without using
gazetteers. *Note: When results are zero after gaz. omission,
values are reported for super-types, such as Holiday for the
sub-type Civic Holiday, or City for US City. A/B scores re-
ported for each class used separately, e.g., Engineer/Inventor.
Class Both Gazetteer Class Label
King 1.2 1.9 1.3
Composer 1.5 1.5 2.1
River 1.9 1.9 1.5
Continent 1.5 1.9 2.0
Planet 1.9 3.2 1.6
Table 3: Average judged acceptability for the top ten at-
tributes extracted for the given classes when using/not-using
gazetteer information.
Collection Size
% of
Original CAN BE
Original total 6,204,184 100 -
Filtered total 5,382,282 87 -
Original CAN BE 2,895,325 46 100
Filtered CAN BE 2,073,417 33 72
Whitelist 812,146 15 28
Blacklist 19,786 1< 1
Table 4: Impact of filtering on volume. For example, those
propositions removed because of the whitelist comprised 15%
of the total propositions extracted, or 28% of those specifi-
cally verbalized as X CAN BE Y.
indicate the benefits of exploiting the explicit ap-
pearance of class labels in text.
5 Related Work
Pas?ca and Van Durme (2007) presented an ap-
proach to attribute extraction based on the use
of search engine query logs, a previously unex-
plored source of data within information extrac-
tion. Results confirmed the intuition that a sig-
nificant number of high quality, characteristic at-
tributes for many classes may be derived based
on the relative frequency with which anonymous
users request particular pieces of information for
known instances of a concept class. Pas?ca et al
(2007) compared the quality of shallow attribute
extraction techniques as applied to documents ver-
sus search engine query logs, concluding that such
methods are more applicable to query logs than to
documents. We note that while search queries do
seem ideally suited for extracting class attributes,
existing large-scale collections of query logs are
proprietary and thus unavailable to the general re-
search community. At least until such a resource
becomes available, it is of interest to the commu-
nity that (qualitatively) similar extraction results
may be achieved exclusively using publicly avail-
able document collections.
Alternative approaches to harvesting large-scale
knowledge repositories based on logical forms in-
clude that reported by Suchanek et al (2007).
The authors used non-linguistic information avail-
926
1 10 100 1,000
Filtered 3.18 3.60 2.74 2.76
Blacklist 3.88 4.00 4.08 4.06
Whitelist 3.78 3.76 3.74 3.80
Table 5: Mean evaluated acceptability for 50 unary at-
tributes randomly sampled from each of the given levels of
support (attribute occurred once, less than 10 times, less than
100 times, ...). Filtered refers to the final ?clean? results,
Blacklist and Whitelist refer to propositions deleted due to
the given list.
Painter
famous, romantic, distinguished, celebrated,
well-known, pre-raphaelite, flemish, dutch,
abstract
Animal
dead, trapped, dangerous, unfortunate, intact,
hungry, wounded, tropical, sick, favourite
Drug
dangerous, powerful, addictive, safe, illegal,
experimental, effective, prescribed, harmful,
hallucinatory
Apple
red, juicy, fresh, bad, substantive, stuffed,
shiny, ripe, green, baked
Earthquake
disastrous, violent, underwater, prolonged,
powerful, popular, monstrous, fatal, famous,
epic
Table 6: Top ten unary attributes for select classes, gathered
exclusively without the use of gazetteers.
able via Wikipedia to populate a KB based on
a variant of the logic underlying the Web On-
tology Language (OWL). Results were limited to
14 predefined relation types, such as diedInYear
and politicianOf, with membership of instances
within particular concept classes inferred based on
Wikipedia?s category pages. Authors report 5 mil-
lion so-called ontological facts being extracted.
Almuhareb and Poesio (2004) performed at-
tribute extraction on webtext using simple extrac-
tion patterns (e.g., ?the * of the C [is|was]?, and
?[a|an|the] * C [is|was]?, which respectively match
The color of the rose was red and A red rose was
...), and showed that such attributes could improve
concept clustering. Subsequently they tested an
alternative approach to the same problem using a
dependency parser, extracting syntactic relations
such as (ncmod, rose, red) and (ncsubj, grow, rose)
(Almuhareb and Poesio, ). They concluded that
syntactic information is relatively expensive to de-
rive, and serves primarily to alleviate data spar-
sity problems (by capturing dependencies between
potentially widely separated words) that may no
longer be an issue given the scale of the Web. We
take a different view, first because attribute extrac-
tion is an offline task for which a 60% overhead
cost (reported by the authors) is not a major is-
sue, but more importantly because we regard ap-
proaches that process language compositionally as
ultimately necessary for deeper meaning represen-
tation and language understanding.
Following intuitions similar to those laid out by
Schubert (2002), Banko et al (2007) presented
TextRunner, the latest in a series of ever more so-
phisticated general information extraction systems
(Cafarella et al, 2005; Etzioni et al, 2004). The
authors constructed a non-parser based extractor
for open domain text designed to efficiently pro-
cess web-sized datasets. Results are in the form of
bracketed text sequences that hint at a sentence?s
underlying semantics. For example, (Bletchley
Park) was location of (Station X).
Cimiano et al (2005) performed a limited form
of class-driven extraction in order to induce class
hierarchies via the methods of Formal Concept
Analysis (FCA). For example, a car is both drive-
able and rentable based on its occurrence in object
position of the relevant verbs. A bike shares these
properties with car, as well as having the property
rideable, leading to these classes being near in the
resultant automatically constructed taxonomy. Ex-
periments were performed on limited domains for
which pre-existing ontologies existed for measur-
ing performance (tourism and finance).
Lin (1999) gave a corpus-based method for find-
ing various types of non-compositional phrases,
including the sort discussed in this paper. Identi-
fication was based on mutual information statistics
conditioned on a given syntactic context (such as
our targeted prenominal adjectival modification).
If the mutual information of, e.g., white house,
shows strong differences from that for construc-
tions with similar components, e.g., red house, and
white barn, then the given phrase was determined
to be non-compositional. The use of this method to
supplement that explored here is a matter of cur-
rent investigation. Early results confirm our in-
tuition regarding the correlation between such au-
tomatically discovered non-compositional phrases
and Wikipedia topic titles, where high scoring
phrases not already in our list tend to suggest miss-
Yes
cooking pot, magic flute, runny nose, skimmed milk,
acquired dyslexia, charged particles, earned income
No
causal connectives, golden oldies, ruling junta,
graduated pension, unsung heroes, viral rna
Table 7: Example high-scoring phrases as ranked by Lin?s
metric when applied to KNEXT logical forms, along with
whether there is, at the time of this writing, an associated
Wikipedia entry.
927
A CAR MAY HAVE A ...
back, boot, side, driver, front, roof, seat, end, interior,
owner, door, control, value, bonnet, wheel, window,
engine, headlights..
A CAR CAN BE ...
black, parked, red, white, armoured, nice, hired, bloody,
open, beautiful, wrecked, unmarked, secondhand,
powerful, brand-new, out-of use, damaged, heavy, dark,
competitive, broken-down..
A CAR MAY BE ... IN SOME WAY
parked, stolen, driven, damaged, serviced, stopped,
lost, clamped, overturned, locked, involved in an accident,
found, turned, transported..
Table 8: Top attributes extracted for the class Car, where
MAY BE relational properties (akin to those used by Cimi-
ano et al (2005)) are similarly acquired via verbalization of
abstracted logical forms.
ing entries in the encyclopedia (see table 7). The
ability to perform such ?missing topic discovery?
should be of interest to those within the emerging
community of Wikipedia-focused AI researchers.
6 Conclusion
We have shown that an open knowledge extraction
system can effectively yield class attributes, even
when named instances of the class are unavailable
or scarce (as a final example see table 8). We stud-
ied the quantitative contributions of instances (as
given in KNEXT gazetteers) and explicitly occur-
ring class nominals to the discovery of attributes,
and found both to be important. We paid partic-
ular attention to the acquisition of unary class at-
tributes, for which access to class labels is of par-
ticular importance because of their typical manner
of expression in text.
Acknowledgements The authors are grateful to
Daniel Gildea for contributing a parsed version of
the BNC. This work was supported by NSF grants
IIS-0328849 and IIS-0535105.
References
Almuhareb, Abdulrahman and Massimo Poesio. Finding con-
cept attributes in the web using a parser. In Proceedings
Corpus Linguistics Conference.
Almuhareb, Abdulrahman and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an evaluation.
In Proceedings of EMNLP.
Banko, Michele, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open Infor-
mation Extraction from the Web. In Proceedings of IJCAI.
BNC Consortium. 2001. The British National Corpus, ver-
sion 2 (BNC World). Distributed by Oxford University
Computing Services.
Brants, Thorsten and Alex Franz. 2006. Web 1T 5-gram Ver-
sion 1. Distributed by the Linguistic Data Consortium.
Cafarella, Michael J., Doug Downey, Stephen Soderland, and
Oren Etzioni. 2005. KnowItNow: Fast, Scalable Infor-
mation Extraction from the Web. In Proceedings of HLT-
EMNLP.
Cimiano, Philipp, Andreas Hotho, and Steffen Stabb. 2005.
Learning concept hierarchies from text corpora using for-
mal concept analysis. Journal of Artificial Inteligence Re-
search.
Collins, Michael. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL.
Etzioni, Oren, Michael Cafarella, Doug Downey, Stanley
Kok, AnaMaria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2004. Web-scale
Information Extraction in KnowItAll. In Proceedings of
WWW.
Fellbaum, Christiane. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Girju, R., A. Badulescu, and D. Moldovan. 2003. Learning
semantic constraints for the automatic discovery of part-
whole relations. In Proceedings of HLT-NAACL.
Kucera, H. and W. N. Francis. 1967. Computational Anal-
ysis of Present-Day American English. Brown University
Press, Providence, RI.
Lin, Dekang. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL.
Pas?ca, Marius and Benjamin Van Durme. 2007. What you
seek is what you get: Extraction of class attributes from
query logs. In Proceedings of IJCAI.
Pas?ca, Marius, Benjamin Van Durme, and Nikesh Garera.
2007. The role of documents vs. queries in extracting class
attributes from text. In Proceedings of CIKM.
Schaeffer, S.A., C.H. Hwang, J. de Haan, and L.K. Schubert.
1993. EPILOG, the computational system for episodic
logic: User?s guide. Technical report, Dept. of Comput-
ing Science, Univ. of Alberta, August.
Schubert, Lenhart K. and Chung Hee Hwang. 2000. Episodic
logic meets little red riding hood: A comprehensive, natu-
ral representation for language understanding. In Iwanska,
L. and S.C. Shapiro, editors, Natural Language Processing
and Knowledge Representation: Language for Knowledge
and Knowledge for Language. MIT/AAAI Press.
Schubert, Lenhart K. 2002. Can we derive general world
knowledge from texts? In Proceedings of HLT.
Schubert, Lenhart K. 2005. Some Knowledge Representa-
tion and Reasoning Requirements for Self-awareness. In
Proc. AAAI Spring Symposium on Metacognition in Com-
putation.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym dis-
covery. In Proceedings of NIPS 17.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A core of semantic knowledge unifying
WordNet and Wikipedia. In Proceedings of WWW.
928
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 808?816,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Deriving Generalized Knowledge from Corpora using WordNet
Abstraction
Benjamin Van Durme, Phillip Michalak and Lenhart K. Schubert
Department of Computer Science
University of Rochester
Rochester, NY 14627, USA
Abstract
Existing work in the extraction of com-
monsense knowledge from text has been
primarily restricted to factoids that serve
as statements about what may possibly ob-
tain in the world. We present an ap-
proach to deriving stronger, more general
claims by abstracting over large sets of
factoids. Our goal is to coalesce the ob-
served nominals for a given predicate ar-
gument into a few predominant types, ob-
tained as WordNet synsets. The results can
be construed as generically quantified sen-
tences restricting the semantic type of an
argument position of a predicate.
1 Introduction
Our interest is ultimately in building systems
with commonsense reasoning and language un-
derstanding abilities. As is widely appreciated,
such systems will require large amounts of gen-
eral world knowledge. Large text corpora are
an attractive potential source of such knowledge.
However, current natural language understand-
ing (NLU) methods are not general and reliable
enough to enable broad assimilation, in a formal-
ized representation, of explicitly stated knowledge
in encyclopedias or similar sources. As well, such
sources typically do not cover the most obvious
facts of the world, such as that ice cream may be
delicious and may be coated with chocolate, or
that children may play in parks.
Methods currently exist for extracting simple
?factoids? like those about ice cream and children
just mentioned (see in particular (Schubert, 2002;
Schubert and Tong, 2003)), but these are quite
weak as general claims, and ? being unconditional
? are unsuitable for inference chaining. Consider
however the fact that when something is said, it
is generally said by a person, organization or text
source; this a conditional statement dealing with
the potential agents of saying, and could enable
useful inferences. For example, in the sentence,
?The tires were worn and they said I had to re-
place them?, they might be mistakenly identified
with the tires, without the knowledge that saying
is something done primarily by persons, organiza-
tions or text sources. Similarly, looking into the
future one can imagine telling a household robot,
?The cat needs to drink something?, with the ex-
pectation that the robot will take into account that
if a cat drinks something, it is usually water or
milk (whereas people would often have broader
options).
The work reported here is aimed at deriving
generalizations of the latter sort from large sets of
weaker propositions, by examining the hierarchi-
cal relations among sets of types that occur in the
argument positions of verbal or other predicates.
The generalizations we are aiming at are certainly
not the only kinds derivable from text corpora (as
the extensive literature on finding isa-relations,
partonomic relations, paraphrase relations, etc. at-
tests), but as just indicated they do seem poten-
tially useful. Also, thanks to their grounding in
factoids obtained by open knowledge extraction
from large corpora, the propositions obtained are
very broad in scope, unlike knowledge extracted
in a more targeted way.
In the following we first briefly review the
method developed by Schubert and collaborators
to abstract factoids from text; we then outline our
approach to obtaining strengthened propositions
from such sets of factoids. We report positive re-
sults, while making only limited use of standard
808
corpus statistics, concluding that future endeav-
ors exploring knowledge extraction and WordNet
should go beyond the heuristics employed in re-
cent work.
2 KNEXT
Schubert (2002) presented an approach to ac-
quiring general world knowledge from text
corpora based on parsing sentences and mapping
syntactic forms into logical forms (LFs), then
gleaning simple propositional factoids from these
LFs through abstraction. Logical forms were
based on Episodic Logic (Schubert and Hwang,
2000), a formalism designed to accommodate in
a straightforward way the semantic phenomena
observed in all languages, such as predication,
logical compounding, generalized quantification,
modification and reification of predicates and
propositions, and event reference. An example
from Schubert and Tong (2003) of factoids
obtained from a sentence in the Brown corpus by
their KNEXT system is the following:
Rilly or Glendora had entered her room while
she slept, bringing back her washed clothes.
A NAMED-ENTITY MAY ENTER A ROOM.
A FEMALE-INDIVIDUAL MAY HAVE A ROOM.
A FEMALE-INDIVIDUAL MAY SLEEP.
A FEMALE-INDIVIDUAL MAY HAVE CLOTHES.
CLOTHES CAN BE WASHED.
((:I (:Q DET NAMED-ENTITY) ENTER[V]
(:Q THE ROOM[N]))
(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V]
(:Q DET ROOM[N]))
(:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V])
(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V]
(:Q DET (:F PLUR CLOTHE[N])))
(:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A]))
Here the upper-case sentences are automatically
generated verbalizations of the abstracted LFs
shown beneath them.1
The initial development of KNEXT was based
on the hand-constructed parse trees in the Penn
Treebank version of the Brown corpus, but sub-
sequently Schubert and collaborators refined and
extended the system to work with parse trees ob-
tained with statistical parsers (e.g., that of Collins
(1997) or Charniak (2000)) applied to larger cor-
pora, such as the British National Corpus (BNC),
a 100 million-word, mixed genre collection, along
with Web corpora of comparable size (see work of
Van Durme et al (2008) and Van Durme and Schu-
bert (2008) for details). The BNC yielded over 2
1Keywords like :i, :q, and :f are used to indicate in-
fix predication, unscoped quantification, and function appli-
cation, but these details need not concern us here.
factoids per sentence on average, resulting in a to-
tal collection of several million. Human judging of
the factoids indicates that about 2 out of 3 factoids
are perceived as reasonable claims.
The goal in this work, with respect to the ex-
ample given, would be to derive with the use of a
large collection of KNEXT outputs, a general state-
ment such as If something may sleep, it is probably
either an animal or a person.
3 Resources
3.1 WordNet and Senses
While the community continues to make gains
in the automatic construction of reliable, general
ontologies, the WordNet sense hierarchy (Fell-
baum, 1998) continues to be the resource of
choice for many computational linguists requiring
an ontology-like structure. In the work discussed
here we explore the potential of WordNet as an un-
derlying concept hierarchy on which to base gen-
eralization decisions.
The use of WordNet raises the challenge of
dealing with multiple semantic concepts associ-
ated with the same word, i.e., employing Word-
Net requires word sense disambiguation in order
to associate terms observed in text with concepts
(synsets) within the hierarchy.
In their work on determining selectional prefer-
ences, both Resnik (1997) and Li and Abe (1998)
relied on uniformly distributing observed frequen-
cies for a given word across all its senses, an ap-
proach later followed by Pantel et al (2007).2 Oth-
ers within the knowledge acquisition community
have favored taking the first, most dominant sense
of each word (e.g., see Suchanek et al (2007) and
Pas?ca (2008)).
As will be seen, our algorithm does not select
word senses prior to generalizing them, but rather
as a byproduct of the abstraction process. More-
over, it potentially selects multiple senses of a
word deemed equally appropriate in a given con-
text, and in that sense provides coarse-grained dis-
ambiguation. This also prevents exaggeration of
the contribution of a term to the abstraction, as a
result of being lexicalized in a particularly fine-
grained way.
3.2 Propositional Templates
While the procedure given here is not tied to a
particular formalism in representing semantic con-
2Personal communication
809
text, in our experiments we make use of proposi-
tional templates, based on the verbalizations aris-
ing from KNEXT logical forms. Specifically, a
proposition F with m argument positions gener-
ates m templates, each with one of the arguments
replaced by an empty slot. Hence, the statement,
A MAN MAY GIVE A SPEECH, gives rise to two
templates, A MAN MAY GIVE A , and A MAY
GIVE A SPEECH. Such templates match statements
with identical structure except at the template?s
slots. Thus, the factoid A POLITICIAN MAY GIVE
A SPEECH would match the second template. The
slot-fillers from matching factoids (e.g., MAN and
POLITICIAN form the input lemmas to our abstrac-
tion algorithm described below.
Additional templates are generated by further
weakening predicate argument restrictions. Nouns
in a template that have not been replaced by a free
slot can be replaced with an wild-card, indicating
that anything may fill its position. While slots
accumulate their arguments, these do not, serv-
ing simply as relaxed interpretive constraints on
the original proposition. For the running exam-
ple we would have; A MAY GIVE A ?, and, A ?
MAY GIVE A , yielding observation sets pertain-
ing to things that may give, and things that may be
given.3
We have not restricted our focus to two-
argument verbal predicates; examples such as A
PERSON CAN BE HAPPY WITH A , and, A CAN
BE MAGICAL, can be seen in Section 5.
4 Deriving Types
Our method for type derivation assumes access to
a word sense taxonomy, providing:
W : set of words, potentially multi-token
N : set of nodes, e.g., word senses, or synsets
P : N ? {N ?} : parent function
S : W? (N+) : sense function
L : N ?N?Q?0 : path length function
L is a distance function based on P that gives
the length of the shortest path from a node to a
dominating node, with base case: L(n, n) = 1.
When appropriate, we write L(w, n) to stand for
the arithmetic mean over L(n?, n) for all senses n?
3It is these most general templates that best correlate with
existing work in verb argument preference selection; how-
ever, a given KNEXT logical form may arise from multiple
distinct syntactic constructs.
function SCORE (n ? N , ? ? R+, C ?W ? W) :
C? ? D(n) \ C
return
P
w?C? L(w,n)
|C?|?
function DERIVETYPES (W ? W , m ? N+, p ? (0, 1]) :
?? 1, C ? {}, R? {}
 while too few words covered
while |C| < p? |W | :
n?? argmin
n?N \R
SCORE(n, ?,C)
R?R ? {n?}
C?C ? D(n?)
if |R| > m :
 cardinality bound exceeded ? restart
?? ?+ ?, C ? {}, R? {}
return R
Figure 1: Algorithm for deriving slot type restrictions, with
? representing a fixed step size.
of w that are dominated by n.4 In the definition of
S, (N+) stands for an ordered list of nodes.
We refer to a given predicate argument position
for a specified propositional template simply as a
slot. W ? W will stand for the set of words found
to occupy a given slot (in the corpus employed),
and D : N?W ? is a function mapping a node to
the words it (partially) sense dominates. That is,
for all n ? N and w ? W , if w ? D(n) then
there is at least one sense n? ? S(w) such that n is
an ancestor of n? as determined through use of P .
For example, we would expect the word bank to be
dominated by a node standing for a class such as
company as well as a separate node standing for,
e.g., location.
Based on this model we give a greedy search al-
gorithm in Figure 1 for deriving slot type restric-
tions. The algorithm attempts to find a set of dom-
inating word senses that cover at least one of each
of a majority of the words in the given set of obser-
vations. The idea is to keep the number of nodes in
the dominating set small, while maintaining high
coverage and not abstracting too far upward.
For a given slot we start with a set of observed
words W , an upper bound m on the number of
types allowed in the result R, and a parameter p
setting a lower bound on the fraction of items inW
that a valid solution must dominate. For example,
when m = 3 and p = 0.9, this says we require the
solution to consist of no more than 3 nodes, which
together must dominate at least 90% of W .
The search begins with initializing the cover set
C, and the result set R as empty, with the variable
4E.g., both senses of female in WN are dominated by the
node for (organism, being), but have different path lengths.
810
? set to 1. Observe that at any point in the exe-
cution of DERIVETYPES, C represents the set of
all words from W with at least one sense having
as an ancestor a node in R. While C continues to
be smaller than the percentage required for a so-
lution, nodes are added to R based on whichever
element of N has the smallest score.
The SCORE function first computes the modi-
fied coverage of n, setting C ? to be all words in W
that are dominated by n that haven?t yet been ?spo-
ken for? by a previously selected (and thus lower
scoring) node. SCORE returns the sum of the path
lengths between the elements of the modified set
of dominated nodes and n, divided by that set?s
size, scaled by the exponent ?. Note when ? = 1,
SCORE simply returns the average path length of
the words dominated by n.
If the size of the result grows beyond the speci-
fied threshold,R andC are reset, ? is incremented
by some step size ?, and the search starts again.
As ? grows, the function increasingly favors the
coverage of a node over the summed path length.
Each iteration of DERIVETYPES thus represents a
further relaxation of the desire to have the returned
nodes be as specific as possible. Eventually, ?
will be such that the minimum scoring nodes will
be found high enough in the tree to cover enough
of the observations to satisfy the threshold p, at
which point R is returned.
4.1 Non-reliance on Frequency
As can be observed, our approach makes no use of
the relative or absolute frequencies of the words in
W , even though such frequencies could be added
as, e.g., relative weights on length in SCORE. This
is a purposeful decision motivated both by practi-
cal and theoretical concerns.
Practically, a large portion of the knowledge ob-
served in KNEXT output is infrequently expressed,
and yet many tend to be reasonable claims about
the world (despite their textual rarity). For ex-
ample, a template shown in Section 5, A MAY
WEAR A CRASH HELMET, was supported by just
two sentences in the BNC. However, based on
those two observations we were able to conclude
that usually If something wears a crash helmet, it
is probably a male person.
Initially our project began as an application of
the closely related MDL approach of Li and Abe
(1998), but was hindered by sparse data. We ob-
served that our absolute frequencies were often too
low to perform meaningful comparisons of rela-
tive frequency, and that different examples in de-
velopment tended to call for different trade-offs
between model cost and coverage. This was due
as much to the sometimes idiosyncratic structure
of WordNet as it was to lack of evidence.5
Theoretically, our goal is distinct from related
efforts in acquiring, e.g., verb argument selec-
tional preferences. That work is based on the de-
sire to reproduce distributional statistics underly-
ing the text, and thus relative differences in fre-
quency are the essential characteristic. In this
work we aim for general statements about the real
world, which in order to gather we rely on text as
a limited proxy view. E.g., given 40 hypothetical
sentences supporting A MAN MAY EAT A TACO,
and just 2 sentences supporting A WOMAN MAY
EAT A TACO, we would like to conclude simply
that A PERSON MAY EAT A TACO, remaining ag-
nostic as to relative frequency, as we?ve no reason
to view corpus-derived counts as (strongly) tied to
the likelihood of corresponding situations in the
world; they simply tell us what is generally possi-
ble and worth mentioning.
5 Experiments
5.1 Tuning to WordNet
Our method as described thus far is not tied to a
particular word sense taxonomy. Experiments re-
ported here relied on the following model adjust-
ments in order to make use of WordNet (version
3.0).
The function P was set to return the union of
a synset?s hypernym and instance hypernym rela-
tions.
Regarding the function L , WordNet is con-
structed such that always picking the first sense
of a given nominal tends to be correct more of-
ten than not (see discussion by McCarthy et al
(2004)). To exploit this structural bias, we em-
ployed a modified version of L that results in
a preference for nodes corresponding to the first
sense of words to be covered, especially when the
number of distinct observations were low (such as
earlier, with crash helmet):
L(n, n) =
{
1? 1|W | ?w ?W : S(w) = (n, ...)
1 otherwise
5For the given example, this method (along with the con-
straints of Table 1) led to the overly general type, living thing.
811
word # gloss
abstraction 6 a general concept formed by extracting common features from specific examples
attribute 2 an abstraction belonging to or characteristic of an entity
matter 3 that which has mass and occupies space
physical entity 1 an entity that has physical existence
whole 2 an assemblage of parts that is regarded as a single entity
Table 1: ?word, sense #? pairs in WordNet 3.0 considered overly general for our purposes.
Propositional Template Num.
A CAN BE WHISKERED 4
GOVERNORS MAY HAVE -S 4
A CAN BE PREGNANT 28
A PERSON MAY BUY A 105
A MAY BARK 6
A COMPANY MAY HAVE A 713
A MAY SMOKE 8
A CAN BE TASTY 33
A SONG MAY HAVE A 31
A CAN BE SUCCESSFUL 664
CAN BE AT A ROAD 20
A CAN BE MAGICAL 96
CAN BE FOR A DICTATOR 5
MAY FLOAT 5
GUIDELINES CAN BE FOR -S 4
A MAY WEAR A CRASH HELMET 2
A MAY CRASH 12
Table 2: Development templates, paired with the number of
distinct words observed to appear in the given slot.
Note that when |W | = 1, then L returns 0 for
the term?s first sense, resulting in a score of 0 for
that synset. This will be the unique minimum,
leading DERIVETYPES to act as the first-sense
heuristic when used with single observations.
Parameters were set for our data based on man-
ual experimentation using the templates seen in
Table 2. We found acceptable results when us-
ing a threshold of p = 70%, and a step size of
? = 0.1. The cardinality bound m was set to 4
when |W | > 4, and otherwise m = 2.
In addition, we found it desirable to add a few
hard restrictions on the maximum level of general-
ity. Nodes corresponding to the word sense pairs
given in Table 1 were not allowed as abstraction
candidates, nor their ancestors, implemented by
giving infinite length to any path that crossed one
of these synsets.
5.2 Observations during Development
Our method assumes that if multiple words occur-
ring in the same slot can be subsumed under the
same abstract class, then this information should
be used to bias sense interpretation of these ob-
served words, even when it means not picking the
first sense. In general this bias is crucial to our ap-
proach, and tends to select correct senses of the
words in an argument set W . But an example
where this strategy errs was observed for the tem-
plate A MAY BARK, which yielded the general-
ization that If something barks, then it is proba-
bly a person. This was because there were numer-
ous textual occurrences of various types of people
?barking? (speaking loudly and aggressively), and
so the occurrences of dogs barking, which showed
no type variability, were interpreted as involving
the unusual sense of dog as a slur applied to cer-
tain people.
The template, A CAN BE WHISKERED, had
observations including both face and head. This
prompted experiments in allowing part holonym
relations (e.g., a face is part of a head) as part
of the definition of P , with the final decision be-
ing that such relations lead to less intuitive gen-
eralizations rather than more, and thus these re-
lation types were not included. The remaining
relation types within WordNet were individually
examined via inspection of randomly selected ex-
amples from the hierarchy. As with holonyms we
decided that using any of these additional relation
types would degrade performance.
A shortcoming was noted in WordNet, regard-
ing its ability to represent binary valued attributes,
based on the template, A CAN BE PREGNANT.
While we were able to successfully generalize to
female person, there were a number of words ob-
served which unexpectedly fell outside that asso-
ciated synset. For example, a queen and a duchess
may each be a female aristocrat, a mum may be a
female parent,6 and a fiancee has the exclusive in-
terpretation as being synonymous with the gender
entailing bride-to-be.
6 Experiments
From the entire set of BNC-derived KNEXT
propositional templates, evaluations were per-
formed on a set of 21 manually selected examples,
6Serving as a good example of distributional preferencing,
the primary sense of mum is as a flower.
812
Propositional Template Num.
A MAY HAVE A BROTHER 28
A ? MAY ATTACK A 23
A FISH MAY HAVE A 38
A CAN BE FAMOUS 665
A ? MAY ENTERTAIN A 8
A MAY HAVE A CURRENCY 18
A MALE MAY BUILD A 42
A CAN BE FAST-GROWING 15
A PERSON MAY WRITE A 47
A ? MAY WRITE A 99
A PERSON MAY TRY TO GET A 11
A ? MAY TRY TO GET A 17
A MAY FALL DOWN 5
A PERSON CAN BE HAPPY WITH A 36
A ? MAY OBSERVE A 38
A MESSAGE MAY UNDERGO A 14
A ? MAY WASH A 5
A PERSON MAY PAINT A 8
A MAY FLY TO A ? 9
A ? MAY FLY TO A 4
A CAN BE NERVOUS 131
Table 3: Templates chosen for evaluation.
together representing the sorts of knowledge for
which we are most interested in deriving strength-
ened argument type restrictions. All modification
of the system ceased prior to the selection of these
templates, and the authors had no knowledge of
the underlying words observed for any particular
slot. Further, some of the templates were purpose-
fully chosen as potentially problematic, such as, A
? MAY OBSERVE A , or A PERSON MAY PAINT
A . Without additional context, templates such
as these were expected to allow for exceptionally
broad sorts of arguments.
For these 21 templates, 65 types were derived,
giving an average of 3.1 types per slot, and allow-
ing for statements such as seen in Table 4.
One way in which to measure the quality of an
argument abstraction is to go back to the under-
lying observed words, and evaluate the resultant
sense(s) implied by the chosen abstraction. We say
senses plural, as the majority of KNEXT propo-
sitions select senses that are more coarse-grained
than WordNet synsets. Thus, we wish to evaluate
these more coarse-grained sense disambiguation
results entailed by our type abstractions.7 We per-
formed this evaluation using as comparisons the
first-sense, and all-senses heuristics.
The first-sense heuristic can be thought of as
striving for maximal specificity at the risk of pre-
cluding some admissible senses (reduced recall),
7Allowing for multiple fine-grained senses to be judged
as appropriate in a given context goes back at least to Sussna
(1993); discussed more recently by, e.g., Navigli (2006).
while the all-senses heuristic insists on including
all admissible senses (perfect recall) at the risk of
including inadmissible ones.
Table 5 gives the results of two judges evaluat-
ing 314 ?word, sense? pairs across the 21 selected
templates. These sense pairs correspond to pick-
ing one word at random for each abstracted type
selected for each template slot. Judges were pre-
sented with a sampled word, the originating tem-
plate, and the glosses for each possible word sense
(see Figure 2). Judges did not know ahead of time
the subset of senses selected by the system (as en-
tailed by the derived type abstraction). Taking the
judges? annotations as the gold standard, we report
precision, recall and F-score with a ? of 0.5 (favor-
ing precision over recall, owing to our preference
for reliable knowledge over more).
In all cases our method gives precision results
comparable or superior to the first-sense heuristic,
while at all times giving higher recall. In partic-
ular, for the case of Primary type, corresponding
to the derived type that accounted for the largest
number of observations for the given argument
slot, our method shows strong performance across
the board, suggesting that our derived abstractions
are general enough to pick up multiple acceptable
senses for observed words, but not so general as to
allow unrelated senses.
We designed an additional test of our method?s
performance, aimed at determining whether the
distinction between admissible senses and inad-
missible ones entailed by our type abstractions
were in accord with human judgement. To this
end, we automatically chose for each template
the observed word that had the greatest num-
ber of senses not dominated by a derived type
A MAY HAVE A BROTHER
1 WOMAN : an adult female person (as opposed to a
man); ?the woman kept house while the man hunted?
2 WOMAN : a female person who plays a significant
role (wife or mistress or girlfriend) in the life of a partic-
ular man; ?he was faithful to his woman?
3 WOMAN : a human female employed to do house-
work; ?the char will clean the carpet?; ?I have a woman
who comes in four hours a day while I write?
*4WOMAN : women as a class; ?it?s an insult to Amer-
ican womanhood?; ?woman is the glory of creation?;
?the fair sex gathered on the veranda?
Figure 2: Example of a context and senses provided for
evaluation, with the fourth sense being judged as inappropri-
ate.
813
If something is famous, it is probably a person1, an artifact1, or a communication2
If ? writes something, it is probably a communication2
If a person is happy with something, it is probably a communication2, a work1, a final result1, or a state of affairs1
If a fish has something, it is probably a cognition1, a torso1, an interior2, or a state2
If something is fast growing, it is probably a group1 or a business3
If a message undergoes something, it is probably a message2, a transmission2, a happening1, or a creation1
If a male builds something, it is probably a structure1, a business3, or a group1
Table 4: Examples, both good and bad, of resultant statements able to be made post-derivation. Authors manually selected
one word from each derived synset, with subscripts referring to sense number. Types are given in order of support, and thus the
first are examples of ?Primary? in Table 5.
Method
?
j
?
j Type
Prec Recall F.5 Prec Recall F.5
derived 80.2 39.2 66.4 61.5 47.5 58.1
All
first 81.5 28.5 59.4 63.1 34.7 54.2
all 59.2 100.0 64.5 37.6 100.0 42.9
derived 90.0 50.0 77.6 73.3 71.0 72.8
Primaryfirst 85.7 33.3 65.2 66.7 45.2 60.9
all 69.2 100.0 73.8 39.7 100.0 45.2
Table 5: Precision, Recall and F-score (? = 0.5) for coarse grained WSD labels using the methods: derive from corpus data,
first-sense heuristic and all-sense heuristic. Results are calculated against both the union
S
j and intersection
T
j of manual
judgements, calculated for all derived argument types, as well as Primary derived types exclusively.
THE STATEMENT ABOVE IS A REASONABLY
CLEAR, ENTIRELY PLAUSIBLE GENERAL
CLAIM AND SEEMS NEITHER TOO SPECIFIC
NOR TOO GENERAL OR VAGUE TO BE USEFUL:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 3: Instructions for evaluating KNEXT propositions.
restriction. For each of these alternative (non-
dominated) senses, we selected the ancestor ly-
ing at the same distance towards the root from the
given sense as the average distance from the dom-
inated senses to the derived type restriction. In
the case where going this far from an alternative
sense towards the root would reach a path passing
through the derived type and one of its subsumed
senses, the distance was cut back until this was no
longer the case.
These alternative senses, guaranteed to not be
dominated by derived type restrictions, were then
presented along with the derived type and the
original template to two judges, who were given
the same instructions as used by Van Durme and
Schubert (2008), which can be found in Figure 3.
Results for this evaluation are found in Table 6,
where we see that the automatically derived type
restrictions are strongly favored over alternative
judge 1 judge 2 corr
derived 1.76 2.10 0.60
alternative 3.63 3.54 0.58
Table 6: Average assessed quality for derived and alterna-
tive synsets, paired with Pearson correlation values.
abstracted types that were possible based on the
given word. Achieving even stronger rejection of
alternative types would be difficult, since KNEXT
templates often provide insufficient context for
full disambiguation of all their constituents, and
judges were allowed to base their assessments on
any interpretation of the verbalization that they
could reasonably come up with.
7 Related Work
There is a wealth of existing research focused on
learning probabilistic models for selectional re-
strictions on syntactic arguments. Resnik (1993)
used a measure he referred to as selectional pref-
erence strength, based on the KL-divergence be-
tween the probability of a class and that class
given a predicate, with variants explored by Ribas
(1995). Li and Abe (1998) used a tree cut model
over WordNet, based on the principle of Minimum
Description Length (MDL). McCarthy has per-
formed extensive work in the areas of selectional
814
preference and WSD, e.g., (McCarthy, 1997; Mc-
Carthy, 2001). Calling the generalization problem
a case of engineering in the face of sparse data,
Clark and Weir (2002) looked at a number of pre-
vious methods, one conclusion being that the ap-
proach of Li and Abe appears to over-generalize.
Cao et al (2008) gave a distributional method
for deriving semantic restrictions for FrameNet
frames, with the aim of building an Italian
FrameNet. While our goals are related, their work
can be summarized as taking a pre-existing gold
standard, and extending it via distributional simi-
larity measures based on shallow contexts (in this
case, n-gram contexts up to length 5). We have
presented results on strengthening type restrictions
on arbitrary predicate argument structures derived
directly from text.
In describing ALICE, a system for lifelong
learning, Banko and Etzioni (2007) gave a sum-
mary of a proposition abstraction algorithm devel-
oped independently that is in some ways similar
to DERIVETYPES. Beyond differences in node
scoring and their use of the first sense heuristic,
the approach taken here differs in that it makes no
use of relative term frequency, nor contextual in-
formation outside a particular propositional tem-
plate.8 Further, while we are concerned with gen-
eral knowledge acquired over diverse texts, AL-
ICE was built as an agent meant for construct-
ing domain-specific theories, evaluated on a 2.5-
million-page collection of Web documents per-
taining specifically to nutrition.
Minimizing word sense ambiguity by focus-
ing on a specific domain was later seen in the
work of Liakata and Pulman (2008), who per-
formed hierarchical clustering using output from
their KNEXT-like system first described in (Li-
akata and Pulman, 2002). Terminal nodes of the
resultant structure were used as the basis for in-
ferring semantic type restrictions, reminiscent of
the use of CBC clusters (Pantel and Lin, 2002) by
Pantel et al (2007), for typing the arguments of
paraphrase rules.
Assigning pre-compiled instances to their first-
sense reading in WordNet, Pas?ca (2008) then gen-
eralized class attributes extracted for these terms,
using as a resource Google search engine query
logs.
Katrenko and Adriaans (2008) explored a con-
8Banko and Etzioni abstracted over subsets of pre-
clustered terms, built using corpus-wide distributional fre-
quencies
strained version of the task considered here. Using
manually annotated semantic relation data from
SemEval-2007, pre-tagged with correct argument
senses, the authors chose the least common sub-
sumer for each argument of each relation consid-
ered. Our approach keeps with the intuition of
preferring specific over general concepts in Word-
Net, but allows for the handling of relations au-
tomatically discovered, whose arguments are not
pre-tagged for sense and tend to be more wide-
ranging. We note that the least common sub-
sumer for many of our predicate arguments would
in most cases be far too abstract.
8 Conclusion
As the volume of automatically acquired knowl-
edge grows, it becomes more feasible to abstract
from existential statements to stronger, more gen-
eral claims on what usually obtains in the real
world. Using a method motivated by that used
in deriving selectional preferences for verb argu-
ments, we?ve shown progress in deriving semantic
type restrictions for arbitrary predicate argument
positions, with no prior knowledge of sense in-
formation, and with no training data other than a
handful of examples used to tune a few simple pa-
rameters.
In this work we have made no use of rela-
tive term counts, nor corpus-wide, distributional
frequencies. Despite foregoing these often-used
statistics, our methods outperform abstraction
based on a strict first-sense heuristic, employed in
many related studies.
Future work may include a return to the MDL
approach of Li and Abe (1998), but using a fre-
quency model that ?corrects? for the biases in texts
relative to world knowledge ? for example, cor-
recting for the preponderance of people as sub-
jects of textual assertions, even for verbs like bark,
glow, or fall, which we know to be applicable to
numerous non-human entities.
Acknowledgements Our thanks to Matthew
Post and Mary Swift for their assistance in eval-
uation, and Daniel Gildea for regular advice. This
research was supported in part by NSF grants IIS-
0328849 and IIS-0535105, as well as a University
of Rochester Provost?s Multidisciplinary Award
(2008).
815
References
Michele Banko and Oren Etzioni. 2007. Strategies for Life-
long Knowledge Extraction from the Web. In Proceedings
of K-CAP.
BNC Consortium. 2001. The British National Corpus, ver-
sion 2 (BNC World). Distributed by Oxford University
Computing Services.
Diego De Cao, Danilo Croce, Marco Pennacchiotti, and
Roberto Basili. 2008. Combining Word Sense and Us-
age for Modeling Frame Semantics. In Proceedings of
Semantics in Text Processing (STEP).
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL.
Stephen Clark and David Weir. 2002. Class-based probabil-
ity estimation using a semantic hierarchy. Computational
Linguistics, 28(2).
Michael Collins. 1997. Three Generative, Lexicalised Mod-
els for Statistical Parsing. In Proceedings of ACL.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
Types of Some Generic Relation Arguments: Detection
and Evaluation. In Proceedings of ACL-HLT.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computational
Linguistics, 24(2).
Maria Liakata and Stephen Pulman. 2002. From Trees to
Predicate Argument Structures. In Proceedings of COL-
ING.
Maria Liakata and Stephen Pulman. 2008. Automatic Fine-
Grained Semantic Classification for Domain Adaption. In
Proceedings of Semantics in Text Processing (STEP).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Using automatically acquired predominant
senses for Word Sense Disambiguation. In Proceedings
of Senseval-3: Third International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text.
Diana McCarthy. 1997. Estimation of a probability distribu-
tion over a hierarchical classification. In The Tenth White
House Papers COGS - CSRP 440.
Diana McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Alternations, Subcatego-
rization Frames and Selectional Preferences. Ph.D. the-
sis, University of Sussex.
Roberto Navigli. 2006. Meaningful Clustering of Senses
Helps Boost Word Sense Disambiguation Performance. In
Proceedings of COLING-ACL.
Marius Pas?ca. 2008. Turning Web Text and Search Queries
into Factual Knowledge: Hierarchical Class Attribute Ex-
traction. In Proceedings of AAAI.
Patrick Pantel and Dekang Lin. 2002. Discovering Word
Senses from Text. In Proceedings of KDD.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. ISP: Learning Infer-
ential Selectional Preferences. In Proceedings of NAACL-
HLT.
Philip Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Philip Resnik. 1997. Selectional preference and sense dis-
ambiguation. In Proceedings of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why, What,
and How?
Francesc Ribas. 1995. On learning more appropriate Selec-
tional Restrictions. In Proceedings of EACL.
Lenhart K. Schubert and Chung Hee Hwang. 2000. Episodic
Logic meets Little Red Riding Hood: A comprehensive,
natural representation for language understanding. In
L. Iwanska and S.C. Shapiro, editors, Natural Language
Processing and Knowledge Representation: Language
for Knowledge and Knowledge for Language. MIT/AAAI
Press.
Lenhart K. Schubert and Matthew H. Tong. 2003. Extracting
and evaluating general world knowledge from the brown
corpus. In Proceedings of HLT/NAACL Workshop on Text
Meaning, May 31.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from texts? In Proceedings of HLT.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A Core of Semantic Knowledge Unifying
WordNet and Wikipedia. In Proceedings of WWW.
Michael Sussna. 1993. Word sense disambiguation for free-
text indexing using a massive semantic network. In Pro-
ceedings of CIKM.
Benjamin Van Durme and Lenhart Schubert. 2008. Open
Knowledge Extraction through Compositional Language
Processing. In Proceedings of Semantics in Text Process-
ing (STEP).
Benjamin Van Durme, Ting Qian, and Lenhart Schubert.
2008. Class-driven Attribute Extraction. In Proceedings
of COLING.
816
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Building a Semantic Lexicon of English Nouns via Bootstrapping
Ting Qian1, Benjamin Van Durme2 and Lenhart Schubert2
1Department of Brain and Cognitive Sciences
2Department of Computer Science
University of Rochester
Rochester, NY 14627 USA
ting.qian@rochester.edu, {vandurme, schubert}@cs.rochester.edu
Abstract
We describe the use of a weakly supervised
bootstrapping algorithm in discovering con-
trasting semantic categories from a source lex-
icon with little training data. Our method pri-
marily exploits the patterns in sentential con-
texts where different categories of words may
appear. Experimental results are presented
showing that such automatically categorized
terms tend to agree with human judgements.
1 Introduction
There are important semantic distinctions between
different types of English nouns. For example, some
nouns typically refer to a concrete physical object,
such as book, tree, etc. Others are used to represent
the process or the result of an event (e.g. birth, cele-
bration). Such information is useful in disambiguat-
ing syntactically similar phrases and sentences, so as
to provide more accurate semantic interpretations.
For instance, A MAN WITH HOBBIES and A MAN
WITH APPLES share the same structure, but convey
very different aspects about the man being referred
to (i.e. activities vs possessions).
Compiling such a lexicon by hand, e.g., WordNet
(Fellbaum, 1998), requires tremendous time and ex-
pertise. In addition, when new words appear, these
will have to be analyzed and added manually. Fur-
thermore, a single, global lexicon may contain er-
roneous categorizations when used within a specific
domain/genre; we would like a ?flexible? lexicon,
adaptable to a given corpus. Also, in adapting se-
mantic classifications of words to a particular genre
or domain, we would like to be able to exploit con-
tinuing improvements in methods of extracting se-
mantic occurrence patterns from text.
We present our initial efforts in discovering se-
mantic classes incrementally under a weakly super-
vised bootstrapping process. The approach is able
to selectively learn from its own discoveries, thereby
minimizing the effort needed to provide seed exam-
ples as well as maintaining a reasonable accuracy
rate. In what follows, we first focus on its appli-
cation to an event-noun classification task, and then
use a physical-object vs non-physical-object experi-
ment as a showcase for the algorithm?s generality.
2 Bootstrapping Algorithm
The bootstrapping algorithm discovers words with
semantic properties similar to a small set of labelled
seed examples. These examples can be manually se-
lected from an existing lexicon. By simply changing
the semantic property of the seed set, this algorithm
can be applied to the task of discovering a variety of
semantic classes.
Features Classification is performed using a
perceptron-based model (Rosenblatt, 1958) that ex-
amines features of each word. We use two kinds
of features in our model: morphological (affix and
word length), and contextual. Suffixes, such as -ion,
often reveal the semantic type that a noun belongs
to (e.g., destruction, explosion). Other suffixes like
-er typically suggest non-event nouns (e.g. waiter,
hanger). The set of affixes can be modified to re-
flect meaningful distinctions in the task at hand. Re-
garding word length, longer words tend to have more
37
syllables, and thus are more likely to contain affixes.
For example, if a word ends with -ment, its num-
ber of letters must be ? 5. We defined a partition
of words based on word length: shortest (fewer than
5 letters), short (5-7), medium (8-12), long (13-19),
and longest (> 19).
Besides morphological features, we also make use
of verbalized propositions resulting from the experi-
ments of Van Durme et al (2008) as contextual fea-
tures. These outputs are in the form of world knowl-
edge ?factoids? abstracted from texts, based on log-
ical forms from parsed sentences, produced by the
KNEXT system (see Schubert (2002) for details).
The followings are some sample factoids about the
word destruction, extracted from the British Na-
tional Corpus.
? A PERSON-OR-ORGANIZATION MAY UNDERGO A DE-
STRUCTION
? INDIVIDUAL -S MAY HAVE A DESTRUCTION
? PROPERTY MAY UNDERGO A DESTRUCTION
We take each verbalization (with the target word
removed) as a contextual feature, such as PROPERTY
MAY UNDERGO A . Words from the same seman-
tic category (e.g., event nouns) should have seman-
tic and syntactic similarities on the sentential level.
Thus their contextual features, which reflect the use
of words both semantically and syntactically, should
be similar. For instance, PROPERTY MAY UNDERGO
A PROTECTION is another verbalization produced
by KNEXT, suggesting the word protection may be-
long to the same category as destruction.
A few rough-and-ready heuristics are already em-
ployed by KNEXT to do the same task as we wish
to automate here. A built-in classifier judges nomi-
nals to be event or non-event ones based on analysis
of endings, plus a list of event nouns whose endings
are unrevealing, and a list of non-event nouns whose
endings tend to suggest they are event nouns. As a
result, the factoids used as contextual features in our
work already reflect the built-in classifier?s attempt
to distinguish event nouns from the rest. Thus, the
use of these contextual features may bias the algo-
rithm to perform seemingly well on event-noun clas-
sification. However, we will show that our algorithm
works for classification of other semantic categories,
for which KNEXT does not yet have discriminative
procedures.
Iterative Training We use a bootstrapping pro-
cedure to iteratively train a perceptron-based lin-
ear classifier. A perceptron algorithm determines
whether the active features of a test case are similar
to those learned from given categories of examples.
In an iterative training process, the classifier first
learns from a small seed set, which contains exam-
ples of all categories (in binary classification, both
positive and negative examples) manually selected
to reflect human knowledge of semantic categories.
The classifier then discovers new instances (and cor-
responding features) of each category. Based on
activation values, these newly discovered instances
are selectively admitted into the original training set,
which increases the size of training examples for the
next iteration.
The iterative training algorithm described above
is adopted from Klementiev and Roth (2006). The
advantage of bootstrapping is the ability to auto-
matically learn from new discoveries, which saves
both time and effort required to manually examine
a source lexicon. However, if implemented exactly
as described above, this process has two apparent
disadvantages: New examples may be wrongly clas-
sified by the model; and it is difficult to evaluate the
discriminative models produced in successive itera-
tions, as there are no standard data against which to
judge them (the new examples are by definition pre-
viously unexamined). We propose two measures to
alleviate these problems. First, we admit into the
training set only those instances whose activation
values are higher than the mean activation of their
corresponding categories in an iteration. This sets
a variable threshold that is correlated with the per-
formance of the model at each iteration. Second, we
evaluate iterative results post hoc, using a Bootstrap-
ping Score. This measures the efficacy of bootstrap-
ping (i.e. the ratio of correct newly discovered in-
stances to training examples) and precision (i.e. the
proportion of correct discoveries among all those re-
turned by the algorithm). We compute this score to
decide which iteration has yielded the optimal dis-
criminative model.
3 Building an Event-noun Lexicon
We applied the bootstrapping algorithm to the task
of discovering event nouns from a source lexicon.
38
Event nouns are words that typically describe the
occurrence, the process, or the result of an event.
We first explore the effectiveness of this algorithm,
and then describe a method of extracting the optimal
model. Top-ranked features in the optimal model are
used to find subcategories of event nouns.
Experimental Setup The WordNet noun-list is
chosen as the source lexicon (Fellbaum, 1998),
which consists of 21,512 nouns. The purpose of
this task is to explore the separability of event nouns
from this collection.
typical suffixes: appeasement, arrival, renewal,
construction, robbery, departure, happening
irregular cases: birth, collapse, crash, death, de-
cline, demise, loss, murder
Table 1: Examples of event-nouns in initial training set.
We manually selected 15 event nouns and 215
non-event nouns for the seed set. Event-noun exam-
ples are representative of subcategories within the
semantic class, as well as their commonly seen mor-
phological structures (Table 1). Non-event examples
are primarily exceptions to morphological regulari-
ties (to prevent the algorithm from overly relying on
affix features), such as, anything, ambition, diago-
nal. The subset of all contextual and morphological
features represented by both event and non-event ex-
amples are used to bootstrap the training process.
Event Noun Discovery Reducing the number of
working features is often an effective strategy in
training a perceptron. We experimented with two
cut-off thresholds for features: in Trial 1, features
must appear at least 10 times (55,354 remaining);
in Trial 2, features must appear at least 15 times
(35,457 remaining).
We set the training process to run for 20 iterations
in both trials. Classification results of each iteration
were collected. We expect the algorithm to discover
few event nouns during early iterations. But with
new instances found in each subsequent iteration,
it ought to utilize newly seen features and discover
more. Figure 1 confirms our intuition.
The number of classified event-noun instances in-
creased sharply at the 15th iteration in Trial 1 and the
11th iteration in Trial 2, which may suggest overfit-
ting of the training examples used in those iterations.
If so, this should also correlate with an increase of
error rate in the classification results (error rate de-
fined as the percentage of non-event nouns identi-
fied as event nouns in all discovered event nouns).
We manually marked all misclassified event noun in-
stances for the first 10 iterations in both trials. The
error rate in Trial 2 is expected to significantly in-
crease at the 10th iteration, while Trial 1 should ex-
hibit little increase in error rate within this interval.
This expectation is confirmed in Figure 2.
Extracting the Optimal Model We further pur-
sued the task of finding the iteration that has yielded
the best model. Optimality is judged from two as-
pects: 1) the number of correctly identified event
nouns should be significantly larger than the size of
seed examples; and 2) the accuracy of classification
results should be relatively high so that it takes lit-
tle effort to clean up the result. Once the optimal
model is determined, we analyze its most heavily
weighted features and try to derive finer categories
from them. Furthermore, the optimal model could
be used to discover new instances from other source
lexicons in the future.
We define a measure called the Bootstrapping
Score (BS), serving a similar purpose as an F-score.
BS is computed as in Formula (1).
BS = 2 ?BR ? PrecisionBR + Precision . (1)
Here the Bootstrapping Rate (BR) is computed as:
BR = |NEW ||NEW |+ |SEED| , (2)
where |NEW | is the number of correctly identi-
fied new instances (seed examples excluded), and
|SEED| is the size of seed examples. The rate
of bootstrapping reveals how large the effect of the
bootstrapping process is. Note that BR is different
from the classic measure recall, for which the total
number of relevent documents (i.e. true event nouns
in English) must be known a priori ? again, this
knowledge is what we are discovering. The score
is a post hoc solution; both BR and precision are
computed for analysis after the algorithm has fin-
ished. Combining Formulas (1) and (2), a higher
Bootstrapping Score means better model quality.
Bootstrapping scores of models in the first ten it-
erations are plotted in Figure 3. Model quality in
39
5 10 15 20
02
000
4000
6000
8000
10000
Iteration
Numb
er of E
vent N
ouns D
iscove
red
Trial 1Trial 2
Figure 1: Classification rate
2 4 6 8 100
.05
0.10
0.15
0.20
0.25
0.30
Iteration
Error r
ate
Trial 1Trial 2
Figure 2: Error rate
2 4 6 8 100
.82
0.84
0.86
0.88
0.90
0.92
0.94
Iteration
Boots
trappin
g Scor
e
Trial 1Trial 2
Figure 3: Bootstrapping score
1 . . . 6 . . . 10
incorrect 5 . . . 32 . . . 176
correct 79 . . . 236 . . . 497
error rate 5.9% . . . 11.9% . . . 26.2%
score 87.0% . . . 90.8% . . . 83.8%
Table 2: From iterations 1 to 10, comparison between
instance counts, error rates, and bootstrapping scores as
the measure of model quality.
Trial 2 is better than in Trial 1 on average. In ad-
dition, within Trial 2, Iteration 6 yielded the best
discriminative model with a bootstrapping score of
90.8%. Compared to instance counts and error rate
measures as shown in Table 2, this bootstrapping
score provides a balanced measure of model qual-
ity. The model at the 6th iteration (hereafter, Model
6) can be considered the optimal model generated
during the bootstrapping training process.
Top-ranked Features in the Optimal Model In
order to understand why Model 6 is optimal, we
extracted its top 15 features that activate the event-
noun target in Model 6, as listed in Table 3. Inter-
estingly, top-ranked features are all contextual ones.
In fact, in later models where the ranks of mor-
phological features are boosted, the algorithm per-
formed worse as a result of relying too much on
those context-insensitive features.
Collectively, top-ranked features define the con-
textual patterns of event nouns. We are interested
in finding semantic subcategories within the set of
event nouns (497 nouns, Trial 2) by exploiting these
features individually. For instance, some events typ-
ically happen to people only (e.g. birth, betrayal),
while others usually happen to inanimate objects
(e.g. destruction, removal). Human actions can also
be distinguished by the number of participants, such
as group activities (e.g. election) or individual ac-
tivities (e.g. death). It is thus worth distinguishing
nouns that describe different sorts of events.
Manual Classification We extracted the top 100
contextual features from Model 6 and grouped
them into feature classes. A feature class con-
sists of contextual features sharing similar mean-
ings. For instance, A COUNTRY MAY UNDERGO
and A STATE MAY UNDERGO both belong to the
class social activity. For each feature class, we enu-
merate all words that correspond to its feature in-
stances. Examples are shown in Table 4.
Not all events can be unambiguously classified
into one of the subcategories. However, this is also
not necessary because these categories overlap with
one another. For example, death describes an event
that tends to occur both individually and briefly. In
addition to the six categories listed here, new cate-
gories can be added by creating more feature classes.
Automatic Clustering Representing each noun as
a frequency vector over the top 100 most discrim-
inating contextual features, we employed k-means
clustering and compared the results to our manually
crafted subcategories.
Through trial-and-error, we set k to 12, with the
smallest resulting cluster containing 2 nouns (inter-
pretation and perception), while the biggest result-
ing cluster contained 320 event nouns (that seemed
to share no apparent semantic properties). Other
clusters varied from 5 to 50 words in size, with ex-
amples shown in Table 5.
The advantage of automatic clustering is that the
results may reflect an English speaker?s impression
of word similarity gained through language use. Un-
40
a person-or-organization may undergo a a state may undergo a a can be attempted
a country may undergo a a child may have a a can be for a country
a company may undergo a a project may undergo a authority may undergo a
an explanation can be for a an empire may undergo a a war may undergo a
days may have a a can be abrupt a can be rapid
Table 3: Top 15 features that promote activation of the event-noun target, ranked from most weighted to least.
human events: adoption, arrival, birth, betrayal,
death, development, disappearance, emancipation,
funeral . . .
events of inanimate objects: collapse, construc-
tion, definition, destruction, identification, incep-
tion, movement, recreation, removal . . .
individual activities: birth, death, execution, fu-
neral, promotion . . .
social activities: abolition, evolution, federation,
fragmentation, invasion . . .
lasting events: campaign, development, growth,
trial . . .
brief events: awakening, collapse, death, mention,
onset, resignation, thunderstorm . . .
Table 4: Six subcategories of event nouns.
fortunately, the discovered clusters do not typically
come with the same obvious semantic properties as
were defined in manual classification. In the exam-
ple given above, neither of Cluster 1 and Cluster 3
seems to have a centralized semantic theme. But
Cluster 2 seems to be mostly about human activities.
Comparison with WordNet To compare our re-
sults with WordNet resources, we enumerated all
children of the gloss ?something that happens at a
given place and time?, giving 7655 terms (phrases
excluded). This gave a broader range of event nouns,
such as proper nouns and procedures (e.g. 9/11, CT,
MRI), onomatopoeias (e.g. mew, moo), and words
whose event reading is only secondary (e.g. pic-
ture, politics, teamwork). These types of words tend
to have very different contextual features from what
our algorithm had discovered.
While our method may be limited by the choice of
seed examples, we were able to discover event nouns
not classified under this set by WordNet, suggest-
ing that the discovery mechanism itself is a robust
one. Among them were low-frequency nouns (e.g.
crescendo, demise, names of processes (e.g. absorp-
Cluster 1 (17): cancellation, cessation, closure,
crackle, crash, demise, disappearance, dismissal, dis-
solution, division, introduction, onset, passing, resig-
nation, reversal, termination, transformation
Cluster 2 (32): alienation, backing, betrayal, contem-
plation, election, execution, funeral, hallucination,
imitation, juxtaposition, killing, mention, moulding,
perfection, prosecution, recognition, refusal, removal,
resurrection, semblance, inspection, occupation, pro-
motion, trial . . .
Cluster 3 (7): development, achievement, arrival,
birth, death, loss, survival
Table 5: Examples resulting from automatic clustering.
tion, evolution), and particular cases like thunder-
storm.
4 Extension to Other Semantic Categories
To verify that our bootstrapping algorithm was not
simply relying on KNEXT?s own event classifica-
tion heuristics, we set the algorithm to learn the
distinction between physical and non-physical ob-
jects/entities.
(Non-)Physical-object Nouns 15 physical-object/
entity nouns (e.g. knife, ring, anthropologist) and
34 non-physical ones (e.g. happiness, knowledge)
were given to the model as the initial training set.
At the 9th iteration, the number of discovered physi-
cal objects (which form the minority group between
the two) approaches 2,200 and levels off. We ran-
domly sampled five 20-word groups (a subset of
these words are listed in Table 6) from this entire
set of discovered physical objects, and computed an
average error rate of 4%. Prominent features of the
model at the 9th iteration are shown in Table 7.
5 Related Work
The method of using distributional patterns in a
large text corpus to find semantically related En-
41
heifer, sheriff, collector, hippie, accountant, cape, scab,
pebble, box, dick, calculator, sago, brow, ship, ?john,
superstar, border, rabbit, poker, garter, grinder, million-
aire, ash, herdsman, ?cwm, pug, bra, fulmar, *cam-
paign, stallion, deserter, boot, tear, elbow, cavalry,
novel, cardigan, nutcase, ?bulge, businessman, cop, fig,
musician, spire, butcher, dog, elk, . . .
Table 6: Physical-object nouns randomly sampled from
results; words with an asterisk are misclassified, ones
with a question mark are doubtful.
a male-individual can be a a can be small
a person can be a a can be large
a can be young a can be german
-S*morphological feature a can be british
a can be old a can be good
Table 7: Top-10 features that promote activation of the
physical-object target in the model.
glish nouns first appeared in Hindle (1990). Roark
and Charniak (1998) constructed a semantic lexicon
using co-occurrence statistics of nouns within noun
phrases. More recently, Liakata and Pulman (2008)
induced a hierarchy over nominals using as features
knowledge fragments similar to the sort given by
KNEXT. Our work might be viewed as aiming for
the same goal (a lexico-semantic based partition-
ing over nominals, tied to corpus-based knowledge),
but allowing for an a priori bias regarding preferred
structure.
The idea of bootstrapping lexical semantic prop-
erties goes back at least to Hearst (1998), where the
idea is suggested of using seed examples of a rela-
tion to discover lexico-syntactic extraction patterns
and then using these to discover further examples
of the desired relation. The Basilisk system devel-
oped by Thelen and Riloff (2002) almost paralleled
our effort. However, negative features ? features
that would prevent a word from being classified into
a semantic category ? were not considered in their
model. In addition, in scoring candidate words, their
algorithm only looked at the average relevance of
syntactic patterns. Our perceptron-based algorithm
examines the combinatorial effect of those patterns,
which has yielded results suggesting improved ac-
curacy and bootstrapping efficacy.
Similar to our experiments here using k-means,
Lin and Pantel (2001) gave a clustering algorithm
for iteratively building semantic classes, using as
features argument positions within fragments from
a syntactic dependency parser.
6 Conclusion
We have presented a bootstrapping approach for cre-
ating semantically tagged lexicons. The method can
effectively classify nouns with contrasting semantic
properties, even when the initial training set is a very
small. Further classification is possible with both
manual and automatic methods by utilizing individ-
ual contextual features in the optimal model.
Acknowledgments
This work was supported by NSF grants IIS-
0328849 and IIS-0535105.
References
BNC Consortium. 2001. The British National Corpus,
version 2 (BNC World). Distributed by Oxford Uni-
versity Computing Services.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Marti A. Hearst. 1998. Automated discovery of Word-
Net relations. In (Fellbaum, 1998), pages 131?153.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In ACL.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In ACL.
Maria Liakata and Stephen Pulman. 2008. Auto-
matic Fine-Grained Semantic Classification for Do-
main Adaption. In Proceedings of Semantics in Text
Processing (STEP).
Dekang Lin and Patrick Pantel. 2001. Induction of se-
mantic classes from natural language text. In KDD.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurrence statistics for semi-automatic semantic
lexicon construction. In ACL, pages 1110?1116.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from text? In HLT.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In EMNLP.
Benjamin Van Durme, Ting Qian, and Lenhart Schubert.
2008. Class-driven Attribute Extraction. In COLING.
42
Proceedings of ACL-08: HLT, pages 19?27,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Weakly-Supervised Acquisition of Open-Domain Classes and Class
Attributes from Web Documents and Query Logs
Marius Pas?ca
Google Inc.
Mountain View, California 94043
mars@google.com
Benjamin Van Durme?
University of Rochester
Rochester, New York 14627
vandurme@cs.rochester.edu
Abstract
A new approach to large-scale information
extraction exploits both Web documents and
query logs to acquire thousands of open-
domain classes of instances, along with rel-
evant sets of open-domain class attributes at
precision levels previously obtained only on
small-scale, manually-assembled classes.
1 Introduction
Current methods for large-scale information ex-
traction take advantage of unstructured text avail-
able from either Web documents (Banko et al,
2007; Snow et al, 2006) or, more recently, logs of
Web search queries (Pas?ca, 2007) to acquire use-
ful knowledge with minimal supervision. Given a
manually-specified target attribute (e.g., birth years
for people) and starting from as few as 10 seed facts
such as (e.g., John Lennon, 1941), as many as a
million facts of the same type can be derived from
unstructured text within Web documents (Pas?ca et
al., 2006). Similarly, given a manually-specified tar-
get class (e.g., Drug) with its instances (e.g., Vi-
codin and Xanax) and starting from as few as 5 seed
attributes (e.g., side effects and maximum dose for
Drug), other relevant attributes can be extracted for
the same class from query logs (Pas?ca, 2007). These
and other previous methods require the manual spec-
ification of the input classes of instances before any
knowledge (e.g., facts or attributes) can be acquired
for those classes.
?Contributions made during an internship at Google.
The extraction method introduced in this paper
mines a collection of Web search queries and a col-
lection of Web documents to acquire open-domain
classes in the form of instance sets (e.g., {whales,
seals, dolphins, sea lions,...}) associated with class
labels (e.g., marine animals), as well as large sets
of open-domain attributes for each class (e.g., circu-
latory system, life cycle, evolution, food chain and
scientific name for the class marine animals). In
this light, the contributions of this paper are four-
fold. First, instead of separately addressing the
tasks of collecting unlabeled sets of instances (Lin,
1998), assigning appropriate class labels to a given
set of instances (Pantel and Ravichandran, 2004),
and identifying relevant attributes for a given set of
classes (Pas?ca, 2007), our integrated method from
Section 2 enables the simultaneous extraction of
class instances, associated labels and attributes. Sec-
ond, by exploiting the contents of query logs during
the extraction of labeled classes of instances from
Web documents, we acquire thousands (4,583, to
be exact) of open-domain classes covering a wide
range of topics and domains. The accuracy reported
in Section 3.2 exceeds 80% for both instance sets
and class labels, although the extraction of classes
requires a remarkably small amount of supervision,
in the form of only a few commonly-used Is-A ex-
traction patterns. Third, we conduct the first study in
extracting attributes for thousands of open-domain,
automatically-acquired classes, at precision levels
over 70% at rank 10, and 67% at rank 20 as de-
scribed in Section 3.3. The amount of supervision is
limited to five seed attributes provided for only one
reference class. In comparison, the largest previous
19
Knowledge extracted from documents and queries
amino acids={phenylalanine, l?cysteine, tryptophan, glutamic acid, lysine, thr,
marine animals={whales, seals, dolphins, turtles, sea lions, fishes, penguins, squids,
movies={jay and silent bob strike back, romeo must die, we were soldiers, matrix,
zoonotic diseases={rabies, west nile virus, leptospirosis, brucellosis, lyme disease,
movies: [opening song, cast, characters, actors, film review, movie script,
zoonotic diseases: [scientific name, causative agent, mode of transmission,
Open?domain labeled classes of instances
marine animals: [circulatory system, life cycle, evolution, food chain, eyesight,
Open?domain class attributes
(2)
  ornithine, valine, serine, isoleucine, aspartic acid, aspartate, taurine, histidine,...}
  pacific walrus, aquatic birds, comb jellies, starfish, florida manatees, walruses,...}
  kill bill, thelma and louise, mad max, field of dreams, ice age, star wars,...}
  cat scratch fever, foot and mouth disease, venezuelan equine encephalitis,...}
amino acids: [titration curve, molecular formula, isoelectric point, density,
  extinction coefficient, pi, food sources, molecular weight, pka values,...]
  scientific name, skeleton, digestion, gestation period, reproduction, taxonomy,...]
  symbolism, special effects, soundboards, history, screenplay, director,...]
  life cycle, pathology, meaning, prognosis, incubation period, symptoms,...]
Qu
ery
 lo
gs
W
eb
 d
oc
um
en
ts
(1)
(2)
Figure 1: Overview of weakly-supervised extraction of
class instances, class labels and class attributes from Web
documents and query logs
study in attribute extraction reports results on a set
of 40 manually-assembled classes, and requires five
seed attributes to be provided as input for each class.
Fourth, we introduce the first approach to infor-
mation extraction from a combination of both Web
documents and search query logs, to extract open-
domain knowledge that is expected to be suitable
for later use. In contrast, the textual data sources
used in previous studies in large-scale information
extraction are either Web documents (Mooney and
Bunescu, 2005; Banko et al, 2007) or, recently,
query logs (Pas?ca, 2007), but not both.
2 Extraction from Documents and Queries
2.1 Open-Domain Labeled Classes of Instances
Figure 1 provides an overview of how Web docu-
ments and queries are used together to acquire open-
domain, labeled classes of instances (phase (1) in the
figure); and to acquire attributes that capture quan-
tifiable properties of those classes, by mining query
logs based on the class instances acquired from the
documents, while guiding the extraction based on a
few attributes provided as seed examples (phase (2)).
As described in Figure 2, the algorithm for de-
riving labeled sets of class instances starts with the
acquisition of candidate pairs {ME} of a class la-
bel and an instance, by applying a few extraction
patterns to unstructured text within Web documents
{D}, while guiding the extraction by the contents
of query logs {Q} (Step 1 in Figure 2). This is fol-
Input: set of Is-A extraction patterns {E}
. large repository of search queries {Q}
. large repository of Web docs {D}
. weighting parameters J?[0,1] and K?1..?
Output: set of pairs of a class label and an instance {<C,I>}
Variables: {S} = clusters of distributionally similar phrases
. {V} = vectors of contextual matches of queries in text
. {ME} = set of pairs of a class label and an instance
. {CS} = set of class labels
. {X}, {Y} = sets of queries
Steps:
01. {ME} = Match patterns {E} in docs {D} around {Q}
02. {V} = Match phrases {Q} in docs {D}
03. {S} = Generate clusters of queries based on vectors {V}
04. For each cluster of phrases S in {S}
05. {CS} = ?
06. For each query Q of S
07. Insert labels of Q from {ME} into {CS}
08. For each label CS of {CS}
09. {X} = Find queries of S with the label CS in {ME}
10. {Y} = Find clusters of {S} containing some query
10. with the label CS in {ME}
11. If |{X}| > J?|{S}|
12. If |{Y}| < K
13. For each query X of {X}
14. Insert pair <CS ,X> into output pairs {<C,I>}
15. Return pairs {<C,I>}
Figure 2: Acquisition of labeled sets of class instances
lowed by the generation of unlabeled clusters {S} of
distributionally similar queries, by clustering vectors
of contextual features collected around the occur-
rences of queries {Q} within documents {D} (Steps
2 and 3). Finally, the intermediate data {ME} and
{S} is merged and filtered into smaller, more accu-
rate labeled sets of instances (Steps 4 through 15).
Step 1 in Figure 2 applies lexico-syntactic pat-
terns {E} that aim at extracting Is-A pairs of an in-
stance (e.g., Google) and an associated class label
(e.g., Internet search engines) from text. The two
patterns, which are inspired by (Hearst, 1992) and
have been the de-facto extraction technique in previ-
ous work on extracting conceptual hierarchies from
text (cf. (Ponzetto and Strube, 2007; Snow et al,
2006)), can be summarized as:
?[..] C [such as|including] I [and|,|.]?,
where I is a potential instance (e.g., Venezuelan
equine encephalitis) and C is a potential class label
for the instance (e.g., zoonotic diseases), for exam-
ple in the sentence: ?The expansion of the farms
increased the spread of zoonotic diseases such as
Venezuelan equine encephalitis [..]?.
During matching, all string comparisons are case-
insensitive. In order for a pattern to match a sen-
tence, two conditions must be met. First, the class
20
label C from the sentence must be a non-recursive
noun phrase whose last component is a plural-form
noun (e.g., zoonotic diseases in the above sentence).
Second, the instance I from the sentence must also
occur as a complete query somewhere in the query
logs {Q}, that is, a query containing the instance and
nothing else. This heuristic acknowledges the dif-
ficulty of pinpointing complex entities within doc-
uments (Downey et al, 2007), and embodies the
hypothesis that, if an instance is prominent, Web
search users will eventually ask about it.
In Steps 4 through 14 from Figure 2, each clus-
ter is inspected by scanning all labels attached to
one or more queries from the cluster. For each la-
bel CS , if a) {ME} indicates that a large number
of all queries from the cluster are attached to the la-
bel (as controlled by the parameter J in Step 12);
and b) those queries are a significant portion of all
queries from all clusters attached to the same label
in {ME} (as controlled by the parameter K in Step
13), then the label CS and each query with that la-
bel are stored in the output pairs {<C,I>} (Steps
13 and 14). The parameters J and K can be used
to emphasize precision (higher J and lower K) or
recall (lower J and higher K). The resulting pairs
of an instance and a class label are arranged into
sets of class instances (e.g., {rabies, west nile virus,
leptospirosis,...}), each associated with a class label
(e.g., zoonotic diseases), and returned in Step 15.
2.2 Open-Domain Class Attributes
The labeled classes of instances collected automat-
ically from Web documents are passed as input
to phase (2) from Figure 1, which acquires class
attributes by mining a collection of Web search
queries. The attributes capture properties that are
relevant to the class. The extraction of attributes ex-
ploits the set of class instances rather than the asso-
ciated class label, and consists of four stages:
1) identification of a noisy pool of candidate at-
tributes, as remainders of queries that also contain
one of the class instances. In the case of the class
movies, whose instances include jay and silent bob
strike back and kill bill, the query ?cast jay and
silent bob strike back? produces the candidate at-
tribute cast;
2) construction of internal search-signature vector
representations for each candidate attribute, based
on queries (e.g., ?cast selection for kill bill?) that
contain a candidate attribute (cast) and a class in-
stance (kill bill). These vectors consist of counts
tied to the frequency with which an attribute occurs
with a given ?templatized? query. The latter replaces
specific attributes and instances from the query with
common placeholders, e.g., ?X for Y?;
3) construction of a reference internal search-
signature vector representation for a small set of
seed attributes provided as input. A reference vec-
tor is the normalized sum of the individual vectors
corresponding to the seed attributes;
4) ranking of candidate attributes with respect to
each class (e.g., movies), by computing similarity
scores between their individual vector representa-
tions and the reference vector of the seed attributes.
The result of the four stages is a ranked list of
attributes (e.g., [opening song, cast, characters,...])
for each class (e.g., movies).
In a departure from previous work, the instances
of each input class are automatically generated as
described earlier, rather than manually assembled.
Furthermore, the amount of supervision is limited
to seed attributes being provided for only one of
the classes, whereas (Pas?ca, 2007) requires seed at-
tributes for each class. To this effect, the extrac-
tion includes modifications such that only one ref-
erence vector is constructed internally from the seed
attributes during the third stage, rather one such vec-
tor for each class in (Pas?ca, 2007); and similarity
scores are computed cross-class by comparing vec-
tor representations of individual candidate attributes
against the only reference vector available during the
fourth stage, rather than with respect to the reference
vector of each class in (Pas?ca, 2007).
3 Evaluation
3.1 Textual Data Sources
The acquisition of open-domain knowledge, in the
form of class instances, labels and attributes, re-
lies on unstructured text available within Web doc-
uments maintained by, and search queries submitted
to, the Google search engine.
The collection of queries is a random sample of
fully-anonymized queries in English submitted by
Web users in 2006. The sample contains approx-
imately 50 million unique queries. Each query is
21
Found in Count Pct. Examples
WordNet?
Yes 1931 42.2% baseball players,
(original) endangered species
Yes 2614 57.0% caribbean countries,
(removal) fundamental rights
No 38 0.8% agrochemicals, celebs,
handhelds, mangas
Table 1: Class labels found in WordNet in original form,
or found in WordNet after removal of leading words, or
not found in WordNet at all
accompanied by its frequency of occurrence in the
logs. The document collection consists of approx-
imately 100 million Web documents in English, as
available in a Web repository snapshot from 2006.
The textual portion of the documents is cleaned of
HTML, tokenized, split into sentences and part-of-
speech tagged using the TnT tagger (Brants, 2000).
3.2 Evaluation of Labeled Classes of Instances
Extraction Parameters: The set of instances that
can be potentially acquired by the extraction algo-
rithm described in Section 2.1 is heuristically lim-
ited to the top five million queries with the highest
frequency within the input query logs. In the ex-
tracted data, a class label (e.g., search engines) is
associated with one or more instances (e.g., google).
Similarly, an instance (e.g., google) is associated
with one or more class labels (e.g., search engines
and internet search engines). The values chosen
for the weighting parameters J and K from Sec-
tion 2.1 are 0.01 and 30 respectively. After dis-
carding classes with fewer than 25 instances, the ex-
tracted set of classes consists of 4,583 class labels,
each of them associated with 25 to 7,967 instances,
with an average of 189 instances per class.
Accuracy of Class Labels: Built over many years of
manual construction efforts, lexical gold standards
such as WordNet (Fellbaum, 1998) provide wide-
coverage upper ontologies of the English language.
Built-in morphological normalization routines make
it straightforward to verify whether a class label
(e.g., faculty members) exists as a concept in Word-
Net (e.g., faculty member). When an extracted label
(e.g., central nervous system disorders) is not found
in WordNet, it is looked up again after iteratively re-
moving its leading words (e.g., nervous system dis-
Class Label={Set of Instances} Parent in C?
WordNet
american composers={aaron copland, composers Y
eric ewazen, george gershwin,...}
modern appliances={built-in oven, appliances S
ceramic hob, tumble dryer,...}
area hospitals={carolinas medical hospitals S
center, nyack hospital,...}
multiple languages={chuukese, languages N
ladino, mandarin, us english,...}
Table 2: Correctness judgments for extracted classes
whose class labels are found in WordNet only after re-
moval of their leading words (C=Correctness, Y=correct,
S=subjectively correct, N=incorrect)
orders, system disorders and disorders).
As shown in Table 1, less than half of the 4,583
extracted class labels (e.g., baseball players) are
found in their original forms in WordNet. The ma-
jority of the class labels (2,614 out of 4,583) can be
found in WordNet only after removal of one or more
leading words (e.g., caribbean countries), which
suggests that many of the class labels correspond to
finer-grained, automatically-extracted concepts that
are not available in the manually-built WordNet. To
test whether that is the case, a random sample of
200 class labels, out of the 2,614 labels found to
be potentially-useful specific concepts, are manually
annotated as correct, subjectively correct or incor-
rect, as shown in Table 2. A class label is: correct,
if it captures a relevant concept although it could not
be found in WordNet; subjectively correct, if it is
relevant not in general but only in a particular con-
text, either from a subjective viewpoint (e.g., mod-
ern appliances), or relative to a particular tempo-
ral anchor (e.g., current players), or in connection
to a particular geographical area (e.g., area hospi-
tals); or incorrect, if it does not capture any use-
ful concept (e.g., multiple languages). The manual
analysis of the sample of 200 class labels indicates
that 154 (77%) are relevant concepts and 27 (13.5%)
are subjectively relevant concepts, for a total of 181
(90.5%) relevant concepts, whereas 19 (9.5%) of the
labels are incorrect. It is worth emphasizing the im-
portance of automatically-collected classes judged
as relevant and not present in WordNet: caribbean
countries, computer manufacturers, entertainment
companies, market research firms are arguably very
useful and should probably be considered as part of
22
Class Label Size of Instance Sets Class Label Size of Instance Sets
M (Manual) E (Extracted) M E M?EM M (Manual) E (Extracted) M E M?EM
Actor actors 1500 696 23.73 Movie movies 626 2201 30.83
AircraftModel - 217 - - NationalPark parks 59 296 0
Award awards 200 283 13 NbaTeam nba teams 30 66 86.66
BasicFood foods 155 3484 61.93 Newspaper newspapers 599 879 16.02
CarModel car models 368 48 5.16 Painter painters 1011 823 22.45
CartoonChar cartoon 50 144 36 ProgLanguage programming 101 153 26.73
characters languages
CellPhoneModel cell phones 204 49 0 Religion religions 128 72 11.71
ChemicalElem chemicals 118 487 1.69 River river systems 167 118 15.56
City cities 589 3642 50.08 SearchEngine search engines 25 133 64
Company companies 738 7036 26.01 SkyBody constellations 97 37 1.03
Country countries 197 677 91.37 Skyscraper - 172 - -
Currency currencies 55 128 25.45 SoccerClub football clubs 116 101 22.41
DigitalCamera digital cameras 534 58 0.18 SportEvent sports events 143 73 12.58
Disease diseases 209 3566 65.55 Stadium stadiums 190 92 6.31
Drug drugs 345 1209 44.05 TerroristGroup terrorist groups 74 134 33.78
Empire empires 78 54 6.41 Treaty treaties 202 200 7.42
Flower flowers 59 642 25.42 University universities 501 1127 21.55
Holiday holidays 82 300 48.78 VideoGame video games 450 282 17.33
Hurricane - 74 - - Wine wines 60 270 56.66
Mountain mountains 245 49 7.75 WorldWarBattle battles 127 135 9.44
Total mapped: 37 out of 40 classes - - 26.89
Table 3: Comparison between manually-assembled instance sets of gold-standard classes (M ) and instance sets of
automatically-extracted classes (E). Each gold-standard class (M ) was manually mapped into an extracted class (E),
unless no relevant mapping was found. Ratios ( M?EM ) are shown as percentages
any refinements to hand-built hierarchies, including
any future extensions of WordNet.
Accuracy of Class Instances: The computation of
the precision of the extracted instances (e.g., fifth el-
ement and kill bill for the class label movies) relies
on manual inspection of all instances associated to
a sample of the extracted class labels. Rather than
inspecting a random sample of classes, the evalua-
tion validates the results against a reference set of 40
gold-standard classes that were manually assembled
as part of previous work (Pas?ca, 2007). A class from
the gold standard consists of a manually-created
class label (e.g., AircraftModel) associated with a
manually-assembled, and therefore high-precision,
set of representative instances of the class.
To evaluate the precision of the extracted in-
stances, the manual label of each gold-standard class
(e.g., SearchEngine) is mapped into a class label ex-
tracted from text (e.g., search engines). As shown
in the first two columns of Table 3, the mapping into
extracted class labels succeeds for 37 of the 40 gold-
standard classes. 28 of the 37 mappings involve
linking an abstract class label (e.g., SearchEngine)
with the corresponding plural forms among the ex-
tracted class labels (e.g., search engines). The re-
maining 9 mappings link a manual class label with
either an equivalent extracted class label (e.g., Soc-
cerClub with football clubs), or a strongly-related
class label (e.g., NationalPark with parks). No map-
ping is found for 3 out of the 40 classes, namely Air-
craftModel, Hurricane and Skyscraper, which are
therefore removed from consideration.
The sizes of the instance sets available for each
class in the gold standard are compared in the third
through fifth columns of Table 3. In the table, M
stands for manually-assembled instance sets, and E
for automatically-extracted instance sets. For ex-
ample, the gold-standard class SearchEngine con-
tains 25 manually-collected instances, while the
parallel class label search engines contains 133
automatically-extracted instances. The fifth col-
umn shows the percentage of manually-collected in-
stances (M ) that are also extracted automatically
(E). In the case of the class SearchEngine, 16 of the
25 manually-collected instances are among the 133
automatically-extracted instances of the same class,
23
Label Value Examples of Attributes
vital 1.0 investors: investment strategies
okay 0.5 religious leaders: coat of arms
wrong 0.0 designers: stephanie
Table 4: Labels for assessing attribute correctness
which corresponds to a relative coverage of 64%
of the manually-collected instance set. Some in-
stances may occur within the manually-collected set
but not the automatically-extracted set (e.g., zoom-
info and brainbost for the class SearchEngine) or,
more frequently, vice-versa (e.g., surfwax, blinkx,
entireweb, web wombat, exalead etc.). Overall,
the relative coverage of automatically-extracted in-
stance sets with respect to manually-collected in-
stance sets is 26.89%, as an average over the 37
gold-standard classes. More significantly, the size
advantage of automatically-extracted instance sets
is not the undesirable result of those sets contain-
ing many spurious instances. Indeed, the manual
inspection of the automatically-extracted instances
sets indicates an average accuracy of 79.3% over the
37 gold-standard classes retained in the experiments.
To summarize, the method proposed in this paper ac-
quires open-domain classes from unstructured text
of arbitrary quality, without a-priori restrictions to
specific domains of interest and with virtually no su-
pervision (except for the ubiquitous Is-A extraction
patterns), at accuracy levels of around 90% for class
labels and 80% for class instances.
3.3 Evaluation of Class Attributes
Extraction Parameters: Given a target class spec-
ified as a set of instances and a set of five seed at-
tributes for a class (e.g., {quality, speed, number of
users, market share, reliability} for SearchEngine),
the method described in Section 2.2 extracts ranked
lists of class attributes from the input query logs.
Internally, the ranking uses Jensen-Shannon (Lee,
1999) to compute similarity scores between internal
representations of seed attributes, on one hand, and
each of the candidate attributes, on the other hand.
Evaluation Procedure: To remove any possible
bias towards higher-ranked attributes during the as-
sessment of class attributes, the ranked lists of at-
tributes to be evaluated are sorted alphabetically into
a merged list. Each attribute of the merged list is
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Holiday
manually assembled instances
automatically extracted instances
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Average-Class
manually assembled instances
automatically extracted instances
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Mountain
manually assembled instances
automatically extracted instances
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50
Pr
ec
isi
on
Rank
Class: Average-Class
manually assembled instances
automatically extracted instances
Figure 3: Accuracy of attributes extracted based on man-
ually assembled, gold standard (M ) vs. automatically ex-
tracted (E) instance sets, for a few target classes (left-
most graphs) and as an average over all (37) target classes
(rightmost graphs). Seed attributes are provided as input
for each target class (top graphs), or for only one target
class (bottom graphs)
manually assigned a correctness label within its re-
spective class. An attribute is vital if it must be
present in an ideal list of attributes of the class; okay
if it provides useful but non-essential information;
and wrong if it is incorrect.
To compute the overall precision score over a
ranked list of extracted attributes, the correctness la-
bels are converted to numeric values as shown in Ta-
ble 4. Precision at some rank N in the list is thus
measured as the sum of the assigned values of the
first N candidate attributes, divided by N .
Accuracy of Class Attributes: Figure 3 plots pre-
cision values for ranks 1 through 50 of the lists of
attributes extracted through several runs over the 37
gold-standard classes described in the previous sec-
tion. The runs correspond to different amounts of
supervision, specified through a particular choice in
the number of seed attributes, and in the source of
instances passed as input to the system:
? number of input seed attributes: seed attributes
are provided either for each of the 37 classes, for a
total of 5?37=185 attributes (the graphs at the top of
Figure 3); or only for one class (namely, Country),
24
Class Precision Top Ten Extracted Attributes
# Class Label={Set of Instances} @5 @10 @15 @20
1 accounting systems={flexcube, 0.70 0.70 0.77 0.70 overview, architecture, interview questions, free
myob, oracle financials, downloads, canadian version, passwords, modules,
peachtree accounting, sybiz,...} crystal reports, property management, free trial
2 antimicrobials={azithromycin, 1.00 1.00 0.93 0.95 chemical formula, chemical structure, history,
chloramphenicol, fusidic acid, invention, inventor, definition, mechanism of
quinolones, sulfa drugs,...} action, side-effects, uses, shelf life
5 civilizations={ancient greece, 1.00 1.00 0.93 0.90 social pyramid, climate, geography, flag,
chaldeans, etruscans, inca population, social structure, natural resources,
indians, roman republic,...} family life, god, goddesses
9 farm animals={angora goats, 1.00 0.80 0.83 0.80 digestive system, evolution, domestication,
burros, cattle, cows, donkeys, gestation period, scientific name, adaptations,
draft horses, mule, oxen,...} coloring pages, p**, body parts, selective breeding
10 forages={alsike clover, rye grass, 0.90 0.95 0.73 0.57 types, picture, weed control, planting, uses,
tall fescue, sericea lespedeza,...} information, herbicide, germination, care, fertilizer
Average-Class (25 classes) 0.75 0.70 0.68 0.67
Table 5: Precision of attributes extracted for a sample of 25 classes. Seed attributes are provided for only one class.
for a total of 5 attributes over all classes (the graphs
at the bottom of Figure 3);
? source of input instance sets: the instance sets
for each class are either manually collected (M from
Table 3), or automatically extracted (E from Ta-
ble 3). The choices correspond to the two curves
plotted in each graph in Figure 3.
The graphs in Figure 3 show the precision over
individual target classes (leftmost graphs), and as an
average over all 37 classes (rightmost graphs). As
expected, the precision of the extracted attributes as
an average over all classes is best when the input in-
stance sets are hand-picked (M ), as opposed to au-
tomatically extracted (E). However, the loss of pre-
cision from M to E is small at all measured ranks.
Table 5 offers an alternative view on the quality
of the attributes extracted for a random sample of
25 classes out of the larger set of 4,583 classes ac-
quired from text. The 25 classes are passed as in-
put for attribute extraction without modifications. In
particular, the instance sets are not manually post-
filtered or otherwise changed in any way. To keep
the time required to judge the correctness of all ex-
tracted attributes within reasonable limits, the eval-
uation considers only the top 20 (rather than 50) at-
tributes extracted per class. As shown in Table 5, the
method proposed in this paper acquires attributes for
automatically-extracted, open-domain classes, with-
out a-priori restrictions to specific domains of inter-
est and relying on only five seed attributes specified
for only one class, at accuracy levels reaching 70%
at rank 10, and 67% at rank 20.
4 Related Work
4.1 Acquisition of Classes of Instances
Although some researchers focus on re-organizing
or extending classes of instances already available
explicitly within manually-built resources such as
Wikipedia (Ponzetto and Strube, 2007) or Word-
Net (Snow et al, 2006) or both (Suchanek et al,
2007), a large body of previous work focuses on
compiling sets of instances, not necessarily labeled,
from unstructured text. The extraction proceeds
either iteratively by starting from a few seed ex-
traction rules (Collins and Singer, 1999), or by
mining named entities from comparable news arti-
cles (Shinyama and Sekine, 2004) or from multilin-
gual corpora (Klementiev and Roth, 2006).
A bootstrapping method (Riloff and Jones, 1999)
cautiously grows very small seed sets of five in-
stances of the same class, to fewer than 300 items
after 50 consecutive iterations, with a final preci-
sion varying between 46% and 76% depending on
the type of semantic lexicon. Experimental results
from (Feldman and Rosenfeld, 2006) indicate that
named entity recognizers can boost the performance
of weakly supervised extraction of class instances,
but only for a few coarse-grained types such as Per-
son and only if they are simpler to recognize in
text (Feldman and Rosenfeld, 2006).
25
In (Cafarella et al, 2005), handcrafted extraction
patterns are applied to a collection of 60 million Web
documents to extract instances of the classes Com-
pany and Country. Based on the manual evaluation
of samples of extracted instances, an estimated num-
ber of 1,116 instances of Company are extracted at
a precision score of 90%. In comparison, the ap-
proach of this paper pursues a more aggressive goal,
by extracting a larger and more diverse number of
labeled classes, whose instances are often more dif-
ficult to extract than country names and most com-
pany names, at precision scores of almost 80%.
The task of extracting relevant labels to describe
sets of documents, rather than sets of instances, is
explored in (Treeratpituk and Callan, 2006). Given
pre-existing sets of instances, (Pantel and Ravichan-
dran, 2004) investigates the task of acquiring appro-
priate class labels to the sets from unstructured text.
Various class labels are assigned to a total of 1,432
sets of instances. The accuracy of the class labels
is computed over a sample of instances, by manu-
ally assessing the correctness of the top five labels
returned by the system for each instance. The result-
ing mean reciprocal rank of 77% gives partial credit
to labels of an evaluated instance, even if only the
fourth or fifth assigned labels are correct. Our eval-
uation of the accuracy of class labels is stricter, as it
considers only one class label of a given instance at a
time, rather than a pool of the best candidate labels.
As a pre-requisite to extracting relations among
pairs of classes, the method described in (Davidov et
al., 2007) extracts class instances from unstructured
Web documents, by submitting pairs of instances as
queries and analyzing the contents of the top 1,000
documents returned by a Web search engine. For
each target class, a small set of instances must be
provided manually as seeds. As such, the method
can be applied to the task of extracting a large set of
open-domain classes only after manually enumerat-
ing through the entire set of target classes, and pro-
viding seed instances for each. Furthermore, no at-
tempt is made to extract relevant class labels for the
sets of instances. Comparatively, the open-domain
classes extracted in our paper have an explicit la-
bel in addition to the sets of instances, and do not
require identifying the range of the target classes
in advance, or providing any seed instances as in-
put. The evaluation methodology is also quite dif-
ferent, as the instance sets acquired based on the in-
put seed instances in (Davidov et al, 2007) are only
evaluated for three hand-picked classes, with preci-
sion scores of 90% for names of countries, 87% for
fish species and 68% for instances of constellations.
Our evaluation of the accuracy of class instances is
again stricter, since the evaluation sample is larger,
and includes more varied classes, whose instances
are sometimes more difficult to identify in text.
4.2 Acquisition of Class Attributes
Previous work on the automatic acquisition of at-
tributes for open-domain classes from text is less
general than the extraction method and experiments
presented in our paper. Indeed, previous evalua-
tions were restricted to small sets of classes (forty
classes in (Pas?ca, 2007)), whereas our evaluations
also consider a random, more diverse sample of
open-domain classes. More importantly, by drop-
ping the requirement of manually providing a small
set of seed attributes for each target class, and rely-
ing on only a few seed attributes specified for one
reference class, we harvest class attributes without
the need of first determining what the classes should
be, what instances they should contain, and from
which resources the instances should be collected.
5 Conclusion
In a departure from previous approaches to large-
scale information extraction from unstructured text
on the Web, this paper introduces a weakly-
supervised extraction framework for mining useful
knowledge from a combination of both documents
and search query logs. In evaluations over labeled
classes of instances extracted without a-priori re-
strictions to specific domains of interest and with
very little supervision, the accuracy exceeds 90%
for class labels, approaches 80% for class instances,
and exceeds 70% (at rank 10) and 67% (at rank 20)
for class attributes. Current work aims at expanding
the number of instances within each class while re-
taining similar precision levels; extracting attributes
with more consistent precision scores across classes
from different domains; and introducing confidence
scores in attribute extraction, allowing for the detec-
tion of classes for which it is unlikely to extract large
numbers of useful attributes from text.
26
References
M. Banko, Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information ex-
traction from the Web. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-07), pages 2670?2676, Hyderabad, India.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), pages 224?231,
Seattle, Washington.
M. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. KnowItNow: Fast, scalable information extrac-
tion from the Web. In Proceedings of the Human
Language Technology Conference (HLT-EMNLP-05),
pages 563?570, Vancouver, Canada.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceed-
ings of the 1999 Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-99), pages 189?196, College
Park, Maryland.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by Web mining. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL-07), pages 232?239, Prague, Czech
Republic.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locat-
ing complex named entities in Web text. In Proceed-
ings of the 20th International Joint Conference on Ar-
tificial Intelligence (IJCAI-07), pages 2733?2739, Hy-
derabad, India.
R. Feldman and B. Rosenfeld. 2006. Boosting unsu-
pervised relation extraction by using NER. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-ACL-
06), pages 473?481, Sydney, Australia.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
A. Klementiev and D. Roth. 2006. Weakly super-
vised named entity transliteration and discovery from
multilingual comparable corpora. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING-ACL-
06), pages 817?824, Sydney, Australia.
L. Lee. 1999. Measures of distributional similarity. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL-99), pages
25?32, College Park, Maryland.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th International
Conference on Computational Linguistics and the 36th
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL-98), pages 768?774, Mon-
treal, Quebec.
R. Mooney and R. Bunescu. 2005. Mining knowledge
from text using information extraction. SIGKDD Ex-
plorations, 7(1):3?10.
M. Pas? ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the World Wide Web
of facts - step one: the one-million fact extraction chal-
lenge. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence (AAAI-06), pages 1400?
1405, Boston, Massachusetts.
M. Pas? ca. 2007. Organizing and searching the World
Wide Web of facts - step two: Harnessing the wisdom
of the crowds. In Proceedings of the 16th World Wide
Web Conference (WWW-07), pages 101?110, Banff,
Canada.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In Proceedings of the
2004 Human Language Technology Conference (HLT-
NAACL-04), pages 321?328, Boston, Massachusetts.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings of the 22nd
National Conference on Artificial Intelligence (AAAI-
07), pages 1440?1447, Vancouver, British Columbia.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474?479, Or-
lando, Florida.
Y. Shinyama and S. Sekine. 2004. Named entity dis-
covery using comparable news articles. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING-04), pages 848?853,
Geneva, Switzerland.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801?808, Sydney, Australia.
F. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of the 16th World Wide
Web Conference (WWW-07), pages 697?706, Banff,
Canada.
P. Treeratpituk and J. Callan. 2006. Automatically la-
beling hierarchical clusters. In Proceedings of the 7th
Annual Conference on Digital Government Research
(DGO-06), pages 167?176, San Diego, California.
27
Proceedings of ACL-08: HLT, pages 994?1002,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mining Parenthetical Translations from the Web by Word Alignment   Dekang Lin Shaojun Zhao? Benjamin Van Durme? Marius Pa?ca Google, Inc. University of Rochester University of Rochester Google, Inc. Mountain View Rochester Rochester Mountain View CA, 94043 NY, 14627 NY, 14627 CA, 94043 lindek@google.com zhao@cs.rochester.edu vandurme@cs.rochester.edu mars@google.com   Abstract 
Documents in languages such as Chinese, Japanese and Korean sometimes annotate terms with their translations in English inside a pair of parentheses. We present a method to extract such translations from a large collec-tion of web documents by building a partially parallel corpus and use a word alignment al-gorithm to identify the terms being translated. The method is able to generalize across the translations for different terms and can relia-bly extract translations that occurred only once in the entire web. Our experiment on Chinese web pages produced more than 26 million pairs of translations, which is over two orders of magnitude more than previous re-sults. We show that the addition of the ex-tracted translation pairs as training data provides significant increase in the BLEU score for a statistical machine translation sys-tem.  
1 Introduction In natural language documents, a term (word or phrase) is sometimes followed by its translation in another language in a pair of parentheses. We call these parenthetical translations. The following examples are from Chinese web pages  (we added underlines to indicate what is being translated): (1) ???????????Brookings Institution??? ??????????????????????????Jeremy Shapiro?????...  (2) ????????????????indigestion?????gastritis????????????. (3) ???????????not going to fly?????? (4) ?????????????(linear programming).                                                              ?Contributions made during an internship at Google 
The parenthetically translated terms are typically new words, technical terminologies, idioms, prod-ucts, titles of movies, books, songs, and names of persons, organizations locations, etc. Commonly, an author might use such a parenthetical when a given term has no standard translation (or translit-eration), and does not appear in conventional dic-tionaries.  That is, an author might expect a term to be an out-of-vocabulary item for the target reader, and thus helpfully provides a reference translation in situ. For example, in (1), the name Shapiro was transliterated as ???. The name has many other transliterations in web documents, such as ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ???, ??? ..., where the three Chinese characters corresponds to the three sylla-bles in Sha-pi-ro respectively. Each syllable may be mapped into different characters: 'Sha' into ? or ?, 'pi' into ?, ?, ?, and 'ro' into ?, ?, ?, .... Variation is not limited to the effects of phonetic similarity.  Story titles, for instance, are commonly translated semantically, often leading to a number of translations that have similar meaning, yet differ greatly in lexicographic form. For example, while the movie title Syriana is sometimes phonetically transliterated as ???, ???, it may also be trans-lated semantically according to the plot of the movie, e.g., ??? (mystery in mystery), ?? (real log), ???  (spy against spy), ????  (oil-triggered secret war), ??? (Syria), ?? (mystery journey), ...  The parenthetical translations are extremely valuable both as a stand-alone on-line dictionary and as training data for statistical machine transla-tion systems. They provide fresh data (new words) and cover a much wider range of topics than typi-cal parallel training data for statistical machine translation systems. 
994
The main contribution of this paper is a method for mining parenthetical translations by treating text snippets containing candidate pairs as a par-tially parallel corpus and using a word alignment algorithm to establish the correspondences be-tween in-parenthesis and pre-parenthesis words.  This technique allows us to identify translation pairs even if they only appeared once on the entire web. As a result, we were able to obtain 26.7 mil-lion Chinese-English translation pairs from web documents in Chinese. This is over two orders of magnitude more than the number of extracted translation pairs in the previously reported results (Cao, et al 2007). The next section presents an overview of our al-gorithm, which is then detailed in Sections 3 and 4. We evaluate our results in Section 5 by comparison with bilingually linked Wikipedia titles and by us-ing the extracted pairs as additional training data in a statistical machine translation system. 2 Mining Parenthetical Translations A parenthetical translation matches the pattern:  (4)             f1f2?fm (e1e2?en) which is a sequence of m non-English words fol-lowed by a sequence of n English words in paren-theses. In the remainder of the paper, we assume the non-English text is Chinese, but our technique works for other languages as well.  There have been two approaches to finding such parenthetical translations. One is to assume that the English term e1e2?en is given and use a search en-gine to retrieve text snippets containing e1e2?en from predominately non-English web pages (Na-gata et al 2001, Kwok et al 2005). Another method (Cao et al 2007) is to go through a non-English corpus and collect all instances that match the parenthetical pattern in (4). We followed the second approach since it does not require a prede-fined list of English terms and is amendable for extraction at large scale. In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text. The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al 2005). For example, Table 1 lists a set of Chinese segments (with word-to-word translation underneath) that 
precede the English term Lower Egypt. Owing to the frequency with which ??? appears as a can-didate, and in varying contexts, one has a good reason to believe???is the correct translation of Lower Egypt. ?   ??   ??  ?   ?  ?? downstream  region is  down Egypt ?   ??            ??       ?     ?? center located-at  down Egypt ?   ??   ??     ?       ?     ?? and  so-called of  down Egypt ?   ??       ?     ?? called down Egypt Table 1: Chinese text preceding Lower Egypt Unfortunately, this heuristic does not hold as of-ten as one might imagine.  Consider the candidates for Channel Spacing in Table 2.  The suffix?? (gap) has the highest frequency count. It is none-theless an incomplete translation of Channel Spac-ing. The correct translations in rows c to h occurred with Channel Spacing only once. a ?     ?   ??    ?? ?  is   channel distance b ?  ?        ??          ?? its   channel distance c ?    ??                  ??                    ??          ?? in-addition-to  reducing  wave-passage  distance d ?  ?     ??      ?                 ??   ?? also showed have wave-passage  gap e ?      ?          ?      ?      ??  ?? also  therefore is   channel   gap f ? ?       ??      ?  ?? and  channel   ?s    gap g ?  ??      ??      ??      ?                 ??   ?? an   important property is signal-passage  gap h ?   ??       ??    ??     ??  ?? already  able   reach passage  gap Table 2: Text preceding Channel Spacing The crucial observation we make here is that al-though the words like ?? (in row g) co-occurred with Channel Spacing only once, there are many co-occurrences of ??and Channel in other candi-date pairs, such as: ? ? ? ? ?? ?? (Speech Channel) ? ? ?? ?? ?? (Block Flat Fading Channel) ? ?? B (Channel B) ? ?? ?? ?? (Fiber Channel Probes) 
995
? ?? ?? (Reverse Channel) ? ?? ?? ?? ?? (Reverse Channel) Unlike previous approaches that rely solely on the preceding text of a single English term to de-termine its translation, we treat the entire collection of candidate pairs as a partially parallel corpus and establish the correspondences between the words using a word alignment algorithm.  At first glance, word alignment appears to be a more difficult problem than the extraction of par-enthetical translations. Extraction of parenthetical translations need only determine the first pre-parenthesis word aligned with an in-parenthesis word, whereas word alignment requires the respec-tive linking of all such (pre,in)-parenthesis word pairs. However, by casting the problem as word alignment, we are able to generalize across in-stances involving different in-parenthesis terms, giving us a larger number of, and more varied, ex-ample contexts per word. For the examples in Table 2, the words??(channel), ?? (wave passage), ?? (signal pas-sage), and ?? (passage) are aligned with Channel, and the words??(distance) and ??  (gap) are aligned with Spacing. Given these alignments, the left boundary of the translated Chinese term is simply the leftmost word that is linked to one of the English words.  Our algorithm consists of two steps: Step 1 constructs a partially parallel corpus. This step takes as input a large collection of Chinese web pages and converts the sentences with pa-rentheses containing English text into pairs of candidates. Step 2 uses an unsupervised algorithm to align English and Chinese and identify the term being translated according to the left-most aligned Chinese word. If no word alignments can be es-tablished, the pair is not considered a translation. The next two sections present the details of each of the two steps. 3 Constructing a Partially Parallel Corpus 
3.1 Filtering out non-translations The first step of our algorithm is to extract paren-theticals and then filter out those that are not trans-lations. This filtering is required as parenthetical translations represent only a small fraction of the 
usages for parentheses (see Sec. 5.1).  Table 3 shows some example of parentheses that are not translations. The input to Step 1 is a collection of arbitrary web documents. We used the following criteria to identify candidate pairs: ? The pre-parenthesis text (Tp) is predominantly in Chinese and the in-parenthesis text (Ti) is pre-dominantly in English. ? The concatenation of the digits in Tp must be identical to the concatenation of the digits in Ti. For example, rows a, b and c in Table 3 can be ruled out this way. ? If Tp contains some text in English, the same text must also appear in Ti. This filters out row d. ? Remove the pairs where Ti is part of anchor text. This rule is often applied to instances like row e where the file type tends to be inside a clickable link to a media file. ? The punctuation characters in Tp must also ap-pear in Ti, unless they are quotation marks. The example in row f  is ruled out because ?/? is not found in the pre-parenthesis text.  Examples with translations in italic Function of the in-parenthesis text a ??????1.4~3.0?? (MacArthur, 1967) The range of its values is within 1.4~3.0 (MacArthur, 1967)  
to provide citation 
b ????/??? (VN901 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30)  
flight information 
c ??????255-8FT? sale of pool table (255-8FT)  product Id.  d // ??? // void main  ( void  ) // main program // void main  (void )  
function declaration 
e ????: ??? (DVD) movie title: Thousand Year Lake (DVD) 
DVD is the file type 
f ?? ? ?? ? ?? ( g/L) mass consumed by water sample (g/L) 
measurement unit 
g ?????? (Sensitive) gentle protective facial cream (Sensitive)  
to indicate the type of the cream  
h ????????????? (Ask Jeeves) Evaluation of Nine Main Search Engines in the US: Chapter 4 (Ask Jeeves) 
Chapter 4 is about Ask Jeeves 
Table 3: Other uses of parentheses 
996
The instances in rows g and h cannot be eliminated by these simple rules, and are filtered only later, as we fail to discover a convincing word alignment. 3.2  Constraining term boundaries Similar to (Cao et al 2007), we segmented the pre-parenthesis Chinese text and restrict the term boundary to be one of the segmentation bounda-ries. Since parenthetical translations are mostly translation of terms, it makes sense to further con-strain the left boundary of the Chinese side to be a term boundary. Determining what should be counted as a term is a difficult task and there are not yet well-accepted solutions (Sag et al 2003).  We compiled an approximate term vocabulary by taking the top 5 million most frequent Chinese queries as according to a fully anonymized collec-tion of search engine query logs. Given a Chinese sentence, we first identify all (possibly overlapping) sequences of words in the sentence that match one of the top-5M queries. A matching sequence is called a maximal match if it is not properly contained in another matching se-quence. We then define the potential boundary positions to be the boundaries of maximal matches or words that are not covered by any of the top-5M queries.  3.3 Length-based trimming If there are numerous Chinese words preceding a pair of parentheses containing two English words, it is very unlikely for all but the right-most few Chinese words to be part of the translation of the English words. Including extremely long se-quences as potential candidates introduces signifi-cantly more noise and makes word alignment harder than necessary. We therefore trimmed the pre-parenthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C ? 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 
proposed (Brown et al 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in de-scending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of align-ment algorithms and found Competitive Linking to have one of the highest precision scores. A disad-vantage of Competitive Linking, however, is that the alignments are restricted word-to-word align-ments, which implies that multi-word expressions can only be partially linked at best.  4.1 Dealing with multi-word alignment We made a small change to Competitive Linking to allow consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose ei is unlinked and fj is linked to ek) none of the words between ei and ek be linked to any word other than fj.  4.2 Link scoring We used ?2 (Gale and Church, 1991) as the link score in the modified competitive linking algo-rithm, although there are many other possible choices for the link scores, such as ?2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al 2005). The ?2 statistics for a pair of words ei and fj is computed as 
( )
( )( )( )( )dcdbcaba
bcad
++++
?
=
2
2?  where a is the number of sentence pairs containing both ei and fj; a+b is the number of sentence pairs containing ei; a+c is the number of sentence pairs containing  fj; d is the number of sentence pairs containing nei-ther ei nor fj. 
997
The ?2 score ranges from 0 to 1. We set a threshold at 0.001, below which the ?2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a candi-date pair are expected to be translated, there should be a preference for linking the words towards the end of the Chinese text. One advantage of Com-petitive Linking is that it is quite easy to introduce such preferences into the algorithm, by using the word positions to break ties of the ?2 scores when sorting the word pairs. 4.4 Capturing syllable-level regularities Many of the parenthetical translations involve proper names, which are often transliterated ac-cording to the sound. Word alignment algorithms have generally ignored syllable-level regularities in transliterated terms.  Consider again the Shapiro example in the introduction section. There are nu-merous correct transliterations for the same Eng-lish word, some of which are not very frequent. For example, the word ???happens to have a similar ?2 score with Shapiro as the word ?? (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.  Previous approaches to parenthetical translations relied on specialized algorithms to deal with trans-literations (Cao et al 2007; Jiang et al 2007; Wu and Chang, 2007). They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model. We adopted a simpler approach that does not re-quire any additional resources such as pronuncia-tion dictionaries and bilingual dictionaries. In addition to computing the ?2 scores between words, we also compute the ?2 scores of prefixes and suffixes of Chinese and English words. For both languages, the prefix of a word is defined as the first three bytes of the word and the suffix is defined as the last three bytes. Since we used UTF-8 encoding, the first and last three bytes of a Chi-nese word, except in very rare cases, correspond to the first and last Chinese character of the word. Table 4 lists the English prefixes and suffixes that have the highest ?2 scores with the Chinese prefix?and suffix?. 
 Type Chinese English prefix ? sha, amo, cha, sum, haw, lav, lun, xia, xal, hnl, shy, eve, she, cfh, ? suffix ? rlo, llo, ouh, low, ilo, owe, lol, lor, zlo, klo, gue, ude, vir, row, oro, olo, aro, ulo, ero, iro, rro, loh, lok, ? Table 4: Example prefixes and suffixes with top ?2 In our modified version of the competitive link-ing algorithm, the link score of a pair of words is the sum of the ?2 scores of the words themselves, their prefixes and their suffixes.  In addition to syllable-level correspondences in transliterations, the ?2 scores of prefixes and suf-fixes can also capture correlations in morphologi-cally composed words. For example, the Chinese prefix ? (three) has a relatively high ?2 score with the English prefix tri. Such scores enable word alignments to be made that may otherwise be missed. Consider the following text snippet: ...... ?  ? ??? (triaziflam)  The correct translation for triaziflam is?????.  However, the Chinese term is segmented as ? + ? + ???. The association between? (three) and triaziflam is very weak because ?is a very frequent word, whereas triaziflam is an extremely rare word. With the addition of the ?2 score be-tween ?and tri, we were able to correctly estab-lish the connection between triaziflam and ?. It turns out to be quite effective to assume pre-fixes and suffixes of words consist of three bytes, despite its apparent simplicity. The benefit of ?2 scores for prefixes and suffixes is not limited to morphemes that happen to be three bytes long.  For example, the English morpheme ?du-? corresponds to the Chinese character ? (two). Although the ?2 between du and? won?t be computed, we do find high ?2 scores between? and due and between? and dua. The three letter prefixes account for many of the words with the du- prefix. 5 Experimental Results We extracted from Chinese web pages about 1.58 billion unique sentences with parentheses that con-tain ASCII text. We removed duplicate sentences so that duplications of web documents will not skew the statistics. By applying the filtering algo-rithm in Sec. 3.1, we constructed a partially paral-
998
lel corpus with 126,612,447 candidate pairs (46,791,841 unique), which is about 8% of the number of sentences. Using the word alignment algorithm in Sec. 4, we extracted 26,753,972 trans-lation pairs between 13,471,221 unique English terms and 11,577,206 unique Chinese terms. Parenthetical translations mined from the Web have mostly been evaluated by manual examina-tion of a small sample of results (usually a few hundred entries) or in a Cross Lingual Information Retrieval setup. There does not yet exist a common evaluation data set.  5.1 Evaluation with Wikipedia Our first evaluation is based on translations in Wikipedia, which contains far more terminology and proper names than bilingual dictionaries. We extracted the titles of Chinese and English Wikipe-dia articles that are linked to each other and treated them as gold standard translations. There are 79,714 such pairs. We removed the following types of pairs because they are not translations or are not terms: ? Pairs with identical strings. For example, both English and Chinese versions have an entry ti-tled ?.ch?; ? Pairs where the English term begins with a digit, e.g., ?245?, ?300 BC?, ?1991 in film?; ? Pairs where the English term matches the regu-lar expression ?List of .*?, e.g., ?List of birds?, ?List of cinemas in Hong Kong?; ? Pairs where the Chinese title does not have any non-ASCII code. For example, the English en-try ?Syncfusion? is linked to ?.NET Frame-work? in the Chinese Wikipedia. The resulting data set contains 68,131 transla-tion pairs between 62,581 Chinese terms and 67,613 English terms. Only a small percentage of terms have more than one translation.  Whenever there is more than one translation, we randomly pick one as the answer key. For each Chinese and English word in the Wikipedia data, we first find whether there is a translation for the word in the extracted translation pairs. The Coverage of the Wikipedia data is measured by the percentage of words for which one or more translations are found. We then see whether our most frequent translation is an Exact Match of the answer key in the Wikipedia data.    
  Coverage Exact Match Full 70.8% 36.4% -term 67.1% 34.8% -pre-suffix 67.6% 34.4% IBM 67.6% 31.2% LDC 10.8% 4.8% Table 5: Chinese to English Results    Coverage Exact Match Full 59.6% 27.9% -term 59.6% 27.5% -pre-suffix 58.9% 27.4% IBM 52.4% 13.4% LDC 3.0% 1.4% Table 6: English to Chinese Results  Table 5 and 6 show the Chinese-to-English and English-to-Chinese results for the following sys-tems: Full refers to our system described in Sec. 3 and 4; -term is the system without the use of query logs to restrict potential term boundary posi-tions (Sec. 3.2); -pre-suffix is the system without using the ?2 score of the prefixes and suffixes; IBM refers to a system where we substitute our word alignment algorithm with IBM Model 1 and Model 2 followed by the HMM alignment (Och and Ney 2003), which is a common configuration for the word align-ment components in machine translations systems; LDC refers to the LDC2.0 English to Chinese bilingual dictionary with 161,117 translation pairs. It can be seen that the use of queries to constrain boundary positions and the addition of ?2 scores of prefixes/suffixes improve the percentage of Exact Match. The IBM Model tends to make many more alignments than Completive Linking. While this is often beneficial for machine translation systems, it is not very suitable for creating bilingual dictionar-ies, where precision is of paramount importance. The LDC dictionary was manually compiled from diverse resources within LDC and (mostly) from the Internet. Its coverage of Wikipedia data is ex-tremely low, compared to our method.  
999
English Wikipedia Translation Parenthetical Translation Pumping lemma ??? ??1 Topic-prominent language ?????? ?????1 Yoido Full Gos-pel Church ???????? ??????1 First Bulgarian Empire ???????? ?????????2 Vespid ?? ??????2 Ibrahim Rugova ???????? ???3 Jerry West ?????? ???3 Nicky Butt ????? ??3 Benito Mussolini ???????? ????3 Ecology of Hong Kong ???? ?????* Paracetamol ?????? ????* Thermidor ?? ??* Udo ?? ?? Public opinion ?? ???? Michael Bay ???? ????? Dagestan ??????? ???? Battle of Leyte Gulf ????? ??????? Glock ????? ??? Ergonomics ????? ??? Frank Sinatra ??????? ?????? Zaragoza ????? ???? Komodo ???? ???? Eli Vance ????? ??????? Manitoba ???? ????? Giant Bottlenose Whale ????? ???? Exclusionary rule ?????? ?????? Computer worm ???? ????? Social network ????? ???? Glasgow School of Art ???????? ???????? Dee Hock ????? ????? Bondage ?? ?? The China Post ?????? ???? Rachel ?? ?? John Nash ????? ????? Hattusa ??? ??? Bangladesh ???? ??? Table 7: A random sample of non-exact-matches                                                            1the extracted translation is too short 2the extracted translation is too long 3the extracted translation contains only the last name *the extracted term is completely wrong.   
 Note that Exact Match is a rather stringent crite-rion. Table 7 shows a random sample of extracted parenthetical translations that failed the Exact Match test. Only a small percentage of them are genuine errors. We nonetheless adopted this meas-ure because it has the advantage of automated evaluation and our goal is mainly to compare the relative performances. To determine the upper bound of the coverage of our web data, for each Wikipedia English term we searched within the total set of available paren-thesized text fragments (our English candidate set before filtering as by Step 1).  We discovered 81% of the Wikipedia titles, which is approximately 10% above the coverage of our final output. This indicates a minor loss of recall because of mistakes made in filtering (Sec. 3.1) and/or word alignment.  5.2 Evaluation with term translation requests To evaluate the coverage of output produced by their method, Cao et al(2007) extracted English queries from the query log of a Chinese search en-gine. They assume that the reason why users typed the English queries in a Chinese search box is mostly to find out their Chinese translations. Ex-amining our own Chinese query logs, however, the most-frequent English queries appear to be naviga-tional queries instead of translation requests. We therefore used the following regular expression to identify queries that are unambiguously translation requests:  /^[a-zA-Z ]* ???$/ where???means ??s Chinese?. This regular ex-pression matched 1579 unique queries in the logs. We manually judged the translation for 200 of them. A small random sample of the 200 is shown in Table 8. The empty cells indicate that the Eng-lish term is missing from our translation pairs. We use * to mark incorrect translations. When com-pared with the sample queries in (Cao et al, 2007), the queries in our sample seem to contain more phrasal words and technical terminology. It is in-teresting to see that even though parenthetical translations tend to be out-of-vocabulary words, as we have remarked in the introduction, the sheer size of the web means that occasionally transla-tions of common words such as ?use? are some-times included as well. 
1000
We compared our results with translations ob-tained from Google and Yahoo?s translation serv-ices. The numbers of correct translations for the random sample of 200 queries are as follows: Systems Google Yahoo! Mined Mined+G Correct 115 84 116 135 Our system?s outputs (Mined) have the same accuracy as the Google Translate. Our outputs have results for 154 out of the 200 queries. The 46 missing results are considered incorrect. If we combine our results with Google Translate by looking up Google results for missing entries, the accuracy increases from 56% to 68% (Mined+G). If we treat the LDC Chinese-English Dictionary 2.0 as a translator, it only covers 20.5% of the 200 queries.  5.3 Evaluation with SMT The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al 2003; Brants et al 2007) with the FBIS Chinese-English parallel text (NIST, 2003). We then added the extracted translation pairs as 
additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. 6 Related Work Nagata et al (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms them-selves may be hard to come by.  Cao et al (2007), like us, used a 300GB collec-tion of web documents as input. They used super-vised learning to build models that deal with phonetic transliterations and semantic translations separately. Our work relies on unsupervised learn-ing and does not make a distinction between trans-lations and transliterations. Furthermore, we are able to extract two orders of magnitude more trans-lations from than (Cao et al, 2007). 7 Conclusion We presented a method to apply a word alignment algorithm on a partially parallel corpus to extract translation pairs from the web. Treating the transla-tion extraction problem as a word alignment prob-lem allowed us to generalize across instances involving different in-parenthesis terms. Our algo-rithm extends Competitive Linking to deal with multi-word alignments and takes advantage of word-internal correspondences between transliter-ated words or morphologically composed words. Finally, through our discussion of parallel Wikipe-dia topic titles as a gold standard, we presented the first evaluation of such an extraction system that went beyond manual judgments on small sized samples. Acknowledgments We would like to thank the anonymous reviewers for their valuable comments.  
buckingham palace ???? chinadaily ???? coo ????? diammonium sulfate  emilio pucci ??????? finishing school ???? gloria ???? horny ?????* jam ?? lean six sigma ?????? meiosis ???? near miss ???? pachycephalosaurus ??? pops ???????? recreation vehicle ????? shanghai ethylene cracker complex  stenonychosaurus ??? theanine ??? use ?? with you all the time ??????????? Table 8: A small sample of manually judged query translations 
1001
References  T. Brants, A. Popat, P. Xu, F. Och and J. Dean, Large Language Models for Machine Translation, EMNLP-CoNLL-2007. P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Compu-tational Linguistics, 19(2):263?311. G. Cao, J. Gao and J.Y. Nie. 2007. A system to mine large-scale bilingual dictionaries from monolingual Web pages, MT Summit, pp. 57-64. T. Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguis-tics 19, 1. W. Gale and K. Church. 1991. Identifying word corre-spondence in parallel text. In Proceedings of the DARPA NLP Workshop. L. Jiang, M. Zhou, L.F. Chien, C. Niu. 2007. Named Entity Translation with Web Mining and Translitera-tion. In Proc. of IJCAI-2007. pp. 1629-1634. P. Koehn, F. Och and D. Marcu, Statistical Phrase-based Translation, In Proc. of HLT-NAACL 2003. K.L. Kwok, P. Deng, N. Dinstl, H.L. Sun, W. Xu, P. Peng, and J. Doyon. 2005. CHINET: a Chinese name finder system for document triage. In Proceedings of 2005 International Conference on Intelligence Analysis. I.D. Melamed. 2000. Models of translational equiva-lence among words. Computational Linguistics, 26(2):221?249. M. Nagata, T. Saito, and K. Suzuki. 2001. Using the Web as a bilingual dictionary. In Proc. of ACL 2001 DD-MT Workshop, pp.95-102. NIST. 2003. The NIST machine translation evaluations. http://www.nist.gov/speech/tests/mt/. F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19?51. I.A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proc. of CICLing-2002, pp 1?15, Mexico City, Mexico. B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-criminative matching approach to word alignment. In Proc. of HLT/EMNLP-05. Vancouver, BC. J. Tiedemann. 2004. Word to word alignment strategies. In Proceedings of the 20th international Conference on Computational Linguistics. Geneva, Switzerland.  
J.C. Wu and J.S. Chang. 2007. Learning to Find English to Chinese Transliterations on the Web. In Proc. of EMNLP-CoNLL-2007.  pp.996-1004. Prague, Czech Republic. Y. Zhang, S. Vogel. 2005 Competitive Grouping in In-tegrated Phrase Segmentation and Alignment Model. in Proceedings of ACL-05 Workshop on Building and Parallel Text. Ann Arbor, MI.  
1002
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168?1179,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning Sentential Paraphrases from Bilingual Parallel Corpora
for Text-to-Text Generation
Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
Abstract
Previous work has shown that high quality
phrasal paraphrases can be extracted from
bilingual parallel corpora. However, it is not
clear whether bitexts are an appropriate re-
source for extracting more sophisticated sen-
tential paraphrases, which are more obviously
learnable from monolingual parallel corpora.
We extend bilingual paraphrase extraction to
syntactic paraphrases and demonstrate its abil-
ity to learn a variety of general paraphrastic
transformations, including passivization, da-
tive shift, and topicalization. We discuss how
our model can be adapted to many text gener-
ation tasks by augmenting its feature set, de-
velopment data, and parameter estimation rou-
tine. We illustrate this adaptation by using
our paraphrase model for the task of sentence
compression and achieve results competitive
with state-of-the-art compression systems.
1 Introduction
Paraphrases are alternative ways of expressing the
same information (Culicover, 1968). Automatically
generating and detecting paraphrases is a crucial as-
pect of many NLP tasks. In multi-document sum-
marization, paraphrase detection is used to collapse
redundancies (Barzilay et al, 1999; Barzilay, 2003).
Paraphrase generation can be used for query expan-
sion in information retrieval and question answer-
ing systems (McKeown, 1979; Anick and Tipirneni,
1999; Ravichandran and Hovy, 2002; Riezler et al,
2007). Paraphrases allow for more flexible matching
of system output against human references for tasks
like machine translation and automatic summariza-
tion (Zhou et al, 2006; Kauchak and Barzilay, 2006;
Madnani et al, 2007; Snover et al, 2010).
Broadly, we can distinguish two forms of para-
phrases: phrasal paraphrases denote a set of surface
text forms with the same meaning:
the committee?s second proposal
the second proposal of the committee
while syntactic paraphrases augment the surface
forms by introducing nonterminals (or slots) that are
annotated with syntactic constraints:
the NP1?s NP2
the NP2 of the NP1
It is evident that the latter have a much higher poten-
tial for generalization and for capturing interesting
paraphrastic transformations.
A variety of different types of corpora (and se-
mantic equivalence cues) have been used to auto-
matically induce paraphrase collections for English
(Madnani and Dorr, 2010). Perhaps the most nat-
ural type of corpus for this task is a monolingual
parallel text, which allows sentential paraphrases to
be extracted since the sentence pairs in such corpora
are perfect paraphrases of each other (Barzilay and
McKeown, 2001; Pang et al, 2003). While rich syn-
tactic paraphrases have been learned from monolin-
gual parallel corpora, they suffer from very limited
data availability and thus have poor coverage.
Other methods obtain paraphrases from raw
monolingual text by relying on distributional simi-
larity (Lin and Pantel, 2001; Bhagat and Ravichan-
dran, 2008). While vast amounts of data are
readily available for these approaches, the distri-
butional similarity signal they use is noisier than
the sentence-level correspondency in parallel cor-
pora and additionally suffers from problems such as
mistaking cousin expressions or antonyms (such as
{boy , girl} or {rise, fall}) for paraphrases.
1168
Abundantly available bilingual parallel corpora
have been shown to address both these issues, ob-
taining paraphrases via a pivoting step over foreign
language phrases (Bannard and Callison-Burch,
2005). The coverage of paraphrase lexica extracted
from bitexts has been shown to outperform that
obtained from other sources (Zhao et al, 2008a).
While there have been efforts pursuing the extrac-
tion of more powerful paraphrases (Madnani et
al., 2007; Callison-Burch, 2008; Cohn and Lapata,
2008; Zhao et al, 2008b), it is not yet clear to what
extent sentential paraphrases can be induced from
bitexts. In this paper we:
? Extend the bilingual pivoting approach to para-
phrase induction to produce rich syntactic para-
phrases.
? Perform a thorough analysis of the types of
paraphrases we obtain and discuss the para-
phrastic transformations we are capable of cap-
turing.
? Describe how training paradigms for syntac-
tic/sentential paraphrase models should be tai-
lored to different text-to-text generation tasks.
? Demonstrate our framework?s suitability for a
variety of text-to-text generation tasks by ob-
taining state-of-the-art results on the example
task of sentence compression.
2 Related Work
Madnani and Dorr (2010) survey a variety of data-
driven paraphrasing techniques, categorizing them
based on the type of data that they use. These
include large monolingual texts (Lin and Pantel,
2001; Szpektor et al, 2004; Bhagat and Ravichan-
dran, 2008), comparable corpora (Barzilay and Lee,
2003; Dolan et al, 2004), monolingual parallel cor-
pora (Barzilay and McKeown, 2001; Pang et al,
2003), and bilingual parallel corpora (Bannard and
Callison-Burch, 2005; Madnani et al, 2007; Zhao et
al., 2008b). We focus on the latter type of data.
Paraphrase extraction using bilingual parallel cor-
pora was proposed by Bannard and Callison-Burch
(2005) who induced paraphrases using techniques
from phrase-based statistical machine translation
(Koehn et al, 2003). After extracting a bilingual
phrase table, English paraphrases are obtained by
pivoting through foreign language phrases. Since
many paraphrases can be extracted for a phrase,
Bannard and Callison-Burch rank them using a para-
phrase probability defined in terms of the translation
model probabilities p(f |e) and p(e|f):
p(e2|e1) =
?
f
p(e2, f |e1) (1)
=
?
f
p(e2|f, e1)p(f |e1) (2)
?
?
f
p(e2|f)p(f |e1). (3)
Several subsequent efforts extended the bilin-
gual pivoting technique, many of which introduced
elements of more contemporary syntax-based ap-
proaches to statistical machine translation. Mad-
nani et al (2007) extended the technique to hier-
archical phrase-based machine translation (Chiang,
2005), which is formally a synchronous context-free
grammar (SCFG) and thus can be thought of as a
paraphrase grammar. The paraphrase grammar can
paraphrase (or ?decode?) input sentences using an
SCFG decoder, like the Hiero, Joshua or cdec MT
systems (Chiang, 2007; Li et al, 2009; Dyer et al,
2010). Like Hiero, Madnani?s model uses just one
nonterminal X instead of linguistic nonterminals.
Three additional efforts incorporated linguistic
syntax. Callison-Burch (2008) introduced syntac-
tic constraints by labeling all phrases and para-
phrases (even non-constituent phrases) with CCG-
inspired slash categories (Steedman and Baldridge,
2011), an approach similar to Zollmann and Venu-
gopal (2006)?s syntax-augmented machine transla-
tion (SAMT). Callison-Burch did not formally de-
fine a synchronous grammar, nor discuss decoding,
since his presentation did not include hierarchical
rules. Cohn and Lapata (2008) used the GHKM
extraction method (Galley et al, 2004), which is
limited to constituent phrases and thus produces
a reasonably small set of syntactic rules. Zhao
et al (2008b) added slots to bilingually extracted
paraphrase patterns that were labeled with part-of-
speech tags, but not larger syntactic constituents.
Before the shift to statistical natural language pro-
cessing, paraphrasing was often treated as syntactic
transformations or by parsing and then generating
1169
from a semantic representation (McKeown, 1979;
Muraki, 1982; Meteer and Shaked, 1988; Shem-
tov, 1996; Yamamoto, 2002). Indeed, some work
generated paraphrases using (non-probabilistic) syn-
chronous grammars (Shieber and Schabes, 1990;
Dras, 1997; Dras, 1999; Kozlowski et al, 2003).
After the rise of statistical machine translation, a
number of its techniques were repurposed for para-
phrasing. These include sentence alignment (Gale
and Church, 1993; Barzilay and Elhadad, 2003),
word alignment and noisy channel decoding (Brown
et al, 1990; Quirk et al, 2004), phrase-based models
(Koehn et al, 2003; Bannard and Callison-Burch,
2005), hierarchical phrase-based models (Chiang,
2005; Madnani et al, 2007), log-linear models and
minimum error rate training (Och, 2003a; Madnani
et al, 2007; Zhao et al, 2008a), and here syntax-
based machine translation (Wu, 1997; Yamada and
Knight, 2001; Melamed, 2004; Quirk et al, 2005).
Beyond cementing the ties between paraphrasing
and syntax-based statistical machine translation, the
novel contributions of our paper are (1) an in-depth
analysis of the types of structural and sentential
paraphrases that can be extracted with bilingual piv-
oting, (2) a discussion of how our English?English
paraphrase grammar should be adapted to specific
text-to-text generation tasks (Zhao et al, 2009) with
(3) a concrete example of the adaptation procedure
for the task of paraphrase-based sentence compres-
sion (Knight and Marcu, 2002; Cohn and Lapata,
2008; Cohn and Lapata, 2009).
3 SCFGs in Translation
The model we use in our paraphrasing approach is
a syntactically informed synchronous context-free
grammar (SCFG). The SCFG formalism (Aho and
Ullman, 1972) was repopularized for statistical ma-
chine translation by Chiang (2005). Formally, a
probabilistic SCFG G is defined by specifying
G = ?N , TS , TT ,R, S?,
whereN is a set of nonterminal symbols, TS and TT
are the source and target language vocabularies, R
is a set of rules and S ? N is the root symbol. The
rules inR take the form:
C ? ??, ?,?, w?,
PP/NN ? mit einer  |  with a
NP ? das leck  |  the leak
VP ?  NP PP/NN detonation zu schliessen  |  closing NP PP/NN blast 
they
VP
VP
PRP VBD NNDTNN
NP NPNP
closing tried the   
S
sie versuchten das zu schliessen
leak
leck
with a   blast
DT IN
PP
VBG
einermit detonation
Figure 1: Synchronous grammar rules for translation are
extracted from sentence pairs in a bixtext which have
been automatically parsed and word-aligned. Extraction
methods vary on whether they extract only minimal rules
for phrases dominated by nodes in the parse tree, or more
complex rules that include non-constituent phrases.
where the rule?s left-hand side C ? N is a nonter-
minal, ? ? (N?TS)? and ? ? (N?TT )? are strings
of terminal and nonterminal symbols with an equal
number of nonterminals cNT (?) = cNT (?) and
?: {1 . . . cNT (?)} ? {1 . . . cNT (?)}
constitutes a one-to-one correspondency function
between the nonterminals in ? and ?. A non-
negative weight w ? 0 is assigned to each rule, re-
flecting the likelihood of the rule.
Rule Extraction Phrase-based approaches to sta-
tistical machine translation (and their successors)
extract pairs of (e, f) phrases from automatically
word-aligned parallel sentences. Och (2003b)
described various heuristics for extracting phrase
alignments from the Viterbi word-level alignments
that are estimated using Brown et al (1993) word-
alignment models.
These phrase extraction heuristics have been ex-
tended so that they extract synchronous grammar
rules (Galley et al, 2004; Chiang, 2005; Zollmann
and Venugopal, 2006; Liu et al, 2006). Most of
these extraction methods require that one side of the
parallel corpus be parsed. This is typically done au-
tomatically with a statistical parser.
Figure 1 shows examples of rules obtained from
a sentence pair. To extract a rule, we first choose a
source side span f like das leck. Then we use phrase
extraction techniques to find target spans e that are
consistent with the word alignment (in this case the
1170
leak is consistent with our f ). The nonterminal sym-
bol that is the left-hand side of the SCFG rule is then
determined by the syntactic constituent that domi-
nates e (in this case NP). To introduce nonterminals
into the right-hand side of the rule, we can apply
rules extracted over sub-phrases of f , synchronously
substituting the corresponding nonterminal symbol
for the sub-phrases on both sides. The synchronous
substitution applied to f and e then yields the corre-
spondency ?.
One significant differentiating factor between the
competing ways of extracting SCFG rules is whether
the extraction method generates rules only for con-
stituent phrases that are dominated by a node in
the parse tree (Galley et al, 2004; Cohn and
Lapata, 2008) or whether they include arbitrary
phrases, including non-constituent phrases (Zoll-
mann and Venugopal, 2006; Callison-Burch, 2008).
We adopt the extraction for all phrases, including
non-constituents, since it allows us to cover a much
greater set of phrases, both in translation and para-
phrasing.
Feature Functions Rather than assigning a single
weight w, we define a set of feature functions ~? =
{?1...?N} that are combined in a log-linear model:
w = ?
N?
i=1
?i log?i. (4)
The weights ~? of these feature functions are set to
maximize some objective function like BLEU (Pap-
ineni et al, 2002) using a procedure called minimum
error rate training (MERT), owing to Och (2003a).
MERT iteratively adjusts the weights until the de-
coder produces output that best matches reference
translations in a development set, according to the
objective function. We will examine appropriate ob-
jective functions for text-to-text generation tasks in
Section 6.2.
Typical features used in statistical machine trans-
lation include phrase translation probabilities (cal-
culated using maximum likelihood estimation over
all phrase pairs enumerable in the parallel cor-
pus), word-for-word lexical translation probabili-
ties (which help to smooth sparser phrase transla-
tion estimates), a ?rule application penalty? (which
governs whether the system prefers fewer longer
they can not be dangerous to the rest of the village
VP/PP
VB+JJ
S
NP
NP/NN
sie k?nnengef?hrlich werdennichtdem rest des dorfes
VP/PP
VB+JJ
S
NP
NP/NN
NP/NN ? dem rest des  |   the rest of the
NP ? NP/NN dorfes  |  NP/NN village
VP/PP ? nicht VB+JJ k?nnen  |  can not VB+JJ
VB+JJ ? gef?hrlich werden  |  be dangerous
S ? sie NP VP/PP  |  they VP/PP to NP
Figure 2: An example derivation produced by a syntactic
machine translation system. Although the synchronous
trees are unlike the derivations found in the Penn Tree-
bank, their yield is a good translation of the German.
phrases or a greater number of shorter phrases), and
a language model probability.
Decoding Given an SCFG and an input source
sentence, the decoder performs a search for the sin-
gle most probable derivation via the CKY algorithm.
In principle the best translation should be the En-
glish sentence e that is the most probable after sum-
ming over all d ? D derivations, since many deriva-
tions yield the same e. In practice, we use a Viterbi
approximation and return the translation that is the
yield of the single best derivation:
e? = arg max
e?Trans(f)
?
d?D(e,f)
p(d, e|f)
? yield(arg max
d?D(e,f)
p(d, e|f)). (5)
Derivations are simply successive applications of the
SCFG rules such as those given in Figure 2.
4 SCFGs in Paraphrasing
Rule Extraction To create a paraphrase grammar
from a translation grammar, we extend the syntac-
tically informed pivot approach of Callison-Burch
(2008) to the SCFG model. For this purpose, we
assume a grammar that translates from a given for-
eign language to English. For each pair of trans-
lation rules where the left-hand side C and foreign
1171
string ? match:
C ? ??, ?1,?1, ~?1?
C ? ??, ?2,?2, ~?2?,
we create a paraphrase rule:
C ? ??1, ?2,?, ~??,
where the nonterminal correspondency relation ?
has been set to reflect the combined nonterminal
alignment:
? = ??11 ? ?2 .
Feature Functions In the computation of the fea-
tures ~? from ~?1 and ~?2 we follow the approximation
in Equation 3, which yields lexical and phrasal para-
phrase probability features. Additionally, we add a
boolean indicator for whether the rule is an iden-
tity paraphrase, ?identity . Another indicator feature,
?reorder , fires if the rule swaps the order of two non-
terminals, which enables us to promote more com-
plex paraphrases that require structural reordering.
Decoding With this, paraphrasing becomes an
English-to-English translation problem which can
be formulated similarly to Equation 5 as:
e?2 ? yield(arg max
d?D(e2,e1)
p(d, e2|e1)).
Figure 3 shows an example derivation produced as a
result of applying our paraphrase rules in the decod-
ing process. Another advantage of using the decoder
from statistical machine translation is that n-gram
language models, which have been shown to be use-
ful in natural language generation (Langkilde and
Knight, 1998), are already well integrated (Huang
and Chiang, 2007).
5 Analysis
A key motivation for the use of syntactic paraphrases
over their phrasal counterparts is their potential to
capture meaning-preserving linguistic transforma-
tions in a more general fashion. A phrasal system is
limited to memorizing fully lexicalized transforma-
tions in its paraphrase table, resulting in poor gener-
alization capabilities. By contrast, a syntactic para-
phrasing system intuitively should be able to address
this issue and learn well-formed and generic patterns
that can be easily applied to unseen data.
twelve cartoons insulting the
prophet
mohammad
CD
NNS
JJ DT
NNP
NP
NP
VP
NP
DT+NNP
12 the prophet mohammad
CD
NNS
JJ DT
NNP
NP
NP
VP
NP
DT+NNP
cartoons offensive
Foreign Pivot PhraseParaphrase Rule
JJ ? offensive  |   insulting
Lexical paraphrase:
NP ? NP that VP  |  NP VP
Reduced relative clause:
NP ? CD of the NNS  |  CD NNS
Partitive construction: 
VP ? are JJ to NP  |  JJ NP
Pred. adjective copula deletion:
JJ -> beleidigend  |  offensive
JJ -> beleidigend  |  insulting
NP -> NP die VP  |  NP VP
NP -> NP die VP  |  NP that VP
NP -> CD der NNS  |  CD of the NNS
NP -> CD der NNS  |  CD NNS
VP ? sind JJ f?r NP  |  are JJ to NP
VP ? sind JJ f?r NP  |  JJ NP
of the that are to
Figure 3: An example of a synchronous paraphrastic
derivation. A few of the rules applied in the parse are
show in the left column, with the pivot phrases that gave
rise to them on the right.
To put this expectation to the test, we investigate
how our grammar captures a number of well-known
paraphrastic transformations.1 Table 1 shows the
transformations along with examples of the generic
grammar rules our system learns to represent them.
When given a transformation to extract a syntactic
paraphrase for, we want to find rules that neither
under- nor over-generalize. This means that, while
replacing the maximum number of syntactic argu-
ments with nonterminals, the rules ideally will both
retain enough lexicalization to serve as sufficient ev-
idence for the applicability of the transformation and
impose constraints on the nonterminals to ensure the
arguments? well-formedness.
The paraphrases implementing the possessive rule
and the dative shift shown in Table 1 are a good
examples of this: the two noun-phrase arguments
to the expressions are abstracted to nonterminals
while each rule?s lexicalization provides an appro-
priate frame of evidence for the transform. This is
important for a good representation of dative shift,
which is a reordering transformation that fully ap-
plies to certain ditransitive verbs while other verbs
are uncommon in one of the forms:
1The data and software used to extract the grammar we draw
these examples from is described in Section 6.5.
1172
Possessive rule NP ? the NN of the NNP | the NNP ?s NNNP ? the NNS 1 made by NNS 2 | the NNS 2?s NNS 1
Dative shift VP ? give NN to NP | give NP the NNVP ? provide NP1 to NP2 | give NP2 NP1
Adv./adj. phrase move S/VP ? ADVP they VBP | they VPB ADVPS ? it is ADJP VP | VP is ADJP
Verb particle shift VP ? VB NP up | VB up NP
Reduced relative clause SBAR/S ? although PRP VBP that | although PRP VBPADJP ? very JJ that S | JJ S
Partitive constructions NP ? CD of the NN | CD NNNP ? all DT\NP | all of the DT\NP
Topicalization S ? NP , VP . | VP , NP .
Passivization SBAR? that NP had VBN | which was VBN by NP
Light verbs VP ? take action ADVP | to act ADVPVP ? TO take a decision PP | TO decide PP
Table 1: A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases that
our system extracts capturing these.
give decontamination equipment to Japan
give Japan decontamination equipment
provide decontamination equipment to Japan
? provide Japan decontamination equipment
Note how our system extracts a dative shift rule for
to give and a rule that both shifts and substitutes a
more appropriate verb for to provide.
The use of syntactic nonterminals in our para-
phrase rules to capture complex transforms also
makes it possible to impose constraints on their ap-
plication. For comparison, as Madnani et al (2007)
do not impose any constraints on how the nontermi-
nal X can be realized, their equivalent of the topi-
calization rule would massively overgeneralize:
S ? X1, X2 . | X2, X1 .
Additional examples of transforms our use of syn-
tax allows us to capture are the adverbial phrase
shift and the reduction of a relative clause, as well
as other phenomena listed in Table 1.
Unsurprisingly, syntactic information alone is not
sufficient to capture all transformations. For in-
stance it is hard to extract generic paraphrases for all
instances of passivization, since our syntactic model
currently has no means of representing the morpho-
logical changes that the verb undergoes:
the reactor leaks radiation
radiation is leaking from the reactor .
Still, for cases where the verb?s morphology does
not change, we manage to learn a rule:
the radiation that the reactor had leaked
the radiation which leaked from the reactor .
Another example of a deficiency in our synchronous
grammar models are light verb constructs such as:
to take a walk
to walk .
Here, a noun is transformed into the corresponding
verb ? something our synchronous syntactic CFGs
are not able to capture except through memorization.
Our survey shows that we are able to extract ap-
propriately generic representations for a wide range
of paraphrastic transformations. This is a surpris-
ing result which shows that bilingual parallel cor-
pora can be used to learn sentential paraphrases, and
that they are a viable alternative to other data sources
like monolingual parallel corpora, which more obvi-
ously contain sentential paraphrases, but are scarce.
6 Text-to-Text Applications
The core of many text-to-text generation tasks is
sentential paraphrasing, augmented with specific
constraints or goals. Since our model borrows much
of its machinery from statistical machine translation
? a sentential rewriting problem itself ? it is straight-
forward to use our paraphrase grammars to generate
new sentences using SMT?s decoding and param-
eter optimization techniques. Our framework can
be adapted to many different text-to-text generation
tasks. These could include text simplification, sen-
1173
tence compression, poetry generation, query expan-
sion, transforming declarative sentences into ques-
tions, and deriving hypotheses for textual entail-
ment. Each individual text-to-text application re-
quires that our framework be adapted in several
ways, by specifying:
? A mechanism for extracting synchronous
grammar rules (in this paper we argue that
pivot-based paraphrasing is widely applicable).
? An appropriate set of rule-level features that
capture information pertinent to the task (e.g.
whether a rule simplifies a phrase).
? An appropriate ?objective function? that scores
the output of the model, i.e. a task-specific
equivalent to the BLEU metric in SMT.
? A development set with examples of the sen-
tential transformations that we are modeling.
? Optionally, a way of injecting task-specific
rules that were not extracted automatically.
In the remainder of this section, we illustrate how
our bilingually extracted paraphrases can be adapted
to perform sentence compression, which is the task
of reducing the length of sentence while preserving
its core meaning. Most previous approaches to sen-
tence compression focused only on the deletion of
a subset of words from the sentence (Knight and
Marcu, 2002). Our approach follows Cohn and La-
pata (2008), who expand the task to include substi-
tutions, insertions and reorderings that are automat-
ically learned from parallel texts.
6.1 Feature Design
In Section 4 we discussed phrasal probabilities.
While these help quantify how good a paraphrase
is in general, they do not make any statement on
task-specific things such as the change in language
complexity or text length. To make this information
available to the decoder, we enhance our paraphrases
with four compression-targeted features. We add the
count features csrc and ctgt , indicating the number of
words on either side of the rule as well as two differ-
ence features: cdcount = ctgt ? csrc and the anal-
ogously computed difference in the average word
length in characters, cdavg .
6.2 Objective Function
Given our paraphrasing system?s connection to
SMT, the naive/obvious choice for parameter op-
timization would be to optimize for BLEU over a
set of paraphrases, for instance parallel English ref-
erence translations for a machine translation task
(Madnani et al, 2007). For a candidate C and a ref-
erence R, (with lengths c and r) BLEU is defined as:
BLEUN (C,R)
=
{
e(1?c/r) ? e
?N
n=1 logwnpn if c/r ? 1
e
?N
n=1 logwnpn otherwise ,
where pn is the modified n-gram precision of C
against R, with typically N = 4 and wn = 1N . The
?brevity penalty? term e(1?c/r) is added to prevent
short candidates from achieving perfect scores.
Naively optimizing for BLEU, however, will re-
sult in a trivial paraphrasing system heavily biased
towards producing identity ?paraphrases?. This is
obviously not what we are looking for. Moreover,
BLEU does not provide a mechanism for directly
specifying a per-sentence compression rate, which
is desirable for the compression task.
Instead, we propose PRE?CIS, an objective func-
tion tailored to the text compression task:
PRE?CIS?,?(I, C,R)
=
{
e?(??c/i) ? BLEU(C,R) if c/i ? ?
BLEU(C,R) otherwise
For an input sentence I , an output C and ref-
erence compression R (with lengths i, c and r),
PRE?CIS combines the precision estimate of BLEU
with an additional ?verbosity penalty? that is ap-
plied to compressions that fail to meet a given target
compression rate ?. We rely on the BLEU brevity
penalty to prevent the system from producing overly
aggressive compressions. The scaling term ? deter-
mines how severely we penalize deviations from ?.
In our experiments we use ? = 10.
It is straightforward to find similar adaptations for
other tasks. For text simplification, for instance, the
penalty term can include a readability metric. For
poetry generation we can analogously penalize out-
puts that break the meter (Greene et al, 2010).
6.3 Development Data
To tune the parameters of our paraphrase system for
sentence compression, we need an appropriate cor-
1174
pus of reference compressions. Since our model is
designed to compress by paraphrasing rather than
deletion, the commonly used deletion-based com-
pression data sets like the Ziff-Davis corpus are not
suitable. We have thus created a corpus of com-
pression paraphrases. Beginning with 9570 tuples
of parallel English?English sentences obtained from
multiple reference translations for machine transla-
tion evaluation, we construct a parallel compression
corpus by selecting the longest reference in each tu-
ple as the source sentence and the shortest reference
as the target sentence. We further retain only those
sentence pairs where the compression rate cr falls in
the range 0.5 < cr ? 0.8. From these, we randomly
select 936 sentences for the development set, as well
as 560 sentences for a test set that we use to gauge
the performance of our system.
6.4 Grammar Augmentations
As we discussed in Section 5, the paraphrase gram-
mar we induce is capable of representing a wide va-
riety of transformations. However, the formalism
and extraction method are not explicitly geared to-
wards a compression application. For instance, the
synchronous nature of our grammar does not allow
us to perform deletions of constituents as done by
Cohn and Lapata (2007)?s tree transducers. One way
to extend the grammar?s capabilities towards the re-
quirements of a given task is by injecting additional
rules designed to capture appropriate operations.
For the compression task, this could include
adding rules to delete target-side nonterminals:
JJ ? JJ | ?
This would render the grammar asynchronous and
require adjustments to the decoding process. Al-
ternatively, we can generate rules that specifically
delete particular adjectives from the corpus:
JJ ? superfluous | ? .
In our experiments we evaluate the latter approach
by generating optional deletion rules for all adjec-
tives, adverbs and determiners.
6.5 Experimental Setup
We extracted a paraphrase grammar from the
French?English Europarl corpus (v5). The bitext
was aligned using the Berkeley aligner and the En-
glish side was parsed with the Berkeley parser. We
Grammar # Rules
total 42,353,318
w/o identity 23,641,016
w/o complex constituents 6,439,923
w/o complex const. & identity 5,097,250
Table 2: Number and distribution of rules in our para-
phrase grammar. Note the significant number of identity
paraphrases and rules with complex nonterminal labels.
obtained the initial translation grammar using the
SAMT toolkit (Venugopal and Zollmann, 2009).
The grammars we extract tend to be extremely
large. To keep their size manageable, we only con-
sider translation rules that have been seen more than
3 times and whose translation probability exceeds
10?4 for pivot recombination. Additionally, we only
retain the top 25 most likely paraphrases of each
phrase, ranked by a uniformly weighted combina-
tion of phrasal and lexical paraphrase probabilities.
We tuned the model parameters to our PRE?CIS
objective function, implemented in the Z-MERT
toolkit (Zaidan, 2009). For decoding we used the
Joshua decoder (Li et al, 2010). The language
model used in our paraphraser and the Clarke and
Lapata (2008) baseline system is a Kneser-Ney dis-
counted 5-gram model estimated on the Gigaword
corpus using the SRILM toolkit (Stolcke, 2002).
6.6 Evaluation
To assess the output quality of the resulting sentence
compression system, we compare it to two state-of-
the-art sentence compression systems. Specifically,
we compare against our implementation of Clarke
and Lapata (2008)?s compression model which uses
a series of constraints in an integer linear program-
ming (ILP) solver, and Cohn and Lapata (2007)?s
tree transducer toolkit (T3) which learns a syn-
chronous tree substitution grammar (STSG) from
paired monolingual sentences. Unlike SCFGs, the
STSG formalism allows changes to the tree topol-
ogy. Cohn and Lapata argue that this is a natural
fit for sentence compression, since deletions intro-
duce structural mismatches. We trained the T3 soft-
ware2 on the 936 ?full, compressed? sentence pairs
that comprise our development set. This is equiva-
lent in size to the training corpora that Cohn and La-
pata (2007) used (their training corpora ranged from
2www.dcs.shef.ac.uk/people/T.Cohn/t3/
1175
882?1020 sentence pairs), and has the advantage of
being in-domain with respect to our test set. Both
these systems reported results outperforming previ-
ous systems such as McDonald (2006). To showcase
the value of the adaptations discussed above, we also
compare variants of our paraphrase-based compres-
sion systems: using Hiero instead of syntax, using
syntax with or without compression features, using
an augmented grammar with optional deletion rules.
We solicit human judgments of the compres-
sions along two five-point scales: grammaticality
and meaning. Judges are instructed to decide how
much the meaning from a reference translation is
retained in the compressed sentence, with a score
of 5 indicating that all of the important information
is present, and 1 being that the compression does
not retain any of the original meaning. Similarly, a
grammar score of 5 indicates perfect grammaticality,
and a grammar score of 1 is assigned to sentences
that are entirely ungrammatical. To ensure fairness,
we perform pairwise system comparisons with com-
pression rates strictly tied on the sentence-level. For
any comparison, a sentence is only included in the
computation of average scores if the difference be-
tween both systems? compression rates is < 0.05.3
Table 4 shows a set of pairwise comparisons for
compression rates ? 0.5. We see that going from
a Hiero-based to a syntactic paraphrase grammar
yields a significant improvement in grammatical-
ity. Adding compression-specific features improves
grammaticality even further. Further augmenting the
grammar with deletion rules significantly helps re-
tain the core meaning at compression rates this high,
however compared to the un-augmented syntactic
system grammaticality scores drop. While our ap-
proach significantly outperforms the T3 system, we
are not able to match ILP?s results in grammaticality.
In Table 3 we compare our system to the ILP ap-
proach at a modest compression rate of? 0.8. Here,
we significantly outperform ILP in meaning reten-
tion while achieving comparable results in gram-
maticality. This improvement is significant at p <
0.0001, using the sign test, while the better gram-
maticality score of the ILP system is not statisti-
3Because evaluation quality correlates linearly with com-
pression rate, the community-accepted practice of not compar-
ing based on a closely tied compression rate is potentially sub-
ject to erroneous interpretation (Napoles et al, 2011).
CR Meaning Grammar
Reference 0.73 4.26 4.35
Syntax+Feat. 0.80 3.67 3.38
ILP 0.80 3.50 3.49
Random Deletions 0.50 1.94 1.57
Table 3: Results of the human evaluation on longer com-
pressions: pairwise compression rates (CR), meaning and
grammaticality scores. Bold indicates a statistically sig-
nificance difference at p < 0.05.
CR Meaning Grammar
Hiero 0.56 2.57 2.35
Syntax 0.56 2.76 2.67
Syntax 0.53 2.70 2.49
Syntax+Feat. 0.53 2.71 2.54
Syntax+Feat. 0.54 2.79 2.71
Syntax+Aug. 0.54 2.96 2.52
Syntax+Aug. 0.52 2.87 2.40
ILP 0.52 2.83 3.09
Syntax+Aug. 0.50 2.41 2.20
T3 0.50 2.01 1.93
Table 4: Human evaluation for shorter compressions and
for variations of our paraphrase system. +Feat. includes
the compression features from Section 6.1, +Aug. in-
cludes optional deletion rules from Section 6.4.
cally significant (p < 0.088). These results indi-
cate that, over a variety of compression rates, our
framework for text-to-text generation is performing
as well as or better than specifically tailored state-
of-the-art methods.
Table 5 shows an example sentence drawn from
our test set and the compressions produced by the
different systems. We see that both the paraphrase
and ILP systems produce good quality results, with
the paraphrase system retaining the meaning of the
source sentence more accurately.
7 Conclusion
In this work we introduced a method to learn syntac-
tically informed paraphrases from bilingual parallel
texts. We discussed the expressive power and limita-
tions of our formalism and outlined straightforward
adaptation strategies for applications in text-to-text
generation. We demonstrated when our paraphras-
ing system was adapted to do sentence compression,
it achieved results competitive with state-of-the-art
compression systems with only minimal effort.
1176
Source he also expected that he would have a role in the future at the level of the islamic movementacross the palestinian territories , even if he was not lucky enough to win in the elections .
Reference he expects to have a future role in the islamic movement in the palestinian territories if he isnot successful in the elections .
Syntax+Feat. he also expected that he would have a role in the future of the islamic movement in thepalestinian territories , although he was not lucky enough to win elections .
ILP he also expected that he would have a role at the level of the islamic movement , even if hewas not lucky enough to win in the elections .
Source in this war which has carried on for the last 12 days , around 700 palestinians , which includea large number of women and children , have died .
Reference about 700 palestinians , mostly women and children , have been killed in the israeli offensiveover the last 12 days .
Syntax+Feat. in this war has done for the last 12 days , around 700 palestinians , including women andchildren , died .
ILP in this war which has carried for the days palestinians , which include a number of womenand children died .
Source hala speaks arabic most of the time with her son , taking into consideration that he can speakenglish with others .
Reference hala speaks to her son mostly in arabic , as he can speak english to others .
Syntax+Feat. hala speaks arabic most of the time with her son , considering that he can speak english withothers .
ILP hala speaks arabic most of the time , taking into consideration that he can speak english withothers .
Table 5: Example compressions produced by the two systems in Table 3 for three input sentences from our test data.
Acknowledgments
We would like to thank Trevor Cohn for kindly pro-
viding us with the T3 compression system. This re-
search was supported by the NSF under grant IIS-
0713448. Opinions, interpretations, and conclusions
are the authors? alone.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Peter G. Anick and Suresh Tipirneni. 1999. The para-
phrase search assistant: terminological feedback for
iterative information seeking. In Proceedings of SI-
GIR.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings of
ACL.
Regina Barzilay. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Genera-
tion. Ph.D. thesis, Columbia University, New York.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Peter Brown, John Cocke, Stephen Della Pietra, Vincent
Della Pietra, Frederick Jelinek, Robert Mercer, and
1177
Paul Poossin. 1990. A statistical approach to language
translation. Computational Linguistics, 16(2), June.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2007. Large margin
synchronous generation and its application to sentence
compression. In Proceedings of EMNLP-CoLing.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research (JAIR), 34:637?674.
Peter W. Culicover. 1968. Paraphrase generation and
information retrieval from stored text. Mechani-
cal Translation and Computational Linguistics, 11(1-
2):78?88.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the COLING.
Mark Dras. 1997. Representing paraphrases using syn-
chronous tree adjoining grammars. In Proceedings of
ACL.
Mark Dras. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text. Ph.D. thesis, Macquarie
University, Australia.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL.
William Gale and Kenneth Church. 1993. A program
for aligning sentences in bilingual corpora. Compu-
atational Linguistics, 19(1):75?90.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010.
Automatic analysis of rhythmic poetry with applica-
tions to generation and translation. In Proceedings of
EMNLP.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
EMNLP.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Raymond Kozlowski, Kathleen McCoy, and K. Vijay-
Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure using
lexico-grammatical resources. In Workshop On Para-
phrasing.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Workshop On
Natural Language Generation, Ontario, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of WMT09.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of WMT10.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of WMT07.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
1178
Kathleen R. McKeown. 1979. Paraphrasing using given
and new information in a question-answer system. In
Proceedings of ACL.
Dan Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
Marie W. Meteer and Varda Shaked. 1988. Strategies for
effective paraphrasing. In Proceedings of COLING.
Kazunori Muraki. 1982. On a semantic model for multi-
lingual paraphrasing. In Proceedings of COLING.
Courtney Napoles, Chris Callison-Burch, and Ben-
jamin Van Durme. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Work-
shop on Monolingual Text-To-Text Generation.
Franz Josef Och. 2003a. Minimum error rate training for
statistical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003b. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual bitext-derived
paraphrases in automatic MT evaluation. In Proceed-
ings of WMT06.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
sufrace text patterns for a question answering system.
In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL.
Hadar Shemtov. 1996. Generation of paraphrases from
ambiguous logical forms. In Proceedings of COLING.
Stuart Shieber and Yves Schabes. 1990. Generation and
synchronous tree-adjoining grammars. In Workshop
On Natural Language Generation.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Non-Transformational
Syntax: Formal and Explicit Models of Grammar.
Wiley-Blackwell.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceeding of the International
Conference on Spoken Language Processing.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP, Pro-
ceedings of EMNLP.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. Prague Bul-
letin of Mathematical Linguistics, 91.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL.
Kazuhide Yamamoto. 2002. Machine translation by in-
teraction between paraphraser and transfer. In Pro-
ceedings of COLING.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008a. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase
patterns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of WMT06.
1179
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 590?600,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Semi-Markov Phrase-based Monolingual Alignment
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Allen Institute for Artificial Intelligence
Seattle, WA, USA
Abstract
We introduce a novel discriminative model for
phrase-based monolingual alignment using a
semi-Markov CRF. Our model achieves state-
of-the-art alignment accuracy on two phrase-
based alignment datasets (RTE and para-
phrase), while doing significantly better than
other strong baselines in both non-identical
alignment and phrase-only alignment. Addi-
tional experiments highlight the potential ben-
efit of our alignment model to RTE, para-
phrase identification and question answering,
where even a naive application of our model?s
alignment score approaches the state of the art.
1 Introduction
Various NLP tasks can be treated as an alignment
problem: machine translation (aligning words in one
language with words in another language), ques-
tion answering (aligning question words with the an-
swer phrase), textual entailment recognition (align-
ing premise with hypothesis), paraphrase detection
(aligning semantically equivalent words), etc. Even
though most of these tasks involve only a single lan-
guage, alignment research has primarily focused on
the bilingual setting (i.e., machine translation) rather
than monolingual. Moreover, most work has con-
sidered token-based approaches over phrase-based.1
Here we seek to address this imbalance by proposing
better phrase-based models for monolingual word
alignment.
?Performed while faculty at Johns Hopkins University.
1In this paper we use the term token-based alignment for
one-to-one alignment and phrase-based for non one-to-one
alignment, and word alignment in general for both.
Most token-based alignment models can extrin-
sically handle phrase-based alignment to some ex-
tent. For instance, in the case of NYC align-
ing to New York City, the single source word
NYC may align three times separately to the tar-
get words: NYC?New, NYC?York, NYC?City.
Or in the case of identical alignment, New York
City aligning to New York City is simply
New?New, York?York, City?City. How-
ever, it is not as clear how to token-align New York
(as a city) with New York City. The problem is
more prominent when aligning phrasal paraphrases
or multiword expressions, such as pass away and
kick the bucket. This suggests an intrinsi-
cally phrase-based alignment model.
The token aligner jacana-align (Yao et al, 2013a)
has achieved state-of-the-art result on the task of
monolingual alignment, based on previous work of
Blunsom and Cohn (2006). It employs a Conditional
Random Field (Lafferty et al, 2001) to align tokens
from the source sentence to tokens in the target sen-
tence, by treating source tokens as ?observation? and
target tokens as ?hidden states?. However, it is not
designed to handle phrase-based alignment, largely
due to the Markov nature of the underlying model:
a state can only span one token each time, making
it unable to align multiple consecutive tokens (i.e. a
phrase). We extend this model by introducing semi-
Markov states for phrase-based alignment: a state
can instead span multiple consecutive time steps,
thus aligning phrases on the source side. Also, we
merge phrases on the target side to phrasal states,
allowing the model to align phrases on the target
side as well. We evaluate the resulting semi-Markov
590
CRF model on the task of phrase-based alignment,
and then show a basic application in the NLP tasks
of recognizing textual entailment, paraphrase iden-
tification, and question answering sentence ranking.
The final phrase-based aligner is open-source.2
2 Related Work
Most work in monolingual alignment employs de-
pendency tree/graph matching algorithms, includ-
ing tree edit distance (Punyakanok et al, 2004;
Kouylekov and Magnini, 2005; Heilman and Smith,
2010; Yao et al, 2013b), Particle Swarm Optimiza-
tion (Mehdad, 2009), linear regression/classification
models (Chambers et al, 2007; Wang and Manning,
2010), and min-cut (Roth and Frank, 2012). These
works inherently only support token-based align-
ment, with phrase-like alignment achieved by first
merging tokens to phrases as a preprocessing step.
The MANLI aligner (MacCartney et al, 2008)
and its derivations (Thadani and McKeown, 2011;
Thadani et al, 2012) are the first known phrase-
based aligners specifically designed for aligning En-
glish sentence pairs. It applies discriminative per-
ceptron learning with various features and handles
phrase-based alignment of arbitrary phrase lengths.
MANLI suffers from slow decoding time due to its
large search space. This was optimized by Thadani
and McKeown (2011) through Integer Linear Pro-
gramming (ILP), where benefiting from modern ILP
solvers they showed an order-of-magnitude speedup
in decoding. Also, various syntactic constraints can
be easily added, significantly improving exact align-
ment match rate for whole sentence pairs. Besides
the common application of textual entailment and
question answering, monolingual alignment has also
been applied in the field of text generation (Barzilay
and Lee, 2003; Pang et al, 2003).
Word alignment has been more explored in ma-
chine translation. The IBM models (Brown et al,
1993) allow many-to-one alignment and are essen-
tially asymmetric. Phrase-based MT historically
relied on heuristics (Koehn, 2010) to merge two
sets of word alignment in opposite directions to
yield phrasal alignment. Later, researchers explored
non-heuristic phrase-based methods. Among them,
Marcu and Wong (2002) described a joint proba-
2http://code.google.com/p/jacana/
bility model that generates both the source and tar-
get sentences simultaneously. All possible pairs of
phrases in both sentences are enumerated and then
pruned with statistical evidence. Deng and Byrne
(2008) explored token-to-phrase alignment based
on HMM models (Vogel et al, 1996) by explic-
itly modeling the token-to-phrase probability and
phrase lengths. However, the token-to-phrase align-
ment is only in one direction: each target state still
only spans one source word, and thus alignment on
the source side is limited to tokens. Andre?s-Ferrer
and Juan (2009) extended the HMM-based method
to Hidden Semi-Markov Models (HSMM) (Osten-
dorf et al, 1996), allowing phrasal alignments on
the source side. Finally, Bansal et al (2011) unified
the HSMM models with the alignment by agreement
framework (Liang et al, 2006), achieving phrasal
alignment that agreed in both directions.
Despite successful usage of generative semi-
Markov models in bilingual alignment, this has not
been followed with models in discriminative mono-
lingual alignment. Essentially monolingual align-
ment would benefit more from discriminative mod-
els with various feature extractions (just like those
defined in MANLI) than generative models without
any predefined feature (just like how they were used
in bilingual alignment). To combine the strengths of
both semi-Markov models and discriminative train-
ing, we propose to use the semi-Markov Conditional
Random Field (Sarawagi and Cohen, 2004), which
was first used in information extraction to tag con-
tinuous segments of input sequences and outper-
formed conventional CRFs in the task of named en-
tity recognition. We describe this model in the fol-
lowing section.
3 The Alignment Model
Our objective is to define a model that supports
phrase-based alignment of arbitrary phrase length.
In this section we first describe a regular CRF
model that supports one-to-one token-based align-
ment (Blunsom and Cohn, 2006; Yao et al, 2013a),
then extend it to phrase-based alignment with the
semi-Markov model.
591
3.1 Token-based Model
Given a source sentence s of length M , and a target
sentence t of lengthN , the alignment from s to t is a
sequence of target word indices a, where ai?[1,M ] ?
[0, N ]. We specify that when ai = 0, source word si
is aligned to a NULL state, i.e., deleted. This models
a many-to-one alignment from source to target: mul-
tiple source words can be aligned to the same target
word, but not vice versa. One-to-many alignment
can be obtained by running the aligner in the other
direction. The probability of alignment sequence a
conditioned on both s and t is then:
p(a | s, t) =
exp(
?
i,k ?kfk(ai?1, ai, s, t))
Z(s, t)
This assumes a first-order Conditional Random
Field (Lafferty et al, 2001). Since the word align-
ment task is evaluated over F1, instead of directly
optimizing it, we choose a much easier objective
(Gimpel and Smith, 2010) and add a cost function
to the normalizing function Z(s, t) in the denomi-
nator:
Z(s, t) =
?
a?
exp(
?
i,k
?kfk(a?i?1, a?i, s, t)
+cost(ay, a?))
where ay is the true alignments. cost(ay, a?) can
be viewed as special ?features? that encourage de-
coding to be consistent with true labels. It is only
computed during training in the denominator be-
cause in the numerator cost(ay,ay) = 0. Ham-
ming cost is used in practice without learning the
weights (i.e., uniform weights). The more inconsis-
tence there is between ay and a?, the more penalized
is the decoding sequence a? through the cost func-
tion.
3.2 Phrase-based Model
The token-based model supports 1 : 1 alignment.
We first extend it in the direction of ls : 1, where
a target state spans ls words on the source side (ls
source words align to 1 target word). Then we ex-
tend it in the direction of 1 : lt, where lt is the tar-
get phrase length a source word aligns to (1 source
word aligns to lt target words). The final combined
shops areShops closed up for now until March
NULL
closed
temp.
are
Shops
down
shops-are
...-... 7..14
0
1
2
3
4
5
6
closed-down 15
Figure 1: A semi-Markov phrase-based model
example and the desired Viterbi decoding path.
Shaded horizontal circles represent the source
sentence (Shops are closed up for now
until March) and hollow vertical circles repre-
sent the hidden states with state IDs for the target
sentence (Shops are temporarily closed
down). State 0, a NULL state, is designated for dele-
tion. One state (e.g. state 3 and 15) can span multi-
ple consecutive source words (a semi-Markov prop-
erty) for aligning phrases on the source side. States
with an ID larger than the target sentence length
indicate ?phrasal states? (states 6-15 in this exam-
ple), where consecutive target tokens are merged for
aligning phrases on the target side. Combining the
semi-Markov property and phrasal states yields for
instance, a 2?2 alignment between closed up in
the source and closed down in the target.
model supports ls : lt alignment. Throughout this
section we use Figure 1 as an illustrative example,
which shows phrasal alignment between the source
sentence: (Shops are closed up for now
until March) and the target sentence: (Shops
are temporarily closed down).
1 : 1 alignment is a special case of ls : 1 align-
ment where the target side state spans ls = 1 source
word, i.e., at each time step i, the source side word
592
si aligns to one state ai and the next aligned state
ai+1 only depends on the current state ai. This is
the Markovian property of the CRF. When ls > 1,
during the time frame [i, i + ls), all source words
[ai, ai+ls) share the same state ai. Or in other words,
the state ai ?spans? the following ls time steps. The
Markovian property still holds ?outside? the time
frame ls, i.e., ai+ls still only depends on ai, the pre-
vious state ls time steps ago. But ?within? the time
frame ls, the Markovian property does not hold any
more: [ai, ..., ai+ls?1] are essentially the same state
ai. This is the semi-Markov property . States can be
distinguished by this property into two types: semi-
Markovian states and Markovian states.
We have generalized the regular CRF to a semi-
Markov CRF. Now we define it by generalizing the
feature function:
p(a | s, t) =
exp(
?
i,k,ls ?kfk(ai?ls , ai, s, t))
Z(s, t)
At time i, the k-th feature function fk mainly
extracts features from the pair of source words
(si?ls , ..., si] and target word tai (still with a spe-
cial case that ai = 0 marks for deletion). Inference
is still Viterbi-like: except for the fact during maxi-
mization, the Viterbi algorithm not only checks the
previous one time step, but all ls time steps. Sup-
pose the allowed maximal source phrase length is
Ls, define Vi(a | s, t) as the highest score along the
decoding path until time i ending with state a:
Vi(a | s, t) = max
a1,a2,...ai?1
p(a1, a2, . . . , ai = a | s, t)
then the recursive maximization is:
Vi(a | s, t) = max
a?
max
ls=1...Ls
[Vi?ls(a
?
| s, t)
+?i(a
?
, a, ls, s, t)]
with factor:
?i(a
?
, a, ls, s, t) =
?
k
?kfk(a
?
i?ls , ai, s, t)
and the best alignment a can be obtained by back-
tracking the last state aM from VM (aM | s, t).
Training a semi-Markov CRF is very similar to
the inference, except for replacing maximization
with summation. The forward-backward algorithm
should also be used to dynamically compute the nor-
malization function Z(s, t). Compared to regular
CRFs, a semi-Markov CRF has a decoding time
complexity of O(LsMN2), a constant factor Ls
(usually 3 or 4) slower.
To extend from 1 : 1 alignment to 1 : lt alignment
with one source word aligning to lt target words,
we simply explode the state space by Lt times with
Lt the maximal allowed target phrase length. Thus
the states can be represented as an N ? Lt ma-
trix. The state at (j, lt) represents the target phrase
[tj , ..., tj+lt). In this paper we distinguish states by
three types: NULL state (j = 0, lt = 0), token state
(lt = 1) and phrasal state (lt > 1).
To efficiently store and compute these states, we
linearize the two dimensional matrix with a linear
function mapping uniquely between the state ID and
the target phrase offset/span. Suppose the target
phrase tj of length ltj ? [1, Lt] holds a position
ptj ? [1, N ], and the source word si is aligned to
this state (ptj , ltj ), a tuple for (position, span). Then
state ID asi is computed as:
asi(ptj , ltj ) =
{
ptj ltj = 1
N + (ptj ? 1)? Lt + ltj 1 < ltj ? Lt
Assume in Figure 1, Lt = 2, then the state ID for
the phrasal state (5, 2) closed-down with ptj = 5
for the position of word down and ltj = 2 for the
span of 2 words (looking ?backward? from the word
down) is: 5 + (5? 1)? 2 + 2 = 15.
Similarly, given a state id asi , the original target
phrase position and length can be recovered through
integer division and modulation. Thus during decod-
ing, if one output state is 15, we would know that it
uniquely comes from the phrasal state (5,2), repre-
senting the target phrase closed down.
This two dimensional definition of state space ex-
pands the number of states from 1 + N to 1 +
LtN . Thus the decoding complexity becomes
O(M(LtN)2) = O(L2tMN
2) with a usual value
of 3 or 4 for Lt.
Now we have defined separately the ls : 1 model
and the 1 : lt model. We can simply merge them to
593
have an ls : lt alignment model. The semi-Markov
property makes it possible for any target states to
align phrases on the source side, while the two di-
mensional state mapping makes it possible for any
source words to align phrases on the target side. For
instance, in Figure 1, the phrasal state a15 repre-
sents the two-word phrase closed down on the
target side, while still spanning for two words on the
source side, allowing a 2? 2 alignment. State a15 is
phrasal, and at source word position 3 and 4 (span-
ning closed up) it is semi-Markovian. The final
decoding complexity is O(LsL2tMN
2), a factor of
30 ? 60 times slower than the token-based model
(with a typical value of 3 or 4 for Ls and Lt).
In the following we describe features.
3.3 Feature Design
We reused features in the original token-based
model based on string similarity, POS tags, position,
WordNet, distortion and context. Then we used an
additional chunker to mark phrase boundaries only
for feature extraction:
Chunking Features are binary indicators of
whether the phrase types of two phrases match.
Also, we added indicators for mappings between
source phrase types and target phrase types, such as
?vp2np?, meaning that a verb phrase in the source is
mapped to a noun phrase in the target.
Moreover, we introduced the following lexical
features:
PPDB Features (Ganitkevitch et al, 2013) in-
clude various similarity scores derived from a para-
phrase database with 73 million phrasal and 8 mil-
lion lexical paraphrases. Various paraphrase condi-
tional probability was employed. For instance, for
the ADJP/VP phrase pair capable of and able
to, there are the following minus-log probabilities:
p(lhs|e1) = 0.1, p(lhs|e2) = 0.3, p(e1|lhs) = 5.0
p(e1|e2) = 1.3, p(e2|lhs) = 6.7, p(e2|e1) = 2.8
p(e1|e2, lhs) = 0.6, p(e2|e1, lhs) = 2.3
where e1/e2 are the phrase pair, and lhs is the
left hand side syntactic non-terminal symbol. We
did not use the syntactic part (e.g., the NP of
NNS ? the NNS of NP) of PPDB as we did not
make the assumption that the input sentence pairs
were well-formed (and newswire-like) English, or
even of a language with a parser available. Also, for
phrasal alignments, we ruled out those paraphrases
spanning multiple syntactic structures, or of differ-
ent syntactic structures (indicated as [X] in PPDB),
for instance, and crazy? , mad.
Semantic Relatedness Feature is a single scaled
number in [0, 1] from the best performing system
(Han et al, 2013) of the *Sem 2013 Semantic Tex-
tual Similarity (STS) task. We included this fea-
ture mainly to deal with cases where ?related? words
cannot be well measured by either paraphrases or
distributional similarities. For instance, in one align-
ment dataset annotators aligned married with
wife. Adding a few other words as comparison, the
Han et al (2013) system gives the following similar-
ity scores:
married/wife: 0.85
married/husband: 0.84
married/child: 0.10
married/stone: 0.01
Name Phylogeny Feature (Andrews et al, 2012)
is a similarity feature with a string transducer to
model how one name evolves to another. Examples
below show how similar is the name Bill associ-
ated with other names in log probability:
Bill/Bill: -0.8
Bill/Billy: -5.2
Bill/William: -13.6
Bill/Mary: -18.6
Finally, one decision we made during feature
design was not to use any parsing-based features,
with a permissive assumption that the input might
not be well-formed English, or even not complete
sentences (such as fragmented snippets from web
search). The ?deepest? linguistic processing stays at
the level of tagging and chunking, making the model
more easily extendable to other languages.
3.4 Feature Value
In this phrase-based model, the width of a state span
over the source words depends on the competition
between features fired on the phrases as a whole vs.
the consecutive but individual tokens. We found it
critical to assign feature values ?fairly? among to-
kens and phrases to make sure that semi-Markov
states and phrasal states fire up often enough for
phrasal alignments.
594
train test length %align.
MSR06 800 800 29/11 36%
Edinburgh++ 715 305 22/22 78%
Table 1: Statistics of the two manually aligned cor-
pora, divided into training and test in sentence pairs.
The length column shows average lengths of source
and target sentences in a pair. %align. is the per-
centage of aligned tokens.
To illustrate this in a simplified way, take
closed up?closed down in Figure 1, and as-
sume the only feature is the normalized number of
matching tokens in the pair. Then this feature firing
on the following pairs would have values (the nor-
malization factor is the maximal phrase length):
closed?closed 1.0
closed up?closed 0.5
closed up?up 0.5
closed up?closed down 0.5
...?... ...
The desired alignment closed up?closed
down would not have survived the state com-
petition due to its weak feature value. In this
case the model would simply prefer a token align-
ment closed?closed and up?... (probably
NULL).
Thus we upweighted feature values by the max-
imum source or target phrase length to encour-
age phrasal alignments, in this case closed up
?closed down:1.0. Then this alignment would
have a better chance to be picked out with additional
features, such as with the PPDB and Semantic Relat-
edness Features, which are also upweighted by max-
imum phrase lengths.
4 Experiment
4.1 Data Preparation
There are two annotated datasets for training and
testing. MSR063 (Brockett, 2007) has annotated
alignments on the 2006 PASCAL RTE2 develop-
ment and test corpora, with 1600 pairs in total.
3http://www.cs.biu.ac.il/?nlp/files/RTE_
2006_Aligned.zip
1x1 1x2 1x3 2x2 2x3 3x3 more
MSR06 89.2 1.9 0.3 5.7 0.0 1.9 0.8
EDB++ 81.9 3.5 0.8 8.3 0.4 3.0 2.1
Table 2: Percentage of various alignment sizes
(undirectional, e.g., 1x2 and 2x1 are merged) af-
ter synthesizing phrasal alignment from token align-
ment in the training portion of two corpora.
Semantically equivalent words and phrases in the
premise and hypothesis sentences are aligned in a
manner analogous to alignments in statistical ma-
chine translation. This dataset is asymmetric: on
average the premises contain 29 words and the hy-
potheses 11 words. Edinburgh++4 (Thadani et al,
2012) is a revised version of the Edinburgh para-
phrase corpus(Cohn et al, 2008) with sentences
from the following resources: 1. the Multiple-
Translation Chinese corpus; 2. Jules Verne?s novel
Twenty Thousand Leagues Under the Sea. 3. the
Microsoft Research paraphrase corpus (Dolan et al,
2004). The corpus is more balanced and symmetric:
the source and target sentences are both 22 words
long on average. Table 1 shows some statistics.
Both corpora contain mostly token-based align-
ment. For MSR06, MacCartney et al (2008) showed
that setting the allowable phrase size to be greater
than one only increased F1 by 0.2%. For Ed-
inburgh++, the annotation guideline5 explicitly in-
structs to ?prefer smaller alignments whenever pos-
sible?. Statistics shows that single token alignment
counts 96% and 95% of total alignments in these two
corpora separately. With such a heavy imbalance to-
wards only token-based alignment, a phrase-based
aligner would learn feature weights that award token
alignments more than phrasal alignments.
Thus we synthesized phrasal alignments from
continuous monotonic token alignments in these two
corpora. We first ran the OpenNLP chunker through
the corpora. Then for each phrase pair, if each token
in the source phrase is aligned to a token in the tar-
get phrase in a monotonic way, and vice versa, we
4http://www.ling.ohio-state.edu/?scott/
#edinburgh-plusplus
5http://staffwww.dcs.shef.ac.uk/people/
T.Cohn/paraphrase_guidelines.pdf
595
merge these alignments to form one single phrasal
alignment.6 Table 2 lists the percentage of vari-
ous alignment sizes after the merge. Two obser-
vations can be made: first, the portion of phrasal
alignments increases to 10% ? 20% after merging;
second, allowing a maximal phrase length of 3 cov-
ers 98% ? 99% of total alignments, thus a phrase
length larger than 3 would be a bad trade-off for cov-
erage vs speed.
4.2 Baselines and Evaluation Metrics
MacCartney et al (2008) and Yao et al (2013a)
showed that the traditional MT bilingual aligner
GIZA++ (Och and Ney, 2003) presented weak re-
sults on the task of monolingual alignment. Thus
we instead used four other strong baselines:
Meteor (Denkowski and Lavie, 2011): a sys-
tem for evaluating machine translation by aligning
MT output with reference sentences. It is designed
for the task of monolingual alignment and supports
phrasal alignment. We used version 1.4 and default
weights to optimize by maximum accuracy.
MANLI-constraint (Thadani and McKeown,
2011): a re-implemented MANLI system with ILP-
powered decoding for speed and hard syntactic con-
straints to boost exact match rate, with reported
numbers on MSR06.
MANLI-joint (Thadani et al, 2012): an im-
proved version of MANLI-constraint that not only
models phrasal alignments, but also alignments be-
tween dependency arcs, with reported numbers on
the original Edinburgh paraphrase corpus.
jacana-token (Yao et al, 2013a): a token-
based aligner with state-of-the-art performance on
MSR06.
Note that the jacana-token aligner is open-source,
so we were able to re-train it with exactly the
same feature set used by our phrase-based model.
This allows a fair comparison of model performance
(token-based vs. phrase-based). The MANLI* sys-
tems are not available, thus we only reported their
numbers from published papers.
The standard evaluation metrics for alignments
are precision (P), recall (R), F1, and exact matching
6a few examples: two Atlanta-based
companies?two Atlanta companies, the
UK?the UK, the 17-year-old?the teenager,
was held?was held.
rate (E) based on either tokens (two tokens are con-
sidered aligned iff they are aligned) or phrases (two
tokens are considered aligned iff they are contained
within phrases that are aligned). Following Thadani
et al (2012), we only report the results based on
token alignments (which allows a partial credit if
their containing phrases are not aligned), even for
the phrase-based alignment task. The reasoning is
that if a phrase-based aligner is already doing bet-
ter than a token aligner in terms of token alignment
scores, then the difference in terms of phrase align-
ment scores will be even larger. Thus showing the
superiority of token alignment scores is sufficient.
4.3 Implementation and Training
The elements in the phrase-based model: dynamic
state indices, semi-Markov and phrasal states, are
not typically found in standard CRF implementa-
tions. Thus we implemented the phrase-based model
in the Scala programming language, which is fully
interoperable with Java, using one semi-Markov
CRF package7 as a reference. We used the L2 reg-
ularizer and LBFGS for optimization. OpenNLP8
provided the POS tagger and chunker and JWNL9
interfaced with WordNet (Fellbaum, 1998).
4.4 Results
Table 3 gives scores (in bigger fonts) of different
aligners on MSR06 and Edinburgh++ and their cor-
responding phrasal versions. Overall, the token-
based aligner did the best on the original corpora, in
which single token alignment counts more than 95%
of total alignment. The phrase-based aligner did
slightly worse. We think the main reason was that it
output more phrasal alignment, which in turn harms
scores in token-based evaluation (for instance, if the
gold alignment is New?New, York?York, then
the phrasal alignment of New York?New York
would only have half the precision because it inher-
ently also aligns New in the source with York in
the target.). Further investigation showed that on the
Edinburgh++ corpus, over-generated phrase-based
alignment, when evaluated under just token align-
ment, contributed hurting about 1.1% of overall F1,
7http://crf.sf.net
8http://opennlp.apache.org/
9http://jwordnet.sf.net/
596
a gap that would make the phrase aligner (85.9%)
outperform the token aligner (86.4%).
On the phrasal alignment corpora (represented by
MSR06P and EDB++P in Table 3), the phrase-based
aligner did significantly better. Note that the over-
all F1 and exact match rate are still much lower
than those scores obtained from the original corpora,
suggesting that the phrasal corpora present a much
harder task. Furthermore, as a more ?fair? com-
parison between the two aligners, we synthesized
phrasal alignments from the output of the token-
based aligner, just as how the phrased-based corpora
were prepared, then evaluated its performance again.
Still, on the EDB++P corpus, the token aligner was
about 1.6% (current difference is 69.1% vs. 72.8%)
worse than the phrase-based aligner.
Also, we want to emphasize that since the token-
based aligner and the phrase-based aligner shared
exactly the same features and lexical resources, the
performance boost of the phrase-based aligner on
the phrasal corpora results from a better model de-
sign: it is the semi-Markov property and phrasal
states making the phrase-based aligner better.
To further investigate the performance of aligners
with respect to different types of alignment, we di-
vided the scores into those for identical alignments
(such as New?New) and non-identical alignments
(such as wife?spouse), indicated by the sub-
scripts i and n in Table 3. In terms of identical
alignment, most aligners were able to score more
than 90%, but for non-identical alignment there was
noticeable decrease. Still, on the phrasal alignment
corpora, the phrase-based model has a much larger
recall score for non-identical alignment than others.
We also divided scores with respect to token-only
alignment and phrase-only alignment. Due to space
limit, we only show results on synthesized Edin-
burgh++, in Table 4. Meteor and the token aligner
inherently have either very limited or no support for
phrasal alignment, thus they had very low scores
on phrase-only alignment. We then ran the align-
ers in two directions and merged the results with the
?union? MT heuristic to get better phrase support.
But that still did not bring F1p?s up to over 5%.
The phrase-based aligner baseline Meteor did
worse than our aligners. We think there are two rea-
sons: First, Meteor was not trained on these corpora.
Second, Meteor only does strict word, stem, syn-
System
P% R% F1%
E%
Pi/Pn Ri/Rn F1i/F1n
M
S
R
06
(7
8.
6%
) Meteor
82.5 81.2 81.9
15.0
89.9/39.9 97.3/24.6 93.5/30.5
MANLI-cons. 89.5 86.2 87.8 33.0
token
93.6 83.5 88.3
32.1
96.6/77.7 96.9/35.6 96.8/48.8
phrase
92.1 82.8 86.8
29.1
95.7/65.0 95.9/34.7 95.8/45.2
M
S
R
06
P
(5
9.
0%
) Meteor
82.5 68.3 74.7
7.3
89.9/40.1 97.3/8.8 93.5/14.5
token
92.9 66.1 77.2
13.5
95.5/77.5 94.3/11.1 94.9/19.5
phrase
83.5 77.0 80.1
14.3
94.9/55.5 94.2/48.1 94.5/51.5
E
D
B
+
+
(7
5.
2%
) Meteor
88.3 80.5 84.2
12.7
94.0/61.4 97.8/24.1 95.9/34.7
MANLI-jnt* 76.6 83.8 79.2 12.2
token
91.3 82.0 86.4
15.0
96.4/63.9 97.4/36.4 96.9/46.4
phrase
90.4 81.9 85.9
13.7
96.0/57.4 97.8/38.3 96.9/46.0
E
D
B
+
+
P
(5
1.
7%
) Meteor
88.4 60.6 71.9
2.9
94.0/61.9 97.0/6.5 95.5/11.7
token
90.7 55.8 69.1
2.3
96.2/58.6 91.3/7.1 93.7/12.7
phrase
82.3 65.3 72.8
1.6
95.6/60.4 93.1/34.3 94.4/43.8
Table 3: Results on original (mostly token) and phrasal
(P) alignment corpora, where (x%) indicates how much
alignment is identical alignment, such as New?New. E%
stands for exact (perfect) match rate. Subscript i stands
for corresponding scores for ?identical? alignment and n
for ?non-identical?. *: scores of MANLI-joint were for
the original Edinburgh corpus instead of Edinburgh++
(with hand corrections) so it is not a direct comparison.
onym and paraphrase matching but does not use any
string similarity measures; this can be supported by
the large difference between, for instance, F1i and
F1n. In general Meteor did well on identical align-
ment, but not so well on non-identical alignment.
5 Applications
Natural language alignment can be applied to vari-
ous NLP tasks. While how to most effectively apply
597
System
P% R% F1%
E%
Pt/Pp Rt/Rp F1t/F1p
E
D
B
+
+
P
Meteor
88.4 60.6 71.9
2.9
59.5/14.9 90.6/1.1 71.8/2.0
token
90.7 55.8 69.1
2.3
59.4/21.4 85.5/0.9 70.1/1.7
phrase
82.3 65.3 72.8
1.6
73.3/48.0 73.5/44.2 73.4/46.0
Table 4: Same results on the phrasal Edinburgh++ cor-
pus but with scores divided by token-only alignment
(subscript t) and phrase-only alignment (subscript p).
it is another topic, we simply show in this section us-
ing just alignment scores in binary prediction prob-
lems. Specifically, we pick the tasks of recognizing
textual entailment (RTE), paraphrase identification
(PP), and question answering sentence ranking (QA)
described in Heilman and Smith (2010):
RTE: predicting whether a hypothesis can be in-
ferred from the premise, with training data from
RTE-1/2 and RTE-3 dev, and test from RTE-3 test.
PP: predicting whether two sentences are para-
phrases, with training and test data from the MSR
Paraphrase Corpus (Dolan et al, 2004).
QA: predicting whether a sentence contains the
answer to the question, with training data from
TREC-8 to TREC-12 and test data from TREC-13.
For each aligned pair, we can compute a normal-
ized decoding score. Following MacCartney et al
(2008), we select a threshold score and predict true
if the decoding score is above this threshold. For the
tasks of RTE and PP, we tuned this threshold w.r.t
the maximal accuracy on the training set, then re-
ported performance on the test set. For the task of
QA, since the evaluation methods in Mean Average
Precision and Mean Reciprocal Rank only need a
ranked list of answer sentences, and the scores on
the test set are sufficient to provide the ranking, we
did not tune anything on training but instead directly
ran the aligner on the test set. All three tasks shared
the same aligner model trained on the superset of
MSR06 and Edinburgh++. Results are reported in
Table 5. We could not report on Meteor as Meteor
does not explicitly output alignment scores.
We did not expect the aligners to beat any of the
system A% P% R%
de Marneffe et al (2006) 60.5 61.8 60.2
MacCartney and Manning (2008) 64.3 65.5 63.9
Heilman and Smith (2010) 62.8 61.9 71.2
the token aligner 59.1 61.2 55.4
our phrasal aligner 57.6 57.2 68.8
(a) Recognizing Textual Entailment
system A% P% R%
Wan et al (2006) 75.6 77 90
Das and Smith (2009) 73.9 74.9 91.3
Heilman and Smith (2010) 73.2 75.7 87.8
the token aligner 70.0 72.6 88.1
our phrasal aligner 68.1 68.6 95.8
(b) Paraphrase Identification
system MAP MRR
Cui et al (2005) 0.4271 0.5259
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Yao et al (2013b) 0.6307 0.7477
the token aligner 0.5982 0.6582
our phrasal aligner 0.6165 0.7333
(c) Question Answering Sentence Ranking
Table 5: Results (Accuracy, Precision, Recall, Mean
Average Precision, Mean Reciprocal Rank) on the
tasks of RTE, PP and QA.
state-of-the-art result since no sophisticated models
were additionally used but only the alignment score.
Still, the aligners showed competitive performance.
It still follows the pattern from the alignment exper-
iment that the phrasal aligner had higher recall and
lower precision than the token aligner in the task of
RTE and PP. In the QA task, the phrasal aligner per-
formed better than all systems except for the top one.
6 Conclusion
We have introduced a phrase-to-phrase alignment
model based on semi-Markov Conditional Random
Fields. The combination of semi-Markov states and
phrasal states makes phrasal alignment on both the
source and target sides possible. The final phrase-
598
based aligner performed the best on two phrasal
alignment corpora and showed its potential usage
in three NLP tasks. Future work includes aligning
discontinuous (gappy) phrases and integrating align-
ment more closely in NLP applications.
Acknowledgement
We thank Vulcan Inc. for funding this work. We also
thank Jason Smith, Travis Wolfe, Frank Ferraro and
the three anonymous reviewers for their comments
and suggestion.
References
Jesu?s Andre?s-Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine trans-
lation. In Procedings of European Association for
Machine Translation (EAMT), Barcelona, Spain, May.
European Association for Machine Translation.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: a generative model of string
variation. In Proceedings of EMNLP 2012.
Mohit Bansal, Chris Quirk, and Robert Moore. 2011.
Gappy phrasal alignment by agreement. In Proceed-
ings of ACL, Portland, Oregon, June.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL, pages
16?23.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of ACL2006, pages 65?72.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical report, Microsoft Research.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational linguistics, 19(2):263?311.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
Christopher D Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 165?170.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597?614, December.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of the 28th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, SIGIR ?05, pages 400?407, New York, NY,
USA. ACM.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 468?476, Suntec,
Singapore, August. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D Manning. 2006. Learning to distinguish valid
textual entailments. In Second Pascal RTE Challenge
Workshop.
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. Audio, Speech, and Language Processing, IEEE
Transactions on, 16(3):494?507.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of COLING, Stroudsburg, PA, USA.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL-HLT, pages 758?
764.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin CRFs: training log-linear models with cost
functions. In NAACL 2010, pages 733?736.
Lushan Han, Abhay Kashyap, Tim Finin, James May-
field, and Jonathan Weese. 2013. UMBC-EBIQUITY-
CORE: Semantic Textual Similarity Systems. In Pro-
ceedings of the Second Joint Conference on Lexical
and Computational Semantics.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia, June.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Milen Kouylekov and Bernardo Magnini. 2005. Recog-
nizing textual entailment with tree edit distance algo-
rithms. In PASCAL Challenges on RTE, pages 17?20.
599
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL.
Bill MacCartney and Christopher D Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of ACL 2008,
pages 521?528.
Bill MacCartney, Michel Galley, and Christopher D Man-
ning. 2008. A phrase-based alignment model for nat-
ural language inference. In Proceedings of EMNLP,
pages 802?811.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP-2002, pages 133?139.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 289?292.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Mari Ostendorf, Vassilios V Digalakis, and Owen A Kim-
ball. 1996. From HMM?s to segment models: a uni-
fied view of stochastic modeling for speech recogni-
tion. IEEE Transactions on Speech and Audio Pro-
cessing, 4(5):360?378.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of NAACL, pages 102?109.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answering. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
Michael Roth and Anette Frank. 2012. Aligning
predicates across monolingual comparable texts using
graph-based clustering. In Proceedings of EMNLP-
CoNLL, pages 171?182, Jeju Island, Korea, July.
Sarawagi Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Advances in Neural Information Processing
Systems, 17:1185?1192.
Kapil Thadani and Kathleen McKeown. 2011. Optimal
and syntactically-informed decoding for monolingual
phrase-based alignment. In Proceedings of ACL short.
Kapil Thadani, Scott Martin, and Michael White. 2012.
A joint phrasal and dependency model for paraphrase
alignment. In Proceedings of COLING 2012: Posters,
pages 1229?1238, Mumbai, India, December. The
COLING 2012 Organizing Committee.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics - Volume 2, COLING ?96, pages
836?841.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2006. Using dependency-based features to take the
?para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of COLING, pages 1164?1172,
Stroudsburg, PA, USA.
Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.
2007. What is the Jeopardy Model? A Quasi-
Synchronous Grammar for QA. In Proceedings of
EMNLP-CoNLL, pages 22?32, Prague, Czech Repub-
lic, June.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013a. A Lightweight and
High Performance Monolingual Word Aligner. In
Proceedings of ACL 2013 short, Sofia, Bulgaria.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013b. Answer Extraction as
Sequence Tagging with Tree Edit Distance. In Pro-
ceedings of NAACL 2013.
600
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1643?1654,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Open Domain Targeted Sentiment
Margaret Mitchell Jacqueline Aguilar Theresa Wilson Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
{m.mitchell,jacqui.aguilar}@jhu.edu, Theresa.Wilson@oberlin.edu, vandurme@cs.jhu.edu
Abstract
We propose a novel approach to sentiment
analysis for a low resource setting. The in-
tuition behind this work is that sentiment
expressed towards an entity, targeted senti-
ment, may be viewed as a span of sentiment
expressed across the entity. This represen-
tation allows us to model sentiment detec-
tion as a sequence tagging problem, jointly
discovering people and organizations along
with whether there is sentiment directed to-
wards them. We compare performance in
both Spanish and English on microblog data,
using only a sentiment lexicon as an exter-
nal resource. By leveraging linguistically-
informed features within conditional random
fields (CRFs) trained to minimize empiri-
cal risk, our best models in Spanish signifi-
cantly outperform a strong baseline, and reach
around 90% accuracy on the combined task of
named entity recognition and sentiment pre-
diction. Our models in English, trained on a
much smaller dataset, are not yet statistically
significant against their baselines.
1 Introduction
Sentiment analysis is a multi-faceted problem. De-
termining when a positive or negative sentiment is
being expressed is a large part of the challenge, but
identifying other attributes, such as the target of the
sentiment, is also crucial if the ultimate goal is to
pinpoint and extract opinions. Consider the exam-
ples below, all of which contain a positive sentiment:
(1) So happy that Kentucky lost to Tennessee!
(2) Kentucky versus Kansas I can hardly wait...
(3) Kentucky is the best alley-oop throwing team
since Sherman Douglas? Syracuse squads!!
The entities in these examples are college basket-
ball teams, and the events referred to are games. In
(1), although there is a positive sentiment, the tar-
get of the sentiment is an event (Kentucky losing to
Tennessee). However, from the positive sentiment
toward this event, we can infer that the speaker has
a negative sentiment toward Kentucky and a positive
sentiment toward Tennessee. In (2), the positive sen-
timent is toward a future event, but we are not given
enough information to infer a sentiment toward the
mentioned entities. In (3), Kentucky is the direct
target of the positive sentiment. We can also in-
fer a positive sentiment toward Douglas?s Syracuse
teams, and even toward Douglas himself.
These examples illustrate the importance of the
target when interpreting sentiment in context. If we
are looking for sentiments toward Kentucky, for ex-
ample, we would want to identify (1) as negative, (2)
as neutral (no sentiment) and (3) as positive. How-
ever, if we are looking for sentiment toward Ten-
nessee, we would want to identify (1) as positive,
and (2) and (3) as neutral.
The expression of these and other kinds of sen-
timent can be understood as involving three items:
(1) An experiencer
(2) An attitude
(3) A target (optionally)
Research in sentiment analysis often focuses on (2),
predicting overall sentiment polarity (Agarwal et al,
2011; Bora, 2012). Recent work has begun to com-
bine (2) with (3), examining how to automatically
predict the sentiment polarity expressed towards a
target entity (Jiang et al, 2011; Chen et al, 2012)
for a fixed set of targets. This topic-dependent sen-
timent classification requires that the target entity be
1643
Figure 1: Sentiment expressed across an entity.
given, and returns statements expressing sentiment
towards the given entity.
In this paper, we take a step towards open-domain,
targeted sentiment analysis by investigating how to
detect both the named entity and the sentiment ex-
pressed toward it. We observe that sentiment ex-
pressed towards a target entity may be possible to
learn in a graphical model along the span of the en-
tity itself: Similar to how named entity recognition
(NER) learns labels along the span of each word in
an entity name, sentiment may be expressed along
the entity as well. A small example is shown in Fig-
ure 1. We focus on people and organizations (voli-
tional named entities), which are the primary targets
of sentiment in our microblog data (see Table 1).
Both NER and opinion expression extraction have
achieved impressive results using conditional ran-
dom fields (CRFs) (Lafferty et al, 2001) to define
the conditional probability of entity categories (Mc-
Callum and Li, 2003; Choi et al, 2006; Yang and
Cardie, 2013). We develop such models to jointly
predict the NE and the sentiment expressed towards
it using minimum risk training (Stoyanov and Eis-
ner, 2012). We learn our models on informal Span-
ish and English language taken from the social net-
work Twitter,1 where the language variety makes
NLP particularly challenging (see Figure 2).
Our ultimate goal is to develop models that will
be useful for low resource languages, where a sen-
timent lexicon may be known or bootstrapped, but
more sophisticated linguistic tools may not be read-
ily available. We therefore do not rely on an external
part-of-speech tagger or parser, which are often used
for features in fine-grained sentiment analysis; such
tools are not available in many languages, and if they
are, are not usually adapted for noisy social media.
Instead, we use information from sentiment lex-
icons and some simple hand-written features, and
otherwise use only features of the word that can be
1www.twitter.com
@[user] le dijo erralo muy por lo bajo jaja un grande
juancito grandes amigos mios
@[user] he told him it was very on the dl haha a great
juancito great friends of mine
@[user] buenos d??as Profe!! Nos quedamos acciden-
tados otra vez en la carretera vieja guarenas echando
gasoil, estamos a la interperie
@[user] good morning, Prof!! We were wrecked again
on the old guarenas highway while getting diesel, we?re
out in the open
Sin a?nimo de ofender a los Militares, que realmente
se merecen ese aumento y ma?s. Pero, do?nde queda la
misma recompensa para Me?dicos.
I do not intend to offend the military in the slightest,
they truly deserve the raise and more. However, I?m
wondering whether doctors will ever receive a similar
compensation.
Figure 2: Messages on Twitter use a wide range of
formality, style, and errors, which makes extracting in-
formation particularly difficult. Examples from Spanish
(screen names anonymized), with approximate transla-
tions in English.
extracted without supervision. These include fea-
tures based on unsupervised word tags (Brown clus-
ters) and a method that automatically syllabifies a
word based on the orthography of the language. All
tools and code used for this research are released
with this paper.2
2 Related Work
As the scale of social media has grown, using
sources such as Twitter to mine public sentiment
has become increasingly promising. Commer-
cial systems include Sentiment1403 (products and
brands) and tweetfeel4 (suggests searching for pop-
ular movies, celebrities and companies).
The majority of academic research has focused on
supervised classification of message sentiment irre-
spective of target (Barbosa and Feng, 2010; Pak and
Paroubek, 2010; Bifet and Frank, 2010; Davidov et
al., 2010; Kouloumpis et al, 2011; Agarwal et al,
2011). Large datasets are collected for this work by
leveraging the sentiment inherent in emoticons (e.g.,
smilies and frownies) and/or select Twitter hashtags
(e.g., #bestdayever, #fail), resulting in noisy collec-
2www.m-mitchell.com/code
3www.sentiment140.com
4www.tweetfeel.com
1644
tions appropriate for initial exploration. Prior work
includes: the use of a social network (Speriosu et
al., 2011; Tan et al, 2011; Calais Guerra et al,
2011; Jiang et al, 2011; Li et al, 2012; Hu et
al., 2013); user-adapted models based on collabo-
rative online-learning (Li et al, 2010b); unsuper-
vised, joint sentiment-topic modeling (Saif et al,
2012); tracking changing sentiment during debates
(Diakopoulos and Shamma, 2010); and how ortho-
graphic conventions such as word-lengthening can
be used to adapt a Twitter-specific sentiment lexicon
(Brody and Diakopoulos, 2011).
Efforts in targeted sentiment (Bermingham and
Smeaton, 2010; Jin and Ho, 2009; Li et al, 2010a;
Jiang et al, 2011; Tan et al, 2011; Wang et al,
2011; Li et al, 2012; Chen et al, 2012), have mostly
focused on topic-dependent analysis. In these ap-
proaches, messages are collected on a fixed set of
topics/targets, such as products or sports teams, and
sentiment is learned for the given set. In contrast,
we aim to predict sentiment in tweets for any named
person or organization. We refer to this task as open
domain targeted sentiment analysis.
Within topic-dependent sentiment analysis, sev-
eral approaches have explored applying CRFs or
HMMs to extract sentiment and target words from
text (Jin and Ho, 2009; Li et al, 2010a). In these
approaches, opinion expressions are extracted, and
polarity is annotated across the opinion expression.
However, as noted by many researchers in senti-
ment, opinion orientation towards a specific target
is often not equal to the orientation of a neighbor-
ing opinion expression; and opinion expressions in
one context may not be opinion expressions in an-
other (Kim and Hovy, 2006), making open domain
approaches particularly challenging.
The above work by Jiang et al (2011) is most
similar to our own. They do not use joint learning,
but they do incorporate a number of parse-based fea-
tures designed to capture relationships between sen-
timent terms and topic references. In our work these
relationships are captured by the CRF model, and
we compare against their approach in Section 6.
Recent work by Yang and Cardie (2013) is sim-
ilar in spirit to our own, where the identification
of opinion holders, opinion targets, and opinion ex-
pressions is modeled as a sequence tagging problem
using a CRF. However, similar to previous work ap-
plying CRFs to extract sentiment, Yang and Cardie
use syntactic relations to connect an opinion target
to an opinion expression. In contrast, we model
the expression of sentiment polarity across the senti-
ment target itself, extracting both the sentiment tar-
get and the sentiment expressed towards it within the
same span of words. This allows us to use surround-
ing context to determine sentiment polarity without
identifying explicit opinion expressions or relying
on a parser to help link expression to target.
Most work in targeted sentiment outside the mi-
croblogging domain has been in relation to prod-
uct review mining (e.g., Yi et al (2003), Hu and
Liu (2004), Popescu and Etzioni (2005), Qiu et al
(2011)). Rather than identify named entities (NEs),
this work seeks to identify products and their fea-
tures mentioned in reviews, and classify these for
sentiment. Recent work by Qui et al jointly learns
targets and opinion words, and Jakob and Gurevych
(2010) use CRFs to extract the targets of opinions,
but do not attempt to classify the sentiment toward
these targets. To the best of our knowledge, this is
the first work to approach targeted sentiment in a low
resource setting and to jointly predict NEs and tar-
geted sentiment.
3 Data
Twitter Collection We use the Spanish/English
Twitter dataset of Etter et al (2013) to train and test
our models. Approximately 30,000 Spanish tweets
and 10,000 English were labeled for named entities
in BIO encoding: The start of an NE is labeled B-
{NE} and the rest of the NE is labeled I-{NE}. The
NE COUNT NEUTRAL POS NEG
PERSON 5462 80% 20% 0%
ORGANIZATION 4408 80% 20% 0%
LOCATION 1405 100% 0% 0%
URL 1030 100% 0% 0%
TIME 535 70% 10% 20%
DATE 222 100% 0% 0%
MONEY 95 90% 0% 10%
PERCENT 81 80% 20% 0%
TELEPHONE 23 100% 0% 0%
EMAIL 8 100% 0% 0%
Table 1: Distribution of named entities in our Spanish
Twitter corpus. Targeted sentiment percentages are based
on expert annotations from a random sample of 10 (or
all) of of each entity. Most entities are not sentiment tar-
gets (NEUTRAL). PERSON and ORGANIZATION are most
frequent, and among the top recipients of sentiment.
1645
full set of NE categories are shown in Table 1. For
example, the sequence ?Mark Twain? would be la-
beled B-PERSON, I-PERSON. We are interested in both
PERSON and ORGANIZATION entities, which make
up the majority of named entities in this data, and we
evaluate these using the more general entity category
VOLITIONAL. Removing retweets, 7,105 Spanish
tweets contained a total of 9,870 volitional entities
and 2,350 English tweets contained a total of 3,577
volitional entities.
Sentiment Lexicons We use two sentiment lex-
icon sources in each language. For English, we
use the MPQA lexicon (Wilson et al, 2005), which
identifies 12,296 manually and semi-automatically
produced subjective terms along with their polarity.
For the second lexicon, we use SentiWordNet 3.0
(Baccianella et al, 2010), which assigns positive and
negative polarity scores to WordNet synsets. We use
the majority polarity of all words with a subjectivity
score above 0.5.
For Spanish, the first lexicon is obtained from
Volkova et al (2013), who automatically trans-
lated strongly subjective terms from the MPQA lex-
icon (Wilson et al, 2005) into Spanish. The re-
sulting Spanish lexicon contains about 65K words.
The second lexicon is available from Perez-Rosas
et al (2012). This contains approximately 1000
sentiment-bearing words collected leveraging man-
ual resources and 2000 collected leveraging auto-
matic resources.
Annotation To collect sentiment labels, we
use crowdsourcing through Amazon?s Mechanical
Turk.5 Annotators (?Turkers?) were shown six
tweets at a time, each with a single highlighted
named entity. Turkers were instructed to (1) se-
lect the sentiment being expressed towards the en-
tity (positive, negative, or no sentiment); and (2)
rate their level of confidence in their selection. Fol-
lowing best practices on collecting language data
with Mechanical Turk (Callison-Burch and Dredze,
2010), two controls were placed among each set of
six tweets to screen out unreliable judgments. An
example prompt is shown in Figure 3.
Each ?tweet, NE? pair was shown to three Turk-
ers, and those with majority consensus on sentiment
polarity were extracted. Tweets without sentiment
5www.mturk.com/mturk
ORGANIZATION PERSON
Named Entity
Freq
uenc
y in T
weet
s
0
500
1000
1500
2000
2500
Positive
Negative
Neutral
Figure 4: Targeted sentiment annotated for Spanish.
Majority
POS NEUTRAL NEG
M
in
or
it
y POS 757 1249 130
NEUTRAL 707 2151 473
NEG 129 726 452
Table 2: Number of targeted sentiment instances where
at least two of the three annotators (Majority) agreed.
Common disagreements with a third annotator (Minority)
were over whether no sentiment or positive sentiment was
expressed, and whether no sentiment or negative sent-
ment was expressed.
consensus on all NEs were removed. In Spanish, this
yielded 6,658 unique ?tweet, NE? pairs. In English,
which is a smaller data set, this yielded 3,288 unique
pairs. We split the data into folds for 10-fold cross-
validation, developing on the data from one fold and
reporting results for the remaining nine.
The distribution of sentiment for the named en-
tities annotated by Turkers is shown in Figure 4.
Neutral (no targeted sentiment) dominates, followed
by positive sentiment for both organizations and
people. As shown in Table 2, common disagree-
ments were over whether or not there was targeted
positive sentiment, and whether or not there was
targeted negative sentiment. This is in line with
previous research showing that distinguishing pos-
itive sentiment from no sentiment (and distinguish-
ing negative sentiment from no sentiment) is often
more challenging than distinguishing between pos-
itive and negative sentiment (Wilson et al, 2009).
Indeed, we see that it was more common for annota-
tors to disagree than to agree on targeted sentiment,
particularly for negative targeted sentiment, where
more instances had NEUTRAL/NEGATIVE disagree-
ment than NEGATIVE three-way agreement.
1646
Figure 3: Example Tweet shown to Turkers.
Variable Possible values
Sentiment (s) NOT-TARG, SENT-TARG
(PIPE & JOINT models)
Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL
(PIPE & JOINT models)
Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG
(COLL models) B+SENT-TARG, I+SENT-TARG
Table 3: Possible values for random variables, targeted
subjectivity (is/is not sentiment target). COLL models
collapse targeted subjectivity and NE label into one node.
Variable Possible values
Sentiment (s) NOT-TARG, POS, NEG
(PIPE & JOINT models)
Named Entity (l) O, B-VOLITIONAL, I-VOLITIONAL
(PIPE & JOINT models)
Combined Sent/NE (y) O, B+NOT-TARG, I+NOT-TARG
(COLL models) B+POS, I+POS
B+NEG, I+NEG
Table 4: Possible values for random variables, targeted
sentiment. The COLL models collapse both targeted sen-
timent and NE label into one node.
4 Targeted Subjectivity and Sentiment
Formally, we define the problem as follows: Given
an observed message w = (w1 . . . wn), where n is
the number of words in the message and wj(1 ?
j ? n) is a word, we learn the probability of a
label sequence l = (l1 . . . ln), where li ? the set
of named entity values; and a sentiment sequence
s = (s1 . . . sn), where si ? the set of sentiment val-
ues. We additionally explore simpler linear-chain
models that learn the probability of a single label
sequence y = (y1 . . . yn), where yi ? the set of con-
joined entity+sentiment values (Tables 3 and 4).
Our basic model is a linear conditional random
field, an undirected graph that represents the con-
ditional distribution p(l, s|w).6 Sentiment towards
a named entity may be modeled in a CRF as a se-
6For the COLL models, this is instead the conditional distri-
bution p(y|w), where entity and sentiment labels are conjoined
in one sequence assignment y.
quence of random variables for sentiment s con-
nected to named entities l. In all models, entity vari-
ables are connected by a factor to their neighbors
in sequence, and we include skip-chains (Finkel and
Manning, 2010) connecting identical words where
at least one is capitalized. Our model strategies in-
clude: a pipeline that first learns volitional entities
then sentiment directed towards them (PIPE); one
that jointly learns volitional entities along with sen-
timent directed towards them (JOINT); and one that
learns volitional entities and targeted sentiment with
combined labels (COLL) (Figure 5).
Using these models, we explore two primary
tasks: (1) the task of detecting whether sentiment
is targeted at an entity, which we refer to as targeted
subjectivity; and (2) the task of detecting whether
positive, negative, or neutral sentiment (no senti-
ment) is targeted at an entity, which we refer to as
targeted sentiment. Moving from targeted subjectiv-
ity prediction to targeted sentiment prediction is pos-
sible by changing the sentiment target (SENT-TARG)
variable into two variables, one for positive targeted
sentiment (POS) and one for negative (NEG). Possi-
ble values for targeted subjectivity are shown in Ta-
ble 3, and possible values for targeted sentiment are
shown in Table 4.
In the pipeline models (PIPE), we first build a
CRF where each word is connected by a factor to
an entity label li ? l. In a second model, every ob-
served volitional entity node is connected by a factor
to a sentiment label si ? s. An example is shown in
Figure 5 (1).
In the joint models (JOINT), each si ? s is con-
nected by a factor to the corresponding entity label
in the sequence, li ? l. Sentiment in this model
is partially observed: All sentiment variables are
treated as latent except for the sentiment connected
to the volitional entity. An example is shown in Fig-
ure 5 (2).
1647
In the collapsed models (COLL), we combine sen-
timent and named entity into one label sequence
(e.g., O, B+SENT-TARG, I+SENT-TARG). An example
is shown in Figure 5 (3). The JOINT and PIPE mod-
els therefore predict named entity sequences, their
category labels, and the sentiment expressed towards
volitional named entities.7 The collapsed models
predict volitional labels and targeted sentiment as
combined categories. The COLL and PIPE models
are considerably faster than JOINT models, where
exact inference is intractable.
1. PIPELINE MODEL (PIPE)
Step 1: Volitional Named Step 2: Sentiment
Entity Recognition
2. JOINT MODEL 3. COLLAPSED MODEL
(JOINT) (COLL)
Figure 5: Example CRFs for targeted subjectivity with
observed variables (dark nodes), predicted variables
(white nodes) and hidden variables (light grey nodes).
5 Training
Minimum-Risk CRF Training We use the
ERMA system (Stoyanov et al, 2011) to learn our
models.8 ERMA (Empirical Risk Minimization un-
der Approximations) learns parameters to minimize
loss on the training data. Predicting NE labels using
a linear-chain CRF trained with empirical risk mini-
mization has been shown to result in a statistically
significant improvement over the common approach
of maximum likelihood estimation (Stoyanov and
Eisner, 2012). All models are trained to optimize
7We found that learning the VOLITIONAL categories dur-
ing training rather than maintaining beliefs about separate
named entities during inference (ORGANIZATION, PERSON)
and then post-processing to VOLITIONAL leads to slightly bet-
ter accuracy.
8sites.google.com/site/ermasoftware
log likelihood using 20 iterations of stochastic
gradient descent, and a maximum of 100 iterations
of belief propagation to compute the marginals for
each example.
Features Features of the models are shown in Ta-
ble 5. For an observed word, features are extracted
for the word itself as well as within a context win-
dow of three words in either direction. Words seen
only once are treated as out-of-vocabulary. Surface
features and linguistic features are concatenated in
groups of two and three to create further features.
All algorithms and code that we have developed for
feature extraction are available online.9
Because we aim to develop models that do not
heavily rely on language-specific resources, we are
interested in exploring unsupervised and lightly
supervised methods for learning relevant features.
Rather than use part-of-speech tags, we therefore
use Brown cluster labels as unsupervised word tags
(Brown et al, 1992; Koo et al, 2008). Brown
clustering is a distributional similarity method that
merges pairs of word clusters in the training data10
to create the smallest decrease in corpus likelihood,
using a bigram language model on the clusters. For
our task, we cut clusters at length 3 and length 5,
and these serve as rough part-of-speech tags without
the need to train additional models. For example,
the word hello is tagged as belonging to cluster 011
(length 3) and 01111 (length 5).
During development, we found that being able
to syllabify the word (break the word into sylla-
bles) was a positive indicator of people names, but
a negative indicator of organization names. This
observation can be approximated automatically us-
ing constraints from the sonority sequencing princi-
ple (Hooper, 1976; Clements, 1990; Blevins, 1996;
Morelli, 2003) on a language?s orthography. This
is a phonotactic principle that states that syllables
will tend to have a sonority peak, usually a vowel,
in the center of the syllable, followed on either side
by consonants with decreasing sonority. Although
languages may violate this principle, the core idea
that a vowel forms the nucleus of a syllable with op-
9www.m-mitchell.com/code
10For Spanish, we train on a sample of ?7 million Spanish
tweets. For English, we train on the essays (Pennebaker et al,
2007) and Facebook data (Kosinskia et al, 2013) available from
ICWSM 2013.
1648
tional consonants before (the onset) and after (the
coda) can be used to begin to automatically learn
syllable structure.11 We learn this in an unsuper-
vised way, using the most frequent (seen more than
1,000 times) word-initial non-vowel sequences from
the Brown cluster data as allowable syllable onset
consonants. Similarly, the most frequent word-final
non-vowel sequences are learned as possible sylla-
ble codas. For each word, we then attempt to seg-
ment syllables using the learned onsets and codas
around each vowel. If a word cannot be syllabified,
it is often an initialism (e.g., CND, lsat).
We follow the approach from the out-of-
vocabulary assignment in the Berkeley parser
(Petrov et al, 2006) to encode common surface
patterns such as capitalization and lexical patterns
such as verb endings as a single feature for words
we have seen once or less. We also use the Jer-
boa toolkit (Van Durme, 2012) to extract further
language-independent features from the data, such
as features for emoticons and binning for repeated
characters (like !!!). In addition, we include features
for whether the word is three or four letters, which
is often used for acronyms and initialisms in several
languages (including Spanish and English); whether
the word is neighbored by a punctuation mark; word
identity; word length; message length; and position
in the sentence.
We utilize a speaker of each language to simply
list word forms for sentiment features that may be
indicative of sentiment, totaling less than two hours
of annotation time. This set includes intensifiers
(e.g., hella, freakin? in English; e.g., muy, suma-
mente in Spanish), positive/negative abbreviations
(WTF, pso), positive/negative slang words, and pos-
itive/negative prefix and suffixes (e.g., anti- in En-
glish and Spanish, -ito in Spanish).
6 Experiments
We are interested in both PERSON and ORGANIZA-
TION entities, and evaluate these in the collapsed
category VOLITIONAL. This suggests that the data
may be pre-processed to label all volitional entities
as VOLITIONAL NEs, or the models may be learned
with the traditional named entities in place, and post-
11Further development is necessary to extend a similar idea
to languages that do not ordinarily mark all vowels in their or-
thography, such as Hebrew and Arabic.
SURFACE FEATURES
binned word length, message length, and sen-
tence position; Jerboa features; word identity; word
lengthening; punctuation characters, has digit; has
dash; is lower case; is 3 or 4 letters; first letter capi-
talized; more than one letter capitalized, etc.
LINGUISTIC FEATURES
function words; can syllabify; curse words; laugh
words; words for good, bad, no, my; slang words; ab-
breviations; intensifiers; subjective suffixes and pre-
fixes (such as diminutive forms); common verb end-
ings; common noun endings
BROWN CLUSTERING FEATURES
cluster at length 3; cluster at length 5
SENTIMENT FEATURES
is sentiment-bearing word; prior sentiment polarity
Table 5: Features used in model.
processed to identify those that are VOLITIONAL.
We explored results using both methods, and found
that training models on VOLITIONAL tags yielded
the best performance overall; we report numbers for
this approach below.
We compare against a baseline (BASE-NS) where
we use our volitional entity labels and assign no
sentiment directed towards the entity (the majority
case). This is a strong baseline to isolate how our
methods perform specifically for the task of identi-
fying sentiment targeted at an entity.
We report on precision, recall, and sensitivity for
the tasks of NER and targeted subjectivity/sentiment
prediction in isolation; and we report on accuracy
for the targeted subjectivity and targeted sentiment
models. For sentiment, a true positive is an instance
where the label has sentiment, and a true negative is
an instance where the label has no sentiment (neu-
tral). For NER, a true positive is an instance where
the label is a B- or I- label; a true negative is an
instance where the label is O. The three systems
are evaluated against one another for NER, subjec-
tivity (entity has/does not have sentiment expressed
towards it), and sentiment (positive/negative/no sen-
timent) using paired t-tests across folds, with a Bon-
ferroni correction to set ? to 0.02.
NER We include results for the isolated task of vo-
litional named entity recognition in Table 6. In both
Spanish and English, all three models are roughly
comparable for precision, recall, and specificity. The
task of finding O tags ? spans that are not named en-
tities ? works especially well (NE spec). Common
1649
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
NE prec 65.2 64.3 65.1 59.8 62.3 60.5
NE rec 65.8 64.7 61.2 60.2 57.2 56.5
NE spec 95.4 95.2 95.6 94.3 95.1 94.7
Table 6: Average precision, recall, and specificity for vo-
litional entity NER (in %).
mistakes include confusing B- labels with I- labels.
Subjectivity and Sentiment Table 7 shows results
for the isolated task of predicting the presence of
sentiment about a volitional entity. In Spanish, the
pipeline models (PIPE) perform optimally for sub-
jectivity recall (Subj rec), and significantly above
the COLL models (p<.001). Precision and speci-
ficity are comparable across models. In English as
in Spanish, the collapsed model is particularly poor
at subjectivity recall.
As discussed in Section 2, the subtask of predict-
ing whether subjectivity is expressed towards an en-
tity is comparable to the main task of Jiang et al
(2011), and so we compare our approach here. The
Jiang et al study is similar to the current study in that
they aim to detect targeted sentiment, but it differs
from the current study in that they focus exclusively
on subjectivity towards five manually selected enti-
ties: {Obama, Google, iPad, Lakers, Lady Gaga}.
They also evaluate on artificially balanced evalu-
ation data, and evaluate sentiment polarity (posi-
tive/negative) separately from subjectivity (has/does
not have sentiment).
Our dataset includes any entity labeled as PERSON
or ORGANIZATION, and is not balanced (most tar-
gets have no sentiment expressed towards them; see
Table 1), thus we can only roughly compare against
their approach. Lakers and Lady Gaga are rare in
our collection (appearing less than 3 times), and so
we updated the comparison set prior to evaluation to:
{Obama, Google, iPad, BBC, Tebow}. On this set, a
baseline that always guesses no sentiment reaches an
accuracy of 66.9%, compared to Jiang et al?s 65.5%
accuracy on a balanced set (not strictly compara-
ble, but provided for reference). The JOINT mod-
els reach an accuracy of 71.04% on this set, demon-
strating this approach as potentially useful for topic-
dependent targeted sentiment.
Table 8 shows results for the task of predicting
the polarity of the sentiment expressed about an en-
tity. In Spanish, the PIPE models significantly out-
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
Subj prec 58.3 58.8 58.9 46.6 52.2 45.9
Subj rec 40.1 50.9 19.1 44.5 48.5 16.4
Subj spec 79.6 77.5 77.8 77.6 80.8 74.0
Table 7: Average precision, recall, and specificity (in %)
for subjectivity prediction (has/does not have sentiment)
along the target entity.
Spanish English
Model Joint Pipe Coll Joint Pipe Coll
Sent prec 36.6 45.8 42.5 31.6 42.9 38.5
Sent rec 38.0 40.6 15.5 36.6 34.8 9.7
Sent spec 67.1 75.2 73.3 72.3 82.0 78.1
Table 8: Average precision, recall, and specificity (in %)
for sentiment prediction (positive/negative/no sentiment)
along the target entity.
perform the COLL models on sentiment recall, and
the JOINT models on sentiment precision (p<.01).
In English, PIPE significantly outperforms JOINT on
precision (p<.001).
Targeted Subjectivity and Targeted Sentiment
The JOINT and PIPE models work reasonably
well for the isolated tasks of NER and subjectiv-
ity/sentiment prediction. We now examine results
for targeted subjectivity ? labeling an entity and pre-
dicting whether there is sentiment directed towards
it ? in Table 9; and targeted sentiment ? labeling an
entity and predicting what the sentiment directed to-
wards it is ? in Table 10.
We evaluate using two accuracy metrics: Acc-all,
which measures the accuracy of the entire named en-
tity span along with the sentiment span; and Acc-
Bsent, which measures the accuracy of identifying
the start of a named entity (B- labels) along with
the sentiment expressed towards it. Acc-all primar-
ily measures the correctness of O labels, while Acc-
Bsent focuses on the beginning of named entities.
For the targeted subjectivity task, our JOINT mod-
els perform optimally in Spanish, and significantly
above their baselines. For the Acc-Bsent task, JOINT
models perform best, significantly outperforming
their baseline for subjectivity prediction. In English,
where our data is half the size, we do not see a statis-
tically significant difference between the predictive
models and the no sentiment baselines.
For the targeted sentiment task, the JOINT mod-
els again perform relatively well in Spanish (Table
10), labeling volitional entities, predicting whether
or not there is sentiment targeted towards them, and
1650
Model Joint Joint
Base
Pipe Pipe
Base
Coll Coll
Base
Sp
a Acc-all 89.5* 89.3 89.3** 89.1 89.5* 89.3
Acc-Bsent 32.1*** 29.5 30.9*** 28.3 30.1** 28.1
E
ng Acc-all 88.0 88.1 88.6 88.6 87.9 88.1
Acc-Bsent 30.4 30.8 30.7 30.3 28.1 29.2
***p<.001 **p<.01 *p<.05
Table 9: Average accuracy on Targeted Subjectivity Pre-
diction: Identifying volitional entities and whether they
are a sentiment target. In the core task, Acc-Bsent, the
best model in Spanish is JOINT, significantly outperform-
ing the baseline. In English, the best model (PIPE) does
not significantly improve over its baseline.
Model Joint Joint
Base
Pipe Pipe
Base
Coll Coll
Base
Sp
a Acc-all 89.4 89.4 89.0 89.0 89.2 89.3
Acc-Bsent 29.7* 29.0 30.0 29.2 28.9 29.0
E
ng Acc-all 88.0 88.1 88.2 88.4 87.7 88.1
Acc-Bsent 30.4 30.6 30.5 30.8 27.9 29.8
*p<.05
Table 10: Average accuracy on Targeted Sentiment Pre-
diction: Identifying volitional entities and the polarity
of the sentiment expressed towards them. The Spanish
JOINT models significantly improve over their baseline
for the core task. In English, no models outperform their
baseline.
the sentiment polarity above their no sentiment base-
lines. We find this to be the most difficult task: It
may be clear that sentiment is being expressed to-
wards an entity, but it is not always clear what the
polarity of that sentiment is. Error analysis is given
below in this section. In the smaller English set, the
models do not outperform the no sentiment baseline.
7 Discussion
Feature Analysis Examples of some of the top-
weighted features in the Spanish models are shown
in Table 11. In addition to lexical identity and Brown
cluster, we find that positive indicators include pos-
itive suffixes such as diminutive forms, whether the
word can be syllabized (Section 5), and whether it is
three or four letters.
Error Analysis Because it is relatively common
for there not to be sentiment targeted at a named en-
tity, it is difficult to tease out the polarity in instances
where there is targeted sentiment. Similarly, our pre-
dictions are most reliable for detecting the absence
of a named entity (O labels).
Label confusions are shown in Table 12. Mistakes
are often made by confusing B- labels (the start of
B-VOLITIONAL FEATURES
Negative is a function word; jerboa tags; followed by a word
with 3 or 4 letters that cannot be syllabified
Positive ends in -a, -o, or -s; is capitalized; has one non-
initial capital letter; is 3 or 4 letters
B-VOLITIONAL, POS FEATURES
Negative preceded by a curse word; followed by a word
with a positive suffix; immediately preceded by a
word with a negative prefix
Positive not in a sentiment lexicon; preceded by a happy
emoticon; followed by an exclamation or a ?my?
word; immediately preceded by a laugh; has two
or more sentiment-bearing words in the sentence
B-VOLITIONAL, NEG FEATURES
Negative is immediately followed by a question mark or
positive abbreviation word
Positive preceded by a ?bad? word or curse word; has four
or more sentiment lexicon items
B-VOLITIONAL, NOT-TARG FEATURES
Negative immediately followed by a ?no? word or word with
a negative prefix; is preceded by a question mark;
is immediately preceded by a curse word or laugh;
is followed by an exclamation mark
Positive not followed by sentiment lexicon word
Table 11: Example strongly weighted features for a
Spanish joint sentiment model. In addition to lexical
identity, we find that curse words and positive and neg-
ative prefixes are used to detect volitional entities and the
sentiment directed towards them.
an entity) with I- labels (inside an entity); and by
predicting sentiment polarity when the gold annota-
tions say there is not sentiment targeted at the entity.
Some example errors are shown in Figure 13. In
(1), ?CANSADO? (?TIRED?) was predicted to be
volitional, while ?Matthew? was not. In (2), ?Ma-
tias del r??o? was not predicted to be an entity, likely
due to the fact that the capitalization patterns we see
in this sentence are indicative of the start of a sen-
tence rather than a proper name (similar to 1). In (3),
a.
Observed
B I O
P
re
di
ct
ed B 423 21 186
I 36 236 135
O 197 90 7168
b.
Observed
POS NEG NEUT
POS 68 24 42
NEG 58 65 102
NEUT 115 61 468
Table 12: Predicted vs. observed values for a joint model.
(a) For named entities, most common confusions were
between B-VOLITIONAL and O labels. (b) For sentiment,
most common mistakes were to predict that a positive
sentiment was neutral (no sentiment), and that a neutral
sentiment was negative.
1651
NE prediction errors
1.
Spanish: Cuando estoy CANSADO , e?l es mi DESCANSO . Mateo . 11 : 29 .
Predicted: O O B-VOLITIONAL O O O O O O O O O O O O
Gold: O O O O O O O O O B-VOLITIONAL O O O O O
English: When I?m TIRED , he is my REST . Matthew . 11 : 29 .
2.
Spanish: Matias del r??o fue una lata . . .
Predicted: O O O O O O . . .
Gold: B-VOLITIONAL I-VOLITIONAL I-VOLITIONAL O O O . . .
English: Matias del r??o was a drag . . .
Sentiment prediction errors
3.
Spanish: Mario que dio este contigo
Predicted: NOT-TARG - - - -
Gold: POSITIVE - - - -
English: Mario may God be with you
4.
Spanish: . . . si de verdad estas en cielo , ayudame Superman !!!
Predicted: - - - - - - - - POSITIVE -
Gold: - - - - - - - - NOT-TARG -
English: . . . if you really are in the skies , help me Superman !!!
Sentiment and NE prediction errors
5.
Spanish: Salen del gobierno de Humala dos connotados izquierdistas, Giesecke y Eiguiguren
Predicted:
O O O O B-VOLITIONAL I-VOLITIONAL O O O B-VOLITIONAL O B-VOLITIONAL
- - - - NOT-TARG NOT-TARG - - - NOT-TARG - NOT-TARG
Gold:
O O O O B-VOLITIONAL O O O O B-VOLITIONAL O B-VOLITIONAL
- - - - NOT-TARG - - - - NEGATIVE - NOT-TARG
English: Leaving the Humala government are two notorious leftists , Giesecke and Eiguiguren
Table 13: Example errors made by joint models.
sentiment may not be clear without spelling correc-
tion: ?dio? should be ?dios?, meaning ?God?; other-
wise, ?dio? is the word for ?gave?. Humans can eas-
ily fix the spelling error, which changes the overall
reading of the expression. In (4), the positive polar-
ity item ?verdad? (?believe?) and the exclamation
marks (!!!) were likely used as indicators of posi-
tive sentiment; however, in this case the annotators
marked the targeted sentiment as neutral. In (5), the
?Humala? entity was predicted to be longer than it is
(?Hamala dos? or ?Hamala two?). It was also pre-
dicted that both ?Giesecke? and ?Eiguiguren? had
no sentiment expressed towards them; annotators
disagreed, with the majority of those who annotated
?Giesecke? marking negative sentiment, and the ma-
jority of those who annotated ?Eiguiguren? mark-
ing no sentiment. This highlights some of the diffi-
culty in predicting sentiment discussed in Section 3,
where annotators will often disagree as to whether
there is no sentiment or positive/negative sentiment.
During development, we found that the collapsed
model (COLL) performed best on small amounts of
data. However, as we scaled up the amount of data
we trained on, the PIPE and JOINT models signif-
icantly improved, while the COLL models did not
have significant performance gains.
8 Conclusion
We have introduced the task of open domain targeted
sentiment: predicting sentiment directed towards an
entity along with discovering the entity itself. Our
approach is developed to find targeted sentiment to-
wards both person and organization named entities
by modeling sentiment as a span along the entity.
We find that by modeling targeted sentiment in
this way, we can reliably detect entities and whether
or not they are sentiment targets above a no senti-
ment baseline. How best to determine the polarity
of the sentiment expressed towards the entity, how-
ever, is still an open issue. Our data suggests that
it is usually not clear-cut whether sentiment is being
expressed or not; the strong disagreement between
annotators suggests that detecting sentiment polar-
ity in microblogs is difficult even for humans.
In future work, we hope to explore further meth-
ods for teasing apart sentiment polarity expressed to-
wards a target. This research has achieved promis-
ing results for detecting sentiment targets without re-
lying on external supervised models, and we hope
that the features and approaches developed here can
aid in sentiment analysis in noisy text and languages
without rich linguistic resources.
1652
References
A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and R. Pas-
sonneau. 2011. Sentiment analysis of twitter data. In
Proceedings of the Workshop on Language in Social
Media.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of Coling: Posters.
Adam Bermingham and Alan F Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proceedings of CIKM-2010.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in Twitter streaming data. In Proceed-
ings of the International Conference on Discovery Sci-
ence (DS-2010).
Juliette Blevins. 1996. The syllable in phonological the-
ory. In John A. Goldswmith, editor, The Handbook
of Phonological Theory. Blackwell Publishing, Black-
well Reference Online.
N. N. Bora. 2012. Summarizing public opinions in
tweets. In Proceedings of CICLing-2012.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP-2011.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Pedro Henrique Calais Guerra, Adriano Veloso, Wagner
Meira Jr, and Virg??lio Almeida. 2011. From bias to
opinion: a transfer-learning approach to real-time sen-
timent analysis. In Proceedings of the KDD-2011.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL:HLT Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P. Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from twitter. In Proceedings of ICWSM-2012.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. Proceedings of EMNLP 2006.
G. N. Clements. 1990. The role of the sonority cycle in
core syllabification. In J. Kingston and M. Beckman,
editors, Papers in Laboratory Phonology, pages 283?
333. CUP, Cambridge.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of Coling: Posters.
Nicholas A Diakopoulos and David A Shamma. 2010.
Characterizing debate performance via aggregated
twitter sentiment. In Proceedings of CHI-2010.
David Etter, Francis Ferraro, Ryan Cotterell, Olivia
Buzek, and Benjamin Van Durme. 2013. Nerit:
Named entity recognition for informal text. Techni-
cal Report 11, Human Language Technology Center
of Excellence, Johns Hopkins University, July.
Jenny Rose Finkel and Christopher D. Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly labeled
data. In Proceedings of ACL-2010.
Joan B. Hooper. 1976. The syllable in phonological the-
ory. Language, 48(3):525?540.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD.
Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu. 2013. Ex-
ploiting social relations for sentiment analysis in mi-
croblogging. In Proceedings of the 6th ACM Inter-
national Conference on Web Search and Data Mining
(WSDM-2013).
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
EMNLP.
Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao. 2011.
Target-dependent twitter sentiment classification. In
Proceedings of ACL-2011.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
hmm-based learning framework for web opinion min-
ing. Proceedings of ICML 2009.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and
analyzing judgment opinions. Proceedings of NAACL
2006.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL/HLT.
Michal Kosinskia, David Stillwell, and Thore Graepel.
2013. Private trains and attributes are predictable from
digital records of human behavior. Proc. of the Na-
tional Academy of Sciences of the USA, 110(5).
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of ICWSM-
2011.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML-2001.
1653
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a.
Structure-aware review mining and summarization.
Proceedings of Coling 2010.
Guangxia Li, Steven CH Hoi, Kuiyu Chang, and Ramesh
Jain. 2010b. Micro-blogging sentiment detection
by collaborative online learning. In Proceedings of
ICDM-2010.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and De-
quan Zheng. 2012. Combining social cognitive theo-
ries with linguistic features for multi-genre sentiment
analysis. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC-2012).
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction, and web-enhanced lexicons.
In Proceedings of CoNLL-2003.
Frida Morelli. 2003. The relative harmony of /s+stop/
onsets: Obstruent clusters and the sonority sequenc-
ing principle. In C. Fery and R. van de Vijver, edi-
tors, The syllable in optimality theory, pages 356?371.
CUP, New York.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of LREC-2010.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis. 2007. Linguistic inquiry and word count:
Liwc2007, operator?s manual.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in span-
ish. Proceedings of the Conference on Language Re-
sources and Evaluations (LREC 2012).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
Coling:ACL-2006.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT:EMNLP-2005.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Linguis-
tics, 37(1).
Hassan Saif, Yulan He, and Harith Alani. 2012. Allevi-
ating data sparsity for twitter sentiment analysis. Pro-
ceedings of the WWW Workshop on Making Sense of
Microposts (# MSM2012).
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the EMNLP-2011
Workshop on Unsupervised Learning in NLP.
Veselin Stoyanov and Jason Eisner. 2012. Minimum-
risk training of approximate crf-based nlp systems. In
Proceedings of NAACL:HLT-2012.
Veselin Stoyanov, Alexander Ropson, and Jason Eis-
ner. 2011. Empirical risk minimization of graphi-
cal model parameters given approximate inference, de-
coding, and model structure. In AIStats.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the KDD-2011.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical report,
Human Language Technology Center of Excellence,
Johns Hopkins University.
Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013. Exploring sentiment in social media: Boot-
strapping subjectivity clues from multilingual twitter
streams. In Association for Computational Linguistics
(ACL).
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and
Ming Zhang. 2011. Topic sentiment analysis in Twit-
ter: A graph-based hashtag sentiment classification ap-
proach. In Proceedings of CIKM-2011.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of HLT-EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman. 2009.
Recognizing contextual polarity: An exploration of
features for phrase-level sentiment analysis. Compu-
tational Linguistics, 35(3).
Bishan Yang and Claire Cardie. 2013. Joint inference for
fine-grained opinion extraction. Proceedings of ACL
2013.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Ex-
tracting sentiments about a given topic using natural
language processing techniques. In Proceedings of
ICDM-2003.
1654
Proceedings of NAACL-HLT 2013, pages 758?764,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
PPDB: The Paraphrase Database
Juri Ganitkevitch1 Benjamin Van Durme1,2 Chris Callison-Burch2,3
1Center for Language and Speech Processing, Johns Hopkins University
2Human Language Technology Center of Excellence, Johns Hopkins University
3Computer and Information Science Department, University of Pennsylvania
Abstract
We present the 1.0 release of our para-
phrase database, PPDB. Its English portion,
PPDB:Eng, contains over 220 million para-
phrase pairs, consisting of 73 million phrasal
and 8 million lexical paraphrases, as well as
140 million paraphrase patterns, which cap-
ture many meaning-preserving syntactic trans-
formations. The paraphrases are extracted
from bilingual parallel corpora totaling over
100 million sentence pairs and over 2 billion
English words. We also release PPDB:Spa, a
collection of 196 million Spanish paraphrases.
Each paraphrase pair in PPDB contains a
set of associated scores, including paraphrase
probabilities derived from the bitext data and a
variety of monolingual distributional similar-
ity scores computed from the Google n-grams
and the Annotated Gigaword corpus. Our re-
lease includes pruning tools that allow users to
determine their own precision/recall tradeoff.
1 Introduction
Paraphrases, i.e. differing textual realizations of the
same meaning, have proven useful for a wide vari-
ety of natural language processing applications. Past
paraphrase collections include automatically derived
resources like DIRT (Lin and Pantel, 2001), the
MSR paraphrase corpus and phrase table (Dolan
et al, 2004; Quirk et al, 2004), among others.
Although several groups have independently ex-
tracted paraphrases using Bannard and Callison-
Burch (2005)?s bilingual pivoting technique (see
Zhou et al (2006), Riezler et al (2007), Snover et
al. (2010), among others), there has never been an
official release of this resource.
In this work, we release version 1.0 of the Para-
Phrase DataBase PPDB,1 a collection of ranked En-
glish and Spanish paraphrases derived by:
? Extracting lexical, phrasal, and syntactic para-
phrases from large bilingual parallel corpora
(with associated paraphrase probabilities).
? Computing distributional similarity scores for
each of the paraphrases using the Google n-
grams and the Annotated Gigaword corpus.
In addition to the paraphrase collection itself, we
provide tools to filter PPDB to only retain high pre-
cision paraphrases, scripts to limit the collection to
phrasal or lexical paraphrases (synonyms), and soft-
ware that enables users to extract paraphrases for
languages other than English.
2 Extracting Paraphrases from Bitexts
To extract paraphrases we follow Bannard and
Callison-Burch (2005)?s bilingual pivoting method.
The intuition is that two English strings e1 and e2
that translate to the same foreign string f can be as-
sumed to have the same meaning. We can thus pivot
over f and extract ?e1, e2? as a pair of paraphrases,
as illustrated in Figure 1. The method extracts a di-
verse set of paraphrases. For thrown into jail, it ex-
tracts arrested, detained, imprisoned, incarcerated,
jailed, locked up, taken into custody, and thrown
into prison, along with a set of incorrect/noisy para-
phrases that have different syntactic types or that are
due to misalignments.
For PPDB, we formulate our paraphrase collec-
tion as a weighted synchronous context-free gram-
mar (SCFG) (Aho and Ullman, 1972; Chiang, 2005)
1Freely available at http://paraphrase.org.
758
... f?nf Landwirte , weil
... 5 farmers were in Ireland ...
...
oder wurden , gefoltert
or have been , tortured
festgenommen 
thrown into jail
festgenommen
imprisoned
...
... ...
...
Figure 1: Phrasal paraphrases are extracted via bilingual
pivoting.
with syntactic nonterminal labels, similar to Cohn
and Lapata (2008) and Ganitkevitch et al (2011).
An SCFG rule has the form:
r
def= C ? ?f, e,?, ~??,
where the left-hand side of the rule,C, is a nontermi-
nal and the right-hand sides f and e are strings of ter-
minal and nonterminal symbols. There is a one-to-
one correspondence, ?, between the nonterminals
in f and e: each nonterminal symbol in f has to
also appear in e. Following Zhao et al (2008), each
rule r is annotated with a vector of feature functions
~? = {?1...?N} which are combined in a log-linear
model (with weights ~?) to compute the cost of ap-
plying r:
cost(r) = ?
N?
i=1
?i log?i. (1)
To create a syntactic paraphrase grammar we
first extract a foreign-to-English translation gram-
mar from a bilingual parallel corpus, using tech-
niques from syntactic machine translation (Koehn,
2010). Then, for each pair of translation rules where
the left-hand side C and foreign string f match:
r1
def= C ? ?f, e1,?1, ~?1?
r2
def= C ? ?f, e2,?2, ~?2?,
we pivot over f to create a paraphrase rule rp:
rp
def= C ? ?e1, e2,?p, ~?p?,
with a combined nonterminal correspondency func-
tion ?p. Note that the common source side f im-
plies that e1 and e2 share the same set of nonterminal
symbols.
The paraphrase rules obtained using this method
are capable of making well-formed generalizations
of meaning-preserving rewrites in English. For
instance, we extract the following example para-
phrase, capturing the English possessive rule:
NP ? the NP1 of NNS 2 | the NNS2 ?s NP1.
The paraphrase feature vector ~?p is computed
from the translation feature vectors ~?1 and ~?2 by
following the pivoting idea. For instance, we esti-
mate the conditional paraphrase probability p(e2|e1)
by marginalizing over all shared foreign-language
translations f :
p(e2|e1) ?
?
f
p(e2|f)p(f |e1). (2)
3 Scoring Paraphrases Using Monolingual
Distributional Similarity
The bilingual pivoting approach anchors para-
phrases that share an interpretation because of a
shared foreign phrase. Paraphrasing methods based
on monolingual text corpora, like DIRT (Lin and
Pantel, 2001), measure the similarity of phrases
based on distributional similarity. This results in a
range of different types of phrases, including para-
phrases, inference rules and antonyms. For instance,
for thrown into prison DIRT extracts good para-
phrases like arrested, detained, and jailed. How-
ever, it also extracts phrases that are temporarily
or causally related like began the trial of, cracked
down on, interrogated, prosecuted and ordered the
execution of, because they have similar distribu-
tional properties. Since bilingual pivoting rarely ex-
tracts these non-paraphrases, we can use monolin-
gual distributional similarity to re-rank paraphrases
extracted from bitexts (following Chan et al (2011))
or incorporate a set of distributional similarity scores
as features in our log-linear model.
Each similarity score relies on precomputed dis-
tributional signatures that describe the contexts that
a phrase occurs in. To describe a phrase e, we gather
counts for a set of contextual features for each oc-
currence of e in a corpus. Writing the context vector
for the i-th occurrence of e as ~se,i, we can aggre-
gate over all occurrences of e, resulting in a distri-
butional signature for e, ~se =
?
i ~se,i. Following the
intuition that phrases with similar meanings occur in
759
the long-term
achieve25
goals 23
plans 97
investment 10
confirmed64
revise43 the long-term
the long-term
the long-term
the long-term
the long-term
.
.
.
.
L-achieve = 25
L-confirmed
= 64
L-revise = 43
?
R-goals 
= 23
R-plans  = 97
R-investment 
= 10
?
the long-term
?
=~sig
?
(a) The n-gram corpus records the long-term as preceded
by revise (43 times), and followed by plans (97 times). We
add corresponding features to the phrase?s distributional
signature retaining the counts of the original n-grams.
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
(b) Here, position-aware lexical and part-of-speech n-
gram features, labeled dependency links , and features
reflecting the phrase?s CCG-style label NP/NN are in-
cluded in the context vector.
Figure 2: Features extracted for the phrase the long term from the n-gram corpus (2a) and Annotated Gigaword (2b).
similar contexts, we can then quantify the goodness
of e? as a paraphrase of e by computing the cosine
similarity between their distributional signatures:
sim(e, e?) = ~se ? ~se?
|~se||~se? |
.
A wide variety of features have been used to de-
scribe the distributional context of a phrase. Rich,
linguistically informed feature-sets that rely on de-
pendency and constituency parses, part-of-speech
tags, or lemmatization have been proposed in work
such as by Church and Hanks (1991) and Lin and
Pantel (2001). For instance, a phrase is described by
the various syntactic relations such as: ?what verbs
have this phrase as the subject??, or ?what adjectives
modify this phrase??. Other work has used simpler
n-gram features, e.g. ?what words or bigrams have
we seen to the left of this phrase??. A substantial
body of work has focussed on using this type of
feature-set for a variety of purposes in NLP (Lapata
and Keller, 2005; Bhagat and Ravichandran, 2008;
Lin et al, 2010; Van Durme and Lall, 2010).
For PPDB, we compute n-gram-based context
signatures for the 200 million most frequent phrases
in the Google n-gram corpus (Brants and Franz,
2006; Lin et al, 2010), and richer linguistic signa-
tures for 175 million phrases in the Annotated Gi-
gaword corpus (Napoles et al, 2012). Our features
extend beyond those previously used in the work by
Ganitkevitch et al (2012). They are:
? n-gram based features for words seen to the left
and right of a phrase.
? Position-aware lexical, lemma-based, part-of-
speech, and named entity class unigram and bi-
gram features, drawn from a three-word win-
dow to the right and left of the phrase.
? Incoming and outgoing (wrt. the phrase) de-
pendency link features, labeled with the corre-
sponding lexical item, lemmata and POS.
? Syntactic features for any constituents govern-
ing the phrase, as well as for CCG-style slashed
constituent labels for the phrase.
Figure 2 illustrates the feature extraction for an ex-
ample phrase.
4 English Paraphrases ? PPDB:Eng
We combine several English-to-foreign bitext cor-
pora to extract PPDB:Eng: Europarl v7 (Koehn,
2005), consisting of bitexts for the 19 European lan-
guages, the 109 French-English corpus (Callison-
Burch et al, 2009), the Czech, German, Span-
ish and French portions of the News Commen-
tary data (Koehn and Schroeder, 2007), the United
Nations French- and Spanish-English parallel cor-
pora (Eisele and Chen, 2010), the JRC Acquis cor-
pus (Steinberger et al, 2006), Chinese and Arabic
760
Identity Paraphrases Total
Lexical 0.6M 7.6M 8.1M
Phrasal 4.9M 68.4M 73.2M
Syntactic 46.5M 93.6M 140.1M
All 52.0M 169.6M 221.4M
Table 1: A breakdown of PPDB:Eng size by paraphrase
type. We distinguish lexical (i.e. one-word) paraphrases,
phrasal paraphrases and syntactically labeled paraphrase
patterns.
newswire corpora used for the GALE machine trans-
lation campaign,2 parallel Urdu-English data from
the NIST translation task,3 the French portion of
the OpenSubtitles corpus (Tiedemann, 2009), and a
collection of Spanish-English translation memories
provided by TAUS.4
The resulting composite parallel corpus has more
than 106 million sentence pairs, over 2 billion En-
glish words, and spans 22 pivot languages. To ap-
ply the pivoting technique to this multilingual data,
we treat the various pivot languages as a joint Non-
English language. This simplifying assumption al-
lows us to share statistics across the different lan-
guages and apply Equation 2 unaltered.
Table 1 presents a breakdown of PPDB:Eng by
paraphrase type. We distinguish lexical (a single
word), phrasal (a continuous string of words), and
syntactic paraphrases (expressions that may con-
tain both words and nonterminals), and separate
out identity paraphrases. While we list lexical and
phrasal paraphrases separately, it is possible that a
single word paraphrases as a multi-word phrase and
vice versa ? so long they share the same syntactic
label.
5 Spanish Paraphrases ? PPDB:Spa
We also release a collection of Spanish paraphrases:
PPDB:Spa is extracted analogously to its English
counterpart and leverages the Spanish portions of the
bitext data available to us, totaling almost 355 mil-
lion Spanish words, in nearly 15 million sentence
pairs. The paraphrase pairs in PPDB:Spa are anno-
2http://projects.ldc.upenn.edu/gale/
data/Catalog.html
3LDC Catalog No. LDC2010T23
4http://www.translationautomation.com/
Identity Paraphrases Total
Lexical 1.0M 33.1M 34.1M
Phrasal 4.3M 73.2M 77.5M
Syntactic 29.4M 55.3M 84.7M
All 34.7M 161.6M 196.3M
Table 2: An overview of PPDB:Spa. Again, we parti-
tion the resource into lexical (i.e. one-word) paraphrases,
phrasal paraphrases and syntactically labeled paraphrase
patterns.
expect
NNS VBP
NP
VP
the data
NP VP
S
to show
JJ
economistsfew
......
S
...
RelArg0 Arg1
Figure 3: To inspect our coverage, we use the Penn
Treebank?s parses to map from Propbank annotations to
PPDB?s syntactic patterns. For the above annotation
predicate, we extract VBP ? expect, which is matched
by paraphrase rules like VBP ? expect | anticipate
and VBP ? expect | hypothesize. To search for
the entire relation, we replace the argument spans
with syntactic nonterminals. Here, we obtain S ?
NP expect S, for which PPDB has matching rules like
S ? NP expect S | NP would hope S, and S ?
NP expect S | NP trust S. This allows us to apply so-
phisticated paraphrases to the predicate while capturing
its arguments in a generalized fashion.
tated with distributional similarity scores based on
lexical features collected from the Spanish portion
of the multilingual release of the Google n-gram
corpus (Brants and Franz, 2009), and the Spanish
Gigaword corpus (Mendonca et al, 2009). Table 2
gives a breakdown of PPDB:Spa.
6 Analysis
To estimate the usefulness of PPDB as a resource
for tasks like semantic role labeling or parsing, we
analyze its coverage of Propbank predicates and
predicate-argument tuples (Kingsbury and Palmer,
2002). We use the Penn Treebank (Marcus et
al., 1993) to map Propbank annotations to patterns
which allow us to search PPDB:Eng for paraphrases
that match the annotated predicate. Figure 3 illus-
761
 1 3 5-30 -25 -20 -15 -10 -5  0Avg. Score Pruning Threshold 0 0.5 1-30 -25 -20 -15 -10 -5  0  0 50 100 150Coverage PP / Type
(a) PPDB:Eng coverage of Propbank predicates
(top), and average human judgment score (bottom)
for varying pruning thresholds.
 0 0.2 0.4 0.6 0.8 1-30 -25 -20 -15 -10 -5  0  0 20 40 60 80 100 120 140 160Coverage Paraphrases / TypePruning ThresholdRelation Tokens CoveredParaphrases / TypeRelation Types Covered
(b) PPDB:Eng?s coverage of Propbank predicates
with up to two arguments. Here we consider rules
that paraphrase the full predicate-argument expres-
sion.
Figure 4: An illustration of PPDB?s coverage of the manually annotated Propbank predicate phrases (4a) and binary
relations with argument non-terminals (4b). The curves indicate the coverage on tokens (solid) and types (dotted), as
well as the average number of paraphrases per covered type (dashed) at the given pruning level.
trates this mapping.
In order to quantify PPDB?s precision-recall
tradeoff in this context, we perform a sweep
over our collection, beginning with the full set of
paraphrase pairs and incrementally discarding the
lowest-scoring ones. We choose a simple estimate
for each paraphrase pair?s score by uniformly com-
bining its paraphrase probability features in Eq. 1.
The top graph in Figure 4a shows PPDB?s cover-
age of predicates (e.g. VBP ? expect) at the type
level (i.e. counting distinct predicates), as well as
the token level (i.e. counting predicate occurrences
in the corpus). We also keep track of average num-
ber of paraphrases per covered predicate type for
varying pruning levels. We find that PPDB has a
predicate type recall of up to 52% (accounting for
97.5% of tokens). Extending the experiment to full
predicate-argument relations with up to two argu-
ments (e.g. S ? NNS expect S), we obtain a 27%
type coverage rate that accounts for 40% of tokens
(Figure 4b). Both rates hold even as we prune the
database down to only contain high precision para-
phrases. Our pruning method here is based on a sim-
ple uniform combination of paraphrase probabilities
and similarity scores.
To gauge the quality of our paraphrases, the au-
thors judged 1900 randomly sampled predicate para-
phrases on a scale of 1 to 5, 5 being the best. The
bottom graph in Figure 4a plots the resulting human
score average against the sweep used in the cover-
age experiment. It is clear that even with a simple
weighing approach, the PPDB scores show a clear
correlation with human judgements. Therefore they
can be used to bias the collection towards greater re-
call or higher precision.
7 Conclusion and Future Work
We present the 1.0 release of PPDB:Eng and
PPDB:Spa, two large-scale collections of para-
phrases in English and Spanish. We illustrate the
resource?s utility with an analysis of its coverage of
Propbank predicates. Our results suggest that PPDB
will be useful in a variety of NLP applications.
Future releases of PPDB will focus on expand-
ing the paraphrase collection?s coverage with regard
to both data size and languages supported. Further-
more, we intend to improve paraphrase scoring by
incorporating additional sources of information, as
well as by better utilizing information present in the
data, like domain or topic. We will also address
points of refinement such as handling of phrase am-
biguity, and effects specific to individual pivot lan-
guages. Our aim is for PPDB to be a continuously
updated and improving resource.
Finally, we will explore extensions to PPDB to in-
clude aspects of related large-scale resources such as
lexical-semantic hierarchies (Snow et al, 2006), tex-
tual inference rules (Berant et al, 2011), relational
patterns (Nakashole et al, 2012), and (lexical) con-
ceptual networks (Navigli and Ponzetto, 2012).
762
Acknowledgements
We would like to thank Frank Ferraro for his Prop-
bank processing tools. This material is based
on research sponsored by the NSF under grant
IIS-1249516 and DARPA under agreement num-
ber FA8750-13-2-0017 (the DEFT program). The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes. The
views and conclusions contained in this publication
are those of the authors and should not be interpreted
as representing official policies or endorsements of
DARPA or the U.S. Government.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Jonathan Berant, Jacob Goldberger, and Ido Dagan.
2011. Global learning of typed entailment rules. In
Proceedings of ACL.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Thorsten Brants and Alex Franz. 2009. Web 1T 5-gram,
10 european languages version 1. Linguistic Data
Consortium, Philadelphia.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Pro-
ceedings of WMT, pages 1?28, Athens, Greece, March.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van
Durme. 2011. Reranking bilingually extracted para-
phrases using monolingual distributional similarity. In
EMNLP Workshop on GEMS.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 6(1):22?29.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the COLING.
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from united nation documents. In Pro-
ceedings of LREC, Valletta, Malta.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2012. Monolingual distributional
similarity for text-to-text generation. In Proceedings
of *SEM. Association for Computational Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of LREC.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of WMT, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1).
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the Penn Treebank. Computational
Linguistics, 19(2).
Angelo Mendonca, David Andrew Graff, and Denise
DiPersio. 2009. Spanish Gigaword Second Edition.
Linguistic Data Consortium.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. PATTY: a taxonomy of rela-
tional patterns with semantic types. In Proceedings
of EMNLP.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP.
763
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the ACL.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the ACL/Coling.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?niel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In Pro-
ceedings of LREC, Genoa, Italy.
Jo?rg Tiedemann. 2009. News from OPUS: A collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume 5.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of HLT/NAACL.
764
Proceedings of NAACL-HLT 2013, pages 858?867,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Answer Extraction as Sequence Tagging with Tree Edit Distance
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Vulcan Inc.
Seattle, WA, USA
Abstract
Our goal is to extract answers from pre-
retrieved sentences for Question Answering
(QA). We construct a linear-chain Conditional
Random Field based on pairs of questions
and their possible answer sentences, learning
the association between questions and answer
types. This casts answer extraction as an an-
swer sequence tagging problem for the first
time, where knowledge of shared structure be-
tween question and source sentence is incor-
porated through features based on Tree Edit
Distance (TED). Our model is free of man-
ually created question and answer templates,
fast to run (processing 200 QA pairs per sec-
ond excluding parsing time), and yields an F1
of 63.3% on a new public dataset based on
prior TREC QA evaluations. The developed
system is open-source, and includes an imple-
mentation of the TED model that is state of the
art in the task of ranking QA pairs.
1 Introduction
The success of IBM?s Watson system for Question
Answering (QA) (Ferrucci et al, 2010) has illus-
trated a continued public interest in this topic. Wat-
son is a sophisticated piece of software engineering
consisting of many components tied together in a
large parallel architecture. It took many researchers
working full time for years to construct. Such re-
sources are not available to individual academic re-
searchers. If they are interested in evaluating new
ideas on some aspect of QA, they must either con-
struct a full system, or create a focused subtask
?Performed while faculty at Johns Hopkins University.
paired with a representative dataset. We follow the
latter approach and focus on the task of answer ex-
traction, i.e., producing the exact answer strings for
a question.
We propose the use of a linear-chain Conditional
Random Field (CRF) (Lafferty et al, 2001) in or-
der to cast the problem as one of sequence tagging
by labeling each token in a candidate sentence as ei-
ther Beginning, Inside or Outside (BIO) of an an-
swer. This is to our knowledge the first time a
CRF has been used to extract answers.1 We uti-
lize not only traditional contextual features based on
POS tagging, dependency parsing and Named Entity
Recognition (NER), but most importantly, features
extracted from a Tree Edit Distance (TED) model
for aligning an answer sentence tree with the ques-
tion tree. The linear-chain CRF, when trained to
learn the associations between question and answer
types, is a robust approach against error propaga-
tion introduced in the NLP pipeline. For instance,
given an NER tool that always (i.e., in both train-
ing and test data) recognizes the pesticide DDT as
an ORG, our model realizes, when a question is
asked about the type of chemicals, the correct an-
swer might be incorrectly but consistently recog-
nized as ORG by NER. This helps reduce errors in-
troduced by wrong answer types, which were esti-
mated as the most significant contributer (36.4%)
of errors in the then state-of-the-art QA system of
Moldovan et al (2003).
The features based on TED allow us to draw the
1CRFs have been used in judging answer-bearing sentences
(Shima et al, 2008; Ding et al, 2008; Wang and Manning,
2010), but not extracting exact answers from these sentences.
858
connection between the question and answer sen-
tences before answer extraction, whereas tradition-
ally the exercise of answer validation (Magnini et
al., 2002; Penas et al, 2008; Rodrigo et al, 2009)
has been performed after as a remedy to ensure the
answer is really ?about? the question.
Motivated by a desire for a fast runtime,2 we
base our TED implementation on the dynamic-
programming approach of Zhang and Shasha
(1989), which helps our final system process 200
QA pairs per second on standard desktop hardware,
when input is syntactically pre-parsed.
In the following we first provide background on
the TED model, going on to evaluate our implemen-
tation against prior work in the context of question
answer sentence ranking (QASR), achieving state of
the art in that task. We then describe how we cou-
ple TED features to a linear-chain CRF for answer
extraction, providing the set of features used, and fi-
nally experimental results on an extraction dataset
we make public (together with the software) to the
community.3 Related prior work is interspersed
throughout the paper.
2 Tree Edit Distance Model
Tree Edit Distance (?2.1) models have been shown
effective in a variety of applications, including tex-
tual entailment, paraphrase identification, answer
ranking and information retrieval (Reis et al, 2004;
Kouylekov and Magnini, 2005; Heilman and Smith,
2010; Augsten et al, 2010). We chose the variant
proposed by Heilman and Smith (2010), inspired by
its simplicity, generality, and effectiveness. Our ap-
proach differs from those authors in their reliance
on a greedy search routine to make use of a complex
tree kernel. With speed a consideration, we opted
for the dynamic-programming solution of Zhang
and Shasha (1989) (?2.1). We added new lexical-
semantic features ?(2.2) to the model and then eval-
uated our implementation on the QASR task, show-
ing strong results ?(2.3).
Feature Description
distance tree edit distance from answer
sentence to question
renNoun
renVerb
renOther
# edits changing POS from or to
noun, verb, or other types
insN, insV,
insPunc,
insDet,
insOtherPos
# edits inserting a noun, verb,
punctuation mark, determiner
or other POS types
delN, delV, ... deletion mirror of above
ins{N,V,P}Mod
insSub, insObj
insOtherRel
# edits inserting a modifier for
{noun, verb, preposition}, sub-
ject, object or other relations
delNMod, ... deletion mirror of above
renNMod, ... rename mirror of above
XEdits # basic edits plus sum of in-
s/del/ren edits
alignNodes,
alignNum,
alignN, alignV,
alignProper
# aligned nodes, and those that
are numbers, nouns, verbs, or
proper nouns
Table 1: Features for ranking QA pairs.
2.1 Cost Design and Edit Search
Following Bille (2005), we define an edit script be-
tween trees T1, T2 as the edit sequence transforming
T1 to T2 according to a cost function, with the total
summed cost known as the tree edit distance. Basic
edit operations include: insert, delete and rename.
With T a dependency tree, we represent each node
by three fields: lemma, POS and the type of depen-
dency relation to the node?s parent (DEP). For in-
stance, Mary/nnp/sub is the proper noun Mary in
subject position.
Basic edits are refined into 9 types, where the
first six (INS LEAF, INS SUBTREE, INS, DEL LEAF,
DEL SUBTREE, DEL) insert or delete a leaf node, a
whole subtree, or a node that is neither a leaf nor
part of a whole inserted subtree. The last three
(REN POS, REN DEP, REN POS DEP) serve to re-
name a POS tag, dependency relation, or both.
2For instance, Watson was designed under the constraint of
a 3 second response time, arising from its intended live use in
the television gameshow, Jeopardy!.
3http://code.google.com/p/jacana/
859
prd
playernn jennifernn
capriatinnp 23cd
bevbzsubj
nmod nmod
tennisnn
nmod
Tennis player Jennifer Capriati is 23
TreeEdit Distance capriatinnp
jennifernnp
whatwp
sportnn
playvbz
dovbzvmod vmod
nmod
What sport does Jennifer Capriati play
insSubTree:
ins(play/vbz/vmod)ins(do/vbz/root)
WordNet
Figure 1: Edits transforming a source sentence (left) to a question (right). Each node consists of: lemma, POS tag
and dependency relation, with root nodes and punctuation not shown. Shown includes deletion (? and strikethrough
on the left), alignment (arrows) and insertion (shaded area). Order of operations is not displayed. The standard TED
model does not capture the alignment between tennis and sport (see Section 2.2).
We begin by uniformly assigning basic edits a
cost of 1.0,4 which brings the cost of a full node in-
sertion or deletion to 3 (all the three fields inserted or
deleted). We allow renaming of POS and/or relation
type iff the lemmas of source and target nodes are
identical.5 When two nodes are identical and thus
do not appear in the edit script, or when two nodes
are renamed due to the same lemma, we say they are
aligned by the tree edit model (see Figure 1).
We used Zhang and Shasha (1989)?s dynamic
programming algorithm to produce an optimal edit
script with the lowest tree edit distance. The ap-
proach explores both trees in a bottom-up, post-
order manner, running in time:
O(|T1| |T2|min(D1, L1)min(D2, L2))
where |Ti| is the number of nodes, Di is the depth,
and Li is the number of leaves, with respect to tree
Ti.
Additionally, we fix the cost of stopword renam-
ing to 2.5, even in the case of identity, regardless
of whether two stopwords have the same POS tags
or relations. Stopwords tend to have fixed POS tags
and dependency relations, which often leads to less
expensive alignments as compared to renaming con-
4This applies separately to each element of the tripartite
structure; e.g., deleting a POS entry, inserting a lemma, etc.
5This is aimed at minimizing node variations introduced by
morphology differences, tagging or parsing errors.
tent terms. In practice this gave stopwords ?too
much say? in guiding the overall edit sequence.
The resultant system is fast in practice, processing
10,000 pre-parsed tree pairs per second on a contem-
porary machine.6
2.2 TED for Sentence Ranking
The task of Question Answer Sentence Ranking
(QASR) takes a question and a set of source sen-
tences, returning a list sorted by the probability
likelihood that each sentence contains an appropri-
ate answer. Prior work in this includes that of:
Punyakanok et al (2004), based on mapping syn-
tactic dependency trees; Wang et al (2007) utiliz-
ing Quasi-Synchronous Grammar (Smith and Eis-
ner, 2006); Heilman and Smith (2010) using TED;
and Shima et al (2008), Ding et al (2008) and Wang
and Manning (2010), who each employed a CRF in
various ways. Wang et al (2007) made their dataset
public, which we use here for system validation. To
date, models based on TED have shown the best per-
formance for this task.
Our implementation follows Heilman and Smith
(2010), with the addition of 15 new features beyond
their original 33 (see Table 1). Based on results
6In later tasks, feature extraction and decoding will slow
down the system, but the final system was still able to process
200 pairs per second.
860
set source #ques. #pairs %pos. len.
TRAIN-ALL TREC8-12 1229 53417 12.0 any
TRAIN TREC8-12 94 4718 7.4 ? 40
DEV TREC13 82 1148 19.3 ? 40
TEST TREC13 89 1517 18.7 ? 40
Table 2: Distribution of data, with imbalance towards
negative examples (sentences without an answer).
in DEV, we extract edits in the direction from the
source sentence to the question.
In addition to syntactic features, we incorporated
the following lexical-semantic relations from Word-
Net: hypernym and synonym (nouns and verbs); en-
tailment and causing (verbs); and membersOf, sub-
stancesOf, partsOf, haveMember, haveSubstance,
havePart (nouns). Such relations have been used
in prior approaches to this task (Wang et al, 2007;
Wang and Manning, 2010), but not in conjunction
with the model of Heilman and Smith (2010).
These were made into features in two ways:
WNsearch loosens renaming and alignment within
the TED model from requiring strict lemma equal-
ity to allowing lemmas that shared any of the
above relations, leading to renaming operations such
as REN ...(country, china) and REN ...(sport,
tennis); WNfeature counts how many words be-
tween the sentence and answer sentence have each
of the above relations, separately as 10 independent
features, plus an aggregate count for a total of 11
new features beyond the earlier 48.
These features were then used to train a logistic
regression model using Weka (Hall et al, 2009).
2.3 QA Sentence Ranking Experiment
We trained and tested on the dataset from Wang et
al. (2007), which spans QA pairs from TREC QA
8-13 (see Table 2). Per question, sentences with
non-stopword overlap were first retrieved from the
task collection, which were then compared against
the TREC answer pattern (in the form of Perl regu-
lar expressions). If a sentence matched, then it was
deemed a (noisy) positive example. Finally, TRAIN,
DEV and TEST were manually corrected for errors.
Those authors decided to limit candidate source sen-
System MAP MRR
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Wang and Manning (2010) 0.5951 0.6951
this paper (48 features) 0.6319 0.7270
+WNsearch 0.6371 0.7301
+WNfeature (11 more feat.) 0.6307 0.7477
Table 3: Results on the QA Sentence Ranking task.
tences to be no longer than 40 words.7 Keeping
with prior work, those questions with only positive
or negative examples were removed, leaving 94 of
the original 100 questions for evaluation.
The data was processed by Wang et al (2007)
with the following tool chain: POS tags via MX-
POST (Ratnaparkhi, 1996); parse trees via MST-
Parser (McDonald et al, 2005) with 12 coarse-
grained dependency relation labels; and named enti-
ties via Identifinder (Bikel et al, 1999). Mean Av-
erage Precision (MAP) and Mean Reciprocal Rank
(MRR) are reported in Table 3. Our implementa-
tion gives state of the art performance, and is fur-
thered improved by our inclusion of semantic fea-
tures drawn from WordNet.8
3 CRF with TED for Answer Extraction
In this section we move from ranking source sen-
tences, to the next QA stage: answer extraction.
Given our competitive TED-based alignment model,
the most obvious solution to extraction would be to
report those spans aligned from a source sentence
to a question?s wh- terms. However, we show that
this approach is better formulated as a (strongly in-
dicative) feature of a larger set of answer extraction
signals.
3.1 Sequence Model
Figure 2 illustrates the task of tagging each token in
a candidate sentence with one of the following la-
7TRAIN-ALL is not used in QASR, but later for answer ex-
traction; TRAIN comes from the first 100 questions of TRAIN-
ALL.
8As the test set is of limited size (94 questions), then while
our MAP/MRR scores are 2.8% ? 5.6% higher than prior
work, this is not statistically significant according to the Paired
Randomization Test (Smucker et al, 2007), and thus should be
considered on par with the current state of the art.
861
prddla yenjri frddlcri tnyiln2l la 3b
vzsum o o o o o
Figure 2: An example of linear-chain CRF for an-
swer sequence tagging.
bels: B-ANSWER (beginning of answer), I-ANSWER
(inside of answer), O (outside of answer).
Besides local POS/NER/DEP features, at each to-
ken we need to inspect the entire input to connect the
answer sentence with the question sentence through
tree edits, drawing features from the question and
the edit script, motivating the use of a linear-chain
CRF model (Lafferty et al, 2001) over HMMs. To
the best of our knowledge this is the first time a
CRF has been used to label answer fragments, de-
spite success in other sequence tagging tasks.
3.2 Feature Design
In this subsection we describe the local and global
features used by the CRF.
Chunking We use the POS/NER/DEP tags directly
just as one would in a chunking task. Specifically,
suppose t represents the current token position and
pos[t] its POS tag, we extract unigram, bigram and
trigram features over the local context, e.g., pos[t?
2], pos[t ? 2] : pos[t ? 1], and pos[t ? 2] : pos[t ?
1] : pos[t]. Similar features are extracted for named
entity types (ner[t]), and dependency relation labels
(dep[t]).
Our intuition is these chunking features should al-
low for learning which types of words tend to be
answers. For instance, we expect adverbs to be as-
signed lower feature weights as they are rarely a
part of answer, while prepositions may have differ-
ent feature weights depending on their context. For
instance, of in kind of silly has an adjective on the
right, and is unlikely to be the Beginning of an an-
swer to a TREC-style question, as compared to in
when paired with a question on time, such as seen in
an answer in 90 days, where the preposition is fol-
lowed by a number then a noun.
Feature Description
edit=X type of edit feature. X: DEL,
DEL SUBTREE, DEL LEAF,
REN POS, REN DEP, REN POS DEP
or ALIGN.
X pos=?
X ner=?
X dep=?
Delete features. X is either DEL,
DEL SUBTREE or DEL LEAF. ?
represents the corresponding
POS/NER/DEP of the current token.
Xpos from=?f
Xpos to=?t
Xpos f t=?f ?t
Xner from=?f
Xner to=?t
Xner f t=?f ?t
Xdep from=?f
Xdep to=?t
Xdep f t=?f ?t
Rename features. X is either
REN POS, REN DEP or
REN POS DEP. Suppose word f in
answer is renamed to word t in
question, then ?f and ?t represent
corresponding POS/NER/DEP of f
and t.
align pos=?
align ner=?
align dep=?
Align features. ? represents the
corresponding POS/NER/DEP of the
current token.
Table 4: Features based on edit script for answer se-
quence tagging.
Question-type Chunking features do not capture
the connection between question word and an-
swer types. Thus they have to be combined
with question types. For instance, how many
questions are usually associated with numeric an-
swer types. We encode each major question-
type: who, whom, when, where, how many, how
much, how long, and then for each token, we
combine the question term with its chunking fea-
tures described in (most tokens have different fea-
tures because they have different POS/NER/DEP
types). One feature example of the QA pair
how much/100 dollars for the word 100 would be:
qword=how much|pos[t]=CD|pos[t+1]=NNS. We ex-
pect high weight for this feature since it is a good
pattern for matching question type and answer type.
Similar features also apply to what, which, why and
how questions, even though they do not indicate an
answer type as clearly as how much does.
Some extra features are designed for what/which
questions per required answer types. The question
862
dependency tree is analyzed and the Lexical Answer
Type (LAT) is extracted. The following are some
examples of LAT for what questions:
? color: what is Crips? gang color?
? animal: what kind of animal is an agouti?
The extra LAT=? feature is also used with chunking
features for what/which questions.
There is significant prior work in building spe-
cialized templates or classifiers for labeling question
types (Hermjakob, 2001; Li and Roth, 2002; Zhang
and Lee, 2003; Hacioglu and Ward, 2003; Metzler
and Croft, 2005; Blunsom et al, 2006; Moschitti
et al, 2007). We designed our shallow question
type features based on the intuitions of these prior
work, with the goal of having a relatively compact
approach that still extracts useful predictive signal.
One possible drawback, however, is that if an LAT is
not observed during training but shows up in testing,
the sequence tagger would not know which answer
type to associate with the question. In this case it
falls back to the more general qword=? feature and
will most likely pick the type of answers that are
mostly associated with what questions in training.
Edit script Our TED module produces an edit
trace for each word in a candidate sentence: the
word is either deleted, renamed (if there is a word
of the same lemma in the question tree) or strictly
aligned (if there is an identical node in the question
tree). A word in the deleted edit sequence is a cue
that it could be the answer. A word being aligned
suggests it is less likely to be an answer. Thus for
each word we extract features based on its edit type,
shown in Table 4.
These features are also appended with the token?s
POS/NER/DEP information. For instance, a deleted
noun usually carries higher edit feature weights than
an aligned adjective.
Alignment distance We observed that a candidate
answer often appears close to an aligned word (i.e.,
answer tokens tend to be located ?nearby? portions
of text that align across the pair), especially in com-
pound noun constructions, restrictive clauses, prepo-
sition phrases, etc. For instance, in the following
pair, the answer Limp Bizkit comes from the leading
compound noun:
? What is the name of Durst ?s group?
? Limp Bizkit lead singer Fred Durst did a lot ...
Past work has designed large numbers of specific
templates aimed at these constructions (Soubbotin,
2001; Ravichandran et al, 2003; Clark et al, 2003;
Sneiders, 2002). Here we use a single general fea-
ture that we expect to pick up much of this signal,
without the significant feature engineering.
Thus we incorporated a simple feature to roughly
model this phenomenon. It is defined as the distance
to the nearest aligned nonstop word in the original
word order. In the above example, the only aligned
nonstop word is Durst. Then this nearest alignment
distance feature for the word Limp is:
nearest dist to align(Limp):5
This is the only integer-valued feature. All other
features are binary-valued. Note this feature does
not specify answer types: an adverb close to an
aligned word can also be wrongly taken as a strong
candidate. Thus we also include a version of the
POS/NER/DEP based feature for each token:
? nearest dist pos(Limp)=NNP
? nearest dist dep(Limp)=NMOD
? nearest dist ner(Limp)=B-PERSON
3.3 Overproduce-and-vote
We make an assumption that each sentence produces
a candidate answer and then vote among all answer
candidates to select the most-voted as the answer to
the original question. Specifically, this overproduce-
and-vote strategy applies voting in two places:
1. If there are overlaps between two answer candi-
dates, a partial vote is performed. For instance,
for a when question, if one answer candidate is
April , 1994 and the other is 1994, then besides
the base vote of 1, both candidates have an ex-
tra partial vote of #overlap/#total words = 1/4. We
call this adjusted vote.
2. If the CRF fails to find an answer, we still try to
?force? an answer out of the tagged sequence,
O?s). thus forced vote. Due to its lower credi-
bility (the sequence tagger does not think it is
an answer), we manually downweight the pre-
diction score by a factor of 0.1 (divide by 10).
863
During what war d id Nimi tz serve ?
O O:0.921060 Conant
O O:0.991168 had
O O:0.997307 been
O O:0.998570 a
O O:0.998608 photographer
O O:0.999005 f o r
O O:0.877619 Adm
O O:0.988293 .
O O:0.874101 Chester
O O:0.924568 Nimi tz
O O:0.970045 dur ing
B?ANS O:0.464799 World
I?ANS O:0.493715 War
I?ANS O:0.449017 I I
O O:0.915448 .
Figure 3: A sample sequence tagging output that
fails to predict an answer. From line 2 on, the first
column is the reference output and the second col-
umn is the model output with the marginal probabil-
ity for predicated labels. Note that World War II has
much lower probabilities as an O than others.
The modified score for an answer candidate is thus:
total vote = adjusted vote + 0.1 ? forced vote. To
compute forced vote, we make the following obser-
vation. Sometimes the sequence tagger does not tag
an answer in a candidate sentence at all, if there
is not enough probability mass accumulated for B-
ANS. However, a possible answer can still be caught
if it has an ?outlier? marginal probability. Figure 3
shows an example. The answer candidate World War
II has a much lower marginal probability as an ?O?
but still not low enough to be part of B-ANS/I-ANS.
To catch such an outlier, we use Median Absolute
Deviation (MAD), which is the median of the abso-
lute deviation from the median of a data sequence.
Given a data sequence x, MAD is defined as:
MAD(x) = median(| x?median(x) |)
Compared to mean value and standard deviation,
MAD is more robust against the influence of out-
liers since it does not directly depend on them. We
select those words whose marginal probability is 50
times of MAD away from the median of the whole
sequence as answer candidates. They contribute to
the forced vote. Downweight ratio (0.1) and MAD
System Train Prec.% Rec.% F1%
CRF
TRAIN 55.7 43.8 49.1
TRAIN-ALL 67.2 50.6 57.7
CRF
+WNsearch
TRAIN 58.6 46.1 51.6
TRAIN-ALL 66.7 49.4 56.8
CRF forced
TRAIN 54.5 53.9 54.2
TRAIN-ALL 60.9 59.6 60.2
CRF forced
+WNsearch
TRAIN 55.2 53.9 54.5
TRAIN-ALL 63.6 62.9 63.3
Table 5: Performance on TEST. ?CRF? only takes
votes from candidates tagged by the sequence tag-
ger. ?CRF forced? (described in ?3.3) further col-
lects answer candidates from sentences that CRF
does not tag an answer by detecting outliers.
ratio (50) were hand-tuned on DEV.9
4 Experiments
4.1 QA Results
The dataset listed in Table 2 was not designed to
include an answer for each positive answer sen-
tence, but only a binary indicator on whether a sen-
tence contains an answer. We used the answer pat-
tern files (in Perl regular expressions) released along
with TREC8-13 to pinpoint the exact answer frag-
ments. Then we manually checked TRAIN, DEV, and
TEST for errors. TRAIN-ALL already came as a noisy
dataset so we did not manually clean it, also due to
its large size.
We trained on only the positive examples of
TRAIN and TRAIN-ALL separately with CRFsuite
(Okazaki, 2007). The reason for training solely with
positive examples is that they only constitute 10% of
all training data and if trained on all, the CRF tagger
was very biased on negative examples and reluctant
to give an answer for most of the questions. The
CRF tagger attempted an answer for about 2/3 of all
questions when training on just positive examples.
DEV was used to help design features. A practi-
cal benefit of our compact approach is that an entire
round of feature extraction, training on TRAIN and
testing on DEV took less than one minute. Table 5
9One might further improve this by leveraging the probabil-
ity of a sentence containing an answer from the QA pair ranker
described in Section 2 or via the conditional probability of the
sequence labels, p(y | x), under the CRF.
864
reports F1 scores on both the positive and negative
examples of TEST.
Our baseline model, which aligns the question
word with some content word in the answer sen-
tence,10 achieves 31.4% in F1. This model does not
require any training. ?CRF? only takes votes from
those sentences with an identified answer. It has the
best precision among all models. ?CRF forced? also
detects outliers from sentences not tagged with an
answer. Large amount of training data, even noisy,
is helpful. In general TRAIN-ALL is able to boost the
F1 value by 7 ? 8%. Also, the overgenerate-and-
vote strategy, used by the ?forced? approach, greatly
increased recall and achieved the best F1 value.
We also experimented with the two methods uti-
lizing WordNet in Section 2.2 , i.e., WNsearch and
WNfeature. In general, WNsearch helps F1 and
yields the best score (63.3%) for this task. For
WNfeature11 we observed that the CRF model con-
verged to a larger objective likelihood with these
features. However, it did not make a difference in
F1 after overgenerate-and-vote.
Finally, we found it difficult to do a head-to-head
comparison with other QA systems on this task.12
Thus we contribute this dataset to the community,
hoping to solicit direct comparisons in the future.
Also, we believe our chunking and question-type
features capture many intuitions most current QA
systems rely on, while our novel features are based
on TED. We further conduct an ablation test to com-
pare traditional and new QA features.
4.2 Ablation Test
We did an ablation test for each of the four types of
features. Note that the question type features are
used in combination with chunking features (e.g.,
qword=how much|pos[t]=CD|pos[t+1]=NN), while
the chunking feature is defined over POS/NER/DEP
10This only requires minimal modification to the original
TED algorithm: the question word is aligned with a certain
word in the answer tree instead of being inserted. Then the
whole subtree headed by the aligned word counts as the answer.
11These are binary features indicating whether an answer
candidate has a WordNet relation ( c.f. ?2.2) with the LAT.
For instance, tennis is a hyponym of the LAT word sport in the
what sport question in Figure 1.
12Reasons include: most available QA systems either retrieve
sentences from the web, have different preprocessing steps, or
even include templates learned from our test set.
CRF Forced CRF Forced
All 49.1 54.2 -above 3 19.4 25.3
-POS 44.7 48.9 -EDIT 44.3 47.5
-NER 44.0 50.8 -ALIGN 47.4 51.1
-DEP 49.4 54.5 -above 2 40.5 42.0
Table 6: F1 based on feature ablation tests.
NONE CHUNKING CHUNKING+TEDFeatures Used
0
10
20
30
40
50
60
F1(%
) 31.4
40.5
49.1
42.0
54.2
F1 with Different Features
BaselineCRFCRF forced
Figure 4: Impact of adding features based on chunk-
ing and question-type (CHUNKING) and tree edits
(TED), e.g., EDIT and ALIGN.
separately. We tested the CRF model with deletion
of one of the following features each time:
? POS, NER or DEP. These features are all com-
bined with question types.
? The three of the above. Deletion of these fea-
tures also deletes question type feature implic-
itly.
? EDIT. Features extracted from edit script.
? ALIGN. Alignment distance features.
? The two of the above, based on the TED model.
Table 6 shows the F1 scores of ablation test when
trained on TRAIN. NER and EDIT are the two single
most significant features. NER is important because
it closely relates question types with answer entity
types (e.g., qword=who|ner[t]=PERSON). EDIT is
also important because it captures the syntactic asso-
ciation between question tree and answer tree. Tak-
ing out all three POS/NER/DEP features means the
chunking and question type features do not fire any-
more. This has the biggest impact on F1. Note the
feature redundancy here: the question type features
are combined with all three POS/NER/DEP features
865
thus taking out a single one does not decrease per-
formance much. However, since TED related fea-
tures do not combine question type features, taking
out all three POS/NER/DEP features decreases F1 by
30%. Without TED related features (both EDIT and
ALIGN) F1 also drops more than 10%.
Figure 4 is a bar chart showing how much im-
provement each feature brings. While having a
baseline model with 31.4% in F1, traditional fea-
tures based on POS/DEP/NER and question types
brings a 10% increase with a simple sequence tag-
ging model (second bar labeled ?CHUNKING? in
the figure). Furthermore, adding TED based features
to the model boosted F1 by another 10%.
5 Conclusion
Answer extraction is an essential task for any text-
based question-answering system to perform. In this
paper, we have cast answer extraction as a sequence
tagging problem by deploying a fast and compact
CRF model with simple features that capture many
of the intuitions in prior ?deep pipeline? approaches.
We introduced novel features based on TED that
boosted F1 score by 10% compared with the use of
more standard features. Besides answer extraction,
our modified design of the TED model is the state
of the art in the task of ranking QA pairs. Finally,
to improve the community?s ability to evaluate QA
components without requiring increasingly imprac-
tical end-to-end implementations, we have proposed
answer extraction as a subtask worth evaluating in
its own right, and contributed a dataset that could
become a potential standard for this purpose. We
believe all these developments will contribute to the
continuing improvement of QA systems in the fu-
ture.
Acknowledgement We thank Vulcan Inc. for
funding this work. We also thank Michael Heil-
man and Mengqiu Wang for helpful discussion and
dataset, and the three anonymous reviewers for in-
sightful comments.
References
Nikolaus Augsten, Denilson Barbosa, Michael Bo?hlen,
and Themis Palpanas. 2010. TASM: Top-k Approx-
imate Subtree Matching. In Proceedings of the Inter-
national Conference on Data Engineering (ICDE-10),
pages 353?364, Long Beach, California, USA, March.
IEEE Computer Society.
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
learning, 34(1):211?231.
P. Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science,
337(1):217?239.
P. Blunsom, K. Kocik, and J.R. Curran. 2006. Question
classification with log-linear models. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, pages 615?616. ACM.
Peter Clark, Vinay Chaudhri, Sunil Mishra, Je?ro?me
Thome?re?, Ken Barker, and Bruce Porter. 2003. En-
abling domain experts to convey questions to a ma-
chine: a modified, template-based approach. In
Proceedings of the 2nd international conference on
Knowledge Capture, pages 13?19, New York, NY,
USA. ACM.
Shilin Ding, Gao Cong, Chin yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
In Proceedings of ACL-08: HLT.
D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek,
A.A. Kalyanpur, A. Lally, J.W. Murdock, E. Nyberg,
J. Prager, et al 2010. Building Watson: An overview
of the DeepQA project. AI Magazine, 31(3):59?79.
K. Hacioglu and W. Ward. 2003. Question classifica-
tion with support vector machines and error correcting
codes. In Proceedings of NAACL 2003, short papers,
pages 28?30.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia.
U. Hermjakob. 2001. Parsing and question classification
for question answering. In Proceedings of the work-
shop on Open-domain question answering-Volume 12,
pages 1?6.
Milen Kouylekov and Bernardo Magnini. 2005. Recog-
nizing textual entailment with tree edit distance algo-
rithms. In PASCAL Challenges on RTE, pages 17?20.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
866
In Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 282?289, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL 2002, pages 1?7.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Is it the right answer?: exploiting web redundancy for
answer validation. In Proceedings of ACL 2002, pages
425?432.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL 2005, pages 91?98.
D. Metzler and W.B. Croft. 2005. Analysis of statistical
question classification for fact-based questions. Infor-
mation Retrieval, 8(3):481?504.
D. Moldovan, M. Pas?ca, S. Harabagiu, and M. Surdeanu.
2003. Performance issues and error analysis in an
open-domain question answering system. ACM Trans-
actions on Information Systems (TOIS), 21(2):133?
154.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar.
2007. Exploiting syntactic and shallow semantic ker-
nels for question answer classification. In Proceedings
of ACL 2007, volume 45, page 776.
Naoaki Okazaki. 2007. CRFsuite: a fast implementation
of Conditional Random Fields (CRFs).
A. Penas, A. Rodrigo, V. Sama, and F. Verdejo. 2008.
Testing the reasoning for question answering valida-
tion. Journal of Logic and Computation, 18(3):459?
474.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answering. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP
1996, volume 1, pages 133?142.
Deepak Ravichandran, Abraham Ittycheriah, and Salim
Roukos. 2003. Automatic derivation of surface text
patterns for a maximum entropy based question an-
swering system. In Proceedings of NAACL 2003, short
papers, pages 85?87, Stroudsburg, PA, USA.
D. C. Reis, P. B. Golgher, A. S. Silva, and A. F. Laen-
der. 2004. Automatic web news extraction using tree
edit distance. In Proceedings of the 13th international
conference on World Wide Web, pages 502?511, New
York, NY, USA. ACM.
A?. Rodrigo, A. Pen?as, and F. Verdejo. 2009. Overview of
the answer validation exercise 2008. Evaluating Sys-
tems for Multilingual and Multimodal Information Ac-
cess, pages 296?313.
H. Shima, N. Lao, E. Nyberg, and T. Mitamura. 2008.
Complex cross-lingual question answering as sequen-
tial classification and multi-document summarization
task. In Proceedings of NTICIR-7 Workshop, Japan.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 23?30, New York, June.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In CIKM ?07:
Proceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
pages 623?632, New York, NY, USA. ACM.
E. Sneiders. 2002. Automated question answering:
template-based approach. Ph.D. thesis, KTH.
Martin M. Soubbotin. 2001. Patterns of potential answer
expressions as clues to the right answers. In Proceed-
ings of the Tenth Text REtrieval Conference (TREC
2001).
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of ACL 2010, pages 1164?1172,
Stroudsburg, PA, USA.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 22?32,
Prague, Czech Republic, June.
D. Zhang and W.S. Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related prob-
lems. SIAM J. Comput., 18(6):1245?1262, December.
867
Proceedings of NAACL-HLT 2013, pages 1010?1019,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Broadly Improving User Classification via
Communication-Based Name and Location Clustering on Twitter
Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, David Yarowsky
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
shane.a.bergsma@gmail.com, mdredze@cs.jhu.edu, vandurme@cs.jhu.edu, taw@jhu.edu, yarowsky@cs.jhu.edu
Abstract
Hidden properties of social media users, such
as their ethnicity, gender, and location, are of-
ten reflected in their observed attributes, such
as their first and last names. Furthermore,
users who communicate with each other of-
ten have similar hidden properties. We pro-
pose an algorithm that exploits these insights
to cluster the observed attributes of hundreds
of millions of Twitter users. Attributes such
as user names are grouped together if users
with those names communicate with other
similar users. We separately cluster millions
of unique first names, last names, and user-
provided locations. The efficacy of these clus-
ters is then evaluated on a diverse set of clas-
sification tasks that predict hidden users prop-
erties such as ethnicity, geographic location,
gender, language, and race, using only pro-
file names and locations when appropriate.
Our readily-replicable approach and publicly-
released clusters are shown to be remarkably
effective and versatile, substantially outper-
forming state-of-the-art approaches and hu-
man accuracy on each of the tasks studied.
1 Introduction
There is growing interest in automatically classify-
ing users in social media by various hidden prop-
erties, such as their gender, location, and language
(e.g. Rao et al (2010), Cheng et al (2010), Bergsma
et al (2012)). Predicting these and other proper-
ties for users can enable better advertising and per-
sonalization, as well as a finer-grained analysis of
user opinions (O?Connor et al, 2010), health (Paul
and Dredze, 2011), and sociolinguistic phenomena
(Eisenstein et al, 2011). Classifiers for user prop-
erties often rely on information from a user?s social
network (Jernigan and Mistree, 2009; Sadilek et al,
2012) or the textual content they generate (Pennac-
chiotti and Popescu, 2011; Burger et al, 2011).
Here, we propose and evaluate classifiers that bet-
ter exploit the attributes that users explicitly provide
in their user profiles, such as names (e.g., first names
like Mary, last names like Smith) and locations (e.g.,
Brasil). Such attributes have previously been used as
?profile features? in supervised user classifiers (Pen-
nacchiotti and Popescu, 2011; Burger et al, 2011;
Bergsma et al, 2012). There are several motivations
for exploiting these data. Often the only informa-
tion available for a user is a name or location (e.g.
for a new user account). Profiles also provide an
orthogonal or complementary source of information
to a user?s social network and textual content; gains
based on profiles alone should therefore add to gains
based on other data. The decisions of profile-based
classifiers could also be used to bootstrap training
data for other classifiers that use complementary fea-
tures.
Prior work has encoded profile attributes via lex-
ical or character-based features (e.g. Pennacchiotti
and Popescu (2011), Burger et al (2011), Bergsma
et al (2012)). Unfortunately, due to the long-tailed
distribution of user attributes, a profile-based classi-
fier will encounter many examples at test time that
were not observed during training. For example,
suppose a user wassim hassan gives their location as
tanger. If the attribute tokens wassim, hassan, and
tanger do not occur in training (nor indicative sub-
1010
strings), then a classifier can only guess at the user?s
ethnicity and location. In social media, the preva-
lence of fake names and large variations in spelling,
slang, and language make matters worse.
Our innovation is to enhance attribute-based clas-
sifiers with new data, derived from the communica-
tions of Twitter users with those attributes. Users
with the name tokens wassim and hassan often talk
to users with Arab names like abdul and hussein.
Users listing their location as tanger often talk to
users from morocco. Since users who communicate
often share properties such as ethnicity and location
(?8), the user wassim hassan might be an Arab who
uses the French spelling of the city Tangier.
Our challenge is to encode these data in a form
readily usable by a classifier. Our approach is to
represent each unique profile attribute (e.g. tanger
or hassan) as a vector that encodes the communi-
cation pattern of users with that attribute (e.g. how
often they talk to users from morocco, etc.); we then
cluster the vectors to discover latent groupings of
similar attributes. Based on transitive (third party)
connections, tanger and tangier can appear in the
same cluster, even if no two users from these loca-
tions talk directly. To use the clusters in an attribute-
based classifier, we add new features that indicate
the cluster memberships of the attributes. Clustering
thus lets us convert a high-dimensional space of all
attribute pairs to a low-dimensional space of cluster
memberships. This makes it easier to share our data,
yields fewer parameters for learning, and creates at-
tribute groups that are interpretable to humans.
We cluster names and locations in a very large
corpus of 168 million Twitter users (?2) and use a
distributed clustering algorithm to separately clus-
ter millions of first names, last names, and user-
provided locations (?3). We evaluate the use of our
cluster data as a novel feature in supervised classi-
fiers, and compare our result to standard classifiers
using character and token-level features (?4). The
cluster data enables significantly improved perfor-
mance in predicting the gender, location, and lan-
guage of social media users, exceeding both ex-
isting state-of-the-art machine and human perfor-
mance (?6). Our cluster data can likewise im-
prove performance in other domains, on both es-
tablished and new NLP tasks as further evaluated
in this paper (?6). We also propose a way to
First names: maria, david, ana, daniel, michael, john,
alex, jessica, carlos, jose, chris, sarah, laura, juan
Last names: silva, santos, smith, garcia, oliveira, ro-
driguez, jones, williams, johnson, brown, gonzalez
Locations: brasil, indonesia, philippines, london,
jakarta, s?o paulo, rio de janeiro, venezuela, brazil
Table 1: Most frequent profile attributes for our collection
of 168 million Twitter users, in descending order
enhance a geolocation system by using commu-
nication patterns, and show strong improvements
over a hand-engineered baseline (?7). We share
our clusters with the community to use with other
tasks. The clusters, and other experimental data, are
available for download from www.clsp.jhu.edu/
~sbergsma/TwitterClusters/.
2 Attribute Associations on Twitter
Data and Processing Our raw Twitter data com-
prises the union of 2.2 billion tweets from 05/2009
to 10/2010 (O?Connor et al, 2010), 1.8 billion
tweets collected from 07/2011 to 08/2012, and 80
million tweets collected from followers of 10 thou-
sand location and language-specific Twitter feeds.
We implemented each stage of processing using
MapReduce (Dean and Ghemawat, 2008). The total
computation (from extracting profiles to clustering
attributes) was 1300 days of wall-clock CPU time.
Attribute Extraction Tweets provide the name
and self-reported location of the tweeter. We find
126M unique users with these attributes in our data.
When tweets mention other users via an @user con-
struction, Twitter also includes the profile name of
the mentioned user; we obtain a further 42M users
from these cases. We then normalize the extracted
attributes by converting to lower-case, deleting sym-
bols, numbers, and punctuation, and removing com-
mon honorifics and suffixes like mr/mrs and jr/sr.
Common prefixes like van and de la are joined to
the last-name token.1 This processing yields 8.3M
1www.clsp.jhu.edu/~sbergsma/TwitterClusters/
also provides our scripts for normalizing attributes. The scripts
can be used to ensure consistency/compatibility between
arbitrary datasets and our shared cluster data. Note we use no
special processing for the companies, organizations, and spam-
mers among our users, nor for names arising from different
conventions (e.g. 1-word names, reversed first/last names).
1011
henrik: fredrik 5.87, henrik 5.82, anders 5.73, johan
5.69, andreas 5.59, martin 5.54, magnus 5.41
courtney: taylor 8.03, ashley 7.92, courtney 7.92,
emily 7.91, lauren 7.82, katie 7.72, brittany 7.69
ilya: sergey 5.85, alexey 5.62, alexander 5.59, dmitry
5.51, ????????? 5.46, anton 5.44, andrey 5.40
Table 2: Top associates and PMIs for three first names.
unique locations, 7.4M unique last names, and 5.5M
unique first names. These three sets provide the tar-
get attributes that we cluster in ?3. Table 1 shows
the most frequent names in each of these three sets.
User-User Links We extract each user mention as
an undirected communication link between the user
tweeting and the mentioned user (including self-
mentions but not retweets). We consider each user-
user link as a single event; we count it once no mat-
ter how often two specific users interact. We extract
436M user-user links in total.
Attribute-Attribute Pairs We use our profile data
to map each user-user link to an attribute-attribute
pair; we separately count each pair of first names,
last names, and locations. For example, the first-
name pair (henrik, fredrik) occurs 181 times. Rather
than using the raw count, we calculate the associa-
tion between attributes a1 and a2 via their pointwise
mutual information (PMI), following prior work in
distributional clustering (Lin and Wu, 2009):
PMI(a1, a2) = log
P(a1, a2)
P(a1)P(a2)
PMI essentially normalizes the co-occurrence by
what we would expect if the attributes were indepen-
dently distributed. We smooth the PMI by adding a
count of 0.5 to all co-occurrence events.
The most highly-associated name attributes re-
flect similarities in ethnicity and gender (Table 2).
The most highly-ranked associates for locations are
often nicknames and alternate/misspellings of those
locations. For example, the locations charm city,
bmore, balto, westbaltimore, b a l t i m o r e, bal-
timoreee, and balitmore each have the U.S. city of
baltimore as their highest-PMI associate. We show
how this can be used to help geolocate users (?7).
3 Attribute Clustering
Representation We first represent each target at-
tribute as a feature vector, where each feature corre-
sponds to another attribute of the same type as the
target and each value gives the PMI between this at-
tribute and the target (as in Table 2).2 To help cluster
the long-tail of infrequent attributes, we also include
orthographic features. For first and last names, we
have binary features for the last 2 characters in the
string. For locations, we have binary features for
(a) any ideographic characters in the string and (b)
each token (with diacritics removed) in the string.
We normalize the feature vectors to unit length.
Distributed K-Means Clustering Our approach
to clustering follows Lin and Wu (2009) who used k-
means to cluster tens of millions of phrases. We also
use cosine similarity to compute the closest centroid
(i.e., we use the spherical k-means clustering algo-
rithm (Dhillon and Modha, 2001)). We keep track
of the average cosine similarity between each vector
and its nearest centroid; this average is guaranteed
to increase at each iteration.
Like Lin and Wu (2009), we parallelize the al-
gorithm using MapReduce. Each mapper finds the
nearest centroids for a portion of the vectors, while
also computing the partial sums of the vectors as-
signed to each centroid. The mappers emit the cen-
troid IDs as keys and the partial sums as values.
The Reducer aggregates the partial sums from each
partition and re-normalizes each sum vector to unit
length to obtain the new centroids. We also use an
inverted index at each iteration that, for each input
feature, lists which centroids each feature belongs
to. Using this index greatly speeds up the centroid
similarity computations.
Clustering Details We cluster with nine separate
configurations: over first names, last names, and lo-
cations, and each with 50, 200, and 1000 cluster
centroids (denoted C50, C200, and C1000). Since k-
2We decided to restrict the features for a target to be at-
tributes of the same type (e.g., we did not use last name as-
sociations for a first name target) because each attribute type
conveys distinct information. For example, first names convey
gender and age more than last names. By separately cluster-
ing representations using first names, last names, and locations,
each clustering can capture its own distinct latent-class associa-
tions.
1012
Cluster 463 (Serbian): pavlovic?, jovanovic, jo-
vanovic?, stankovic?, srbija, markovic?, petrovic?,
radovic, nenad, milenkovic, nikolic, sekulic, todor-
ovic, stojanovic, petrovic, aleksic, ilic, markovic
Cluster 544 (Black South African): ngcobo, nkosi,
dlamini, ndlovu, mkhize, mtshali, sithole, mathebula,
mthembu, khumalo, ngwenya, shabangu, nxumalo,
buthelezi, radebe, mabena, zwane, mbatha, sibiya
Cluster 449 (Turkish): s?ahin, ?elik, ?zt?rk, ko?, ?ak?r,
karatas?, aktas?, g?ng?r, ?zkan, balc?, g?m?s?, akkaya,
gen?, sar?, y?ksel, g?nes?, yig?it, yal??n, orhan, sag?lam,
g?ler, demirci, k???k, yavuz, bayrak, ?zcan, altun
Cluster 656 (Indonesian): utari, oktaviana, apriani,
mustika, septiana, febrianti, kurniawati, indriani, nur-
janah, septian, cahya, anggara, yuliani, purnamasari,
sukma, wijayanti, pramesti, ningrum, yanti, wulansari
Table 3: Example C1000 last-name clusters
Cluster 56 [sim=0.497]: gregg, bryn, bret, stewart,
lyndsay, howie, elyse, jacqui, becki, rhett, meaghan,
kirstie, russ, jaclyn, zak, katey, seamus, brennan,
fraser, kristie, stu, jaimie, kerri, heath, carley, griffin
Cluster 104 [sim=0.442]: stephon, devonte, deion,
demarcus, janae, tyree, jarvis, donte, dewayne, javon,
destinee, tray, janay, tyrell, jamar, iesha, chyna,
jaylen, darion, lamont, marquise, domonique, alexus
Cluster 132 [sim=0.292]: moustafa, omnya, menna-
tallah, ?C?@, shorouk, ragab, ?


??, radwa, moemen,
mohab, hazem, yehia, ? K
Q k, Z @Q?? @, mennah, ?
 Q?? ?,
abdelrahman, ?


	
????, H. 	Qk, Q?A

K, nermeen, hebatallah
...
Table 4: C200 soft clustering for first name yasmeen
means is not guaranteed to reach a global optimum,
we use ten different random initializations for each
configuration, and select the one with the highest av-
erage similarity after 20 iterations. We run this one
for an additional 30 iterations and take the output as
our final set of centroids for that configuration.
The resulting clusters provide data that could help
classify hidden properties of social media users. For
example, Table 3 shows that last names often clus-
ter by ethnicity, even at the sub-national level (e.g.
Zulu tribe surnames nkosi, dlamini, mathebula, etc.).
Note the Serbian names include two entries that are
not last names: srbija, the Serbian word for Serbia,
and nenad, a common Serbian first name.
Soft Clustering Rather than assigning each at-
tribute to its single highest-similarity cluster, we can
assign each vector to its N most similar clusters.
These soft-cluster assignments often reflect different
social groups where a name or location is used. For
example, the name yasmeen is similar to both com-
mon American names (Cluster 56), African Ameri-
can names (Cluster 104), and Arabic names (Clus-
ter 132) (Table 4). As another example, the C1000
assignments for the location trujillo comprise sep-
arate clusters containing towns and cities in Peru,
Venezuela, Colombia, etc., reflecting the various
places in the Latin world with this name. In general,
the soft cluster assignment is a low-dimensional rep-
resentation of each of our attributes. Although it can
be interpretable to humans, it need not be in order to
be useful to a classifier.
4 Classification with Cluster Features
Our motivating problem is to classify users for hid-
den properties such as their gender, location, race,
ethnicity, and language. We adopt a discriminative
solution. We encode the relevant data for each in-
stance in a feature vector and train a (linear) support
vector machine classifier (Cortes and Vapnik, 1995).
SVMs represent the state-of-the-art on many NLP
classification tasks, but other classifiers could also
be used. For multi-class classification, we use a one-
versus-all strategy, a competitive approach on most
multi-class problems (Rifkin and Klautau, 2004).
The input to our system is one or more observed
user attributes (e.g. name and location fields from
a user profile). We now describe how features are
created from these attributes in both state-of-the-art
systems and via our new cluster data.
Token Features (Tok) are binary features that in-
dicate the presence of a specific attribute (e.g., first-
name=bob). Burger et al (2011) and Bergsma et al
(2012) used Tok features to encode user profile fea-
tures. For multi-token fields (e.g. location), our Tok
features also indicate the specific position of each
token (e.g., loc1=s?o, loc2=paulo, locN=brasil).
Character N-gram Features (Ngm) give the
count of all character n-grams of length 1-to-4 in the
input. Ngm features have been used in user classifi-
cation (Burger et al, 2011) and represent the state-
1013
of-the-art in detecting name ethnicity (Bhargava and
Kondrak, 2010). We add special begin/end charac-
ters to the attributes to mark the prefix and suffix po-
sitions. We also use a smoothed log-count; we found
this to be most effective in preliminary work.
Cluster Features (Clus) indicate the soft-cluster
memberships of the attributes. We have features for
the top-2, 5, and 20 most similar clusters in the C50,
C200, and C1000 clusterings, respectively. Like Lin
and Wu (2009), we ?side-step the matter of choos-
ing the optimal value k in k-means? by using fea-
tures from clusterings at different granularities. Our
feature dimensions correspond to cluster IDs; fea-
ture values give the similarity to the cluster centroid.
Other strategies (e.g. hard clustering, binary fea-
tures) were less effective in preliminary work.
5 Classification Experiments
5.1 Methodology
Our main objective is to assess the value of us-
ing cluster features (Clus). We add these features
to classifiers using Tok+Ngm features, which repre-
sents the current state-of-the-art. We compare these
feature settings on both Twitter tasks (?5.2) and
tasks not related to social-media (?5.3). For each
task, we randomly divide the gold standard data into
50% train, 25% development and 25% test, unless
otherwise noted. As noted above, the gold-standard
datasets for all of our experiments are available for
download. We train our SVM classifiers using the
LIBLINEAR package (Fan et al, 2008). We optimize
the classifier?s regularization parameter on develop-
ment data, and report our final results on the held-
out test examples. We report accuracy: the propor-
tion of test examples classified correctly. For com-
parison, we report the accuracy of a majority-class
baseline on each task (Base).
Classifying hidden properties of social media
users is challenging (Table 5). Pennacchiotti and
Popescu (2011) even conclude that ?profile fields do
not contain enough good-quality information to be
directly used for user classification.? To provide in-
sight into the difficulty of the tasks, we had two hu-
mans annotate 120 examples from each of the test
sets, and we average their results to give a ?Human?
performance number. The two humans are experts in
Country: 53 possible countries
United States courtland dante cali baby
United States tinas twin on the court
Brazil thamires gomez macap? ap
Denmark marte clason NONE
Lang. ID: 9 confusable languages
Bulgarian valentina getova NONE
Russian borisenko yana edinburgh
Bulgarian NONE blagoevgrad
Ukrainian andriy kupyna ternopil
Farsi kambiz barahouei NONE
Urdu musadiq sanwal jammu
Ethnicity: 13 European ethnicities
German dennis hustadt
Dutch bernhard hofstede
French david coste
Swedish mattias bjarsmyr
Portuguese helder costa
Race: black or white
black kerry swain
black darrell foskey
white ty j larocca
black james n jones
white sean p farrell
Table 5: Examples of class (left) and input (names, loca-
tions) for some of our evaluation tasks.
this domain and have very wide knowledge of global
names and locations.
5.2 Twitter Applications
Country A number of recent papers have consid-
ered the task of predicting the geolocation of users,
using both user content (Cheng et al, 2010; Eisen-
stein et al, 2010; Hecht et al, 2011; Wing and
Baldridge, 2011; Roller et al, 2012) and social net-
work (Backstrom et al, 2010; Sadilek et al, 2012).
Here, we first predict user location at the level of
the user?s location country. To our knowledge, we
are the first to exploit user locations and names for
this prediction. For this task, we obtain gold data
from the portion of Twitter users who have GPS en-
abled (geocoded tweets). We were able to obtain a
very large number of gold instances for this task, so
selected only 10K for testing, 10K for development,
and retained the remaining 782K for training.
Language ID Identifying the language of users
is an important prerequisite for building language-
specific social media resources (Tromp and Pech-
1014
enizkiy, 2011; Carter et al, 2013). Bergsma et al
(2012) recently released a corpus of tweets marked
for one of nine languages grouped into three confus-
able character sets: Arabic, Farsi, and Urdu tweets
written in Arabic characters; Hindi, Nepali, and
Marathi written in Devanagari, and Russian, Bulgar-
ian, and Ukrainian written in Cyrillic. The tweets
were marked for language by native speakers via
Amazon Mechanical Turk. We again discard the
tweet content and extract each user?s first name, last
name, and user location as our input data, while tak-
ing the annotated language as the class label.
Gender We predict whether a Twitter user is male
or female using data from Burger et al (2011). This
data was created by linking Twitter users to struc-
tured profile pages on other websites where users
must select their gender. Unlike prior systems using
this data (Burger et al, 2011; Van Durme, 2012), we
make the predictions using only user names.
5.3 Other Applications
Origin Knowing the origin of a name can improve
its automatic pronunciation (Llitjos and Black,
2001) and transliteration (Bhargava and Kondrak,
2010). We evaluate our cluster data on name-origin
prediction using a corpus of names marked as ei-
ther Indian or non-Indian by Bhargava and Kondrak
(2010). Since names in this corpus are not marked
for entity type, we include separate cluster features
from both our first and last name clusters.
Ethnicity We also evaluate on name-origin data
from Konstantopoulos (2007). This data derives
from lists of football players on European national
teams; it marks each name (with diacritics removed)
as arising from one of 13 European languages. Fol-
lowing prior work, we test in two settings: (1) using
last names only, and (2) using first and last names.
Race We also evaluate our ability to identify eth-
nic groups at a sub-national level. To obtain data
for this task, we mined the publicly-available arrest
records on mugshots.com for the U.S. state of New
Jersey (a small but diverse and densely-populated
area). Over 99% of users were listed as either black
or white, and we structure the task as a binary clas-
sification problem between these two classes. We
predict the race of each person based purely on their
name; this contrasts with prior work in social media
which looked at identifying African Americans on
the basis of their Twitter content (Eisenstein et al,
2011; Pennacchiotti and Popescu, 2011).
6 Classification Results
Table 6 gives the results on each task. The system in-
corporating our novel Clus features consistently im-
proves over the Ngm+Tok system; all differences be-
tween All and Ngm+Tok are significant (McNemar?s,
p<0.01). The relative reduction in error from adding
Clus features ranges between 7% and 51%. The All
system including Clus features also exceeds human
performance on all studied tasks.
On Country, the U.S. is the majority class, oc-
curring in 42.5% of cases.3 It is impressive that
All so significantly exceeds Tok+Ngm (86.7% vs.
84.8%); with 782K training examples, we did not
expect such room for improvement. Both names and
locations play an important role: All achieves 66%
using names alone and 70% with only location. On
the subset of data where all three attributes are non-
empty, the full system achieves 93% accuracy.
Both feature classes are likewise important for
Lang. ID; All achieves 67% with only first+last
names, 72% with just locations, but 83% with both.
Our smallest improvement is on Gender. This
task is easier (with higher human/system accuracy)
and has plenty of training data (more data per class
than any other task); there is thus less room to im-
prove. Looking at the feature weights, the strongest-
weighted female cluster apparently captures a sub-
community of Justin Bieber fans (showing loyalty
with ?first names? jbieber, belieb, biebz, beliebing,
jbiebs, etc.). Just because a first name like madison
has a high similarity to this cluster does not imply
girls named Madison are Justin Bieber fans; it sim-
ply means that Madisons have similar names to the
friends of Justin Bieber fans (who tend to be girls).
Also, note that while the majority of the 34K users in
our training data are assigned this cluster somewhere
in their soft clustering, only 6 would be assigned this
3We tried other baselines: e.g., we predict countries if they
are substrings of the location (otherwise predicting U.S.); and
we predict countries if they often occur as a string following
the given location in our profile data (e.g., we predict Spain for
Madrid since Madrid, Spain is common). Variations on these
approaches consistently performed between 48% and 56%.
1015
Task Input
Num. Num.
Base Human Tok Ngm Clus
Tok+
All ?
Train Class Ngm
Country first+last+loc 781920 53 42.5 71.7 83.0 84.5 80.2 84.8 86.7 12.5
Lang. ID first+last+loc 2492 9 27.0 74.2 74.6 80.6 71.1 80.4 82.7 11.7
Gender first+last 33805 2 52.4 88.3 85.3 88.6 79.5 89.5 90.2 6.7
Origin entity name 500 2 52.4 80.4 - 75.6 81.2 75.6 88.0 50.8
Ethnicity last 6026 13 20.8 47.9 - 54.6 48.5 54.6 62.4 17.2
Ethnicity first+last 7457 13 21.2 53.3 67.6 77.5 73.6 78.4 81.3 13.4
Race first+last 7977 2 54.7 71.4 80.4 81.6 84.6 82.4 84.6 12.5
Table 6: Task details and accuracy (%) for attribute-based classification tasks. ? = relative error reduction (%) of All
(Tok+Ngm+Clus) over Ngm+Tok. All always exceeds both Tok+Ngm and the human performance.
cluster in a hard clustering. This clearly illustrates
the value of the soft clustering representation.
Note the All system performed between 83% and
90% on each Twitter task. This level of performance
strongly refutes the prevailing notion that Twitter
profile information is useless in general (Pennac-
chiotti and Popescu, 2011) and especially for geolo-
cation (Cheng et al, 2010; Hecht et al, 2011).
We now move to applications beyond social me-
dia. Bhargava and Kondrak (2010) have the current
state-of-the-art on Origin and Ethnicity based on an
SVM using character-n-gram features; we reimple-
mented this as Ngm. We obtain a huge improvement
over their work using Clus, especially on Origin
where we reduce error by >50%.4 This improve-
ment can partly be attributed to the small amount of
training data; with fewer parameters to learn, Clus
learns more from limited data than Ngm. We like-
wise see large improvements over the state-of-the-
art on Ethnicity, on both last name and full name
settings.
Finally, Clus features also significantly improve
accuracy on the new Race task. Our cluster data can
therefore help to classify names into sub-national
groups, and could potentially be used to infer other
interesting communities such as castes in India and
religious divisions in many countries.
In general, the relative value of our cluster models
varies with the amount of training data; we see huge
gains on the smaller Origin data but smaller gains
on the large Gender set. Figure 1 shows how per-
formance of Clus and Ngm varies with training data
on Race. Again, Clus is especially helpful with less
4Note Tok is not used here because the input is a single token
and training and test splits have distinct instances.
 60
 65
 70
 75
 80
 85
 10  100  1000  10000
A
cc
ur
ac
y
Number of training examples
Clus
Ngm
Figure 1: Learning curve on Race: Clus perform as well
with 30 training examples as Ngm features do with 1000.
data; thousands of training examples are needed for
Ngm to rival the performance of Clus using only a
handful. Since labeled data is generally expensive
to obtain or in short supply, our method for exploit-
ing unlabeled Twitter data can both save money and
improve top-end performance.
7 Geolocation by Association
There is a tradition in computational linguistics of
grouping words both by the similarity of their con-
text vectors (Hindle, 1990; Pereira et al, 1993; Lin,
1998) and directly by their statistical association in
text (Church and Hanks, 1990; Brown et al, 1992).
While the previous sections explored clusters built
by vector similarity, we now explore a direct appli-
cation of our attribute association data (?2).
We wish to use this data to improve an existing
Twitter geolocation system based on user profile lo-
cations. The system operates as follows: 1) normal-
1016
ize user-provided locations using a set of regular ex-
pressions (e.g. remove extra spacing, punctuation);
2) look up the normalized location in an alias list;
3) if found, map the alias to a unique string (target
location), corresponding to a structured location ob-
ject that includes geo-coordinates.
The alias list we are currently using is based on
extensive work in hand-writing aliases for the most
popular Twitter locations. For example, the current
aliases for Nashville, Tennessee include nashville,
nashville tn, music city, etc. Our objective is to im-
prove on this human-designed list by automatically
generating aliases using our association data.
Aliases by Association For each target, we pro-
pose new aliases from the target?s top-PMI asso-
ciates (?2). To become an alias, the PMI between
the alias and target must be above a threshold,
the alias must occur more than a fixed number of
times in our profile data, the alias must be within
the top-N1 associates of the target, and the target
must be within the top-N2 associates of the alias.
We merge our automatic aliases with the manually-
written aliases. The new aliases for Nashville, Ten-
nessee include east nashville, nashville tenn, music
city usa, nashvegas, cashville tn, etc.
Experiments To evaluate the geolocation system,
we use tweets from users with GPS enabled (?5.2).
For each tweet, we resolve the location using the
system and compare to the gold coordinates. The
system can skip a location if it does not match the
alias list; more than half of the locations are skipped,
which is consistent with prior work (Hecht et al,
2011). We evaluate the alias lists using two mea-
sures: (1) its coverage: the percentage of locations it
resolves, and (2) its precision: of the ones resolved,
the percentage that are correct. We define a correct
resolution to be one where the resolved coordinates
are within 50 miles of the gold coordinates.
We use 56K gold tweets to tune the parameters of
our automatic alias-generator, trading off coverage
and precision. We tune such that the system using
these aliases obtains the highest possible coverage,
while being at least as precise as the baseline system.
We then evaluate both the baseline set of aliases and
our new set on 56K held-out examples.
Results On held-out test data, the geolocation sys-
tem using baseline aliases has a coverage of 38.7%
and a precision of 59.5%. Meanwhile, the system
using the new aliases has a coverage of 44.6% and
a precision of 59.4%. With virtually the same pre-
cision, the new aliases are thus able to resolve 15%
more users. This provides an immediate benefit to
our existing Twitter research efforts.
Note that our alias lists can be viewed as clus-
ters of locations. In ongoing work, we are exploring
techniques based on discriminative learning to infer
alias lists using not only Clus information but also
Ngm and Tok features as in the previous sections.
8 Related Work
In both real-world and online social networks, ?peo-
ple socialize with people who are like them in terms
of gender, sexual orientation, age, race, education,
and religion? (Jernigan and Mistree, 2009). So-
cial media research has exploited this for two main
purposes: (1) to predict friendships based on user
properties, and (2) to predict user properties based
on friendships. Friendship prediction systems (e.g.
Facebook?s friend suggestion tool) use features such
as whether both people are computer science ma-
jors (Taskar et al, 2003) or whether both are at the
same location (Crandall et al, 2010; Sadilek et al,
2012). The inverse problem has been explored in the
prediction of a user?s location given the location of
their peers (Backstrom et al, 2010; Cho et al, 2011;
Sadilek et al, 2012). Jernigan and Mistree (2009)
predict a user?s sexuality based on the sexuality of
their Facebook friends, while Garera and Yarowsky
(2009) predict a user?s gender partly based on the
gender of their conversational partner. Jha and El-
hadad (2010) predict the cancer stage of users of
an online cancer discussion board; they derive com-
plementary information for prediction from both the
text a user generates and the cancer stage of the peo-
ple that a user interacts with.
The idea of clustering data in order to provide fea-
tures for supervised systems has been successfully
explored in a range of NLP tasks, including named-
entity-recognition (Miller et al, 2004; Lin and Wu,
2009; Ratinov and Roth, 2009), syntactic chunking
(Turian et al, 2010), and dependency parsing (Koo
et al, 2008; T?ckstr?m et al, 2012). In each case,
1017
the clusters are derived from the distribution of the
words or phrases in text, not from their communica-
tion pattern. It would be interesting to see whether
prior distributional clusters can be combined with
our communication-based clusters to achieve even
better performance. Indeed, there is evidence that
features derived from text can improve the predic-
tion of name ethnicity (Pervouchine et al, 2010).
There has been an explosion of work in recent
years in predicting user properties in social net-
works. Aside from the work mentioned above that
analyzes a user?s social network, a large amount
of work has focused on inferring user properties
based on the content they generate (e.g. Burger
and Henderson (2006), Schler et al (2006), Rao
et al (2010), Mukherjee and Liu (2010), Pennac-
chiotti and Popescu (2011), Burger et al (2011), Van
Durme (2012)).
9 Conclusion and Future Work
We presented a highly effective and readily repli-
cable algorithm for generating language resources
from Twitter communication patterns. We clustered
user attributes based on both the communication of
users with those attributes as well as substring sim-
ilarity. Systems using our clusters significantly out-
perform state-of-the-art algorithms on each of the
tasks investigated, and exceed human performance
on each task as well. The power and versatility of
our clusters is exemplified by the fact we reduce er-
ror by a larger margin on each of the non-Twitter
tasks than on any Twitter task itself.
Twitter provides a remarkably large sample and
effectively a partial census of much of the world?s
population, with associated metadata, descriptive
content and sentiment information. Our ability to
accurately assign numerous often unspecified prop-
erties such as race, gender, language and ethnicity to
such a large user sample substantially increases the
sociological insights and correlations one can derive
from such data.
References
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical predic-
tion with social and spatial proximity. In Proc. WWW,
pages 61?70.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
HLT-NAACL, pages 693?696.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
John D. Burger and John C. Henderson. 2006. An ex-
ploration of observable features related to blogger age.
In Proc. AAAI Spring Symposium: Computational Ap-
proaches to Analyzing Weblogs, pages 15?20.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on Twitter. In
Proc. EMNLP, pages 1301?1309.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog Language Identification: Overcom-
ing the Limitations of Short, Unedited and Idiomatic
Text. Language Resources and Evaluation Journal.
(forthcoming).
Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010.
You are where you tweet: a content-based approach
to geo-locating Twitter users. In Proc. CIKM, pages
759?768.
Eunjoon Cho, Seth A. Myers, and Jure Leskovec. 2011.
Friendship and mobility: user movement in location-
based social networks. In Proc. KDD, pages 1082?
1090.
Kenneth W. Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273?297.
David J. Crandall, Lars Backstrom, Dan Cosley, Sid-
dharth Suri, Daniel Huttenlocher, and Jon Kleinberg.
2010. Inferring social ties from geographic coinci-
dences. Proceedings of the National Academy of Sci-
ences, 107(52):22436?22441.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Mach. Learn., 42(1-2):143?175.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. EMNLP, pages
1277?1287.
1018
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. 2011.
Discovering sociolinguistic associations with struc-
tured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proc. ACL-IJCNLP, pages 710?718.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H. Chi.
2011. Tweets from Justin Bieber?s heart: the dynamics
of the location field in user profiles. In Proc. CHI,
pages 237?246.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proc. ACL, pages
268?275.
Carter Jernigan and Behram F. T. Mistree. 2009. Gaydar:
Facebook friendships expose sexual orientation. First
Monday, 14(10). [Online].
Mukund Jha and Noemie Elhadad. 2010. Cancer stage
prediction based on patient online discourse. In Proc.
2010 Workshop on Biomedical Natural Language Pro-
cessing, pages 64?71.
Stasinos Konstantopoulos. 2007. What?s in a name? In
Proc. Computational Phonology Workshop, RANLP.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL-08: HLT, pages 595?603.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proc. ACL-IJCNLP,
pages 1030??1038.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. Coling-ACL, pages 768?774.
Ariadna Font Llitjos and Alan W. Black. 2001. Knowl-
edge of language origin improves pronunciation accu-
racy of proper names. In Proceedings of EuroSpeech-
01, pages 1919?1922.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proc. HLT-NAACL, pages 337?342.
Arjun Mukherjee and Bing Liu. 2010. Improving gender
classification of blog authors. In Proc. EMNLP, pages
207?217.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. ICWSM, pages 122?129.
Michael Paul and Mark Dredze. 2011. You are what you
tweet: Analyzing Twitter for public health. In Proc.
ICWSM, pages 265?272.
Marco Pennacchiotti and Ana-Maria Popescu. 2011. A
machine learning approach to Twitter user classifica-
tion. In Proc. ICWSM, pages 281?288.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Proc.
ACL, pages 183?190.
Vladimir Pervouchine, Min Zhang, Ming Liu, and
Haizhou Li. 2010. Improving name origin recogni-
tion with context features and unlabelled data. In Col-
ing 2010: Posters, pages 972?978.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. International Workshop on
Search and Mining User-Generated Contents, pages
37?44.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
CoNLL, pages 147?155.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101?
141.
Stephen Roller, Michael Speriosu, Sarat Rallapalli, Ben-
jamin Wing, and Jason Baldridge. 2012. Supervised
text-based geolocation using language models on an
adaptive grid. In Proc. EMNLP-CoNLL, pages 1500?
1510.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proc. WSDM, pages 723?732.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proc. AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 199?205.
Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer
of linguistic structure. In Proc. NAACL-HLT, pages
477?487.
Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne
Koller. 2003. Link prediction in relational data. In
Proc. NIPS, volume 15.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identication on short texts. In
Proc. 20th Machine Learning conference of Belgium
and The Netherlands, pages 27?34.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. ACL, pages
384?394.
Benjamin Van Durme. 2012. Streaming analysis of dis-
course participants. In Proc. EMNLP-CoNLL, pages
48?58.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proc. ACL, pages 955?964.
1019
Proceedings of the ACL 2010 Conference Short Papers, pages 231?235,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Online Generation of Locality Sensitive Hash Signatures
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Baltimore, MD 21211 USA
Ashwin Lall
College of Computing
Georgia Institute of Technology
Atlanta, GA 30332 USA
Abstract
Motivated by the recent interest in stream-
ing algorithms for processing large text
collections, we revisit the work of
Ravichandran et al (2005) on using the
Locality Sensitive Hash (LSH) method of
Charikar (2002) to enable fast, approxi-
mate comparisons of vector cosine simi-
larity. For the common case of feature
updates being additive over a data stream,
we show that LSH signatures can be main-
tained online, without additional approxi-
mation error, and with lower memory re-
quirements than when using the standard
offline technique.
1 Introduction
There has been a surge of interest in adapting re-
sults from the streaming algorithms community to
problems in processing large text collections. The
term streaming refers to a model where data is
made available sequentially, and it is assumed that
resource limitations preclude storing the entirety
of the data for offline (batch) processing. Statis-
tics of interest are approximated via online, ran-
domized algorithms. Examples of text applica-
tions include: collecting approximate counts (Tal-
bot, 2009; Van Durme and Lall, 2009a), finding
top-n elements (Goyal et al, 2009), estimating
term co-occurrence (Li et al, 2008), adaptive lan-
guage modeling (Levenberg and Osborne, 2009),
and building top-k ranklists based on pointwise
mutual information (Van Durme and Lall, 2009b).
Here we revisit the work of Ravichandran et al
(2005) on building word similarity measures from
large text collections by using the Locality Sensi-
tive Hash (LSH) method of Charikar (2002). For
the common case of feature updates being addi-
tive over a data stream (such as when tracking
lexical co-occurrence), we show that LSH signa-
tures can be maintained online, without additional
approximation error, and with lower memory re-
quirements than when using the standard offline
technique.
We envision this method being used in conjunc-
tion with dynamic clustering algorithms, for a va-
riety of applications. For example, Petrovic et al
(2010) made use of LSH signatures generated over
individual tweets, for the purpose of first story de-
tection. Streaming LSH should allow for the clus-
tering of Twitter authors, based on the tweets they
generate, with signatures continually updated over
the Twitter stream.
2 Locality Sensitive Hashing
We are concerned with computing the cosine sim-
ilarity of feature vectors, defined for a pair of vec-
tors ~u and ~v as the dot product normalized by their
lengths:
cosine?similarity(~u,~v) =
~u ? ~v
|~u||~v|
.
This similarity is the cosine of the angle be-
tween these high-dimensional vectors and attains
a value of one (i.e., cos (0)) when the vectors are
parallel and zero (i.e., cos (pi/2)) when orthogo-
nal.
Building on the seminal work of Indyk and
Motwani (1998) on locality sensitive hashing
(LSH), Charikar (2002) presented an LSH that
maps high-dimensional vectors to a much smaller
dimensional space while still preserving (cosine)
similarity between vectors in the original space.
The LSH algorithm computes a succinct signature
of the feature set of the words in a corpus by com-
puting d independent dot products of each feature
vector ~v with a random unit vector ~r, i.e.,
?
i viri,
and retaining the sign of the d resulting products.
Each entry of ~r is drawn from the distribution
N(0, 1), the normal distribution with zero mean
and unit variance. Charikar?s algorithm makes use
of the fact (proved by Goemans and Williamson
231
(1995) for an unrelated application) that the an-
gle between any two vectors summarized in this
fashion is proportional to the expected Hamming
distance of their signature vectors. Hence, we can
retain length d bit-signatures in the place of high
dimensional feature vectors, while preserving the
ability to (quickly) approximate cosine similarity
in the original space.
Ravichandran et al (2005) made use of this al-
gorithm to reduce the computation in searching
for similar nouns by first computing signatures for
each noun and then computing similarity over the
signatures rather than the original feature space.
3 Streaming Algorithm
In this work, we focus on features that can be
maintained additively, such as raw frequencies.1
Our streaming algorithm for this problem makes
use of the simple fact that the dot product of the
feature vector with random vectors is a linear op-
eration. This permits us to replace the vi ? ri op-
eration by vi individual additions of ri, once for
each time the feature is encountered in the stream
(where vi is the frequency of a feature and ri is the
randomly chosen Gaussian-distributed value asso-
ciated with this feature). The result of the final
computation is identical to the dot products com-
puted by the algorithm of Charikar (2002), but
the processing can now be done online. A simi-
lar technique, for stable random projections, was
independently discussed by Li et al (2008).
Since each feature may appear multiple times
in the stream, we need a consistent way to retrieve
the random values drawn from N(0, 1) associated
with it. To avoid the expense of computing and
storing these values explicitly, as is the norm, we
propose the use of a precomputed pool of ran-
dom values drawn from this distribution that we
can then hash into. Hashing into a fixed pool en-
sures that the same feature will consistently be as-
sociated with the same value drawn from N(0, 1).
This introduces some weak dependence in the ran-
dom vectors, but we will give some analysis show-
ing that this should have very limited impact on
the cosine similarity computation, which we fur-
ther support with experimental evidence (see Ta-
ble 3).
Our algorithm traverses a stream of words and
1Note that Ravichandran et al (2005) used pointwise mu-
tual information features, which are not additive since they
require a global statistic to compute.
Algorithm 1 STREAMING LSH ALGORITHM
Parameters:
m : size of pool
d : number of bits (size of resultant signature)
s : a random seed
h1, ..., hd : hash functions mapping ?s, fi? to {0, . . . ,m?1}
INITIALIZATION:
1: Initialize floating point array P [0, . . . ,m? 1]
2: Initialize H , a hashtable mapping words to floating point
arrays of size d
3: for i := 0 . . .m? 1 do
4: P [i] := random sample from N(0, 1), using s as seed
ONLINE:
1: for each word w in the stream do
2: for each feature fi associated with w do
3: for j := 1 . . . d do
4: H[w][j] := H[w][j] + P [hj(s, fi)]
SIGNATURECOMPUTATION:
1: for each w ? H do
2: for i := 1 . . . d do
3: if H[w][i] > 0 then
4: S[w][i] := 1
5: else
6: S[w][i] := 0
maintains some state for each possible word that
it encounters (cf. Algorithm 1). In particular, the
state maintained for each word is a vector of float-
ing point numbers of length d. Each element of the
vector holds the (partial) dot product of the feature
vector of the word with a random unit vector. Up-
dating the state for a feature seen in the stream for
a given word simply involves incrementing each
position in the word?s vector by the random value
associated with the feature, accessed by hash func-
tions h1 through hd. At any point in the stream,
the vector for each word can be processed (in time
O(d)) to create a signature computed by checking
the sign of each component of its vector.
3.1 Analysis
The update cost of the streaming algorithm, per
word in the stream, is O(df), where d is the target
signature size and f is the number of features asso-
ciated with each word in the stream.2 This results
in an overall cost of O(ndf) for the streaming al-
gorithm, where n is the length of the stream. The
memory footprint of our algorithm isO(n0d+m),
where n0 is the number of distinct words in the
stream and m is the size of the pool of normally
distributed values. In comparison, the original
LSH algorithm computes signatures at a cost of
O(nf + n0dF ) updates and O(n0F + dF + n0d)
memory, where F is the (large) number of unique
2For the bigram features used in ? 4, f = 2.
232
features. Our algorithm is superior in terms of
memory (because of the pooling trick), and has the
benefit of supporting similarity queries online.
3.2 Pooling Normally-distributed Values
We now discuss why it is possible to use a
fixed pool of random values instead of generating
unique ones for each feature. Let g be the c.d.f.
of the distribution N(0, 1). It is easy to see that
picking x ? (0, 1) uniformly results in g?1(x) be-
ing chosen with distribution N(0, 1). Now, if we
select for our pool the values
g?1(1/m), g?1(2/m), . . . , g?1(1? 1/m),
for some sufficiently large m, then this is identical
to sampling from N(0, 1) with the caveat that the
accuracy of the sample is limited. More precisely,
the deviation from sampling from this pool is off
from the actual value by at most
max
i=1,...,m?2
{g?1((i+ 1)/m)? g?1(i/m)}.
By choosing m to be sufficiently large, we can
bound the error of the approximate sample from
a true sample (i.e., the loss in precision expressed
above) to be a small fraction (e.g., 1%) of the ac-
tual value. This would result in the same relative
error in the computation of the dot product (i.e.,
1%), which would almost never affect the sign of
the final value. Hence, pooling as above should
give results almost identical to the case where all
the random values were chosen independently. Fi-
nally, we make the observation that, for large m,
randomly choosing m values from N(0, 1) results
in a set of values that are distributed very similarly
to the pool described above. An interesting avenue
for future work is making this analysis more math-
ematically precise.
3.3 Extensions
Decay The algorithm can be extended to support
temporal decay in the stream, where recent obser-
vations are given higher relative weight, by mul-
tiplying the current sums by a decay value (e.g.,
0.9) on a regular interval (e.g., once an hour, once
a day, once a week, etc.).
Distributed The algorithm can be easily dis-
tributed across multiple machines in order to pro-
cess different parts of a stream, or multiple differ-
ent streams, in parallel, such as in the context of
the MapReduce framework (Dean and Ghemawat,
(a)
(b)
Figure 1: Predicted versus actual cosine values for 50,000
pairs, using LSH signatures generated online, with d = 32 in
Fig. 1(a) and d = 256 in Fig. 1(b).
2004). The underlying operation is a linear op-
erator that is easily composed (i.e., via addition),
and the randomness between machines can be tied
based on a shared seed s. At any point in process-
ing the stream(s), current results can be aggregated
by summing the d-dimensional vectors for each
word, from each machine.
4 Experiments
Similar to the experiments of Ravichandran et
al. (2005), we evaluated the fidelity of signature
generation in the context of calculating distribu-
tional similarity between words across a large
text collection: in our case, articles taken from
the NYTimes portion of the Gigaword corpus
(Graff, 2003). The collection was processed as a
stream, sentence by sentence, using bigram fea-
233
d 16 32 64 128 256
SLSH 0.2885 0.2112 0.1486 0.1081 0.0769
LSH 0.2892 0.2095 0.1506 0.1083 0.0755
Table 1: Mean absolute error when using signatures gener-
ated online (StreamingLSH), compared to offline (LSH).
tures. This gave a stream of 773,185,086 tokens,
with 1,138,467 unique types. Given the number
of types, this led to a (sparse) feature space with
dimension on the order of 2.5 million.
After compiling signatures, fifty-thousand
?x, y? pairs of types were randomly sampled
by selecting x and y each independently, with
replacement, from those types with at least 10 to-
kens in the stream (where 310,327 types satisfied
this constraint). The true cosine values between
each such x and y was computed based on offline
calculation, and compared to the cosine similarity
predicted by the Hamming distance between the
signatures for x and y. Unless otherwise specified,
the random pool size was fixed at m = 10, 000.
Figure 1 visually reaffirms the trade-off in LSH
between the number of bits and the accuracy of
cosine prediction across the range of cosine val-
ues. As the underlying vectors are strictly posi-
tive, the true cosine is restricted to [0, 1]. Figure 2
shows the absolute error between truth and predic-
tion for a similar sample, measured using signa-
tures of a variety of bit lengths. Here we see hori-
zontal bands arising from truly orthogonal vectors
leading to step-wise absolute error values tracked
to Hamming distance.
Table 1 compares the online and batch LSH al-
gorithms, giving the mean absolute error between
predicted and actual cosine values, computed for
the fifty-thousand element sample, using signa-
tures of various lengths. These results confirm that
we achieve the same level of accuracy with online
updates as compared to the standard method.
Figure 3 shows how a pool size as low as m =
100 gives reasonable variation in random values,
and that m = 10, 000 is sufficient. When using a
standard 32 bit floating point representation, this
is just 40 KBytes of memory, as compared to, e.g.,
the 2.5 GBytes required to store 256 random vec-
tors each containing 2.5 million elements.
Table 2 is based on taking an example for each
of three part-of-speech categories, and reporting
the resultant top-5 words as according to approx-
imated cosine similarity. Depending on the in-
tended application, these results indicate a range
Figure 2: Absolute error between predicted and true co-
sine for a sample of pairs, when using signatures of length
log2(d) ? {4, 5, 6, 7, 8}, drawn with added jitter to avoid
overplotting.
Pool Size
Mean
 Abso
lute E
rror
0.2
0.4
0.6
0.8 l
l
l l l l l
101 102 103 104 105
Figure 3: Error versus pool size, when using d = 256.
of potentially sufficient signature lengths.
5 Conclusions
We have shown that when updates to a feature vec-
tor are additive, it is possible to convert the offline
LSH signature generation method into a stream-
ing algorithm. In addition to allowing for on-
line querying of signatures, our approach leads to
space efficiencies, as it does not require the ex-
plicit representation of either the feature vectors,
nor the random matrix. Possibilities for future
work include the pairing of this method with algo-
rithms for dynamic clustering, as well as exploring
algorithms for different distances (e.g., L2) and es-
timators (e.g., asymmetric estimators (Dong et al,
2009)).
234
London
Milan.97, Madrid.96, Stockholm.96, Manila.95, Moscow.95
ASHER0, Champaign0, MANS0, NOBLE0, come0
Prague1, Vienna1, suburban1, synchronism1, Copenhagen2
Frankfurt4, Prague4, Taszar5, Brussels6, Copenhagen6
Prague12, Stockholm12, Frankfurt14, Madrid14, Manila14
Stockholm20, Milan22, Madrid24, Taipei24, Frankfurt25
in
during.99, on.98, beneath.98, from.98, onto.97
Across0, Addressing0, Addy0, Against0, Allmon0
aboard0, mishandled0, overlooking0, Addressing1, Rejecting1
Rejecting2, beneath2, during2, from3, hamstringing3
during4, beneath5, of6, on7, overlooking7
during10, on13, beneath15, of17, overlooking17
sold
deployed.84, presented.83, sacrificed.82, held.82, installed.82
Bustin0, Diors0, Draining0, Kosses0, UNA0
delivered2, held2, marks2, seared2, Ranked3
delivered5, rendered5, presented6, displayed7, exhibited7
held18, rendered18, presented19, deployed20, displayed20
presented41, rendered42, held47, leased47, reopened47
Table 2: Top-5 items based on true cosine (bold), then using
minimal Hamming distance, given in top-down order when
using signatures of length log2(d) ? {4, 5, 6, 7, 8}. Ties bro-
ken lexicographically. Values given as subscripts.
Acknowledgments
Thanks to Deepak Ravichandran, Miles Osborne,
Sasa Petrovic, Ken Church, Glen Coppersmith,
and the anonymous reviewers for their feedback.
This work began while the first author was at the
University of Rochester, funded by NSF grant IIS-
1016735. The second author was supported in
part by NSF grant CNS-0905169, funded under
the American Recovery and Reinvestment Act of
2009.
References
Moses Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings
of STOC.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified Data Processing on Large Clusters.
In Proceedings of OSDI.
Wei Dong, Moses Charikar, and Kai Li. 2009. Asym-
metric distance estimation with sketches for similar-
ity search in high-dimensional spaces. In Proceed-
ings of SIGIR.
Michel X. Goemans and David P. Williamson. 1995.
Improved approximation algorithms for maximum
cut and satisfiability problems using semidefinite
programming. JACM, 42:1115?1145.
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language Modeling. In Proceedings of NAACL.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium, Philadelphia.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Abby Levenberg and Miles Osborne. 2009. Stream-
based Randomised Language Models for SMT. In
Proceedings of EMNLP.
Ping Li, Kenneth W. Church, and Trevor J. Hastie.
2008. One Sketch For All: Theory and Application
of Conditional Random Sampling. In Advances in
Neural Information Processing Systems 21.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming First Story Detection with appli-
cation to Twitter. In Proceedings of NAACL.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized Algorithms and NLP:
Using Locality Sensitive Hash Functions for High
Speed Noun Clustering. In Proceedings of ACL.
David Talbot. 2009. Succinct approximate counting of
skewed data. In Proceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2009a. Proba-
bilistic Counting with Randomized Storage. In Pro-
ceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming Pointwise Mutual Information. In Ad-
vances in Neural Information Processing Systems
22.
235
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 18?23,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Efficient Online Locality Sensitive Hashing via Reservoir Counting
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Ashwin Lall
Mathematics and Computer Science
Denison University
Abstract
We describe a novel mechanism called Reser-
voir Counting for application in online Local-
ity Sensitive Hashing. This technique allows
for significant savings in the streaming setting,
allowing for maintaining a larger number of
signatures, or an increased level of approxima-
tion accuracy at a similar memory footprint.
1 Introduction
Feature vectors based on lexical co-occurrence are
often of a high dimension, d. This leads to O(d) op-
erations to calculate cosine similarity, a fundamental
tool in distributional semantics. This is improved in
practice through the use of data structures that ex-
ploit feature sparsity, leading to an expected O(f)
operations, where f is the number of unique features
we expect to have non-zero entries in a given vector.
Ravichandran et al (2005) showed that the Lo-
cality Sensitive Hash (LSH) procedure of Charikar
(2002), following from Indyk and Motwani (1998)
and Goemans and Williamson (1995), could be suc-
cessfully used to compress textually derived fea-
ture vectors in order to achieve speed efficiencies
in large-scale noun clustering. Such LSH bit signa-
tures are constructed using the following hash func-
tion, where ~v ? Rd is a vector in the original feature
space, and ~r is randomly drawn from N(0, 1)d:
h(~v) =
{
1 if ~v ? ~r ? 0,
0 otherwise.
If hb(~v) is the b-bit signature resulting from b such
hash functions, then the cosine similarity between
vectors ~u and ~v is approximated by:
cos(~u,~v) = ~u?~v|~u||~v| ? cos(
D(hb(~u),hb(~v))
b ? pi),
where D(?, ?) is Hamming distance, the number of
bits that disagree. This technique is used when
b  d, which leads to faster pair-wise comparisons
between vectors, and a lower memory footprint.
Van Durme and Lall (2010) observed1 that if
the feature values are additive over a dataset (e.g.,
when collecting word co-occurrence frequencies),
then these signatures may be constructed online by
unrolling the dot-product into a series of local oper-
ations: ~v ?~ri = ?t~vt ?~ri, where ~vt represents features
observed locally at time t in a data-stream.
Since updates may be done locally, feature vec-
tors do not need to be stored explicitly. This di-
rectly leads to significant space savings, as only one
counter is needed for each of the b running sums.
In this work we focus on the following observa-
tion: the counters used to store the running sums
may themselves be an inefficient use of space, in
that they may be amenable to compression through
approximation.2 Since the accuracy of this LSH rou-
tine is a function of b, then if we were able to reduce
the online requirements of each counter, we might
afford a larger number of projections. Even if a
chance of approximation error were introduced for
each hash function, this may be justified in greater
overall fidelity from the resultant increase in b.
1A related point was made by Li et al (2008) when dis-
cussing stable random projections.
2A b bit signature requires the online storage of b?32 bits of
memory when assuming a 32-bit floating point representation
per counter, but since here the only thing one cares about these
sums are their sign (positive or negative) then an approximation
to the true sum may be sufficient.
18
Thus, we propose to approximate the online hash
function, using a novel technique we call Reservoir
Counting, in order to create a space trade-off be-
tween the number of projections and the amount of
memory each projection requires. We show experi-
mentally that this leads to greater accuracy approx-
imations at the same memory cost, or similar accu-
racy approximations at a significantly reduced cost.
This result is relevant to work in large-scale distribu-
tional semantics (Bhagat and Ravichandran, 2008;
Van Durme and Lall, 2009; Pantel et al, 2009; Lin
et al, 2010; Goyal et al, 2010; Bergsma and Van
Durme, 2011), as well as large-scale processing of
social media (Petrovic et al, 2010).
2 Approach
While not strictly required, we assume here to be
dealing exclusively with integer-valued features. We
then employ an integer-valued projection matrix in
order to work with an integer-valued stream of on-
line updates, which is reduced (implicitly) to a
stream of positive and negative unit updates. The
sign of the sum of these updates is approximated
through a novel twist on Reservoir Sampling. When
computed explicitly this leads to an impractical
mechanism linear in each feature value update. To
ensure our counter can (approximately) add and sub-
tract in constant time, we then derive expressions for
the expected value of each step of the update. The
full algorithms are provided at the close.
Unit Projection Rather than construct a projec-
tion matrix from N(0, 1), a matrix randomly pop-
ulated with entries from the set {?1, 0, 1} will suf-
fice, with quality dependent on the relative propor-
tion of these elements. If we let p be the percent
probability mass allocated to zeros, then we create
a discrete projection matrix by sampling from the
multinomial: (1?p2 : ?1, p : 0,
1?p
2 : +1). An
experiment displaying the resultant quality is dis-
played in Fig. 1, for varied p. Henceforth we assume
this discrete projection matrix, with p = 0.5.3 The
use of such sparse projections was first proposed by
Achlioptas (2003), then extended by Li et al (2006).
3Note that if using the pooling trick of Van Durme and Lall
(2010), this equates to a pool of the form: (-1,0,0,1).
Percent.Zeros
Mean
.Abso
lute.E
rror
0.1
0.2
0.3
0.4
0.5
0.2 0.4 0.6 0.8 1.0
MethodDiscreteNormal
Figure 1: With b = 256, mean absolute error in cosine
approximation when using a projection based onN(0, 1),
compared to {?1, 0, 1}.
Unit Stream Based on a unit projection, we can
view an online counter as summing over a stream
drawn from {?1, 1}: each projected feature value
unrolled into its (positive or negative) unary repre-
sentation. For example, the stream: (3,-2,1), can be
viewed as the updates: (1,1,1,-1,-1,1).
Reservoir Sampling We can maintain a uniform
sample of size k over a stream of unknown length
as follows. Accept the first k elements into an reser-
voir (array) of size k. Each following element at po-
sition n is accepted with probability kn , whereupon
an element currently in the reservoir is evicted, and
replaced with the just accepted item. This scheme
is guaranteed to provide a uniform sample, where
early items are more likely to be accepted, but also at
greater risk of eviction. Reservoir sampling is a folk-
lore algorithm that was extended by Vitter (1985) to
allow for multiple updates.
Reservoir Counting If we are sampling over a
stream drawn from just two values, we can implic-
itly represent the reservoir by counting only the fre-
quency of one or the other elements.4 We can there-
fore sample the proportion of positive and negative
unit values by tracking the current position in the
stream, n, and keeping a log2(k + 1)-bit integer
4For example, if we have a reservoir of size 5, containing
three values of ?1, and two values of 1, then the exchangeabil-
ity of the elements means the reservoir is fully characterized by
knowing k, and that there are two 1?s.
19
counter, s, for tracking the number of 1 values cur-
rently in the reservoir.5 When a negative value is
accepted, we decrement the counter with probability
s
k . When a positive update is accepted, we increment
the counter with probability (1? sk ). This reflects an
update evicting either an element of the same sign,
which has no effect on the makeup of the reservoir,
or decreasing/increasing the number of 1?s currently
sampled. An approximate sum of all values seen up
to position n is then simply: n(2sk ? 1). While this
value is potentially interesting in future applications,
here we are only concerned with its sign.
Parallel Reservoir Counting On its own this
counting mechanism hardly appears useful: as it is
dependent on knowing n, then we might just as well
sum the elements of the stream directly, counting in
whatever space we would otherwise use in maintain-
ing the value of n. However, if we have a set of tied
streams that we process in parallel,6 then we only
need to track n once, across b different streams, each
with their own reservoir.
When dealing with parallel streams resulting from
different random projections of the same vector, we
cannot assume these will be strictly tied. Some pro-
jections will cancel out heavier elements than oth-
ers, leading to update streams of different lengths
once elements are unrolled into their (positive or
negative) unary representation. In practice we have
found that tracking the mean value of n across b
streams is sufficient. When using a p = 0.5 zeroed
matrix, we can update n by one half the magnitude
of each observed value, as on average half the pro-
jections will cancel out any given element. This step
can be found in Algorithm 2, lines 8 and 9.
Example To make concrete what we have cov-
ered to this point, consider a given feature vec-
tor of dimensionality d = 3, say: [3, 2, 1]. This
might be projected into b = 4, vectors: [3, 0, 0],
[0, -2, 1], [0, 0, 1], and [-3, 2, 0]. When viewed as
positive/negative, loosely-tied unit streams, they re-
spectively have length n: 3, 3, 1, and 5, with mean
length 3. The goal of reservoir counting is to effi-
ciently keep track of an approximation of their sums
(here: 3, -1, 1, and -1), while the underlying feature
5E.g., a reservoir of size k = 255 requires an 8-bit integer.
6Tied in the sense that each stream is of the same length,
e.g., (-1,1,1) is the same length as (1,-1,-1).
k n m mean(A) mean(A?)
10 20 10 3.80 4.02
10 20 1000 37.96 39.31
50 150 1000 101.30 101.83
100 1100 100 8.88 8.72
100 10100 10 0.13 0.10
Table 1: Average over repeated calls to A and A?.
vector is being updated online. A k = 3 reservoir
used for the last projected vector, [-3, 2, 0], might
reasonably contain two values of -1, and one value
of 1.7 Represented explicitly as a vector, the reser-
voir would thus be in the arrangement:
[1, -1, -1], [-1, 1, -1], or [-1, -1, 1].
These are functionally equivalent: we only need to
know that one of the k = 3 elements is positive.
Expected Number of Samples Traversingm con-
secutive values of either 1 or ?1 in the unit stream
should be thought of as seeing positive or negative
m as a feature update. For a reservoir of size k, let
A(m,n, k) be the number of samples accepted when
traversing the stream from position n+ 1 to n+m.
A is non-deterministic: it represents the results of
flipping m consecutive coins, where each coin is in-
creasingly biased towards rejection.
Rather than computing A explicitly, which is lin-
ear inm, we will instead use the expected number of
updates, A?(m,n, k) = E[A(m,n, k)], which can
be computed in constant time. Where H(x) is the
harmonic number of x:8
A?(m,n, k) =
n+m?
i=n+1
k
i
= k(H(n+m)?H(n))
? k loge(
n+m
n
).
For example, consider m = 30, encountered at
position n = 100, with a reservoir of k = 10. We
will then accept 10 loge(
130
100) ? 3.79 samples of 1.
As the reservoir is a discrete set of bins, fractional
portions of a sample are resolved by a coin flip: if
a = k loge(
n+m
n ), then accept u = dae samples
with probability (a ? bac), and u = bac samples
7Other options are: three -1?s, or one -1 and two 1?s.
8With x a positive integer,H(x) =
?x
i=1 1/x ? loge(x)+
?, where ? is Euler?s constant.
20
otherwise. These steps are found in lines 3 and 4
of Algorithm 1. See Table 1 for simulation results
using a variety of parameters.
Expected Reservoir Change We now discuss
how to simulate many independent updates of the
same type to the reservoir counter, e.g.: five updates
of 1, or three updates of -1, using a single estimate.
Consider a situation in which we have a reservoir of
size k with some current value of s, 0 ? s ? k, and
we wish to perform u independent updates. We de-
note by U ?k(s, u) the expected value of the reservoir
after these u updates have taken place. Since a sin-
gle update leads to no change with probability sk , we
can write the following recurrence for U ?k:
U ?k(s, u) =
s
k
U ?k(s, u?1)+
k ? s
k
U ?k(s+1, u?1),
with the boundary condition: for all s, U ?k(s, 0) = s.
Solving the above recurrence, we get that the ex-
pected value of the reservoir after these updates is:
U ?k(s, u) = k + (s? k)
(
1?
1
k
)u
,
which can be mechanically checked via induction.
The case for negative updates follows similarly (see
lines 7 and 8 of Algorithm 1).
Hence, instead of simulating u independent up-
dates of the same type to the reservoir, we simply
update it to this expected value, where fractional up-
dates are handled similarly as when estimating the
number of accepts. These steps are found in lines 5
through 9 of Algorithm 1, and as seen in Fig. 2, this
can give a tight estimate.
Comparison Simulation results over Zipfian dis-
tributed data can be seen in Fig. 3, which shows the
use of reservoir counting in Online Locality Sensi-
tive Hashing (as made explicit in Algorithm 2), as
compared to the method described by Van Durme
and Lall (2010).
The total amount of space required when using
this counting scheme is b log2(k + 1) + 32: b reser-
voirs, and a 32 bit integer to track n. This is com-
pared to b 32 bit floating point values, as is standard.
Note that our scheme comes away with similar lev-
els of accuracy, often at half the memory cost, while
requiring larger b to account for the chance of ap-
proximation errors in individual reservoir counters.
Expected
True
50
100
150
200
250
50 100 150 200 250
Figure 2: Results of simulating many iterations of U ?,
for k = 255, and various values of s and u.
Algorithm 1 RESERVOIRUPDATE(n, k,m, ?, s)
Parameters:
n : size of stream so far
k : size of reservoir, also maximum value of s
m : magnitude of update
? : sign of update
s : current value of reservoir
1: if m = 0 or ? = 0 then
2: Return without doing anything
3: a := A?(m,n, k) = k loge
(
n+m
n
)
4: u := dae with probability a? bac, bac otherwise
5: if ? = 1 then
6: s? := U ?(s, a) = k + (s? k) (1? 1/k)u
7: else
8: s? := U ?(s, a) = s (1? 1/k)u
9: Return ds?e with probability s??bs?c, bs?c otherwise
Bits.Required
Mean
.Abso
lute.E
rror
0.06
0.07
0.08
0.09
0.10
0.11
0.12
l
l
l l
l
l
l
l l
l
l
1000 2000 3000 4000 5000 6000 7000 8000
log2.kl 8l 32
bl 64128192256512
Figure 3: Online LSH using reservoir counting (red) vs.
standard counting mechanisms (blue), as measured by the
amount of total memory required to the resultant error.
21
Algorithm 2 COMPUTESIGNATURE(S ,k,b,p)
Parameters:
S : bit array of size b
k : size of each reservoir
b : number of projections
p : percentage of zeros in projection, p ? [0, 1]
1: Initialize b reservoirs R[1, . . . , b], each represented
by a log2(k + 1)-bit unsigned integer
2: Initialize b hash functions hi(w) that map features w
to elements in a vector made up of ?1 and 1 each
with proportion 1?p2 , and 0 at proportion p.
3: n := 0
4: {Processing the stream}
5: for each feature value pair (w,m) in stream do
6: for i := 1 to b do
7: R[i] := ReservoirUpdate(n, k,m, hi(w), R[i])
8: n := n+ bm(1? p)c
9: n := n+1 with probabilitym(1?p)?bm(1?p)c
10: {Post-processing to compute signature}
11: for i := 1 . . . b do
12: if R[i] > k2 then
13: S[i] := 1
14: else
15: S[i] := 0
3 Discussion
Time and Space While we have provided a con-
stant time, approximate update mechanism, the con-
stants involved will practically remain larger than
the cost of performing single hardware addition
or subtraction operations on a traditional 32-bit
counter. This leads to a tradeoff in space vs. time,
where a high-throughput streaming application that
is not concerned with online memory requirements
will not have reason to consider the developments in
this article. The approach given here is motivated
by cases where data is not flooding in at breakneck
speed, and resource considerations are dominated by
a large number of unique elements for which we
are maintaining signatures. Empirically investigat-
ing this tradeoff is a matter of future work.
Random Walks As we here only care for the sign
of the online sum, rather than an approximation of
its actual value, then it is reasonable to consider in-
stead modeling the problem directly as a random
walk on a linear Markov chain, with unit updates
directly corresponding to forward or backward state
-4 -3 -2 -1 0 1 2 3
Figure 4: A simple 8-state Markov chain, requiring
lg(8) = 3 bits. Dark or light states correspond to a
prediction of a running sum being positive or negative.
States are numerically labeled to reflect the similarity to
a small bit integer data type, one that never overflows.
transitions. Assuming a fixed probability of a posi-
tive versus negative update, then in expectation the
state of the chain should correspond to the sign.
However if we are concerned with the global statis-
tic, as we are here, then the assumption of a fixed
probability update precludes the analysis of stream-
ing sources that contain local irregularities.9
In distributional semantics, consider a feature
stream formed by sequentially reading the n-gram
resource of Brants and Franz (2006). The pair: (the
dog : 3,502,485), can be viewed as a feature value
pair: (leftWord=?the? : 3,502,485), with respect to
online signature generation for the word dog. Rather
than viewing this feature repeatedly, spread over a
large corpus, the update happens just once, with
large magnitude. A simple chain such as seen in
Fig. 4 will be ?pushed? completely to the right or
the left, based on the polarity of the projection, irre-
spective of previously observed updates. Reservoir
Counting, representing an online uniform sample, is
agnostic to the ordering of elements in the stream.
4 Conclusion
We have presented a novel approximation scheme
we call Reservoir Counting, motivated here by a de-
sire for greater space efficiency in Online Locality
Sensitive Hashing. Going beyond our results pro-
vided for synthetic data, future work will explore ap-
plications of this technique, such as in experiments
with streaming social media like Twitter.
Acknowledgments
This work benefited from conversations with Daniel
S?tefonkovic? and Damianos Karakos.
9For instance: (1,1,...,1,1,-1,-1,-1), is overall positive, but
locally negative at the end.
22
References
Dimitris Achlioptas. 2003. Database-friendly random
projections: Johnson-lindenstrauss with binary coins.
J. Comput. Syst. Sci., 66:671?687, June.
Shane Bergsma and Benjamin Van Durme. 2011. Learn-
ing Bilingual Lexicons using the Visual Similarity of
Labeled Web Images. In Proc. of the International
Joint Conference on Artificial Intelligence (IJCAI).
Rahul Bhagat and Deepak Ravichandran. 2008. Large
Scale Acquisition of Paraphrases for Learning Surface
Patterns. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of STOC.
Michel X. Goemans and David P. Williamson. 1995.
Improved approximation algorithms for maximum cut
and satisfiability problems using semidefinite pro-
gramming. JACM, 42:1115?1145.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and
Suresh Venkatasubramanian. 2010. Sketch Tech-
niques for Scaling Distributional Similarity to the
Web. In Proceedings of the ACL Workshop on GEo-
metrical Models of Natural Language Semantics.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
Very sparse random projections. In Proceedings of
the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ?06,
pages 287?296, New York, NY, USA. ACM.
Ping Li, Kenneth W. Church, and Trevor J. Hastie. 2008.
One Sketch For All: Theory and Application of Con-
ditional Random Sampling. In Proc. of the Confer-
ence on Advances in Neural Information Processing
Systems (NIPS).
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New Tools for Web-Scale
N-grams. In Proceedings of LREC.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-Scale
Distributional Similarity and Entity Set Expansion. In
Proc. of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming First Story Detection with applica-
tion to Twitter. In Proceedings of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Benjamin Van Durme and Ashwin Lall. 2009. Streaming
Pointwise Mutual Information. In Proc. of the Confer-
ence on Advances in Neural Information Processing
Systems (NIPS).
Benjamin Van Durme and Ashwin Lall. 2010. Online
Generation of Locality Sensitive Hash Signatures. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).
Jeffrey S. Vitter. 1985. Random sampling with a reser-
voir. ACM Trans. Math. Softw., 11:37?57, March.
23
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 710?720,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Conceptual Class Attributes to Characterize Social Media Users
Shane Bergsma and Benjamin Van Durme
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
Abstract
We describe a novel approach for automat-
ically predicting the hidden demographic
properties of social media users. Building
on prior work in common-sense knowl-
edge acquisition from third-person text,
we first learn the distinguishing attributes
of certain classes of people. For exam-
ple, we learn that people in the Female
class tend to have maiden names and en-
gagement rings. We then show that this
knowledge can be used in the analysis of
first-person communication; knowledge of
distinguishing attributes allows us to both
classify users and to bootstrap new train-
ing examples. Our novel approach enables
substantial improvements on the widely-
studied task of user gender prediction, ob-
taining a 20% relative error reduction over
the current state-of-the-art.
1 Introduction
There has been growing interest in characteriz-
ing social media users based on the content they
generate; that is, automatically labeling users with
demographic categories such as age and gender
(Burger and Henderson, 2006; Schler et al, 2006;
Rao et al, 2010; Mukherjee and Liu, 2010; Pen-
nacchiotti and Popescu, 2011; Burger et al, 2011;
Van Durme, 2012). Automatic user character-
ization has applications in targeted advertising
and personalization, and could also lead to finer-
grained assessment of public opinion (O?Connor
et al, 2010) and health (Paul and Dredze, 2011).
Consider the following tweet and suppose we
wish to predict the user?s gender:
Dirac was one of my boyhood heroes.
I?m glad I met him once. RT Paul Dirac
image by artist Eric Handy: http:...
State-of-the-art approaches cast this problem as a
classification task and train classifiers using super-
vised learning (Section 2). The features of the
classifier are indicators of specific words in the
user-generated text. While a human would as-
sume that someone with boyhood heroes is male,
a standard classifier has no way of exploiting such
knowledge unless the phrase occurs in training
data. We present an algorithm that improves user
characterization by collecting and exploiting such
common-sense knowledge.
Our work is inspired by algorithms that pro-
cesses large text corpora in order to discover the
attributes of semantic classes, e.g. (Berland and
Charniak, 1999; Schubert, 2002; Almuhareb and
Poesio, 2004; Tokunaga et al, 2005; Girju et al,
2006; Pas?ca and Van Durme, 2008; Alfonseca et
al., 2010). We learn the distinguishing attributes
of different demographic groups (Section 3), and
then automatically assign users to these groups
whenever they refer to a distinguishing attribute in
their writings (Section 4). Our approach obviates
the need for expensive annotation efforts, and al-
lows us to rapidly bootstrap training data for new
classification tasks.
We validate our approach by advancing the
state-of-the-art on the most well-studied user clas-
sification task: predicting user gender (Section 5).
Our bootstrapped system, trained purely from
automatically-annotated Twitter data, significantly
reduces error over a state-of-the-art system trained
on thousands of gold-standard training examples.
2 Supervised User Characterization
The current state-of-the-art in user characteriza-
tion is to use supervised classifiers trained on an-
notated data. For each instance to be classified, the
output is a decision about a distinct demographic
property, such as Male/Female or Over/Under-18.
A variety of classification algorithms have been
employed, including SVMs (Rao et al, 2010), de-
710
cision trees (Pennacchiotti and Popescu, 2011), lo-
gistic regression (Van Durme, 2012), and the Win-
now algorithm (Burger et al, 2011).
Content Features: BoW Prior classifiers use a
set of features encoding the presence of specific
words in the user-generated text. We call these
features BoW features as they encode the stan-
dard Bag-of-Words representation which has been
highly effective in text categorization and informa-
tion retrieval (Sebastiani, 2002).
User-Profile Features: Usr Some researchers
have explored features for user-profile meta-
information in addition to user content. This may
include the user?s communication behavior and
network of contacts (Rao et al, 2010), their full
name (Burger et al, 2011) and whether they pro-
vide a profile picture (Pennacchiotti and Popescu,
2011). We focus on the case where we only
have access to the user?s screen-name (a.k.a. user-
name). Using a combination of content and user-
name features ?represents a use case common to
many different social media sites, such as chat
rooms and news article comment streams? (Burger
et al, 2011). We refer to features derived from a
username as Usr features in our experiments.
3 Learning Class Attributes
We aim to improve the automated classification
of users into various demographic categories by
learning and applying the distinguishing attributes
of those categories, e.g. that males have boyhood
heroes. Our approach builds on lexical-semantic
research on the topic of class-attribute extraction.
In this research, the objective is to discover vari-
ous attributes or parts of classes of entities. For
example, Berland and Charniak (1999) learn that
the class car has parts such as headlight, wind-
shield, dashboard, etc. Berland and Charniak ex-
tract these attributes by mining a corpus for fillers
of patterns such as ?car?s X? or ?X of a car?. Note
their patterns explicitly include the class itself
(car). Another approach is to use patterns that are
based on instances (i.e. hyponyms or sub-classes)
of the class. For example, Pas?ca and Van Durme
(2007) learn the attributes of the class car via pat-
terns involving instances of cars, e.g. Chevrolet
Corvette?s X and X of a Honda Civic. For these ap-
proaches, lists of instances are typically collected
from publicly-available resources such as Word-
Net or Wikipedia (Pas?ca and Van Durme, 2007;
Van Durme et al, 2008), acquired automatically
from corpora (Pas?ca and Van Durme, 2008; Al-
fonseca et al, 2010), or simply specified by hand
(Schubert, 2002).
Creation of Instance Lists We use an instance-
based approach; our instances are derived from
collections of common nouns that are associated
with roles and occupations of people. For the
gender task that we study in our experiments, we
acquire class instances by filtering the dataset of
nouns and their genders created by Bergsma and
Lin (2006). This dataset indicates how often a
noun is referenced by a male, female, neutral or
plural pronoun. We extract prevalent common
nouns for males and females by selecting only
those nouns that (a) occur more than 200 times
in the dataset, (b) mostly occur with male or fe-
male pronouns, and (c) occur as lower-case more
often than upper-case in a web-scale N-gram cor-
pus (Lin et al, 2010). We then classify a noun as
Male (resp. Female) if the noun is indicated to
occur with male (resp. female) pronouns at least
85% of the time. Since the gender data is noisy,
we also quickly pruned by hand any instances that
were malformed or obviously incorrectly assigned
by our automatic process. This results in 652 in-
stances in total. Table 1 provides some examples.
Male: bouncer, altar boy, army officer, dictator,
assailant, cameraman, drifter, chauffeur, bad guy
Female: young lady, lesbian, ballerina, waitress,
granny, chairwoman, heiress, soprano, socialite
Table 1: Example instances used for extraction of
class attributes for the gender classification task
Attribute Extraction We next collect and rank
attributes for each class. We first look for fillers of
attribute-patterns involving each of the instances.
Let I represent an instance of one of our classes.
We find fillers of the single high-precision pattern:
{word=I ,tag=NN}
| {z }
instance
{word=?s}
| {z }
?s
[{word=.*}* {tag=N.*}]
| {z }
attribute
(E.g. dictator ?s [former mistress]). The expres-
sion ?tag=NN? means that I must be tagged as
a noun. The expression in square brackets is the
filler, i.e. the extracted attribute, A. The notation
?{word=.*}* tag=N.*? means that A can be any
sequence of tokens ending in a noun. We use an
711
equivalent pattern when I is multi-token. The out-
put of this process is a set of (I ,A) pairs.
In attribute extraction, typically one must
choose between the precise results of rich pat-
terns (involving punctuation and parts-of-speech)
applied to small corpora (Berland and Charniak,
1999) and the high-coverage results of superficial
patterns applied to web-scale data, e.g. via the
Google API (Almuhareb and Poesio, 2004). We
obtain the best of both worlds by matching our
precise pattern against a version of the Google N-
gram Corpus that includes the part-of-speech tag
distributions for every N-gram (Lin et al, 2010).
We found that applying this pattern to web-scale
data is effective in extracting useful attributes. We
acquired around 20,000 attributes in total.
Finding Distinguishing Attributes Unlike
prior work, we aim to find distinguishing proper-
ties of each class; that is, the kinds of properties
that uniquely distinguish a particular category.
Prior work has mostly focused on finding ?rel-
evant? attributes (Alfonseca et al, 2010) or
?correct? parts (Berland and Charniak, 1999). A
leg is a relevant and correct part of both a male and
a female (and many other living and inanimate
objects), but it does not help us distinguish males
from females in social media. We therefore rank
our attributes for each class by their strength of
association with instances of that specific class.1
To calculate the association, we first disregard
the count of each (I ,A) pair and consider each
unique pair to be a single probabilistic event.
We then convert the (I ,A) pairs to corresponding
(C,A) pairs by replacing I with the corresponding
class, C. We then calculate the pointwise mutual
information (Church and Hanks, 1990) between
each C and A over the set of events:
PMI(C,A) = log p(C,A)p(C)p(A) (1)
If the PMI>0, the observed probability of a class
and attribute co-occurring is greater than the prob-
ability of co-occurrence that we would expect if C
and A were independently distributed. For each
class, we rank the attributes by their PMI scores.
1Reisinger and Pas?ca (2009) considered the related prob-
lem of finding the most appropriate class for each attribute;
they take an existing ontology of concepts (WordNet) as a
class hierarchy and use a Bayesian approach to decide ?the
correct level of abstraction for each attribute.?
Filtering Attributes We experimented with two
different methods to select a final set of distin-
guishing attributes for each class: (1) we used
a threshold to select the top-ranked attributes for
each class, and (2) we manually filtered the at-
tributes. For the gender classification task, we
manually filtered the entire set of attributes to se-
lect around 1000 attributes that were judged to be
discriminative (two thirds of which are female).
This filtering took one annotator only a few hours
to complete. Because this process was so trivial,
we did not invest in developing annotation guide-
lines or measuring inter-annotator agreement. We
make these filter attributes available online as an
attachment to this article, available through the
ACL Anthology.
Ultimately, we discovered that manual filter-
ing was necessary to avoid certain pathological
cases in our Twitter data. For example, our PMI
scoring finds homepage to be strongly associated
with males. In our gold-standard gender data
(Section 5), however, every user has a home-
page [by dataset construction]; we might there-
fore incorrectly classify every user as Male. We
agree with Richardson et al (1998) that ?auto-
matic procedures ... provide the only credible
prospect for acquiring world knowledge on the
scale needed to support common-sense reasoning?
but ?hand vetting? might be needed to ensure ?ac-
curacy and consistency in production level sys-
tems.? Since our approach requires manual in-
volvement in the filtering of the attribute list, one
might argue that one should simply manually enu-
merate the most relevant attributes directly. How-
ever, the manual generation of conceptual features
by a single researcher results in substantial vari-
ability both across and within participants (McRae
et al, 2005). Psychologists therefore generate
such lists by pooling the responses across many
participants: future work may compare our ?auto-
matically generate, manually prune? approach to
soliciting attributes via crowdsourcing.2
Table 2 gives examples of our extracted at-
2One can also view the work of manually filtering at-
tributes as a kind of ?feature labeling.? There is evidence
from Zaidan et al (2007) that a few hours of feature labeling
can be more productive than annotating new training exam-
ples. In fact, since Zaidan et al (2007) label features at the
token level (e.g., in our case one would highlight ?handbag?
in a given tweet), while we label features at the type level
(e.g., deciding whether to mark the word ?handbag? as fem-
inine in general), our process is likely even more efficient.
Future work may also wish to consider this connection to so-
called ?annotator rationales? more deeply.
712
Male: wife, widow, wives, ex-girlfriend, erec-
tion, testicles, wet dream, bride, buddies, ex-
wife, first-wife, penis, death sentence, manhood
Female: vagina, womb, maiden name, dresses,
clitoris, wedding dress, uterus, shawl, necklace,
ex-husband, ex-boyfriend, dowry, nightgown
Table 2: Example attributes for gender classes, in
descending order of class-association score
tributes. Our approach captures many multi-token
attributes; these are often distinguishing even
though the head noun is ambiguous (e.g. name
is ambiguous, maiden name is not). Our attributes
also go beyond the traditional meronyms that were
the target of earlier work. As we discuss further
in Related Work (Section 7), previous researchers
have worried about a proper definition of parts or
attributes and relied on human judgments for eval-
uation (Berland and Charniak, 1999; Girju et al,
2006; Van Durme et al, 2008). For us, whether
a property such as dowry should be considered
an ?attribute? of the class Female is immaterial;
we echo Almuhareb and Poesio (2004) who (on a
different task) noted that ?while the notion of ?at-
tribute? is not completely clear... our results sug-
gest that trying to identify attributes is beneficial.?
4 Applying Class Attributes
To classify users using the extracted attributes, we
look for cases where users refer to such attributes
in their first-person writings. We performed a pre-
liminary analysis of a two-week sample of tweets
from the TREC Tweets2011 Corpus.3 We found
that users most often reveal their attributes in the
possessive construction, ?my X? where X is an at-
tribute, quality or event that they possess (in a lin-
guistic sense). For example, we found over 1000
tweets with the phrase ?my wife.? In contrast, ?I
have a wife? occurs only 5 times.4
We therefore assign a user to a demographic
category as follows: We first part-of-speech tag
our data using CRFTagger (Phan, 2006) and then
look for ?my X? patterns where X is a sequence
of tokens terminating in a noun, analogous to our
3http://trec.nist.gov/data/tweets/ This corpus was de-
veloped for the TREC Microblog track (Soboroff et al, 2012).
4Note that ?I am a man? occurs only 20 times. Users
also reveal their names in ?my name is X? patterns in several
hundred tweets, but this is small compared to cases of self-
distinguishing attributes. Exploiting these alternative pat-
terns could nevertheless be a possible future direction.
attribute-extraction pattern (Section 3).5 When a
user uses such a ?my X? construction, we match
the filler X against our attribute lists for each
class. If the filler is on a list, we call it a self-
distinguishing attribute of a user. We then apply
our knowledge of the self-distinguishing attribute
and its corresponding class in one of the following
three ways:
(1) ARules: Using Attribute-Based Rules to
Override a Classifier When human-annotated
data is available for training and testing a su-
pervised classifier, we refer to it as gold stan-
dard data. Our first technique provides a sim-
ple way to use our identified self-distinguishing
attributes in conjunction with a classifier trained
on gold-standard data. If the user has any self-
distinguishing attributes, we assign the user to the
corresponding class; otherwise, we trust the output
of the classifier.
(2) Bootstrapped: Automatic Labeling of Train-
ing Examples Even without gold standard train-
ing data, we can use our self-distinguishing at-
tributes to automatically bootstrap annotations.
We collect a large pool of unlabeled users and their
tweets, and we apply the ARules described above
to label those users that have self-distinguishing
attributes. Once an example is auto-annotated,
we delete the self-distinguishing attributes from
the user?s content. This prevents the subsequent
learning algorithm from trivially learning the rules
with which we auto-annotated the data. Next, the
auto-annotated examples are used as training data
for a supervised system.6 Finally, when applying
the Bootstrapped classifiers, we can still apply the
ARules as a post-process (although in practice this
made little difference in our final results).
(3) BootStacked: Gold Standard and Boot-
strapped Combination Although we show that
an accurate classifier can be trained using auto-
annotated Bootstrapped data alone, we also test
whether we can combine this data with any gold-
standard training examples to achieve even better
performance. We use the following simple but
5While we used an ?off the shelf? POS tagger in this
work, we note that taggers optimized specifically for social
media are now available and would likely have resulted in
higher tagging accuracy (e.g. Owoputi et al (2013)).
6Note that while our target gender task presents mutually-
exclusive output classes, we can still train classifiers for other
categories without clear opposites (e.g. for labeling users
as Parents or Doctors) by using the 1-class classification
paradigm (Koppel and Schler, 2004).
713
effective method for combining data from these
two sources, inspired by prior techniques used in
the domain adaptation literature (Daume? III and
Marcu, 2006). We first use the trained Boot-
strapped system to make predictions on the entire
set of gold standard data (gold train, development,
and test sets). We then use these predictions as
features in a classifier trained on the gold standard
data. We refer to this system as the BootStacked
system in our evaluation.
5 Twitter Gender Prediction
To test the use of self-distinguishing attributes
in user classification, we apply our methods to
the task of gender classification on Twitter. This
is an important and intensely-studied task within
academia and industry. Furthermore, for this task
it is possible to semi-automatically acquire large
amounts of ground truth (Burger et al, 2011).
We can therefore benchmark our approach against
state-of-the-art supervised systems trained with
plentiful gold-standard data, giving us an idea of
how well our Bootstrapped system might compare
to theoretically top-performing systems on other
tasks, domains, and social media platforms where
such gold-standard training data is not available.
Gold Data Our data is derived from the corpus
created by Burger et al (2011). Burger et al ob-
served that many Twitter users link their Twitter
profile to homepages on popular blogging web-
sites. Since ?many of these [sites] have well-
structured profile pages [where users] must se-
lect gender and other attributes from dropdown
menus,? they were able to link these attributes to
the Twitter users. Using this process, they created
a large multi-lingual corpus of Twitter users and
genders.
We filter non-English tweets from this corpus
using the LID system of Bergsma et al (2012)
and also tweets containing URLs (since many of
these are spam) and re-tweets. We then filter users
with <40 tweets and randomly divide the remain-
ing users into 2282 training, 1140 development,
and 1141 test examples.
Classifier Set-up We train logistic-regression
classifiers on this gold standard data via the LI-
BLINEAR package (Fan et al, 2008). We optimize
the classifier?s regularization parameter on devel-
opment data and report final results on the held-
out test examples. We also report the results of
our new attribute-based strategies (Section 4) on
the test data. We report accuracy: the percentage
of examples labeled correctly.
Our classifiers use both BoW and Usr features
(Section 2). To increase the generality of our
BoW features, we preprocess the text by lower-
casing and converting all digits to special ?#? sym-
bols. We then create real-valued features that
encode the log-count of each word in the input.
While Burger et al (2011) found ?no apprecia-
ble difference in performance? when using either
binary presence/absence features or encoding the
frequency of the word, we found real-valued fea-
tures worked better in development experiments.
For the Usr features, we add special beginning and
ending characters to the username, and then create
features for all character n-grams of length two-
to-four in the modified username string. We in-
clude n-gram features with the original capitaliza-
tion pattern and separate features with the n-grams
lower-cased.
Unlabeled Data For Bootstrapped training, we
also use a pool of unlabeled Twitter data. This
pool comprises the union of 2.2 billion tweets
from 05/2009 to 10/2010 (O?Connor et al, 2010),
1.9 billion tweets collected from 07/2011 to
11/2012, and 80 million tweets collected from the
followers of 10-thousand location and language-
specific Twitter feeds. We filter this corpus as
above, except we do not put any restrictions on the
number of tweets needed per user. We also filter
any users that overlap with our gold standard data.
Bootstrapping Analysis We apply our Boot-
strapped auto-annotation strategy to this unlabeled
data, yielding 789,285 auto-annotated examples
of users and their tweets. The decisions of our
bootstrapping process reflect the true gender dis-
tribution; the auto-annotated data is 60.5% Fe-
male, remarkably close to the 60.9% proportion
in our gold standard test set. Figure 1 shows that
a wide range of self-distinguishing attributes are
used in the auto-annotation process. This is impor-
tant because if only a few attributes are used (e.g.
wife/husband or penis/vagina), we might system-
atically miss a segment of users (e.g. young people
that don?t have husbands or wives, or people that
don?t frequently talk about their genitalia). Thus a
wide range of common-sense knowledge is useful
for bootstrapping, which is one reason why auto-
matic approaches are needed to acquire it.
714
050000
100000
150000
200000
engagement ring ? 
Note: showing only first 10% of attributes used boyfriend ? 
hubby ? 
bra ? future wife ?  
natural hair ? 
jewelry ? 
bride ? beard 
? 
due date ? 
wife ? 
husband ? 
tux ? 
purse ? 
Figure 1: Frequency with which attributes are used to auto-annotate examples in the bootstrapping ap-
proach. The plot identifies some attributes and their corresponding class (labeled via gender symbol).
Majority-class baseline 60.9
Supervised on 100 examples 72.0
Supervised on 2282 examples 84.0
Supervised on 100 examples + ARules 74.7
Supervised on 2282 examples + ARules 84.7
Bootstrapped 86.0
BootStacked 87.2
Table 3: Classification accuracy (%) on gold stan-
dard test data for user gender prediction on Twitter
6 Results
Our main classification results are presented in Ta-
ble 3. The majority-class baseline for this task
is to always choose Female; this achieves an ac-
curacy of 60.9%. A standard classifier trained
on 100 gold-standard training examples improves
over this baseline, to 72.0%, while one with 2282
training examples achieves 84.0%. This latter re-
sult represents the current state-of-the-art: a clas-
sifier trained on thousands of gold standard exam-
ples, making use of both Usr and BoW features.
Our performance compares favourably to Burger
et al (2011), who achieved 81.4% using the same
features, but on a very different subset of the data
(also including tweets in other languages).7
Applying the ARules as a post-process signifi-
cantly improves performance in both cases (Mc-
Nemar?s, p<0.05). It is also possible to use the
ARules as a stand-alone system rather than as a
post-process, however the coverage is low: we find
a distinguishing attribute in 18.3% of the 695 Fe-
male instances in the test data, and make the cor-
7Note that it is possible to achieve even higher perfor-
mance on gender classification in social media if you have
further information about a user, such as their full first and
last name (Burger et al, 2011; Bergsma et al, 2013).
rect decision in 96.9% of these cases. We find a
distinguishing attribute in 11.4% of the 446 Male
instances, with 86.3% correct decisions.
The Bootstrapped system substantially im-
proves over the state-of-the-art, achieving 86% ac-
curacy and doing so without using any gold stan-
dard training data. This is important because hav-
ing thousands of gold standard annotations for ev-
ery possible user characterization task, in every
domain and social media platform, is not realis-
tic. Combining the bootstrapped classifier with
the gold standard annotations in the BootStacked
model results in further gains in performance.8
These results provide strong validation for both
the inherent utility of class-attributes knowledge in
user characterization and the effectiveness of our
specific strategies for exploiting such knowledge.
Figure 2 shows the learning curve of the Boot-
strapped classifier. Performance rises consistently
across all the auto-annotated training data; this
is encouraging because there is theoretically no
reason not to vastly increase the amount of auto-
annotated data by collecting an even larger col-
lection of tweets. Finally, note that most of the
gains of the Bootstrapped system appear to derive
from the tweet content itself, i.e. the BoW fea-
tures. However, the Usr features are also helpful
at most training sizes.
We provide some of the top-ranked features of
the Bootstrapped system in Table 4. We see that
a variety of other common-sense knowledge is
learned by the system (e.g., the association be-
tween males and urinals, boxers, fatherhood, etc.),
as well as stylistic clues (e.g. Female users using
betcha and xox in their writing). The username
8We observed no further gains in accuracy when applying
the ARules as a post-process on top of these systems.
715
 60
 65
 70
 75
 80
 85
 90
 100  1000  10000  100000  1e+06
A
cc
u
ra
cy
Number of auto-annotated training pts.
BoW+Usr
BoW
Usr
Figure 2: Learning curve for Bootstrapped
logistic-regression classifier, with automatically-
labeled data, for different feature classes.
features capture reasonable associations between
gender classes and particular names (such as mike,
tony, omar, etc.) and also between gender classes
and common nouns (such as guy, dad, sir, etc.).
7 Related Work
User Characterization The field of sociolin-
guistics has long been concerned with how various
morphological, phonological and stylistic aspects
of language can vary with a person?s age, gender,
social class, etc. (Fischer, 1968; Labov, 1972).
This early work therefore had an emphasis on ana-
lyzing the form of language, as opposed to its con-
tent. This emphasis continued into early machine
learning approaches, which predicted author prop-
erties based on the usage of function words, parts-
of-speech, punctuation (Koppel et al, 2002) and
spelling/grammatical errors (Koppel et al, 2005).
Recently, researchers have focused less on the
sociolinguistic implications and more on the tasks
themselves, naturally leading to classifiers with
feature representations capturing content in ad-
dition to style (Schler et al, 2006; Garera and
Yarowsky, 2009; Mukherjee and Liu, 2010). Our
work represents a logical next step for content-
based classification, a step partly suggested by
Schler et al (2006) who noted that ?those who
are interested in automatically profiling bloggers
for commercial purposes would be well served by
considering additional features - which we delib-
erately ignore in this study - such as author self-
identification.?
Male BoW features: wife, wifey, sucked, shave,
boner, boxers, missus, installed, manly, in-laws,
brah, urinal, kickoff, golf, comics, ubuntu, homo,
nhl, jedi, fatherhood, nigga, movember, algebra
Male Usr features: boy, mike, ben, guy, mr, dad,
jr, kid, tony, dog, lord, sir, omar, dude, man, big
Female BoW features: hubby, hubs, jewelry,
sewing, mascara, fabulous, bf, softball, betcha,
motherhood, perky, cozy, zumba, xox, cuddled,
belieber, bridesmaid, anorexic, jammies, pad
Female Usr features: mrs, mom, jen, lady, wife,
mary, joy, mama, pink, kim, diva, elle, woma, ms
Table 4: Examples of highly-weighted BoW (con-
tent) and Usr (username) features (in descending
order of weight) in the Bootstrapped system for
predicting user gender in Twitter.
Many recent papers have analyzed the lan-
guage of social media users, along dimensions
such as ethnicity (Eisenstein et al, 2011; Rao et
al., 2011; Pennacchiotti and Popescu, 2011; Fink
et al, 2012) time zone (Kiciman, 2010), polit-
ical orientation (Rao et al, 2010; Pennacchiotti
and Popescu, 2011) and gender (Rao et al, 2010;
Burger et al, 2011; Van Durme, 2012).
Class-Attribute Extraction The idea of using
simple patterns to extract useful semantic relations
goes back to Hearst (1992) who focused on hy-
ponyms. Hearst reports that she ?tried applying
this technique to meronymy (i.e., the part/whole
relation), but without great success.? Berland and
Charniak (1999) did have success using Hearst-
style patterns for part-whole detection, which they
attribute to their ?very large corpus and the use of
more refined statistical measures for ranking the
output.? Girju et al (2006) devised a supervised
classification scheme for part/whole relation dis-
covery that integrates the evidence from multiple
patterns. These efforts focused exclusively on the
meronymy relation as used in WordNet (Miller et
al., 1990). Indeed, Berland and Charniak (1999)
attempted to filter out attributes that were regarded
as qualities (like driveability) rather than parts
(like steering wheels) by removing words end-
ing with the suffixes -ness, -ing, and -ity. In our
work, such qualities are not filtered and are ulti-
mately valuable in classification; for example, the
attributes peak fertility and loveliness are highly
716
associated with females.
As subsequent research became more focused
on applications, looser definitions of class at-
tributes were adopted. Almuhareb and Poesio
(2004) automatically mined class attributes that in-
clude parts, qualities, and those with an ?agen-
tive? or ?telic? role with the class. Their ex-
tended set of attributes was shown to enable an
improved representation of nouns for the purpose
of clustering these nouns into semantic concepts.
Tokunaga et al (2005) define attributes as prop-
erties that can serve as focus words in questions
about a target class; e.g. director is an attribute
of a movie since one might ask, ?Who is the di-
rector of this movie?? Another line of research
has been motivated by the observation that much
of Internet search consists of people looking for
values of various class attributes (Bellare et al,
2007; Pas?ca and Van Durme, 2007; Pas?ca and Van
Durme, 2008; Alfonseca et al, 2010). By knowing
the attributes of different classes, search engines
can better recognize that queries such as ?altitude
guadalajara? or ?population guadalajara? are seek-
ing values for a particular city?s ?altitude? and
?population? attributes (Pas?ca and Van Durme,
2007). Finally, note that Van Durme et al (2008)
compared instance-based and class-based patterns
for broad-definition attribute extraction, and found
both to be effective.
Of course, text-mining with custom-designed
patterns is not the only way to extract class-
attribute information. Experts can manually spec-
ify the attributes of entities, as in the WordNet
project (Miller et al, 1990). Others have auto-
matically extracted attribute relations from dictio-
nary definitions (Richardson et al, 1998), struc-
tured online sources such as Wikipedia infoboxes,
(Wu and Weld, 2007) and large-scale collections
of high-quality tabular web data (Cafarella et al,
2008). Attribute extraction has also been viewed
as a sub-component or special case of the infor-
mation obtained by general-purpose knowledge
extractors (Schubert, 2002; Pantel and Pennac-
chiotti, 2006).
NLP Applications of Common-Sense Knowl-
edge The kind of information derived from
class-attribute extraction is sometimes referred to
as a type of common-sense knowledge. The need
for computer programs to represent common-
sense knowledge has been recognized since the
work of McCarthy (1959). Lenat et al (1990)
defines common sense as ?human consensus re-
ality knowledge: the facts and concepts that you
and I know and which we each assume the other
knows.?
While we are the first to exploit common-
sense knowledge in user characterization, com-
mon sense has been applied to a range of other
problems in natural language processing. In many
ways WordNet can be regarded as a collection of
common-sense relationships. WordNet has been
applied in a myriad of NLP applications, includ-
ing in seminal works on semantic-role labeling
(Gildea and Jurafsky, 2002), coreference resolu-
tion (Soon et al, 2001) and spelling correction
(Budanitsky and Hirst, 2006). Also, many ap-
proaches to the task of sentiment analysis ?be-
gin with a large lexicon of words marked with
their prior polarity? (Wilson et al, 2009). Like
our class-attribute associations, the common-sense
knowledge that the word cool is positive while
unethical is negative can be learned from asso-
ciations in web-scale data (Turney, 2002). We
might also view information about synonyms or
conceptually-similar words as a kind of common-
sense knowledge. In this perspective, our work
is related to recent work that has extracted
distributionally-similar words from web-scale data
and applied this knowledge in tasks such as
named-entity recognition (Lin and Wu, 2009) and
dependency parsing (Ta?ckstro?m et al, 2012).
8 Conclusion
We have proposed, developed and successfully
evaluated a novel approach to user characteriza-
tion based on exploiting knowledge of user class
attributes. The knowledge is obtained using a new
algorithm that discovers distinguishing attributes
of particular classes. Our approach to discovering
distinguishing attributes represents a significant
new direction for research in class-attribute extrac-
tion, and provides a valuable bridge between the
fields of user characterization and lexical knowl-
edge extraction.
We presented three effective techniques for
leveraging this knowledge within the framework
of supervised user characterization: rule-based
post-processing, a learning-by-bootstrapping ap-
proach, and a stacking approach that integrates the
predictions of the bootstrapped system into a sys-
tem trained on annotated gold-standard training
data. All techniques lead to significant improve-
717
ments over state-of-the-art supervised systems on
the task of Twitter gender classification.
While our technique has advanced the state-of-
the-art on this important task, our approach may
prove even more useful on other tasks where train-
ing on thousands of gold-standard examples is not
even an option. Currently we are exploring the
prediction of finer-grained user roles, such as stu-
dent, waitress, parent, and so forth, based on ex-
tensions to the process laid out here.
References
Enrique Alfonseca, Marius Pas?ca, and Enrique
Robledo-Arnuncio. 2010. Acquisition of instance
attributes via labeled and related instances. In Proc.
SIGIR, pages 58?65.
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: An
evaluation. In Proc. EMNLP, pages 158?165.
Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2007. Lightly-Supervised
Attribute Extraction. In NIPS Workshop on Machine
Learning for Web Search.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proc. Coling-
ACL, pages 33?40.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74.
Shane Bergsma, Mark Dredze, Benjamin Van
Durme, Theresa Wilson, and David Yarowsky.
2013. Broadly improving user classification via
communication-based name and location clustering
on twitter. In Proc. NAACL.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proc. ACL, pages
57?64.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
John D. Burger and John C. Henderson. 2006. An
exploration of observable features related to blogger
age. In Proc. AAAI Spring Symposium: Computa-
tional Approaches to Analyzing Weblogs, pages 15?
20.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proc. EMNLP, pages 1301?1309.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang,
Eugene Wu, and Yang Zhang. 2008. WebTables:
exploring the power of tables on the web. Proc.
PVLDB, 1(1):538?549.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1).
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365?1374.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. J. Mach.
Learn. Res., 9:1871?1874.
Clayton Fink, Jonathon Kopecky, Nathan Bos, and
Max Thomas. 2012. Mapping the Twitterverse in
the developing world: An analysis of social media
use in Nigeria. In Proc. International Conference on
Social Computing, Behavioral Modeling, and Pre-
diction, pages 164?171.
John L. Fischer. 1968. Social influences on the choice
of a linguistic variant. Word, 14:47?56.
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proc. ACL-IJCNLP, pages 710?718.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28:245?288.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. Coling,
pages 539?545.
Emre Kiciman. 2010. Language differences and meta-
data features on Twitter. In Proc. SIGIR 2010 Web
N-gram Workshop, pages 47?51.
Moshe Koppel and Jonathan Schler. 2004. Authorship
verification as a one-class classification problem. In
Proc. ICML, pages 489?495.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Linguistic
Computing, 17(4):401?412.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author?s native language by
mining a text for errors. In Proc. KDD, pages 624?
628.
718
William Labov. 1972. Sociolinguistic Patterns. Uni-
versity of Pennsylvania Press.
Douglas B. Lenat, R. V. Guha, Karen Pittman, Dex-
ter Pratt, and Mary Shepherd. 1990. CYC: toward
programs with common sense. Commun. ACM,
33(8):30?49.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proc. ACL-IJCNLP,
pages 1030?1038.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale N-grams. In Proc. LREC, pages 2221?
2227.
John McCarthy. 1959. Programs with common sense.
In Proc. Teddington Conference on the Mechaniza-
tion of Thought Processes, pages 75?91. London:
Her Majesty?s Stationery Office.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547?
559.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller.
1990. Introduction to WordNet: an on-line lexical
database. International Journal of Lexicography,
3(4).
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proc. EMNLP,
pages 207?217.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proc. ICWSM, pages
122?129.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. Coling-
ACL, pages 113?120.
Marius Pas?ca and Benjamin Van Durme. 2007. What
you seek is what you get: extraction of class at-
tributes from query logs. In Proc. IJCAI, pages
2832?2837.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proc. ACL-08: HLT, pages 19?27.
Michael Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proc. ICWSM, pages 265?272.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to Twitter user classi-
fication. In Proc. ICWSM, pages 281?288.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English
POS Tagger. crftagger.sourceforge.net.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. International Work-
shop on Search and Mining User-Generated Con-
tents, pages 37?44.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hi-
erarchical bayesian models for latent attribute detec-
tion in social media. In Proc. ICWSM, pages 598?
601.
Joseph Reisinger and Marius Pas?ca. 2009. Latent
variable models of concept-attribute attachment. In
Proc. ACL-IJCNLP, pages 620?628.
Stephen D. Richardson, William B. Dolan, and Lucy
Vanderwende. 1998. MindNet: Acquiring and
structuring semantic information from text. In Proc.
ACL-Coling, pages 1098?1102.
Jonathan Schler, Moshe Koppel, Shlomo Argamon,
and James W. Pennebaker. 2006. Effects of age and
gender on blogging. In Proc. AAAI Spring Sympo-
sium: Computational Approaches to Analyzing We-
blogs, pages 199?205.
Lenhart Schubert. 2002. Can we derive general world
knowledge from texts? In Proc. HLT, pages 84?87.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Comput. Surv.,
34:1?47.
Ian Soboroff, Dean McCullough, Jimmy Lin, Craig
Macdonald, Iadh Ounis, and Richard McCreadie.
2012. Evaluating real-time search over tweets. In
Proc. ICWSM.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4).
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proc. NAACL-
HLT, pages 477?487.
Kosuke Tokunaga, Jun?ichi Kazama, and Kentaro Tori-
sawa. 2005. Automatic discovery of attribute words
from web documents. In Proc. IJCNLP, pages 106?
118.
719
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proc. ACL, pages 417?424.
Benjamin Van Durme, Ting Qian, and Lenhart Schu-
bert. 2008. Class-driven attribute extraction. In
Proc. Coling, pages 921?928.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proc. EMNLP-CoNLL,
pages 48?58.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics., 35(3):399?433.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In Proc. CIKM, pages 41?
50.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In Proc.
NAACL-HLT.
720
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 63?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PARMA: A Predicate Argument Aligner
Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,
Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,
Jonathan Weese, Tan Xu?, and Xuchen Yao
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland USA
?University of Maryland, College Park, Maryland USA
Abstract
We introduce PARMA, a system for cross-
document, semantic predicate and argu-
ment alignment. Our system combines a
number of linguistic resources familiar to
researchers in areas such as recognizing
textual entailment and question answering,
integrating them into a simple discrimina-
tive model. PARMA achieves state of the
art results on an existing and a new dataset.
We suggest that previous efforts have fo-
cussed on data that is biased and too easy,
and we provide a more difficult dataset
based on translation data with a low base-
line which we beat by 17% F1.
1 Introduction
A key step of the information extraction pipeline
is entity disambiguation, in which discovered en-
tities across many sentences and documents must
be organized to represent real world entities. The
NLP community has a long history of entity dis-
ambiguation both within and across documents.
While most information extraction work focuses
on entities and noun phrases, there have been a
few attempts at predicate, or event, disambigua-
tion. Commonly a situational predicate is taken to
correspond to either an event or a state, lexically
realized in verbs such as ?elect? or nominaliza-
tions such as ?election?. Similar to entity coref-
erence resolution, almost all of this work assumes
unanchored mentions: predicate argument tuples
are grouped together based on coreferent events.
The first work on event coreference dates back to
Bagga and Baldwin (1999). More recently, this
task has been considered by Bejan and Harabagiu
(2010) and Lee et al (2012). As with unanchored
entity disambiguation, these methods rely on clus-
tering methods and evaluation metrics.
Another view of predicate disambiguation seeks
to link or align predicate argument tuples to an ex-
isting anchored resource containing references to
events or actions, similar to anchored entity dis-
ambiguation (entity linking) (Dredze et al, 2010;
Han and Sun, 2011). The most relevant, and per-
haps only, work in this area is that of Roth and
Frank (2012) who linked predicates across docu-
ment pairs, measuring the F1 of aligned pairs.
Here we present PARMA, a new system for pred-
icate argument alignment. As opposed to Roth and
Frank, PARMA is designed as a a trainable plat-
form for the incorporation of the sort of lexical se-
mantic resources used in the related areas of Rec-
ognizing Textual Entailment (RTE) and Question
Answering (QA). We demonstrate the effective-
ness of this approach by achieving state of the art
performance on the data of Roth and Frank despite
having little relevant training data. We then show
that while the ?lemma match? heuristic provides a
strong baseline on this data, this appears to be an
artifact of their data creation process (which was
heavily reliant on word overlap). In response, we
evaluate on a new and more challenging dataset for
predicate argument alignment derived from multi-
ple translation data. We release PARMA as a new
framework for the incorporation and evaluation of
new resources for predicate argument alignment.1
2 PARMA
PARMA (Predicate ARguMent Aligner) is a
pipelined system with a wide variety of features
used to align predicates and arguments in two doc-
uments. Predicates are represented as mention
spans and arguments are represented as corefer-
ence chains (sets of mention spans) provided by
in-document coreference resolution systems such
as included in the Stanford NLP toolkit. Results
indicated that the chains are of sufficient quality
so as not to limit performance, though future work
1https://github.com/hltcoe/parma
63
RF
? Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic
[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7
? Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , the
suspect s [attorney]6 [said]7 Tuesday
LDC MTC
? As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5
in which the poetic lines in praise of Prophet Mohammad are recited -RRB-
? I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4
[meeting]5 .
Figure 1: Example of gold-standard alignment pairs from Roth and Frank?s data set and our data set
created from the LDC?s Multiple Translation Corpora. The RF data set exhibits high lexical overlap,
where most of the alignments are between identical words like police-police and said-said. The LDC
MTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-
balcony and tent-camp
may relax this assumption.
We refer to a predicate or an argument as an
?item? with type predicate or argument. An align-
ment between two documents is a subset of all
pairs of items in either documents with the same
type.2 We call the two documents being aligned
the source document S and the target document
T . Items are referred to by their index, and ai,j is a
binary variable representing an alignment between
item i in S and item j in T . A full alignment is an
assignment ~a = {aij : i ? NS , j ? NT }, where
NS and NT are the set of item indices for S and T
respectively.
We train a logistic regression model on exam-
ple alignmentsand maximize the likelihood of a
document alignment under the assumption that the
item alignments are independent. Our objective
is to maximize the log-likelihood of all p(S, T )
with an L1 regularizer (with parameter ?). After
learning model parameters w by regularized max-
imum likelihood on training data, we introducing
a threshold ? on alignment probabilities to get a
classifier. We perform line search on ? and choose
the value that maximizes F1 on dev data. Train-
ing was done using the Mallet toolkit (McCallum,
2002).
2.1 Features
The focus of PARMA is the integration of a diverse
range of features based on existing lexical seman-
tic resources. We built PARMA on a supervised
framework to take advantage of this wide variety
of features since they can describe many different
correlated aspects of generation. The following
features cover the spectrum from high-precision
2Note that type is not the same thing as part of speech: we
allow nominal predicates like ?death?.
to high-recall. Each feature has access to the pro-
posed argument or predicate spans to be linked and
the containing sentences as context. While we use
supervised learning, some of the existing datasets
for this task are very small. For extra training data,
we pool material from different datasets and use
the multi-domain split feature space approach to
learn dataset specific behaviors (Daume?, 2007).
Features in general are defined over mention
spans or head tokens, but we split these features
to create separate feature-spaces for predicates and
arguments.3
For argument coref chains we heuristically
choose a canonical mention to represent each
chain, and some features only look at this canon-
ical mention. The canonical mention is cho-
sen based on length,4 information about the head
word,5 and position in the document.6 In most
cases, coref chains that are longer than one are
proper nouns and the canonical mention is the first
and longest mention (outranking pronominal ref-
erences and other name shortenings).
PPDB We use lexical features from the Para-
phrase Database (PPDB) (Ganitkevitch et al,
2013). PPDB is a large set of paraphrases ex-
tracted from bilingual corpora using pivoting tech-
niques. We make use of the English lexical portion
which contains over 7 million rules for rewriting
terms like ?planet? and ?earth?. PPDB offers a
variety of conditional probabilities for each (syn-
chronous context free grammar) rule, which we
3While conceptually cleaner, In practice we found this
splitting to have no impact on performance.
4in tokens, not counting some words like determiners and
auxiliary verbs
5like its part of speech tag and whether the it was tagged
as a named entity
6mentions that appear earlier in the document and earlier
in a given sentence are given preference
64
treat as independent experts. For each of these rule
probabilities (experts), we find all rules that match
the head tokens of a given alignment and have a
feature for the max and harmonic mean of the log
probabilities of the resulting rule set.
FrameNet FrameNet is a lexical database based
on Charles Fillmore?s Frame Semantics (Fill-
more, 1976; Baker et al, 1998). The database
(and the theory) is organized around seman-
tic frames that can be thought of as descrip-
tions of events. Frames crucially include spec-
ification of the participants, or Frame Elements,
in the event. The Destroying frame, for in-
stance, includes frame elements Destroyer or
Cause Undergoer. Frames are related to other
frames through inheritance and perspectivization.
For instance the frames Commerce buy and
Commerce sell (with respective lexical real-
izations ?buy? and ?sell?) are both perspectives of
Commerce goods-transfer (no lexical re-
alizations) which inherits from Transfer (with
lexical realization ?transfer?).
We compute a shortest path between headwords
given edges (hypernym, hyponym, perspectivized
parent and child) in FrameNet and bucket by dis-
tance to get features. We also have a binary feature
for whether two tokens evoke the same frame.
TED Alignments Given two predicates or argu-
ments in two sentences, we attempt to align the
two sentences they appear in using a Tree Edit
Distance (TED) model that aligns two dependency
trees, based on the work described by (Yao et al,
2013). We represent a node in a dependency tree
with three fields: lemma, POS tag and the type
of dependency relation to the node?s parent. The
TED model aligns one tree with the other using
the dynamic programming algorithm of Zhang and
Shasha (1989) with three predefined edits: dele-
tion, insertion and substitution, seeking a solution
yielding the minimum edit cost. Once we have
built a tree alignment, we extract features for 1)
whether the heads of the two phrases are aligned
and 2) the count of how many tokens are aligned
in both trees.
WordNet WordNet (Miller, 1995) is a database
of information (synonyms, hypernyms, etc.) per-
taining to words and short phrases. For each entry,
WordNet provides a set of synonyms, hypernyms,
etc. Given two spans, we use WordNet to deter-
mine semantic similarity by measuring how many
synonym (or other) edges are needed to link two
terms. Similar words will have a short distance.
For features, we find the shortest path linking the
head words of two mentions using synonym, hy-
pernym, hyponym, meronym, and holonym edges
and bucket the length.
String Transducer To represent similarity be-
tween arguments that are names, we use a stochas-
tic edit distance model. This stochastic string-to-
string transducer has latent ?edit? and ?no edit?
regions where the latent regions allow the model
to assign high probability to contiguous regions of
edits (or no edits), which are typical between vari-
ations of person names. In an edit region, param-
eters govern the relative probability of insertion,
deletion, substitution, and copy operations. We
use the transducer model of Andrews et al (2012).
Since in-domain name pairs were not available, we
picked 10,000 entities at random from Wikipedia
to estimate the transducer parameters. The entity
labels were used as weak supervision during EM,
as in Andrews et al (2012).
For a pair of mention spans, we compute the
conditional log-likelihood of the two mentions go-
ing both ways, take the max, and then bucket to get
binary features. We duplicate these features with
copies that only fire if both mentions are tagged as
PER, ORG or LOC.
3 Evaluation
We consider three datasets for evaluating PARMA.
For richer annotations that include lemmatiza-
tions, part of speech, NER, and in-doc corefer-
ence, we pre-processed each of the datasets using
tools7 similar to those used to create the Annotated
Gigaword corpus (Napoles et al, 2012).
Extended Event Coreference Bank Based on
the dataset of Bejan and Harabagiu (2010), Lee et
al. (2012) introduced the Extended Event Coref-
erence Bank (EECB) to evaluate cross-document
event coreference. EECB provides document clus-
ters, within which entities and events may corefer.
Our task is different from Lee et al but we can
modify the corpus setup to support our task. To
produce source and target document pairs, we se-
lect the first document within every cluster as the
source and each of the remaining documents as
target documents (i.e. N ? 1 pairs for a cluster
of size N ). This yielded 437 document pairs.
Roth and Frank The only existing dataset for
our task is from Roth and Frank (2012) (RF), who
7https://github.com/cnap/anno-pipeline
65
annotated documents from the English Gigaword
Fifth Edition corpus (Parker et al, 2011). The data
was generated by clustering similar news stories
from Gigaword using TF-IDF cosine similarity of
their headlines. This corpus is small, containing
only 10 document pairs in the development set and
60 in the test set. To increase the training size,
we train PARMA with 150 randomly selected doc-
ument pairs from both EECB and MTC, and the
entire dev set from Roth and Frank using multi-
domain feature splitting. We tuned the threshold
? on the Roth and Frank dev set, but choose the
regularizer ? based on a grid search on a 5-fold
version of the EECB dataset.
Multiple Translation Corpora We constructed
a new predicate argument alignment dataset
based on the LDC Multiple Translation Corpora
(MTC),8 which consist of multiple English trans-
lations for foreign news articles. Since these mul-
tiple translations are semantically equivalent, they
provide a good resource for aligned predicate ar-
gument pairs. However, finding good pairs is a
challenge: we want pairs with significant overlap
so that they have predicates and arguments that
align, but not documents that are trivial rewrites
of each other. Roth and Frank selected document
pairs based on clustering, meaning that the pairs
had high lexical overlap, often resulting in mini-
mal rewrites of each other. As a result, despite ig-
noring all context, their baseline method (lemma-
alignment) worked quite well.
To create a more challenging dataset, we se-
lected document pairs from the multiple transla-
tions that minimize the lexical overlap (in En-
glish). Because these are translations, we know
that there are equivalent predicates and arguments
in each pair, and that any lexical variation pre-
serves meaning. Therefore, we can select pairs
with minimal lexical overlap in order to create
a system that truly stresses lexically-based align-
ment systems.
Each document pair has a correspondence be-
tween sentences, and we run GIZA++ on these
sentences to produce token-level alignments. We
take all aligned nouns as arguments and all aligned
verbs (excluding be-verbs, light verbs, and report-
ing verbs) as predicates. We then add negative ex-
amples by randomly substituting half of the sen-
tences in one document with sentences from an-
8LDC2010T10, LDC2010T11, LDC2010T12,
LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,
LDC2003T18, and LDC2005T05
0.3 0.4 0.5 0.6 0.7 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Performance vs Lexical Overlap
Doc-pair Cosine Similarity
F1
Figure 2: We plotted the PARMA?s performance on
each of the document pairs. Red squares show the
F1 for individual document pairs drawn from Roth
and Frank?s data set, and black circles show F1 for
our Multiple Translation Corpora test set. The x-
axis represents the cosine similarity between the
document pairs. On the RF data set, performance
is correlated with lexical similarity. On our more
lexically diverse set, this is not the case. This
could be due to the fact that some of the docu-
ments in the RF sets are minor re-writes of the
same newswire story, making them easy to align.
other corpus, guaranteed to be unrelated. The
amount of substitutions we perform can vary the
?relatedness? of the two documents in terms of
the predicates and arguments that they talk about.
This reflects our expectation of real world data,
where we do not expect perfect overlap in predi-
cates and arguments between a source and target
document, as you would in translation data.
Lastly, we prune any document pairs that have
more than 80 predicates or arguments or have a
Jaccard index on bags of lemmas greater than 0.5,
to give us a dataset of 328 document pairs.
Metric We use precision, recall, and F1. For the
RF dataset, we follow Roth and Frank (2012) and
Cohn et al (2008) and evaluate on a version of F1
that considers SURE and POSSIBLE links, which
are available in the RF data. Given an alignment
to be scored A and a reference alignment B which
contains SURE and POSSIBLE links, Bs andBp re-
spectively, precision and recall are:
P = |A ?Bp||A| R =
|A ?Bs|
|Bs|
(1)
66
F1 P R
EECB lemma 63.5 84.8 50.8
PARMA 74.3 80.5 69.0
RF lemma 48.3 40.3 60.3
Roth and Frank 54.8 59.7 50.7
PARMA 57.6 52.4 64.0
MTC lemma 42.1 51.3 35.7
PARMA 59.2 73.4 49.6
Table 1: PARMA outperforms the baseline lemma
matching system on the three test sets, drawn from
the Extended Event Coreference Bank, Roth and
Frank?s data, and our set created from the Multiple
Translation Corpora. PARMA achieves a higher F1
and recall score than Roth and Frank?s reported
result.
and F1 as the harmonic mean of the two. Results
for EECB and MTC reflect 5-fold cross validation,
and RF uses the given dev/test split.
Lemma baseline Following Roth and Frank we
include a lemma baseline, in which two predicates
or arguments align if they have the same lemma.9
4 Results
On every dataset PARMA significantly improves
over the lemma baselines (Table 1). On RF,
compared to Roth and Frank, the best published
method for this task, we also improve, making
PARMA the state of the art system for this task.
Furthermore, we expect that the smallest improve-
ments over Roth and Frank would be on RF, since
there is little training data. We also note that com-
pared to Roth and Frank we obtain much higher
recall but lower precision.
We also observe that MTC was more challeng-
ing than the other datasets, with a lower lemma
baseline. Figure 2 shows the correlation between
document similarity and document F1 score for
RF and MTC. While for RF these two measures
are correlated, they are uncorrelated for MTC. Ad-
ditionally, there is more data in the MTC dataset
which has low cosine similarity than in RF.
5 Conclusion
PARMA achieves state of the art performance on
three datasets for predicate argument alignment.
It builds on the development of lexical semantic
resources and provides a platform for learning to
utilize these resources. Additionally, we show that
9We could not reproduce lemma from Roth and Frank
(shown in Table 1) due to a difference in lemmatizers. We ob-
tained 55.4; better than their system but worse than PARMA.
task difficulty can be strongly tied to lexical simi-
larity if the evaluation dataset is not chosen care-
fully, and this provides an artificially high baseline
in previous work. PARMA is robust to drops in lex-
ical similarity and shows large improvements in
those cases. PARMA will serve as a useful bench-
mark in determining the value of more sophis-
ticated models of predicate-argument alignment,
which we aim to address in future work.
While our system is fully supervised, and thus
dependent on manually annotated examples, we
observed here that this requirement may be rela-
tively modest, especially for in-domain data.
Acknowledgements
We thank JHU HLTCOE for hosting the winter
MiniSCALE workshop that led to this collabora-
tive work. This material is based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes. The views and con-
clusions contained in this publication are those of
the authors and should not be interpreted as repre-
senting official policies or endorsements of NSF,
DARPA, or the U.S. Government.
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Empirical Methods in Natural
Language Processing (EMNLP).
Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 1?8. Association for Computational Linguis-
tics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
67
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Annual meeting-association for computa-
tional linguistics, volume 45, page 256.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20?
32.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945?
954. Association for Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Robert Parker, David Graff, Jumbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword fifth
edition.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218?
227, Montre?al, Canada, 7-8 June. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as
sequence tagging with tree edit distance. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
68
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 159?165,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatic Coupling of Answer Extraction and Information Retrieval
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Peter Clark
Vulcan Inc.
Seattle, WA, USA
Abstract
Information Retrieval (IR) and Answer
Extraction are often designed as isolated
or loosely connected components in Ques-
tion Answering (QA), with repeated over-
engineering on IR, and not necessarily per-
formance gain for QA. We propose to
tightly integrate them by coupling auto-
matically learned features for answer ex-
traction to a shallow-structured IR model.
Our method is very quick to implement,
and significantly improves IR for QA
(measured in Mean Average Precision and
Mean Reciprocal Rank) by 10%-20%
against an uncoupled retrieval baseline
in both document and passage retrieval,
which further leads to a downstream 20%
improvement in QA F1.
1 Introduction
The overall performance of a Question Answer-
ing system is bounded by its Information Re-
trieval (IR) front end, resulting in research specif-
ically on Information Retrieval for Question An-
swering (IR4QA) (Greenwood, 2008; Sakai et al,
2010). Common approaches such as query expan-
sion, structured retrieval, and translation models
show patterns of complicated engineering on the
IR side, or isolate the upstream passage retrieval
from downstream answer extraction. We argue
that: 1. an IR front end should deliver exactly
what a QA1 back end needs; 2. many intuitions
employed by QA should be and can be re-used in
IR, rather than re-invented. We propose a coupled
retrieval method with prior knowledge of its down-
stream QA component, that feeds QA with exactly
the information needed.
1After this point in the paper we use the term QA in a
narrow sense: QA without the IR component, i.e., answer
extraction.
As a motivating example, using the ques-
tion When was Alaska purchased from
the TREC 2002 QA track as the query to the In-
dri search engine, the top sentence retrieved from
the accompanying AQUAINT corpus is:
Eventually Alaska Airlines will
allow all travelers who have
purchased electronic tickets
through any means.
While this relates Alaska and purchased, it
is not a useful passage for the given question.2 It
is apparent that the question asks for a date. Prior
work proposed predictive annotation (Prager et al,
2000; Prager et al, 2006): text is first annotated in
a predictive manner (of what types of questions it
might answer) with 20 answer types and then in-
dexed. A question analysis component (consisting
of 400 question templates) maps the desired an-
swer type to one of the 20 existing answer types.
Retrieval is then performed with both the question
and predicated answer types in the query.
However, predictive annotation has the limita-
tion of being labor intensive and assuming the un-
derlying NLP pipeline to be accurate. We avoid
these limitations by directly asking the down-
stream QA system for the information about which
entities answer which questions, via two steps:
1. reusing the question analysis components from
QA; 2. forming a query based on the most relevant
answer features given a question from the learned
QA model. There is no query-time overhead and
no manual template creation. Moreover, this ap-
proach is more robust against, e.g., entity recog-
nition errors, because answer typing knowledge is
learned from how the data was actually labeled,
not from how the data was assumed to be labeled
(e.g., manual templates usually assume perfect la-
beling of named entities, but often it is not the case
2Based on a non-optimized IR configuration, none of the
top 1000 returned passages contained the correct answer:
1867.
159
in practice).
We use our statistically-trained QA system (Yao
et al, 2013) that recognizes the association be-
tween question type and expected answer types
through various features. The QA system employs
a linear chain Conditional Random Field (CRF)
(Lafferty et al, 2001) and tags each token as either
an answer (ANS) or not (O). This will be our off-
the-shelf QA system, which recognizes the associ-
ation between question type and expected answer
types through various features based on e.g., part-
of-speech tagging (POS) and named entity recog-
nition (NER).
With weights optimized by CRF training (Ta-
ble 1), we can learn how answer features are cor-
related with question features. These features,
whose weights are optimized by the CRF train-
ing, directly reflect what the most important an-
swer types associated with each question type are.
For instance, line 2 in Table 1 says that if there is a
when question, and the current token?s NER label
is DATE, then it is likely that this token is tagged
as ANS. IR can easily make use of this knowledge:
for a when question, IR retrieves sentences with
tokens labeled as DATE by NER, or POS tagged as
CD. The only extra processing is to pre-tag and
index the text with POS and NER labels. The ana-
lyzing power of discriminative answer features for
IR comes for free from a trained QA system. Un-
like predictive annotation, statistical evidence de-
termines the best answer features given the ques-
tion, with no manual pattern or templates needed.
To compare again predictive annotation with
our approach: predictive annotation works in a
forward mode, downstream QA is tailored for up-
stream IR, i.e., QA works on whatever IR re-
trieves. Our method works in reverse (backward):
downstream QA dictates upstream IR, i.e., IR re-
trieves what QA wants. Moreover, our approach
extends easily beyond fixed answer types such as
named entities: we are already using POS tags as a
demonstration. We can potentially use any helpful
answer features in retrieval. For instance, if the
QA system learns that in order to is highly
correlated with why question through lexicalized
features, or some certain dependency relations are
helpful in answering questions with specific struc-
tures, then it is natural and easy for the IR compo-
nent to incorporate them.
There is also a distinction between our method
and the technique of learning to rank applied in
feature label weight
qword=when|POS0=CD ANS 0.86
qword=when|NER0=DATE ANS 0.79
qword=when|POS0=CD O -0.74
Table 1: Learned weights for sampled features with respect
to the label of current token (indexed by [0]) in a CRF. The
larger the weight, the more ?important? is this feature to help
tag the current token with the corresponding label. For in-
stance, line 1 says when answering a when question, and
the POS of current token is CD (cardinal number), it is likely
(large weight) that the token is tagged as ANS.
QA (Bilotti et al, 2010; Agarwal et al, 2012). Our
method is a QA-driven approach that provides su-
pervision for IR from a learned QA model, while
learning to rank is essentially an IR-driven ap-
proach: the supervision for IR comes from a la-
beled ranking list of retrieval results.
Overall, we make the following contributions:
? Our proposed method tightly integrates QA
with IR and the reuse of analysis from QA does
not put extra overhead on the IR queries. This
QA-driven approach provides a holistic solution
to the task of IR4QA.
? We learn statistical evidence about what the
form of answers to different questions look like,
rather than using manually authored templates.
This provides great flexibility in using answer
features in IR queries.
We give a full spectrum evaluation of all three
stages of IR+QA: document retrieval, passage re-
trieval and answer extraction, to examine thor-
oughly the effectiveness of the method.3 All of
our code and datasets are publicly available.4
2 Background
Besides Predictive Annotation, our work is closest
to structured retrieval, which covers techniques of
dependency path mapping (Lin and Pantel, 2001;
Cui et al, 2005; Kaisser, 2012), graph matching
with Semantic Role Labeling (Shen and Lapata,
2007) and answer type checking (Pinchak et al,
2009), etc. Specifically, Bilotti et al (2007) pro-
posed indexing text with their semantic roles and
named entities. Queries then include constraints
of semantic roles and named entities for the pred-
icate and its arguments in the question. Improve-
ments in recall of answer-bearing sentences were
shown over the bag-of-words baseline. Zhao and
3Rarely are all three aspects presented in concert (see ?2).
4http://code.google.com/p/jacana/
160
Callan (2008) extended this work with approx-
imate matching and smoothing. Most research
uses parsing to assign deep structures. Com-
pared to shallow (POS, NER) structured retrieval,
deep structures need more processing power and
smoothing, but might also be more precise. 5
Most of the above (except Kaisser (2012)) only
reported on IR or QA, but not both, assuming that
improvement in one naturally improves the other.
Bilotti and Nyberg (2008) challenged this assump-
tion and called for tighter coupling between IR and
QA. This paper is aimed at that challenge.
3 Method
Table 1 already shows some examples of features
associating question types with answer types. We
store the features and their learned weights from
the trained model for IR usage.
We let the trained QA system guide the query
formulation when performing coupled retrieval
with Indri (Strohman et al, 2005), given a corpus
already annotated with POS tags and NER labels.
Then retrieval runs in four steps (Figure 1):
1. Question Analysis. The question analysis com-
ponent from QA is reused here. In this imple-
mentation, the only information we have cho-
sen to use from the question is the question
word (e.g., how, who) and the lexical answer
types (LAT) in case of what/which questions.
2. Answer Feature Selection. Given the question
word, we select the 5 highest weighted features
(e.g., POS[0]=CD for a when question).
3. Query Formulation. The original question is
combined with the top features as the query.
4. Coupled Retrieval. Indri retrieves a ranked list
of documents or passages.
As motivated in the introduction, this framework
is aimed at providing the following benefits:
Reuse of QA components on the IR side. IR
reuses both code for question analysis and top
weighted features from QA.
Statistical selection of answer features. For in-
stance, the NER tagger we used divides location
into two categories: GPE (geo locations) and LOC
5Ogilvie (2010) showed in chapter 4.3 that keyword and
named entities based retrieval actually outperformed SRL-
based structured retrieval in MAP for the answer-bearing sen-
tence retrieval task in their setting. In this paper we do not
intend to re-invent another parse-based structure matching al-
gorithm, but only use shallow structures to show the idea of
coupling QA with IR; in the future this might be extended to
incorporate ?deeper? structure.
(non-GPE ). Both of them are learned to be impor-
tant to where questions.
Error tolerance along the NLP pipeline. IR
and QA share the same processing pipeline. Sys-
tematic errors made by the processing tools are
tolerated, in the sense that if the same pre-
processing error is made on both the question
and sentence, an answer may still be found.
Take the previous where question, besides
NER[0]=GPE and NER[0]=LOC, we also found
oddly NER[0]=PERSON an important feature, due
to that the NER tool sometimes mistakes PERSON
for LOC. For instance, the volcano name Mauna
Loa is labeled as a PERSON instead of a LOC. But
since the importance of this feature is recognized
by downstream QA, the upstream IR is still moti-
vated to retrieve it.
Queries were lightly optimized using the fol-
lowing strategies:
Query Weighting In practice query words are
weighted:
#weight(1.0 When 1.0 was 1.0 Alaska 1.0 purchased
? #max(#any:CD #any:DATE))
with a weight ? for the answer types tuned via
cross-validation.
Since NER and POS tags are not lexicalized
they accumulate many more counts (i.e. term fre-
quency) than individual words, thus we in gen-
eral downweight by setting ? < 1.0, giving the
expected answer types ?enough say? but not ?too
much say?:
NER Types First We found NER labels better in-
dicators of expected answer types than POS tags.
The reasons are two-fold: 1. In general POS tags
are too coarse-grained in answer types than NER
labels. E.g., NNP can answer who and where
questions, but is not as precise as PERSON and
GPE. 2. POS tags accumulate even more counts
than NER labels, thus they need separate down-
weighting. Learning the interplay of these weights
in a joint IR/QA model, is an interesting path for
future work. If the top-weighted features are based
on NER, then we do not include POS tags for that
question. Otherwise POS tags are useful, for in-
stance, in answering how questions.
Unigram QA Model The QA system uses up to
trigram features (Table 1 shows examples of uni-
gram and bigram features). Thus it is able to learn,
for instance, that a POS sequence of IN CD NNS
is likely an answer to a when question (such as:
in 5 years). This requires that the IR queries
161
When was Alaska purchased?
qword=when
qword=when|POS[0]=CD ? ANS: 0.86qword=when|NER[0]=DATE ? ANS: 0.79...
#combine(Alaska purchased #max(#any:CD  #any:DATE))
1. Simple question analysis(reuse from QA)2. Get top weighted features w.r.t qword(from trained QA model)
3. Query formulation
4. Coupled retrieval
On <DATE>March 30, <CD> 1867 </CD> </DATE>, U.S. ... reached agreement ... to purchase ... Alaska ...The islands were sold to the United States in <CD>1867</CD> with the purchase of Alaska.?...Eventually Alaska Airlines will allow all travelers who have purchased electronic tickets ...
12...50
Figure 1: Coupled retrieval with queries directly con-
structed from highest weighted features of downstream QA.
The retrieved and ranked list of sentences is POS and NER
tagged, but only query-relevant tags are shown due to space
limit. A bag-of-words retrieval approach would have the sen-
tence shown above at rank 50 at its top position instead.
look for a consecutive IN CD NNS sequence. We
drop this strict constraint (which may need further
smoothing) and only use unigram features, not by
simply extracting ?good? unigram features from
the trained model, but by re-training the model
with only unigram features. In answer extraction,
we still use up to trigram features. 6
4 Experiments
We want to measure and compare the performance
of the following retrieval techniques:
1. uncoupled retrieval with an off-the-shelf IR en-
gine by using the question as query (baseline),
2. QA-driven coupled retrieval (proposed), and
3. answer-bearing retrieval by using both the
question and known answer as query, only eval-
uated for answer extraction (upper bound),
at the three stages of question answering:
1. Document retrieval (for relevant docs from cor-
pus), measured by Mean Average Precision
(MAP) and Mean Reciprocal Rank (MRR).
2. Passage retrieval (finding relevant sentences
from the document), also by MAP and MRR.
3. Answer extraction, measured by F1.
6This is because the weights of unigram to trigram fea-
tures in a loglinear CRF model is a balanced consequence for
maximization. A unigram feature might end up with lower
weight because another trigram containing this unigram gets
a higher weight. Then we would have missed this feature
if we only used top unigram features. Thus we re-train the
model with only unigram features to make sure weights are
?assigned properly? among only unigram features.
set questions sentences#all #pos. #all #pos.
TRAIN 2205 1756 (80%) 22043 7637 (35%)
TESTgold 99 88 (89%) 990 368 (37%)
Table 2: Statistics for AMT-collected data (total cost was
around $800 for paying three Turkers per sentence). Positive
questions are those with an answer found. Positive sentences
are those bearing an answer.
All coupled and uncoupled queries are performed
with Indri v5.3 (Strohman et al, 2005).
4.1 Data
Test Set for IR and QA The MIT109 test col-
lection by Lin and Katz (2006) contains 109
questions from TREC 2002 and provides a near-
exhaustive judgment of relevant documents for
each question. We removed 10 questions that do
not have an answer by matching the TREC answer
patterns. Then we call this test set MIT99.
Training Set for QA We used Amazon Mechani-
cal Turk to collect training data for the QA system
by issuing answer-bearing queries for TREC1999-
2003 questions. For the top 10 retrieved sen-
tences for each question, three Turkers judged
whether each sentence contained the answer. The
inter-coder agreement rate was 0.81 (Krippen-
dorff, 2004; Artstein and Poesio, 2008).
The 99 questions of MIT99 were extracted from
the Turk collection as our TESTgold with the re-
maining as TRAIN, with statistics shown in Table
2. Note that only 88 questions out of MIT99 have
an answer from the top 10 query results.
Finally both the training and test data were
sentence-segmented and word-tokenized by
NLTK (Bird and Loper, 2004), dependency-
parsed by the Stanford Parser (Klein and
Manning, 2003), and NER-tagged by the Illinois
Named Entity Tagger (Ratinov and Roth, 2009)
with an 18-label type set.
Corpus Preprocessing for IR The AQUAINT
(LDC2002T31) corpus, on which the MIT99
questions are based, was processed in exactly the
same manner as was the QA training set. But
only sentence boundaries, POS tags and NER la-
bels were kept as the annotation of the corpus.
4.2 Document and Passage Retrieval
We issued uncoupled queries consisting of ques-
tion words, and QA-driven coupled queries con-
sisting of both the question and expected answer
types, then retrieved the top 1000 documents, and
162
type coupled uncoupledMAP MRR MAP MRR
document 0.2524 0.4835 0.2110 0.4298
sentence 0.1375 0.2987 0.1200 0.2544
Table 3: Coupled vs. uncoupled document/sentence re-
trieval in MAP and MRR on MIT99. Significance level
(Smucker et al, 2007) for both MAP: p < 0.001 and for
both MRR: p < 0.05.
finally computed MAP and MRR against the gold-
standard MIT99 per-document judgment.
To find the best weighting ? for coupled re-
trieval, we used 5-fold cross-validation and final-
ized at ? = 0.1. Table 3 shows the results.
Coupled retrieval outperforms (20% by MAP with
p < 0.001 and 12% by MRR with p < 0.01) un-
coupled retrieval significantly according to paired
randomization test (Smucker et al, 2007).
For passage retrieval, we extracted relevant sin-
gle sentences. Recall that MIT99 only contains
document-level judgment. To generate a test set
for sentence retrieval, we matched each sentence
from relevant documents provided by MIT99 for
each question against the TREC answer patterns.
We found no significant difference between re-
trieving sentences from the documents returned
by document retrieval or directly from the corpus.
Numbers of the latter are shown in Table 3. Still,
coupled retrieval is significantly better by about
10% in MAP and 17% in MRR.
4.3 Answer Extraction
Lastly we sent the sentences to the downstream
QA engine (trained on TRAIN) and computed F1
per K for the top K retrieved sentences, 7 shown
in Figure 2. The best F1 with coupled sentence re-
trieval is 0.231, 20% better than F1 of 0.192 with
uncoupled retrieval, both at K = 1.
The two descending lines at the bottom reflect
the fact that the majority-voting mechanism from
the QA system was too simple: F1 drops as K in-
creases. Thus we also computed F1?s assuming
perfect voting: a voting oracle that always selects
the correct answer as long as the QA system pro-
duces one, thus the two ascending lines in the cen-
ter of Figure 2. Still, F1 with coupled retrieval is
always better: reiterating the fact that coupled re-
trieval covers more answer-bearing sentences.
7Lin (2007), Zhang et al (2007), and Kaisser (2012) also
evaluated on MIT109. However their QA engines used web-
based search engines, thus leading to results that are neither
reproducible nor directly comparable with ours.
Finally, to find the upper bound for QA, we
drew the two upper lines, testing on TESTgold de-
scribed in Table 2. The test sentences were ob-
tained with answer-bearing queries. This is as-
suming almost perfect IR. The gap between the
top two and other lines signals more room for im-
provements for IR in terms of better coverage and
better rank for answer-bearing sentences.
1 2 3 5 10 15 20 50 100 200 500 1000Top K Sentences Retrieved
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
F1
Coupled (0.231)Uncoupled (0.192)
Gold Oracle (0.755)Gold (0.596)Coupled Oracle (0.609)Uncoupled Oracle (0.569)
Figure 2: F1 values for answer extraction on MIT99. Best
F1?s for each method are parenthesized in the legend. ?Or-
acle? methods assumed perfect voting of answer candidates
(a question is answered correctly if the system ever produced
one correct answer for it). ?Gold? was tested on TESTgold.
5 Conclusion
We described a method to perform coupled in-
formation retrieval with a prior knowledge of the
downstream QA system. Specifically, we coupled
IR queries with automatically learned answer fea-
tures from QA and observed significant improve-
ments in document/passage retrieval and boosted
F1 in answer extraction. This method has the mer-
its of not requiring hand-built question and answer
templates and being flexible in incorporating vari-
ous answer features automatically learned and op-
timized from the downstream QA system.
Acknowledgement
We thank Vulcan Inc. for funding this work. We
also thank Paul Ogilvie, James Mayfield, Paul Mc-
Namee, Jason Eisner and the three anonymous re-
viewers for insightful comments.
163
References
Arvind Agarwal, Hema Raghavan, Karthik Subbian,
Prem Melville, Richard D. Lawrence, David C.
Gondek, and James Fan. 2012. Learning to rank
for robust question answering. In Proceedings of
the 21st ACM international conference on Informa-
tion and knowledge management, CIKM ?12, pages
833?842, New York, NY, USA. ACM.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555?596.
M.W. Bilotti and E. Nyberg. 2008. Improving text
retrieval precision and answer accuracy in question
answering systems. In Coling 2008: Proceedings
of the 2nd workshop on Information Retrieval for
Question Answering, pages 1?8.
M.W. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg.
2007. Structured retrieval for question answer-
ing. In Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 351?358. ACM.
M.W. Bilotti, J. Elsas, J. Carbonell, and E. Nyberg.
2010. Rank learning for factoid question answer-
ing with linguistic and semantic constraints. In Pro-
ceedings of the 19th ACM international conference
on Information and knowledge management, pages
459?468. ACM.
Steven Bird and Edward Loper. 2004. Nltk: The nat-
ural language toolkit. In The Companion Volume to
the Proceedings of 42st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 214?
217, Barcelona, Spain, July.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. 2005. Question answering passage
retrieval using dependency relations. In Proceed-
ings of the 28th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?05, pages 400?407, New
York, NY, USA. ACM.
Mark A. Greenwood, editor. 2008. Coling 2008: Pro-
ceedings of the 2nd workshop on Information Re-
trieval for Question Answering. Coling 2008 Orga-
nizing Committee, Manchester, UK, August.
Michael Kaisser. 2012. Answer Sentence Retrieval by
Matching Dependency Paths acquired from Ques-
tion/Answer Sentence Pairs. In EACL, pages 88?98.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In In Proc. the 41st An-
nual Meeting of the Association for Computational
Linguistics.
Klaus H. Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage Publications,
Inc, 2nd edition.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
J. Lin and B. Katz. 2006. Building a reusable test
collection for question answering. Journal of the
American Society for Information Science and Tech-
nology, 57(7):851?861.
D. Lin and P. Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language
Engineering, 7(4):343?360.
Jimmy Lin. 2007. An exploration of the principles un-
derlying redundancy-based factoid question answer-
ing. ACM Trans. Inf. Syst., 25(2), April.
P. Ogilvie. 2010. Retrieval using Document Struc-
ture and Annotations. Ph.D. thesis, Carnegie Mellon
University.
Christopher Pinchak, Davood Rafiei, and Dekang Lin.
2009. Answer typing for information retrieval. In
Proceedings of the 18th ACM conference on In-
formation and knowledge management, CIKM ?09,
pages 1955?1958, New York, NY, USA. ACM.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive an-
notation. In Proceedings of the 23rd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?00,
pages 184?191, New York, NY, USA. ACM.
J. Prager, J. Chu-Carroll, E. Brown, and K. Czuba.
2006. Question answering by predictive annota-
tion. Advances in Open Domain Question Answer-
ing, pages 307?347.
L. Ratinov and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, 6.
Tetsuya Sakai, Hideki Shima, Noriko Kando, Rui-
hua Song, Chuan-Jie Lin, Teruko Mitamura, Miho
Sugimito, and Cheng-Wei Lee. 2010. Overview
of the ntcir-7 aclia ir4qa task. In Proceedings of
NTCIR-8 Workshop Meeting, Tokyo, Japan.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL, pages 12?21.
M.D. Smucker, J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for in-
formation retrieval evaluation. In Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 623?
632. ACM.
164
T. Strohman, D. Metzler, H. Turtle, and W.B. Croft.
2005. Indri: A language model-based search engine
for complex queries. In Proceedings of the Interna-
tional Conference on Intelligent Analysis, volume 2,
pages 2?6. Citeseer.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer Extraction as
Sequence Tagging with Tree Edit Distance. In Pro-
ceedings of NAACL 2013.
Xian Zhang, Yu Hao, Xiaoyan Zhu, Ming Li, and
David R. Cheriton. 2007. Information distance
from a question to an answer. In Proceedings of
the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ?07,
pages 874?883, New York, NY, USA. ACM.
L. Zhao and J. Callan. 2008. A generative retrieval
model for structured documents. In Proceedings of
the 17th ACM conference on Information and knowl-
edge management, pages 1163?1172. ACM.
165
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 702?707,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Lightweight and High Performance Monolingual Word Aligner
Xuchen Yao and Benjamin Van Durme
Johns Hopkins University
Baltimore, MD, USA
Chris Callison-Burch?
University of Pennsylvania
Philadelphia, PA, USA
Peter Clark
Vulcan Inc.
Seattle, WA, USA
Abstract
Fast alignment is essential for many nat-
ural language tasks. But in the setting of
monolingual alignment, previous work has
not been able to align more than one sen-
tence pair per second. We describe a dis-
criminatively trained monolingual word
aligner that uses a Conditional Random
Field to globally decode the best align-
ment with features drawn from source and
target sentences. Using just part-of-speech
tags and WordNet as external resources,
our aligner gives state-of-the-art result,
while being an order-of-magnitude faster
than the previous best performing system.
1 Introduction
In statistical machine translation, alignment is typ-
ically done as a one-off task during training. How-
ever for monolingual tasks, like recognizing tex-
tual entailment or question answering, alignment
happens repeatedly: once or multiple times per
test item. Therefore, the efficiency of the aligner is
of utmost importance for monolingual alignment
tasks. Monolingual word alignment also has a va-
riety of distinctions than the bilingual case, for ex-
ample: there is often less training data but more
lexical resources available; semantic relatedness
may be cued by distributional word similarities;
and, both the source and target sentences share the
same grammar.
These distinctions suggest a model design that
utilizes arbitrary features (to make use of word
similarity measure and lexical resources) and ex-
ploits deeper sentence structures (especially in the
case of major languages where robust parsers are
available). In this setting the balance between
precision and speed becomes an issue: while we
might leverage an extensive NLP pipeline for a
?Performed while faculty at Johns Hopkins University.
language like English, such pipelines can be com-
putationally expensive. One earlier attempt, the
MANLI system (MacCartney et al, 2008), used
roughly 5GB of lexical resources and took 2 sec-
onds per alignment, making it hard to be deployed
and run in large scale. On the other extreme, a sim-
ple non-probabilistic Tree Edit Distance (TED)
model (c.f. ?4.2) is able to align 10, 000 pairs
per second when the sentences are pre-parsed, but
with significantly reduced performance. Trying to
embrace the merits of both worlds, we introduce a
discriminative aligner that is able to align tens to
hundreds of sentence pairs per second, and needs
access only to a POS tagger and WordNet.
This aligner gives state-of-the-art performance
on the MSR RTE2 alignment dataset (Brockett,
2007), is faster than previous work, and we re-
lease it publicly as the first open-source monolin-
gual word aligner: Jacana.Align.1
2 Related Work
The MANLI aligner (MacCartney et al, 2008)
was first proposed to align premise and hypothe-
sis sentences for the task of natural language in-
ference. It applies perceptron learning and han-
dles phrase-based alignment of arbitrary phrase
lengths. Thadani and McKeown (2011) opti-
mized this model by decoding via Integer Linear
Programming (ILP). Benefiting from modern ILP
solvers, this led to an order-of-magnitude speedup.
With extra syntactic constraints added, the exact
alignment match rate for whole sentence pairs was
also significantly improved.
Besides the above supervised methods, indirect
supervision has also been explored. Among them,
Wang and Manning (2010) extended the work of
McCallum et al (2005) and modeled alignment
as latent variables. Heilman and Smith (2010)
used tree kernels to search for the alignment that
1http://code.google.com/p/jacana/
702
yields the lowest tree edit distance. Other tree
or graph matching work for alignment includes
that of (Punyakanok et al, 2004; Kouylekov and
Magnini, 2005; Chambers et al, 2007; Mehdad,
2009; Roth and Frank, 2012).
Finally, feature and model design in monolin-
gual alignment is often inspired by bilingual work,
including distortion modeling, phrasal alignment,
syntactic constraints, etc (Och and Ney, 2003;
DeNero and Klein, 2007; Bansal et al, 2011).
3 The Alignment Model
3.1 Model Design
Our work is heavily influenced by the bilingual
alignment literature, especially the discriminative
model proposed by Blunsom and Cohn (2006).
Given a source sentence s of length M , and a tar-
get sentence t of length N , the alignment from s
to t is a sequence of target word indices a, where
am?[1,M ] ? [0, N ]. We specify that when am = 0,
source word st is aligned to a NULL state, i.e.,
deleted. This models a many-to-one alignment
from source to target. Multiple source words can
be aligned to the same target word, but not vice
versa. One-to-many alignment can be obtained
by running the aligner in the other direction. The
probability of alignment sequence a conditioned
on both s and t is then:
p(a | s, t) =
exp(
?
m,k ?kfk(am?1, am, s, t))
Z(s, t)
This assumes a first-order Conditional Random
Field (Lafferty et al, 2001). The word alignment
task is evaluated over F1. Instead of directly op-
timizing F1, we employ softmax-margin training
(Gimpel and Smith, 2010) and add a cost function
to the normalizing function Z(s, t) in the denom-
inator, which becomes:
?
a?
exp(
?
m,k
?kfk(a?m?1, a?m, s, t) + cost(at, a?))
where at is the true alignments. cost(at, a?)
can be viewed as special ?features? with uniform
weights that encourage consistent with true align-
ments. It is only computed during training in the
denominator because cost(at,at) = 0 in the nu-
merator. Hamming cost is used in practice.
One distinction of this alignment model com-
pared to other commonly defined CRFs is that
the input is two dimensional: at each position m,
the model inspects both the entire sequence of
source words (as the observation) and target words
(whose offset indices are states). The other dis-
tinction is that the size of its state space is not
fixed (e.g., unlike POS tagging, where states are
for instance 45 Penn Treebank tags), but depends
on N , the length of target sentence. Thus we can
not ?memorize? what features are mostly associ-
ated with what states. For instance, in the task of
tagging mail addresses, a feature of ?5 consecu-
tive digits? is highly indicative of a POSTCODE.
However, in the alignment model, it does not make
sense to design features based on a hard-coded
state, say, a feature of ?source word lemma match-
ing target word lemma? fires for state index 6.
To avoid this data sparsity problem, all features
are defined implicitly with respect to the state. For
instance:
fk(am?1, am, s, t) =
{
1 lemmas match: sm, tam
0 otherwise
Thus this feature fires for, e.g.:
(s3 = sport, t5 = sports, a3 = 5), and:
(s2 = like, t10 = liked, a2 = 10).
3.2 Feature Design
String Similarity Features include the following
similarity measures: Jaro Winkler, Dice Sorensen,
Hamming, Jaccard, Levenshtein, NGram overlap-
ping and common prefix matching.2 Also, two
binary features are added for identical match and
identical match ignoring case.
POS Tags Features are binary indicators of
whether the POS tags of two words match. Also,
a ?possrc2postgt? feature fires for each word pair,
with respect to their POS tags. This would capture,
e.g., ?vbz2nn?, when a verb such as arrests aligns
with a noun such as custody.
Positional Feature is a real-valued feature for the
positional difference of the source and target word
(abs(mM ? amN )).WordNet Features indicate whether two words
are of the following relations of each other: hyper-
nym, hyponym, synonym, derived form, entailing,
causing, members of, have member, substances of,
have substances, parts of, have part; or whether
2Of these features the trained aligner preferred Dice
Sorensen and NGram overlapping.
703
their lemmas match.3
Distortion Features measure how far apart the
aligned target words of two consecutive source
words are: abs(am + 1 ? am?1). This learns a
general pattern of whether these two target words
aligned with two consecutive source words are
usually far away from each other, or very close.
We also added special features for corner cases
where the current word starts or ends the source
sentence, or both the previous and current words
are deleted (a transition from NULL to NULL).
Contextual Features indicate whether the left or
the right neighbor of the source word and aligned
target word are identical or similar. This helps
especially when aligning functional words, which
usually have multiple candidate target functional
words to align to and string similarity features can-
not help. We also added features for neighboring
POS tags matching.
3.3 Symmetrization
To expand from many-to-one alignment to many-
to-many, we ran the model in both directions and
applied the following symmetrization heuristics
(Koehn, 2010): INTERSECTION, UNION, GROW-
DIAG-FINAL.
4 Experiments
4.1 Setup
Since no generic off-the-shelf CRF software is de-
signed to handle the special case of dynamic state
indices and feature functions (Blunsom and Cohn,
2006), we implemented this aligner model in the
Scala programming language, which is fully in-
teroperable with Java. We used the L2 regular-
izer and LBFGS for optimization. OpenNLP4 pro-
vided the POS tagger and JWNL5 interfaced with
WordNet (Fellbaum, 1998).
To make results directly comparable, we closely
followed the setup of MacCartney et al (2008) and
Thadani and McKeown (2011). Training and test
data (Brockett, 2007) each contains 800 manually
aligned premise and hypothesis pairs from RTE2.
Note that the premises contain 29 words on av-
erage, and the hypotheses only 11 words. We take
the premise as the source and hypothesis as the tar-
get, and use S2T to indicate the model aligns from
3We found that each word has to be POS tagged to get an
accurate relation, otherwise this feature will not help.
4http://opennlp.apache.org/
5http://jwordnet.sf.net/
source to target and T2S from target to source.
4.2 Simple Baselines
We additionally used two baseline systems for
comparison. One was GIZA++, with the IN-
TERSECTION tricks post-applied, which worked
the best among all other symmetrization heuris-
tics. The other was a Tree Edit Distance (TED)
model, popularly used in a series of NLP appli-
cations (Punyakanok et al, 2004; Kouylekov and
Magnini, 2005; Heilman and Smith, 2010). We
used uniform cost for deletion, insertion and sub-
stitutions, and applied a dynamic program algo-
rithm (Zhang and Shasha, 1989) to decode the
tree edit sequence with the minimal cost, based
on the Stanford dependency tree (De Marneffe
and Manning, 2008). This non-probabilistic ap-
proach turned out to be extremely fast, processing
about 10,000 sentence pairs per second with pre-
parsed trees, performing quantitatively better than
the Stanford RTE aligner (Chambers et al, 2007).
4.3 MANLI Baselines
MANLI was first developed by MacCartney et al
(2008), and then improved by Thadani and McKe-
own (2011) with faster and exact decoding via ILP.
There are four versions to be compared here:
MANLI the original version.
MANLI-approx. re-implemented version by
Thadani and McKeown (2011).
MANLI-exact decoding via ILP solvers.
MANLI-constraint MANLI-exact with hard
syntactic constraints, mainly on common ?light?
words (determiners, prepositions, etc.) attachment
to boost exact match rate.
4.4 Results
Following Thadani and McKeown (2011), perfor-
mance is evaluated by macro-averaged precision,
recall, F1 of aligned token pairs, and exact (per-
fect) match rate for a whole pair, shown in Ta-
ble 1. As our baselines, GIZA++ (with align-
ment intersection of two directions) and TED are
on par with previously reported results using the
Stanford RTE aligner. The MANLI-family of sys-
tems provide stronger baselines, notably MANLI-
constraint, which has the best F1 and exact match
rate among themselves.
We ran our aligner in two directions: S2T and
T2S, then merged the results with INTERSECTION,
UNION and GROW-DIAG-FINAL. Our system beats
704
System P % R % F1 % E %
GIZA++, ? 82.5 74.4 78.3 14.0
TED 80.6 79.0 79.8 13.5
Stanford RTE? 82.7 75.8 79.1 -
MANLI? 85.4 85.3 85.3 21.3
MANLI-approx./ 87.2 86.3 86.7 24.5
MANLI-exact/ 87.2 86.1 86.8 24.8
MANLI-constraint/ 89.5 86.2 87.8 33.0
this work, S2T 91.8 83.4 87.4 25.9
this work, T2S 93.7 84.0 88.6 35.3
S2T ? T2S 95.4 80.8 87.5 31.3
S2T ? T2S 90.3 86.6 88.4 29.6
GROW-DIAG-FINAL 94.4 81.8 87.6 30.8
Table 1: Results on the 800 pairs of test data. E% stands
for exact (perfect) match rate. Systems marked with ? are
reported by MacCartney et al (2008), with / by Thadani and
McKeown (2011).
the weak and strong baselines6 in all measures ex-
cept recall. Some patterns are very clearly shown:
Higher precision, lower recall is due to the
higher-quality and lower-coverage of WordNet,
where the MANLI-family systems used addi-
tional, automatically derived lexical resources.
Imbalance of exact match rate between S2T and
T2S with a difference of 9.4% is due to the many-
to-one nature of the aligner. When aligning from
source (longer) to target (shorter), multiple source
words can align to the same target word. This
is not desirable since multiple duplicate ?light?
words are aligned to the same ?light? word in the
target, which breaks perfect match. When align-
ing T2S, this problem goes away: the shorter tar-
get sentence contains less duplicate words, and in
most cases there is an one-to-one mapping.
MT heuristics help, with INTERSECTION and
UNION respectively improving precision and re-
call.
4.5 Runtime Test
Table 2 shows the runtime comparison. Since the
RTE2 corpus is imbalanced, with premise length
(words) of 29 and hypothesis length of 11, we
also compare on the corpus of FUSION (McKeown
et al, 2010), with both sentences in a pair aver-
aging 27. MANLI-approx. is the slowest, with
quadratic growth in the number of edits with sen-
tence length. MANLI-exact is in second place, re-
lying on the ILP solver. This work has a precise
O(MN2) decoding time, with M the source sen-
tence length and N the target sentence length.
6Unfortunately both MacCartney and Thadani no longer
have their original output files (personal communication), so
we cannot run a significance test against their result.
corpus sent. pair
length
MANLI-
approx.
MANLI-
exact
this
work
RTE2 29/11 1.67 0.08 0.025
FUSION 27/27 61.96 2.45 0.096
Table 2: Alignment runtime in seconds per sentence pair on
two corpora: RTE2 (Cohn et al, 2008) and FUSION (McKe-
own et al, 2010). The MANLI-* results are from Thadani
and McKeown (2011), on a Xeon 2.0GHz with 6MB Cache.
The runtime for this work takes the longest timing from S2T
and T2S, on a Xeon 2.2GHz with 4MB cache (the closest
we can find to match their hardware). Horizontally in a real-
world application where sentences have similar length, this
work is roughly 20x faster (0.096 vs. 2.45). Vertically, the
decoding time for our work increases less dramatically when
sentence length increases (0.025?0.096 vs. 0.08?2.45).
features P % R % F1 % E %
full (T2S) 93.7 84.0 88.6 35.3
- POS 93.2 83.5 88.1 31.4
- WordNet 93.2 83.7 88.2 33.5
- both 93.1 83.2 87.8 30.1
Table 3: Performance without POS and/or Word-
Net features.
While MANLI-exact is about twenty-fold faster
than MANLI-approx., our aligner is at least an-
other twenty-fold faster than MANLI-exact when
the sentences are longer and balanced. We also
benefit from shallower pre-processing (no parsing)
and can store all resources in main memory.7
4.6 Ablation Test
Since WordNet and the POS tagger is the only used
external resource, we removed them8 from the fea-
ture sets and reported performance in Table 3. This
somehow reflects how the model would perform
for a language without a suitable POS tagger, or
more commonly, WordNet in that language. At
this time, the model falls back to relying on string
similarities, distortion, positional and contextual
features, which are almost language-independent.
A loss of less than 1% in F1 suggests that the
aligner can still run reasonably well without a POS
tagger and WordNet.
7WordNet (?30MB) is a smaller footprint than the 5GB of
external resources used by MANLI.
8per request of reviewers. Note that WordNet is less pre-
cise without a POS tagger. When we removed the POS tag-
ger, we enumerated all POS tags for a word to find its hyper-
nym/synonym/... synsets.
705
4.7 Error Analysis
There were three primary categories of error:9
1. Token-based paraphrases that are not covered
by WordNet, such as program and software,
business and venture. This calls for broader-
coverage paraphrase resources.
2. Words that are semantically related but not
exactly paraphrases, such as married and
wife, beat and victory. This calls for re-
sources of close distributional similarity.
3. Phrases of the above kinds, such as elected
and won a seat, politician and presidential
candidate. This calls for further work on
phrase-based alignment.10
There is a trade-off using WordNet vs. larger,
noisier resources in exchange of higher preci-
sion vs. recall and memory/disk allocation. We
think this is an application-specific decision; other
resources could be easily incorporated into our
model, which we may explore in the future to ex-
plore the trade-off in addressing items 1 and 2.
5 Conclusion
We presented a model for monolingual sentence
alignment that gives state-of-the-art performance,
and is significantly faster than prior work. We re-
lease our implementation as the first open-source
monolingual aligner, which we hope to be of ben-
efit to other researchers in the rapidly expanding
area of natural language inference.
Acknowledgement
We thank Vulcan Inc. for funding this work. We
also thank Jason Smith, Travis Wolfe, Frank Fer-
raro for various discussion, suggestion, comments
and the three anonymous reviewers.
References
Mohit Bansal, Chris Quirk, and Robert Moore. 2011.
Gappy phrasal alignment by agreement. In Proceed-
ings of ACL, Portland, Oregon, June.
9We submitted a browser in JavaScript
(AlignmentBrowser.html) in the supporting material
that compares the gold alignment and test output; readers are
encouraged to try it out.
10Note that MacCartney et al (2008) showed that in the
MANLI system setting phrase size to larger than one there
was only a 0.2% gain in F1, while the complexity became
much larger.
P. Blunsom and T. Cohn. 2006. Discriminative word
alignment with conditional random fields. In Pro-
ceedings of ACL2006, pages 65?72.
Chris Brockett. 2007. Aligning the RTE 2006 corpus.
Technical report, Microsoft Research.
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M.C. de Marneffe, D. Ramage,
E. Yeh, and C.D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing, pages 165?170.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL2007.
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin crfs: training log-linear models with cost
functions. In NAACL 2010, pages 733?736.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
NAACL 2010, pages 1011?1019, Los Angeles, Cali-
fornia, June.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In PASCAL Challenges on RTE, pages
17?20.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA.
B. MacCartney, M. Galley, and C.D. Manning. 2008.
A phrase-based alignment model for natural lan-
guage inference. In Proceedings of EMNLP2008,
pages 802?811.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A Conditional Random Field
for Discriminatively-trained Finite-state String Edit
Distance. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence (UAI 2005),
July.
706
Kathleen McKeown, Sara Rosenthal, Kapil Thadani,
and Coleman Moore. 2010. Time-efficient creation
of an accurate sentence fusion corpus. In ACL2010
short, pages 317?320.
Y. Mehdad. 2009. Automatic cost estimation for tree
edit distance using particle swarm optimization. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 289?292.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Vasin Punyakanok, Dan Roth, and Wen T. Yih. 2004.
Mapping Dependencies Trees: An Application to
Question Answerin. In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort Lauderdale, Florida.
Michael Roth and Anette Frank. 2012. Aligning pred-
icates across monolingual comparable texts using
graph-based clustering. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 171?182, Jeju Island,
Korea, July.
Kapil Thadani and Kathleen McKeown. 2011. Opti-
mal and syntactically-informed decoding for mono-
lingual phrase-based alignment. In Proceedings of
ACL short.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question an-
swering. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
?10, pages 1164?1172, Stroudsburg, PA, USA.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
707
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 186?196,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Inferring User Political Preferences from Streaming Communications
Svitlana Volkova,
1
Glen Coppersmith
2
and Benjamin Van Durme
1,2
1
Center for Language and Speech Processing,
2
Human Language Technology Center of Excellence,
Johns Hopkins University, Baltimore, MD 21218
svitlana@jhu.edu, coppersmith@jhu.edu, vandurme@cs.jhu.edu
Abstract
Existing models for social media per-
sonal analytics assume access to thou-
sands of messages per user, even though
most users author content only sporadi-
cally over time. Given this sparsity, we:
(i) leverage content from the local neigh-
borhood of a user; (ii) evaluate batch mod-
els as a function of size and the amount
of messages in various types of neighbor-
hoods; and (iii) estimate the amount of
time and tweets required for a dynamic
model to predict user preferences. We
show that even when limited or no self-
authored data is available, language from
friend, retweet and user mention commu-
nications provide sufficient evidence for
prediction. When updating models over
time based on Twitter, we find that polit-
ical preference can be often be predicted
using roughly 100 tweets, depending on
the context of user selection, where this
could mean hours, or weeks, based on the
author?s tweeting frequency.
1 Introduction
Inferring latent user attributes such as gender, age,
and political preferences (Rao et al, 2011; Za-
mal et al, 2012; Cohen and Ruths, 2013) auto-
matically from personal communications and so-
cial media including emails, blog posts or public
discussions has become increasingly popular with
the web getting more social and volume of data
available. Resources like Twitter
1
or Facebook
2
become extremely valuable for studying the un-
derlying properties of such informal communica-
tions because of its volume, dynamic nature, and
diverse population (Lunden, 2012; Smith, 2013).
1
http://www.demographicspro.com/
2
http://www.wolframalpha.com/facebook/
The existing batch models for predicting latent
user attributes rely on thousands of tweets per
author (Rao et al, 2010; Conover et al, 2011;
Pennacchiotti and Popescu, 2011a; Burger et al,
2011; Zamal et al, 2012; Nguyen et al, 2013).
However, most Twitter users are less prolific than
those examined in these works, and thus do not
produce the thousands of tweets required to obtain
their levels of accuracy e.g., the median number of
tweets produced by a random Twitter user per day
is 10. Moreover, recent changes to Twitter API
querying rates further restrict the speed of access
to this resource, effectively reducing the amount of
data that can be collected in a given time period.
In this paper we analyze and go beyond static
models formulating personal analytics in social
media as a streaming task. We first evaluate batch
models that are cognizant of low-resource predic-
tion setting described above, maximizing the effi-
ciency of content in calculating personal analytics.
To the best of our knowledge, this is the first work
that makes explicit the tradeoff between accuracy
and cost (manifest as calls to the Twitter API),
and optimizes to a different tradeoff than state-of-
the-art approaches, seeking maximal performance
when limited data is available. In addition, we
propose streaming models for personal analytics
that dynamically update user labels based on their
stream of communications which has been ad-
dressed previously by Van Durme (2012b). Such
models better capture the real-time nature of evi-
dence being used in latent author attribute predic-
tions tasks. Our main contributions include:
- develop low-resource and real-time dynamic
approaches for personal analytics using as an
example the prediction of political preference
of Twitter users;
- examine the relative utility of six different
notions of ?similarity? between users in an
implicit Twitter social network for personal
analytics;
186
- experiments are performed across multiple
datasets supporting the prediction of politi-
cal preference in Twitter, to highlight the sig-
nificant differences in performance that arise
from the underlying collection and annota-
tion strategies.
2 Identifying Twitter Social Graph
Twitter users interact with one another and en-
gage in direct communication in different ways
e.g., using retweets, user mentions e.g., @youtube
or hashtags e.g., #tcot, in addition to having ex-
plicit connections among themselves such as fol-
lowing, friending. To investigate all types of social
relationships between Twitter users and construct
Twitter social graphs we collect lists of followers
and friends, and extract user mentions, hashtags,
replies and retweets from communications.
3
2.1 Social Graph Definition
Lets define an attributed, undirected graph G =
(V,E), where V is a set of vertices and E is a set
of edges. Each vertex v
i
represents someone in
a communication graph i.e., communicant: here
a Twitter user. Each vertex is attributed with a
feature vector
~
f(v
i
) which encodes communica-
tions e.g., tweets available for a given user. Each
vertex is associated with a latent attribute a(v
i
),
in our case it is binary a(v
i
) ? {D,R}, where
D stands for Democratic and R for Republican
users. Each edge e
ij
? E represents a connec-
tion between v
i
and v
j
, e
ij
= (v
i
, v
j
) and defines
different social circles between Twitter users e.g.,
follower (f), friend (b), user mention (m), hash-
tag (h), reply (y) and retweet (w). Thus, E ?
V
(2)
?{f, b, h,m,w, y}. We denote a set of edges
of a given type as ?
r
(E) for r ? {f, b, h,m,w, y}.
We denote a set of vertices adjacent to v
i
by so-
cial circle type r as N
r
(v
i
) which is equivalent to
{v
j
| e
ij
? ?
r
(E)}. Following Filippova (2012)
we refer to N
r
(v
i
) as v
i
?s social circle, otherwise
known as a neighborhood. In most cases, we only
work with a sample of a social circle, denoted by
N
?
r
(v
i
) where |N
?
r
(v
i
)| = k is its size for v
i
.
Figure 1 presents an example of a social graph
derived from Twitter. Notably, users from differ-
ent social circles can be shared across the users of
the same or different classes e.g., a user v
j
can be
3
The code and detailed explanation on how we col-
lected all six types of user neighbors and their com-
munications using Twitter API can be found here:
http://www.cs.jhu.edu/ svitlana/
Figure 1: An example of a social graph with follower, friend,
@mention, reply, retweet and hashtag social circles for each
user of interest e.g., blue: Democratic, red: Republican.
in both follower circle v
j
? N
f
(v
i
), v
i
? D and
retweet circle v
j
? N
w
(v
k
), v
k
? R.
2.2 Candidate-Centric Graph
We construct candidate-centric graph G
cand
by
looking into following relationships between the
users and Democratic or Republican candidates
during the 2012 US Presidential election. In the
Fall of 2012, leading up to the elections, we ran-
domly sampled n = 516 Democratic and m =
515 Republican users. We labeled users as Demo-
cratic if they exclusively follow both Democratic
candidates
4
? BarackObama and JoeBiden but
do not follow both Republican candidates ? Mit-
tRomney and RepPaulRyan and vice versa. We
collectively refer to D and R as our ?users of in-
terest? for which we aim to predict political prefer-
ence. For each such user we collect recent tweets
and randomly sample their immediate k = 10
neighbors from follower, friend, user mention, re-
ply, retweet and hashtag social circles.
2.3 Geo-Centric Graph
We construct a geo-centric graph G
geo
by col-
lecting n = 135 Democratic and m = 135 Re-
publican users from the Maryland, Virginia and
Delaware region of the US with self-reported po-
litical preference in their biographies. Similar to
the candidate-centric graph, for each user we col-
lect recent tweets and randomly sample user social
circles in the Fall of 2012. We collect this data to
get a sample of politically less active users com-
pared to the users from candidate-centric graph.
2.4 ZLR Graph
We also consider a G
ZLR
graph constructed from
a dataset previously used for political affiliation
4
As of Oct 12, 2012, the number of followers for Obama,
Biden, Romney and Ryan were 2m, 168k, 1.3m and 267k.
187
classification (Zamal et al, 2012). This dataset
consists of 200 Republican and 200 Democratic
users associated with 925 tweets on average per
user.
5
Each user has on average 6155 friends with
642 tweets per friend. Sharing restrictions and rate
limits on Twitter data collection only allowed us to
recreate a semblance of ZLR data
6
? 193 Demo-
cratic and 178 Republican users with 1K tweets
per user, and 20 neighbors of four types including
follower, friends, user mention and retweet with
200 tweets per neighbor for each user of interest.
3 Batch Models
Baseline User Model As input we are given a set
of vertices representing users of interest v
i
? V
along with feature vectors
~
f(v
i
) derived from con-
tent authored by the user of interest. Each user
is associated with a non-zero number of publicly
posted tweets. Our goal is assign to a category
each user of interest v
i
based on
~
f(v
i
). Here we
focus on a binary assignment into the categories
Democratic D or Republican R. The log-linear
model
7
for such binary classification is:
?
v
i
=
{
D (1 + exp[?
~
? ?
~
f(v
i
)])
?1
? 0.5,
R otherwise.
(1)
where features are normalized word ngram counts
extracted from v
i
?s tweets
~
f
t
(v
i
) : D?t(v
i
)? R.
The proposed baseline model follows the same
trends as the existing state-of-the-art approaches
for user attribute classification in social media as
described in Section 8. Next we propose to ex-
tend the baseline model by taking advantage of
language in user social circles as describe below.
Neighbor Model As input we are given user-local
neighborhood N
r
(v
i
), where r is a neighborhood
type. Besides the neighborhood?s type r, each is
characterized by:
? the number of communications per neighbor
~
f
t
(N
r
), t = {5, 10, 15, 25, 50, 100, 200};
5
The original dataset was collected in 2012 and has
been recently released at http://icwsm.cs.mcgill.ca/. Politi-
cal labels are extracted from http://www.wefollow.com as de-
scribed by Pennacchiotti and Popescu (2011b).
6
This inability to perfectly replicate prior work based on
Twitter is a recognized problem throughout the community of
computational social science, arising from the data policies of
Twitter itself, it is not specific to this work.
7
We use log-linear models over reasonable alternatives
such as perceptron or SVM, following the practice of a wide
range of previous work in related areas (Smith, 2004; Liu et
al., 2005; Poon et al, 2009) including text classification in so-
cial media (Van Durme, 2012b; Yang and Eisenstein, 2013).
? the order of the social circle ? the num-
ber of neighbors per user of interest |N
r
| =
deg(v
i
), n = {1, 2, 5, 10}.
Our goal is to classify users of interest using
evidence (e.g., communications) from their local
neighborhood
?
n
~
f
t
[N
r
(v
i
)] ?
~
f(N
r
) as Demo-
cratic or Republican. The corresponding log-
linear model is defined as:
?
N
r
=
{
D (1 + exp[?
~
? ?
~
f(N
r
)])
?1
? 0.5,
R otherwise.
(2)
To check whether our static models are cog-
nizant of low-resource prediction settings we com-
pare the performance of the user model from Eq.1
and the neighborhood model from Eq.2. Follow-
ing the streaming nature of social media, we see
the scarce available resource as the number of re-
quests allowed per day to the Twitter API. Here
we abstract this to a model assumption where we
receive one tweet t
k
at a time and aim to maximize
classification performance with as few tweets per
user as possible:
8
? for the baseline user model:
minimize
k
?
k
t
k
(v
i
),
(3)
? for the neighborhood model:
minimize
k
?
n
?
k
t
k
[N
r
(v
i
)].
(4)
4 Streaming Models
We rely on straightforward Bayesian rule update
to our batch models in order to simulate a real-
time streaming prediction scenario as a first step
beyond the existing models as shown in Figure 2.
The model makes predictions of a latent user at-
tribute e.g., Republican under a model assumption
of sequentially arriving, independent and identi-
cally distributed observations T = (t
1
, . . . , t
k
)
9
.
The model dynamically updates posterior proba-
bility estimates p(a(v
i
) = R|t
k
) for a given user
8
The separate issue is that many authors simply don?t
tweet very often. For instance, 85.3% of all Twitter
users post less than one update per day as reported at
http://www.sysomos.com/insidetwitter/. Thus, their commu-
nications are scare even if we could get al of them without
rate limiting from Twitter API.
9
Given the dynamic character of online discourse it will
clearly be of interest in the future to consider models that go
beyond the iid assumption.
188
p(R|t
1
)
0.6
v
i
v
i
v
i
p(R|t
1
, t
2
)
0.7
p(R|t
1
, . . . t
k
)
0.9
N
r
N
r
N
r
Time, ??2?1 ?k
Figure 2: Stream-based classification of an attribute a(v
i
) ?
{R,D} given a stream of communications t
1
, t
2
, . . . , t
k
au-
thored by a user v
i
or user immediate neighbors from N
r
social circles at time ?
1
, ?
2
, . . . , ?
k
.
v
i
as an additional evidence t
k
is acquired, as de-
fined in a general form below for any latent at-
tribute a(v
i
) ? A given the tweets T of user v
i
:
p(a(v
i
) = x ? A | T ) =
p(T | a(v
i
) = x) ? p(a(v
i
) = x)
?
y?A
p(T | a(v
i
) = y) ? p(a(v
i
) = y)
=
?
k
p(t
k
| a(v
i
) = x) ? p(a(v
i
) = x)
?
y?A
?
k
p(t
k
| a(v
i
) = y) ? p(a(v
i
) = y),
(5)
where y is the number of all possible attribute val-
ues, and k is the number of tweets per user.
For example, to predict user political prefer-
ence, we start with a prior P (R) = 0.5, and se-
quentially update the posterior p(R | T ) by accu-
mulating evidence from the likelihood p(t
k
|R):
p(R | T ) =
?
k
p(t
k
|R) ? p(R)
?
k
P (t
k
|R) ? p(R) +
?
k
P (t
k
|D) ? p(D).
(6)
Our goal is to maximize posterior probability
estimates given a stream of communications for
each user in the data over (a) time ? and (b) the
number of tweets T . For that, for each user we
take tweets that arrive continuously over time and
apply two different streaming models:
? User Model with Dynamic Updates: re-
lies exclusively on user tweets t
(v
i
)
1
, . . . , t
(v
i
)
k
following the order they arrive over time ? ,
where for each user v
i
we dynamically up-
date the posterior p(R | t
(v
i
)
1
, . . . , t
(v
i
)
k
).
? User-Neighbor Model with Dynamic Up-
dates: relies on both neighbor N
r
commu-
nications including friend, follower, retweet,
user mention and user tweets t
(v
i
)
1
, . . . , t
(N
r
)
k
following the order they arrive over time ? ;
here we dynamically update the posterior
probability p(R | t
(v
i
)
1
, . . . , t
(N
r
)
k
).
5 Experimental Setup
We design a set of experiments to analyze static
and dynamic models for political affiliation classi-
fication defined in Sections 3 and 4.
5.1 Batch Classification Experiments
We first answer whether communications from
user-local neighborhoods can help predict politi-
cal preference for the user. To explore the con-
tribution of different neighborhood types we learn
static user and neighbor models on G
cand
, G
geo
and G
ZLR
graphs. We also examine the ability of
our static models to predict user political prefer-
ences in low-resource setting e.g., 5 tweets.
The existing models follow a standard setup
when either user or neighbor tweets are available
during train and test. For a static neighbor model
we go beyond that, and train our the model on all
data available per user, but only apply part of the
data at the test time, pushing the boundaries of
how little is truly required for classification. For
example, we only use follower tweets for G
test
,
but we use tweets from all types of neighbors for
G
train
. Such setup will simulate different real-
world prediction scenarios which have not been
previously explored, to our knowledge e.g., when
a user has a private profile or has not tweeted yet,
and only user neighbor tweets are available.
We experiment with our static neighbor model
defined in Eq.2 with the aim to:
1. evaluate neighborhood size influence, we
change the number of neighbors and try n =
[1, 2, 5, 10] neighbor(s) per user;
2. estimate neighbor content influence, we alter-
nate the amount of content per neighbor and
try t = [5, 10, 15, 25, 50, 100, 200] tweets.
We perform 10-fold cross validation
10
and run
100 random restarts for every n and t parame-
ter combination. We compare our static neigh-
bor and user models using the cost functions
from Eq.3 and Eq.4. For all experiments we use
LibLinear (Fan et al, 2008), integrated in the
Jerboa toolkit (Van Durme, 2012a). Both mod-
els defined in Eq.1 and Eq.2 are learned using
normalized count-based word ngram features ex-
tracted from either user or neighbor tweets.
11
10
For each fold we split the data into 3 parts: 70% train,
10% development and 20% test.
11
For brevity we omit reporting results for bigram and tri-
gram features, since unigrams showed superior performance.
189
5.2 Streaming Classification Experiments
We evaluate our models with dynamic Bayesian
updates on a continuous stream of communica-
tions over time as shown in Figure 2. Unlike static
model experiments, we are not modeling the in-
fluence of the number of neighbors or the amount
of content per neighbor. Here, we order user and
neighbor communication streams by real world
time of posting and measure changes in posterior
probabilities over time. The main purpose of these
experiments is to quantitatively evaluate (1) the
number of tweets and (2) the amount of real world
time it takes to observe enough evidence on Twit-
ter to make reliable predictions.
We experiment with log-linear models defined
in Eq. 1 and 2 and continuously estimate the poste-
rior probabilities P (R | T ) as defined in Eq.6. We
average the posterior probability results over the
users in G
cand
, G
geo
and G
ZLR
graphs. We train
streaming models on an attribute balanced subset
of tweets for each user v
i
excluding v
i
?s tweets (or
v
i
?s neighbor tweets for a joint model). This setup
is similar to leave-one-out classification. The clas-
sifier is learned using binary word ngram features
extracted from user or user-neighbor communi-
cations. We prefer binary to normalized count-
based features to overcome sparsity issues caused
by making predictions on each tweet individually.
6 Static Classification Results
6.1 Modeling User Content Influence
We investigate classification decision probabilities
for our static user model ?
v
i
by making predic-
tions on a random set of 5 vs. 100 tweets per user.
To our knowledge only limited work on personal
0 20 40 60 80 100
0.0
0.2
0.4
0.6
0.8
1.0
User
Clas
sific
ation
 dec
ision
 (pro
babi
lity)
misclassified
misclassifiedcorrect
correct
Figure 3: Classification probabilities for ?
v
i
estimated over
100 users in G
cand
tested on 5 (blue) vs. 100 (green) tweets
per user where Republican = 1, Democratic = 0, filled mark-
ers = correctly classified, not filled = misclassified users.
l l l l
l l l
5 10 20 50 100 2000
.500
.550
.600
.650
.70
log(Tweets Per Neighbor)
Accurac
y
10 50 100 200 400
FriendFollowerHashtagUsermention
RetweetReplyUser
l l l l l
l l
l l l l
l l l
l l
l l l l
l
l l l l l
l l
l l l
l l
(a) G
geo
: 2 neighbors
l l l l
l l l
5 10 20 50 100 2000
.500
.550
.600
.650
.70
log(Tweets Per Neighbor)
Accurac
y
50 100 250 500 1000 2000
FriendFollowerHashtagUsermention
RetweetReplyUser
l l l l l l ll l l l
l l
l l
l l l l
l
l l l
l l l l
l l l l
l l l
(b) G
geo
: 10 neighbors
l
l l
l l
l l
5 10 20 50 100 2000.
500.5
50.60
0.650
.700.7
5
log(Tweets Per Neighbor)
Accurac
y
10 50 100 200 400
FriendFollowerHashtagUsermention
RetweetReplyUser
l
l
l l
l l l l ll
l l
l l
l l
l l
l l
l l
l l
l l
l l l
(c) G
cand
: 2 neighbors
l
l l
l l l
l
5 10 20 50 100 2000.
500.5
50.60
0.650
.700.7
5
log(Tweets Per Neighbor)
Accurac
y
50 100 250 500 1000 2000
FriendFollowerHashtagUsermention
RetweetReplyUser
l l
l l l
l l l
l l l l
l
l l l
l
l l
l l l
l l
l l
l l l
(d) G
cand
: 10 neighbors
Figure 4: Modeling the influence of the number of tweets per
neighbor t=[5, .., 200] for G
cand
and G
geo
graphs.
analytics (Burger et al, 2011; Van Durme, 2012b)
have performed this straight-forward comparison.
For that purpose, we take a random partition con-
taining 100 users ofG
cand
graph and perform four
independent classification experiments ? two runs
using 5 and two runs using 100 tweets per user.
Figure 3 demonstrates that more tweets during
prediction time lead to higher accuracy by show-
ing that more users with 100 tweets are correctly
classified e.g., filled green markers in the right up-
per quadrant are true Republicans and in the left
lower quadrant are true Democrats. Moreover, a
lot of users with 100 tweets are close to 0.5 deci-
sion probability which suggests that the classifier
is just uncertain rather then being completely off,
e.g., misclassified Republican users with 5 tweets
(not filled blue markers in the right lower quad-
rant) are close to 0. These results follow natu-
rally from the underlying feature representation:
having more tweets per user leads to a lower vari-
ance estimate of a target multinomial distribution.
The more robustly this distribution is estimated
(based on having more tweets) the more confident
we should be in the classifier output.
6.2 Modeling Neighbor Content Influence
Here we discuss the results for our static neighbor-
hood model. We study the influence of the neigh-
borhood type r and size in terms of the number of
neighbors n and tweets t per neighbor.
190
l
l
l
l
1 2 5 100.50
0.550
.600.
650.7
00.75
log(Number of Neighbors)
Accurac
y
5 10 25 50
FriendFollowerHashtag UsermentionRetweetReply
l
l
l
l l
l
l
l
l
l
l
l
l
l l
l
l
(a) G
cand
: 5 tweets
l
l
l
l
1 2 5 100.50
0.550
.600.
650.7
00.75
log(Number of Neighbors)
Accurac
y
200 400 1000 2000
FriendFollowerHashtag UsermentionRetweetReply
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
(b) G
cand
: 200 tweets
l
l
l l
1 2 5 100.50
0.55
0.60
0.65
0.70
log(Number of Neighbors)
Accurac
y
5 10 25 50
FriendFollowerHashtag UsermentionRetweetReply
l l
l l
l l
l l
l
l
l
l
l
l l
l
l
l
(c) G
geo
: 5 tweets
l
l
l l
1 2 5 100.50
0.55
0.60
0.65
0.70
log(Number of Neighbors)
Accurac
y
200 400 1000 2000
FriendFollowerHashtag UsermentionRetweetReply
l l
l l
l l
l
l
l
l
l
l
l
l
l
l
l l
l l
(d) G
geo
: 200 tweets
Figure 5: Modeling the influence of the number of neighbors
per user n=[1, .., 10] for G
cand
and G
geo
graphs.
In Figure 4 we present accuracy results for
G
cand
and G
geo
graphs. Following Eq.3 and 4, we
spent an equal amount of resources to obtain 100
user tweets and 10 tweets from 10 neighbors. We
annotate these ?points of equal number of commu-
nications? with a line on top marked with a corre-
sponding number of user tweets.
We show that three of six social circles ? friend,
retweet and user-mention yield better accuracy
compared to the user model for all graphs when
t ? 250. Thus, for effectively classifying a given
user v
i
it is better to take 200 tweets each from 10
neighbors rather than 2,000 tweets from the user.
The best accuracy for G
cand
is 0.75 for friend,
follower, retweet and user-mention neighborhoods
which is 0.03 higher than the user baseline; for
G
geo
is 0.67 for user-mention and 0.64 for retweet
circles compared to 0.57 for the user model; for
G
ZLR
is 0.863 for retweet and 0.849 for friend
circles which is 0.11 higher that the user baseline.
Finally, similarly to the results for the user model
given in Figure 3, increasing the number of tweets
per neighbor from 5 to 200 leads to a significant
gain in performance for all neighborhood types.
6.3 Modeling Neighborhood Size
In Figure 5 we present accuracy results to show
neighborhood size influence on classification per-
formance for G
geo
and G
cand
graphs. Our re-
sults demonstrate that even small changes to the
neighborhood size n lead to better performance
which does not support the claims by Zamal et al
(2012). We demonstrate that increasing the size
of the neighborhood leads to better performance
across six neighborhood types. Friend, user men-
tion and retweet neighborhoods yield the highest
accuracy for all graphs. We observe that when the
number of neighbors is n = 1, the difference in
accuracy across all neighborhood types is less sig-
nificant but for n ? 2 it becomes more significant.
7 Streaming Classification Results
7.1 Modeling Dynamic Posterior Updates
from a User Stream
Figures 6a and 6b demonstrate dynamic user
model prediction results averaged over users from
G
cand
and G
ZLR
graphs. Each figure outlines
changes in sequential average probability esti-
mates p
?
(R | T ) for each individual self-authored
tweet t
k
as defined in Eq. 6. The average proba-
bility estimates p
?
(R | T ) are reported for every 5
tweets in a stream T = (t
1
, . . . t
k
) as
?
n
P (R|t
k
)
n
,
where n is the total number of users with the same
attribute R or D. We represent p
?
(R | T ) as a
box and whisker plot with the median, lower and
upper quantiles to show the variance; the length of
whiskers indicate lower and upper extreme values.
We find similar behavior across all three graphs.
In particular, the posterior estimates converge
faster when predicting Democratic than Republi-
can users but it has been trained on an equal num-
ber of tweets per class. We observe that average
posterior estimates P
?
(R | T ) converge faster to 0
0.5
0.6
0.7
0.8
0.9
1.0
0 20 40 60
p(R
epu
blic
an|T
)
0.0
0.1
0.2
0.3
0.4
0.5
0 20 40 60
Tweet Stream (T)
p(R
epu
blic
an|T
)
(a) User G
cand
0.5
0.6
0.7
0.8
0.9
1.0
0 50 100 150
p(R
epu
blic
an|T
)
0.0
0.1
0.2
0.3
0.4
0.5
0 50 100 150
Tweet Stream (T)
p(R
epu
blic
an|T
)
(b) User G
ZLR
Figure 6: Streaming classification results from user commu-
nications for G
cand
and G
ZLR
graphs averaged over every 5
tweets (red - Republican, blue - Democratic).
191
300
400
500
0 5 10 15
Time in Weeks
Us
ers
(a) User G
cand
0
100
200
0 20 40 60 80
Time in Weeks
Us
ers
(b) User G
ZLR
300
400
500
0 1 2 3 4 5
Time in Weeks
Us
ers
(c) User-Neigh G
cand
0
100
200
0 10 20 30 40
Time in Weeks
Us
ers
(d) User-Neigh G
ZLR
Figure 7: Time needed for (a) - (b) dynamic user model and
(c) - (d) joint user-neighbor model to infer political prefer-
ences of Democratic (blue) and Republican (red) users at
75% (dotted line) and 95% (solid line) accuracy levels.
(Democratic) than to 1 (Republican) in Figures 6a
and 6b. It suggests that language of Democrats is
more expressive of their political preference than
language of Republicans. For example, frequent
politically influenced terms used widely by Demo-
cratic users include faith4liberty, constitutionally,
pass, vote2012, terroristic.
The variance for average posterior estimates
decreases when the number of tweets increases
for all three datasets. Moreover, we detect that
P
?
(R|T ) estimates for users in G
cand
converge 2-
3 times faster in terms of number of tweets than
for users in G
ZLR
. The lowest convergence is de-
tected for G
geo
where after t
k
= 250 tweets the
average posterior estimate P
?
(R | t
k
) = 0.904 ?
0.044 and P
?
(D | t
k
) = 0.861 ? 0.008. It means
that users inG
cand
are more politically vocal com-
pared to users in G
ZLR
and G
geo
. As a result,
less active users in G
geo
just need more than 250
tweets to converge to a true 0 or 1 class. These re-
sults are coherent with the outcomes for our static
models shown in Figures 4 and 5. These findings
further confirm that differences in performance are
caused by various biases present in the data due to
distinct sampling and annotation approaches.
Figure 7a and 7b illustrate the amount of time
required for the user model to infer political pref-
erences estimated for 1,031 users inG
cand
and 371
users inG
ZLR
. The amount of time needed can be
evaluated for different accuracy levels e.g., 0.75
and 0.95. Thus, with 75% accuracy we classify:
? 100 (?20%) Republican users in 3.6 hours
and Democratic users in 2.2 hours for G
cand
;
? 100 (?56%) R users in 20 weeks and 100
(?52%) D users in 8.9 weeks for G
ZLR
which is 800 times longer that for G
cand
;
? 100 (?75%) R users in 12 weeks and 80
(?60%) D users in 19 weeks for G
geo
.
Such extreme divergences in the amount of time
required for classification across all graphs should
be of strong interest to researchers concerned with
latent attribute prediction tasks because Twitter
users produce messages with extremely different
frequencies. In our case, users in G
ZLR
tweet ap-
proximately 800 times less frequently than users
in G
cand
.
7.2 Modeling Dynamic Posterior Updates
from a Joint User-Neighbor Stream
We estimate dynamic posterior updates from a
joint stream of user and neighbor communications
in G
geo
, G
cand
and G
ZLR
graphs. To make a fair
comparison with a streaming user model, we start
with the same user tweet t
0
(v
i
). Then instead of
waiting for the next user tweet we rely on any
neighbor tweets that appear until the user produces
the next tweet t
1
(v
i
). We rely on communications
from four types of neighbors such as friends, fol-
lowers, retweets and user mentions.
The convergence rate for the average posterior
probability estimates P
?
(R|T ) depending on the
number of tweets is similar to the user model re-
sults presented in Figure 6. However, for G
geo
the variance for P
?
(R|T ) is higher for Democratic
users; for G
ZLR
P
?
(R|T ) ? 1 for Republicans
in less than 110 tweets which is ?t = 40 tweets
faster than the user model; for G
cand
the conver-
gence for both P
?
(R|T ) ? 1 and P
?
(D|T ) ? 0
is not significantly different than the user model.
Figures 7c and 7d show the amount of time re-
quired for a joint user-neighbor model to infer po-
litical preferences estimated for users inG
cand
and
G
ZLR
. We find that with 75% accuracy we can
classify 100 users for:
? G
cand
: Republican users in 23 minutes and
Democratic users in 10 minutes;
? G
ZLR
: R users in 3.2 weeks and D users in
1.1 weeks which is 7 times faster on average
across attributes than for the user model;
? G
geo
: R users in 1.2 weeks and D users in
3.5 weeks which is on average 6 times faster
across attributes than for the user model.
Similar or better P
?
(R|T ) convergence in terms
of the number of tweets and, especially, in the
amount of time needed for user and user-neighbor
192
models further confirms that neighborhood con-
tent is useful for political preference prediction.
Moreover, communications from a joint stream al-
low to make an inference up to 7 times faster.
8 Related Work
Supervised Batch Approaches The vast major-
ity of work on predicting latent user attributes in
social media apply supervised static SVM mod-
els for discrete categorical e.g., gender and re-
gression models for continuous attributes e.g., age
with lexical bag-of-word features for classifying
user gender (Garera and Yarowsky, 2009; Rao et
al., 2010; Burger et al, 2011; Van Durme, 2012b),
age (Rao et al, 2010; Nguyen et al, 2011; Nguyen
et al, 2013) or political orientation. We present an
overview of the existing models for political pref-
erence prediction in Table 1.
Bergsma et al (2012) following up on Rao?s
work (2010) on adding socio-linguistic features
to improve gender, ethnicity and political prefer-
ence prediction show that incorporating stylistic
and syntactic information to the bag-of-word fea-
tures improves gender classification.
Other methods characterize Twitter users by ap-
plying limited amounts of network structure in-
formation in addition to lexical features. Con-
Approach Users Tweets Features Accur.
Rao et al
(2010)
1K 2M
ngrams
socio-ling
stacked
0.824
0.634
0.809
Pennacchiotti
and Popescu
(2011a)
10.3K ?
ling-all
soc-all
full
0.770
0.863
0.889
Conover et
al. (2011)
1,000 1M
full-text
hashtags
clusters
0.792
0.908
0.949
Zamal et al
(2012)
400
400K
3.85M
4.25M
UserOnly
Nbr
User-Nbr
11
0.890
0.920
0.932
Cohen and
Ruths
(2013)
397
1.8K
262
196
397K
1.8M
262K
196K
features
from (Za-
mal et al,
2012)
0.910
0.840
0.680
0.870
This paper
(batch clas-
sification)
G
cand
1,031
G
geo
270
G
ZLR
371
206K
2M
54K
540K
371K
1.5M
user ngrams
neighbor
user ngrams
neighbor
user ngrams
neighbor
0.720
0.750
0.570
0.670
0.886
0.920
This paper
(dynamic
Bayesian
update clas-
sification)
G
cand
1,031
G
geo
270
G
ZLR
371
103K
130K
54K
67K
74K
185K
user stream
user-neigh.
user stream
user-neigh.
user stream
user-neigh.
0.995
0.999
0.843
0.882
0.892
0.999
Table 1: Overview of the existing approaches for political
preference classification in Twitter.
nover et al (2011) rely on identifying strong parti-
san clusters of Democratic and Republican users
in a Twitter network based on retweet and user
mention degree of connectivity, and then combine
this clustering information with the follower and
friend neighborhood size features. Pennacchiotti
et al (2011a; 2011b) focus on user behavior, net-
work structure and linguistic features. Similar to
our work, they assume that users from a partic-
ular class tend to reply and retweet messages of
the users from the same class. We extend this as-
sumption and study other relationship types e.g.,
friends, user mentions etc. Recent work by Wong
et al (2013) investigates tweeting and retweet-
ing behavior for political learning during 2012 US
Presidential election. The most similar work to
ours is by Zamal et al (2012), where the authors
apply features from the tweets authored by a user?s
friend to infer attributes of that user. In this paper,
we study different types of user social circles in
addition to a friend network.
Additionally, using social media for mining po-
litical opinions (O?Connor et al, 2010a; May-
nard and Funk, 2012) or understanding socio-
political trends and voting outcomes (Tumasjan
et al, 2010; Gayo-Avello, 2012; Lampos et al,
2013) is becoming a common practice. For in-
stance, Lampos et al (2013) propose a bilinear
user-centric model for predicting voting intentions
in the UK and Australia from social media data.
Other works explore political blogs to predict what
content will get the most comments (Yano et al,
2013) or analyze communications from Capitol
Hill
12
to predict campaign contributors based on
this content (Yano and Smith, 2013).
Unsupervised Batch Approaches Bergsma et
al. (2013) show that large-scale clustering of user
names improves gender, ethnicity and location
classification on Twitter. O?Connor et al (2010b)
following the work by Eisenstein (2010) propose
a Bayesian generative model to discover demo-
graphic language variations in Twitter. Rao et
al. (2011) suggest a hierarchical Bayesian model
which takes advantage of user name morphology
for predicting user gender and ethnicity. Golbeck
et al (2010) incorporate Twitter data in a spatial
model of political ideology.
Streaming Approaches Van Durme (2012b)
proposed streaming models to predict user gen-
der in Twitter. Other works suggested to process
12
http://www.tweetcongress.org
193
text streams for a variety of NLP tasks e.g., real-
time opinion mining and sentiment analysis in so-
cial media (Pang and Lee, 2008), named entity
disambiguation (Sarmento et al, 2009), statistical
machine translation (Levenberg et al, 2011), first
story detection (Petrovi?c et al, 2010), and unsu-
pervised dependency parsing (Goyal and Daum?e,
2011). Massive Online Analysis (MOA) toolkit
developed by Bifet et al (2010) is an alternative to
the Jerboa package used in this work developed
by Van Durme (2012a). MOA has been effec-
tively used to detect sentiment changes in Twitter
streams (Bifet et al, 2011).
9 Conclusions and Future Work
In this paper, we extensively examined state-of-
the-art static approaches and proposed novel mod-
els with dynamic Bayesian updates for streaming
personal analytics on Twitter. Because our stream-
ing models rely on communications from Twitter
users and content from various notions of user-
local neighborhood they can be effectively applied
to real-time dynamic data streams. Our results
support several key findings listed below.
Neighborhood content is useful for personal
analytics. Content extracted from various notions
of a user-local neighborhood can be as effective
or more effective for political preference classifi-
cation than user self-authored content. This may
be an effect of ?sparseness? of relevant user data,
in that users talk about politics very sporadically
compared to a random sample of their neighbors.
Substantial signal for political preference
prediction is distributed in the neighborhood.
Querying for more neighbors per user is more ben-
eficial than querying for extra content from the
existing neighbors e.g., 5 tweets from 10 neigh-
bors leads to higher accuracy than 25 tweets from
2 neighbors or 50 tweets from 1 neighbor. This
may be also the effect of data heterogeneity in
social media compared to e.g., political debate
text (Thomas et al, 2006). These findings demon-
strate that a substantial signal is distributed over
the neighborhood content.
Neighborhoods constructed from friend,
user mention and retweet relationships are
most effective. Friend, user mention and retweet
neighborhoods show the best accuracy for predict-
ing political preferences of Twitter users. We think
that friend relationships are more effective than
e.g., follower relationships because it is very likely
that users share common interests and preferences
with their friends, e.g. Facebook friends can even
be used to predict a user?s credit score.
13
User
mentions and retweets are two primary ways of in-
teraction on Twitter. They both allow to share in-
formation e.g., political news, events with others
and to be involved in direct communication e.g.,
live political discussions, political groups.
Streaming models are more effective than
batch models for personal analytics. The predic-
tions made using dynamic models with Bayesian
updates over user and joint user-neighbor commu-
nication streams demonstrate higher performance
with lower resources spent compared to the batch
models. Depending on user political involvement,
expressiveness and activeness, the perfect predic-
tion (approaching 100% accuracy) can be made
using only 100 - 500 tweets per user.
Generalization of the classifiers for political
preference prediction. This work raises a very
important but under-explored problem of the gen-
eralization of classifiers for personal analytics in
social media, also recently discussed by Cohen
and Ruth (2013). For instance, the existing models
developed for political preference prediction are
all trained on Twitter data but report significantly
different results even for the same baseline mod-
els trained using bag-of-word lexical features as
shown in Table 1. In this work we experiment with
three different datasets. Our results for both static
and dynamic models show that the accuracy in-
deed depends on the way the data was constructed.
Therefore, publicly available datasets need to be
released for a meaningful comparison of the ap-
proaches for personal analytics in social media.
In future work, we plan to incorporate itera-
tive model updates from newly classified com-
munications similar to online perceptron-style up-
dates. In addition, we aim to experiment with
neighborhood-specific classifiers applied towards
the tweets from neighborhood-specific streams
e.g., friend classifier used for friend tweets,
retweet classifier applied to retweet tweets etc.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their helpful comments.
13
http://money.cnn.com/2013/08/26/technology/social/
facebook-credit-score/
194
References
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 327?337.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 1010?1019.
Albert Bifet, Geoff Holmes, Bernhard Pfahringer,
Philipp Kranen, Hardy Kremer, Timm Jansen, and
Thomas Seidl. 2010. MOA: Massive online analy-
sis, a framework for stream classification and clus-
tering. Journal of Machine Learning Research,
11:44?50.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald`a. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, 17:5?11.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1301?1309.
Raviv Cohen and Derek Ruths. 2013. Classifying Po-
litical Orientation on Twitter: It?s Not Easy! In Pro-
ceedings of the International AAAI Conference on
Weblogs and Social Media (ICWSM), pages 91?99.
Michael D. Conover, Bruno Gonc?alves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011. Predicting the political alignment
of Twitter users. In Proceedings of Social Comput-
ing, pages 192?199.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1277?1287.
Rong En Fan, Kai Wei Chang, Cho Jui Hsieh, Xi-
ang Rui Wang, and Chih Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. Jour-
nal of Machine Learning Research, 9:1871?1874.
Katja Filippova. 2012. User demographics and lan-
guage in an implicit social network. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1478?1488.
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 710?718.
Daniel Gayo-Avello. 2012. No, you cannot predict
elections with Twitter. Internet Computing, IEEE,
16(6):91?94.
Jennifer Golbeck, Justin M. Grimes, and Anthony
Rogers. 2010. Twitter use by the u.s. congress.
Journal of the American Society for Information Sci-
ence and Technology, 61(8):1612?1621.
Amit Goyal and Hal Daum?e, III. 2011. Approxi-
mate scalable bounded space sketch for large data
NLP. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 250?261.
Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from social media. In Proceedings of the Asso-
ciation for Computational Linguistics (ACL), pages
993?1003.
Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statis-
tical machine translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT), pages 177?186.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of the Annual Meeting on Association for Computa-
tional Linguistics (ACL), pages 459?466.
Ingrid Lunden. 2012. Analyst: Twitter
passed 500M users in june 2012, 140m of
them in US; Jakarta ?biggest tweeting? city.
http://techcrunch.com/2012/07/30/analyst-twitter-
passed-500m-users-in-june-2012-140m-of-them-in-
us-jakarta-biggest-tweeting-city/.
Diana Maynard and Adam Funk. 2012. Automatic de-
tection of political opinions in tweets. In Proceed-
ings of the 8th International Conference on The Se-
mantic Web (ESWC), pages 88?99.
Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and
Mung Chiang. 2013. Quantifying political leaning
from tweets and retweets. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).
Dong Nguyen, Noah A. Smith, and Carolyn P. Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities
(LaTeCH), pages 115?123.
195
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and
Theo Meder. 2013. ?How old do you think I am??
A study of language and age in Twitter. In Proceed-
ings of the AAAI Conference on Weblogs and Social
Media (ICWSM), pages 439?448.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 122?129.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010b. A mixture model of de-
mographic lexical variation. In Proceedings of the
NIPS Workshop on Machine Learning and Social
Computing.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations of Trends in Infor-
mation Retrieval, 2(1-2):1?135, January.
Marco Pennacchiotti and Ana-Maria Popescu. 2011a.
Democrats, republicans and starbucks afficionados:
user classification in twitter. In Proceedings of the
17th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 430?438.
Marco Pennacchiotti and Ana Maria Popescu. 2011b.
A machine learning approach to Twitter user clas-
sification. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM),
pages 281?288.
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to Twitter. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL).
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 209?217.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the 2nd In-
ternational Workshop on Search and Mining User-
generated Contents (SMUC), pages 37?44.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hier-
archical Bayesian models for latent attribute detec-
tion in social media. In Proceedings of the Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM).
Lu??s Sarmento, Alexander Kehlenbeck, Eug?enio
Oliveira, and Lyle Ungar. 2009. An approach to
web-scale named-entity disambiguation. In Pro-
ceedings of the 6th International Conference on Ma-
chine Learning and Data Mining in Pattern Recog-
nition (MLDM), pages 689?703.
Noah A. Smith. 2004. Log-linear models.
Craig Smith. 2013. May 2013 by the
numbers: 16 amazing Twitter stats.
http://expandedramblings.com/index.php/march-
2013-by-the-numbers-a-few-amazing-twitter-stats/.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 327?335.
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.
Welpe. 2010. Predicting elections with Twitter:
What 140 characters reveal about political senti-
ment. In Proceedings of the International AAAI
Conference on Weblogs and Social Media, pages
178?185.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical re-
port, Human Language Technology Center of Excel-
lence.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 48?58.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing, pages 61?72.
Tao Yano and Noah A. Smith. 2013. What?s worthy
of comment? content and comment volume in po-
litical blogs. In International AAAI Conference on
Weblogs and Social Media (ICWSM).
Tao Yano, Dani Yogatama, and Noah A. Smith. 2013.
A penny for your tweets: Campaign contributions
and capitol hill microblogs. In Proceedings of the
International AAAI Conference on Weblogs and So-
cial Media (ICWSM).
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media, pages 387?390.
196
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 956?966,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Information Extraction over Structured Data:
Question Answering with Freebase
Xuchen Yao
1
and Benjamin Van Durme
1,2
1
Center for Language and Speech Processing
2
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD, USA
Abstract
Answering natural language questions us-
ing the Freebase knowledge base has re-
cently been explored as a platform for ad-
vancing the state of the art in open do-
main semantic parsing. Those efforts map
questions to sophisticated meaning repre-
sentations that are then attempted to be
matched against viable answer candidates
in the knowledge base. Here we show
that relatively modest information extrac-
tion techniques, when paired with a web-
scale corpus, can outperform these sophis-
ticated approaches by roughly 34% rela-
tive gain.
1 Introduction
Question answering (QA) from a knowledge base
(KB) has a long history within natural language
processing, going back to the 1960s and 1970s,
with systems such as Baseball (Green Jr et al,
1961) and Lunar (Woods, 1977). These systems
were limited to closed-domains due to a lack of
knowledge resources, computing power, and abil-
ity to robustly understand natural language. With
the recent growth in KBs such as DBPedia (Auer
et al, 2007), Freebase (Bollacker et al, 2008)
and Yago2 (Hoffart et al, 2011), it has be-
come more practical to consider answering ques-
tions across wider domains, with commercial sys-
tems including Google Now, based on Google?s
Knowledge Graph, and Facebook Graph
Search, based on social network connections.
The AI community has tended to approach this
problem with a focus on first understanding the in-
tent of the question, via shallow or deep forms of
semantic parsing (c.f. ?3 for a discussion). Typ-
ically questions are converted into some mean-
ing representation (e.g., the lambda calculus), then
mapped to database queries. Performance is thus
bounded by the accuracy of the original seman-
tic parsing, and the well-formedness of resultant
database queries.
1
The Information Extraction (IE) community ap-
proaches QA differently: first performing rela-
tively coarse information retrieval as a way to
triage the set of possible answer candidates, and
only then attempting to perform deeper analysis.
Researchers in semantic parsing have recently
explored QA over Freebase as a way of moving
beyond closed domains such as GeoQuery (Tang
and Mooney, 2001). While making semantic pars-
ing more robust is a laudable goal, here we provide
a more rigorous IE baseline against which those
efforts should be compared: we show that ?tradi-
tional? IE methodology can significantly outper-
form prior state-of-the-art as reported in the se-
mantic parsing literature, with a relative gain of
34% F
1
as compared to Berant et al (2013).
2 Approach
We will view a KB as an interlinked collection of
?topics?. When given a question about one or sev-
eral topics, we can select a ?view? of the KB con-
cerning only involved topics, then inspect every
related node within a few hops of relations to the
topic node in order to extract the answer. We call
such a view a topic graph and assume answers can
be found within the graph. We aim to maximally
automate the answer extraction process, by mas-
sively combining discriminative features for both
the question and the topic graph. With a high per-
formance learner we have found that a system with
millions of features can be trained within hours,
leading to intuitive, human interpretable features.
For example, we learn that given a question con-
cerning money, such as: what money is used in
1
As an example, 50% of errors of the CCG-backed
(Kwiatkowski et al, 2013) system were contributed by pars-
ing or structural matching failure.
956
ukraine, the expected answer type is likely cur-
rency. We formalize this approach in ?4.
One challenge for natural language querying
against a KB is the relative informality of queries
as compared to the grammar of a KB. For exam-
ple, for the question: who cheated on celebrity
A, answers can be retrieved via the Freebase rela-
tion celebrity.infidelity.participant, but the con-
nection between the phrase cheated on and the
formal KB relation is not explicit. To allevi-
ate this problem, the best attempt so far is to
map from ReVerb (Fader et al, 2011) predicate-
argument triples to Freebase relation triples (Cai
and Yates, 2013; Berant et al, 2013). Note that
to boost precision, ReVerb has already pruned
down less frequent or credible triples, yielding not
as much coverage as its text source, ClueWeb.
Here we instead directly mine relation mappings
from ClueWeb and show that both direct relation
mapping precision and indirect QA F
1
improve by
a large margin. Details in ?5.
Finally, we tested our system, jacana-
freebase,
2
on a realistic dataset generously
contributed by Berant et al (2013), who collected
thousands of commonly asked questions by
crawling the Google Suggest service. Our
method achieves state-of-the-art performance
with F
1
at 42.0%, a 34% relative increase from
the previous F
1
of 31.4%.
3 Background
QA from a KB faces two prominent challenges:
model and data. The model challenge involves
finding the best meaning representation for the
question, converting it into a query and exe-
cuting the query on the KB. Most work ap-
proaches this via the bridge of various interme-
diate representations, including combinatory cat-
egorial grammar (Zettlemoyer and Collins, 2005,
2007, 2009; Kwiatkowski et al, 2010, 2011,
2013), synchronous context-free grammars (Wong
and Mooney, 2007), dependency trees (Liang et
al., 2011; Berant et al, 2013), string kernels (Kate
and Mooney, 2006; Chen and Mooney, 2011),
and tree transducers (Jones et al, 2012). These
works successfully showed their effectiveness in
QA, despite the fact that most of them require
hand-labeled logic annotations. More recent re-
search started to minimize this direct supervision
by using latent meaning representations (Berant et
2
https://code.google.com/p/jacana
al., 2013; Kwiatkowski et al, 2013) or distant su-
pervision (Krishnamurthy and Mitchell, 2012).
We instead attack the problem of QA from a KB
from an IE perspective: we learn directly the pat-
tern of QA pairs, represented by the dependency
parse of questions and the Freebase structure of
answer candidates, without the use of intermedi-
ate, general purpose meaning representations.
The data challenge is more formally framed as
ontology or (textual) schema matching (Hobbs,
1985; Rahm and Bernstein, 2001; Euzenat and
Shvaiko, 2007): matching structure of two on-
tologies/databases or (in extension) mapping be-
tween KB relations and NL text. In terms of
the latter, Cai and Yates (2013) and Berant et al
(2013) applied pattern matching and relation inter-
section between Freebase relations and predicate-
argument triples from the ReVerb OpenIE sys-
tem (Fader et al, 2011). Kwiatkowski et al
(2013) expanded their CCG lexicon with Wik-
tionary word tags towards more domain indepen-
dence. Fader et al (2013) learned question para-
phrases from aligning multiple questions with the
same answers generated by WikiAnswers. The
key factor to their success is to have a huge text
source. Our work pushes the data challenge to the
limit by mining directly from ClueWeb, a 5TB
collection of web data.
Finally, the KB community has developed other
means for QA without semantic parsing (Lopez et
al., 2005; Frank et al, 2007; Unger et al, 2012;
Yahya et al, 2012; Shekarpour et al, 2013). Most
of these work executed SPARQL queries on in-
terlinked data represented by RDF (Resource De-
scription Framework) triples, or simply performed
triple matching. Heuristics and manual templates
were also commonly used (Chu-Carroll et al,
2012). We propose instead to learn discriminative
features from the data with shallow question anal-
ysis. The final system captures intuitive patterns
of QA pairs automatically.
4 Graph Features
Our model is inspired by an intuition on how ev-
eryday people search for answers. If you asked
someone: what is the name of justin bieber
brother,
3
and gave them access to Freebase, that
person might first determine that the question
3
All examples used in this paper come from the train-
ing data crawled from Google Suggest. They are low-
ercased and some contain typos.
957
is about Justin Bieber (or his brother), go to
Justin Bieber?s Freebase page, and search for his
brother?s name. Unfortunately Freebase does not
contain an exact relation called brother, but in-
stead sibling. Thus further inference (i.e., brother
? male sibling) has to be made. In the following
we describe how we represent this process.
4.1 Question Graph
In answering our example query a person might
take into consideration multiple constraints. With
regards to the question, we know we are looking
for the name of a person based on the following:
? the dependency relation nsubj(what, name)
and prep of(name, brother) indicates that the
question seeks the information of a name;
4
? the dependency relation prep of(name,
brother) indicates that the name is about a
brother (but we do not know whether it is a
person name yet);
? the dependency relation nn(brother, bieber)
and the facts that, (i) Bieber is a person and (ii)
a person?s brother should also be a person, indi-
cate that the name is about a person.
This motivates the design of dependency-based
features. We show one example in Figure 1(a),
left side. The following linguistic information is
of interest:
? question word (qword), such as what/who/how
many. We use a list of 9 common qwords.
5
? question focus (qfocus), a cue of expected an-
swer types, such as name/money/time. We
keep our analysis simple and do not use a ques-
tion classifier, but simply extract the noun de-
pendent of qword as qfocus.
? question verb (qverb), such as is/play/take, ex-
tracted from the main verb of the question.
Question verbs are also good hints of answer
types. For instance, play is likely to be followed
by an instrument, a movie or a sports team.
? question topic (qtopic). The topic of the ques-
tion helps us find relevant Freebase pages. We
simply apply a named entity recognizer to find
the question topic. Note that there can be more
than one topic in the question.
Then we convert the dependency parse into a more
generic question graph, in the following steps:
4
We use the Stanford collapsed dependency form.
5
who, when, what, where, how, which, why, whom,
whose.
1. if a node was tagged with a question feature,
then replace this node with its question feature,
e.g., what? qword=what;
2. (special case) if a qtopic node was tagged as
a named entity, then replace this node with
its its named entity form, e.g., bieber ?
qtopic=person;
3. drop any leaf node that is a determiner, prepo-
sition or punctuation.
The converted graph is shown in Figure 1(a),
right side. We call this a question feature graph,
with every node and relation a potential feature
for this question. Then features are extracted
in the following form: with s the source and
t the target node, for every edge e(s, t) in the
graph, extract s, t, s | t and s | e | t as
features. For the edge, prep of(qfocus=name,
brother), this would mean the following features:
qfocus=name, brother, qfocus=name|brother,
and qfocus=name|prep of|brother.
We show with examples why these features
make sense later in ?6 Table 6. Furthermore, the
reason that we have kept some lexical features,
such as brother, is that we hope to learn from
training a high correlation between brother and
some Freebase relations and properties (such as
sibling and male) if we do not possess an exter-
nal resource to help us identify such a correlation.
4.2 Freebase Topic Graph
Given a topic, we selectively roll out the Free-
base graph by choosing those nodes within a few
hops of relationship to the topic node, and form
a topic graph. Besides incoming and/or outgo-
ing relationships, nodes also have properties: a
string that describes the attribute of a node, for
instance, node type, gender or height (for a per-
son). One major difference between relations and
properties is that both arguments of a relation are
nodes, while only one argument of a property is a
node, the other a string. Arguments of relations are
usually interconnected, e.g., London can be the
place of birth for Justin Bieber, or capital of
the UK. Arguments of properties are attributes that
are only ?attached? to certain nodes and have no
outgoing edges. Figure 1(b) shows an example.
Both relationship and property of a node are
important to identifying the answer. They con-
nect the nodes with the question and describe
some unique characteristics. For instance, with-
out the properties type:person and gender:male,
958
what
is name
the brother
justin bieber
nsubj     cop    
nn  
prep_of    det  
nn    
qword
qtopic
qfocus
qtopic
qword=what
qfocus=name
brother
qtopic=person qtopic=person
nsubj    
nn  
prep_of    
nn    
qverb=be
 cop    
qverb
(a) Dependence parse with annotated question features in dashed boxes (left) and converted feature graph (right) with
only relevant and general information about the original question kept. Note that the left is a real but incorrect parse.
Justin Bieber
dummy node
Jazmyn Bieber
person.sibling_s    
Jaxon Bieber
sibling sibling
person
type
person
type
female
gender
male
gender
London
awards_won  
place_of_birth
?... type person
male
gender
(b) A view of Freebase graph on the Justin Bieber topic with nodes in solid boxes and properties in
dashed boxes. The hatching node, Jaxon Bieber, is the answer. Freebase uses a dummy parent node
for a list of nodes with the same relation.
Figure 1: Dependency parse and excerpted Freebase topic graph on the question what is the name of
justin bieber brother.
959
we would not have known the node Jaxon Bieber
represents a male person. These properties, along
with the sibling relationship to the topic node, are
important cues for answering the question. Thus
for the Freebase graph, we use relations (with di-
rections) and properties as features for each node.
Additionally, we have analyzed how Freebase
relations map back to the question. Some of the
mapping can be simply detected as paraphras-
ing or lexical overlap. For example, the per-
son.parents relationship helps answering ques-
tions about parenthood. However, most Freebase
relations are framed in a way that is not com-
monly addressed in natural language questions.
For instance, for common celebrity gossip ques-
tions like who cheated on celebrity A, it is
hard for a system to find the Freebase relation
celebrity.infidelity.participant as the target rela-
tion if it had not observed this pattern in training.
Thus assuming there is an alignment model that
is able to tell how likely one relation maps to the
original question, we add extra alignment-based
features for the incoming and outgoing relation of
each node. Specifically, for each relation rel in
a topic graph, we compute P (rel | question) to
rank the relations. Finally the ranking (e.g., top
1/2/5/10/100 and beyond) of each relation is used
as features instead of a pure probability. We de-
scribe such an alignment model in ? 5.
4.3 Feature Production
We combine question features and Freebase fea-
tures (per node) by doing a pairwise concatena-
tion. In this way we hope to capture the associa-
tion between question patterns and answer nodes.
For instance, in a loglinear model setting, we ex-
pect to learn a high feature weight for features like:
qfocus=money|node type=currency
and a very low weight for:
qfocus=money|node type=person.
This combination greatly enlarges the total
number of features, but owing to progress in large-
scale machine learning such feature spaces are less
of a concern than they once were (concrete num-
bers in ? 6 Model Tuning).
5 Relation Mapping
In this section we describe a ?translation? table be-
tween Freebase relations and NL words was built.
5.1 Formula
The objective is to find the most likely rela-
tion a question prompts. For instance, for the
question who is the father of King George
VI, the most likely relation we look for is peo-
ple.person.parents. To put it more formally,
given a question Q of a word vector w, we want
to find out the relation R that maximizes the prob-
ability P (R | Q).
More interestingly, for the question who is
the father of the Periodic Table, the ac-
tual relation that encodes its original mean-
ing is law.invention.inventor, rather than peo-
ple.person.parents. This simple example points
out that every part of the question could change
what the question inquires eventually. Thus we
need to count for each word w in Q. Due to the
bias and incompleteness of any data source, we
approximate the true probability of P with
?
P un-
der our specific model. For the simplicity of com-
putation, we assume conditional independence be-
tween words and apply Naive Bayes:
?
P (R | Q) ?
?
P (Q | R)
?
P (R)
?
?
P (w | R)
?
P (R)
?
?
w
?
P (w | R)
?
P (R)
where
?
P (R) is the prior probability of a relation
R and
?
P (w | R) is the conditional probability of
word w given R.
It is possible that we do not observe a certain
relation R when computing the above equation.
In this case we back off to the ?sub-relations?: a
relation R is a concatenation of a series of sub-
relations R = r = r
1
.r
2
.r
3
. . . .. For instance, the
sub-relations of people.person.parents are peo-
ple, person, and parents. Again, we assume con-
ditional independence between sub-relations and
apply Naive Bayes:
?
P
backoff
(R | Q) ?
?
P (r | Q)
?
?
r
?
P (r | Q)
?
?
r
?
P (Q | r)
?
P (r)
?
?
r
?
w
?
P (w | r)
?
P (r)
One other reason that we estimated
?
P (w | r) and
?
P (r) for sub-relations is
that Freebase relations share some com-
mon structures in between them. For in-
stance, both people.person.parents and
fictional universe.fictional character.parents
960
indicate the parent relationship but the latter is
much less commonly annotated. We hope that the
shared sub-relation, parents, can help better esti-
mate for the less annotated. Note that the backoff
model would have a much smaller value than the
original, due to double multiplication
?
r
?
w
. In
practice we normalize it by the sub-relations size
to keep it at the same scale with
?
P (R | Q).
Finally, to estimate the prior and conditional
probability, we need a massive data collection.
5.2 Steps
The ClueWeb09
6
dataset is a collection of 1 billion
webpages (5TB compressed in raw HTML) in 10
languages by Carnegie Mellon University in 2009.
FACC1, the Freebase Annotation of the ClueWeb
Corpus version 1 (Gabrilovich et al, 2013), con-
tains index and offset of Freebase entities within
the English portion of ClueWeb. Out of all 500
million English documents, 340 million were au-
tomatically annotated with at least one entity, with
an average of 15 entity mentions per document.
The precision and recall of annotation were esti-
mated at 80?85% and 70?85% (Orr et al, 2013).
Given these two resources, for each binary Free-
base relation, we can find a collection of sentences
each of which contains both of its arguments, then
simply learn how words in these sentences are as-
sociated with this relation, i.e.,
?
P (w | R) and
?
P (w | r). By counting how many times each rela-
tion R was annotated, we can estimate
?
P (R) and
?
P (r). The learning task can be framed in the fol-
lowing short steps:
1. We split each HTML document by sentences
(Kiss and Strunk, 2006) using NLTK (Bird and
Loper, 2004) and extracted those with at least
two Freebase entities which has at least one di-
rect established relation according to Freebase.
2. The extraction formed two parallel corpora,
one with ?relation - sentence? pairs (for esti-
mating
?
P (w | R) and
?
P (R)) and the other with
?subrelations - sentence? pairs (for
?
P (w | r)
and
?
P (r)). Each corpus has 1.2 billion pairs.
3. The tricky part was to align these 1.2 billion
pairs. Since the relations on one side of these
pairs are not natural sentences, we ran the
most simple IBM alignment Model 1 (Brown
et al, 1993) to estimate the translation proba-
bility with GIZA++ (Och and Ney, 2003). To
speed up, the 1.2 billion pairs were split into
6
http://lemurproject.org/clueweb09/
0 ? 10 ? 10
2
? 10
3
? 10
4
> 10
4
7.0% 0.7% 1.2% 0.4% 1.3% 89.5%
Table 1: Percentage of answer relations (the in-
coming relation connected to the answer node)
with respect to how many sentences we learned
this relation from in CluewebMapping. For in-
stance, the first column says there are 7% of an-
swer relations for which we cannot find a mapping
(so we had to use the backoff probability estima-
tion); the last column says there are 89.5% of an-
swer relations that we were able to learn the map-
ping between this relation and text based on more
than 10 thousand relation-sentence pairs. The total
number of answer relations is 7886.
100 even chunks. We ran 5 iterations of EM on
each one and finally aligned the 1.2 billion pairs
from both directions. To symmetrize the align-
ment, common MT heuristics INTERSECTION,
UNION, GROW-DIAG-FINAL, and GROW-DIAG-
FINAL-AND (Koehn, 2010) were separately ap-
plied and evaluated later.
4. Treating the aligned pairs as observation, the
co-occurrence matrix between aligning rela-
tions and words was computed. There were
10,484 relations and sub-relations in all, and we
kept the top 20,000 words.
5. From the co-occurrence matrix we computed
?
P (w | R),
?
P (R),
?
P (w | r) and
?
P (r).
Hand-checking the learned probabilities shows
both success, failure and some bias. For in-
stance, for the film.actor.film relation (mapping
from film names to actor names), the top words
given by
?
P (w | R) are won, star, among, show.
For the film.film.directed by relation, some im-
portant stop words that could indicate this re-
lation, such as by and with, rank directly after
director and direct. However, due to signifi-
cant popular interest in certain news categories,
and the resultant catering by websites to those
information desires, then for example we also
learned a heavily correlated connection between
Jennifer Aniston and celebrity.infidelity.victim,
and between some other you-know-who names
and celebrity.infidelity.participant.
We next formally evaluate how the learned map-
ping help predict relations from words.
961
5.3 Evaluation
Both ClueWeb and its Freebase annotation has a
bias. Thus we were firstly interested in the cov-
erage of mined relation mappings. As a com-
parison, we used a dataset of relation mapping
contributed by Berant et al (2013) and Lin et al
(2012). The idea is very similar: they intersected
Freebase relations with predicates in (arg1, predi-
cate, arg2) triples extracted from ReVerb to learn
the mapping between Freebase relations and triple
predicates. Note the scale difference: although
ReVerb was also extracted from ClueWeb09,
there were only 15 million triples to intersect with
the relations, while we had 1.2 billion alignment
pairs. We call this dataset ReverbMapping and
ours CluewebMapping.
The evaluation dataset, WEBQUESTIONS, was
also contributed by Berant et al (2013). It con-
tains 3778 training and 2032 test questions col-
lected from the Google Suggest service. All ques-
tions were annotated with answers from Freebase.
Some questions have more than one answer, such
as what to see near sedona arizona?.
We evaluated on the training set in two aspects:
coverage and prediction performance. We define
answer node as the node that is the answer and
answer relation as the relation from the answer
node to its direct parent. Then we computed how
much and how well the answer relation was trig-
gered by ReverbMapping and CluewebMapping.
Thus for the question, who is the father of King
George VI, we ask two questions: does the map-
ping, 1. (coverage) contain the answer relation
people.person.parents? 2. (precision) predict
the answer relation from the question?
Table 1 shows the coverage of CluewebMap-
ping, which covers 93.0% of all answer rela-
tions. Among them, we were able to learn the rule
mapping using more than 10 thousand relation-
sentence pairs for each of the 89.5% of all an-
swer relations. In contrast, ReverbMapping covers
89.7% of the answer relations.
Next we evaluated the prediction performance,
using the evaluation metrics of information re-
trieval. For each question, we extracted all rela-
tions in its corresponding topic graph, and ranked
each relation with whether it is the answer re-
lation. For instance, for the previous exam-
ple question, we want to rank the relation peo-
ple.person.parents as number 1. We com-
puted standard MAP (Mean Average Precision)
and MRR (Mean Reciprocal Rank), shown in Ta-
ble 2(a). As a simple baseline, ?word overlap?
counts the overlap between relations and the ques-
tion. CluewebMapping ranks each relation by
?
P (R | Q). ReverbMapping does the same, ex-
cept that we took a uniform distribution on
?
P (w |
R) and
?
P (R) since the contributed dataset did
not include co-occurrence counts to estimate these
probabilities.
7
Note that the median rank from
CluewebMapping is only 12, indicating that half
of all answer relations are ranked in the top 12.
Table 2(b) further shows the percentage of
answer relations with respect to their rank-
ing. CluewebMapping successfully ranked 19%
of answer relations as top 1. A sample
of these includes person.place of birth, loca-
tion.containedby, country.currency used, reg-
ular tv appearance.actor, etc. These percentage
numbers are good clue for feature design: for in-
stance, we may be confident in a relation if it is
ranked top 5 or 10 by CluewebMapping.
To conclude, we found that CluewebMapping
provides satisfying coverage on the 3778 training
questions: only 7% were missing, despite the bi-
ased nature of web data. Also, CluewebMapping
gives reasonably good precision on its prediction,
despite the noisy nature of web data. We move on
to fully evaluate the final QA F
1
.
6 Experiments
We evaluate the final F
1
in this section. The sys-
tem of comparison is that of Berant et al (2013).
Data We re-used WEBQUESTIONS, a dataset
collected by Berant et al (2013). It contains 5810
questions crawled from the Google Suggest ser-
vice, with answers annotated on Amazon Mechan-
ical Turk. All questions contain at least one an-
swer from Freebase. This dataset has been split by
65%/35% into TRAIN-ALL and TEST. We further
randomly divided TRAIN-ALL by 80%/20% to a
smaller TRAIN and development set DEV. Note
that our DEV set is different from that of Berant
et al (2013), but the final result on TEST is di-
rectly comparable. Results are reported in terms
of macro F
1
with partial credit (following Berant
et al (2013)) if a predicted answer list does not
have a perfect match with all gold answers, as a
7
The way we used ReverbMapping was not how Berant et
al. (2013) originally used it: they employed a discriminative
log-linear model to judge relations and that might yield better
performance. As a fair comparison, ranking of CluewebMap-
ping under uniform distribution is also included in Table 2(a).
962
Median Rank MAP MRR
word overlap 471 0.0380 0.0590
ReverbMapping 60 0.0691 0.0829
CluewebMapping 12 0.2074 0.2900
with uniform dist. 61 0.0544 0.0561
(a) Ranking on answer relations. Best result on
CluewebMapping was under the GROW-DIAG-FINAL-AND
heuristics (row 3) when symmetrizing alignment from both
directions. The last row shows ranking of CluewebMapping
under uniform distribution (assuming counting on words and
relations is not known).
1 ? 5 ? 10 ? 50 ? 100 > 100
w. o. 3.5 4.7 2.5 3.9 4.1 81.3
R.M. 2.6 9.1 8.6 26.0 13.0 40.7
C.M. 19.0 19.9 8.9 22.3 7.5 22.4
(b) Percentage of answer relations w.r.t. ranking number
(header). w.o.: word overlap; R.M.: ReverbMapping; C.M.:
CluewebMapping.
Table 2: Evaluation on answer relation ranking
prediction on 3778 training questions.
lot of questions in WEBQUESTIONS contain more
than one answer.
Search With an Information Retrieval (IR)
front-end, we need to locate the exact Freebase
topic node a question is about. For this pur-
pose we used the Freebase Search API (Freebase,
2013a).All named entities
8
in a question were sent
to this API, which returned a ranked list of rele-
vant topics. We also evaluated how well the search
API served the IR purpose. WEBQUESTIONS not
only has answers annotated, but also which Free-
base topic nodes the answers come from. Thus
we evaluated the ranking of retrieval with the gold
standard annotation on TRAIN-ALL, shown in Ta-
ble 3. The top 2 results of the Search API con-
tain gold standard topics for more than 90% of the
questions and the top 10 results contain more than
95%. We took this as a ?good enough? IR front-
end and used it on TEST.
Once a topic is obtained we query the Freebase
Topic API (Freebase, 2013b) to retrieve all rele-
vant information, resulting in a topic graph. The
API returns almost identical information as dis-
played via a web browser to a user viewing this
topic. Given that turkers annotated answers based
on the topic page via a browser, this supports the
assumption that the same answer would be located
in the topic graph, which is then passed to the QA
engine for feature extraction and classification.
8
When no named entities are detected, we fall back to
noun phrases.
top 1 2 3 5 10
# 3263 3456 3532 3574 3604
% 86.4 91.5 93.5 94.6 95.4
Table 3: Evaluation on the Freebase Search API:
how many questions? top n retrieved results con-
tain the gold standard topic. Total number of ques-
tions is 3778 (size of TRAIN-ALL). There were
only 5 questions with no retrieved results.
P R F
1
basic 57.3 30.1 39.5
+ word overlap 56.0 31.4 40.2
+ CluewebMapping 59.9 35.4 44.5
+both 59.0 35.4 44.3
Table 4: F
1
on DEV with different feature settings.
Model Tuning We treat QA on Freebase as a
binary classification task: for each node in the
topic graph, we extract features and judge whether
it is the answer node. Every question was pro-
cessed by the Stanford CoreNLP suite with the
caseless model. Then the question features (?4.1)
and node features (?4.2) were combined (?4.3)
for each node. The learning problem is chal-
lenging: for about 3000 questions in TRAIN,
there are 3 million nodes (1000 nodes per topic
graph), and 7 million feature types. We em-
ployed a high-performance machine learning tool,
Classias (Okazaki, 2009). Training usually
took around 4 hours. We experimented with vari-
ous discriminative learners on DEV, including lo-
gistic regression, perceptron and SVM, and found
L1 regularized logistic regression to give the best
result. The L1 regularization encourages sparse
features by driving feature weights towards zero,
which was ideal for the over-generated feature
space. After training, we had around 30 thousand
features with non-zero weights, a 200 fold reduc-
tion from the original features.
Also, we did an ablation test on DEV about
how additional features on the mapping between
Freebase relations and the original questions help,
with three feature settings: 1) ?basic? features in-
clude feature productions read off from the fea-
ture graph (Figure 1); 2) ?+ word overlap? adds
additional features on whether sub-relations have
overlap with the question; and 3) ?+ CluewebMap-
ping? adds the ranking of relation prediction given
the question according to CluewebMapping. Ta-
ble 4 shows that the additional CluewebMapping
963
P R F
1
Gold Retrieval 45.4 52.2 48.6
Freebase Search API 38.8 45.8 42.0
Berant et al (2013) - - 31.4
Table 5: F
1
on TEST with Gold Retrieval and
Freebase Search API as the IR front end. Berant
et al (2013) actually reported accuracy on this
dataset. However, since their system predicted an-
swers for almost every question (p.c.), it is roughly
that precision=recall=F
1
=accuracy for them.
features improved overall F
1
by 5%, a 13% rel-
ative improvement: a remarkable gain given that
the model already learned a strong correlation be-
tween question types and answer types (explained
more in discussion and Table 6 later).
Finally, the ratio of positive vs. negative exam-
ples affect final F
1
: the more positive examples,
the lower the precision and the higher the recall.
Under the original setting, this ratio was about
1 : 275. This produced precision around 60%
and recall around 35% (c.f. Table 4). To optimize
for F
1
, we down-sampled the negative examples to
20%, i.e., a new ratio of 1 : 55. This boosted the
final F
1
on DEV to 48%. We report the final TEST
result under this down-sampled training. In prac-
tice the precision/recall balance can be adjusted by
the positive/negative ratio.
Test Results Table 5 gives the final F
1
on TEST.
?Gold Retrieval? always ranked the correct topic
node top 1, a perfect IR front-end assumption. In
a more realistic scenario, we had already evaluated
that the Freebase Search API returned the correct
topic node 95% of the time in its top 10 results (c.f.
Table 3), thus we also tested on the top 10 results
returned by the Search API. To keep things sim-
ple, we did not perform answer voting, but sim-
ply extracted answers from the first (ranked by the
Search API) topic node with predicted answer(s)
found. The final F
1
of 42.0% gives a relative im-
provement over previous best result (Berant et al,
2013) of 31.4% by one third.
One question of interest is whether our system,
aided by the massive web data, can be fairly com-
pared to the semantic parsing approaches (note
that Berant et al (2013) also used ClueWeb in-
directly through ReVerb). Thus we took out
the word overlapping and CluewebMapping based
features, and the new F
1
on TEST was 36.9%.
The other question of interest is that whether
our system has acquired some level of ?machine
wgt. feature
5.56 qfocus=money|type=Currency
5.35 qverb=die|type=Cause Of Death
5.11 qword=when|type=datetime
4.56 qverb=border|rel=location.adjoins
3.90 qword=why|incoming relation rank=top 3
2.94 qverb=go|qtopic=location|type=Tourist attraction
-3.94 qtopic=location|rel=location.imports exports.date
-2.93 qtopic=person|rel=education.end date
Table 6: A sample of the top 50 most positive/neg-
ative features. Features are production between
question and node features (c.f. Figure 1).
intelligence?: how much does it know what the
question inquires? We discuss it below through
feature and error analysis.
Discussion The combination between questions
and Freebase nodes captures some real gist of QA
pattern typing, shown in Table 6 with sampled fea-
tures and weights. Our system learned, for in-
stance, when the question asks for geographic ad-
jacency information (qverb=border), the correct
answer relation to look for is location.adjoins.
Detailed comparison with the output from Berant
et al (2013) is a work in progress and will be pre-
sented in a follow-up report.
7 Conclusion
We proposed an automatic method for Question
Answering from structured data source (Free-
base). Our approach associates question features
with answer patterns described by Freebase and
has achieved state-of-the-art results on a balanced
and realistic QA corpus. To compensate for the
problem of domain mismatch or overfitting, we
exploited ClueWeb, mined mappings between KB
relations and natural language text, and showed
that it helped both relation prediction and an-
swer extraction. Our method employs relatively
lightweight machinery but has good performance.
We hope that this result establishes a new baseline
against which semantic parsing researchers can
measure their progress towards deeper language
understanding and answering of human questions.
Acknowledgments We thank the Allen Institute
for Artificial Intelligence for funding this work.
We are also grateful to Jonathan Berant, Tom
Kwiatkowski, Qingqing Cai, Adam Lopez, Chris
Callison-Burch and Peter Clark for helpful discus-
sion and to the reviewers for insightful comments.
964
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBPedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In Proceedings of EMNLP.
Steven Bird and Edward Loper. 2004. NLTK: The Nat-
ural Language Toolkit. In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of ACL.
David L Chen and Raymond J Mooney. 2011. Learn-
ing to Interpret Natural Language Navigation In-
structions from Observations. In AAAI, volume 2,
pages 1?2.
J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012. Finding needles
in the haystack: Search and candidate generation.
IBM Journal of Research and Development.
J?er?ome Euzenat and Pavel Shvaiko. 2007. Ontology
matching. Springer.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-Driven Learning for Open Ques-
tion Answering. In Proceedings of ACL.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte J?org, and
Ulrich Sch?afer. 2007. Question answering from
structured knowledge sources. Journal of Applied
Logic, 5(1):20?48.
Freebase. 2013a. Freebase Search API.
https://developers.google.com/freebase/v1/search-
overview.
Freebase. 2013b. Freebase Topic API.
https://developers.google.com/freebase/v1/topic-
overview.
Evgeniy Gabrilovich, Michael Ringgaard, , and Amar-
nag Subramanya. 2013. FACC1: Freebase anno-
tation of ClueWeb corpora, Version 1 (Release date
2013-06-26, Format version 1, Correction level 0).
http://lemurproject.org/clueweb09/FACC1/, June.
Bert F Green Jr, Alice K Wolf, Carol Chomsky, and
Kenneth Laughery. 1961. Baseball: an automatic
question-answerer. In Papers presented at the May
9-11, 1961, western joint IRE-AIEE-ACM computer
conference, pages 219?224. ACM.
Jerry R Hobbs. 1985. Ontological promiscuity. In
Proceedings of ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World Wide
Web, pages 229?232. ACM.
Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian tree
transducers. In Proceedings of ACL.
Rohit J Kate and Raymond J Mooney. 2006. Using
string-kernels for learning semantic parsers. In Pro-
ceedings of ACL.
Tibor Kiss and Jan Strunk. 2006. Unsupervised mul-
tilingual sentence boundary detection. Computa-
tional Linguistics, 32(4):485?525.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Jayant Krishnamurthy and Tom M Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of EMNLP-CoNLL.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP, pages
1223?1233.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of EMNLP.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling Semantic Parsers with
On-the-fly Ontology Matching. In Proceedings of
EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein.
2011. Learning Dependency-Based Compositional
Semantics. In Proceedings of ACL.
Thomas Lin, Oren Etzioni, et al 2012. Entity Linking
at Web Scale. In Proceedings of Knowledge Extrac-
tion Workshop (AKBC-WEKEX), pages 84?88.
965
Vanessa Lopez, Michele Pasin, and Enrico Motta.
2005. Aqualog: An ontology-portable question an-
swering system for the semantic web. In The Seman-
tic Web: Research and Applications, pages 546?562.
Springer.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Naoaki Okazaki. 2009. Classias: a collection of
machine-learning algorithms for classification.
Dave Orr, Amar Subramanya, Evgeniy Gabrilovich,
and Michael Ringgaard. 2013. 11 billion
clues in 800 million documents: A web re-
search corpus annotated with freebase concepts.
http://googleresearch.blogspot.com/2013/07/11-
billion-clues-in-800-million.html, July.
Erhard Rahm and Philip A Bernstein. 2001. A survey
of approaches to automatic schema matching. the
VLDB Journal, 10(4):334?350.
Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,
and S?oren Auer. 2013. Question answering on in-
terlinked data. In Proceedings of WWW.
Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001, pages 466?477. Springer.
Christina Unger, Lorenz B?uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over RDF data. In Proceedings of the
21st international conference on World Wide Web.
Yuk Wah Wong and Raymond J Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of ACL.
William A Woods. 1977. Lunar rocks in natural en-
glish: Explorations in natural language question an-
swering. Linguistic structures processing, 5:521?
569.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In Proceedings of EMNLP.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. Uncertainty in Artificial Intelligence
(UAI).
Luke S Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of ACL-
CoNLL.
966
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1177?1187,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Low-Resource Semantic Role Labeling
Matthew R. Gormley
1
Margaret Mitchell
2
Benjamin Van Durme
1
Mark Dredze
1
1
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD 21211
2
Microsoft Research
Redmond, WA 98052
mrg@cs.jhu.edu | memitc@microsoft.com | vandurme@cs.jhu.edu | mdredze@cs.jhu.edu
Abstract
We explore the extent to which high-
resource manual annotations such as tree-
banks are necessary for the task of se-
mantic role labeling (SRL). We examine
how performance changes without syntac-
tic supervision, comparing both joint and
pipelined methods to induce latent syn-
tax. This work highlights a new applica-
tion of unsupervised grammar induction
and demonstrates several approaches to
SRL in the absence of supervised syntax.
Our best models obtain competitive results
in the high-resource setting and state-of-
the-art results in the low resource setting,
reaching 72.48% F1 averaged across lan-
guages. We release our code for this work
along with a larger toolkit for specifying
arbitrary graphical structure.
1
1 Introduction
The goal of semantic role labeling (SRL) is to
identify predicates and arguments and label their
semantic contribution in a sentence. Such labeling
defines who did what to whom, when, where and
how. For example, in the sentence ?The kids ran
the marathon?, ran assigns a role to kids to denote
that they are the runners; and a role to marathon to
denote that it is the race course.
Models for SRL have increasingly come to rely
on an array of NLP tools (e.g., parsers, lem-
matizers) in order to obtain state-of-the-art re-
sults (Bj?orkelund et al, 2009; Zhao et al, 2009).
Each tool is typically trained on hand-annotated
data, thus placing SRL at the end of a very high-
resource NLP pipeline. However, richly annotated
data such as that provided in parsing treebanks is
expensive to produce, and may be tied to specific
domains (e.g., newswire). Many languages do
1
http://www.cs.jhu.edu/
?
mrg/software/
not have such supervised resources (low-resource
languages), which makes exploring SRL cross-
linguistically difficult.
The problem of SRL for low-resource lan-
guages is an important one to solve, as solutions
pave the way for a wide range of applications: Ac-
curate identification of the semantic roles of enti-
ties is a critical step for any application sensitive to
semantics, from information retrieval to machine
translation to question answering.
In this work, we explore models that minimize
the need for high-resource supervision. We ex-
amine approaches in a joint setting where we
marginalize over latent syntax to find the optimal
semantic role assignment; and a pipeline setting
where we first induce an unsupervised grammar.
We find that the joint approach is a viable alterna-
tive for making reasonable semantic role predic-
tions, outperforming the pipeline models. These
models can be effectively trained with access to
only SRL annotations, and mark a state-of-the-art
contribution for low-resource SRL.
To better understand the effect of the low-
resource grammars and features used in these
models, we further include comparisons with (1)
models that use higher-resource versions of the
same features; (2) state-of-the-art high resource
models; and (3) previous work on low-resource
grammar induction. In sum, this paper makes
several experimental and modeling contributions,
summarized below.
Experimental contributions:
? Comparison of pipeline and joint models for
SRL.
? Subtractive experiments that consider the re-
moval of supervised data.
? Analysis of the induced grammars in un-
supervised, distantly-supervised, and joint
training settings.
1177
Modeling contributions:
? Simpler joint CRF for syntactic and semantic
dependency parsing than previously reported.
? New application of unsupervised grammar
induction: low-resource SRL.
? Constrained grammar induction using SRL
for distant-supervision.
? Use of Brown clusters in place of POS tags
for low-resource SRL.
The pipeline models are introduced in ? 3.1 and
jointly-trained models for syntactic and semantic
dependencies (similar in form to Naradowsky et
al. (2012)) are introduced in ? 3.2. In the pipeline
models, we develop a novel approach to unsu-
pervised grammar induction and explore perfor-
mance using SRL as distant supervision. The joint
models use a non-loopy conditional random field
(CRF) with a global factor constraining latent syn-
tactic edge variables to form a tree. Efficient exact
marginal inference is possible by embedding a dy-
namic programming algorithm within belief prop-
agation as in Smith and Eisner (2008).
Even at the expense of no dependency path fea-
tures, the joint models best pipeline-trained mod-
els for state-of-the-art performance in the low-
resource setting (? 4.4). When the models have ac-
cess to observed syntactic trees, they achieve near
state-of-the-art accuracy in the high-resource set-
ting on some languages (? 4.3).
Examining the learning curve of the joint and
pipeline models in two languages demonstrates
that a small number of labeled SRL examples may
be essential for good end-task performance, but
that the choice of a good model for grammar in-
duction has an even greater impact.
2 Related Work
Our work builds upon research in both seman-
tic role labeling and unsupervised grammar in-
duction (Klein and Manning, 2004; Spitkovsky
et al, 2010a). Previous related approaches to se-
mantic role labeling include joint classification of
semantic arguments (Toutanova et al, 2005; Jo-
hansson and Nugues, 2008), latent syntax induc-
tion (Boxwell et al, 2011; Naradowsky et al,
2012), and feature engineering for SRL (Zhao et
al., 2009; Bj?orkelund et al, 2009).
Toutanova et al (2005) introduced one of
the first joint approaches for SRL and demon-
strated that a model that scores the full predicate-
argument structure of a parse tree could lead to
significant error reduction over independent clas-
sifiers for each predicate-argument relation.
Johansson and Nugues (2008) and Llu??s et al
(2013) extend this idea by coupling predictions of
a dependency parser with predictions from a se-
mantic role labeler. In the model from Johans-
son and Nugues (2008), the outputs from an SRL
pipeline are reranked based on the full predicate-
argument structure that they form. The candidate
set of syntactic-semantic structures is reranked us-
ing the probability of the syntactic tree and seman-
tic structure. Llu??s et al (2013) use a joint arc-
factored model that predicts full syntactic paths
along with predicate-argument structures via dual
decomposition.
Boxwell et al (2011) and Naradowsky et al
(2012) observe that syntax may be treated as la-
tent when a treebank is not available. Boxwell
et al (2011) describe a method for training a se-
mantic role labeler by extracting features from a
packed CCG parse chart, where the parse weights
are given by a simple ruleset. Naradowsky et
al. (2012) marginalize over latent syntactic depen-
dency parses.
Both Boxwell et al (2011) and Naradowsky
et al (2012) suggest methods for SRL without
supervised syntax, however, their features come
largely from supervised resources. Even in their
lowest resource setting, Boxwell et al (2011) re-
quire an oracle CCG tag dictionary extracted from
a treebank. Naradowsky et al (2012) limit their
exploration to a small set of basic features, and
included high-resource supervision in the form
of lemmas, POS tags, and morphology available
from the CoNLL 2009 data.
There has not yet been a comparison of tech-
niques for SRL that do not rely on a syntactic
treebank, and no exploration of probabilistic mod-
els for unsupervised grammar induction within an
SRL pipeline that we have been able to find.
Related work for the unsupervised learning of
dependency structures separately from semantic
roles primarily comes from Klein and Manning
(2004), who introduced the Dependency Model
with Valence (DMV). This is a robust generative
model that uses a head-outward process over word
classes, where heads generate arguments.
Spitkovsky et al (2010a) show that Viterbi
(hard) EM training of the DMV with simple uni-
form initialization of the model parameters yields
higher accuracy models than standard soft-EM
1178
  
ParsingModel SemanticDependencyModelCorpusText
Text LabeledWith SemanticRoles
Train Time, Constrained Grammar Induction:Observed Constraints
Figure 1: Pipeline approach to SRL. In this sim-
ple pipeline, the first stage syntactically parses the
corpus, and the second stage predicts semantic
predicate-argument structure for each sentence us-
ing the labels of the first stage as features. In our
low-resource pipelines, we assume that the syntac-
tic parser is given no labeled parses?however, it
may optionally utilize the semantic parses as dis-
tant supervision. Our experiments also consider
?longer? pipelines that include earlier stages: a
morphological analyzer, POS tagger, lemmatizer.
training. In Viterbi EM, the E-step finds the max-
imum likelihood corpus parse given the current
model parameters. The M-step then finds the
maximum likelihood parameters given the corpus
parse. We utilize this approach to produce unsu-
pervised syntactic features for the SRL task.
Grammar induction work has further demon-
strated that distant supervision in the form of
ACE-style relations (Naseem and Barzilay, 2011)
or HTML markup (Spitkovsky et al, 2010b)
can lead to considerable gains. Recent work in
fully unsupervised dependency parsing has sup-
planted these methods with even higher accuracies
(Spitkovsky et al, 2013) by arranging optimiz-
ers into networks that suggest informed restarts
based on previously identified local optima. We do
not reimplement these approaches within the SRL
pipeline here, but provide comparison of these
methods against our grammar induction approach
in isolation in ? 4.5.
In both pipeline and joint models, we use fea-
tures adapted from state-of-the-art approaches to
SRL. This includes Zhao et al (2009) features,
who use feature templates from combinations
of word properties, syntactic positions including
head and children, and semantic properties; and
features from Bj?orkelund et al (2009), who utilize
features on syntactic siblings and the dependency
path concatenated with the direction of each edge.
Features are described further in ? 3.3.
3 Approaches
We consider an array of models, varying:
1. Pipeline vs. joint training (Figures 1 and 2)
2. Types of supervision
3. The objective function at the level of syntax
3.1 Unsupervised Syntax in the Pipeline
Typical SRL systems are trained following a
pipeline where the first component is trained on
supervised data, and each subsequent component
is trained using the 1-best output of the previous
components. A typical pipeline consists of a POS
tagger, dependency parser, and semantic role la-
beler. In this section, we introduce pipelines that
remove the need for a supervised tagger and parser
by training in an unsupervised and distantly super-
vised fashion.
Brown Clusters We use fully unsupervised
Brown clusters (Brown et al, 1992) in place of
POS tags. Brown clusters have been used to good
effect for various NLP tasks such as named entity
recognition (Miller et al, 2004) and dependency
parsing (Koo et al, 2008; Spitkovsky et al, 2011).
The clusters are formed by a greedy hierachi-
cal clustering algorithm that finds an assignment
of words to classes by maximizing the likelihood
of the training data under a latent-class bigram
model. Each word type is assigned to a fine-
grained cluster at a leaf of the hierarchy of clusters.
Each cluster can be uniquely identified by the path
from the root cluster to that leaf. Representing this
path as a bit-string (with 1 indicating a left and 0
indicating a right child) allows a simple coarsen-
ing of the clusters by truncating the bit-strings. We
train 1000 Brown clusters for each of the CoNLL-
2009 languages on Wikipedia text.
2
Unsupervised Grammar Induction Our first
method for grammar induction is fully unsuper-
vised Viterbi EM training of the Dependency
Model with Valence (DMV) (Klein and Manning,
2004), with uniform initialization of the model pa-
rameters. We define the DMV such that it gener-
ates sequences of word classes: either POS tags
or Brown clusters as in Spitkovsky et al (2011).
The DMV is a simple generative model for pro-
jective dependency trees. Children are generated
recursively for each node. Conditioned on the par-
ent class, the direction (right or left), and the cur-
rent valence (first child or not), a coin is flipped to
decide whether to generate another child; the dis-
tribution over child classes is conditioned on only
the parent class and direction.
2
The Wikipedia text was tokenized for Polyglot (Al-Rfou?
et al, 2013): http://bit.ly/embeddings
1179
Constrained Grammar Induction Our second
method, which we will refer to as DMV+C, in-
duces grammar in a distantly supervised fashion
by using a constrained parser in the E-step of
Viterbi EM. Since the parser is part of a pipeline,
we constrain it to respect the downstream SRL an-
notations during training. At test time, the parser
is unconstrained.
Dependency-based semantic role labeling can
be described as a simple structured prediction
problem: the predicted structure is a labeled di-
rected graph, where nodes correspond to words
in the sentence. Each directed edge indicates that
there is a predicate-argument relationship between
the two words; the parent is the predicate and the
child the argument. The label on the edge indi-
cates the type of semantic relationship. Unlike
syntactic dependency parsing, the graph is not re-
quired to be a tree, nor even a connected graph.
Self-loops and crossing arcs are permitted.
The constrained syntactic DMV parser treats
the semantic graph as observed, and constrains the
syntactic parent to be chosen from one of the se-
mantic parents, if there are any. In some cases,
imposing this constraint would not permit any pro-
jective dependency parses?in this case, we ignore
the semantic constraint for that sentence. We parse
with the CKY algorithm (Younger, 1967; Aho and
Ullman, 1972) by utilizing a PCFG corresponding
to the DMV (Cohn et al, 2010). Each chart cell al-
lows only non-terminals compatible with the con-
strained sets. This can be viewed as a variation of
Pereira and Schabes (1992).
Semantic Dependency Model As described
above, semantic role labeling can be cast as a
structured prediction problem where the structure
is a labeled semantic dependency graph. We de-
fine a conditional random field (CRF) (Lafferty et
al., 2001) for this task. Because each word in a
sentence may be in a semantic relationship with
any other word (including itself), a sentence of
length n has n
2
possible edges. We define a single
L+1-ary variable for each edge, whose value can
be any of L semantic labels or a special label indi-
cating there is no predicate-argument relationship
between the two words. In this way, we jointly
perform identification (determining whether a se-
mantic relationship exists) and classification (de-
termining the semantic label). This use of an L+1-
ary variable is in contrast to the model of Narad-
owsky et al (2012), which used a more complex
  
DEPTREE
Dep
1,1
Role
1,1
Role
1,2
Role
1,3
Role
n,n
Dep
1,2
Dep
1,3
Dep
n,n ...
 ...
Figure 2: Factor graph for the joint syntac-
tic/semantic dependency parsing model.
set of binary variables and required a constraint
factor permitting AT-MOST-ONE. We include one
unary factor for each variable.
We optionally include additional variables that
perform word sense disambiguation for each pred-
icate. Each has a unary factor and is completely
disconnected from the semantic edge (similar to
Naradowsky et al (2012)). These variables range
over all the predicate senses observed in the train-
ing data for the lemma of that predicate.
3.2 Joint Syntactic and Semantic Parsing
Model
In Section 3.1, we introduced pipeline-trained
models for SRL, which used grammar induction
to predict unlabeled syntactic parses. In this sec-
tion, we define a simple model for joint syntactic
and semantic dependency parsing.
This model extends the CRF model in Section
3.1 to include the projective syntactic dependency
parse for a sentence. This is done by includ-
ing an additional n
2
binary variables that indicate
whether or not a directed syntactic dependency
edge exists between a pair of words in the sen-
tence. Unlike the semantic dependencies, these
syntactic variables must be coupled so that they
produce a projective dependency parse; this re-
quires an additional global constraint factor to en-
sure that this is the case (Smith and Eisner, 2008).
The constraint factor touches all n
2
syntactic-edge
variables, and multiplies in 1.0 if they form a pro-
jective dependency parse, and 0.0 otherwise. We
couple each syntactic edge variable to its semantic
edge variable with a binary factor. Figure 2 shows
the factor graph for this joint model.
Note that our factor graph does not contain any
loops, thereby permitting efficient exact marginal
inference just as in Naradowsky et al (2012). We
1180
Property Possible values
1 word form all word forms
2 lower case word form all lower-case forms
3 5-char word form prefixes all 5-char form prefixes
4 capitalization True, False
5 top-800 word form top-800 word forms
6 brown cluster 000, 1100, 010110001, ...
7 brown cluster, length 5 length 5 prefixes of brown clusters
8 lemma all word lemmas
9 POS tag NNP, CD, JJ, DT, ...
10 morphological features Gender, Case, Number, ...
(different across languages)
11 dependency label SBJ, NMOD, LOC, ...
12 edge direction Up, Down
Table 1: Word and edge properties in templates.
i, i-1, i+1 noFarChildren(w
i
) linePath(w
p
, w
c
)
parent(w
i
) rightNearSib(w
i
) depPath(w
p
, w
c
)
allChildren(w
i
) leftNearSib(w
i
) depPath(w
p
, w
lca
)
rightNearChild(w
i
) firstVSupp(w
i
) depPath(w
c
, w
lca
)
rightFarChild(w
i
) lastVSupp(w
i
) depPath(w
lca
, w
root
)
leftNearChild(w
i
) firstNSupp(w
i
)
leftFarChild(w
i
) lastNSupp(w
i
)
Table 2: Word positions used in templates. Based
on current word position (i), positions related to
current word w
i
, possible parent, child (w
p
, w
c
),
lowest common ancestor between parent/child
(w
lca
), and syntactic root (w
root
).
train our CRF models by maximizing conditional
log-likelihood using stochastic gradient descent
with an adaptive learning rate (AdaGrad) (Duchi
et al, 2011) over mini-batches.
The unary and binary factors are defined with
exponential family potentials. In the next section,
we consider binary features of the observations
(the sentence and labels from previous pipeline
stages) which are conjoined with the state of the
variables in the factor.
3.3 Features for CRF Models
Our feature design stems from two key ideas.
First, for SRL, it has been observed that fea-
ture bigrams (the concatenation of simple fea-
tures such as a predicate?s POS tag and an ar-
gument?s word) are important for state-of-the-art
(Zhao et al, 2009; Bj?orkelund et al, 2009). Sec-
ond, for syntactic dependency parsing, combining
Brown cluster features with word forms or POS
tags yields high accuracy even with little training
data (Koo et al, 2008).
We create binary indicator features for each
model using feature templates. Our feature tem-
plate definitions build from those used by the top
performing systems in the CoNLL-2009 Shared
Task, Zhao et al (2009) and Bj?orkelund et al
(2009) and from features in syntactic dependency
parsing (McDonald et al, 2005; Koo et al, 2008).
Template Possible values
relative position before, after, on
distance, continuity Z
+
binned distance > 2, 5, 10, 20, 30, or 40
geneological relationship parent, child, ancestor, descendant
path-grams the NN went
Table 3: Additional standalone templates.
Template Creation Feature templates are de-
fined over triples of ?property, positions, order?.
Properties, listed in Table 1, are extracted from
word positions within the sentence, shown in Ta-
ble 2. Single positions for a word w
i
include
its syntactic parent, its leftmost farthest child
(leftFarChild), its rightmost nearest sibling (rightNearSib),
etc. Following Zhao et al (2009), we include the
notion of verb and noun supports and sections of
the dependency path. Also following Zhao et al
(2009), properties from a set of positions can be
put together in three possible orders: as the given
sequence, as a sorted list of unique strings, and re-
moving all duplicated neighbored strings. We con-
sider both template unigrams and bigrams, com-
bining two templates in sequence.
Additional templates we include are the relative
position (Bj?orkelund et al, 2009), geneological re-
lationship, distance (Zhao et al, 2009), and binned
distance (Koo et al, 2008) between two words in
the path. From Llu??s et al (2013), we use 1, 2, 3-
gram path features of words/POS tags (path-grams),
and the number of non-consecutive token pairs in
a predicate-argument path (continuity).
3.4 Feature Selection
Constructing all feature template unigrams and bi-
grams would yield an unwieldy number of fea-
tures. We therefore determine the top N template
bigrams for a dataset and factor a according to an
information gain measure (Martins et al, 2011):
IG
a,m
=
?
f?T
m
?
x
a
p(f, x
a
) log
2
p(f, x
a
)
p(f)p(x
a
)
where T
m
is the mth feature template, f is a par-
ticular instantiation of that template, and x
a
is an
assignment to the variables in factor a. The proba-
bilities are empirical estimates computed from the
training data. This is simply the mutual informa-
tion of the feature template instantiation with the
variable assignment.
This filtering approach was treated as a sim-
ple baseline in Martins et al (2011) to contrast
with increasingly popular gradient based regular-
ization approaches. Unlike the gradient based ap-
1181
proaches, this filtering approach easily scales to
many features since we can decompose the mem-
ory usage over feature templates.
As an additional speedup, we reduce the dimen-
sionality of our feature space to 1 million for each
clique using a common trick referred to as fea-
ture hashing (Weinberger et al, 2009): we map
each feature instantiation to an integer using a hash
function
3
modulo the desired dimentionality.
4 Experiments
We are interested in the effects of varied super-
vision using pipeline and joint training for SRL.
To compare to prior work (i.e., submissions to the
CoNLL-2009 Shared Task), we also consider the
joint task of semantic role labeling and predicate
sense disambiguation. Our experiments are sub-
tractive, beginning with all supervision available
and then successively removing (a) dependency
syntax, (b) morphological features, (c) POS tags,
and (d) lemmas. Dependency syntax is the most
expensive and difficult to obtain of these various
forms of supervision. We explore the importance
of both the labels and structure, and what quantity
of supervision is useful.
4.1 Data
The CoNLL-2009 Shared Task (Haji?c et al, 2009)
dataset contains POS tags, lemmas, morpholog-
ical features, syntactic dependencies, predicate
senses, and semantic roles annotations for 7 lan-
guages: Catalan, Chinese, Czech, English, Ger-
man, Japanese,
4
Spanish. The CoNLL-2005 and
-2008 Shared Task datasets provide English SRL
annotation, and for cross dataset comparability we
consider only verbal predicates (more details in
? 4.4). To compare with prior approaches that use
semantic supervision for grammar induction, we
utilize Section 23 of the WSJ portion of the Penn
Treebank (Marcus et al, 1993).
4.2 Feature Template Sets
Our primary feature set IG
C
consists of 127 tem-
plate unigrams that emphasize coarse properties
(i.e., properties 7, 9, and 11 in Table 1). We also
explore the 31 template unigrams
5
IG
B
described
3
To reduce hash collisions, We use MurmurHash v3
https://code.google.com/p/smhasher.
4
We do not report results on Japanese as that data was
only made freely available to researchers that competed in
CoNLL 2009.
5
Because we do not include a binary factor between pred-
icate sense and semantic role, we do not include sense as a
by Bj?orkelund et al (2009). Each of IG
C
and IG
B
also include 32 template bigrams selected by in-
formation gain on 1000 sentences?we select a
different set of template bigrams for each dataset.
We compare against the language-specific fea-
ture sets detailed in the literature on high-resource
top-performing SRL systems: From Bj?orkelund et
al. (2009), these are feature sets for German, En-
glish, Spanish and Chinese, obtained by weeks of
forward selection (B
de,en,es,zh
); and from Zhao et
al. (2009), these are features for Catalan Z
ca
.
6
4.3 High-resource SRL
We first compare our models trained as a pipeline,
using all available supervision (syntax, morphol-
ogy, POS tags, lemmas) from the CoNLL-2009
data. Table 4(a) shows the results of our model
with gold syntax and a richer feature set than
that of Naradowsky et al (2012), which only
looked at whether a syntactic dependency edge
was present. This highlights an important advan-
tage of the pipeline trained model: the features can
consider any part of the syntax (e.g., arbitrary sub-
trees), whereas the joint model is limited to those
features over which it can efficiently marginalize
(e.g., short dependency paths). This holds true
even in the pipeline setting where no syntactic su-
pervision is available.
Table 4(b) contrasts our high-resource results
for the task of SRL and sense disambiguation
with the top systems in the CoNLL-2009 Shared
Task, giving further insight into the performance
of the simple information gain feature selection
technique. With supervised syntax, our sim-
ple information gain feature selection technique
(? 3.4) performs admirably. However, the orig-
inal unigram Bj?orkelund features (B
de,en,es,zh
),
which were tuned for a high-resource model, ob-
tain higher F1 than our information gain set us-
ing the same features in unigram and bigram tem-
plates (IG
B
). This suggests that further work on
feature selection may improve the results. We
find that IG
B
obtain higher F1 than the original
Bj?orkelund feature sets (B
de,en,es,zh
) in the low-
resource pipeline setting with constrained gram-
mar induction (DMV+C).
feature for argument prediction.
6
This covers all CoNLL languages but Czech, where fea-
ture sets were not made publicly available in either work. In
Czech, we disallowed template bigrams involving path-grams.
1182
(a)
(b)
(c)
SRL Approach Feature Set Dep. Parser Avg. ca cs de en es zh
Pipeline IG
C
Gold 84.98 84.97 87.65 79.14 86.54 84.22 87.35
Pipeline IG
B
Gold 84.74 85.15 86.64 79.50 85.77 84.40 86.95
Naradowsky et al (2012) Gold 72.73 69.59 74.84 66.49 78.55 68.93 77.97
Bj?orkelund et al (2009) Supervised 81.55 80.01 85.41 79.71 85.63 79.91 78.60
Zhao et al (2009) Supervised 80.85 80.32 85.19 75.99 85.44 80.46 77.72
Pipeline IG
C
Supervised 78.03 76.24 83.34 74.19 81.96 76.12 76.35
Pipeline Z
ca
Supervised *77.62 77.62 ? ? ? ? ?
Pipeline B
de,en,es,zh
Supervised *76.49 ? ? 72.17 81.15 76.65 75.99
Pipeline IG
B
Supervised 75.68 74.59 81.61 69.08 78.86 74.51 75.44
Joint IG
C
Marginalized 72.48 71.35 81.03 65.15 76.16 71.03 70.14
Joint IG
B
Marginalized 72.40 71.55 80.04 64.80 75.57 71.21 71.21
Naradowsky et al (2012) Marginalized 71.27 67.99 73.16 67.26 76.12 66.74 76.32
Pipeline IG
C
DMV+C (bc) 70.08 68.21 79.63 62.25 73.81 68.73 67.86
Pipeline Z
ca
DMV+C (bc) *69.67 69.67 ? ? ? ? ?
Pipeline IG
C
DMV (bc) 69.26 68.04 79.58 58.47 74.78 68.36 66.35
Pipeline IG
B
DMV (bc) 66.81 63.31 77.38 59.91 72.02 65.96 62.28
Pipeline IG
B
DMV+C (bc) 65.61 61.89 77.48 58.97 69.11 63.31 62.92
Pipeline B
de,en,es,zh
DMV+C (bc) *63.06 ? ? 57.75 68.32 63.70 62.45
Table 4: Test F1 for SRL and sense disambiguation on CoNLL?09 in high-resource and low-resource
settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are
ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
*Indicates partial averages for the language-specific feature sets (Z
ca
and B
de,en,es,zh
), for which we show results only on the
languages for which the sets were publicly available.
train
test
2008
heads
2005
spans
2005
spans
(oracle
tree)
X PRY?08
2
0
0
5
s
p
a
n
s
84.32 79.44
 B?11 (tdc) ? 71.5
 B?11 (td) ? 65.0
X JN?08
2
0
0
8
h
e
a
d
s
85.93 79.90
 Joint, IG
C
72.9 35.0 72.0
 Joint, IG
B
67.3 37.8 67.1
Table 5: F1 for SRL approaches (without sense
disambiguation) in matched and mismatched
train/test settings for CoNLL 2005 span and 2008
head supervision. We contrast low-resource ()
and high-resource settings (X), where latter uses a
treebank. See ? 4.4 for caveats to this comparison.
4.4 Low-Resource SRL
CoNLL-2009 Table 4(c) includes results for our
low-resource approaches and Naradowsky et al
(2012) on predicting semantic roles as well as
sense. In the low-resource setting of the CoNLL-
2009 Shared task without syntactic supervision,
our joint model (Joint) with marginalized syntax
obtains state-of-the-art results with features IG
C
described in ? 4.2. This model outperforms prior
work (Naradowsky et al, 2012) and our pipeline
model (Pipeline) with contrained (DMV+C) and
unconstrained grammar induction (DMV) trained
on brown clusters (bc).
In the low-resource setting, training and decod-
ing times for the pipeline and joint methods are
similar as computation time tends to be dominated
by feature extraction.
These results begin to answer a key research
question in this work: The joint models outper-
form the pipeline models in the low-resource set-
ting. This holds even when using the same feature
selection process. Further, the best-performing
low-resource features found in this work are those
based on coarse feature templates and selected
by information gain. Templates for these fea-
tures generalize well to the high-resource setting.
However, analysis of the induced grammars in
the pipeline setting suggests that the book is not
closed on the issue. We return to this in ? 4.5.
CoNLL-2008, -2005 To finish out comparisons
with state-of-the-art SRL, we contrast our ap-
proach with that of Boxwell et al (2011), who
evaluate on SRL in isolation (without sense disam-
biguation, as in CoNLL-2009). They report results
on Prop-CCGbank (Boxwell and White, 2008),
which uses the same training/testing splits as the
CoNLL-2005 Shared Task. Their results are there-
fore loosely
7
comparable to results on the CoNLL-
2005 dataset, which we can compare here.
There is an additional complication in com-
paring SRL approaches directly: The CoNLL-
2005 dataset defines arguments as spans instead of
7
The comparison is imperfect for two reasons: first, the
CCGBank contains only 99.44% of the original PTB sen-
tences (Hockenmaier and Steedman, 2007); second, because
PropBank was annotated over CFGs, after converting to CCG
only 99.977% of the argument spans were exact matches
(Boxwell and White, 2008). However, this comparison was
adopted by Boxwell et al (2011), so we use it here.
1183
heads, which runs counter to our head-based syn-
tactic representation. This creates a mismatched
train/test scenario: we must train our model to pre-
dict argument heads, but then test on our models
ability to predict argument spans.
8
We therefore
train our models on the CoNLL-2008 argument
heads,
9
and post-process and convert from heads
to spans using the conversion algorithm available
from Johansson and Nugues (2008).
10
The heads
are either from an MBR tree or an oracle tree. This
gives Boxwell et al (2011) the advantage, since
our syntactic dependency parses are optimized to
pick out semantic argument heads, not spans.
Table 5 presents our results. Boxwell et al
(2011) (B?11) uses additional supervision in the
form of a CCG tag dictionary derived from su-
pervised data with (tdc) and without (tc) a cut-
off. Our model does very poorly on the ?05 span-
based evaluation because the constituent bracket-
ing of the marginalized trees are inaccurate. This
is elucidated by instead evaluating on the ora-
cle spans, where our F1 scores are higher than
Boxwell et al (2011). We also contrast with rela-
vant high-resource methods with span/head con-
versions from Johansson and Nugues (2008): Pun-
yakanok et al (2008) (PRY?08) and Johansson and
Nugues (2008) (JN?08).
Subtractive Study In our subsequent experi-
ments, we study the effectiveness of our models
as the available supervision is decreased. We in-
crementally remove dependency syntax, morpho-
logical features, POS tags, then lemmas. For these
experiments, we utilize the coarse-grained feature
set (IG
C
), which includes Brown clusters.
Across languages, we find the largest drop in
F1 when we remove POS tags; and we find a
gain in F1 when we remove lemmas. This indi-
cates that lemmas, which are a high-resource an-
notation, may not provide a significant benefit for
this task. The effect of removing morphological
features is different across languages, with little
change in performance for Catalan and Spanish,
8
We were unable to obtain the system output of Boxwell
et al (2011) in order to convert their spans to dependencies
and evaluate the other mismatched train/test setting.
9
CoNLL-2005, -2008, and -2009 were derived from Prop-
Bank and share the same source text; -2008 and -2009 use
argument heads.
10
Specifically, we use their Algorithm 2, which produces
the span dominated by each argument, with special handling
of the case when the argument head dominates that of the
predicate. Also following Johansson and Nugues (2008), we
recover the ?05 sentences missing from the ?08 evaluation set.
Rem #FT ca de es
? 127+32 74.46 72.62 74.23
Dep 40+32 67.43 64.24 67.18
Mor 30+32 67.84 59.78 66.94
POS 23+32 64.40 54.68 62.71
Lem 21+32 64.85 54.89 63.80
Table 6: Subtractive experiments. Each row con-
tains the F1 for SRL only (without sense disam-
biguation) where the supervision type of that row
and all above it have been removed. Removed su-
pervision types (Rem) are: syntactic dependencies
(Dep), morphology (Mor), POS tags (POS), and
lemmas (Lem). #FT indicates the number of fea-
ture templates used (unigrams+bigrams).
20
30
40
50
60
70
0 20000 40000 60000
Number of Training Sentences
Lab
eled
 F1
Language / Dependency Parser
Catalan / Marginalized
Catalan / DMV+C
German / Marginalized
German / DMV+C
Figure 3: Learning curve for semantic dependency
supervision in Catalan and German. F1 of SRL
only (without sense disambiguation) shown as the
number of training sentences is increased.
but a drop in performance for German. This may
reflect a difference between the languages, or may
reflect the difference between the annotation of the
languages: both the Catalan and Spanish data orig-
inated from the Ancora project,
11
while the Ger-
man data came from another source.
Figure 3 contains the learning curve for SRL su-
pervision in our lowest resource setting for two
example languages, Catalan and German. This
shows how F1 of SRL changes as we adjust
the number of training examples. We find that
the joint training approach to grammar induction
yields consistently higher SRL performance than
its distantly supervised counterpart.
4.5 Analysis of Grammar Induction
Table 7 shows grammar induction accuracy in
low-resource settings. We find that the gap be-
tween the supervised parser and the unsupervised
methods is quite large, despite the reasonable ac-
curacy both methods achieve for the SRL end task.
11
http://clic.ub.edu/corpus/ancora
1184
Dependency
Parser
Avg. ca cs de en es zh
Supervised* 87.1 89.4 85.3 89.6 88.4 89.2 80.7
DMV (pos) 30.2 45.3 22.7 20.9 32.9 41.9 17.2
DMV (bc) 22.1 18.8 32.8 19.6 22.4 20.5 18.6
DMV+C (pos) 37.5 50.2 34.9 21.5 36.9 49.8 32.0
DMV+C (bc) 40.2 46.3 37.5 28.7 40.6 50.4 37.5
Marginal, IG
C
43.8 50.3 45.8 27.2 44.2 46.3 48.5
Marginal, IG
B
50.2 52.4 43.4 41.3 52.6 55.2 56.2
Table 7: Unlabeled directed dependency accuracy
on CoNLL?09 test set in low-resource settings.
DMV models are trained on either POS tags (pos)
or Brown clusters (bc). *Indicates the supervised parser
outputs provided by the CoNLL?09 Shared Task.
WSJ
?
Distant
Supervision
SAJM?10 44.8 none
SAJ?13 64.4 none
SJA?10 50.4 HTML
NB?11 59.4 ACE05
DMV (bc) 24.8 none
DMV+C (bc) 44.8 SRL
Marginalized, IG
C
48.8 SRL
Marginalized, IG
B
58.9 SRL
Table 8: Comparison of grammar induction ap-
proaches. We contrast the DMV trained with
Viterbi EM+uniform initialization (DMV), our
constrained DMV (DMV+C), and our model?s
MBR decoding of latent syntax (Marginalized)
with other recent work: Spitkovsky et al (2010a)
(SAJM?10), Spitkovsky et al (2010b) (SJA?10),
Naseem and Barzilay (2011) (NB?11), and the CS
model of Spitkovsky et al (2013) (SAJ?13).
This suggests that refining the low-resource gram-
mar induction methods may lead to gains in SRL.
Interestingly, the marginalized grammars best
the DMV grammar induction method; however,
this difference is less pronounced when the DMV
is constrained using SRL labels as distant super-
vision. This could indicate that a better model for
grammar induction would result in better perfor-
mance for SRL. We therefore turn to an analysis of
other approaches to grammar induction in Table 8,
evaluated on the Penn Treebank. We contrast with
methods using distant supervision (Naseem and
Barzilay, 2011; Spitkovsky et al, 2010b) and fully
unsupervised dependency parsing (Spitkovsky et
al., 2013). Following prior work, we exclude
punctuation from evaluation and convert the con-
stituency trees to dependencies.
12
The approach from Spitkovsky et al (2013)
12
Naseem and Barzilay (2011) and our results use the
Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et
al. (2010b; 2013) use Collins (1999) head percolation rules.
(SAJ?13) outperforms all other approaches, in-
cluding our marginalized settings. We therefore
may be able to achieve further gains in the pipeline
model by considering better models of latent syn-
tax, or better search techniques that break out
of local optima. Similarly, improving the non-
convex optimization of our latent-variable CRF
(Marginalized) may offer further gains.
5 Discussion and Future Work
We have compared various approaches for low-
resource semantic role labeling at the state-of-the-
art level. We find that we can outperform prior
work in the low-resource setting by coupling the
selection of feature templates based on informa-
tion gain with a joint model that marginalizes over
latent syntax.
We utilize unlabeled data in both generative and
discriminative models for dependency syntax and
in generative word clustering. Our discriminative
joint models treat latent syntax as a structured-
feature to be optimized for the end-task of SRL,
while our other grammar induction techniques op-
timize for unlabeled data likelihood?optionally
with distant supervision. We observe that careful
use of these unlabeled data resources can improve
performance on the end task.
Our subtractive experiments suggest that lemma
annotations, a high-resource annotation, may not
provide a large benefit for SRL. Our grammar in-
duction analysis indicates that relatively low accu-
racy can still result in reasonable SRL predictions;
still, the models do not outperform those that use
supervised syntax, and we aim to explore how well
the pipeline models in particular improve when we
apply higher accuracy unsupervised grammar in-
duction techniques.
We have utilized well studied datasets in order
to best understand the quality of our models rela-
tive to prior work. In future work, we hope to ex-
plore the effectiveness of our approaches on truly
low resource settings by using crowdsourcing to
develop semantic role datasets in other languages
and domains.
Acknowledgments We thank Richard Johans-
son, Dennis Mehay, and Stephen Boxwell for help
with data. We also thank Jason Naradowsky, Jason
Eisner, and anonymous reviewers for comments
on the paper.
1185
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling.
Prentice-Hall, Inc.
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of the 17th
Conference on Computational Natural Language
Learning (CoNLL 2013). Association for Computa-
tional Linguistics.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task. Association for Computational Linguistics.
Stephen Boxwell and Michael White. 2008. Project-
ing propbank roles onto the CCGbank. In Proceed-
ings of the International Conference on Language
Resources and Evaluation (LREC 2008). European
Language Resources Association.
Stephen Boxwell, Chris Brew, Jason Baldridge, Dennis
Mehay, and Sujith Ravi. 2011. Semantic role label-
ing without treebanks? In Proceedings of the 5th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP). Asian Federation of Natural
Language Processing.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4).
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. The
Journal of Machine Learning Research, 11.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL 2009): Shared Task. Association for
Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008). Association for Computational Lin-
guistics.
Dan Klein and Christopher Manning. 2004. Corpus-
Based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL 2004). Association for Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT. Association for
Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Ma-
chine Learning (ICML 2001). Morgan Kaufmann.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics (TACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The Penn Treebank. Com-
putational linguistics, 19(2).
Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011. Structured sparsity in struc-
tured prediction. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011). Association for Compu-
tational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005).
Association for Computational Linguistics.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Susan Dumais, Daniel
Marcu, and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings. Association for Compu-
tational Linguistics.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization of
hidden syntactic structure. In Proceedings of the
2012 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2012). Association
for Computational Linguistics.
Tahira Naseem and Regina Barzilay. 2011. Using
semantic cues to learn syntax. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI 2011). AAAI Press.
1186
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting of
the Association for Computational Linguistics (ACL
1992).
Nugues Pierre and Kalep Heiki-Jaan. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. NODALIDA 2007 Proceedings.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Juraf-
sky, and Christopher D Manning. 2010a. Viterbi
training improves unsupervised dependency parsing.
In Proceedings of the 14th Conference on Computa-
tional Natural Language Learning (CoNLL 2010).
Association for Computational Linguistics.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010b. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2010). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011). Association for Computational Lin-
guistics.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2013). Association for
Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics (ACL 2005). Association for Computational Lin-
guistics.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In L?eon
Bottou and Michael Littman, editors, Proceedings
of the 26th Annual International Conference on Ma-
chine Learning (ICML 2009). Omnipress.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n
3
. Information and
Control, 10(2).
Hai Zhao, Wenliang Chen, Chunyu Kity, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task. Association for Com-
putational Linguistics.
1187
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7?12,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Biases in Predicting the Human Language Model
Alex B. Fine
University of Illinois at Urbana-Champaign
abfine@illinois.edu
Austin F. Frank
Riot Games
aufrank@riotgames.com
T. Florian Jaeger
University of Rochester
fjaeger@bcs.rochester.edu
Benjamin Van Durme
Johns Hopkins University
vandurme@cs.jhu.edu
Abstract
We consider the prediction of three hu-
man behavioral measures ? lexical deci-
sion, word naming, and picture naming ?
through the lens of domain bias in lan-
guage modeling. Contrasting the predic-
tive ability of statistics derived from 6 dif-
ferent corpora, we find intuitive results
showing that, e.g., a British corpus over-
predicts the speed with which an Amer-
ican will react to the words ward and
duke, and that the Google n-grams over-
predicts familiarity with technology terms.
This study aims to provoke increased con-
sideration of the human language model
by NLP practitioners: biases are not lim-
ited to differences between corpora (i.e.
?train? vs. ?test?); they can exist as well
between corpora and the intended user of
the resultant technology.
1 Introduction
Computational linguists build statistical language
models for aiding in natural language processing
(NLP) tasks. Computational psycholinguists build
such models to aid in their study of human lan-
guage processing. Errors in NLP are measured
with tools like precision and recall, while errors in
psycholinguistics are defined as failures to model
a target phenomenon.
In the current study, we exploit errors of the lat-
ter variety?failure of a language model to predict
human performance?to investigate bias across
several frequently used corpora in computational
linguistics. The human data is revealing because
it trades on the fact that human language process-
ing is probability-sensitive: language processing
reflects implicit knowledge of probabilities com-
puted over linguistic units (e.g., words). For ex-
ample, the amount of time required to read a word
varies as a function of how predictable that word is
(McDonald and Shillcock, 2003). Thus, failure of
a language model to predict human performance
reveals a mismatch between the language model
and the human language model, i.e., bias.
Psycholinguists have known for some time that
the ability of a corpus to explain behavior depends
on properties of the corpus and the subjects (cf.
Balota et al (2004)). We extend that line of work
by directly analyzing and quantifying this bias,
and by linking the results to methodological con-
cerns in both NLP and psycholinguistics.
Specifically, we predict human data from
three widely used psycholinguistic experimental
paradigms?lexical decision, word naming, and
picture naming?using unigram frequency esti-
mates from Google n-grams (Brants and Franz,
2006), Switchboard (Godfrey et al, 1992), spoken
and written English portions of CELEX (Baayen
et al, 1995), and spoken and written portions
of the British National Corpus (BNC Consor-
tium, 2007). While we find comparable overall
fits of the behavioral data from all corpora un-
der consideration, our analyses also reveal spe-
cific domain biases. For example, Google n-
grams overestimates the ease with which humans
will process words related to the web (tech, code,
search, site), while the Switchboard corpus?a
collection of informal telephone conversations be-
tween strangers?overestimates how quickly hu-
mans will react to colloquialisms (heck, darn) and
backchannels (wow, right).
7
Figure 1: Pairwise correlations between log frequency es-
timates from each corpus. Histograms show distribution over
frequency values from each corpus. Lower left panels give
Pearson (top) and Spearman (bottom) correlation coefficients
and associated p-values for each pair. Upper right panels plot
correlations
2 Fitting Behavioral Data
2.1 Data
Pairwise Pearson correlation coefficients for log
frequency were computed for all corpora under
consideration. Significant correlations were found
between log frequency estimates for all pairs (Fig-
ure 1). Intuitive biases are apparent in the corre-
lations, e.g.: BNCw correlates heavily with BNCs
(0.91), but less with SWBD (0.79), while BNCs
correlates more with SWBD (0.84).
1
Corpus Size (tokens)
Google n-grams (web release) ? 1 trillion
British National Corpus (written, BNCw) ? 90 million
British National Corpus (spoken, BNCs) ? 10 million
CELEX (written, CELEXw) ? 16.6 million
CELEX (spoken, CELEXs) ? 1.3 million
Switchboard (Penn Treebank subset 3) ? 800,000
Table 1: Summary of the corpora under consideration.
2.2 Approach
We ask whether domain biases manifest as sys-
tematic errors in predicting human behavior. Log
unigram frequency estimates were derived from
each corpus and used to predict reaction times
(RTs) from three experiments employing lexical
1
BNCw and BNCs are both British, while BNCs and
SWBD are both spoken.
decision (time required by subjects to correctly
identify a string of letters as a word of English
(Balota et al, 1999)); word naming (time required
to read aloud a visually presented word (Spieler
and Balota, 1997); (Balota and Spieler, 1998));
and picture naming (time required to say a pic-
ture?s name (Bates et al, 2003)). Previous work
has shown that more frequent words lead to faster
RTs. These three measures provide a strong test
for the biases present in these corpora, as they
span written and spoken lexical comprehension
and production.
To compare the predictive strength of log fre-
quency estimates from each corpus, we fit mixed
effects regression models to the data from each
experiment. As controls, all models included (1)
mean log bigram frequency for each word, (2)
word category (noun, verb, etc.), (3) log mor-
phological family size (number of inflectional and
derivational morphological family members), (4)
number of synonyms, and (5) the first principal
component of a host of orthographic and phono-
logical features capturing neighborhood effects
(type and token counts of orthographic and phono-
logical neighbors as well as forward and backward
inconsistent words; (Baayen et al, 2006)). Mod-
els of lexical decision and word naming included
random intercepts of participant age to adjust for
differences in mean RTs between old (mean age
= 72) vs. young (mean age = 23) subjects, given
differences between younger vs. older adults? pro-
cessing speed (cf. (Ramscar et al, 2014)). (All
participants in the picture naming study were col-
lege students.)
2.3 Results
For each of the six panels corresponding to fre-
quency estimates from a corpus A, Figure 2 gives
the ?
2
value resulting from the log-likelihood ra-
tio of (1) a model containing A and an estimate
from one of the five remaining corpora (given on
the x axis) and (2) a model containing just the cor-
pus indicated on the x axis. Thus, for each panel,
each bar in Figure 2 shows the explanatory power
of estimates from the corpus given at the top of the
panel after controlling for estimates from each of
the other corpora.
Model fits reveal intuitive, previously undocu-
mented biases in the ability of each corpus to pre-
dict human data. For example, corpora of British
English tend to explain relatively little after con-
8
trolling for other British corpora in modeling lexi-
cal decision RTs (yellow). Similarly, Switchboard
provides relatively little explanatory power over
the other corpora in predicting picture naming
RTs (blue bars), possibly because highly image-
able nouns and verbs frequent in everyday interac-
tions are underrepresented in telephone conversa-
tions between people with no common visual ex-
perience. In other words, idiosyncratic facts about
the topics, dialects, etc. represented in each cor-
pus lead to systematic patterns in how well each
corpus can predict human data relative to the oth-
ers. In some cases, the predictive value of one
corpus after controlling for another?apparently
for reasons related to genre, dialect?can be quite
large (cf. the ?
2
difference between a model with
both Google and Switchboard frequency estimates
compared to one with only Switchboard [top right
yellow bar]).
In addition to comparing the overall predictive
power of the corpora, we examined the words
for which behavioral predictions derived from the
corpora deviated most from the observed behav-
ior (word frequencies strongly over- or under-
estimated by each corpora). First, in Table 2 we
give the ten words with the greatest relative differ-
ence in frequency for each corpus pair. For exam-
ple, fife is deemed more frequent according to the
BNC than to Google.
2
These results suggest that particular corpora
may be genre-biased in systematic ways. For in-
stance, Google appears to be biased towards termi-
nology dealing with adult material and technology.
Similarly, BNCw is biased, relative to Google, to-
wards Britishisms. For these words in the BNC
and Google, we examined errors in predicted lexi-
cal decision times. Figure 3 plots errors in the lin-
ear model?s prediction of RTs for older (top) and
younger (bottom) subjects.
The figure shows a positive correlation between
how large the difference is between the lexical de-
cision RT predicted by the model and the actu-
ally observed RT, and how over-estimated the log
frequency of that word is in the BNC relative to
Google (left panel) or in Google relative to the
BNC (right panel). The left panel shows that BNC
produces a much greater estimate of the log fre-
2
Surprisingly, fife was determined to be one of the words
with the largest frequency asymmetry between Switchboard
and the Google n-grams corpus. This was a result of lower-
casing all of the words in in the analyses, and the fact that
Barney Fife was mentioned several times in the BNC.
quency of the word lee relative to Google, which
leads the model to predict a lower RT for this word
than is observed (i.e., the error is positive; though
note that the error is less severe for older relative to
younger subjects). By contrast, the asymmetry be-
tween the two corpora in the estimated frequency
of sir is less severe, so the observed RT deviates
less from the predicted RT. In the right panel, we
see that Google assigns a much greater estimate
of log frequency to the word tech than the BNC,
which leads a model predicting RTs from Google-
derived frequency estimates to predict a far lower
RT for this word than observed.
3 Discussion
Researchers in computational linguistics often as-
sume that more data is always better than less
data (Banko and Brill, 2001). This is true in-
sofar as larger corpora allow computational lin-
guists to generate less noisy estimates of the av-
erage language experience of the users of compu-
tational linguistics applications. However, corpus
size does not necessarily eliminate certain types of
biases in estimates of human linguistic experience,
as demonstrated in Figure 3.
Our analyses reveal that 6 commonly used cor-
pora fail to reflect the human language model in
various ways related to dialect, modality, and other
properties of each corpus. Our results point to
a type of bias in commonly used language mod-
els that has been previously overlooked. This bias
may limit the effectiveness of NLP algorithms in-
tended to generalize to a linguistic domains whose
statistical properties are generated by humans.
For psycholinguists these results support an im-
portant methodological point: while each corpus
presents systematic biases in how well it predicts
human behavior, all six corpora are, on the whole,
of comparable predictive value and, specifically,
the results suggest that the web performs as well
as traditional instruments in predicting behavior.
This has two implications for psycholinguistic re-
search. First, as argued by researchers such as
Lew (2009), given the size of the Web compared to
other corpora, research focusing on low-frequency
linguistic events?or requiring knowledge of the
distributional characteristics of varied contexts?
is now more tractable. Second, the viability of
the web in predicting behavior opens up possibil-
ities for computational psycholinguistic research
in languages for which no corpora exist (i.e., most
9
CELEX written BNC written Google
CELEX spoken BNC spoken Switchboard
0
40
80
120
0
10
20
30
40
0
10
20
30
0
10
20
30
0
10
20
30
40
0
5
10
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
Comparison
? ?2
task
lexical decision
picture naming
word naming
Pairwise model comparisons
Figure 2: Results of log likelihood ratio model comparisons. Large values indicate that the reference predictor (panel title)
explained a large amount of variance over and above the predictor given on the x-axis.
Google and BNC written
Standardized difference score
Err
or 
in l
ine
ar 
mo
de
l
cent dame
doleduke
fife
glen
god
gulf hall
hank
king
lee
lord march
nick
prime
prince
sir ward
cent damedoleduke
fife
glen
god
gulf
hallhank king
lee
lord march
nick
primeprince
sir ward
ass
bin
bug
butt cartchat click
code
darn
den
dialdikefileflip gayheckhop hunklink log
mail
map page
pee
prep
print
quote
ranchscript
search
self
sex
site
skipslotstore
suck
tag
tech
teens
thread
tiretoe
twain
webwhiz
wow
zip
ass
bin
bugbutt
cartchat click
codedarndendial
dike
file
flip
gay
heck
hop hunk
link log mailmap page
peepr p
print quoteranchscript
search
selfsex
site
skip
slot
store sucktag
tech
teens
thread
tire
toe
twain web
whizwow
zip
-0.1
0.0
0.1
0.2
0.3
0.4
-0.1
0.0
0.1
0.2
0.3
0.4
old
young
-3.5 -3.0 -2.5 2.5 3.0 3.5 4.0 4.5 5.0 5.5
Google < BNC written Google > BNC written
goog.f
-4
-2
0
2
Figure 3: Errors in the linear model predicting lexical decision RTs from log frequency are plotted against the standardized
difference in log frequency in the Google n-grams corpus versus the written portion of the BNC. Top and bottom panels show
errors for older and younger subjects, respectively. The left panel plots words with much greater frequency in the written
portion of the BNC relative to Google; the right panel plots words occurring more frequently in Google. Errors in the linear
model are plotted against the standardized difference in log frequency across the corpora, and word color encodes the degree to
which each word is more (red) or less (blue) frequent in Google. That the fit line in each graph is above 0 in the y-axis means
that on average these biased words in each domain are being over-predicted, i.e., the corpus frequencies suggest humans will
react (sometimes much) faster than they actually did in the lab.
10
Greater Lesser Top-10
google bnc.s web, ass, gay, tire, text, tool, code, woe, site, zip
google bnc.w ass, teens, tech, gay, bug, suck, site, cart, log, search
google celex.s teens, cart, gay, zip, mail, bin, tech, click, pee, site
google celex.w web, full, gay, bin, mail, zip, site, sake, ass, log
google swbd gay, thread, text, search, site, link, teens, seek, post, sex
bnc.w google fife, lord, duke, march, dole, god, cent, nick, dame, draught
bnc.w bnc.s pact, corps, foe, tract, hike, ridge, dine, crest, aide, whim
bnc.w celex.s staff, nick, full, waist, ham, lap, knit, sheer, bail, march
bnc.w celex.w staff, lord, last, nick, fair, glen, low, march, should, west
bnc.w swbd rose, prince, seek, cent, text, clause, keen, breach, soul, rise
celex.s google art, yes, pound, spoke, think, mean, say, thing, go, drove
celex.s bnc.s art, hike, pact, howl, ski, corps, peer, spoke, jazz, are
celex.s bnc.w art, yes, dike, think, thing, sort, mean, write, pound, lot
celex.s celex.w yes, sort, thank, think, jazz, heck, tape, well, fife, get
celex.s swbd art, cell, rose, spoke, aim, seek, shall, seed, text, knight
celex.w google art, plod, pound, shake, spoke, dine, howl, sit, say, draught
celex.w bnc.s hunch, stare, strife, hike, woe, aide, rout, yell, glaze, flee
celex.w bnc.w dike, whiz, dine, shake, grind, jerk, whoop, say, are, cram
celex.w celex.s wrist, pill, lawn, clutch, stare, spray, jar, shark, plead, horn
celex.w swbd art, rose, seek, aim, rise, burst, seed, cheek, grin, lip
swbd google mow, kind, lot, think, fife, corps, right, cook, sort, do
swbd bnc.s creek, mow, guess, pact, strife, tract, hank, howl, foe, nap
swbd bnc.w stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kid
swbd celex.s wow, sauce, mall, deck, full, spray, flute, rib, guy, bunch
swbd celex.w heck, guess, right, full, stuff, lot, last, well, guy, fair
Table 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife,
lord, and duke, in the BNC are far greater than in Google).
languages). This furthers the arguments of the ?the
web as corpus? community (Kilgarriff and Grefen-
stette, 2003) with respect to psycholinguistics.
Finally, combining multiple sources of fre-
quency estimates is one way researchers may be
able to reduce the prediction bias from any sin-
gle corpus. This relates to work in automatically
building domain specific corpora (e.g., Moore and
Lewis (2010), Axelrod et al (2011), Daum?e III
and Jagarlamudi (2011), Wang et al (2014), Gao
et al (2002), and Lin et al (1997)). Those efforts
focus on building representative document collec-
tions for a target domain, usually based on a seed
set of initial documents. Our results prompt the
question: can one use human behavior as the tar-
get in the construction of such a corpus? Con-
cretely, can we build corpora by optimizing an ob-
jective measure that minimizes error in predicting
human reaction times? Prior work in building bal-
anced corpora used either rough estimates of the
ratio of genre styles a normal human is exposed to
daily (e.g., the Brown corpus (Kucera and Fran-
cis, 1967)), or simply sampled text evenly across
genres (e.g., COCA: the Corpus of Contemporary
American English (Davies, 2009)). Just as lan-
guage models have been used to predict reading
grade-level of documents (Collins-Thompson and
Callan, 2004), human language models could be
used to predict the appropriateness of a document
for inclusion in an ?automatically balanced? cor-
pus.
4 Conclusion
We have shown intuitive, domain-specific biases
in the prediction of human behavioral measures
via corpora of various genres. While some psy-
cholinguists have previously acknowledged that
different corpora carry different predictive power,
this is the first work to our knowledge to system-
atically document these biases across a range of
corpora, and to relate these predictive errors to do-
main bias, a pressing issue in the NLP community.
With these results in hand, future work may now
consider the automatic construction of a ?prop-
erly? balanced text collection, such as originally
desired by the creators of the Brown corpus.
Acknowledgments
The authors wish to thank three anonymous ACL
reviewers for helpful feedback. This research
was supported by a DARPA award (FA8750-13-2-
0017) and NSF grant IIS-0916599 to BVD, NSF
IIS-1150028 CAREER Award and Alfred P. Sloan
Fellowship to TFJ, and an NSF Graduate Research
Fellowship to ABF.
11
References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 11).
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2). Linguis-
tic Data Consortium, Philadelphia.
R. H. Baayen, L. F. Feldman, and R. Schreuder.
2006. Morphological influences on the recognition
of monosyllabic monomorphemic words. Journal of
Memory and Language, 53:496?512.
D. A. Balota and D. H. Spieler. 1998. The utility of
item-level analyses in model evaluation: A reply to
Seidenberg & Plaut (1998). Psychological Science.
D. A. Balota, M. J. Cortese, and M. Pilotti. 1999. Item-
level analyses of lexical decision performance: Re-
sults from a mega-study. In Abstracts of the 40th An-
nual Meeting of the Psychonomics Society, page 44.
D. Balota, M. Cortese, S. Sergent-Marshall, D. Spieler,
and M. Yap. 2004. Visual word recognition for
single-syllable words. Journal of Experimental Psy-
chology:General, (133):283316.
M. Banko and E. Brill. 2001. Mitigating the paucity of
data problem. Human Language Technology.
E. Bates, S. D?Amico, T. Jacobsen, A. Szkely, E. An-
donova, A. Devescovi, D. Herron, CC Lu, T. Pech-
mann, C. Plh, N. Wicha, K. Federmeier, I. Gerd-
jikova, G. Gutierrez, D. Hung, J. Hsu, G. Iyer,
K. Kohnert, T. Mehotcheva, A. Orozco-Figueroa,
A. Tzeng, and O. Tzeng. 2003. Timed picture nam-
ing in seven languages. Psychonomic Bulletin & Re-
view, 10(2):344?380.
BNC Consortium. 2007. The British National Corpus,
version 3 (BNC XML Edition). Distributed by Ox-
ford University Computing Services on behalf of the
BNC Consortium.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium (LDC).
Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In HLT-NAACL, pages 193?200.
H. Daum?e III and J. Jagarlamudi. 2011. Domain
adaptation for machine translation by mining unseen
words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT 11).
M. Davies. 2009. The 385+ million word corpus of
contemporary american english (19902008+): De-
sign, architecture, and linguistic insights. Inter-
national Journal of Corpus Linguistics, 14(2):159?
190.
J. Gao, J. Goodman, M. Li, and K. F. Lee. 2002. To-
ward a unified approach to statistical language mod-
eling for chinese. In Proceedings of the ACM Trans-
actions on Asian Language Information Processing
(TALIP 02).
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone Speech Corpus for
Research and Development. In Proceedings of
ICASSP-92, pages 517?520.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333?348.
H. Kucera and W.N. Francis. 1967. Computational
analysis of present-day american english. provi-
dence, ri: Brown university press.
R. Lew, 2009. Contemporary Corpus Linguistics,
chapter The Web as corpus versus traditional cor-
pora: Their relative utility for linguists and language
learners, pages 289?300. London/New York: Con-
tinuum.
S. C. Lin, C. L. Tsai, L. F. Chien, K. J. Chen, and
L. S. Lee. 1997. Chinese language model adapta-
tion based on document classification and multiple
domain-specific language models. In Proceedings
of the 5th European Conference on Speech Commu-
nication and Technology.
S.A. McDonald and R.C. Shillcock. 2003. Eye
movements reveal the on-line computation of lexical
probabilities during reading. Psychological science,
14(6):648?52, November.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 10).
M. Ramscar, P. Hendrix, C. Shaoul, P. Milin, and R. H.
Baayen. 2014. The myth of cognitive decline: non-
linear dynamics of lifelong learning. Topics in Cog-
nitive Science, 32:5?42.
D. H. Spieler and D. A. Balota. 1997. Bringing com-
putational models of word naming down to the item
level. 6:411?416.
L. Wang, D.F. Wong, L.S. Chao, Y. Lu, and J. Xing.
2014. A systematic comparison of data selection
criteria for smt domain adaptation. The Scientific
World Journal.
12
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181?186,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
I?m a Belieber:
Social Roles via Self-identification and Conceptual Attributes
Charley Beller, Rebecca Knowles, Craig Harman
Shane Bergsma
?
, Margaret Mitchell
?
, Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
?
University of Saskatchewan, Saskatoon, Saskatchewan Canada
?
Microsoft Research, Redmond, Washington USA
charleybeller@jhu.edu, rknowles@jhu.edu, craig@craigharman.net,
shane.a.bergsma@gmail.com, memitc@microsoft.com, vandurme@cs.jhu.edu
Abstract
Motivated by work predicting coarse-
grained author categories in social me-
dia, such as gender or political preference,
we explore whether Twitter contains infor-
mation to support the prediction of fine-
grained categories, or social roles. We
find that the simple self-identification pat-
tern ?I am a ? supports significantly
richer classification than previously ex-
plored, successfully retrieving a variety of
fine-grained roles. For a given role (e.g.,
writer), we can further identify character-
istic attributes using a simple possessive
construction (e.g., writer?s ). Tweets
that incorporate the attribute terms in first
person possessives (my ) are confirmed
to be an indicator that the author holds the
associated social role.
1 Introduction
With the rise of social media, researchers have
sought to induce models for predicting latent au-
thor attributes such as gender, age, and politi-
cal preferences (Garera and Yarowsky, 2009; Rao
et al, 2010; Burger et al, 2011; Van Durme,
2012b; Zamal et al, 2012). Such models are
clearly in line with the goals of both computa-
tional advertising (Wortman, 2008) and the grow-
ing area of computational social science (Conover
et al, 2011; Nguyen et al, 2011; Paul and Dredze,
2011; Pennacchiotti and Popescu, 2011; Moham-
mad et al, 2013) where big data and computa-
tion supplement methods based on, e.g., direct hu-
man surveys. For example, Eisenstein et al (2010)
demonstrated a model that predicted where an au-
thor was located in order to analyze regional dis-
tinctions in communication. While some users ex-
plicitly share their GPS coordinates through their
Twitter clients, having a larger collection of au-
tomatically identified users within a region was
preferable even though the predictions for any
given user were uncertain.
We show that media such as Twitter can sup-
port classification that is more fine-grained than
gender or general location. Predicting social roles
such as doctor, teacher, vegetarian, christian,
may open the door to large-scale passive surveys
of public discourse that dwarf what has been pre-
viously available to social scientists. For exam-
ple, work on tracking the spread of flu infections
across Twitter (Lamb et al, 2013) might be en-
hanced with a factor based on aggregate predic-
tions of author occupation.
We present two studies showing that first-
person social content (tweets) contains intuitive
signals for such fine-grained roles. We argue that
non-trivial classifiers may be constructed based
purely on leveraging simple linguistic patterns.
These baselines suggest a wide range of author
categories to be explored further in future work.
Study 1 In the first study, we seek to determine
whether such a signal exists in self-identification:
we rely on variants of a single pattern, ?I am a ?,
to bootstrap data for training balanced-class binary
classifiers using unigrams observed in tweet con-
tent. As compared to prior research that required
actively polling users for ground truth in order to
construct predictive models for demographic in-
formation (Kosinski et al, 2013), we demonstrate
that some users specify such properties publicly
through direct natural language.
Many of the resultant models show intuitive
strongly-weighted features, such as a writer be-
ing likely to tweet about a story, or an ath-
lete discussing a game. This demonstrates self-
identification as a viable signal in building predic-
tive models of social roles.
181
Role Tweet
artist I?m an Artist..... the last of a dying breed
belieber @justinbieber I will support you in ev-
erything you do because I am a belieber
please follow me I love you 30
vegetarian So glad I?m a vegetarian.
Table 1: Examples of self-identifying tweets.
# Role # Role # Role
29,924 little 5,694 man 564 champion
21,822 big ... ... 559 teacher
18,957 good 4,007 belieber 556 writer
13,069 huge 3,997 celebrity 556 awful
13,020 bit 3,737 virgin ... ...
12,816 fan 3,682 pretty 100 cashier
10,832 bad ... ... 100 bro
10,604 girl 2,915 woman ... ...
9,981 very 2,851 beast 10 linguist
... ... ... ... ... ...
Table 2: Number of self-identifying users per ?role?. While
rich in interesting labels, cases such as very highlight the pur-
poseful simplicity of the current approach.
Study 2 In the second study we exploit a com-
plementary signal based on characteristic con-
ceptual attributes of a social role, or concept
class (Schubert, 2002; Almuhareb and Poesio,
2004; Pas?ca and Van Durme, 2008). We identify
typical attributes of a given social role by collect-
ing terms in the Google n-gram corpus that occur
frequently in a possessive construction with that
role. For example, with the role doctor we extract
terms matching the simple pattern ?doctor?s ?.
2 Self-identification
All role-representative users were drawn from
the free public 1% sample of the Twitter Fire-
hose, over the period 2011-2013, from the sub-
set that selected English as their native language
(85,387,204 unique users). To identify users of
a particular role, we performed a case-agnostic
search of variants of a single pattern: I am a(n)
, and I?m a(n) , where all single tokens filling
the slot were taken as evidence of the author self-
reporting for the given ?role?. Example tweets can
be seen in Table 1, examples of frequency per role
in Table 2. This resulted in 63,858 unique roles
identified, of which 44,260 appeared only once.
1
We manually selected a set of roles for fur-
ther exploration, aiming for a diverse sample
across: occupation (e.g., doctor, teacher), family
(mother), disposition (pessimist), religion (chris-
1
Future work should consider identifying multi-word role
labels (e.g., Doctor Who fan, or dog walker).
0.60
0.65
0.70
0.75
0.80
ll
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l
direc
tione
r
belie
ber
optim
ist
sold
ier
soph
omo
re
pess
imis
t
ran
dom
.0
danc
er
hips
ter
ran
dom
.2
sing
er
fresh
man
mo
ther
ran
dom
.1
chee
rlead
er
rapp
er
chris
tian
artis
t
sm
oker acto
r
vege
taria
n
wo
ma
n
athle
te
geek engi
neer
wa
itres
s
nur
se
ma
n
stud
ent doct
or poet writ
er
athe
ist
gran
dmalawy
er
teac
her
Role
Cha
nce 
of S
ucce
ss
Figure 1: Success rate for querying a user. Random.0,1,2
are background draws from the population, with the mean of
those three samples drawn horizontally. Tails capture 95%
confidence intervals.
tian), and ?followers? (belieber, directioner).
2
We filtered users via language ID (Bergsma et al,
2012) to better ensure English content.
3
For each selected role, we randomly sampled up
to 500 unique self-reporting users and then queried
Twitter for up to 200 of their recent publicly
posted tweets.
4
These tweets served as represen-
tative content for that role, with any tweet match-
ing the self-reporting patterns filtered. Three sets
of background populations were extracted based
on randomly sampling users that self-reported En-
glish (post-filtered via LID).
Twitter users are empowered to at any time
delete, rename or make private their accounts.
Any given user taken to be representative based on
a previously posted tweet may no longer be avail-
able to query on. As a hint of the sort of user stud-
ies one might explore given access to social role
prediction, we see in Figure 1 a correlation be-
tween self-reported role and the chance of an ac-
count still being publicly visible, with roles such
as belieber and directioner on the one hand, and
doctor and teacher on the other.
The authors examined the self-identifying tweet
of 20 random users per role. The accuracy of the
self-identification pattern varied across roles and
is attributable to various factors including quotes,
e.g. @StarTrek Jim, I?m a DOCTOR not a down-
load!. While these samples are small (and thus
estimates of quality come with wide variance), it
2
Those that follow the music/life of the singer Justin
Bieber and the band One Direction, respectively.
3
This removes users that selected English as their primary
language, used a self-identification phrase, e.g. I am a be-
lieber, but otherwise tended to communicate in non-English.
4
Roughly half of the classes had less than 500 self-
reporting users in total, in those cases we used all matches.
182
actorartistatheist
athletebeliebercheerleader
christiandancerdirectioner
doctorengineerfreshman
geekgrandmahipster
lawyermanmother
nurseoptimistpessimist
poetrappersinger
smokersoldiersophomore
studentteachervegetarian
waitresswomanwriter
0 5 10 15
Figure 2: Valid self-identifying tweets from sample of 20.
is noteworthy that a non-trivial number for each
were judged as actually self-identifying.
Indicative Language Most work in user clas-
sification relies on featurizing language use,
most simply through binary indicators recording
whether a user did or did not use a particular word
in a history of n tweets. To explore whether lan-
guage provides signal for future work in fine-grain
social role prediction, we constructed a set of ex-
periments, one per role, where training and test
sets were balanced between users from a random
background sample and self-reported users. Base-
line accuracy in these experiments was thus 50%.
Each training set had a target of 600 users (300
background, 300 self-identified); for those roles
with less than 300 users self-identifying, all users
were used, with an equal number background. We
used the Jerboa (Van Durme, 2012a) platform
to convert data to binary feature vectors over a un-
igram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al, 2008). We used the pos-
itively annotated data to form test sets, balanced
with data from the background set. Each test set
had a theoretical maximum size of 40, but for sev-
eral classes it was in the single digits (see Fig-
ure 2). Despite the varied noisiness of our simple
pattern-bootstrapped training data, and the small
size of our annotated test set, we see in Figure 3
that we are able to successfully achieve statisti-
cally significant predictions of social role for the
majority of our selected examples.
Table 3 highlights examples of language indica-
tive of role, as determined by the most positively
weighted unigrams in the classification experi-
0.2
0.4
0.6
0.8
1.0
l
l l l
l
ll
l ll
l
l l
l
ll
l ll
l
l
ll l
l
l
l
l l l
l
l
l
sold
ier
wo
ma
n
pess
imis
t
chris
tian
gran
dma
nur
se
rapp
er
ma
n poet
chee
rlead
er
stud
ent
engi
neer acto
r
teac
her
vege
taria
n
mo
ther sing
er
lawy
er
optim
ist
wa
itres
s
sm
oker hips
ter doct
or
danc
er
artis
t
fresh
man
direc
tione
r
geek
soph
omo
re
athe
ist
athle
te
writ
er
belie
ber
Role
Acc
urac
y
Figure 3: Accuracy in classifying social roles.
Role :: Feature ( Rank)
artist morning, summer, life, most, amp, studio
atheist fuck, fucking, shit, makes, dead, ..., religion
19
athlete lol, game, probably, life, into, ..., team
9
belieber justin, justinbeiber, believe, beliebers, bieber
cheerleader cheer, best, excited, hate, mom, ..., prom
16
christian lol, ..., god
12
, pray
13
, ..., bless
17
, ..., jesus
20
dancer dance, since, hey, never, been
directioner harry, d, follow, direction, never, liam, niall
doctor sweet, oh, or, life, nothing
engineer (, then, since, may, ), test
9
, -
17
, =
18
freshman summer, homework, na, ..., party
19
, school
20
geek trying, oh, different, dead, been
grandma morning, baby, around, night, excited
hipster fucking, actually, thing, fuck, song
lawyer did, never, his, may, pretty, law, even, office
man man, away, ai, young, since
mother morning, take, fuck, fucking, trying
nurse lol, been, morning, ..., night
10
, nursing
11
, shift
13
optimist morning, enough, those, everything, never
poet feel, song, even, say, yo
rapper fuck, morning, lol, ..., mixtape
8
, songs
15
singer sing, song, music, lol, never
smoker fuck, shit, fucking, since, ass, smoke, weed
20
solider ai, beautiful, lol, wan, trying
sophmore summer, >, ..., school
11
, homework
12
student anything, summer, morning, since, actually
teacher teacher, morning, teach, ..., students
7
, ..., school
20
vegetarian actually, dead, summer, oh, morning
waitress man, try, goes, hate, fat
woman lol, into, woman, morning, never
writer write, story, sweet, very, working
Table 3: Most-positively weighted features per role, along
with select features within the top 20. Surprising mother
features come from ambigious self-identification, as seen in
tweets such as: I?m a mother f!cking starrrrr.
ment. These results qualitatively suggest many
roles under consideration may be teased out from a
background population by focussing on language
that follows expected use patterns. For example
the use of the term game by athletes, studio by
artists, mixtape by rappers, or jesus by Christians.
3 Characteristic Attributes
Bergsma and Van Durme (2013) showed that the
183
task of mining attributes for conceptual classes can
relate straightforwardly to author attribute predic-
tion. If one views a role, in their case gender, as
two conceptual classes, male and female, then ex-
isting attribute extraction methods for third-person
content (e.g., news articles) can be cheaply used to
create a set of bootstrapping features for building
classifiers over first-person content (e.g., tweets).
For example, if we learn from news corpora that:
a man may have a wife, then a tweet saying: ...my
wife... can be taken as potential evidence of mem-
bership in the male conceptual class.
In our second study, we test whether this idea
extends to our wider set of fine-grained roles. For
example, we aimed to discover that a doctor may
have a patient, while a hairdresser may have a
salon; these properties can be expressed in first-
person content as possessives like my patient or my
salon. We approached this task by selecting target
roles from the first experiment and ranking charac-
teristic attributes for each using pointwise mutual
information (PMI) (Church and Hanks, 1990).
First, we counted all terms matching a target
social role?s possessive pattern (e.g., doctor?s )
in the web-scale n-gram corpus Google V2 (Lin
et al, 2010)
5
. We ranked the collected terms
by computing PMI between classes and attribute
terms. Probabilities were estimated from counts of
the class-attribute pairs along with counts match-
ing the generic possessive patterns his and
her which serve as general background cate-
gories. Following suggestions by Bergsma and
Van Durme, we manually filtered the ranked list.
6
We removed attributes that were either (a) not
nominal, or (b) not indicative of the social role.
This left fewer than 30 attribute terms per role,
with many roles having fewer than 10.
We next performed a precision test to identify
potentially useful attributes in these lists. We ex-
amined tweets with a first person possessive pat-
tern for each attribute term from a small corpus
of tweets collected over a single month in 2013,
discarding those attribute terms with no positive
matches. This precision test is useful regardless
of how attribute lists are generated. The attribute
5
In this corpus, follower-type roles like belieber and di-
rectioner are not at all prevalent. We therefore focused on
occupational and habitual roles (e.g., doctor, smoker).
6
Evidence from cognitive work on memory-dependent
tasks suggests that such relevance based filtering (recogni-
tion) involves less cognitive effort than generating relevant
attributes (recall) see (Jacoby et al, 1979). Indeed, this filter-
ing step generally took less than a minute per class.
term chart, for example, had high PMI with doc-
tor; but a precision test on the phrase my chart
yielded a single tweet which referred not to a med-
ical chart but to a top ten list (prompting removal
of this attribute). Using this smaller high-precision
set of attribute terms, we collected tweets from the
Twitter Firehose over the period 2011-2013.
4 Attribute-based Classification
Attribute terms are less indicative overall than
self-ID, e.g., the phrase I?m a barber is a clearer
signal than my scissors. We therefore include a
role verification step in curating a collection of
positively identified users. We use the crowd-
sourcing platform Mechanical Turk
7
to judge
whether the person tweeting held a given role
Tweets were judged 5-way redundantly. Me-
chanical Turk judges (?Turkers?) were presented
with a tweet and the prompt: Based on this
tweet, would you think this person is a BAR-
BER/HAIRDRESSER? along with four response
options: Yes, Maybe, Hard to tell, and No.
We piloted this labeling task on 10 tweets per
attribute term over a variety of classes. Each an-
swer was associated with a score (Yes = 1, Maybe
= .5, Hard to tell = No = 0) and aggregated across
the five judges. We found in development that an
aggregate score of 4.0 (out of 5.0) led to an ac-
ceptable agreement rate between the Turkers and
the experimenters, when the tweets were randomly
sampled and judged internally. We found that
making conceptual class assignments based on a
single tweet was often a subtle task. The results of
this labeling study are shown in Figure 4, which
gives the percent of tweets per attribute that were
4.0 or above. Attribute terms shown in red were
manually discarded as being inaccurate (low on
the y-axis) or non-prevalent (small shape).
From the remaining attribute terms, we identi-
fied users with tweets scoring 4.0 or better as posi-
tive examples of the associated roles. Tweets from
those users were scraped via the Twitter API to
construct corpora for each role. These were split
intro train and test, balanced with data from the
same background set used in the self-ID study.
Test sets were usually of size 40 (20 positive, 20
background), with a few classes being sparse (the
smallest had only 16 instances). Results are shown
in Figure 5. Several classes in this balanced setup
can be predicted with accuracies in the 70-90%
7
https://www.mturk.com/mturk/
184
l l l l ll lll l l l
l l l l l l
l
l l ll
l
l l l l l
l
l l l
Actor/Actress Athlete Barber/Hairdresser Bartender Blogger Cheerleader
Christian College Student Dancer Doctor/Nurse Drummer Hunter
Jew Mom Musician Photographer Professor Rapper/Songwriter
Reporter Sailor Skier Smoker Soldier Student
Swimmer Tattoo Artist Waiter/Waitress Writer
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
rehear
sal theater directo
r lines
conc
ussionplayin
gprotein sport squadcondit
ioningjerseypositioncoach calves clien
t
scissor
s
shears salon bar blog bloggi
ng pom
hope testimo
ny
church bible schola
rship
syllabu
s
adviso
r
tuition campu
s
univer
sity
college tu
tu
scrub patient stethos
cope drum stand
shul angel deliver
y kid parent
ing set alum guitar piano shoot shutter lecture faculty studen
t lyrics
cove
rage editor article shi
p
goggle
s pipe smokin
g
tobacc
o
smoke cigaret
te
billet comba
t duffel orders bunk deploy
mentbarrac
ks stats cap lab philoso
phy
pool ink station tip apron script memo
ir poemKeyword
Above
 Thres
hold
log10(Count)
l l l l1 2 3 4Keep
l FALSE TRUE
Figure 4: Turker judged quality of attributes selected as
candidate features for bootstrapping positive instances of the
given social role.
0.5
0.6
0.7
0.8
acto
r
athle
te
barbe
r
blogg
er
chee
rlead
er
chris
tian docto
r
drum
mer
mo
m
mu
sicia
n
photo
graph
er
profe
ssor
repor
ter
smo
ker
soldi
er
stude
nt
waite
r
write
r
Accu
racy
Figure 5: Classifier accuracy on balanced set contrasting
agreed upon Twitter users of a given role against users pulled
at random from the 1% stream.
range, supporting our claim that there is discrimi-
nating content for a variety of these social roles.
Conditional Classification How accurately we
can predict membership in a given class when a
Twitter user sends a tweet matching one of the tar-
geted attributes? For example, if one sends a tweet
saying my coach, then how likely is it that author
Figure 6: Results of positive vs negative by attribute term.
Given that a user tweets . . . my lines . . . we are nearly 80%
accurate in identifying whether or not the user is an actor.
is an athlete?
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given attribute term. Positive instances were taken
to be those with a score of 4.0 or higher, with neg-
ative instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in Figure 6.
5 Conclusion
We have shown that Twitter contains sufficiently
robust signal to support more fine-grained au-
thor attribute prediction tasks than have previously
been attempted. Our results are based on simple,
intuitive search patterns with minimal additional
filtering: this establishes the feasibility of the task,
but leaves wide room for future work, both in the
sophistication in methodology as well as the diver-
sity of roles to be targeted. We exploited two com-
plementary types of indicators: self-identification
and self-possession of conceptual class (role) at-
tributes. Those interested in identifying latent de-
mographics can extend and improve these indica-
tors in developing ways to identify groups of inter-
est within the general population of Twitter users.
Acknowledgements This material is partially
based on research sponsored by the NSF un-
der grants DGE-123285 and IIS-1249516 and by
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
185
References
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an eval-
uation. In Proceedings of EMNLP.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clay Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the NAACL Workshop
on Language and Social Media.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of EMNLP.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Jacob Eisenstein, Brendan O?Connor, Noah Smith, and
Eric P. Xing. 2010. A latent variable model of
geographical lexical variation. In Proceedings of
EMNLP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Michal Kosinski, David Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. Proceed-
ings of the National Academy of Sciences.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In Proceedings of NAACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Saif M. Mohammad, Svetlana Kiritchenko, and Joel
Martin. 2013. Identifying purpose behind elec-
toral tweets. In Proceedings of the Second Interna-
tional Workshop on Issues of Sentiment Discovery
and Opinion Mining, WISDOM ?13, pages 1?9.
Dong Nguyen, Noah A Smith, and Carolyn P Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 115?123. Association for Computational Lin-
guistics.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-Supervised Acquisition of Open-Domain
Classes and Class Attributes from Web Documents
and Query Logs. In Proceedings of ACL.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 430?
438. ACM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Lenhart K. Schubert. 2002. Can we derive general
world knowledge from texts? In Proceedings of
HLT.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Jennifer Wortman. 2008. Viral marketing and the
diffusion of trends on social networks. Technical
Report MS-CIS-08-19, University of Pennsylvania,
May.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
186
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 446?451,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Particle Filter Rejuvenation and Latent Dirichlet Allocation
Chandler May,
?
Alex Clemmer
?
and Benjamin Van Durme
?
?
Human Language Technology Center of Excellence
Johns Hopkins University
?
Microsoft
cjmay@jhu.edu, clemmer.alexander@gmail.com, vandurme@cs.jhu.edu
Abstract
Previous research has established sev-
eral methods of online learning for la-
tent Dirichlet alocation (LDA). How-
ever, streaming learning for LDA?
allowing only one pass over the data and
constant storage complexity?is not as
well explored. We use reservoir sam-
pling to reduce the storage complexity
of a previously-studied online algorithm,
namely the particle filter, to constant. We
then show that a simpler particle filter im-
plementation performs just as well, and
that the quality of the initialization dom-
inates other factors of performance.
1 Introduction
We extend a popular model, latent Dirichlet al
location (LDA), to unbounded streams of docu-
ments. In order for inference to be practical in
this setting it must use constant space asymptoti-
cally and run in pseudo-linear time, perhaps O(n)
or O(n log n).
Canini et al (2009) presented a method for LDA
inference based on particle filters, where a sam-
ple set of models is updated online with each new
token observed from a stream. In general, these
models should be regularly resampled and rejuve-
nated using Markov Chain Monte Carlo (MCMC)
steps over the history in order to improve the ef-
ficiency of the particle filter (Gilks and Berzuini,
2001). The particle filter of Canini et al (2009) re-
juvenates over independent draws from the history
by storing all past observations and states. This al-
gorithm thus has linear storage complexity and is
not an online learning algorithm in a strict sense
(B?orschinger and Johnson, 2012).
In the current work we propose using reservoir
sampling in the rejuvenation step to reduce the
storage complexity of the particle filter to O(1).
This improvement is practically useful in the
large-data setting and is also scientifically interest-
ing in that it recovers some of the cognitive plau-
sibility which originally motivated B?orschinger
and Johnson (2012). However, in experiments on
the dataset studied by Canini et al (2009), we
show that rejuvenation does not benefit the par-
ticle filter?s performance. Rather, performance
is dominated by the effects of random initializa-
tion (a problem for which we provide a correction
while abiding by the same constraints as Canini et
al. (2009)). This result re-opens the question of
whether rejuvenation is of practical importance in
online learning for static Bayesian models.
2 Latent Dirichlet Allocation
For a sequence of N words collected into doc-
uments of varying length, we denote the j-th
word as w
j
, and the document it occurs in as d
i
.
LDA (Blei et al, 2003) ?explains? the occurrence
of each word by postulating that a document was
generated by repeatedly: (1) sampling a topic z
from ?
(d)
, the document-specific mixture of T top-
ics, and (2) sampling a word w from ?
(z)
, the
probability distribution the z-th topic defines over
the vocabulary.
The goal is to infer ? and ?, under the model:
w
i
| z
i
, ?
(z
i
)
? Categorical(?
(z
i
)
)
?
(z)
? Dirichlet(?)
z
i
| ?
(d
i
)
? Categorical(?
(d
i
)
)
?
(d)
? Dirichlet(?)
446
initialize weights ?
(p)
0
= 1/P for p = 1, . . . , P
for i = 1, . . . , N do
for p = 1, . . . , P do
set ?
(p)
i
= ?
(p)
i?1
P(w
i
| z
(p)
i?1
,w
i?1
)
sample z
(p)
i
w.p. P(z
(p)
i
| z
(p)
i?1
,w
i
).
if ???
?2
2
? ESS then
for j ? R(i) do
for p = 1, . . . , P do
sample z
(p)
j
w.p.
P(z
(p)
j
| z
(p)
i\j
,w
i
)
set ?
(p)
i
= 1/P for each particle
Algorithm 1: Particle filtering for LDA.
Computing ? and ? exactly is generally in-
tractable, motivating methods for approximate in-
ference such as variational Bayesian inference
(Blei et al, 2003), expectation propagation (Minka
and Lafferty, 2002), and collapsed Gibbs sampling
(Griffiths and Steyvers, 2004).
A limitation of these techniques is they require
multiple passes over the data to obtain good sam-
ples of ? and ?. This requirement makes them im-
practical when the corpus is too large to fit directly
into memory and in particular when the corpus
grows without bound. This motivates online learn-
ing techniques, including sampling-based meth-
ods (Banerjee and Basu, 2007; Canini et al, 2009)
and stochastic variational inference (Hoffman et
al., 2010; Mimno et al, 2012; Hoffman et al,
2013). However, where these approaches gener-
ally assume the ability to draw independent sam-
ples from the full dataset, we consider the case
when it is infeasible to access arbitrary elements
from the history. The one existing algorithm that
can be directly applied under this constraint, to
our knowledge, is the streaming variational Bayes
framework (Broderick et al, 2013) in which the
posterior is recursively updated as new data arrives
using a variational approximation.
3 Online LDA Using Particle Filters
Particle filters are a family of sequential Monte
Carlo (SMC) sampling algorithms designed to es-
timate the posterior distribution of a system with
dynamic state (Doucet et al, 2001). A particle fil-
ter approximates the posterior by a weighted sam-
ple of points, or particles, from the state space.
The particle cloud is updated recursively for each
new observation using importance sampling (an
approach called sequential importance sampling).
Canini et al (2009) apply this approach to LDA
after analytically integrating out ? and ?, obtain-
ing a Rao-Blackwellized particle filter (Doucet et
al., 2000) that estimates the collapsed posterior
P(z | w). In this setting, the P particles are sam-
ples of the topic assignment vector z
(p)
, and they
are propagated forward in state space one token at
a time. In general, the larger P is, the more ac-
curately we approximate the posterior; for small
P , the approximation of the tails of the poste-
rior will be particularly poor (Pitt and Shephard,
1999). However, a larger value of P increases the
runtime and storage requirements of the algorithm.
We now describe the Rao-Blackwellized parti-
cle filter for LDA in detail (pseudocode is given in
Algorithm 1). At the moment token i is observed,
the particles form a discrete approximation of the
posterior up to the (i? 1)-th word:
P(z
i?1
| w
i?1
) ?
?
p
?
(p)
i?1
I
z
i?1
(z
(p)
i?1
)
where I
z
(z
?
) is the indicator function, evaluating
to 1 if z = z
?
and 0 otherwise. Now each par-
ticle p is propagated forward by drawing a topic
z
(p)
i
from the conditional posterior distribution
P(z
(p)
i
| z
(p)
i?1
,w
i
) and scaling the particle weight
by P(w
i
| z
(p)
i?1
,w
i?1
). The particle cloud now
approximates the posterior up to the i-th word:
P(z
i
| w
i
) ?
?
p
?
(p)
i
I
z
i
(z
(p)
i
).
Dropping the superscript (p) for notational conve-
nience, the conditional posterior used in the prop-
agation step is given by
P(z
i
|z
i?1
,w
i
) ? P(z
i
, w
i
| z
i?1
,w
i?1
)
=
n
(w
i
)
z
i
,i\i
+ ?
n
(?)
z
i
,i\i
+W?
n
(d
i
)
z
i
,i\i
+ ?
n
(d
i
)
?,i\i
+ T?
where n
(w
i
)
z
i
,i\i
is the number of times word w
i
has
been assigned topic z
i
so far, n
(?)
z
i
,i\i
is the num-
ber of times any word has been assigned topic z
i
,
n
(d
i
)
z
i
,i\i
is the number of times topic z
i
has been as-
signed to any word in document d
i
, and n
(d
i
)
?,i\i
is the
number of words observed in document d
i
. The
particle weights are scaled as
?
(p)
i
?
(p)
i?1
?
P(w
i
| z
(p)
i
,w
i
)P(z
(p)
i
| z
(p)
i?1
)
Q(z
(p)
i
| z
(p)
i?1
,w
i
)
= P(w
i
| z
(p)
i?1
,w
i?1
)
447
where Q is the proposal distribution for the parti-
cle state transition; in our case,
Q(z
(p)
i
| z
(p)
i?1
,w
i
) = P(z
(p)
i
| z
(p)
i?1
,w
i
),
minimizing the variance of the importance weights
conditioned on w
i
and z
i?1
(Doucet et al, 2000).
Over time the particle weights tend to diverge.
To combat this inefficiency, after every state tran-
sition we estimate the effective sample size (ESS)
of the particle weights as ??
i
?
?2
2
(Liu and Chen,
1998) and resample the particles when that esti-
mate drops below a prespecified threshold. Sev-
eral resampling strategies have been proposed
(Doucet et al, 2000); we perform multinomial
resampling as in Pitt and Shephard (1999) and
Ahmed et al (2011), treating the weights as un-
normalized probability masses on the particles.
After resampling we are likely to have several
copies of the same particle, yielding a degenerate
approximation to the posterior. To reintroduce di-
versity to the particle cloud we take MCMC steps
over a sequence of states from the history (Doucet
et al, 2000; Gilks and Berzuini, 2001). We call the
indices of these states the rejuvenation sequence,
denoted R(i) (Canini et al, 2009). The transition
probability for a state j ? R(i) is given by
P(z
j
| z
N\j
,w
N
) ?
n
(w
j
)
z
j
,N\j
+ ?
n
(?)
z
j
,N\j
+W?
n
(d
j
)
z
j
,N\j
+ ?
n
(d
j
)
?,N\j
+ T?
where subscript N\j denotes counts up to token
N , excluding those for token j.
The rejuvenation sequence can be chosen by
the practitioner. Choosing a long sequence (large
|R(i)|) may result in a more accurate posterior ap-
proximation but also increases runtime and stor-
age requirements. The tokens inR(i) may be cho-
sen uniformly at random from the history or under
a biased scheme that favors recent observations.
The particle filter studied empirically by Canini et
al. (2009) stored the entire history, incurring lin-
ear storage complexity in the size of the stream.
Ahmed et al (2011) instead sampled ten docu-
ments from the most recent 1000, achieving con-
stant storage complexity at the cost of a recency
bias. If we want to fit a model to a long non-
i.i.d. stream, we require an unbiased rejuvenation
sequence as well as sub-linear storage complexity.
4 Reservoir Sampling
Reservoir sampling is a widely-used family of al-
gorithms for choosing an array (?reservoir?) of k
items. The most common example, presented in
Vitter (1985) as Algorithm R, chooses k elements
of a stream such that each possible subset of k el-
ements is equiprobable. This effects sampling k
items uniformly without replacement, using run-
timeO(n) (constant per update) and storageO(k).
Initialize k-element array R ;
Stream S ;
for i = 1, . . . , k do
R[i]? S[i] ;
for i = k + 1, . . . , length(S) do
j ? random(1, i);
if j ? k then
R[j]? S[i] ;
Algorithm 2: Algorithm R for reservoir sampling
To ensure constant space over an unbounded
stream, we draw the rejuvenation sequence R(i)
uniformly from a reservoir. As each token of the
training data is ingested by the particle filter, we
decide to insert that token into the reservoir, or not,
independent of the other tokens in the current doc-
ument. Thus, at the end of step i of the particle fil-
ter, each of the i tokens seen so far in the training
sequence has an equal probability of being in the
reservoir, hence being selected for rejuvenation.
5 Experiments
We evaluate our particle filter on three datasets
studied in Canini et al (2009): diff3, rel3,
and sim3. Each of these datasets is a collection
of posts under three categories from the 20 News-
groups dataset.
1
We use a 60% training/40% test-
ing split of this data that is available online.
2
We preprocess the data by splitting each line
on non-alphabet characters, converting the result-
ing tokens to lower-case, and filtering out any to-
kens that appear in a list of common English stop
words. In addition, we remove the header of ev-
ery file and filter every line that does not contain
a non-trailing space (which removes embedded
ASCII-encoded attachments). Finally, we shuffle
the order of the documents. After these steps, we
compute the vocabulary for each dataset as the set
of all non-singleton types in the training data aug-
mented with a special out-of-vocabulary symbol.
1
diff3: {rec.sport.baseball, sci.space,
alt.atheism}; rel3: talk.politics.{misc,
guns, mideast}; and sim3: comp.{graphics,
os.ms-windows.misc, windows.x}.
2
http://qwone.com/
?
jason/20Newsgroups/
20news-bydate.tar.gz
448
Figure 1: Fixed initialization with different reservoir sizes.
During training we report the out-of-sample
NMI, calculated by holding the word proportions
? fixed, running five sweeps of collapsed Gibbs
sampling on the test set, and computing the topic
for each document as the topic assigned to the
most tokens in that document. Two Gibbs sweeps
have been shown to yield good performance in
practice (Yao et al, 2009); we increase the num-
ber of sweeps to five after inspecting the stability
on our dataset. The variance of the particle filter is
often large, so for each experiment we perform 30
runs and plot the mean NMI inside bands spanning
one sample standard deviation in either direction.
Fixed Initialization. Our first set of experi-
ments has a similar parameterization
3
to the exper-
iments of Canini et al (2009) except we draw the
rejuvenation sequence from a reservoir. We initial-
ize the particle filter with 200 Gibbs sweeps on the
first 10% of each dataset. Then, for each dataset,
for rejuvenation disabled, rejuvenation based on
a reservoir of size 1000, and rejuvenation based
on the entire history (in turn), we perform 30 runs
of the particle filter from that fixed initial model.
Our results (Figure 1) resemble those of Canini et
al. (2009); we believe the discrepancies are mostly
attributable to differences in preprocessing.
In these experiments, the initial model was not
chosen arbitrarily. Rather, an initial model that
yielded out-of-sample NMI close to the initial out-
of-sample NMI scores reported in the previous
3
T = 3, ? = ? = 0.1, P = 100, ess = 20, |R(i)| = 30
Figure 2: Variable initialization with different initialization
sample sizes.
study was chosen from a set of 100 candidates.
Variable Initialization. We now investigate the
significance of the initial model selection step used
in the previous experiments. We run a new set
of experiments in which the reservoir size is held
fixed at 1000 and the size of the initialization sam-
ple is varied. Specifically, we vary the size of
the initialization sample, in documents, between
zero (corresponding to no Gibbs initialization), 30,
100, and 300, and also perform a run of batch
Gibbs sampling (with no particle filter). In each
case, 2000 Gibbs sweeps are performed. In these
experiments, the initial models are not held fixed;
for each of the 30 runs for each dataset, the initial
model was generated by a different Gibbs chain.
The results for these experiments, depicted in Fig-
ure 2, indicate that the size of the initialization
sample improves mean NMI and reduces variance,
and that the variance of the particle filter itself is
dominated by the variance of the initial model.
Tuned Initialization. We observed previously
that variance in the Gibbs initialization of the
model contributes significantly to variance of the
overall algorithm, as measured by NMI. With
this in mind, we consider whether we can reduce
variance in the initialization by tuning the initial
model. Thus we perform a set of experiments in
which we perform Gibbs initialization 20 times
on the initialization set, setting the particle filter?s
initial model to the model out of these 20 with
449
Figure 3: Variable initialization with tuning.
the highest in-sample NMI. This procedure is per-
formed independently for each run of the particle
filter. We may not always have labeled data for
initialization, so we also consider a variation in
which Gibbs initialization is performed 20 times
on the first 80% of the initialization sample, held-
out perplexity (per word) is estimated on the re-
maining 20%, using a first-moment particle learn-
ing approximation (Scott and Baldridge, 2013),
and the particle filter is started from the model
out of these 20 with the lowest held-out perplex-
ity. The results, shown in Figure 3, show that we
can ameliorate the variance due to initialization by
tuning the initial model to NMI or perplexity.
6 Discussion
Motivated by a desire for cognitive plausibility,
B?orschinger and Johnson (2011) used a particle
filter to learn Bayesian word segmentation mod-
els, following the work of Canini et al (2009).
They later showed that rejuvenation improved per-
formance (B?orschinger and Johnson, 2012), but
this impaired cognitive plausibility by necessitat-
ing storage of all previous states and observations.
We attempted to correct this by drawing the re-
juvenation sequence from a reservoir, but our re-
sults indicate that the particle filter for LDA on our
dataset is highly sensitive to initialization and not
influenced by rejuvenation.
In the experiments of B?orschinger and Johnson
(2012), the particle cloud appears to be resampled
once per utterance with a large rejuvenation se-
quence;
4
each particle takes many more rejuvena-
tion MCMC steps than new state transitions and
thus resembles a batch MCMC sampler. In our ex-
periments resampling is done on the order of once
per document, leading to less than one rejuvena-
tion step per transition. Future work should care-
fully note this ratio: sampling history much more
often than new states improves performance but
contradicts the intuition behind particle filters.
We have also shown that tuning the initial model
using in-sample NMI or held-out perplexity can
improve mean NMI and reduce variance. Perplex-
ity (or likelihood) is often used to estimate model
performance in LDA (Blei et al, 2003; Griffiths
and Steyvers, 2004; Wallach et al, 2009; Hoff-
man et al, 2010), and does not compare the in-
ferred model against gold-standard labels, yet it
appears to be a good proxy for NMI in our experi-
ment. Thus, if initialization continues to be crucial
to performance, at least we may have the flexibil-
ity of initializing without gold-standard labels.
We have focused on NMI as our evaluation met-
ric for comparison with Canini et al (2009). How-
ever, evaluation of topic models is a subject of con-
siderable debate (Wallach et al, 2009; Yao et al,
2009; Newman et al, 2010; Mimno et al, 2011)
and it may be informative to investigate the effects
of initialization and rejuvenation using other met-
rics such as perplexity or semantic coherence.
7 Conclusion
We have proposed reservoir sampling for reduc-
ing the storage complexity of a particle filter from
linear to constant. This work was motivated as
an expected improvement on the model of Canini
et al (2009). However, in the process of estab-
lishing an empirical baseline we discovered that
rejuvenation does not play a significant role in
the experiments of Canini et al (2009). More-
over, we found that performance of the particle
filter was strongly affected by the random initial-
ization of the model, and suggested a simple ap-
proach to reduce the variability therein without
using additional data. In conclusion, it is now
an open question whether?and if so, under what
assumptions?rejuvenation benefits particle filters
for LDA and similar static Bayesian models.
Acknowledgments We thank Frank Ferraro,
Keith Levin, and Mark Dredze for discussions.
4
The ESS threshold isP ; the rejuvenation sequence is 100
or 1600 utterances, almost one sixth of the training data.
450
References
Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric P.
Xing, Alexander J. Smola, and Choon Hui Teo.
2011. Unified analysis of streaming news. In Pro-
ceedings of the 20th International World Wide Web
Conference (WWW), pages 267?276.
Arindam Banerjee and Sugato Basu. 2007. Topic
models over text streams: A study of batch and on-
line unsupervised learning. In Proceedings of the
7th SIAM International Conference on Data Mining
(SDM), pages 431?436.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022, Jan.
Benjamin B?orschinger and Mark Johnson. 2011. A
particle filter algorithm for Bayesian wordsegmen-
tation. In Proceedings of the Australasian Lan-
guage Technology Association Workshop (ALTA),
pages 10?18.
Benjamin B?orschinger and Mark Johnson. 2012. Us-
ing rejuvenation to improve particle filtering for
Bayesian word segmentation. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 85?89.
Tamara Broderick, Nicholas Boyd, Andre Wibisono,
Ashia C. Wilson, and Michael I. Jordan. 2013.
Streaming variational Bayes. In Advances in Neu-
ral Information Processing Systems 26 (NIPS).
Kevin R. Canini, Lei Shi, and Thomas L. Griffiths.
2009. Online inference of topics with latent Dirich-
let alocation. In Proceedings of the 12th Inter-
national Conference on Artificial Intelligence and
Statistics (AISTATS).
Arnaud Doucet, Nando de Freitas, Kevin Murphy, and
Stuart Russell. 2000. Rao-Blackwellised particle
filtering for dynamic Bayesian networks. In Pro-
ceedings of the 16th Conference on Uncertainty in
Artificial Intelligence (UAI), pages 176?183.
Arnaud Doucet, Nando de Freitas, and Neil Gordon,
editors. 2001. Sequential Monte Carlo Methods in
Practice. Springer, New York.
Walter R. Gilks and Carlo Berzuini. 2001. Following a
moving target?Monte Carlo inference for dynamic
Bayesian models. Journal of the Royal Statistical
Society, 63(1):127?146.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl 1):5228?5235, Apr.
Matthew D. Hoffman, David M. Blei, and Francis
Bach. 2010. Online learning for latent Dirichlet
allocation. In Advances in Neural Information Pro-
cessing Systems 23 (NIPS).
Matthew D. Hoffman, David M. Blei, Chong Wang,
and John Paisley. 2013. Stochastic variational in-
ference. Journal of Machine Learning Research,
14:1303?1347, May.
Jun S. Liu and Rong Chen. 1998. Sequential Monte
Carlo methods for dynamic systems. Journal of
the American Statistical Association, 93(443):1032?
1044, Sep.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods on Natural Language Processing (EMNLP),
pages 262?272.
David Mimno, Matthew D. Hoffman, and David M.
Blei. 2012. Sparse stochastic inference for la-
tent Dirichlet alocation. In Proceedings of the
29th International Conference on Machine Learning
(ICML).
Thomas Minka and John Lafferty. 2002. Expectation-
propagation for the generative aspect model. In Pro-
ceedings of the 18th Conference on Uncertainty in
Artificial Intelligence (UAI), pages 352?359.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In Human Language Technologies: 11th
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT), pages 100?108.
Michael K. Pitt and Neil Shephard. 1999. Filtering via
simulation: Auxiliary particle filters. Journal of the
American Statistical Association, 94(446):590?599,
Jun.
James G. Scott and Jason Baldridge. 2013. A recur-
sive estimate for the predictive likelihood in a topic
model. In Proceedings of the 16th International
Conference on Artificial Intelligence and Statistics
(AISTATS).
Jeffrey S. Vitter. 1985. Random sampling with a reser-
voir. ACM Transactions on Mathematical Software,
11(1):37?57, Mar.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In Proceedings of the 26th Interna-
tional Conference on Machine Learning (ICML).
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. In 15th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 937?946.
451
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 687?692,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Exponential Reservoir Sampling for Streaming Language Models
Miles Osborne
?
School of Informatics
University of Edinburgh
Ashwin Lall
Mathematics and Computer Science
Denison University
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Abstract
We show how rapidly changing textual
streams such as Twitter can be modelled in
fixed space. Our approach is based upon
a randomised algorithm called Exponen-
tial Reservoir Sampling, unexplored by
this community until now. Using language
models over Twitter and Newswire as a
testbed, our experimental results based on
perplexity support the intuition that re-
cently observed data generally outweighs
that seen in the past, but that at times,
the past can have valuable signals enabling
better modelling of the present.
1 Introduction
Work by Talbot and Osborne (2007), Van Durme
and Lall (2009) and Goyal et al (2009) consid-
ered the problem of building very large language
models via the use of randomized data structures
known as sketches.
1
While efficient, these struc-
tures still scale linearly in the number of items
stored, and do not handle deletions well: if pro-
cessing an unbounded stream of text, with new
words and phrases being regularly added to the
model, then with a fixed amount of space, errors
will only increase over time. This was pointed
out by Levenberg and Osborne (2009), who inves-
tigated an alternate approach employing perfect-
hashing to allow for deletions over time. Their
deletion criterion was task-specific and based on
how a machine translation system queried a lan-
guage model.
?
Corresponding author: miles@inf.ed.ac.uk
1
Sketches provide space efficiencies that are measured on
the order of individual bits per item stored, but at the cost
of being lossy: sketches trade off space for error, where the
less space you use, the more likely you will get erroneous
responses to queries.
Here we ask what the appropriate selection
criterion is for streaming data based on a non-
stationary process, when concerned with an in-
trinsic measure such as perplexity. Using Twitter
and newswire, we pursue this via a sampling strat-
egy: we construct models over sentences based on
a sample of previously observed sentences, then
measure perplexity of incoming sentences, all on
a day by day, rolling basis. Three sampling ap-
proaches are considered: A fixed-width sliding
window of most recent content, uniformly at ran-
dom over the stream and a biased sample that
prefers recent history over the past.
We show experimentally that a moving window
is better than uniform sampling, and further that
exponential (biased) sampling is best of all. For
streaming data, recently encountered data is valu-
able, but there is also signal in the previous stream.
Our sampling methods are based on reser-
voir sampling (Vitter, 1985), a popularly known
method in some areas of computer science, but
which has seen little use within computational lin-
guistics.
2
Standard reservoir sampling is a method
for maintaining a uniform sample over a dynamic
stream of elements, using constant space. Novel
to this community, we consider a variant owing to
Aggarwal (2006) which provides for an exponen-
tial bias towards recently observed elements. This
exponential reservoir sampling has all of the guar-
antees of standard reservoir sampling, but as we
show, is a better fit for streaming textual data. Our
approach is fully general and can be applied to any
streaming task where we need to model the present
and can only use fixed space.
2
Exceptions include work by Van Durme and Lall (2011)
and Van Durme (2012), aimed at different problems than that
explored here.
687
2 Background
We address two problems: language changes over
time, and the observation that space is a problem,
even for compact sketches.
Statistical language models often assume either
a local Markov property (when working with ut-
terances, or sentences), or that content is gener-
ated fully i.i.d. (such as in document-level topic
models). However, language shows observable
priming effects, sometimes called triggers, where
the occurrence of a given term decreases the sur-
prisal of some other term later in the same dis-
course (Lau et al, 1993; Church and Gale, 1995;
Beeferman et al, 1997; Church, 2000). Conven-
tional cache and trigger models typically do not
deal with new terms and can be seen as adjusting
the parameters of a fixed model.
Accounting for previously unseen entries in a
language model can be naively simple: as they ap-
pear in new training data, add them to the model!
However in practice we are constrained by avail-
able space: how many unique phrases can we
store, given the target application environment?
Our work is concerned with modeling language
that might change over time, in accordance with
current trending discourse topics, but under a strict
space constraint. With a fixed amount of memory
available, we cannot allow our list of unique words
or phrases to grow over time, even while new top-
ics give rise to novel names of people, places, and
terms of interest. Thus we need an approach that
keeps the size of the model constant, but that is
geared to what is being discussed now, as com-
pared to some time in the past.
3 Reservoir Sampling
3.1 Uniform Reservoir Sampling
The reservoir sampling algorithm (Vitter, 1985) is
the classic method of sampling without replace-
ment from a stream in a single pass when the
length of the stream is of indeterminate or un-
bounded length. Say that the size of the desired
sample is k. The algorithm proceeds by retain-
ing the first k items of the stream and then sam-
pling each subsequent element with probability
f(k, n) = k/n, where n is the length of the stream
so far. (See Algorithm 1.) It is easy to show via in-
duction that, at any time, all the items in the stream
so far have equal probability of appearing in the
reservoir.
The algorithm processes the stream in a single
pass?that is, once it has processed an item in the
stream, it does not revisit that item unless it is
stored in the reservoir. Given this restriction, the
incredible feature of this algorithm is that it is able
to guarantee that the samples in the reservoir are a
uniformly random sample with no unintended bi-
ases even as the stream evolves. This makes it an
excellent candidate for situations when the stream
is continuously being updated and it is computa-
tionally infeasible to store the entire stream or to
make more than a single pass over it. Moreover,
it is an extremely efficient algorithm as it requires
O(1) time (independent of the reservoir size and
stream length) for each item in the stream.
Algorithm 1 Reservoir Sampling Algorithm
Parameters:
k: maximum size of reservoir
1: Initialize an empty reservoir (any container
data type).
2: n := 1
3: for each item in the stream do
4: if n < k then
5: insert current item into the reservoir
6: else
7: with probability f(n, k), eject an ele-
ment of the reservoir chosen uniformly
at random and insert current item into the
reservoir
8: n := n+ 1
3.2 Non-uniform Reservoir Sampling
Here we will consider generalizations of the reser-
voir sampling algorithm in which the sample
items in the reservoir are more biased towards the
present. Put another way, we will continuously
decay the probability that an older item will ap-
pear in the reservoir. Models produced using such
biases put more modelling stress on the present
than models produced using data that is selected
uniformly from the stream. The goal here is to
continuously update the reservoir sample in such
a way that the decay of older items is done consis-
tently while still maintaining the benefits of reser-
voir sampling, including the single pass and mem-
ory/time constraints.
The time-decay scheme we will study in this
paper is exponential bias towards newer items in
the stream. More precisely, we wish for items that
688
0 2000 4000 6000 8000 10000time
0.0
0.2
0.4
0.6
0.8
1.0
prob
abil
ity o
f ap
pea
ring
 in r
ese
rvoi
r
uniformexponential (various beta)
Figure 1: Different biases for sampling a stream
have age a in the stream to appear with probability
g(a) = c ? exp (?a/?),
where a is the age of the item, ? is a scale param-
eter indicating how rapidly older items should be
deemphasized, and c is a normalization constant.
To give a sense of what these time-decay proba-
bilities look like, some exponential distributions
are plotted (along with the uniform distribution)
in Figure 1.
Aggarwal (2006) studied this problem and
showed that by altering the sampling probability
(f(n, k) in Algorithm 1) in the reservoir sampling
algorithm, it is possible to achieve different age-
related biases in the sample. In particular, he
showed that by setting the sampling probability to
the constant function f(n, k) = k/?, it is possible
to approximately achieve exponential bias in the
sample with scale parameter ? (Aggarwal, 2006).
Aggarwal?s analysis relies on the parameter ? be-
ing very large. In the next section we will make
the analysis more precise by omitting any such as-
sumption.
3.3 Analysis
In this section we will derive an expression for the
bias introduced by an arbitrary sampling function
f in Algorithm 1. We will then use this expression
to derive the precise sampling function needed to
achieve exponential decay.
3
Careful selection of
f allows us to achieve anything from zero decay
(i.e., uniform sampling of the entire stream) to
exponential decay. Once again, note that since
we are only changing the sampling function, the
3
Specifying an arbitrary decay function remains an open
problem.
one-pass, memory- and time-efficient properties
of reservoir sampling are still being preserved.
In the following analysis, we fix n to be the size
of the stream at some fixed time and k to be the
size of the reservoir. We assume that the ith el-
ement of the stream is sampled with probability
f(i, k), for i ? n. We can then derive the proba-
bility that an element of age a will still be in the
reservoir as
g(a) = f(n? a, k)
n
?
t=n?a+1
(
1?
f(t, k)
k
)
,
since it would have been sampled with probability
f(n? a, k) and had independent chances of being
replaced at times t = n?a+1, . . . , n with proba-
bility f(t, k)/k. For instance, when f(x, k) =
k
x
,
the above formula simplifies down to g(a) =
k
n
(i.e., the uniform sampling case).
For the exponential case, we fix the sampling
rate to some constant f(n, k) = p
k
, and we wish
to determine what value to use for p
k
to achieve
a given exponential decay rate g(a) = ce
?a/?
,
where c is the normalization constant (to make g a
probability distribution) and ? is the scale param-
eter of the exponential distribution. Substituting
f(n, k) = p
k
in the above formula and equating
with the decay rate, we get that p
k
(1 ? p
k
/k)
a
?
ce
?a/?
, which must hold true for all possible val-
ues of a. After some algebra, we get that when
f(x, k) = p
k
= k(1 ? e
?1/?
), the probability
that an item with age a is included in the reser-
voir is given by the exponential decay rate g(a) =
p
k
e
?a/?
. Note that, for very large values of ?, this
probability is approximately equal to p
k
? k/?
(by using the approximation e
?x
? 1 ? x, when
|x| is close to zero), as given by Aggarwal, but our
formula gives the precise sampling probability and
works even for smaller values of ?.
4 Experiments
Our experiments use two streams of data to illus-
trate exponential sampling: Twitter and a more
conventional newswire stream. The Twitter data is
interesting as it is very multilingual, bursty (for ex-
ample, it talks about memes, breaking news, gos-
sip etc) and written by literally millions of differ-
ent people. The newswire stream is a lot more well
behaved and serves as a control.
4.1 Data, Models and Evaluation
We used one month of chronologically ordered
Twitter data and divided it into 31 equal sized
689
Stream Interval Total (toks) Test (toks)
Twitter Dec 2013 3282M 105M
Giga 1994 ? 2010 635.5M 12M
Table 1: Stream statistics
blocks (roughly corresponding with days). We
also used the AFP portion of the Giga Word corpus
as another source of data that evolves at a slower
pace. This data was divided into 50 equal sized
blocks. Table 1 gives statistics about the data. As
can be seen, the Twitter data is vastly larger than
newswire and arrives at a much faster rate.
We considered the following models. Each one
(apart from the exact model) was trained using the
same amount of data:
? Static. This model was trained using data
from the start of the duration and never var-
ied. It is a baseline.
? Exact. This model was trained using all
available data from the start of the stream and
acts as an upper bound on performance.
? Moving Window. This model used all data
in a fixed-sized window immediately before
the given test point.
? Uniform. Here, we use uniform reservoir
sampling to select the data.
? Exponential. Lastly, we use exponen-
tial reservoir sampling to select the data.
This model is parameterised, indicating how
strongly biased towards the present the sam-
ple will be. The ? parameter is a multiplier
over the reservoir length. For example, a ?
value of 1.1 with a sample size of 10 means
the value is 11. In general, ? always needs to
be bigger than the reservoir size.
We sample over whole sentences (or Tweets)
and not ngrams.
4
Using ngrams instead would
give us a finer-grained control over results, but
would come at the expense of greatly complicat-
ing the analysis. This is because we would need to
reason about not just a set of items but a multiset
of items. Note that because the samples are large
5
,
variations across samples will be small.
4
A consequence is that we do not guarantee that each sam-
ple uses exactly the same number of grams. This can be tack-
led by randomly removing sampled sentences.
5
Each day consists of approximately four million Tweets
and we evaluate on a whole day.
Day Uniform ? value
? 1.1 1.3 1.5 2.0
5 619.4 619.4 619.4 619.4 619.4
6 601.0 601.0 603.8 606.6 611.1
7 603.0 599.4 602.7 605.6 612.1
8 614.6 607.7 611.9 614.3 621.6
9 623.3 611.5 615.0 620.0 628.1
10 656.2 643.1 647.2 650.1 658.0
12 646.6 628.9 633.0 636.5 644.6
15 647.7 628.7 630.4 634.5 641.6
20 636.7 605.3 608.4 610.8 618.4
25 631.5 601.9 603.3 604.4 610.0
Table 2: Perplexities for different ? values over
Twitter (sample size = five days). Lower is better.
We test the model on unseen data from all of the
next day (or block). Afterwards, we advance to the
next day (block) and repeat, potentially incorpo-
rating the previously seen test data into the current
training data. Evaluation is in terms of perplexity
(which is standard for language modelling).
We used KenLM for building models and eval-
uating them (Heafield, 2011). Each model was
an unpruned trigram, with Kneser-Ney smoothing.
Increasing the language model order would not
change the results. Here the focus is upon which
data is used in a model (that is, which data is added
and which data is removed) and not upon making
it compact or making retraining efficient.
4.2 Varying the ? Parameter
Table 2 shows the effect of varying the ? param-
eter (using Twitter). The higher the ? value, the
more uniform the sampling. As can be seen, per-
formance improves when sampling becomes more
biased. Not shown here, but for Twitter, even
smaller ? values produce better results and for
newswire, results degrade. These differences are
small and do not affect any conclusions made here.
In practise, this value would be set using a devel-
opment set and to simplify the rest of the paper, all
other experiments use the same ? value (1.1).
4.3 Varying the Amount of Data
Does the amount of data used in a model affect re-
sults? Table 3 shows the results for Twitter when
varying the amount of data in the sample and us-
ing exponential sampling (? = 1.1). In paren-
theses for each result, we show the corresponding
moving window results. As expected, using more
data improves results. We see that for each sample
size, exponential sampling outperforms our mov-
ing window. In the limit, all sampling methods
would produce the same results.
690
Day Sample Size (Days)
1 2 3
5 652.5 (661.2) 629.1 (635.8) 624.8 (625.9)
6 635.4 (651.6) 611.6 (620.8) 604.0 (608.7)
7 636.0 (647.3) 611.0 (625.2) 603.7 (612.5)
8 654.8 (672.7) 625.6 (641.6) 614.6 (626.9)
9 653.9 (662.8) 628.3 (643.0) 618.8 (632.2)
10 679.1 (687.8) 654.3 (666.8) 646.6 (659.7)
12 671.1 (681.9) 645.8 (658.6) 633.8 (647.5)
15 677.7 (697.9) 647.4 (668.0) 636.4 (652.6)
20 648.1 (664.6) 621.4 (637.9) 612.2 (627.6)
25 657.5 (687.5) 625.3 (664.4) 613.4 (641.8)
Table 3: Perplexities for different sample sizes
over Twitter. Lower is better.
4.4 Alternative Sampling Strategies
Table 4 compares the two baselines against the two
forms of reservoir sampling. For Twitter, we see
a clear recency effect. The static baseline gets
worse and worse as it recedes from the current
test point. Uniform sampling does better, but it
in turn is beaten by the Moving Window Model.
However, this in turn is beaten by our exponential
reservoir sampling.
Day Static Moving Uniform Exp Exact
5 619.4 619.4 619.4 619.4 619.4
6 664.8 599.7 601.8 601.0 597.6
7 684.4 602.8 603.0 599.3 595.6
8 710.1 612.0 614.6 607.7 603.5
9 727.0 617.9 623.3 613.0 608.7
10 775.6 651.2 656.2 642.0 640.5
12 776.7 639.0 646.6 628.7 627.5
15 777.1 638.3 647.7 626.7 627.3
20 800.9 619.1 636.7 604.9 607.3
25 801.4 621.7 631.5 601.5 597.6
Table 4: Perplexities for differently selected sam-
ples over Twitter (sample size = five days, ? =
1.1). Results in bold are the best sampling results.
Lower is better.
4.5 GigaWord
Twitter is a fast moving, rapidly changing multi-
lingual stream and it is not surprising that our ex-
ponential reservoir sampling proves beneficial. Is
it still useful for a more conventional stream that
is drawn from a much smaller population of re-
porters? We repeated our experiments, using the
same rolling training and testing evaluation as be-
fore, but this time using newswire for data.
Table 5 shows the perplexities when using the
Gigaword stream. We see the same general trends,
albeit with less of a difference between exponen-
tial sampling and our moving window. Perplexity
values are all lower than for Twitter.
Block Static Moving Uniform Exp
11 416.5 381.1 382.0 382.0
15 436.7 353.3 357.5 352.8
20 461.8 347.0 354.4 344.6
25 315.6 214.9 222.2 211.3
30 319.1 200.5 213.5 199.5
40 462.5 304.4 313.2 292.9
Table 5: Perplexities for differently selected sam-
ples over Gigaword (sample size = 10 blocks, ? =
1.1). Lower is better.
4.6 Why does this work for Twitter?
Although the perplexity results demonstrate that
exponential sampling is on average beneficial, it
is useful to analyse the results in more detail. For
a large stream size (25 days), we built models us-
ing uniform, exponential (? = 1.1) and our moving
window sampling methods. Each approach used
the same amount of data. For the same test set
(four million Tweets), we computed per-Tweet log
likelihoods and looked at the difference between
the model that best explained each tweet and the
second best model (ie the margin). This gives us
an indication of how much a given model better
explains a given Tweet. Analysing the results, we
found that most gains came from short grams and
very few came from entire Tweets being reposted
(or retweeted). This suggests that the Twitter re-
sults follow previously reported observations on
how language can be bursty and not from Twitter-
specific properties.
5 Conclusion
We have introduced exponential reservoir sam-
pling as an elegant way to model a stream of un-
bounded size, yet using fixed space. It naturally al-
lows one to take account of recency effects present
in many natural streams. We expect that our lan-
guage model could improve other Social Media
tasks, for example lexical normalisation (Han and
Baldwin, 2011) or even event detection (Lin et
al., 2011). The approach is fully general and not
just limited to language modelling. Future work
should look at other distributions for sampling and
consider tasks such as machine translation over
Social Media.
Acknowledgments This work was carried out
when MO was on sabbatical at the HLTCOE and
CLSP.
691
References
Charu C Aggarwal. 2006. On biased reservoir sam-
pling in the presence of stream evolution. In Pro-
ceedings of the 32nd international conference on
Very large data bases, pages 607?618. VLDB En-
dowment.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. A model of lexical attractions and repulsion.
In Proceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics and Eighth
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 373?380.
Association for Computational Linguistics.
K. Church and W. A. Gale. 1995. Poisson mixtures.
Natural Language Engineering, 1:163?190.
Kenneth W Church. 2000. Empirical estimates of
adaptation: the chance of two noriegas is closer to
p/2 than p 2. In Proceedings of the 18th conference
on Computational linguistics-Volume 1, pages 180?
186. Association for Computational Linguistics.
Amit Goyal, Hal Daum?e III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language Modeling. In Proceedings of NAACL.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 368?378, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Raymond Lau, Ronald Rosenfeld, and SaIim Roukos.
1993. Trigger-based language models: A maximum
entropy approach. In Acoustics, Speech, and Signal
Processing, 1993. ICASSP-93., 1993 IEEE Interna-
tional Conference on, volume 2, pages 45?48. IEEE.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 756?764. Association for Compu-
tational Linguistics.
Jimmy Lin, Rion Snow, and William Morgan. 2011.
Smoothing techniques for adaptive online language
models: topic tracking in tweet streams. In Proceed-
ings of the 17th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 422?429. ACM.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2009. Proba-
bilistic Counting with Randomized Storage. In Pro-
ceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2011. Effi-
cient online locality sensitive hashing via reservoir
counting. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 18?23. Association for Computa-
tional Linguistics.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 48?58. Association for
Computational Linguistics.
Jeffrey S. Vitter. 1985. Random sampling with a reser-
voir. ACM Trans. Math. Softw., 11:37?57, March.
692
Open Knowledge Extraction
through Compositional
Language Processing
Benjamin Van Durme
Lenhart Schubert
University of Rochester (USA)
email: vandurme@cs.rochester.edu
Abstract
We present results for a system designed to perform Open Knowledge
Extraction, based on a tradition of compositional language processing,
as applied to a large collection of text derived from the Web. Evaluation
through manual assessment shows that well-formed propositions of rea-
sonable quality, representing general world knowledge, given in a logical
form potentially usable for inference, may be extracted in high volume
from arbitrary input sentences. We compare these results with those ob-
tained in recent work on Open Information Extraction, indicating with
some examples the quite different kinds of output obtained by the two
approaches. Finally, we observe that portions of the extracted knowledge
are comparable to results of recent work on class attribute extraction.
239
240 Van Durme and Schubert
1 Introduction
Several early studies in large-scale text processing (Liakata and Pulman, 2002; Gildea
and Palmer, 2002; Schubert, 2002) showed that having access to a sentence?s syn-
tax enabled credible, automated semantic analysis. These studies suggest that the
use of increasingly sophisticated linguistic analysis tools could enable an explosion
in available symbolic knowledge. Nonetheless, much of the subsequent work in ex-
traction has remained averse to the use of the linguistic deep structure of text; this
decision is typically justified by a desire to keep the extraction system as computa-
tionally lightweight as possible.
The acquisition of background knowledge is not an activity that needs to occur
online; we argue that as long as the extractor will finish in a reasonable period of
time, the speed of such a system is an issue of secondary importance. Accuracy and
usefulness of knowledge should be of paramount concern, especially as the increase
in available computational power makes such ?heavy? processing less of an issue.
The system explored in this paper is designed for Open Knowledge Extraction: the
conversion of arbitrary input sentences into general world knowledge represented in a
logical form possibly usable for inference. Results show the feasibility of extraction
via the use of sophisticated natural language processing as applied to web texts.
2 Previous Work
Given that the concern here is with open knowledge extraction, the myriad projects
that target a few prespecified types of relations occurring in a large corpus are set
aside.
Among early efforts, one might count work on deriving selectional preferences
(e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicate-
argument structure (e.g., Abney (1996)) as steps in the direction of open knowledge
extraction, though typically few of the tuples obtained (often a type of subject plus a
verb, or a verb plus a type of object) can be interpreted as complete items of world
knowledge. Another somewhat relevant line of research was initiated by Zelle and
Mooney (1996), concerned with learning to map NL database queries into formal DB
queries (a kind of semantic interpretation). This was pursued further, for instance,
by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning
log-linear models, or (in the latter case) synchronous CF grammars augmented with
lambda operators, for mapping English queries to DB queries. However, this approach
requires annotation of texts with logical forms, and extending this approach to gen-
eral texts would seemingly require a massive corpus of hand-annotated text ? and the
logical forms would have to cover far more phenomena than are found in DB queries
(e.g., attitudes, generalized quantifiers, etc.).
Another line of relevant work is that on semantic role labelling. One early example
was MindNet (Richardson et al, 1998), which was based on collecting 24 semantic
role relations from MRDs such as the American Heritage Dictionary. More recent
representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer
(2002), and Punyakanok et al (2008). The relevance of this work comes from the fact
that identifying the arguments of the verbs in a sentence is a first step towards forming
predications, and these may in many cases correspond to items of world knowledge.
Open Knowledge Extraction through Compositional Language Processing 241
Liakata and Pulman (2002) built a system for recovering Davidsonian predicate-
argument structures from the Penn Treebank through the application of a small set
of syntactic templates targeting head nodes of verb arguments. The authors illustrate
their results for the sentence ?Apple II owners, for example, had to use their television
sets as screens and stored data on audiocassettes? (along with the Treebank anno-
tations); they obtain the following QLF, where verb stems serve as predicates, and
arguments are represented by the head words of the source phrases:
have(e1,owner, (use(e3,owner,set), and as(e3,screen)),
and (store(e2,owner,datum), and on(e2,audiocassette)))
For a test set of 100 Treebank sentences, the authors report recall figures for vari-
ous aspects of such QLFs ranging from 87% to 96%. While a QLF like the one above
cannot in itself be regarded as world knowledge, one can readily imagine postprocess-
ing steps that could in many cases obtain credible propositions from such QLFs. How
accurate the results would be with machine-parsed sentences is at this point unknown.
In the same year, Schubert (2002) described a project aimed directly at the extrac-
tion of general world knowledge from Treebank text, and Schubert and Tong (2003)
provided the results of hand-assessment of the resulting propositions. The Brown cor-
pus yielded about 117,000 distinct simple propositions (somewhat more than 2 per
sentence, of variable quality). Like Liakata and Pulman?s approach the method relied
on the computation of unscoped logical forms from Treebank trees, but it abstracted
propositional information along the way, typically discarding modifiers at deeper lev-
els from LFs at higher levels, and also replacing NPs (including named entities) by
their types as far as possible. Judges found about 2/3 of the output propositions (when
automatically verbalized in English) acceptable as general claims about the world. The
next section provides more detail on the extraction system, called KNEXT, employed
in this work.
Clark et al (2003), citing the 2002 work of Schubert, report undertaking a sim-
ilar extraction effort for the 2003 Reuters corpus, based on parses produced by the
Boeing parser, (see Holmback et al (2000)), and obtained 1.1 million subject-verb-
object fragments. Their goal was eventually to employ such tuples as common-sense
expectations to guide the interpretation of text and the retrieval of possibly relevant
knowledge in question-answering. This goal, unlike the goal of inferential use of ex-
tracted knowledge, does not necessarily require the extracted information to be in the
form of logical propositions. Still, since many of their tuples were in a form that could
be quite directly converted into propositional forms similar to those of Schubert, their
work indicated the potential for scalability in parser-based approaches to information
extraction or knowledge extraction.
A recent project aimed at large-scale, open extraction of tuples of text fragments
representing verbal predicates and their arguments is TextRunner (Banko et al, 2007).
This systems does part-of-speech tagging of a corpus, identifies noun phrases with a
noun phrase chunker, and then uses tuples of nearby noun phrases within sentences to
form apparent relations, using intervening material to represent the relation. Apparent
modifiers such as prepositional phrases after a noun or adverbs are dropped. Every
candidate relational tuple is classified as trustworthy (or not) by a Bayesian classifier,
using such features as parts of speech, number of relevant words between the noun
242 Van Durme and Schubert
phrases, etc. The Bayesian classifier is obtained through training on a parsed corpus,
where a set of heuristic rules determine the trustworthiness of apparent relations be-
tween noun phrases in that corpus. As a preview of an example we will discuss later,
here are two relational tuples in the format extracted by TextRunner:1
(the people) use (force),
(the people) use (force) to impose (a government).
No attempt is made to convert text fragments such as ?the people? or ?use _ to impose?
into logically formal terms or predicates. Thus much like semantic role-labelling sys-
tems, TextRunner is an information extraction system, under the terminology used
here; however, it comes closer to knowledge extraction than the former, in that it often
strips away much of the modifying information of complex terms (e.g., leaving just a
head noun phrase).
2.1 KNEXT
KNEXT (Schubert, 2002) was originally designed for application to collections of
manually annotated parse trees, such as the Brown corpus. In order to extract knowl-
edge from larger text collections, the system has been extended for processing arbi-
trary text through the use of third-party parsers. In addition, numerous improvements
have been made to the semantic interpretation rules, the filtering techniques, and other
components of the system. The extraction procedure is as follows:
1. Parse each sentence using a Treebank-trained parser (Collins, 1997; Charniak,
1999).
2. Preprocess the parse tree, for better interpretability (e.g., distinguish different
types of SBAR phrases and different types of PPs, identify temporal phrases,
etc.).
3. Apply a set of 80 interpretive rules for computing unscoped logical forms (ULFs)
of the sentence and all lower-level constituents in a bottom-up sweep; at the
same time, abstract and collect phrasal logical forms that promise to yield
stand-alone propositions (e.g., ULFs of clauses and of pre- or post-modified
nominals are prime candidates). The ULFs are rendered in Episodic Logic
(e.g., (Schubert and Hwang, 2000)), a highly expressive representation allowing
for generalized quantifiers, predicate modifiers, predicate and sentence reifica-
tion operators, and other devices found in NL. The abstraction process drops
modifiers present in lower-level ULFs (e.g., adjectival premodifiers of nominal
predicates) in constructing higher-level ULFs (e.g., for clauses). In addition,
named entities are generalized as far as possible using several gazetteers (e.g.,
for male and female given names, US states, world cities, actors, etc.) and some
morphological processing.
4. Construct complete sentential ULFs from the phrasal ULFs collected in the pre-
vious step; here some filtering is performed to exclude vacuous or ill-formed
results.
1Boldface indicates items recognized as head nouns.
Open Knowledge Extraction through Compositional Language Processing 243
5. Render the propositions from the previous step in (approximate) English; again
significant heuristic filtering is done here.
As an example of KNEXT output, the sentence:
Cock fights, however, are still legal in six of the United States, perhaps
because we still eat chicken regularly, but no-longer dogs.
yields a pair of propositions expressed logically as:
[(K (NN cock.n (PLUR fight.n))) legal.a],
[(DET (PLUR person.n)) eat.v (K chicken.n)]
and these are automatically rendered in approximate English as:
COCK FIGHTS CAN BE LEGAL.
PERSONS MAY EAT CHICKEN.
As can be seen, KNEXT output does not conform to the ?relation, arg1, arg2, ...?, tuple
style of knowledge representation favored in information extraction (stemming from
that community?s roots in populating DB tables under a fixed schema). This is further
exemplified by the unscoped logical form:2
[(DET (PLUR person.n)) want.v (Ka (rid.a (of.p (DET dictator.n))))]
which is verbalized as PERSONS MAY WANT TO BE RID OF A DICTATOR and is sup-
ported by the text fragment:
... and that if the Spanish people wanted to be rid of Franco, they must
achieve this by ...
Later examples will be translated into a more conventional logical form.
One larger collection we have processed since the 2002-3 work on Treebank cor-
pora is the British National Corpus (BNC), consisting of 100 million words of mixed-
genre text passages. The quality of resulting propositions has been assessed by the
hand-judging methodology of Schubert and Tong (2003), yielding positive judge-
ments almost as frequently as for the Brown Treebank corpus. The next section,
concerned with the web corpus collected and used by Banko et al (2007), contains
a fuller description of the judging method. The BNC-based KB, containing 6,205,877
extracted propositions, is publicly searchable via a recently developed online knowl-
edge browser.3
2Where Ka is an action/attribute reification operator.
3http://www.cs.rochester.edu/u/vandurme/epik
244 Van Durme and Schubert
3 Experiments
The experiments reported here were aimed at a comparative assessment of linguisti-
cally based knowledge extraction (by KNEXT), and pattern-based information extrac-
tion (by TextRunner, and by another system, aimed at class attribute discovery). The
goal being to show that logically formal results (i.e. knowledge) based on syntactic
parsing may be obtained at a subjective level of accuracy similar to methods aimed
exclusively at acquiring correspondences between string pairs based on shallow tech-
niques.
Dataset Experiments were based on sampling 1% of the sentences from each doc-
ument contained within a corpus of 11,684,774 web pages harvested from 1,354,123
unique top level domains. The top five contributing domains made up 30% of the
documents in the collection.4 There were 310,463,012 sentences in all, the sample
containing 3,000,736. Of these, 1,373 were longer than a preset limit of 100 tokens,
and were discarded.5 Sentences containing individual tokens of length greater than
500 characters were similarly removed.6
As this corpus derives from the work of Banko et al (2007), each sentence in the
collection is paired with zero or more tuples as extracted by the TextRunner system.
Note that while websites such as Wikipedia.org contain large quantities of (semi-
)structured information stored in lists and tables, the focus here is entirely on natural
language sentences. In addition, as the extraction methods discussed in this paper do
not make use of intersentential features, the lack of sentence to sentence coherence
resulting from random sampling had no effect on the results.
ExtractionSentences were processed using the syntactic parser of Charniak (1999).
From the resultant trees, KNEXT extracted 7,406,371 propositions, giving a raw av-
erage of 2.47 per sentence. Of these, 4,151,779 were unique, so that the average
extraction frequency per sentence is 1.78 unique propositions. Post-processing left
3,975,197 items, giving a per sentence expectation of 1.32 unique, filtered proposi-
tions. Selected examples regarding knowledge about people appear in Table 1.
For the same sample, TextRunner extracted 6,053,983 tuples, leading to a raw av-
erage of 2.02 tuples per sentence. As described by its designers, TextRunner is an
information extraction system; one would be mistaken in using these results to say
that KNEXT ?wins? in raw extraction volume, as these numbers are not in fact directly
comparable (see section on Comparison).
Table 1: Verbalized propositions concerning the class PERSON
A PERSON MAY...
SING TO A GIRLFRIEND RECEIVE AN ORDER FROM A GENERAL KNOW STUFF PRESENT A PAPER
EXPERIENCE A FEELING CARRY IMAGES OF A WOMAN BUY FOOD PICK_UP A PHONE
WALK WITH A FRIEND CHAT WITH A MALE-INDIVIDUAL BURN A SAWMILL FEIGN A DISABILITY
DOWNLOAD AN ALBUM MUSH A TEAM OF (SEASONED SLED DOGS) RESPOND TO A QUESTION
SING TO A GIRLFRIEND OBTAIN SOME_NUMBER_OF (PERCULA CLOWNFISH) LIKE (POP CULTURE)
4en.wikipedia.org, www.answers.com, www.amazon.com, www.imdb.com, www.britannica.com
5Typically enumerations, e.g., There have been 29 MET deployments in the city of Florida since the
inception of the program : three in Ft. Pierce , Collier County , Opa Locka , ... .
6For example, Kellnull phenotypes can occur through splice site and splice-site / frameshift muta-
tions301,302 450039003[...]3000 premature stop codons and missense mutations.
Open Knowledge Extraction through Compositional Language Processing 245
1. A REASONABLE GENERAL CLAIM
e.g., A grand-jury may say a proposition
2. TRUE BUT TOO SPECIFIC TO BE USEFUL
e.g., Bunker walls may be decorated with seashells
3. TRUE BUT TOO GENERAL TO BE USEFUL
e.g., A person can be nearest an entity
4. SEEMS FALSE
e.g., A square can be round
5. SOMETHING IS OBVIOUSLY MISSING
e.g., A person may ask
6. HARD TO JUDGE
e.g., Supervision can be with a company
Figure 1: Instructions for categorical judging
Evaluation Extraction quality was determined through manual assessment of ver-
balized propositions drawn randomly from the results. Initial evaluation was done
using the method proposed in Schubert and Tong (2003), in which judges were asked
to label propositions according to their category of acceptability; abbreviated instruc-
tions may be seen in Figure 1.7 Under this framework, category one corresponds to
a strict assessment of acceptability, while an assignment to any of the categories be-
tween one and three may be interpreted as a weaker level of acceptance. As seen in
Table 2, average acceptability was judged to be roughly 50 to 60%, with associated
Kappa scores signalling fair (0.28) to moderate (0.48) agreement.
Table 2: Percent propositions labeled under the given category(s), paired with Fleiss?
Kappa scores. Results are reported both for the authors (judges one and two), along
with two volunteers
Category % Selected Kappa % Selected Kappa
1 49% 0.4017 50% 0.2822
1, 2, or 3 54% 0.4766 60% 0.3360
judges judges w/ volunteers
Judgement categories at this level of specificity are useful both for system analysis
at the development stage, as well as for training judges to recognize the disparate ways
in which a proposition may not be acceptable. However, due to the rates of agreement
observed, evaluation moved to the use of a five point sliding scale (Figure 2). This
scale allows for only a single axis of comparison, thus collapsing the various ways
in which a proposition may or may not be flawed into a single, general notion of
acceptability.
7Judges consisted of the authors and two volunteers, each with a background in linguistics and knowl-
edge representation.
246 Van Durme and Schubert
THE STATEMENT ABOVE IS A REASONABLY
CLEAR, ENTIRELY PLAUSIBLE GENERAL
CLAIM AND SEEMS NEITHER TOO SPECIFIC
NOR TOO GENERAL OR VAGUE TO BE USEFUL:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 2: Instructions for scaled judging
The authors judged 480 propositions sampled randomly from amongst bins corre-
sponding to frequency of support (i.e., the number of times a given proposition was
extracted). 60 propositions were sampled from each of 8 such ranges.8 As seen in
Figure 3, propositions that were extracted at least twice were judged to be more ac-
ceptable than those extracted only once. While this is to be expected, it is striking that
as frequency of support increased further, the level of judged acceptability remained
roughly the same.
4 Comparison
To highlight differences between an extraction system targeting knowledge (repre-
sented as logical statements) as compared to information (represented as segmented
text fragments), the output of KNEXT is compared to that of TextRunner for two select
inputs.
4.1 Basic
Consider the following sentence:
A defining quote from the book, ?An armed society is a polite society?,
is very popular with those in the United States who support the personal
right to bear arms.
From this sentence TextRunner extracts the tuples:9
(A defining quote) is a (polite society ?),
(the personal right) to bear (arms).
We might manually translate this into a crude sort of logical form:
IS-A(A-DEFINING-QUOTE, POLITE-SOCIETY-?),
TO-BEAR(THE-PERSONAL-RIGHT, ARMS).
8(0,20,21,23,24,26,28,210,212), i.e., (0,1], (1,2], (2,8], ... .
9Tuple arguments are enclosed in parenthesis, with the items recognized as head given in bold. All
non-enclosed, conjoining text makes up the tuple predicate.
Open Knowledge Extraction through Compositional Language Processing 247
0 2 4 6 8 10 12
5
4
3
2
1
Natural
Number of Classes (lg scale)
Av
er
ag
e 
As
se
ss
m
en
t
judge 1
judge 2
Figure 3: As a function of frequency of support, average assessment for propositions
derived from natural sentences
Better would be to consider only those terms classified as head, and make the assump-
tion that each tuple argument implicitly introduces its own quantified variable:
?x,y. QUOTE(x) & SOCIETY(y) & IS-A(x,y),
?x,y. RIGHT(x) & ARMS(y) & TO-BEAR(x,y).
Compare this to the output of KNEXT:10
?x. SOCIETY(x) & POLITE(x),
?x,y,z. THING-REFERRED-TO(x)& COUNTRY(y) & EXEMPLAR-OF(z,y) & IN(x,z),
?x. RIGHT(x) & PERSONAL(x),
?x,y. QUOTE(x) & BOOK(y) & FROM(x,y),
?x. SOCIETY(x) & ARMED(x),
which is automatically verbalized as:
A SOCIETY CAN BE POLITE,
A THING-REFERRED-TO CAN BE IN AN EXEMPLAR-OF A COUNTRY,
A RIGHT CAN BE PERSONAL,
A QUOTE CAN BE FROM A BOOK,
A SOCIETY CAN BE ARMED.
10For expository reasons, scoped, simplified versions of KNEXT?s ULFs are shown. More accurately
propositions are viewed as weak generic conditionals, with a non-zero lower bound on conditional fre-
quency, e.g., [?x. QUOTE(x)] ?0.1 [?y. BOOK(y) & FROM(x,y)], where x is dynamically bound in the
consequent.
248 Van Durme and Schubert
0 2 4 6 8 10 12
5
4
3
2
1
Core
Number of Classes (lg scale)
Av
er
ag
e 
As
se
ss
m
en
t
judge 1
judge 2
Figure 4: As a function of frequency of support, average assessment for propositions
derived from core sentences
4.2 Extended Tuples
While KNEXT uniquely recognizes, e.g., adjectival modification and various types of
possessive constructions, TextRunner more aggressively captures constructions with
extended cardinality. For example, from the following:
James Harrington in The Commonwealth of Oceana uses the term anarchy to
describe a situation where the people use force to impose a government on an
economic base composed of either solitary land ownership, or land in the owner-
ship of a few.
TextRunner extracts 19 tuples, some with three or even four arguments, thus aiming
beyond the binary relations that most current systems are limited to. That so many
tuples were extracted for a single sentence is explained by the fact that for most tuples
containing N > 2 arguments, TextRunner will also output the same tuple with N? 1
arguments, such as:
(the people) use (force),
(the people) use (force) to impose (a government),
(the people) use (force) to impose (a government) on (an economic base).
In addition, tuples may overlap, without one being a proper subset of another:
Open Knowledge Extraction through Compositional Language Processing 249
(a situation) where (the people) use (force),
(force) to impose (a government),
(a government) on (an economic base) composed of
(either solitary land ownership).
This overlap raises the question of how to accurately quantify system performance.
Whenmeasuring average extraction quality, should samples be drawn randomly across
tuples, or from originating sentences? If from tuples, then sample sets will be biased
(for good or ill) towards fragments derived from complex syntactic constructions. If
sentence based, the system fails to be rewarded for extracting as much from an input
as possible, as it may conservatively target only those constructions most likely to be
correct. With regards to volume, it is not clear whether adjuncts should each give
rise to additional facts added to a final total; optimal would be the recognition of such
optionality. Failing this, perhaps a tally may be based on unique predicate head terms?
As a point of merit according to its designers, TextRunner does not utilize a parser
(though as mentioned it does part of speech tagging and noun phrase chunking). This
is said to be justified in view of the known difficulties in reliably parsing open domain
text as well as the additional computational costs. However, a serious consequence
of ignoring syntactic structure is that incorrect bracketing across clausal boundaries
becomes all too likely, as seen for instance in the following tuple:
(James Harrington) uses (the term anarchy) to describe (a situation)
where (the people),
or in the earlier example where from the book, ?An armed society appears to have been
erroneously treated as a post-nominal modifier, intervening between the first argument
and the is-a predicate.
KNEXT extracted the following six propositions, the first of which was automati-
cally filtered in post-processing for being overly vague:11
? A MALE-INDIVIDUAL CAN BE IN A NAMED-ENTITY OF A NAMED-ENTITY,
A MALE-INDIVIDUAL MAY USE A (TERM ANARCHY),
PERSONS MAY USE FORCE,
A BASE MAY BE COMPOSED IN SOME WAY,
A BASE CAN BE ECONOMIC,
A (LAND OWNERSHIP) CAN BE SOLITARY.
5 Extracting from Core Sentences
We have noted the common argument against the use of syntactic analysis when per-
forming large-scale extraction viz. that it is too time consuming to be worthwhile.
We are skeptical of such a view, but decided to investigate whether an argument-
bracketing system such as TextRunner might be used as an extraction preprocessor to
limit what needed to be parsed.
For each TextRunner tuple extracted from the sampled corpus, core sentences were
constructed from the predicate and noun phrase arguments,12 which were then used as
input to KNEXT for extraction.
11The authors judge the third, fifth and sixth propositions to be both well-formed and useful.
12Minor automated heuristics were used to recover, e.g., missing articles dropped during tuple
construction.
250 Van Durme and Schubert
From 6,053,981 tuples came an equivalent number of core sentences. Note that
since TextRunner tuples may overlap, use of these reconstructed sentences may lead to
skewed propositional frequencies relative to ?normal? text. This bias was very much
in evidence in the fact that of the 10,507,573 propositions extracted from the core
sentences, only 3,787,701 remained after automatic postprocessing and elimination
of duplicates. This gives a per-sentence average of 0.63, as compared to 1.32 for the
original text.
While the raw number of propositions extracted for each version of the underlying
data look similar, 3,975,197 (natural) vs. 3,787,701 (core), the actual overlap was less
than would be expected. Just 2,163,377 propositions were extracted jointly from both
natural and core sentences, representing a percent overlap of 54% and 57% respec-
tively.
Table 3: Mean judgements (lower is better) on propositions sampled from those sup-
ported either exclusively by natural or core sentences, or those supported by both
Natural Core Overlap
judge 1 3.35 3.85 2.96
judge 2 2.95 3.59 2.55
Quality was evaluated by each judge assessing 240 randomly sampled propositions
for each of: those extracted exclusively from natural sentences, those extracted ex-
clusively from core sentences, those extracted from both (Table 3). Results show that
propositions exclusively derived from core sentences were most likely to be judged
poorly. Propositions obtained both by KNEXT alone and by KNEXT- processing of
TextRunner-derived core sentences (the overlap set) were particularly likely to be
judged favorably.
On the one hand, many sentential fragments ignored by TextRunner yield KNEXT
propositions; on the other, TextRunner?s output may be assembled to produce sen-
tences yielding propositions that KNEXT otherwise would have missed. Ad-hoc anal-
ysis suggests these new propositions derived with the help of TextRunner are a mix
of noise stemming from bad tuples (usually a result of the aforementioned incorrect
clausal bracketing), along with genuinely useful propositions coming from sentences
with constructions such as appositives or conjunctive enumerations where TextRun-
ner outguessed the syntactic parser as to the correct argument layout. Future work
may consider whether (syntactic) language models can be used to help prune core
sentences before being given to KNEXT.
Figure 4 differs from Figure 3 at low frequency of support. This is the result of the
partially redundant tuples extracted by TextRunner for complex sentences; the core
verb-argument structures are those most likely to be correctly interpreted by KNEXT,
while also being those most likely to be repeated across tuples for the same sentence.
6 Class Properties
While TextRunner is perhaps the extraction system most closely related to KNEXT
in terms of generality, there is also significant overlap with work on class attribute
Open Knowledge Extraction through Compositional Language Processing 251
Table 4: By frequency, the top ten attributes a class MAY HAVE. Emphasis added to
entries overlapping with those reported by Pas?ca and Van Durme. Results for starred
classes were derived without the use of prespecified lists of instances
COUNTRY government, war, team, history, rest, coast,census, economy, population, independence
DRUG*
side effects, influence, uses, doses,
manufacturer, efficacy, release, graduates,
plasma levels, safety
CITY* makeup, heart, center, population, history,side, places, name, edge, area
PAINTER* works, art, brush, skill, lives, sons,friend, order quantity, muse, eye
COMPANY windows, products, word, page, review, film,team, award, studio, director
extraction. Pas?ca and Van Durme (2007) recently described this task, going on to
detail an approach for collecting such attributes from search engine query logs. As an
example, the search query ?president of Spain? suggests that a Country may have a
president.
If one were to consider attributes to correspond, at least in part, to things a class
MAY HAVE, CAN BE, or MAY BE, then a subset of KNEXT?s results may be dis-
cussed in terms of this specialized task. For example, for the five classes used in those
authors? experiments, Table 4 contains the top ten most frequently extracted things
each class MAY HAVE, as determined by KNEXT, without any targeted filtering or
adaptation to the task.
Table 5: Mean assessed acceptability for properties occurring for a single class (1),
and more than a single class (2+). Final column contains Pearson correlation scores
1 2+ 1 2+ corr.
MAY HAVE 2.80 2.35 2.50 2.28 0.68
MAY BE 3.20 2.85 2.35 2.13 0.59
CAN BE 3.78 3.58 3.28 2.75 0.76
judge 1 judge 2
For each of these three types of attributive categories the authors judged 80 ran-
domly drawn propositions, constrained such that half (40 for each) were supported by
a single sentence, while the other half were required only to have been extracted at
least twice, but potentially many hundreds or even thousands of times. As seen in Ta-
ble 5, the judges were strongly correlated in their assessments, where for MAY HAVE
and MAY BE they were lukewarm (3.0) or better on the majority of those seen.
In a separate evaluation judges considered whether the number of classes sharing a
given attribute was indicative of its acceptability. For each unique attributive propo-
252 Van Durme and Schubert
0 2 4 6 8 10 12
5
4
3
2
1
Number of Classes (lg scale)
Av
er
ag
e 
As
se
ss
m
en
t
judge 1
judge 2
Figure 5: Mean quality of class attributes as a function of the number of classes sharing
a given property
sition the class in ?subject? position was removed, leaving fragments such as that
bracketed: A ROBOT [CAN BE SUBHUMAN]. These attribute fragments were tallied
and binned by frequency,13 with 40 then sampled from each. For a given attribute
selected, a single attributive proposition matching that fragment was randomly drawn.
For example, having selected the attribute CAN BE FROM A US-CITY, the proposition
SOME_NUMBER_OF SHERIFFS CAN BE FROM A US-CITY was drawn from the
390 classes sharing this property. As seen in Figure 5, acceptability rose as a property
became more common.
7 Conclusions
Work such as TextRunner (Banko et al, 2007) is pushing extraction researchers to
consider larger and larger datasets. This represents significant progress towards the
greater community?s goal of having access to large, expansive stores of general world
knowledge.
The results presented here support the position that advances made over decades
of research in parsing and semantic interpretation do have a role to play in large-
scale knowledge acquisition from text. The price paid for linguistic processing is not
excessive, and an advantage is the logical formality of the results, and their versatility,
as indicated by the application to class attribute extraction.
13Ranges: (0,20,21,23,26,?)
Open Knowledge Extraction through Compositional Language Processing 253
Acknowledgements We are especially grateful to Michele Banko and her colleagues
for generously sharing results, and to Daniel Gildea for helpful feedback. This work
was supported by NSF grants IIS-0328849 and IIS-0535105.
References
Abney, S. (1996). Partial Parsing via Finite-State Cascades. Natural Language Engi-
neering 2(4), 337?344.
Banko, M., M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni (2007). Open
Information Extraction from the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI-07), pp. 2670?2676.
Charniak, E. (1999). A Maximum-Entropy-Inspired Parser. In Proceedings of the 1st
Conference of the North American Chapter of the Association for Computational
Linguistics (NAACL 2000), pp. 132?139.
Clark, P., P. Harrison, and J. Thompson (2003). A Knowledge-Driven Approach to
Text Meaning Processing. In Proceedings of the HLT-NAACL 2003 Workshop on
Text Meaning, pp. 1?6.
Clark, S. and D. Weir (1999). An iterative approach to estimating frequencies over
a semantic hierarchy. In Proceedings of the 1999 Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing and Very Large Corpora
(EMNLP/VLC-99), pp. 258?265.
Collins, M. (1997). Three Generative, Lexicalised Models for Statistical Parsing. In
Proceedings of the 35th Annual Conference of the Association for Computational
Linguistics (ACL-97), pp. 16?23.
Gildea, D. and D. Jurafsky (2002). Automatic labeling of semantic roles. Computa-
tional Linguistics 28(3), 245?288.
Gildea, D. and M. Palmer (2002). The necessity of syntactic parsing for predicate
argument recognition. In Proceedings of the 40th Annual Conference of the Asso-
ciation for Computational Linguistics (ACL-02), Philadelphia, PA, pp. 239?246.
Holmback, H., L. Duncan, and P. Harrison (2000). A word sense checking applica-
tion for Simplified English. In Proceedings of the 3rd International Workshop on
Controlled Language Applications (CLAW00), pp. 120?133.
Liakata, M. and S. Pulman (2002). From Trees to Predicate Argument Structures.
In Proceedings of the 19th International Conference on Computational Linguistics
(COLING-02), pp. 563?569.
Pas?ca, M. and B. Van Durme (2007). What You Seek is What You Get: Extraction of
Class Attributes from Query Logs. In Proceedings of the 20th International Joint
Conference on Artificial Intelligence (IJCAI-07), pp. 2832?2837.
254 Van Durme and Schubert
Punyakanok, V., D. Roth, andW. tau Yih (2008). The Importance of Syntactic Parsing
and Inference in Semantic Role Labeling. Computational Linguistics 34(2), 257?
287.
Resnik, P. (1993). Semantic classes and syntactic ambiguity. In Proceedings of ARPA
Workshop on Human Language Technology, pp. 278?283.
Richardson, S. D., W. B. Dolan, and L. Vanderwende (1998). MindNet: Acquiring
and Structuring Semantic Information from Text. In Proceedings of the 17th Inter-
national Conference on Computational linguistics (COLING-98), pp. 1098?1102.
Schubert, L. K. (2002). Can we derive general world knowledge from texts? In
Proceedings of the 2nd International Conference on Human Language Technology
Research (HLT 2002), pp. 94?97.
Schubert, L. K. and C. H. Hwang (2000). Episodic Logic meets Little Red Riding
Hood: A comprehensive, natural representation for language understanding. In
L. Iwanska and S. Shapiro (Eds.), Natural Language Processing and Knowledge
Representation: Language for Knowledge and Knowledge for Language, pp. 111?
174.
Schubert, L. K. and M. H. Tong (2003). Extracting and evaluating general world
knowledge from the Brown corpus. In Proceedings of the HLT-NAACL 2003Work-
shop on Text Meaning, pp. 7?13.
Wong, Y. W. and R. J. Mooney (2007). Learning Synchronous Grammars for Seman-
tic Parsing with Lambda Calculus. In Proceedings of the 45th Annual Conference
of the Association for Computational Linguistics (ACL-07), pp. 960?967.
Zelle, J. M. and R. J. Mooney (1996). Learning to Parse Database Queries using
Inductive Logic Programming. In Proceedings of the 13th National Conference on
Artificial Intelligence (AAAI-96), pp. 1050?1055.
Zernik, U. (1992). Closed yesterday and closed minds: Asking the right questions
of the corpus to distinguish thematic from sentential relations. In Proceedings of
the 19th International Conference on Computational Linguistics (COLING-02), pp.
1305?1311.
Zettlemoyer, L. and M. Collins (2005). Learning to Map Sentences to Logical Form:
Structured Classification with Probabilistic Categorial Grammars. In Proceedings
of the 21st Conference on Uncertainty in Artificial Intelligence (UAI-05), pp. 658?
666.
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 159?162,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Evaluation of Commonsense Knowledge with Mechanical Turk
Jonathan Gordon
Dept. of Computer Science
University of Rochester
Rochester, NY, USA
jgordon@cs.rochester.edu
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Baltimore, MD, USA
vandurme@cs.jhu.edu
Lenhart K. Schubert
Dept. of Computer Science
University of Rochester
Rochester, NY, USA
schubert@cs.rochester.edu
Abstract
Efforts to automatically acquire world knowl-
edge from text suffer from the lack of an easy
means of evaluating the resulting knowledge.
We describe initial experiments using Mechan-
ical Turk to crowdsource evaluation to non-
experts for little cost, resulting in a collection
of factoids with associated quality judgements.
We describe the method of acquiring usable
judgements from the public and the impact
of such large-scale evaluation on the task of
knowledge acquisition.
1 Introduction
The creation of intelligent artifacts that can achieve
human-level performance at problems like question-
answering ultimately depends on the availability of
considerable knowledge. Specifically, what is needed
is commonsense knowledge about the world in a form
suitable for reasoning. Open knowledge extraction
(Van Durme and Schubert, 2008) is the task of mining
text corpora to create useful, high-quality collections
of such knowledge.
Efforts to encode knowledge by hand, such as Cyc
(Lenat, 1995), require expensive man-hours of labor
by experts. Indeed, results from Project Halo (Fried-
land et al, 2004) suggest that properly encoding the
(domain-specific) knowledge from just one page of a
textbook can cost $10,000. OKE, on the other hand,
creates logical formulas automatically from existing
stores of human knowledge, such as books, newspa-
pers, and the Web. And while crowdsourced efforts to
gather knowledge, such as Open Mind (Singh, 2002),
learn factoids people come up with off the tops of
their heads to contribute, OKE learns from what peo-
ple normally write about and thus consider important.
Open knowledge extraction differs from open infor-
mation extraction (Banko et al, 2007) in the focus
on everyday, commonsense knowledge rather than
specific facts, and on the logical interpretability of
the outputs. While an OIE system might learn that
Tolstoy wrote using a dip pen, an OKE system would
prefer to learn that an author may write using a pen.
An example of an OKE effort is the KNEXT sys-
tem1 (Schubert, 2002), which uses compositional se-
mantic interpretation rules to produce logical formu-
las from the knowledge implicit in parsed text. These
formulas are then automatically expressed as English-
like ?factoids?, such as ?A PHILOSOPHER MAY HAVE
A CONVICTION? or ?NEGOTIATIONS CAN BE LIKELY
TO GO ON FOR SOME HOURS?.
While it is expected that eventually sufficiently
clean knowledge bases will be produced for infer-
ences to be made about everyday things and events,
currently the average quality of automatically ac-
quired knowledge is not good enough to be used in
traditional reasoning systems. An obstacle for knowl-
edge extraction is the lack of an easy method for
evaluating ? and thus improving ? the quality of re-
sults. Evaluation in acquisition systems is typically
done by human judging of random samples of output,
usually by the reporting authors themselves (e.g., Lin
and Pantel, 2002; Schubert and Tong, 2003; Banko et
al., 2007). This is time-consuming, and it has the po-
tential for bias: it would be preferable to have people
other than AI researchers label whether an output is
commonsense knowledge or not. We explore the use
of Amazon?s Mechanical Turk service, an online la-
bor market, as a means of acquiring many non-expert
judgements for little cost.
2 Related Work
While Open Mind Commons (Speer, 2007) asks users
to vote for or against commonsense statements con-
tributed by others users in order to come to a consen-
sus, we seek to evaluate an automatic system. Snow
et al (2008) compared the quality of labels produced
by non-expert Turkers against those made by experts
for a variety of NLP tasks and found that they re-
quired only four responses per item to emulate expert
annotations. Kittur et al (2008) describe the use and
1Public release of the basic KNEXT engine is forthcoming.
159
The statement above is a reasonably clear, entirely
plausible, generic claim and seems neither too spe-
cific nor too general or vague to be useful:
? I agree.
? I lean towards agreement.
? I?m not sure.
? I lean towards disagreement.
? I disagree.
Figure 1: Rating instructions and answers.
necessity of verifiable questions in acquiring accurate
ratings of Wikipedia articles from Mechanical Turk
users. These results contribute to our methods below.
3 Experiments
Previous evaluations of KNEXT output have tried to
judge the relative quality of knowledge learned from
different sources and by different techniques. Here
the goal is simply to see whether the means of evalu-
ation can be made to work reasonably, including at
what scale it can be done for limited cost. For these
experiments, we relied on $100 in credit provided by
Amazon as part of the workshop shared task. This
amount was used for several small experiments in or-
der to empirically estimate what $100 could achieve,
given a tuned method of presentation and evaluation.
We took a random selection of factoids generated
from the British National Corpus (BNC Consortium,
2001), split into sets of 20, and removed those most
easily filtered out as probably being of low quality
or malformed. We skipped the more stringent filters
(originally created for dealing with noisy Web text),
leaving more variety in the quality of the factoids
Turkers were asked to rate.
The first evaluation followed the format of previ-
ous, offline ratings. For each factoid, Turkers were
given the instructions and choices in Fig. 1, where
the options correspond in our analysis to the num-
bers 1?5, with 1 being agreement. To help Turkers
make such judgements, they were given a brief back-
ground statement: ?We?re gathering the sort of every-
day, commonsense knowledge an intelligent computer
system should know. You?re asked to rate several pos-
sible statements based on how well you think they
meet this goal.? Mason and Watts (2009) suggest that
while money may increase the number and speed of
responses, other motivations such as wanting to help
with something worthwhile or interesting are more
likely to lead to high-quality responses.
Participants were then shown the examples and
explanations in Fig. 2. Note that while they are told
some categories that bad factoids can fall into, the
Turkers are not asked to make such classifications
Examples of good statements:
? A SONG CAN BE POPULAR
? A PERSON MAY HAVE A HEAD
? MANEUVERS MAY BE HOLD -ED IN SECRET
It?s fine if verb conjugations are not attached or are a bit
unnatural, e.g. ?hold -ed? instead of ?held?.
Examples of bad statements:
? A THING MAY SEEK A WAY
This is too vague. What sort of thing? A way for/to
what?
? A COCKTAIL PARTY CAN BE AT
SCOTCH_PLAINS_COUNTRY_CLUB
This is too specific. We want to know that a cocktail
party can be at a country club, not at this particular one.
The underscores are not a problem.
? A PIG MAY FLY
This is not literally true even though it happens to be an
expression.
? A WORD MAY MEAN
This is missing information. What might a word mean?
Figure 2: The provided examples of good and bad factoids.
themselves, as this is a task where even experts have
low agreement (Van Durme and Schubert, 2008).
An initial experiment (Round 1) only required
Turkers to have a high (90%) approval rate. Under
these conditions, out of 100 HITs2, 60 were com-
pleted by participants whose IP addresses indicated
they were in India, 38 from the United States, and
2 from Australia. The average Pearson correlation
between the ratings of different Indian Turkers an-
swering the same questions was a very weak 0.065,
and between the Indian responders and those from
the US and Australia was 0.132. On the other hand,
the average correlation among non-Indian Turkers
was 0.508, which is close to the 0.6?0.8 range seen
between the authors in the past, and which can be
taken as an upper bound on agreement for the task.
Given the sometimes subtle judgements of mean-
ing required, being a native English speaker has pre-
viously been assumed to be a prerequisite. This differ-
ence in raters? agreements may thus be due to levels
of language understanding, or perhaps to different
levels of attentiveness to the task. However, it does
not seem to be the case that the Indian respondents
rushed: They took a median time of 201.5 seconds
(249.18 avg. with a high standard deviation of 256.3s
? some took more than a minute per factoid). The non-
Indian responders took a median time of just 115.5 s
(124.5 avg., 49.2 std dev.).
Regardless of the cause, given these results, we re-
stricted the availability of all following experiments
to Turkers in the US.Ideally we would include other
English-speaking countries, but there is no straight-
2Human Intelligence Tasks ? Mechanical Turk assignments.
In this case, each HIT was a set of twenty factoids to be rated.
160
All High Corr. (> 0.3)
Round Avg. Std. Dev. Avg. Std. Dev.
1 (BNC) 2.59 1.55 2.71 1.64
3 (BNC) 2.80 1.66 2.83 1.68
4 (BNC) 2.61 1.64 2.62 1.64
5 (BNC) 2.76 1.61 2.89 1.68
6 (Weblogs) 2.83 1.67 2.85 1.67
7 (Wikipedia) 2.75 1.64 2.75 1.64
Table 1: Average ratings for all responses and for highly
correlated responses. to other responses. Lower numbers
are more positive. Round 2 was withdrawn without being
completed.
forward way to set multiple allowable countries on
Mechanical Turk.When Round 2 was posted with
a larger set of factoids to be rated and the location
requirement, responses fell off sharply, leading us
to abort and repost with a higher payrate (7? for 20
factoids vs 5? originally) in Round 3.
To avoid inaccurate ratings, we rejected submis-
sions that were improbably quick or were strongly
uncorrelated with other Turkers? responses. We col-
lected five Turkers? ratings for each set of factoids,
and for each persons? response to a HIT computed
the average of their three highest correlations with
others? responses. We then rejected if the correla-
tions were so low as to indicate random responses.
The scores serve a second purpose of identifying a
more trustworthy subset of the responses. (A cut-off
score of 0.3 was chosen based on hand-examination.)
In Table 1, we can see that these more strongly corre-
lated responses rate factoids as slightly worse overall,
possibly because those who either casual or uncertain
are more likely to judge favorably on the assumption
that this is what the task authors would prefer, or they
are simply more likely to select the top-most option,
which was ?I agree?.
An example of a factoid that was labeled incor-
rectly by one of the filtered out users is ?A PER-
SON MAY LOOK AT SOME THING-REFERRED-TO OF
PRESS RELEASES?, for which a Turker from Madras
in Round 1 selected ?I agree?. Factoids containing
the vague ?THING-REFERRED-TO? are often filtered
out of our results automatically, but leaving them in
gave us some obviously bad inputs for checking Turk-
ers? responses. Another (US) Turker chose ?I agree?
when told ?TES MAY HAVE 1991ES? but ?I disagree?
when shown ?A TRIP CAN BE TO A SUPERMARKET?.
We are interested not only in whether there is a gen-
eral consensus to be found among the Turkers but also
how that consensus correlates with the judgements
of AI researchers. To this end, one of the authors
rated five sets (100 factoids) presented in Round 3,
0
500
1000
1500
2000
2500
0 1 2 3 4 5
Fr
eq
ue
nc
y
Rating
Figure 3: Frequency of ratings in the high-corr. results of
Round 3.
which yielded an average correlation between all the
Turkers and the author of 0.507, which rises slightly
to 0.532 if we only count those Turkers considered
?highly correlated? as described above.
As another test of agreement, for ten of the sets in
Round 3, two factoids were designated as fixpoints ?
the single best and worst factoid in the set, assigned
ratings 1 and 5 respectively. From the Turkers who
rated these factoids, 65 of the 100 ratings matched
the researchers? designations and 77 were within one
point of the chosen rating.3
A few of the Turkers who participated had fairly
strong negative correlations to the other Turkers, sug-
gesting that they may have misunderstood the task
and were rating backwards.4 Furthermore, one Turker
commented that she was unsure whether the state-
ment she was being asked to agree with (Fig. 1) ?was
a positive or negative?. To see how it would affect the
results, we ran (as Round 4) twenty sets of factoids,
asking simplified question ?Do you agree this is a
good statement of general knowledge?? The choices
were also reversed in order, running from ?I disagree?
to ?I agree? and color-coded, with agree being green
and disagree red. This corresponded to the coloring
of the good and bad examples at the top of the page,
which the Turkers were told to reread when they were
halfway through the HIT. The average correlation for
responses in Round 4 was 0.47, which is an improve-
ment over the 0.34 avg. correlation of Round 3.
Using the same format as Round 4, we ran factoids
from two other corpora. Round 6 consisted of 300 ran-
dom factoids taken from running KNEXT on weblog
data (Gordon et al, 2009) and Round 7 300 random
factoids taken from running KNEXT on Wikipedia.
3If we only look at the highly correlated responses, this in-
creases slightly to 68% exact match, 82% within one point.
4This was true for one Turker who completed many HITs, a
problem that might be prevented by accepting/rejecting HITs as
soon as all scores for that set of factoids were available rather
than waiting for the entire experiment to finish.
161
The average ratings for factoids from these sources
are lower than for the BNC, reflecting the noisy na-
ture of much writing on weblogs and the many overly
specific or esoteric factoids learned from Wikipedia.
The results achieved can be quite sensitive to the
display of the task. For instance, the frequency of
ratings in Fig. 3 shows that Turkers tended toward
the extremes: ?I agree? and ?I disagree? but rarely
?I?m not sure?. This option might have a negative
connotation (?Waffling is undesirable?) that another
phrasing would not. As an alternative presentation of
the task (Round 5), for 300 factoids, we asked Turk-
ers to first decide whether a factoid was ?incoher-
ent (not understandable)? and, otherwise, whether it
was ?bad?, ?not very good?, ?so-so?, ?not so bad?, or
?good? commonsense knowledge. Turkers indicated
factoids were incoherent 14% of the time, with a cor-
responding reduction in the number rated as ?bad?,
but no real increase in middle ratings. The average
ratings for the ?coherent? factoids are in Table 1.
4 Uses of Results
Beyond exploring the potential of Mechanical Turk
as a mechanism for evaluating the output of KNEXT
and other open knowledge extraction systems, these
experiments have two useful outcomes:
First, they give us a large collection of almost 3000
factoids that have associated average ratings and al-
low for the release of the subset of those factoids
that are believed to probably be good (rated 1?2).
This data set is being publicly released at http://
www.cs.rochester.edu/research/knext, and
it includes a wide range of factoids, such as ?A REP-
RESENTATION MAY SHOW REALITY? and ?DEMON-
STRATIONS MAY MARK AN ANNIVERSARY OF AN
UPRISING?.
Second, the factoids rated from Round 2 onward
were associated with the KNEXT extraction rules used
to generate them: The factoids generated by different
rules have average ratings from 1.6 to 4.8. We hope in
future to use this data to improve KNEXT?s extraction
methods, improving or eliminating rules that often
produce factoids judged to be bad. Inexpensive, fast
evaluation of output on Mechanical Turk could be a
way to measure incremental improvements in output
quality coming from the same source.
5 Conclusions
These initial experiments have shown that untrained
Turkers evaluating the natural-language verbaliza-
tions of an open knowledge extraction system will
generally give ratings that correlate strongly with
those of AI researchers. Some simple methods were
described to find those responses that are likely to
be accurate. This work shows promise for cheap and
quick means of measuring the quality of automati-
cally constructed knowledge bases and thus improv-
ing the tools that create them.
Acknowledgements
This work was supported by NSF grants IIS-0535105
and IIS-0916599.
References
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the Web. In Proc. of IJCAI-07.
BNC Consortium. 2001. The British National Corpus,
v.2. Dist. by Oxford University Computing Services.
Noah S. Friedland et al. 2004. Project Halo: Towards a
digital Aristotle. AI Magazine, 25(4).
Jonathan Gordon, Benjamin Van Durme, and Lenhart K.
Schubert. 2009. Weblogs as a source for extracting
general world knowledge. In Proc. of K-CAP-09.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proc. of CHI ?08.
Douglas B. Lenat. 1995. Cyc: A Large-scale Investment
in Knowledge Infrastructure. Communications of the
ACM, 38(11):33?48.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of COLING-02.
Winter Mason and Duncan J. Watts. 2009. Financial
incentives and the ?performance of crowds?. In Proc.
of HCOMP ?09.
Lenhart K. Schubert and Matthew H. Tong. 2003. Extract-
ing and evaluating general world knowledge from the
Brown corpus. In Proc. of the HLT-NAACL Workshop
on Text Meaning.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from texts? In Proc. of HLT-02.
Push Singh. 2002. The public acquisition of common-
sense knowledge. In Proc. of AAAI Spring Sympo-
sium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast ? but is it good? In
Proc. of EMNLP-08.
Robert Speer. 2007. Open mind commons: An inquisitive
approach to learning common sense. In Workshop on
Common Sense and Intelligent User Interfaces.
Benjamin Van Durme and Lenhart K. Schubert. 2008.
Open knowledge extraction through compositional lan-
guage processing. In Proc. of STEP 2008.
162
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 33?40,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
WikiTopics: What is Popular on Wikipedia and Why
Byung Gyu Ahn1 and Benjamin Van Durme1,2 and Chris Callison-Burch1
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We establish a novel task in the spirit of news sum-
marization and topic detection and tracking (TDT):
daily determination of the topics newly popular with
Wikipedia readers. Central to this effort is a new
public dataset consisting of the hourly page view
statistics of all Wikipedia articles over the last three
years. We give baseline results for the tasks of:
discovering individual pages of interest, clustering
these pages into coherent topics, and extracting the
most relevant summarizing sentence for the reader.
When compared to human judgements, our system
shows the viability of this task, and opens the door
to a range of exciting future work.
1 Introduction
In this paper we analyze a novel dataset: we have
collected the hourly page view statistics1 for every
Wikipedia page in every language for the last three years.
We show how these page view statistics, along with other
features like article text and inter-page hyperlinks, can
be used to identify and explain popular trends, including
popular films and music, sports championships, elections,
natural disasters, etc.
Our approach is to select a set of articles whose daily
pageviews for the last fifteen days dramatically increase
above those of the preceding fifteen day period. Rather
than simply selecting the most popular articles for a given
day, this selects articles whose popularity is rapidly in-
creasing. These popularity spikes tend to be due to sig-
nificant current events in the real world. We examine 100
such articles for each of 5 randomly selected days in 2009
and attempt to group the articles into clusters such that
the clusters coherently correspond to current events and
extract a summarizing sentence that best explains the rel-
evant event. Quantitative and qualitative analyses are pro-
vided along with the evaluation dataset.
1The data does not contain any identifying information about who
viewed the pages. See http://dammit.lt/wikistats
Barack Obama
Joe Biden
White House
Inauguration
. . .
US Airways Flight 1549
Chesley Sullenberger
Hudson River
. . .
Super Bowl
Arizona Cardinals
Figure 1: Automatically selected articles for Jan 27, 2009.
We compare our automatically collected articles to
those in the daily current events portal of Wikipedia
where Wikipedia editors manually chronicle current
events, which comprise armed conflicts, international re-
lations, law and crime, natural disasters, social, political,
sports events, etc. Each event is summarized with a sim-
ple phrase or sentence that links to related articles. We
view our work as an automatic mechanism that could po-
tentially supplant this hand-curated method of selecting
current events by editors.
Figure 1 shows examples of automatically selected ar-
ticles for January 27, 2009. We would group the arti-
cles into 3 clusters, {Barack Obama, Joe Biden, White
House, Inauguration} which corresponds to the inaugu-
ration of Barack Obama, {US Airways Flight 1549, Ches-
ley Sullenburger, Hudson River} which corresponds to
the successful ditching of an airplane into the Hudson
river without loss of life, and {Superbowl, Arizona Car-
dinals} which corresponds to the then upcoming Super-
bowl XLIII.
We further try to explain the clusters by selecting sen-
tences from the articles. For the first cluster, a good se-
lection would be ?the inauguration of Barack Obama as
the 44th president . . . took place on January 20, 2009?.
For the second cluster, ?Chesley Burnett ?Sully? Sullen-
berger III (born January 23, 1951) is an American com-
33
mercial airline pilot, . . . , who successfully carried out the
emergency water landing of US Airways Flight 1549 on
the Hudson River, offshore from Manhattan, New York
City, on January 15, 2009, . . . ? would be a nice sum-
mary, which also provides links to the other articles in
the same cluster. For the third cluster, ?Superbowl XLIII
will feature the American Football Conference champion
Pittsburgh Steelers (14-4) and the National Football Con-
ference champion Arizona Cardinals (12-7) .? would be
a good choice which delineates the association with Ari-
zona Cardinals.
Different clustering methods and sentence selection
features are evaluated and results are compared. Topic
models, such as K-means (Manning et al, 2008) vector
space clustering and latent Dirichlet alocation (Blei et
al., 2003), are compared to clustering using Wikipedia?s
link structure. To select sentences we make use of NLP
technologies such as coreference resolution, and named
entity and date taggers. Note that the latest revision of
each article on the day on which the article is selected is
used in clustering and textualization to simulate the situa-
tion where article selection, clustering, and textualization
are performed once every day.
Figure 2 illustrates the pipeline of our WikiTopics sys-
tem: article selection, clustering, and textualization.
2 Article selection
We would like to identify an uptrend in popularity of ar-
ticles. In an online encyclopedia such as Wikipedia, the
pageviews for an article reflect its popularity. Following
the Trending Topics software2, WikiTopics?s articles se-
lection algorithm determines each articles? monthly trend
value as increase in pageviews within last 30 days. The
monthly trend value tk of an article k is defined as be-
low:
tk =
15?
i=1
dki ?
30?
i=16
dki
where
dki = daily pageviews i? 1 days ago for an article k
We selected 100 articles of the highest trend value for
each day in 2009. We call the articles WikiTopics articles.
We leave as future work other possibilities to determine
the trend value and choose articles3, and only briefly dis-
cuss some alternatives in this section.
Wikipedia has a portal page called ?current events?,
in which significant current events are listed manu-
ally by Wikipedia editors. Figure 3 illustrates spikes in
2http://www.trendingtopics.org
3For example, one might leverage additional signals of real world
events, such as Twitter feeds, etc.
 100
 1000
 10000
 100000
 1e+06
 1e+07
 1e+08
Dec 06 Dec 20 Jan 03 Jan 17 Jan 31
P
ag
e 
vi
ew
s
Barack Obama
United States
List of Presidents of the United States
President of the United States
African American
List of African-American firsts
Figure 3: Pageviews for all the hand-curated articles related
to the inauguration of Barack Obama. Pageviews spike on the
same day as the event took place?January 20, 2009.
pageviews of the hand-curated articles related to the in-
auguration of Barack Obama, which shows clear correla-
tion between the spikes and the day on which the relevant
event took place. It is natural to contrast WikiTopics ar-
ticles to this set of hand-curated articles. We evaluated
WikiTopics articles against hand-curated articles as gold
standard and had negative results with precision of 0.13
and recall of 0.28.
There are a few reasons for this. First, there are
much fewer hand-curated articles than WikiTopics arti-
cles: 17,253 hand-selected articles vs 36,4004 WikiTopics
articles; so precision cannot be higher than 47%. Second,
many of the hand-selected articles turned out to have very
low pageviews: 6,294 articles (36.5%) have maximum
daily pageviews less than 1,000 whereas WikiTopics arti-
cles have increase in pageviews of at least 10,000. It is ex-
tremely hard to predict the hand-curated articles based on
pageviews. Figure 4 further illustrates hand-curated arti-
cles? lack of increase in pageviews as opposed to Wiki-
Topics articles. On the contrary, nearly half of the hand-
curated articles have decrease in pageviews. For the hand-
curated articles, it seems that spikes in pageviews are
an exception rather than a commonality. We therefore
concluded that it is futile to predict hand-curated arti-
cles based on pageviews. The hand-curated articles suffer
from low popularity and do not spike in pageviews often.
Figure 5 contrasts the WikiTopics articles and the hand-
curated articles. The WikiTopics articles shown here do
not appear in the hand-curated articles within fifteen days
before or after, and vice versa. WikiTopics selected arti-
cles about people who played a minor role in the relevant
event, recently released films, their protagonists, popular
TV series, etc. Wikipedia editors selected articles about
4One day is missing from our 2009 pageviews statistics.
34
Daily Page Views Topic Selection Clustering Textualization
Figure 2: Process diagram: (a) Topic selection: select interesting articles based on increase in pageviews. (b) Clustering: cluster the
articles according to relevant events using topic models or Wikipedia?s hyperlink structure. (c) Textualization: select the sentence
that best summarizes the relevant event.
-2
 0
 2
 4
 6
 8
 0  0.2  0.4  0.6  0.8  1
lo
g 
ra
tio
quantile
WikiTopics articles
hand-curated articles
Figure 4: Log ratio of the increase in pageviews:
log
?
i = 115dik/
?
i = 1630. Zero means no change
in pageviews. WikiTopics articles show pageviews increase in
a few orders of magnitude as opposed to hand-curated articles.
actions, things, geopolitical or organizational names in
the relevant event and their event description mentions
all of them.
For this paper we introduce the problem of topic se-
lection along with a baseline solution. There are vari-
ous viable alternatives to the monthly trend value. As
one of them, we did some preliminary experiments with
the daily trend value, which is defined by dk1 ? d
k
2 , i.e.
the difference of the pageviews between the day and the
previous day: we found that articles selected using the
daily trend value have little overlap?less than half the ar-
ticles overlapped with the monthly trend value. Future
work will consider the addition of sources other than
pageviews, such as edit histories and Wikipedia category
information, along with more intelligent techniques to
combine these different sources.
3 Clustering
Clustering plays a central role to identify current events;
a group of coherently related articles corresponds to a
WikiTopics articles
Joe Biden
Notorious (2009 film)
The Notorious B.I.G.
Lost (TV series)
. . .
hand-curated articles
Fraud
Florida
Hedge fund
Arthur Nadel
Federal Bureau of Investigation
Figure 5: Illustrative articles for January 27, 2009. WikiTopics
articles here do not appear in hand-curated articles within fifteen
days before or after, and vice versa. The hand-curated articles
shown here are all linked from a single event ?Florida hedge
fund manager Arthur Nadel is arrested by the United States Fed-
eral Bureau of Investigation and charged with fraud.?
current event. Clusters, in general, may have hierarchies
and an element may be a member of multiple clusters.
Whereas Wikipedia?s current events are hierarchically
compiled into different levels of events, we focus on flat
clustering, leaving hierarchical clustering as future work,
but allow multiple memberships.
In addition to clustering using Wikipedia?s inter-page
hyperlink structure, we experimented with two families
of clustering algorithms pertaining to topic models: the
K-means clustering vector space model and the latent
Dirichlet alocation (LDA) probabilistic topic model. We
used the Mallet software (McCallum, 2002) to run these
topic models. We retrieve the latest revision of each arti-
cle on the day that WikiTopics selected it. We strip unnec-
essary HTML tags and Wiki templates with mwlib5 and
split sentences with NLTK (Loper and Bird, 2002). Nor-
malization, tokenization, and stop words removal were
performed, but no stemming was performed. The uni-
gram (bag-of-words) model was used and the number
5http://code.pediapress.com/wiki/wiki/mwlib
35
Test set # Clusters B3 F-score
Human-1 48.6 0.70 ? 0.08
Human-2 50.0 0.71 ? 0.11
Human-3 53.8 0.74 ? 0.10
ConComp 31.8 0.42 ? 0.18
OneHop 45.2 0.58 ? 0.17
K-means tf 50 0.52 ? 0.04
K-means tf-idf 50 0.58 ? 0.09
LDA 44.8 0.43 ? 0.08
Table 1: Clustering evaluation: F-scores are averaged across
gold standard datasets. ConComp and OneHop are using the
link structure. K-means clustering with tf-idf performs best.
Manual clusters were evaluated against those of the other two
annotators to determine inter-annotator agreement.
of clusters/topics K was set to 50, which is the average
number of clusters in the human clusters6. For K-means,
the common settings were used: tf and tf-idf weighting
and cosine similarity (Allan et al, 2000). For LDA, we
chose the most probable topic for each article as the clus-
ter ID. Two different clustering schemes make use of the
inter-page hyperlink structure: ConComp and OneHop.
In these schemes, the link structure is treated as a graph,
in which each page corresponds to a vertex and each link
to an undirected edge. ConComp groups a set of arti-
cles that are connected together. OneHop chooses an ar-
ticle and groups a set of articles that are directly linked.
The number of resulting clusters depends on the order
in which you choose an article. To find the minimum or
maximum number of such clusters would be computa-
tionally expensive. Instead of attempting to find the op-
timal number of clusters, we take a greedy approach and
iteratively create clusters that maximize the central node
connectivity, stopping when all nodes are in at least one
cluster. This allows for singleton clusters.
Three annotators manually clustered WikiTopics arti-
cles for five randomly selected days. The three manual
clusters were evaluated against each other to measure
inter-annotator agreement, using the multiplicity B3 met-
ric (Amigo? et al, 2009). Table 1 shows the results. The
B3 metric is an extrinsic clustering evaluation metric and
needs a gold standard set of clusters to evaluate against.
The multiplicity B3 works nicely for overlapping clus-
ters: the metric does not need to match cluster IDs and
only considers the number of the clusters that a pair of
data points shares. For a pair of data points e and e?, let
C(e) be the set of the test clusters that e belongs to, and
L(e) be the set of e?s gold standard clusters. The multi-
6K=50 worked reasonably well for the most cases. We are planning
to explore a more principled way to set the number.
Airbus A320 family
Air Force One
Chesley Sullenberger
US Airways Flight 1549
Super Bowl XLIII
Arizona Cardinals
Super Bowl
Kurt Warner
2009 flu pandemic by country
Severe acute respiratory syndrome
2009 flu pandemic in the United States
Figure 6: Examples of clusters: K-means clustering on the arti-
cles of January 27, 2009 and May 12, 2009. The centroid article
for each cluster, defined as the closest article to the center of the
cluster in vector space, is in bold.
plicity B3 scores are evaluated as follows:
Prec(e, e?) =
min (|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|C(e) ? C(e?)|
Recall(e, e?) =
min (|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|L(e) ? L(e?)|
The overall B3 scores are evaluated as follows:
Prec = AvgeAvge?.C(e)?C(e?)6=0Prec(e, e
?)
Recall = AvgeAvge?.L(e)?L(e?)6=0Recall(e, e
?)
The inter-annotator agreement in the B3 scores are in the
range of 67%?74%. K-means clustering performs best,
achieving 79% precision compared to manual cluster-
ing. OneHop clustering using the link structure achieved
comparable performance. LDA performed significantly
worse, comparable to ConComp clustering.
Clustering the articles according to the relevance to re-
cent popularity is not trivial even for humans. In Wiki-
Topics articles for February 10, 2009, Journey (band) and
Bruce Springsteen may seem to be relevant to Grammy
Awards, but in fact they are relevant on this day because
they performed the halftime show at the Super Bowl. K-
means fails to recognize this and put them into the cluster
of Grammy Awards, while ConComp merged Grammy
Awards and Super Bowl into the same cluster. OneHop
kept the two clusters intact and benefited from putting
Bruce Springsteen into both the clusters. LDA cluster-
ing does not have such a benefit; its performance might
have suffered from our allowing only a single member-
ship for an article. Clustering using the link structure per-
forms comparably with other clustering algorithms with-
out using topic models. It is worth noting that there are
a few ?octopus? articles that have links to many articles.
The United States on January 27, 2009 was disastrous,
with its links to 58 articles, causing ConComp clustering
to group 89 articles into a single cluster. OneHop clus-
tering?s condition that groups only articles that are one
hop away alleviates the issue and it also benefited from
putting an article into multiple clusters.
36
To see if external source help better clustering, we ex-
plored the use of news articles. We included the news ar-
ticles that we crawled from various news websites into
the same vector space as the Wikipedia articles, and ran
K-means clustering with the same settings as before. For
each day, we experimented with news articles within dif-
ferent numbers of past days. The results did not show
significant improvement over clustering without external
news articles. This needs further investigation7.
4 Textualization
We would like to generate textual descriptions for the
clustered articles to explain why they are popular and
what current event they are relevant to. We started with
a two-step approach similar to multi-document extrac-
tive summarization approaches (Mckeown et al, 2005).
The first step is sentence selection; we extract the best
sentence that describes the relevant event for each arti-
cle. The second step is combining the selected sentences
of a cluster into a coherent summary. Here, we focus on
the first step of selecting a sentence and evaluate the se-
lected sentences. The selected sentences for each clus-
ter are then put together without modification, where the
quality of generated summary mainly depends on the ex-
tracted sentences at the first step. We consider each article
separately, using as features only information such as date
expressions and references to the topic of the article. Fu-
ture work will consider sentence extraction, aware of the
related articles in the same cluster, and better summariza-
tion techniques, such as sentence fusion or paraphrasing.
We preprocess the Wikipedia articles using the Serif
system (Boschee et al, 2005) for date tagging and coref-
erence resolution. The identified temporal expressions
are in various formats such as exact date (?February 12,
1809?), a season (?spring?), a month (?December 1808?),
a date without a specific year (?November 19?), and even
relative time (?now?, ?later that year?, ?The following
year?). Some examples are shown in Figure 7. The en-
tities mentioned in a given article are compiled into a list
and the mentions of each entity, including pronouns, are
linked to the entity as a coreference chain. Some exam-
ples are shown in Figure 9.
In our initial scheme, we picked the first sentence
of each article because the first sentence is usually an
overview of the topic of the article and often relevant to
the current event. For example, a person?s article often
has the first line with one?s recent achievement or death.
An article about an album or a film often begins with the
release date. We call this First.
7News articles tend to group with other news articles. We are cur-
rently experimenting with different filtering and parameters. Also note
that we only experimented with all news articles on a given day. Clus-
tering with selective news articles might help.
February 12, 1809
1860
now
the 17th century
some time
December 1808
34 years old
spring
September
Later that year
November 19
that same month
The following winter
The following year
April 1865
late 1863
Figure 7: Selected examples of temporal expressions identified
by Serif from 247 such date and time expressions extracted from
the article Abraham Lincoln.
We also picked the sentence with the most recent date
to the day on which the article was selected. Dates in the
near future are considered in the same way as the recent
dates. Dates may appear in various formats, so we make a
more specific format take precedence, i.e. ?February 20,
2009? is selected over vaguer dates such as ?February
2009? or ?2009?. We call this scheme Recent.
As the third scheme, we picked the sentence with the
most recent date among those with a reference to the ar-
ticle?s title. The reasoning behind this is if the sentence
refers to the title of the article, it is more likely to be rel-
evant to the current event. We call this scheme Self.
After selecting a sentence for each cluster, we substi-
tute personal pronouns in the sentence with their proper
names. This step enhances readability of the sentence,
which often refers to people by a pronoun such as ?he?,
?his?, ?she?, or ?her?. The examples of substituted proper
names appear in Figure 9 in bold. The Serif system classi-
fies which entity mentions are proper names for the same
person, but choosing the best name among the names is
not a trivial task: proper names may vary from John to
John Kennedy to John Fitzgerald ?Jack? Kennedy. We
choose the most frequent proper name.
For fifty randomly chosen articles over the five se-
lected days, two annotators selected the sentences that
best describes why an article gained popularity recently,
among 289 sentences per each article on average from
the article text. For each article, annotators picked a sin-
gle best sentence, and possibly multiple alternative sen-
tences. If there is no such single sentence that best de-
scribes a relevant event, annotators marked none as the
best sentence and listed alternative sentences that par-
tially explain the relevant event. The evaluation results
for all the selection schemes are shown in Table 2. To
see inter-annotator agreement, two annotators? selections
were evaluated against each other. The other selection
schemes are evaluated against both the two annotators?
selection and their scores in the table are averaged across
the two. The precision and recall score for best sentences
are determined by evaluating a scheme?s selection of the
37
2009-01-27: Inauguration of Barack Obama
Gold: The inauguration of Barack Obama as the forty-fourth President
of the United States took place on January 20, 2009.
Alternatives: 1. The inauguration, with a record attendance for any
event held in Washington, D.C., marked the commencement of the
four-year term of Barack Obama as President and Joseph Biden as
Vice President. 2. With his inauguration as President of the United
States, Obama became the first African American to hold the office
and the first President born in Hawaii. 3. Official events were held in
Washington, D.C. from January 18 to 21, 2009, including the We Are
One: The Obama Inaugural Celebration at the Lincoln Memorial, a day
of service on the federal observance of the Martin Luther King, Jr. Day,
a ?Kids? Inaugural: We Are the Future? concert event at the Verizon
Center, the inaugural ceremony at the U.S. Capitol, an inaugural
luncheon at National Statuary Hall, a parade along Pennsylvania
Avenue, a series of inaugural balls at the Washington Convention
Center and other locations, a private White House gala and an inaugural
prayer service at the Washington National Cathedral.
First: The inauguration of Barack Obama as the forty-fourth President
of the United States took place on January 20, 2009.
Recent: On January 22, 2009, a spokesperson for the Joint Committee
on Inaugural Ceremonies also announced that holders of blue, purple
and silver tickets who were unable to enter the Capitol grounds to view
the inaugural ceremony would receive commemorative items.
Self: On January 21, 2009, President Obama, First Lady Michelle
Obama, Vice President Biden and Dr. Jill Biden attended an inaugural
prayer service at the Washington National Cathedral.
2009-02-10: February 2009 Great Britain and Ireland snowfall
Gold: The snowfall across Great Britain and Ireland in February 2009
is a prolonged period of snowfall that began on 1 February 2009.
Alternative: Many areas experienced their largest snowfall levels in 18
years.
First: The snowfall across Great Britain and Ireland in February 2009
is a prolonged period of snowfall that began on 1 February 2009.
Recent: BBC regional summary - 4 February 2009
Self: The snowfall across Great Britain and Ireland in February 2009 is
a prolonged period of snowfall that began on 1 February 2009.
2009-04-19: Wilkins Sound
Gold: On 5 April 2009 the thin bridge of ice to the Wilkins Ice Shelf
off the coast of Antarctica splintered, and scientists expect it could
cause the collapse of the Shelf.
Alternatives: 1. There are reports the shelf has exploded into hundreds
of small ice bergs. 2. On 5 April 2009, the ice bridge connecting part
of the ice shelf to Charcot Island collapsed.
First: Wilkins Sound is a seaway in Antarctica that is largely occupied
by the Wilkins Ice Shelf.
Recent: On 5 April 2009 the thin bridge of ice to the Wilkins Ice Shelf
off the coast of Antarctica splintered, and scientists expect it could
cause the collapse of the Shelf.
Self: On 25 March 2008 a chunk of the Wilkins ice shelf disintegrated,
putting an even larger portion of the glacial ice shelf at risk.
Figure 8: Sentence selection: First selects the first sentence, and
often fails to relate the current event. Recent tend to pinpoint the
exact sentence that describes the relevant current event, but fails
when there are several sentences with a recent temporal expres-
sion. Self helps avoid sentences that does not refer to the topic
of the article, but suffers from errors propagated from corefer-
ence resolution.
2009-01-27: Barack Obama
Before: He was inaugurated as President on January 20, 2009.
After: Obama was inaugurated as President on January 20,
2009.
Coref: {Barack Hussein Obama II (brk hsen obm; born August
4,, Barack Obama, Barack Obama as the forty-fourth President,
Barack Obama, Sr. , Crain?s Chicago Business naming Obama,
Michelle Obama, Obama, Obama in Indonesian, Senator
Obama,}
2009-02-10: Rebirth (Lil Wayne album)
Before: He also stated the album will be released on April 7,
2009.
After: Lil Wayne also stated the album will be released on
April 7, 2009.
Coref: {American rapper Lil Wayne, Lil Wayne, Wayne}
2009-04-19: Phil Spector
Before: His second trial resulted in a conviction of second
degree murder on April 13, 2009.
After: Spector?s second trial resulted in a conviction of second
degree murder on April 13, 2009.
Coref: {Mr. Spector, Phil Spector, Phil Spector? The character
of Ronnie ?Z, Spector, Spector-, Spector (as a producer),
Spector himself, Spector of second-degree murder, Spector,
who was conducting the band for all the acts,, Spektor, wife
Ronnie Spector}
2009-05-12: Eminem
Before: He is planning on releasing his first album since 2004,
Relapse, on May 15, 2009.
After: Eminem is planning on releasing his first album since
2004, Relapse, on May 15, 2009.
Coref: {Eminem, Marshall Bruce Mathers, Marshall Bruce
Mathers III, Marshall Bruce Mathers III (born October 17,,
Mathers}
2009-10-12: Brett Favre
Before: He came out of retirement for the second time and
signed with the Minnesota Vikings on August 18, 2009.
After: Favre came out of retirement for the second time and
signed with the Minnesota Vikings on August 18, 2009.
Coref: {Bonita Favre, Brett Favre, Brett Lorenzo Favre, Brett?s
father Irvin Favre, Deanna Favre, Favre, Favre,, Favre (ISBN
978-1590710364) which discusses their personal family and
Green Bay Packers family, Irvin Favre, Southern Miss. Favre,
the Brett Favre, The following season Favre, the jersey Favre}
Figure 9: Pronoun replacement: Personal pronouns are substi-
tuted with their proper names, which are italicized. The coref-
erence chain for the entity is also shown; our method correctly
avoids names wrongly placed in the chain. Note that unlike the
other sentences, the last one is not related to the current event,
Brett Favre?s victory against Green Bay Packers.
38
Single best Alternatives
Scheme Precision Recall Precision Recall
Human 0.50 0.55 0.85 0.75
First 0.14 0.20 0.33 0.40
Recent 0.31 0.44 0.51 0.60
Self 0.31 0.36 0.49 0.48
Self fallback 0.33 0.46 0.52 0.62
Table 2: Textualization: evaluation results of sentence selection
schemes. Self fallback scheme first tries to select the best sen-
tence as the Self scheme, and if it fails to select one it falls back
to the Recent scheme.
best sentences against a gold standard?s selection. To
evaluate alternative sentences, precision is measured as
the fraction of articles where the test and gold standard
selections overlap (share at least one sentence), compared
to the total number of articles that have at least one sen-
tence selected according to the test set. Recall is defined
by instead dividing by the number of articles that have at
least one sentence selected in the gold standard.
The low inter-annotator agreement for selecting the
best sentence shows the difficulty of the task. However,
when their sentence selection is evaluated by allowing
multiple alternative gold standard sentences, the agree-
ment is higher. It seems that there are a set of articles for
which it is easy to pick the best sentence that two anno-
tators and automatic selection schemes easily agree on,
and another set of articles for which it is difficult to find
such a sentence. In the easier articles, the best sentence
often includes a recent date expression, which is easily
picked up by the Recent scheme. Figure 8 illustrates such
cases. In the more difficult articles, there are no such sen-
tences with recent dates. X2 (film) is such an example; it
was released in 2003. The release of the prequel X-Men
Origins: Wolverine in 2009 renewed its popularity and
the X2 (film) article still does not have any recent dates.
There is a more subtle case: the article Farrah Fawcett
includes many sentences with recent dates in a section,
which describes the development of a recent event. It is
hard to pinpoint the best one among them.
Sentence selection heavily depends on other NLP com-
ponents, so errors in them could result in the error in sen-
tence selection. Serena Williams is an example where an
error in sentence splitting propagates to sentence selec-
tion. The best sentence manually selected was the first
sentence in the article ?Serena Jameka Williams . . . , as of
February 2, 2009, is ranked World No. 1 by the Women?s
Tennis Association . . . .? The sentence was disastrously
divided into two sentences right after ?No.? by NLTK
during preprocessing. In other words, the gold standard
sentence could not be selected no matter how well se-
lection performs. Another source of error propagation is
coreference resolution. The Self scheme limits sentence
selection to the sentences with a reference to the articles?
title, and it failed to improve over Recent. In qualitative
analysis, 3 out of 4 cases that made a worse choice re-
sulted from failing to recognize a reference to the topic
of the article. By having it fall back to Recent?s selection
when it failed to find any best sentence, its performance
marginally improved. Improvements of the components
would result in better performance of sentence selection.
WikiTopics?s current sentence extraction succeeded in
generating the best or alternative sentences that summa-
rizes the relevant current event for more than half of the
articles, in enhanced readability through coreference res-
olution. For the other difficult cases, it needs to take dif-
ferent strategies rather than looking for the most recent
date expressions. Alternatives may consider references to
other related articles. In future work, selected sentences
will be combined to create summary of a current event,
and will use sentence compression, fusion and paraphras-
ing to create more succinct summaries.
5 Related work
WikiTopics?s pipeline architecture resembles that of news
summarization systems such as Columbia Newsblaster
(McKeown et al, 2002). Newsblaster?s pipeline is com-
prised of components for performing web crawls, article
text extraction, clustering, classification, summarization,
and web page generation. The system processes a con-
stant stream of newswire documents. In contrast, Wiki-
Topics analyzes a static set of articles. Hierarchical clus-
tering like three-level clustering of Newsblaster (Hatzi-
vassiloglou et al, 2000) could be applied to WikiTopics
to organize current events hierarchically. Summarizing
multiple sentences that are extracted from the articles in
the same cluster would provide a comprehensive descrip-
tion about the current event. Integer linear programming-
based models (Woodsend and Lapata, 2010) may prove to
be useful to generate summaries while global constraints
like length, grammar, and coverage are met.
The problem of Topic Detection and Tracking (TDT)
is to identify and follow new events in newswire, and
to detect the first story about a new event (Allan et al,
1998). Allan et al (2000) evaluated a variety of vector
space clustering schemes, where the best settings from
those experiments were then used in our work. This was
followed recently by Petrovic? et al (2010), who took an
approximate approach to first story detection, as applied
to Twitter in an on-line streaming setting. Such a system
might provide additional information to WikiTopics by
helping to identify and describe current events that have
yet to be explicitly described in a Wikipedia article. Svore
et al (2007) explored enhancing single-document sum-
mariation using news query logs, which may also be ap-
plicable to WikiTopics.
Wikipedia?s inter-article links have been utilized to
39
construct a topic ontology (Syed et al, 2008), word seg-
mentation corpora (Gabay et al, 2008), or to compute
semantic relatedness (Milne and Witten, 2008). In our
work, we found the link structure to be as useful to cluster
topically related articles as well as the article text. In fu-
ture work, the text and the link structure will be combined
as Chaudhuri et al (2009) explored multi-view hierarchi-
cal clustering for Wikipedia articles.
6 Conclusions
We have described a pipeline for article selection, clus-
tering, and textualization in order to identify and describe
significant current events as according to Wikipedia con-
tent, and metadata. Similarly to Wikipedia editors main-
taining that site?s ?current events? pages, we are con-
cerned with neatly collecting articles of daily relevance,
only automatically, and more in line with expressed user
interest (through the use of regularly updated page view
logs). We have suggested that Wikipedia?s hand-curated
articles cannot be predicted solely based on pageviews.
Clustering methods based on topic models and inter-
article link structure are shown to be useful to group
a set of articles that are coherently related to a current
event. Clustering based on only link structure achieved
comparable performance with clustering based on topic
models. In a third of cases, the sentence that best de-
scribed a current event could be extracted from the ar-
ticle text based on temporal expressions within an article.
We employed a coreference resolution system assist in
text generation, for improved readability. As future work,
sentence compression, fusion, and paraphrasing could be
applied to selected sentences with various strategies to
more succinctly summarize the current events. Our ap-
proach is language independent, and may be applied to
multi-lingual current event detection, exploiting further
the online encyclopedia?s cross-language references. Fi-
nally, we plan to leverage social media such as Twit-
ter as an additional signal, especially in cases where es-
sential descriptive information has yet to be added to a
Wikipedia article of interest.
Acknowledgments
We appreciate Domas Mituzas and Fre?de?ric Schu?tz for
the pageviews statistics and Peter Skomoroch for the
Trending Topics software. We also thank three anony-
mous reviewers for their thoughtful advice. This re-
search was supported in part by the NSF under grant IIS-
0713448 and the EC through the EuroMatrixPlus project.
The first author was funded by Samsung Scholarship.
Opinions, interpretations, and conclusions are those of
the authors and not necessarily endorsed by the sponsors.
References
James Allan, Jaime Carbonell, George Doddington, Jonathan
Yamron, and Yiming Yang. 1998. Topic Detection and
Tracking Pilot Study Final Report. In Proceedings of the
DARPA Broadcast News Transcription and Understanding
Workshop.
James Allan, Victor Lavrenko, Daniella Malin, and Russell
Swan. 2000. Detections, bounds, and timelines: UMass
and TDT-3. In Proceedings of Topic Detection and Track-
ing Workshop.
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Inf. Retr.,
12(4):461?486.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Dirichlet
allocation. Journal of Machine Learning Research.
Elizabeth Boschee, Ralph Weischedel, and Alex Zamanian.
2005. Automatic information extraction. In Proceedings of
IA.
Kamalika Chaudhuri, Sham M. Kakade, Karen Livescu, and
Karthik Sridharan. 2009. Multi-view clustering via canoni-
cal correlation analysis. In Proceedings of ICML.
David Gabay, Ziv Ben-Eliahu, and Michael Elhadad. 2008. Us-
ing wikipedia links to construct word segmentation corpora.
In Proceedings of AAAI Workshops.
Vasileios Hatzivassiloglou, Luis Gravano, and Ankineedu Ma-
ganti. 2000. An investigation of linguistic features and clus-
tering algorithms for topical document clustering. In Pro-
ceedings of SIGIR.
Edward Loper and Steven Bird. 2002. NLTK: the Natural Lan-
guage Toolkit. In Proceedings of ACL.
C. Manning, P. Raghavan, and H. Schu?tze. 2008. Introduction
to information retrieval. Cambridge University Press.
Andrew Kachites McCallum. 2002. MALLET: A Machine
Learning for Language Toolkit. http://mallet.cs.umass.edu.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova,
Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002.
Tracking and summarizing news on a daily basis with
Columbia?s Newsblaster. In Proceedings of HLT.
Kathleen Mckeown, Rebecca J. Passonneau, David K. Elson,
Ani Nenkova, and Julia Hirschberg. 2005. Do summaries
help? a task-based evaluation of multi-document summariza-
tion. In Proceedings of SIGIR.
David Milne and Ian H. Witten. 2008. An effective, low-cost
measure of semantic relatedness obtained from Wikipedia
links. In Proceedings of AAAI Workshops.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko. 2010.
Streaming first story dectection with application to Twitter.
In Proceedings of NAACL.
Krysta M. Svore, Lucy Vanderwende, and Christopher J.C.
Burges. 2007. Enhancing single-document summarization
by combining ranknet and third-party sources. In Proceed-
ings of EMNLP-CoLing.
Zareen Saba Syed, Tim Finin, and Anupam Joshi. 2008.
Wikipedia as an ontology for describing documents. In Pro-
ceedings of ICWSM.
Kristian Woodsend and Mirella Lapata. 2010. Automatic gen-
eration of story highlights. In Proceedings of ACL.
40
Proceedings of the TextGraphs-6 Workshop, pages 10?14,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
Nonparametric Bayesian Word Sense Induction
Xuchen Yao1 and Benjamin Van Durme1,2
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We propose the use of a nonparametric Bayesian
model, the Hierarchical Dirichlet Process (HDP),
for the task of Word Sense Induction. Results are
shown through comparison against Latent Dirich-
let Allocation (LDA), a parametric Bayesian model
employed by Brody and Lapata (2009) for this task.
We find that the two models achieve similar levels
of induction quality, while the HDP confers the ad-
vantage of automatically inducing a variable num-
ber of senses per word, as compared to manually
fixing the number of senses a priori, as in LDA.
This flexibility allows for the model to adapt to
terms with greater or lesser polysemy, when ev-
idenced by corpus distributional statistics. When
trained on out-of-domain data, experimental results
confirm the model?s ability to make use of a re-
stricted set of topically coherent induced senses,
when then applied in a restricted domain.
1 Introduction
Word Sense Induction (WSI) is the task of automat-
ically discovering latent senses for each word type,
across a collection of that word?s tokens situated in
context. WSI differs from Word Sense Disambigua-
tion (WSD) in that the task does not assume access
to some prespecified sense inventory. This amounts
to a clustering task: instances of a word are parti-
tioned into the same bin based on whether a sys-
tem deems them to have the same underlying mean-
ing. A large body of related work can be found
in (Schu?tze, 1998; Pantel and Lin, 2002; Dorow
and Widdows, 2003; Purandare and Pedersen, 2004;
Bordag, 2006; Niu et al, 2007; Pedersen, 2007;
Brody and Lapata, 2009; Li et al, 2010; Klapaftis
and Manandhar, 2010).
Brody and Lapata (2009) (B&L herein) showed
that the parametric Bayesian model, Latent Dirich-
let Allocation (LDA), could be successfully em-
ployed for this task, as compared to previous re-
sults published for the WSI component of SemEval-
20071 (Agirre and Soroa, 2007). A deficiency of the
LDA model for WSI is that the number of senses
needs to be manually specified a priori, either sepa-
rately for each word type, or (as done by B&L) some
fixed value that is shared globally across all types.
Nonparametric methods instead have the flexibil-
ity of automatically deciding the number of sense
cluters (Vlachos et al, 2009; Reisinger and Mooney,
2010). In this work we first independently verify
the results of B&L, and then tackle the limitation
on fixing the number of senses through the use of
the Hierarchical Dirichlet Process (HDP) (Teh et al,
2006), a nonparametric Bayesian model. We show
this approach leads to results of similar quality as
LDA, when using a bag-of-words context model, in
addition to allowing for variability in the number of
senses across different words and domains. When
trained on a restricted domain corpus for which
manually labeled sense data was present, we verify
that the model may be tuned to posit a similar num-
ber of senses as determined by human judges. When
trained on a broader domain collection, we show that
the number of induced senses increase, in line with
the intuition that a wider set of genres should lead
to a greater diversity in underlying meanings. Auto-
matically inducing the proper number of senses has
great practical implications, especially in areas that
require word sense disambiguation. For instance, in-
ducing more senses for bank helps to tell differ-
1Klapaftis and Manandhar (2010) and Brody and Lapata
(2009) reported the best scores so far on this dataset.
10
ent word senses apart for naturally more ambigu-
ous words, and inducing less senses for job helps
to prevent assigning too fined-grained senses in case
the same words in two similar contexts are mistak-
enly regarded as carrying different senses.
2 Bayesian Word Sense Induction
wm,n
sm ,n
??m??
??k?? n?[1,Nm]m?[1,M ]k?[1,K ]
Figure 1: Latent Dirichlet Allocation (LDA) for WSI.
As in prior work including B&L, we rely on
the intuition that the senses of words are hinted at
by their contextual information (Yarowsky, 1992).
From the perspective of a generative process, neigh-
boring words of a target are generated by the target?s
underlying sense.2
Both LDA and HDP define graphical models that
generate collections of discrete data. The sense of
a target word is first drawn from a distribution and
then the context of this word is generated according
to that distribution. But while LDA assumes a fixed,
finite set of distributions, the HDP draws from an
infinite set of distributions generated by a Dirichlet
Process. This section details the distinction.
Figure 1 shows the LDA model for word sense
induction. The conventional notion of document is
replaced by a pseudo-document, consisting of every
word in an Nm-word window centered on the target
item. wm,n is the n-th token of the m-th pseudo-
document for target word w. sm,n is the correspond-
ing sense for wm,n. Suppose there are K senses for
the target word w, then the distribution over a con-
text word wm,n is:
2For instance, given the word bank with a sense river
bank, it is more likely that the neighboring words are river,
lake and water than finance, money and loan.
wm,n
sm ,n
k m? 1
n??,? Nm?m? ?,? M?
k 1
K
?
Figure 2: Hierarchical Dirichlet Process (HDP) for WSI.
p(wm,n) =
K?
k=1
p(wm,n | sm,n = k)p(sm,n = k).
Let the word distribution given a sense be
p(wm,n | sm,n = k) = ~?k, which is a vector of
length V (vocabulary size) that is generated from
a Dirichlet distribution: ~?k ? Dir(~?). Let the
sense distribution given a document be p(sm,n | d =
m) = ~?m, which is a vector of length K that is gen-
erated from a Dirichlet distribution: ~?m ? Dir(~?).
The generative story for the data under LDA is then:
For k ? (1, ...,K) senses:
Sample mixture component: ~?k ? Dir(~?).
For m ? (1, ...,M) pseudo-documents:
Sample topic components ~?m ? Dir(~?).
For n ? (1, ..., Nm) words in pseudo-document m:
Sample sense index sm,n ?Mult(~?m).
Sample word wm,n ?Mult(~?sm,n).
The sense distribution over a word is captured
as K mixture components. In the HDP however,
we assume the number of active components is un-
known, and should be inferred from the data. For
each pseudo-document, the sense component sm,n
for word wm,n has a nonparametric prior Gm. Gm
is nonparametric in the sense that for every new
pseudo-document m, a new Gm is sampled from a
base distribution G0. As the corpus grows, there are
11
more and more Gm?s. However, the mixture com-
ponent sm,n, drawn from Gm, can be shared among
pseudo-documents. Thus the number of senses do
not simply multiply out as m grows. Both G0 and
Gm?s are distributed according to a Dirichlet Process
(DP) (Ferguson, 1973). The generative story is:
Select base distribution G0 ? DP (?,H) which
provides an unlimited inventory of senses.
For m ? (1, ...,M) pseudo-documents:
Draw Gm ? DP (?0, G0).
For n ? (1, ..., Nm) words in pseudo-document m:
Sample sm,n ? Gm.
Sample wm,n ?Mult(~?sm,n).
Hyperparameters ? and ?0 are the concentration
parameters of the DP, controlling the variability of
the distributionsG0 andGm. In a Chinese restaurant
franchise metaphor of the HDP, multiple restaurants
(documents) share a set of dishes (senses). Then
? controls the variability of the global sense distri-
bution and ?0 controls the variability of each cus-
tomer?s (word) choice of dishes (senses).3
3 Experiment Setting
Model B&L experimented with variations to the
LDA model that allowed for generating multiple lay-
ers of features, such as smaller (5w) and larger (10w)
bag-of-word contexts, and syntactic features. The
additional complexity beyond the standard model
led to only tenuous performance gains. Normal
LDA, when trained on pseudo-documents built from
10 words of surrounding context, performed only
slightly below their best reported results.4 Espe-
cially as our goal here was to investigate the sense-
specification problem, rather than eking out further
improvements in the base WSI evaluation measure,
we chose to compare a standard LDA model to HDP,
both strictly using a 10 word context.5
Test Data Following B&L, we perform WSI on
nouns. The evaluation data comes from the WSI
task of SemEval-2007 (Agirre and Soroa, 2007). It
is derived from the Wall Street Journal portion of
3Gibbs sampling (Geman and Geman, 1990) can be applied
for inference. Specifically, Teh et al (2006) describes the pos-
terior sampling in the Chinese restaurant franchise.
4F-score of 86.9% (10w), as compared to 87.3% (10w+5w).
5We relied on implementations of LDA and HDP respec-
tively from MALLET (McCallum, 2002), and Wang (2010).
the Penn TreeBank (Marcus et al, 1994) and con-
tains 15,852 instances of excerpts on 35 nouns. All
the nouns are hand-annotated with their OntoNotes
senses (Hovy et al, 2006), with an average of 3.9
senses per word.
Evaluation Method WSI is an unsupervised task
that results in sense clusters with no explicit map-
ping to manually annotated sense data. To derive
such a mapping, we follow the supervised evalua-
tion strategy of Agirre and Soroa (2007). Anno-
tated senses from SemEval-2007 are partitioned into
a standard mapping set (72%), a dev set (14%) and a
test set (14%). After an WSI system has tagged the
elements in the mapping set with their ?cluster IDs?,
then a cluster to sense derivation is constructed by
simply assigning to each cluster the manual sense
label that has the highest in-cluster frequency. Once
such a mapping has been established, then results
on the dev or test set are reported based on treating
cluster assignment as a WSD operation.
Training Data As out-of-domain source, we ex-
tracted 930K instances of the 35 nouns from the
British National Corpus (BNC) (Clear, 1993). As
in-domain source we extracted another 930K in-
stances from WSJ in years 87/88/90/94. All pseudo-
documents use the ?10 contextual window.
4 Evaluation
We trained the LDA and HDP models on the WSJ
and BNC datasets separately. In their experiments
with LDA, B&L iteratively tried 3 up to 9 senses,
and then reported the number that led to best re-
sults in evaluation (4 senses for WSJ, 8 for BNC).
We repeated this approach for LDA, with hyper-
parameters ? = 0.02 and ? = 0.1. For the HDP
model, we tuned hyper-parameters on the SemEval-
2007 dev set.6 See Table 1 for results, averaged over
5 runs of LDA and 3 runs of HDP.
We report several findings based on this experi-
ment. First, for the LDA models trained on WSJ
and BNC, our F1 measures are 0.8% lower than
reported by B&L.7 Second, based on our own ex-
periment, the HDP model performance is slightly
better than that of LDA when training with BNC.
6Final parameters: H = 0.1, ?0 ? Gamma(0.1, 0.028),
? ? Gamma(1, 0.1).
7We consider this acceptable experimental deviation, given
the minor variation in respective training data.
12
WSJ BNC
LDA-4s* 86.9 LDA-8s* 84.6
LDA-4s 86.1 LDA-8s 83.8
HDP 86.7 HDP 85.74
Table 1: F-measure when training with WSJ (in-domain) and
BNC (out-of-domain). Results with * are taken from B&L. 4
or 8 senses were used per word. 4: statistically significant
against LDA-8s by paired permutation test with p < 0.001.
The standard baseline, always picking the most frequent sense
observed in training, scores 80.9.
WSJ BNC
Train Test Train Test
LDA 4.0 3.9 8.0 7.4
HDP 5.8 3.9 9.4 4.6
Table 2: The average number of senses the LDA and HDP
models output when training with WSJ/BNC and testing on
SemEval-2007, which has 3.9 senses per word on average.
Third, the HDP model appears to better adapt to data
in other domains. When switching the training set
from WSJ (in-domain) to BNC (out-of-domain), we,
along with B&L, found a 2.3% drop with LDA mod-
els. However, with the HDP model, there is only a
1% drop in F1. Moreover, even trained on out-of-
domain data, HDP can still better infer the number
of senses from the test data, which is illustrated next.
Table 2 shows the number of senses induced from
each dataset. When training on WSJ and test on
SemEval-2007, HDP induced the correct number of
senses (3.9 on average) from test, while LDA did
this by assuming 4 senses from the training data.
When there is a domain mismatch between train-
ing (BNC) and test (SemEval-2007, which comes
from the 1989 WSJ), the LDA model preferred far
more than the annotated number of senses (7.4 vs.
3.9), largely due to the fact that it assumed 8 senses
during training. However, even though the HDP
model induced more senses (9.4) when training on
the broader coverage BNC set, it still inferred a
much reduced average of 4.6 senses on test.
The BNC, being a balanced corpus, covers more
diverse genres than the WSJ: we would expect it to
lead to a more inclusive model of word sense. Fig-
ure 3 illustrates this comparison through the differ-
ence between sense numbers. For the 35 human-
annotated nouns, HDP induced the number of senses
mostly within an error of ?2, whereas LDA tended
to prefer 3 ? 6 more senses than recognized by an-
-4 -3 -2 -1 0 1 2 3 4 5 60
2
4
6
8
10
12 HDPLDA
# induced senses - # annotated senses
frequ
enc y
Figure 3: The difference between induced number of senses
and annotated senses. The training set is BNC. The test set
is SemEval-2007, containing 35 nouns with 3.9 senses. LDA
induced 7.4 senses and HDP induced 4.6 senses on average.
WSJ BNC
LDA-5.8s 86.0 LDA-9.4s 82.7
LDA-3.9s 85.3 LDA-3.9s 81.4
HDP-5.8s 86.7 HDP-9.4s 85.74
Table 3: F1 measure when training LDA with three other set-
tings: 5.8s, 9.4s and 3.9s. 4: statistically significant against
both LDA-9.4s and LDA-3.9s (for BNC) by paired permutation
test with p < 0.001.
notators (on average the HDP model was off by 1.6
senses, as compared to 3.6 by LDA). Finally, the
F1 performance of HDP is 1.9% better than LDA
(85.7% vs. 83.8%).
We further evaluated the LDA model by training
separately for each of the 35 nouns, first setting as
the number of topics the amount induced by HDP
(on average, 5.8/9.4 senses for WSJ/BNC), then us-
ing the number of senses as used by the human anno-
tators in SemEval-2007 (an average of 3.8). As seen
in Table 3, in each of these cases HDP remained the
superior model.
5 Conclusion
We proposed the use of a nonparametric Bayesian
model (HDP) for word sense induction and com-
pared it with the parametric model by Brody and
Lapata (2009), based on LDA. The HDP model con-
fers the advantage of automatically identifying the
number of senses, besides having equivalent (or bet-
ter) performance than the LDA model, verified us-
ing the SemEval-2007 dataset. Future work includes
large scale sense induction over a larger vocabulary,
in tasks such as Paraphrase Acquisition.
13
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 Task 02:
Evaluating Word Sense Induction And Discrimination Sys-
tems. In Proceedings of the 4th International Workshop on
Semantic Evaluations, SemEval ?07, pages 7?12.
Stefan Bordag. 2006. Word Sense Induction: Triplet-Based
Clustering And Automatic Evaluation. In Proceedings of the
11th EACL, pages 137?144.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word Sense
Induction. In EACL ?09: Proceedings of the 12th Confer-
ence of the European Chapter of the Association for Compu-
tational Linguistics, pages 103?111.
Jeremy H. Clear, 1993. The British national corpus, pages 163?
187. MIT Press, Cambridge, MA, USA.
Beate Dorow and Dominic Widdows. 2003. Discovering
Corpus-Specific Word Senses. In Proceedings of the tenth
conference on European chapter of the Association for Com-
putational Linguistics, EACL ?03, pages 79?82.
T. S. Ferguson. 1973. A Bayesian Analysis of Some Nonpara-
metric Problems. The Annals of Statistics, 1(2):209?230.
S. Geman and D. Geman, 1990. Stochastic Relaxation, Gibbs
Distributions, And The Bayesian Restoration Of Images,
pages 452?472.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the
90% solution. In Proceedings of the Human Language
Technology Conference of the NAACL, Companion Volume:
Short Papers, NAACL-Short ?06, pages 57?60.
Ioannis Klapaftis and Suresh Manandhar. 2010. Word Sense
Induction & Disambiguation Using Hierarchical Random
Graphs. In Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing, pages 745?
755, October.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic
Models For Word Sense Disambiguation And Token-Based
Idiom Detection. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL ?10,
pages 1138?1147.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Andrew Kachites McCallum. 2002. MALLET: A Machine
Learning for Language Toolkit.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2R:
Three Systems For Word Sense Discrimination, Chinese
Word Sense Disambiguation, And English Word Sense Dis-
ambiguation. In Proceedings of the 4th International Work-
shop on Semantic Evaluations, SemEval ?07, pages 177?
182.
Patrick Pantel and Dekang Lin. 2002. Discovering Word
Senses From Text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge discovery
and data mining, KDD ?02, pages 613?619.
Ted Pedersen. 2007. UMND2: SenseClusters applied to the
sense induction task of Senseval-4. In Proceedings of the 4th
International Workshop on Semantic Evaluations, SemEval
?07, pages 394?397.
Amruta Purandare and Ted Pedersen. 2004. Word Sense Dis-
crimination by Clustering Contexts in Vector and Similarity
Spaces. In Proceedings of CoNLL-2004, pages 41?48.
Joseph Reisinger and Raymond J. Mooney. 2010. A Mixture
Model with Sharing for Lexical Semantics. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2010), pages 1173?1182, MIT,
Massachusetts, USA.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrimination.
Comput. Linguist., 24:97?123, March.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet Processes. Journal of the American
Statistical Association, 101(476):1566?1581.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani.
2009. Unsupervised and constrained Dirichlet process mix-
ture models for verb clustering. In Proceedings of the Work-
shop on Geometrical Models of Natural Language Seman-
tics, GEMS ?09, pages 74?82.
Chong Wang. 2010. An implementation of hierarchical dirich-
let process (HDP) with split-merge operations.
David Yarowsky. 1992. Word-Sense Disambiguation Using
Statistical Models Of Roget?s Categories Trained On Large
Corpora. In Proceedings of the 14th conference on Compu-
tational linguistics, pages 454?460.
14
Workshop on Monolingual Text-To-Text Generation, pages 84?90,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 84?90,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Paraphrastic Sentence Compression with a Character-based Metric:
Tightening without Deletion
Courtney Napoles1 and Chris Callison-Burch1 and Juri Ganitkevitch1 and
Benjamin Van Durme1,2
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present a substitution-only approach to
sentence compression which ?tightens? a sen-
tence by reducing its character length. Replac-
ing phrases with shorter paraphrases yields
paraphrastic compressions as short as 60% of
the original length. In support of this task,
we introduce a novel technique for re-ranking
paraphrases extracted from bilingual corpora.
At high compression rates1 paraphrastic com-
pressions outperform a state-of-the-art dele-
tion model in an oracle experiment. For fur-
ther compression, deleting from oracle para-
phrastic compressions preserves more mean-
ing than deletion alone. In either setting, para-
phrastic compression shows promise for sur-
passing deletion-only methods.
1 Introduction
Sentence compression is the process of shortening a
sentence while preserving the most important infor-
mation. Because it was developed in support of ex-
tractive summarization (Knight and Marcu, 2000),
much of the previous work considers deletion-based
models, which extract a subset of words from a long
sentence to create a shorter sentence such that mean-
ing and grammar are maximally preserved. This
framework imposes strict constraints on the task and
does not accurately model human-written compres-
sions, which tend to be abstractive rather than ex-
tractive (Marsi et al, 2010). This is one sense in
which paraphrastic compression can improve exist-
ing compression methodologies.
1Compression rate is defined as the compression length over
original length, so lower values indicate shorter sentences.
We distinguish two non-identical notions of sen-
tence compression: making a sentence substantially
shorter versus ?tightening? a sentence by remov-
ing unnecessary verbiage. We propose a method to
tighten sentences with just substitution and no dele-
tion operations. Using paraphrases extracted from
bilingual text and re-ranked on monolingual data,
our system selects the set of paraphrases that min-
imizes the character length of a sentence.
While not currently the standard, character-based
lengths have been considered before in compres-
sion, and we believe that it is relevant for current
and future applications. Character lengths have been
used for document summarization (DUC 2004, Over
and Yen (2004)), summarizing for mobile devices
(Corston-Oliver, 2001), and subtitling (Glickman et
al., 2006). Although in the past strict word limits
have been imposed for various documents, informa-
tion transmitted electronically is often limited by the
number of bytes, which directly relates to the num-
ber of characters. Mobile devices, SMS messages,
and microblogging sites such as Twitter are increas-
ingly important for quickly spreading information.
In this context, it is important to consider character-
based constraints.
We examine whether paraphrastic compression
allows more information to be conveyed in the same
number of characters as deletion-only compressions.
For example, the length constraint of Twitter posts or
tweets is 140 characters, and many article lead sen-
tences exceed this limit. A paraphrase substitution
oracle compresses the sentence in the table below to
76% of its original length (162 to 123 characters; the
first is the original). The compressed tweet is 140
84
characters, including spaces 17-character shortened
link to the original article.2
Congressional leaders reached a last-gasp agreement
Friday to avert a shutdown of the federal government,
after days of haggling and tense hours of brinksman-
ship.
Congress made a final agreement Fri. to avoid govern-
ment shutdown, after days of haggling and tense hours
of brinkmanship. on.wsj.com/h8N7n1
In contrast, using deletion to compress to the same
length may not be as expressive:
Congressional leaders reached agreement Friday to
avert a shutdown of federal government, after haggling
and tense hours. on.wsj.com/h8N7n1
This work presents a model that makes paraphrase
choices to minimize the character length of a sen-
tence. An oracle paraphrase-substitution experiment
shows that human judges rate paraphrastic compres-
sions higher than deletion-based compressions. To
achieve further compression, we shortened the or-
acle compressions using a deletion model to yield
compressions 80% of the original sentence length
and compared these to compressions generated us-
ing just deletions. Manual evaluation found that
the oracle-then-deletion compressions to preserve
more meaning than deletion-only compressions at
uniform compression rates.
2 Related work
Most of the previous research on sentence compres-
sion focuses on deletion using syntactic informa-
tion, (e.g., Galley and McKeown (2007), Knight
and Marcu (2002), Nomoto (2009), Galanis and An-
droutsopoulos (2010), Filippova and Strube (2008),
McDonald (2006), Yamangil and Shieber (2010),
Cohn and Lapata (2008), Cohn and Lapata (2009),
Turner and Charniak (2005)). Woodsend et al
(2010) incorporate paraphrase rules into a deletion
model. Previous work in subtitling has made one-
word substitutions to decrease character length at
high compression rates (Glickman et al, 2006).
More recent approaches in steganography have used
paraphrase substitution to encode information in text
but focus on grammaticality, not meaning preserva-
tion (Chang and Clark, 2010). Zhao et al (2009) ap-
plied an adaptable paraphrasing pipeline to sentence
2Taken from the main page of http://wsj.com, April 9, 2011.
compression, optimizing for F-measure over a man-
ually annotated set of gold standard paraphrases.
Sentence compression has been considered be-
fore in contexts outside of summarization, such as
headline, title, and subtitle generation (Dorr et al,
2003; Vandeghinste and Pan, 2004; Marsi et al,
2009). Corston-Oliver (2001) deleted characters
from words to shorten the character length of sen-
tences. To our knowledge character-based compres-
sion has not been examined before with the surging
popularity and utility of Twitter.
3 Sentence Tightening
The distinction between tightening and compression
can be illustrated by considering how much space
needs to be preserved. In the case of microblogging,
often a sentence has just a few too many characters
and needs to be ?tightened?. On the other hand, if a
sentence is much longer than a desired length, more
drastic compression is necessary. The first subtask
is relevant in any context with strict word or charac-
ter limits. Some sentences may not be compressible
beyond a certain limit. For example, we found that
near 10% of the compressions generated by Clarke
and Lapata (2008) were identical to the original sen-
tence. In situations where the sentence must meet
a minimum length, tightening can be used to meet
these requirements.
Multi-reference translations provide an instance
of the natural length variation of human-generated
sentences. These translations represent different
ways to express the foreign same sentence, so there
should be no meaning lost between the different ref-
erence translations. The character-based length of
different translations of a given sentence varies on
average by 80% when compared to the shortest sen-
tence in a set.3 This provides evidence that sen-
tences can be tightened to some extent without los-
ing any meaning.
Through the lens of sentence tightening, we con-
sider whether paraphrase substitutions alone can
yield compressions competitive with a deletion at
the same length. A character-based compression
rate is crucial in this framework, as two compres-
3This value will vary by collection and with the number of
references: for example, the NIST05 Arabic reference set has a
mean compression rate of 0.92 with 4 references per set.
85
sions having the same character-based compres-
sion rate may have different word-based compres-
sion rates. The advantage of a character-based sub-
stitution model is in choosing shorter words when
possible, freeing space for more content words. Go-
ing by word length alone would exclude the many
paraphrases with fewer characters than the original
phrase and the same number of words (or more).
3.1 Paraphrase Acquisition
To generate paraphrases for use in our experiments,
we took the approach described by Bannard and
Callison-Burch (2005), which extracts paraphrases
from bilingual parallel corpora. Figure 1 illustrates
the process. A phrase to be paraphrased, like thrown
into jail, is found in a German-English parallel cor-
pus. The corresponding foreign phrase (festgenom-
men) is identified using word alignment and phrase
extraction techniques from phrase-based statistical
machine translation (Koehn et al, 2003). Other oc-
currences of the foreign phrase in the parallel corpus
may align to another English phrase like jailed. Fol-
lowing Bannard and Callison-Burch, we treated any
English phrases that share a common foreign phrase
as potential paraphrases of each other.
As the original phrase occurs several times and
aligns with many different foreign phrases, each of
these may align to a variety of other English para-
phrases. Thus, thrown into jail not only paraphrases
as jailed, but also as arrested, detained, impris-
oned, incarcerated, locked up, taken into custody,
and thrown into prison . Moreover, because the
method relies on noisy and potentially inaccurate
word alignments, it is prone to generate many bad
paraphrases, such as maltreated, thrown, cases, cus-
tody, arrest, owners, and protection.
To rank candidates, Bannard and Callison-Burch
defined the paraphrase probability p(e2|e1) based
on the translation model probabilities p(e|f) and
p(f |e) from statistical machine translation. Follow-
ing Callison-Burch (2008), we refine selection by re-
quiring both the original phrase and paraphrase to
be of the same syntactic type, which leads to more
grammatical paraphrases.
Although many excellent paraphrases are ex-
tracted from parallel corpora, many others are un-
suitable and the translation score does not always
accurately distinguish the two. Therefore, we re-
Paraphrase Monlingual Bilingual
study in detail 1.00 0.70
scrutinise 0.94 0.08
consider 0.90 0.20
keep 0.83 0.03
learn 0.57 0.10
study 0.42 0.07
studied 0.28 0.01
studying it in detail 0.16 0.05
undertook 0.06 0.06
Table 1: Candidate paraphrases for study in detail with
corresponding approximate cosine similarity (Monolin-
gual) and translation model (Bilingual) scores.
ranked our candidates based on monolingual distri-
butional similarity, employing the method described
by Van Durme and Lall (2010) to derive approxi-
mate cosine similarity scores over feature counts us-
ing single token, independent left and right contexts.
Features were computed from the web-scale n-gram
collection of Lin et al (2010). As 5-grams are the
highest order of n-gram in this collection, the al-
lowable set of paraphrases have at most four words
(which allows at least one word of context).
To our knowledge this is the first time such tech-
niques have been used in combination in order to
derive higher quality paraphrase candidates. See Ta-
ble 1 for an example.
The monolingual-filtering technique we describe
is by no means limited to paraphrases extracted from
bilingual corpora. It could be applied to other data-
driven paraphrasing techniques (see Madnani and
Dorr (2010) for a survey). Although it is particularly
well suited to the bilingual extracted corpora, since
the information that it adds is orthogonal to that
model, it would presumably add less to paraphras-
ing techniques that already take advantage of mono-
lingual distributional similarity (Pereira et al, 1993;
Lin and Pantel, 2001; Barzilay and Lee, 2003).
In order to evaluate the paraphrase candidates
and scoring techniques, we randomly selected 1,000
paraphrase sets where the source phrase was present
in the corpus described in Clarke and Lapata (2008).
For each phrase and set of candidate paraphrases, we
extracted all of the contexts from the corpus in which
the source phrase appeared. Human judges were
presented each sentence with the original phrase and
the same sentences with each paraphrase candidate
86
... letzteWoche wurden in Irland f?nf Landwirte festgenommen , weil sie verhindern wollten
... last week five farmers were thrown into jail in Ireland because they resisted ...
...
Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und get?tet .
Quite a few journalists have disappeared or have been imprisoned , tortured and killed .
Figure 1: Using a bilingual parallel corpus to extract paraphrases.
substituted in. Each paraphrase substitution was
graded based on the extent to which it preserved
the meaning and affected the grammaticality of the
sentence. While both the bilingual translation score
and monolingual cosine similarity positively corre-
lated with human judgments, the monolingual score
proved a stronger predictor of quality in both dimen-
sions. Using Kendall?s tau correlation coefficient,
the agreement between the ranking imposed by the
monolingual score and human ratings surpassed that
of the original ranking as derived during the bilin-
gual extraction, for both meaning and grammar.4 In
our substitution framework, we ignore the transla-
tion probabilities and use only the approximate co-
sine similarity in the paraphrase decision task.
4 Framework for Sentence Tightening
Our sentence tightening approach uses a dynamic
programming strategy to find the combination of
non-overlapping paraphrases that minimizes a sen-
tence?s character length. The threshold of the mono-
lingual score for paraphrases can be varied to widen
or narrow the search space, which may be further in-
creased by considering any lexical paraphrases not
subject to syntactic constraints. Sentences with a
compression rate as low as 0.6 can be generated
without thresholding the paraphrase scores. Because
the system can generate multiple paraphrased sen-
tences of equal length, we apply two layers of filter-
ing to generate a single output. First we calculate a
word-overlap score between the original and candi-
date sentences to favor compressions similar to the
original sentence; then, from among the sentences
4For meaning and grammar respectively, ? = 0.28 and 0.31
for monolingual scores and 0.19 and 0.15 for bilingual scores.
with the highest word overlap, we select the com-
pression with the best language model score.
Higher paraphrase thresholds guarantee more ap-
propriate paraphrases but yield longer compressions.
Using a cosine-similarity threshold of 0.95, the av-
erage compression rate is 0.968, which is consider-
ably longer than the compressions using no thresh-
old (0.60). In these experiments we did not syntac-
tically constrain paraphrases. However, we believe
that our monolingual refining of paraphrase sets im-
proves paraphrase selection and is a reasonable al-
ternative to using syntactic constraints.
In case judges favor compressions that have high
word overlap with the original sentence, we com-
pressed the longest sentence from each set of ref-
erence translations (Huang et al, 2002) and ran-
domly chose a sentence from the set of reference
translations to use as the standard for comparison.
Paraphrastic compressions were generated at cosine-
similarity thresholds ranging from 0.60 to 0.95.
We implemented a state-of-the-art deletion model
(Clarke and Lapata, 2008) to generate deletion-only
compressions. We fixed the compression length
to ?5 characters of the length of each paraphras-
tic compression, in order to isolate the compression
quality from the effect of compression rate (Napoles
et al, 2011). Manual evaluation used Amazon?s
Mechanical Turk with three-way redundancy and
positive and negative controls to filter bad workers.
Meaning and grammar judgments were collected us-
ing two 5-point scales (5 being the highest score).
5 Evaluation
The initial results of our substitution system show
room for improvement in future work (Table 2). We
believe this is due to erroneous paraphrase substi-
87
System Grammar Meaning CompR Cos.
Substitution 3.8 3.7 0.97 0.95
Deletion 4.1 4.0 0.97 -
Substitution 3.4 3.2 0.89 0.85
Deletion 4.0 3.8 0.89 -
Substitution 3.1 3.0 0.85 0.75
Deletion 3.9 3.7 0.85 -
Substitution 2.9 2.9 0.82 0.65
Deletion 3.8 3.5 0.82 -
Table 2: Mean ratings of compressions using just deletion
or substitution at different paraphrase thresholds (Cos.).
Deletion performed better in all settings.
tutions, since phrases with the same syntactic cate-
gory and distributional similarity are not necessarily
semantically identical. Illustrative examples include
WTO for United Nations and east or west for south.
Because the quality of the multi-reference transla-
tions is not uniformly high, for the following exper-
iment we used a dataset of English newspaper arti-
cles.
To control against these errors and test the viabil-
ity of a substitution-only approach, we generated all
possible paraphrase substitutions above a threshold
of 0.80 within a set of 20 randomly chosen sentences
from the written corpus of Clarke and Lapata (2008).
We solicited humans to make a ternary decision of
whether a paraphrase was acceptable in the context
(good, bad, or not sure). We applied our model to
generate compressions using only paraphrase substi-
tutions on which all three annotators agreed that the
paraphrase was good. The oracle generated com-
pressions with an average compression rate of 0.90.
On the same set of original sentences, we used
the deletion model to generate compressions con-
strained to ?5 characters of the length of the ora-
cle compression. Next, we examined whether apply-
ing the deletion model to paraphrastic compressions
would improve compression quality. In manual eval-
uation along the dimensions of grammar and mean-
ing, both the oracle compressions and oracle-plus-
deletion compressions outperformed the deletion-
only compressions at uniform lengths (Table 3)5.
These results suggest that improvements in para-
phrase acquisition will make our system competitive
with deletion-only models.
5Paraphrastic compressions were rated significantly higher
for meaning, p < 0.05
Model Grammar Meaning CompR
Oracle 4.1 4.3 0.90
Deletion 4.0 4.1 0.90
Gold 4.3 3.8 0.75
Oracle+deletion 3.4 3.7 0.80
Deletion 3.2 3.4 0.80
Table 3: Mean ratings of compressions generated by a
substitution oracle, deletion only, deletion on the oracle
compression, and the gold standard. Being able to choose
the best paraphrases would enable our substitution model
to outperform the deletion model.
6 Conclusion
This work shows promise for the use of only sub-
stitution in the task of sentence tightening. There
are myriad possible extensions and improvements
to this method, most notably richer features be-
yond paraphrase length. We do not currently use
syntactic information in our paraphrastic compres-
sion model because it places limits on the number
of paraphrases available for a sentence and thereby
limits the possible compression rate. The current
method for paraphrase extraction does not include
certain types of rewriting, such as passivization, and
should be extended to incorporate even more short-
ening paraphrases. Future work can directly apply
these methods to Twitter and extract additional para-
phrases and abbreviations from Twitter and/or SMS
data. Our substitution approach can be improved by
applying more sophisticated techniques to choosing
the best candidate compression, or by framing it as
an optimization problem over more than just mini-
mal length. Overall, we find these results to be en-
couraging for the possibility of sentence compres-
sion without deletion.
Acknowledgments
We are grateful to John Carroll for helping us obtain
the RASP parser. This research was partially funded
by the JHU Human Language Technology Center of
Excellence. This research was funded in part by the
NSF under grant IIS-0713448. The views and find-
ings are the authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
88
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Ching-Yun Chang and Stephen Clark. 2010. Linguis-
tic steganography using automatically generated para-
phrases. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
591?599. Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Simon Corston-Oliver. 2001. Text compaction for dis-
play on very small screens. In Proceedings of the
NAACL Workshop on Automatic Summarization.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
Workshop on Text summarization Workshop.
Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference. Association for Computational Linguis-
tics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen R. McKeown. 2007. Lex-
icalized Markov grammars for sentence compression.
the Proceedings of NAACL/HLT.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-
gio, and Walter Daelemans. 2006. Investigating lexi-
cal substitution scoring for subtitle generation. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning, pages 45?52. Associa-
tion for Computational Linguistics.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? Step one: Sentence compression. In
Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New Tools for Web-Scale
N-grams. In Proceedings of LREC.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2009. Is sentence compression an NLG
task? In Proceedings of the 12th European Workshop
on Natural Language Generation.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2010. On the limits of sentence com-
pression by deletion. Empirical Methods in Natural
Language Generation, pages 45?66.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In In Proceed-
ings of EACL.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence compres-
sion: Pitfalls and suggested remedies. In Proceedings
of ACL, Workshop on Monolingual Text-To-Text Gen-
eration.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
Paul Over and James Yen. 2004. An introduction to
DUC 2004: Intrinsic evaluation of generic news text
summarization systems. In Proceedings of DUC 2004
Document Understanding Workshop, Boston.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In ACL-93.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automated subtitling: A hybrid approach.
In Proceedings of the ACL workshop on Text Summa-
rization.
89
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of EMNLP.
Elif Yamangil and Stuart M. Shieber. 2010. Bayesian
synchronous tree-substitution grammar induction and
its application to sentence compression. In Proceed-
ings of ACL.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
90
Workshop on Monolingual Text-To-Text Generation, pages 91?97,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 91?97,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Evaluating sentence compression: Pitfalls and suggested remedies
Courtney Napoles1 and Benjamin Van Durme1,2 and Chris Callison-Burch1
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
This work surveys existing evaluation
methodologies for the task of sentence
compression, identifies their shortcomings,
and proposes alternatives. In particular,
we examine the problems of evaluating
paraphrastic compression and comparing the
output of different models. We demonstrate
that compression rate is a strong predictor
of compression quality and that perceived
improvement over other models is often a side
effect of producing longer output.
1 Introduction
Sentence compression is the natural language gen-
eration (NLG) task of automatically shortening sen-
tences. Because good compressions should be gram-
matical and retain important meaning, they must be
evaluated along these two dimensions. Evaluation is
a difficult problem for NLG, and many of the prob-
lems identified in this work are relevant for other
generation tasks. Shared tasks are popular in many
areas as a way to compare system performance in an
unbiased manner. Unlike other tasks, such as ma-
chine translation, there is no shared-task evaluation
for compression, even though some compression
systems are indirectly evaluated as a part of DUC.
The benefits of shared-task evaluation have been dis-
cussed before (e.g., Belz and Kilgarriff (2006) and
Reiter and Belz (2006)), and they include compar-
ing systems fairly under the same conditions.
One difficulty in evaluating compression systems
fairly is that an unbiased automatic metric is hard
to define. Automatic evaluation relies on a com-
parison to a single gold standard at a predetermined
length, which greatly limits the types of compres-
sions that can be fairly judged. As we will discuss
in Section 2.1.1, automatic evaluation assumes that
deletions are independent, considers only a single
gold standard, and cannot handle compressions with
paraphrasing. Like for most areas in NLG, human
evaluation is preferable. However, as we discuss in
Section 2.2, there are some subtleties to appropri-
ate experiment design, which can give misleading
results if not handled properly.
This work identifies the shortcomings of widely
practiced evaluation methodologies and proposes al-
ternatives. We report on the effect of compression
rate on perceived quality and suggest ways to control
for this dependency when evaluating across different
systems. In this work we:
? highlight the importance of comparing systems
with similar compression rates,
? argue that comparisons in many previous pub-
lications are invalid,
? provide suggestions for unbiased evaluation.
While many may find this discussion intuitive, these
points are not addressed in much of the existing re-
search, and therefore it is crucial to enumerate them
in order to improve the scientific validity of the task.
2 Current Practices
Because it was developed in support of extractive
summarization (Knight and Marcu, 2000), com-
pression has mostly been framed as a deletion task
(e.g., McDonald (2006), Galanis and Androutsopou-
los (2010), Clarke and Lapata (2008), and Galley
91
Words Sentence
31 Kaczynski faces charges contained in a 10-count federal indictment naming him as the person responsible for trans-
porting bombs and bomb parts from Montana to California and mailing them to victims .
17 Kaczynski faces charges naming him responsible for transporting bombs to California and mailing them to victims .
18 Kaczynski faces charges naming him responsible for transporting bombs and bomb parts and mailing them to victims .
18 Kaczynski faces a 10-count federal indictment for transporting bombs and bomb parts and mailing them to victims .
Table 1: Three acceptable compressions of a sentence created by different annotators (the first is the original).
and McKeown (2007)). In this context, a compres-
sion is an extracted subset of words from a long
sentence. There are limited compression corpora
because, even when an aligned corpus exists, the
number of extractive sentence pairs will be few and
therefore gold-standard compressions must be man-
ually annotated. The most popular corpora are the
Ziff-Davis corpus (Knight and Marcu, 2000), which
contains a small set of 1067 extracted sentences
from article/abstract pairs, and the manually anno-
tated Clarke and Lapata (2008) corpus, consisting of
nearly 3000 sentences from news articles and broad-
cast news transcripts. These corpora contain one
gold standard for each sentence.
2.1 Automatic Techniques
One of the most widely used automatic metrics is the
F1 measure over grammatical relations of the gold-
standard compressions (Riezler et al, 2003). This
metric correlates significantly with human judg-
ments and is better than Simple String Accuracy
(Bangalore et al, 2000) for judging compression
quality (Clarke and Lapata, 2006). F1 has also been
used over unigrams (Martins and Smith, 2009) and
bigrams (Unno et al, 2006). Unno et al (2006)
compared the F1 measures to BLEU scores (using
the gold standard as a single reference) over vary-
ing compression rates, and found that BLEU be-
haves similarly to both F1 measures. A syntactic
approach considers the alignment over parse trees
(Jing, 2000), and a similar technique has been used
with dependency trees to evaluate the quality of sen-
tence fusions (Marsi and Krahmer, 2005).
The only metric that has been shown to correlate
with human judgments is F1 (Clarke and Lapata,
2006), but even this is not entirely reliable. F1 over
grammatical relations also depends on parser accu-
racy and the type of dependency relations used.1
1For example, the RASP parser uses 16 grammatical depen-
2.1.1 Pitfalls of Automatic Evaluation
Automatic evaluation operates under three often
incorrect assumptions:
Deletions are independent. The dependency
structure of a sentence may be unaltered when de-
pendent words are not deleted as a unit. Examples
of words that should be treated as a single unit in-
clude negations and negative polarity items or cer-
tain multi-word phrases (such as deleting Latin and
leaving America). F1 treats all deletions equally,
when in fact errors of this type may dramatically al-
ter the meaning or the grammaticality of a sentence
and should be penalized more than less serious er-
rors, such as deleting an article.
The gold standard is the single best compres-
sion. Automatic evaluation considers a single
gold-standard compression. This ignores the pos-
sibility of different length compressions and equally
good compressions of the same length, where mul-
tiple non-overlapping deletions are acceptable. For
an example, see Table 1.
Having multiple gold standards would provide
references at different compression lengths and re-
flect different deletion choices (see Section 3). Since
no large corpus with multiple gold standards exists
to our knowledge, systems could instead report the
quality of compressions at several different com-
pression rates, as Nomoto (2008) did. Alternatively,
systems could evaluate compressions that are of a
similar length as the gold standard compression, to
fix a length for the purpose of evaluation. Output
length is controlled for evaluation in some other ar-
eas, notably DUC.
Systems compress by deletion and not substitu-
tion. More recent approaches to compression in-
troduce reordering and paraphrase operations (e.g.,
dencies (Briscoe, 2006) while there are over 50 Stanford De-
pendencies (de Marneffe and Manning, 2008).
92
Cohn and Lapata (2008), Woodsend et al (2010),
and Napoles et al (2011)). For paraphrastic com-
pressions, manual evaluation alone reliably deter-
mines the compression quality. Because automatic
evaluation metrics compare shortened sentences to
extractive gold standards, they cannot be applied to
paraphrastic compression.
To apply automatic techniques to substitution-
based compression, one would need a gold-standard
set of paraphrastic compressions. These are rare.
Cohn and Lapata (2008) created an abstractive cor-
pus, which contains word reordering and paraphras-
ing in addition to deletion. Unfortunately, this cor-
pus is small (575 sentences) and only includes one
possible compression for each sentence.
Other alternatives include deriving such corpora
from existing corpora of multi-reference transla-
tions. The longest reference translation can be
paired with the shortest reference to represent a
long sentence and corresponding paraphrased gold-
standard compression.
Similar to machine translation or summarization,
automatic translation of paraphrastic compressions
would require multiple references to capture allow-
able variation, since there are often many equally
valid ways of compressing an input. ROUGE
or BLEU could be applied to a set of multiple-
reference compressions, although BLEU is not with-
out its own shortcomings (Callison-Burch et al,
2006). One benefit of both ROUGE and BLEU is
that they are based on n-gram recall and precision
(respectively) instead of word-error rate, so reorder-
ing and word substitutions can be evaluated. Dorr et
al. (2003) used BLEU for evaluation in the context
of headline generation, which uses rewording and
is related to sentence compression. Alternatively,
manual evalation can be adapted from other NLG
domains, such as the techniques described in the fol-
lowing section.
2.2 Manual Evaluation
In order to determine semantic and syntactic suit-
ability, manual evaluation is preferable over au-
tomatic techniques whenever possible. The most
widely practiced manual evaluation methodology
was first used by Knight and Marcu (2002). Judges
grade each compressed sentence against the original
and make two separate decisions: how grammatical
is the compression and how much of the meaning
from the original sentence is preserved. Decisions
are rated along a 5-point scale (LDC, 2005).
Most compression systems consider sentences out
of context (a few exceptions exist, e.g., Daume? III
and Marcu (2002), Martins and Smith (2009), and
Lin (2003)). Contextual cues and discourse struc-
ture may not be a factor to consider if the sentences
are generated for use out of context. An example
of a context-aware approach considered the sum-
maries formed by shortened sentences and evalu-
ated the compression systems based on how well
people could answer questions about the original
document from the summaries (Clarke and Lapata,
2007). This technique has been used before for
evaluating summarization and text comprehension
(Mani et al, 2002; Morris et al, 1992).
2.2.1 Pitfalls of Manual Evaluation
Grammar judgments decrease when the compres-
sion is presented alongside the original sentence.
Figure 1 shows that the mean grammar rating for the
same compressions is on average about 0.3 points
higher when the compression is judged in isolation.
Researchers should be careful to state when gram-
mar is judged on compressions lacking reference
sentences.
Another factor is the group of judges. Obvi-
ously different studies will rely on different judges,
so whenever possible the sentences from an exist-
ing model should be re-evaluated alongside the new
model. The ?McD? entries in Table 2 represent a set
of sentences generated from the exact same model
evaluated by two different sets of judges. The mean
grammar and meaning ratings in each evaluation
setup differ by 0.5?0.7 points.
3 Compression Rate Predicts Performance
The dominant assumption in compression research
is that the system makes the determination about the
optimal compression length. For this reason, com-
pression rates can vary drastically across systems. In
order to get unbiased evaluations, systems should be
compared only when they are compressing at similar
rates.
Compression rate is defined as:
# of tokens in compressed sentence
# of tokens in original sentence
? 100 (1)
93
CR
Mean
ing
1
2
3
4
5
l l
l l
l l
l l
0 20 40 60 80 100
Modell DeletionGold
CR
Gram
mar
1
2
3
4
5
l l
l l
l
l l
l
0 20 40 60 80 100
Modell DeletionGold.1Gold.2
Figure 1: Compression rate strongly correlates with human judgments of meaning and grammaticality. Gold represents
gold-standard compression and Deletion the results of a leading deletion model. Gold.1 grammar judgments were
made alongside the original sentence and Gold.2 were made in isolation.
It seems intuitive that sentence quality diminishes
in relation to the compression rate. Each word
deleted increases the probability that errors are intro-
duced. To verify this notion, we generated compres-
sions at decreasing compression rates of 250 sen-
tences randomly chosen from the written corpus of
Clarke and Lapata (2008), generated by our imple-
mentation of a leading extractive compression sys-
tem (Clarke and Lapata, 2008). We collected hu-
man judgments using the 5-point scales of meaning
and grammar described above. Both quality judg-
ments decreased linearly with the compression rate
(see ?Deletion? in Figure 1).
As this behavior could have been an artifact of
the particular model employed, we next developed
a unique gold-standard corpus for 50 sentences se-
lected at random from the same corpus described
above. The authors manually compressed each sen-
tence at compression rates ranging from less than
10 to 100. Using the same setup as before, we
collected human judgments of these gold standards
to determine an upper bound of perceived quality
at a wide range of compression rates. Figure 1
demonstrates that meaning and grammar ratings de-
cay more drastically at compression rates below 40
(see ?Gold?). Analysis suggests that humans are of-
ten able to practice ?creative deletion? to tighten a
sentence up to a certain point, before hitting a com-
pression barrier, shortening beyond which leads to
significant meaning and grammatically loss.
4 Mismatched Comparisons
We have observed that a difference in compression
rates as small as 5 percentage points can influence
the quality ratings by as much as 0.1 points and
conclude: systems must be compared using simi-
lar levels of compression. In particular, if system
A?s output is higher quality, but longer than system
B?s, then it is not necessarily the case that A is better
than B. Conversely, if B has results at least as good
as system A, one can claim that B is better, since B?s
output is shorter.
Here are some examples in the literature of mis-
matched comparisons:
? Nomoto (2009) concluded their system signif-
icantly outperformed that of Cohn and Lapata
(2008). However, the compression rate of their
system ranged from 45 to 74, while the com-
pression rate of Cohn and Lapata (2008) was
35. This claim is unverifiable without further
comparison.
? Clarke and Lapata (2007), when comparing
against McDonald (2006), reported signifi-
cantly better results at a 5-point higher com-
pression rate. At first glance, this does not
seem like a remarkable difference. However,
94
Model Meaning Grammar CompR
C&L 3.83 3.66 64.1
McD 3.94 3.87 64.2
C&L 3.76? 3.53? 78.4?
McD 3.50? 3.17? 68.5?
Table 2: Mean quality ratings of two competing mod-
els once the compression rates have been standardized,
and as reported in the original work (denoted ?). There
is no significant improvement, but the numerically better
model changes.
the study evaluated the quality of summaries
containing automatically shortened sentences.
The average document length in the test set was
20 sentences, and with approximately 24 words
per sentence, a typical 65.4% compressed doc-
ument would have 80 more words than a typical
60.1% McDonald compression. The aggregate
loss from 80 words can be considerable, which
suggests that this comparison is inconclusive.
We re-evaluated the model described in Clarke
and Lapata (2008) (henceforth C&L) against the
McDonald (2006) model with global constraints, but
fixed the compression rates to be equal. We ran-
domly selected 100 sentences from that same cor-
pus and generated compressions with the same com-
pression rate as the sentences generated by the Mc-
Donald model (McD), using our implementation of
C&L. Although not statistically significant, this new
evaluation reversed the polarity of the results re-
ported by Clarke and Lapata (Table 2). This again
stresses the importance of using similar compression
rates to draw accurate conclusions about different
models.
An example of unbiased evaluation is found in
Cohn and Lapata (2009). In this work, their model
achieved results significantly better than a compet-
ing system (McDonald, 2006). Recognizing that
their compression rate was about 15 percentage
points higher than the competing system, they fixed
the target compression rate to one similar to McDon-
ald?s output, and still found significantly better per-
formance using automatic measures. This work is
one of the few that controls their output length in
order to make an objective comparison (another ex-
ample is found in McDonald (2006)), and this type
of analysis should be emulated in the future.
5 Suggestions
Models should be tested on the same corpus, be-
cause different corpora will likely have different fea-
tures that make them easier or harder to compress. In
order to make non-vacuous comparisons of different
models, a system also needs to be constrained to pro-
duce the same length output as another system, or
report results at least as good for shorter compres-
sions. Using the multi-reference gold-standard col-
lection described in Section 3, relative performance
could be estimated through comparison to the gold-
standard curve. The reference set we have annotated
is yet small, but this is an area for future work based
on feedback from the community.2
Other methods for limiting quality disparities in-
troduced by the compression rate include fixing the
target length to that of the gold standard (e.g., Unno
et al (2006)). Alternately, results for a system at
varying compression levels can be reported,3 allow-
ing for comparisons at similar lengths. This is a
practice to be emulated, if possible, because systems
that cannot control output length can make compar-
isons against the appropriate compression rate.
In conclusion, we have provided justification for
the following practices in evaluating compressions:
? Compare systems at similar compression rates.
? Provide results across multiple compression
rates when possible.
? Report that system A surpasses B iff: A and
B have the same compression rate and A does
better than B, or A produces shorter output than
B and A does at least as well B.
? New corpora for compression should have mul-
tiple gold standards for each sentence.
Acknowledgments
We are very grateful to James Clarke for helping us
obtain the results of existing systems and to the re-
viewers for their helpful comments and recommen-
dations. The first author was supported by the JHU
Human Language Technology Center of Excellence.
This research was funded in part by the NSF under
grant IIS-0713448. The views and findings are the
authors? alone.
2This data is available on request.
3For example, Nomoto (2008) reported results ranging over
compression rates: 0.50?0.70.
95
References
Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
2000. Evaluation metrics for generation. In Proceed-
ings of the first international conference on Natural
language generation-Volume 14, pages 1?8. Associa-
tion for Computational Linguistics.
A. Belz and A. Kilgarriff. 2006. Shared-task eval-
uations in HLT: Lessons for NLG. In Proceedings
of the Fourth International Natural Language Gen-
eration Conference, pages 133?135. Association for
Computational Linguistics.
Ted Briscoe. 2006. An introduction to tag sequence
grammars and the RASP system parser. Computer
Laboratory Technical Report, 662.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL,
Trento, Italy.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 377?384. Association for Computational Lin-
guistics.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1?11.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 449?456. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
Workshop on Text summarization Workshop.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen R. McKeown. 2007. Lex-
icalized Markov grammars for sentence compression.
the Proceedings of NAACL/HLT.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the sixth con-
ference on Applied natural language processing, pages
310?315. Association for Computational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? Step one: Sentence compression. In
Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression: a pilot study. In
Proceedings of the sixth international workshop on In-
formation retrieval with Asian languages-Volume 11,
pages 1?8. Association for Computational Linguistics.
Inderjeet Mani, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
2002. SUMMAC: a text summarization evaluation.
Natural Language Engineering, 8(01):43?68.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the European
Workshop on Natural Language Generation, pages 8?
10.
Andre? F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In In Proceed-
ings of EACL.
Andrew H. Morris, George M. Kasper, and Dennis A.
Adams. 1992. The effects and limitations of auto-
mated text condensing on reading comprehension per-
formance. INFORMATION SYSTEMS RESEARCH,
3(1):17?35.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphrastic
sentence compression with a character-based metric:
Tightening without deletion. In Proceedings of ACL,
Workshop on Monolingual Text-To-Text Generation.
96
Tadashi Nomoto. 2008. A generic sentence trimmer with
CRFs. Proceedings of ACL-08: HLT, pages 299?307.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
E. Reiter and A. Belz. 2006. GENEVAL: A proposal
for shared-task evaluation in NLG. In Proceedings
of the Fourth International Natural Language Gen-
eration Conference, pages 136?138. Association for
Computational Linguistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 118?125. Association for Computational Lin-
guistics.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun?ichi Tsujii. 2006. Trimming CFG parse trees
for sentence compression using machine learning ap-
proaches. In Proceedings of the COLING/ACL on
Main conference poster sessions, pages 850?857. As-
sociation for Computational Linguistics.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of EMNLP.
97
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 33?42,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Reranking Bilingually Extracted Paraphrases
Using Monolingual Distributional Similarity
Tsz Ping Chan, Chris Callison-Burch and Benjamin Van Durme
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
Abstract
This paper improves an existing bilingual
paraphrase extraction technique using mono-
lingual distributional similarity to rerank can-
didate paraphrases. Raw monolingual data
provides a complementary and orthogonal
source of information that lessens the com-
monly observed errors in bilingual pivot-
based methods. Our experiments reveal that
monolingual scoring of bilingually extracted
paraphrases has a significantly stronger cor-
relation with human judgment for grammat-
icality than the probabilities assigned by the
bilingual pivoting method does. The results
also show that monolingual distribution simi-
larity can serve as a threshold for high preci-
sion paraphrase selection.
1 Introduction
Paraphrasing is the rewording of a phrase such
that meaning is preserved. Data-driven paraphrase
acquisition techniques can be categorized by the
type of data that they use (Madnani and Dorr,
2010). Monolingual paraphrasing techniques clus-
ter phrases through statistical characteristics such
as dependency path similarities or distributional co-
occurrence information (Lin and Pantel, 2001; Pasca
and Dienes, 2005). Bilingual paraphrasing tech-
niques use parallel corpora to extract potential para-
phrases by grouping English phrases that share the
same foreign translations (Bannard and Callison-
Burch, 2005). Other efforts blur the lines between
the two, applying techniques from statistical ma-
chine translation to monolingual data or extract-
ing paraphrases from multiple English translations
of the same foreign text (Barzilay and McKeown,
2001; Pang et al, 2003; Quirk et al, 2004).
We exploit both methodologies, applying a
monolingually-derived similarity metric to the out-
put of a pivot-based bilingual paraphrase model. In
this paper we investigate the strengths and weak-
nesses of scoring paraphrases using monolingual
distributional similarity versus the bilingually calcu-
lated paraphrase probability. We show that monolin-
gual cosine similarity calculated on large volumes
of text ranks bilingually-extracted paraphrases bet-
ter than the paraphrase probability originally defined
by Bannard and Callison-Burch (2005). While our
current implementation shows improvement mainly
in grammaticality, other contextual features are ex-
pected to enhance the meaning preservation of para-
phrases. We also show that monolingual scores can
provide a reasonable threshold for picking out high
precision paraphrases.
2 Related Work
2.1 Paraphrase Extraction from Bitexts
Bannard and Callison-Burch (2005) proposed iden-
tifying paraphrases by pivoting through phrases in a
bilingual parallel corpora. Figure 1 illustrates their
paraphrase extraction process. The target phrase,
e.g. thrown into jail, is found in a German-English
parallel corpus. The corresponding foreign phrase
(festgenommen) is identified using word alignment
and phrase extraction techniques from phrase-based
statistical machine translation (Koehn et al, 2003).
Other occurrences of the foreign phrase in the par-
allel corpus may align to a distinct English phrase,
such as jailed. As the original phrase occurs sev-
eral times and aligns with many different foreign
phrases, each of these may align to a variety of other
English paraphrases. Thus, thrown into jail not only
paraphrases as jailed, but also as arrested, detained,
imprisoned, incarcerated, locked up, and so on. Bad
paraphrases, such as maltreated, thrown, cases, cus-
tody, arrest, and protection, may also arise due to
poor word alignment quality and other factors.
33
... letzteWoche wurden in Irland f?nf Landwirte festgenommen , weil sie verhindern wollten
... last week five farmers were thrown into jail in Ireland because they resisted ...
...
Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und get?tet .
Quite a few journalists have disappeared or have been imprisoned , tortured and killed .
Figure 1: Using a bilingual parallel corpus to extract
paraphrases.
Bannard and Callison-Burch (2005) defined a
paraphrase probability to rank these paraphrase can-
didates, as follows:
e?2 = arg max
e2 6=e1
p(e2|e1) (1)
p(e2|e1) =
?
f
p(e2, f |e1) (2)
=
?
f
p(e2|f, e1)p(f |e1) (3)
?
?
f
p(e2|f)p(f |e1) (4)
where p(e2|e1) is the paraphrase probability, and
p(e|f) and p(f |e) are translation probabilities from
a statistical translation model.
Anecdotally, this paraphrase probability some-
times seems unable to discriminate between good
and bad paraphrases, so some researchers disregard
it and treat the extracted paraphrases as an unsorted
set (Snover et al, 2010). Callison-Burch (2008)
attempts to improve the ranking by limiting para-
phrases to be the same syntactic type.
We attempt to rerank the paraphrases using other
information. This is similar to the efforts of Zhao
et al (2008), who made use of multiple resources to
derive feature functions and extract paraphrase ta-
bles. The paraphrase that maximizes a log-linear
combination of various feature functions is then se-
lected as the optimal paraphrase. Feature weights
in the model are optimized by minimizing a phrase
substitution error rate, a measure proposed by the
authors, on a development set.
2.2 Monolingual Distributional Similarity
Prior work has explored the acquisition of para-
phrases using distributional similarity computed
from monolingual resources, such as in the DIRT
results of Lin and Pantel (2001). In these models,
phrases are judged to be similar based on the cosine
distance of their associated context vectors. In some
cases, such as by Lin and Pantel, or the seminal work
of Church and Hanks (1991), distributional context
is defined using frequencies of words appearing in
various syntactic relations with other lexical items.
For example, the nouns apple and orange are con-
textually similar partly because they both often ap-
pear as the object of the verb eat. While syntac-
tic contexts provide strong evidence of distributional
preferences, it is computationally expensive to parse
very large corpora, so it is also common to represent
context vectors with simpler representations like ad-
jacent words and n-grams (Lapata and Keller, 2005;
Bhagat and Ravichandran, 2008; Lin et al, 2010;
Van Durme and Lall, 2010). In these models, ap-
ple and orange might be judged similar because both
tend to be one word to the right of some, and one to
the left of juice.
Here we calculate distributional similarity using a
web-scale n-gram corpus (Brants and Franz, 2006;
Lin et al, 2010). Given both the size of the collec-
tion, and that the n-grams are sub-sentential (the n-
grams are no longer than 5 tokens by design), it was
not feasible to parse, which led to the use of n-gram
contexts. Here we use adjacent unigrams. For each
phrase x we wished to paraphrase, we extracted the
context vector of x from the n-gram collection as
such: every (n-gram, frequency) pair of the form:
(ax, f ), or (xb, f ), gave rise to the (feature, value)
pair: (wi?1=a, f ), or (wi+1=b, f ), respectively. In
order to scale to this size of a collection, we relied
on Locality Sensitive Hashing (LSH), as was done
previously by Ravichandran et al (2005) and Bha-
gat and Ravichandran (2008). To avoid computing
feature vectors explicitly, which can be a memory
intensive bottleneck, we employed the online LSH
variant described by Van Durme and Lall (2010).
This variant, based on the earlier work of Indyk
and Motwani (1998) and Charikar (2002), approxi-
mates the cosine similarity between two feature vec-
tors based on the Hamming distance in a reduced bit-
wise representation. In brief, for the feature vectors
~u, ~v, each of dimension d, then the cosine similarity
is defined as: ~u?~v|~u||~v| . If we project ~u and ~v through
a d by b random matrix populated with draws from
34
huge amount of
BiP SyntBiP BiP-MonoDS
large number of, .33 large number of, .38 huge amount of, 1.0
in large numbers, .11 great number of, .09 large quantity of, .98
great number of, .08 huge amount of, .06 large number of, .98
large numbers of, .06 vast number of, .06 great number of, .97
vast number of, .06 vast number of, .94
huge amount of, .06 in large numbers, .10
large quantity of, .03 large numbers of, .08
Table 1: Paraphrases for huge amount of according to the
bilingual pivoting (BiP), syntactic-constrainted bilingual
pivoting (SyntBiP) translation score and the monolingual
similarity score via LSH (MonoDS), ranked by corre-
sponding scores listed next to each paraphrase. Syntactic
type of the phrase is [JJ+NN+IN].
N(0, 1), then we convert our feature vectors to bit
signatures of length b, by setting each bit of the sig-
nature conditioned on whether or not the respective
projected value is greater than or equal to 0. Given
the bit signatures h(~u) and h(~v), we approximate
cosine with the formula: cos(D(h(~u),h(~v))b pi), where
D() is Hamming distance.
3 Ranking Paraphrases
We use several different methods to rank candidate
sets of paraphrases that are extracted from bilingual
parallel corpora. Our three scoring methods are:
? MonoDS ? monolingual distributional similar-
ity calculated over the Google n-gram corpus
via LSH, as described in Section 2.2.
? BiP ? bilingual pivoting is calculated as in
Equation 4 following Bannard and Callison-
Burch (2005). The translation model probabili-
ties are estimated from a French-English paral-
lel corpus.
? SyntBiP ? syntactically-constrained bilingual
pivoting. This refinement to BiP, proposed in
Callison-Burch (2008), constrains paraphrases
to be the same syntactic type as the original
phrase in the pivoting step of the paraphrase ta-
ble construction.
When we use MonoDS to re-score a candidate set,
we indicate which bilingual paraphrase extraction
method was used to extract the candidates as prefix,
as in BiP-MonoDS or SyntBiP-MonoDS.
reluctant
MonoDShand?selected BiP
*willing, .99 not, .56
loath, .98 unwilling, .04
*eager, .98 reluctance, .03
somewhat reluctant, .98 reticent, .03
unable, .98 hesitant, .02
denied access, .98 reticent about, .01
disinclined, .98 reservations, .01
very unwilling, .97 reticence, .01
conducive, .97 hesitate, .01
linked, .97 are reluctant, .01
Table 2: Ordered reranked paraphrase candidates for the
phrase reluctant according to monolingual distributional
similarity (MonoDShand?selected) and bilingual pivoting
paraphrase (BiP) method. Two hand-selected phrases are
labeled with asterisks.
3.1 Example Paraphrase Scores
Table 1 shows the paraphrase candidates for the
phrase huge amount of along with the values for each
of our three scoring methods. Although MonoDS
does not explicitly impose syntactic restrictions, the
syntactic structure of the paraphrase in large num-
bers contributes to the large difference in the left
and right context of the paraphrase and of the orig-
inal phrase. Hence, the paraphrase was assigned a
low score of 0.098 as compared to other paraphrase
candidates with the correct syntactic type. Note that
the SyntBiP produced significantly fewer paraphrase
candidates, since its paraphrase candidates must be
the same syntactic type as the original phrase. Iden-
tity paraphrases are excluded for the rest of the dis-
cussion in this paper.
3.2 Susceptibility to Antonyms
Monolingual distributional similarity is widely
known to conflate words with opposite meaning and
has motivated a large body of prior work on antonym
detection (Lin and Zhao, 2003; Lin and Pantel,
2001; Mohammad et al, 2008a; Mohammad et al,
2008b; Marneffe et al, 2008; Voorhees, 2008). In
contrast, the antonyms of a phrase are rarely pro-
duced during pivoting of the BiP methods because
they tend not to share the same foreign translations.
Since the reranking framework proposed here be-
gins with paraphrases acquired by the BiP methodol-
35
ogy, MonoDS can considerably enhance the quality
of ranking while sidestepping the antonym problem
that arises from using MonoDS alone.
To support this intuition, an example of a para-
phrase list with inserted hand-selected phrases
ranked by each reranking methods is shown in Ta-
ble 21. Hand-selected antonyms of reluctant are in-
serted into the paraphrase candidates extracted by
BiP before they are reranked by MonoDS. This is
analogous to the case without pre-filtering of para-
phrases by BiP and all phrases are treated equally
by MonoDS alone. BiP cannot rank these hand-
selected paraphrases since, by construction, they do
not share any foreign translation and hence their
paraphrase scores are not defined. As expected from
the drawbacks of monolingual-based statistics, will-
ing and eager are assigned top scores by MonoDS,
although good paraphrases such as somewhat reluc-
tant and disinclined are also ranked highly. This
illustrates how BiP complements the monolingual
reranking technique by providing orthogonal infor-
mation to address the issue of antonyms for Mon-
oDS.
3.3 Implementation Details
For BiP and SyntBiP, the French-English parallel
text from the Europarl corpus (Koehn, 2005) was
used to train the paraphrase model. The parallel
corpus was extracted from proceedings of the Eu-
ropean parliament with a total of about 1.3 million
sentences and close to 97 million words in the En-
glish text. Word alignments were generated with
the Berkeley aligner. For SyntBiP, the English side
of the parallel corpus was parsed using the Stan-
ford parser (Klein and Manning, 2003). The transla-
tion models were trained with Thrax, a grammar ex-
tractor for machine translation (Weese et al, 2011).
Thrax extracts phrase pairs that are labeled with
complex syntactic labels following Zollmann and
Venugopal (2006).
For MonoDS, the web-scale n-gram collection of
Lin et al (2010) was used to compute the mono-
lingual distributional similarity features, using 512
bits per signature in the resultant LSH projection.
Following Van Durme and Lall (2010), we implic-
1Generating a paraphrase list by MonoDS alone requires
building features for all phrases in the corpus, which is com-
putationally impractical and hence, was not considered here.
itly represented the projection matrix with a pool of
size 10,000. In order to expand the coverage of the
candidates scored by the monolingual method, the
LSH signatures are obtained only for the phrases in
the union set of the phrase-level outputs from the
original and from the syntactically constrained para-
phrase models. Since the n-gram corpus consists
of at most 5-gram and each distributional similar-
ity feature requires a single neighboring token, the
LSH signatures are generated only for phrases that
are 4-gram or less. Phrases that didn?t appear in the
n-grams with at least one feature were discarded.
4 Human Evaluation
The different paraphrase scoring methods were com-
pared through a manual evaluation conducted on
Amazon Mechanical Turk. A set of 100 test phrases
were selected and for each test phrase, five distinct
sentences were randomly sampled to capture the fact
that paraphrases are valid in some contexts but not
others (Szpektor et al, 2007). Judges evaluated the
paraphrase quality through a substitution test: For
each sampled sentence, the test phrase is substituted
with automatically-generated paraphrases. The sen-
tences and the phrases are drawn from the English
side of the Europarl corpus. Judges indicated the
amount of the original meaning preserved by the
paraphrases and the grammaticality of the resulting
sentences. They assigned two values to each sen-
tence using the 5-point scales defined in Callison-
Burch (2008).
The 100 test phrases consisted of 25 unigrams,
25 bigrams, 25 trigrams and 25 4-grams. These 25
phrases were randomly sampled from the paraphrase
table generated by the bilingual pivoting method,
with the following restrictions:
? The phrase must have occurred at least 5 times
in the parallel corpus and must have appeared
in the web-scale n-grams.
? The size of the union of paraphrase candidates
from BiP and SyntBiP must be 10 or more.
4.1 Calculating Correlation
In addition to their average scores on the 5-point
scales, the different paraphrase ranking methods
were quantitatively evaluated by calculating their
correlation with human judgments. Their correla-
tion is calculated using Kendall?s tau coefficient, a
36
Reranking Method Meaning Grammar
BiP 0.14 0.04
BiP-MonoDS 0.14 0.24?
SyntBiP 0.19 0.08
SyntBip-MonoDS 0.15 0.22?
SyntBiPmatched 0.20 0.15
SyntBiPmatched-MonoDS 0.17 0.16
SyntBiP* 0.21 0.09
SyntBiP-MonoDS* 0.16 0.22?
Table 3: Kendall?s Tau rank correlation coefficients be-
tween human judgment of meaning and grammaticality
for the different paraphrase scoring methods. Bottom
panel: SyntBiPmatched is the same as SyntBiP except
paraphrases must match with the original phrase in syn-
tactic type. SyntBiP* and MonoDS* are the same as
before except they share the same phrase support with
SyntBiPmatched. (?: MonoDS outperforms the corre-
sponding BiP reranking at p-value?0.01, and ? at?0.05)
common measure of correlation between two ranked
lists. Kendall?s tau coefficient ranges between -1 and
1, where 1 indicates a perfect agreement between a
pair of ranked lists.
Since tied rankings occur in the human judgments
and reranking methods, Kendall?s tau b, which ig-
nores pairs with ties, is used in our analysis. An
overall Kendall?s tau coefficient presented in the re-
sults section is calculated by averaging all Kendall?s
tau coefficients of a particular reranking method
over all phrase-sentence combinations.
5 Experimental Results
5.1 Correlation
The Kendall?s tau coefficients for the three para-
phrase ranking methods are reported Table 3. A
total of 100 phrases and 5 sentence per phrase are
selected for the experiment, resulting in a max-
imum support size of 500 for Kendall?s tau co-
efficient calculation. The overall sizes of sup-
port are 500, 335, and 304 for BiP, SyntBiP and
SyntBiPmatched, respectively. The positive values of
Kendall?s tau confirm both monolingual and bilin-
gual approaches for paraphrase reranking are posi-
tively correlated with human judgments overall. For
grammaticality, monolingual distributional simi-
larity reranking correlates stronger with human
judgments than bilingual pivoting methods. For
example, in the top panel, given a paraphrase ta-
ble generated through bilingual pivoting, Kendall?s
tau for monolingual distributional similarity (BiP-
MonoDS) achieves 0.24 while that of the bilin-
gual pivoting ranking (BiP) is only 0.04. Simi-
larly, reranking of the paraphrases extracted with
syntactically-constrained bilingual pivoting shows a
stronger correlation between SyntBiP-MonoDS and
grammar judgments (0.22) than the SyntBiP (0.08).
This result further supports the intuition of distri-
butional similarity being suitable for paraphrase
reranking in terms of grammaticality.
In terms of meaning preservation, the Kendall?s
tau coefficient for MonoDS is often lower than the
bilingual approaches, suggesting that paraphrase
probability from the bilingual approach correlates
better with phrasal meaning than the monolingual
metric. For instance, SyntBiP reaches a Kendall?s
tau of 0.19, which is a slightly stronger correlation
than that of SyntBiP-MonoDS. Although paraphrase
candidates were generated by bilingual pivoting,
distributional similarity depends only on contextual
similarity and does not guarantee paraphrases that
match with the original meaning; whereas Bilingual
pivoting methods are derived based on shared for-
eign translations which associate meaning.
In the bottom panel of Table 3, only paraphrases
of the same syntactic type as the source phrase are
included in the ranked list for Kendall?s tau calcula-
tion. The phrases associated with these paraphrases
are used for calculating Kendall?s tau for the orig-
inal reranking methods (labeled as SyntBiP* and
SyntBiP-MonoDS*). Comparing only the bilingual
methods across panels, syntactic matching increases
the correlation of bilingual pivoting metrics with
human judgments in grammaticality (e.g., 0.15 for
SyntBiPmatched and 0.08 for SyntBiP) but with only
minimal effects on meaning. The maximum values
in the bottom panel for both categories are roughly
the same as that in the corresponding category in
the upper panel ({0.21,0.19} in meaning and {0.22,
0.24} in grammar for lower and upper panels, re-
spectively.) This suggests that syntactic type match-
ing offers similar improvement in grammaticality
as MonoDS, although syntactically-constrained ap-
proaches have more confined paraphrase coverage.
We performed a one-tailed sign test on the
Kendall?s Tau values across phrases to examine
37
Figure 2: Averaged scores in the top K paraphrase can-
didates as a function of K for different reranking metrics.
All methods performs similarly in meaning preservation,
but SyntBiP-MonoDS outperforms other scoring meth-
ods in grammaticality, as shown in the bottom graph.
the statistical significance of the performance gain
due to MonoDS. For grammaticality, except for the
case of syntactic type matching (SyntBiPmatched), p-
values are less than 0.05, confirming the hypothesis
that MonoDS outperforms BiP. The p-value for com-
paring MonoDS and SyntBiPmatched exceeds 0.05,
agreeing with our conclusion from Table 3 that the
two methods perform similarly.
5.2 Thresholding Using MonoDS Scores
One possible use for the paraphrase scores would be
as a cutoff threshold where any paraphrases exceed-
ing that value would be selected. Ideally, this would
retain only high precision paraphrases.
To verify whether scores from each method corre-
spond to human judgments for paraphrases extracted
by BiP, human evaluation scores are averaged for
meaning and grammar within each range of para-
phrase score for BiP and approximate cosine dis-
tance for MonoDS, as shown in Table 4. The BiP
paraphrase score bin sizes are linear in log scale.
BiP Paraphrase Score MonoDS LSH Score
Region M G Region M G
1.00 ? x > 0.37 3.6 3.7 1 ? x > 0.95 4.0 4.4
0.37 ? x > 0.14 3.6 3.7 0.95 ? x > 0.9 3.2 4.0
0.14 ? x > 0.05 3.4 3.6 0.9 ? x > 0.85 3.3 4.0
0.05 ? x > 1.8e-2 3.4 3.6 0.85 ? x > 0.8 3.3 4.0
1.8e-2 ? x > 6.7e-3 3.4 3.6 0.8 ? x > 0.7 3.2 3.9
6.7e-3 ? x > 2.5e-3 3.2 3.7 0.7 ? x > 0.6 3.3 3.8
2.5e-3 ? x > 9.1e-4 3.0 3.6 0.6 ? x > 0.5 3.1 3.7
9.1e-4 ? x > 3.4e-4 3.0 3.8 0.5 ? x > 0.4 3.1 3.6
3.4e-4 ? x > 1.2e-4 2.6 3.6 0.4 ? x > 0.3 3.1 3.5
1.2e-4 ? x > 4.5e-5 2.7 3.6 0.3 ? x > 0.2 2.9 3.4
x ? 4.5e-5 2.5 3.7 0.2 ? x > 0.1 3.0 3.3
0.1 ? x > 0 2.9 3.2
Table 4: Averaged human judgment scores as a func-
tion of binned paraphrase scores and binned LSH scores.
MonoDS serves as much better thresholding score for ex-
tracting high precision paraphrases.
MonoDS LSH BiP Paraphrase Threshold
Threshold ? 0.05 ? 0.01 ? 6.7e-3
? 0.9 4.2 / 4.4 4.1 / 4.4 4.0 / 4.4
? 0.8 4.0 / 4.3 3.9 / 4.3 3.9 / 4.2
? 0.7 3.9 / 4.1 3.8 / 4.2 3.8 / 4.1
Table 5: Thresholding using both the MonoDS and BiP
scores further improves the average human judgment of
Meaning / Grammar.
Observe that for the BiP paraphrase scores on the
left panel, no trend on the averaged grammar scores
across all score bins is present. While a mild cor-
relation exists between the averaged meaning scores
and the paraphrase scores, the top score region (1> x
? 0) corresponds to merely an averaged value of 3.6
on a 5-point scale. Therefore, thresholding on BiP
scores among a set of candidates would not guaran-
tee accurate paraphrases in grammar or meaning.
On the right panel, MonoDS LSH scores on para-
phrase candidates produced by BiP are uniformly
higher in grammar than meaning across all score
bins, similar to the correlation results in Table 3.
The averaged grammar scores decreases monoton-
ically and proportionally to the change in LSH val-
ues. With regard to meaning scores, the averaged
values roughly correspond to the decrease of LSH
values, implying distributional similarity correlates
weakly with human judgment in the meaning preser-
38
vation of paraphrase. Note that the drop in averaged
scores is the largest from the top bin (1? x > 0.95)
to the second bin (0.95 ? x > 0.9) is the largest
within both meaning and grammar. This suggests
that thresholding on top tiered MonoDS scores
can be a good filter for extracting high precision
paraphrases. BiP scores, by comparison, are not as
useful for thresholding grammaticality.
Additional performance gain attained by combin-
ing the two thresholding are illustrated in Table 5,
where averaged meaning and grammar scores are
listed for each combination of thresholding. At a
threshold of 0.9 for MonoDS LSH score and 0.05
for BiP paraphrase score, the averaged meaning ex-
ceeds the highest value reported in Table 4, whereas
the grammar scores reaches the value in the top bin
in Table 4. General trends of improvement from uti-
lizing the two reranking methods are observed by
comparing Tables 4 and 5.
5.3 Top K Analysis
Figure 2 shows the mean human assigned score
within the top K candidates averaged across all
phrases. Compared across the two categories,
meaning scores have lower range of score and
a more uniform trend of decreasing values as K
grows. In grammaticality, BiP clearly underper-
forms whereas the SyntBiP-MonoDS maintains the
best score among all methods over all values of K.
In addition, a slow drop-off up until K = 4 in the
curve for SyntBiP-MonoDS implies that the quality
of paraphrases remains relatively high going from
top 1 to top 4 candidates.
In applications such as question answering or
search, the order of answers presented is important
because the lower an answer is ranked, the less likely
it would be looked at by a user. Based on this intu-
ition, the paraphrase ranking methods are evaluated
using the maximum human judgment score among
the top K candidates obtained by each method. As
shown in Table 6, when only the top candidate
is considered, the averaged score corresponding to
the monolingual reranking methods are roughly the
same as that to the bilingual methods in meaning, but
as K grows, the bilingual methods outperforms the
monolingual methods. In terms of grammaticality,
scores associated with monolingual reranking meth-
ods are consistently higher than the bilingual meth-
Reranking Method
K BiP BiP-MonoDS SyntBiP SyntBiP-MonoDS
M
1 3.62 3.67 3.58 3.58
3 4.13 4.07 4.13 4.01
5 4.26 4.19 4.20 4.09
10 4.39 4.30 4.25 4.23
G
1 3.83 4.11 4.04 4.23
3 4.22 4.45 4.47 4.54
5 4.38 4.54 4.55 4.62
10 4.52 4.62 4.63 4.67
Table 6: Average of the maximum human evaluation
score from top K candidates for each reranking method.
Support sizes for BiP- and SyntBiP-based metrics are 500
and 335, respectively. (M = Meaning, G = Grammar)
ods but the difference tapers off as K increases. This
suggests that when only limited top paraphrase can-
didates can be evaluated, MonoDS is likely to pro-
vide better quality of results.
6 Detailed Examples
6.1 MonoDS Filters Bad BiP Paraphrases
The examples in the top panel of Table 7 illustrates a
few disadvantages of the bilingual paraphrase scores
and how monolingual reranking complements the
bilingual methods. Translation models based on
bilingual corpora are known to suffer from misalign-
ment of the parallel text (Bannard and Callison-
Burch, 2005), producing incorrect translations that
propagate through in the paraphrase model. This is-
sue is exemplified in the phrase pairs {considerable
changes, caused quite}, {always declared, always
been}, and {significantly affected, known} listed Ta-
ble 7. The paraphrases are clearly unrelated to the
corresponding phrases as evident from the low rank-
ings from human judges. Nonetheless, they were in-
cluded as candidates likely due to misalignment and
were ranked relatively high by BiP metric. For ex-
ample, considerable changes was aligned to modifie
conside?rablement correctly. However, due to a com-
bination of loose translations and difficulty in align-
ing multiple words that are spread out in a sentence,
the French phrase was inaccurately matched with
caused quite by the aligner, inducing a bad para-
phrase. Note that in these cases LSH produces the
results that agrees with the human rankings.
39
Ranking
Phrase Paraphrase Sizepool Meaning Grammar BiP BiP-MonoDS
significantly affected known 20 19 18.5 1 17
considerable changes caused quite 23 23 23 2.5 23
always declared always been 20 20 20 2 13
hauled delivered 23 7 5.5 21.5 5.0
fiscal burden? taxes 18 13.5 18 6 16
fiscal burden? taxes 18 2 8 6 16
legalise legalize 23 1 1 10 1
to deal properly with address 35 4.5 5.5 4 29.5
you have just stated you have just suggested 31 13.5 8.5 4 30
Table 7: Examples of phrase pair rankings by different reranking methods and human judgments in terms of meaning
and grammar. Higher rank (smaller numbers) corresponds to more favorable paraphrases by the associated metric.
(?: Phrases are listed twice to show the ranking variation when substitutions are evaluated in different sentences.)
6.2 Context Matters
Occasionally, paraphrases are context-dependent,
meaning the relevance of the paraphrase depends on
the context in a sentence. Bilingual methods can
capture limited context through syntactic constraints
if the POS tags of the paraphrases and the sentence
are available, while the distributional similarity met-
ric, in its current implementation, is purely based on
the pattern of co-occurrence with neighboring con-
text n-grams. As a result, LSH scores should be
slightly better at gauging the paraphrases defined by
context, as suggested by some examples in Table 7.
The phrase pair {hauled, delivered} differ slightly
in how they describe the manner that an object is
moved. However, in the context of the following
sentence, they roughly correspond to the same idea:
countries which do not comply with community
legislation should be hauled before the court of
justice and i think mrs palacio will do so .
As a result, out of 23 candidates, human judges
ranked delivered 7 and 5.5 for meaning and gram-
mar, respectively. The monolingual-based metric
also assigns a higher rank to the paraphrase while
BiP puts it near the lowest rank.
Another example of context-dependency is the
phrase pair {fiscal burden, taxes}, which could have
some foreign translations in common. The original
phrase appears in the following sentence:
... the member states can reduce the fiscal burden
consisting of taxes and social contributions .
The paraphrase candidate taxes is no longer ap-
propriate with the consideration of the context sur-
rounding the original phrase. As such, taxes re-
ceived rankings of 13.5, 18 and 16 out of 18
for meaning, grammar, and MonoDS, respectively,
whereas BiP assigns a 6 to the paraphrase. The same
phrase pair but a different sentence, the context in-
duces opposite effects on the paraphrase judgments,
where the paraphrase received 2 and 8 in the two
categories as shown in Table 7:
the economic data for our eu as regards employ-
ment and economic growth are not particularly
good , and , in addition , the fiscal burden in eu-
rope , which is to be borne by the citizen , has
reached an all-time high of 46 % .
Hence, distributional similarity offers additional
advantages over BiP only when the paraphrase ap-
pears in a context that also defines most of the non-
zero dimensions of the LSH signature vector.
The phrase pair {legalise, legalize} exemplifies
the effect of using different corpora to train 2 para-
phrase reranking models as shown in Table 7. Mean-
ing, grammar and MonoDS all received top rank out
of all paraphrases, whereas BiP ranks the paraphrase
10 out of 23. Since the BiP method was trained
with Europarl data, which is dominated by British
English, BiP fails to acknowledge the American
spelling of the same word. On the other hand, dis-
tributional similarity feature vectors were extracted
from the n-gram corpus with different variations of
English, which was informative for paraphrase rank-
ing. This property can be exploited for adaptation of
specific domain of paraphrases selection.
40
6.3 Limitations of MonoDS Implementation
While the monolingual distributional similarity
shows promise as a paraphrase ranking method,
there are a number of additional drawbacks associ-
ated with the implementation.
The method is currently limited to phrases with
up to 4 contiguous words that are present in the
n-gram corpus for LSH feature vector extraction.
Since cosine similarity is a function of the angle
between 2 vectors irrespective of the vector mag-
nitudes, thresholding on low occurrences of higher
n-grams in the corpus construction causes larger n-
grams to suffer from feature sparsity and be sus-
ceptible to noise. A few examples from the exper-
iment demonstrate such scenario. For a phrase to
deal properly with, a paraphrase candidate address
receives rankings of 4.5, 5.5 and 4 out of 35 for
meaning, grammar and BiP, respectively, it is ranked
29.5 by BiP-MonoDS. The two phrases are expected
to have similar neighboring context in regular En-
glish usage, but it might be misrepresented by the
LSH feature vector due to the lack of occurrences of
the 4-gram in the corpus.
Another example of how sparsity affects LSH fea-
ture vectors is the phrase you have just stated. An
acceptable paraphrase you have just suggested was
ranked 13.5, 8.5 and 6.5 out of a total of 31 can-
didates by meaning, grammar and BiP, respectively,
but MonoDS only ranks it at 30. The cosine sim-
ilarity between the phrases are 0.05, which is very
low. However, the only tokens that differentiate the
4-gram phrases, i.e. {stated,suggested}, have a sim-
ilarity score of 0.91. This suggests that even though
the additional words in the phrase don?t alter the
meaning significantly, the feature vectors are mis-
represented due to the sparsity of the 4-gram. This
highlights a weakness of the current implementa-
tion of distributional similarity, namely that context
within a phrase is not considered for larger n-grams.
7 Conclusions and Future Work
We have presented a novel paraphrase ranking met-
ric that assigns a score to paraphrase candidates ac-
cording to their monolingual distributional similar-
ity to the original phrase. While bilingual pivoting-
based paraphrase models provide wide coverage
of paraphrase candidates and syntactic constraints
on the model confines the structural match, addi-
tional contextual similarity information provided by
monolingual semantic statistics increases the accu-
racy of paraphrase ranking within the target lan-
guage. Through a manual evaluation, it was shown
that monolingual distributional scores strongly cor-
relate with human assessment of paraphrase quality
in terms of grammaticality, yet have minimal effects
on meaning preservation of paraphrases.
While we speculated that MonoDS would im-
prove both meaning and grammar scoring for para-
phrases, we found in the results that only gram-
maticality was improved from the monolingual ap-
proach. This is likely due to the choice of how con-
text is represented, which in this case is only single
neighboring words. A consideration for future work
to enhance paraphrasal meaning preservation would
be to explore other contextual representations, such
as syntactic dependency parsing (Lin, 1997), mu-
tual information between co-occurences of phrases
Church and Hanks (1991), or increasing the number
of neighboring words used in n-gram based repre-
sentations.
In future work we will make use of other com-
plementary bilingual and monolingual knowledge
sources by combining other features such as n-gram
length, language model scores, etc. One approach
would be to perform minimum error rate training
similar to Zhao et al (2008) in which linear weights
of a feature function for a set of paraphrases candi-
date are trained iteratively to minimize the phrasal-
substitution-based error rate. Instead of phrasal sub-
stitution in Zhao?s method, quantitative measure of
correlation with human judgment can be used as
the objective function to be optimized during train-
ing. Other techniques such as SVM-rank (Joachims,
2002) may also be investigated for aggregating re-
sults from multiple ranked lists.
8 Acknowledgements
Thanks to Courtney Napoles for advice regarding
a pilot version of this work. Thanks to Jonathan
Weese, Matt Post and Juri Ganitkevitch for their as-
sistance with Thrax. This research was supported by
the EuroMatrixPlus project funded by the European
Commission (7th Framework Programme), and by
the NSF under grant IIS-0713448. Opinions, inter-
pretations, and conclusions are the authors? alone.
41
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of STOC.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 6(1):22?29.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. Advances in NIPS, 15:3?10.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1).
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7:343?360.
Dekang Lin and Shaojun Zhao. 2003. Identifying syn-
onyms among distributionally similar words. In Pro-
ceedings of IJCAI-03, pages 1492?1493.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Dekang Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Proceed-
ings of ACL.
Nitin Madnani and Bonnie Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Marie-Catherine De Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of ACL 2008.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008a. Computing word-pair antonymy. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 982?991. Association
for Computational Linguistics.
Saif Mohammad, Bonnie J. Dorr, Melissa Egan, Nitin
Madnani, David Zajic, and Jimmy Lin. 2008b. Mul-
tiple alternative sentence compressions and word-pair
antonymy for automatic text summarization and rec-
ognizing textual entailment.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Marius Pasca and Peter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Proceedings of IJCNLP, pages 119?130.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP, pages 142?149.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proceedings of ACL.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. TER-plus: paraphrase,
semantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. EMNLP 2011 - Workshop on sta-
tistical machine translation.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of WMT06.
42
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82?86,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Freebase QA: Information Extraction or Semantic Parsing?
Xuchen Yao
1
Jonathan Berant
3
Benjamin Van Durme
1,2
1
Center for Language and Speech Processing
2
Human Language Technology Center of Excellence
Johns Hopkins University
3
Computer Science Department
Stanford University
Abstract
We contrast two seemingly distinct ap-
proaches to the task of question answering
(QA) using Freebase: one based on infor-
mation extraction techniques, the other on
semantic parsing. Results over the same
test-set were collected from two state-of-
the-art, open-source systems, then ana-
lyzed in consultation with those systems?
creators. We conclude that the differ-
ences between these technologies, both
in task performance, and in how they
get there, is not significant. This sug-
gests that the semantic parsing commu-
nity should target answering more com-
positional open-domain questions that are
beyond the reach of more direct informa-
tion extraction methods.
1 Introduction
Question Answering (QA) from structured data,
such as DBPedia (Auer et al., 2007), Freebase
(Bollacker et al., 2008) and Yago2 (Hoffart et
al., 2011), has drawn significant interest from
both knowledge base (KB) and semantic pars-
ing (SP) researchers. The majority of such work
treats the KB as a database, to which standard
database queries (SPARQL, MySQL, etc.) are is-
sued to retrieve answers. Language understand-
ing is modeled as the task of converting natu-
ral language questions into queries through inter-
mediate logical forms, with the popular two ap-
proaches including: CCG parsing (Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009; Kwiatkowski et
al., 2010; Kwiatkowski et al., 2011; Krishna-
murthy and Mitchell, 2012; Kwiatkowski et al.,
2013; Cai and Yates, 2013a), and dependency-
based compositional semantics (Liang et al., 2011;
Berant et al., 2013; Berant and Liang, 2014).
We characterize semantic parsing as the task
of deriving a representation of meaning from lan-
guage, sufficient for a given task. Traditional
information extraction (IE) from text may be
coarsely characterized as representing a certain
level of semantic parsing, where the goal is to
derive enough meaning in order to populate a
database with factoids of a form matching a given
schema.
1
Given the ease with which reasonably
accurate, deep syntactic structure can be automat-
ically derived over (English) text, it is not surpris-
ing that IE researchers would start including such
?features? in their models.
Our question is then: what is the difference be-
tween an IE system with access to syntax, as com-
pared to a semantic parser, when both are targeting
a factoid-extraction style task? While our conclu-
sions should hold generally for similar KBs, we
will focus on Freebase, such as explored by Kr-
ishnamurthy and Mitchell (2012), and then others
such as Cai and Yates (2013a) and Berant et al.
(2013). We compare two open-source, state-of-
the-art systems on the task of Freebase QA: the
semantic parsing system SEMPRE (Berant et al.,
2013), and the IE system jacana-freebase (Yao
and Van Durme, 2014).
We find that these two systems are on par with
each other, with no significant differences in terms
of accuracy between them. A major distinction be-
tween the work of Berant et al. (2013) and Yao
and Van Durme (2014) is the ability of the for-
mer to represent, and compose, aggregation oper-
ators (such as argmax, or count), as well as in-
tegrate disparate pieces of information. This rep-
resentational capability was important in previous,
closed-domain tasks such as GeoQuery. The move
to Freebase by the SP community was meant to
1
So-called Open Information Extraction (OIE) is simply
a further blurring of the distinction between IE and SP, where
the schema is allowed to grow with the number of verbs, and
other predicative elements of the language.
82
provide richer, open-domain challenges. While
the vocabulary increased, our analysis suggests
that compositionality and complexity decreased.
We therefore conclude that the semantic parsing
community should target more challenging open-
domain datasets, ones that ?standard IE? methods
are less capable of attacking.
2 IE and SP Systems
jacana-freebase
2
(Yao and Van Durme, 2014)
treats QA from a KB as a binary classification
problem. Freebase is a gigantic graph with mil-
lions of nodes (topics) and billions of edges (re-
lations). For each question, jacana-freebase
first selects a ?view? of Freebase concerning only
involved topics and their close neighbors (this
?view? is called a topic graph). For instance,
for the question ?who is the brother of justin
bieber??, the topic graph of Justin Bieber, con-
taining all related nodes to the topic (think of the
?Justin Bieber? page displayed by the browser), is
selected and retrieved by the Freebase Topic API.
Usually such a topic graph contains hundreds to
thousands of nodes in close relation to the central
topic. Then each of the node is judged as answer
or not by a logistic regression learner.
Features for the logistic regression learner are
first extracted from both the question and the
topic graph. An analysis of the dependency
parse of the question characterizes the question
word, topic, verb, and named entities of the
main subject as the question features, such as
qword=who. Features on each node include the
types of relations and properties the node pos-
sesses, such as type=person. Finally features
from both the question and each node are com-
bined as the final features used by the learner, such
as qword=who|type=person. In this way the as-
sociation between the question and answer type
is enforced. Thus during decoding, for instance,
if there is a who question, the nodes with a per-
son property would be ranked higher as the an-
swer candidate.
SEMPRE
3
is an open-source system for training
semantic parsers, that has been utilized to train a
semantic parser against Freebase by Berant et al.
(2013). SEMPRE maps NL utterances to logical
forms by performing bottom-up parsing. First, a
2
https://code.google.com/p/jacana/
3
http://www-nlp.stanford.edu/software/
sempre/
lexicon is used to map NL phrases to KB predi-
cates, and then predicates are combined to form a
full logical form by a context-free grammar. Since
logical forms can be derived in multiple ways from
the grammar, a log-linear model is used to rank
possible derivations. The parameters of the model
are trained from question-answer pairs.
3 Analysis
3.1 Evaluation Metrics
Both Berant et al. (2013) and Yao and
Van Durme (2014) tested their systems on
the WEBQUESTIONS dataset, which contains
3778 training questions and 2032 test questions
collected from the Google Suggest API. Each
question came with a standard answer from
Freebase annotated by Amazon Mechanical Turk.
Berant et al. (2013) reported a score of 31.4%
in terms of accuracy (with partial credit if inexact
match) on the test set and later in Berant and Liang
(2014) revised it to 35.7%. Berant et al. focused
on accuracy ? how many questions were correctly
answered by the system. Since their system an-
swered almost all questions, accuracy is roughly
identical to F
1
. Yao and Van Durme (2014)?s sys-
tem on the other hand only answered 80% of all
test questions. Thus they report a score of 42%
in terms of F
1
on this dataset. For the purpose of
comparing among all test questions, we lowered
the logistic regression prediction threshold (usu-
ally 0.5) on jacana-freebase for the other 20%
of questions where jacana-freebase had not pro-
posed an answer to, and selected the best-possible
prediction with the highest prediction score as the
answer. In this way jacana-freebase was able
to answer all questions with a lower accuracy of
35.4%. In the following we present analysis re-
sults based on the test questions where the two
systems had very similar performance (35.7% vs.
35.4%).
4
The difference is not significant accord-
ing to the paired permutation test (Smucker et al.,
2007).
3.2 Accuracy vs. Coverage
First, we were interested to see the proportions of
questions SEMPRE and jacana-freebase jointly
and separately answered correctly. The answer to
4
In this setting accuracy equals averaged macro F
1
: first
the F
1
value on each question were computed, then averaged
among all questions, or put it in other words: ?accuracy with
partial credit?. In this section our usage of the terms ?accu-
racy? and ?F
1
? can be exchanged.
83
jacana (F
1
= 1) jacana (F
1
? 0.5)
S
E
M
P
R
E
?
?
?
?
?
153 (0.08) 383 (0.19) 429 (0.21) 321 (0.16)
? 136 (0.07) 1360 (0.67) 366 (0.18) 916 (0.45)
Table 1: The absolute and proportion of ques-
tions SEMPRE and jacana-freebase answered
correctly (
?
) and incorrectly (?) jointly and sep-
arately, running a threshold F
1
of 1 and 0.5.
many questions in the dataset is a set of answers,
for example what to see near sedona arizona?.
Since turkers did not exhaustively pick out all pos-
sible answers, evaluation is performed by comput-
ing the F
1
between the set of answers given by
the system and the answers provided by turkers.
With a strict threshold of F
1
= 1 and a permis-
sive threshold of F
1
? 0.5 to judge the correct-
ness, we list the pair-wise correctness matrix in
Table 1. Not surprisingly, both systems had most
questions wrong given that the averaged F
1
?s were
only around 35%. With the threshold F
1
= 1,
SEMPRE answered more questions exactly cor-
rectly compared to jacana-freebase, while when
F
1
? 0.5, it was the other way around. This
shows that SEMPRE is more accurate in certain
questions. The reason behind this is that SEMPRE
always fires queries that return exactly one set of
answers from Freebase, while jacana-freebase
could potentially tag multiple nodes as the answer,
which may lower the accuracy.
We have shown that both systems can be more
accurate in certain questions, but when? Is there
a correlation between the system confidence and
accuracy? Thus we took the logistic decoding
score (between 0 and 1) from jacana-freebase
and the probability from the log-linear model used
by SEMPRE as confidence, and plotted an ?accu-
racy vs. coverage? curve, which shows the accu-
racy of a QA engine with respect to its coverage
of all questions. The curve basically answers one
question: at a fixed accuracy, what is the propor-
tion of questions that can be answered? A better
system should be able to answer more questions
correctly with the same accuracy.
The curve was drawn in the following way. For
each question, we select the best answer candidate
with the highest confidence score. Then for the
whole test set, we have a list of (question, highest
ranked answer, confidence score) tuples. Running
0 10 20 30 40 50 60 70 80 90 100Percent Answered
20
30
40
50
60
70
Acc
ura
cy
Accuracy vs. Coverage
jacana-freebase
SEMPRE
Figure 1: Precision with respect to proportion of
questions answered
a threshold from 1 to 0, we select those questions
with an answer confidence score above the thresh-
old and compute accuracy at this point. The X-
axis indicates the percentage of questions above
the threshold and the Y-axis the accuracy, shown
in Figure 1.
The two curves generally follow a similar trend,
but while jacana-freebase has higher accuracy
when coverage is low, SEMPRE obtains slightly
better accuracy when more questions are an-
swered.
3.3 Accuracy by Question Length and Type
Do accuracies of the two systems differ with re-
spect to the complexity of questions? Since there
is no clear way to measure question complexity,
we use question length as a surrogate and report
accuracies by question length in Figure 2. Most of
the questions were 5 to 8 words long and there was
no substantial difference in terms of accuracies.
The major difference lies in questions of length 3,
12 and 13. However, the number of such ques-
tions was not high enough to show any statistical
significance.
Figure 3 further shows the accuracies with re-
spect to the question types (as reflected by the
WH-word). Again, there is no significant differ-
ence between the two systems.
3.4 Learned Features
What did the systems learn during training? We
compare them by presenting the top features by
weight, as listed in Table 2. Clearly, the type of
knowledge learned by the systems in these fea-
tures is similar: both systems learn to associate
certain phrases with predicates from the KB.
84
0	 ?0.05	 ?
0.1	 ?0.15	 ?
0.2	 ?0.25	 ?
0.3	 ?0.35	 ?
0.4	 ?0.45	 ?
0.5	 ?
3	 ?(9)	 ? 4	 ?(78)
	 ?
5	 ?(299
)	 ?
6	 ?(432
)	 ?
7	 ?(395
)	 ?
8	 ?(273
)	 ?
9	 ?(122
)	 ?
10	 ?(48
)	 ?
11	 ?(19
)	 ?
12	 ?(10
)	 ? 13	 ?(4)
	 ?
15	 ?(1)
	 ?
<=	 ?5(	 ?
386)	 ?
<=	 ?10
	 ?(1270
)	 ?
<=15	 ?
(34)	 ?
Jacana-??freebase	 ?
SEMPRE	 ?
Figure 2: Accuracy (Y-axis) by question length.
The X-axis specifies the question length in words
and the total number of questions in parenthesis.
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
0.4	 ?
0.45	 ?
what	 ?
(929)	 ?
where	 ?
(357)	 ?
who	 ?
(261)	 ?
which	 ?
(35)	 ?
when	 ?
(100)	 ?
how	 ?	 ?
(8)	 ?
Jacana-??freebase	 ?
SEMPRE	 ?
Figure 3: Accuracy by question type (and the
number of questions).
We note, however, that SEMPRE also obtains in-
formation from the fully constructed logical form.
For instance, SEMPRE learns that logical forms
that return an empty set when executed against the
KB are usually incorrect (the weight for this fea-
ture is -8.88). In this respect the SP approach ?un-
derstands? more than the IE approach.
We did not further compare on other datasets
such as GeoQuery (Tang and Mooney, 2001) and
FREE917 (Cai and Yates, 2013b). The first one
involves geographic inference and multiple con-
traints in queries, directly fitting the compositional
nature of semantic parsing. The second one was
manually generated by looking at Freebase top-
ics. Both datasets were less realistic than the
WEBQUESTIONS dataset. Both datasets were also
less challenging (accuracy/F
1
were between 80%
and 90%) compared to WEBQUESTIONS (around
40%).
4 Discussion and Conclusion
Our analysis of two QA approaches, semantic
parsing and information extraction, has shown no
significant difference between them. Note the
feature weight
qfocus=religion|type=Religion 8.60
qfocus=money|type=Currency 5.56
qverb=die|type=CauseOfDeath 5.35
qword=when|type=datetime 5.11
qverb=border|rel=location.adjoins 4.56
(a) jacana-freebase
feature weight
die from=CauseOfDeath 10.23
die of=CauseOfDeath 7.55
accept=Currency 7.30
bear=PlaceOfBirth 7.11
in switzerland=Switzerland 6.86
(b) SEMPRE
Table 2: Learned top features and their weights for
jacana-freebase and SEMPRE.
similarity between features used in both systems
shown in Table 2: the systems learned the same
?knowledge? from data, with the distinction that
the IE approach acquired this through a direct as-
sociation between dependency parses and answer
properties, while the SP approach acquired this
through optimizing on intermediate logic forms.
With a direct information extraction technol-
ogy easily getting on par with the more sophis-
ticated semantic parsing method, it suggests that
SP-based approaches for QA with Freebase has
not yet shown its power from a ?deeper? under-
standing of the questions, among questions of var-
ious lengths. We suggest that more compositional
open-domain datasets should be created, and that
SP researchers should focus on utterances in exist-
ing datasets that are beyond the reach of direct IE
methods.
5 Acknowledgement
We thank the Allen Institute for Artificial Intelli-
gence for assistance in funding this work. This
material is partially based on research sponsored
by the NSF under grant IIS-1249516 and DARPA
under agreements number FA8750-13-2-0017 and
FA8750-13-2-0040 (the DEFT program).
85
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBPedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Proceedings of ACL.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In Proceedings of EMNLP.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of ACL.
Qingqing Cai and Alexander Yates. 2013b. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of ACL.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World Wide
Web, pages 229?232. ACM.
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of EMNLP.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP, pages
1223?1233.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of EMNLP.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling Semantic Parsers with
On-the-fly Ontology Matching. In Proceedings of
EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein.
2011. Learning Dependency-Based Compositional
Semantics. In Proceedings of ACL.
M.D. Smucker, J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for in-
formation retrieval evaluation. In Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 623?
632. ACM.
Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001. Springer.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of ACL.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. Uncertainty in Artificial Intelligence
(UAI).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of ACL-
CoNLL.
86
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 50?55,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Predicting Fine-grained Social Roles with Selectional Preferences
Charley Beller Craig Harman Benjamin Van Durme
charleybeller@jhu.edu craig@craigharman.net vandurme@cs.jhu.edu
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
Abstract
Selectional preferences, the tendencies of
predicates to select for certain semantic
classes of arguments, have been success-
fully applied to a number of tasks in
computational linguistics including word
sense disambiguation, semantic role label-
ing, relation extraction, and textual infer-
ence. Here we leverage the information
encoded in selectional preferences to the
task of predicting fine-grained categories
of authors on the social media platform
Twitter. First person uses of verbs that se-
lect for a given social role as subject (e.g.
I teach ... for teacher) are used to quickly
build up binary classifiers for that role.
1 Introduction
It has long been recognized that linguistic pred-
icates preferentially select arguments that meet
certain semantic criteria (Katz and Fodor, 1963;
Chomsky, 1965). The verb eat for example se-
lects for an animate subject and a comestible ob-
ject. While the information encoded by selectional
preferences can and has been used to support nat-
ural language processing tasks such as word sense
disambiguation (Resnik, 1997), syntactic disam-
biguation (Li and Abe, 1998) and semantic role
labeling (Gildea and Jurafsky, 2002), much of
the work on the topic revolves around developing
methods to induce selectional preferences from
data. In this setting, end-tasks can be used for
evaluation of the resulting collection. Ritter et al.
(2010) gave a recent overview of this work, break-
ing it down into class-based approaches (Resnik,
1996; Li and Abe, 1998; Clark and Weir, 2002;
Pantel et al., 2007), similarity-based approaches
(Dagan et al., 1999; Erk, 2007), and approaches
using discriminative (Bergsma et al., 2008) or gen-
erative probabilistic models (Rooth et al., 1999)
like their own.
One of our contributions here is to show that
the literature on selectional preferences relates to
the analysis of the first person content transmitted
through social media. We make use of a ?quick
and dirty? method for inducing selectional pref-
erences and apply the resulting collections to the
task of predicting fine-grained latent author at-
tributes on Twitter. Our method for inducing se-
lectional preferences is most similar to class-based
approaches, though unlike approaches such as by
Resnik (1996) we do not require a WordNet-like
ontology.
The vast quantity of informal, first-person text
data made available by the rise of social me-
dia platforms has encouraged researchers to de-
velop models that predict broad user categories
like age, gender, and political preference (Garera
and Yarowsky, 2009; Rao et al., 2010; Burger et
al., 2011; Van Durme, 2012b; Zamal et al., 2012).
Such information is useful for large scale demo-
graphic research that can fuel computational social
science advertising.
Similarly to Beller et al. (2014), we are inter-
ested in classification that is finer-grained than
gender or political affiliation, seeking instead to
predict social roles like smoker, student, and
artist. We make use of a light-weight, unsuper-
vised method to identify selectional preferences
and use the resulting information to rapidly boot-
strap classification models.
2 Inducing Selectional Preferences
Consider the task of predicting social roles in more
detail: For a given role, e.g. artist, we want a way
to distinguish role-bearing from non-role-bearing
users. We can view each social role as being a
fine-grained version of a semantic class of the sort
required by class-based approaches to selectional
preferences (e.g. the work by Resnik (1996) and
those reviewed by Light and Greiff (2002)). The
goal then is to identify a set of verbs that preferen-
50
tially select that particular class as argument. Once
we have a set of verbs for a given role, simple pat-
tern matches against first person subject templates
like I can be used to identify authors that bear
that social role.
In order to identify verbs that select for a given
role r as subject we use an unsupervised method
inspired by Bergsma and Van Durme (2013) that
extracts features from third-person content (i.e.
newswire) to build classifiers on first-person con-
tent (i.e. tweets). For example, if we read in a
news article that an artist drew ..., we can take a
tweet saying I drew ... as potential evidence that
the author bears the artist social role.
We first count all verbs v that appear with role r
as subject in the web-scale, part-of-speech tagged
n-gram corpus, Google V2 (Lin et al., 2010).
The resulting collection of verbs is then ranked
by computing their pointwise mutual information
(Church and Hanks, 1990) with the subject role r.
The PMI of a given role r and a verb v that takes
r as subject is given as:
PMI(r, v) = log
P (r, v)
P (r)P (v)
Probabilities are estimated from counts of the
role-verb pairs along with counts matching the
generic subject patterns he and she which
serve as general background cases. This gives us a
set of verbs that preferentially select for the subset
of persons filling the given role.
The output of the PMI ranking is a high-recall
list of verbs that preferentially select the given so-
cial role as subject over a background population.
Each such list then underwent a manual filtering
step to rapidly remove non-discriminative verbs
and corpus artifacts. One such artifact from our
corpus was the term wannabe which was spuri-
ously elevated in the PMI ranking based on the
relative frequency of the bigram artist wannabe as
compared to she wannabe. Note that in the first
case wannabe is best analyzed as a noun, while in
the second case a verbal analysis is more plausi-
ble. The filtering was performed by one of the au-
thors and generally took less than two minutes per
list. The rapidity of the filtering step is in line with
findings such as by Jacoby et al. (1979) that rele-
vance based filtering involves less cognitive effort
than generation. After filtering the lists contained
fewer than 40 verbs selecting each social role.
In part because of the pivot from third- to first-
person text we performed a precision test on the
remaining verbs to identify which of them are
likely to be useful in classifying twitter users. For
each remaining verb we extracted all tweets that
contained the first person subject pattern I from
a small corpus of tweets drawn from the free pub-
lic 1% sample of the Twitter Firehose over a single
month in 2013. Verbs that had no matches which
appeared to be composed by a member of the as-
sociated social role were discarded. Using this
smaller high-precision set of verbs, we collected
tweets from a much larger corpus drawn from 1%
sample over the period 2011-2013.
One notable feature of the written English in
social media is that sentence subjects can be op-
tionally omitted. Subject-drop is a recognized fea-
ture of other informal spoken and written registers
of English, particularly ?diary dialects? (Thrasher,
1977; Napoli, 1982; Haegeman and Ihsane, 2001;
Weir, 2012; Haegeman, 2013; Scott, 2013). Be-
cause of the prevalence of subjectless cases we
collected two sets of tweets: those matching the
first person subject pattern I and those where
the verb was tweet initial. Example tweets for each
of our social roles can be seen in Table 2.
3 Classification via selectional
preferences
We conducted a set of experiments to gauge the
strength of the selectional preference indicators
for each social role. For each experiment we used
balanced datasets for training and testing with half
of the users taken from a random background sam-
ple and half from a collection of users identified
as belonging to the social role. Base accuracy was
thus 50%.
To curate the collections of positively identified
users we crowdsourced a manual verification pro-
cedure. We use the popular crowdsourcing plat-
form Mechanical Turk
1
to judge whether, for a
tweet containing a given verb, the author held the
role that verb prefers as subject. Each tweet was
judged using 5-way redundancy.
Mechanical Turk judges (?Turkers?) were pre-
sented with a tweet and the prompt: Based on this
tweet, would you think this person is a ARTIST?
along with four response options: Yes, Maybe,
Hard to tell, and No. An example is shown in Fig-
ure 1.
We piloted this labeling task with a goal of
20 tweets per verb over a variety of social roles.
1
https://www.mturk.com/mturk/
51
Artist
draw Yeaa this a be the first time I draw my
shit onn
Athlete
play @[user] @[user] i have got the night off
tonight because I played last night and I
am going out for dinner so won?t be able
to come?
Blogger
blogged @[user] I decided not to renew. I
blogged about it on the fan club. a bit
shocked no neg comments back to me
Cheerleader
cheer I really dont wanna cheer for this game
I have soo much to do
Christian
thank Had my bday yesterday 3011 nd had a
good night with my friends. I thank God
4 His blessings in my life nd praise Him
4 adding another year.
DJ
spin Quick cut session before I spin tonight
Filmmaker
film @[user] apparently there was no au-
dio on the volleyball game I filmed
so...there will be no ?NAT sound? cause
I have no audio at all
Media Host
interview Oh. I interviewed her on the @[user] .
You should listen to the interview. Its
awesome! @[user] @[user] @[user]
Performer
perform I perform the flute... kareem shocked...
Producer
produce RT @[user]: Wow 2 films in Urban-
world this year-1 I produced ... [URL]
Smoker
smoke I smoke , i drank .. was my shit bra !
Stoner
puff I?m a cigarello fiend smokin weed like
its oxygen Puff pass, nigga I puff grass
till I pass out
Student
finish I finish school in March and my friend
birthday in March ...
Teacher
teach @[user] home schooled I really wanna
find out wat it?s like n making new
friends but home schooling is cool I
teach myself mums ill
Table 1: Example verbs and sample tweets collected using
them in the first person subject pattern (I ).
Each answer was associated with a score (Yes = 1,
Maybe = .5, Hard to tell = No = 0) and aggregated
across the five judges, leading to a range of pos-
sible scores from 0.0 to 5.0 per tweet. We found
in development that an aggregate score of 4.0 led
to an acceptable agreement rate between the Turk-
ers and the experimenters, when the tweets were
randomly sampled and judged internally.
Verbs were discarded for being either insuffi-
ciently accurate or insufficiently prevalent in the
corpus. From the remaining verbs, we identified
users with tweets scoring 4.0 or better as the posi-
tive examples of the associated social roles. These
positively identified user?s tweets were scraped us-
ing the Twitter API in order to construct user-
specific corpora of positive examples for each role.
Figure 1: Mechanical Turk presentation
0.5
0.6
0.7
0.8
Art
ist
Ath
lete
Blo
gge
r
Che
erle
ade
r
Chr
istia
n
DJ
Film
mak
er
Ho
st
Per
form
er
Pro
duc
er
Sm
oke
r
Sto
ner
Stu
den
t
Tea
che
r
Acc
ura
cy
Figure 2: Accuracy of classifier trained and tested on bal-
anced set contrasting agreed upon Twitter users of a given
role, against users pulled at random from the 1% stream.
3.1 General Classification
The positively annotated examples were balanced
with data from a background set of Twitter users
to produce training and test sets. These test sets
were usually of size 40 (20 positive, 20 back-
ground), with a few classes being sparser (the
smallest test set had only 28 instances). We used
the Jerboa (Van Durme, 2012a) platform to con-
vert our data to binary feature vectors over a uni-
gram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al., 2008). Results are shown
in Figure 2. As can be seen, a variety of classes in
this balanced setup can be predicted with accura-
cies in the range of 80%. This shows that the in-
formation encoded in selectional preferences con-
tains discriminating signal for a variety of these
social roles.
3.2 Conditional Classification
How accurately can we predict membership in a
given class when a Twitter user sends a tweet
matching one of the collected verbs? For exam-
ple, if one sends a tweet saying I race ..., then how
likely is it that the author is an athlete?
52
0.5
0.6
0.7
0.8
Art
ist :
 dra
w
Ath
lete
 : ra
ce
Ath
lete
 : ru
n
Blo
gge
r : b
log
ged
Che
erle
ade
r : c
hee
r
Chr
istia
n : 
pra
y
Chr
istia
n : 
serv
e
Chr
istia
n : 
than
k
DJ 
: sp
in
Film
mak
er 
: fil
m
Ho
st : 
inte
rvie
w
Per
form
er :
 per
form
Pro
duc
er :
 pro
duc
e
Sm
oke
r : 
sm
oke
Sto
ner
 : p
uff
Sto
ner
 : sp
ark
Stu
den
t : e
nro
ll
Stu
den
t : f
inis
h
Tea
che
r : t
eac
h
Acc
ura
cy
Figure 3: Results of positive vs negative by verb. Given
that a user writes a tweet containing I interview . . . or Inter-
viewing . . . we are about 75% accurate in identifying whether
or not the user is a Radio/Podcast Host.
# Users # labeled # Pos # Neg Attribute
199022 516 63 238 Artist-draw
45162 566 40 284 Athlete-race
1074289 1000 54 731 Athlete-run
9960 15 14 0 Blogger-blog
2204 140 57 18 College Student-enroll
247231 1000 85 564 College Student-finish
60486 845 61 524 Cheerleader-cheer
448738 561 133 95 Christian-pray
92223 286 59 180 Christian-serve
428337 307 78 135 Christian-thank
17408 246 17 151 DJ-spin
153793 621 53 332 Filmmaker-film
36991 554 42 223 Radio Host-interview
43997 297 81 97 Performer-perform
69463 315 71 100 Producer-produce
513096 144 74 8 Smoker-smoke
5542 124 49 15 Stoner-puff
5526 229 59 51 Stoner-spark
149244 495 133 208 Teacher-teach
Table 2: Numbers of positively and negatively identified
users by indicative verb.
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given verb term. Positive instances were taken to
be those with a score of 4.0 or higher, with nega-
tive instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in figure 3.
Note that for a number of verb terms these thresh-
olds left very sparse collections of users. There
were only 8 users, for example, that tweeted the
phrase I smoke ... but were labeled as negative in-
stances of Smokers. Counts are given in Table 2.
Despite the sparsity of some of these classes,
many of the features learned by our classifiers
make intuitive sense. Highlights of the most
highly weighted unigrams from the classification
Verb Feature ( Rank)
draw drawing, art, book
4
, sketch
14
, paper
19
race race, hard, winter, won
11
, training
16
, run
17
run awesome, nike
6
, fast
9
, marathon
20
blog notes, boom, hacked
4
, perspective
9
cheer cheer, pictures, omg, text, literally
pray through, jesus
3
, prayers
7
, lord
14
, thank
17
serve lord, jesus, church, blessed, pray, grace
thank [ ], blessed, lord, trust
11
, pray
12
enroll fall, fat, carry, job, spend, fail
15
finish hey, wrong, may
8
, move
9
, officially
14
spin show, dj, music, dude, ladies, posted, listen
film please, wow, youtube, send, music
8
perform [ ], stuck, act, song, tickets
7
, support
16
produce follow, video
8
, listen
10
, single
11
, studio
13
,
interview fan, latest, awesome, seems
smoke weakness, runs, ti, simply
puff bout, $
7
, smh
9
, weed
10
spark dont, fat
5
, blunt
6
, smoke
11
teach forward, amazing, students, great, teacher
7
Table 3: Most-highly indicative features that a user holds
the associated role given that they used the phrase I VERB
along with select features within the top 20.
experiments are shown in Table 3. Taken together
these features suggest that several of our roles can
be distinguished from the background population
by focussing on typical language use. The use of
terms like, e.g., sketch by artists, training by ath-
letes, jesus by Chrisitians, and students by teach-
ers conforms to expected pattern of language use.
4 Conclusion
We have shown that verb-argument selectional
preferences relates to the content-based classifica-
tion strategy for latent author attributes. In particu-
lar, we have presented initial studies showing that
mining selectional preferences from third-person
content, such as newswire, can be used to inform
latent author attribute prediction based on first-
person content, such as that appearing in social
media services like Twitter.
Future work should consider the question of
priors. Our study here relied on balanced class
experiments, but the more fine-grained the social
role, the smaller the subset of the population we
might expect will possess that role. Estimating
these priors is thus an important point for future
work, especially if we wish to couple such demo-
graphic predictions within a larger automatic sys-
tem, such as the aggregate prediction of targeted
sentiment (Jiang et al., 2011).
Acknowledgements This material is partially based
on research sponsored by the NSF under grant IIS-1249516
and by DARPA under agreement number FA8750-13-2-0017
(DEFT).
53
References
Charley Beller, Rebecca Knowles, Craig Harman,
Shane Bergsma, Margaret Mitchell, and Benjamin
Van Durme. 2014. I?m a belieber: Social roles via
self-identification and conceptual attributes. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 59?68. Association for
Computational Linguistics.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of EMNLP.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. Number 11. MIT press.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2).
Ido Dagan, Lillian Lee, and Fernando CN Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 34(1-3):43?
69.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceeding of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, volume 45, page 216.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Liliane Haegeman and Tabea Ihsane. 2001. Adult null
subjects in the non-pro-drop languages: Two diary
dialects. Language acquisition, 9(4):329?346.
Liliane Haegeman. 2013. The syntax of registers: Di-
ary subject omission and the privilege of the root.
Lingua, 130:88?110.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao.
2011. Target-dependent twitter sentiment classifi-
cation. In Proceedings of ACL.
Jerrold J Katz and Jerry A Fodor. 1963. The structure
of a semantic theory. language, pages 170?210.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational linguistics, 24(2):217?244.
Marc Light and Warren Greiff. 2002. Statistical mod-
els for the induction and use of selectional prefer-
ences. Cognitive Science, 26(3):269?281.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Donna Jo Napoli. 1982. Initial material deletion in
English. Glossa, 16(1):5?111.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard H Hovy. 2007.
ISP: Learning inferential selectional preferences. In
HLT-NAACL, pages 564?571.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127?159.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52?57. Washington,
DC.
Alan Ritter, Masaum, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 424?434. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
54
Kate Scott. 2013. Pragmatically motivated null sub-
jects in English: A relevance theory perspective.
Journal of Pragmatics, 53:68?83.
Randolph Thrasher. 1977. One way to say more by
saying less: A study of so-called subjectless sen-
tences. Kwansei Gakuin University Monograph Se-
ries, 11.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Andrew Weir. 2012. Left-edge deletion in English and
subject omission in diaries. English Language and
Linguistics, 16(01):105?129.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
55
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 1?5,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Augmenting FrameNet Via PPDB
Pushpendre Rastogi
1
and Benjamin Van Durme
1,2
1
Center for Language and Speech Processing
2
Human Language Technology Center of Excellence
Johns Hopkins University
pushpendre@jhu.edu, vandurme@cs.jhu.edu
Abstract
FrameNet is a lexico-semantic dataset that
embodies the theory of frame semantics.
Like other semantic databases, FrameNet
is incomplete. We augment it via the para-
phrase database, PPDB, and gain a three-
fold increase in coverage at 65% precision.
1 Introduction
Frame semantics describes the meaning of a word
in relation to real world events and entities. In
frame semantics the primary unit of lexical analy-
sis is the frame and the lexical unit. A frame aims
to capture the most salient properties of a con-
cept, situation or event. For example, the frame
representing the concept of Abandonment con-
tains eight attributes:
1
Agent, Theme, Place,
Time, Manner, Duration, Explanation
and Depictive. A lexical unit is a tuple of three
elements: the lemma of a word, its POS tag and
the associated frame.
FrameNet is large lexico-semantic dataset that
contains manually annotated information includ-
ing frame descriptions, frame-frame relations and
frame annotated sentences. It has been used build
to frame semantic parsers, which are systems that
can analyze a sentence and annotate its words with
the frames that they evoke and the correspond-
ing frame elements. The task of frame seman-
tic parsing was introduced by Gildea and Jurafsky
(2002) and later it matured into a community-wide
shared task (Baker et al., 2007), with CMU?s SE-
MAFOR system being the current state-of-the-art
parser (Das et al., 2013).
Common to rich, manually constructed seman-
tic resources, the coverage of FrameNet across its
1
An attribute of a frame is also called a Frame Element.
targetted language (English) is incomplete. State-
of-the-art frame semantic parsers thus employ var-
ious heuristics to identify the frame evoked by
out-of-vocabulary items (OOVs) at test-time.
2
For
instance, an OOV if present in WordNet might
be aligned to frame(s) assigned to in-vocabulary
items in shared synsets (see the work by Ferr?andez
et al. (2010) and the related works section therein).
In this work we take a different approach and
attempt to directly increase the coverage of the
FrameNet corpus by automatically expanding the
collection of training examples via PPDB, The
Paraphrase Database (Ganitkevitch et al., 2013).
In Section 2 we analyze FrameNet and com-
ment on the sparsity in its different parts. In Sec-
tion 3 we describe PPDB, and how it was used to
augment FrameNet. We present our evaluation ex-
periments and results in the latter half of the sec-
tion followed by conclusions.
2 FrameNet Coverage
FrameNet is a rich semantic resource, yet cur-
rently lacks complete coverage of the language.
In the following we give examples of this incom-
pleteness, in particular the OOV issue that we will
focus on in latter sections.
Frames A frame represents an event, a situ-
ation or a real life concept; FrameNet version
1.5 contains 1,019 such frames. These thou-
sand frames do not cover all possible situa-
tions that we might encounter. For example,
FrameNet does not have a frame for the activity
of Programming even though it has frames for
Creating, Text Creation, etc. The situa-
2
For example the Abandonment frame lacks jettison as
one of its lexical units, and further, that word is not listed
as a lexical unit in FrameNet v1.5, making jettison an OOV.
1
tion of writing a computer program is stereotypi-
cal and attributes that a reader might associate with
such an activity are: agent (who wrote the pro-
gram), language (the programming language
used) and function (the program?s purpose).
Further, FrameNet?s granularity is at times un-
even. For example, the Explosion frame
and the Become Triggered frames do not
have agentive attributes, instead there exist
separate frames Detonate Explosive and
Triggering which have the attributes Agent
and Actor respectively. This suggests a pattern
that events which are frequently described without
an agent are assigned their own frames. However,
there is no Burial frame which focuses on the
event corresponding to frame of Burying, which
itself focuses on the Actor.
This difference in granularity could be resolved
by either making distinctions more evenly fine-
grained: trying to automatically inducing new
frames; or by making things more evenly-coarse
grained: automatically merging existing frames
that are deemed similar. Researchers have ex-
plored methods for automatically learning frames
and on learning collocations of frames to their
syntactic dependent phrases. Recent examples in-
clude using either a Bayesian topic model to learn
clusters of words (O? Connor, 2012; Materna,
2012), or attempting to learn symbolic concepts
and attributes from dictionary definitions of words
(Orfan and Allen, 2013).
Frame-Frame Relations FrameNet encodes
certain types of correlations between situations
and events by adding defeasible typed-relations
between frames encoding pairwise dependencies.
There are eight types of frame-frame rela-
tions: Inherits from, Perspective on,
Precedes, Subframe of, See also,
Uses, Is Inchoative of, and
Is Causative of.
3
For example the frame
Being Born is related to Death through the
relation Is Preceded By. Such common-
sense knowledge of event-event relationships
would be of significant utility to general AI,
however it is a large space to fill: with 1,019
frames and 8 binary relations there is a large
upper bound on the number of total possible
3
Five frame-frame relations also have an antonym
relation: Is Used by, Is Inherited by,
Is Perspectivized in, Has Subframe(s),
Is Preceded by, however an antonym relation does not
add any extra information over its corresponding relation.
relation pairs, even if not considering the pre-
vious issue of incomplete frame coverage. For
example, the Experience bodily harm and
Hostile encounter frames are not related
through the Is Causative Of relation, even
though it is reasonable to expect that a hostile
encounter would result in bodily harm.
4
Though
researchers have used FrameNet relations for
tasks such as recognizing textual entailment
(RTE) (Aharon et al., 2010) and for text under-
standing (Fillmore and Baker, 2001), to the best
of our knowledge there has been no work on
automatically extending frame-frame relations.
Frame Annotated Sentences FrameNet con-
tains annotated sentences providing examples of:
lexical units, frames those lexical units evoked,
and frame elements present in the sentence (along
with additional information). These annotated
sentences can be divided into two types based on
whether all the frame evoking words were marked
as targets or not.
The first type, which we call lexicographic,
contains sentences with a single target per sen-
tence. The second type, called fulltext, contains
sentences that have been annotated more com-
pletely and they contain multiple targets per sen-
tence. There are 4,026 fulltext sentences con-
taining 23,921 targets. This data has proved to
be useful for lexico-semantic tasks like RTE and
paraphrasing e.g. (Aharon et al., 2010; Coyne
and Rambow, 2009). As compared to Prop-
Bank (Palmer et al., 2005), which annotated all
predicates occurring within a collection of pre-
existing documents, FrameNet provides examples,
but not a corpus that allows for directly estimating
relative frequencies.
Frame-Lemma Mappings As said earlier, lexi-
cal units are tuples of the lemma form of a word,
its POS-tag and its associated frame. One compo-
nent of FrameNet is its information about which
words/lemmas prompt a particular frame. An an-
notated word that evokes a frame in a sentence
is referred to as a Target. There are two areas
where these mappings could be incomplete: (1)
lemmas contained within FrameNet may have al-
ternate senses such that they should be placed
in more Frames (or related: a currently missing
frame might then give rise to another sense of
4
Reasonable highlights the issue that we would opti-
mally like to know things that are even just possible/not-too-
unlikely, even if not strictly entailed.
2
such a lemma); and (2) lemmas from the language
may not be in FrameNet in any form. Most re-
search on mitigating this limitation involves map-
ping FrameNet?s frames to WordNet?s synsets.
5
Fossati et al. (2013) explored the feasibility of
crowdsourcing FrameNet coverage, using the dis-
tributed manual labor of Mechanical Turk to com-
plete the lemma coverage.
3 Augmenting FrameNet with PPDB
In order to expand the coverage of FrameNet, we
performed an initial study on the use of a new
broad-coverage lexical-semantic resource, PPDB,
to first add new lemmas as potential triggers for
a frame, and then automatically rewrite existing
example sentences with these new triggers. The
eventual goal of would be to enable any existing
FrameNet semantic parser to simply retrain on this
expanded dataset, rather than needing to encode
methods for dynamic OOV-resolution at test-time
(such as employed by SEMAFOR).
PPDB Ganitkevitch et al. (2013) released a large
collection of lexical, phrasal and syntactic para-
phrases
6
collectively called PPDB. We used the
lexical rules in PPDB to find potential paraphrases
of target words of frame annotated sentences. A
lexical rule in PPDB looks like the following:
[VB] ||| help ||| assist |||
p(e|f)=2.832, p(f|e)=1.551, ...
This rule conveys that the log-probability that
help would be paraphrased by the word assist is
-2.832 but the log probability of assist being para-
phrased as help is -1.551.
7
Ganitkevitch et al.
(2013) released quality-sorted subsets of the full
(large) collection, varying in size from S to XXXL
by applying thresholds over a linear combination
of the feature values to prune away low quality
paraphrases. They verified that the average human
judgement score of randomly sampled paraphrases
from smaller sized collections was higher than the
5
It is worth noting that substituting a larger automatically
derived WordNet (as derived in Snow et al. (2004)) could im-
prove the recall of some of the methods which automatically
learn a mapping from FrameNet frames to WordNet synsets.
6
Lexical: Two words with the same meaning; phrasal:
two strings of words with the same meaning; syntactic:
strings of surface words and non-terminal categories that have
the same meaning. These strings are templates with the non-
terminals serving the role of constraints over what can go in
the blanks.
7
See complete list at http://github.com/
jweese/thrax/wiki/Feature-functions .
average human judgement score of a random sam-
ple from a larger collection.
Approach We used the lexical rules sans fea-
tures along with a 5-gram Kneser-Ney smoothed
language model trained using KenLM (Heafield
et al., 2013) on the raw English sequence of An-
notated Gigaword (Napoles et al., 2012) to para-
phrase the fulltext frame annotated sentences of
FrameNet. We used a combination of the WordNet
morphological analyzer and Morpha
8
for lemma-
tization and Morphg
9
for generation.
Evaluation We present our evaluation of the
quantity and quality of generated paraphrases in
this section. Note that we did not use syntac-
tic reordering to generate the paraphrases. Also
we paraphrased the frame evoking targets individ-
ually i.e. we did not consider combinations of
paraphrases of individual targets to be a new para-
phrase of a sentence and we ignored those frame
evoking targets that contained multiple words.
10
With the above mentioned constraints we
conducted the following experiments with dif-
ferent sizes of PPDB. In Experiment 1 we
generated a set of candidate paraphrases for
every target word in a sentence by directly
querying that word and its dictionary form in
PPDB. In Experiment 2 we first enlarged the set
of lexical units mapped to a frame by merging
lexical units of frames that were related to the
target word?s frame through either of the fol-
lowing relations: Is Perspectivized In,
Is Inherited By, Has Subframe (s).
For example, if frame A has a subframe B then
lexical units mapped to A can evoke B. We then
queried PPDB to gather paraphrases for all the
lexical units collected so far. This experiment
simulates the situation where a frame has been
mapped to a set of words, e.g. synsets in WordNet,
so that every word in that larger set is a paraphrase
of any word that evokes a frame. This procedure
increases the average number of paraphrases
mapped to a frame and we present those averages
in Table 1.
For both these experiments we also calculated
the incremental benefit of PPDB over WordNet by
8
http://ilexir.co.uk/applications/
rasp/download
9
http://cl.naist.jp/?eric-n/
ubuntu-nlp/pool/hardy/english/morph_
0.0.20030918-2nlp1?0hardy1.tar.gz
10
Among fulltext sentences less than 3% of targets con-
tained multiple tokens.
3
Database Lexical Unit/Frame
Framenet 20.24
PPDB S 23.15
PPDB M 32.00
PPDB L 74.08
PPDB XL 214.99
Table 1: Average count of lexical units per frame for differ-
ent sizes of PPDB in experiment 2.
The General Assembly should set aside money for a
new state health lab , millions of doses of antiviral
drugs and a fund to help meet basic needs after a disas-
ter , a legislative panel recommended Thursday .
1: The General Assembly should set aside cash ...
2: The General Assembly should set aside fund ...
1: The General Assembly should set aside dough ...
3: The General Assembly should set aside silver ...
Table 2: Examples and their judgements, with the last being
debatable.
filtering out paraphrases that could have been re-
trieved as synonyms
11
from WordNet v3.0. The
results of these experiments are in Table 3.
To evaluate the quality of our additional output
over WordNet we assigned one of the following
labels to 25 paraphrase sets generated at the end of
Experiment 1b
12
: 1, the paraphrase (a) invoked the
correct frame and (b) was grammatical; or 2, only
(a) held; or 3, (a) did not hold. Table 4 presents
aggregates over the labels.
PPDB 1a 1b 2a 2b
S 4,582 2,574 1,064,926 1,022,533
M 15,459 9,752 1,314,169 1,263,087
L 73,763 55,517 2,417,760 2,347,656
XL 340,406 283,126 ? ?
Table 3: The total number of paraphrases generated for the
23,226 input targets versus different sizes of PPDB. The para-
phrase count excludes the input. Column 1a and 2a represent
unfiltered paraphrases as opposed to 1b and 2b where they
have been filtered using WordNet v3.0.
4 Discussion And Conclusion
We presented initial experiments on using PPDB
to automatically expand FrameNet through para-
phrastic re-writing. We found that over a sample
of 25 target words the top three paraphrases pro-
duced by PPDB XL evoked the correct frame and
were grammatical 65% of the time.
13
However,
11
Two lemmas that appear in the same synset at least once
are synonyms in our experiments.
12
Experiment 2 generated significantly more candidates;
here we consider only the potential scope of expansion and
rely on Experiment 1 to gauge the likely paraphrase quality.
13
We have released the generated corpus as well as the
manual annotations at cs.jhu.edu/?prastog3/res/
fnppdb.html
PPDB Size 1 2 3 %(1+2) %(1)
S 0 0 0 ? ?
M 6 1 2 77.77 66.67
L 27 15 11 86.25 50.94
L rank 3 23 12 7 83.33 54.76
XL 110 85 50 79.60 44.89
XL rank 3 47 16 9 87.5 65.27
XL rank 5 69 28 13 88.18 62.72
XL rank 10 105 52 32 83.07 55.55
Table 4: Average quality of all paraphrases for 25 ran-
dom sentences. Rows marked A rank B convey that we used
PPDB of size A and kept only the top B sentences after sorting
them by their language model score. Column %(1) indicates
the percentage of output which was grammatical and evoked
the correct frame. Column%(1+2) represents the output that
evoked the correct frame.
work remains in recognizing the contexts in which
a paraphrase is appropriately applied, and in im-
proving the quality of PPDB itself.
Upon error analysis, we found two major rea-
sons for ungrammaticality of lexical paraphrases.
First: within FrameNet some sentences will have
a single token annotated as trigger, when in fact it
is part of a multi-word expression. For example, it
was grammatically infelicitous to replace part by
portion in the expression part of the answer. The
other major source of error was the inaccuracy in
PPDB itself. We found that for a large number of
cases when PPDB XL did not have a high number
of paraphrases the paraphrases were wrong (e.g.,
PPDB XL had only 2 paraphrases for the words
lab and millions.)
Going forward we aim to increase the precision
of our paraphrases and our ability to recognize
their appropriate contexts for application. Fur-
ther, we wish to augment additional resources in
a similar way, for example PropBank or the ACE
corpus (Walker et al., 2006). We should be able
to increase the precision by using the paraphrase
probability features of a PPDB rule and by using
better language models with lower perplexity than
n-grams e.g. recurrent neural net based language
models. Improving the accuracy of PPDB, espe-
cially in the large settings, would be another fo-
cus area. Also, we would use Amazon Mechani-
cal Turk to evaluate the quality of a larger set of
paraphrases to make our evaluation robust and so
that we can evaluate the efficacy of our second ex-
periment.
Acknowledgments This material is based on re-
search sponsored by the NSF under grant IIS-
1249516 and DARPA under agreement number
FA8750-13-2-0017.
4
References
Roni Ben Aharon, Idan Szpektor, and Ido Dagan.
2010. Generating entailment rules from framenet.
In Proceedings of the ACL 2010 Conference Short
Papers, pages 241?246.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval?07 task 19: frame semantic structure
extraction. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 99?104.
ACL.
Bob Coyne and Owen Rambow. 2009. Lexpar: A
freely available english paraphrase lexicon automat-
ically extracted from framenet. 2012 IEEE Sixth
International Conference on Semantic Computing,
pages 53?58.
Dipanjan Das, Desai Chen, Andr?e F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2013.
Frame-semantic parsing. Computational Linguis-
tics, 40(1):9?56.
Oscar Ferr?andez, Michael Ellsworth, Rafael Munoz,
and Collin F Baker. 2010. Aligning framenet
and wordnet based on semantic neighborhoods. In
LREC, volume 10, pages 310?314.
Charles J Fillmore and Collin F Baker. 2001. Frame
semantics for text understanding. In Proceedings
of WordNet and Other Lexical Resources Workshop,
NAACL.
Marco Fossati, Claudio Giuliano, and Sara Tonelli.
2013. Outsourcing framenet to the crowd. In Pro-
ceedings of the 51st Annual Meeting of the ACL,
pages 742?747.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of NAACL-HLT, pages
758?764, Atlanta, Georgia, June. ACL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the ACL,
Sofia, Bulgaria.
Ji?r?? Materna. 2012. Lda-frames: An unsupervised ap-
proach to generating semantic frames. In Compu-
tational Linguistics and Intelligent Text Processing,
volume 7181 of Lecture Notes in Computer Science,
pages 376?387. Springer Berlin Heidelberg.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, pages 95?100. ACL.
Brendan O? Connor. 2012. Learning frames from text
with an unsupervised latent variable model. In Tech-
nical Report. Carnegie Mellon University.
Jansen Orfan and James Allen. 2013. Toward learning
high-level semantic frames from definitions. In Pro-
ceedings of the Second Annual Conference on Ad-
vances in Cognitive Systems, volume 125.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS, volume 17, pages 1297?1304.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. Ace 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia.
5
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 45?53,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
A Comparison of the Events and Relations Across ACE, ERE, TAC-KBP,
and FrameNet Annotation Standards
Jacqueline Aguilar and Charley Beller and Paul McNamee and Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD, USA
Stephanie Strassel and Zhiyi Song and Joe Ellis
University of Pennsylvania
Linguistic Data Consortium (LDC)
Philadelphia, PA, USA
Abstract
The resurgence of effort within computa-
tional semantics has led to increased in-
terest in various types of relation extrac-
tion and semantic parsing. While var-
ious manually annotated resources exist
for enabling this work, these materials
have been developed with different stan-
dards and goals in mind. In an effort
to develop better general understanding
across these resources, we provide a sum-
mary overview of the standards underly-
ing ACE, ERE, TAC-KBP Slot-filling, and
FrameNet.
1 Overview
ACE and ERE are comprehensive annotation stan-
dards that aim to consistently annotate Entities,
Events, and Relations within a variety of doc-
uments. The ACE (Automatic Content Extrac-
tion) standard was developed by NIST in 1999 and
has evolved over time to support different evalua-
tion cycles, the last evaluation having occurred in
2008. The ERE (Entities, Relations, Events) stan-
dard was created under the DARPA DEFT pro-
gram as a lighter-weight version of ACE with the
goal of making annotation easier, and more con-
sistent across annotators. ERE attempts to achieve
this goal by consolidating some of the annotation
type distinctions that were found to be the most
problematic in ACE, as well as removing some
more complex annotation features.
This paper provides an overview of the relation-
ship between these two standards and compares
them to the more restricted standard of the TAC-
KBP slot-filling task and the more expansive stan-
dard of FrameNet. Sections 3 and 4 examine Rela-
tions and Events in the ACE/ERE standards, sec-
tion 5 looks at TAC-KBP slot-filling, and section
6 compares FrameNet to the other standards.
2 ACE and ERE Entity Tagging
Many of the differences in Relations and Events
annotation across the ACE and ERE standards
stem from differences in entity mention tagging.
This is simply because Relation and Event tagging
relies on the distinctions established in the entity
tagging portion of the annotation process. For ex-
ample, since ERE collapses the ACE Facility and
Location Types, any ACE Relation or Event that
relied on that distinction is revised in ERE. These
top-level differences are worth keeping in mind
when considering how Events and Relations tag-
ging is approached in ACE and ERE:
? Type Inventory: ACE and ERE share the Per-
son, Organization, Geo-Political Entity, and
Location Types. ACE has two additional
Types: Vehicle and Weapon. ERE does not
account for these Types and collapses the Fa-
cility and Location Types into Location. ERE
also includes a Title Type to address titles,
honorifics, roles, and professions (Linguis-
tic Data Consortium, 2006; Linguistic Data
Consortium, 2013a).
? Subtype Annotation: ACE further classifies
entity mentions by including Subtypes for
each determined Type; if the entity does not
fit into any Subtype, it is not annotated. ERE
annotation does not include any Subtypes.
? Entity Classes: In addition to Subtype, ACE
also classifies each entity mention according
45
1996 1998 2000 2002 2004 2006 2008 2010 2012
F
r
a
m
e
N
e
t
p
r
o
j
e
c
t
c
r
e
a
t
e
d
A
C
E
d
e
v
e
l
o
p
e
d
m
o
s
t
c
o
m
p
r
e
h
e
n
s
i
v
e
A
C
E
c
o
r
p
u
s
l
a
s
t
A
C
E
e
v
a
l
fi
r
s
t
T
A
C
-
K
B
P
E
R
E
c
r
e
a
t
e
d
Figure 1: Important Dates for the ACE, ERE, TAC-KBP, and FrameNet Standards
to entity class (Specific, Generic, Attributive,
and Underspecified).
? Taggability: ACE tags Attributive, Generic,
Specific, and Underspecified entity mentions.
ERE only tags Specific entity mentions.
? Extents and Heads: ACE marks the full noun
phrase of an entity mention and tags a head
word. ERE handles tagging based on the
mention level of an entity; in Name mentions
(NAM) the name is the extent, in Nominal
mentions (NOM) the full noun phrase is the
extent, in Pronoun mentions (PRO) the pro-
noun is the extent.
? Tags: ERE only specifies Type and Men-
tion level (NAM, NOM, PRO). ACE speci-
fies Type, Subtype, Entity Class (Attributive,
Generic, Specific, Underspecified), and Men-
tion Level (NAM, NOM, PRO, Headless).
3 Relations in ACE and ERE
In the ACE and ERE annotation models, the goal
of the Relations task is to detect and character-
ize relations of the targeted types between enti-
ties (Linguistic Data Consortium, 2008; Linguistic
Data Consortium, 2013c). The purpose of this task
is to extract a representation of the meaning of the
text, not necessarily tied to underlying syntactic
or lexical semantic representations. Both models
share similar overarching guidelines for determin-
ing what is taggable. For relations the differences
lie in the absence or presence of additional fea-
tures, syntactic classes, as well as differences in
assertion, trigger words, and minor subtype varia-
tions.
3.1 Similarities in Relations Annotation
In addition to comprising similar Types (both
models include Physical and Part.Whole Types as
well as slightly different Types to address Affilia-
tion and Social relations) used to characterize each
relation, ACE and ERE share important similar-
ities concerning their relation-tagging guidelines.
These include:
? Limiting relations to only those expressed in
a single sentence
? Tagging only for explicit mention
? No ?promoting? or ?nesting? of taggable en-
tities. In the sentence, Smith went to a hotel
in Brazil, (Smith, hotel) is a taggable Phys-
ical.Located relation, but (Smith, Brazil) is
not. This is because in order to tag this as
such, one would have to promote ?Brazil?.
? Tagging for past and former relations
? Two different Argument slots (Arg1 and
Arg2) are provided for each relation to cap-
ture the importance of Argument ordering.
? Arguments can be more than one token (al-
though ACE marks the head as well)
? Using ?templates? for each relation
Type/Subtype (e.g., in a Physical.Located
relation, the Person that is located some-
where will always be assigned to Arg1 and
the place in which the person is located will
always be assigned to Arg2).
? Neither model tags for negative relations
? Both methods contain argument span bound-
aries. That is, the relations should only in-
clude tagged entities within the extent of a
sentence.
3.2 Differences in Assertion, Modality, and
Tense
A primary difference between these two annota-
tion models is a result of ERE only annotating as-
serted events while ACE also includes hypothet-
icals. ACE accounts for these cases by including
two Modality attributes: ASSERTED and OTHER
46
(Linguistic Data Consortium, 2008). For exam-
ple, in the sentence, We are afraid that Al-Qaeda
terrorists will be in Baghdad, ACE would tag this
as an OTHER attribute, where OTHER pertains to
situations in ?some other world defined by coun-
terfactual constraints elsewhere in the context?,
whereas ERE would simply not tag a relation in
this sentence. Additionally, while both ACE and
ERE tag past and former relations, ACE goes fur-
ther to mark the Tense of each relation by means
of four attributes: Past, Future, Present and Un-
specified.
3.3 Syntactic Classes
ACE further justifies the tagging of each Relation
through Syntactic Classes. The primary function
of these classes is to serve as a sanity check on
taggability and as an additional constraint for tag-
ging. These classes include: Possessive, Prepo-
sition, PreMod, Coordination, Formulaic, Partic-
ipal, Verbal, Relations Expressed by Verbs, and
Other. Syntactic classes are not present in ERE
relations annotation.
3.4 Triggers
Explicit trigger words do not exist in ACE relation
annotation; instead, the model annotates the full
syntactic clause that serves as the ?trigger? for the
relation. ERE attempts to minimize the annotated
span by allowing for the tagging of an optional
trigger word, defined as ?the smallest extent of text
that indicates a relation Type and Subtype? (Lin-
guistic Data Consortium, 2013c). These triggers
are not limited to a single word, but can also be
composed of a phrase or any extent of the text that
indicates a Type/Subtype relation, left to the dis-
cretion of the annotator. It is common for preposi-
tions to be triggers, as in John is in Chicago. How-
ever, sometimes no trigger is needed because the
syntax of the sentence is such that it indicates a
particular relation Type/Subtype without a word to
explicitly signal the relation.
3.5 Types and Subtypes of Relations
There are three types of relations that contain var-
ied Subtypes between ERE and ACE. These are
the Physical, Part-Whole, Social and Affiliation
Types. The differences are a result of ERE collaps-
ing ACE Types and Subtypes into more concise, if
less specific, Type groups.
Physical Relation Type Differences The main
differences in the handling of the physical rela-
tions between ACE and ERE are shown in Table
1. ACE only marks Location for PERSON enti-
ties (for Arg1). ERE uses Location for PERSON
entities being located somewhere as well as for
a geographical location being part of another ge-
ographical location. Additionally, ACE includes
?Near? as a Subtype. This is used for when an en-
tity is explicitly near another entity, but neither en-
tity is a part of the other or located in/at the other.
ERE does not have an equivalent Subtype to ac-
count for this physical relation. Instead, ERE in-
cludes ?Origin? as a Subtype. This is used to de-
scribe the relation between a PER and an ORG.
ACE does not have a Physical Type equivalent,
but it does account for this type of relation within
a separate General Affiliation Type and ?Citizen-
Resident-Religion-Ethnicity? Subtype.
Part-Whole Relation Differences In Table 2,
note that ACE has a ?Geographical? Subtype
which captures the location of a FAC, LOC, or
GPE in or at, or as part of another FAC, LOC,
or GPE. Examples of this would be India con-
trolled the region or a phrase such as the Atlanta
area. ERE does not include this type of annota-
tion option. Instead, ERE tags these regional re-
lations as Physical.Located. ACE and ERE do
share a ?Subsidiary? Subtype which is defined in
both models as a ?category to capture the own-
ership, administrative and other hierarchical rela-
tionships between ORGs and/or GPEs? (Linguis-
tic Data Consortium, 2008; Linguistic Data Con-
sortium, 2013c).
Social and Affiliation Relation Differences
The most evident discrepancy in relation anno-
tation between the two models lies in the So-
cial and Affiliation Relation Types and Subtypes.
For social relations, ACE and ERE have three
Subtypes with similar goals (Business, Family,
Unspecified/Lasting-Personal) but ERE has an ad-
ditional ?Membership? Subtype, as shown in Ta-
ble 3. ACE addresses all ?Membership? relations
in its Affiliation Type. ERE also includes the ?So-
cial.Role? Subtype in order to address the TITLE
entity type, which only applies to ERE. How-
ever, both models agree that the arguments for
each relation must be PERSON entities and that
they should not include relationships implied from
interaction between two entities (e.g., President
47
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Physical Located PER, GPE, LOC GPE, LOC
Physical Origin PER, ORG GPE, LOC
ACE
Physical Located PER FAC, LOC, GPE
Physical Near PER, FAC, GPE, LOC FAC, GPE, LOC
Table 1: Comparison of Permitted Relation Arguments for the Physical Type Distinction in the ERE and
ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Part-Whole Subsidiary ORG ORG, GPE
ACE
Part-Whole Geographical FAC, LOC, GPE FAC, LOC, GPE
Part-Whole Subsidiary ORG ORG, GPE
Table 2: Comparison of Permitted Relation Arguments for the Part-Whole Type and Subtype Distinctions
in the ERE and ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Social Business PER PER
Social Family PER PER
Social Membership PER PER
Social Role TTL PER
Social Unspecified PER PER
ACE
Personal-Social Business PER PER
Personal-Social Family PER PER
Personal-Social Lasting-Personal PER PER
Table 3: Comparison of Permitted Relation Arguments for the Social Type and Subtype Distinctions in
the ERE and ACE Guidelines
Relation Type Relation Subtype ARG1 Type ARG2 Type
ERE
Affiliation Employment/Membership PER, ORG,
GPE
ORG, GPE
Affiliation Leadership PER ORG, GPE
ACE
ORG-Affiliation Employment PER ORG, GPE
ORG-Affiliation Ownership PER ORG
ORG-Affiliation Founder PER, ORG ORG, GPE
ORG-Affiliation Student-Alum PER ORG.Educational
ORG-Affiliation Sports-Affiliation PER ORG
ORG-Affiliation Investor-Shareholder PER, ORG,
GPE
ORG, GPE
ORG-Affiliation Membership PER, ORG,
GPE
ORG
Agent-Artifact User-Owner-Inventor-
Manufacturer
PER, ORG,
GPE
FAC
Gen-Affiliation Citizen-Resident-Religion-
Ethnicity
PER PER.Group,
LOC, GPE,
ORG
Gen-Affiliation Org-Location-Origin ORG LOC, GPE
Table 4: Comparison of Permitted Relation Arguments for the Affiliation Type and Subtype Distinctions
in the ERE and ACE Guidelines
48
Clinton met with Yasser Arafat last week would
not be considered a social relation).
As for the differences in affiliation relations,
ACE includes many Subtype possibilities which
can more accurately represent affiliation, whereas
ERE only observes two Affiliation Subtype op-
tions (Table 4).
4 Events in ACE and ERE
Events in both annotation methods are defined as
?specific occurrences?, involving ?specific partic-
ipants? (Linguistic Data Consortium, 2005; Lin-
guistic Data Consortium, 2013b). The primary
goal of Event tagging is to detect and character-
ize events that include tagged entities. The central
Event tagging difference between ACE and ERE
is the level of specificity present in ACE, whereas
ERE tends to collapse tags for a more simplified
approach.
4.1 Event Tagging Similarities
Both annotation schemas annotate the same ex-
act Event Types: LIFE, MOVEMENT, TRANS-
ACTION, BUSINESS, CONFLICT, CONTACT,
PERSONNEL, and JUSTICE events. Both anno-
tation ontologies also include 33 Subtypes for each
Type. Furthermore, both rely on the expression
of an occurrence through the use of a ?Trigger?.
ACE, however, restricts the trigger to be a single
word that most clearly expresses the event occur-
rence (usually a main verb), while ERE allows for
the trigger to be a word or a phrase that instanti-
ates the event (Linguistic Data Consortium, 2005;
Linguistic Data Consortium, 2013b). Both meth-
ods annotate modifiers when they trigger events
as well as anaphors, when they refer to previously
mentioned events. Furthermore, when there is
any ambiguity about which trigger to select, both
methods have similar rules established, such as
the Stand-Alone Noun Rule (In cases where more
than one trigger is possible, the noun that can be
used by itself to refer to the event will be selected)
and the Stand-Alone Adjective Rule (Whenever a
verb and an adjective are used together to express
the occurrence of an Event, the adjective will be
chosen as the trigger whenever it can stand-alone
to express the resulting state brought about by the
Event). Additionally, both annotation guidelines
agree on the following:
? Tagging of Resultative Events (states that re-
sult from taggable Events)
? Nominalized Events are tagged as regular
events
? Reported Events are not tagged
? Implicit events are not tagged
? Light verbs are not tagged
? Coreferential Events are tagged
? Tagging of multi-part triggers (both parts are
tagged only if they are contiguous)
4.2 Event Tagging Differences
One of the more general differences between ERE
and ACE Event tagging is the way in which each
model addresses Event Extent. ACE defines the
extent as always being the ?entire sentence within
which the Event is described? (Linguistic Data
Consortium, 2005). In ERE, the extent is the
entire document unless an event is coreferenced
(in which case, the extent is defined as the ?span
of a document from the first trigger for a par-
ticular event to the next trigger for a particular
event.? This signifies that the span can cross
sentence boundaries). Unlike ACE, ERE does
not delve into indicating Polarity, Tense, Gener-
icity, and Modality. ERE simplifies any anno-
tator confusion engendered by these features by
simply not tagging negative, future, hypotheti-
cal, conditional, uncertain or generic events (al-
though it does tag for past events). While ERE
only tags attested Events, ACE allows for irrealis
events, and includes attributes for marking them
as such: Believed Events; Hypothetical Events;
Commanded and Requested Events; Threatened,
Proposed and Discussed Events; Desired Events;
Promised Events; and Otherwise Unclear Con-
structions. Additionally both ERE and ACE tag
Event arguments as long as the arguments occur
within the event mention extent (another way of
saying that a taggable Event argument will occur
in the same sentence as the trigger word for its
Event). However, ERE and ACE have a diverging
approach to argument tagging:
? ERE is limited to pre-specified arguments for
each event and relation subtype. The pos-
sible arguments for ACE are: Event partici-
pants (limited to pre-specified roles for each
event type); Event-specific attributes that are
associated with a particular event type (e.g.,
the victim of an attack); and General event
attributes that can apply to most or all event
types (e.g., time, place).
49
? ACE tags arguments regardless of modal cer-
tainty of their involvement in the event. ERE
only tags asserted participants in the event.
? The full noun phrase is marked in both ERE
and ACE arguments, but the head is only
specified in ACE. This is because ACE han-
dles entity annotation slightly differently than
ERE does; ACE marks the full noun phrase
with a head word for entity mention, and ERE
treats mentions differently based on their syn-
tactic features (for named or pronominal en-
tity mentions the name or pronominal itself
is marked, whereas for nominal mentions the
full noun phrase is marked).
Event Type and Subtype Differences Both an-
notation methods have almost identical Event
Type and Subtype categories. The only differences
between both are present in the Contact and Move-
ment Event Types.
A minor distinction in Subtype exists as a re-
sult of the types of entities that can be trans-
ported within the Movement Type category. In
ACE, ARTIFACT entities (WEAPON or VEHI-
CLE) as well as PERSON entities can be trans-
ported, whereas in ERE, only PERSON entities
can be transported. The difference between the
Phone-Write and Communicate Subtypes merely
lies in the definition. Both Subtypes are the de-
fault Subtype to cover all Contact events where
a ?face-to-face? meeting between sender and re-
ceiver is not explicitly stated. In ACE, this contact
is limited to written or telephone communication
where at least two parties are specified to make
this event subtype less open-ended. In ERE, this
requirement is simply widened to comprise elec-
tronic communication as well, explicitly including
those via internet channels (e.g., Skype).
5 TAC-KBP
After the final ACE evaluation in 2008 there was
interest in the community to form an evaluation
explicitly focused on knowledge bases (KBs) cre-
ated from the output of extraction systems. NIST
had recently started the Text Analysis Conference
series for related NLP tasks such as Recognizing
Textual Entailment, Summarization, and Question
Answering. In 2009 the first Knowledge Base
Population track (TAC-KBP) was held featuring
two initial tasks: (a) Entity Linking ? linking en-
tities to KB entities, and (b) Slot Filling ? adding
information to entity profiles that is missing from
the KB (McNamee et al., 2010). Due to its gener-
ous license and large scale, a snapshot of English
Wikipedia from late 2008 has been used as the ref-
erence KB in the TAC-KBP evaluations.
5.1 Slot Filling Overview
Unlike ACE and ERE, Slot Filling does not have
as its primary goal the annotation of text. Rather,
the aim is to identify knowledge nuggets about a
focal named entity using a fixed inventory of re-
lations and attributes. For example, given a fo-
cal entity such as former Ukrainian prime minister
Yulia Tymoshenko, the task is to identify attributes
such as schools she attended, occupations, and im-
mediate family members. This is the same sort
of information commonly listed about prominent
people in Wikipedia Infoboxes and in derivative
databases such as FreeBase and DBpedia.
Consequently, Slot Filling is somewhat of a hy-
brid between relation extraction and question an-
swering ? slot fills can be considered as the cor-
rect responses to a fixed set of questions. The rela-
tions and attributes used in the 2013 task are pre-
sented in Table 5.
5.2 Differences with ACE-style relation
extraction
Slot Filling in TAC-KBP differs from extraction in
ACE and ERE in several significant ways:
? information is sought for named entities,
chiefly PERs and ORGs;
? the focus is on values not mentions;
? assessment is more like QA; and,
? events are handled as uncorrelated slots
In traditional IE evaluation, there was an
implicit skew towards highly attested in-
formation such as leader(Bush, US), or
capital(Paris, France). In contrast, TAC-KBP
gives full credit for finding a single instance of a
correct fill instead of every attestation of that fact.
Slot Filling assessment is somewhat simpler
than IE annotation. The assessor must decide
if provenance text is supportive of a posited fact
about the focal entity instead of annotating a doc-
ument with all evidenced relations and events for
any entity. For clarity and to increase assessor
agreement, guidelines have been developed to jus-
tify when a posited relation is deemed adequately
supported from text. Additionally, the problem of
50
Relations Attributes
per:children org:shareholders per:alternate names org:alternate names
per:other family org:founded by per:date of birth org:political religious affiliation
per:parents org:top members employees per:age org:number of employees members
per:siblings org:member of per:origin org:date founded
per:spouse org:members per:date of death org:date dissolved
per:employee or member of org:parents per:cause of death org:website
per:schools attended org:subsidiaries per:title
per:city of birth org:city of headquarters per:religion
per:stateorprovince of birth org:stateorprovince of headquarters per:charges
per:country of birth org:country of headquarters
per:cities of residence
per:statesorprovinces of residence
per:countries of residence
per:city of death
per:stateorprovince of death
per:country of death
Table 5: Relation and attributes for PERs and ORGs.
slot value equivalence becomes an issue - a sys-
tem should be penalized for redundantly asserting
that a person has four children named Tim, Beth,
Timothy, and Elizabeth, or that a person is both a
cardiologist and a doctor.
Rather than explicitly modeling events, TAC-
KBP created relations that capture events, more
in line with the notion of Infobox filling or ques-
tion answering (McNamee et al., 2010). For exam-
ple, instead of a criminal event, there is a slot fill
for charges brought against an entity. Instead of a
founding event, there are slots like org:founded by
(who) and org:date founded (when). Thus a state-
ment that ?Jobs is the founder and CEO of Apple?
is every bit as useful for the org:founded by rela-
tion as ?Jobs founded Apple in 1976.? even though
the date is not included in the former sentence.
5.3 Additional tasks
Starting in 2012 TAC-KBP introduced the ?Cold
Start? task, which is to literally produce a KB
based on the Slot Filling schema. To date, Cold
Start KBs have been built from collections of
O(50,000) documents, and due to their large size,
they are assessed by sampling. There is also
an event argument detection evaluation in KBP
planned for 2014.
Other TAC-KBP tasks have been introduced in-
cluding determining the timeline when dynamic
slot fills are valid (e.g., CEO of Microsoft), and
targeted sentiment.
6 FrameNet
The FrameNet project has rather different moti-
vations than either ACE/ERE or TAC-KBP, but
shares with them a goal of capturing informa-
tion about events and relations in text. FrameNet
stems from Charles Fillmore?s linguistic and lex-
icographic theory of Frame Semantics (Fillmore,
1976; Fillmore, 1982). Frames are descriptions
of event (or state) types and contain information
about event participants (frame elements), infor-
mation as to how event types relate to each other
(frame relations), and information about which
words or multi-word expressions can trigger a
given frame (lexical units).
FrameNet is designed with text annotation in
mind, but unlike ACE/ERE it prioritizes lexico-
graphic and linguistic completeness over ease of
annotation. As a result Frames tend to be much
finer grained than ACE/ERE events, and are more
numerous by an order of magnitude. The Berkeley
FrameNet Project (Baker et al., 1998) was devel-
oped as a machine readable database of distinct
frames and lexical units (words and multi-word
constructions) that were known to trigger specific
frames.
1
FrameNet 1.5 includes 1020 identified
frames and 11830 lexical units.
One of the most widespread uses of FrameNet
has been as a resource for Semantic Role Label-
ing (SRL) (Gildea and Jurafsky, 2002). FrameNet
related SRL was promoted as a task by the
SENSEVAL-3 workshop (Litkowski, 2004), and
the SemEval-2007 workshop (Baker et al., 2007).
(Das et al., 2010) is a current system for automatic
FrameNet annotation.
The relation and attribute types of TAC-KBP
and the relation and event types in the ACE/ERE
standards can be mapped to FrameNet frames.
The mapping is complicated by two factors.
The first is that FrameNet frames are gener-
ally more fine-grained than the ACE/ERE cate-
gories. As a result the mapping is sometimes
one-to-many. For example, the ERE relation Af-
1
This database is accessible via webpage (https:
//framenet.icsi.berkeley.edu/fndrupal/)
and as a collection of XML files by request.
51
Relations
FrameNet ACE ERE TAC-KBP
Kinship Personal-Social.Family Social.Family per:children
per:other family
per:parents
per:siblings
per:spouse
Being Employed ORG-Affiliation.Employment Affiliation.Employment/Membership per:employee or member of
Membership org:member of
Being Located Physical.Located Physical.Located org:city of headquarters
org:stateorprovince of headquarters
org:country of headquarters
Events
FrameNet ACE ERE
Contacting Phone-Write Communicate
Extradition Justice-Extradition Justice-Extradition
Attack Conflict-Attack Conflict-Attack
Being Born Life-Be Born Life-Be Born
Attributes
FrameNet TAC-KBP
Being Named per:alternate names
Age per:age
Table 6: Rough mappings between subsets of FrameNet, ACE, ERE, and TAC-KBP
filiation.Employment/Membership covers both
the Being Employed frame and the Member-
ship frame. At the same time, while TAC-
KBP has only a handful of relations relative to
FrameNet, some of these relations are more fine-
grained than the analogous frames or ACE/ERE
relations. For example, the frame Kinship, which
maps to the single ERE relation Social.Family,
maps to five TAC-KBP relations, and the Be-
ing Located, which maps to the ACE/ERE rela-
tion Being.Located, maps to three TAC-KBP re-
lations. Rough mappings from a selection of rela-
tions, events, and attributes are given in Table 6.
The second complication arises from the fact
that FrameNet frames are more complex objects
than ERE/ACE events, and considerably more
complex than TAC-KBP relations. Rather than the
two entities related via a TAC-KBP or ACE/ERE
relation, some frames have upwards of 20 frame
elements. Table 7 shows in detail the mapping be-
tween frame elements in the Extradition frame and
ACE?s and ERE?s Justice-Extradition events. The
?core? frame elements map exactly to the ERE
event, the remaining two arguments in the ACE
event map to two non-core frame elements, and
the frame includes several more non-core elements
with no analogue in either ACE or ERE standards.
7 Conclusion
The ACE and ERE annotation schemas have
closely related goals of identifying similar in-
formation across various possible types of docu-
ments, though their approaches differ due to sepa-
rate goals regarding scope and replicability. ERE
differs from ACE in collapsing different Type dis-
tinctions and in removing annotation features in
order to eliminate annotator confusion and to im-
FrameNet ACE ERE
Authorities Agent-Arg Agent-Arg
Crime jursidiction Destination-Arg Destination-Arg
Current jursidiction Origin-Arg Origin-Arg
Suspect Person-Arg Person-Arg
Reason Crime-Arg
Time Time-Arg
Legal Basis
Manner
Means
Place
Purpose
Depictive
Table 7: Mapping between frame elements of Ex-
tradition (FrameNet), and arguments of Justice-
Extradition (ACE/ERE): A line divides core frame
elements (above) from non-core (below).
prove consistency, efficiency, and higher inter-
annotator agreement. TAC-KPB slot-filling shares
some goals with ACE/ERE, but is wholly fo-
cused on a set collection of questions (slots to
be filled) concerning entities to the extent that
there is no explicit modeling of events. At the
other extreme, FrameNet seeks to capture the
full range of linguistic and lexicographic varia-
tion in event representations in text. In general, all
events, relations, and attributes that can be repre-
sented by ACE/ERE and TAC-KBP standards can
be mapped to FrameNet representations, though
adjustments need to be made for granularity of
event/relation types and granularity of arguments.
Acknowledgements
This material is partially based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
52
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics-Volume 1, pages 86?90. Associ-
ation for Computational Linguistics.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic
structure extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99?104, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Proceedings of NAACL-HLT, pages 948?
956. Association for Computational Linguistics.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lancec Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ace) program- tasks, data, and evaluation. In
Proceedings of LREC 2004: Fourth International
Conference on Language Resources and Evaluation,
Lisbon, May 24-30.
Charles J Fillmore. 1976. Frame semantics and the na-
ture of language. Annals of the New York Academy
of Sciences, 280(1):20?32.
Charles Fillmore. 1982. Frame semantics. In Linguis-
tics in the morning calm, pages 111?137. Hanshin
Publishing Co.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Linguistic Data Consortium. 2005. ACE (automatic
content extraction) English annotation guidelines
for events. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 5.4.3 2005.07.01.
Linguistic Data Consortium. 2006. ACE (automatic
content extraction) English annotation guidelines
for entities. https://www.ldc.upenn.edu/
collaborations/past-projects/ace,
Version 5.6.6 2006.08.01.
Linguistic Data Consortium. 2008. ACE (automatic
content extraction) English annotation guidelines for
relations. https://www.ldc.upenn.edu/
collaborations/past-projects/ace.
Version 6.0 2008.01.07.
Linguistic Data Consortium. 2013a. DEFT ERE anno-
tation guidelines: Entities v1.1, 05.17.2013.
Linguistic Data Consortium. 2013b. DEFT ERE anno-
tation guidelines: Events v1.1. 05.17.2013.
Linguistic Data Consortium. 2013c. DEFT ERE anno-
tation guidelines: Relations v1.1. 05.17.2013.
Ken Litkowski. 2004. Senseval-3 task: Automatic
labeling of semantic roles. In Rada Mihalcea and
Phil Edmonds, editors, Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9?12, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
Paul McNamee, Hoa Trang Dang, Heather Simpson,
Patrick Schone, and Stephanie Strassel. 2010. An
evaluation of technologies for knowledge base pop-
ulation. In Proceedings of LREC.
53
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 54?58,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Is the Stanford Dependency Representation Semantic?
Rachel Rudinger
1
and Benjamin Van Durme
1,2
Center for Language and Speech Processing
1
Human Language Technology Center of Excellence
2
Johns Hopkins University
rudinger@jhu.edu, vandurme@cs.jhu.edu
Abstract
The Stanford Dependencies are a deep
syntactic representation that are widely
used for semantic tasks, like Recognizing
Textual Entailment. But do they capture
all of the semantic information a meaning
representation ought to convey? This pa-
per explores this question by investigating
the feasibility of mapping Stanford depen-
dency parses to Hobbsian Logical Form,
a practical, event-theoretic semantic rep-
resentation, using only a set of determin-
istic rules. Although we find that such a
mapping is possible in a large number of
cases, we also find cases for which such a
mapping seems to require information be-
yond what the Stanford Dependencies en-
code. These cases shed light on the kinds
of semantic information that are and are
not present in the Stanford Dependencies.
1 Introduction
The Stanford dependency parser (De Marneffe et
al., 2006) provides ?deep? syntactic analysis of
natural language by layering a set of hand-written
post-processing rules on top of Stanford?s sta-
tistical constituency parser (Klein and Manning,
2003). Stanford dependency parses are commonly
used as a semantic representation in natural lan-
guage understanding and inference systems.
1
For
example, they have been used as a basic meaning
representation for the Recognizing Textual Entail-
ment task proposed by Dagan et al. (2005), such as
by Haghighi et al. (2005) or MacCartney (2009)
and in other inference systems (Chambers et al.,
2007; MacCartney, 2009).
Because of their popular use as a semantic rep-
resentation, it is important to ask whether the Stan-
ford Dependencies do, in fact, encode the kind of
1
Statement presented by Chris Manning at the
*SEM 2013 Panel on Language Understanding
http://nlpers.blogspot.com/2013/07/the-sem-2013-panel-
on-language.html.
information that ought to be present in a versa-
tile semantic form. This paper explores this ques-
tion by attempting to map the Stanford Depen-
dencies into Hobbsian Logical Form (henceforth,
HLF), a neo-Davidsonian semantic representation
designed for practical use (Hobbs, 1985). Our ap-
proach is to layer a set of hand-written rules on
top of the Stanford Dependencies to further trans-
form the representation into HLFs. This approach
is a natural extension of the Stanford Dependen-
cies which are, themselves, derived from manually
engineered post-processing routines.
The aim of this paper is neither to demonstrate
the semantic completeness of the Stanford Depen-
dencies, nor to exhaustively enumerate their se-
mantic deficiencies. Indeed, to do so would be to
presuppose HLF as an entirely complete seman-
tic representation, or, a perfect semantic standard
against which to compare the Stanford Dependen-
cies. We make no such claim. Rather, our intent is
to provide a qualitative discussion of the Stanford
Dependencies as a semantic resource through the
lens of this HLF mapping task. It is only necessary
that HLF capture some subset of important seman-
tic phenomena to make this exercise meaningful.
Our results indicate that in a number of cases,
it is, in fact, possible to directly derive HLFs from
Stanford dependency parses. At the same time,
however, we also find difficult-to-map phenomena
that reveal inherent limitations of the dependen-
cies as a meaning representation.
2 Background
This section provides a brief overview of the HLF
and Stanford dependency formalisms.
2.1 Hobbsian Logical Form
The key insight of event-theoretic semantic repre-
sentations is the reification of events (Davidson,
1967), or, treating events as entities in the world.
As a logical, first-order representation, Hobbsian
54
Logical Form (Hobbs, 1985) employs this ap-
proach by allowing for the reification of any pred-
icate into an event variable. Specifically, for any
predicate p(x
1
, ? ? ? , x
n
), there is a corresponding
predicate, p
?
(E, x
1
, ? ? ? , x
n
), where E refers to
the predicate (or event) p(x
1
, ? ? ? , x
n
). The reified
predicates are related to their non-reified forms
with the following axiom schema:
(?x
1
? ? ?x
n
)p(x
1
? ? ?x
n
) ? (?e)Exist(e) ?
p
?
(e, x
1
? ? ?x
n
)
In HLF, ?A boy runs? would be represented as:
(?e, x)Exist(e) ? run
?
(e, x) ? boy(x)
and the sentence ?A boy wants to build a boat
quickly? (Hobbs, 1985) would be represented as:
(?e
1
, e
2
, e
3
, x, y)Exist(e
1
) ? want
?
(e
1
, x, e
2
) ?
quick
?
(e
2
, e
3
)?build
?
(e
3
, x, y)?boy(x)?boat(y)
2.2 Stanford Dependencies
A Stanford dependency parse is a set of triples
consisting of two tokens (a governor and a depen-
dent), and a labeled syntactic or semantic relation
between the two tokens. Parses can be rendered
as labeled, directed graphs, as in Figure 1. Note
that this paper assumes the collapsed version of
the Stanford Dependencies.
2
Figure 1: Dependency parse of ?A boy wants to
build a boat quickly.?
3 Mapping to HLF
We describe in this section our deterministic algo-
rithm for mapping Stanford dependency parses to
HLF. The algorithm proceeds in four stages: event
2
The collapsed version is more convenient for our pur-
poses, but using the uncollapsed version would not signifi-
cantly affect our results.
extraction, argument identification, predicate-
argument assignment, and formula construction.
We demonstrate these steps on the above example
sentence ?A boy wants to build a boat quickly.?
3
The rule-based algorithm operates on the sen-
tence level and is purely a function of the depen-
dency parse or other trivially extractible informa-
tion, such as capitalization.
3.1 Event Extraction
The first step is to identify the set of event predi-
cates that will appear in the final HLF and assign
an event variable to each. Most predicates are gen-
erated by a single token in the sentence (e.g., the
main verb). For each token t in the sentence, an
event (e
i
, p
t
) (where e
i
is the event variable and p
t
is the predicate) is added to the set of events if any
of the following conditions are met:
1. t is the dependent of the relation root,
ccomp, xcomp, advcl, advmod, or
partmod.
2. t is the governor of the relation nsubj, dobj,
ccomp, xcomp, xsubj, advcl, nsubjpass,
or agent.
Furthermore, an event (e
i
, p
r
) is added for any
triple (rel, gov, dep) where rel is prefixed with
?prep ? (e.g., prep to, prep from, prep by, etc.).
Applying this step to our example sentence ?A
boy wants to build a boat quickly.? yields the fol-
lowing set:
(e
1
, wants), (e
2
, quickly), (e
3
, build)
3.2 Argument Identification
Next, the set of entities that will serve as predicate
arguments are identified. Crucially, this set will
include some event variables generated in the pre-
vious step. For each token, t, an argument (x
i
, t)
is added to the set of arguments if one of the fol-
lowing conditions is met:
1. t is the dependent of the relation nsubj,
xsubj, dobj, ccomp, xcomp, nsubjpass,
agent, or iobj.
2. t is the governor of the relation advcl,
advmod, or partmod.
3
Hobbs (1985) uses the example sentence ?A boy wanted
to build a boat quickly.?
55
Applying this step to our example sentence, we
get the following argument set:
(x
1
, boat), (x
2
, build), (x
3
, boy)
Notice that the token build has generated both
an event predicate and an argument. This is be-
cause in our final HLF, build will be both an event
predicate that takes the arguments boy and boat,
as well as an argument to the intensional predicate
want.
3.3 Predicate-Argument Assignment
In this stage, arguments are assigned to each pred-
icate. p
t
.arg
i
denotes the i
th
argument of pred-
icate p
t
and arg(t) denotes the argument associ-
ated with token t. For example, arg(boy) = x
2
and arg(quickly) = e
3
. We also say that if the
token t
1
governs t
2
by some relation, e.g. nsubj,
then t
1
nsubj-governs t
2
, or t
2
nsubj-depends on
t
1
. Note that arg
i
refers to any slot past arg
2
. Ar-
guments are assigned as follows.
For each predicate p
t
(corresponding to token
t):
1. If there is a token t
?
such that t nsubj-,
xsubj-, or agent-governs t
?
, then p
t
.arg
1
=
arg(t
?
).
2. If there is a token t
?
such that t dobj-governs
t
?
, then p
t
.arg
2
= arg(t
?
).
3. If there is a token t
?
such that t nsubjpass-
governs t
?
, then p
t
.arg
i
= arg(t
?
).
4. If there is a token t
?
such that t partmod-
depends on t
?
, then p
t
.arg
2
= arg(t
?
).
5. If there is a token t
?
such that t iobj-governs
t
?
, then p
t
.arg
i
= arg(t
?
).
6. If there is a token t
?
such that t ccomp- or
xcomp-governs t
?
, then p
t
.arg
i
= arg(t
?
)
(a) UNLESS there is a token t
??
such that
t
?
advmod-governs t
??
, in which case
p
t
.arg
i
= arg(t
??
).
7. If there is a token t
?
such that t advmod- or
advcl-depends on t
?
, then p
t
.arg
i
= arg(t
?
).
And for each p
r
generated from relation
(rel, gov, dep) (i.e. all of the ?prep ? relations):
1. p
r
.arg
1
= arg(gov)
2. p
r
.arg
i
= arg(dep)
After running this stage on our example sen-
tence, the predicate-argument assignments are as
follows:
wants(x
3
, e
2
), build(x
3
, x
1
), quickly(e
3
)
Each predicate can be directly replaced with its
reified forms (i.e., p
?
):
wants
?
(e
1
, x
3
, e
2
),build
?
(e
3
, x
3
, x
1
),
quickly
?
(e
2
, e
3
)
Two kinds of non-eventive predicates still need
to be formed. First, every entity (x
i
, t) that is
neither a reified event nor a proper noun, e.g.,
(x
3
, boy), generates a predicate of the form t(x
i
).
Second, we generate Hobbs?s Exist predicate,
which identifies which event actually occurs in the
?real world.? This is simply the event generated
by the dependent of the root relation.
3.4 Formula Construction
In this stage, the final HLF is pieced together. We
join all of the predicates formed above with the
and conjunction, and existentially quantify over
every variable found therein. For our example sen-
tence, the resulting HLF is:
A boy wants to build a boat quickly.
(?e
1
, e
2
, e
3
, x
1
, x
3
)[Exist(e
1
) ? boat(x
1
) ?
boy(x
3
) ? wants
?
(e
1
, x
3
, e
2
) ? build
?
(e
3
, x
3
, x
1
)
? quickly
?
(e
2
, e
3
)]
4 Analysis of Results
This section discusses semantic phenomena that
our mapping does and does not capture, providing
a lens for assessing the usefulness of the Stanford
Dependencies as a semantic resource.
4.1 Successes
Formulas 1-7 are correct HLFs that our mapping
rules successfully generate. They illustrate the di-
versity of semantic information that is easily re-
coverable from Stanford dependency parses.
Formulas 1-2 show successful parses in sim-
ple transitive sentences with active/passive alter-
nations, and Formula 3 demonstrates success in
parsing ditransitives. Also easily recovered from
the dependency structures are semantic parses of
sentences with adverbs (Formula 4) and reporting
verbs (Formula 5). Lest it appear that these phe-
nomena may only be handled in isolation, Equa-
tions 6-7 show successful parses for sentences
56
with arbitrary combinations of the above phenom-
ena.
A boy builds a boat.
(?e
1
, x
1
, x
2
)[Exist(e
1
) ? boy(x
2
) ? boat(x
1
)
? builds
?
(e
1
, x
2
, x
1
)]
(1)
A boat was built by a boy.
(?e
1
, x
1
, x
2
)[Exist(e
1
) ? boat(x
2
) ? boy(x
1
)
? built
?
(e
1
, x
1
, x
2
)]
(2)
John gave Mary a boat.
(?e
1
, x
1
)[Exist(e
1
) ? boat(x
1
)
? gave
?
(e
1
, John, x
1
,Mary)]
(3)
John built a boat quickly.
OR John quickly built a boat.
(?e
1
, e
2
, x
1
)[Exist(e
1
) ? boat(x
1
) ?
quickly(e
2
, e
1
) ? built
?
(e
1
, John, x
1
)]
(4)
John told Mary that a boy built a boat.
(?e
1
, e
2
, x
1
, x
4
)[Exist(e
1
)?boy(x
1
)?boat(x
4
)?
built
?
(e
2
, x
1
, x
4
) ? told
?
(e
1
, John,Mary, e
2
)]
(5)
John told Mary that Sue told Joe
that Adam loves Eve.
(?e
1
, e
2
, e
3
)[Exist(e
1
)?told
?
(e
2
, Sue, Joe, e
3
)?
loves
?
(e
3
, Adam,Eve) ?
told
?
(e
1
, John,Mary, e
2
)]
(6)
John was told by Mary that Sue wants
Joe to build a boat quickly.
(?e
1
, e
2
, e
3
, e
4
, x
7
)[Exist(e
1
) ? boat(x
7
) ?
build
?
(e
2
, Joe, x
7
)?told
?
(e
1
,Mary, John, e
4
)?
wants
?
(e
4
, Sue, e
3
) ? quickly
?
(e
3
, e
2
)]
(7)
4.2 Limitations
Though our mapping rules enable us to directly ex-
tract deep semantic information directly from the
Stanford dependency parses in the above cases,
there are a number of difficulties with this ap-
proach that shed light on inherent limitations of
the Stanford Dependencies as a semantic resource.
A major such limitation arises in cases of event
nominalizations. Because dependency parses are
syntax-based, their structures do not distinguish
between eventive noun phrases like ?the bombing
of the city? and non-eventive ones like ?the mother
of the child?; such a distinction, however, would
be found in the corresponding HLFs.
Certain syntactic alternations also prove prob-
lematic. For example, the dependency structure
does not recognize that ?window? takes the same
semantic role in the sentences ?John broke the mir-
ror.? and ?The mirror broke.? The use of addi-
tional semantic resources, like PropBank (Palmer
et al., 2005), would be necessary to determine this.
Prepositional phrases present another problem
for our mapping task, as the Stanford dependen-
cies will typically not distinguish between PPs
indicating arguments and adjuncts. For exam-
ple, ?Mary stuffed envelopes with coupons? and
?Mary stuffed envelopes with John? have identical
dependency structures, yet ?coupons? and ?John?
are (hopefully for John) taking on different seman-
tic roles. This is, in fact, a prime example of how
Stanford dependency parses may resolve syntactic
ambiguity without resolving semantic ambiguity.
Of course, one might manage more HLF cov-
erage by adding more rules to our system, but the
limitations discussed here are fundamental. If two
sentences have different semantic interpretations
but identical dependency structures, then there can
be no deterministic mapping rule (based on depen-
dency structure alone) that yields this distinction.
5 Conclusion
We have presented here our attempt to map the
Stanford Dependencies to HLF via a second layer
of hand-written rules. That our mapping rules,
which are purely a function of dependency struc-
ture, succeed in producing correct HLFs in some
cases is good evidence that the Stanford Depen-
dencies do contain some practical level of seman-
tic information. Nevertheless, we were also able to
quickly identify aspects of meaning that the Stan-
ford Dependencies did not capture.
Our argument does not require that HLF be an
optimal representation, only that it capture worth-
while aspects of semantics and that it not be read-
ily derived from the Stanford representation. This
is enough to conclude that the Stanford Dependen-
cies are not complete as a meaning representation.
While not surprising (as they are intended as a
syntactic representation), we hope this short study
will help further discussion on what the commu-
nity wants or needs in a meaning representation:
what gaps are acceptable, if any, and whether a
more ?complete? representation is needed.
Acknowledgments
This material is partially based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
57
References
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing, pages 165?170. Associa-
tion for Computational Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entail-
ment.
Donald Davidson. 1967. The logical form of action
sentences. In The Logic of Decision and Action,
pages 81?120. Univ. of Pittsburgh Press.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Aria D Haghighi, Andrew Y Ng, and Christopher D
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 387?394.
Association for Computational Linguistics.
Jerry R Hobbs. 1985. Ontological promiscuity. In
Proceedings of the 23rd annual meeting on Associ-
ation for Computational Linguistics, pages 60?69.
Association for Computational Linguistics.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Bill MacCartney. 2009. Natural language inference.
Ph.D. thesis, Stanford University.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
58
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1?11,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Efficient Elicitation of Annotations for Human Evaluation of Machine
Translation
Keisuke Sakaguchi
?
, Matt Post
?
, Benjamin Van Durme
?
?
Center for Language and Speech Processing
?
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland
{keisuke,post,vandurme}@cs.jhu.edu
Abstract
A main output of the annual Workshop
on Statistical Machine Translation (WMT)
is a ranking of the systems that partici-
pated in its shared translation tasks, pro-
duced by aggregating pairwise sentence-
level comparisons collected from human
judges. Over the past few years, there
have been a number of tweaks to the ag-
gregation formula in attempts to address
issues arising from the inherent ambigu-
ity and subjectivity of the task, as well as
weaknesses in the proposed models and
the manner of model selection.
We continue this line of work by adapt-
ing the TrueSkill
TM
algorithm ? an online
approach for modeling the relative skills
of players in ongoing competitions, such
as Microsoft?s Xbox Live ? to the hu-
man evaluation of machine translation out-
put. Our experimental results show that
TrueSkill outperforms other recently pro-
posed models on accuracy, and also can
significantly reduce the number of pair-
wise annotations that need to be collected
by sampling non-uniformly from the space
of system competitions.
1 Introduction
The Workshop on Statistical Machine Translation
(WMT) has long been a central event in the ma-
chine translation (MT) community for the evalua-
tion of MT output. It hosts an annual set of shared
translation tasks focused mostly on the translation
of western European languages. One of its main
functions is to publish a ranking of the systems
for each task, which are produced by aggregating
a large number of human judgments of sentence-
level pairwise rankings of system outputs. While
the performance on many automatic metrics is also
# score range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.571 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
Table 1: System rankings presented as clusters
(WMT13 French-English competition). The score
column is the percentage of time each system was
judged better across its comparisons (?2.1).
reported (e.g., BLEU (Papineni et al., 2002)), the
human evaluation is considered primary, and is in
fact used as the gold standard for its metrics task,
where evaluation metrics are evaluated.
In machine translation, the longstanding dis-
agreements about evaluation measures do not go
away when moving from automatic metrics to hu-
man judges. This is due in no small part to the in-
herent ambiguity and subjectivity of the task, but
also arises from the particular way that the WMT
organizers produce the rankings. The system-
level rankings are produced by collecting pairwise
sentence-level comparisons between system out-
puts. These are then aggregated to produce a com-
plete ordering of all systems, or, more recently, a
partial ordering (Koehn, 2012), with systems clus-
tered where they cannot be distinguished in a sta-
tistically significant way (Table 1, taken from Bo-
jar et al. (2013)).
A number of problems have been noted with
this approach. The first has to do with the na-
ture of ranking itself. Over the past few years, the
WMT organizers have introduced a number of mi-
nor tweaks to the ranking algorithm (?2) in reac-
tion to largely intuitive arguments that have been
1
raised about how the evaluation is conducted (Bo-
jar et al., 2011; Lopez, 2012). While these tweaks
have been sensible (and later corroborated), Hop-
kins and May (2013) point out that this is essen-
tially a model selection task, and should prop-
erly be driven by empirical performance on held-
out data according to some metric. Instead of in-
tuition, they suggest perplexity, and show that a
novel graphical model outperforms existing ap-
proaches on that metric, with less amount of data.
A second problem is the deficiency of the mod-
els used to produce the ranking, which work by
computing simple ratios of wins (and, option-
ally, ties) to losses. Such approaches do not con-
sider the relative difficulty of system matchups,
and thus leave open the possibility that a system
is ranked highly from the luck of comparisons
against poorer opponents.
Third, a large number of judgments need to be
collected in order to separate the systems into clus-
ters to produce a partial ranking. The sheer size of
the space of possible comparisons (all pairs of sys-
tems times the number of segments in the test set)
requires sampling from this space and distributing
the annotations across a number of judges. Even
still, the number of judgments needed to produce
statistically significant rankings like those in Ta-
ble 1 grows quadratically in the number of par-
ticipating systems (Koehn, 2012), often forcing
the use of paid, lower-quality annotators hired on
Amazon?s Mechanical Turk. Part of the prob-
lem is that the sampling strategy collects data uni-
formly across system pairings. Intuitively, we
should need many fewer annotations between sys-
tems with divergent base performance levels, in-
stead focusing the collection effort on system pairs
whose performance is more matched, in order to
tease out the gaps between similarly-performing
systems. Why spend precious human time on re-
dundantly affirming predictable outcomes?
To address these issues, we developed a varia-
tion of the TrueSkill model (Herbrich et al., 2006),
an adaptative model of competitions originally de-
veloped for the Xbox Live online gaming commu-
nity. It assumes that each player?s skill level fol-
lows a Gaussian distribution N (?, ?
2
), in which
? represents a player?s mean performance, and ?
2
the system?s uncertainty about its current estimate
of this mean. These values are updated after each
?game? (in our case, the value of a ternary judg-
ment) in proportion to how surprising the outcome
is. TrueSkill has been adapted to a number of
areas, including chess, advertising, and academic
conference management.
The rest of this paper provides an empirical
comparison of a number of models of human eval-
uation (?2). We evaluate on perplexity and also
on accuracy, showing that the two are not always
correlated, and arguing for the primacy of the lat-
ter (?3). We find that TrueSkill outperforms other
models (?4). Moreover, TrueSkill also allows us to
drastically reduce the amount of data that needs to
be collected by sampling non-uniformly from the
space of all competitions (?5), which also allows
for greater separation of the systems into ranked
clusters (?6).
2 Models
Before introducing our adaptation of the TrueSkill
model for ranking translation systems with human
judgments (?2.3), we describe two comparisons:
the ?Expected Wins? model used in recent evalu-
ations, and the Bayesian model proposed by Hop-
kins and May (?2.2).
As we described briefly in the introduction,
WMT produces system rankings by aggregating
sentence-level ternary judgments of the form:
(i, S
1
, S
2
, pi)
where i is the source segment (id), S
1
and S
2
are the system pair drawn from a set of systems
{S}, and pi ? {<,>,=} denotes whether the
first system was judged to be better than, worse
than, or equivalent to the second. These ternary
judgments are obtained by presenting judges with
a randomly-selected input sentence and the refer-
ence, followed by five randomly-selected transla-
tions of that sentence. Annotators are asked to
rank these systems from best (rank 1) to worst
(rank 5), ties permitted, and with no meaning as-
cribed to the absolute values or differences be-
tween ranks. This is done to accelerate data collec-
tion, since it yields ten pairwise comparisons per
ranking. Tens of thousands of judgments of this
form constitute the raw data used to compute the
system-level rankings. All the work described in
this section is computed over these pairwise com-
parisons, which are treated as if they were col-
lected independently.
2.1 Expected Wins
The ?Expected Wins? model computes the per-
centage of times that each system wins in its
2
pairwise comparisons. Let A be the complete
set of annotations or judgments of the form
{i, S
1
, S
2
, pi
R
}. We assume these judgments have
been converted into a normal form where S
1
is ei-
ther the winner or is tied with S
2
, and therefore
pi
R
? {<,=}. Let ?(x, y) be the Kronecker delta
function.
1
We then define the function:
wins(S
i
, S
j
) =
|A|
?
n=1
?(S
i
, S
(n)
1
)?(S
j
, S
(n)
2
)?(pi
(n)
R
, <)
which counts the number of annotations for which
system S
i
was ranked better than system S
j
. We
define a single-variable version that marginalizes
over all annotations:
wins(S
i
) =
?
S
j
6=S
i
wins(S
i
, S
j
)
We also define analogous functions for loses and
ties. Until the WMT11 evaluation (Callison-Burch
et al., 2011), the score for each system S
i
was
computed as follows:
score(S
i
) =
wins(S
i
) + ties(S
i
)
wins(S
i
) + ties(S
i
) + loses(S
i
)
Bojar et al. (2011) suggested that the inclusion of
ties biased the results, due to their large numbers,
the underlying similarity of many of the models,
and the fact that they are counted for both systems
in the tie, and proposed the following modified
scoring function:
score(S
i
) =
1
|{S}|
?
S
j
6=S
i
wins(S
i
, S
j
)
wins(S
i
, S
j
) + wins(S
j
, S
i
)
This metric computes an average relative fre-
quency of wins, excluding ties, and was used
in WMT12 and WMT13 (Callison-Burch et al.,
2012; Bojar et al., 2013).
The decision to exclude ties isn?t without
its problems; for example, an evaluation where
two systems are nearly always judged equivalent
should be relevant in producing the final ranking
of systems. Furthermore, as Hopkins and May
(2013) point out, throwing out data to avoid bi-
asing a model suggests a problem with the model.
We now turn to a description of their model, which
addresses these problems.
1
?(x, y) =
{
1 if x = y
0 o.w.
2.2 The Hopkins and May (2013) model
Recent papers (Koehn, 2012; Hopkins and May,
2013) have proposed models focused on the rel-
ative ability of the competition systems. These
approaches assume that each system has a mean
quality represented by a Gaussian distribution with
a fixed variance shared across all systems. In the
graphical model formulation of Hopkins and May
(2013), the pairwise judgments (i, S
1
, S
2
, pi) are
imagined to have been generated according to the
following process:
? Select a source sentence i
? Select two systems S
1
and S
2
. A system
S
j
is associated with a Gaussian distribution
N (?
S
j
, ?
2
a
), samples from which represent
the quality of translations
? Draw two ?translations?, adding random
Gaussian noise with variance ?
2
obs
to simulate
the subjectivity of the task and the differences
among annotators:
q
1
? N (?
S
1
, ?
2
a
) +N (0, ?
2
obs
)
q
2
? N (?
S
2
, ?
2
a
) +N (0, ?
2
obs
)
? Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating pi
according to:
2
pi =
?
?
?
< q
1
? q
2
> d
> q
2
? q
1
> d
= otherwise
The task is to then infer the posterior parameters,
given the data: the system means ?
S
j
and, by ne-
cessity, the latent values {q
i
} for each of the pair-
wise comparison training instances. Hopkins and
May do not publish code or describe details of this
algorithm beyond mentioning Gibbs sampling, so
we used our own implementation,
3
and describe it
here for completeness.
After initialization, we have training instances
of the form (i, S
1
, S
2
, pi
R
, q
1
, q
2
), where all but the
q
i
are observed. At a high level, the sampler iter-
ates over the training data, inferring values of q
1
and q
2
for each annotation together in a single step
of the sampler from the current values of the sys-
tems means, {?
j
}.
4
At the end of each iteration,
2
Note that better systems have higher relative abilities
{?
S
j
}. Better translations subsequently have on-average
higher values {q
i
}, which translate into a lower ranking pi.
3
github.com/keisks/wmt-trueskill
4
This worked better than a version of the sampler that
changed one at a time.
3
these means are then recomputed by re-averaging
all values of {q
i
} associated with that system. Af-
ter the burn-in period, the ?s are stored as samples,
which are averaged when the sampling concludes.
During each iteration, q
1
and q
2
are resampled
from their corresponding system means:
q
1
? N (?
S
1
, ?
2
a
)
q
2
? N (?
S
2
, ?
2
a
)
We then update these values to respect the annota-
tion pi as follows. Let t = q
1
?q
2
(S
1
is the winner
by human judgments), and ensure that the values
are outside the decision radius, d:
q
?
1
=
{
q
1
t ? d
q
1
+
1
2
(d? t) otherwise
q
?
2
=
{
q
2
t ? d
q
2
?
1
2
(d? t) otherwise
In the case of a tie:
q
?
1
=
?
?
?
?
?
?
?
?
?
q
1
+
1
2
(d? t) t ? d
q
1
t < d
q
1
+
1
2
(?d? t) t ? ?d
q
?
2
=
?
?
?
?
?
?
?
?
?
q
2
?
1
2
(d? t) t ? d
q
2
t < d
q
2
?
1
2
(?d? t) t ? ?d
These values are stored for the current iteration
and averaged at its end to produce new estimates
of the system means. The quantity d? t can be in-
terpreted as a loss function, returning a high value
when the observed outcome is unexpected and a
low value otherwise (Figure 1).
2.3 TrueSkill
Prior to 2012, the WMT organizers included refer-
ence translations among the system comparisons.
These were used as a control against which the
evaluators could be measured for consistency, on
the assumption that the reference was almost al-
ways best. They were also included as data points
in computing the system ranking. Another of
Bojar et al. (2011)?s suggestions was to exclude
this data, because systems compared more of-
ten against the references suffered unfairly. This
can be further generalized to the observation that
not all competitions are equal, and a good model
should incorporate some notion of ?match diffi-
culty? when evaluating system?s abilities. The
inference procedure above incorporates this no-
tion implicitly in the inference procedure, but the
model itself does not include a notion of match
difficulty or outcome surprisal.
A model that does is TrueSkill
5
(Herbrich et al.,
2006). TrueSkill is an adaptive, online system that
also assumes that each system?s skill level follows
a Gaussian distribution, maintaining a mean ?
S
j
for each system S
j
representing its current esti-
mate of that system?s native ability. However, it
also maintains a per-system variance, ?
2
S
j
, which
represents TrueSkill?s uncertainty about its esti-
mate of each mean. After an outcome is observed
(a game in which the result is a win, loss, or draw),
the size of the updates is proportional to how sur-
prising the outcome was, which is computed from
the current system means and variances. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates.
Before defining the update equations, we need
to be more concrete about how this notion of sur-
prisal is incorporated. Let t = ?
S
1
? ?
S
2
, the dif-
ference in system relative abilities, and let  be a
fixed hyper-parameter corresponding to the earlier
decision radius. We then define two loss functions
of this difference for wins and for ties:
v
win
(t, ) =
N (?+ t)
?(?+ t)
v
tie
(t, ) =
N (?? t)?N (? t)
?(? t)? ?(?? t)
where ?(x) is the cumulative distribution function
and theN s are Gaussians. Figures 1 and 2 display
plots of these two functions compared to the Hop-
kins and May model. Note how v
win
(Figure 1) in-
creases exponentially as ?
S
2
becomes greater than
the (purportedly) better system, ?
S
1
.
As noted above, TrueSkill maintains not only
estimates {?
S
j
} of system abilities, but also
system-specific confidences about those estimates
5
The goal of this section is to provide an intuitive descrip-
tion of TrueSkill as adapted for WMT manual evaluations,
with enough detail to carry the main ideas. For more details,
please see Herbrich et al. (2006).
4
?1.0 ?0.5 0.0 0.5 1.0t = ?S
1
? ?S
2
0.0
0.5
1.0
1.5
v(t,?
)
TrueSkillHM
Figure 1: TrueSkill?s v
win
and the corresponding
loss function in the Hopkins and May model as
a function of the difference t of system means
( = 0.5, c = 0.8 for TrueSkill, and d = 0.5 for
Hopkins and May model).
?1.5 ?1.0 ?0.5 0.0 0.5 1.0 1.5t = ?S
1
? ?S
2
?1.0
?0.5
0.0
0.5
1.0
v(t,?
)
TrueSkillHM
Figure 2: TrueSkills v
tie
and the corresponding
loss function in the Hopkins and May model as
a function of the difference t of system means
( = 0.5, c = 0.3, and d = 0.5).
{?
S
j
}. These confidences also factor into the up-
dates: while surprising outcomes result in larger
updates to system means, higher confidences (rep-
resented by smaller variances) result in smaller
updates. TrueSkill defines the following value:
c
2
= 2?
2
+ ?
2
S
1
+ ?
2
S
2
which accumulates the variances along ?, another
free parameter. We can now define the update
equations for the system means:
?
S
1
= ?
S
1
+
?
2
S
1
c
? v
(
t
c
,

c
)
?
S
2
= ?
S
2
?
?
2
S
2
c
? v
(
t
c
,

c
)
The second term in these equations captures the
idea about balancing surprisal with confidence,
described above.
In order to update the system-level confidences,
TrueSkill defines another set of functions, w, for
the cases of wins and ties. These functions are
multiplicative factors that affect the amount of
change in ?
2
:
w
win
(t, ) = v
win
? (v
win
+ t? )
w
tie
(t, ) = v
tie
+
(? t) ? N (? t) + (+ t) ? N (+ t)
?(? t)? ?(?? t)
The underlying idea is that these functions cap-
ture the outcome surprisal via v. This update al-
ways decreases the size of the variances ?
2
, which
means uncertainty of ? decreases as comparisons
go on. With these defined, we can conclude by
defining the updates for ?
2
S
1
and ?
2
S
2
:
?
2
S
1
= ?
2
S
1
?
[
1?
?
2
S
1
c
2
? w
(
t
c
,

c
)
]
?
2
S
2
= ?
2
S
2
?
[
1?
?
2
S
2
c
2
? w
(
t
c
,

c
)
]
One final complication not presented here but rel-
evant to adapting TrueSkill to the WMT setting:
the parameter ? and another parameter (not dis-
cussed) ? are incorporated into the update equa-
tions to give more weight to recent matches. This
?latest-oriented? property is useful in the gaming
setting for which TrueSkill was built, where play-
ers improve over time, but is not applicable in the
WMT competition setting. To cancel this property
in TrueSkill, we set ? = 0 and ? = 0.025 ? |A| ??
2
in order to lessen the impact of the order in which
annotations are presented to the system.
2.4 Data selection with TrueSkill
A drawback of the standard WMT data collection
method is that it samples uniformly from the space
of pairwise system combinations. This is undesir-
able: systems with vastly divergent relative abil-
ity need not be compared as often as systems that
are more evenly matched. Unfortunately, one can-
not sample non-uniformly without knowing ahead
of time which systems are better. TrueSkill pro-
vides a solution to this dilemma with its match-
selection ability: systems with similar means and
low variances can be confidently considered to be
close matches. This presents a strong possibility
of reducing the amount of data that needs to be
5
collected in the WMT competitions. In fact, the
TrueSkill formulation provides a way to compute
the probability of a draw between two systems,
which can be used to compute for a system S
i
a
conditional distribution over matches with other
systems {S
j 6=i
}.
Formally, in the TrueSkill model, the match-
selection (chance to draw) between two players
(systems in WMT) is computed as follows:
p
draw
=
?
2?
2
c
2
? exp(?
(?
a
? ?
b
)
2
2c
2
)
However, our setting for canceling the ?latest-
oriented? property affects this matching quality
equation, where most systems are almost equally
competitive (? 1). Therefore, we modify the equa-
tion in the following manner which simply de-
pends on the difference of ?.
p?
draw
=
1
exp(|?
a
? ?
b
|)
TrueSkill selects the matches it would like to
create, according to this selection criteria. We do
this according to the following process:
1. Select a system S
1
(e.g., the one with the
highest variance)
2. Compute a normalized distribution over
matches with other systems pairs p?
draw
3. Draw a system S
2
from this distribution
4. Draw a source sentence, and present to the
judge for annotation
3 Experimental setup
3.1 Datasets
We used the evaluation data released by WMT13.
6
The data contains (1) five-way system rankings
made by either researchers or Turkers and (2)
translation data consisting of source sentences, hu-
man reference translations, and submitted transla-
tions. Data exists for 10 language pairs. More de-
tails about the dataset can be found in the WMT
2013 overview paper (Bojar et al., 2013).
Each five-way system ranking was converted
into ten pairwise judgments (?2). We trained the
models using randomly selected sets of 400, 800,
1,600, 3,200, and 6,400 pairwise comparisons,
6
statmt.org/wmt13/results.html
each produced in two ways: selecting from all re-
searchers, or split between researchers and Turk-
ers. An important note is that the training data
differs according to the model. For the Expected
Wins and Hopkins and May model, we sim-
ply sample uniformly at random. The TrueSkill
model, however, selects its own training data (with
replacement) according to the description in Sec-
tion 2.4.
7
For tuning hyperparameters and reporting test
results, we used development and test sets of 2,000
comparisons drawn entirely from the researcher
judgments, and fixed across all experiments.
3.2 Perplexity
We first compare the Hopkins and May model and
TrueSkill using perplexity on the test data T , com-
puted as follows:
ppl(p|T ) = 2
?
?
(i,S
1
,S
2
,pi)?T
log
2
p(pi|S
1
,S
2
)
where p is the model under consideration. The
probability of each observed outcome pi between
two systems S
1
and S
2
is computed by taking a
difference of the Gaussian distributions associated
with those systems:
N (?
?
, ?
2
?
) = N (?
S
1
, ?
2
S
1
)?N (?
S
2
, ?
2
S
2
)
= N (?
S
1
? ?
S
2
, ?
2
S
1
+ ?
2
S
2
)
This Gaussian can then be carved into three pieces:
the area where S
1
loses, the middle area represent-
ing ties (defined by a decision radius, r, whose
value is fit using development data), and a third
area representing where S
1
wins. By integrating
over each of these regions, we have a probability
distribution over these outcomes:
p(pi | S
1
, S
2
) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0
??
N (?
?
, ?
2
?
) if pi is >
?
r
0
N (?
?
, ?
2
?
) if pi is =
?
?
r
N (?
?
, ?
2
?
) if pi is <
We do not compute perplexity for the Expected
Wins model, which does not put any probability
mass on ties.
7
We use a Python implementation of TrueSkill
(github.com/sublee/trueskill).
6
3.3 Accuracy
Perplexity is often viewed as a neutral metric, but
without access to unbounded training data or the
true model parameters, it can only be approxi-
mated. Furthermore, it does not always corre-
late perfectly with evaluation metrics. As such,
we also present accuracy results, measuring each
model?s ability to predict the values of the ternary
pairwise judgments made by the annotators. These
are computed using the above equation, picking
the highest value of p(pi) for all annotations be-
tween each system pair (S
i
, S
j
). As with perplex-
ity, we emphasize that these predictions are func-
tions of the system pair only, and not the individual
sentences under consideration, so the same out-
come is always predicted for all sentences between
a system pair.
3.4 Parameter Tuning
We follow the settings described in Hopkins and
May (2013) for their model: ?
a
= 0.5, ?
obs
= 1.0,
and d = 0.5. In TrueSkill, in accordance with the
Hopkins and May model, we set the initial ? and
? values for each system to 0 and 0.5 respectively,
and  to 0.25.
For test data, we tuned the ?decision ra-
dius? parameter r by doing grid search over
{0.001, 0.01, 0.1, 0.3, 0.5}, searching for the
value which minimized perplexity and maximized
accuracy on the development set. We do this for
each model and language pair. When tuned by
perplexity, r is typically either 0.3 or 0.5 for both
models and language pairs, whereas, for accuracy,
the best r is either 0.001, 0.01, or 0.1.
4 Results
4.1 Model Comparison
Figure 3 shows the perplexity of the two mod-
els with regard to the number of training compar-
isons. The perplexities in the figure are averaged
over all ten language pairs in the WMT13 dataset.
Overall, perplexities decrease according to the in-
crease of training size. The Hopkins and May
and TrueSkill models trained on both researcher
and Turker judgments are comparable, whereas
the Hopkins and May model trained on researcher
judgments alone shows lower perplexity than the
corresponding TrueSkill model.
In terms of accuracy, we see that the TrueSkill
model has the highest accuracies, saturating at just
over 3,000 training instances (Figure 4). TrueSkill
1000 2000 3000 4000 5000 6000Training Data Size2.80
2.85
2.90
2.95
3.00
Perp
lexit
y
HM-allHM-resTS-allTS-res
Figure 3: Model Perplexities for WMT13 dataset.
?all? indicates that models are trained on both re-
searcher and Turker judgements, and ?res? means
that models are trained on only researcher judge-
ments.
outperforms Expected Win and the Hopkins and
May, especially when the training size is small
(Table 2). We also note that training on researcher
judgments alone (dashed lines) results in better
performance than training on both researchers and
Turker judgments. This likely reflects both a bet-
ter match between training and test data (recall the
test data consists of researcher judgments only),
as well as the higher consistency of this data, as
evidenced by the annotator agreement scores pub-
lished in the WMT overview paper (Bojar et al.,
2013). Recall that the models only have access
to the system pair (and not the sentences them-
selves), and thus make the same prediction for pi
for a particular system pair, regardless of which
source sentence was selected. As an upper bound
for performance on this metric, Table 2 contains
an oracle score, which is computed by selecting,
for each pair of systems, the highest-probability
ranking.
8
Comparing the plots, we see there is not a per-
fect relationship between perplexity and accuracy
among the models; the low perplexity does not
mean the high accuracy, and in fact the order of
the systems is different.
4.2 Free-for-all matches
TrueSkill need not deal with judgments in pairs
only, but was in fact designed to be used in a vari-
ety of settings, including N-way free-for-all games
8
Note that this might not represent a consistent ranking
among systems, but is itself an upper bound on the highest-
scoring consistent ranking.
7
1000 2000 3000 4000 5000 6000Training Data Size0.460
0.465
0.470
0.475
0.480
0.485
0.490
0.495
0.500
Accu
racy
ExpWin-allExpWin-resHM-allHM-resTS-allTS-res
Figure 4: Model accuracies with different training
domain for WMT13 dataset.
Train Size Exp-Win HM TrueSkill
400 0.465 0.471 0.479
800 0.471 0.475 0.483
all 1600 0.479 0.477 0.493
3200 0.486 0.489 0.493
6400 0.487 0.490 0.495
400 0.460 0.463 0.484
800 0.475 0.473 0.488
res 1600 0.481 0.482 0.493
3200 0.492 0.494 0.497
6400 0.495 0.496 0.497
Upper Bound 0.525
Table 2: Model accuracies: models are tuned by
accuracy instead of perplexity. Upper bound is
computed by selecting the most frequent choice
(<,>,=) for each system pair.
with many players all competing for first place.
This adapts nicely to WMT?s actual collection set-
ting. Recall that annotators are presented with five
translations which are then ranked; we can treat
this setting as a 5-way free-for-all match. While
the details of these updates are beyond the scope of
this paper, they are presented in the original model
and are implemented in the toolkit we used. We
thus also conducted experiments varying the value
of N from 2 to 5.
The results are shown in Tables 3 and 4, which
hold constant the number of matches and pairwise
judgments, respectively. When fixing the num-
ber of matches, the 5-way setting is at an advan-
tage, since there is much more information in each
match; in contrast, when fixing the number of pair-
wise comparisons, the 5-way setting is at a dis-
advantage, since many fewer competitions consti-
# N=2 N=3 N=4 N=5
400 0.479 0.482 0.491 0.492
800 0.483 0.493 0.495 0.495
1600 0.493 0.492 0.497 0.495
3200 0.493 0.494 0.498 0.497
6400 0.495 0.498 0.498 0.498
Table 3: Accuracies when training with N-way
free-for-all models, fixing the number of matches.
# N=2 N=3 N=4 N=5
400 0.479 0.475 0.470 0.459
800 0.483 0.488 0.476 0.466
1600 0.493 0.488 0.481 0.481
3200 0.493 0.492 0.487 0.489
6400 0.495 0.496 0.494 0.495
Table 4: Accuracies when training with N-way
free-for-all models, fixing the number of pairwise
comparisons.
tute these comparisons. The results bear this out,
but also suggest that the standard WMT setting
? which extracts ten pairwise comparisons from
each 5-way match and treats them independently
? works well. We will not speculate further here,
but provide this experiment purely to motivate po-
tential future work. Here we will focus our con-
clusions to the pair-wise ranking scenario.
5 Reduced Data Collection with
Non-uniform Match Selection
As mentioned earlier, a drawback of the selection
of training data for annotation is that it is sampled
uniformly from the space of system pair compe-
titions, and an advantage of TrueSkill is its abil-
ity to instead compute a distribution over pairings
and thereby focus annotation efforts on competi-
tive matches. In this section, we report results in
the form of heat maps indicating the percentage of
pairwise judgments requested by TrueSkill across
the full cross-product of system pairs, using the
WMT13 French-English translation task.
Figure 5 depicts a system-versus-system heat
map for all judgments in the dataset. Across this
figure and the next two, systems are sorted along
each axis by the final values of ? inferred by
TrueSkill during training, and the heat of each
square is proportional to the percentage of judg-
ments obtained between those two systems. The
diagonal reflects the fact that systems do not com-
pete against themselves, and the stripe at row and
column 5 reflects a system that was entered late
8
1 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Figure 5: Heat map for the ratio of pairwise judg-
ments across the full cross-product of systems in
the WMT13 French-English translation task.
1 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Figure 6: Heat map for the ratio of pairwise judg-
ments across the full cross-product of systems
used in the first 20% of TrueSkill model.
1 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Figure 7: Heat map for the ratio of pairwise judg-
ments across the full cross-product of systems
used in the last 20% of TrueSkill model.
into the WMT13 competition and thus had many
fewer judgments. It is clear that these values are
roughly uniformly distributed. This figure serves
as a sort of baseline, demonstrating the lack of pat-
terns in the data-selection process.
The next two figures focus on the data that
TrueSkill itself selected for its use from among all
of the available data. Figure 6 is a second heat
map presenting the set of system pairs selected by
TrueSkill for the first 20% of its matches chosen
during training, while Figure 7 presents a heat map
of the last 20%. The contrast is striking: whereas
the judgments are roughly uniformly distributed at
the beginning, the bulk of the judgments obtained
for the last set are clustered along the diagonal,
where the most competitive matches lie.
Together with the higher accuracy of TrueSkill,
this suggests that it could be used to decrease the
amount of data that needs to be collected in future
WMT human evaluations by focusing the annota-
tion effort on more closely-matched systems.
6 Clustering
As pointed out by Koehn (2012), a ranking pre-
sented as a total ordering among systems con-
ceals the closeness of comparable systems. In the
WMT13 competition, systems are grouped into
clusters, which is equivalent to presenting only
a partial ordering among the systems. Clusters
are constructed using bootstrap resampling to in-
fer many system rankings. From these rankings,
rank ranges are then collected, which can be used
to construct 95% confidence intervals, and, in turn,
to cluster systems whose ranges overlap. We use
a similar approach for clustering in the TrueSkill
model. We obtain rank ranges for each system by
running the TrueSkill model 100 times,
9
throw-
ing out the top and bottom 2 rankings for each
system, and clustering where rank ranges overlap.
For comparison, we also do this for the other two
models, altering the amount of training data from
1k to 25k in increments of 1,000, and plotting the
number of clusters that can be obtained from each
technique on each amount of training data.
Figure 8 show the number of clusters according
to the increase of training data for three models.
TrueSkill efficiently split the systems into clusters
compared to other two methods. Figure 9 and 10
present the result of clustering two different size of
9
We also tried the sampling 1,000 times and the clustering
granularities were the same.
9
5000 10000 15000 20000 25000Pairwise Comparisons
0
1
2
3
4
5
6
7
Num
. of 
Clu
ster
s
ExpWin
HM
TS
Figure 8: The number of clusters according to
the increase of training data for WMT13 French-
English (13 systems in total).
training data (1K and 25K pairwise comparisons)
on the TrueSkill model, which indicates that the
rank ranges become narrow and generate clusters
reasonably as the number of training samples in-
creases. The ranking and clusters are slightly dif-
ferent from the official result (Table 1) mainly be-
cause the official result is based on Expected Wins.
One noteworthy observation is that the ranking
of systems between Figure 9 and Figure 10 is the
same, further corroborating the stability and ac-
curacy of the TrueSkill model even with a small
amount of data. Furthermore, while the need
to cluster systems forces the collection of sig-
nificantly more data than if we wanted only to
report a total ordering, TrueSkill here produces
nicely-sized clusters with only 25K pairwise com-
parisons, which is nearly one-third large of that
used in the WMT13 campaign (80K for French-
English, yielding 8 clusters).
7 Conclusion
Models of ?relative ability? (Koehn, 2012; Hop-
kins and May, 2013) are a welcome addition to
methods for inferring system rankings from hu-
man judgments. The TrueSkill variant presented
in this paper is a promising further development,
both in its ability to achieve higher accuracy levels
than alternatives, and in its ability to sample non-
uniformly from the space of system pair match-
ings. It?s possible that future WMT evaluations
could significantly reduce the amount of data they
need to collect, also potentially allowing them to
draw from expert annotators alone (the developers
uedin-h on.Buedin-w LIMSI KIT on.A MES-S DCU CMURWTH cu-z JHU Shef
1
2
3
4
5
6
7
8
9
10
11
12
13
Figure 9: The result of clustering by TrueSkill
model with 1K training data from WMT13
French-English. The boxes range from the lower
to upper quartile values, with means in the middle.
The whiskers show the full range of each system?s
rank after the bootstrap resampling.
uedin-h on.Buedin-w LIMSI KIT on.A MES-S DCU CMURWTH cu-z JHU Shef
1
2
3
4
5
6
7
8
9
10
11
12
13
Figure 10: The result of clustering by TrueSkill
model with 25K training data. Dashed lines sep-
arate systems with non-overlapping rank ranges,
splitting the data into clusters.
of the participating systems), without the need to
hire non-experts on Mechanical Turk.
One piece missing from the methods explored
and proposed in this paper is models of the actual
translations being compared by judges. Clearly,
it is properties of the sentences themselves that
judges use to make their judgments, a fact which
is captured only indirectly by modeling transla-
tion qualities sampled from system abilities. This
observation has been used in the development
of automatic evaluation metrics (Song and Cohn,
2011), and is something we hope to explore in fu-
ture work for system ranking.
10
References
Ond?rej Bojar, Milo?s Ercegov?cevi?c, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
1?11, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, pages 22?64, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Ralf Herbrich, Tom Minka, and Thore Graepel. 2006.
TrueSkill
TM
: A Bayesian Skill Rating System. In
Proceedings of the Twentieth Annual Conference on
Neural Information Processing Systems, pages 569?
576, Vancouver, British Columbia, Canada, Decem-
ber. MIT Press.
Mark Hopkins and Jonathan May. 2013. Models of
translation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1416?1424, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Philipp Koehn. 2012. Simulating Human Judgment
in Machine Translation Evaluation Campaigns. In
Proceedings of the 9th International Workshop on
Spoken Language Translation (IWSLT), pages 179?
184, Hong Kong, China, December. International
Speech Communication Association.
Adam Lopez. 2012. Putting Human Assessments of
Machine Translation Systems in Order. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 1?9, Montr?eal, Canada, June. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, July. Association for Computational
Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
Ranking based Optimisation for Sentence Level MT
Evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 123?129,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
11
