Coling 2008: Companion volume ? Posters and Demonstrations, pages 35?38
Manchester, August 2008
Underspecified Modelling of Complex Discourse Constraints
Markus Egg
egg@let.rug.nl
University of Groningen
Michaela Regneri
regneri@coli.uni-sb.de
Saarland University
Abstract
We introduce a new type of discourse con-
straints for the interaction of discourse re-
lations with the configuration of discourse
segments. We examine corpus-extracted
examples as soft constraints. We show how
to use Regular Tree Gramamrs to process
such constraints, and how the representa-
tion of some constraints depends on the ex-
pressive power of this formalism.
1 Introduction
Discourse structures cannot always be described
completely, either because they are ambiguous
(Stede, 2004), or because a discourse parser fails
to analyse them completely. In either case, un-
derspecification formalisms (UFs) can be used to
represent partial information on discourse struc-
ture. UFs are used in semantics to model structural
ambiguity without disjunctive enumeration of the
readings (van Deemter and Peters, 1996).
Underspecified descriptions of discourse must
handle two kinds of incomplete information, on
the configuration of discourse segments (how
they combine into larger units), and on the dis-
course relations that bring about this configura-
tion: Our corpus studies on the RST Discourse
Treebank (Carlson et al, 2002) showed interde-
pendencies between relations and configuration, a
phenomenon first noted by (Corston-Oliver, 1998).
These interdependencies can be formulated as con-
straints that contribute to the disambiguation of un-
derspecified descriptions of discourse structure.
E.g., in discourse segments constituted by the
relation Condition, the premiss tends to be a dis-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
course atom (or at least, maximally short).
1
Simi-
larly, there is evidence for an interdependency con-
straint for the relation Purpose(1)
2
. In most cases,
Purpose(1) has a discourse atom as its nucleus.
The corpus evaluation furthermore shows that
those patterns never occur exclusively but only as
tendencies. Realised as soft constraints, such ten-
dencies can help to sort the set of readings ac-
cording to the established preferences, which al-
lows to focus on the best reading or the n-best
readings. This is of high value for an UF-based
approach to discourse structure, which must cope
with extremely high numbers of readings. To
model interdependency constraints, we will use
Regular Tree Grammars (RTGs) (Comon et al,
2007). RTGs can straightforwardly be extended
to weighted Regular Tree Grammars (wRTGs),
which can represent both soft and hard constraints.
Apart from our corpus-extracted examples, we
also consider a hard interdependency constraint
similar to the Right Frontier Constraint. We show
that we can integrate this attachment constraint
with our formalism, and how its representation de-
pends on the expressiveness of RTGs.
2 Underspecified Discourse Structure
We describe (partial) information on discourse
structure by expressions of a suitable UF, here,
dominance graphs (Althaus et al, 2003). Consider
e.g. Fig. 1(a), the dominance graph for (1):
(1) [C
1
I try to read a novel] [C
2
if I feel bored]
[C
3
because the TV programs disappoint me]
[C
4
but I can?t concentrate on anything.]
1
Following Rhetorical Structure Theory (Mann and
Thompson, 1988), most discourse relations have a central nu-
cleus argument, and a peripheral satellite argument. For Con-
dition, the premiss is the satellite, the nucleus, the conclusion.
2
?(n)? as part of a relation name indicates that the nucleus
is its n-th argument; relations with names without such an
affix are multinuclear, i.e., link two segments of equal promi-
nence. We sometimes omit the numbers where the position of
the nucleus is clear from the context.
35
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
C
1
C
2
C
3
C
4
C
2
C
3
C
4
C
1
C
4
C
1
C
2
C
3
Condition
(1)
Condition
(1)
Condition
(1)
Condition
(1)
Cause
(2)Cause
(2)
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
Condition
(1)
Cause
(2)
Contrast
Contrast
Contrast
Condition
(1)
1
2
3 5
4
7
6
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
(a) (b) (c) (d) (e) (f)
Figure 1: An underspecified discourse structure and its five configurations
{1-7}? Condition({1}, {3-7}) [1] {1-7}? Cause({1-3}, {5-7}) [1] {3-7}? Contrast({3-5}, {7}) [1]
{3-5}? Cause({3}, {5}) [1] {1-7}? Contrast({1-5}, {7}) [1] {1-5}? Cause({1-3}, {5}) [1]
{5-7}? Contrast({5}, {7}) [1] {1-5}? Condition({1}, {3-5}) [3] {1-3}? Condition({1}, {3}) [9]
{3-7}? Cause({3}, {5-7}) [1] {1} ? C
1
[1] {3} ? C
2
[1] {5} ? C
3
[1] {7} ? C
4
[1]
Figure 2: A wRTG modelling the interdependency constraint for Fig. 1
Such constraints describe a set of discourse
structures (formalised as binary tree structures).
Their key ingredient are (reflexive, transitive and
antisymmetric) dominance relations, which are in-
dicated by dotted lines. Dominance of X
1
over X
2
means that X
2
is part of the structure below (and
including) X
1
, but there might be additional mate-
rial intervening between X
1
and X
2
.
Fig. 1(a) states that C
1
is linked to a part of the
following discourse (including at leastC
2
) byCon-
dition, Cause(2) connects two discourse segments
(comprising at least C
2
and C
3
, respectively), and
Contrast links a discourse segment to its left (in-
cluding at least C
3
) to C
4
.
This constraint describes (is compatible with)
exactly the five tree structures in Fig. 1(b-f), if
described tree structures may only comprise ma-
terial that is already introduced in the constraint.
They model the potential discourse structures for
(1) (see Webber (2004)). Dominance graphs like
Fig. 1a. are pure chains. Pure chains describe all
binary trees with the same leaf language, here the
discourse segments, in their textual order. Pure
chains define a left-to-right order, in that not only
the leaves always form the same sequence, but also
the inner nodes: If a labelled node X is further to
the left in the chain than another node Y, in every
described tree, X will either be Y?s left child, or Y
will be X?s right child, or there will be a fragment
F of which X is a successor on the left and Y is a
right successor. Henceforth we will refer to frag-
ments with their index in the chain (indicated by
encircled numbers in Fig. 1a).
3 Representing Soft Interdependencies
The interdependency constraint for Condition(1) is
that its satellite tends to be maximally short, i.e.,
mostly consists of only one discourse atom, and
in most remaining cases, of two atoms. Thus, (b)
and (d) are preferred among the configurations in
Fig. 1, (c) is less preferred, and (e) and (f) are the
least preferred. Regular Tree Grammars (RTGs) as
UF (Koller et al, 2008) can express such complex
constraints straightforwardly, and provide a con-
venient framework to process them. They allow
to extract a best configuration with standard algo-
rithms very efficiently.
Koller et al (2008) show how to generate an
RTG describing the same set of trees as a domi-
nance graph. Similar to a context free grammar, an
RTG uses production rules with terminal symbols
and nonterminal symbols (NTs), whereby the left-
hand side (LHS) is always a nonterminal and the
right-hand side (RHS) contains at least one termi-
nal symbol. One NT is the start symbol. A tree
is accepted by the grammar if the grammar con-
tains a derivation for it. An example for an RTG is
given in Fig. 2, which describes the same trees as
the dominance graph in Fig. 1a. The start symbol
is {1-7}. To derive e.g. the tree in Fig. 1d, we first
select the rule {1-7} ? Cause({1-3}, {5-7}) that
determines Condition as root for the whole tree.
The left child of Condition is then derived from
{1-7}, and the right child from {5-7} respectively.
To emphasize the association with the dominance
graph, we mark nonterminals as the subgraphs they
represent, e.g., {1-7} denotes the whole graph.
The terminal in the RHS of a grammar rule deter-
mines the root of the LHS subgraph.
Koller et al (2008) also use weighted RTGs
(wRTGs, an extension of RTG with weights) to
express soft dominance constraints (which, unlike
hard constraints, do not restrict but rather rank the
set of configurations). We use wRTGs to model
the soft interdependency constraints. The gram-
mar in Fig. 2 is also a wRTG that assigns a weight
to each derived tree: Its weight is the product over
all weights of all rules used for the derivation.
Weights appear in squared brackets after the rules.
36
The (merely expository) weights in our example
encode the preference of Condition for a maxi-
mally short right child: There are three grammar
rules that establish Condition as the root of a sub-
graph (shaded in Fig. 2), which are distinguished
by the size of the right child of the root (one ({3}),
three ({3-5}) or five ({3-7}) nodes). The shorter
the right child, the higher the weight associated
with the rule. (1 is a neutral weight by definition.)
The grammar thus assigns different weights to the
trees in Fig. 1; (b) and (d) get the maximum weight
of 9, (b), a medium weight of 3, and (e) and (f), the
lowest weight of 1.
4 Expressive Power of RTGs
As Koller et al (2008) show, the expressive power
of RTGs is superior to other common underspec-
ification formalism. We show an important appli-
cation of the increased expressiveness with Ex. 2,
where a. can be continued by b. but not by c:
(2) a. [C
1
Max and Mary are falling apart.]
[C
2
They no longer meet for lunch.]
[C
3
And, last night, Max went to the
pub] [C
4
but Mary visited her parents.]
b. [C
5a
She complained bitterly about his
behaviour.]
c. [C
5b
He left after his fifth pint of lager.]
Segment C
5a
continues the preceding clause
about Mary?s visit with additional information
about the visit, it thus attaches directly to C
4
. To
find a coherent integration of C
5b
, we would have
to connect it to C
3
, as it provides more details
about Max? night at the pub. However, in the given
constellation of C
3
and C
4
, that form a Contrast
together, C
3
is not available any longer for attach-
ment of further discourse units. (This constraint is
reminiscent of the Right Frontier Constraint, as it
is used by Asher and Lascarides (2003). However,
it is unclear how the Right Frontier Constraint in
its exact definition can carry over to binary trees.)
The given attachment constraint is not express-
ible with dominance graphs: it excludes the config-
urations of its dominance graph (Fig. 3) in which
Contrast shows up as a direct left child, e.g.,
(3b/e/f) as opposed to (3c/d). For instance, the
excluded structure emerges in (3e/f) by choosing
Cause as root of the the subgraph 5-9 (i.e., includ-
ing the Contrast- and Sequence-fragments). For
convenience, we will talk about this constraint as
the ?left child constraint? (LCC).
S ? Contrast(S, S) L ? Evid(S, S)
S ? Sequ(L, S) L ? List(S, S)
S ? L
L ? C
1
L ? C
2
L ? C
3
L ? C
4
L ? C
5
Figure 5: A filter RTG corresponding to Ex. 2
This additional constraint, however, can be ex-
pressed by an RTG like Fig. 4. We explicitly
distinguish between subgraphs (referred to with
numbers) and their associated NTs here. Cru-
cially, some subgraphs can be processed in dif-
ferent derivations here, e.g., {5-9} (as right child
of List, irrespective of the relative scope of Ev-
idence and List), or {3-7} (in the expansions of
both {EvLiCo} and {LiCoSe}, like in (3c) as
opposed to (3d)). Sometimes this derivation his-
tory is irrelevant, like in the case of {5-9} (here,
only Contrast may be chosen as root anyway), but
there are cases where it matters: If {3-7} is the left
child of Sequence, as in (3b/d), the choice of Con-
trast as its root is excluded, since this would make
Contrast the left child of Sequence, as in (3b). In
contrast, {3-7} as the right child of Evidence, like
in (3c), allows both Contrast and List as root, be-
cause Contrast emerges as a right child in either
case. Thus, the two occurrences of {3-7} are dis-
tinguished in terms of different NTs in the gram-
mar, and only in the NT for the latter occurrence is
there more than one further expansion rule.
Regular tree languages are closed under inter-
section. Thus, one can derive a grammar like Fig. 4
by intersecting a completely underspecified RTG
(here, the one derived from Fig. 3a) with a suitable
filter grammar, e.g., Fig. 4. The filter grammar
produces an infinite language, containing the frag-
ments of Fig. 3a and excluding any derivation in
which Sequence is the direct parent of Contrast.
This is guaranteed by introducing the nonterminal
L (the left child NT for Sequence), for which there
is no derivation with Contrast as its root.
For an arbitrary pure chain with n fragments, the
filter grammar generating the LCC is constructed
as follows: S is the start symbol. For every frag-
ment i s.t. 0 < i < n, there is a derivation rule
with S as its LHS and i in its RHS, thus either
S ? i, for singleton fragments, or S ? i(A,S),
for binary fragments. If i is binary, we must de-
termine A: If there is at least one fragment f < i
s.t. the LCC is assumed for f , we create a new
NT L
i
; every derivation rule with i on its RHS fol-
lows the pattern X ? i(L
i
, S) (thus A = L
i
in
particular). If there is no LCC fragment to the left
37
C5
C
2
C
3
C
4
Evidence
(1)
SequenceContrast
C
1
List
C
1
C
5
C
3
C
4
Evid
Contr
Sequ
C
2
List
C
1
C
3
C
4
Evid
Contr
C
2
List
C
5
Sequ
C
1
C
5
C
4
Evid
Contr
Sequ
C
3
C
2
List
List
C
1
Evid
C
2
C
4
C
5
Contr
C
3
(a) (b)
(c) (d) (e)
C
1
C
5
C
4
Evid
Contr
List
C
3
C
2
Sequ
Sequ
(f)
1
2
3
5
4
7
6
8
9
Figure 3: An underspecified discourse structure for Ex. 2 and five of its configurations
{EvLiCoSe} ? Evid({C
1
}, {LiCoSe}) {EvLiCo} ? List({Ev}, {Co}) {Ev} ? Evid({C
1
}, {C
2
})
{EvLiCoSe} ? List({Ev}, {CoSe}) {CoSe} ? Cont({C
3
}, {Se}) {Li} ? List({C
2
}, {C
3
})
{EvLiCoSe} ? Cont({EvLi}, {Se}) {EvLi} ? Evid({C
1
}, {Li}) {Co} ? Cont({C
3
}, {C
4
})
{EvLiCoSe} ? Sequ({EvLiCo}, {C
5
}) {EvLi} ? List({Ev}, {C
3
}) {Se} ? Sequ({C
4
}, {C
5
})
{LiCoSe} ? Sequ({LiCo}
L
, {C
5
}) {LiCo}
L
? List({C
2
}, {Co})
{LiCoSe} ? List({C
2
}, {CoSe}) {LiCo}
S
? Cont({Li}, {C
4
}) {C
1
} ? C
1
{C
2
} ? C
2
{LiCoSe} ? Cont({Li}, {Se}) {LiCo}
S
? Li({Li}, {C
4
}) {C
3
} ? C
3
{EvLiCo} ? Evid({C
1
}, {LiCo}
S
) {C
4
} ? C
4
{C
5
} ? C
5
Figure 4: A RTG integrating the attachment constraint for Contrast from Ex. 2 into Fig. 3
of i, A = S. If a new NT L
i
was created, we
need to create its RHSs: For every fragment h s.t.
0 < h < i and there is no LCC for h, there is a
rewrite rule directly deriving h from L
i
. If h is a
singleton fragment, the rule is L
i
? h. Otherwise
the rule is L
i
? h(A
?
, S), whereby A
?
= S, if
there is no L
h
, or A
?
= L
h
if there is some LCC
fragment on the left of h.
3
The grammar in Fig. 4 can be generated with
that scheme; it has been reduced afterwards in that
a general rule S ? L substitutes for all rules of the
form S ? NT for which there is a corresponding
rule L ? NT (e.g., S ? Evid(S, S)).
5 Conclusion
Interdependency constraints that arise from the in-
teraction of discourse relations and their surround-
ing structures are introduced as a new technique
for disambiguating discourse structure. We inte-
grate those constraints in underspecified discourse
structures by exploiting the expressive power of
Regular Tree Grammars as UF. As the corpus anal-
ysis yields in many cases only soft interdepen-
dency constraints, we use the weighted extension
of RTGs, which allows to sort the readings of an
underspecified representation and to identify pre-
ferred discourse structures. We then showed that
the representation of some discourse constraints
depend on the expressive power of RTGs. For
notes on implementation and tractability of our ap-
proach, see Regneri et al (2008).
3
To model this as a preference rather than as a hard con-
straint, no rules for the L-NTs are omitted, but rather weighted
low. An intersection with a preference-neutral wRTG would
rank the configurations violating the constraint low, and all
others with neutral weights.
References
Althaus, Ernst, Denys Duchier, Alexander Koller, Kurt
Mehlhorn, Joachim Niehren, and Sven Thiel. 2003.
An efficient graph algorithm for dominance con-
straints. Journal of Algorithms, 48:194?219.
Asher, Nicholas and Alex Lascarides. 2003. Logics of
Conversation. Cambridge UP, Cambridge.
Carlson, Lynn, Daniel Marcu, and Mary Ellen
Okurowski. 2002. RST Discourse Treebank. LDC.
Comon, H., M. Dauchet, R. Gilleron, C. L?oding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree Automata Techniques and Ap-
plications. Available on: http://www.grappa.
univ-lille3.fr/tata. Release 12-10-2007.
Corston-Oliver, Simon H. 1998. Computing Represen-
tations of Discourse Structure. Ph.D. thesis, Dept. of
Linguistics, University of California, Santa Barbara.
van Deemter, Kees and Stanley Peters, editors. 1996.
Semantic ambiguity and underspecification. CSLI,
Stanford.
Koller, Alexander, Michaela Regneri, and Stefan
Thater. 2008. Regular tree grammars as a formal-
ism for scope underspecification. In Proceedings of
the ACL 08.
Mann, William C. and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8:243?281.
Regneri, Michaela, Markus Egg, and Alexander Koller.
2008. Efficient Processing of Underspecified Dis-
course Representations. In Proceedings of the ACL
08 (Short Papers).
Stede, Manfred. 2004. The Potsdam Commentary Cor-
pus. In Webber, Bonnie and Donna K. Byron, edi-
tors, ACL 2004 Workshop on Discourse Annotation.
Webber, Bonnie. 2004. D-LTAG: extending lexicalized
TAG to discourse. Cognitive Science, 28:751?779.
38
Book Reviews
A Computational Model of Natural Language Communication
Roland Hausser
(Friedrich-Alexander-Universita?t Erlangen-Nu?rnberg)
Springer, 2006, xii+365 pp; hardbound, ISBN 3-540-35476-X/978-3-540-35476-5, e69.50
Reviewed by
Markus Egg
University of Groningen
The work presented in this book is motivated by the goal of applying linguistic theory-
building to the concrete needs of potential linguistic applications such as question
answering, dialogue systems, andmachine translation. To pursue this goal, a translation
of linguistic theory into a framework of ?practical linguistics? is suggested. Database
Semantics (DBS) is presented as a first step towards such a framework. It models the
communication between cognitive agents, which can be used, for example, to imple-
ment the communicative abilities of a cognitive robot.
DBS serves as a single underlying format for modeling communication in that it
lends itself to an account of both language processing and language production (think-
ing is added as a separate component, which refers to inferencing on stored information,
and activating content to be verbalized). As such an underlying format, it can be used to
describe linguistic as well as extralinguistic content (to represent utterances and the con-
text, respectively). Being explicitly designed for practical applications, DBS deliberately
ignores linguistic phenomena considered irrelevant for these (e.g., quantifier scope).
The structure of the book is as follows. It has threemain parts, which introduce DBS,
outline the range of constructions covered by DBS so far, and specify fragments that can
be processed or produced in the framework of DBS. There is also an appendix with two
sections on the treatment of word-order variation in DBS and on the global architecture
of DBS systems, and a glossary.
The first part of the book starts with general principles of linguistic analysis that ap-
ply to DBS. These principles include incrementality (input is to be processed successively
as it comes in, which yields an analysis for incomplete as well as complete chunks of
input; the syntactic basis for this strategy is Left-Associative Grammar [Hausser 1992]),
surface orientation (no empty categories), and a focus on communication (description for-
malisms must be able to handle turn-taking, i.e., language processing and production).
After a sketch of the general theory of communication of which DBS is a part, DBS
is presented in detail. It is implemented as a non-recursive data structure, that is, a list of
feature structures called proplets (usually, one per word1) that are linked by coindexing
the values of specific features.2 For example, subcategorizing elements (?functors?)
have features whose values indicate their arguments and the other way around.
In spite of its name, DBS does not offer a purely semantic representation of linguistic
expressions. Although it does abstract away from purely syntactic phenomena such
1 Function words such as determiners, auxiliaries, and conjunctions have no proplets of their own but
contribute to other proplets.
2 This technique makes it resemble minimal recursion semantics (Copestake et al 2005).
Computational Linguistics Volume 34, Number 2
as word order and diatheses, it still preserves much syntactic structure, for example,
in its representation of modification and of elliptical expressions. Semantics proper is
encoded within proplets (except those for deictic expressions and proper names) by
defining a concept as the value of their ?core attribute.?
DBS also serves for the representation of the extralinguistic context. The context is
described in terms of proplet sets that are linked by feature value coindexation; the only
difference to proplet sets for the modeling of linguistic content is that proplet sets for
context do not comprise explicit pointers to specific words.
The similarity between the representations of utterances and of context makes the
move between them straightforward, which is crucial for the proposed analysis of
language processing and production: Language processing consists of deriving lists
of proplets (including the coindexations between proplet values) from utterances and
storing them in the context representation, which is modeled as a database. Language
production consists of the activation of such lists of proplets from this database and
their translation into utterances.
The second part of the book is devoted to three classes of linguistic phenomena and
their description in DBS. The first class is called ?functor-argument structure? and cov-
ers the relations between subcategorizing elements and their arguments and modifica-
tion. This includes sentential arguments, subordinate clauses, and relative clauses. The
second class consists of coordination phenomena, ranging from simple coordination on
the word or phrase level to gapping and right-node raising. The last class is cases of
coreference. A wide range of these cases is represented in DBS, including even Bach?
Peters sentences (where there are two NPs that constitute anaphors whose antecedent
is the respective other NP). The DBS framework is used to formulate a version of the
Langacker?Ross condition dating back to Langacker (1969) and Ross (1969): Pronouns
can precede a coreferential NP only if they are part of a clause that is embedded within
the clause of the NP.
In the third part, three fragments are presented in detail, the first two from the
processing and production perspective, the last one only from the processing perspec-
tive. The first fragment prepares the ground by illustrating how the approach handles
extremely simple texts consisting of intransitive present-tense sentences whose NP
is a proper name. The second fragment extends the coverage to pronouns, complex
NPs (Det-Adj*-N), and transitive and ditransitive verbs in simple and complex tenses.
Finally, the third fragment offers a treatment of intensifiers (very, rather) and adverbials,
and an outlook on a syntactically underspecified approach to modifier attachment
ambiguities. The fragments are described in terms of ?grammars,? which specify start
and end states (in terms of the first and the last proplet of a list to be processed or
verbalized) and a set of rules. The rules are ordered in that every rule is accompanied
by a set of potential successors, and in that rules to start and to end a derivation with
are specified.
The book is written in a highly accessible way. The formalism itself as well as its
application to the fragments is described thoroughly, whichmakes it easy to understand
and evaluate DBS. The underlying perspective on linguistic theory-building and the
theory of communication of which DBS is a part are also explicated clearly. The formal
details of the analysis are presented carefully. A remaining point of dispute is in my
view the set of readings of sentences where several PPs have more than one attachment
possibility (Chapter 15.1).
However, the book does not offer much discussion of the relation between the
proposed analysis and competing approaches. This shows up in specific parts of the
analysis?for example, in the discussion of coreference in Chapter 10, which does not
312
Book Reviews
integrate previous work that formulates constraints on potential coreferences in terms
of syntactic constellations such as c- or o-command (e.g., Pollard and Sag 1994; Reuland
2006), and in the treatment of quantifier scope and scope ambiguity in Chapter 6 (as
opposed to, e.g., the papers in vanDeemter and Peters [1996]). But evenmore important,
it would have been interesting to hear more about the way in which DBS compares to
other approaches whose goal is the application of linguistic theory-building to concrete
needs of potential linguistic applications. Although the completion of the manuscript
admittedly antedates much of the ongoing work in the field (e.g., the application of
deep linguistic processing in the analysis of biomedical and other scientific texts), a
comparison of DBS to wide-coverage systems such as the LinGO English Resource
Grammar (Copestake and Flickinger 2000) (including also related activities such as
the development of Robust Minimal Recursion Semantics [Copestake 2007]) or Alpino
(analysis of unrestricted Dutch texts [Bouma, van Noord, and Malouf 2001]) would
have been a welcome complementation to the presentation of DBS in the book.
References
Bouma, Gosse, Gertjan van Noord, and
Robert Malouf. 2001. Alpino: Wide
coverage computational analysis of Dutch.
In Walter Daelemans, Khalil Sima?an,
Jorn Veenstra, and Jakub Zavrel, editors,
Computational Linguistics in the Netherlands
(CLIN) 2000, Rodopi, Amsterdam,
pages 45?59.
Copestake, Ann. 2007. Semantic composition
with (robust) minimal recursion semantics.
In Proceedings of the Workshop on Deep
Processing, pages 73?80, Prague.
Copestake, Ann and Dan Flickinger. 2000.
An open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings of the
2nd International Conference on Language
Resources and Evaluation, Athens.
Copestake, Ann, Daniel Flickinger, Carl
Pollard, and Ivan Sag. 2005. Minimal
recursion semantics. An introduction.
Research on Language and Computation,
3:281?332.
van Deemter, Kees and Stanley Peters,
editors. 1996. Semantic Ambiguity and
Underspecification. CSLI, Stanford, CA.
Hausser, Roland. 1992. Complexity in
left-associative grammar. Theoretical
Computer Science, 106:283?308.
Langacker, R. 1969. On pronominalization
and the chain of command. In D. Reibel
and S. Schane, editors,Modern Studies in
English. Prentice Hall, Englewood Cliffs,
NJ, pages 160?186.
Pollard, Carl and Ivan Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI and
University of Chicago Press, Chicago, IL.
Reuland, Eric. 2006. Binding theory: Terms
and concepts. In Martin Everaert and
Henk van Riemsdijk, editors, The Blackwell
Companion to Syntax, volume 1. Blackwell,
Malden, UK, chapter 9, pages 260?283.
Ross, J. 1969. On the cyclic nature of English
pronominalization. In D. Reibel and
S. Schane, editors,Modern Studies in
English. Prentice Hall, Englewood Cliffs,
NJ, pages 187?200.
Markus Egg is an associate professor in Discourse Studies at the University of Groningen. His
main areas of interest are semantics and discourse and their interfaces with syntax. His address
is Centre for Language and Cognition Groningen, Rijksuniversiteit Groningen, Oude Kijk in
?t Jatstraat 26, 9712 EK Groningen, The Netherlands; e-mail: k.m.m.egg@rug.nl.
313

Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 245?248,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Efficient Processing of Underspecified Discourse Representations
Michaela Regneri? ?
regneri@coli.uni-sb.de
? Saarland University
Markus Egg?
egg@let.rug.nl
? University of Groningen
Alexander Koller?
a.koller@ed.ac.uk
? University of Edinburgh
Abstract
Underspecification-based algorithms for pro-
cessing partially disambiguated discourse
structure must cope with extremely high num-
bers of readings. Based on previous work on
dominance graphs and weighted tree gram-
mars, we provide the first possibility for com-
puting an underspecified discourse description
and a best discourse representation efficiently
enough to process even the longest discourses
in the RST Discourse Treebank.
1 Introduction
Discourse processing has emerged as a highly rele-
vant source of information for applications such as
information extraction and automatic summarisation
(Taboada and Mann (2006) outline this and further
applications). But discourse structures cannot al-
ways be described completely, either due to genuine
ambiguity (Stede, 2004) or to the limitations of a
discourse parser. In either case, only partial infor-
mation on discourse structure is available. To han-
dle such information, underspecification formalisms
can be used. Underspecification was originally in-
troduced in computational semantics to model struc-
tural ambiguity without disjunctively enumerating
the readings, and later applied to discourse parsing
(Gardent and Webber, 1998; Schilder, 2002).
However, while the existing algorithms for un-
derspecification processing work well for seman-
tic structures, they were not designed for discourse
structures, which can be much larger. Indeed, it
has never been shown that underspecified discourse
reprentations (UDRs) can be processed efficiently,
since the general-purpose implementations are too
slow for that task.
In this paper, we present a new way to imple-
ment and process discourse underspecification in
terms of regular tree grammars (RTGs). RTGs are
used as an underspecification formalism in seman-
tics (Koller et al, 2008). We show how to compute
RTGs for discourse from dominance-based under-
specified representations more efficiently (by a typ-
ical factor of 100) than before. Furthermore, we
show how weighted RTGs can be used to represent
constraints and preferences on the discourse struc-
ture. Taking all these results together, we show for
the first time how the globally optimal discourse rep-
resentation based on some preference model can be
computed efficiently from an UDR.
2 Underspecified Discourse Representation
Following annotation schemes like the one of Stede
(2004), we model discourse structures by binary
trees. Fig. (1b-f) represent the potential structures of
(1). We write each elementary discourse unit (EDU)
in square brackets.
(1) [C1 I try to read a novel] [C2 if I feel bored]
[C3 because the TV programs disappoint me]
[C4 but I can?t concentrate on anything.]
Underspecification formalisms such as domi-
nance graphs (Althaus et al, 2003) can model par-
tial information about such trees; see Fig. (1a) for
the underspecified discourse representation (UDR)
of (1). These graphs consist of labelled roots and
unlabelled holes; the solid edges indicate that a
node must be the parent of another, and the dashed
edges indicate (transitive) dominance requirements.
A configuration of a dominance graph is an arrange-
ment of the (labelled) graph nodes into a tree that
satisfies all (immediate and transitive) dominance
requirements. Subgraphs that are connected by solid
edges are called fragments and must be tree-shaped.
Using UDRs, discourse parsing can be modu-
larised into three separate steps. First, a discourse
parser segments the text and generates an UDR from
it. The node labels in the UDR aren?t necessarily
fully specified (Egg and Redeker, 2007; Schilder,
245
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
C
1
C
2
C
3
C
4
C
2
C
3
C
4
C
1
C
4
C
1
C
2
C
3
Condition
(1)
Condition
(1)
Condition
(1)
Condition
(1)
Cause
(2)Cause
(2)
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
Condition
(1)
Cause
(2)
Contrast
Contrast
Contrast
Condition
(1)
1
2
3 5
4
7
6
Cause
(2)
Contrast
C
1
C
2
C
3
C
4
(a) (b) (c) (d) (e) (f)
Figure 1: An underspecified discourse structure and its five configurations
2002); here we pretend that they are to simplify the
presentation, as nothing in this paper hinges on it.
Then weights are added to the UDR that incorporate
preferences for discourse structures based on lin-
guistic cues. Finally, the weighted UDR can either
be processed directly by other applications, or, if a
tree structure is required, we can compute the best
configuration. In this paper, we show how an UDR
dominance graph can be converted into a regular tree
grammar efficiently. This simplifies the specifica-
tion of weights in Step 2; we also show how to ef-
ficiently compute a best tree from a weighted RTG
(Step 3). We do not discuss Step 1 in this paper.
The dominance graphs used in discourse under-
specification are constrained chains. A constrained
chain of length n consists of n upper fragments with
two holes each and n+ 1 lower fragments with no
holes. There must also be a numbering 1, . . . ,2n+1
of the fragments such that for every 1? i? n, frag-
ment 2i is an upper fragment, fragments 2i? 1 and
2i+1 are lower fragments, and there are dominance
edges from the left hole of 2i to the root of 2i?1 and
from the right hole of 2i to the root of 2i+ 1 (and
possibly further dominance edges). These numbers
are shown in circles in Fig. (1a). In discourse dom-
inance graphs, upper fragments correspond to dis-
course relations, and lower fragments correspond to
EDUs; the EDUs are ordered according to their ap-
pearance in the text, and the upper fragments con-
nect the two text spans to which they are adjacent.
3 Underspecified Processing for Discourses
Recently, Koller et al (2008) showed how to pro-
cess dominance graphs with regular tree grammars
(Comon et al, 2007, RTGs). RTGs are a grammar
formalism that describes sets of trees using produc-
tion rules which rewrite non-terminal symbols (NTs)
into terms consisting of tree constructors and possi-
bly further NTs. A tree (without NTs) is accepted
by the grammar if it can be derived by a sequence
of rule applications from a given start symbol. An
example RTG is shown in Fig. 2; its start symbol
is {1;7}, and it describes exactly the five trees in
{1;7} ? Cond({1},{3;7}) [1] {5;7} ? Contr({5},{7}) [1]
{3;7} ? Contr({3;5},{7}) [1] {3;5} ? Cause({3},{5}) [1]
{1;7} ? Contr({1;5},{7}) [1] {1;3} ? Cond({1},{3}) [5]
{1;7} ? Cause({1;3},{5;7}) [1] {1;5} ? Cond({1},{3;5}) [3]
{1;5} ? Cause({1;3},{5}) [1] {3;7} ? Cause({3},{5;7}) [1]
{1} ? C1 [1] {3} ? C2 [1] {5} ? C3 [1] {7} ? C4 [1]
Figure 2: A wRTG modelling Fig. 1
Fig. (1b-f). For example, Fig. (1e) is derived by ex-
panding the start symbol with the first rule in Fig. 2.
This determines that the tree root is labelled with
Condition; we then derive the left subtree from the
NT {1} and the right subtree from the NT {3;7}.
The NTs in the grammar correspond to subgraphs
in the dominance graph: The NT {1;7} repre-
sents the subgraph {1,2,3,4,5,6,7} (i.e. the whole
graph); the NT {1} represents the subgraph contain-
ing only the fragment 1; and so forth. The trees that
can be derived from each nonterminal correspond
exactly to the configurations of the subgraph.
Koller and Thater (2005b) presented an algorithm
for generating, from a very general class of dom-
inance graphs, an RTG that describes exactly the
same trees. For each subgraph S that is to be the
LHS of a rule, the algorithm determines the free
fragments of S, i.e. the fragments that may serve
as the root of one of its configurations, by a certain
graph algorithm. For every free fragment in S with
n holes and a root label f , the algorithm generates a
new rule of the form S? f (S1, . . . ,Sn), where each
Si corresponds to the remaining subgraph under the
i-th hole. The procedure calls itself recursively on
the subgraphs until it reaches singleton subgraphs.
While this algorithm works well with underspec-
ified semantic representations in semantics, it is too
slow for the larger discourse graphs, as we will see in
Section 5. However, we will now optimise it for the
special case of constrained chains. First, we observe
that all subgraphs ever visited by the algorithm are
connected subchains. A subchain is uniquely identi-
fiable by the positions of the first and last fragment
in the left-to-right order of the chain; we can thus
read the nonterminal {i; j} simply as a pair of inte-
gers that identifies the subchain from the i-th to the
246
Algorithm 1: GenerateRules({i; j},G,C)
if G contains rules for {i; j} then return1
if i=j then G.add({ {i; j}? Label(i) } ) else2
/* Loop over upper fragments */
for k = i+1 to j-1 step 2 do3
if ?? edge=(s,t) ? C s.t. (i ? s < k ? t ? j) ? (i ? t4
? k < s ? j) then
lSub?{i;k-1}, rSub?{k+1; j}5
G.add({i; j}? Label(i)(lSub, rSub))6
GenerateRules(lSub, G, C)7
GenerateRules(rSub, G, C)8
j-th fragment (rather than an abbreviation for a set
of fragments). i and j will generally represent lower
fragments. In the grammar in Fig. 2, {i} is an abbre-
viation of {i; i}.
We can now rephrase the Koller & Thater algo-
rithm in our terms (Algorithm 1). The most impor-
tant change is that we can now test whether an up-
per fragment k in a subgraph {i; j} is free simply by
checking whether there is no dominance edge from
some upper fragment l to some upper fragment r
such that i? l < k ? r ? j, and no dominance edge
from r to l such that i? l ? k < r ? j. For instance,
if there was a dominance edge from the right hole of
2 to the root of 6 in Fig. (1a), then 4 and 6 would
not be free, but 2 would be; and indeed, all config-
urations of this graph would have to have 2 as their
roots. Hence we can replace the graph algorithm for
freeness by a simple comparison of integers. The
general structure of the algorithm remains the same
as in (Koller and Thater, 2005b): It takes a domi-
nance graphC as its input, and recursively calls itself
on pairs {i; j} representing subgraphs while adding
rules and NTs to an RTG G.
4 Soft Discourse Constraints
RTGs can be extended to weighted regular tree
grammars (Knight and Graehl, 2005, wRTGs) by
adding numeric weights to the rules. WRTG deriva-
tions assign weights to each tree: The weight of a
tree is the product of the weights of all rules that
were used in its derivation.
Egg and Regneri (2008) motivate the use of
wRTGs in discourse processing. They assign rule
weights based on corpus-extracted constraints which
express the interdependencies between discourse re-
lations and their surrounding tree structure. One
such constraint states that the right subtree of a Con-
1.00
15.65
244.95
3833.66
60000.00
0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230
new total utool total
Figure 3: Runtime Comparison
dition node should be of minimal size, which ranks
the readings of Fig. 1 (a): (b), (d) > (c) > (e), (f).
In order to state this constraint in a wRTG, we
annotate the grammar in Fig. 2 with the weights
shown in brackets. The Condition rules get higher
weights if the second NT on the RHS represents a
smaller subgraph. The grammar assigns the maxi-
mum weight of 5 to (b) and (d) (fragment 2 has a
leaf as right child), the medium weight 3 to (c) (the
right subgraph of fragment 2 contains two EDUs),
and the minimum weight 1 to (e) and (f). i.e. it ranks
the readings as intended.
Based on our implementation of nonterminals as
integer pairs, we can efficiently compute a con-
figuration with maximal weight using a version of
Knight and Graehl?s (2005) algorithm for comput-
ing the best derivation of a wRTG that is specialised
to the grammars we use.
5 Evaluation
We compare our runtimes with those of Utool
(Koller and Thater, 2005a), the fasted known solver
for general dominance graphs; it implements the
Koller & Thater algorithm. Utool runs very fast for
underspecified representations in semantics, but the
representations for discourse parsing are consider-
ably larger: The largest underspecified semantic rep-
resentation found in the Rondane treebank analysed
with the English Resource Grammar (Copestake and
Flickinger, 2000, ERG) has 4.5? 1012 structural
scope readings, but for 59% of the discourses in the
RST Discourse Treebank (Carlson et al, 2002, RST-
DT), there are more ways of configuring all EDUs
into a binary tree than that.
We evaluate the efficiency of our algorithm on 364
texts from the RST-DT, by converting each discourse
247
into a chain with one lower fragment for each EDU
and one upper fragment labelled with each anno-
tated discourse relation. We use our algorithm and
Utool to generate the RTG from the chain, assign
all soft constraints of Egg and Regneri (2008) to the
grammar, and finally compute the best configuration
according to this model. The evaluation results are
shown in Fig. 3. The horizontal axis shows the chain
length (= number of EDUs minus 1), rounded down
to multiples of ten; the (logarithmic) vertical axis
shows the average runtime in milliseconds for dis-
courses of that length. Both algorithms spend a bit
over half the runtime on computing the RTGs.
As the diagram shows, our algorithm is up to 100
times faster than Utool for the same discourses. It
is capable of computing the best configuration for
every tested discourse ? in less than one second for
86% of the texts. Utool exceeded the OS memory
limit on 77 discourses, and generally couldn?t pro-
cess any text with more than 100 EDUs. The longest
text in the RST-DT has 304 EDUs, so the UDR has
about 2.8?10178 different configurations. Our algo-
rithm computes the best configuration for this UDR
in about three minutes.
6 Conclusion
We presented the first solver for underspecified dis-
course representations that is efficient enough to
compute the globally best configurations of every
discourse in the RST discourse treebank, by exploit-
ing the fact that UDRs are very large but obey very
strong structural restrictions. Our solver converts
a dominance graph into an RTG, adds weights to
the RTG to represent discourse constraints, and then
computes the globally optimal configuration.
It takes about three minutes to compute a best
configuration with a given probability model for the
longest discourse in the treebank, out of 10178 pos-
sible configurations. For comparison, an algorithm
that enumerates a billion configurations per second
to find the best one could have inspected only about
1026 within the estimated age of the universe. So our
algorithm is useful and necessary to process real-
world underspecified discourse representations.
We have thus demonstrated that discourse pro-
cessing based on underspecification is computation-
ally feasible. Nothing in our algorithm hinges on
using RST in particular; it is compatible with any
approach that uses binary trees. In future research,
it would be interesting to complete our system into
a full-blown discourse parser by adding a module
that computes an UDR for a given text, and evaluate
whether its ability to delay decisions about discourse
structure would improve accuracy.
References
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn,
J. Niehren, and S. Thiel. 2003. An efficient graph
algorithm for dominance constraints. Journal of Algo-
rithms, 48:194?219.
L. Carlson, D. Marcu, and M. E. Okurowski. 2002. RST
Discourse Treebank. LDC.
H. Comon, M. Dauchet, R. Gilleron, C. Lo?ding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree Automata Techniques and Ap-
plications. Available on: http://www.grappa.
univ-lille3.fr/tata. Release 12-10-2007.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Confer-
ence on Language Resources and Evaluation.
M. Egg and G. Redeker. 2007. Underspecified discourse
representation. In A. Benz and P. Ku?hnlein, editors,
Constraints in Discourse, Amsterdam. Benjamins.
M. Egg and M. Regneri. 2008. Underspecified Mod-
elling of Complex Discourse Constraints. Submitted.
C. Gardent and B. Webber. 1998. Describing Discourse
Semantics. In Proceedings of the 4th TAG+Workshop,
University of Pennsylvania, Philadelphia.
K. Knight and J. Graehl. 2005. An overview of proba-
bilistic tree transducers for natural language process-
ing. In Computational linguistics and intelligent text
processing, pages 1?24. Springer.
A. Koller and S. Thater. 2005a. Efficient solving and
exploration of scope ambiguities. Proceedings of the
ACL-05 Demo Session.
A. Koller and S. Thater. 2005b. The evolution of dom-
inance constraint solvers. In Proceedings of the ACL-
05 Workshop on Software, Ann Arbor.
A. Koller, M. Regneri, and S. Thater. 2008. Regular tree
grammars as a formalism for scope underspecification.
In Proceedings of ACL-08: HLT.
F. Schilder. 2002. Robust discourse parsing via discourse
markers, topicality and position. Natural Language
Engineering, 8:235?255.
M. Stede. 2004. The Potsdam Commentary Corpus. In
B. Webber and D. Byron, editors, ACL-04 Workshop
on Discourse Annotation.
M. Taboada andW.Mann. 2006. Applications of Rhetor-
ical Structure Theory. Discourse Studies, 8:567?588.
248
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 511?522,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Comparison of Selectional Preference Models for Automatic Verb
Classification
Will Roberts and Markus Egg
Institut f?r Anglistik und Amerikanistik, Humboldt University
10099 Berlin, Germany
{will.roberts,markus.egg}@anglistik.hu-berlin.de
Abstract
We present a comparison of different selec-
tional preference models and evaluate them
on an automatic verb classification task in
German. We find that all the models we
compare are effective for verb clustering;
the best-performing model uses syntactic
information to induce nouns classes from
unlabelled data in an unsupervised man-
ner. A very simple model based on lexical
preferences is also found to perform well.
1 Introduction
Selectional preferences (Katz and Fodor, 1963;
Wilks, 1975; Resnik, 1993) are the tendency for
a word to semantically select or constrain which
other words may appear in a direct syntactic re-
lation with it. Selectional preferences (SPs) have
been a perennial knowledge source for NLP tasks
such as word sense disambiguation (Resnik, 1997;
Stevenson and Wilks, 2001; McCarthy and Car-
roll, 2003) and semantic role labelling (Erk, 2007);
and recognising selectional violations is thought
to play a role in identifying and interpreting meta-
phor (Wilks, 1978; Shutova et al., 2013). We focus
on the SPs of verbs, since determining which argu-
ments are typical of a given verb sheds light on the
semantics of that verb.
In this study, we present the first empirical com-
parison of different SP models from the perspective
of automatic verb classification (Schulte im Walde,
2009; Sun, 2012), the task of grouping verbs to-
gether based on shared syntactic and semantic prop-
erties.
We cluster German verbs using features captur-
ing their valency or subcategorisation, following
prior work (Schulte im Walde, 2000; Esteve Ferrer,
2004; Schulte im Walde, 2006; Sun et al., 2008;
Korhonen et al., 2008; Li and Brew, 2008), and
investigate the effect of adding information about
verb argument preferences. SPs are represented
by features capturing lexical information about the
heads of arguments to the verbs; we restrict our
focus here to nouns.
We operationalise a selectional preference model
as a function which maps such an argument head
to a concept label. We submit that the primary
characteristic of such a model is its granularity. In
our baseline condition, all nouns are mapped to the
same label; this effectively captures no information
about a verb?s SPs (i.e., we cluster verbs using sub-
categorisation information only). On the other ex-
treme, each noun is its own concept label; we term
this condition lexical preferences (LP). Between
the baseline and LP lie a spectrum of models, in
which multiple concepts are distinguished, and
each concept label can represent multiple nouns.
Our main hypothesis is that verb clustering will
work best using a model of such intermediate gran-
ularity. This follows the intuition that verbs would
seem to select for classes of nouns; for instance,
we suppose that essen ?eat? would tend to prefer as
a direct object a noun from the abstract concept Es-
sen (?food?). We assume that these concepts can be
expressed independently of particular predicates;
that is, there exist selectional preference models
that will work for all verbs (and all grammatical
relations). Further benefits of grouping nouns into
classes include combating data sparsity, as well
as deriving models which can generalise to nouns
unseen in training data.
Another parameter of a selectional preference
model is the methodology used to induce the con-
ceptual classes; put another way, the success of
an SP model hinges on how it represents concepts.
In this paper, we investigate the choice of noun
categorisation method through an empirical com-
parison of selectional preference models previously
used in the literature.
We set out to investigate the following questions:
1. What classes of nouns are effective descriptors
511
of selectional preference concepts? For ex-
ample, do they correspond to features such as
ANIMATE?
2. What is the appropriate granularity of selec-
tional preference concepts?
3. Which methods of classifying nouns into con-
cepts are most effective at capturing selec-
tional preferences for verb clustering?
This paper is structured as follows: In Section 2,
we introduce our baseline method of clustering
verbs using subcategorisation information and de-
scribe evaluation; Section 3 lists the models of se-
lectional preferences that we compare in this work;
Section 4 presents results and discussion; Section 5
summarises related work; and Section 6 concludes
with directions for future research.
2 Automatic verb classification
Verb classifications such as VerbNet (Kipper-
Schuler, 2005) allow generalisations about the syn-
tax and semantics of verbs and have proven useful
for a range of NLP tasks; however, creation of these
resources is expensive and time-consuming. Auto-
matic verb classification seeks to learn verb classes
automatically from corpus data in a cheaper and
faster way. This endeavour is possible due to the
link between a verb?s semantics and its syntactic be-
haviour (Levin, 1993). Recent research has found
that even automatically-acquired classifications can
be useful for NLP applications (Shutova et al., 2010;
Guo et al., 2011). In this section, we introduce the
verb classification method used by our baseline
model, which clusters verbs based on subcategor-
isation information. Following this, Section 2.2 ex-
plains the gold standard verb clustering and cluster
purity metric which we use for evaluation.
2.1 Baseline model
In this work, we take subcategorisation to mean
the requirement of a verb for particular types of
argument or concomitant. For example, the English
verb put subcategorises for subject, direct object,
and a prepositional phrase (PP) like on the shelf :
(1) [
NP
Al] put [
NP
the book] [
PP
on the shelf].
A subcategorisation frame (SCF) describes a
combination of arguments required by a specific
verb; a description of the set of SCFs which a verb
may take is called its subcategorisation preference.
We acquire descriptions of verbal SCF preferences
on the basis of unannotated corpus data.
Our experiments use the SdeWaC corpus (Faa?
and Eckart, 2013), containing 880 million words
in 45 million sentences; this is a subset of deWaC
(Baroni et al., 2009), a corpus of 10
9
words extrac-
ted from Web search results. SdeWaC is filtered
to include only those sentences which are max-
imally parsable
1
. We parsed SdeWaC with the
mate-tools dependency parser (Bohnet et al.,
2013)
2
, which performs joint POS and morpholo-
gical tagging, as well as lemmatisation. Our sub-
categorisation analyses are delivered by the rule-
based SCF tagger described by Roberts et al. (2014),
which operates using the dependency parses and as-
signs each finite verb an SCF type. The SCF tags are
taken from the SCF inventory proposed by Schulte
im Walde (2002), which indicates combinations
of nominal and verbal complement types, such as
nap:f?r.Acc (transitive verb, with a PP headed
by f?r ?for?). Examples of complements are n for
nominative subject, and a for accusative direct ob-
ject; in SCFs which include PPs (p), the SCF tag
specifies the head of the PP and the case of the pre-
positional argument (Acc in our example indicates
the accusative case of the prepositional argument).
The SCF tagger undoes passivisation and analyses
verbs embedded in modal and tense constructions.
We record 673 SCF types in SdeWaC.
From SdeWaC, we extracted the first 3,000,000
verb instances assigned an SCF tag by the SCF tag-
ger, where the verb lemma is one of the 168 listed
in our gold standard clustering (this requires ap-
proximately 270 million words of parsed text, or
25% of SdeWaC). We refer to this as our test set.
In this set, each verb is seen on average 17,857
times; the most common is geben (?give?, 328,952
instances), and the least is grinsen (?grin?, 50).
We represent verbs as vectors, where each di-
mension represents a different SCF type. Vector
entries are initialised with SCF code counts over
the test set, and each vector is then normalised to
sum to 1, so that a vector represents a discrete prob-
ability distribution over the SCF inventory. We use
the Jensen-Shannon divergence as a dissimilarity
measure between pairs of verb vectors. The Jensen-
Shannon divergence (Lin, 1991) is an information-
theoretic, symmetric measure (Equation (2)) re-
1
The filtering used a rule-based dependency parser to es-
timate a per-token parse error rate for each sentence, and
removed those sentences with very high error rates.
2
https://code.google.com/p/mate-tools/
512
lated to the Kullback-Leibler divergence (Equa-
tion (3)).
JS(p, q) = D(p||
p+ q
2
) +D(q||
p+ q
2
) (2)
D(p||q) =
?
i
p
i
log
p
i
q
i
(3)
With this dissimilarity measure, we use hier-
archical clustering with Ward?s criterion (Ward,
Jr, 1963) to partition the verbs into K disjoint sets
(i.e., hard clustering), where we match K to the
number of classes in our gold standard (described
below).
2.2 Evaluation paradigm
We evaluate the automatically induced verb cluster-
ings against a manually-constructed gold standard,
published by Schulte im Walde (2006, page 162ff.).
This Levin-style classification groups 168 high-
and low-frequency verbs into 43 semantic classes;
examples include Aspect (e.g., anfangen ?begin?),
Propositional Attitude (e.g., denken ?think?), and
Weather (e.g., regnen ?rain?). Some of the classes
are further sub-classified; for the purposes of our
evaluation, we ignore the hierarchical structure of
the classification and consider each class or sub-
class to be a separate entity. In this way, we obtain
classes of fairly comparable size and sufficient se-
mantic consistency.
3
We evaluate a given verb clustering against
the gold standard using the pairwise F -score
(Hatzivassiloglou and McKeown, 1993). To calcu-
late this statistic, we construct a contingency table
over the
(
n
2
)
pairs of verbs, the idea being that the
gold standard provides binary judgements about
whether two verbs should be clustered together or
not. If a clustering agrees with the gold standard as
to whether a pair of verbs belong together or not,
this is a ?correct? answer. Using the contingency
table, the standard information retrieval measures
of precision (P ) and recall (R) can be computed;
the F -score is then the harmonic mean of these:
F = 2PR/(P +R). The random baseline is 2.08
(calculated as the average score of 50 random parti-
tions), and the optimal score is 95.81, calculated by
evaluating the gold standard against itself. As the
gold standard includes polysemous verbs, which
3
In contrast, a top-level class like ?Transfer of Possession
(Obtaining)?, not only covers 25% of the gold standard, it also
comprises the semantically very diverse subclasses ?Transfer
of Possession (Giving)?, ?Manner of Motion?, and ?Emotion?.
belong to more than one cluster, the optimal score is
calculated by randomly picking one of their senses;
the average is then taken over 50 such trials.
The pairwise F -score is known to be somewhat
nonlinear (Schulte im Walde, 2006), penalising
early clustering ?mistakes? more than later ones,
but it has the advantage that we can easily determ-
ine statistical significance using the contingency
table and McNemar?s test.
We use only one clustering algorithm and one
purity metric, because our prior work shows that
the most important choices for verb clustering are
the distance measure used, and how verbs are rep-
resented. These factors set, we expect similar per-
formance trends from different algorithms, with
predictable variation (e.g., spectral tends to outper-
form hierarchical clustering, which in turn outper-
forms k-means). Combining Ward?s criterion and
F -score is a trade-off at this point; the criterion is
deterministic, giving reproducible results without
computational complexity, but disallows estimates
of density over our evaluation metric and is greedy
(see discussion in Section 4.3).
3 Selectional preference models
In this section, we introduce the various SP models
that we compare in this paper. In all cases, we
hold the verb clustering procedure described in the
previous section unchanged, with the exception
that SCF tags for verbs are parameterised for
selectional preferences. As an example, a verb
instance observed in a simple transitive frame with
a nominal subject and accusative object would
receive the SCF tag na. Assuming that a given SP
model places the subject noun in the SP concept
animate and the object noun in the concept
concrete, the parameterised SCF tag would be
na
*
subj-{animate}
*
obj-{concrete}.
This process captures argument co-occurrence
information about verb instances, and has the effect
of multiplying the SCF inventory size, making the
verb vectors described in Section 2.1 both longer
and sparser.
We evaluate various types of SP models: the
simple lexical preferences model; three models
which perform automatic unsupervised induction
of noun concepts from unlabelled data; and one
which uses a manually-built lexical resource. As
far as we are aware, two of these, the word space
and LDA models, have never been applied to verb
classification before.
513
N Coverage of test set
100 12.08%
200 17.18%
500 26.11%
1,000 32.70%
5,000 45.31%
10,000 49.09%
50,000 55.69%
100,000 57.67%
Table 1: Fraction of verb instances in the test set
parameterised by LP as a function of the number of
nouns N included in the LP model.
3.1 Lexical preferences
The LP model is the simplest in our study after the
baseline condition; it simply maps a noun to its own
lemma. We include as a parameter of the LP model
a maximum number of nouns N to admit as LP
tags. In this way, the LP model parameterises SCFs
using only the N most frequent nouns in SdeWaC;
nouns beyond rank N are treated as if they were
unseen. Table 1 indicates what fraction of the 3
million verb instances receive SCF tags specifying
one or more LPs as a function of this parameter.
Note that the coverage approaches an asymptote
of around 60%. This is due to the fact that noun
arguments are not observed for every verb instance;
many verbs? arguments are pronominal or verbal
and are not treated by our SP models. Setting N
allows a simple way of tuning the LP model: With
increasing N , the LP model should capture more
data about verb instances, but after a point this
benefit should be cancelled out by the increasing
sparsity in the verb vectors.
3.2 Sun and Korhonen model
The SP model described in this section (SUN) was
first used by Sun and Korhonen (2009) to de-
liver state-of-the-art verb classification perform-
ance for English; more recently, the technique was
applied to successfully identify metaphor in free
text (Shutova et al., 2010; Shutova et al., 2013).
It uses co-occurrence counts that describe which
nouns are found with which verbs in which gram-
matical relations; this information is used to sort the
nouns into classes in a procedure almost identical
to our verb clustering method described in Sec-
tion 2.1.
We extract all verb instances in SdeWaC which
are analysed by the SCF tagger, and count all
(verb, grammatical relation, nominal argument
head) triples, where the grammatical relation is
subject, direct (accusative) object, indirect (dative)
object, or prepositional object
4
, and is listed in the
verb instance?s SCF tag; we undo passivisation, re-
move instances of auxiliary and modal verbs, and
filter out those triples seen less than 10 times in the
corpus.
These observations cover 60,870 noun types and
33,748,390 tokens, co-occurring with 6,705 verb
types (11,426 verb-grammatical-relation types); an
example is (sprechen, obj, Wort) (?speak? with dir-
ect object ?word?, occurring 1,585 times)
5
. We rep-
resent each noun by a vector whose 11,426 dimen-
sions are the different verb-grammatical-relation
pairs; coordinates in the vector indicate the ob-
served corpus counts. The vectors are then norm-
alised to sum to 1, such that each represents some
particular noun?s discrete probability distribution
over the set of verb-grammatical-relation pairs. The
distance between two noun vectors is defined to
be the Jensen-Shannon divergence between their
probability distributions, and we partition the set
of nouns into M groups using hierarchical Ward?s
clustering.
The SP model then maps a noun to an arbitrary
label indicating which of the M disjoint sets that
noun is to be found in (i.e., all nouns in the first
noun class map to the concept label concept1);
we employ the parameter M to model SP concept
granularity. As with the LP model, we use the
parameter N to indicate how many nouns are in-
cluded in the SUN model; we search the parameter
values N = {300, 500, 1000, 5000, 10000} and
N
M
= {5, 10, 15, 20, 30, 50}.
3.3 Word space model
Word space models (WSMs, (Sahlgren, 2006;
Turney and Pantel, 2010)) use word co-occurrence
counts to represent the distributional semantics of a
word. This strategy makes possible a clustering of
nouns that does not depend on verbal dependencies
in the first place.
4
We have also experimented with adding features for each
noun showing nominal modification features (e.g., (schwarz,
nmod, Haar), ?hair? modified by ?black?), but these seem to
hurt performance.
5
Triples representing prepositional object relations are dis-
tinguished by preposition (e.g., the triple (geben, prep-in,
Auftrag), ?give? with PP headed by ?in? with argument head
?contract?, an idiomatic expression meaning ?to commission?
something).
514
Dagan et al. (1999) address the problem of data
sparseness for the automatic determination of word
co-occurrence probabilities, which includes selec-
tional preferences. They introduce the idea of es-
timating the probability of hitherto unseen word
combinations using available information on words
that are closest w.r.t. distributional word similar-
ity. Following this idea, Erk (2007) and Pad? et al.
(2007) describe a memory-based SP model, using a
WSM similarity measure to generalise the model to
unseen data.
We build a WSM of German nouns and use it to
partition nouns into disjoint sets, which we then
employ as with the SUN model. We compute word
co-occurrence counts across the whole SdeWaC
corpus, using as features the 50,000 most common
words in SdeWaC, skipping the first 50 most com-
mon words (i.e., we use words 50 through 50,050),
with sentences as windows. We lemmatise the cor-
pus and remove all punctuation; no other normalisa-
tion is performed. Co-occurrence counts between
a word w
i
and a feature c
j
are weighted using the
t-test scheme:
ttest(w
i
, c
j
) =
p(w
i
, c
j
)? p(w
i
)p(c
j
)
?
p(w
i
)p(c
j
)
We use a recent technique called context selec-
tion (Polajnar and Clark, 2014) to improve the word
space model, whereby only the C most highly
weighted features are kept for each word vector.
We set C by optimising the correlation between the
word space model?s cosine similarity and a data
set of human semantic relatedness judgements for
65 word pairs (Gurevych and Niederlich, 2005); at
C = 380, we obtain Spearman ? = 0.813 and Pear-
son r = 0.707 (human inter-annotator agreement
for this data set is given as r = 0.810).
After this, we build a similarity matrix between
all pairs of nouns using the cosine similarity, and
then partition the set of N nouns into M disjoint
classes using spectral clustering with the MNCut
algorithm (Meil
?
a and Shi, 2001). As with the SUN
model, this SP model assigns labels to nouns indic-
ating which noun class they belong to. We search
the same parameter space for N and M as for the
SUN model.
3.4 GermaNet
Statistical models of SPs have often used WordNet
as a convenient and well-motivated inventory of
concepts (e.g., Resnik (1997), Li and Abe (1998),
Clark and Weir (2002)). Typically, such models
make use of probabilistic treatments to determine
an appropriate concept granularity separately for
each predicate; we opt here for a simple model that
allows more direct control over concept granularity.
We take the set of concepts relevant to describing
selectional preferences to be a target set of synsets
in GermaNet (Hamp and Feldweg, 1997), and rep-
resent the target set as the set of synsets which are
at some depth d or less in the GermaNet noun hier-
archy: {s | depth(s) ? d} where depth(s) counts
the number of hypernym links separating s from
the root of the hierarchy. We model concept gran-
ularity by varying d = 1 . . . 6; at d = 1, the target
set is of size 5, and at d = 6, it is of size 17,125.
Nouns are attributed to concepts as follows: Given
a noun belonging to a synset s, either s is in the
target set, or we take s?s lowest hypernym in the
target set. For polysemous nouns, each synset list-
ing a sense of the noun votes for a member of the
target set; the noun observation is then spread over
the target set using the votes as weights.
This procedure makes our GermaNet SP model a
soft clustering over nouns (i.e., a noun can belong
to more than one SP concept); a consequence of
this is that a single verb occurrence in the corpus
can contribute fractional counts to multiple SCF
types.
3.5 LDA
Latent Dirichlet allocation (Blei et al., 2003) is a
generative model that discovers similarities in data
using latent variables; it is frequently used for topic
modelling. LDA models of SPs have been proposed
by ? S?aghdha (2010) and Ritter et al. (2010);
previous to this, Rooth et al. (1999) also described
a latent variable model of SPs.
We implement the LDA model of selectional pref-
erences described by ? S?aghdha (2010). Gener-
atively, the model produces nominal arguments to
verbs as follows: For a given (verb, grammatical re-
lation) pair (v, r), (1) Sample a noun class z from a
from a multinomial distribution ?
v,r
with a Dirich-
let prior parameterised by ?; (2) Sample a noun n
from a multinomial distribution ?
z
with a Dirichlet
prior parameterised by ?. Like ? S?aghdha, we use
an asymmetric Dirichlet prior for ?
v,r
(i.e., ? can
differ for each noun class) and a symmetric prior
for ?
z
(? is the same for each ?
z
). We estimate
the LDA model using the MALLET software (Mc-
Callum, 2002) using the same (verb, grammatical
515
relation, argument head) co-occurrence statistics
used for the SUN model. We train for 1,000 it-
erations using the software?s default parameters,
allowing the LDA hyperparameters ? and ? to be
re-estimated every 10 iterations. We build mod-
els with 50 or 100 topics as a proxy to concept
granularity; models include number of nouns N of
{500, 1000, 5000, 10000, 50000, 100000}.
As with the GermaNet-based model, the LDA
model creates a soft clustering of nouns; the abil-
ity of a noun to have degrees of membership in
multiple concepts might be a good way to model
polysemy. We also experiment with a hard cluster-
ing version of the LDA model; to do this, we assign
each noun n its most likely class label z using the
model?s estimate for P (z|n).
4 Results
We experimented with applying the SP models to
different combinations of grammatical relations
(e.g., only subject, only object, subject+object,
etc.), but generally obtained better results by para-
meterising SCF tags for all grammatical relations.
Table 2 summarises the evaluation scores and para-
meter settings for the best-performing SP models,
applied to verb arguments in all four grammatical
relations (subject, direct, indirect and prepositional
object)
6
. The table also indicates the number of
SCF types constructed by each SP model (i.e., the
number of dimensions of the vectors representing
verbs).
All the SP models we compare help with auto-
matic verb clustering. Using McNemar?s test on
the contingency tables underlying the F -scores, all
models score better than the baseline at at least the
p < 0.01 level. LDA-hard is better than the Ger-
maNet, LDA-soft, WSM and LP models at at least
the p < 0.05 level; SUN is better (p ? 0.05) than
all models except LDA-hard. All other performance
differences are not statistically significant
7
.
We can also demonstrate the effectiveness of the
SP models with a regression analysis on the models?
coverage of the test set. By varying the number of
nouns N included in the SP models which use this
parameter (LP, SUN, WSM, LDA), or by paramet-
erising SCF tags with SP information only for par-
6
Due to space constraints, we do not present here a de-
tailed per-model study of performance as a function of para-
meter settings; we feel a summary to be adequate, since the
relative performances of the models reflect trends across a
range of parameter settings.
7
Using a significance criterion of p < 0.05.
ticular combinations of grammatical relations, dif-
ferent numbers of the verb instances in the test data
will end up with SP information in their SCF tags
(this is the ?coverage? statistic in Table 1); with
the exception of the GermaNet model, all of the SP
models we examine here show positive correlation
between the number of verb instances tagged for
SP information and verb clustering performance.
This effect is independent of parameter settings,
indicating the performance benefit conferred by the
SP models is robust.
4.1 Comparison of SP models
The GermaNet model is the least successful in our
study. It achieves its best performance with a depth
of 5; after this, verb clustering performance drops
off again. Verb clustering using the GermaNet
SP model is only slightly better than the baseline
condition.
Against our expectations, the hard clustering
LDA models perform better than the soft cluster-
ing ones, achieving the second highest score in our
evaluation; also, in contrast to the other SP mod-
els studied in this paper, LDA performs best with
fewer, coarser-grained topics. We observe that the
soft clustering models produce verb vectors more
than an order of magnitude longer than the hard
clustering models, and suggest that simple soft clus-
tering may be causing problems with data sparsity
that interfere with verb clustering. We have also
observed that the topics found by LDA do not rep-
resent polysemy as we had hoped. While some
of the topics discovered by the LDA models can
be easily assigned labels (e.g., body parts, people,
quantities, emotions, places, buildings, tools, etc.),
others are less cohesive. We found that frequent
words (e.g., time, person) are generated with high
probability by multiple topics in ways that do not
appear to reflect multiple word senses, and that the
100-topic models exhibit this property to a greater
extent. For instance, Zeit ?time? is highly predict-
ive of three topics in the 50-topic models, of which
only the highest-weighted topic groups time ex-
pressions together; in the 100-topic models, Zeit
is found in six topics. Again, of these six, only
the topic with the highest ? consists of time expres-
sions. In the 50-topic models, we find 11 topics that
we cannot assign a coherent label; in the 100-topic
models, there are 38 of these mismatched topics.
In our work to date, we have not found that LDA
models with greater numbers of topics find more
516
SP model Parameters Granularity F -score Number of SCF types
SUN 10,000 nouns 1,000 noun classes 39.76 248,665
LDA (hard) 10,000 nouns 50 topics 39.10 78,409
LP 5,000 nouns 38.02 388,691
WSM 10,000 nouns 500 noun classes 36.95 149,797
LDA (soft) 10,000 nouns 50 topics 35.91 1,524,338
GermaNet depth = 5 8,196 synsets 34.41 851,265
Baseline 33.47 673
Table 2: Evaluation of the best SP models.
10
0
10
1
10
2
10
3
10
4
10
5
N
31
32
33
34
35
36
37
38
P
a
i
r
F
0.0
0.1
0.2
0.3
0.4
0.5
0.6
C
o
v
e
r
a
g
e
Figure 1: Verb clustering performance (black) and
test set coverage (grey) of the LP model as a func-
tion of the number of nouns N included in the
model.
specific concepts; it is possible that this problem
might be alleviated by careful filtering of the (verb,
grammatical relation, noun) triples, but we leave
this question to future research.
The LP model is very effective, which is surpris-
ing given its simplicity. As expected, with increas-
ing N , we do observe sparsity effects which hurt
verb clustering performance (see Figure 1).
Our best performing model is SUN. Our best res-
ult is obtained with 10,000 nouns (the maximum
value of N that we tried) in 1,000 classes, giving
relatively fine-grained classes (on average 10 nouns
per class). Table 3 shows some example noun
classes learned by the SUN model. These include:
groups with synonyms or near synonyms, often in-
cluding alternate spellings of the same word (such
as in the truck grouping); and groups of closely-
related co-hyponyms, such as the body part group-
ing and the clothing grouping. In the latter, bill,
joint responsibility, complicity and inscription are
also included as things which can be borne, this
is due to the fact that the SUN noun clustering is
based on triples of verbs, grammatical relations,
and nouns.
LKW (truck), Lkw (truck), Lastwagen (truck),
Castor (container for highly radioactive mater-
ial), Laster (truck), Krankenwagen (ambulance),
Transporter (van), Traktor (tractor)
Hand (hand), Kopf (head), Fu? (foot), Haar
(hair), Bein (leg), Arm (arm), Zahn (tooth), Fell
(fur)
Leiche (corpse), Leichnam (body), Sch?del
(skull), Skelett (skeleton), Wrack (wreck), Mu-
mie (mummy), Tr?mmer (debris)
Sauna (sauna), Badezimmer (bathroom),
Schwimmbad (swimming pool), Nachbildung
(replica), Kamin (fireplace), Aufenthaltsraum
(common room), Mensa (cafeteria)
Rechnung (bill), Kopftuch (headscarf), Uniform
(uniform), Anzug (suit), Helm (helmet), Gewand
(garment), Handschuh (glove), Mitverantwor-
tung (joint responsibility), Bart (beard), R?s-
tung (armour), Mitschuld (complicity), Socke
(sock), Jeans (jeans), Sonnenbrille (sunglasses),
Aufschrift (inscription), Pullover (sweater),
Weste (vest), Handschellen (handcuffs), H?rner
(horns), Kennzeichen (marking), Tracht (tradi-
tional costume), Korsett (corset), Schuhwerk
(footwear), Kopfbedeckung (headgear), Pelz
(fur), Maulkorb (muzzle)
Missionar (missionary), Weihnachtsmann
(Santa Claus), Selbstmordattent?ter (sui-
cide bomber), Bote (messenger), Nikolaus
(Nicholas), Killer (killer), Bomber (bomber),
Osterhase (Easter bunny)
Table 3: Example noun clusters in the SUN SP
model.
517
Furthermore, there are thematically related
groups (corpse, body, etc., and sauna, bathroom,
etc.). All months are placed together in one 12-
word group.
Some classes can be easily subdivided into sep-
arate groups, and sometimes the source for this can
be guessed: For example, sports (football, golf, ten-
nis) are lumped together with musical instruments
(guitar, piano, violin) and film roles (starring role,
supporting role), these all being things that can
be played. Many groups of personal roles (such
as various kinds of government ministers) are dis-
tinguished, as are diseases and medications; other
groupings contain proper names or geographical
locations, sometimes of surprising specificity (e.g.,
authors, Biblical names, philosophers, NGOs, East-
ern European countries, foreign currencies, Ger-
man male first names, newspapers, television chan-
nels). The last group in Table 3 shows a grouping
which appears to combine two of these semantic-
ally narrow categories, in which Santa Claus and
the Easter bunny are united with killers and suicide
bombers.
4.2 Noun classes as SP concepts
The WSM SP model is not as successful as SUN, but,
due to the methodological similarity between these
two (SP concepts modelled as hard partitions of
nouns), it affords us an opportunity to investigate
the question of what properties might make for an
effective noun partition.
The WSM model partitions nouns based on
paradigmatic information (which sentence con-
texts a noun appears in), rather than SUN?s use
of syntagmatic information (which grammatical
contexts a noun appears in). Therefore, it is per-
haps not surprising that the noun classes derived
by the WSM are organised thematically, and the
synonym/co-hyponym structure observed in the
SUN noun classes is in many cases absent (e.g.,
{Pferd (horse), Reiter (rider), Stall (stable), Sattel
(saddle), Stute (mare)}; these classes can easily
conflate semantic roles (e.g., Agent for rider and
Location for stable), which is presumably unhelp-
ful for representing selectional preferences.
The distribution of noun classes also differs
between SUN and WSM. The largest noun class
in the WSM model contains 1,076 high-frequency
nouns which are semantically unrelated (day, ques-
tion, case, part, reason, kind, form, week, person,
month, . . . ). We suppose that these nouns are them-
10
5
10
6
10
7
10
8
Number of verb instances
15
20
25
30
35
40
45
P
a
i
r
F
Baseline
LP
WSM
SUN
LDA-hard
Figure 2: Verb clustering performance of SP mod-
els as a function of number of verb instances.
atically ?neutral? and are classed together by virtue
of their usage in a wide variety of sentences. This
one noun class by itself subsumes 13.6% of all
noun tokens in SdeWaC. WSM also includes 56
singleton noun classes; the variance in noun class
size is 2800. For comparison, in SUN, the largest
noun class has 73 words, and the smallest, 2 (there
are 12 of these two-word classes); noun class size
variance is 37. The 73-word class in SUN does in-
deed appear to be a grab bag (including gas, taboo,
pioneer, mustard, spy, mafia, and skinhead), but
these are uncommon words and account for only
0.1% of noun tokens in SdeWaC. The next two
most common classes (with some 40 nouns each)
are lists of names (politicians? surnames, and male
first names). The noun class in the SUN model con-
taining the largest number of high-frequency nouns
(28 nouns: human, child, woman, man, people, Mr.,
mother, father, . . . ) only covers 3.6% of noun us-
ages in SdeWaC and is both semantically cohesive
and intuitively useful as a SP concept.
These issues raise the question of why the WSM
model is effective at all for verb classification.
We think that the larger less-related noun classes
neither help nor hurt verb clustering, and we find
that some of the thematic classes represent abstrac-
tions that should be useful for describing SPs. Ex-
amples include lists of body parts, countries (separ-
ate classes for Europe, Africa, Asia, etc.), diseases,
human names, articles of clothing, and the group
{fruit, apple, banana, pear, strawberry}.
4.3 Effects of test set size
We were curious if the success of the LP model
might be due to the size of the test set preventing
518
sparsity from becoming a problem. To pursue this
question, we take the four best performing SP mod-
els and run the verb clustering evaluation with the
number of verb instances in the test set varying
between 10,000 and the full SdeWaC corpus (11
million). The results are displayed in Figure 2. This
graph indicates that below 3? 10
5
verb instances,
sparsity seems to become a problem for all mod-
els on this task, and the baseline delivers the best
performance. Above this threshold, it seems that
sparsity is not a major issue: LP performs fairly con-
sistently, and is competitive with the SUN model.
We attribute this to our use of the Jensen-Shannon
divergence as a verb dissimilarity measure, which
seems relatively robust to data sparsity. The LDA-
hard model with its fewer topics seems to do quite
well with fewer data; as the test set size increases, it
drops off in the rankings. At the maximum number
of verb instances, the best-performing models are
SUN, WSM and the lexical preferences. The figure
also shows that our evaluation metric is not smooth
(note, e.g., the fluctuations in the baseline score).
We believe that this reflects a degree of instability
in the Ward?s hierarchical clustering algorithm; this
clustering method is greedy, and clustering errors
can be expected to propagate, which might explain
the jaggedness of the plot.
4.4 Conclusions
To conclude, we summarise the results of our ana-
lysis, using the questions formulated in the Intro-
duction as guidelines.
First, we wanted to compare the efficiency of
different classes of nouns as descriptors of selec-
tional preference concepts. Our findings suggest
that noun classes are most effective when they are
semantically highly consistent, representing groups
of strongly related nouns. It seems reasonable that
SP concepts representing collections of synonyms
would be useful for generalising observations, and
should represent arguments better than simple LP.
A classification of proper names (e.g., as human,
corporation, country, medication) is also useful.
This implies that we can expect features such as
ANIMATE to be shared by all members of a noun
cluster.
Second, we were interested in the appropriate
granularity of selectional preference concepts. In
our evaluation, we have observed a tendency for
smaller, more specific noun classes to be superior;
this holds because data sparsity is not a problem
in our experiment. Beyond this finding, we would
have liked to present a direct juxtaposition of differ-
ent models on ?granularity? but this is difficult: We
have not yet identified a strong abstraction of gran-
ularity from the proxies we use (e.g., GermaNet
depth, or SUN?s N/M ).
Finally, which methods of classifying nouns into
concepts are most effective at capturing selectional
preferences for verb clustering? In our experiments,
the SUN and LDA-hard models proved to be more
effective than lexical preferences, supporting our
primary hypothesis that some level of SP concept
granularity above the lexical level is desirable for
verb clustering. On the other hand, the LP model is
only slightly worse than SUN and LDA-hard, mak-
ing it attractive because it is so simple. As we have
shown, the potential data sparsity issues with LP
can be alleviated by judiciously choosing the value
of the N parameter that controls the number of
nouns included in the model. In addition, compar-
ing the SUN and WSM models, and observing the
performance of the LDA-hard method, we conclude
that inducing noun classes using syntagmatic in-
formation is more effective than using paradigmatic
relations.
5 Related work
In this study, we have looked at the utility of selec-
tional preferences for automatic verb classification.
Some previous research has followed this line of
inquiry, though prior studies have not compared
alternative methods of modelling SPs. Schulte im
Walde (2006) presented a detailed examination of
parameters for k-means-based verb clustering in
German, using the same gold standard that we em-
ploy here. She reports on the effects of adding SP
information to a SCF-based verb clustering using
15 high-level GermaNet synsets as SP concepts; SP
information for some combinations of grammatical
relations improves clustering performance slightly,
but neither are the effects consistent, nor is the
improvement delivered by the SP model over the
SCF-based baseline statistically significant. Schulte
im Walde et al. (2008) used expectation maximisa-
tion to induce latent verb clusters from the British
National Corpus while simultaneously building a
tree cut model of SPs on the WordNet hierarchy
using a minimum description length method; their
evaluation focuses on the induced soft verb clusters,
reporting the model?s estimated perplexity of (verb,
grammatical relation, argument head) triples. The
519
SPs are described qualitatively by presenting two
example cases. Sun and Korhonen (2009) study
the effect of adding selectional preferences to a
subcategorisation-based verb clustering in Eng-
lish using the SUN model (see Section 3.2). They
demonstrate that adding SPs to the SCF preference
data leads to the best results on their two clustering
evaluations; overall, their best results come from
using SP information only for the subject gram-
matical relation. They employ coarse SP concepts
(20 or 30 noun clusters) which capture general se-
mantic categories (Human, Building, Idea, etc.).
Selectional preferences are usually evaluated
either from a word sense disambiguation stand-
point using pseudo-words (Chambers and Juraf-
sky, 2010), or in terms of how acceptable an ar-
gument is with a verb, via regression against hu-
man plausibility judgements. Several studies have
compared SP methodologies from the latter per-
spective. These include Brockmann and Lapata
(2003), who compared three GermaNet-based mod-
els of SP, showing that different models were most
effective for describing different grammatical re-
lations; ? S?aghdha (2010), who compared dif-
ferent LDA-based models of SP, showing these to
be effective for a variety of grammatical relations;
and ? S?aghdha and Korhonen (2012), who show
that WordNet tree cut models, LDA, and a hybrid
LDA-WordNet model are effective for describing
verb-object relations.
6 Future work
Our GermaNet model delivered disappointing per-
formance in this study; we would be interested in
seeing whether a more sophisticated implementa-
tion such as the tree cut model of Li and Abe (1998)
would be more competitive. We also would like to
explore alternative noun clustering methods such
as CBC (Pantel and Lin, 2002) and Brown clusters
(Brown et al., 1992), which were not covered in
this work; these would fit easily into our SP eval-
uation paradigm. More challenging would be a
verb classification-based evaluation of the SP mod-
els of (Rooth et al., 1999) and (Schulte im Walde
et al., 2008), which use expectation maximisation
to simultaneously cluster verbs into verb classes
and nominal arguments into noun classes; these ap-
proaches are not compatible with the evaluation
framework we have used here. Finally, the SP
model of Bergsma et al. (2008) has also achieved
impressive results on a number of tasks, but has not
been investigated for use in verb classification.
Our verb clustering evaluation in this work
has matched K, the number of clusters found by
Ward?s method, to the number of classes in the
gold standard. Since the number of clusters has an
influence on the quality of the ensuing semantic
classification (Schulte im Walde, 2006, page 180f.),
we will also be running our experiments with dif-
ferent settings of K to explore whether this also
influences the overall results of our evaluation.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed Web-
crawled corpora. Language Resources and Evalu-
ation, 43(3):209?226.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference
from unlabeled text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 59?68.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Rich?rd Farkas, Filip Ginter, and Jan Haji?c. 2013.
Joint morphological and syntactic analysis for richly
inflected languages. Transactions of the Association
for Computational Linguistics, 1:415?428.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional pref-
erence acquisition. In Proceedings of the Tenth Con-
ference on European Chapter of the Association for
Computational Linguistics, pages 27?34.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Nathanael Chambers and Daniel Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 445?453.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Ido Dagan, Lillian Lee, and Fernando C.N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 34(1?3):43?
69.
520
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 216?223.
Eva Esteve Ferrer. 2004. Towards a semantic classi-
fication of Spanish verbs based on subcategorisation
information. In Proceedings of the Student Research
Workshop at the Annual Meeting of the Association
for Computational Linguistics, pages 37?42.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC - A
corpus of parsable sentences from the Web. In Lan-
guage processing and knowledge in the Web, pages
61?68. Springer, Berlin, Heidelberg.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumentat-
ive zoning of scientific documents. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 273?283.
Iryna Gurevych and Hendrik Niederlich. 2005. Com-
puting semantic relatedness in German with revised
information content metrics. In Proceedings of "On-
toLex 2005 - Ontologies and Lexical Resources"
IJCNLP?05 Workshop, pages 28?33.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet:
A lexical-semantic net for German. In Proceedings
of ACL Workshop on Automatic Information Extrac-
tion and Building of Lexical Semantic Resources for
NLP Applications, pages 9?15.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1993. Towards the automatic identification of ad-
jectival scales: Clustering adjectives according to
meaning. In Proceedings of the 31st Annual Meet-
ing on Association for Computational Linguistics,
pages 172?182.
Jerrold J. Katz and Jerry A. Fodor. 1963. The structure
of a semantic theory. Language, 39:170?210.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classifica-
tion of verbs in biomedical texts. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 449?456.
Beth Levin. 1993. English verb classes and altern-
ations: A preliminary investigation. University of
Chicago Press, Chicago.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational Linguistics, 24(2):217?244.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In Pro-
ceedings of ACL-08: HLT, pages 434?442.
Jianhua Lin. 1991. Divergence measures based on the
Shannon entropy. IEEE Transactions on Informa-
tion Theory, 37(1):145?151.
Andrew McCallum. 2002. MALLET: A machine
learning for language toolkit.
Diana McCarthy and John Carroll. 2003. Disambig-
uating nouns, verbs, and adjectives using automat-
ically acquired selectional preferences. Computa-
tional Linguistics, 29(4):639?654.
Marina Meil?a and Jianbo Shi. 2001. A random walks
view of spectral segmentation. In Proceedings of the
International Conference on Artificial Intelligence
and Statistics (AISTATS).
Diarmuid ? S?aghdha and Anna Korhonen. 2012.
Modelling selectional preferences in a lexical hier-
archy. In Proceedings of the 1st Joint Conference on
Lexical and Computational Semantics, pages 170?
179.
Diarmuid ? S?aghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 435?444.
Sebastian Pad?, Ulrike Pad?, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plausib-
ility judgements. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 400?409.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the Eighth
ACM SIGKDD International Conference on Know-
ledge Discovery and Data Mining, pages 613?619.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 230?
238.
Philip Resnik. 1993. Selection and information: A
class-based approach to lexical relationships. Ph.D.
thesis, University of Pennsylvania.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52?57.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A lat-
ent Dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 424?434.
Will Roberts, Markus Egg, and Valia Kordoni. 2014.
Subcategorisation acquisition from raw text for a
free word-order language. In Proceedings of the
521
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 298?
307.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantic-
ally annotated lexicon via EM-based clustering. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics, pages 104?111.
Magnus Sahlgren. 2006. The word-space model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining
EM training and the MDL principle for an automatic
verb classification incorporating selectional prefer-
ences. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics,
pages 496?504.
Sabine Schulte im Walde. 2000. Clustering verbs se-
mantically according to their alternation behaviour.
In Proceedings of the 18th Conference on Computa-
tional Linguistics, pages 747?753.
Sabine Schulte im Walde. 2002. A subcategorisation
lexicon for German verbs induced from a lexicalised
PCFG. In Proceedings of the 3rd Conference on
Language Resources and Evaluation (LREC), pages
1351?1357.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?194.
Sabine Schulte im Walde. 2009. The induction of
verb frames and verb classes from corpora. In
Anke L?deling and Merja Kyt?, editors, Corpus
linguistics: An international handbook, volume 2,
chapter 44, pages 952?971. Mouton de Gruyter, Ber-
lin.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
1002?1010.
Ekaterina Shutova, Simone Teufel, and Anna
Korhonen. 2013. Statistical metaphor processing.
Computational Linguistics, 39(2):301?353.
Mark Stevenson and Yorick Wilks. 2001. The interac-
tion of knowledge sources in word sense disambigu-
ation. Computational Linguistics, 27(3):321?349.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638?647.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the Ninth International Confer-
ence on Intelligent Text Processing and Computa-
tional Linguistics, pages 16?27.
Lin Sun. 2012. Automatic induction of verb classes
using clustering. Ph.D. thesis, University of Cam-
bridge, Cambridge.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Joe H. Ward, Jr. 1963. Hierarchical grouping to optim-
ize an objective function. Journal of the American
Statistical Association, 58(301):236?244.
Yorick Wilks. 1975. An intelligent analyzer and un-
derstander of English. Communications of the ACM,
18(5):264?274.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197?223.
522
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 298?307,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Subcategorisation Acquisition from Raw Text for a Free Word-Order
Language
Will Roberts and Markus Egg and Valia Kordoni
Institute f?ur Anglistik und Amerikanistik, Humboldt University
10099 Berlin, Germany
{will.roberts,markus.egg,evangelia.kordoni}@anglistik.hu-berlin.de
Abstract
We describe a state-of-the-art automatic
system that can acquire subcategorisation
frames from raw text for a free word-order
language. We use it to construct a subcate-
gorisation lexicon of German verbs from a
large Web page corpus. With an automatic
verb classification paradigm we evaluate
our subcategorisation lexicon against a pre-
vious classification of German verbs; the
lexicon produced by our system performs
better than the best previous results.
1 Introduction
We introduce a state-of-the-art system for the ac-
quisition of subcategorisation frames (SCFs) from
large corpora, which can deal with languages with
very free word order. The concrete language we
treat is German; its word order variability is illus-
trated in (1)?(4), all of which express the sentence
The man gave the old dog a chop:
(1) Dem alten Hund gab der Mann ein Schnitzel.
(2) Ein Schnitzel gab dem alten Hund der Mann.
(3) Ein Schnitzel gab der Mann dem alten Hund.
(4) Der Mann gab dem alten Hund ein Schnitzel.
On the basis of raw text, the system can be
used to build extensive SCF lexicons for German
verbs. Subcategorisation means that lexical items
require specific obligatory concomitants or argu-
ments; we focus on verb subcategorisation. E.g.,
the verb geben ?give? requires three arguments, the
nominative subject der Mann ?the man?, the dative
indirect object dem alten Hund ?the old dog?, and
the accusative direct object ein Schnitzel ?a chop?.
Other syntactic items may be subcategorised for,
too, e.g. both stellen and its English translation
put subcategorise for subject, direct object, and a
prepositional phrase (PP) like on the shelf :
(5) [
NP
Al] put [
NP
the book] [
PP
on the shelf].
Subcategorisation frames describe a combina-
tion of arguments required by a specific verb. The
set of SCFs for a verb is called its subcategori-
sation preference. Our system follows much pre-
vious work by counting PPs that accompany the
verb among its complements, even though they are
not obligatory (so-called ?adjuncts?), because PP
adjuncts are excellent clues to a verb?s semantics
(Sun et al., 2008). However, nominal and clausal
adjuncts do not count as verbal complements.
SCF information can benefit all applications
that need information on predicate-argument struc-
ture, e.g., parsing, verb clustering, semantic role la-
belling, or machine translation. Automatic acquisi-
tion of SCF information with minimal supervision
is also crucial to construct useful resources quickly.
The main innovation of the presented new sys-
tem is to address two challenges simultaneously,
viz., SCF acquisition from raw text and the focus
on languages with a very free word order. With
this system, we create an SCF lexicon for German
verbs and evaluate this lexicon against a previously
published manual verb classification, showing bet-
ter performance than has been reported until now.
After an overview of previous work on SCF ac-
quisition in Section 2, Section 3 describes our sub-
categorisation acquisition system, and Section 4
the SCF lexicon that we build using it. In Sec-
tions 5 and 6 we evaluate the SCF lexicon on a verb
classification task and discuss our results; Section 7
then concludes with directions for future work.
298
2 Previous work
To date, research on SCF acquisition from corpora
has mostly targeted English. Brent and Berwick
(1991) detect five SCFs by looking for attested
contexts where argument slots are filled by closed-
class lexical items (pronouns or proper names).
Briscoe and Carroll (1997) detect 163 SCFs with
a system that builds an SCF lexicon whose en-
tries include the relative frequency of SCF classes.
Potential SCF patterns are extracted from a cor-
pus parsed with a dependency-based parser, and
then filtered by hypothesis testing on binomial fre-
quency data. Korhonen (2002) refines Briscoe and
Carroll (1997)?s system using back-off estimates
on the WordNet semantic class of the verb?s pre-
dominant sense, assuming that semantically similar
verbs have similar SCFs, following Levin (1993).
Some current statistical methods for Semantic Role
Labelling build models that also capture subcat-
egorisation information, e.g., Grenager and Man-
ning (2006). Schulte im Walde (2009) offers a re-
cent survey of the SCF acquisition literature.
SCF acquisition is also an important step in the
automatic semantic role labelling (Grenager and
Manning, 2006; Lang and Lapata, 2010; Titov and
Klementiev, 2012). Semantic roles of a verb de-
scribe the kind of involvement of entities in the
event introduced by the verb, e.g., as agent (active,
often not affected by the event) or patient (passive,
often affected). On the basis of these SCFs, se-
mantic roles can be assigned due to the interdepen-
dence between semantic roles and their syntactic
realisations, called Argument Linking (Levin, 1993;
Levin and Rappaport Hovav, 2005).
Acquiring SCFs for languages with a very fixed
word order like English needs only a simple syn-
tactic analysis, which mainly relies on the prede-
termined sequencing of arguments in the sentence,
e.g., Grenager and Manning (2006). When word
order is freer, the analysis gets more complicated,
and must include a full syntactic parse.
What is more, German is a counterexample to
Manning?s (1993) expectation that freedom of
word order should be matched by an increase in
case and/or agreement marking. This is due to a
very high degree of syncretism (identity of word
forms) in German paradigms for nouns, adjectives,
and determiners. E.g., the noun Auto ?car? has only
two forms, Auto for nominative, dative, and ac-
cusative singular, and Autos for genitive singular
and all four plural forms. This is in contrast to some
other free word order languages for which SCF
acquisition has been studied, like Modern Greek
(Maragoudakis et al., 2000) and Czech (Sarkar and
Zeman, 2000). A one-many relation between word
forms and case is also one of the problems for SCF
acquisition in Urdu (Ghulam, 2011).
For German, initial studies used semi-automatic
techniques and manual evaluation (Eckle-Kohler,
1999; Wauschkuhn, 1999). The first automatic sub-
categorisation acquisition system for German is de-
scribed by Schulte im Walde (2002a), who defined
an SCF inventory and manually wrote a grammar
to analyse verb constructions according to these
frames. A lexicalised PCFG parser using this gram-
mar was trained on 18.7 million words of German
newspaper text; the trained parser model contained
explicit subcategorisation frequencies, which could
then be extracted to construct a subcategorisation
lexicon for 14,229 German verbs. This work was
evaluated against a German dictionary, the Duden
Stilw?orterbuch (Schulte im Walde, 2002b).
Schulte im Walde and Brew (2002) used the sub-
categorisation lexicon created by the system to au-
tomatically induce a set of semantic verb classes
with an unsupervised clustering algorithm. This
clustering was evaluated against a small manually
created semantic verb classification. Schulte im
Walde (2006) continues this work using a larger
manual verb classification. The SCFs used in this
study are defined at three levels of granularity. The
first level (38 different SCFs) lists only the comple-
ments in the frame; the second one adds head and
case information for PP complements (183 SCFs).
The third level examined the effect of adding selec-
tional preferences, but results were inconclusive.
A recent paper (Scheible et al., 2013) describes a
system similar to ours, built on a statistical depen-
dency parser, and using some of the same kinds
of rules as we describe in Section 3.1; this system
is evaluated in a task-based way (e.g., to improve
the performance of a SMT system) and cannot be
directly compared to our system in this paper.
3 The SCF acquisition system
This section describes the first contribution of this
paper, a state-of-the-art subcategorisation acquisi-
tion system for German. Its core component is a
rule-based SCF tagger which operates on phrase
structure analyses, as delivered by a statistical
parser. Given a parse of a sentence, the tagger as-
signs each finite verb in the sentence an SCF type.
299
We use the SCF inventory of Schulte im Walde
(2002a), which includes complements like n for
nominative subject, a for accusative direct object,
d for dative indirect object, r for reflexive pronoun,
and x for expletive es (?it?) subject. Clausal com-
plements can be infinite (i); finite ones can have
the verb in second position (S-2) or include the
complementiser dass ?that? (S-dass). Comple-
ments can be combined as in na (transitive verb);
for PPs in SCFs, the head is specified, e.g., p:f?ur
for PP complements headed by f?ur ?for?
1
.
Due to the free word order, simple phrase struc-
ture like that used for analysis of English is not
enough to specify the syntax of German sentences.
Therefore we use the annotation scheme in the
manually constructed German treebanks NEGRA
and TIGER (Skut et al., 1997; Brants et al., 2002),
which decorate parse trees with edge labels specify-
ing the syntactic roles of constituents. We automat-
ically annotate the parse trees from our statistical
parser using a simple machine learning model.
In the next section, we illustrate the operation of
the SCF tagger with reference to examples; then in
Section 3.2 we describe our edge labeller.
3.1 The SCF tagger
The SCF tagger begins by collecting complements
co-occurring with a verb instance using the phrase
structure of the sentence. In our system, we obtain
phrase structure information for unannotated text
using the Berkeley Parser (Petrov et al., 2006), a
statistical unlexicalised parser trained on TIGER.
Fig. 1 illustrates the phrase structure analysis and
edge labels in the TIGER corpus for (6):
(6) Das hielte ich f?ur moralisch au?erordentlich
fragw?urdig.
?I?d consider that morally extremely
questionable?.
Its finite verb hielte (from halten ?hold?) has
three complements, the subject ich ?I?, edge-
labelled with SB, the direct object das ?that?, la-
belled with OA, and a PP headed by f?ur ?for? (MO
stands for ?modifier?). After collecting comple-
ments, the SCF tagger uses this edge label infor-
mation to determine the complements? syntactic
roles, and assigns the verb the corresponding SCF;
in the case of halten above, the SCF is nap:f?ur.
1
We digress from Schulte im Walde?s original SCF inven-
tory in that we do not indicate case information in PPs.
The rule-based SCF tagger handles auxiliary and
modal verb constructions, passive alternations, sep-
arable verb prefixes, and raising and control con-
structions. E.g., the subject sie ?they? of anfangen
?begin? in (7) doubles as the subject of its infinite
clausal complement; hence, it shows up in the SCF
of the complement?s head geben ?give?, too:
(7) Sie fingen an, mir Stromschl?age zu geben.
?They started to give me electric shocks.?
The tagger also handles involved cases with
many complements, including PPs and clauses as
in (8). As the SCF inventory allows at most three
complements in an SCF, such cases call for pri-
oritising of verbal complements (e.g., subjects, ob-
jects, and clausal complements are preferred over
PP complements). Consequently, the main verb
empfehlen ?recommend? in (8), which has a subject,
a dative object, a PP, and an infinitival clausal com-
plement, is assigned the SCF ndi. Another chal-
lenging task which relies on edge label information
is filtering out clausal adjuncts (relative clauses and
parentheticals) so as not to include them in SCFs.
(8) [
PP
Am Freitag] empfahl [
NP:Nom
der
Aufsichtsrat] [
NP:Dat
den Aktion?aren], [
S
das
Angebot abzulehnen].
?On Friday the board of directors advised
shareholders to turn down the offer.?
The 17 rules of the SCF tagger are simple; most
of them categorise the complements of a specific
verb instance; e.g., if a nominal complement to the
verb is edge-labelled as a nominative subject, add n
to the verb?s SCF, unless the verb is in the passive,
in which case add a to the SCF.
Our system was optimised by progressively re-
fining the SCF tagger?s rules through manual error
analysis on sentences from TIGER. The result is
an automatic SCF tagger that is resilient to varia-
tions in sentence structure and is firmly based on
linguistically motivated knowledge. As a test case
for its linguistic soundness, we chose the perfect
parses in the TIGER treebank and found that the
tagger is very accurate in capturing subcategorisa-
tion information inherent in these data.
3.2 The edge labeller
To obtain edge label information for the parses de-
livered by the Berkeley Parser, we built a novel
machine learning classifier to annotate parse trees
300
SPDS
Das
VVFIN
hielte
PPER
ich
PP
APPR
f?ur
AP
moralisch au?erordentlich fragw?urdig
$.
.
OA
HD
SB
MO
AC
NK
Figure 1: Edge labels in the TIGER corpus.
with TIGER edge label information. This edge la-
beller is a maximum entropy (multiclass logistic
regression) model built using the Stanford Classi-
fier package
2
. We include features such as:
? The part of speech of the complement;
? The first word of the complement;
? The lexical head of the complement;
? N-grams on the end of the lexical head of the
complement;
? The kind of article of a complement;
? The presence or absence of specific article
forms in other complements to the same verb;
? Position of the complement with respect to a
reflexive pronoun in the sentence;
? The lemmatised form of the verb governing
the complement (i.e., the verb on which the
complement depends syntactically);
? The clause type of the governing verb; and,
? Active or passive voice of the governing verb.
We do no tuning and use the software?s default
hyperparameters (L2 regularisation with ? = 3).
This classifier was trained from edge label data
extracted from the NEGRA and TIGER corpora;
our training set contained 300,000 samples (ap-
proximately 25% from NEGRA and 75% from
TIGER). On a held-out test set of 10% (contain-
ing 34,000 samples), the classifier achieves a final
F-score of 95.5% on the edge labelling task.
The edge labeller makes the simplifying assump-
tion that verbal complements can be labelled inde-
pendently. Consequently, it tends to annotate multi-
ple complements as subject for each verb. This has
to do with the numerical dominance of subjects,
which make up about 40% of all verb complements,
more than three times the number of the next most
common complement type (direct object).
Therefore we first collect all possible labels with
associated probabilities that the edge labeller as-
2
http://nlp.stanford.edu/software/
classifier.shtml
signs to each complement of a verb. We then
choose the set of labels with the highest probability
that includes at most one subject and at most one
accusative direct object for the verb, assuming that
the joint probability of a set of labels is the product
of the individual label probabilities.
We use our edge labeller in this work for mor-
phological disambiguation of nominals and for
identifying clausal adjuncts, but the edge labeller
is a standalone reusable component, which might
be equally well be used to mark up parse trees for,
e.g., a semantic role labelling system.
4 The subcategorisation lexicon
With the system described in Sec. 3, we build a Ger-
man subcategorisation lexicon that collects counts
of ?lemma,SCF? on deWaC (Baroni et al., 2009),
a corpus of text extracted from Web search re-
sults, with 10
9
words automatically POS-tagged
and lemmatised by the TreeTagger (Schmid, 1994).
A subset of this corpus, SdeWaC (Faa? and Eckart,
2013), has been preprocessed to include only sen-
tences which are maximally parsable; this smaller
corpus includes 880 million words in 45 million
sentences. We parsed 3 million sentences (80 mil-
lion words) of SdeWaC; after filtering out those
verb lemmas seen only five times or fewer in the
corpus, we are left with statistics on 8 million verb
instances, representing 9,825 verb lemmas.
As a concrete example for the resulting SCF lexi-
con, consider the entry for sprechen ?talk? in Fig. 2,
which occurs 16,254 times in our SCF lexicon.
Sprechen refers to a conversation with speaker,
hearer, topic, message, and code: Speakers are ex-
pressed by nominative NPs, hearers, by mit-, bei-
or zu-PPs, topics, by von- and ?uber-PPs. The code
is expressed in in-PPs, and the message, by ac-
cusative NPs (einige Worte sprechen ?to say a few
words?), main-clause complements or subordinate
dass (?that?) sentences. Other uses of the verb are
301
np:von (2715), n (2696), na (1380), np:mit
(1247), np:in (1132), nS-2 (1064), np:?uber
(853), np:f?ur (695), nS-dass (491), np:zu
(307), nap:in (280), nap:von (275), ni (261),
np:bei (212), np:gegen (192), np:an (186),
naS-2 (172), np:aus (168), np:auf (112),
nap:?uber (112)
Figure 2: SCF lexicon for sprechen
figurative , e.g., sprechen gegen ?be a counterar-
gument to?. As the distinction between arguments
and adjuncts is gradual in our system, some adjunct
patterns appear in the lexicon, too, but only with
low frequency, e.g., np:auf, in which the auf -PP
expresses the setting of the conversation, as in auf
der Tagung sprechen ?speak at the convention?.
For reference, we also constructed an SCF lexi-
con from the NEGRA and TIGER corpora, which
together comprise about 1.2 million words. This
SCF lexicon contains statistics on 133,897 verb
instances (5,316 verb lemmas). While the manual
annotations in NEGRA and TIGER mean that this
SCF lexicon has virtually no noise, the small size
of the corpora results in problems with data spar-
sity and negatively impacts the utility of this re-
source (see discussion in Section 6.2).
5 Automatic verb classification
The remainder of the paper sets out to establish the
relevance of our SCF acquisition system by com-
parison to previous work. As stated in Sec. 2, the
only prior automatic German SCF acquisition sys-
tem is that of Schulte im Walde (2002a), which was
evaluated directly against an electronic version of
a large dictionary; as this is not an open access
resource, we cannot perform a similar evaluation.
We opt therefore to use a task-based evaluation
to compare our system directly with Schulte im
Walde?s, and leave manual evaluation for future
work. We refer back to the experiment set up by
Schulte im Walde (2006) to automatically induce
classifications of German verbs by clustering them
on the basis of their SCF preferences as listed in
her SCF lexicon. By casting this experiment as a
fixed task, we can compare our system directly to
hers. The link between subcategorisation and verb
semantics is linguistically sound, due to the inter-
dependence between verb meanings and the num-
ber and kinds of their syntactic arguments (Levin,
1993; Levin and Rappaport Hovav, 2005). E.g.,
only transitive verbs that denote a change of state
like cut and break enter in the middle construction
(The bread cuts easily.), with the patient or theme
argument appearing as the syntactic subject. Thus,
verbs whose SCF preferences show such an alter-
nation can be predicted to denote a change of state.
We adopt the automatic verb classification
paradigm to evaluate our system, replicating
Schulte im Walde?s (2006) experiment to the best
of our ability. We argue that by evaluating our
SdeWaC SCF lexicon described in the previous
section, we simultaneously evaluate our subcate-
gorisation acquisition system; this technique also
allows us to demonstrate the semantic relevance of
our SCF lexicon. Section 5.1 introduces the man-
ual verb classification we use as a gold standard
and Section 5.2 describes our unsupervised clus-
tering technique. Our evaluation of the clustering
against the gold standard then follows in Section 6.
5.1 Manual verb classifications
The semantic verb classification proposed by
Schulte im Walde (2006, page 162ff.), hereafter
SiW2006, comprises 168 high- and low-frequency
verbs grouped into 43 semantic classes, with be-
tween 2 and 7 verbs per class. Examples of these
classes are Aspect (e.g., anfangen ?begin?), Propo-
sitional Attitude (e.g., denken ?think?), Transfer of
Possession (Obtaining) (e.g., bekommen ?get?), and
Weather (e.g., regnen ?rain?). Some of the classes
are subclassified
3
, e.g., Manner of Motion, with
the subclasses Locomotion (klettern ?climb?), Ro-
tation (rotieren ?rotate?), Rush (eilen ?hurry?), Ve-
hicle (fliegen ?fly?), and Flotation (gleiten ?glide?).
These classes are related to Levin classes in that
some are roughly equivalent to a Levin class (e.g.,
Aspect and Levin?s Begin class), others are sub-
groups of Levin classes, e.g., Position is a sub-
group of Levin?s Dangle class; finally, some classes
lump together Levin classes, e.g., Transfer of Pos-
session (Obtaining) combines Levin?s Get and Ob-
tain classes. This shows that these classes could be
integrated into a large-scale classification of Ger-
man verbs in the style of Levin (1993).
5.2 Clustering
From the counts of ?lemma,SCF? in the SCF lexi-
con, we can estimate the conditional probability
that a particular verb v appears with an SCF f :
3
For the purpose of our evaluation, we disregard class-
subclass relations and consider subclasses as separate entities.
302
P (scf = f |lemma = v). We smooth these con-
ditional probability distributions by backing off to
the prior probability P (scf) (Katz, 1987).
With these smoothed conditional probabilities,
we cluster verbs with k-means clustering (Forgy,
1965), a hard clustering technique, which partitions
a set of objects into k clusters. The algorithm is ini-
tialised with a starting set of k cluster centroids; it
then proceeds iteratively, first assigning each ob-
ject to the cluster whose centroid is closest under
some distance measure, and then calculating new
centroids to represent the centres of the updated
clusters. The algorithm terminates when the assign-
ment of objects to clusters no longer changes.
D(p?q) =
?
i
p
i
log
p
i
q
i
(9)
irad(p, q) = D(p?
p+ q
2
) +D(q?
p+ q
2
) (10)
skew(p, q) = D(p??q + (1? ?)p) (11)
In our experiments, verbs are represented by
their conditional probability distributions over
SCFs. As distance measures, we use two variants
of the Kullback-Leibler divergence (9), a measure
of the dissimilarity of two probability distributions.
The KL divergence from p to q is undefined if at
some point q but not p is zero, so we use measures
based on KL without this problem, viz., the in-
formation radius (aka Jensen-Shannon divergence,
a symmetric metric, (10)), as well as skew diver-
gence (an asymmetric dissimilarity measure which
smoothes q by interpolating it to a small degree
with p, (11)), where we set the interpolation param-
eter to be ? = 0.9, to make our results comparable
to Schulte im Walde?s (2006)
4
.
As mentioned, the k-means algorithm is ini-
tialised with a set of cluster centroids; in this study,
we initialise the centroids by random partitions
(each of the n objects is randomly assigned to one
of k clusters, and the centroids are then computed
as the means of these random partitions). Because
the random initial centroids influence the final clus-
tering, we repeat the clustering a number of times.
We also initialise the k-means cluster centroids
using agglomerative hierarchical clustering, a de-
terministic iterative bottom-up process. Hierarchi-
cal clustering initially assigns verbs to singleton
clusters; the two clusters which are ?nearest? to
4
Schulte im Walde (2006) takes ? = 0.9 although Lee
(1999) recommends ? = 0.99 or higher values in her original
description of skew divergence.
each other are then joined together, and this pro-
cess is repeated until the desired number of clusters
is obtained. Hierarchical clustering is performed
to group the verbs into k clusters; the centroids
of these clusters are then used to initialise the k-
means algorithm. While there exist several variants
of hierarchical clustering, we use Ward?s method
(Ward, Jr, 1963) for merging clusters, which at-
tempts to minimise the variance inside clusters;
Ward?s criterion was previously found to be the
most effective hierarchical clustering technique for
verb classification (Schulte im Walde, 2006).
6 Evaluation
This section presents the results of evaluating the
unsupervised verb clustering based on our SCF lex-
ica against the gold standard described in Sec. 5.1.
6.1 Results
We use two cluster purity measures, defined in
Fig. 3; we intentionally target our numerical eval-
uations to be directly comparable with previous
results in the literature. As k-means is a hard clus-
tering algorithm, we consider a clustering C to be
an equivalence relation that partitions n verbs into
k disjoint subsets C = {C
1
, . . . , C
k
}.
The first of these purity measures, adjusted Rand
index (Rand
a
in Eq. (12)) judges clustering simi-
larity using the notion of the overlap between a
cluster C
i
in a given clustering C and a cluster G
j
in a gold standard clustering G, this value being
denoted by CG
ij
= |C
i
? G
j
|; values of Rand
a
range between 0 for chance and 1 for perfect cor-
relation. The other metric, the pairwise F -score
(PairF, Eq. (13)), operates by constructing a con-
tingency table on the
(
n
2
)
pairs of verbs, the idea
being that the gold standard provides binary judge-
ments about whether two verbs should be clustered
together or not. If a clustering agrees with the gold
standard in clustering a pair of verbs together or
separately, this is a ?correct? answer; by extension,
information retrieval measures such as precision
(P ) and recall (R) can be computed.
Table 1 shows the performance of our SCF lex-
ica, evaluated against the SiW2006 gold standard.
The random baseline is given by PairF = 2.08 and
Rand
a
= ?0.004 (calculated as the average of 50
random partitions). The optimal baseline is PairF
= 95.81 and Rand
a
= 0.909, calculated by evalu-
ating the gold standard against itself. As the gold
standard includes polysemous verbs, which belong
303
Rand
a
(C,G) =
?
i,j
(
CG
ij
2
)
?
[
?
i
(
|C
i
|
2
)
?
j
(
|G
j
|
2
)
]
/
(
n
2
)
1
2
[
?
i
(
|C
i
|
2
)
+
?
j
(
|G
j
|
2
)
]
?
[
?
i
(
|C
i
|
2
)
?
j
(
|G
j
|
2
)
]
/
(
n
2
)
(12)
PairF(C,G) =
2P (C,G)R(C,G)
P (C,G) +R(C,G)
(13)
Figure 3: Evaluation metrics used to compare clusterings to gold standards.
Data Set Eval Distance Manual Random Best Random Mean Ward
Schulte im Walde PairF IRad 40.23 1.34? 16.15 13.37 17.86? 17.49
Skew 47.28 2.41? 18.01 14.07 15.86? 15.23
Rand
a
IRad 0.358 0.001? 0.118 0.093 0.145? 0.142
Skew 0.429 ?0.002? 0.142 0.102 0.158? 0.158
NEGRA/TIGER PairF IRad 30.77 2.06? 14.67 12.39 16.13? 15.52
Skew 40.19 3.47? 12.95 11.48 14.05? 14.31
Rand
a
IRad 0.281 0.000? 0.122 0.094 0.134? 0.129
Skew 0.382 ?0.015? 0.102 0.089 0.112? 0.114
SdeWaC PairF IRad 42.66 1.62? 20.36 18.26 26.94? 27.50
Skew 50.38 2.99? 20.75 17.80 24.60? 24.94
Rand
a
IRad 0.387 ?0.006? 0.167 0.146 0.232? 0.238
Skew 0.465 0.008? 0.170 0.143 0.208? 0.211
Table 1: Evaluation of the NEGRA/TIGER and SdeWaC SCF lexica using the SiW2006 gold standard.
to more than one cluster, the optimal baseline is
calculated by randomly picking one of their senses;
the average is then taken over 50 such trials.
We cluster using k = 43, matching the number
of clusters in the gold standard. Of the 168 verbs in
SiW2006, 159 are attested in NEGRA and TIGER
(17,285 instances), and 167 are found in SdeWaC
(1,047,042 instances)
5
.
We report the results using k-means clustering
initialised under a variety of conditions. ?Manual?
shows the quality of the clustering achieved when
initialising k-means with the gold standard classes.
We also initialise clustering 10 times using ran-
dom partitions. For the best clustering
6
in these
10, ?Random Best? shows the evaluation of both
the starting random partition and the final cluster-
ing found by k-means; ?Random Mean? shows the
average cluster purity of the 10 final clusterings.
?Ward? shows the evaluation of the clustering ini-
tialised with centroids found by hierarchical clus-
5
Verbs missing from the clustering reduce the maximum
achievable cluster purity score.
6
Specifically, we take the clustering result with the mini-
mum intra-cluster distance (not the clustering result with the
best performance on the gold standard).
tering of the verbs using Ward?s method. Again,
both the initial partition found by Ward?s method
and the k-means solution based on it are shown.
For comparison, we list the results of Schulte
im Walde (2006, p. 174, Table 7) for the second
level of SCF granularity, with PP head and case
information (see Sec. 2 for Schulte im Walde?s
analysis). While this seems the most appropriate
comparison to draw, since we also collect statis-
tics about PPs, it is ambitious because, as noted
in Section 3, our SCF lexica lack case informa-
tion about PPs.
7
Compared to Schulte im Walde?s
numbers, the NEGRA/TIGER SCF lexicon scores
significantly worse on the PairF evaluation metric
under all conditions, and also on the Rand
a
metric
using the skew divergence measure (Rand
a
/IRad
is not significantly different). The SdeWaC SCF
lexicon scores better on all metrics and conditions;
these results are significant at the p < 0.001 level
8
.
7
PP case information is relevant for prepositions that can
take both locative and directional readings, as in in der Stadt
(dative) ?in town? und in die Stadt (accusative) ?to town?.
8
Statistical significance is calculated by running repeated
k-means clusterings with random partition initialisation and
evaluating the results using the relevant purity metrics. These
repeated clustering scores represent a random variable (a func-
304
6.2 Discussion
Sec. 6.1 compared the SCF lexicon created us-
ing SdeWaC with the lexicon built by Schulte im
Walde (2002a), showing that our lexicon achieves
significantly better results on the verb clustering
task. We interpret this to be indicative of a more
accurate subcategorisation lexicon, and, by exten-
sion, of a more accurate SCF acquisition system.
We attribute this superior performance primar-
ily to our use of a statistical parser as opposed to
a hand-written grammar. This design choice has
several advantages. First, the parser delivers robust
syntactic analyses, which we can expect to be rel-
atively domain-independent. Second, we make no
prior assumptions about the variety of subcategori-
sation phenomena that might appear in text, decou-
pling the identification of SCFs from the ability
to parse natural language. Third, the fact that our
parser and edge labeller are trained on the 800,000
word NEGRA/TIGER corpus means that we bene-
fit from the linguistic expertise that went into build-
ing that treebank. Our use of off-the-shelf tools
(the parser and our simple yet effective machine
learning model describing edge label information)
makes our system considerably simpler and easier
to implement than Schulte im Walde?s. We see our
system as more easily extensible to other languages
for which there is a parser and an initial syntacti-
cally annotated corpus to train the edge labeller on.
The NEGRA/TIGER SCF lexicon performs not
as well on the verb clustering evaluations, as fewer
verbs are attested in NEGRA/TIGER compared to
the SdeWaC SCF lexicon and gold standard clus-
terings. Data sparsity can be a problem in SCF ac-
quisition; all other factors being equal, using more
data to construct an SCF lexicon should make pat-
terns in the language more readily visible and re-
duce the chance of missing a particular lemma-
SCF combination accidentally. A secondary ef-
fect is that models of verb subcategorisation prefer-
ences like the ones used here can be more precisely
estimated as the counts of observed verb instances
increase, particularly for low-frequency verbs.
Error analysis of our SCF lexicon reveals low
counts of expletive subjects. The edge labeller is
supposed to annotate semantically empty subjects
(es, ?it?) as expletive; for clusterings examined in
Sec. 5.1, this would affect weather verbs (e.g., es
tion of the random cluster centroids used to initialise the k-
means clustering). These samples are normally distributed, so
we determine statistical significance using a t-test against the
?Random Mean? results reported by Schulte im Walde (2006).
regnet, ?it?s raining?). However, in our SdeWaC
SCF lexicon, expletive subjects are clearly under-
represented. Our SCF lexicon built on TIGER,
where expletive subjects are systematically la-
belled, has the SCF xa as the most common SCF
for the verb geben (in es gibt ?there is?). In con-
trast, in our SdeWaC SCF lexicon, the most com-
mon SCF is the transitive na, with xa in seventh
place. I.e., the edge labeller does not identify all
expletive subjects, which is due to the fact that ex-
pletive subjects are syntactically indistinguishable
from neuter pronominal subjects, so the edge la-
beller does not have a rich feature set to inform it
about this category. But since, statistically, exple-
tive pronouns make up less than 1% of subjects
in TIGER, the prior probability of labelling a con-
stituent as expletive is very low. Due to these fig-
ures, we do not expect this issue to seriously impact
the quality of our verb classification evaluations.
7 Future work
In this paper we have presented a state-of-
the-art subcategorisation acquisition system for
free-word order languages, and used it to cre-
ate a large subcategorisation frame lexicon for
German verbs. Our SCF lexicon resource is
available at http://amor.cms.hu-berlin.
de/?robertsw/scflex.html. We are per-
forming a manual evaluation of the output of our
system, which we will report soon.
We plan to continue this work first by expanding
our SCF lexicon with case information and selec-
tional preferences, second by using our SCF clas-
sifier and lexicon for verbal Multiword Expression
identification in German, and last by comparing
it to existing verb classifications, either by using
available resources for German like the SALSA
corpus (Burchardt et al., 2006), or by translating
parts of VerbNet into German to create a more
extensive gold standard for verb clustering in the
spirit of Sun et al. (2010) who found that Levin?s
verb classification can be translated to French and
still usefully allow generalisation over verb classes.
Finally, we plan to perform in vivo evaluation
of our SCF lexicon, to determine what benefit
it can deliver for NLP applications such as Se-
mantic Role Labelling and Word Sense Disam-
biguation. Recent research has found that even
automatically-acquired verb classifications can be
useful for NLP applications (Shutova et al., 2010;
Guo et al., 2011).
305
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In TLT, pages 24?41.
Michael R. Brent and Robert C. Berwick. 1991. Auto-
matic acquisition of subcategorization frames from
tagged text. In HLT, pages 342?345. Morgan Kauf-
mann.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. CoRR,
cmp-lg/9702002.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
2006. The SALSA corpus: A German corpus re-
source for lexical semantics. In LREC.
Judith Eckle-Kohler. 1999. Linguistic knowledge for
automatic lexicon acquisition from German text cor-
pora. Ph.D. thesis, Universit?at Stuttgart.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC - A
corpus of parsable sentences from the Web. In Lan-
guage processing and knowledge in the Web, pages
61?68. Springer, Berlin, Heidelberg.
Edward W. Forgy. 1965. Cluster analysis of multivari-
ate data: Efficiency versus interpretability of classifi-
cations. Biometrics, 21:768?769.
Raza Ghulam. 2011. Subcategorization acquisition
and classes of predication in Urdu. Ph.D. thesis,
Universit?at Konstanz.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In EMNLP, pages 1?8.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In EMNLP,
pages 273?283.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing, 35(3):400?401.
Anna Korhonen. 2002. Subcategorization acquisi-
tion. Technical report, University of Cambridge,
Computer Laboratory.
Joel Lang and Mirella Lapata. 2010. Unsupervised
induction of semantic roles. In HLT, pages 939?947.
Lillian Lee. 1999. Measures of distributional similar-
ity. In ACL, pages 25?32.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment realization. Cambridge University Press, Cam-
bridge.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press, Chicago.
Christopher D. Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In ACL, pages 235?242.
Manolis Maragoudakis, Katia Lida Kermanidis, and
George Kokkinakis. 2000. Learning subcategoriza-
tion frames from corpora: A case study for modern
Greek. In Proceedings of COMLEX 2000, Work-
shop on Computational Lexicography and Multime-
dia Dictionaries, pages 19?22.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL, pages 433?440.
Anoop Sarkar and Daniel Zeman. 2000. Automatic
extraction of subcategorization frames for Czech. In
COLING, pages 691?697.
Silke Scheible, Sabine Schulte im Walde, Marion
Weller, and Max Kisselew. 2013. A compact but lin-
guistically detailed database for German verb subcat-
egorisation relying on dependency parses from Web
corpora: Tool, guidelines and resource. In Web as
Corpus Workshop.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In NeMLaP, vol-
ume 12, pages 44?49.
Sabine Schulte im Walde and Chris Brew. 2002. Induc-
ing German semantic verb classes from purely syn-
tactic subcategorisation information. In ACL, pages
223?230.
Sabine Schulte im Walde. 2002a. A subcategorisation
lexicon for German verbs induced from a lexicalised
PCFG. In LREC, pages 1351?1357.
Sabine Schulte im Walde. 2002b. Evaluating verb sub-
categorisation frames learned by a German statisti-
cal grammar against manual definitions in the Duden
Dictionary. In EURALEX, pages 187?197.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?194.
Sabine Schulte im Walde. 2009. The induction of
verb frames and verb classes from corpora. In Anke
L?udeling and Merja Kyt?o, editors, Corpus linguis-
tics: An international handbook, volume 2, chap-
ter 44, pages 952?971. Mouton de Gruyter, Berlin.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In COLING, pages 1002?1010.
306
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In ANLP, pages 88?95.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In CICLing, pages 16?27, Haifa, Israel.
Lin Sun, Anna Korhonen, Thierry Poibeau, and C?edric
Messiant. 2010. Investigating the cross-linguistic
potential of VerbNet-style classification. In COL-
ING, pages 1056?1064, Beijing, China.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In EACL, pages 12?22.
Joe H. Ward, Jr. 1963. Hierarchical grouping to opti-
mize an objective function. Journal of the American
Statistical Association, 58(301):236?244.
Oliver Wauschkuhn. 1999. Automatische Extrak-
tion von Verbvalenzen aus deutschen Textkorpora.
Shaker Verlag.
307
Tutorial Abstracts of ACL 2010, page 3,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Discourse Structure: Theory, Practice and Use
Bonnie Webber,? Markus Egg,? Valia Kordoni?
? University of Edinburgh ? Humboldt University ? Saarland University
bonnie@inf.ed.ac.uk markus.egg@anglistik.hu-berlin.de kordoni@dfki.de
1 Introduction
This tutorial aims to provide attendees with a clear
notion of how discourse structure is relevant for
language technology (LT), what is needed for ex-
ploiting discourse structure, what methods and re-
sources are available to support its use, and what
more could be done in the future.
2 Content Overview
This tutorial consists of four parts. Part I starts
with a brief introduction to different bases for dis-
course structuring, properties of discourse struc-
ture that are relevant to LT, and accessible evi-
dence for discourse structure.
For discourse structure to be useful for lan-
guage technologies, one must be able to automati-
cally recognize or generate with it. Hence, Part II
surveys computational approaches to recognizing
and generating discourse structure, both manually-
authored approaches and ones developed through
Machine Learning.
Part III of the tutorial describes applications
of discourse structure recognition and generation
in LT, as well as discourse-related resources be-
ing made available in English, German, Turkish,
Hindi, Czech, Arabic and Chinese. Part IV con-
cludes with a list of future possibilities.
3 Tutorial Outline
1. PART I ? General Overview
(a) Bases for structure in monologic, dia-
logic and multiparty discourse
(b) Aspects of discourse structure relevant
to Language Technology
(c) Evidence for discource structure
2. PART II ? Computational Recognition and
Generation of discourse structure
(a) Discourse chunking and parsing
(b) Recognizing arguments and sense of
discourse connectives
(c) Recognizing and generating entity-
based discourse structure
(d) Dialogue parsing
3. PART III ? Applications and Resources
(a) Applications to Language Technology
(b) Discourse structure resources (mono-
lingual and multilingual)
4. PART IV ? Future Developments
4 References
? Regina Barzilay and Lillian Lee (2004). Catching the Drift:
Probabilistic Content Models, with Applications to Genera-
tion and Summarization. Proc. 2nd Human Language Tech-
nology Conference and Annual Meeting of the North Ameri-
can Chapter, Association for Computational Linguistics, pp.
113-120.
? Regina Barzilay and Mirella Lapata (2008). Modeling Lo-
cal Coherence: An Entity-based Approach. Computational
Linguistics 34(1), pp. 1-34.
? Daniel Marcu (2000). The theory and practice of discourse
parsing and summarization. Cambridge: MIT Press.
? Umangi Oza, Rashmi Prasad, Sudheer Kolachina, Dipti
Misra Sharma and Aravind Joshi (2009). The Hindi Dis-
course Relation Bank. Proc. Third Linguistic Annotation
Workshop (LAW III). Singapore.
? Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki
et al (2008). The Penn Discourse TreeBank 2.0. Proc. 6th
Int?l Conference on Language Resources and Evaluation.
? Manfred Stede (2008). RST revisited: Disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke Ramm
(eds.), Subordination versus Coordination in Sentence and
Text. Amsterdam: John Benjamins.
? Ben Wellner (2008). Sequence Models and Ranking Meth-
ods for Discourse Parsing. Brandeis University.
? Deniz Zeyrek, ?Umit Deniz Turan, Cem Bozsahin, Ruket
C?akici et al (2009). Annotating Subordinators in the Turkish
Discourse Bank. Proc. Third Linguistic Annotation Work-
shop (LAW III). Singapore.
3
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7?8,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Robust Automated Natural Language Processing
with Multiword Expressions and Collocations
Valia Kordoni and Markus Egg
Humboldt-Universita?t zu Berlin (Germany)
kordonie@anglistik.hu-berlin.de,
markus.egg@anglistik.hu-berlin.de
1 Introduction
This tutorial aims to provide attendees with a clear
notion of the linguistic and distributional charac-
teristics of multiword expressions (MWEs), their
relevance for robust automated natural language
processing and language technology, what meth-
ods and resources are available to support their
use, and what more could be done in the future.
Our target audience are researchers and practition-
ers in language technology, not necessarily experts
in MWEs, who are interested in tasks that involve
or could benefit from considering MWEs as a per-
vasive phenomenon in human language and com-
munication.
2 Topic Overview
Multiword expressions (MWEs) like break down,
bus stop and make ends meet, are expressions con-
sisting of two or more lexical units that correspond
to some conventional way of saying things (Sag et
al., 2001). They range over linguistic construc-
tions such as fixed phrases (per se, by and large),
noun compounds (telephone booth, cable car),
compound verbs (give a presentation), idioms (a
frog in the throat, kill some time), etc. They are
also widely known as collocations, for the frequent
co-occurrence of their components (Manning and
Schu?tze, 2001).
From a natural language processing perspective,
the interest in MWEs comes from the very im-
portant role they play forming a large part of hu-
man language, which involves the use of linguistic
routines or prefabricated sequences in any kind of
text or speech, from the terminology of a specific
domain (parietal cortex, substantia nigra, splice
up) to the more colloquial vocabulary (freak out,
make out, mess up) and the language of the social
media (hash tag, fail whale, blackbird pie). New
MWEs are constantly being introduced in the lan-
guage (cloud services, social networking site, se-
curity apps), and knowing how they are used re-
flects the ability to successfully understand and
generate language.
While easily mastered by native speakers, their
treatment and interpretation involves consider-
able effort for computational systems (and non-
native speakers), due to their idiosyncratic, flexi-
ble and heterogeneous nature (Rayson et al, 2010;
Ramisch et al, to appear). First of all, there is
the task of identifying whether a given sequence
of words is an MWE or not (e.g. give a gift vs.
a presentation) (Pecina, 2008; Green et al, 2013;
Seretan, 2012). For a given MWE, there is also the
problem of determining whether it forms a com-
positional (take away the dishes), semi-idiomatic
(boil up the beans) or idiomatic combination (roll
up your sleeves) (Kim and Nakov, 2011; Shutova
et al, 2013). Furthermore, MWEs may also be
polysemous: bring up as carrying (bring up the
bags), raising (bring up the children) and men-
tioning (bring up the subject). Unfortunately, so-
lutions that are successfully employed for treating
similar problems in the context of simplex works
may not be adequate for MWEs, given the com-
plex interactions between their component words
(e.g. the idiomatic use of spill in spilling beans
as revealing secrets vs. its literal usage in spilling
lentils).
3 Content Overview
This tutorial consists of four parts. Part I starts
with a thorough introduction to different types of
MWEs and collocations, their linguistic dimen-
sions (idiomaticity, syntactic and semantic fixed-
ness, specificity, etc.), as well as their statisti-
cal characteristics (variability, recurrence, associa-
tion, etc.). This part concludes with an overview of
linguistic and psycholinguistic theories of MWEs
to date.
For MWEs to be useful for language tech-
nology, they must be recognisable automatically.
7
Hence, Part II surveys computational approaches
for MWEs recognition, both manually-authored
approaches and using machine learning tech-
niques, and for modeling syntactic and semantic
variability. We will also review token identifica-
tion and disambiguation of MWEs in context (e.g.
bus stop in Does the bus stop here? vs. The bus
stop is here) and methods for the automatic detec-
tion of the degree of compositionality of MWEs
and their interpretation. Part II finishes with a dis-
cussion of evaluation for MWE tasks.
Part III of the tutorial describes resources made
available for a wide range of languages as well
as MWE-related multi-level annotation platforms
and examples of where MWEs treatment can con-
tribute to language technology tasks and appli-
cations such as parsing, word sense disambigua-
tion, machine translation, information extraction
and information retrieval. Part IV concludes with
a list of future possibilities and open challenges in
the computational treatment of MWEs in current
NLP models and techniques.
4 Tutorial Outline
1. PART I ? General overview:
(a) Introduction
(b) Types and examples of MWEs and collocations
(c) Linguistic dimensions of MWEs: idiomaticity,
syntactic and semantic fixedness, specificity, etc.
(d) Statistical dimensions of MWEs: variability, re-
currence, association, etc.
(e) Linguistic and psycholinguistic theories of
MWEs
2. PART II ? Computational methods
(a) Recognising the elements of MWEs: type iden-
tification
(b) Recognising how elements of MWEs are com-
bined: syntactic and semantic variability
(c) Token identification and disambiguation of
MWEs
(d) Compositionality and Interpretation of MWEs
(e) Evaluation of MWE tasks
3. PART III ? Resources, tasks and applications:
(a) MWEs in resources: corpora, lexica and ontolo-
gies (e.g. Wordnet and Genia)
(b) Tools for MWE identification and annotation
(e.g. NSP, mwetoolkit, UCS and jMWE)
(c) MWEs and Collocations in NLP tasks: Pars-
ing, POS-tagging, Word Sense Disambiguation
(WSD)
(d) MWes and Collocations in Language Technol-
ogy applications: Information Retrieval (IR), In-
formation Extraction (IE), Machine Translation
(MT)
4. PART IV ? Future challenges and open prob-
lems
References
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Su Nam Kim and Preslav Nakov. 2011. Large-scale noun
compound interpretation using bootstrapping and the web
as a corpus. In EMNLP, pages 648?658.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
recognising multiword expressions improve shallow pars-
ing? In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 636?644, Los
Angeles, California, June. Association for Computational
Linguistics.
Christopher D. Manning and Hinrich Schu?tze. 2001. Foun-
dations of statistical natural language processing. MIT
Press.
Pavel Pecina. 2008. A machine learning approach to mul-
tiword expression extraction. In Nicole Gre?goire, Ste-
fan Evert, and Brigitte Krenn, editors, Proceedings of the
LREC Workshop Towards a Shared Task for Multiword
Expressions (MWE 2008), pages 54?57.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the
extraction of multiword expressions. In Nicole Gre?goire,
Stefan Evert, and Brigitte Krenn, editors, Proceedings of
the LREC Workshop Towards a Shared Task for Multiword
Expressions (MWE 2008), pages 50?53.
Carlos Ramisch, Aline Villavicencio, and Valia Kordoni. to
appear. Special Issue on Multiword Expressions. ACM
TSLP.
Paul Rayson, Scott Songlin Piao, Serge Sharoff, Stefan Evert,
and Begon?a Villada Moiro?n. 2010. Multiword expres-
sions: hard going or plain sailing? Language Resources
and Evaluation, 44(1-2):1?5.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2001. Multiword expressions:
A pain in the neck for NLP. In Proc. of the 3rd Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002, pages 1?15.
Violeta Seretan. 2012. Syntax-Based Collocation Extrac-
tion, volume 44, Text, Speech and Language Technology.
Springer.
Ekaterina Shutova, Simone Teufel, and Anna Korhonen.
2013. Statistical metaphor processing. Comput. Linguist.,
39(2):301?353, June.
Aline Villavicencio, Francis Bond, Anna Korhonen, and Di-
ana McCarthy. 2005. Introduction to the special issue
on multiword expressions: Having a crack at a hard nut.
Computer Speech & Language, 19(4):365?377.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco Idiart,
and Carlos Ramisch. 2007. Validation and evaluation of
automatically acquired multiword expressions for gram-
mar engineering. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1034?1043, Prague, Czech Re-
public, June. Association for Computational Linguistics.
8
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 132?138,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
A unified account of the semantics of discourse particles
Markus Egg
Humboldt-Universita?t Berlin
markus.egg@anglistik.hu-berlin.de
Abstract
The paper investigates discourse particles
on the example of German doch, assigning
to them very specific semantic interpreta-
tions that still cover a wide range of their
uses.
The analysis highlights the role of dis-
course particles in managing the common
ground and crucially takes into account
that discourse particles can refer not only
to utterances they are a part of and to previ-
ously uttered utterances, but also to felicity
conditions of these utterances.
1 Introduction
This paper advocates very specific semantic in-
terpretations for discourse particles, concentrating
on German doch. There is a very wide range of
concrete usages of discourse particles in context
(which has motivated analysing them as polyse-
mous, e.g., in Helbig (1988)).
Assigning them a uniform semantic interpreta-
tion seems to be subject to two conflicting require-
ments:
? the interpretation must be sufficiently specific
to allow deriving the interpretation of con-
crete uses
? it must be sufficiently general to cover a wide
range of concrete uses
So far, research on the interpretation of doch
focusses on the second requirement (e.g., Thur-
mair (1989), Ko?nig (1997), Karagjosova (2004),
or Ko?nig and Requardt (1997)).
The meaning of doch emerges as a two-place
relation between the utterance doch is a part of and
a previous utterance to which the doch-utterance is
a reaction.
This relation is described by the features
KNOWN and CORRECTION in Thurmair (1989),
i.e., doch-utterances correct a previous utterance
by introducing old information. Karagjosova
(2004) regards doch-utterances act as reminders,
which present old information to hearers. Ac-
cording to Ko?nig and Requardt (1997), doch-
utterances point out inconsistencies between old
information and a new piece of information or ac-
tion.
Such general descriptions of doch apply to cases
like Karagjosova?s (1): B reminds A of Peter?s
illness, which seems inconsistent with A?s an-
nouncement and therefore can act as a correction
of A:
(1) A: Peter wird auch mitkommen. B: Er ist
doch krank.
?A: Peter will come along, too. B: But he is
ill.?
While these general descriptions (excepting
Karagjosova (2004)) do not spell out in detail the
way in which doch contributes to the meaning of
larger discourses, they can capture a wide range of
uses of these particles.
There remain a number of problematic cases,
including discourse-initial uses of doch-utterances
like Ko?nig and Requardt?s (2), which functions as
an opening line in a conversation, it neither cor-
rects nor reminds the hearer, nor is there an incon-
sistency between the utterance and the context:
(2) Sie sind doch Paul Meier.
?You must be Paul Meier.?
The proposed analysis of the particle doch is
sufficiently general to account for a wide range of
uses yet being specific enough to specify the se-
mantic construction for discourses that comprise
doch.
I follow much previous work in developing my
analysis on the basis of simple examples like (1),
132
and then extending it to cases like (2). Most ex-
amples consist of two utterances, the second utter-
ance comprises a discourse particle and is a reac-
tion to the first one. In the remainder of this pa-
per, these two utterances are called ?involved ut-
terances?.
In (1), the (propositional) semantic arguments
of the particle are the meanings of these two utter-
ances. But the semantic arguments of a discourse
particle may differ from the meanings of the in-
volved utterances, as illustrated by (3) (from Thur-
mair 1989):
(3) A: Seit wann hast du denn den
,,Zauberberg?? B: Den hast du mir doch vor
zwei Jahren geschenkt.
A: ?Since when have you owned the
?Zauberberg?? B: But you gave it to me two
years ago.?
B reacts to the implicit statement that A does not
know the answer to his question. This statement is
an argument of doch in (3), even though it is not
the meaning of A?s utterance. This shows that the
semantic arguments of discourse particles must be
distinguished from the meanings of their involved
utterances.
Utterances with a discourse particle and pre-
ceding utterances to which they react are called
?p(article)-utterances? (or ?doch-utterances?) and
?a(ntecedent)-utterances?. They are distinguished
from the semantic arguments of the particle,
which are referred to as ?p-proposition? and ?a-
proposition?, respectively.
This is not just a question of nomenclature but
reflects a fundamentally different view on the role
of discourse particles. Instead of indicating the re-
lation between two already identified propositions,
the meaning of the particle applied to its first ar-
gument (very often but not always the interpreta-
tion of the p-utterance) determines the range of po-
tential a-propositions in the context of utterance.
From this range, the hearer selects the appropriate
proposition.
This resembles the intuition of Ko?nig and
Requardt (1997) that discourse particles are
?metapragmatic instructions? which tell hearers
how to deal with the p-utterance in a communica-
tive situation.
Consequently, a- and p-utterances do not deter-
mine the semantic arguments for all uses of dis-
course particles, which might account for some
problems of defining the semantics of the particles
in the literature, which is characteristically based
on the meanings of a- and p-utterance.
My claim is that there is a link between a- and p-
proposition and a- and p-utterance, respectively, in
that the propositions can either be the meanings of
the utterances or emerge through the felicity con-
ditions of the utterances.
E.g., in (3) the doch-proposition reminds A of
the fact that the first preparatory condition for a
question (that the speaker does not know the an-
swer) does not hold, since A (as the one who gave
the book to B) should know since when the book
has been in B?s possession.
The plan of the paper is to introduce back-
ground assumptions on discourse particles in sec-
tion 2, to apply the proposed approach to the (un-
stressed) particle doch in section 3, and to con-
clude with an outlook on further research.
2 Formal background
This paper follows much previous work in assum-
ing that discourse particles refer to the common
ground (CG), e.g., Ko?nig (1997), Karagjosova
(2004), or Zimmermann (2009).
Common ground and the interlocutors? individ-
ual backgrounds are modelled as common or in-
dividual belief (Stalnaker, 2002). Individual be-
lief is equated with the set of propositions that are
true in all possible worlds compatible with the in-
dividual?s beliefs; common belief, with the set of
propositions believed by all members of the re-
spective group of believers.
Stalnaker notes that this is an idealisation in that
the CG might comprise propositions not shared by
the background of every member of the group. But
this idealisation is not a problem for the analysis
presented in this paper.
Reasoning on CG and individual backgrounds
often uses defeasible deduction (Asher and Las-
carides, 2003). I.e., from statements of the form
?p defeasibly entails q? (p > q) together with p
one can defeasibly deduce q.
This defeasible Modus Ponens applies if ?q
does not hold and ?q cannot be deduced simulta-
neously (Asher and Lascarides, 2003). Defeasible
deducability of p from a set of propositions C is
written as ?C |? p?.
Reference to the common ground makes the se-
mantics of discourse particles context-dependent,
because the CG is relative to the interlocutor(s) of
133
a- and p-utterances. This shows up in the shifting
effects observed in Do?ring (2010). Consider e.g.
what happens if one embeds (1) in a quotation like
in (4):
(4) A sagte, Peter komme auch mit. B
entgegnete, er sei doch krank.
?A said Peter would come along, too. B
retorted that he was ill.?
The shift in (4) arises because doch presents a
proposition (here, that Peter is ill) as part of the
common ground, and the relevant CG is calcu-
lated with respect to A and B, not with respect to
the interlocutors of (4). I.e., (4) does not express
that Peter?s illness is in the common ground of the
speaker and hearer of (4).
3 The analysis
The proposed approach to discourse particle is
now applied to doch, which introduces a notion
of tension between the a- and the p-proposition.
3.1 Declarative a- and p-utterances
I will first illustrate this notion with simple ex-
amples in which the a-utterance expresses the a-
proposition, and the meaning of the declarative p-
utterance provides the p-proposition.
In (5) [= (1)] and (6), adapted from Karagjosova
(2004), there is tension between being ill on the
one hand and going out and living healthily on the
other hand, respectively:
(5) A: Peter wird auch mitkommen. B: Er ist
doch krank.
?A: Peter will come along, too. B: But he is
ill.?
(6) Ich bin oft krank. Dabei lebe ich doch
gesund.
?I am often ill. But I have a healthy lifestyle.?
The intuitive notion of tension between two
propositions p and q is formalised as defeasible
entailment q > ?p. I.e., given q, one would ex-
pect p, but the propositions are not incompatible,
even though q is a potential impediment for p.
The effect of doch q as a reaction to an a-
proposition p against the common ground C is to
remind the hearer that C comprises a potential im-
pediment q for p, which expresses either surprise
at the fact that p nevertheless holds or puts doubt
on p. Still, p is not explicitly denied.
Formally, doch states that the common ground
C defeasibly entails q and the fact that q defeasi-
bly entails ?p (which by defeasible Modus Ponens
would allow one to infer ?p, if the conditions for
defeasible Modus Ponens are met):
(7) [[doch]](q)(p) iff C |? q ? C |? q > ?p
This analysis differs from the one of Ko?nig
(1997), who assumes that doch q points out a con-
tradiction in the CG, in that p is incompatible with
a consequence of q. In contrast, I regard this
incompatibility as a default only. The status of
q as derivable from the CG is also expressed in
Karagjosova (2004) claim that doch introduces q
as a reminder and in Thurmair?s (1989) feature
KNOWN.
In (5) and (6), p and q are the semantics of the a-
and the doch-utterance. Being ill is a potential im-
pediment for going out, so, by pointing out Peter?s
illness in (5), B expresses surprise or disbelief at
A?s announcement but does not necessarily correct
it or refute it, because even ill people can go out in
principle.
Similarly, the speaker of (6) is surprised at his
frequent illness, in spite of his healthy lifestyle.
(6) shows that q is only a default impediment: If q
and p were contradictory, (6) would be nonsensi-
cal, but, intuitively, it is not.
The use of defeasible implications to model the
tension between two propositions as indicated by
doch is closely related to accounts of the discourse
relation of CONCESSION in Grote et al (1997),
Oversteegen (1997), Lagerwerf (1998), and Knott
(1996).
They assume the same kind of defeasible impli-
cation for this discourse relation and model it as a
presupposition, which is compatible with giving it
common ground status.
3.2 Non-declarative a-utterances
In (5) and (6), the a-proposition enters the CG
as the meaning of an a-utterance. But the a-
proposition can also emerge as a felicity condition
of the a-utterance. Consider e.g. doch-utterances
as reactions to questions, as in (8) [= (3)]:
(8) A: Seit wann hast du denn den
,,Zauberberg?? B: Den hast du mir doch vor
zwei Jahren geschenkt.
A: ?Since when have you owned the
?Zauberberg?? B: But you gave it to me two
years ago.?
134
Doch in (8) expresses surprise at the question
being asked, since A himself gave the book to B
and hence should know that B owns it.
The proposed analysis reconstructs this intu-
ition: B?s utterance expresses a proposition q (that
A gave the book to B) and points out that q is part
of the CG. It is also part of the CG that q is a po-
tential obstacle for a specific a-proposition p (for-
mally, the CG entails q > ?p).
Such p-utterances restrict the range of poten-
tial a-propositions p, and their hearers try to iden-
tify the a-propositions in the given context. The
a-utterance in (8), however, cannot directly con-
tribute p in any context, since its meaning is not a
proposition.
But due to the assumption that A is coopera-
tive, the question introduces into the CG the felic-
ity conditions for questions, among them the first
preparatory condition, viz., that A does not know
the answer to his question. This is a suitable p, be-
cause it is reasonable to assume that if A gave the
book to B (= q), he should know the answer to the
question (= ?p).
The intuition that the semantic arguments of
discourse particles need not be identical to the
meanings of the involved utterances is related to
suggestions to let discourse relations relate ei-
ther to the content of the discourse segments that
they link or to the corresponding intensions of
the speaker or the intended effencts on the hearer,
which is suggested by Sweetser (1990) and Knott
(2001).
Doch-utterances in reaction to imperatives work
analogously, e.g., (9):
(9) A: U?bersetze mir bitte diesen Brief. B: Ich
kann doch kein Baskisch.
A: ?Please translate this letter for me.? B:
?But I do not know Basque?.
Here B?s lack of proficiency in Basque (= q)
and A?s belief that B can translate a Basque letter
(i.e., the first preparatory condition of the request,
our p) are in tension.
Now q can be deduced from the common
ground either because it has been explicitly in-
troduced before or because it makes sense to as-
sume defeasibly that someone does not speak a
less known language like Basque. In either case, A
should not take for granted that B speaks Basque,
i.e., has a reason not to require B to translate letters
written in Basque.
This use of doch also shows up in reactions
to declarative statements: The p-utterance of (10)
states no potential impediment for the proposition
expressed by A.
Rather, B?s use of doch refers to A?s surprise,
suggesting that A should not be surprised at all.
The felicity condition of expressing surprise that
is cast into doubt by B is considering the fact
about which one is surprised as something extraor-
dinary, which would not have happened in a nor-
mal course of events.
(10) A: Peter sieht schlecht aus. B: Er war doch
lange im Krankenhaus.
?A: Peter does not look healthy. B: But he
has been in hospital for a long time.?
Peter?s long stay in the hospital (= q) is no
potential obstacle to looking unhealthy, on the
contrary, it entails defeasibly that his looking un-
healthy is quite normal (= ?p). This would negate
the abovementioned felicity condition for A?s sur-
prise (= p), hence suggests that A should not be
surprised.
3.3 Non-declarative p-utterances
Another group of doch-utterances are imperative
or interrogative (the latter adapted from Thurmair
(1989)):
(11) Verklag mich doch!
?Go ahead and sue me.?
(12) Komm doch nach Hause!
?Do come home.?
(13) Wie hei?t doch diese Kneipe in der
Sredzkistra?e?
?What is the name of this pub in the
Sredzkistra?e??
(14) Wie sagt Goethe doch so treffend?
?What was this piercing remark of Goethe
again??
(15) Du kommst doch?
?I presume that you will be there.?
Doch is used provocatively in imperatives like
(11); it suggests that the hearer cannot fulfil the
request. In cases like (12), doch signals that the re-
quested or suggested action is a very natural thing
to do. Doch-questions refer to a piece of knowl-
edge that the speaker knows or is supposed to
135
know (Thurmair, 1989), e.g., (13) indicates that
the speaker knows the answer at least in principle,
(14) announces a quotation, and (15) suggests that
the answer can only be affirmative.
There are two issues in interpreting these sen-
tences; the p-utterance does not denote a propo-
sition (which could be the semantic argument of
doch), and there need not be an a-utterance at all
from which to derive the a-proposition.
But in all these utterances, speakers use doch to
point out that they are aware of evidence from the
CG which suggests that a felicity condition of the
utterance itself does not hold. This can be mod-
elled by identifying the p-proposition q (the argu-
ment of doch) with the fact that the sentences were
uttered, which can be (trivially) deduced from the
common ground C (the condition C |? q in (7)).
Then the felicity conditions associated with dif-
ferent kinds of illocutionary acts emerge from the
common ground C as default entailments from the
utterance of the respective illocutionary type (the
condition C |? q > ?p in (7); here ?p refers to
one of the felicity conditions).
I.e., using doch in these cases triggers a search
for a suitable a-proposition p in the CG which
negates a felicity condition of the utterance. E.g.,
doch in (11) shows that the first preparatory con-
dition of a request (the speaker believes that the
hearer can do it) does not hold, even though this
condition follows defeasibly from the fact that the
request was made.
In (12), doch addresses the second preparatory
condition of a request or advice (that it is not obvi-
ous to speaker and hearer that the hearer complies
with the request in a normal course of events).
Thus, doch suggests that it is obvious that the
hearer will do anyway what is requested or ad-
vised, even though uttering (12) defeasibly entails
the contrary. Consequently, (12) presents a request
or advice as a very natural thing to do.
I.e., doch-imperatives do not correct unwanted
behaviour by the hearer (pace Thurmair (1989)),
which is confirmed by examples like (16), which
can be uttered between future lovers during their
courtship to take the process of courting one step
further:
(16) Komm doch mal vorbei!
?Just drop by.?
(16) does not request the hearer to change his
behaviour, because calling on the speaker was not
an option yet. Instead, visiting the speaker is pre-
sented as a very natural thing to do for the hearer,
i.e., once more the second preparatory condition
of a request does not hold.
Using doch in questions also indicates that a
felicity condition of the utterance does not hold,
even though its validity could be deduced defeasi-
bly from the fact that the question has been asked.
The relevant condition is the first preparatory con-
dition for questions (that the speaker does not
know the answer already).
Doch signals that this condition is not fulfilled,
either because the answer escapes the speaker only
momentarily, as in (13), because he obviously
knows, as in the conventionalised announcement
(14), or because he would not accept a refusal,
which settles the question, like in (15).
The analysis predicts that doch is not acceptable
in ordinary questions, which is borne out e.g. by
(17), because in these questions there is no ten-
sion between uttering the question and potential
obstacles for its felicity conditions:
(17) *Wer schreibt dir doch?
?But who is corresponding with you??
Rhetorical questions are also incompatible with
doch, but for a different reason. Consider e.g.
the contrasting dialogue pairs (18a)/(18b) and
(18a)/(18c):
(18) (a) A: Ich werde meinen 30. Geburtstag
mit einem gro?en Fest feiern.
A: ?I?ll throw a big party on the
occasion of my 30th birthday.?
(b) B: Es wu?rde doch keiner zu deinem Fest
kommen.
B: ?But no one would come to your
party.?
(c) B: *Wer wu?rde doch zu deinem Fest
kommen?
B: ?But who would come to your
party??
The inacceptability of (18a)/(18c) - and of the
rhetorical doch-question in particular - is not due
to the function of the rhetorical question as a
negated statement: In this case, (18b) should be
an inacceptable response to (18a), too.
(18c) is inacceptable because rhetorical ques-
tions characterise statements as CG information
(Egg, 2007). This is also one of the effects of
doch; consequently, (18c) is as informative as
136
(18b) but more complex, hence, its use would
not comply to conversation maxims (Grice, 1975;
Krifka, 1989).
To sum up, non-declarative doch-utterances re-
fer to their own felicity conditions; since they do
not denote propositions, the first semantic argu-
ment of doch cannot be the meaning of the doch-
utterance.
Instead, doch applies to the fact that the speaker
uttered the sentence. In contrast, declarative doch-
utterances like in (8) or (9) refer to a felicity con-
dition of the non-declarative a-utterance.
This analysis of non-declarative doch-
utterances also applies to the hitherto extremely
problematic group of discourse-initial doch-
utterances:
(19) Morgen fahre ich doch nach Wien.
?Well, I?ll go to Vienna tomorrow.?
(20) Du hast doch ein Auto.
?Well, you have a car.?
These examples are characterised by doch as a
reminder. This means that the p-utterance (the
speaker?s travel plans or the fact that the hearer has
a car) contributes information semantically that is
already in the CG. However, this information is
not obviously in tension to any other information.
This raises the question of what the semantic ar-
guments of doch are in these cases.
Here doch addresses the first preparatory condi-
tion for statements, viz., that it is not obvious to
the speaker that the hearer already knows what the
speaker will say. Uttering the statement (= q) de-
feasibly implies this condition (= ?p), but accord-
ing to the CG the speaker knows that the hearer
knows (= p).
(21) [= (2)] instantiates this case, too:
(21) Sie sind doch Paul Meier.
?You must be Paul Meier.?
Telling someone his name obviously violates
the first preparatory condition for statements,
whence the use of doch.
Another such case is the use of doch in expres-
sions of outrage. Here doch signals that it is com-
mon knowledge that the hearer knows that the sit-
uation or action to which the speaker refers is out-
rageous:
(22) Das ist doch die Ho?he!
?That is the limit!?
Finally, the sincerity condition of a statement
can also be targeted by doch:
(23) Da sagt er doch im letzten Moment ab!
?I can?t believe that he cancelled the
appointment at the last moment.?
In (23), doch expresses disbelief of the speaker,
he cannot believe what he is saying. This violates
the sincerity condition for statements. The effect
of doch here is one of expressing surprise.
The same effect shows up in exclamative wh-
sentences:
(24) Wie scho?n Ame?lie doch ist!
?How beautiful Ame?lie is!?
Following analyses of these sentences like
Zanuttini and Portner (2003) or Rett (2009), (24)
characterises the degree of Ame?lie?s beauty as un-
expectedly or surprisingly high. Hence, doch nat-
urally occurs in these exclamatives to deny a belief
of the speaker in what he is stating.
In sum, I offered a uniform semantic analysis
of doch, which still covers a wide range of its us-
ages. Doch relates two propositions p and q iff q is
derivable from the common ground as well as the
fact that q defeasibly implies ?p, i.e., q presents a
potential impediment for p.
The correlation of p and q with utterances is
flexible, however: Often q is the meaning of
the doch-utterance, but for non-declarative and
discourse-initial declarative doch-utterances, q is
the fact that this utterance has been made.
The proposition p can be the meaning of a pre-
ceding a-utterance to which the doch-utterance is
a reaction. But especially for non-declarative a-
utterances, p can also be one of its felicity con-
ditions, or, for discourse-initial doch-utterances, a
felicity condition of the utterance itself.
4 Conclusion and outlook
The paper outlined a research programme for dis-
course particles that captures their meanings in
very specific semantic descriptions that neverthe-
less account for the wide range of their uses. These
two competing goals can be pursued simultane-
ously because doch-utterances can be integrated
flexibly into the meaning of the surrounding dis-
course.
While discourse particles like doch uniformly
relate two propositions semantically, the meaning
of the utterance of which the particle is a part, and
137
the meaning of the utterance to which this first ut-
terance reacts are not the only feasible semantic
arguments of the particles: They can also have fe-
licity conditions of these two utterances as seman-
tic arguments.
This research programme was illustrated by in-
vestigating the particle doch. The next steps now
are to extend the coverage of this analysis to other
particles, in particular, schon, and to contrast ?min-
imal pairs? of discourses which differ only by dis-
course particles (e.g., Komm schon! as opposed
to Komm doch!, which both require the hearer to
come).
This analysis can also be used for investigations
of stressed and unstressed forms of discourse par-
ticles and of the relation between them. Here it is
particularly interesting to take prosody seriously
and to look into the semantic effects of emphasis-
ing a discourse particle.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of conversation. Cambridge University Press, Cam-
bridge.
Sophia Do?ring. 2010. On context shift in German dis-
course particles. BA thesis, Humboldt-Universita?t
Berlin.
Markus Egg. 2007. Meaning and use of rhetorical
questions. In Maria Aloni, Paul Dekker, and Floris
Roelofsen, editors, Proceedings of the 16th Amster-
dam Colloquium, pages 73?78. Universiteit van Am-
sterdam, ILLC.
Paul Grice. 1975. Logic and conversation. In Peter
Cole and Jerry Morgan, editors, Syntax and seman-
tics 3: Speech acts, pages 41?58. Academic Press,
New York.
Brigitte Grote, Nils Lenke, and Manfred Stede. 1997.
Ma(r)king concessions in English and German. Dis-
course Processes, 24:87?118.
Gerhard Helbig. 1988. Lexikon deutscher Partikeln.
Verlag Enzyklopa?die, Leipzig.
Elena Karagjosova. 2004. The meaning and function
of German modal particles. Ph.D. thesis, Univer-
sita?t des Saarlandes.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. the-
sis, University of Edinburgh.
Alistair Knott. 2001. Semantic and pragmatic relations
and their intended effects. In T. Sanders, J. Schilper-
oord, and W. Spooren, editors, Text representation:
linguistic and psycholinguistic aspects, pages 127?
151. Benjamins, Amsterdam.
Ekkehard Ko?nig and Susanne Requardt. 1997. A
relevance-theoretic approach to the analysis of
modal particles. Multilingua, 10:63?77.
Ekkehard Ko?nig. 1997. Zur Bedeutung von Modal-
partikeln im Deutschen: Ein Neuansatz im Rah-
men der Relevanztheorie. Germanistische Linguis-
tik, 136:57?75.
Manfred Krifka. 1989. Nominalreferenz und Zeitkon-
stitution. Fink, Mu?nchen.
Luuk Lagerwerf. 1998. Causal connectives have
presuppositions. Effects on coherence and discourse
structure. Ph.D. thesis, Universiteit van Tilburg.
Leonoor Oversteegen. 1997. On the pragmatic nature
of causal and contrastive connectives. Discourse
Processes, 24:51?85.
Jessica Rett. 2009. A degree account of exclamatives.
In Proceedings of SALT XVIII, pages 601?608. CLC
Publications.
Robert Stalnaker. 2002. Common ground. Linguistics
& Philosophy, 25:701?721.
Eve Sweetser. 1990. From etymology to pragmat-
ics. Metaphorical and cultural aspects of semantic
structure. Cambridge University Press, Cambridge.
Maria Thurmair. 1989. Modalpartikeln und ihre Kom-
binationen. Niemeyer, Tu?bingen.
Raffaela Zanuttini and Paul Portner. 2003. Exclama-
tive clauses: At the syntax-semantics interface. Lan-
guage, 79:39?81.
Malte Zimmermann. 2009. Discourse particles.
In Claudia Maienborn, Klaus von Heusinger, and
Paul Portner, editors, Semantics: an international
handbook of natural language meaning. Mouton de
Gruyter, Berlin. In press.
138
