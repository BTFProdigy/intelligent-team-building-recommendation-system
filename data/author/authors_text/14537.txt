Proceedings of NAACL-HLT 2013, pages 1072?1081,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Emergence of Gricean Maxims from Multi-Agent Decision Theory
Adam Vogel, Max Bodoia, Christopher Potts, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{acvogel,mbodoia,cgpotts,jurafsky}@stanford.edu
Abstract
Grice characterized communication in terms
of the cooperative principle, which enjoins
speakers to make only contributions that serve
the evolving conversational goals. We show
that the cooperative principle and the associ-
ated maxims of relevance, quality, and quan-
tity emerge from multi-agent decision theory.
We utilize the Decentralized Partially Observ-
able Markov Decision Process (Dec-POMDP)
model of multi-agent decision making which
relies only on basic definitions of rationality
and the ability of agents to reason about each
other?s beliefs in maximizing joint utility. Our
model uses cognitively-inspired heuristics to
simplify the otherwise intractable task of rea-
soning jointly about actions, the environment,
and the nested beliefs of other actors. Our
experiments on a cooperative language task
show that reasoning about others? belief states,
and the resulting emergent Gricean commu-
nicative behavior, leads to significantly im-
proved task performance.
1 Introduction
Grice (1975) famously characterized communica-
tion among rational agents in terms of an overarch-
ing cooperative principle and a set of more specific
maxims, which enjoin speakers to make contribu-
tions that are truthful, informative, relevant, clear,
and concise. Since then, there have been many at-
tempts to derive the maxims (or perhaps just their ef-
fects) from more basic cognitive principles concern-
ing how people make decisions, formulate plans,
and collaborate to achieve goals. This research
traces to early work by Lewis (1969) on signaling
systems. It has recently been the subject of ex-
tensive theoretical discussion (Clark, 1996; Merin,
1997; Blutner, 1998; Parikh, 2001; Beaver, 2002;
van Rooy, 2003; Benz et al, 2005; Franke, 2009)
and has been tested experimentally using one-step
games in which the speaker produces a message and
the hearer ventures a guess as to its intended refer-
ent (Rosenberg and Cohen, 1964; Dale and Reiter,
1995; Golland et al, 2010; Stiller et al, 2011; Frank
and Goodman, 2012; Krahmer and van Deemter,
2012; Degen and Franke, 2012; Rohde et al, 2012).
To date, however, these theoretical models and ex-
periments have not been extended to multi-step in-
teractions extending over time and involving both
language and action together, which leaves this work
relatively disconnected from research on planning
and goal-orientation in artificial agents (Perrault
and Allen, 1980; Allen, 1991; Grosz and Sidner,
1986; Bratman, 1987; Hobbs et al, 1993; Allen
et al, 2007; DeVault et al, 2005; Stone et al,
2007; DeVault, 2008). We attribute this in large
part to the complexity of Gricean reasoning itself,
which requires agents to model each other?s belief
states. Tracking these as they evolve over time in re-
sponse to experiences is extremely demanding. Our
approach complements slot-filling dialog systems,
where the focus is on managing speech recogni-
tion uncertainty (Young et al, 2010; Thomson and
Young, 2010).
However, recent years have seen significant ad-
vances in multi-agent decision-theoretic models and
their efficient implementation. With the current pa-
per, we seek to show that the Decentralized Par-
1072
tially Observable Markov Decision Process (Dec-
POMDP) provides a robust, flexible foundation for
implementing agents that communicate in a Gricean
manner. Dec-POMDPs are multi-agent, partially-
observable models in which agents maintain be-
lief distributions over the underlying, hidden world
state, including the beliefs of the other players, and
speech actions change those beliefs. In this setting,
informative, relevant communication emerges as the
best way to maximize joint utility.
The complexity of pragmatic reasoning is still
forbidding, though. Correspondingly, optimal de-
cision making in Dec-POMDPs is NEXP complete
(Bernstein et al, 2002). To manage this issue, we
introduce several cognitively-plausible approxima-
tions which allow us to simplify the Dec-POMDP to
a single-agent POMDP, for which relatively efficient
solvers exist (Spaan and Vlassis, 2005). We demon-
strate our algorithms on a variation of the Cards task,
a partially-observable collaborative search problem
(Potts, 2012). Spatial language comprises the bulk
of communication in the Cards task, and we dis-
cuss a model of spatial semantics in Section 3. Us-
ing this task and a model of the meaning of spatial
language, we next discuss two agents that play the
game: ListenerBot (Section 4) makes decisions us-
ing a single-agent POMDP that does not take into
account the beliefs or actions of its partner, whereas
DialogBot (Section 5) maintains a model of its part-
ner?s beliefs. As a result of the cooperative structure
of the underlying model and the effects of commu-
nication within it, DialogBot?s contributions are rel-
evant, truthful, and informative, which leads to sig-
nificantly improved task performance.
2 The Cards Task and Corpus
The Cards corpus consists of 1,266 transcripts1 from
an online, two-person collaborative game in which
two players explore a maze-like environment, com-
municating with each other via a text chat window
(Figure 1). A deck of playing cards has been dis-
tributed randomly around the environment, and the
players? task is to find six consecutive cards of the
same suit. Our implemented agents solve a sim-
plified version of this task in which the two agents
1Released by Potts (2012) at http://cardscorpus.
christopherpotts.net
Figure 1: The Cards corpus gameboard. Player 1?s
location is marked ?P1?. The nearby yellow boxes
mark card locations. The dialogue history and chat
window are at the top. This board, the one we use
throughout, consists of 231 open grid squares.
must both end up co-located with a single card, the
Ace of Spades (AS). This is much simpler than the
six-card version from the human?human corpus, but
it involves the same kind of collaborative goal and
forces our agents to deal with the same kind of par-
tial knowledge about the world as the humans did.
Each agent knows its own location, but not his part-
ner?s, and a player can see the AS only when co-
located with it. The agents use (simplified) English
to communicate with each other.
3 Spatial Semantics
Much of the communication in the Cards task in-
volves referring to spatial locations on the board.
Accordingly, we focus on spatial language for our
artificial agents. In this section, we present a model
of spatial semantics, which we create by leveraging
the human?human Cards transcripts. We discuss the
spatial semantic representation, how we classify the
semantics of new locative expressions, and our use
of spatial semantics to form a high-level state space
for decision making.
3.1 Semantic Representation
Potts (2012) released annotations, derived from the
Cards corpus, which reduce 599 of the players?
statements about their locations to formulae of the
form ? (?1 ? ?? ? ? ?k), where ? is a domain and
?1, . . . ,?k are semantic literals. For example, the ut-
terance ?(I?m) at the top right of the board? is anno-
tated as BOARD(top? right), and ?(I?m) in bottom
1073
of the C room? is annotated as C room(bottom). Ta-
ble 1 lists the full set of semantic primitives that ap-
pear as domain expressions and literals.
Because the Cards transcripts are so highly struc-
tured, we can interpret these expressions in terms
of the Cards world itself. For a given formula
? = ? (?1 ? ?? ? ? ?k), we compute the number of
times that a player identified its location with (an
utterance translated as) ? while standing on grid
square (x,y). These counts are smoothed using
a simple 2D-smoothing scheme, detailed in (Potts,
2012), and normalized in the usual manner to form a
distribution over board squares Pr((x,y)|?). These
grounded interpretations are the basis for commu-
nication between the artificial agents we define in
Section 4.
BOARD, SQUARE, right, middle, top, left, bot-
tom, corner, approx, precise, entrance, C room,
hall, room, sideways C, loop, reverse C,
U room, T room, deadend, wall, sideways F
Table 1: The spatial semantic primitives.
3.2 Semantics Classifier
Using the corpus examples of utterances paired with
their spatial semantic representations, we learn a set
of classifiers to predict a spatial utterance?s semantic
representation. We train a binary classifier for each
semantic primitive ?i using a log-linear model with
simple bag of words features. The words are not
normalized or stemmed and we use whitespace tok-
enization. We additionally train a multi-class clas-
sifier for all possible domains ? . At test time, we
use the domain classifier and each primitive binary
classifier to produce a semantic representation.
3.3 Semantic State Space
The decision making algorithms that we discuss in
Section 4 are highly sensitive to the size of the state
space. The full representation of the game board
consists of 231 squares. Representing the location
of both players and the location of the card requires
3233 = 12,326,391 states, well beyond the capabil-
ities of current decision-making algorithms.
To ameliorate this difficulty, we cluster squares
together using the spatial referring expression cor-
Figure 2: Semantic state space clusters with k = 16.
pus. This approach follows from research that
shows that humans? mental spatial representations
are influenced by their language (Hayward and Tarr,
1995). Our intuition is that human players do not
consider all possible locations of the card and play-
ers, but instead lump them into semantically coher-
ent states, such as ?the card is in the top right cor-
ner.? Following this intuition, we cluster states to-
gether which have similar referring expressions, al-
lowing our agents to use language as a cognitive
technology and not just a tool for communication.
For each board square (x,y) we form a vector
?(x,y) with ?i(x,y) = Pr((x,y)|?i), where ?i is the
ith distinct semantic representation in the corpus.
This forms a 136-dimensional vector for each board
square. We then use k-means clustering with a Eu-
clidean distance metric in this semantic space to
cluster states which are referred to similarly.
Figure 2 shows a clustering for k = 16 which we
utilize for the remainder of the paper. Denoting the
board regions by {1, . . . ,Nregions}, we compute the
probability of an expression ? referring to a region
r by averaging over the squares in the region:
Pr(r|?i) ? ?
(x,y)? region r
Pr((x,y)|?i)
|{(x,y)|(x,y) ? region r}|
4 ListenerBot
We first introduce ListenerBot, an agent that does
not take into account the actions or beliefs of its
partner. ListenerBot decides what actions to take
using a Partially Observable Markov Decision Pro-
cess (POMDP). This allows ListenerBot to track its
beliefs about the location of the card and to incor-
porate linguistic advice. However, ListenerBot does
not produce utterances.
1074
A POMDP is defined by a tuple
(S,A,T,O,?,R,b0,?). We explicate each com-
ponent with examples from our task. Figure 3(a)
provides the POMDP influence diagram.
States S is the finite state space of the world. The
state space S of ListenerBot consists of the location
of the player p and the location of the card c. As
discussed above in Section 3.3, we cluster squares of
the board into Nregions semantically coherent regions,
denoted by {1, . . . ,Nregions}. The state space over
these regions is defined as
S := {(p,c)|p,c ? {1, . . . ,Nregions}}
Two regions r1 and r2 are called adjacent, written
adj(r1,r2), if any of their constituent squares touch.
Actions A is the set of actions available to the
agent. ListenerBot can only take physical actions
and has no communicative ability. Physical actions
in our region-based state space are composed of two
types: traveling to a region and searching a region.
? travel(r): travel to region r
? search: player exhaustively searches the cur-
rent region
Transition Distributions The transition distribu-
tion T (s?|a,s) models the dynamics of the world.
This represents the ramifications of physical actions
such as moving around the map. For a state s =
(p,c) and action a = travel(r), the player moves to
region r if it is adjacent to p, and otherwise stays in
the same place:
T ((p?,c?)|travel(r),(p,c))=
?
???????
???????
1 adj(r, p)? p? = r
?c = c?
1 ?adj(r, p)? p = p?
?c = c?
0 otherwise
Search actions are only concerned with observations
and do not change the state of the world:2
T ((p?,c?)|search,(p,c)) = 1
[
p? = p? c? = c
]
The travel and search high-level actions are trans-
lated into low-level (up, down, left, right) actions
using a simple A? path planner.
Observations Agents receive observations from
a set O according to an observation distribution
2
1[Q] is the indicator function, which is 1 if proposition Q
is true and 0 otherwise.
?(o|s?,a). Observations include properties of the
physical world, such as the location of the card, and
also natural language utterances, which serve to in-
directly change agents? beliefs about the world and
the beliefs of their interlocutors.
Search actions generate two possible observa-
tions: ohere and o?here, which denote the presence
or absence of the card from the current region.
?(ohere|(p
?,c?),search) = 1
[
p? = c?
]
?(o?here|(p
?,c?),search) = 1
[
p? 6= c?
]
Travel actions do not generate meaningful observa-
tions:
?(o?here|(p
?,c?), travel) = 1
Linguistic Advice We model linguistic advice as
another form of observation. Agents receive mes-
sages from a finite set ?, and each message ? ? ?
has a semantics, or distribution over the state space
Pr(s|?). In the Cards task, we use the semantic dis-
tributions defined in Section 3. To combine the se-
mantics of language with the standard POMDP ob-
servation model, we apply Bayes? rule:
Pr(? |s) = Pr(s|?)Pr(?)
?? ? Pr(s|? ?)Pr(? ?)
(1)
The prior, Pr(?), can be derived from corpus data.
By treating language as just another form of ob-
servation, we are able to leverage existing POMDP
solution algorithms. This approach contrasts with
previous work on communication in Dec-POMDPs,
where agents directly share their perceptual obser-
vations (Pynadath and Tambe, 2002; Spaan et al,
2008), an assumption which does not fit natural lan-
guage.
Reward The reward function R(s,a) : S? R rep-
resents the goals of the agent, who chooses actions
to maximize reward. The goal of the Cards task is
for both players to be on top of the card, so any ac-
tion that leads to this state receives a high reward R+.
All other actions receive a small negative reward R?,
which gives agents an incentive to finish the task as
quickly as possible.
R((p,c),a) =
{
R+ p = c
R? p 6= c
Lastly, ? ? [0,1) is the discount factor, specifying
the trade-off between immediate and future rewards.
1075
s s?
o o?a
R
(a) ListenerBot POMDP
s s?
o1 o?1
o2 o?2
a1
a2
R
(b) Full Dec-POMDP
s s?
o o?a
R
s? s??
(c) DialogBot POMDP
Figure 3: The decision diagram for the ListenerBot POMDP, the full Dec-POMDP, and the DialogBot ap-
proximation POMDP. The ListenerBot (a) only considers his own location p and the card location c. In the
full Dec-POMDP (b), both agents receive individual observations and choose actions independently. Opti-
mal decision making requires tracking all possible histories of beliefs of the other agent. In diagram (c), Di-
alogBot approximates the full Dec-POMDP as single-agent POMDP. At each time step, DialogBot marginal-
izes out the possible observations o? that ListenerBot received, yielding an expected belief state b?.
Initial Belief State The initial belief state, b0 ?
?(S), is a distribution over the state space S. Lis-
tenerBot begins each game with a known initial lo-
cation p0 but a uniform distribution over the location
of the card c:
b0(p,c) ?
{
1
Nregions
p = p0
0 otherwise
Belief Update and Decision Making The key de-
cision making problem in POMDPs is the construc-
tion of a policy pi : ?(S)? A, a function from beliefs
to actions which dictates how the agent acts. Deci-
sion making in POMDPs proceeds as follows. The
world starts in a hidden state s0 ? b0. The agent
executes action a0 = pi(b0). The underlying hid-
den world state transitions to s1 ? T (s?|a0,s0), the
world generates observation o0 ? ?(o|s1,a0), and
the agent receives reward R(s0,a0). Using the obser-
vation o0, the agent constructs a new belief b1 ??(S)
using Bayes? rule:
bat ,ott+1 (s
?) = Pr(s?|at ,ot ,bt)
=
Pr(ot |at ,s?,bt)Pr(s?|at ,bt)
Pr(ot |bt ,at)
=
?(ot |s?,at)?s?S T (s
?|at ,s)bt(s)
?s???(ot |s??,at)?s?S T (s??|at ,s)bt(s)
This process is referred to as belief update and is
analogous to the forward algorithm in HMMs. To in-
corporate communication into the standard POMDP
model, we consider observations (o,?) ? O ? ?
which are a combination of a perceptual observation
o and a received message ? . The semantics of the
message ? is included in the belief update equation
using Pr(s|?), derived in Equation 1:
bat ,ot ,?tt+1 (s
?) =
?(o|s?,a) Pr(s
?|?)Pr(?)
?? ??? Pr(s
?|? ?)Pr(? ?) ?s?S T (s
?|a,s)bt(s)
?s???S?(o|s??,a)
Pr(s??|?)Pr(?)
?? ??? Pr(s
??|? ?)Pr(? ?) ?s?S T (s
??|a,s)bt(s)
Using this new belief state b1, the agent selects an
action a1 = pi(b1), and the process continues.
An initial belief state b0 and a policy pi to-
gether define a Markov chain over pairs of states
and actions. For a given policy pi , we define a
value function V pi : ?(S)? R which represents the
expected discounted reward with respect to that
Markov chain:
V pi(b0) =
?
?
t=0
? t E[R(bt ,at)|b0,pi]
The goal of the agent is find a policy pi? which max-
imizes the value of the initial belief state:
pi? = argmax
pi
V pi(b0)
Exact computation of pi? is PSPACE-complete (Pa-
padimitriou and Tsitsiklis, 1987), making approx-
imation algorithms necessary for all but the sim-
plest problems. We use Perseus (Spaan and Vlassis,
2005), an anytime approximate point-based value it-
1076
eration algorithm.
5 DialogBot
We now introduce DialogBot, a Cards agent which
is capable of producing linguistic advice. To decide
when and how to speak, DialogBot maintains a dis-
tribution over its partner?s beliefs and reasons about
the effects his utterances will have on those beliefs.
To handle these complexities, DialogBot models
the world as a Decentralized Partially Observable
Markov Decision Process (Dec-POMDP) (Bernstein
et al, 2002). See Figure 3(b) for the influence dia-
gram. The definition of Dec-POMDPs mirrors that
of the POMDP, with the following changes.
There is a finite set I of agents, which we re-
strict to two. Each agent takes an action ai at
each time step, forming a joint action ~a = (a1,a2).
Each agent receives its own observation oi accord-
ing to ?(o1,o2|a1,a2,s?). The transition distribu-
tions T (s?|a1,a2,s) and the reward R(s,a1,a2) both
depend on both agents? actions.
Optimal decision making in Dec-POMDPs re-
quires maintaining a probability distribution over
all possible sequences of actions and observations
(a?1, o?1, . . . , a?t , o?t) that the other player might have
received. As t increases, we have an exponential in-
crease in the belief states an agent must consider.
Confirming this informal intuition, decision mak-
ing in Dec-POMDPs is NEXP-complete, a complex-
ity class above P-SPACE (Bernstein et al, 2002).
This computational complexity limits the applica-
tion of Dec-POMDPs to very small problems. To
address this difficulty we make several simplifying
assumptions, allowing us to construct a single-agent
POMDP which approximates the full Dec-POMDP.
Firstly, we assume that other agents do not take
into account our own beliefs, i.e., the other agent
acts like a ListenerBot. This bypasses the infinitely
nested belief problem by assuming that other agents
track one less level of nested beliefs, a common
approach (Goodman and Stuhlmu?ller, 2012; Gmy-
trasiewicz and Doshi, 2005).
Secondly, instead of tracking the full tree of pos-
sible observation histories, we maintain a point es-
timate b? of the other agent?s beliefs, which we
term the expected belief state. Rather than track-
ing each possible observation/action history of the
other agent, at each time step we marginalize out
the observations they could have received. Figure 4
compares this approach with exact belief update.
Thirdly, we assume that the other agent acts ac-
cording to a variant of the QMDP approximation
(Littman et al, 1995). Under this approximation, the
other agent solves a fully-observable MDP version
of the ListenerBot POMDP, yielding an MDP pol-
icy p?i : S? A. This critically allows us to approxi-
mate the other agent?s belief update using a specially
formed POMDP, which we detail next.
State Space To construct the approximate single-
agent POMDP from the full Dec-POMDP problem,
we formulate the state space as S? S. (See Figure
3(c) for the influence diagram.) We write a state
(s, s?) ? S? S, where s is DialogBot?s beliefs about
the true state of the world, and s? is DialogBot?s esti-
mate of the other agent?s beliefs.
Transition Distribution The main difficulty
in constructing the approximate single-agent
POMDP is specifying the transition distribu-
tion T ((s?, s??)|a,(s, s?)). To address this, we
break this distribution into two components:
T ((s?, s??)|a,(s, s?)) = T? (s??|s?,a,(s, s?))T (s?|a,s, s?).
The first term dictates how DialogBot updates its
beliefs about the other agent?s beliefs:
T? (s??|s?,a,(s, s?)) = Pr(s??|s?,a,(s, s?))
=?
o??O
Pr(s??|a, o?, s?,s)Pr(o?|s?,a, p?i(s?))
=?
o??O
(
?(o?|s??,a, p?i(s?))T (s??|a, p?i(s?), s?)
?s????(o?|s???,a, p?i(s?))T (s???|a, p?i(s?), s?)
??(o?|s?,a, p?i(s?))
)
We sum over all observations o? the other agent could
have received, updating our probability of s?? as Lis-
tenerBot would have, multiplied by the probability
that ListenerBot would have received that observa-
tion, ?(o?|s?, p?i(s?)). The QMDP approximation al-
lows us to simulate ListenerBot?s belief update in
T? (s??|s?,a,(s, s?)). Exact belief update would require
access to b?: by using p?i(s?) we can estimate the action
that ListenerBot would have taken.
In cases where s? contradicts s such that for all o? ei-
ther ?(o?|s?, p?i(s?)) = 0 or ?(o?|s??, p?i(s?)) = 0, we redis-
tribute the belief mass uniformly: T? (s??|s?,a,(s, s?))?
1077
b?t
b?o1t+1
o1
b?o2t+1
o2
b?o1,o1t+2
o1
b?o1,o2t+2
o2
b?o2,o1t+2
o1
b?o2,o2t+2
o2
(a) Exact multi-agent belief tracking
b?t
o1
o2
o1
o2
b?t+1
o1
o2
o1
o2
b?t+2
(b) Approximate multi-agent belief tracking
Figure 4: Exact multi-agent belief tracking compared with our approximate approach. Each node represents
a belief state. In exact tracking (a), the agent tracks every possible history of observations that its partner
could have received, which grows exponentially in time. In approximate update (b), the agent considers each
possible observation and then averages the resulting belief states, weighted by the probability the other agent
received that observation, resulting in a single summary belief state b?t+1. Under the QMDP approximation,
the agent considers what action the other agent would have taken if it completely believed the world was in
a certain state. Thus, there are four belief states resulting from b?t , as opposed to two in the exact case.
1 ?s?? 6= s?. This approach to managing contradiction
is analogous to logical belief revision (Alchourrono?n
et al, 1985; Ga?rdenfors, 1988; Ferme? and Hansson,
2011).
Speech Actions Speech actions are modeled by
how they change the beliefs of the other agent.
The effects of a speech actions are modeled in
T? (s??|s?,a,(s, s?)), our model of how ListenerBot?s be-
liefs change. For a speech action a = say(?) with
? ? ?,
T? (s??|s?,a,(s, s?)) =
?
o??O
(
?(o?|s??,a, p?i(s?))Pr(? |s??)T (s??|a, p?i(s?), s?)
?s????(o?|s???,a, p?i(s?))Pr(? |s???)T (s???|a, p?i(s?), s?)
??(o?|s?,a, p?i(s?))
)
DialogBot is equipped with the five most
frequent speech actions: BOARD(middle),
BOARD(top), BOARD(bottom), BOARD(left),
and BOARD(right). It produces concrete utterances
by selecting a sentence from the training corpus
with the desired semantics.
Reward DialogBot receives a large reward when
both it and its partner are located on the card, and a
negative cost when moving or speaking:
R((p,c, p?, c?),a) =
{
R+ p = c? p? = c
R? p 6= c? p? 6= c
DialogBot?s reward is not dependent on the beliefs
of the other player, only the true underlying state of
the world.
6 Experimental Results
We now experimentally evaluate our semantic clas-
sifiers and the agents? task performance.
6.1 Spatial Semantics Classifiers
We report the performance of our spatial seman-
tics classifiers, although their accuracy is not the fo-
cus of this paper. We use 10-fold cross validation
on a corpus of 577 annotated utterances. We used
simple bag-of-words features, so overfitting the data
with cross validation is not a pressing concern. Of
the 577 utterances, our classifiers perfectly labeled
325 (56.3% accuracy). The classifiers correctly pre-
dicted the domain ? of 515 (89.3%) utterances. The
1078
precision of our binary semantic primitive classifiers
was 9691126 = .861 and recall
969
1242 = .780, yielding F1
measure .818.
6.2 Cards Task Evaluation
We evaluated our ListenerBot and DialogBot agents
in the Cards task. Using 500 randomly generated
initial player and card locations, we tested each
combination of ListenerBot and DialogBot partners.
Agents succeeded at a given initial position if they
both reached the card within 50 moves. Table 2
shows how many trials each dyad won and how
many high-level actions they took to do so.
Agents % Success Moves
LB & LB 84.4% 19.8
LB & DB 87.2% 17.5
DB & DB 90.6% 16.6
Table 2: The evaluation for each combination of
agents. LB = ListenerBot; DB = DialogBot.
Collaborating DialogBots performed the best,
completing more trials and using fewer moves than
the ListenerBots. The DialogBots initially explore
the space in a similar manner to the ListenerBots,
but then share card location information. This leads
to shorter interactions, as once the DialogBot finds
the card, the other player can find it more quickly.
In the combination of ListenerBot and DialogBot,
we see about half of the improvement over two Lis-
tenerBots. Roughly 50% of the time, the Listener-
Bot finds the card first, which doesn?t help the Di-
alogBot find the card any faster.
7 Emergent Pragmatics
Grice?s original model of pragmatics (Grice, 1975)
involves the cooperative principle and four maxims:
quality (?say only what you know to be true?), rela-
tion (?be relevant?), quantity (?be as informative as
is required; do not say more than is required?), and
manner (roughly, be clear and concise).
In most interactions, DialogBot searches for the
card and then reports its location to the other agent.
These reports obey quality in that they are made only
when based on actual observations. The behavior
is not hard-coded, but rather emerges, because only
accurate information serves the agents? goals. In
contrast, sub-optimal policies generated early in the
POMDP solving process sometimes lie about card
locations. Since this behavior confuses the other
agent and thus has a lower utility, it gets replaced
by truthful communication as the policies improve.
We also capture the effects of relation and the first
clause of quantity, because the nature of the reward
function and the nested belief structures ensure that
DialogBot offers only relevant, informative informa-
tion. For instance, when DialogBot finds the card in
the lower left corner, it alternates saying ?left? and
?bottom?, effectively overcoming its limited gener-
ation capabilities. Again, early sub-optimal policies
sometimes do not report the location of the card at
all, thereby failing to fulfill these maxims.
We expect these models to produce behavior con-
sistent with manner and the second clause of quan-
tity, but evaluating this claim will require a richer ex-
perimental paradigm. For example, if DialogBot had
a larger and more structured vocabulary, it would
have to choose between levels of specificity as well
as more or less economical forms.
8 Conclusion
We have shown that cooperative pragmatic behavior
can arise from multi-agent decision-theoretic mod-
els in which the agents share a joint utility func-
tion and reason about each other?s belief states.
Decision-making in these models is intractable,
which has been a major obstacle to achieving exper-
imental results in this area. We introduced a series
of approximations to manage this intractability: (i)
combining low-level states into semantically coher-
ent high-level ones; (ii) tracking only an averaged
summary of the other agent?s potential beliefs; (iii)
limiting belief state nesting to one level, and (iv)
simplifying each agent?s model of the other?s be-
liefs so as to reduce uncertainty. These approxima-
tions bring the problems under sufficient control that
they can be solved with current POMDP approxi-
mation algorithms. Our experimental results high-
light the rich pragmatic behavior this gives rise to
and quantify the communicative value of such be-
havior. While there remain insights from earlier the-
oretical proposals and logic-based methods that we
have not fully captured, our current results support
1079
the notion that probabilistic decision-making meth-
ods can yield robust, widely applicable models that
address the real-world difficulties of partial observ-
ability and uncertainty.
Acknowledgments
This research was supported in part by ONR
grants N00014-10-1-0109 and N00014-13-1-0287
and ARO grant W911NF-07-1-0216.
References
Carlos E. Alchourrono?n, Peter Ga?rdenfors, and David
Makinson. 1985. On the logic of theory change: Par-
tial meets contradiction and revision functions. Jour-
nal of Symbolic Logic, 50(2):510?530.
James F. Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. PLOW: A collaborative
task learning agent. In Proceedings of the Twenty-
Second AAAI Conference on Artificial Intelligence,
pages 1514?1519. AAAI Press, Vancouver, British
Columbia, Canada.
James F. Allen. 1991. Reasoning About Plans. Morgan
Kaufmann, San Francisco.
David Beaver. 2002. Pragmatics, and that?s an order. In
David Barker-Plummer, David Beaver, Johan van Ben-
them, and Patrick Scotto di Luzio, editors, Logic, Lan-
guage, and Visual Information, pages 192?215. CSLI,
Stanford, CA.
Anton Benz, Gerhard Ja?ger, and Robert van Rooij, edi-
tors. 2005. Game Theory and Pragmatics. Palgrave
McMillan, Basingstoke, Hampshire.
Daniel S. Bernstein, Robert Givan, Neil Immerman, and
Shlomo Zilberstein. 2002. The complexity of decen-
tralized control of Markov decision processes. Mathe-
matics of Operations Research, 27(4):819?840.
Reinhard Blutner. 1998. Lexical pragmatics. Journal of
Semantics, 15(2):115?162.
Michael Bratman. 1987. Intentions, Plans, and Practical
Reason. Harvard University Press.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Judith Degen and Michael Franke. 2012. Optimal rea-
soning about referential expressions. In Proceedings
of SemDIAL 2012, Paris, September.
David DeVault, Natalia Kariaeva, Anubha Kothari, Iris
Oved, and Matthew Stone. 2005. An information-
state approach to collaborative reference. In Proceed-
ings of the ACL Interactive Poster and Demonstration
Sessions, pages 1?4, Ann Arbor, MI, June. Association
for Computational Linguistics.
David DeVault. 2008. Contribution Tracking: Partici-
pating in Task-Oriented Dialogue under Uncertainty.
Ph.D. thesis, Rutgers University, New Brunswick, NJ.
Eduardo Ferme? and Sven Ove Hansson. 2011. AGM 25
years: Twenty-five years of research in belief change.
Journal of Philosophical Logic, 40(2):295?331.
Michael C. Frank and Noah D. Goodman. 2012. Predict-
ing pragmatic reasoning in language games. Science,
336(6084):998.
Michael Franke. 2009. Signal to Act: Game Theory
in Pragmatics. ILLC Dissertation Series. Institute for
Logic, Language and Computation, University of Am-
sterdam.
Peter Ga?rdenfors. 1988. Knowledge in Flux: Modeling
the Dynamics of Epistemic States. MIT Press.
Piotr J. Gmytrasiewicz and Prashant Doshi. 2005. A
framework for sequential planning in multi-agent set-
tings. Journal of Artificial Intelligence Research,
24:24?49.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 410?419, Cambridge, MA, October. ACL.
Noah D. Goodman and Andreas Stuhlmu?ller. 2012.
Knowledge and implicature: Modeling language un-
derstanding as social cognition. In Proceedings of the
Thirty-Fourth Annual Conference of the Cognitive Sci-
ence Society.
H. Paul Grice. 1975. Logic and conversation. In Peter
Cole and Jerry Morgan, editors, Syntax and Semantics,
volume 3: Speech Acts, pages 43?58. Academic Press,
New York.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175?204, July.
William G. Hayward and Michael J. Tarr. 1995. Spa-
tial language and spatial representation. Cognition,
55:39?84.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1?2):69?142.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38(1):173?218.
David Lewis. 1969. Convention. Harvard University
Press, Cambridge, MA. Reprinted 2002 by Blackwell.
1080
Michael L. Littman, Anthony R. Cassandra, and
Leslie Pack Kaelbling. 1995. Learning policies for
partially observable environments: Scaling up. In Ar-
mand Prieditis and Stuart J. Russell, editors, ICML,
pages 362?370. Morgan Kaufmann.
Arthur Merin. 1997. If all our arguments had to be con-
clusive, there would be few of them. Arbeitspapiere
SFB 340 101, University of Stuttgart, Stuttgart.
Christos Papadimitriou and John N. Tsitsiklis. 1987. The
complexity of markov decision processes. Math. Oper.
Res., 12(3):441?450, August.
Prashant Parikh. 2001. The Use of Language. CSLI,
Stanford, CA.
C. Raymond Perrault and James F. Allen. 1980. A plan-
based analysis of indirect speech acts. American Jour-
nal of Computational Linguistics, 6(3?4):167?182.
Christopher Potts. 2012. Goal-driven answers in the
Cards dialogue corpus. In Nathan Arnett and Ryan
Bennett, editors, Proceedings of the 30th West Coast
Conference on Formal Linguistics, Somerville, MA.
Cascadilla Press.
David V. Pynadath and Milind Tambe. 2002. The com-
municative multiagent team decision problem: Ana-
lyzing teamwork theories and models. Journal of Ar-
tificial Intelligence Research, 16:2002.
Hannah Rohde, Scott Seyfarth, Brady Clark, Gerhard
Ja?ger, and Stefan Kaufmann. 2012. Communicat-
ing with cost-based implicature: A game-theoretic ap-
proach to ambiguity. In The 16th Workshop on the Se-
mantics and Pragmatics of Dialogue, Paris, Septem-
ber.
Robert van Rooy. 2003. Questioning to resolve decision
problems. Linguistics and Philosophy, 26(6):727?
763.
Seymour Rosenberg and Bertram D. Cohen. 1964.
Speakers? and listeners? processes in a word commu-
nication task. Science, 145:1201?1203.
Matthijs T. J. Spaan and Nikos Vlassis. 2005. Perseus:
Randomized point-based value iteration for POMDPs.
Journal of Artificial Intelligence Research, 24(1):195?
220, August.
Matthijs T. J. Spaan, Frans A. Oliehoek, and Nikos Vlas-
sis. 2008. Multiagent planning under uncertainty with
stochastic communication delays. In In Proc. of the
18th Int. Conf. on Automated Planning and Schedul-
ing, pages 338?345.
Alex Stiller, Noah D. Goodman, and Michael C. Frank.
2011. Ad-hoc scalar implicature in adults and chil-
dren. In Proceedings of the 33rd Annual Meeting of
the Cognitive Science Society, Boston, July.
Matthew Stone, Richmond Thomason, and David De-
Vault. 2007. Enlightened update: A computational
architecture for presupposition and other pragmatic
phenomena. To appear in Donna K. Byron; Craige
Roberts; and Scott Schwenter, Presupposition Accom-
modation.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A pomdp framework for spoken
dialogue systems. Comput. Speech Lang., 24(4):562?
588, October.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The hidden information state model: A
practical framework for pomdp-based spoken dialogue
management. Comput. Speech Lang., 24(2):150?174,
April.
1081
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806?814,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning to Follow Navigational Directions
Adam Vogel and Dan Jurafsky
Department of Computer Science
Stanford University
{acvogel,jurafsky}@stanford.edu
Abstract
We present a system that learns to fol-
low navigational natural language direc-
tions. Where traditional models learn
from linguistic annotation or word distri-
butions, our approach is grounded in the
world, learning by apprenticeship from
routes through a map paired with English
descriptions. Lacking an explicit align-
ment between the text and the reference
path makes it difficult to determine what
portions of the language describe which
aspects of the route. We learn this corre-
spondence with a reinforcement learning
algorithm, using the deviation of the route
we follow from the intended path as a re-
ward signal. We demonstrate that our sys-
tem successfully grounds the meaning of
spatial terms like above and south into ge-
ometric properties of paths.
1 Introduction
Spatial language usage is a vital component for
physically grounded language understanding sys-
tems. Spoken language interfaces to robotic assis-
tants (Wei et al, 2009) and Geographic Informa-
tion Systems (Wang et al, 2004) must cope with
the inherent ambiguity in spatial descriptions.
The semantics of imperative and spatial lan-
guage is heavily dependent on the physical set-
ting it is situated in, motivating automated learn-
ing approaches to acquiring meaning. Tradi-
tional accounts of learning typically rely on lin-
guistic annotation (Zettlemoyer and Collins, 2009)
or word distributions (Curran, 2003). In con-
trast, we present an apprenticeship learning sys-
tem which learns to imitate human instruction fol-
lowing, without linguistic annotation. Solved us-
ing a reinforcement learning algorithm, our sys-
tem acquires the meaning of spatial words through
1. go vertically down until you?re underneath eh
diamond mine
2. then eh go right until you?re
3. you?re between springbok and highest view-
point
Figure 1: A path appears on the instruction giver?s
map, who describes it to the instruction follower.
grounded interaction with the world. This draws
on the intuition that children learn to use spatial
language through a mixture of observing adult lan-
guage usage and situated interaction in the world,
usually without explicit definitions (Tanz, 1980).
Our system learns to follow navigational direc-
tions in a route following task. We evaluate our
approach on the HCRC Map Task corpus (Ander-
son et al, 1991), a collection of spoken dialogs
describing paths to take through a map. In this
setting, two participants, the instruction giver and
instruction follower, each have a map composed
of named landmarks. Furthermore, the instruc-
tion giver has a route drawn on her map, and it
is her task to describe the path to the instruction
follower, who cannot see the reference path. Our
system learns to interpret these navigational direc-
tions, without access to explicit linguistic annota-
tion.
We frame direction following as an apprentice-
ship learning problem and solve it with a rein-
forcement learning algorithm, extending previous
work on interpreting instructions by Branavan et
al. (2009). Our task is to learn a policy, or mapping
806
from world state to action, which most closely fol-
lows the reference route. Our state space com-
bines world and linguistic features, representing
both our current position on the map and the com-
municative content of the utterances we are inter-
preting. During training we have access to the ref-
erence path, which allows us to measure the util-
ity, or reward, for each step of interpretation. Us-
ing this reward signal as a form of supervision, we
learn a policy to maximize the expected reward on
unseen examples.
2 Related Work
Levit and Roy (2007) developed a spatial seman-
tics for the Map Task corpus. They represent
instructions as Navigational Information Units,
which decompose the meaning of an instruction
into orthogonal constituents such as the reference
object, the type of movement, and quantitative as-
pect. For example, they represent the meaning of
?move two inches toward the house? as a reference
object (the house), a path descriptor (towards), and
a quantitative aspect (two inches). These represen-
tations are then combined to form a path through
the map. However, they do not learn these rep-
resentations from text, leaving natural language
processing as an open problem. The semantics
in our paper is simpler, eschewing quantitative as-
pects and path descriptors, and instead focusing
on reference objects and frames of reference. This
simplifies the learning task, without sacrificing the
core of their representation.
Learning to follow instructions by interacting
with the world was recently introduced by Brana-
van et al (2009), who developed a system which
learns to follow Windows Help guides. Our re-
inforcement learning formulation follows closely
from their work. Their approach can incorpo-
rate expert supervision into the reward function
in a similar manner to this paper, but is also able
to learn effectively from environment feedback
alone. The Map Task corpus is free form conversa-
tional English, whereas the Windows instructions
are written by a professional. In the Map Task cor-
pus we only observe expert route following behav-
ior, but are not told how portions of the text cor-
respond to parts of the path, leading to a difficult
learning problem.
The semantics of spatial language has been
studied for some time in the linguistics literature.
Talmy (1983) classifies the way spatial meaning is
Figure 2: The instruction giver and instruction fol-
lower face each other, and cannot see each others
maps.
encoded syntactically, and Fillmore (1997) studies
spatial terms as a subset of deictic language, which
depends heavily on non-linguistic context. Levin-
son (2003) conducted a cross-linguistic semantic
typology of spatial systems. Levinson categorizes
the frames of reference, or spatial coordinate sys-
tems1, into
1. Egocentric: Speaker/hearer centered frame
of reference. Ex: ?the ball to your left?.
2. Allocentric: Speaker independent. Ex: ?the
road to the north of the house?
Levinson further classifies allocentric frames of
reference into absolute, which includes the cardi-
nal directions, and intrinsic, which refers to a fea-
tured side of an object, such as ?the front of the
car?. Our spatial feature representation follows
this egocentric/allocentric distinction. The intrin-
sic frame of reference occurs rarely in the Map
Task corpus and is ignored, as speakers tend not
to mention features of the landmarks beyond their
names.
Regier (1996) studied the learning of spatial
language from static 2-D diagrams, learning to
distinguish between terms with a connectionist
model. He focused on the meaning of individual
terms, pairing a diagram with a given word. In
contrast, we learn from whole texts paired with a
1Not all languages exhibit all frames of reference. Terms
for ?up? and ?down? are exhibited in most all languages, while
?left? and ?right? are absent in some. Gravity breaks the sym-
metry between ?up? and ?down? but no such physical distinc-
tion exists for ?left? and ?right?, which contributes to the dif-
ficulty children have learning them.
807
path, which requires learning the correspondence
between text and world. We use similar geometric
features as Regier, capturing the allocentric frame
of reference.
Spatial semantics have also been explored in
physically grounded systems. Kuipers (2000) de-
veloped the Spatial Semantic Hierarchy, a knowl-
edge representation formalism for representing
different levels of granularity in spatial knowl-
edge. It combines sensory, metrical, and topolog-
ical information in a single framework. Kuipers
et al demonstrate its effectiveness on a physical
robot, but did not address the learning problem.
More generally, apprenticeship learning is well
studied in the reinforcement learning literature,
where the goal is to mimic the behavior of an ex-
pert in some decision making domain. Notable ex-
amples include (Abbeel and Ng, 2004), who train
a helicopter controller from pilot demonstration.
3 The Map Task Corpus
The HCRC Map Task Corpus (Anderson et al,
1991) is a set of dialogs between an instruction
giver and an instruction follower. Each participant
has a map with small named landmarks. Addition-
ally, the instruction giver has a path drawn on her
map, and must communicate this path to the in-
struction follower in natural language. Figure 1
shows a portion of the instruction giver?s map and
a sample of the instruction giver language which
describes part of the path.
The Map Task Corpus consists of 128 dialogs,
together with 16 different maps. The speech has
been transcribed and segmented into utterances,
based on the length of pauses. We restrict our
attention to just the utterances of the instruction
giver, ignoring the instruction follower. This is to
reduce redundancy and noise in the data - the in-
struction follower rarely introduces new informa-
tion, instead asking for clarification or giving con-
firmation. The landmarks on the instruction fol-
lower map sometimes differ in location from the
instruction giver?s. We ignore this caveat, giving
the system access to the instruction giver?s land-
marks, without the reference path.
Our task is to build an automated instruction
follower. Whereas the original participants could
speak freely, our system does not have the ability
to query the instruction giver and must instead rely
only on the previously recorded dialogs.
Figure 3: Sample state transition. Both actions get
credit for visiting the great rock after the indian
country. Action a1 also gets credit for passing the
great rock on the correct side.
4 Reinforcement Learning Formulation
We frame the direction following task as a sequen-
tial decision making problem. We interpret ut-
terances in order, where our interpretation is ex-
pressed by moving on the map. Our goal is to
construct a series of moves in the map which most
closely matches the expert path.
We define intermediate steps in our interpreta-
tion as states in a set S, and interpretive steps as
actions drawn from a set A. To measure the fi-
delity of our path with respect to the expert, we
define a reward function R : S ? A? R+ which
measures the utility of choosing a particular action
in a particular state. Executing action a in state s
carries us to a new state s?, and we denote this tran-
sition function by s? = T (s, a). All transitions are
deterministic in this paper.2
For training we are given a set of dialogs D.
Each dialog d ? D is segmented into utter-
ances (u1, . . . , um) and is paired with a map,
which is composed of a set of named landmarks
(l1, . . . , ln).
4.1 State
The states of our decision making problem com-
bine both our position in the dialog d and the path
we have taken so far on the map. A state s ? S is
composed of s = (ui, l, c), where l is the named
landmark we are located next to and c is a cardinal
direction drawn from {North,South,East,West}
which determines which side of l we are on.
Lastly, ui is the utterance in d we are currently
interpreting.
2Our learning algorithm is not dependent on a determin-
istic transition function and can be applied to domains with
stochastic transitions, such as robot locomotion.
808
4.2 Action
An action a ? A is composed of a named land-
mark l, the target of the action, together with a
cardinal direction c which determines which side
to pass l on. Additionally, a can be the null action,
with l = l? and c = c?. In this case, we interpret
an utterance without moving on the map. A target
l together with a cardinal direction c determine a
point on the map, which is a fixed distance from l
in the direction of c.
We make the assumption that at most one in-
struction occurs in a given utterance. This does not
always hold true - the instruction giver sometimes
chains commands together in a single utterance.
4.3 Transition
Executing action a = (l?, c?) in state s = (ui, l, c)
leads us to a new state s? = T (s, a). This tran-
sition moves us to the next utterance to interpret,
and moves our location to the target of the action.
If a is the null action, s = (ui+1, l, c), otherwise
s? = (ui+1, l?, c?). Figure 3 displays the state tran-
sitions two different actions.
To form a path through the map, we connect
these state waypoints with a path planner3 based
on A?, where the landmarks are obstacles. In a
physical system, this would be replaced with a
robot motion planner.
4.4 Reward
We define a reward function R(s, a) which mea-
sures the utility of executing action a in state s.
We wish to construct a route which follows the
expert path as closely as possible. We consider a
proposed route P close to the expert path Pe if P
visits landmarks in the same order as Pe, and also
passes them on the correct side.
For a given transition s = (ui, l, c), a = (l?, c?),
we have a binary feature indicating if the expert
path moves from l to l?. In Figure 3, both a1 and
a2 visit the next landmark in the correct order.
To measure if an action is to the correct side of
a landmark, we have another binary feature indi-
cating if Pe passes l? on side c. In Figure 3, only
a1 passes l? on the correct side.
In addition, we have a feature which counts the
number of words in ui which also occur in the
name of l?. This encourages us to choose poli-
cies which interpret language relevant to a given
3We used the Java Path Planning Library, available at
http://www.cs.cmu.edu/?ggordon/PathPlan/.
landmark.
Our reward function is a linear combination of
these features.
4.5 Policy
We formally define an interpretive strategy as a
policy pi : S ? A, a mapping from states to ac-
tions. Our goal is to find a policy pi which max-
imizes the expected reward Epi[R(s, pi(s))]. The
expected reward of following policy pi from state
s is referred to as the value of s, expressed as
V pi(s) = Epi[R(s, pi(s))] (1)
When comparing the utilities of executing an ac-
tion a in a state s, it is useful to define a function
Qpi(s, a) = R(s, a) + V pi(T (s, a))
= R(s, a) +Qpi(T (s, a), pi(s)) (2)
which measures the utility of executing a, and fol-
lowing the policy pi for the remainder. A given Q
function implicitly defines a policy pi by
pi(s) = max
a
Q(s, a). (3)
Basic reinforcement learning methods treat
states as atomic entities, in essence estimating V pi
as a table. However, at test time we are following
new directions for a map we haven?t previously
seen. Thus, we represent state/action pairs with a
feature vector ?(s, a) ? RK . We then represent
the Q function as a linear combination of the fea-
tures,
Q(s, a) = ?T?(s, a) (4)
and learn weights ? which most closely approxi-
mate the true expected reward.
4.6 Features
Our features ?(s, a) are a mixture of world and
linguistic information. The linguistic information
in our feature representation includes the instruc-
tion giver utterance and the names of landmarks
on the map. Additionally, we furnish our algo-
rithm with a list of English spatial terms, shown
in Table 1. Our feature set includes approximately
200 features. Learning exactly which words in-
fluence decision making is difficult; reinforcement
learning algorithms have problems with the large,
sparse feature vectors common in natural language
processing.
For a given state s = (u, l, c) and action a =
(l?, c?), our feature vector ?(s, a) is composed of
the following:
809
above, below, under, underneath, over, bottom,
top, up, down, left, right, north, south, east, west,
on
Table 1: The list of given spatial terms.
? Coherence: The number of wordsw ? u that
occur in the name of l?
? Landmark Locality: Binary feature indicat-
ing if l? is the closest landmark to l
? Direction Locality: Binary feature indicat-
ing if cardinal direction c? is the side of l?
closest to (l, c)
? Null Action: Binary feature indicating if l? =
NULL
? Allocentric Spatial: Binary feature which
conjoins the side c we pass the landmark on
with each spatial term w ? u. This allows us
to capture that the word above tends to indi-
cate passing to the north of the landmark.
? Egocentric Spatial: Binary feature which
conjoins the cardinal direction we move in
with each spatial term w ? u. For instance, if
(l, c) is above (l?, c?), the direction from our
current position is south. We conjoin this di-
rection with each spatial term, giving binary
features such as ?the word down appears in
the utterance and we move to the south?.
5 Approximate Dynamic Programming
Given this feature representation, our problem is
to find a parameter vector ? ? RK for which
Q(s, a) = ?T?(s, a) most closely approximates
E[R(s, a)]. To learn these weights ? we use
SARSA (Sutton and Barto, 1998), an online learn-
ing algorithm similar to Q-learning (Watkins and
Dayan, 1992).
Algorithm 1 details the learning algorithm,
which we follow here. We iterate over training
documents d ? D. In a given state st, we act ac-
cording to a probabilistic policy defined in terms
of the Q function. After every transition we up-
date ?, which changes how we act in subsequent
steps.
Exploration is a key issue in any RL algorithm.
If we act greedily with respect to our current Q
function, we might never visit states which are ac-
Input: Dialog set D
Reward function R
Feature function ?
Transition function T
Learning rate ?t
Output: Feature weights ?
1 Initialize ? to small random values
2 until ? converges do
3 foreach Dialog d ? D do
4 Initialize s0 = (l1, u1, ?),
a0 ? Pr(a0|s0; ?)
5 for t = 0; st non-terminal; t++ do
6 Act: st+1 = T (st, at)
7 Decide: at+1 ? Pr(at+1|st+1; ?)
8 Update:
9 ?? R(st, at) + ?T?(st+1, at+1)
10 ? ?T?(st, at)
11 ? ? ? + ?t?(st, at)?
12 end
13 end
14 end
15 return ?
Algorithm 1: The SARSA learning algorithm.
tually higher in value. We utilize Boltzmann ex-
ploration, for which
Pr(at|st; ?) =
exp( 1? ?
T?(st, at))
?
a? exp(
1
? ?
T?(st, a?))
(5)
The parameter ? is referred to as the tempera-
ture, with a higher temperature causing more ex-
ploration, and a lower temperature causing more
exploitation. In our experiments ? = 2.
Acting with this exploration policy, we iterate
through the training dialogs, updating our fea-
ture weights ? as we go. The update step looks
at two successive state transitions. Suppose we
are in state st, execute action at, receive reward
rt = R(st, at), transition to state st+1, and there
choose action at+1. The variables of interest are
(st, at, rt, st+1, at+1), which motivates the name
SARSA.
Our current estimate of the Q function is
Q(s, a) = ?T?(s, a). By the Bellman equation,
for the true Q function
Q(st, at) = R(st, at) + max
a?
Q(st+1, a
?) (6)
After each action, we want to move ? to minimize
the temporal difference,
R(st, at) +Q(st+1, at+1)?Q(st, at) (7)
810
Map 4g Map 10g
Figure 4: Sample output from the SARSA policy. The dashed black line is the reference path and the
solid red line is the path the system follows.
For each feature ?i(st, at), we change ?i propor-
tional to this temporal difference, tempered by a
learning rate ?t. We update ? according to
? = ?+?t?(st, at)(R(st, at)
+ ?T?(st+1, at+1)? ?
T?(st, at)) (8)
Here ?t is the learning rate, which decays over
time4. In our case, ?t = 1010+t , which was tuned on
the training set. We determine convergence of the
algorithm by examining the magnitude of updates
to ?. We stop the algorithm when
||?t+1 ? ?t||? <  (9)
6 Experimental Design
We evaluate our system on the Map Task corpus,
splitting the corpus into 96 training dialogs and 32
test dialogs. The whole corpus consists of approx-
imately 105,000 word tokens. The maps seen at
test time do not occur in the training set, but some
of the human participants are present in both.
4To guarantee convergence, we require
P
t ?t = ? andP
t ?
2
t < ?. Intuitively, the sum diverging guarantees we
can still learn arbitrarily far into the future, and the sum of
squares converging guarantees that our updates will converge
at some point.
6.1 Evaluation
We evaluate how closely the path P generated by
our system follows the expert path Pe. We mea-
sure this with respect to two metrics: the order
in which we visit landmarks and the side we pass
them on.
To determine the order Pe visits landmarks we
compute the minimum distance from Pe to each
landmark, and threshold it at a fixed value.
To score path P , we compare the order it visits
landmarks to the expert path. A transition l ? l?
which occurs in P counts as correct if the same
transition occurs in Pe. Let |P | be the number
of landmark transitions in a path P , and N the
number of correct transitions in P . We define the
order precision as N/|P |, and the order recall as
N/|Pe|.
We also evaluate how well we are at passing
landmarks on the correct side. We calculate the
distance of Pe to each side of the landmark, con-
sidering the path to visit a side of the landmark
if the distance is below a threshold. This means
that a path might be considered to visit multiple
sides of a landmark, although in practice it is usu-
811
Figure 5: This figure shows the relative weights of spatial features organized by spatial word. The top
row shows the weights of allocentric (landmark-centered) features. For example, the top left figure shows
that when the word above occurs, our policy prefers to go to the north of the target landmark. The bottom
row shows the weights of egocentric (absolute) spatial features. The bottom left figure shows that given
the word above, our policy prefers to move in a southerly cardinal direction.
ally one. If C is the number of landmarks we pass
on the correct side, define the side precision as
C/|P |, and the side recall as C/|Pe|.
6.2 Comparison Systems
The baseline policy simply visits the closest land-
mark at each step, taking the side of the landmark
which is closest. It pays no attention to the direc-
tion language.
We also compare against the policy gradient
learning algorithm of Branavan et al (2009). They
parametrize a probabilistic policy Pr(s|a; ?) as a
log-linear model, in a similar fashion to our explo-
ration policy. During training, the learning algo-
rithm adjusts the weights ? according to the gradi-
ent of the value function defined by this distribu-
tion.
Reinforcement learning algorithms can be clas-
sified into value based and policy based. Value
methods estimate a value function V for each
state, then act greedily with respect to it. Pol-
icy learning algorithms directly search through
the space of policies. SARSA is a value based
method, and the policy gradient algorithm is pol-
icy based.
Visit Order Side
P R F1 P R F1
Baseline 28.4 37.2 32.2 46.1 60.3 52.2
PG 31.1 43.9 36.4 49.5 69.9 57.9
SARSA 45.7 51.0 48.2 58.0 64.7 61.2
Table 2: Experimental results. Visit order shows
how well we follow the order in which the answer
path visits landmarks. ?Side? shows how success-
fully we pass on the correct side of landmarks.
7 Results
Table 2 details the quantitative performance of the
different algorithms. Both SARSA and the policy
gradient method outperform the baseline, but still
fall significantly short of expert performance. The
baseline policy performs surprisingly well, espe-
cially at selecting the correct side to visit a land-
mark.
The disparity between learning approaches and
gold standard performance can be attributed to
several factors. The language in this corpus is con-
versational, frequently ungrammatical, and con-
tains troublesome aspects of dialog such as con-
versational repairs and repetition. Secondly, our
action and feature space are relatively primitive,
and don?t capture the full range of spatial expres-
sion. Path descriptors, such as the difference be-
tween around and past are absent, and our feature
812
representation is relatively simple.
The SARSA learning algorithm accrues more
reward than the policy gradient algorithm. Like
most gradient based optimization methods, policy
gradient algorithms oftentimes get stuck in local
maxima, and are sensitive to the initial conditions.
Furthermore, as the size of the feature vectorK in-
creases, the space becomes even more difficult to
search. There are no guarantees that SARSA has
reached the best policy under our feature space,
and this is difficult to determine empirically. Thus,
some accuracy might be gained by considering
different RL algorithms.
8 Discussion
Examining the feature weights ? sheds some light
on our performance. Figure 5 shows the relative
strength of weights for several spatial terms. Re-
call that the two main classes of spatial features in
? are egocentric (what direction we move in) and
allocentric (on which side we pass a landmark),
combined with each spatial word.
Allocentric terms such as above and below tend
to be interpreted as going to the north and south
of landmarks, respectively. Interestingly, our sys-
tem tends to move in the opposite cardinal direc-
tion, i.e. the agent moves south in the egocen-
tric frame of reference. This suggests that people
use above when we are already above a landmark.
South slightly favors passing on the south side of
landmarks, and has a heavy tendency to move in
a southerly direction. This suggests that south is
used more frequently in an egocentric reference
frame.
Our system has difficulty learning the meaning
of right. Right is often used as a conversational
filler, and also for dialog alignment, such as
?right okay right go vertically up then
between the springboks and the highest
viewpoint.?
Furthermore, right can be used in both an egocen-
tric or allocentric reference frame. Compare
?go to the uh right of the mine?
which utilizes an allocentric frame, with
?right then go eh uh to your right hori-
zontally?
which uses an egocentric frame of reference. It
is difficult to distinguish between these meanings
without syntactic features.
9 Conclusion
We presented a reinforcement learning system
which learns to interpret natural language direc-
tions. Critically, our approach uses no semantic
annotation, instead learning directly from human
demonstration. It successfully acquires a subset
of spatial semantics, using reinforcement learning
to derive the correspondence between instruction
language and features of paths. While our results
are still preliminary, we believe our model repre-
sents a significant advance in learning natural lan-
guage meaning, drawing its supervision from hu-
man demonstration rather than word distributions
or hand-labeled semantic tags. Framing language
acquisition as apprenticeship learning is a fruitful
research direction which has the potential to con-
nect the symbolic, linguistic domain to the non-
symbolic, sensory aspects of cognition.
Acknowledgments
This research was partially supported by the Na-
tional Science Foundation via a Graduate Re-
search Fellowship to the first author and award
IIS-0811974 to the second author and by the Air
Force Research Laboratory (AFRL), under prime
contract no. FA8750-09-C-0181. Thanks to
Michael Levit and Deb Roy for providing digital
representations of the maps and a subset of the cor-
pus annotated with their spatial representation.
References
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the Twenty-first International Con-
ference on Machine Learning. ACM Press.
A. Anderson, M. Bader, E. Bard, E. Boyle, G. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. Mcallister,
J. Miller, C. Sotillo, H. Thompson, and R. Weinert.
1991. The HCRC map task corpus. Language and
Speech, 34, pages 351?366.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In ACL-IJCNLP
?09.
James Richard Curran. 2003. From Distributional to
Semantic Similarity. Ph.D. thesis, University of Ed-
inburgh.
Charles Fillmore. 1997. Lectures on Deixis. Stanford:
CSLI Publications.
Benjamin Kuipers. 2000. The spatial semantic hierar-
chy. Artificial Intelligence, 119(1-2):191?233.
813
Stephen Levinson. 2003. Space In Language And
Cognition: Explorations In Cognitive Diversity.
Cambridge University Press.
Michael Levit and Deb Roy. 2007. Interpretation
of spatial language in a map navigation task. In
IEEE Transactions on Systems, Man, and Cybernet-
ics, Part B, 37(3), pages 667?679.
Terry Regier. 1996. The Human Semantic Potential:
Spatial Language and Constrained Connectionism.
The MIT Press.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
Leonard Talmy. 1983. How language structures space.
In Spatial Orientation: Theory, Research, and Ap-
plication.
Christine Tanz. 1980. Studies in the acquisition of de-
ictic terms. Cambridge University Press.
Hongmei Wang, Alan M. Maceachren, and Guoray
Cai. 2004. Design of human-GIS dialogue for com-
munication of vague spatial concepts. In GIScience.
C. J. C. H. Watkins and P. Dayan. 1992. Q-learning.
Machine Learning, pages 8:279?292.
Yuan Wei, Emma Brunskill, Thomas Kollar, and
Nicholas Roy. 2009. Where to go: interpreting nat-
ural directions using global inference. In ICRA?09:
Proceedings of the 2009 IEEE international con-
ference on Robotics and Automation, pages 3761?
3767, Piscataway, NJ, USA. IEEE Press.
Luke S. Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In ACL-IJCNLP ?09, pages
976?984.
814
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 74?80,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs
Adam Vogel, Christopher Potts, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{acvogel,cgpotts,jurafsky}@stanford.edu
Abstract
Conversational implicatures involve rea-
soning about multiply nested belief struc-
tures. This complexity poses significant
challenges for computational models of
conversation and cognition. We show that
agents in the multi-agent Decentralized-
POMDP reach implicature-rich interpreta-
tions simply as a by-product of the way
they reason about each other to maxi-
mize joint utility. Our simulations involve
a reference game of the sort studied in
psychology and linguistics as well as a
dynamic, interactional scenario involving
implemented artificial agents.
1 Introduction
Gricean conversational implicatures (Grice, 1975)
are inferences that listeners make in order to
reconcile the speaker?s linguistic behavior with
the assumption that the speaker is cooperative.
As Grice conceived of them, implicatures cru-
cially involve reasoning about multiply-nested be-
lief structures: roughly, for p to count as an impli-
cature, the speaker must believe that the listener
will infer that the speaker believes p. This com-
plexity makes implicatures an important testing
ground for models of conversation and cognition.
Implicatures have received considerable atten-
tion in the context of simple reference games in
which the listener uses the speaker?s utterance
to try to identify the speaker?s intended referent
(Rosenberg and Cohen, 1964; Clark and Wilkes-
Gibbs, 1986; Dale and Reiter, 1995; DeVault and
Stone, 2007; Krahmer and van Deemter, 2012).
Many implicature patterns can be embedded in
these games using specific combinations of poten-
tial referents and message sets. The paradigm has
proven fruitful not only for evaluating computa-
tional models (Golland et al, 2010; Degen and
Franke, 2012; Frank and Goodman, 2012; Rohde
et al, 2012; Bergen et al, 2012) but also for study-
ing children?s pragmatic abilities without implic-
itly assuming they have mastered challenging lin-
guistic structures (Stiller et al, 2011).
In this paper, we extend these results beyond
simple reference games to full decision-problems
in which the agents reason about language and ac-
tion together over time. To do this, we use the De-
centralized Partially Observable Markov Decision
Process (Dec-POMDP) to implement agents that
are capable of manipulating the multiply-nested
belief structures required for implicature calcula-
tion. Optimal decision making in Dec-POMDPs
is NEXP complete, so we employ the single-agent
POMDP approximation of Vogel et al (2013).
We show that agents in the Dec-POMDP reach
implicature-rich interpretations simply as a by-
product of the way they reason about each other
to maximize joint utility. Our simulations involve
a reference game and a dynamic, interactional sce-
nario involving implemented artificial agents.
2 Decision-Theoretic Communication
The Decentralized Partially Observable Markov
Decision Process (Dec-POMDP) (Bernstein et
al., 2002) is a multi-agent generalization of the
POMDP, where agents act to maximize a shared
utility function. Formally, a Dec-POMDP con-
sists of a tuple (S,A,O,R, T,?, b0, ?). S is a
finite set of states, A is the set of actions, O is
the set of observations, and T (s?|a1, a2, s) is the
transition distribution which determines what ef-
fect the joint action (a1, a2) has on the state of the
world. The true state s ? S is not observable to
the agents, who must utilize observations o ? O,
which are emitted after each action according to
the observation distribution ?(o1, o2|s?, a). The
reward functionR(s, a1, a2) represents the goal of
the agents, who act to maximize expected reward.
Lastly, b0 ? ?(S) is the initial belief state and
74
? ? [0, 1) is the discount factor.
The true state of the world s ? S is not ob-
servable to either agent. In single-agent POMDPs,
agents maintain a belief state b(s) ? ?(S), which
is a distribution over states. Agents acting in Dec-
POMDPs must take into account not only their
beliefs about the state of the world, but also the
beliefs of their partners, leading to nested belief
states. In the model presented here, our agent
models the other agent?s beliefs about the state of
the world, and assumes that the other agent does
not take into account our own beliefs, a common
approach (Gmytrasiewicz and Doshi, 2005).
Agents make decisions according to
a policy pii : ?(S) ? A which max-
imizes the discounted expected reward??
t=0 ?tE[R(st, at1, at2)|b0, pi1, pi2]. Using
the assumption that the other agent tracks one less
level of belief, we can solve for the other agent?s
policy p?i, which allows us to estimate his actions
and beliefs over time. To construct policies,
we use Perseus (Spaan and Vlassis, 2005), a
point-based value iteration algorithm.
Even tracking just one level of nested beliefs
quickly leads to a combinatorial explosion in the
number of belief states the other agent might have.
This causes decision making in Dec-POMDPs to
be NEXP complete, limiting their application to
problems with only a handful of states (Bernstein
et al, 2002). To ameliorate this difficulty, we
use the method of Vogel et al (2013), which cre-
ates a single-agent approximation to the full Dec-
POMDP. To form this single-agent POMDP, we
augment the state space to be S ? S, where the
second set of state variables allows us to model
the other agent?s beliefs. We maintain a point
estimate b? of the other agent?s beliefs, which
is formed by summing out observations O that
the other player might have received. To ac-
complish this, we factor the transition distribu-
tion into two terms: T ((s?, s??)|a, p?i(s?), (s, s?)) =
T? (s??|s?, a, p?i(s?), (s, s?))T (s?|a, p?i(s?), (s, s?)). This
observation marginalization can be folded into the
transition distribution T? (s??|s?, a, p?i(s?), (s, s?)):
T? (s??| s?, a, p?i(s?), (s, s?)) = Pr(s??|s?, a, p?i(s?), (s, s?))
=
?
o??O
( ?(o?|s??, a, p?i(s?))T (s??|a, p?i(s?), s?)?
s??? ?(o?|s???, a, p?i(s?))T (s???|a, p?i(s?), s?)
? ?(o?|s?, a, p?i(s?))
)
(1)
Communication is treated as another type of ob-
servation, with messages coming from a finite set
M . Each message m ? M has the semantics
Pr(s|m), which represents the probability that the
world is in state s ? S given that m is true. Mes-
sages m received from a partner are combined
with perceptual observations o ? O, to form a
joint observation (m, o).
A literal listener, denoted L, interprets mes-
sages according to this semantics, without taking
into account the beliefs of the speaker. L assumes
that the perceptual observations and messages are
conditionally independent given the state of the
world. Using Bayes? rule, the literal listener?s joint
observation/message distribution is
Pr((o,m)|s, s?, a) = ?(o|s?, a) Pr(m|s)
= ?(o|s?, a) Pr(s|m) Pr(m)?
m??M Pr(s|m?) Pr(m?)
(2)
The Pr(m) prior over messages can be estimated
from corpus data, but we use a uniform prior for
simplicity.
A literal speaker, denoted S, produces mes-
sages according to the most descriptive term:
piS(s) = arg max
m?M
p(s|m). (3)
The literal speaker does not model the beliefs of
the listener.
To interpret implicatures, a level-one lis-
tener, denoted L(S), models the beliefs a literal
speaker must have had to produce an utterance:
Pr(m|s) = 1[p?iS(s) = m], where p?iS is the level-
one listener?s estimate of the speaker?s policy. In
this setting, we denote the level-one listener?s es-
timate of the speaker?s belief as s?, yielding the be-
lief update equation
Pr((o,m)|(s, s?), (s?, s??), a, p?iS(s?)) =
?(o|s?, a)1[p?iS(s?) = m] (4)
The literal semantics of messages is not explicitly
included in the level-one listener?s belief update.
Instead, when he solves for the literal speaker?s
policy p?iS , the meaning of a message is the set of
beliefs that would lead the literal speaker to pro-
duce the utterance.
A level-one speaker, S(L), produces utterances
to influence a literal listener, and a level-two lis-
tener, L(S(L)), uses two levels of belief nesting to
interpret utterances as the beliefs that a level-one
speaker might have to produce that utterance. At
each level of nesting, we apply the marginalized
75
r1 0 0 1
r2 0 1 1
r3 1 1 0
hat
glasses
mustache
r1 r2 r3
(a) Scenario.
Message r1 r2 r3
moustache 12 12 0
glasses 0 12 12
hat 0 0 1
(b) Literal interpretations.
Message r1 r2 r3
moustache 1 0 0
glasses 0 1 0
hat 0 0 1
(c) Implicature-rich interpretations.
Figure 1: A simple reference game. The matrices
give distributions Pr(t = ri|utterance)
belief-state approach of (Vogel et al, 2013), aug-
menting the state space with another copy of the
underlying world state space, where the new copy
represents the next level of belief. For instance, the
L(S(L)) agent will make decisions in the S?S?S
space. For an L(S(L)) state (s, s?, s?), s is the true
state of the world, s? is the speaker?s belief of the
state of the world, and s? is the speaker?s belief of
the listener?s beliefs. In the next two sections we
show how a level-one and level-two listener infer
implicatures.
3 Reference Game Implicatures
Fig. 1a is the scenario for a reference game of the
sort pioneered by Rosenberg and Cohen (1964)
and Dale and Reiter (1995). The potential refer-
ents are r1, r2, and r3. Speakers use a restricted
vocabulary consisting of three messages: ?mous-
tache?, ?glasses?, and ?hat?. The speaker is as-
signed a referent ri (hidden from the listener) and
produces a message on that basis. The speaker and
listener share the goal of having the listener iden-
tify the speaker?s intended referent ri.
Fig. 1b depicts the literal interpretations for
this game. It looks like the listener?s chances
of success are low. Only ?hat? refers unambigu-
ously. However, the language and scenario fa-
cilitate scalar implicature (Horn, 1972; Harnish,
1979; Gazdar, 1979). Briefly, the scalar implica-
ture pattern is that a speaker who is knowledgeable
about the relevant domain will choose a commu-
nicatively weak utterance U over a communica-
tively stronger utterance U ? iff U ? is false (assum-
ing U and U ? are relevant). The required sense of
communicative strength encompasses logical en-
tailments as well as more particularized pragmatic
partial orders (Hirschberg, 1985).
In our scenario, ?hat? is stronger than ?glasses?:
the referents wearing a hat are a proper subset
of those wearing glasses. Thus, given the play-
ers? goal, if the speaker says ?glasses?, the lis-
tener should draw the scalar implicature that ?hat?
is false. Thus, ?glasses? comes to unambiguously
refer to r2 (Fig. 1c, line 2). Similarly, though
?moustache? and ?glasses? do not literally stand in
the specific?general relationship needed for scalar
implicature, they do with ?glasses? pragmatically
associated with r2 (Fig. 1c, line 1).
Our implementation of these games as Dec-
POMDPs mirrors their intuitive description and
their treatment in iterated best response models
(Ja?ger, 2007; Ja?ger, 2012; Franke, 2009; Frank
and Goodman, 2012). The state space S encodes
the attributes of the referents (e.g., hat(r2) = T,
glasses(r1) = F) and includes a target variable t
identifying the speaker?s referent (hidden from the
listener). The speaker has three speech actions,
identified with the three messages. The listener
has four actions: ?listen? plus a ?choose? action ci
for each referent ri. The set of observations O is
just the set of messages (construed as utterances).
The agents receive a positive reward iff the listener
action ci corresponds to the speaker?s target t. Be-
cause this is a one-step reference game, the transi-
tion distribution T is the identity distribution.
The literal listener L interprets utterances as
a truth-conditional speaker would produce them
(Fig. 1b). The level-one speaker S(L) augments
the state space with a variable ?listener target? and
models L?s beliefs b? using the approximate meth-
ods of Sec. 2. Crucially, the optimal speaker pol-
icy piS(L) is such that piS(L)(t=r3) = ?hat? and
piS(L)(t=r1) = ?moustache?. The level-two lis-
tener L(S(L)) models S(L) via an estimate of the
?listener target? variable. For each speech action
m, L(S(L)) considers all values of t and the likeli-
76
hood that S(L) would have produced m:
Pr(t=ri|m) ? 1[p?iS(L)(t=ri) = m]
Since S(L) uses ?hat? to describe r3 and
?moustache? to describe r1, L(S(L)) correctly in-
fers that ?glasses? refers to r2, completing Fig. 1c?s
full implicature-rich pattern of mutual exclusivity
(Clark, 1987; Frank et al, 2009).
This basic pattern is robustly attested empiri-
cally in human data. The experimental data are,
of course, invariably less crisp than our idealized
model predicts, but many important sources of
variation could be brought into our model, with
the addition of strong salience priors (Frank and
Goodman, 2012; Stiller et al, 2011), assumptions
about bounded rationality (Camerer et al, 2004;
Franke, 2009), and a ?soft-max? view of the lis-
tener (Frank et al, 2009).
4 Cards World Implicatures
The Cards corpus1 contains 1266 metadata-rich
transcripts from a two-player chat-based game.
The world is a simple maze in which a deck of
cards has been distributed. The players? goal is to
find specific subsets of the cards, subject to a vari-
ety of constraints on what they can see and do. The
Dec-POMDP-based agents of Vogel et al (2013)
play a simplified version in which the goal is to be
co-located with a single card. Vogel et al show
that their agents? linguistic behavior is broadly
Gricean. However, their agents? language is too
simple to reveal implicatures. The present section
remedies this shortcoming. Implicature-rich inter-
pretations are an immediate consequence.
We implement the simplified Cards tasks as fol-
lows. The state space S is composed of the loca-
tion of each player and the location of the card.
The transition distribution T (s?|s, a1, a2) encodes
the outcome of movement actions. Agents receive
one of two sensor observations, indicating whether
the card is at their current location. The players are
rewarded when they are both located on the card.
Each player begins knowing his own location, but
not the location of the other player nor of the card.
The players have four movement actions (?up?,
?down?, ?left?, ?right?) and nine speech actions in-
terpreted as identifying card locations. Fig. 2 de-
picts these utterances as a partial order determined
by entailment. These general-to-specific relation-
1http://cardscorpus.christopherpotts.net
top right top left bottom right bottom left
top right left bottom middle
,,
,,
\\
\\
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 499?504,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generating Recommendation Dialogs by Extracting Information from
User Reviews
Kevin Reschke, Adam Vogel, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{kreschke,acvogel,jurafsky}@stanford.edu
Abstract
Recommendation dialog systems help
users navigate e-commerce listings by ask-
ing questions about users? preferences to-
ward relevant domain attributes. We
present a framework for generating and
ranking fine-grained, highly relevant ques-
tions from user-generated reviews. We
demonstrate our approach on a new dataset
just released by Yelp, and release a new
sentiment lexicon with 1329 adjectives for
the restaurant domain.
1 Introduction
Recommendation dialog systems have been devel-
oped for a number of tasks ranging from product
search to restaurant recommendation (Chai et al,
2002; Thompson et al, 2004; Bridge et al, 2005;
Young et al, 2010). These systems learn user re-
quirements through spoken or text-based dialog,
asking questions about particular attributes to fil-
ter the space of relevant documents.
Traditionally, these systems draw questions
from a small, fixed set of attributes, such as cuisine
or price in the restaurant domain. However, these
systems overlook an important element in users?
interactions with online product listings: user-
generated reviews. Huang et al (2012) show that
information extracted from user reviews greatly
improves user experience in visual search inter-
faces. In this paper, we present a dialog-based in-
terface that takes advantage of review texts. We
demonstrate our system on a new challenge cor-
pus of 11,537 businesses and 229,907 user reviews
released by the popular review website Yelp1, fo-
cusing on the dataset?s 4724 restaurants and bars
(164,106 reviews).
This paper makes two main contributions. First,
we describe and qualitatively evaluate a frame-
1https://www.yelp.com/dataset_challenge/
work for generating new, highly-relevant ques-
tions from user review texts. The framework
makes use of techniques from topic modeling and
sentiment-based aspect extraction to identify fine-
grained attributes for each business. These at-
tributes form the basis of a new set of questions
that the system can ask the user.
Second, we use a method based on information-
gain for dynamically ranking candidate questions
during dialog production. This allows our system
to select the most informative question at each di-
alog step. An evaluation based on simulated di-
alogs shows that both the ranking method and the
automatically generated questions improve recall.
2 Generating Questions from Reviews
2.1 Subcategory Questions
Yelp provides each business with category labels
for top-level cuisine types like Japanese, Coffee
& Tea, and Vegetarian. Many of these top-level
categories have natural subcategories (e.g., ramen
vs. sushi). By identifying these subcategories, we
enable questions which probe one step deeper than
the top-level category label.
To identify these subcategories, we run Latent
Dirichlet Analysis (LDA) (Blei et al, 2003) on
the reviews of each set of businesses in the twenty
most common top-level categories, using 10 top-
ics and concatenating all of a business?s reviews
into one document.2 Several researchers have used
sentence-level documents to model topics in re-
views, but these tend to generate topics about fine-
grained aspects of the sort we discuss in Section
2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010).
We then manually labeled the topics, discarding
junk topics and merging similar topics. Table 1
displays sample extracted subcategories.
Using these topic models, we assign a business
2We use the Topic Modeling Toolkit implementation:
http://nlp.stanford.edu/software/tmt
499
Category Topic Label Top Words
pizza crust sauce pizza garlic sausage slice salad
Italian traditional pasta sauce delicious ravioli veal dishes gnocchi
bistro bruschetta patio salad valet delicious brie panini
deli sandwich deli salad pasta delicious grocery meatball
brew pub beers peaks ale brewery patio ipa brew
grill steak salad delicious sliders ribs tots drinks
bar drinks vig bartender patio uptown dive karaoke
American (New) bistro drinks pretzel salad fondue patio sanwich windsor
brunch sandwich brunch salad delicious pancakes patio
burger burger fries sauce beef potato sandwich delicious
mediterranean pita hummus jungle salad delicious mediterranean wrap
italian deli sandwich meats cannoli cheeses authentic sausage
new york deli beef sandwich pastrami corned fries waitress
Delis bagels bagel sandwiches toasted lox delicious donuts yummy
mediterranean pita lemonade falafel hummus delicious salad bakery
sandwiches sandwich subs sauce beef tasty meats delicious
sushi sushi kyoto zen rolls tuna sashimi spicy
Japanese teppanyaki sapporo chef teppanyaki sushi drinks shrimp fried
teriyaki teriyaki sauce beef bowls veggies spicy grill
ramen noodles udon dishes blossom delicious soup ramen
Table 1: A sample of subcategory topics with hand-labels and top words.
to a subcategory based on the topic with high-
est probability in that business?s topic distribution.
Finally, we use these subcategory topics to gen-
erate questions for our recommender dialog sys-
tem. Each top-level category corresponds to a sin-
gle question whose potential answers are the set of
subcategories: e.g., ?What type of Japanese cui-
sine do you want??
2.2 Questions from Fine-Grained Aspects
Our second source for questions is based on as-
pect extraction in sentiment summarization (Blair-
Goldensohn et al, 2008; Brody and Elhadad,
2010). We define an aspect as any noun-phrase
which is targeted by a sentiment predicate. For
example, from the sentence ?The place had great
atmosphere, but the service was slow.? we ex-
tract two aspects: +atmosphere and ?service.
Our aspect extraction system has two steps.
First we develop a domain specific sentiment lex-
icon. Second, we apply syntactic patterns to iden-
tify NPs targeted by these sentiment predicates.
2.2.1 Sentiment Lexicon
Coordination Graph We generate a list of
domain-specific sentiment adjectives using graph
propagation. We begin with a seed set combin-
ing PARADIGM+ (Jo and Oh, 2011) with ?strongly
subjective? adjectives from the OpinionFinder lex-
icon (Wilson et al, 2005), yielding 1342 seeds.
Like Brody and Elhadad (2010), we then construct
a coordination graph that links adjectives modify-
ing the same noun, but to increase precision we
require that the adjectives also be conjoined by
and (Hatzivassiloglou and McKeown, 1997). This
reduces problems like propagating positive sen-
timent to orange in good orange chicken. We
marked adjectives that follow too or lie in the
scope of negation with special prefixes and treated
them as distinct lexical entries.
Sentiment Propagation Negative and positive
seeds are assigned values of 0 and 1 respectively.
All other adjectives begin at 0.5. Then a stan-
dard propagation update is computed iteratively
(see Eq. 3 of Brody and Elhadad (2010)).
In Brody and Elhadad?s implementation of this
propagation method, seed sentiment values are
fixed, and the update step is repeated until the non-
seed values converge. We found that three modifi-
cations significantly improved precision. First, we
omit candidate nodes that don?t link to at least two
positive or two negative seeds. This eliminated
spurious propagation caused by one-off parsing er-
rors. Second, we run the propagation algorithm for
fewer iterations (two iterations for negative terms
and one for positive terms). We found that addi-
tional iterations led to significant error propaga-
tion when neutral (italian) or ambiguous (thick)
terms were assigned sentiment.3 Third, we update
both non-seed and seed adjectives. This allows us
to learn, for example, that the negative seed deca-
dent is positive in the restaurant domain.
Table 2 shows a sample of sentiment adjectives
3Our results are consistent with the recent finding of Whit-
ney and Sarkar (2012) that cautious systems are better when
bootstrapping from seeds.
500
Negative Sentiment
institutional, underwhelming, not nice, burn-
tish, unidentifiable, inefficient, not attentive,
grotesque, confused, trashy, insufferable,
grandiose, not pleasant, timid, degrading,
laughable, under-seasoned, dismayed, torn
Positive Sentiment
decadent, satisfied, lovely, stupendous,
sizable, nutritious, intense, peaceful,
not expensive, elegant, rustic, fast, affordable,
efficient, congenial, rich, not too heavy,
wholesome, bustling, lush
Table 2: Sample of Learned Sentiment Adjectives
derived by this graph propagation method. The
final lexicon has 1329 adjectives4, including 853
terms not in the original seed set. The lexicon is
available for download.5
Evaluative Verbs In addition to this adjective
lexicon, we take 56 evaluative verbs such as love
and hate from admire-class VerbNet predicates
(Kipper-Schuler, 2005).
2.2.2 Extraction Patterns
To identify noun-phrases which are targeted by
predicates in our sentiment lexicon, we develop
hand-crafted extraction patterns defined over syn-
tactic dependency parses (Blair-Goldensohn et al,
2008; Somasundaran and Wiebe, 2009) generated
by the Stanford parser (Klein and Manning, 2003).
Table 3 shows a sample of the aspects generated by
these methods.
Adj + NP It is common practice to extract any
NP modified by a sentiment adjective. However,
this simple extraction rule suffers from precision
problems. First, reviews often contain sentiment
toward irrelevant, non-business targets (Wayne is
the target of excellent job in (1)). Second, hypo-
thetical contexts lead to spurious extractions. In
(2), the extraction +service is clearly wrong?in
fact, the opposite sentiment is being expressed.
(1) Wayne did an excellent job addressing our
needs and giving us our options.
(2) Nice and airy atmosphere, but service could be
more attentive at times.
4We manually removed 26 spurious terms which were
caused by parsing errors or propagation to a neutral term.
5http://nlp.stanford.edu/projects/
yelp.shtml
We address these problems by filtering out sen-
tences in hypothetical contexts cued by if, should,
could, or a question mark, and by adopting the fol-
lowing, more conservative extractions rules:
i) [BIZ + have + adj. + NP] Sentiment adjec-
tive modifies NP, main verb is have, subject
is business name, it, they, place, or absent.
(E.g., This place has some really great yogurt
and toppings).
ii) [NP + be + adj.] Sentiment adjective linked
to NP by be?e.g., Our pizza was much too
jalapeno-y.
?Good For? + NP Next, we extract aspects us-
ing the pattern BIZ + positive adj. + for + NP, as in
It?s perfect for a date night. Examples of extracted
aspects include +lunch, +large groups, +drinks,
and +quick lunch.
Verb + NP Finally, we extract NPs that appear
as direct object to one of our evaluative verbs (e.g.,
We loved the fried chicken).
2.2.3 Aspects as Questions
We generate questions from these extracted as-
pects using simple templates. For example, the as-
pect +burritos yields the question: Do you want a
place with good burritos?
3 Question Selection for Dialog
To utilize the questions generated from reviews in
recommendation dialogs, we first formalize the di-
alog optimization task and then offer a solution.
3.1 Problem Statement
We consider a version of the Information Retrieval
Dialog task introduced by Kopec?ek (1999). Busi-
nesses b ? B have associated attributes, coming
from a set Att. These attributes are a combination
of Yelp categories and our automatically extracted
aspects described in Section 2. Attributes att ? Att
take values in a finite domain dom(att). We denote
the subset of businesses with an attribute att tak-
ing value val ? dom(att), as B|att=val. Attributes
are functions from businesses to subsets of values:
att : B ? P(dom(att)). We model a user in-
formation need I as a set of attribute/value pairs:
I = {(att1, val1), . . . , (att|I|, val|I|)}.
Given a set of businesses and attributes, a rec-
ommendation agent pi selects an attribute to ask
501
Chinese: Mexican:
+beef +egg roll +sour soup +orange chicken +salsa bar +burritos +fish tacos +guacamole
+noodles +crab puff +egg drop soup +enchiladas +hot sauce +carne asade +breakfast burritos
+dim sum +fried rice +honey chicken +horchata +green salsa +tortillas +quesadillas
Japanese: American (New)
+rolls +sushi rolls +wasabi +sushi bar +salmon +environment +drink menu +bar area +cocktails +brunch
+chicken katsu +crunch +green tea +sake selection +hummus +mac and cheese +outdoor patio +seating area
+oysters +drink menu +sushi selection +quality +lighting +brews +sangria +cheese plates
Table 3: Sample of the most frequent positive aspects extracted from review texts.
Input: Information need I
Set of businesses B
Set of attributes Att
Recommendation agent pi
Dialog length K
Output: Dialog history H
Recommended businesses B
Initialize dialog history H = ?
for step = 0; step < K; step++ do
Select an attribute: att = pi(B,H)
Query user for the answer: val = I(att)
Restrict set of businesses: B = B|att=val
Append answer: H = H ? {(att, val)}
end
Return (H,B)
Algorithm 1: Procedure for evaluating a recom-
mendation agent
the user about, then uses the answer value to nar-
row the set of businesses to those with the de-
sired attribute value, and selects another query.
Algorithm 1 presents this process more formally.
The recommendation agent can use both the set of
businesses B and the history of question and an-
swers H from the user to select the next query.
Thus, formally a recommendation agent is a func-
tion pi : B ? H ? Att. The dialog ends after a
fixed number of queries K.
3.2 Information Gain Agent
The information gain recommendation agent
chooses questions to ask the user by selecting
question attributes that maximize the entropy of
the resulting document set, in a manner similar to
decision tree learning (Mitchell, 1997). Formally,
we define a function infogain : Att? P(B)? R:
infogain(att, B) =
?
?
vals?P(dom(att))
|Batt=vals|
|B| log
|Batt=vals|
|B|
The agent then selects questions att ? Att that
maximize the information gain with respect to the
set of businesses satisfying the dialog history H:
pi(B,H) = argmax
att?Att
infogain(att, B|H)
4 Evaluation
4.1 Experimental Setup
We follow the standard approach of using the at-
tributes of an individual business as a simulation
of a user?s preferences (Chung, 2004; Young et al,
2010). For every business b ? B we form an in-
formation need composed of all of b?s attributes:
Ib =
?
{att?Att|att(b)6=?}
(att, att(b))
To evaluate a recommendation agent, we use
the recall metric, which measures how well an in-
formation need is satisfied. For each information
need I , let BI be the set of businesses that satisfy
the questions of an agent. We define the recall of
the set of businesses with respect to the informa-
tion need as
recall(BI , I) =
?
b?BI
?
(att,val)?I 1[val ? att(b)]
|BI ||I|
We average recall across all information needs,
yielding average recall.
We compare against a random agent baseline
that selects attributes att ? Att uniformly at ran-
dom at each time step. Other recommendation di-
alog systems such as Young et al (2010) select
questions from a small fixed hierarchy, which is
not applicable to our large set of attributes.
4.2 Results
Figure 1 shows the average recall for the ran-
dom agent versus the information gain agent with
varying sets of attributes. ?Top-level? repeatedly
queries the user?s top-level category preferences,
?Subtopic? additionally uses our topic modeling
subcategories, and ?All? uses these plus the as-
pects extracted from reviews. We see that for suf-
ficiently long dialogs, ?All? outperforms the other
systems. The ?Subtopic? and ?Top-level? systems
plateau after a few dialog steps once they?ve asked
502
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10
Ave
rag
e R
eca
ll
Dialog Length
Average Recall by Agent
RandomTop-levelSubtopicAll
Figure 1: Average recall for each agent.
all useful questions. For instance, most businesses
only have one or two top-level categories, so af-
ter the system has identified the top-level cate-
gory that the user is interested in, it has no more
good questions to ask. Note that the information
gain agent starts dialogs with the top-level and ap-
propriate subcategory questions, so it is only for
longer dialogs that the fine-grained aspects boost
performance.
Below we show a few sample output dialogs
from our ?All? information gain agent.
Q: What kind of place do you want?
A: American (New)
Q: What kind of American (New) do you want:
bar, bistro, standard, burgers, brew pub, or
brunch?
A: bistro
Q: Do you want a place with a good patio?
A: Yes
Q: What kind of place do you want?
A: Chinese
Q: What kind of Chinese place do you want:
buffet, dim sum, noodles, pan Asian, Panda
Express, sit down, or veggie?
A: sit down
Q: Do you want a place with a good lunch
special?
A: Yes
Q: What kind of place do you want?
A: Mexican
Q: What kind of Mexican place do you want:
dinner, taqueria, margarita bar, or tortas?
A: Margarita bar
Q: Do you want a place with a good patio?
A: Yes
5 Conclusion
We presented a system for extracting large sets
of attributes from user reviews and selecting rel-
evant attributes to ask questions about. Using
topic models to discover subtypes of businesses, a
domain-specific sentiment lexicon, and a number
of new techniques for increasing precision in sen-
timent aspect extraction yields attributes that give
a rich representation of the restaurant domain. We
have made this 1329-term sentiment lexicon for
the restaurant domain available as useful resource
to the community. Our information gain recom-
mendation agent gives a principled way to dynam-
ically combine these diverse attributes to ask rele-
vant questions in a coherent dialog. Our approach
thus offers a new way to integrate the advantages
of the curated hand-build attributes used in statisti-
cal slot and filler dialog systems, and the distribu-
tionally induced, highly relevant categories built
by sentiment aspect extraction systems.
6 Acknowledgments
Thanks to the anonymous reviewers and the Stan-
ford NLP group for helpful suggestions. The au-
thors also gratefully acknowledge the support of
the Nuance Foundation, the Defense Advanced
Research Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-13-2-0040, ONR grants
N00014-10-1-0109 and N00014-13-1-0287 and
ARO grant W911NF-07-1-0216, and the Center
for Advanced Study in the Behavioral Sciences.
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for local
service reviews. In WWW Workshop on NLP in the
Information Explosion Era.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. The Journal of
Machine Learning Research, 3:993?1022.
Derek Bridge, Mehmet H. Go?ker, Lorraine McGinty,
and Barry Smyth. 2005. Case-based recom-
mender systems. Knowledge Engineering Review,
20(3):315?320.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
503
In Proceedings of HLT NAACL 2010, pages 804?
812.
Joyce Chai, Veronika Horvath, Nicolas Nicolov, Margo
Stys, A Kambhatla, Wlodek Zadrozny, and Prem
Melville. 2002. Natural language assistant - a di-
alog system for online product recommendation. AI
Magazine, 23:63?75.
Grace Chung. 2004. Developing a flexible spoken dia-
log system using simulation. In Proceedings of ACL
2004, pages 63?70.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of EACL 1997, pages 174?
181.
Jeff Huang, Oren Etzioni, Luke Zettlemoyer, Kevin
Clark, and Christian Lee. 2012. Revminer: An ex-
tractive interface for navigating reviews on a smart-
phone. In Proceedings of UIST 2012.
Yohan Jo and Alice H Oh. 2011. Aspect and sentiment
unification model for online review analysis. In Pro-
ceedings of the Fourth ACM International Confer-
ence on Web Search and Data Mining, pages 815?
824.
Karin Kipper-Schuler. 2005. Verbnet: A broad-
coverage, comprehensive verb lexicon.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings ACL
2003, pages 423?430.
I. Kopec?ek. 1999. Modeling of the information re-
trieval dialogue systems. In Proceedings of the
Workshop on Text, Speech and Dialogue-TSD 99,
Lectures Notes in Artificial Intelligence 1692, pages
302?307. Springer-Verlag.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of ACL 2009, pages 226?234.
Cynthia A. Thompson, Mehmet H. Goeker, and Pat
Langley. 2004. A personalized system for conver-
sational recommendations. Journal of Artificial In-
telligence Research (JAIR), 21:393?428.
Max Whitney and Anoop Sarkar. 2012. Bootstrapping
via graph propagation. In Proceedings of the ACL
2012, pages 620?628, Jeju Island, Korea.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005. Opinionfinder: A system for subjectivity
analysis. In Proceedings of HLT/EMNLP 2005 on
Interactive Demonstrations, pages 34?35.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174, April.
504
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 33?41,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
He Said, She Said: Gender in the ACL Anthology
Adam Vogel
Stanford University
av@cs.stanford.edu
Dan Jurafsky
Stanford University
jurafsky@stanford.edu
Abstract
Studies of gender balance in academic com-
puter science are typically based on statistics
on enrollment and graduation. Going beyond
these coarse measures of gender participation,
we conduct a fine-grained study of gender
in the field of Natural Language Processing.
We use topic models (Latent Dirichlet Allo-
cation) to explore the research topics of men
and women in the ACL Anthology Network.
We find that women publish more on dialog,
discourse, and sentiment, while men publish
more than women in parsing, formal seman-
tics, and finite state models. To conduct our
study we labeled the gender of authors in the
ACL Anthology mostly manually, creating a
useful resource for other gender studies. Fi-
nally, our study of historical patterns in fe-
male participation shows that the proportion
of women authors in computational linguis-
tics has been continuously increasing, with
approximately a 50% increase in the three
decades since 1980.
1 Introduction
The gender imbalance in science and engineering is
particularly striking in computer science, where the
percentage of graduate students in computer science
that are women seems to have been declining rather
than increasing recently (Palma, 2001; Beaubouef
and Zhang, 2011; Spertus, 1991; Hill et al, 2010;
Singh et al, 2007).
While many studies have examined enrollment
and career advancement, less attention has been
paid to gender differences in scientific publications.
This paper studies author gender in the Associa-
tion for Computational Linguistics Anthology Net-
work (AAN) corpus (Radev et al, 2009), (based on
the ACL Anthology Reference Corpus (Bird et al,
2008)) from which we used 13,000 papers by ap-
proximately 12,000 distinct authors from 1965 to
2008.
The AAN corpus disambiguates author names,
but does not annotate these names for gender. We
first performed a mostly-manual annotation of the
gender of each author (details in Section 2). We
make these annotation available as a useful resource
for other researchers.1
We then study a number of properties of the ACL
authors. We first address surface level questions re-
garding the balance of genders in publications. In
2008, women were granted 20.5% of computer sci-
ence PhDs (CRA, 2008). Does this ratio hold also
for the percentages of papers written by women in
computational linguistics as well? We explore dif-
ferences in publication count between genders, look-
ing at total publications and normalized values like
publications per year and trends over time.
Going beyond surface level analysis, we then turn
to document content. We utilize Latent Dirichlet Al-
location (LDA) topic models (Blei et al, 2003) to
study the difference in topics that men and women
write about.
2 Determining Gender
The gender of an author is in general difficult to
determine automatically with extremely high pre-
cision. In many languages, there are gender-
differentiated names for men and women that can
make gender-assignment possible based on gen-
1http://nlp.stanford.edu/projects/
gender.shtml
33
dered name dictionaries. But the fact that ACL
authors come from many different language back-
ground makes this method prone to error. For exam-
ple, while U.S. Census lists of frequently occurring
names by gender (Census, 2012) can resolve a large
proportion of commonly occurring names from au-
thors in the United States and Canada, they incor-
rectly list the name ?Jan? as female. It turns out
that authors in the ACL Anthology who are named
?Jan? are in fact male, since the name is a very com-
mon male name in many parts of Europe, and since
US female researchers named ?Jan? often use the
full form of their name rather than the shortening
?Jan? when publishing. Furthermore, a significant
percentage of ACL authors have Chinese language
names, which are much less clearly linked with per-
sonal names (e.g., Weiwei Sun is female whereas
Weiwei Ding is male).
We found that Chinese names as well as ambigu-
ous names like ?Jan? were poorly predicted by on-
line name gender website algorithms we looked at,
leading to a high error rate. To insure high precision,
we therefore instead chose to annotate the authors
in the corpus with a high-precision method; mainly
hand labeling the names but also using some auto-
matic help.
We used unambiguous name lists for various lan-
guages to label a large proportion of the name; for
example we used the subset of given names (out
of the 4221 first names reported in the 1990 U.S.
Census) that were unambiguous (occurring consis-
tently with only one gender in all of our name lists)
used morphological gender for languages like Czech
or Bulgarian which mark morphological gender on
names, and relied on lists of Indian and Basque
names (from which we had removed any ambigu-
ous names). For all ambiguous names, we next used
our personal cognizance of many of the ACL au-
thors, also asking for help from ACL researchers
in China, Taiwan, and Singapore (to help label Chi-
nese names of researchers they were familiar with)
and other researchers for help on the Japanese and
Korean names. Around 1100 names were hand-
labeled from personal cognizance or photos of the
ACL researchers on their web pages. The combina-
tion of name lists and personal cognizance left only
2048 names (15% of the original 12,692) still unla-
beled. We then used a baby name website, www.
Total First Author
Gender Papers % Papers %
Female 6772 33% 4034 27%
Male 13454 64% 10813 71%
Unknown 702 3% 313 2%
Table 1: Number of publications by gender. The to-
tal publications column shows the number of papers for
which at least one author was a given gender, in any au-
thorship position. The first authored publications column
shows the number of papers for which a given gender is
the first author.
gpeters.com/names/, originally designed for
reporting the popularity and gender balance of first
names, to find the gender of 1287 of these 2048
names.2 The remaining 761 names remained unla-
beled.
 0 1000 2000
 3000 4000 5000
 6000 7000 8000
 9000
Female Male UnknownN
u
m
b
e
r
 
o
f
 
A
u
t
h
o
r
s
Gender
Authorship by Gender
Figure 1: The total number of authors of a given gender.
3 Overall Statistics
We first discuss some overall gender statistics for the
ACL Anthology. Figure 1 shows the number of au-
thors of each gender. Men comprised 8573 of the
12692 authors (67.5%) and there were 3359 female
authors (26.5%). We could not confidently deter-
mine the gender of 761 out of 12692 (6.0%) of the
authors. Some of these are due to single letter first
names or problems with ill-formatted data.
Table 1 lists the number of papers for each gen-
der. About twice as many papers had at least one
2The gender balance of these 1287 automatically-
determined names was 34% female, 66% male, slightly
higher than the average for the whole corpus.
34
male author (64%) as had at least one female au-
thor (33%). The statistics for first authorship were
slightly more skewed; women were the first author
of 27% of papers, whereas men first authored 71%.
In papers with at least one female author, the first au-
thor was a woman 60% of the time, whereas papers
with at least one male author had a male first author
80% of the time. Thus men not only write more pa-
pers, but are also more frequently first authors.
 0 500
 1000 1500
 2000 2500
 1960  1970  1980  1990  2000  2010
P
u
b
l
i
c
a
t
i
o
n
s
Year
Authorship by YearFemaleMaleUNK
Figure 2: The number of authors of a given gender for a
given year.
Figure 2 shows gender statistics over time, giving
the number of authors of a given gender for a given
year. An author is considered active for a year if he
or she was an author of at least one paper. The num-
ber of both men and women authors increases over
the years, reflecting the growth of computational lin-
guistics.
Figure 3 shows the percentage of authors of a
given gender over time. We overlay a linear re-
gression of authorship percentage for each gender
showing that the proportion of women is growing
over time. The male best fit line has equation y =
?0.3025x + 675.49(R2 = 0.41, p = 1.95 ? 10?5)
and the female best fit line is y = 0.3429x ?
659.48(R2 = 0.51, p = 1.48?10?5). Female author-
ship percentage grew from 13% in 1980 to 27% in
2007, while male authorship percentage decreased
from 79% in 1980 to 71% in 2007. Using the best
fit lines as a more robust estimate, female authorship
grew from 19.4% to 29.1%, a 50% relative increase.
This increase of the percentage of women author-
ship is substantial. Comparable numbers do not
seem to exist for computer science in general, but
according to the CRA Taulbee Surveys of computer
science (CRA, 2008), women were awarded 18% of
the PhDs in 2002 and 20.5% in 2007. In computa-
tional linguistics in the AAN, women first-authored
26% of papers in 2002 and 27% of papers in 2007.
Although of course these numbers are not directly
comparable, they at least suggest that women partic-
ipate in computational linguistics research at least as
much as in the general computer science population
and quite possibly significantly more.
We next turn attention to how the most prolific
authors of each gender compare. Figure 4 shows the
number of papers published by the top 400 authors
of each gender, sorted in decreasing order. We see
that the most prolific authors are men.
There is an important confound in interpreting the
number of total papers by men and the statistics on
prolific authors. Since, as Figure 3 shows, there was
a smaller proportion of women in the field in the
early days of computational linguistics, and since
authors publish more papers the longer they are in
the field, it?s important to control for length of ser-
vice.
Figure 5 shows the average number of active years
for each gender. An author is considered active in
the years between his or her first and last publication
in the anthology. Comparing the number of years
of service for each gender, we find that on average
men indeed have been in the field longer (t-test, p =
10?6).
Accounting for this fact, Figure 6 shows the aver-
age number of publications per active year. Women
published an average of 1.07 papers per year active,
while men published 1.03 papers per active year.
This difference is significant (t-test, p = 10?3), sug-
gesting that women are in fact slightly more prolific
than men per active year.
In the field of Ecology, Sih and Nishikawa (1988)
found that men and women published roughly the
same number of papers per year of service. They
used a random sample of 100 researchers in the field.
In contrast, Symonds et al (2006) found that men
published more papers per year than women in ecol-
ogy and evolutionary biology. This study also used
random sampling, so it is unclear if the differing re-
sults are caused by a sampling error or by some other
source.
35
 0
 20
 40
 60
 80
 100
 1980  1990  2000  2010P
e
r
c
e
n
t
a
g
e
 
o
f
 
A
u
t
h
o
r
s
Year
Percentage Authorship by Year
FemaleMaleUNK
Figure 3: The percentage of authors of a given gender per year. Author statistics before 1980 are sparse and noisy, so
we only display percentages from 1980 to 2008.
 0 20
 40 60
 80 100
 120 140
 0  50  100 150 200 250 300 350 400
P
u
b
l
i
c
a
t
i
o
n
s
Rank
Number of Publications by GenderFemaleMale
Figure 4: The number of publications per author sorted
in decreasing order.
4 Topic Models
In this section we discuss the relationship between
gender and document content. Our main tool is La-
tent Dirichlet Allocation (LDA), a model of the top-
ics in a document. We briefly describe LDA; see
(Blei et al, 2003) for more details. LDA is a genera-
tive model of documents, which models documents
as a multinomial mixture of topics, which in turn are
 0 0.5
 1 1.5
 2 2.5
 3 3.5
Female MaleNum
b
e
r
 
o
f
 
A
c
t
i
v
e
 
Y
e
a
r
s
Gender
Average Number of Active Years
Figure 5: The average number of active years by gender
multinomial distributions over words. The genera-
tive story proceeds as follows: a document first picks
the number of words N it will contain and samples a
multinomial topic distribution p(z|d) from a Dirich-
let prior. Then for each word to be generated, it picks
a topic z for that word, and then a word from the
multinomial distribution p(w|z).
Following earlier work like Hall et al (2008), we
ran LDA (Blei et al, 2003) on the ACL Anthology,
36
 0
 0.5
 1
 1.5
 2
Female MaleN
u
m
b
e
r
 
o
f
 
P
a
p
e
r
s
Gender
Average Papers Per Year Active
Figure 6: The average number of papers per active year,
where an author is considered active in years between his
or her first and last publication.
producing 100 generative topics. The second author
and another senior expert in the field (Christopher D.
Manning) collaboratively assigned labels to each of
the 100 topics including marking those topics which
were non-substantive (lists of function words or af-
fixes) to be eliminated. Their consensus labeling
eliminated 27 topics, leaving 73 substantive topics.
In this study we are interested in how documents
written by men and women differ. We are mainly in-
terested in Pr(Z|G), the probability of a topic being
written about by a given gender, and Pr(Z|Y,G),
the probability of a topic being written about by a
particular gender in a given year. Random variable
Z ranges over topics, Y over years, and G over gen-
der. Our topic model gives us Pr(z|d), where d is a
particular document. For a document d ? D, let dG
be the gender of the first author, and dY the year it
was written.
To compute Pr(z|g), we sum over documents
whose first author is gender g:
Pr(z|g) =
?
{d?D|dG=g}
Pr(z|d) Pr(d|g)
=
?
{d?D|dG=g}
Pr(z|d)
|{d ? D|dG = g}|
To compute Pr(z|y, g), we additionally condition
on the year a document was written:
Pr(z|y, g) =
?
{d?D|dY =y}
Pr(z|d) Pr(d|y, g)
=
?
{d?D|dY =y,dG=g}
Pr(z|d)
|{d ? D|dY = y, dG = g}|
To determine fields in which one gender publishes
more than another, we compute the odds-ratio
Pr(z|g = female)(1? Pr(z|g = female))
Pr(z|g = male)(1? Pr(z|g = male))
for each of the 73 topics in our corpus.
5 Topic Modeling Results
Using the odds-ratio defined above, we computed
the top eight male and female topics. The top
female-published topics are speech acts + BDI,
prosody, sentiment, dialog, verb subcategorization,
summarization, anaphora resolution, and tutoring
systems. Figure 9 shows the top words for each of
those topics. Figure 7 shows how they have evolved
over time.
The top male-published topics are categorial
grammar + logic, dependency parsing, algorithmic
efficiency, parsing, discriminative sequence models,
unification based grammars, probability theory, and
formal semantics. Figure 8 and 10 display these top-
ics over time and their associated words.
There are interesting possible generalizations in
these topic differences. At least in the ACL cor-
pus, women tend to publish more in speech, in social
and conversational topics, and in lexical semantics.
Men tend to publish more in formal mathematical
approaches and in formal syntax and semantics.
Of course the fact that a certain topic is more
linked with one gender doesn?t mean the other gen-
der does not publish in this topic. In particular, due
to the larger number of men in the field, there can be
numerically more male-authored papers in a female-
published topic. Instead, what our analysis yields
are topics that each gender writes more about, when
adjusted by the number of papers published by that
gender in total.
Nonetheless, these differences do suggest that
women and men in the ACL corpus may, at least
to some extent, exhibit some gender-specific tenden-
cies to favor different areas of research.
37
 0 0.01 0.02
 0.03 0.04 0.05
 0.06 0.07 0.08
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Speech Acts + BDIFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
ProsodyFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Sentiment AnalysisFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 0.045 0.05
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
DialogFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Lexical Acquisition Of Verb SubcategorizationFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
SummarizationFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 0.045
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Anaphora ResolutionFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Tutoring SystemsFemaleMale
Figure 7: Plots of some topics for which P (topic|female) > P (topic|male). Note that the scale of the y-axis differs
between plots.
38
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Categorial GrammarFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Dependency ParsingFemaleMale
 0 0.002 0.004
 0.006 0.008 0.01
 0.012 0.014 0.016
 0.018
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Algorithmic EfficiencyFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
ParsingFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Discriminative Sequence ModelsFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 0.045
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Unification Based GrammarsFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Probability TheoryFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 0.045
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Formal Computational SemanticsFemaleMale
Figure 8: Plots of some topics for which P (topic|male) > P (topic|female). Note that the scale of the y-axis differs
between plots.
39
Speech Acts + BDI speaker utterance act hearer belief proposition acts beliefs focus evidence
Prosody prosodic pitch boundary accent prosody boundaries cues repairs speaker phrases
Sentiment question answer questions answers answering opinion sentiment negative trec positive
Dialog dialogue utterance utterances spoken dialog dialogues act turn interaction conversation
Verb Subcategorization class classes verbs paraphrases classification subcategorization paraphrase frames
Summarization topic summarization summary document news summaries documents topics articles
Anaphora Resolution resolution pronoun anaphora antecedent pronouns coreference anaphoric definite
Tutoring Systems students student reading course computer tutoring teaching writing essay native
Figure 9: Top words for each topic that women publish in more than men
Categorial Grammar + Logic proof logic definition let formula theorem every defined categorial axioms
Dependency Parsing dependency dependencies head czech depen dependent treebank structures
Algorithmic Efficiency search length size space cost algorithms large complexity pruning efficient
Parsing grammars parse chart context-free edge edges production symbols symbol cfg
Discriminative Sequence Models label conditional sequence random labels discriminative inference crf fields
Unification Based Grammars unification constraints structures value hpsg default head grammars values
Probability Theory probability probabilities distribution probabilistic estimation estimate entropy
Formal Semantics semantics logical scope interpretation logic meaning representation predicate
Figure 10: Top words for each topic that men publish in more than women
6 Conclusion
Our study of gender in the ACL Anthology shows
important gains in the percentage of women in the
field over the history of the ACL (or at least the last
30 years of it). More concretely, we find approxi-
mately a 50% increase in the proportion of female
authors since 1980. While women?s smaller num-
bers means that they have produced less total pa-
pers in the anthology, they have equal (or even very
slightly higher) productivity of papers per year.
In topics, we do notice some differing tenden-
cies toward particular research topics. In current
work, we are examining whether these differences
are shrinking over time, as a visual overview of Fig-
ure 7 seems to suggest, which might indicate that
gender balance in topics is a possible outcome, or
possibly that topics first addressed by women are
likely to to be taken up by male researchers. Ad-
ditionally, other applications of topic models to the
ACL Anthology allow us to study the topics a sin-
gle author publishes in over time (Anderson et al,
2012). These techniques would allow us to study
how gender relates to an author?s topics throughout
his or her career.
Our gender labels for ACL authors (available
at http://nlp.stanford.edu/projects/
gender.shtml) provide an important resource
for other researchers to expand on the social study
of computational linguistics research.
7 Acknowledgments
This research was generously supported by the Of-
fice of the President at Stanford University and the
National Science Foundation under award 0835614.
Thanks to Steven Bethard and David Hall for cre-
ating the topic models, Christopher D. Manning for
helping label the topics, and Chu-Ren Huang, Olivia
Kwong, Heeyoung Lee, Hwee Tou Ng, and Nigel
Ward for helping with labeling names for gender.
Additional thanks to Martin Kay for the initial pa-
per idea.
References
Ashton Anderson, Dan McFarland, and Dan Jurafsky.
2012. Towards a computational history of the acl:
1980 - 2008. In ACL 2012 Workshop: Rediscovering
50 Years of Discoveries.
Theresa Beaubouef and Wendy Zhang. 2011. Where are
the women computer science students? J. Comput.
Sci. Coll., 26(4):14?20, April.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
40
2008. The ACL Anthology Reference Corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In LREC-08, pages 1755?1759.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, March.
US Census. 2012. First name frequency by gender.
http://www.census.gov/genealogy/names/names files.html.
CRA. 2008. CRA Taulbee Survey (web site).
http://www.cra.org/resources/taulbee/.
David L.W. Hall, Daniel Jurafsky, and Christopher D.
Manning. 2008. Studying the history of ideas using
topic models. In Proceedings of Conference on Em-
pirical Methods on Natural Language Processing.
Catherine Hill, Christianne Corbett, and Andresse
St Rose. 2010. Why So Few? Women in Science,
Technology, Engineering, and Mathematics. Ameri-
can Association of University Women.
Paul De Palma. 2001. Viewpoint: Why women avoid
computer science. Commun. ACM, 44:27?30, June.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The ACL Anthology Network cor-
pus. In Proceedings of the 2009 Workshop on Text
and Citation Analysis for Scholarly Digital Libraries,
NLPIR4DL ?09, pages 54?61, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Andrew Sih and Kiisa Nishikawa. 1988. Do men
and women really differ in publication rates and con-
tentiousness? an empirical survey. Bulletin of the Eco-
logical Society of America, 69(1):pp. 15?18.
Kusum Singh, Katherine R Allen, Rebecca Scheckler,
and Lisa Darlington. 2007. Women in computer-
related majors: A critical synthesis of research and
theory from 1994 to 2005. Review of Educational Re-
search, 77(4):500?533.
Ellen Spertus. 1991. Why are there so few female com-
puter scientists? Technical report, Massachusetts In-
stitute of Technology, Cambridge, MA, USA.
Matthew R.E. Symonds, Neil J. Gemmell, Tamsin L.
Braisher, Kylie L. Gorringe, and Mark A. Elgar. 2006.
Gender differences in publication output: Towards an
unbiased metric of research performance. PLoS ONE,
1(1):e127, 12.
41
