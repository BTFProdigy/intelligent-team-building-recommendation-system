Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 263?271,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
May All Your Wishes Come True:
A Study of Wishes and How to Recognize Them
Andrew B. Goldberg, Nathanael Fillmore, David Andrzejewski
Zhiting Xu, Bryan Gibson, Xiaojin Zhu
Computer Sciences Department, University of Wisconsin-Madison, Madison, WI 53706, USA
{goldberg, nathanae, andrzeje, zhiting, bgibson, jerryzhu}@cs.wisc.edu
Abstract
A wish is ?a desire or hope for something
to happen.? In December 2007, people from
around the world offered up their wishes to
be printed on confetti and dropped from the
sky during the famous New Year?s Eve ?ball
drop? in New York City?s Times Square. We
present an in-depth analysis of this collection
of wishes. We then leverage this unique re-
source to conduct the first study on building
general ?wish detectors? for natural language
text. Wish detection complements traditional
sentiment analysis and is valuable for collect-
ing business intelligence and insights into the
world?s wants and desires. We demonstrate
the wish detectors? effectiveness on domains
as diverse as consumer product reviews and
online political discussions.
1 Introduction
Each year, New York City rings in the New Year
with the famous ?ball drop? in Times Square. In
December 2007, the Times Square Alliance, co-
producer of the Times Square New Year?s Eve Cele-
bration, launched a Web site called the Virtual Wish-
ing Wall1 that allowed people around the world to
submit their New Year?s wishes. These wishes were
then printed on confetti and dropped from the sky
at midnight on December 31, 2007 in sync with the
ball drop.
We obtained access to this set of nearly 100,000
New Year?s wishes, which we call the ?WISH cor-
pus.? Table 1 shows a selected sample of the WISH
1http://www.timessquarenyc.org/nye/nye interactive.html
corpus. Some are far-reaching fantasies and aspi-
rations, while others deal with everyday concerns
like economic and medical distress. We analyze this
first-of-its-kind corpus in Section 2.
The New Oxford American Dictionary defines
?wish? as ?a desire or hope for something to hap-
pen.? How wishes are expressed, and how such
wishful expressions can be automatically recog-
nized, are open questions in natural language pro-
cessing. Leveraging the WISH corpus, we conduct
the first study on building general ?wish detectors?
for natural language text, and demonstrate their ef-
fectiveness on domains as diverse as consumer prod-
uct reviews and online political discussions. Such
wish detectors have tremendous value in collecting
business intelligence and public opinions. We dis-
cuss the wish detectors in Section 3, and experimen-
tal results in Section 4.
1.1 Relation to Prior Work
Studying wishes is valuable in at least two aspects:
1. Being a special genre of subjective expression,
wishes add a novel dimension to sentiment analy-
sis. Sentiment analysis is often used as an auto-
matic market research tool to collect valuable busi-
ness intelligence from online text (Pang and Lee,
2008; Shanahan et al, 2005; Koppel and Shtrim-
berg, 2004; Mullen and Malouf, 2008). Wishes
differ from the recent focus of sentiment analysis,
namely opinion mining, by revealing what people
explicitly want to happen, not just what they like or
dislike (Ding et al, 2008; Hu and Liu, 2004). For ex-
ample, wishes in product reviews could contain new
feature requests. Consider the following (real) prod-
263
514 peace on earth
351 peace
331 world peace
244 happy new year
112 love
76 health and happiness
75 to be happy
51 i wish for world peace
21 i wish for health and happiness for my family
21 let there be peace on earth
16 i wish u to call me if you read this 555-1234
16 to find my true love
8 i wish for a puppy
7 for the war in iraq to end
6 peace on earth please
5 a free democratic venezuela
5 may the best of 2007 be the worst of 2008
5 to be financially stable
1 a little goodness for everyone would be nice
1 i hope i get accepted into a college that i like
1 i wish to get more sex in 2008
1 please let name be healthy and live all year
1 to be emotionally stable and happy
1 to take over the world
Table 1: Example wishes and their frequencies in the
WISH corpus.
uct review excerpt: ?Great camera. Indoor shots
with a flash are not quite as good as 35mm. I wish
the camera had a higher optical zoom so that I could
take even better wildlife photos.? The first sentence
contains positive opinion, the second negative opin-
ion. However, wishful statements like the third sen-
tence are often annotated as non-opinion-bearing in
sentiment analysis corpora (Hu and Liu, 2004; Ding
et al, 2008), even though they clearly contain im-
portant information. An automatic ?wish detector?
text-processing tool can be useful for product manu-
facturers, advertisers, politicians, and others looking
to discover what people want.
2. Wishes can tell us a lot about people: their in-
nermost feelings, perceptions of what they?re lack-
ing, and what they desire (Speer, 1939). Many
psychology researchers have attempted to quantify
the contents of wishes and how they vary with
factors such as location, gender, age, and per-
sonality type (Speer, 1939; Milgram and Riedel,
1969; Ehrlichman and Eichenstein, 1992; King and
Broyles, 1997). These studies have been small scale
with only dozens or hundreds of participants. The
WISH corpus provides the first large-scale collec-
tion of wishes as a window into the world?s desires.
Beyond sentiment analysis, classifying sentences
as wishes is an instance of non-topical classifica-
tion. Tasks under this heading include compu-
tational humor (Mihalcea and Strapparava, 2005),
genre classification (Boese and Howe, 2005), au-
thorship attribution (Argamon and Shimoni, 2003),
and metaphor detection (Krishnakumaran and Zhu,
2007), among others (Mishne et al, 2007; Mihal-
cea and Liu, 2006). We share the common goal of
classifying text into a unique set of target categories
(in our case, wishful and non-wishful), but use dif-
ferent techniques catered to our specific task. Our
feature-generation technique for wish detection re-
sembles template-based methods for information ex-
traction (Brin, 1999; Agichtein and Gravano, 2000).
2 Analyzing the WISH Corpus
We analyze the WISH corpus with a variety of sta-
tistical methods. Our analyses not only reveal what
people wished for on New Year?s Eve, but also pro-
vide insight for the development of wish detectors in
Section 3.
The complete WISH corpus contains nearly
100,000 wishes collected over a period of 10 days
in December 2007, most written in English, with the
remainder in Portuguese, Spanish, Chinese, French,
and other languages. For this paper, we consider
only the 89,574 English wishes. Most of these En-
glish wishes contain optional geographic meta data
provided by the wisher, indicating a variety of coun-
tries (not limited to English-speaking) around the
world. We perform minimal preprocessing, includ-
ing TreeBank-style tokenization, downcasing, and
punctuation removal. Each wish is treated as a sin-
gle entity, regardless of whether it contains multiple
sentences. After preprocessing, the average length
of a wish is 8 tokens.
2.1 The Topic and Scope of Wishes
As a first step in understanding the content of the
wishes, we asked five annotators to manually an-
notate a random subsample of 5,000 wishes. Sec-
tions 2.1 and 2.2 report results on this subsample.
The wishes were annotated in terms of two at-
264
(a) Topic of Wishes
(b) Scope of Wishes
Figure 1: Topic and scope distributions based on manual
annotations of a random sample of 5,000 wishes in the
WISH corpus.
tributes: topic and scope. We used 11 pre-defined
topic categories, and their distribution in this sub-
sample of the WISH corpus is shown in Figure 1(a).
The most frequent topic is love, while health,
happiness, and peace are also common themes.
Many wishes also fell into an other category, in-
cluding specific individual requests (?i wish for a
new puppy?), solicitations or advertisements (?call
me 555-1234?, ?visit website.com?), or sinister
thoughts (?to take over the world?).
The 5,000 wishes were also manually assigned
a scope. The scope of a wish refers to the range
of people that are targeted by the wish. We used
6 pre-defined scope categories: self (?I want to be
happy?), family (?For a cure for my husband?), spe-
cific person by name (?Prayers for name?), country
(?Bring our troops home!?), world (?Peace to every-
one in the world?), and other. In cases where mul-
tiple scope labels applied, the broadest scope was
selected. Figure 1(b) shows the scope distribution.
It is bimodal: over one third of the wishes are nar-
rowly directed at one?s self, while broad wishes at
the world level are also frequent. The in-between
scopes are less frequent.
2.2 Wishes Differ by Geographic Location
As mentioned earlier, wishers had the option to enter
a city/country when submitting wishes. Of the man-
ually annotated wishes, about 4,000 included valid
location information, covering all 50 states in the
U.S., and all continents except Antarctica.
We noticed a statistically significant difference
between wishes submitted from the United States
(about 3600) versus non-U.S. (about 400), both in
terms of their topic and scope distributions. For each
comparison, we performed a Pearson ?2-test using
location as the explanatory variable and either topic
or scope as the response variable.2 The null hypoth-
esis is that the variables are independent. For both
tests we reject the null hypothesis, with p < 0.001
for topic, and p = 0.006 for scope. This indicates a
dependence between location and topic/scope. As-
terisks in Figure 2 denote the labels that differ sig-
nificantly between U.S. and non-U.S. wishes.3
In particular, we observed that there are signif-
icantly more wishes about love, peace, and travel
from non-U.S. locales, and more about religion from
the U.S. There are significantly more world-scoped
wishes from non-U.S. locales, and more country-
and family-scoped wishes from the U.S.
We also compared wishes from ?red states? ver-
sus ?blue states? (U.S. states that voted a majority
for the Republican and Democratic presidential can-
didates in 2008, respectively), but found no signifi-
cant differences.
2The topic test examined a 2 ? 11 contingency table, while
the scope test used a 2 ? 6 contingency table. In both tests, all
of the cells in the tables had an expected frequency of at least 5,
so the ?2 approximation is valid.
3To identify the labels that differ significantly by location,
we computed the standardized residuals for the cells in the two
contingency tables. Standardized residuals are approximately
N (0, 1)-distributed and can be used to locate the major con-
tributors to a significant ?2-test statistic (Agresti, 2002). The
asterisks in Figure 2 indicate the surprisingly large residuals,
i.e., the difference between observed and expected frequencies
is outside a 95% confidence interval.
265
(a) Wish topics differ by Location
(b) Wish scopes differ by Location
Figure 2: Geographical breakdown of topic and scope
distributions based on approximately 4,000 location-
tagged wishes. Asterisks indicate statistically significant
differences.
2.3 Wishes Follow Zipf?s Law
We now move beyond the annotated subsample and
examine the full set of 89,574 English wishes. We
noticed that a small fraction (4%) of unique wishes
account for a relatively large portion (16%) of wish
occurrences, while there are also many wishes that
only occur once. The question naturally arises: do
wishes obey Zipf?s Law (Zipf, 1932; Manning and
Schu?tze, 1999)? If so, we should expect the fre-
quency of a unique wish to be inversely proportional
to its rank, when sorted by frequency. Figure 3
plots rank versus frequency on a log-log scale and
reveals an approximately linear negative slope, thus
suggesting that wishes do follow Zipf?s law. It also
shows that low-occurrence wishes dominate, hence
learning might be hindered by data sparseness.
2.4 Latent Topic Modeling for Wishes
The 11 topics in Section 2.1 were manually pre-
defined based on domain knowledge. In contrast,
in this section we applied Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003) to identify the latent
topics in the full set of 89,574 English wishes in an
100 101 102 103 104 10510
0
101
102
103 peace
to find my true love
to take overthe world
log(rank)
log(f
requ
ency
)
Figure 3: The rank vs. frequency plot of wishes, approx-
imately obeying Zipf?s law. Note the log-log scale.
unsupervised fashion. The goal is to validate and
complement the study in Section 2.1.
To apply LDA to the wishes, we treated each indi-
vidual wish as a short document. We used 12 topics,
Collapsed Gibbs Sampling (Griffiths and Steyvers,
2004) for inference, hyperparameters ? = 0.5 and
? = 0.1, and ran Markov Chain Monte Carlo for
2000 iterations.
The resulting 12 LDA topics are shown in Ta-
ble 2, in the form of the highest probability words
p(word|topic) in each topic. We manually added
summary descriptors for readability. With LDA, it is
also possible to observe which words were assigned
to which topics in each wish. For example, LDA as-
signed most words in the wish ?world(8) peace(8)
and my friends(4) in iraq(1) to come(1) home(1)?
to two topics: peace and troops (topic numbers in
parentheses). Interestingly, these LDA topics largely
agree with the pre-defined topics in Section 2.1.
3 Building Wish Detectors
We now study the novel NLP task of wish detection,
i.e., classifying individual sentences as being wishes
or not. Importantly, we want our approach to trans-
fer to domains other than New Year?s wishes, in-
cluding consumer product reviews and online politi-
cal discussions. It should be pointed out that wishes
are highly domain dependent. For example, ?I wish
for world peace? is a common wish on New Year?s
Eve, but is exceedingly rare in product reviews; and
vice versa: ?I want to have instant access to the vol-
ume? may occur in product reviews, but is an un-
266
Topic Summary Top words in the topic, sorted by p(word|topic)
0 New Year year, new, happy, 2008, best, everyone, great, years, wishing, prosperous, may, hope
1 Troops all, god, home, come, may, safe, s, us, bless, troops, bring, iraq, return, 2008, true, dreams
2 Election wish, end, no, more, 2008, war, stop, president, paul, not, ron, up, free, less, bush, vote
3 Life more, better, life, one, live, time, make, people, than, everyone, day, wish, every, each
4 Prosperity health, happiness, good, family, friends, all, love, prosperity, wealth, success, wish, peace
5 Love love, me, find, wish, true, life, meet, want, man, marry, call, someone, boyfriend, fall, him
6 Career get, wish, job, out, t, hope, school, better, house, well, want, back, don, college, married
7 Lottery wish, win, 2008, money, want, make, become, lottery, more, great, lots, see, big, times
8 Peace peace, world, all, love, earth, happiness, everyone, joy, may, 2008, prosperity, around
9 Religion love, forever, jesus, know, loves, together, u, always, 2, 3, 4, much, best, mom, christ
10 Family healthy, happy, wish, 2008, family, baby, life, children, long, safe, husband, stay, marriage
11 Health com, wish, s, me, lose, please, let, cancer, weight, cure, mom, www, mother, visit, dad
Table 2: Wish topics learned from Latent Dirichlet Allocation. Words are sorted by p(word|topic).
likely New Year?s wish. For this initial study, we do
assume that there are some labeled training data in
the target domains of interest.
To transfer the knowledge learned from the out-
of-domain WISH corpus to other domains, our key
insight is the following: while the content of wishes
(e.g., ?world peace?) may not transfer across do-
mains, the ways wishes are expressed (e.g., ?I wish
for ?) may. We call these expressions wish tem-
plates. Our novel contribution is an unsupervised
method for discovering candidate templates from the
WISH corpus which, when applied to other target
domains, improve wish detection in those domains.
3.1 Two Simple Wish Detectors
Before describing our template discovery method,
we first describe two simple wish detectors, which
serve as baselines.
1. [Manual]: It may seem easy to locate
wishes. Perhaps looking for sentences containing
the phrases ?i wish,? ?i hope,? or some other sim-
ple patterns is sufficient for identifying the vast ma-
jority of wishes in a domain. To test this hypothe-
sis, we asked two native English speakers (not the
annotators, nor affiliated with the project; no expo-
sure to any of the wish datasets) to come up with
text patterns that might be used to express wishes.
They were shown three dictionary definitions of ?to
wish (v)? and ?wish (n)?. They produced a ranked
list of 13 templates; see Table 3. The underscore
matches any string. These templates can be turned
into a simple rule-based classifier: If part of a sen-
tence matches one of the templates, the sentence is
i wish
i hope
i want
hopefully
if only
would be better if
would like if
should
would that
can?t believe didn?t
don?t believe didn?t
do want
i can has
Table 3: Manual templates for identifying wishes.
classified as a wish. By varying the depth of the list,
one can produce different precision/recall behaviors.
Overall, we expect [Manual] to have relatively high
precision but low recall.
2. [Words]: Another simple method for detecting
wishes is to train a standard word-based text clas-
sifier using the labeled training set in the target do-
main. Specifically, we represent each sentence as
a binary word-indicator vector, normalized to sum
to 1. We then train a linear Support Vector Ma-
chine (SVM). This method may have higher recall,
but precision may suffer. For instance, the sentence
?Her wish was carried out by her husband? is not a
wish, but could be misclassified as one because of
the word ?wish.?
Note that neither of the two baseline methods uses
the WISH corpus.
267
3.2 Automatically Discovering Wish Templates
We now present our method to automatically dis-
cover high quality wish templates using the WISH
corpus. The key idea is to exploit redundancy in
how the same wish content is expressed. For ex-
ample, as we see in Table 1, both ?world peace? and
?i wish for world peace? are common wishes. Sim-
ilarly, both ?health and happiness? and ?i wish for
health and happiness? appear in the WISH corpus.
It is thus reasonable to speculate that ?i wish for ?
is a good wish template. Less obvious templates can
be discovered in this way, too, such as ?let there be
? from ?peace on earth? and ?let there be peace
on earth.?
We formalize this intuition as a bipartite graph, il-
lustrated in Figure 4. Let W = {w1, . . . , wn} be the
set of unique wishes in the WISH corpus. The bi-
partite graph has two types of nodes: content nodes
C and template nodes T , and they are generated as
follows. If a wish wj (e.g., ?i wish for world peace?)
contains another wish wi (e.g., ?world peace?), we
create a content node c1 = wi and a template node
t1 =?i wish for ?. We denote this relationship by
wj = c1+ t1. Note the order of c1 and t1 is insignif-
icant, as how the two combine is determined by the
underscore in t1, and wj = t1 + c1 is just fine. In
addition, we place a directed edge from c1 to t1 with
edge weight count(wj), the frequency of wish wj in
the WISH corpus. Then, a template node appears to
be a good one if many heavy edges point to it.
On the other hand, a template is less desirable
if it is part of a content node. For example, when
wj =?health and happiness? and wi =?health?, we
create the template t2 =? and happiness? and the
content node c3 = wi. If there is another wish
wk =?i wish for health and happiness?, then there
will be a content node c2 = wj . The template t2
thus contains some content words (since it matches
c2), and may not generalize well in a new domain.
We capture this by backward edges: if ?c? ? C, and
? string s (s not necessarily in C or W ) such that
c? = s+ t, we add a backward edge from t to c? with
edge weight count(c?).
Based on such considerations, we devised the fol-
lowing scheme for scoring templates:
score(t) = in(t)? out(t), (1)
health and happiness
c1
c2
c3
t1
t2
i wish for ___
___ and happiness
world peace
health
count(c1+t1)
count(c2)
Figure 4: The bipartite graph to create templates.
where in(t) is the in-degree of node t, defined as the
sum of edge weights coming into t; out(t) is the out-
degree of node t, defined similarly. In other words, a
template receives a high score if it is ?used? by many
frequent wishes but does not match many frequent
content-only wishes. To create the final set of tem-
plate features, we apply the threshold score(t) ? 5.
This produces a final list of 811 templates. Table 4
lists some of the top templates ranked by score(t).
While some of these templates still contain time- or
scope-related words (?for my family?), they are de-
void of specific topical content. Notice that we have
automatically identified several of the manually de-
rived templates in Table 3, and introduce many new
variations that a learning algorithm can leverage.
Top 10 Others in Top 200
in 2008 i want to
i wish for for everyone
i wish i hope
i want my wish is
this year please
i wish in 2008 wishing for
i wish to may you
for my family i wish i had
i wish this year to finally
in the new year for my family to have
Table 4: Top templates according to Equation 1.
3.3 Learning with Wish Template Features
After discovering wish templates as described
above, we use them as features for learning in a new
domain (e.g., product reviews). For each sentence in
the new domain, we assign binary features indicat-
ing which templates match the sentence. Two types
of matching are possible. Strict matching requires
that the template must match an entire sentence from
beginning to end, with at least one word filling in for
the underscore. (All matching during the template
generation process was strict.) Non-strict matching
268
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 
ManualWordsTemplatesWords + Templates
Figure 5: Politics domain precision-recall curves.
requires only that template match somewhere within
a sentence. Rather than choose one type of match-
ing, we create both strict and non-strict template fea-
tures (1622 binary features total) and let the machine
learning algorithm decide what is most useful.
Our third wish detector, [Templates], is a linear
SVM with the 1622 binary wish template features.
Our fourth wish detector, [Words + Templates], is
a linear SVM with both template and word features.
4 Experimental Results
4.1 Target Domains and Experimental Setup
We experimented with two domains, manually la-
beled at the sentence-level as wishes or non-wishes.4
Example wishes are listed in Table 6.
Products. Consumer product reviews: 1,235 sen-
tences selected from a collection of amazon.com and
cnet.com reviews (Hu and Liu, 2004; Ding et al,
2008). 12% of the sentences are labeled as wishes.
Politics. Political discussion board postings:
6,379 sentences selected from politics.com (Mullen
and Malouf, 2008). 34% are labeled as wishes.
We automatically split the corpora into sen-
tences using MxTerminator (Reynar and Ratna-
parkhi, 1997). As preprocessing before learning, we
tokenized the text in the Penn TreeBank style, down-
4These wish-annotated corpora are available for download
at http://pages.cs.wisc.edu/?goldberg/wish data.
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 
ManualWordsTemplatesWords + Templates
Figure 6: Products domain precision-recall curves.
cased, and removed all punctuation.
For all four wish detectors, we performed 10-fold
cross validation. We used the default parameter in
SVMlight for all trials (Joachims, 1999). As the
data sets are skewed, we compare the detectors us-
ing precision-recall curves and the area under the
curve (AUC). For the manual baseline, we produce
the curve by varying the number of templates ap-
plied (in rank order), which gradually predicts more
sentences as wishes (increasing recall at the expense
of precision). A final point is added at recall 1.0,
corresponding to applying an empty template that
matches all sentences. For the SVM-based meth-
ods, we vary the threshold applied to the real-valued
margin prediction to produce the curves. All curves
are interpolated, and AUC measures are computed,
using the techniques of (Davis and Goadrich, 2006).
4.2 Results
Figure 5 shows the precision-recall curves for the
Politics corpus. All curves are averages over 10
folds (i.e., for each of 100 evenly spaced, interpo-
lated recall points, the 10 precision values are aver-
aged). As expected, [Manual] can be very precise
with low recall?only the very top few templates
achieve high precision and pick out a small num-
ber of wishes with ?i wish? and ?i hope.? As we
introduce more templates to cover more true wishes,
precision drops off quickly. [Templates] is similar,
269
Corpus [Manual] [Words] [Templates] [Words + Templates]
Politics 0.67? 0.03 0.77? 0.03 0.73? 0.03 0.80? 0.03
Products 0.49? 0.13 0.52? 0.16 0.47? 0.16 0.56? 0.16
Table 5: AUC results (10-fold averages ? one standard deviation).
Products:
the only area i wish apple had improved upon would be the screen
i just want music to eminate from it when i want how i want
the dial on the original zen was perfect and i wish it was on this model
i would like album order for my live albums and was just wondering
Politics:
all children should be allowed healthcare
please call on your representatives in dc and ask them to please stop the waste in iraq
i hope that this is a new beginning for the middle east
may god bless and protect the brave men and that we will face these dangers in the future
Table 6: Example target-domain wishes correctly identified by [Words + Templates].
with slightly better precision in low recall regions.
[Words] is the opposite: bad in high recall but good
in low recall regions. [Words + Templates] is the
best, taking the best from both kinds of features to
dominate other curves. Table 5 shows the average
AUC across 10 folds. [Words + Templates] is sig-
nificantly better than all other detectors under paired
t-tests (p = 1 ? 10?7 vs. [Manual], p = 0.01 vs.
[Words], and p = 4 ? 10?7 vs. [Templates]). All
other differences are statistically significant, too.
Figure 6 shows the precision-recall curves for
the Products corpus. Again, [Words + Templates]
mostly dominates other detectors. In terms of av-
erage AUC across folds (Table 5), [Words + Tem-
plates] is also the best. However, due to the small
size of this corpus, the AUC values have high vari-
ance, and the difference between [Words + Tem-
plates] and [Words] is not statistically significant un-
der a paired t-test (p = 0.16).
Finally, to understand what is being learned in
more detail, we take a closer look at the SVM mod-
els? weights for one fold of the Products corpus
(Table 7). The most positive and negative features
make intuitive sense. Note that [Words + Templates]
seems to rely on templates for selecting wishes and
words for excluding non-wishes. This partially ex-
plains the synergy of combining the feature types.
Sign [Words] [Templates] [Words +Templates]
+ wish i hope hoping
+ hope i wish i hope
+ hopefully hoping i just want
+ hoping i just want i wish
+ want i would like i would like
- money family micro
- find forever about
- digital let me fix
- again d digital
- you for my dad you
Table 7: Features with the largest magnitude weights in
the SVM models for one fold of the Products corpus.
5 Conclusions and Future Work
We have presented a novel study of wishes from
an NLP perspective. Using the first-of-its-kind
WISH corpus, we generated domain-independent
wish templates that improve wish detection perfor-
mance across product reviews and political discus-
sion posts. Much work remains in this new research
area, including the creation of more types of fea-
tures. Also, due to the difficulty in obtaining wish-
annotated training data, we plan to explore semi-
supervised learning for wish detection.
Acknowledgements We thank the Times Square Al-
liance for providing the WISH corpus, and the Wisconsin
Alumni Research Foundation. AG is supported in part by
a Yahoo! Key Technical Challenges Grant.
270
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In In Proceedings of the 5th ACM International Con-
ference on Digital Libraries, pages 85?94.
Alan Agresti. 2002. Categorical Data Analysis. Wiley-
Interscience, second edition.
Shlomo Argamon and Anat Rachel Shimoni. 2003. Au-
tomatically categorizing written texts by author gen-
der. Literary and Linguistic Computing, 17:401?412.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elizabeth Sugar Boese and Adele Howe. 2005. Genre
classification of web documents. In Proceedings of
the 20th National Conference on Artificial Intelligence
(AAAI-05), Poster paper.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In WebDB ?98: Selected
papers from the International Workshop on The World
Wide Web and Databases, pages 172?183. Springer-
Verlag.
Jesse Davis and Mark Goadrich. 2006. The relationship
between precision-recall and roc curves. In ICML ?06:
Proceedings of the 23rd international conference on
Machine learning, New York, NY, USA. ACM.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
WSDM ?08: Proceedings of the international confer-
ence on Web search and web data mining, pages 231?
240. ACM.
Howard Ehrlichman and Rosalind Eichenstein. 1992.
Private wishes: Gender similarities and difference.
Sex Roles, 26(9):399?422.
Thomas Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. Proceedings of the National Academy of
Sciences, 101(suppl. 1):5228?5235.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD ?04,
the ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177. ACM
Press.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
Laura A. King and Sheri J. Broyles. 1997. Wishes, gen-
der, personality, and well-being. Journal of Personal-
ity, 65(1):49?76.
Moshe Koppel and Itai Shtrimberg. 2004. Good news
or bad news? let the market decide. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text,
pages 86?88.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Rada Mihalcea and Hugo Liu. 2006. A corpus-based ap-
proach to finding happiness. In Proceedings of AAAI-
CAAW-06, the Spring Symposia on Computational Ap-
proaches to Analyzing Weblogs.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Empirical Methods in Natural Lan-
guage Processing.
Norman A. Milgram and Wolfgang W. Riedel. 1969.
Developmental and experiential factors in making
wishes. Child Development, 40(3):763?771.
Gilad Mishne, Krisztian Balog, Maarten de Rijke, and
Breyten Ernsting. 2007. Moodviews: Tracking and
searching mood-annotated blog posts. In Proceed-
ings International Conf. on Weblogs and Social Media
(ICWSM-2007), pages 323?324.
Tony Mullen and Robert Malouf. 2008. Taking sides:
User classification for informal online political dis-
course. Internet Research, 18:177?190.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Fifth Conference on Applied Natural
Language Processing.
James Shanahan, Yan Qu, and Janyce Wiebe, editors.
2005. Computing attitude and affect in text. Springer,
Dordrecht, The Netherlands.
George S. Speer. 1939. Oral and written wishes of
rural and city school children. Child Development,
10(3):151?155.
G. K. Zipf. 1932. Selected Studies of the Principle of
Relative Frequency in Language. Harvard University
Press.
271
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 87?93,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
How Creative is Your Writing? A Linguistic Creativity Measure from
Computer Science and Cognitive Psychology Perspectives
Xiaojin Zhu, Zhiting Xu and Tushar Khot
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI, USA 53706
{jerryzhu, zhiting, tushar}@cs.wisc.edu
Abstract
We demonstrate that subjective creativity in
sentence-writing can in part be predicted us-
ing computable quantities studied in Com-
puter Science and Cognitive Psychology. We
introduce a task in which a writer is asked to
compose a sentence given a keyword. The
sentence is then assigned a subjective creativ-
ity score by human judges. We build a linear
regression model which, given the keyword
and the sentence, predicts the creativity score.
The model employs features on statistical lan-
guage models from a large corpus, psycholog-
ical word norms, and WordNet.
1 Introduction
One definition of creativity is ?the ability to tran-
scend traditional ideas, rules, patterns, relationships,
or the like, and to create meaningful new ideas,
forms, methods, interpretations, etc.? Therefore,
any computational measure of creativity needs to ad-
dress two aspects simultaneously:
1. The item to be measured has to be different
from other existing items. If one can model ex-
isting items with a statistical model, the new
item should be an ?outlier?.
2. The item has to be meaningful. An item con-
sists of random noise might well be an outlier,
but it is not of interest.
In this paper, we consider the task of measuring hu-
man creativity in composing a single sentence, when
the sentence is constrained by a given keyword. This
simple task is a first step towards automatically mea-
suring creativity in more complex natural language
text. To further simplify the task, we will focus on
the first aspect of creativity, i.e., quantifying how
novel the sentence is. The second aspect, how mean-
ingful the sentence is, requires the full power of Nat-
ural Language Processing, and is beyond the scope
of this initial work. This, of course, raises the con-
cern that we may regard a nonsense sentence as
highly creative. This is a valid concern. However,
in many applications where a creativity measure is
needed, the input sentences are indeed well-formed.
In such applications, our approach will be useful.
We will leave this issue to future work. The present
paper uses a data set (see the next section) in which
all sentences are well-formed.
A major difficulty in studying creativity is the
lack of an objective definition of creativity. Because
creative writing is highly subjective (?I don?t know
what is creativity, but I recognize it when I see one?),
we circumvent this problem by using human judg-
ment as the ground truth. Our experiment procedure
is the following. First, we give a keyword z to a
human writer, and ask her to compose a sentence
x about z. Then, the sentence x is evaluated by a
group of human judges who assign it a subjective
?creativity score? y. Finally, given a dataset con-
sisting of many such keyword-sentence-score triples
(z,x, y), we develop a statistical predictor f(x, z)
that predicts the score y from the sentence x and
keyword z.
There has been some prior attempts on charac-
terizing creativity from a computational perspec-
tive, for examples see (Ritchie, 2001; Ritchie, 2007;
87
Pease et al, 2001). The present work distinguishes
itself in the use of a statistical machine learning
framework, the design of candidate features, and its
empirical study.
2 The Creativity Data Set
We select 105 keywords from the English version of
the Leuven norms dataset (De Deyne and Storms,
2008b; De Deyne and Storms, 2008a). This ensures
that each keyword has their norms feature defined,
see Section 3.2. These are common English words.
The keywords are randomly distributed to 21 writ-
ers, each writer receives 5 keywords. Each writer
composes one sentence per keyword. These 5 key-
words are further randomly split into two groups:
1. The first group consists of 1 keyword. The
writers are instructed to ?write a not-so-creative
sentence? about the keyword. Two examples
are given: ?Iguana has legs? for ?Iguana?, and
?Anvil can get rusty? for ?Anvil.? The purpose
of this group is to establish a non-creative base-
line for the writers, so that they have a sense
what does not count as creative.
2. The second group consists of 4 keywords. The
writers are instructed to ?try to write a creative
sentence? about each keyword. They are also
told to write a sentence no matter what, even if
they cannot come up with a creative one. No
example is given to avoid biasing their creative
thinking.
In the next stage, all sentences are given to four
human judges, who are native English speakers. The
judges are not the writers nor the authors of this
paper. The order of the sentences are randomized.
The judges see the sentences and their correspond-
ing keywords, but not the identity of the writers,
nor which group the keywords are in. The judges
work independently. For each keyword-sentence
pair, each judge assigns a subjective creativity score
between 0 and 10, with 0 being not creative at all
(the judges are given the Iguana and Anvil exam-
ples for this), and 10 the most creative. The judges
are encouraged to use the full scale when scoring.
There is statistically significant (p < 10?8) linear
correlation among the four judges? scores, showing
their general agreement on subjective creativity. Ta-
ble 1 lists the pairwise linear correlation coefficient
between all four judges.
Table 1: The pairwise linear correlation coefficient be-
tween four judges? creativity scores given to the 105 sen-
tences. All correlations are statistically significant with
p < 10?8.
judge 2 judge 3 judge 4
judge 1 0.68 0.61 0.74
judge 2 0.55 0.74
judge 3 0.61
The scores from four judges on each sentence are
then averaged to produce a consensus score y. Ta-
ble 2 shows the top and bottom three sentences as
sorted by y.
As yet another sanity check, note that the judges
have no information which sentences are from group
1 (where the writers are instructed to be non-
creative), and which are from group 2. We would
expect that if both the writers and the judges share
some common notion of creativity, the mean score
of group 1 should be smaller than the mean score of
group 2. Figure 1 shows that this is indeed the case,
with the mean score of group 1 being 1.5? 0.6, and
that of group 2 being 5.1 ? 0.4 (95% confidence in-
terval). A t-test shows that this difference is signifi-
cant (p < 10?11).
1 20
2
4
6
group
sco
re
Figure 1: The mean creativity score for group 1 is signif-
icantly smaller than that for group 2. That is, the judges
feel that sentences in group 2 are more creative.
To summarize, in the end our dataset con-
sists of 105 keyword, sentence, creativity
score tuples {(zi,xi, yi)} for i = 1, . . . , 105.
The sentence group information is not in-
cluded. This ?Wisconsin Creative Writ-
ing? dataset is publicly available at http:
88
Table 2: Example sentences with the largest and smallest consensus creativity scores.
consensus score y keyword z sentence x
9.25 hamster She asked if I had any pets, so I told her I once did until I discovered
that I liked taste of hamster.
9.0 wasp The wasp is a dinosaur in the ant world.
8.5 dove Dove can still bring war by the information it carries.
...
0.25 guitar A Guitar has strings.
0.25 leech Leech lives in the water.
0.25 elephant Elephant is a mammal.
//pages.cs.wisc.edu/?jerryzhu/pub/
WisconsinCreativeWriting.txt.
3 Candidate Features for Predicting
Creativity
In this section, we discuss two families of candi-
date features we use in a statistical model to pre-
dict the creativity of a sentence. One family comes
from a Computer Science perspective, using large-
corpus statistics (how people write). The other fam-
ily comes from a Cognitive Psychology perspective,
specifically the word norms data and WordNet (how
people think).
3.1 The Computer Science Perspective:
Language Modeling
We start from the following hypothesis: if the words
in the sentence x frequently co-occur with the key-
word z, then x is probably not creative. This is of
course an over-simplification, as many creative sen-
tences are about novel usage of common words1.
Nonetheless, this hypothesis inspires some candi-
date features that can be computed from a large cor-
pus.
In this study, we use the Google Web 1T 5-
gram Corpus (Brants et al, 2007). This corpus
was generated from about 1012 word tokens from
Web pages. It consists of counts of N-gram for
N = 1, . . . , 5. We denote the words in a sentence
by x = x1, . . . , xn, where x1 = ?s? and xn = ?/s?
are special start- and end-of-sentence symbols. We
1For example, one might argue that Lincoln?s famous sen-
tence on government: ?of the people, by the people, for the
people? is creative, even though the keyword ?government? fre-
quently co-occurs with all the words in that sentence.
design the following candidate features:
[f1: Zero N-gram Fraction] Let c(xi+N?1i ) be
the count of the N-gram xi . . . xi+N?1 in the corpus.
Let ?(A) be the indicator function with value 1 if
the predicate A is true, and 0 otherwise. A ?Zero
N-gram Fraction? feature is the fraction of zero N-
gram counts in the sentence:
f1,N (x) =
?n?N+1
i=1 ?(c(xi+N?1i ) = 0)
n ? N + 1 . (1)
This provided us with 5 features, namely N-gram
zero count fractions for each value of N. These fea-
tures are a crude measure of how surprising the sen-
tence x is. A feature value of 1 indicates that none of
the N-grams in the sentence appeared in the Google
corpus, a rather surprising situation.
[f2: Per-Word Sentence Probability] This fea-
ture is the per-word log likelihood of the sentence,
to normalize for sentence length:
f2(x) = 1n log p(x). (2)
We use a 5-gram language model to estimate
p(x), with ?naive Jelinek-Mercer? smoothing. As
in Jelinek-Mercer smoothing (Jelinek and Mercer,
1980), it is a linear interpolation of N-gram language
models for N = 1 . . . 5. Let the Maximum Likeli-
hood (ML) estimate of a N-gram language model be
pNML(xi|xi?1i?N+1) =
c(xii?N+1)
c(xi?1i?N+1)
, (3)
which is the familiar frequency estimate of proba-
bility. The denominator is the count of the history
of length N ? 1, and the numerator is the count of
the history plus the word to be predicted. A 5-gram
89
Jelinek-Mercer smoothing language model on sen-
tence x is
p(x) =
n?
i=1
p(xi|xi?1i?5+1) (4)
p(xi|xi?1i?5+1) =
5?
N=1
?NPNML(xi|xi?1i?N+1),(5)
where the linear interpolation weights ?1 + . . . +
?5 = 1. The optimal values of ??s are a function of
history counts (binned into ?buckets?) c(xi?1i?N+1),
and should be optimized with convex optimiza-
tion from corpus. However, because our corpus is
large, and because we do not require precise lan-
guage modeling, we instead set the ??s in a heuris-
tic manner. Starting from N=5 to 1, ?N is set
to zero until the N where we have enough history
count for reliable estimate. Specifically, we require
c(xi?1i?N+1) > 1000. The first N that this happens
receives ?N = 0.9. The next lower order model
receives 0.9 fraction of the remaining weight, i.e.,
?N?1 = 0.9 ? (1 ? 0.9), and so on. Finally, ?1 re-
ceives all remaining weight to ensure ?1+. . .+?5 =
1. This heuristic captures the essence of Jelinek-
Mercer smoothing and is highly efficient, at the price
of suboptimal interpolation weights.
[f3: Per-Word Context Probability] The previ-
ous feature f2 ignores the fact that our sentence x
is composed around a given keyword z. Given that
the writer was prompted with the keyword z, we are
interested in the novelty of the sentence surround-
ing the keyword. Let xk be the first occurrence of
z in the sentence, and let x?k be the context of the
keyword, i.e., the sentence with the k-th word (the
keyword) removed. This notion of context novelty
can be captured by
p(x?k|xk = z) = p(x?k, xk = z)p(xk = z) =
p(x)
p(z) , (6)
where p(x) is estimated from the naive Jelinek-
Mercer 5-gram language model above, and p(z) is
estimated from a unigram language model. Our third
feature is the length-normalized log likelihood of the
context:
f3(x, z) = 1n ? 1 (log p(x) ? log p(z)) . (7)
3.2 The Cognitive Psychology Perspective:
Word Norms and WordNet
A text corpus like the one above captures how peo-
ple write sentences related to a keyword. However,
this can be different from how people think about re-
lated concepts in their head for the same keyword.
In fact, common sense knowledge is often under-
represented in a corpus ? for example, why bother
repeating ?A duck has a long neck? over and over
again? However, this lack of co-occurrence does not
necessarily make the duck sentence creative.
The way people think about concepts can in part
be captured by word norms experiments in psychol-
ogy. In such experiments, a human subject is pro-
vided with a keyword z, and is asked to write down
the first (or a few) word x that comes to mind.
When aggregated over multiple subjects on the same
keyword, the experiment provides an estimate of
the concept transition probability p(x|z). Given
enough keywords, one can construct a concept net-
work where the nodes are the keywords, and the
edges describe the transitions (Steyvers and Tenen-
baum, 2005). For our purpose, we posit that a sen-
tence x may not be creative with respect to a key-
word z, if many words in x can be readily retrieved
as the norms of keyword z. In a sense, the writer
was thinking the obvious.
[f4: Word Norms Fraction] We use the Leu-
ven dataset, which consists of norms for 1,424 key-
words (De Deyne and Storms, 2008b; De Deyne and
Storms, 2008a). The original Leuven dataset is in
Dutch, we use a version that is translated into En-
glish. For each sentence x, we first exclude the key-
word z from the sentence. We also remove punctu-
ations, and map all words to lower case. We further
remove all stopwords using the Snowball stopword
list (Porter, 2001), and stem all words in the sentence
and the norm word list using NLTK (Loper and Bird,
2002). We then count the number of words xi that
appear in the norm list of the keyword z in the Leu-
ven data. Let this count be cnorm(x, z). The feature
is the fraction of such norm words in the original
sentence:
f4(x, z) = cnorm(x, z)n . (8)
It is worth noting that the Leuven dataset is relatively
small, with less than two thousand keywords. This
90
is a common issue with psychology norms datasets,
as massive number of human subjects are difficult
to obtain. To scale our method up to handle large
vocabulary in the future, one possible method is to
automatically infer the norms of novel keywords us-
ing corpus statistics (e.g., distributional similarity).
[f5 ? f13: WordNet Similarity] WordNet is an-
other linguistic resource motivated by cognitive psy-
chology. For each sentence x, we compute Word-
Net 3.0 similarity between the keyword z and each
word xi in the sentence. Specifically, we use the
?path similarity? provided by NLTK (Loper and
Bird, 2002). Path similarity returns a score denot-
ing how similar two word senses are, based on the
shortest path that connects the senses in the hyper-
nym/hyponym taxonomy. The score is in the range
0 to 1, except in those cases where a path cannot be
found, in which case -1 is returned. A score of 1
represents identity, i.e., comparing a sense with it-
self. Let the similarities be s1 . . . sn. We experiment
with the following features: The mean, median, and
variance of similarities:
f5(x, z) = mean(s1 . . . sn) (9)
f6(x, z) = median(s1 . . . sn) (10)
f7(x, z) = var(s1 . . . sn). (11)
Features f8, . . . , f12 are the top five similarities.
When the length of the sentence is shorter than five,
we fill the remaining features with -1. Finally, fea-
ture f13 is the fraction of positive similarity:
f13(x, z) =
?n
i=1 ?(si > 0)
n . (12)
4 Regression Analysis on Creativity
With the candidate features introduced in Section 3,
we construct a linear regression model to predict the
creativity scores given a sentence and its keyword.
The first question one asks in regression analy-
sis is whether the features have a (linear) correlation
with the creativity score y. We compute the correla-
tion coefficient
?i = Cov(fi, y)?fi?y (13)
for each candidate feature fi separately on the first
row in Table 3. Some observations:
? The feature f4 (Word Norms Fraction) has the
largest correlation coefficient -0.48 in terms of
magnitude. That is, the more words in the sen-
tence that are also in the norms of the keyword,
the less creative the sentence is.
? The feature f12 (the 5-th WordNet similarity in
the sentence to the keyword) has a large posi-
tive coefficient 0.47. This is rather unexpected.
A closer inspection reveals that f12 equals -1
for about half of the sentences, and is around
0.05 for the other half. Furthermore, the second
half has on average higher creativity scores. Al-
though we hypothesized earlier that more simi-
lar words means lower creativity, this (together
with the positive ? for f10, f11) suggests the
other way around: more similar words are cor-
related with higher creativity.
? The feature f5 (mean WordNet similarity) has
a negative correlation with creativity. This fea-
ture is related to f12, but in a different direc-
tion. We speculate that this feature measures
the strength of similar words, while f12 indi-
rectly measures the number of similar words.
? The feature f3 (Per-Word Context Probability)
has a negative correlation with creativity. The
more predictable the sentence around the key-
word using a language model, the lower the
creativity.
Next, we build a linear regression model to pre-
dict creativity. We use stepwise regression, which
is a technique for feature selection by iteratively
including / excluding candidate features from the
regression model based on statistical significance
tests (Draper and Smith, 1998). The result is a lin-
ear regression model with a small number of salient
features. For the creativity dataset, the features (and
their regression coefficients) included by stepwise
regression are shown on the second row in Table 3.
The corresponding linear regression model is
y?(x, z) = ?5.06 ? f4 + 1.80 ? f12 ? 0.76 ? f3
?3.39 ? f5 + 0.92. (14)
A plot comparing y? and y is given in Figure 2. The
root mean squared error (RMSE) of this model is
91
Table 3: ?: The linear correlation coefficients between a candidate feature and the creativity score y. ?: The selected
features and their regression coefficients in stepwise linear regression.
f1,1 f1,2 f1,3 f1,4 f1,5 f2 f3 f4 f5
? 0.09 0.09 0.17 0.06 -0.04 -0.07 -0.32 -0.48 -0.41
? -0.76 -5.06 -3.39
f6 f7 f8 f9 f10 f11 f12 f13
? -0.19 -0.25 -0.02 0.06 0.23 0.30 0.47 -0.01
? 1.80
0 5 100
2
4
6
8
10
predicted score
tru
e s
cor
e
Figure 2: The creativity score y? as predicted by the linear
regression model in equation 14, compared to the true
score y. Each dot is a sentence.
1.51. In contrast, the constant predictor would have
RMSE 2.37 (i.e., the standard deviation of y).
We make two comments:
1. It is interesting to note that our intuitive fea-
tures are able to partially predict subjective cre-
ativity scores. On the other hand, we certainly
do not claim that our features or model solved
this difficult problem.
2. All three kinds of knowledge: corpus statistics
(f3), word norms (f4), and WordNet (f5, f12)
are included in the regression model. Coin-
cidentally, these features have the largest cor-
relation coefficients with the creativity score.
The fact that they are all included suggests that
these are not redundant features, and each cap-
tures some aspect of creativity.
5 Conclusions and Future Work
We presented a simplified creativity prediction task,
and showed that features derived from statistical
language modeling, word norms, and WordNet can
partially predict human judges? subjective creativity
scores.
Our problem setting is artificial, in that the cre-
ativity of the sentences are judged with respect to
their respective keywords, which are assumed to be
known beforehand. This allows us to design features
centered around the keywords. We hope our analysis
can be extended to the setting where the only input is
the sentence, without the keyword. This can poten-
tially be achieved by performing keyword extraction
on the sentence first, and apply our analysis on the
extracted keyword.
As discussed in the introduction, our analysis
is susceptible to nonsense input sentences, which
could be predicted as highly creative. Combining
our analysis with a ?sensibility analysis? is an im-
portant future direction.
Finally, our model might be adapted to explain
why a sentence is deemed creative, by analyzing the
contribution of individual features in the model.
6 Acknowledgments
We thank the anonymous reviewers for suggestions
on related work and other helpful comments, and
Chuck Dyer, Andrew Goldberg, Jake Rosin, and
Steve Yazicioglu for assisting the project. This work
is supported in part by the Wisconsin Alumni Re-
search Foundation.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In EMNLP.
S. De Deyne and G Storms. 2008a. Word associations:
Network and semantic properties. Behavior Research
Methods, 40:213?231.
S. De Deyne and G Storms. 2008b. Word associations:
Norms for 1,424 Dutch words in a continuous task.
Behavior Research Methods, 40:198?205.
92
Norman R. Draper and Harry Smith. 1998. Applied
Regression Analysis (Wiley Series in Probability and
Statistics). John Wiley & Sons Inc, third edition.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Workshop on Pattern Recognition in
Practice.
Edward Loper and Steven Bird. 2002. NLTK: The nat-
ural language toolkit. In The ACL Workshop on Ef-
fective Tools and Methodologies for Teaching Natural
Language Processing and Computational Linguistics,
pages 62?69.
Alison Pease, Daniel Winterstein, and Simon Colton.
2001. Evaluating machine creativity. In Workshop
on Creative Systems, 4th International Conference on
Case Based Reasoning, pages 129?137.
Martin F. Porter. 2001. Snowball: A language for stem-
ming algorithms. Published online.
Graeme Ritchie. 2001. Assessing creativity. In Pro-
ceedings of the AISB01 Symposium on Artificial Intel-
ligence and Creativity in Arts and Science, pages 3?11.
Graeme Ritchie. 2007. Some empirical criteria for at-
tributing creativity to a computer program. Minds and
Machines, 17(1):67?99.
Mark Steyvers and Joshua Tenenbaum. 2005. The large
scale structure of semantic networks: Statistical anal-
yses and a model of semantic growth. Cognitive Sci-
ence, 29(1):41?78.
93
  
CRF-based Hybrid Model for Word Segmentation, NER and even 
POS Tagging 
Zhiting Xu, Xian Qian, Yuejie Zhang,  Yaqian Zhou 
Department of Computer Science & Engineering, 
Shanghai Key Laboratory of Intelligent Information Processing, 
Fudan University, Shanghai 200433, P. R. China 
 {zhiting, qianxian, yjzhang, zhouyaqian}@fudan.edu.cn 
 
  
Abstract 
This paper presents systems submitted to 
the close track of Fourth SIGHAN Bakeoff. 
We built up three systems based on Condi-
tional Random Field for Chinese Word 
Segmentation, Named Entity Recognition 
and Part-Of-Speech Tagging respectively. 
Our systems employed basic features as 
well as a large number of linguistic features. 
For segmentation task, we adjusted the BIO 
tags according to confidence of each char-
acter. Our final system achieve a F-score of 
94.18 at CTB, 92.86 at NCC, 94.59 at SXU 
on Segmentation, 85.26 at MSRA on 
Named Entity Recognition, and 90.65 at 
PKU on Part-Of-Speech Tagging. 
1 Introduction 
Fourth SIGHAN Bakeoff includes three tasks, that 
is, Word Segmentation, Named Entity Recognition 
(NER) and Part-Of-Speech (POS) Tagging. In the 
POS Tagging task, the testing corpora are pre-
segmented. Word Segmentation, NER and POS 
Tagging could be viewed as classification prob-
lems. In a Segmentation task, each character 
should be classified into three classes, B, I, O, in-
dicating whether this character is the Beginning of 
a word, In a word or Out of a word. For NER, each 
character is assigned a tag indicating what kind of 
Named Entity (NE) this character is (Beginning of 
a Person Name (PN), In a PN, Beginning of a Lo-
cation Name (LN), In a LN, Beginning of an Or-
ganization Name (ON), In an ON or not-a-NE). In 
POS tagging task defined by Fourth SIGHAN Ba-
keoff, we only need to give a POS tag for each 
given word in a context. 
We attended the close track of CTB, NCC, SXU 
on Segmentation, MSRA on NER and PKU on 
POS Tagging. In the close track, we cannot use 
any external resource, and thus we extracted sev-
eral word lists from training corpora to form multi-
ple features beside basic features. Then we trained 
CRF models based on these feature sets. In CRF 
models, a margin of each character can be gotten, 
and the margin could be considered as the confi-
dence of that character. For the Segmentation task, 
we performed the Maximum Probability Segmen-
tation first, through which each character is as-
signed a BIO tag (B represents the Beginning of a 
word, I represents In a word and O represents Out 
of a word). If the confidence of a character is lower 
than the threshold, the tag of that character will be 
adjusted to the tag assigned by the Maximum 
Probability Segmentation (R. Zhang et al, 2006). 
2 Conditional Random Fields 
Conditional Random Fields (CRFs) are a class of 
undirected graphical models with exponent distri-
bution (Lafferty et al, 2001). A common used spe-
cial case of CRFs is linear chain, which has a dis-
tribution of: 
)),,,(exp(1)|(
1
1??
=
?? =
T
t k
ttkk
x
txyyf
Z
xyP rrr
r
?  (1) 
where ),,( 1 txyyf ttk
r
? is a function which is usu-
ally an indicator function; k?  is the learned weight 
of feature kf ; and xZ r is the normalization factor. 
The feature function actually consists of two kinds 
of features, that is, the feature of single state and 
the feature of transferring between states. Features 
will be discussed in section 3. 
167
Sixth SIGHAN Workshop on Chinese Language Processing
  
Several methods (e.g. GIS, IIS, L-BFGS) could 
be used to estimate k? , and L-BFGS has been 
showed to converge faster than GIS and IIS. To 
build up our system, we used Pocket CRF1. 
3 Feature Representation 
We used three feature sets for three tasks respec-
tively, and will describe them respectively. 
3.1 Word Segmentation 
We mainly adopted features from (H. T. Ng et al, 
2004, Y. Shi et al, 2007), as following: 
a) Cn(n=-2, -1, 0, 1, 2) 
b) CnCn+1(n=-2,-1,0,1) 
c) C-1C1 
d) CnCn+1Cn+2 (n=-1, 0, 1) 
e) Pu(C0) 
f) T(C-2)T(C-1)T(C0)T(C1)T(C2) 
g) LBegin(C0), Lend(C0) 
h) Single(C0) 
where C0 represents the current character and Cn 
represents the nst character from the current charac-
ter. Pu(C0) indicates whether current word is a 
punctuation. this feature template helps to indicate 
the end of a sentence. T(C) represents the type of 
character C. There are four types we used: (1) Chi-
nese Number (??/one?, ??/two?, ??/ten?); (2) 
Chinese Dates (??/day?, ??/month?, ??/year?); 
(3) English letters; and (4) other characters. The (f) 
feature template is used to recognize the Chinese 
dates for the construction of Chinese dates may 
cause the sparseness problem. LBegin(C0) represents 
the maximum length of the word beginning with 
the character C0, and Lend(C0) presents the maxi-
mum length of the word ending with the character 
C0. The (g) feature template is used to decide the 
boundary of a word. Single(C0) shows whether cur-
rent character can form a word solely. 
3.2 Named Entity Recognition 
Most features described in (Y. Wu et al, 2005) are 
used in our systems. Specifically, the following is 
the feature templates we used: 
a) Surname(C0): Whether current character is in 
a Surname List, which includes all first char-
acters of PNs in the training corpora. 
                                                 
1 
http://sourceforge.net/project/showfiles.php?group_id=201943 
b) PersonName(C0C1C2, C0C1): Whether C0C1C2, 
C0C1 is in the Person Name List, which con-
tains all PNs in the training corpora. 
c) PersonTitle(C-2C-1): Whether C-2C-1 is in the 
Person Title List, which is extracted from the 
previous two characters of each PN in the 
training corpora. 
d) LocationName(C0C1,C0C1C2,C0C1C2C3): 
Whether C0C1,C0C1C2,C0C1C2C3 is in the Lo-
cation Name List, which includes all LNs in 
the training corpora. 
e) LocationSuffix(C0): Whether current character 
is in the Location Suffix List, which is con-
structed using the last character of each LN in 
the training corpora. 
f) OrgSuffix(C0): Whether current character is in 
the Organization Suffix List, which contains 
the last-two-character of each ON in the train-
ing corpora. 
3.3 Part-Of-Speech Tagging 
We employed part of feature templates described 
in (H. T. Ng et al, 2004, Y. Shi et al, 2007). Since 
we are in the close track, we cannot use morpho-
logical features from external resources such as 
HowNet, and we used features that are available 
just from the training corpora. 
a) Wn, (n=-2,-1,0,1,2) 
b) WnWn+1, (n=-2,-1,0,1) 
c) W-1W1 
d) Wn-1WnWn+1 (n=-1, 1) 
e) Cn(W0) (n=0,1,2,3) 
f) Length(W0) 
where Cn represents the nth character of the current 
word, and Length(W0) indicates the length of the 
current word. 
4 Reliability Evaluation 
In the task of Word Segmentation, the label of each 
character is adjusted according to their reliability. 
For each sentence, we perform Maximum Prob-
ability Segmentation first, through which we can 
get a BIO tagging for each character in the sen-
tence. 
After that, the features are extracted according 
to the feature templates, and the weight of each 
feature has already been estimated in the step of 
training. Then marginal probability for each char-
acter can be computed as follows: 
168
Sixth SIGHAN Workshop on Chinese Language Processing
  
)),(exp(
)(
1)|( yxf
xZ
xyp ii
rr ?=     (2) 
The value of )|( xyp
r
 becomes the original re-
liability value of BIO label y for the current char-
acter under the current contexts. If the probability 
of y  with the largest probability is lower than 0.75, 
which is decided according to the experiment re-
sults, the tag given by Maximum Probability Seg-
mentation will be used instead of tag given by CRF. 
The motivation of this method is to use the Maxi-
mum Probability method to enhance the F-measure 
of In-Vocabulary (IV) Words. According to the 
results reported in (R. Zhang et al, 2006), CRF 
performs relatively better on Out-of-Vocabulary 
(OOV) words while Maximum Probability per-
forms well on IV words, so a model combining the 
advantages of these two methods is appealing. One 
simplest way to combine them is the method we 
described. Besides, there are some complex meth-
ods, such as estimation using Support Vector Ma-
chine (SVM) for CRF, CRF combining boosting 
and combining Margin Infused Relaxed Algorithm 
(MIRA) with CRF, that might perform better. 
However, we did not have enough time to imple-
ment these methods, and we will compare them 
detailedly in the future work. 
5 Experiments 
5.1 Results on Fourth SIGHAN Bakeoff 
We participated in the close track on Word Seg-
mentation on CTB, NCC and SXU corpora, NER 
on MSRA corpora and POS Tagging on PKU cor-
pora. 
For Word Segmentation and NER, our memory 
was enough to use all features. However, for POS 
tagging, we did not have enough memory to use all 
features, and we set a frequency cutoff of 10; that 
is, we could only estimate variables for those fea-
tures that occurred more than ten times. 
Our results of Segmentation are listed in the Ta-
bel 1, the results of NER are listed in the Tabel 2, 
and the results of POS Tagging are listed in the 
Tabel 3. 
 R P F Roov Riv 
CTB 0.9459 0.9418 0.9439 0.6589 0.9628 
NCC 0.9396 0.9286 0.9341 0.5007 0.9614 
SXU 0.9554 0.9459 0.9507 0.6206 0.9735 
Tabel 1. Results of Word Segmentation 
MSRA P R F 
PER 0.8084 0.8557 0.8314 
LOC 0.9138 0.8576 0.8848 
ORG 0.8666 0.773 0.8171 
Overall 0.873 0.8331 0.8526 
Tabel 2. Results of NER 
 
 Total-A IV-R OOV-R MT-R 
PKU 0.9065 0.9259 0.5836 0.8903 
Tabel 3. Results of POS Tagging 
5.2 Errors Analysis 
Observing our results of Word Segmentation and 
POS Tagging, we found that the recall of OOV is 
relatively low, this may be improved through in-
troducing features aiming to enhance the perform-
ance of OOV.  
On NER task, we noticed that precision of PN 
recognition is relative low, and we found that our 
system may classify some ONs as PNs, such as ??
??(Guinness)/ORG? and ?????(World Re-
cord)/)?. Besides, the bound of PN is sometimes 
confusing and may cause problems. For example, 
???/PER ?/ ?/ ??? may be segmented as 
????/PER ?/ ???. Further, some words be-
ginning with Chinese surname, such as ????
??, may be classified as PN.  
For List may not be the real suffix. For example, 
?????? should be a LN, but it is very likely 
that ????? is recognized as a LN for its suffix 
???.  Another problem involves the characters in 
the Location Name list may not a LN all the time. 
In the context ???/ ??/?, for example, ??? 
means Chinese rather than China.  
For ONs, the correlative dictionary also exists. 
Consider sequence ??????, which should be a 
single word, ???? is in the Organization Name 
List and thus it is recognized as an ON in our sys-
tem. Another involves the subsequence of a word. 
For example, the sequence ?????????
??, which should be a person title, but ?????
????? is an ON. Besides, our recall of ON is 
low for the length of an ON could be very long. 
6 Conclusions and Future Works 
We built up our systems based on the CRF model 
and employed multiple linguistics features based 
on the knowledge extracted from training corpora. 
169
Sixth SIGHAN Workshop on Chinese Language Processing
  
We found that these features could greatly improve 
the performance of all tasks. Besides, we adjusted 
the tag of segmentation result according to the reli-
ability of each character, which also helped to en-
hance the performance of segmentation.  
As many other NLP applications, feature plays a 
very important role in sequential labeling tasks. In 
our POS tagging task, we could only use features 
with high frequency, but some low-frequency fea-
tures may also play a vital role in the task; good 
non-redundant features could greatly improve clas-
sification performance while save memory re-
quirement of classifiers. In our further research, we 
will focus on feature selection on CRFs. 
Acknowledgement 
This research was sponsored by National Natural 
Science Foundation of China (No. 60773124, No. 
60503070). 
References 
O. Bender, F. J. Och, and H. Ney. 2003. Maximum En-
tropy Models for Named Entity Recognition. Pro-
ceeding of CoNLL-2003. 
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 
1996. A Maximum Entropy Approach to Natural 
Language Processing. Computational Linguistics, 
22(1). 
H. L. Chieu, H. T. Ng. 2002. Named Entity Recognition: 
A Maximum Entropy Approach Using Global Infor-
mation. International Conference on Computational 
Linguistics (COLING). 
J. N. Darroch and D. Ratcliff. 1972. Generalized Itera-
tive Scaling for Log-Linear Models. The Annals of 
Mathematical Statistics, 43(5). 
J. Lafferty, A McCallum, and F. Pereira..2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of the 18th International Conf. on Machine 
Learning (ICML). 
R. Li, J. Wang, X. Chen, X. Tao, and Y. Hu. 2004. Us-
ing Maximum Entropy Model for Chinese Text 
Categorization. Computer Research and Develop-
ment, 41(4). 
H. T. Ng and J. K. Low. 2004. Chinese Part-Of-Speech 
Tagging: One-at-a-Time or All-at-Once? Word-Base 
or Character-Based? Proceedings of Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP). 
A. Ratnaparkhi. 1997. A Simple Introduction to Maxi-
mum Entropy Models for Natural Language Process-
ing. Institute for Research in Cognitive Science Re-
port, 97(8). 
F. Sha and F.Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL. 
Y. Shi and M. Wang. 2007. A Dual-Layer CRFs Based 
Joint Decoding Method for Cascaded Segmentation 
and Labeling Tasks. In International Joint Confer-
ences on Artificial Intelligence (IJCAI). 
C. A. Sutton, K. Rohanimanesh, A. McCallum. 2004. 
Dynamic conditional random fields: factorized prob-
abilistic models for labeling and segmenting se-
quence data. In International Conference on Machine 
Learning (ICML). 
M. Volk, and S. Clematide. 2001. Learn - Filter - Apply 
-- Forget Mixed Approaches to Named Entity Rec-
ognition. Proceeding of the 6th International Work-
shop on Applications of Natural Language for Infor-
mation Systems. 
Y. Wu, J. Zhao, B. Xu and H. Yu. 2005. Chinese 
Named Entity Recognition Based on Multiple Fea-
tures. Proceedings of Human Language Technology 
Conference and Conference on Empirical Methods in 
Natural Language Processing (HLT/EMNLP). 
H. Zhang, Q. Liu, H. Zhang, and X. Cheng. 2002. Au-
tomatic Recognition of Chinese Unknown Words 
Based on Roles Tagging. Proceeding of the 19th In-
ternational Conference on Computational Linguistics. 
R. Zhang, G. Kikui and E. Sumita. 2006. Subword-
based tagging by conditional random fields for Chi-
neseword segmentation. Companion volume to the-
proceedings of the North American chapter of the 
Association for Computational Linguistics (NAACL). 
Y. Zhou, Y. Guo, X. Huang, and L. Wu. 2003. Chinese 
and English BaseNP Recognition Based on a Maxi-
mum Entropy Model. Journal of Computer Research 
and Development, 40(3). 
 
170
Sixth SIGHAN Workshop on Chinese Language Processing
