Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 193?202, Dublin, Ireland, August 23-29 2014.
A Study of using Syntactic and Semantic Structures
for Concept Segmentation and Labeling
Iman Saleh
?
, Shafiq Joty, Llu??s M
`
arquez,
Alessandro Moschitti, Preslav Nakov
ALT Research Group
Qatar Computing Research Institute
{sjoty,lmarquez,amoschitti,pnakov}
@qf.org.qa
Scott Cyphers, Jim Glass
MIT CSAIL
Cambridge, Massachusetts 02139
USA
{cyphers,glass}@mit.edu
Abstract
This paper presents an empirical study on using syntactic and semantic information for Concept
Segmentation and Labeling (CSL), a well-known component in spoken language understand-
ing. Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser. We
perform extensive experimentation by comparing different tree-based kernels with a variety of
representations of the available linguistic information, including semantic concepts, words, POS
tags, shallow and full syntax, and discourse trees. The results show that the structured representa-
tion with the semantic concepts yields significant improvement over the base CSL parser, much
larger compared to learning with an explicit feature vector representation. We also show that
shallow syntax helps improve the results and that discourse relations can be partially beneficial.
1 Introduction
Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms,
or, equivalently, database queries, which can then be used to satisfy the user?s information needs. This
process is known as Concept Segmentation and Labeling (CSL): it maps utterances into meaning repre-
sentations based on semantic constituents. The latter are basically sequences of semantic entities, often
referred to as concepts, attributes or semantic tags. Traditionally, grammar-based methods have been
used for CSL, but more recently machine learning approaches to semantic structure computation have
been shown to yield higher accuracy. However, most previous work did not exploit syntactic/semantic
structures of the utterances, and the state-of-the-art is represented by conditional models for sequence la-
beling, such as Conditional Random Fields (Lafferty et al., 2001) trained with simple morphological and
lexical features. In our study, we measure the impact of syntactic and discourse structures by also com-
bining them with innovative features. In the following subsections, we present the application context
for our CSL task and then we outline the challenges and the findings of our research.
1.1 Semantic parsing for the ?restaurant? domain
We experiment with the dataset of McGraw et al. (2012), containing spoken and typed questions about
restaurants, which are to be answered using a database of free text such as reviews, categorical data such
as names and locations, and semi-categorical data such as user-reported cuisines and amenities.
Semantic parsing, in the form of sequential segmentation and labeling, makes it easy to convert spoken
and typed questions such as ?cheap lebanese restaurants in doha with take out? into database queries.
First, a language-specific semantic parser tokenizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in] [
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to the segments, with the option to possibly relabel mis-
labeled segments; at this point, discourse history may be incorporated as well.
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
?
Iman Saleh (iman.saleh@fci-cu.edu.eg) is affiliated to Faculty of Computers and Information, Cairo University.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
193
Finally, a database query is formed from the list of labels and values, and is then executed against the
database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed.
{$and [{cuisine:"lebanese"}, {city:"doha"}, {price:"low"}, {amenity:"carry out"}]}
1.2 Related work on CSL
Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were
word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein
and Hastie, 1997; Santaf?e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other genera-
tive models were applied, which model the joint probability of a word sequence and a concept sequence,
as well as discriminative models, which directly model a conditional probability over the concepts in the
input text.
Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied
stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local
syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al.
(1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty
et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an
approach for CSL that is specific to query understanding for web applications. A general survey of CSL
approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on
shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview.
Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels
for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used
explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins,
2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al.,
2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL
hypotheses using structures built on top of concepts, words and features that are simpler than those
studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar
to ours, as it models the extraction of semantics as a reranking task using string kernels.
1.3 Syntactic and semantic structures for CSL
The related work has highlighted that automatic CSL is mostly based on powerful machine learning al-
gorithms and simple feature representations based on word and tag n-grams. In this paper, we study the
impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and
discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts
derived by a local model, where the hypotheses are represented as trees enriched with semantic con-
cepts similarly to (Dinarelli et al., 2011). These tree-based structures can capture dependencies between
sentence constituents and concepts. However, extracting features from them is rather difficult as their
number is exponentially large. Thus, we rely on structural kernels (e.g., see (Moschitti, 2006)) for au-
tomatically encoding tree fragments, which represent syntactic and semantic dependencies from words
and concepts, and we train the reranking functions with Support Vector Machines (e.g., see (Joachims,
1999)). Additionally, we experiment with several types of kernels and newly designed feature vectors.
We test our models on the above-mentioned Restaurant domain. The results show that (i) the basic
CRF model, in fact semi-CRF (see below), is very accurate, achieving more than 83% in F
1
-score, which
indicates that improving over the semi-CRF approach is very hard; (ii) the upper-bound performance
of the reranking approach is very high as well, i.e., the correct annotation is generated in the first 100
hypotheses in 98.72% of the cases; (iii) our feature vectors show improvement only when all feature
groups are used together; otherwise, we only observe marginal improvement; (iv) structural kernels yield
a 10% relative error reduction from the semi-CRF baseline, which is more than double the feature vector
result; (v) syntactic information significantly improves on the best model, but only when using shallow
syntax; and finally, (vi) although, discourse structures provide good improvement over the semi-CRF
model, they perform lower than shallow syntax (thus, a valuable use of discourse features is still an open
problem that we plan to pursue in future work).
194
2 CSL reranking
Reranking is based on a list of N annotation hypotheses, which are generated and sorted by probability
using local classifiers. Then a reranker, typically a meta-classifier, tries to select the best hypothesis from
the list. The reranker can exploit global information, and, specifically, the dependencies between the
different concepts that are made available by the local model. We use semi-CRF as our local model since
it yields the highest accuracy in CSL (when using a single model), and preference reranking with kernel
machines to rerank the N hypotheses generated by the semi-CRF.
2.1 Basic parser using semi-CRF
We use a semi-Markov CRF (Sarawagi and Cohen, 2004), or semi-CRF, a variation of a linear-chain
CRF (Lafferty et al., 2001), to produce the N -best list of labeled segment hypotheses that serve as the
input to reranking. In a linear-chain CRF, with a sequence of tokens x and labels y, we approximate
p(y|x) as a product of factors of the form p(y
i
|y
i?1
, x), which corresponds to features of the form
f
j
(y
i?1
, y
i
, i, x), where i iterates over the token/label positions. This supports a Viterbi search for the
approximateN best values of y. WithM label values, if for each label y
m
we know the bestN sequences
of labels y
1
, y
2
, . . . , y
i?1
= y
m
, then we can use p(y
i
|y
i?1
, x) to get the probability for extending each
path by each possible label y
i
= y
?
m
. Then for each label y
?
m
, we will have MN paths and scores, one
from each of the paths of length i? 1 ending with y
m
. For each y
?
m
, we pick the N best extended paths.
With semi-CRF, we want a labeled segmentation s rather than a sequence of labels. Each segment
s
i
= (y
i
, t
i
, u
i
) has a label y
i
as well as a starting and ending token position for the segment, t
i
and
u
i
respectively, where u
i
+ 1 = t
i+1
. We approximate p(s|x), with factors of the form p(s
i
|s
i?1
, x),
which we simplify to p(y
i
, u
i
|y
i?1
, t
i
), so features take the form f
j
(y
i?1
, y
i
, t
i
, u
i
), i.e., they can use the
previous segment?s label and the current segment?s label and endpoints. The Viterbi search is extended
to search for a pair of label and segment end. Whereas for M labels we kept track of MN paths, we
must keep track of MLN paths, where L is the maximum segment length.
We use token n-gram features relative to the segment boundaries, n-grams within the segment, token
regular expression and lexicon features within a segment. Each of these features also includes the labels
of the previous and current segment, and the segment length.
2.2 Preference reranking with kernel machines
Preference reranking (PR) uses a classifier C of pairs of hypotheses ?H
i
, H
j
?, which decides if H
i
is
better thanH
j
. Given each training question Q, positive and negative examples are generated for training
the classifier. We adopt the following approach for example generation: the pairs ?H
1
, H
i
? constitute
positive examples, where H
1
has the lowest error rate with respect to the gold standard among the
hypotheses for Q, and vice versa, ?H
i
, H
1
? are considered as negative examples. At testing time, given
a new question Q
?
, C classifies all pairs ?H
i
, H
j
? generated from the annotation hypotheses of Q
?
: a
positive classification is a vote for H
i
, otherwise the vote is for H
j
. Also, the classifier score can be used
as a weighted vote. H
k
are then ranked according to the number (sum) of the (weighted) votes they get.
We build our reranker with kernel machines. The latter, e.g., SVMs, classify an input object o using
the following function: C(o) =
?
i
?
i
y
i
K(o, o
i
), where ?
i
are model parameters estimated from the
training data, o
i
are support objects and y
i
are the labels of the support objects. K(?, ?) is a kernel
function, which computes the scalar product between the two objects in an implicit vector space. In the
case of the reranker, the objects o are ?H
i
, H
j
?, and the kernel is defined as follow:
K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) = S(H
1
, H
?
1
) + S(H
2
, H
?
2
)? S(H
1
, H
?
2
)? S(H
2
, H
?
1
).
Our reranker also includes traditional feature vectors in addition to the trees. Therefore, we define each
hypothesis H as a tuple ?T,~v? composed of a tree T and a feature vector ~v. We then define a structural
kernel (similarity) between two hypotheses H and H
?
as follows: S(H,H
?
) = S
TK
(T, T
?
) + S
v
(~v,~v
?
),
where S
TK
is one of the tree kernel functions defined in Section 3.1, and S
v
is a kernel over feature
vectors (see Section 3.3), e.g., linear, polynomial, gaussian, etc.
195
(a) Basic Tree (BT). (b) Discourse Tree (DT).
(c) Shallow Syntactic Tree (ShT).
(d) Syntactic Tree (ST).
(e) BT with POS (BTP).
Figure 1: Syntactic/semantic trees. The numeric semantic tagset is defined in Table 7.
3 Structural kernels for semantic parsing
In this section, we briefly describe the kernels we use in S(H,H
?
) for preference reranking. We engineer
them by combining three aspects: (i) different types of existing tree kernels, (ii) new syntactic/semantic
structures for representing CSL, and (iii) new feature vectors.
3.1 Tree kernels
Structural kernels, e.g., tree and sequence kernels, measure the similarity between two structures in terms
of their shared substructures. One interesting aspect is that these kernels correspond to a scalar product
in the fragment space, where each substructure is a feature. Therefore, they can be used in the training
and testing algorithms of kernel machines (see Section 2.2). Below, we briefly describe different types of
kernels we tested in our study, which are made available in the SVM-Light-TK toolkit (Moschitti, 2006).
Subtree Kernel (K0) is one of the simplest tree kernels, as it only generates complete subtrees, i.e., tree
fragments that, given any arbitrary starting node, necessarily include all its descendants.
Syntactic Tree Kernel (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob-
jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannot
be separated from their parents. In other words, substructures are composed of atomic building blocks
corresponding to nodes, along with all of their direct children. In the case of a syntactic parse tree, these
are complete production rules for the associated parser grammar.
Syntactic Tree Kernel + BOW (K2) extends ST by allowing leaf nodes to be part of the feature space.
The leaves of the trees correspond to words, i.e., we allow bag-of-words (BOW).
Partial Tree Kernel (K3) can be effectively applied to both constituency and dependency parse trees.
It generates all possible connected tree fragments, e.g., sibling nodes can be also separated and be part
of different tree fragments. In other words, a fragment is any possible tree path from whose nodes other
tree paths can depart. Thus, it can generate a very rich feature space.
Sequence Kernel (K4) is the traditional string kernel applied to the words of a sentence. In our case, we
apply it to the sequence of concepts.
3.2 Semantic/syntactic structures
As mentioned before, tree kernels allow us to compute structural similarities between two trees without
explicitly representing them as feature vectors. For the CSL task, we experimented with a number of tree
representations that incorporate different levels of syntactic and semantic information.
To capture the structural dependencies between the semantic tags, we use a basic tree (Figure 1a)
where the words of a sentence are tagged with their semantic tags. More specifically, the words in the
sentence constitute the leaves of the tree, which are in turn connected to the pre-terminals containing the
semantic tags in BIO notation (?B?=begin, ?I?=inside, ?O?=outside). The BIO tags are then generalized
in the upper level, and so on. The basic tree does not include any syntactic information.
196
However, part-of-speech (POS) and phrasal information could be informative for both segmentation
and labeling in semantic parsing. To incorporate this information, we use two extensions of the basic
tree: one that includes the POS tags of the words (Figure 1e), and another one that includes both POS
tags and syntactic chunks (Figure 1c). The POS tags are children of the semantic tags, whereas the
chunks (i.e., phrasal information) are included as parents of the semantic tags.
We also experiment with full syntactic trees (Figure 1d) to see the impact of deep syntactic informa-
tion. The semantic tags are attached to the pre-terminals (i.e., POS tags) in the syntactic tree. We use the
Stanford POS tagger and syntactic parser and the Twitter NLP tool
1
for the shallow trees.
A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example,
the first clause ?along my route tell me the next steak house? is elaborated by the second clause ?that is
within a mile?. The relations by which clauses in a text are linked are called coherence relations (e.g.,
Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide addi-
tional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse
structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates
discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson,
1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a
nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is.
3.3 New features
In order to compare to the structured representation, we also devoted significant effort towards engineer-
ing a set of features to be used in a flat feature-vector representation; they can be used in isolation or in
combination with the kernel-based approach (as a composite kernel using a linear combination):
CRF-based: these include the basic features used to train the initial semi-CRF model (cf. Section 2.1).
n-gram based: we collected 3- and 4-grams of the output label sequence at the level of concepts, with
artificial tags inserted to identify the start (?S?) and end (?E?) of the sequence.
2
Probability-based: two features computing the probability of the label sequence as an average of the
probabilities at the word level p(l
i
|w
i
) (i.e., assuming independence between words). The unigram prob-
abilities are estimated by frequency counts using maximum likelihood in two ways: (i) from the complete
100-best list of hypotheses; (ii) from the training set (according to the gold standard annotation).
DB-based: a single feature encoding the number of results returned from the database when constructing
a query using the conjunction of all semantic segments in the hypothesis. Three possible values are
considered by using a threshold t: 0 (if the query result is void), 1 (if the number of results is in [1, t]),
and 2 (if the number of results is greater than t). In our case, t is empirically set to 10,000.
4 Experiments
The experiments aim at investigating which structures, and thus which linguistic models and combination
with other models, are the most appropriate for our reranker. We first calculate the oracle accuracy in
order to compute an upper bound of the reranker. Then we present experiments with the feature vectors,
tree kernels, and representations of linguistic information introduced in the previous sections.
4.1 Experimental setup
In our experiments, we use questions annotated with semantic tags in the restaurant domain,
3
which were
collected by McGraw et al. (2012) through crowdsourcing on Amazon Mechanical Turk.
4
We split the
dataset into training, development and test sets. Table 1 shows statistics about the dataset and about the
size of the parts we used for training, development and testing (see the semi-CRF line).
We subsequently split the training data randomly into ten folds. We generated the N -best lists on
the training set in a cross-validation fashion, i.e., iteratively training on nine folds and annotating the
remaining fold. We computed the 100-best hypotheses for each example.
1
Available from http://nlp.stanford.edu/software/index.shtml and https://github.com/aritter/twitter nlp, respectively.
2
For instance, if the output sequence is Other-Rating-Other-Amenity the 3-gram patterns would be: S-Other-Rating, Other-
Rating-Other, Rating-Other-Amenity, and Other-Amenity-E.
3
http://www.sls.csail.mit.edu/downloads/restaurant
4
We could not use the datasets used by Dinarelli et al. (2011), because they use French and Italian corpora for which there
are no reliable syntactic and discourse parsers.
197
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 28,482 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
-score for N -best lists
of different lengths.
We used the development set to experiment and tune the hyper-parameters of the reranking model. The
results on the development set presented in Section 4.2 were obtained by semi-CRF and reranking models
learned on the training set. The results on the test set were obtained by models trained on the training
plus development sets. Similarly, the N -best lists for the development and test sets were generated using
a single semi-CRF model trained on the training set and the training+development sets, respectively.
Each generated hypothesis is represented using a semantic tree and a feature vector (explained in
Section 3) and two extra features accounting for (i) the semi-CRF probability of the hypothesis, and
(ii) the hypothesis reciprocal rank in the N -best list. SVM-Light-TK
5
is used to train the reranker with
a combination of tree kernels and feature vectors (Moschitti, 2006; Joachims, 1999). Although we
tried several parameters on the validation set, we observed that the default values yielded the highest
results. Thus, we used the default c (trade-off) and tree kernel parameters and a linear kernel for the
feature vectors. Table 1 shows the sizes of the train, the development and the test sets used for the
semi-CRF as well as the number of pairs generated for the reranker. As a baseline, we picked the best-
scored hypothesis in the list, according to the semi-CRF tagger. The evaluation measure used in all
the experiments is the harmonic mean of precision and recall, i.e., the F
1
-score (van Rijsbergen, 1979),
computed at the token level and micro-averaged over the different semantic types.
6
We used paired t-test
to measure the statistical significance of the improvements: we split the test set into 31 equally-sized
samples and performed t-tests based on the F
1
-scores of different models on the resulting samples.
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
-score for N -best lists of different lengths, i.e., which
can be achieved by picking the best candidate of the N -best list for various values of N . We can see that
going to 5-best increases the oracle F
1
-score by almost ten points absolute. Going down to 10-best only
adds 2.5 extra F
1
points absolute, and a 100-best list adds 3.5 F
1
points more to yield a respectable F
1
-
score of 98.72. This high result can be explained considering that the size of the complete hypothesis set
is smaller than 100 for most questions. Thus, we can conclude that theN -best lists do include many good
options and do offer quite a large space for potential improvement. We can further observe that going to
5-best lists offers a good balance between the length of the list and the possibility to improve F
1
-score:
generally, we do not want too long N -best lists since they slow down computation and also introduce
more opportunities to make the wrong choice for a reranker (since there are just more candidates to
choose from). In our experiments with larger N , we observed improvements only for 10 and only on the
development set; thus, we will focus on 5-best lists in our experiments below.
K0 K1 K2 K3 K4
Dev 84.21 82.92 83.07 85.07 83.78
Test 84.08 83.19 83.20 84.61 82.93
Table 3: Results for using different tree kernels on the basic tree (BT) representation.
Choosing the best tree kernel. We first select the most appropriate tree kernel to limit the number
of experiment variables. Table 3 shows the results of different tree kernels using the basic tree (BT)
representation (see Figure 1a). We can observe that for both the development set and the test set, kernel
K3 (see Section 3.1) yields the highest F
1
-score.
Impact of feature vectors. Table 4 presents the results for the feature vector experiments in terms
of F
1
-scores and relative error reductions (row RER). The first column shows the baseline, when no
reranking is used; the following four columns contain the results when using vectors including different
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
?Other? is not considered a semantic type, thus ?Other? tokens are not included in the F
1
calculation.
198
Baseline n-grams CRF features Count DB ProbBased AllFeat
Dev 83.86 83.79 83.96 83.80 83.86 83.87 84.49
RER -0.4 0.6 -0.4 0.0 0.0 3.9
Test 83.03 82.90 83.44 82.90 83.01 83.09 83.86
RER -0.7 2.4 -0.7 -0.1 0.3 4.8
Table 4: Feature vector experiments: F
1
score and relative error reduction (in %).
Combining AllFeat and
Baseline BT BTP ShT ST AllFeat +BT +ShT +ShT +BT
Dev 83.86 85.07 85.41 85.06 84.30 84.49 85.57 85.58 85.33
RER 7.5 9.6 7.4 2.8 3.9 10.6 10.7 9.1
Test 83.03 84.61 84.63 84.07 83.81 83.86 84.67 84.79 84.76
RER 9.3 9.4 6.1 4.5 4.8 9.6 10.2 10.2
p.v. 0.00049 0.0002 0.012 0.032 0.00018 0.00028 0.00004 0.000023
Table 5: Tree kernel experiments: F
1
-score, relative error reduction (in %), and p-values.
kinds of features: (i) n-gram features, (ii) all features used by the semi-CRF, (iii) count features, and
(iv) database (DB) features. In each case, we include two additional features: the semi-CRF score
(i.e., the probability) and the reciprocal rank of the hypothesis in the N -best list. Among (i)?(iv), only
the semi-CRF features seem to help; the rest either show no improvements or degrade the performance.
However, putting all these features together (AllFeat) yields sizable gains in terms of F
1
-score and a
relative error reduction of 4-5%; the improvement is statistically significant, and it is slightly larger on
the test dataset compared to the development dataset.
Impact of structural kernels and combinations. Table 5 shows the results when experimenting with
various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with
part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We
can see that the basic tree works rather well, yielding +1.6 F
1
-score on the test dataset, but adding POS
information can help a bit more, especially for the tuning dataset. Interestingly, the syntactic tree kernels,
ShT and ST, perform worse than BT and BTP, especially on the test dataset. The last three columns in the
table show the results when we combine the AllFeat feature vector (see Table 4) with BT and ShT. We can
see that combining AllFeat with ShT works better, on both development and test sets, than combining it
with BT or with both ShT and BT. Also note the big jump in performance from AllFeat to AllFeat+ShT.
Overall, we can conclude that shallow syntax has a lot to offer over AllFeat, and it is preferable over BT
in the combination with AllFeat. The improvements reported in Tables 5 and 6 are statistically significant
when compared to the semi-CRF baseline as shown by the p.v. (value) row. Moreover, the improvement
of AllFeat + ShT over BT is also statistically significant (p.v.<0.05).
Combining AllFeat and
Baseline DS +DS +DS +BT +DS +ShT
Dev 83.86 84.61 85.14 85.43 85.46
RER 4.7 7.9 9.7 9.9
Test 83.03 84.38 84.55 84.63 84.67
RER 7.9 8.9 9.4 9.6
p.v. 0.0005 0.0001 0.00066 0.00015
Table 6: Experiments with discourse kernels: F
1
score, relative error reduction (in %), and p-values.
Discourse structure. Finally, Table 6 shows the results for the discourse tree kernel (DS), which we
designed and experimented with for the first time in this paper. We see that DS yields sizable improve-
ments over the baseline. We also see that further gains can be achieved by combining DS with AllFeat,
and also with BT and ShT, the best combination being AllFeat+DS+ShT (see last column). However,
comparing to Table 5, we see that it is better to use just AllFeat+ShT and leave DS out. We would like
to note though that the discourse parser produced non-trivial trees for only 30% of the hypotheses (due
to the short, simple nature of the questions); in the remaining cases, it probably hurt rather than helped.
We conclude that discourse structure has clear potential, but how to make best use of it, especially in the
case of short simple questions, remains an open question that deserves further investigation.
199
Tag ID Other Rating Restaurant Amenity Cuisine Dish Hours Location Price
0 Other 8260 35 43 110 15 19 55 113 9
1 Rating 29 266 0 14 3 6 0 0 8
2 Restaurant 72 6 657 20 19 15 0 5 0
3 Amenity 117 9 10 841 27 27 7 12 7
4 Cuisine 36 2 12 26 543 44 3 1 0
5 Dish 23 0 4 20 33 324 1 4 0
6 Hours 61 0 1 2 6 1 426 9 1
7 Location 104 1 14 20 2 1 1 1457 0
8 Price 22 1 0 7 0 2 0 1 204
Table 7: Confusion matrix for the output of the best performing system.
4.3 Error analysis and discussion
Table 7 shows the confusion matrix for our best-performing model AllFeat+ShT (rows = gold standard
tags; columns = system predicted tags). Given the good results of the semantic parser, the numbers in the
diagonal are clearly dominating the weight of the matrix. The largest errors correspond to missed (first
column) and over-generated (first row) entity tokens. Among the proper confusions between semantic
types, Dish and Cuisine tend to mislead each other most. This is due to the fact that these two tags
are semantically similar, thus making them hard to distinguish. We can also notice that it is difficult to
identify Amenity correctly, and the model mistakenly tags many other tags as Amenity. We looked into
some examples to further investigate the errors. Our findings are as follow:
Inaccuracies and inconsistencies in human annotations. Since the annotations were done in Me-
chanical Turk, they have many inaccuracies and inconsistencies. For example, the word good with
exactly the same sense was tagged as both Other and Rating by the Turkers in the following examples:
Gold: [
Other
any good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Model: [
Other
any] [
Rating
good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Gold: [
Other
any place] [
Location
along the road] [
Other
has a] [
Rating
good] [
Dish
beer] [
Other
selection that also serves] ...
Requires lexical semantics and more coverage. In some cases our model fails to generalize well. For
instance, it fails to correctly tag establishments and tameles for the following examples. This suggests
that we need to consider other forms of semantic information, e.g., distributional and compositional
semantics computed from large corpora and/or using Web resources such as Wikipedia.
Gold: [
Other
any] [
Location
dancing establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Model: [
Other
any] [
Amenity
dancing] [
Other
establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Gold: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Dish
tameles] [
Amenity
special today]
Model: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Amenity
tameles] [
Other
special] [
Hours
today]
5 Conclusions
We have presented a study on the usage of syntactic and semantic structured information for improved
Concept Segmentation and Labeling (CSL). Our approach is based on reranking a set of N -best se-
quences generated by a state-of-the-art semi-CRF model for CSL. The syntactic and semantic informa-
tion was encoded in tree-based structures, which we used to train a reranker with kernel-based Support
Vector Machines. We empirically compared several variants of syntactic/semantic structured representa-
tions and kernels, including also a vector of manually engineered features.
The first and foremost conclusion from our study is that structural kernels yield significant improve-
ment over the strong baseline system, with a relative error reduction of ?10%. This more than doubles
the improvement when using the explicit feature vector. Second, we observed that shallow syntactic
information also improves results significantly over the best model. Unfortunately, the results obtained
using full syntax and discourse trees are not so clear. This is probably explained by the fact that user
queries are rather short and linguistically not very complex. We also observed that the upper bound per-
formance for the reranker still leaves large room for improvement. Thus, it remains to be seen whether
some alternative kernel representations can be devised to make better use of discourse and other syntac-
tic/semantic information. Also, we think that some innovative features based on analyzing the results
obtained from our database (or the Web) when querying with the segments represented in each hypothe-
ses have the potential to improve the results. All these concerns will be addressed in future work.
200
Acknowledgments
This research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Re-
search Institute (QCRI) within the Qatar Foundation in collaboration with MIT. It is part of the Interactive
sYstems for Answer Search (Iyas) project.
References
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 263?270, Philadelphia, PA, USA.
Renato De Mori, Dilek Hakkani-T?ur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur. 2008. Spoken
language understanding: a survey. IEEE Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2011. Discriminative reranking for spoken lan-
guage understanding. IEEE Transactions on Audio, Speech and Language Processing, 20(2):526?539.
Ruifang Ge and Raymond Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the
21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL?06, pages 263?270, Sydney, Australia.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Thorsten Joachims. 1999. Advances in kernel methods. In Bernhard Sch?olkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Making Large-scale Support Vector Machine Learning Practical, pages 169?184,
Cambridge, MA, USA. MIT Press.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level dis-
course analysis. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages 904?915, Jeju Island, Korea.
Rohit Kate and Raymond Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of
the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL ?06, pages 913?920, Sydney, Australia.
Terry Koo and Michael Collins. 2005. Hidden-variable models for discriminative reranking. In Proceedings
of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,
HLT-EMNLP ?05, pages 507?514, Vancouver, British Columbia, Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 189?
196, Ann Arbor, MI, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, ICML ?01, pages 282?289, Williamstown, MA, USA.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Llu??s M`arquez, Xavier Carreras, Kenneth Litkowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An
Introduction to the Special Issue. Computational Linguistics, 34(2):145?159.
Ian McGraw, Scott Cyphers, Panupong Pasupat, Jingjing Liu, and Jim Glass. 2012. Automating crowd-supervised
learning for spoken language systems. In Proceedings of 13th Annual Conference of the International Speech
Communication Association, INTERSPEECH ?12, Portland, OR, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and Robert Ingria. 1994. Statistical language processing using
hidden understanding models. In Proceedings of the workshop on Human Language Technology, HLT ?94,
pages 278?282, Morristown, NJ, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint
inference. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),
pages 61?68, New York City, June.
201
Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Machine Learning, ECML?06, pages 318?329, Berlin, Hei-
delberg. Springer-Verlag.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2012. Structural reranking models for named entity recognition.
Intelligenza Artificiale, 6(2):177?190.
Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maximum likelihood and discriminative training of
direct translation models. In Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing, volume 1, pages 189?192, Seattle, WA, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee. 1991. Stochastic representation of conceptual structure in
the ATIS task. In Proceedings of the Workshop on Speech and Natural Language, HLT ?91, pages 121?124,
Pacific Grove, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language
understanding. In Proceedings of 8th Annual Conference of the International Speech Communication Associa-
tion, INTERSPEECH ?07, pages 1605?1608, Antwerp, Belgium.
Yigal Dan Rubinstein and Trevor Hastie. 1997. Discriminative vs informative learning. In Proceedings of the
Third International Conference on Knowledge Discovery and Data Mining, KDD ?97, pages 49?53, Newport
Beach, CA, USA.
Guzm?an Santaf?e, Jose Lozano, and Pedro Larra?naga. 2007. Discriminative vs. generative learning of Bayesian
network classifiers. Lecture Notes in Computer Science, 4724:453?464.
Sunita Sarawagi and William Cohen. 2004. Semi-Markov conditional random fields for information extraction.
In Proceedings of the 18th Annual Conference on Neural Information Processing Systems, NIPS ?04, pages
1185?1192, Vancouver, British Columbia, Canada.
Stephanie Seneff. 1989. TINA: A probabilistic syntactic parser for speech understanding systems. In Proceedings
of the Workshop on Speech and Natural Language, HLT ?89, pages 168?178, Philadelphia, PA, USA.
Manfred Stede. 2011. Discourse Processing. Synthesis Lectures on Human Language Technologies. Morgan and
Claypool Publishers.
Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Butterworth.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Jakub Szymanski. 2009. Semi-supervised learning of semantic
classes for query understanding: from the web and for the web. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, CIKM ?09, pages 37?46, New York, NY, USA.
202
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 436?442,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semantic Kernels for Semantic Parsing
Iman Saleh
Faculty of Computers and Information
Cairo University
iman.saleh@fci-cu.edu.eg
Alessandro Moschitti, Preslav Nakov,
Llu??s M
`
arquez, Shafiq Joty
ALT Research Group
Qatar Computing Research Institute
{amoschitti,pnakov,lmarquez,sjoty}@qf.org.qa
Abstract
We present an empirical study on the use
of semantic information for Concept Seg-
mentation and Labeling (CSL), which is
an important step for semantic parsing.
We represent the alternative analyses out-
put by a state-of-the-art CSL parser with
tree structures, which we rerank with a
classifier trained on two types of seman-
tic tree kernels: one processing structures
built with words, concepts and Brown
clusters, and another one using semantic
similarity among the words composing the
structure. The results on a corpus from the
restaurant domain show that our semantic
kernels exploiting similarity measures out-
perform state-of-the-art rerankers.
1 Introduction
Spoken Language Understanding aims to inter-
pret user utterances and to convert them to logical
forms or, equivalently, to database queries, which
can then be used to satisfy the user?s information
needs. This process is known as Concept Segmen-
tation and Labeling (CSL), also called semantic
parsing in the speech community: it maps utter-
ances into meaning representations based on se-
mantic constituents. The latter are basically word
sequences, often referred to as concepts, attributes
or semantic tags. CSL makes it easy to convert
spoken questions such as ?cheap lebanese restau-
rants in doha with take out? into database queries.
First, a language-specific semantic parser tok-
enizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in]
[
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to
the segments, with the option to possibly relabel
mislabeled segments:
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
Finally, a database query is formed from the list
of labels and values, and is then executed against
the database, e.g., MongoDB; a backoff mecha-
nism may be used if the query has not succeeded.
{$and [{cuisine:"lebanese"},{city:"doha"},
{price:"low"},{amenity:"carry out"}]}
The state-of-the-art of CSL is represented by
conditional models for sequence labeling such as
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) trained with simple morphological and
lexical features. The basic CRF model was im-
proved by means of reranking (Moschitti et al.,
2006; Dinarelli et al., 2012) using structural ker-
nels (Moschitti, 2006). Although these meth-
ods exploited sentence structure, they did not use
syntax at all. More recently, we applied shal-
low syntactic structures and discourse parsing with
slightly better results (Saleh et al., 2014). How-
ever, the most obvious models for semantic pars-
ing, i.e., rerankers based on semantic structural
kernels (Bloehdorn and Moschitti, 2007b), had not
been applied to semantic structures yet.
In this paper, we study the impact of semantic
information conveyed by Brown Clusters (BCs)
(Brown et al., 1992) and semantic similarity, while
also combining them with innovative features. We
use reranking, similarly to (Saleh et al., 2014),
to select the best hypothesis annotated with con-
cepts predicted by a local model. The competing
hypotheses are represented as innovative trees en-
riched with the semantic concepts and BC labels.
The trees can capture dependencies between sen-
tence constituents, concepts and BCs. However,
extracting explicit features from them is rather
difficult as their number is exponentially large.
Thus, we rely on (i) Support Vector Machines
(Joachims, 1999) to train the reranking functions
and on (ii) structural kernels (Moschitti, 2010;
Moschitti, 2012; Moschitti, 2013) to automatically
encode tree fragments that represent syntactic and
semantic dependencies from words and concepts.
436
(a) Semantic Kernel Structure (SKS)
(b) SKS with Brown Clusters
Figure 1: CSL structures: standard and with Brown Clusters.
We further apply a semantic kernel (SK),
namely the Smoothed Partial Tree Kernel (Croce
et al., 2011), which uses the lexical similarity be-
tween the tree nodes, while computing the sub-
structure space. This is the first time that SKs are
applied to reranking hypotheses. This (i) makes
the global sentence structure along with concepts
available to the learning algorithm, and (ii) enables
computing the similarity between lexicals in syn-
tactic patterns that are enriched by concepts.
We tested our models on the Restaurant do-
main. Our results show that: (i) The basic CRF
parser, which uses semi-Markov CRF, or semi-
CRF (Sarawagi and Cohen, 2004), is already very
accurate; it achieves F
1
scores over 83%, mak-
ing any further improvement very hard. (ii) The
upper-bound performance of the reranker is very
high as well, i.e., the correct annotation is gen-
erated in the list of the first 100 hypotheses in
98.72% of the cases. (iii) SKs significantly im-
prove over the semi-CRF baseline and our pre-
vious state-of-the-art reranker exploiting shallow
syntactic patterns (Saleh et al., 2014), as shown
by extensive comparisons using several systems.
(iv) Making BCs effective requires a deeper study.
2 Related Work
One of the early approaches to CSL was that
of Pieraccini et al. (1991), where the word se-
quences and concepts were modeled using Hid-
den Markov Models (HMMs) as observations and
hidden states, respectively. Generative models
were exploited by Seneff (1989) and Miller et
al. (1994), who used stochastic grammars for
CSL. Other discriminative models followed such
preliminary work, e.g., (Rubinstein and Hastie,
1997; Santaf?e et al., 2007; Raymond and Riccardi,
2007). CRF-based models are considered to be the
state of the art in CSL (De Mori et al., 2008).
Another relevant line of research are the seman-
tic kernels, i.e., kernels that use lexical similarity
between features. One of the first that applyed
LSA was (Cristianini et al., 2002), whereas (Bloe-
hdorn et al., 2006; Basili et al., 2006) used Word-
Net. Semantic structural kernels of the type we
use in this paper were first introduced in (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b). The most advanced model based on
tree kernels, which we also use in this paper, is the
Smoothed PTK (Croce et al., 2011).
3 Reranking for CSL
Reranking is applied to a list of N annotation hy-
potheses, which are generated and sorted by the
probability to be globally correct as estimated us-
ing local classifiers or global classifiers that only
use local features. Then, a reranker, typically a
meta-classifier, tries to select the best hypothe-
sis from the list. The reranker can exploit global
information, and specifically, the dependencies
between the different concepts, which are made
available by the local model. We use semi-CRFs
for the local model as they yield the highest ac-
curacy in CSL (when using a single model) and
preference reranking for the global reranker.
3.1 Preference Reranking (PR)
PR uses a classifier C, which takes a pair of hy-
potheses ?H
i
, H
j
? and decides whether H
i
is bet-
ter than H
j
. Given a training question Q, posi-
tive and negative examples are built for training
the classifier. Let H
1
be the hypothesis with the
lowest error rate with respect to the gold standard
among all hypotheses generated for question Q.
We adopt the following approach for example gen-
eration: the pairs ?H
1
, H
i
? (i = 2, 3, . . . , N ) are
positive examples, while ?H
i
, H
1
? are considered
negative.
437
At testing time, given a new question Q
?
, C clas-
sifies all pairs ?H
i
, H
j
? generated from the anno-
tation hypotheses of Q
?
: a positive classification is
a vote for H
i
, otherwise the vote is for H
j
, where
the classifier score can be used as a weighted vote.
H
k
are then ranked according to the number (sum)
of the votes (weighted by score) they receive.
We build our reranker with SVMs using the
following kernel: K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) =
?(?H
1
, H
2
?) ? ?(?H
?
1
, H
?
2
?) ,
(
?(H
1
) ?
?(H
2
)
)
?
(
?(H
?
1
) ? ?(H
?
2
)
)
= ?(H
1
)?(H
?
1
) +
?(H
2
)?(H
?
2
) ? ?(H
1
)?(H
?
2
) ? ?(H
2
)?(H
?
1
) =
S(H
1
, H
?
1
) + S(H
2
, H
?
2
) ? S(H
1
, H
?
2
) ?
S(H
2
, H
?
1
). We consider H as a tuple ?T,~v? com-
posed of a tree T and a feature vector ~v. Then, we
define S(H,H
?
) = S
TK
(T, T
?
)+S
v
(~v,~v
?
), where
S
TK
computes one of the tree kernel functions
defined in 3.2 and 3.3; and S
v
is a kernel (see 3.4),
e.g., linear, polynomial, Gaussian, etc.
3.2 Tree kernels (TKs)
TKs measure the similarity between two structures
in terms of the number of substructures they share.
We use two types of tree kernels: (i) Partial Tree
Kernel (PTK), which can be effectively applied
to both constituency and dependency parse trees
(Moschitti, 2006). It generates all possible con-
nected tree fragments, e.g., sibling nodes can be
also separated and can be part of different tree
fragments: a fragment is any possible tree path,
and other tree paths are allowed to depart from its
nodes. Thus, it can generate a very rich feature
space. (ii) The smoothed PTK or semantic kernel
(SK) (Croce et al., 2011), which extends PTK by
allowing soft matching (i.e., via similarity compu-
tation) between nodes associated with different but
related lexical items. The node similarity can be
derived from manually annotated resources, e.g.,
WordNet or Wikipedia, as well as using corpus-
based clustering approaches, e.g., latent semantic
analysis (LSA), as we do in this paper.
3.3 Semantic structures
Tree kernels allow us to compute structural simi-
larities between two trees; thus, we engineered a
special structure for the CSL task. In order to cap-
ture the structural dependencies between the se-
mantic tags,
1
we use a basic tree (see for exam-
ple Figure 1a), where the words of a sentence are
tagged with their semantic tags.
1
They are associated with the following IDs: 0-Other,
1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6-
Hours, 7-Location, and 8-Price.
More specifically, the words in the sentence
constitute the leaves of the tree, which are in
turn connected to the pre-terminals containing
the semantic tags in BIO notation (?B?=begin,
?I?=inside, ?O?=outside). The BIO tags are then
generalized in the upper level, and joined to the
Root node. Additionally, part-of-speech (POS)
tags
2
are added to each word by concatenating
it with the string ?::L?, where L is the first let-
ter of the POS-tags of the words, e.g., along, my
and route, receive i, p and n, which are the first
letters of the POS-tags IN, PRN and NN, respec-
tively. SK applied to the above structure can gen-
erate powerful semantic patterns such as [Root
[4-Cuisine [similar to(stake house)]][7-Loc [simi-
lar to(within a mile)]]], e.g., for correctly labeling
new clauses like Pizza Parlor in three kilometers.
The BC labels, represented as cluster IDs, are sim-
ply added as siblings of words as shown in Fig. 1b.
3.4 Feature Vectors
For the sake of comparison, we also devoted
some effort towards engineering a set of features
to be used in a flat feature-vector representation.
These features can be used in isolation to learn
the reranking function, or in combination with the
kernel-based approach (as a composite kernel us-
ing a linear combination). They belong to the fol-
lowing four categories: (i) CRF-based: these in-
clude the basic features used to train the initial
semi-CRF model; (ii) n-gram based: we collected
3- and 4-grams of the output label sequence at
the level of concepts, with artificial tags inserted
to identify the start (?S?) and end (?E?) of the se-
quence.
3
(iii) Probability-based, computing the
probability of the label sequence as an average of
the probabilities at the word level in the N -best
list; and (iv) DB-based: a single feature encoding
the number of results returned from the database
when constructing a query using the conjunction
of all semantic segments in the hypothesis.
4 Experiments
The experiments aim at investigating the role of
feature vectors, PTK, SK and BCs in reranking.
We first describe the experimental setting and then
we move into the analysis of the results.
2
We use the Stanford tagger (Toutanova et al., 2003).
3
For instance, if the output sequence is Other-Rating-
Other-Amenity the 3-gram patterns would be: S-Other-
Rating, Other-Rating-Other, Rating-Other-Amenity, and
Other-Amenity-E.
438
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 7,000 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
4.1 Experimental setup
Dataset. In our experiments, we used questions
annotated with semantic tags, which were col-
lected through crowdsourcing on Amazon Me-
chanical Turk and made available
4
by McGraw et
al. (2012). We split the dataset into training, de-
velopment and test sets. Table 1 shows the num-
ber of examples and example pairs we used for
the semi-CRF and the reranker, respectively. We
subsequently split the training data randomly into
10 folds. We used cross-validation, i.e., iteratively
training with 9 folds and annotating the remaining
fold, in order to generate the N -best lists of hy-
potheses for the entire training dataset. We com-
puted the 100-best hypotheses for each example.
We then used the development dataset to test and
tune the hyper-parameters of our reranking model.
The results on the development set, which we will
present in Section 4.2 below, were obtained us-
ing semi-CRF and reranking models trained on the
training set.
Data representation. Each hypothesis is repre-
sented by a semantic tree, a feature vector (ex-
plained in Section 3), and two extra features:
(i) the semi-CRF probability of the hypothesis,
and (ii) its reciprocal rank in the N -best list.
Learning algorithm. We used the SVM-Light-
TK
5
to train the reranker with a combination of
tree kernels and feature vectors (Moschitti, 2006;
Joachims, 1999). We used the default parameters
and a linear kernel for the feature vectors. As a
baseline, we picked the best-scoring hypothesis in
the list, i.e., the output by the regular semi-CRF
parser. The setting is exactly the same as that de-
scribed in (Saleh et al., 2014).
Evaluation measure. In all experiments, we used
the harmonic mean of precision and recall (F
1
)
(van Rijsbergen, 1979), computed at the token
level and micro-averaged across the different se-
mantic types.
6
4
http://groups.csail.mit.edu/sls/downloads/restaurant/
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
We do not consider ?Other? to be a semantic type; thus,
we did not include it in the F
1
calculation.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
score for N -best lists.
Brown Clusters. Clustering groups of similar
words together provides a way of generalizing
them. In this work, we explore the use of Brown
clusters (Brown et al., 1992) in both feature vec-
tors and tree kernels. The Brown clustering al-
gorithm uses an n-gram class model. It first as-
signs each word to a distinct cluster, and then it
merges different clusters in a bottom-up fashion.
The merge step is done in a way that minimizes the
loss in average mutual information between clus-
ters. The outcome is hierarchical clustering, which
we use in our reranking algorithm. To create the
Brown clusters, we used the Yelp dataset of re-
views.
7
It contains 335,022 reviews about 15,585
businesses; 5,575 of the businesses and 233,839 of
the reviews are restaurant-related. This dataset is
very similar to the dataset of queries about restau-
rants we use in our experiments.
Similarity matrix for SK. We compute the lexi-
cal similarity for SK by applying LSA (Furnas et
al., 1988) to Tripadvisor data. The dataset and the
exact procedure for creating the LSA matrix are
described in (Castellucci et al., 2013; Croce and
Previtali, 2010).
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
score for N -best lists of different lengths, i.e., the
F
1
that is achieved by picking the best candidate
in the N -best list for various values of N . Con-
sidering 5-best lists yields an increase in oracle F
1
of almost ten absolute points. Going up to 10-best
lists only adds 2.5 extra F
1
points. The complete
100-best lists add 3.5 extra F
1
points, for a total
of 98.72. This very high value is explained by the
fact that often the total number of different anno-
tations for a given question is smaller than 100. In
our experiments, we will focus on 5-best lists.
Baseline accuracy. We computed F
1
for the semi-
CRF model on both the development and the test
sets, obtaining 83.86 and 83.03, respectively.
Learning Curves. The semantic information in
terms of BCs or semantic similarity derived by
LSA can have a major impact in case of data
scarcity. Therefore, we trained our reranking mod-
els with increasing sizes of training data.
7
http://www.yelp.com/dataset challenge/
439
Development set
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?PTK+BC	 ? PTK+all	 ?PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Test set
74	 ?
76	 ?
78	 ?
80	 ?
82	 ?
84	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?
PTK+BC	 ? PTK+all	 ?
PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Figure 2: Learning curves for different reranking models on the development and on the testing sets.
The first two graphs in Fig. 2 show the plots
on the development set whereas the last two are
computed on the test set. The reranking models
reported are Baseline, PTK, PTK+BC, PTK+all
(features), PTK+BC+all, SK, SK+BC, SK+all and
SK+BC+all.
8
We can see that: (i) PTK alone, i.e.,
without semantic information, has the lowest ac-
curacy; (ii) BCs do not improve significantly any
model; (iii) SK almost always achieves the high-
est accuracy; (iv) PTK+all (i.e., the model also us-
ing features) improves on PTK, but its accuracy
is lower than for any model using SK, i.e., us-
ing semantic similarity; and (v) all features pro-
vide an initial boost to SK, but as soon as the data
increases, their impact decreases.
5 Conclusion and Future Work
In summary, the learning curves clearly show the
good generalization ability of SK, which improve
the CRF baseline using little data (?3,000). The
semantic kernel significantly improves over the
semi-CRF baseline and our previous state-of-the-
art reranker exploiting shallow syntactic patterns
(Saleh et al., 2014), which corresponds to PTK+all
in the above comparison.
8
Models are split between 2 plots in order to ease reading.
The improvement falls between 1-2 absolute
percent points. This is remarkable as (i) it corre-
sponds to ?10% relative error reduction, and (ii)
the state-of-the-art baseline system is very difficult
to beat, as confirmed by the low impact of tradi-
tional features and BCs. Although the latter can
generalize over concepts and words, their use is
not straightforward, resulting in no improvement.
In the future, we plan to investigate the use of
semantic similarity from distributional and other
sources (Mihalcea et al., 2006; Pad?o and Lapata,
2007), e.g., Wikipedia (Strube and Ponzetto, 2006;
Mihalcea and Csomai, 2007), Wiktionary (Zesch
et al., 2008), WordNet (Pedersen et al., 2004;
Agirre et al., 2009), FrameNet, VerbNet (Shi and
Mihalcea, 2005), BabelNet (Navigli and Ponzetto,
2010), and LSA, and for different domains.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation. We would like to
thank Danilo Croce, Roberto Basili and Giuseppe
Castellucci for helping and providing us with the
similarity matrix for the semantic kernels.
440
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Boulder, Colorado, June.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica (Slovenia),
30(2):163?172.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Advances in Information Retrieval
- Proceedings of the 29th European Conference on
Information Retrieval (ECIR 2007), pages 307?318,
Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In Proceedings of the 16th ACM Conference on
Information and Knowledge Management (CIKM
2007), pages 861?864, Lisbon, Portugal.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining (ICDM
06), pages 808?812, Hong Kong.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Giuseppe Castellucci, Simone Filice, Danilo Croce,
and Roberto Basili. 2013. UNITOR: Combining
Syntactic and Semantic Kernels for Twitter Senti-
ment Analysis. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
369?374, Atlanta, Georgia, USA.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent Semantic Kernels. Journal
of Intelligent Information Systems, 18(2):127?152.
Danilo Croce and Daniele Previtali. 2010. Mani-
fold learning for the semi-supervised induction of
framenet predicates: An empirical investigation. In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 7?16,
Uppsala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1034?1046,
Edinburgh, Scotland, UK.
Renato De Mori, Frederic B?echet, Dilek Hakkani-T?ur,
Michael McTear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken Language Understanding. IEEE
Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2012. Discriminative reranking for
spoken language understanding. IEEE Transac-
tions on Audio, Speech and Language Processing,
20(2):526?539.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proceedings of the 11th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval (SIGIR ?88),
pages 465?480, New York, USA.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press, Cambridge,
MA, USA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), pages 282?289, Williamstown, MA, USA.
Ian McGraw, Scott Cyphers, Panupong Pasupat,
Jingjing Liu, and Jim Glass. 2012. Automating
crowd-supervised learning for spoken language sys-
tems. In Proceedings of the 13th Annual Conference
of the International Speech Communication Asso-
ciation (INTERSPEECH 2012), pages 2473?2476,
Portland, OR, USA.
Rada Mihalcea and Andras Csomai. 2007. Wikify!
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment (CIKM 2007), pages 233?242, Lisbon, Portu-
gal.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st National Conference on Artificial In-
telligence - Volume 1 (AAAI 2006), pages 775?780,
Boston, MA, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and
Robert Ingria. 1994. Statistical Language Process-
ing using Hidden Understanding Models. In Pro-
ceedings of the workshop on Human Language Tech-
nology (HLT 1994), pages 278?282, Plainsboro, NJ,
USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
441
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 61?68, New York City, USA.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML 2006), pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2010. Kernel engineering
for fast and easy design of natural language ap-
plications. In Coling 2010: Kernel Engineering
for Fast and Easy Design of Natural Language
Applications?Tutorial notes, pages 1?91, Beijing,
China.
Alessandro Moschitti. 2012. State-of-the-art kernels
for natural language processing. In Tutorial Ab-
stracts of ACL 2012, page 2, Jeju Island, Korea.
Alessandro Moschitti. 2013. Kernel-based learning to
rank with syntactic and semantic structures. In Tu-
torial abstracts of the 36th Annual ACM SIGIR Con-
ference, page 1128, Dublin, Ireland.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th annual meet-
ing of the association for computational linguistics
(ACL 2010), pages 216?225, Uppsala, Sweden.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the
relatedness of concepts. In HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee.
1991. Stochastic Representation of Conceptual
Structure in the ATIS Task. In Proceedings of the
Fourth Joint DARPA Speech and Natural Language
Workshop, pages 121?124, Los Altos, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proceedings
of the 8th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2007), pages 1605?1608, Antwerp, Bel-
gium, August.
Y. Dan Rubinstein and Trevor Hastie. 1997. Discrimi-
native vs Informative Learning. In Proceedings of
the Third International Conference on Knowledge
Discovery and Data Mining (KDD-1997), pages 49?
53, Newport Beach, CA, USA.
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014. A study of using syntactic and seman-
tic structures for concept segmentation and labeling.
In Proceedings of the 25th International Conference
on Computational Linguistics, COLING ?14, pages
193?202, Dublin, Ireland.
G. Santaf?e, J.A. Lozano, and P. Larra?naga. 2007.
Discriminative vs. Generative Learning of Bayesian
Network Classifiers. In Proceedings of the 9th Euro-
pean Conference on Symbolic and Quantitative Ap-
proaches to Reasoning with Uncertainty (ECSQARU
2007), pages 453?546, Hammamet, Tunisia.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems 17 (NIPS 2004), Vancouver, British
Columbia, Canada.
Stephanie Seneff. 1989. TINA: A Probabilistic Syn-
tactic Parser for Speech Understanding Systems.
In Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing (ICASSP-
89), pages 711?714, Glasgow, UK.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Computational Lin-
guistics and Intelligent Text Processing, pages 100?
111. Springer Berlin Heidelberg.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence (AAAI?06), pages
1419?1424, Boston, Massachusetts, USA.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 173?180, Edmon-
ton, Canada.
Cornelis J. van Rijsbergen. 1979. Information
Retrieval. Butterworth-Heinemann Newton, MA,
USA.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In Proceedings of the 23rd National Con-
ference on Artificial Intelligence (AAAI?08), pages
861?866, Chicago, Illinois,USA.
442
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 85?92,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Classifying Wikipedia Articles into NE?s using SVM?s with Threshold 
Adjustment 
Iman Saleh 
Faculty of Computers and 
Information, Cairo University 
Cairo, Egypt 
iman.saleh@fci-
cu.edu.eg 
Kareem Darwish 
Cairo Microsoft Innovation 
Center 
Cairo, Egypt 
kareemd@microsoft.com 
Aly Fahmy 
Faculty of Computers and 
Information, Cairo University 
Cairo, Egypt 
a.fahmy@fci-
cu.edu.eg 
 
Abstract 
In this paper, a method is presented to 
recognize multilingual Wikipedia named entity 
articles. This method classifies multilingual 
Wikipedia articles using a variety of structured 
and unstructured features and is aided by 
cross-language links and features in 
Wikipedia.  Adding multilingual features helps 
boost classification accuracy and is shown to 
effectively classify multilingual pages in a 
language independent way.  Classification is 
done using Support Vectors Machine (SVM) 
classifier at first, and then the threshold of 
SVM is adjusted in order to improve the recall 
scores of classification. Threshold adjustment 
is performed using beta-gamma threshold 
adjustment algorithm which is a post learning 
step that shifts the hyperplane of SVM. This 
approach boosted recall with minimal effect on 
precision. 
1 Introduction 
Since its launch in 2001, Wikipedia has grown to 
be the largest and most popular knowledge base 
on the web.  The collaboratively authored 
content of Wikipedia has grown to include more 
than 13 million articles in 240 languages.1  Of 
these, there are more than 3 million English 
articles covering a wide range of subjects, 
supported by 15 million discussion, 
disambiguation, and redirect pages.2 Wikipedia 
provides a variety of structured, semi-structured 
and unstructured resources that can be valuable 
in areas such information retrieval, information 
extraction, and natural language processing.  As 
shown in Figure 1, these resources include page 
redirects, disambiguation pages, informational 
summaries (infoboxes), cross-language links 
between articles covering the same topic, and a 
                                                          
1
 http://en.wikipedia.org/wiki/Wikipedia  
2
 http://en.wikipedia.org/wiki/Special:Statistics  
hierarchical tree of categories and their mappings 
to articles.  
Many of the Wikipedia pages provide 
information about concepts and named entities 
(NE).  Identifying pages that provide information 
about different NE?s can be of great help in a 
variety of NLP applications such as named entity 
recognition, question answering, information 
extraction, and machine translation (Babych and 
Hartley, 2003; Dakka and Cucerzan, 2008).  This 
paper attempts to identify multilingual Wikipedia 
pages that provide information about different 
types of NE, namely persons, locations, and 
organizations.  The identification is done using a 
Support Vector Machines (SVM) classifier that 
is trained on a variety of Wikipedia features such 
as infobox attributes, tokens in text, and category 
links for different languages aided by cross-
language links in pages. Using features from 
different languages helps in two ways, namely: 
clues such infobox attributes may exist in one 
language, but not in the other, and this allows for 
tagging pages in multiple languages 
simultaneously.  To improve SVM classification 
beta-gamma threshold adjustment was used to 
improve recall of different NE classes and 
consequently overall F measure. 
The separating hyperplane suggested by the 
SVM typically favors precision at the cost of 
recall and needs to be translated (via threshold 
adjustment) to tune for the desired evaluation 
metric. 
Beta-gamma threshold adjustment was 
generally used when certain classes do not have a 
sufficient number of training examples, which 
may lead to poor SVM recall scores (Shanahan 
and Roma, 2003). It was used by Shanahan and 
Roma (2003) to binary classify a set of articles 
and proved to improve recall with little effect on 
precision. 
85
However, the technique seems to generalize 
beyond cases where very few training examples 
are present, and it is shown in this paper to yield 
improvements in recall and overall F-measure in 
the presence of hundreds of training examples, 
performing better than threshold adjustment 
using cross validation for the specific task at 
hand.   
The contribution of this paper lies in: 
introducing a language independent system that 
utilizes multilingual features from Wikipedia 
articles in different languages and can be used to 
effectively classify Wikipedia articles written in 
any language to the NE classes of types person, 
location, and organization; and modifying beta-
gamma threshold adjustment to improve overall 
classification quality even when many training 
examples are available. The features and 
techniques proposed in this paper are compared 
to previous work in the literature. 
The rest of the paper is organized as follows:  
Section 2 provides information about the 
structure and feature of Wikipedia; Section 3 
surveys prior work on the problem; Section 4 
describes the classification approach including 
features and threshold adjustment algorithm; 
Section 5 describes the datasets used for 
evaluation; Section 6 presents the results of the 
experiments; and Section 7 concludes the paper. 
2 Wikipedia Pages 
Wikipedia pages have a variety of types 
including: 
? Content pages which constitute entries in 
Wikipedia (as in Figure 1).  Content pages 
typically begin with an abstract containing a 
brief description of the article. They may 
contain semi-structured data such as 
infoboxes and persondata, which provide 
factoids about concepts or entities in pages 
using attribute-value pairs.  Persondata 
structures are found only in people pages.  
Most of the articles in Wikipedia belong to 
one or more category, and the categories a 
page belongs to are listed in the footer of the 
page. As in Figure 1, the entry for Alexander 
Pushkin belongs to categories such as 
?Russian Poets? and ?1799 births?.  Content 
pages provide information about common 
concepts or named entities of type person, 
location, or organization (Dakka and 
Cucerzan, 2008).  A page in Wikipedia is 
linked to its translations in other languages 
through cross language links. These links 
redirects user to the same Wikipedia article 
written in different language. 
? Category pages which lists content pages that 
belong to a certain category.  Since 
Figure 1.  Sample Wikipedia article 
86
categories are hierarchical, a category page 
lists its parent category and sub-categories 
below it. 
? Disambiguation pages which help 
disambiguate content pages with the same 
titles.  For example, a disambiguation page 
for ?jaguar? provides links to jaguar the cat, 
the car, the guitar, etc. 
? Redirect pages redirect users to the correct 
article if the name of the article entered was 
not exactly the same. For example, 
?President Obama? is redirected to ?Barak 
Obama?. 
 
3 Related Work 
This section presents some of the effort 
pertaining to identifying NE pages in Wikipedia 
and some background on SVM threshold 
adjustment.   
3.1 Classifying Wikipedia Articles 
Toral and Munoz (2006) proposed an approach 
to build and maintain gazetteers for NER using 
Wikipedia. The approach makes use of a noun 
hierarchy obtained from WordNet in addition to 
the first sentence in an article to recognize 
articles about NE?s.  A POS tagger can be used 
in order to improve the effectiveness of the 
algorithm. They reported F-measure scores of 
78% and 68% for location and person classes 
respectively. The work in this paper relies on 
using the content of Wikipedia pages only. 
Watanabe et al (2007) considered the 
problem of tagging NE?s in Wikipedia as the 
problem of categorizing anchor texts in articles. 
The novelty of their approach is in exploiting 
dependencies between these anchor texts, which 
are induced from the HTML structure of pages. 
They used Conditional Random Fields (CRF) for 
classification and achieved F-measure scores of 
79.8 for persons, 72.7 for locations, and 71.6 for 
organizations. This approach tags only NE?s 
referenced inside HTML anchors in articles and 
not Wikipedia articles themselves. 
Bhole et al (2007) and Dakka and Cucerzan 
(2008) used SVM classifiers to classify 
Wikipedia articles. Both used a bag of words 
approach to construct feature vectors. In Bhole et 
al. (2007), the feature vector was constructed 
over the whole text of an article.  They used a 
linear SVM and achieved 72.6, 70.5, and 41.6 F-
measure for tagging persons, locations, and 
organizations respectively.  For a Wikipedia 
article, Dakka and Cucerzan (2008) used feature 
vectors constructed using words in the full text of 
the article, the first paragraph, the abstract, the 
values in infoboxes, and the hypertext of 
incoming links with surrounding words.  They 
reported 95% and 93% F-measure for person and 
location respectively. Using a strictly bag of 
words approach does not make use of the 
structure of Wikipedia articles and is compared 
against in the evaluation. 
Richman and Schone (2008) and Nothman et 
al. (2008) annotated Wikipedia text with NE tags 
to build multilingual training data for NE 
taggers. The approach of Richman and Schone 
(2008) is based on using Wikipedia category 
structure to classify Wikipedia titles. Identifying 
NE?s in other languages is done using cross 
language links of articles or categories of 
articles. Nothman et al  (2008) used a 
bootstrapping approach with heuristics based on 
the head nouns of categories and the opening 
sentence of an article. Evaluating the system is 
done by training a NE tagger using the generated 
training data. They reported an average 92% F-
measure for all NE?s.   
Silberer et al (2008) presented work on the 
translation of English NE to 15 different 
languages based on Wikipedia cross-language 
links with a reported precision of 95%. The 
resulting NE?s were not classified.  This paper 
extends the work on cross language links and 
uses features from multilingual pages to aid 
classification and to enable simultaneous tagging 
of entities across languages. 
3.2 SVM Threshold Adjustment 
Support Vector Machines (SVM) is a popular 
classification technique that was introduced by 
Vapnik (Vapnik, 1995). The technique is used in 
text classification and proved to provide 
excellent performance compared to other 
classification techniques such as k-nearest 
neighbor and na?ve Bayesian classifiers. As in 
Figure 2, SVM attempts to find a maximum 
margin hyperplane that separates positive and 
negative examples. The separating hyperplane 
can be described as follows: <W, X> + b = 0 or 
?              Where W is the normal to the 
hyperplane, X is an input feature vector, and b is 
the bias (the perpendicular distance from the 
origin to the hyperplane).  When the number of 
examples for each class is not equivalent, the 
SVM may overfit the class that has fewer 
training examples.  Further, the SVM training is 
not informed by the evaluation metric. Thus, 
SVM training may lead to a sub-optimal 
87
separating hyperplane.  Several techniques were 
proposed to rectify the problem by translating the 
hyperplane by only adjusting bias b, which is 
henceforth referred to as threshold adjustment.  
Some of these techniques adjust SVM 
threshold during learning (Vapnik 1998; Lewis 
2001), while others consider threshold 
adjustment as a post learning step (Shanahan and 
Roma, 2003). One type of the later is beta-
gamma threshold adjustment algorithm 
(Shanahan and Roma, 2003; Zhai et al, 1998), 
which is a post learning algorithm that has been 
shown to provide significant improvements for 
classification tasks in which very few training 
examples are present such as in adaptive text 
filtering.  Such threshold adjustment allows for 
the tuning of an SVM to the desired measure of 
goodness (ex. F1 measure).  A full discussion of 
beta-gamma threshold adjustment is provided in 
the experimental setup section.  In the presence 
of many training examples, some of the training 
examples are set aside as a validation set to help 
pick an SVM threshold.  Further, multi-fold cross 
validation is often employed. 
 
Figure 2. SVMs try to maximize the margin of 
separation between positive and negative 
examples 
 
4 Classification Approach 
Features:  The classification features 
included content-based features such as words in 
page abstracts and structure-based features such 
category links.  All the features are binary. The 
features are: 
? Stemmed content words extracted from 
abstracts:  an abstract for a NE may include 
keywords that may tell of the entity type.  
For example, an abstract for an NE of type 
person would typically include words such as 
?born?, ?pronounced?, and more specific 
words that point to profession, role, or job 
(ex. president, poet, etc.). 
? White space delimited attribute names from 
infoboxes:  in the presence of infoboxes 
structures, the attribute names provide hints 
of the entity type.  For example, an infobox 
of location may include attribute names such 
as ?latitude?, ?longitude?, ?area?, and 
?population?. 
? White space delimited words in category 
links for a page:  category names may 
include keywords that would help 
disambiguate a NE type.  For example, 
categories of NE of type person may include 
the words ?births?, ?deaths?, ?people?, 
occupation such as ?poet? or ?president?, 
nationality such ?American? or ?Russian?, 
etc. 
? Persondata structure attributes:  persondata 
only exist if the entity refers to a person. 
 The features used herein combine structural 
as well as content-based features from multiple 
languages unlike features used in the literature 
which were monolingual. Using multilingual 
features enables language independent 
classification of any Wikipedia article written in 
any language.  Moreover, using primarily 
structural features in classification instead of the 
whole content of the articles allows for the 
effective use of multilingual pages without the 
need for language specific stemmers and 
stopword lists, the absence of which may 
adversely affect content based features. 
Classification:  Classifying Wikipedia pages 
was done in two steps: First training an SVM 
classifier; and then adjusting SVM thresholds 
based on beta-gamma adjustment to improve 
recall.  Beta-gamma threshold adjustment was 
compared to cross-fold validation threshold 
adjustment. All Wikipedia articles were 
classified using a linear SVM. Classification was 
done using the Liblinear SVM package which is 
optimized for SVM classification problems with 
thousands of features (Fan et al, 2008).  A 
variant of the beta-gamma threshold adjustment 
algorithm as described by (Shanahan and Roma, 
2003; Zhai et al, 1998) is used to adjust the 
threshold of SVM. The basic steps of the 
algorithm are as follows: 
? Divide the validation set into n folds such 
that each fold contains the same number of 
positive examples 
? For each fold i, 
88
o Classify examples in a fold and sort them in 
descending order based on SVM scores, 
where the SVM score of SVM is the 
perpendicular distance between an example 
and the separating hyperplane. 
o Calculate F-measure, which is the goodness 
measure used in the paper, at each example. 
o Determine the point of maximum F-
measure and set ?Ni to the SVM score at this 
point. 
o Repeat previous steps for the set consisting 
of all folds other than i and set ?Max = ?Ni 
and ?Min = ?Mi, where ?Mi is the SVM score 
at the point of minimum F-measure. 
o Compute    
        
         
 
?   
?  
 
 
?  The optimal threshold is obtained by 
interpolating between ?Max and ?Min obtained 
from the whole validation set as follows: 
?     ???                  ???
 where               ,  M is the 
number of documents in the validation set, 
and   is the inverse of the estimated number 
of documents at the point of the optimal 
threshold (Zhai et al, 1998). In this work, it 
is assigned a value that is equivalent to the 
inverse of the number of examples at ?Max. 
Since the number of training examples in 
Shanahan and Roma (2003) were small, n-fold 
cross-validation was done using the training set.  
In this work, the validation and training sets were 
non-overlapping.  Further, in the work of 
Shanahan and Roma (2003), ?Min was set to the 
point that yields utility = 0 as they used a 
filtering utility measure that can produce a utility 
of 0.  Since no F-measure was found to equal 
zero in this work, minimum F-measure point was 
used instead.   
For comparison, n-fold cross validation was 
used to obtain ?Ni for each of the folds and then 
?opt as the average of all ?Ni.  Further, using a 
bag-of-words approach is used for comparison, 
where a feature vector in constructed based on 
the full text of an article. 
5 Data Set 
To train and test the tagging of Wikipedia pages 
with NE tags, a dataset of 4,936 English 
Wikipedia pages was developed by the authors 
and with split using a 60/20/20 training, 
validation, and testing split. The characteristics 
of the dataset, which is henceforth referred to as 
MAIN, are presented in Table 1. The English 
articles had links to 128 different languages, 
with: 16,912 articles having cross-language 
links; 93.3 pages on average per language; 97 
languages with fewer than 100 links; with a 
minimum of 1 page per language (for 14 
languages); and a maximum of 918 pages for 
French.  To compare the inclusion of 
multilingual pages in training and testing, two 
variants of MAIN were used, namely:  MAIN-E 
which has only English pages, and MAIN-EM 
which has English and multilingual pages from 
13 languages with the most pages ? Spanish, 
French, Finnish, Dutch, Polish, Portuguese, 
Italian, Norwegian, German, Danish, Hungarian, 
Russian, and Swedish.  Other languages had too 
few pages.  To stem text, Porter stemmer was 
used for English and snowball stemmers3 were 
used for the other 13 languages.  For all the 
languages, stopwords were removed.  For 
completeness, another set was constructed to 
include all 128 languages to which the English 
pages had cross language links.  This set is 
referred to as the MAIN-EM+ set.  The authors 
did not have access to stemmers and stopword 
lists in all these languages, so simple 
tokenization was performed by breaking text on 
whitespaces and punctuation.  Since many 
English pages don?t have cross language links 
and most languages have too few pages, a new 
dataset was constructed as a subset of the 
aforementioned dataset such that each document 
in the collection has an English page with at least 
one cross language link to one of the 13 
languages with the most pages in the bigger 
dataset.  Table 2 details the properties of the 
smaller dataset, which is henceforth referred to 
as SUB.  SUB had five variants, namely: 
? SUB-E with English pages only  
? SUB-EM with English and multilingual 
pages from the 13 languages in MAIN-EM 
? SUB-M which the same as SUM-EM 
excluding English. 
? SUB-EM+ with English pages and 
multilingual pages in 128 languages. 
? SUB-M+ which is the same as SUB-EM+ 
excluding English.   
The articles used in the experiments were 
randomly selected out of all the content articles 
in Wikipedia, about 3 million articles. Articles 
were randomly assigned to training and test sets 
                                                          
3 http://snowball.tartarus.org/ 
89
and manually annotated in accordance to the 
CONLL ? 2003 annotation guidelines4 which are 
based on (Chinchor et al, 1999). Annotation was 
based on reading the contents of the article and 
then labeling it with the appropriate class. All the 
data, including first sentence in an article, 
infobox attributes, persondata attributes, and 
category links, were parsed from a 2010 
Wikipedia XML dump. 
6 Evaluation and Results 
The results of classifying Wikipedia articles 
using SVM and threshold adjustment for MAIN-
E, MAIN-EM, and MAIN-M are reported in 
Tables 3, 4, and 5 respectively.  Tables 6, 7, 8, 9, 
and 10 report results for SUB-E, SUB-EM, SUB-
M, SUB-EM+, and SUB-M+ respectively.  In all, 
n is the number of cross folds used to calculate  , 
with n ranging between 3 and 10. The first row is 
the baseline scores of SVM classification without 
threshold adjustment. The remaining rows are the 
scores of SVM classification after adjusting 
threshold. The adjustment is performed by 
adding      to the bias value b learned by the 
SVM.  A t-test with 95% confidence (p-value < 
0.05) is used to determine statistical significance. 
For the MAIN-E dataset, SVM threshold 
relaxation yielded statistically significant 
improvement over the baseline of using an SVM 
directly for location named entity.  For other 
types of named entities improvements were not 
statistically significant.  
Threshold adjustment led to statistically 
significant improvement for: all NE types for 
SUB-EM and SUB-EM+; for organizations for 
SUB-E and SUB-M+; and for locations and 
organization for SUB-EM.  The improvements 
were most pronounced when recall was very low.  
For example, F1 measure for organization in the 
SUB-M dataset improved by 18 points due to a 
26 point improvement in recall ? though at the 
expense of precision.   
It seems that threshold adjustment tends to 
benefit classification more when: using smaller 
training sets ? as is observed when comparing 
the results for MAIN and SUB datasets, and 
when classification leads to very low recall ? as 
indicated by organization NE for SUB datasets. 
Tables 11 and 12 compare the results for the 
different variations of the MAIN and SUB 
datasets respectively.  As indicated in the Tables 
11 and 12, the inclusion of more and more 
                                                          
4
 http://www.cnts.ua.ac.be/conll2003/ner/annotation.txt 
language pages with English led to improved 
classification with consistent improvements in 
precision and recall for MAIN and consistent 
improvements in precision for SUB.  For the 
SUB-M and SUB-M+ datasets, the exclusion of 
English led to degradation on F1 measure, with 
the degradation being particularly pronounced 
for organizations.  The drop can be attributed to 
the loss of much valuable training examples, 
because there are more English pages compared 
to other languages. Despite the loss, proper 
identification of persons and locations remained 
high enough for many practical applications.  
Further, the results suggest that given more 
training data in the other languages, the features 
suggested in the paper would likely yield good 
classification results. Unlike the MAIN datasets, 
the inclusion of more languages for training and 
testing (from SUB-M to SUB-M+ & from SUB-
EM to SUB-EM+) did not yield any 
improvements except for location and 
organization types from SUB-EM to SUB-EM+.  
This requires more investigation. 
Tables 13 and 14 report the results of using 
term frequency representation of the entire page 
as features ? a bag of words (BOWs)? as in 
Bhole et al (2007). Using semi-structured data as 
classification features is better than using BOW 
representation. This could be due to the smaller 
number of features of higher value. In the BOW 
results with multilingual page inclusion, except 
for location NE type only in the SUB dataset, the 
use of term frequencies of multilingual words 
hurt F1-measure for the SUB and MAIN 
datasets. This can be attributed to the increased 
sparseness of the training and test data. 
7 Conclusions 
This paper presented a language independent 
method for identifying multilingual Wikipedia 
articles referring to named entities.  An SVM 
was trained using multilingual features that make 
use of unstructured and semi-structured portions 
of Wikipedia articles. It was shown that using 
multilingual features was better than using 
features obtained from English articles only. 
Multilingual features can be used in classifying 
multilingual articles and is particularly useful for 
languages other than English, where fewer useful 
features are present. The number of Infobox 
properties and category links in English MAIN 
was 32,262 and 9,221 respectively, while in 
German there are 4,618 properties and 1,657 
category links. These numbers are even lower in 
all other languages.  
90
Table 1. Characteristics of MAIN dataset: the number 
of Wikipedia pages in the dataset 
Table 2. Characteristics of SUB dataset: the number 
of Wikipedia pages in the dataset 
cross 
folds 
Person Location Organization 
 P R F1 P R F1 P R F1 
Baseline 98.7 90.4 94.4 94.6 85.7 89.9 90 73.6 81.0 
n = 3 97.9 92.0 94.9 94.4 89.0 91.6 87.2 74.5 80.4 
n = 4 96.7 92.0 94.3 94.4 89.0 91.6 87.2 74.5 80.4 
n = 5 96.6 92.0 94.3 94.4 89.0 91.6 80.0 76.4 78.0 
n =6 96.7 92.4 94.5 94.4 89.4 91.9 85.6 75.4 80.2 
n =7 96.7 92.8 94.7 94.4 89.4 91.9 85.6 75.4 80.2 
n =8 96.7 92.8 94.7 94.0 90.6 92.3 80.0 76.4 78.0 
n =9 95.2 94.0 94.6 94.0 89.8 91.9 80.8 76.4 78.5 
n = 10 94.8 94.0 94.4 94.0 90.6 92.3 77.9 80.0 78.9 
Table 3.  Results for MAIN-E: Best F1 bolded and 
italicized if significantly better than baseline. 
cross 
folds 
Person Location Organization 
 P R F1 P R F1 P R F1 
Baseline 99.1 91.6 95.2 94.7 87.2 90.8 91.0 73.6 81.4 
n = 3 99.7 91.2 95.2 94.7 87.2 90.8 90.1 74.5 81.6 
n = 4 99.1 91.6 95.2 94.7 87.9 91.2 90.2 75.4 82.2 
n = 5 99.1 92.4 95.7 94.4 89 91.6 86.4 75.4 80.6 
n =6 98.3 92.4 95.3 94.7 87.9 91.2 87.4 75.4 81.0 
n =7 98.3 92.4 95.3 93.7 90.2 91.9 82.3 76.4 79.2 
n =8 98.3 92.8 95.5 93.7 90.2 91.9 85.7 76.4 80.8 
n =9 98.3 92.8 95.5 92.4 92.4 92.4 82.3 76.4 79.2 
n = 10 97.9 92.8 95.3 92.8 92.1 92.4 82.3 76.4 79.2 
Table 4.  Results for MAIN-EM: Best F1 bolded and 
italicized if significantly better than baseline. 
cross 
folds 
Person Location Organization 
 P R F1 P R F1 P R F1 
Baseline 99.6 92.0 95.6 95.0 87.2 90.9 91.0 73.6 81.4 
n = 3 98.3 92.4 95.3 94.3 88.3 91.2 91.0 74.5 82.0 
n = 4 98.3 92.8 95.5 93.7 90.2 91.9 91.0 74.5 82.0 
n = 5 98.3 92.8 95.5 93.8 90.9 92.3 89.2 75.4 81.8 
n =6 97.9 93.2 95.5 93.0 91.3 92.2 88.3 75.4 81.4 
n =7 95.5 93.6 94.6 93.4 90.9 92.2 87.4 75.4 81.0 
n =8 95.5 93.6 94.6 91.8 92.8 92.3 85.7 76.4 80.8 
n =9 95.9 93.2 94.5 92.0 92.0 92.0 84.0 76.4 80.0 
n = 10 95.2 94.8 95.0 91.7 92.0 91.9 85.7 76.4 80.8 
Table 5.  Results for MAIN-EM+: Best F1 bolded and 
italicized if significantly better than baseline. 
 
The effect of using SVM and beta-gamma 
threshold adjustment algorithm to improve 
recognizing NE?s in Wikipedia was also 
demonstrated. The algorithm was shown to 
improve scores of location NE?s particularly. The 
appropriate number of folds was found to be 8 
using our dataset. Finally, the results suggest that 
the use of semi-structured data as classification 
features is significantly better than the using 
unstructured data only or BOWs. The paper also 
showed that the use of multilingual features with 
BOWs was not very useful.  
For future work, the proposed technique can 
be used to create large sets of tagged Wikipedia 
pages in a variety of languages to aid in building 
parallel lists of named entities that can be used to 
improve MT and in training transliterator 
engines.  Further, this work can help in building 
resources such gazetteers and tagged NE data in 
many languages for the rapid development of NE 
taggers in general text.  Wikipedia has the 
advantage of covering many topics beyond those 
that are typically covered in news articles. 
cross 
folds 
Person Location Organization 
 P R F1 P R F1 P R F1 
Baseline 100 92.6 96.2 98.5 91.7 95 87.0 49.0 62.5 
n = 3 100 92.6 96.2 97.8 91.7 94.6 84 51.2 63.6 
n = 4 100 92.6 96.2 97.8 91.7 94.6 85.2 56 67.7 
n = 5 100 93.7 96.7 96.4 92.4 94.3 87.0 65.8 75.0 
n =6 100 93.7 96.7 95.7 93.7 94.7 85.7 58.5 69.6 
n =7 100 93.7 96.7 95.7 93.7 94.7 87.0 65.8 75.0 
n =8 100 93.7 96.7 95.7 93.7 94.7 87.0 65.8 75.0 
n =9 100 94.7 97.3 95.0 94.4 94.8 87.0 65.8 75.0 
n = 10 100 94.7 97.3 95.0 94.4 94.8 87.0 65.8 75.0 
Table 6.  Results for SUB-E: Best F1 bolded and 
italicized if significantly better than baseline. 
cross 
folds 
Person Location Organization 
 P R F1 P R F1 P R F1 
Baseline 100 91.6 95.6 99.2 88.9 93.8 100 46.3 63.3 
n = 3 98.9 92.6 95.6 99.2 88.2 93.3 100 53.6 69.8 
n = 4 98.9 92.6 95.6 98.5 91.7 95.0 92.0 56.0 69.7 
n = 5 98.9 92.6 95.6 99.2 88.9 93.8 92.0 56.0 69.7 
n =6 98.9 92.6 95.6 98.5 93.7 96.0 92.0 56.0 69.7 
n =7 99.0 92.6 95.6 98.5 93.7 96.0 92.0 56.0 69.7 
n =8 99.0 92.6 95.6 97.8 93.7 95.7 92.0 56.0 69.7 
n =9 98.9 95.7 97.3 95.2 95.8 95.5 92.0 56.0 69.7 
n = 10 98.9 95.7 97.3 93.2 96.5 94.9 92.0 56.0 69.7 
Table 7.  Results for SUB-EM: Best F1 bolded and 
italicized if significantly better than baseline. 
References 
Babych, Bogdan, and Hartley, Anthony (2003). 
Improving Machine Translation quality with 
automatic Named Entity recognition. 7th Int. 
EAMT workshop on MT and other lang. tech. 
tools -- EACL?03, Budapest, Hungary. 
  Training Validation Test  
Person 822 300 251 
Locations 676 221 266 
Organizations 313 113 110 
Non 1085 366 414 
Total 2896 1000 1040 
 Training Validation Test 
Person 332 128 95 
Locations 360 115 144 
Organizations 102 30 42 
Non 435 150 184 
Total 1229 423 465 
91
Bhole, Abhijit, Fortuna, Blaz, Grobelnik, Marko, and 
Mladenic, Dunja. (2007). Extracting Named 
Entities and Relating Them over Time Based on 
Wikipedia. Informatica (Slovenia), 31, 463-468. 
Chinchor, Nancy, Brown, Erica, Ferro, Lisa, and 
Robinson, Patty. (1999). 1999 Named Entity 
Recognition Task Definition: MITRE. 
Dakka, Wisam., and Cucerzan, Silviu. (2008). 
Augmenting Wikipedia with Named Entity Tags. 
3rd IJCNLP, Hyderabad, India. 
Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, 
Wang, Xiang-Rui, and Lin, Chih-Jen. (2008). 
LIBLINEAR: A Library for Large Linear 
Classication. Journal of Machine Learning 
Research 9, 1871-1874. 
Nothman, Joel, Curran, James R., and Murphy, Tara. 
(2008). Transforming Wikipedia into Named 
Entity Training Data. Australian Lang. Tech. 
Workshop. 
Richman, Alexander E., and Schone, Patrick. (2008, 
June). Mining Wiki Resources for Multilingual 
Named Entity Recognition. ACL-08: HLT, 
Columbus, Ohio. 
Shanahan, James G., and Roma, Norbert. (2003). 
Boosting support vector machines for text 
classification through parameter-free threshold 
relaxation. CIKM'03. New Orleans, LA, US 
Silberer, Carina, Wentland, Wolodja, Knopp, 
Johannes, and Hartung, Matthias. (2008). Building 
a Multilingual Lexical Resource for Named Entity 
Disambiguation, Translation and Transliteration. 
LREC'08, Marrakech, Morocco. 
Toral, Antonio, and Mu?noz, Rafael (2006). A 
proposal to automatically build and maintain 
gazetteers for Named Entity Recognition by using 
Wikipedia, EACL-2008. Italy. 
Vapnik, Vladimir N. (1995). The nature of statistical 
learning theory: Springer-Verlag New York, Inc. 
Watanabe, Yotaro, Asahara, Masayuki, and 
Matsumoto, Yuji. (2007). A Graph-Based 
Approach to Named Entity Categorization in 
Wikipedia using Conditional Random Fields. 
EMNLP-CoNLL, Prague, Czech Republic 
Zhai, Chengxiang, Jansen, Peter, Stoica, Emilia, Grot, 
Norbert, and Evans, David A. (1998). Threshold 
Calibration in CLARIT Adaptive Filtering. TREC-
7, Gaithersburg, Maryland, US. 
cross 
folds 
Person Location Organization 
 P R F1 P R F1 P R F1 
Baseline 100 90.5 95 99.2 90.3 94.5 100 47.6 64.5 
n = 3 98.9 92.6 95.6 98.5 91.7 95 100 47.6 64.5 
n = 4 98.9 92.6 95.6 98.5 91 94.6 96 57 71.6 
n = 5 98.9 92.6 95.6 98.5 92.4 95.3 96 57 71.6 
n =6 98.9 92.6 95.6 95.8 94.4 95 96 57 71.6 
n =7 98.9 92.6 95.6 97 93 95 100 54.8 70.8 
n =8 98.9 92.6 95.6 95.8 94.4 95 92.6 59.5 72.5 
n =9 98.8 93.7 96.2 95 95 95 96 59.5 73.5 
n = 10 98.9 94.7 96.8 94.5 95.8 95.2 92.6 59.5 72.5 
Table 8.  Results for SUB-EM+: Best F1 bolded and 
italicized if significantly better than baseline. 
cross 
folds 
Person Location Organization 
 P R F1 P R F1 P R F1 
Baseline 97.4 77.9 86.5 97.4 78.5 86.9 100 21.9 36.0 
n = 3 97.4 78.9 87.2 96.7 80.5 87.9 100 24.4 39.2 
n = 4 97.5 82.0 89.0 95.3 84.0 89.3 71.4 36.6 48.4 
n = 5 97.5 82.0 89.0 94.6 84.7 89.4 100 24.4 39.2 
n =6 96.3 83.0 89.3 94.6 84.7 89.4 100 24.4 39.2 
n =7 95.2 83.0 88.8 94.6 86.0 90.2 77.8 34 47.4 
n =8 97.5 83.0 89.8 91.8 86.0 88.9 70.8 41.5 52.3 
n =9 95.2 84.2 89.4 94.6 86.0 90.0 61.3 46.3 52.8 
n = 10 91.2 87.4 89.2 64.9 96.5 77.6 60.6 48.8 54.0 
Table 9.  Results for SUB-M: Best F1 bolded and 
italicized if significantly better than baseline. 
cross 
folds 
Person Location Organization 
 P R F1 P R F1 P R F1 
Baseline 97.3 76.8 85.9 97.4 77 86 100 19 32 
n = 3 97.4 77.9 86.5 95 81.2 87.6 100 23.8 38.5 
n = 4 97.4 77.9 86.5 95.8 78.5 86.2 91.7 26.2 40.7 
n = 5 97.4 80 87.9 95.9 80.5 87.5 86.7 30.9 45.6 
n =6 96.2 80 87.3 91 84.7 87.8 91.7 26.2 40.7 
n =7 96.2 80 87.3 92.4 84.7 88.4 91.7 26.2 40.7 
n =8 95 80 86.8 75 93.7 83.3 79.2 45.2 57.6 
n =9 92.8 82 87 89.3 86.8 88 79.2 45.2 57.6 
n = 10 90.9 84.2 87.4 65.9 97.9 78.8 79.2 45.2 57.6 
Table 10.  Results for SUB-M+: Best F1 bolded and 
italicized if significantly better than baseline. 
MAIN 
F1-measure 
E EM EM+ 
Person 94.4 95.2 95.6 
Location 89.9 90.8 90.9 
Organization 81.0 81.4 81.4 
Table 11.  Comparing results for MAIN-{E, EM, and 
EM+}: Best F1 bolded and italicized if significantly 
better than MAIN-E 
SUB 
F1-Measure 
E EM M EM+ M+ 
Person 96.2 95.6 86.5 95 85.9 
Location 95 93.8 86.9 94.5 86 
Organization 62.5 63.3 36.0 64.5 32 
Table 12.  Comparing results for SUB-{E, EM, M, 
EM+, and M+}:  Best F1 bolded  
MAIN 
F1-measure 
E EM EM+ 
Person 86.8 85.0 84.5 
Location 87.4 85.8 85.5 
Organization 58.0 51.8 53.4 
Table 13.  Comparing results of BOWs for MAIN-{E, 
EM, and EM+}: Best F1 bolded  
SUB 
F-Measure 
E EM M EM+ M+ 
Person 82.0 80.6 68.0 79.3 61.9 
Location 88.5 90.7 83.8 90.0 82.3 
Organization 35.6 22.6 21.4 33.3 22.6 
Table 14.  Comparing results of BOWs for SUB-{E, 
EM, M, EM+, and M+}: Best F1 bolded 
 
92
