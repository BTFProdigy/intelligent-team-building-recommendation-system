Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 75?78,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
NUKTI: English-Inuktitut Word Alignment System Description
Philippe Langlais, Fabrizio Gotti, Guihong Cao
RALI
De?partement d?Informatique et de Recherche Ope?rationnelle
Universite? de Montre?al
Succursale Centre-Ville
H3C 3J7 Montre?al, Canada
http://rali.iro.umontreal.ca
Abstract
Machine Translation (MT) as well as
other bilingual applications strongly
rely on word alignment. Efficient align-
ment techniques have been proposed
but are mainly evaluated on pairs of
languages where the notion of word
is mostly clear. We concentrated our
effort on the English-Inuktitut word
alignment shared task and report on
two approaches we implemented and a
combination of both.
1 Introduction
Word alignment is an important step in exploiting
parallel corpora. When efficient techniques have
been proposed (Brown et al, 1993; Och and Ney,
2003), they have been mostly evaluated on ?safe?
pairs of languages where the notion of word is
rather clear.
We devoted two weeks to the intriguing task
of aligning at the word level pairs of sentences
of English and Inuktitut. We experimented with
two different approaches. For the first one, we re-
lied on an in-house sentence alignment program
(JAPA) where English and Inuktitut tokens were
considered as sentences. The second approach
we propose takes advantage of associations com-
puted between any English word and roughly any
subsequence of Inuktitut characters seen in the
training corpus. We also investigated the combi-
nation of both approaches.
2 JAPA: Word Alignment as a Sentence
Alignment Task
To adjust our systems, the organizers made avail-
able to the participants a set of 25 pairs of sen-
tences where words had been manually aligned.
A fast inspection of this material reveals that in
most of the cases, the alignment produced are
monotonic and involve cepts of n adjacent En-
glish words aligned to a single Inuktitut word.
Many sentence alignment techniques strongly
rely on the monotonic nature of the inherent align-
ment. Therefore, we conducted a first experi-
ment using an in-house sentence alignment pro-
gram called JAPA that we developed within the
framework of the Arcade evaluation campaign
(Langlais et al, 1998). The implementation de-
tails of this aligner can be found in (Langlais,
1997), but in a few words, JAPA aligns pairs of
sentences by first grossly aligning their words
(making use of either cognate-like tokens, or a
specified bilingual dictionary). A second pass
aligns the sentences in a way similar1 to the algo-
rithm described by Gale and Church (1993), but
where the search space is constrained to be close
to the one delimited by the word alignment. This
technique happened to be among the most accu-
rate of the ones tested during the Arcade exercise.
To adapt JAPA to our needs, we only did two
things. First, we considered single sentences as
documents, and tokens as sentences (we define
a token as a sequence of characters delimited by
1In our case, the score we seek to globally maximize by
dynamic programming is not only taking into account the
length criteria described in (Gale and Church, 1993) but also
a cognate-based one similar to (Simard et al, 1992).
75
1-1 0.406 4-1 0.092 4-2 0.015
2-1 0.172 5-1 0.038 5-2 0.011
3-1 0.123 7-1 0.027 3-2 0.011
Table 1: The 9 most frequent English-Inuktitut
patterns observed on the development set. A total
of 24 different patterns have been observed.
white space). Second, since in its default setting,
JAPA only considers n-m sentence-alignment pat-
terns with n,m ? [0, 2], we provided it with a new
pattern distribution we computed from the devel-
opment corpus (see Table 1). It is interesting to
note that although English and Inuktitut have very
different word systems, the length ratio (in char-
acters) of the two sides of the TRAIN corpus is
1.05.
Each pair of documents (sentences) were then
aligned separately with JAPA. 1-n and n-1
alignments identified by JAPA where output with-
out further processing. Since the word alignment
format of the shared task do not account directly
for n-m alignments (n,m > 1) we generated the
cartesian product of the two sets of words for all
these n-m alignments produced by JAPA.
The performance of this approach is reported
in Table 2. Clearly, the precision is poor. This
is partly explained by the cartesian product we re-
sorted to when n-m alignments were produced by
JAPA. We provide in section 4 a way of improving
upon this scenario.
Prec. Rec. F-meas. AER
22.34 78.17 34.75 74.59
Table 2: Performance of the JAPA alignment tech-
nique on the DEV corpus.
3 NUKTI: Word and Substring
Alignment
Martin et al (2003) documented a study in build-
ing and using an English-Inuktitut bitext. They
described a sentence alignment technique tuned
for the specificity of the Inuktitut language, and
described as well a technique for acquiring cor-
respondent pairs of English tokens and Inuktitut
substrings. The motivation behind their work was
to populate a glossary with reliable such pairs.
We extended this line of work in order to achieve
word alignment.
3.1 Association Score
As Martin et al (2003) pointed out, the strong ag-
glutinative nature of Inuktitut makes it necessary
to consider subunits of Inuktitut tokens. This is
reflected by the large proportion of token types
and hapax words observed on the Inuktitut side
of the training corpus, compared to the ratios ob-
served on the English side (see table 3).
Inutktitut % English %
tokens 2 153 034 3 992 298
types 417 407 19.4 27 127 0.68
hapax 337 798 80.9 8 792 32.4
Table 3: Ratios of token types and happax words
in the TRAIN corpus.
The main idea presented in (Martin et al, 2003)
is to compute an association score between any
English word seen in the training corpus and all
the Inuktitut substrings of those tokens that were
seen in the same region. In our case, we com-
puted a likelihood ratio score (Dunning, 1993) for
all pairs of English tokens and Inuktitut substrings
of length ranging from 3 to 10 characters. A max-
imum of 25 000 associations were kept for each
English word (the top ranked ones).
To reduce the computation load, we used a suf-
fix tree structure and computed the association
scores only for the English words belonging to the
test corpus we had to align. We also filtered out
Inuktitut substrings we observed less than three
times in the training corpus. Altogether, it takes
about one hour for a good desktop computer to
produce the association scores for one hundred
English words.
We normalize the association scores such that
for each English word e, we have a distribution of
likely Inuktitut substrings s:
?
s pllr(s|e) = 1.
3.2 Word Alignment Strategy
Our approach for aligning an Inuktitut sentence
of K tokens IK1 with an English sentence of N
tokens EN1 (where K ? N )2 consists of finding
2As a matter of fact, the number of Inuktitut words in
the test corpus is always less than or equal to the number of
English tokens for any sentence pair.
76
K ? 1 cutting points ck?[1,K?1] (ck ? [1, N ? 1])
on the English side. A frontier ck delimits adja-
cent English words Eckck?1+1 that are translation of
the single Inuktitut word Ik. With the convention
that c0 = 0, cK = N and ck?1 < ck, we can for-
mulate our alignment problem as seeking the best
word alignment A = A(IK1 |EN1 ) by maximizing:
A = argmax
cK1
K?
k=1
p(Ik|E
ck
ck?1+1)
?1 ? p(dk)
?2
(1)
where dk = ck?ck?1 is the number of English
words associated to Ik; p(dk) is the prior proba-
bility that dk English words are aligned to a single
Inuktitut word, which we computed directly from
Table 1; and ?1 and ?2 are two weighting coeffi-
cients.
We tried the following two approximations to
compute p(Ik|Eckck?1+1). The second one led to
better results.
p(Ik|E
ck
ck?1+1) '
?
??
??
maxckj=ck?1+1 p(Ik|Ej)
or
?ck
j=ck?1+1
p(Ik|Ej)
We considered several ways of computing the
probability that an Inuktitut token I is the transla-
tion of an English one E; the best one we found
being:
p(I|E) '
?
s?I
?pllr(s|E) + (1? ?)pibm2(s|E)
where the summation is carried over all sub-
strings s of I of 3 characters or more. pllr(s|E)
is the normalized log-likelihood ratio score de-
scribed above and pibm2(s|E) is the probability
obtained from an IBM model 2 we trained after
the Inuktitut side of the training corpus was seg-
mented using a recursive procedure optimizing a
frequency-based criterion. ? is a weighting coef-
ficient.
We tried to directly embed a model trained
on whole (unsegmented) Inuktitut tokens, but no-
ticed a degradation in performance (line 2 of Ta-
ble 4).
3.3 A Greedy Search Strategy
Due to its combinatorial nature, the maximiza-
tion of equation 1 was barely tractable. There-
fore we adopted a greedy strategy to reduce the
search space. We first computed a split of the En-
glish sentence into K adjacent regions cK1 by vir-
tually drawing a diagonal line we would observe
if a character in one language was producing a
constant number of characters in the other one.
An initial word alignment was then found by sim-
ply tracking this diagonal at the word granularity
level.
Having this split in hand (line 1 of Table 4), we
move each cutting point around its initial value
starting from the leftmost cutting point and going
rightward. Once a locally optimal cutting point
has been found (that is, maximizing the score of
equation 1), we proceed to the next one directly
to its right.
3.4 Results
We report in Table 4 the performance of different
variants we tried as measured on the development
set. We used these performances to select the best
configuration we eventually submitted.
variant Prec. Rec. F-m. AER
start (diag) 51.7 53.66 52.66 49.54
greedy (word) 61.6 63.94 62.75 35.93
greedy (best) 63.5 65.92 64.69 34.21
Table 4: Performance of several NUKTI align-
ment techniques measured on the DEV corpus.
It is interesting to note that the starting point
of the greedy search (line 1) does better than our
first approach. However, moving from this ini-
tial split clearly improves the performance (line
3). Among the greedy variants we tested, we no-
ticed that putting much of the weight ? on the
IBM model 2 yielded the best results. We also no-
ticed that p(dk) in equation 1 did not help (?2 was
close to zero). A character-based model might
have been more appropriate to the case.
4 Combination of JAPA and NUKTI
One important weakness of our first approach lies
in the cartesian product we generate when JAPA
produces a n-m (n,m > 1) alignment. Thus,
we tried a third approach: we apply NUKTI on
any n-m alignment JAPA produces as if this ini-
tial alignment were in fact two (small) sentences
to align, n- and m-word long respectively. We can
77
therefore avoid the cartesian product and select
word alignments more discerningly. As can be
seen in Table 5, this combination improved over
JAPA alone, while being worse than NUKTI alone.
5 Results
We submitted 3 variants to the organizers. The
performances for each method are gathered in Ta-
ble 5. The order of merit of each approach was
consistent with the performance we measured on
the DEV corpus, the best method being the NUKTI
one. Curiously, we did not try to propose any Sure
alignment but did receive a credit for it for two of
the variants we submitted.
variant T. Prec. Rec. F-m. AER
JAPA P 26.17 74.49 38.73 71.27
JAPA + S 9.62 67.58 16.84
NUKTI P 51.34 53.60 52.44 46.64
NUKTI S 12.24 86.01 21.43
p 63.09 65.87 64.45 30.6
Table 5: Performance of the 3 alignments we sub-
mitted for the TEST corpus. T. stands for the type
of alignment (Sure or Possible).
6 Discussion
We proposed two methods for aligning an
English-Inuktitut bitext at the word level and a
combination of both. The best of these meth-
ods involves computing an association score be-
tween English tokens and Inuktitut substrings. It
relies on a greedy algorithm we specifically de-
vised for the task and which seeks a local opti-
mum of a cumulative function of log-likelihood
ratio scores. This method obtained a precision
and a recall above 63% and 65% respectively.
We believe this method could easily be im-
proved. First, it has some intrinsic limitations,
as for instance, the fact that NUKTI only recog-
nizes 1-n cepts and do not handle at all unaligned
words. Indeed, our method is not even suited to
aligning English sentences with fewer words than
their respective Inuktitut counterpart. Second, the
greedy search we devised is fairly aggressive and
only explores a tiny bit of the full search. Last,
the computation of the association scores is fairly
time-consuming.
Our idea of redefining word alignment as a sen-
tence alignment task did not work well; but at the
same time, we adapted poorly JAPA to this task.
In particular, JAPA does not benefit here from all
the potential of the underlying cognate system be-
cause of the scarcity of these cognates in very
small sequences (words).
If we had to work on this task again, we would
consider the use of a morphological analyzer. Un-
fortunately, it is only after the submission dead-
line that we learned of the existence of such a tool
for Inuktitut3.
Acknowledgement
We are grateful to Alexandre Patry who turned
the JAPA aligner into a nicely written and efficient
C++ program.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?311.
T. Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1).
W. A. Gale and K. W. Church. 1993. A Program for
Aligning Sentences in Bilingual Corpora. In Com-
putational Linguistics, volume 19, pages 75?102.
P. Langlais, M. Simard, and J. Ve?ronis. 1998. Meth-
ods and Practical Issues in Evaluating Alignment
Techniques. In 36th annual meeting of the ACL,
Montreal, Canada.
P. Langlais. 1997. A System to Align Complex Bilin-
gual Corpora. QPSR 4, TMH, Stockholm, Sweden.
J. Martin, H. Johnson, B. Farley, and A. Maclach-
lan. 2003. Aligning and Using an English-Inuktitut
Parallel Corpus. In Building and using Parallel
Texts: Data Driven Machine Translation and Be-
yond, pages 115?118, Edmonton, Canada.
F.J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Compu-
tational Linguistics, 29:19?51.
M. Simard, G.F. Foster, and P. Isabelle. 1992. Using
Cognates to Align Sentences in Bilingual Corpora.
In Conference on Theoretical and Methodological
Issues in Machine Translation, pages 67?81.
3See http://www.inuktitutcomputing.ca/
Uqailaut/
78
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 137?140,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
RALI: SMT shared task system description
Philippe Langlais, Guihong Cao and Fabrizio Gotti
RALI
De?partement d?Informatique et de Recherche Ope?rationnelle
Universite? de Montre?al
Succursale Centre-Ville
H3C3J7 Montre?al, Canada
http://rali.iro.umontreal.ca
Abstract
Thanks to the profusion of freely avail-
able tools, it recently became fairly
easy to built a statistical machine trans-
lation (SMT) engine given a bitext. The
expectations we can have on the quality
of such a system may however greatly
vary from one pair of languages to an-
other. We report on our experiments
in building phrase-based translation en-
gines for the four pairs of languages we
had to consider for the SMT shared-
task.
1 Introduction
Machine translation is nowadays mature enough
that it is possible without too much effort to de-
vise automatically a statistical translation system
from just a parallel corpus. This is possible
thanks to the dissemination of valuable packages.
The performance of such a system may however
greatly vary from one pair of languages to an-
other. Indeed, there is no free lunch for system
developers, and if a black box approach can some-
times be good enough for some applications (we
can surely accomplish translation gisting with the
French-English and Spanish-English systems we
developed during this exercice), making use of
the output of such a system for, let?s say, qual-
ity translation is another kettle of fish (especially
in our case with the Finnish-English system we
ended-up with).
We devoted two weeks to the SMT shared task,
the aim of which was precisely to see how well
systems can do across different language families.
We began with a core system which is described
in the next section and from which we obtained
baseline performances that we tried to improve
upon.
Since the French- and Spanish-English sys-
tems produced output that were comprehensi-
ble enough1, we focussed on the two languages
whose translations were noticeably worse: Ger-
man and Finnish. For German, we tried to move
around words in order to mimic English word or-
der; and we tried to split compound words. This
is described in section 4. For the Finnish/English
pair, we tried to decompose Finnish words into
smaller substrings (see section 5).
In parallel to that, we tried to smooth a phrase-
based model (PBM) making use of WORDNET.
We report on this experiment in section 3. We de-
scribe in section 6 the final setting of the systems
we used for submitting translations and their of-
ficial results as computed by the organizers. Fi-
nally, we conclude our two weeks of efforts in
section 7.
2 The core system
We assembled up a phrase-based statistical engine
by making use of freely available packages. The
translation engine we used is the one suggested
within the shared task: PHARAOH (Koehn, 2004).
The input of this decoder is composed of a phrase-
based model (PBM), a trigram language model
and an optional set of coefficients and thresholds
1What we mean by this is nothing more than we were
mostly able to infer the original meaning of the source sen-
tence by reading its automatic translation.
137
pair WER SER NIST BLEU
fi-en 66.53 99.20 5.3353 18.73
de-en 60.70 98.40 5.8411 21.11
fr-en 53.77 98.20 6.4717 27.69
es-en 53.84 98.60 6.5571 28.08
Table 1: Baseline performances measured on the
500 top sentences of the DEV corpus in terms of
WER (word error rate), SER (sentence error rate),
NIST and BLEU scores.
which control the decoder.
For acquiring a PBM, we followed the ap-
proach described by Koehn et al (2003). In brief,
we relied on a bi-directional word alignment of
the training corpus to acquire the parameters of
the model. We used the word alignment pro-
duced by Giza (Och and Ney, 2000) out of an
IBM model 2. We did try to use the alignment
produced with IBM model 4, but did not notice
significant differences over our experiments; an
observation consistent with the findings of Koehn
et al (2003). Each parameter in a PBM can be
scored in several ways. We considered its rela-
tive frequency as well as its IBM-model 1 score
(where the transfer probabilities were taken from
an IBM model 2 transfer table). The language
model we used was the one provided within the
shared task.
We obtained baseline performances by tuning
the engine on the top 500 sentences of the devel-
opment corpus. Since we only had a few param-
eters to tune, we did it by sampling the parameter
space uniformly. The best performance we ob-
tained, i.e., the one which maximizes the BLEU
metric as measured by the mteval script2 is re-
ported for each pair of languages in Table 1.
3 Smoothing PBMs with WORDNET
Among the things we tried but which did not
work well, we investigated whether smoothing
the transfer table of an IBM model (2 in our case)
with WORDNET would produce better estimates
for rare words. We adapted an approach proposed
by Cao et al (2005) for an Information Retrieval
task, and computed for any parameter (ei, fj) be-
2http://www.nist.gov/speech/tests/mt/
mt2001/resource
longing to the original model the following ap-
proximation:
p?(ei|fj) ?
?
e?E
pwn(ei|e)? pn(e|fj)
where E is the English vocabulary, pn desig-
nates the native distribution and pwn is the proba-
bility that two words in the English side are linked
together. We estimated this distribution by co-
occurrence counts over a large English corpus3.
To avoid taking into account unrelated but co-
occurring words, we used WORDNET to filter in
only the co-occurrences of words that are in re-
lation according to WORDNET. However, since
many words are not listed in this resource, we had
to smooth the bigram distribution, which we did
by applying Katz smoothing (Katz, 1997):
pkatz(ei|e) =
{
c?(ei,e|W,L)P
ej
c(ej ,e|W,L)
if c(ei, e|W,L) > 0
?(e)pkatz(ei) otherwise
where c?(a, b|W,L) is the good-turing dis-
counted count of times two words a and b that are
linked together by a WORDNET relation, co-occur
in a window of 2 sentences.
We used this smoothed model to score the pa-
rameters of our PBM instead of the native trans-
fer table. The results were however disappoint-
ing for both the G-E and S-E translation direc-
tions we tested. One reason for that, may be
that the English corpus we used for computing
the co-occurrence counts is an out-of-domain cor-
pus for the present task. Another possible ex-
planation lies in the fact that we considered both
synonymic and hyperonymic links in WORDNET;
the latter kind of links potentially introducing too
much noise for a translation task.
4 The German-English task
We identified two major problems with our ap-
proach when faced with this pair of languages.
First, the tendency in German to put verbs at the
end of a phrase happens to ruin our phrase acqui-
sition process, which basically collects any box
of aligned source and target adjacent words. This
3For this, we used the English side of the provided train-
ing corpus plus the English side of our in-house Hansard bi-
text; that is, a total of more than 7 million pairs of sentences.
138
can be clearly seen in the alignment matrix of fig-
ure 1 where the verbal construction could clarify
is translated by two very distant German words
ko?nnten and erla?utern. Second, there are many
compound words in German that greatly dilute
the various counts embedded in the PBM table.
. . . . . . . . . . . . . ?
erla?utern . . . . . . . ? . . . . .
punkt . . . . . . . . . ? . . .
einen . . . . . . . . ? . " . .
mir . . . . . . . . . . . ? .
sie . . . . . ? . . . . . . .
oder . . . . ? . . . . . . . .
kommission . . . ? . . . . . . . . .
die . . ? . . . . . . . . . .
ko?nnten . . . . . . ? . . . . . .
vielleicht . ? . . . . . . . . . . .
NULL . . . . . . . . . . . . .
N p t c o y c c a p f m .
U e h o r o o l o o e
L r e m u u a i r
L h m l r n
English perhaps the commission or you could
clarify a point for me .
German vielleicht ko?nnten die kommission oder
sie mir einen punkt erla?utern .
Figure 1: Bidirectional alignment matrix. A cross
in this matrix designates an alignment valid in
both directions, while the " symbol indicates an
uni-directional alignment (for has been aligned
with einen, but not the other way round).
4.1 Moving around German words
For the first problem, we applied a memory-based
approach to move around words in the German
side in order to better synchronize word order
in both languages. This involves, first, to learn-
ing transformation rules from the training corpus,
second, transforming the German side of this cor-
pus; then training a new translation model. The
same set of rules is then applied to the German
text to be translated.
The transformation rules we learned concern a
few (five in our case) verbal constructions that
we expressed with regular expressions built on
POS tags in the English side. Once the locus
e
v
u of a pattern has been identified, a rule is col-
lected whenever the following conditions apply:
for each word e in the locus, there is a target word
f which is aligned to e in both alignment direc-
tions; these target words when moved can lead to
a diagonal going from the target word (l) associ-
ated to eu?1 to the target word r which is aligned
to ev+1.
The rules we memorize are triplets (c, i, o)
where c = (l, r) is the context of the locus and i
and o are the input and output German word order
(that is, the order in which the tokens are found,
and the order in which they should be moved).
For instance, in the example of Figure 1,
the Verb Verb pattern match the locus could
clarify and the following rule is acquired:
(sie einen, ko?nnten erla?utern,
ko?nnten erla?utern), a paraphrase of
which is: ?whenever you find (in this order)
the word ko?nnten and erla?utern in a German
sentence containing also (in this order) sie and
einen, move ko?nnten and erla?utern between sie
and einen.
A set of 124 271 rules have been acquired
this way from the training corpus (for a total of
157 970 occurrences). The most frequent rule ac-
quired is (ich herrn, mo?chte danken,
mo?chte danken), which will transform a sen-
tence like ?ich mo?chte herrn wynn fu?r seinen
bericht danken.? into ?ich mo?chte danken herrn
wynn fu?r seinen bericht.?.
In practice, since this acquisition process does
not involve any generalization step, only a few
rules learnt really fire when applied to the test ma-
terial. Also, we devised a fairly conservative way
of applying the rules, which means that in prac-
tice, only 3.5% of the sentences of the test corpus
where actually modified.
The performance of this procedure as measured
on the development set is reported in Table 2. As
simple as it is, this procedure yields a relative gain
of 7% in BLEU. Given the crudeness of our ap-
proach, we consider this as an encouraging im-
provement.
4.2 Compound splitting
For the second problem, we segmented German
words before training the translation models. Em-
pirical methods for compound splitting applied to
139
system WER SER NIST BLEU
baseline 60.70 98.40 5.8411 21.11
swap 60.73 98.60 5.9643 22.58
split 60.67 98.60 5.7511 21.99
swap+split 60.57 98.40 5.9685 23.10
Table 2: Performances of the swapping and the
compound splitting approaches on the top 500
sentences of the development set.
German have been studied by Koehn and Knight
(2003). They found that a simple splitting strat-
egy based on the frequency of German words was
the most efficient method of the ones they tested,
when embedded in a phrase-based translation en-
gine. Therefore, we applied such a strategy to
split German words in our corpora. The results
of this approach are shown in Table 2.
Note: Both the swapping strategy and the com-
pound splitting yielded improvements in terms of
BLEU score. Only after the deadline did we find
time to train new models with a combination of
both techniques; the results of which are reported
in the last line of Table 2.
5 The Finnish-English task
The worst performances were registered on the
Finnish-English pair. This is due to the aggluti-
native nature of Finnish. We tried to segment the
Finnish material into smaller units (substrings) by
making use of the frequency of all Finnish sub-
strings found in the training corpus. We main-
tained a suffix tree structure for that purpose.
We proceeded by recursively finding the most
promising splitting points in each Finnish token
of C characters FC1 by computing split(FC1 )
where:
split(F ji ) =
?
?
?
|F ji | if j ? i < 2
maxc?[i+2,j?2] |F
c
i |?
split(F jc+1) otherwise
This approach yielded a significant degradation
in performance that we still have to analyze.
6 Submitted translations
At the time of the deadline, the best translations
we had were the baselines ones for all the lan-
guage pairs, except for the German-English one
where the moving of words ranked the best. This
defined the configuration we submitted, whose re-
sults (as provided by the organizers) are reported
in Table 3.
pair BLEU p1/p2/p3/p4
fi-en 18.87 55.2/24.7/13.1/7.1
de-en 22.91 58.9/29.0/16.8/10.3
es-en 28.49 62.4/34.5/21.9/14.4
fr-en 28.89 62.6/34.7/22.0/14.6
Table 3: Results measured by the organizers for
the TEST corpus.
7 Conclusion
We found that, while comprehensible translations
were produced for pairs of languages such as
French-English and Spanish-English; things did
not go as well for the German-English pair and
especially not for the Finnish-English pair. We
had a hard time improving our baseline perfor-
mance in such a tight schedule and only man-
aged to improve our German-English system. We
were less lucky with other attempts we imple-
mented, among them, the smoothing of a trans-
fer table with WORDNET, and the segmentation
of the Finnish corpus into smaller units.
References
G. Cao, J. Nie, and J. Bai. 2005. Integrating Word
relationships into Language Models. In to appear
in Proc. of SIGIR.
S. Katz. 1997. Estimation of Probabilities from
Sparse Data for the Language Model Component of
a Speech Recognizer. IEEE Transactions on Acous-
tics Speech and Signal Processing, 35.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of HLT,
pages 127?133.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder
for Phrase-Based SMT. In Proceedings of AMTA,
pages 115?124.
F.J. Och and H. Ney. 2000. Improved Statistical
Alignment Models. In Proceedings of ACL, pages
440?447, Hongkong, China.
140
Proceedings of the Workshop on Statistical Machine Translation, pages 39?46,
New York City, June 2006. c?2006 Association for Computational Linguistics
Phrase-Based SMT with Shallow Tree-Phrases
Philippe Langlais and Fabrizio Gotti
RALI ? DIRO
Universite? de Montre?al,
C.P. 6128 Succ. Centre-Ville
H3C 3J7, Montre?al, Canada
{felipe,gottif}@iro.umontreal.ca
Abstract
In this article, we present a translation
system which builds translations by glu-
ing together Tree-Phrases, i.e. associ-
ations between simple syntactic depen-
dency treelets in a source language and
their corresponding phrases in a target
language. The Tree-Phrases we use in
this study are syntactically informed and
present the advantage of gathering source
and target material whose words do not
have to be adjacent. We show that the
phrase-based translation engine we imple-
mented benefits from Tree-Phrases.
1 Introduction
Phrase-based machine translation is now a popular
paradigm. It has the advantage of naturally cap-
turing local reorderings and is shown to outper-
form word-based machine translation (Koehn et al,
2003). The underlying unit (a pair of phrases), how-
ever, does not handle well languages with very dif-
ferent word orders and fails to derive generalizations
from the training corpus.
Several alternatives have been recently proposed
to tackle some of these weaknesses. (Matusov et
al., 2005) propose to reorder the source text in or-
der to mimic the target word order, and then let a
phrase-based model do what it is good at. (Simard
et al, 2005) detail an approach where the standard
phrases are extended to account for ?gaps? either on
the target or source side. They show that this repre-
sentation has the potential to better exploit the train-
ing corpus and to nicely handle differences such as
negations in French and English that are poorly han-
dled by standard phrase-based models.
Others are considering translation as a syn-
chronous parsing process e.g. (Melamed, 2004;
Ding and Palmer, 2005)) and several algorithms
have been proposed to learn the underlying produc-
tion rule probabilities (Graehl and Knight, 2004;
Ding and Palmer, 2004). (Chiang, 2005) proposes
an heuristic way of acquiring context free transfer
rules that significantly improves upon a standard
phrase-based model.
As mentioned in (Ding and Palmer, 2005), most
of these approaches require some assumptions on
the level of isomorphism (lexical and/or structural)
between two languages. In this work, we consider
a simple kind of unit: a Tree-Phrase (TP), a com-
bination of a fully lexicalized treelet (TL) and an
elastic phrase (EP), the tokens of which may be in
non-contiguous positions. TPs capture some syntac-
tic information between two languages and can eas-
ily be merged with standard phrase-based engines.
A TP can be seen as a simplification of the treelet
pairs manipulated in (Quirk et al, 2005). In particu-
lar, we do not address the issue of projecting a source
treelet into a target one, but take the bet that collect-
ing (without structure) the target words associated
with the words encoded in the nodes of a treelet will
suffice to allow translation. This set of target words
is what we call an elastic phrase.
We show that these units lead to (modest) im-
provements in translation quality as measured by au-
tomatic metrics. We conducted all our experiments
39
on an in-house version of the French-English Cana-
dian Hansards.
This paper is organized as follows. We first define
a Tree-Phrase in Section 2, the unit with which we
built our system. Then, we describe in Section 3
the phrase-based MT decoder that we designed to
handle TPs. We report in Section 4 the experiments
we conducted combining standard phrase pairs and
TPs. We discuss this work in Section 5 and then
conclude in Section 6.
2 Tree-Phrases
We call tree-phrase (TP) a bilingual unit consisting
of a source, fully-lexicalized treelet (TL) and a tar-
get phrase (EP), that is, the target words associated
with the nodes of the treelet, in order. A treelet can
be an arbitrary, fully-lexicalized subtree of the parse
tree associated with a source sentence. A phrase can
be an arbitrary sequence of words. This includes
the standard notion of phrase, popular with phrased-
based SMT (Koehn et al, 2003; Vogel et al, 2003)
as well as sequences of words that contain gaps (pos-
sibly of arbitrary size).
In this study, we collected a repository of tree-
phrases using a robust syntactic parser called SYN-
TEX (Bourigault and Fabre, 2000). SYNTEX identi-
fies syntactic dependency relations between words.
It takes as input a text processed by the TREETAG-
GER part-of-speech tagger.1 An example of the out-
put SYNTEX produces for the source (French) sen-
tence ?on a demande? des cre?dits fe?de?raux? (request
for federal funding) is presented in Figure 1.
We parsed with SYNTEX the source (French) part
of our training bitext (see Section 4.1). From this
material, we extracted all dependency subtrees of
depth 1 from the complete dependency trees found
by SYNTEX. An elastic phrase is simply the list of
tokens aligned with the words of the corresponding
treelet as well as the respective offsets at which they
were found in the target sentence (the first token of
an elastic phrase always has an offset of 0).
For instance, the two treelets in Figure 2 will be
collected out of the parse tree in Figure 1, yielding
2 tree-phrases. Note that the TLs as well as the EPs
might not be contiguous as is for instance the case
1www.ims.uni-stuttgart.de/projekte/
corplex/.
a demande?
SUB
ll
ll
ll
ll
ll OBJ
YYY
YYY
YYY
YYY
YYY
YYY
on cre?dits
DET
ll
ll
ll
ll
ll ADJ
RR
RR
RR
RR
RR
des fe?de?raux
Figure 1: Parse of the sentence ?on a demande? des
cre?dits fe?de?raux? (request for federal funding). Note
that the 2 words ?a? and ?demande?? (literally ?have?
and ?asked?) from the original sentence have been
merged together by SYNTEX to form a single token.
These tokens are the ones we use in this study.
with the first pair of structures listed in the example.
3 The Translation Engine
We built a translation engine very similar to the sta-
tistical phrase-based engine PHARAOH described in
(Koehn, 2004) that we extended to use tree-phrases.
Not only does our decoder differ from PHARAOH by
using TPs, it also uses direct translation models. We
know from (Och and Ney, 2002) that not using the
noisy-channel approach does not impact the quality
of the translation produced.
3.1 The maximization setting
For a source sentence f , our engine incrementally
generates a set of translation hypotheses H by com-
bining tree-phrase (TP) units and phrase-phrase (PP)
units.2 We define a hypothesis in this set as h =
{Ui ? (Fi, Ei)}i?[1,u], a set of u pairs of source
(Fi) and target sequences (Ei) of ni and mi words
respectively:
Fi ? {fjin : j
i
n ? [1, |f |]}n?[1,ni]
Ei ? {elim : l
i
m ? [1, |e|]}m?[1,mi]
under the constraints that for all i ? [1, u], jin <
jin+1 ,?n ? [1, ni[ for a source treelet (similar con-
straints apply on the target side), and jin+1 = j
i
n +
1 ,?n ? [1, ni[ for a source phrase. The way the
hypotheses are built imposes additional constraints
between units that will be described in Section 3.3.
Note that, at decoding time, |e|, the number of words
2What we call here a phrase-phrase unit is simply a pair of
source/target sequences of words.
40
alignment:
a demande? ? request for, fe?de?raux ? federal,
cre?dits ? funding
treelets:
a demande?
q
q
q
q
q
q
q
M
M
M
M
M
M
M
on cre?dits
cre?dits
q
q
q
q
q
q
q
M
M
M
M
M
M
M
des fe?de?raux
tree-phrases:
TL? {{on@-1} a_demande? {cre?dits@2}}
EP? |request@0||for@1||funding@3|
TL {{des@-1} cre?dits {fe?de?raux@1}}
EP |federal@0||funding@1|
Figure 2: The Tree-Phrases collected out of the
SYNTEX parse for the sentence pair of Figure 1.
Non-contiguous structures are marked with a star.
Each dependent node of a given governor token is
displayed as a list surrounding the governor node,
e.g. {governor {right-dependent}}. Along with the
tokens of each node, we present their respective off-
set (the governor/root node has the offset 0 by defi-
nition). The format we use to represent the treelets
is similar to the one proposed in (Quirk et al, 2005).
of the translation is unknown, but is bounded accord-
ing to |f | (in our case, |e|max = 2? |f |+ 5).
We define the source and target projection of a
hypothesis h by the proj operator which collects in
order the words of a hypothesis along one language:
projF (h) =
{
fp : p ?
?u
i=1{j
i
n}n?[1,ni]
}
projE(h) =
{
ep : p ?
?u
i=1{l
i
m}m?[1,mi]
}
If we denote by Hf the set of hypotheses that
have f as a source projection (that is, Hf = {h :
projF (h) ? f}), then our translation engine seeks
e? = projE(h?) where:
h? = argmax
h?Hf
s(h)
The function we seek to maximize s(h) is a log-
linear combination of 9 components, and might be
better understood as the numerator of a maximum
entropy model popular in several statistical MT sys-
tems (Och and Ney, 2002; Bertoldi et al, 2004; Zens
and Ney, 2004; Simard et al, 2005; Quirk et al,
2005). The components are the so-called feature
functions (described below) and the weighting co-
efficients (?) are the parameters of the model:
s(h) = ?pprf log ppprf (h) + ?p|h|+
?tprf log ptprf (h) + ?t|h|+
?ppibm log pppibm(h)+
?tpibm log ptpibm(h)+
?lm log plm(projE(h))+
?d d(h) + ?w|projE(h)|
3.2 The components of the scoring function
We briefly enumerate the features used in this study.
Translation models Even if a tree-phrase is a gen-
eralization of a standard phrase-phrase unit, for in-
vestigation purposes, we differentiate in our MT
system between two kinds of models: a TP-based
model ptp and a phrase-phrase model ppp. Both rely
on conditional distributions whose parameters are
learned over a corpus. Thus, each model is assigned
its own weighting coefficient, allowing the tuning
process to bias the engine toward a special kind of
unit (TP or PP).
We have, for k ? {rf, ibm}:
pppk(h) =
?u
i=1 ppp(Ei|Fi)
ptpk(h) =
?u
i=1 ptp(Ei|Fi)
with p?rf standing for a model trained by rel-
ative frequency, whereas p?ibm designates a non-
normalized score computed by an IBM model-1
translation model p, where f0 designates the so-
called NULL word:
p?ibm(Ei|Fi) =
mi?
m=1
ni?
n=1
p(elim |fjin) + p(ekim |f0)
Note that by setting ?tprf and ?tpibm to zero, we
revert back to a standard phrase-based translation
engine. This will serve as a reference system in the
experiments reported (see Section 4).
The language model Following a standard prac-
tice, we use a trigram target language model
plm(projE(h)) to control the fluency of the trans-
lation produced. See Section 3.3 for technical sub-
tleties related to their use in our engine.
41
Distortion model d This feature is very similar to
the one described in (Koehn, 2004) and only de-
pends on the offsets of the source units. The only
difference here arises when TPs are used to build a
translation hypothesis:
d(h) = ?
n?
i=1
abs(1 + F i?1 ? F i)
where:
F i =
{ ?
n?[1,ni] j
i
n/ni if Fi is a treelet
jini otherwise
F i = j
i
1
This score encourages the decoder to produce a
monotonous translation, unless the language model
strongly privileges the opposite.
Global bias features Finally, three simple fea-
tures help control the translation produced. Each
TP (resp. PP) unit used to produce a hypothesis
receives a fixed weight ?t (resp. ?p). This allows
the introduction of an artificial bias favoring either
PPs or TPs during decoding. Each target word pro-
duced is furthermore given a so-called word penalty
?w which provides a weak way of controlling the
preference of the decoder for long or short transla-
tions.
3.3 The search procedure
The search procedure is described by the algorithm
in Figure 3. The first stage of the search consists in
collecting all the units (TPs or PPs) whose source
part matches the source sentence f . We call U the
set of those matching units.
In this study, we apply a simple match policy that
we call exact match policy. A TL t matches a source
sentence f if its root matches f at a source position
denoted r and if all the other words w of t satisfy:
fow+r = w
where ow designates the offset of w in t.
Hypotheses are built synchronously along with
the target side (by appending the target material to
the right of the translation being produced) by pro-
gressively covering the positions of the source sen-
tence f being translated.
Require: a source sentence f
U ? {u : s-match(u, f)}
FUTURECOST(U)
for s? 1 to |f | do
S[s]? ?
S[0]? {(?, , 0)}
for s? 0 to |f | ? 1 do
PRUNE(S[s], ?)
for all hypotheses alive h ? S[s] do
for all u ? U do
if EXTENDS(u, h) then
h? ? UPDATE(u, h)
k ? |projF (h?)|
S[k]? S[k] ? {h?}
return argmaxh?S[|f |] ? : h? (ps, t, ?)
Figure 3: The search algorithm. The symbol ? is
used in place of assignments, while? denotes uni-
fication (as in languages such as Prolog).
The search space is organized into a set S of |f |
stacks, where a stack S[s] (s ? [1, |f |]) contains all
the hypotheses covering exactly s source words. A
hypothesis h = (ps, t, ?) is composed of its target
material t, the source positions covered ps as well as
its score ?. The search space is initialized with an
empty hypothesis: S[0] = {(?, , 0)}.
The search procedure consists in extending each
partial hypothesis h with every unit that can con-
tinue it. This process ends when all partial hypothe-
ses have been expanded. The translation returned is
the best one contained in S[|f |]:
e? = projE(argmax
h?S[|f |]
? : h? (ps, t, ?))
PRUNE ? In order to make the search tractable,
each stack S[s] is pruned before being expanded.
Only the hypotheses whose scores are within a frac-
tion (controlled by a meta-parameter ? which typi-
cally is 0.0001 in our experiments) of the score of
the best hypothesis in that stack are considered for
expansion. We also limit the number of hypotheses
maintained in a given stack to the top maxStack
ones (maxStack is typically set to 500).
Because beam-pruning tends to promote in a stack
partial hypotheses that translate easy parts (i.e. parts
42
that are highly scored by the translation and lan-
guage models), the score considered while pruning
not only involves the cost of a partial hypothesis so
far, but also an estimation of the future cost that will
be incurred by fully expanding it.
FUTURECOST ? We followed the heuristic de-
scribed in (Koehn, 2004), which consists in comput-
ing for each source range [i, j] the minimum cost
c(i, j) with which we can translate the source se-
quence f ji . This is pre-computed efficiently at an
early stage of the decoding (second line of the algo-
rithm in Figure 3) by a bottom-up dynamic program-
ming scheme relying on the following recursion:
c(i, j) = min
{
mink?[i,j[c(i, k) + c(k, j)
minu?U/us?fji =us
score(us)
where us stands for the projection of u on the tar-
get side (us ? projE(u)), and score(u) is com-
puted by considering the language model and the
translation components ppp of the s(h) score. The
future cost of h is then computed by summing the
cost c(i, j) of all its empty source ranges [i, j].
EXTENDS ? When we simply deal with standard
(contiguous) phrases, extending a hypothesis h by a
unit u basically requires that the source positions of
u be empty in h. Then, the target material of u is
appended to the current hypothesis h.
Because we work with treelets here, things are
a little more intricate. Conceptually, we are con-
fronted with the construction of a (partial) source
dependency tree while collecting the target mate-
rial in order. Therefore, the decoder needs to check
whether a given TL (the source part of u) is compati-
ble with the TLs belonging to h. Since we decided in
this study to use depth-one treelets, we consider that
two TLs are compatible if either they do not share
any source word, or, if they do, this shared word
must be the governor of one TL and a dependent in
the other TL.
So, for instance, in the case of Figure 2, the
two treelets are deemed compatible (they obviously
should be since they both belong to the same orig-
inal parse tree) because cre?dit is the governor
in the right-hand treelet while being the depen-
dent in the left-hand one. On the other hand, the
two treelets in Figure 4 are not, since pre?sident
is the governor of both treelets, even though mr.
le pre?sident supple?ant would be a valid
source phrase. Note that it might be the case that
the treelet {{mr.@-2} {le@-1} pre?sident
{supple?ant@1}} has been observed during
training, in which case it will compete with the
treelets in Figure 2.
pre?sident
mr.
pre?sident
q
q
q
q
q
q
q
M
M
M
M
M
M
M
le supple?ant
Figure 4: Example of two incompatible treelets.
mr. speaker and the acting speaker
are their respective English translations.
Therefore, extending a hypothesis containing a
treelet with a new treelet consists in merging the two
treelets (if they are compatible) and combining the
target material accordingly. This operation is more
complicated than in a standard phrase-based decoder
since we allow gaps on the target side as well. More-
over, the target material of two compatible treelets
may intersect. This is for instance the case for the
two TPs in Figure 2 where the word funding is
common to both phrases.
UPDATE ? Whenever u extends h, we add a
new hypothesis h? in the corresponding stack
S[|projF (h?)|]. Its score is computed by adding to
that of h the score of each component involved in
s(h). For all but the one language model compo-
nent, this is straightforward. However, care must be
taken to update the language model score since the
target material of u does not come necessarily right
after that of h as would be the case if we only ma-
nipulated PP units.
Figure 5 illustrates the kind of bookkeeping
required. In practice, the target material of
a hypothesis is encoded as a vector of triplets
{?wi, log plm(wi|ci), li?}i?[1,|e|max] where wi is the
word at position i in the translation, log plm(wi|ci)
is its score as given by the language model, ci de-
notes the largest conditioning context possible, and
li indicates the length (in words) of ci (0 means a
unigram probability, 1 a bigram probability and 2 a
trigram probability). This vector is updated at each
extension.
43
u
des f?d?rauxon a_demand? cr?dits
TL: {on@?1}  a_demand?  {cr?dits@2}
EP: request@0  for@1  funding@3
TL: {des@?1}  cr?dits  {f?d?raux@1}
EP: federal@0  funding@1
U B F Urequest for funding
cr?ditson a_demand? des f?d?raux
forrequest fundingU B T Tfederal
h
h?
S[3]
S[4]
u
Figure 5: Illustration of the language model up-
dates that must be made when a new target unit
(circles with arrows represent dependency links) ex-
tends an existing hypothesis (rectangles). The tag
inside each occupied target position shows whether
this word has been scored by a Unigram, a Bigram
or a Trigram probability.
4 Experimental Setting
4.1 Corpora
We conducted our experiments on an in-house ver-
sion of the Canadian Hansards focussing on the
translation of French into English. The split of this
material into train, development and test corpora is
detailed in Table 1. The TEST corpus is subdivided
in 16 (disjoints) slices of 500 sentences each that
we translated separately. The vocabulary is atypi-
cally large since some tokens are being merged by
SYNTEX, such as e?taient#finance?es (were
financed in English).
The training corpus has been aligned at the
word level by two Viterbi word-alignments
(French2English and English2French) that we
combined in a heuristic way similar to the refined
method described in (Och and Ney, 2003). The
parameters of the word models (IBM model 2) were
trained with the GIZA++ package (Och and Ney,
2000).
TRAIN DEV TEST
sentences 1 699 592 500 8000
e-toks 27 717 389 8 160 130 192
f-toks 30 425 066 8 946 143 089
e-toks/sent 16.3 (? 9.0) 16.3 (? 9.1) 16.3 (? 9.0)
f-toks/sent 17.9 (? 9.5) 17.9 (? 9.5) 17.9 (? 9.5)
e-types 164 255 2 224 12 591
f-types 210 085 2 481 15 008
e-hapax 68 506 1 469 6 887
f-hapax 90 747 1 704 8 612
Table 1: Main characteristics of the corpora used in
this study. For each language l, l-toks is the number
of tokens, l-toks/sent is the average number of to-
kens per sentence (? the standard deviation), l-types
is the number of different token forms and l-hapax
is the number of tokens that appear only once in the
corpus.
4.2 Models
Tree-phrases Out of 1.7 million pairs of sen-
tences, we collected more than 3 million different
kinds of TLs from which we projected 6.5 million
different kinds of EPs. Slightly less than half of
the treelets are contiguous ones (i.e. involving a se-
quence of adjacent words); 40% of the EPs are con-
tiguous. When the respective frequency of each TL
or EP is factored in, we have approximately 11 mil-
lion TLs and 10 million EPs. Roughly half of the
treelets collected have exactly two dependents (three
word long treelets).
Since the word alignment of non-contiguous
phrases is likely to be less accurate than the align-
ment of adjacent word sequences, we further filter
the repository of TPs by keeping the most likely EPs
for each TL according to an estimate of p(EP |TL)
that do not take into account the offsets of the EP or
the TL.
PP-model We collected the PP parameters by sim-
ply reading the alignment matrices resulting from
the word alignment, in a way similar to the one
described in (Koehn et al, 2003). We use an in-
house tool to collect pairs of phrases of up to 8
words. Freely available packages such as THOT
(Ortiz-Mart??nez et al, 2005) could be used as well
for that purpose.
44
Language model We trained a Kneser-Ney tri-
gram language model using the SRILM toolkit (Stol-
cke, 2002).
4.3 Protocol
We compared the performances of two versions of
our engine: one which employs TPs ans PPs (TP-
ENGINE hereafter), and one which only uses PPs
(PP-ENGINE). We translated the 16 disjoint sub-
corpora of the TEST corpus with and without TPs.
We measure the quality of the translation pro-
duced with three automatic metrics. Two error
rates: the sentence error rate (SER) and the word
error rate (WER) that we seek to minimize, and
BLEU (Papineni et al, 2002), that we seek to
maximize. This last metric was computed with
the multi-bleu.perl script available at www.
statmt.org/wmt06/shared-task/.
We separately tuned both systems on the DEV cor-
pus by applying a brute force strategy, i.e. by sam-
pling uniformly the range of each parameter (?) and
picking the configuration which led to the best BLEU
score. This strategy is inelegant, but in early experi-
ments we conducted, we found better configurations
this way than by applying the Simplex method with
multiple starting points. The tuning roughly takes
24 hours of computation on a cluster of 16 comput-
ers clocked at 3 GHz, but, in practice, we found that
one hour of computation is sufficient to get a con-
figuration whose performances, while subobptimal,
are close enough to the best one reachable by an ex-
haustive search.
Both configurations were set up to avoid distor-
tions exceeding 3 (maxDist = 3). Stacks were
allowed to contain no more than 500 hypotheses
(maxStack = 500) and we further restrained the
number of hypotheses considered by keeping for
each matching unit (treelet or phrase) the 5 best
ranked target associations. This setting has been
fixed experimentally on the DEV corpus.
4.4 Results
The scores for the 16 slices of the test corpus are re-
ported in Table 2. TP-ENGINE shows slightly better
figures for all metrics.
For each system and for each metric, we had
16 scores (from each of the 16 slices of the test cor-
pus) and were therefore able to test the statistical sig-
nicance of the difference between the TP-ENGINE
and PP-ENGINE using a Wilcoxon signed-rank test
for paired samples. This test showed that the dif-
ference observed between the two systems is signif-
icant at the 95% probability level for BLEU and sig-
nificant at the 99% level for WER and SER.
Engine WER% SER% BLEU%
PP 52.80 ? 1.2 94.32 ? 0.9 29.95 ? 1.2
TP 51.98 ? 1.2 92.83 ? 1.3 30.47 ? 1.4
Table 2: Median WER, SER and BLEU scores
(? value range) of the translations produced by the
two engines on a test set of 16 disjoint corpora of
500 sentences each. The figures reported are per-
centages.
On the DEV corpus, we measured that, on aver-
age, each source sentence is covered by 39 TPs (their
source part, naturally), yielding a source coverage of
approximately 70%. In contrast, the average number
of covering PPs per sentence is 233.
5 Discussion
On a comparable test set (Canadian Hansard texts),
(Simard et al, 2005) report improvements by adding
non-contiguous bi-phrases to their engine without
requiring a parser at all. At the same time, they also
report negative results when adding non-contiguous
phrases computed from the refined alignment tech-
nique that we used here.
Although the results are not directly comparable,
(Quirk et al, 2005) report much larger improve-
ments over a phrase-based statistical engine with
their translation engine that employs a source parser.
The fact that we consider only depth-one treelets in
this work, coupled with the absence of any particular
treelet projection algorithm (which prevents us from
training a syntactically motivated reordering model
as they do) are other possible explanations for the
modest yet significant improvements we observe in
this study.
6 Conclusion
We presented a pilot study aimed at appreciating the
potential of Tree-Phrases as base units for example-
based machine translation.
45
We developed a translation engine which makes
use of tree-phrases on top of pairs of source/target
sequences of words. The experiments we conducted
suggest that TPs have the potential to improve trans-
lation quality, although the improvements we mea-
sured are modest, yet statistically significant.
We considered only one simple form of tree in this
study: depth-one subtrees. We plan to test our en-
gine on a repository of treelets of arbitrary depth. In
theory, there is not much to change in our engine
to account for such units and it would offer an al-
ternative to the system proposed recently by (Liu et
al., 2005), which performs translations by recycling
a collection of tree-string-correspondence (TSC) ex-
amples.
References
Nicola Bertoldi, Roldano Cattoni, Mauro Cettolo, and
Marcello Federico. 2004. The ITC-irst statistical ma-
chine translation system for IWSLT-2004. In IWSLT,
pages 51?58, Kyoto, Japan.
Didier Bourigault and Ce?cile Fabre. 2000. Ap-
proche linguistique pour l?analyse syntaxique de cor-
pus. Cahiers de Grammaire, (25):131?151. Toulouse
le Mirail.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In 43rd ACL, pages
263?270, Ann Arbor, Michigan, USA.
Yuang Ding and Martha Palmer. 2004. Automatic learn-
ing of parallel dependency treelet pairs. In Proceed-
ings of the first International Joint Conference on NLP.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In 43rd ACL, pages 541?548, Ann
Arbor, Michigan, June.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In HLT-NAACL 2004, pages 105?112,
Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT, pages 127?133.
Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder
for Phrase-Based SMT. In Proceedings of AMTA,
pages 115?124.
Zhanyi Liu, Haifeng Wang, and Hua Wu. 2005.
Example-based machine translation based on tsc and
statistical generation. In Proceedings of MT Summit
X, pages 25?32, Phuket, Thailand.
Evgeny Matusov, Stephan Kanthak, and Hermann Ney.
2005. Efficient statistical machine translation with
constraint reordering. In 10th EAMT, pages 181?188,
Budapest, Hongary, May 30-31.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In 42nd ACL, pages 653?660, Barcelona,
Spain.
Franz Joseph Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440?447, Hongkong, China.
Franz Joseph Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the ACL,
pages 295?302.
Franz Joseph Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Daniel Ortiz-Mart??nez, Ismael Garcia?-Varea, and Fran-
cisco Casacuberta. 2005. Thot: a toolkit to train
phrase-based statistical translation models. In Pro-
ceedings of MT Summit X, pages 141?148, Phuket,
Thailand, Sep.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In 40th ACL, pages 311?
318, Philadelphia, Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In 43rd ACL, pages 271?279, Ann Ar-
bor, Michigan, June.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, Eric Gaussier, Cyril Goutte,
Kenji Yamada, Philippe Langlais, and Arne Mauser.
2005. Translating with non-contiguous phrases. In
HLT/EMNLP, pages 755?762, Vancouver, British
Columbia, Canada, Oct.
Andreas Stolcke. 2002. Srilm - an Extensible Language
Modeling Toolkit. In Proceedings of ICSLP, Denver,
Colorado, Sept.
Stephan Vogel, Ying Zhang, Fei Huang, Alicai Trib-
ble, Ashish Venugopal, Bing Zao, and Alex Waibel.
2003. The CMU Statistical Machine Translation Sys-
tem. InMachine Translation Summit IX, New Orleans,
Louisina, USA, Sep.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of the HLT/NAACL, pages 257?264, Boston,
MA, May.
46
Proceedings of the Workshop on Statistical Machine Translation, pages 126?129,
New York City, June 2006. c?2006 Association for Computational Linguistics
Mood at work: Ramses versus Pharaoh
Alexandre Patry, Fabrizio Gotti and Philippe Langlais
RALI ? DIRO
Universite? de Montre?al
{patryale,gottif,felipe}@iro.umontreal.ca
Abstract
We present here the translation system we
used in this year?s WMT shared task. The
main objective of our participation was
to test RAMSES, an open source phrase-
based decoder. For that purpose, we used
the baseline system made available by the
organizers of the shared task1 to build the
necessary models. We then carried out a
pair-to-pair comparison of RAMSES with
PHARAOH on the six different translation
directions that we were asked to perform.
We present this comparison in this paper.
1 Introduction
Phrase-based (PB) machine translation (MT) is now
a popular paradigm, partly because of the relative
ease with which we can automatically create an ac-
ceptable translation engine from a bitext. As a mat-
ter of fact, deriving such an engine from a bitext con-
sists in (more or less) gluing together dedicated soft-
ware modules, often freely available. Word-based
models, or the so-called IBM models, can be trained
using the GIZA or GIZA++ toolkits (Och and Ney,
2000). One can then train phrase-based models us-
ing the THOT toolkit (Ortiz-Mart??nez et al, 2005).
For their part, language models currently in use in
SMT systems can be trained using packages such as
SRILM (Stolcke, 2002) and the CMU-SLM toolkit
(Clarkson and Rosenfeld, 1997).
1www.statmt.org/wmt06/shared-task/
baseline.html
Once all the models are built, one can choose
to use PHARAOH (Koehn, 2004), an efficient full-
fledged phrase-based decoder. We only know of
one major drawback when using PHARAOH: its
licensing policy. Indeed, it is available for non-
commercial use in its binary form only. This
severely limits its use, both commercially and sci-
entifically (Walker, 2005).
For this reason, we undertook the design of a
generic architecture called MOOD (Modular Object-
Oriented Decoder), especially suited for instantiat-
ing SMT decoders. Two major goals directed our
design of this package: offering open source, state-
of-the-art decoders and providing an architecture to
easily build these decoders. This effort is described
in (Patry et al, 2006).
As a proof of concept that our framework (MOOD)
is viable, we attempted to use its functionalities to
implement a clone of PHARAOH, based on the com-
prehensive user manual of the latter. This clone,
called RAMSES, is now part of the MOOD distribu-
tion, which can be downloaded freely from the page
http://smtmood.sourceforge.net.
We conducted a pair-to-pair comparison between
the two engines that we describe in this paper. We
provide an overview of the MOOD architecture in
Section 2. Then we describe briefly RAMSES in Sec-
tion 3. The comparison between the two decoders in
terms of automatic metrics is analyzed in Section 4.
We confirm this comparison by presenting a man-
ual evaluation we conducted on an random sample
of the translations produced by both decoders. This
is reported in Section 5. We conclude in Section 6.
126
2 The MOOD Framework
A decoder must implement a specific combination of
two elements: a model representation and a search
space exploration strategy. MOOD is a framework
designed precisely to allow such a combination, by
clearly separating its two elements. The design of
the framework is described in (Patry et al, 2006).
MOOD is implemented with the C++ program-
ming language and is licensed under the Gnu Gen-
eral Public License (GPL)2. This license grants the
right to anybody to use, modify and distribute the
program and its source code, provided that any mod-
ified version be licensed under the GPL as well.
As explained in (Walker, 2005), this kind of license
stimulates new ideas and research.
3 MOOD at work: RAMSES
As we said above, in order to test our design, we
reproduced the most popular phrase-based decoder,
PHARAOH (Koehn, 2004), by following as faithfully
as possible its detailed user manual. The command-
line syntax RAMSES recognizes is compatible with
that of PHARAOH. The output produced by both
decoders are compatible as well and RAMSES can
also output its n-best lists in the same format as
PHARAOH does, i.e. in a format that the CARMEL
toolkit can parse (Knight and Al-Onaizan, 1999).
Switching decoders is therefore straightforward.
4 RAMSES versus PHARAOH
To compare the translation performances of both
decoders in a meaningful manner, RAMSES and
PHARAOH were given the exact same language
model and translation table for each translation ex-
periment. Both models were produced with the
scripts provided by the organizers. This means in
practice that the language model was trained using
the SRILM toolkit (Stolcke, 2002). The word align-
ment required to build the phrase table was pro-
duced with the GIZA++ package. A Viterbi align-
ment computed from an IBM model 4 (Brown et al,
1993) was computed for each translation direction.
Both alignments were then combined in a heuristic
way (Koehn et al, ). Each pair of phrases in the
2http://www.gnu.org/copyleft/gpl.html
model is given 5 scores, described in the PHARAOH
training manual.3
To tune the coefficients of the log-linear
combination that both PHARAOH and RAMSES
use when decoding, we used the organizers?
minimum-error-rate-training.perl
script. This tuning step was performed on the
first 500 sentences of the dedicated development
corpora. Inevitably, RAMSES differs slightly
from PHARAOH, because of some undocumented
embedded heuristics. Thus, we found appropriate
to tune each decoder separately (although with
the same material). In effect, each decoder does
slightly better (with BLEU) when it uses its own best
parameters obtained from tuning, than when it uses
the parameters of its counterpart.
Eight coefficents were adjusted this way: five for
the translation table (one for each score associated
to each pair of phrases), and one for each of the fol-
lowing models: the language model, the so-called
word penalty model and the distortion model (word
reordering model). Each parameter is given a start-
ing value and a range within which it is allowed to
vary. For instance, the language model coefficient?s
starting value is 1.0 and the coefficient is in the range
[0.5?1.5]. Eventually, we obtained two optimal con-
figurations (one for each decoder) with which we
translated the TEST material.
We evaluated the translations produced by both
decoders with the organizers? multi-bleu.perl
script, which computes a BLEU score (and displays
the n-gram precisions and brevity penalty used). We
report the scores we gathered on the test corpus of
2000 pairs of sentences in Table 1. Overall, both
decoders offer similar performances, down to the
n-gram precisions. To assess the statistical signifi-
cance of the observed differences in BLEU, we used
the bootstrapping technique described in (Zhang
and Vogel, 2004), randomly selecting 500 sentences
from each test set, 1000 times. Using a 95% con-
fidence interval, we determined that the small dif-
ferences between the two decoders are not statis-
tically significant, except for two tests. For the
direction English to French, RAMSES outperforms
PHARAOH, while in the German to English direc-
3http://www.statmt.org/wmt06/
shared-task/training-release-1.3.tgz
127
tion, PHARAOH is better. Whenever a decoder is
better than the other, Table 1 shows that it is at-
tributable to higher n-gram precisions; not to the
brevity penalty.
We further investigated these two cases by calcu-
lating BLEU for subsets of the test corpus sharing
similar sentence lengths (Table 2). We see that both
decoders have similar performances on short sen-
tences, but can differ by as much as 1% in BLEU on
longer ones. In contrast, on the Spanish-to-English
translation direction, where the two decoders offer
similar performances, the difference between BLEU
scores never exceeds 0.23%.
Expectedly, Spanish and French are much easier
to translate than German. This is because, in this
study, we did not apply any pre-processing strat-
egy that we know can improve performances, such
as clause reordering or compound-word splitting
(Collins et al, 2005; Langlais et al, 2005).
Table 2 shows that it does not seem much more
difficult to translate into English than from English.
This is surprising: translating into a morphologically
richer language should be more challenging. The
opposite is true for German here: without doing any-
thing specific for this language, it is much easier to
translate from German to English than the other way
around. This may be attributed in part to the lan-
guage model: for the test corpus, the perplexity of
the language models provided is 105.5 for German,
compared to 59.7 for English.
5 Human Evaluation
In an effort to correlate the objective metrics with
human reviews, we undertook the blind evaluation
of a sample of 100 pairwise translations for the three
Foreign language-to-English translation tasks. The
pairs were randomly selected from the 3064 trans-
lations produced by each engine. They had to be
different for each decoder and be no more than 25
words long.
Each evaluator was presented with a source sen-
tence, its reference translation and the translation
produced by each decoder. The last two were in ran-
dom order, so the evaluator did not know which en-
gine produced the translation. The evaluator?s task
was two-fold. (1) He decided whether one transla-
tion was better than the other. (2) If he replied ?yes?
D BLEU p1 p2 p3 p4 BP
es ? en
P 30.65 64.10 36.52 23.70 15.91 1.00
R 30.48 64.08 36.30 23.52 15.76 1.00
fr ? en
P 30.42 64.28 36.45 23.39 15.64 1.00
R 30.43 64.58 36.59 23.54 15.73 0.99
de ? en
P 25.15 61.19 31.32 18.53 11.61 0.99
R 24.49 61.06 30.75 17.73 10.81 1.00
en ? es
P 29.40 61.86 35.32 22.77 15.02 1.00
R 28.75 62.23 35.03 22.32 14.58 0.99
en ? fr
P 30.96 61.10 36.56 24.49 16.80 1.00
R 31.79 61.57 37.38 25.30 17.53 1.00
en ? de
P 18.03 52.77 22.70 12.45 7.25 0.99
R 18.14 53.38 23.15 12.75 7.47 0.98
Table 1: Performance of RAMSES and PHARAOH
on the provided test set of 2000 pairs of sentences
per language pair. P stands for PHARAOH, R for
RAMSES. All scores are percentages. pn is the n-
gram precision and BP is the brevity penalty used
when computing BLEU.
in test (1), he stated whether the best translation was
satisfactory while the other was not. Two evalua-
tors went through the 3 ? 100 sentence pairs. None
of them understands German; subject B understands
Spanish, and both understand French and English.
The results of this informal, yet informative exercise
are reported in Table 3.
Overall, in many cases (64% and 48% for subject
A and B respectively), the evaluators did not pre-
fer one translation over the other. On the Spanish-
and French-to-English tasks, both subjects slightly
preferred the translations produced by RAMSES. In
about one fourth of the cases where one translation
was preferred did the evaluators actually flag the se-
lected translation as significantly better.
6 Discussion
We presented a pairwise comparison of two de-
coders, RAMSES and PHARAOH. Although RAM-
SES is roughly twice as slow as PHARAOH, both de-
128
Test set [0,15] [16,25] [26,?[
en ? fr (P) 33.52 30.65 30.39
en ? fr (R) 33.78 31.19 31.35
de ? en (P) 29.74 24.30 24.76
de ? en (R) 29.85 23.92 23.78
es ? en (P) 34.23 28.32 30.60
es ? en (R) 34.46 28.39 30.40
Table 2: BLEU scores on subsets of the test corpus
filtered by sentence length ([min words, max words]
intervals), for Pharaoh and Ramses.
Preferred Improved
P R No P R
es ? en
subject A 13 16 71 6 1
subject B 23 31 46 3 8
fr ? en
subject A 18 19 63 5 3
subject B 20 21 59 8 8
de ? en
subject A 24 18 58 5 9
subject B 30 31 39 3 3
Total 128 136 336 30 32
Table 3: Human evaluation figures. The column
Preferred indicates the preference of the subject
(Pharaoh, Ramses or No preference). The column
Improved shows when a subject did prefer a trans-
lation and also said that the preferred translation was
correct while the other one was not.
coders offer comparable performances, according to
automatic and informal human evaluations.
Moreover, RAMSES is the product of clean frame-
work: MOOD, a solid tool for research projects. Its
code is open source and the architecture is modular,
making it easier for researchers to experiment with
SMT. We hope that the availability of the source
code and the clean design of MOOD will make it a
useful platform to implement new decoders.
Acknowledgments
We warmly thanks Elliott Macklovitch for his par-
ticipation in the manual annotation task. This work
has been partially funded by an NSERC grant.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L.
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, 19(2):263?311.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the CMU-cambridge toolkit. In Proc.
of Eurospeech, pages 2707?2710, Rhodes, Greece.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of the 43rd ACL, pages 531?540, Ann Arbor, MI.
K. Knight and Y. Al-Onaizan, 1999. A Primer on
Finite-State Software for Natural Language Process-
ing. www.isi.edu/licensed-sw/carmel.
P. Koehn, F. Joseph Och, and D. Marcu. Statistical
Phrase-Based Translation. In Proc. of HLT, Edmon-
ton, Canada.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder for
Phrase-Based SMT. In Proc. of the 6th AMTA, pages
115?124, Washington, DC.
P. Langlais, G. Cao, and F. Gotti. 2005. RALI: SMT
shared task system description. In 2nd ACL workshop
on Building and Using Parallel Texts, pages 137?140,
Ann Arbor, MI.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proc. of ACL, pages 440?447,
Hongkong, China.
D. Ortiz-Mart??nez, I. Garcia?-Varea, and F. Casacuberta.
2005. Thot: a toolkit to train phrase-based statistical
translation models. In Proc. of MT Summit X, pages
141?148, Phuket, Thailand.
A. Patry, F. Gotti, and P. Langlais. 2006. MOOD
a modular object-oriented decoder for statistical ma-
chine translation. In Proc. of LREC, Genoa, Italy.
A. Stolcke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In Proc. of ICSLP, Denver, USA.
D.J. Walker. 2005. The open ?a.i.? kitTM: General ma-
chine learning modules from statistical machine trans-
lation. In Workshop of MT Summit X, ?Open-Source
Machine Translation?, Phuket, Thailand.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proc. of the 10th TMI, Baltimore, MD.
129
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 80?89,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Translating Government Agencies? Tweet Feeds:
Specificities, Problems and (a few) Solutions
Fabrizio Gotti, Philippe Langlais
{gottif,felipe}@iro.umontreal.ca
RALI-DIRO
Universite? de Montre?al
C.P. 6128, Succ Centre-Ville
Montre?al (Que?bec) Canada
H3C 3J7
Atefeh Farzindar
farzindar@nlptechnologies.ca
NLP Technologies Inc.
52 Le Royer
Montre?al
(Que?bec) Canada
H2Y 1W7
Abstract
While the automatic translation of tweets has
already been investigated in different scenar-
ios, we are not aware of any attempt to trans-
late tweets created by government agencies.
In this study, we report the experimental re-
sults we obtained when translating 12 Twitter
feeds published by agencies and organizations
of the government of Canada, using a state-of-
the art Statistical Machine Translation (SMT)
engine as a black box translation device. We
mine parallel web pages linked from the URLs
contained in English-French pairs of tweets in
order to create tuning and training material.
For a Twitter feed that would have been other-
wise difficult to translate, we report significant
gains in translation quality using this strategy.
Furthermore, we give a detailed account of the
problems we still face, such as hashtag trans-
lation as well as the generation of tweets of
legal length.
1 Introduction
Twitter is currently one of the most popular online
social networking service after Facebook, and is the
fastest-growing, with the half-a-billion user mark
reached in June 2012.1 According to Twitter?s blog,
no less than 65 millions of tweets are published each
day, mostly in a single language (40% in English).
This hinders the spread of information, a situation
witnessed for instance during the Arab Spring.
1http://semiocast.com/publications/
2012_07_30_Twitter_reaches_half_a_billion_
accounts_140m_in_the_US
Solutions for disseminating tweets in different
languages have been designed. One solution con-
sists in manually translating tweets, which of course
is only viable for a very specific subset of the ma-
terial appearing on Twitter. For instance, the non-
profit organization Meedan2 has been founded in or-
der to organize volunteers willing to translate tweets
written in Arabic on Middle East issues. Another
solution consists in using machine translation. Sev-
eral portals are facilitating this,3 mainly by using
Google?s machine translation API.
Curiously enough, few studies have focused on
the automatic translation of text produced within so-
cial networks, even though a growing number of
these studies concentrate on the automated process-
ing of messages exchanged on social networks. See
(Gimpel et al, 2011) for a recent review of some of
them.
Some effort has been invested in translating short
text messages (SMSs). Notably, Munro (2010) de-
scribes the service deployed by a consortium of vol-
unteer organizations named ?Mission 4636? during
the earthquake that struck Haiti in January 2010.
This service routed SMSs alerts reporting trapped
people and other emergencies to a set of volunteers
who translated Haitian Creole SMSs into English,
so that primary emergency responders could under-
stand them. In Lewis (2010), the authors describe
how the Microsoft translation team developed a sta-
tistical translation engine (Haitian Creole into En-
glish) in as little as 5 days, during the same tragedy.
2http://news.meedan.net/
3http://www.aboutonlinetips.com/
twitter-translation-tools/
80
Jehl (2010) addresses the task of translating En-
glish tweets into German. She concludes that the
proper treatment of unknown words is of the utmost
importance and highlights the problem of producing
translations of up to 140 characters, the upper limit
on tweet lengths. In (Jehl et al, 2012), the authors
describe their efforts to collect bilingual tweets from
a stream of tweets acquired programmatically, and
show the impact of such a collection on developing
an Arabic-to-English translation system.
The present study participates in the effort for the
dissemination of messages exchanged over Twitter
in different languages, but with a very narrow focus,
which we believe has not been addressed specifically
yet: Translating tweets written by government in-
stitutions. What sets these messages apart is that,
generally speaking, they are written in a proper lan-
guage (without which their credibility would pre-
sumably be hurt), while still having to be extremely
brief to abide by the ever-present limit of 140 char-
acters. This contrasts with typical social media texts
in which a large variability in quality is observed
(Agichtein et al, 2008).
Tweets from government institutions can also dif-
fer somewhat from some other, more informal so-
cial media texts in their intended audience and ob-
jectives. Specifically, such tweet feeds often attempt
to serve as a credible source of timely information
presented in a way that engages members of the
lay public. As such, translations should present a
similar degree of credibility, ease of understanding,
and ability to engage the audience as in the source
tweet?all while conforming to the 140 character
limits.
This study attempts to take these matters into ac-
count for the task of translating Twitter feeds emitted
by Canadian governmental institutions. This could
prove very useful, since more than 150 Canadian
agencies have official feeds. Moreover, while only
counting 34 million inhabitants, Canada ranks fifth
in the number of Twitter users (3% of all users) after
the US, the UK, Australia, and Brazil.4 This cer-
tainly explains why Canadian governments, politi-
cians and institutions are making an increasing use
of this social network service. Given the need of
4http://www.techvibes.com/blog/how-
canada-stacks-up-against-the-world-on-
twitter-2012-10-17
Canadian governmental institutions to disseminate
information in both official languages (French and
English), we see a great potential value in targeted
computer-aided translation tools, which could offer
a significant reduction over the current time and ef-
fort required to manually translate tweets.
We show that a state-of-the-art SMT toolkit, used
off-the-shelf, and trained on out-domain data is un-
surprisingly not up to the task. We report in Sec-
tion 2 our efforts in mining bilingual material from
the Internet, which proves eventually useful in sig-
nificantly improving the performance of the engine.
We test the impact of simple adaptation scenarios
in Section 3 and show the significant improvements
in BLEU scores obtained thanks to the corpora we
mined. In Section 4, we provide a detailed account
of the problems that remain to be solved, including
the translation of hashtags (#-words) omnipresent
in tweets and the generation of translations of legal
lengths. We conclude this work-in-progress and dis-
cuss further research avenues in Section 5.
2 Corpora
2.1 Bilingual Twitter Feeds
An exhaustive list of Twitter feeds published by
Canadian government agencies and organizations
can be found on the GOV.PoliTWiTTER.ca web
site.5 As of this writing, 152 tweet feeds are listed,
most of which are available in both French and En-
glish, in keeping with the Official Languages Act
of Canada. We manually selected 20 of these feed
pairs, using various exploratory criteria, such as
their respective government agency, the topics being
addressed and, importantly, the perceived degree of
parallelism between the corresponding French and
English feeds.
All the tweets of these 20 feed pairs were gathered
using Twitter?s Streaming API on 26 March 2013.
We filtered out the tweets that were marked by the
API as retweets and replies, because they rarely have
an official translation. Each pair of filtered feeds
was then aligned at the tweet level in order to cre-
ate bilingual tweet pairs. This step was facilitated
by the fact that timestamps are assigned to each
tweet. Since a tweet and its translation are typi-
5http://gov.politwitter.ca/directory/
network/twitter
81
Tweets URLs mis. probs sents
. HealthCanada
1489 995 1 252 78,847
. DFAIT MAECI ? Foreign Affairs and Int?l Trade
1433 65 0 1081 10,428
. canadabusiness
1265 623 1 363 138,887
. pmharper ? Prime Minister Harper
752 114 2 364 12,883
. TCS SDC ? Canadian Trade Commissioner Service
694 358 1 127 36,785
. Canada Trade
601 238 1 92 22,594
. PHAC GC ? Public Health Canada
555 140 0 216 14,617
. cida ca ? Canadian Int?l Development Agency
546 209 2 121 18,343
. LibraryArchives
490 92 1 171 6,946
. CanBorder ? Canadian Border matters
333 88 0 40 9,329
. Get Prepared ? Emergency preparedness
314 62 0 11 10,092
. Safety Canada
286 60 1 17 3,182
Table 1: Main characteristics of the Twitter and URL cor-
pora for the 12 feed pairs we considered. The (English)
feed name is underlined, and stands for the pair of feeds
that are a translation of one another. When not obvious,
a short description is provided. Each feed name can be
found as is on Twitter. See Sections 2.1 and 2.3 for more.
cally issued at about the same time, we were able to
align the tweets using a dynamic programming al-
gorithm miminizing the total time drift between the
English and the French feeds. Finally, we tokenized
the tweets using an adapted version of Twokenize
(O?Connor et al, 2010), accounting for the hashtags,
usernames and urls contained in tweets.
We eventually had to narrow down further the
number of feed pairs of interest to the 12 most pro-
lific ones. For instance, the feed pair PassportCan6
that we initially considered contained only 54 pairs
of English-French tweets after filtering and align-
ment, and was discarded because too scarce.
6https://twitter.com/PassportCan
Did you know it?s best to test for #radon in
the fall/winter? http://t.co/CDubjbpS
#health #safety
L?automne/l?hiver est le meilleur moment pour
tester le taux de radon.
http://t.co/4NJWJmuN #sante? #se?curite
Figure 1: Example of a pair of tweets extracted from the
feed pair HealthCanada .
The main characteristics of the 12 feed pairs we
ultimately retained are reported in Table 1, for a to-
tal of 8758 tweet pairs. The largest feed, in terms
of the number of tweet pairs used, is that of Health
Canada7 with over 1 489 pairs of retained tweets
pairs at the time of acquisition. For reference, that
is 62% of the 2 395 ?raw? tweets available on the
English feed, before filtering and alignment. An ex-
ample of a retained pair of tweets is shown in Fig-
ure 1. In this example, both tweets contain a short-
ened url alias that (when expanded) leads to web-
pages that are parallel. Both tweets also contain so-
called hashtags (#-words): 2 of those are correctly
translated when going from English to French, but
the hashtag #radon is not translated into a hashtag in
French, instead appearing as the plain word radon,
for unknown reasons.
2.2 Out-of-domain Corpora: Parliament
Debates
We made use of two different large corpora in or-
der to train our baseline SMT engines. We used the
2M sentence pairs of the Europarl version 7 corpus.8
This is a priori an out-of-domain corpus, and we did
not expect much of the SMT system trained on this
dataset. Still, it is one of the most popular parallel
corpus available to the community and serves as a
reference.
We also made use of 2M pairs of sentences we
extracted from an in-house version of the Canadian
Hansard corpus. This material is not completely out-
of-domain, since the matters addressed within the
Canadian Parliament debates likely coincide to some
degree with those tweeted by Canadian institutions.
The main characteristics of these two corpora are re-
ported in Table 2. It is noteworthy that while both
7https://twitter.com/HealthCanada
8http://www.statmt.org/europarl/
82
Corpus sents tokens types s length
hansard en 2M 27.1M 62.2K 13.6
hansard fr 2M 30.7M 82.2K 15.4
europarl en 2M 55.9M 94.5K 28.0
europarl fr 2M 61.6M 129.6K 30.8
Table 2: Number of sentence pairs, token and token types
in the out-of-domain training corpora we used. s length
stands for the average sentence length, counted in tokens.
corpora contain an equal number of sentence pairs,
the average sentence length in the Europarl corpus is
much higher, leading to a much larger set of tokens.
2.3 In-domain Corpus: URL Corpus
As illustrated in Figure 1, many tweets act as
?teasers?, and link to web pages containing (much)
more information on the topic the tweet feed typi-
cally addresses. Therefore, a natural way of adapt-
ing a corpus-driven translation engine consists in
mining the parallel text available at those urls.
In our case, we set aside the last 200 tweet pairs of
each feed as a test corpus. The rest serves as the url-
mining corpus. This is necessary to avoid testing our
system on test tweets whose URLs have contributed
to the training corpus.
Although simple in principle, this data collection
operation consists in numerous steps, outlined be-
low:
1. Split each feed pair in two: The last 200 tweet
pairs are set aside for testing purposes, the rest
serves as the url-mining corpus used in the fol-
lowing steps.
2. Isolate urls in a given tweet pair using our to-
kenizer, adapted to handle Twitter text (includ-
ing urls).
3. Expand shortened urls. For instance, the url
in the English example of Figure 1 would
be expanded into http://www.hc-sc.
gc.ca/ewh-semt/radiation/radon/
testing-analyse-eng.php, using the
expansion service located at the domain t.co.
There are 330 such services on the Web.
4. Download the linked documents.
5. Extract all text from the web pages, without tar-
geting any content in particular (the site menus,
breadcrumb, and other elements are therefore
retained).
6. Segment the text into sentences, and tokenize
them into words.
7. Align sentences with our in-house aligner.
We implemented a number of restrictions during
this process. We did not try to match urls in cases
where the number of urls in each tweet differed (see
column mis.?mismatches?in Table 1). The col-
umn probs. (problems) in Table 1 shows the count of
url pairs whose content could not be extracted. This
happened when we encountered urls that we could
not expand, as well as those returning a 404 HTTP
error code. We also rejected urls that were identi-
cal in both tweets, because they obviously could not
be translations. We also filtered out documents that
were not in html format, and we removed document
pairs where at least one document was difficult to
convert into text (e.g. because of empty content, or
problematic character encoding). After inspection,
we also decided to discard sentences that counted
less than 10 words, because shorter sentences are
too often irrelevant website elements (menu items,
breadcrumbs, copyright notices, etc.).
This 4-hour long operation (including download)
yielded a number of useful web documents and ex-
tracted sentence pairs reported in Table 1 (columns
URLs and sents respectively). We observed that the
density of url pairs present in pairs of tweets varies
among feeds. Still, for all feeds, we were able to
gather a set of (presumably) parallel sentence pairs.
The validity of our extraction process rests on the
hypothesis that the documents mentioned in each
pair of urls are parallel. In order to verify this, we
manually evaluated (a posteriori) the parallelness of
a random sample of 50 sentence pairs extracted for
each feed. Quite fortunately, the extracted material
was of excellent quality, with most samples contain-
ing all perfectly aligned sentences. Only canadabusi-
ness, LibraryArchives and CanBorder counted a sin-
gle mistranslated pair. Clearly, the websites of the
Canadian institutions we mined are translated with
great care and the tweets referring to them are metic-
ulously translated in terms of content links.
83
3 Experiments
3.1 Methodology
All our translation experiments were conducted with
Moses? EMS toolkit (Koehn et al, 2007), which in
turn uses gizapp (Och and Ney, 2003) and SRILM
(Stolcke, 2002).
As a test bed, we used the 200 bilingual tweets
we acquired that were not used to follow urls, as de-
scribed in Sections 2.1 and 2.3. We kept each feed
separate in order to measure the performance of our
system on each of them. Therefore we have 12 test
sets.
We tested two configurations: one in which an
out-of-domain translation system is applied (with-
out adaptation) to the translation of the tweets of
our test material, another one where we allowed the
system to look at in-domain data, either at training
or at tuning time. The in-domain material we used
for adapting our systems is the URL corpus we de-
scribed in section 2.3. More precisely, we prepared
12 tuning corpora, one for each feed, each contain-
ing 800 heldout sentence pairs. The same number of
sentence pairs was considered for out-domain tuning
sets, in order not to bias the results in favor of larger
sets. For adaptation experiments conducted at train-
ing time, all the URL material extracted from a spe-
cific feed (except for the sentences of the tuning sets)
was used. The language model used in our experi-
ments was a 5-gram language model with Kneser-
Ney smoothing.
It must be emphasized that there is no tweet mate-
rial in our training or tuning sets. One reason for this
is that we did not have enough tweets to populate our
training corpus. Also, this corresponds to a realistic
scenario where we want to translate a Twitter feed
without first collecting tweets from this feed.
We use the BLEU metric (Papineni et al, 2002)
as well as word-error rate (WER) to measure trans-
lation quality. A good translation system maximizes
BLEU and minimizes WER. Due to initially poor
results, we had to refine the tokenizer mentioned
in Section 2.1 in order to replace urls with serial-
ized placeholders, since those numerous entities typ-
ically require rule-based translations. The BLEU
and WER scores we report henceforth were com-
puted on such lowercased, tokenized and serialized
texts, and did not incur penalties that would have
train tune canadabusiness DFAIT MAECI
fr?en wer bleu wer bleu
hans hans 59.58 21.16 61.79 19.55
hans in 58.70 21.35 60.73 20.14
euro euro 64.24 15.88 62.90 17.80
euro in 63.23 17.48 60.58 21.23
en?fr wer bleu wer bleu
hans hans 62.42 21.71 64.61 21.43
hans in 61.97 22.92 62.69 22.00
euro euro 64.66 19.52 63.91 21.65
euro in 64.61 18.84 63.56 22.31
Table 3: Performance of generic systems versus systems
adapted at tuning time for two particular feeds. The tune
corpus ?in? stands for the URL corpus specific to the feed
being translated. The tune corpora ?hans? and ?euro? are
considered out-of-domain for the purpose of this experi-
ment.
otherwise been caused by the non-translation of urls
(unknown tokens), for instance.
3.2 Translation Results
Table 3 reports the results observed for the two main
configurations we tested, in both translation direc-
tions. We show results only for two feeds here:
canadabusiness, for which we collected the largest
number of sentence pairs in the URL corpus, and
DFAIT MAECI for which we collected very little
material. For canadabusiness, the performance of the
system trained on Hansard data is higher than that
of the system trained on Europarl (? ranging from
2.19 to 5.28 points of BLEU depending on the con-
figuration considered). For DFAIT MAECI , supris-
ingly, Europarl gives a better result, but by a more
narrow margin (? ranging from 0.19 to 1.75 points
of BLEU). Both tweet feeds are translated with
comparable performance by SMT, both in terms
of BLEU and WER. When comparing BLEU per-
formances based solely on the tuning corpus used,
the in-domain tuning corpus created by mining urls
yields better results than the out-domain tuning cor-
pus seven times out of eight for the results shown in
Table 3.
The complete results are shown in Figure 2, show-
ing BLEU scores obtained for the 12 feeds we con-
sidered, when translating from English to French.
Here, the impact of using in-domain data to tune
84
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
in out in out in out in out in out in out in out in out in out in out in out in out
Canada_Trade canadabusiness CanBorder cida_ca DFAIT_MAECI Get_Prepared HealthCanada LibraryArchives PHAC_GC pmharper Safety_Canada TCS_SDC
B
LE
U
 s
co
re
 
euro hans
train
corpus tune
Moyenne de bleu
direction
Figure 2: BLEU scores measured on the 12 feed pairs we considered for the English-to-French translation direction.
For each tweet test corpus, there are 4 results: a dark histogram bar refers to the Hansard training corpus, while a
lighter grey bar refers to an experiment where the training corpus was Europarl. The ?in? category on the x-axis
designates an experiment where the tuning corpus was in-domain (URL corpus), while the ?out? category refers to an
out-of-domain tuning set. The out-of-domain tuning corpus is Europarl or Hansard, and always matches the nature of
training corpora.
the system is hardly discernible, which in a sense
is good news, since tuning a system for each feed
is not practical. The Hansard corpus almost always
gives better results, in keeping with its status as a
corpus that is not so out-of-domain as Europarl, as
mentioned above. The results for the reverse trans-
lation direction show the same trends.
In order to try a different strategy than using only
tuning corpora to adapt the system, we also investi-
gated the impact of training the system on a mix of
out-of-domain and in-domain data. We ran one of
the simplest adaptation scenarios where we concate-
nated the in-domain material (train part of the URL
corpus) to the out-domain one (Hansard corpus) for
the two feeds we considered in Table 3. The results
are reported in Table 4.
We measured significant gains both in WER and
BLEU scores in conducting training time versus tun-
ing time adaptation, for the canadabusiness feed (the
largest URL corpus). For this corpus, we observe
an interesting gain of more than 6 absolute points in
BLEU scores. However, for the DFAIT MAECI (the
smallest URL corpus) we note a very modest loss in
translation quality when translating from French and
a significant gain in the other translation direction.
These figures could show that mining parallel sen-
tences present in URLs is a fruitful strategy for adapt-
ing the translation engine for feeds like canadabusi-
ness that display poor performance otherwise, with-
out harming the translation quality for feeds that per-
Train corpus WER BLEU
fr?en
hans+canbusiness 53.46 (-5.24) 27.60 (+6.25)
hans+DFAIT 60.81 (+0.23) 20.83 (-0.40)
en?fr
hans+canbusiness 57.07 (-4.90) 26.26 (+3.34)
hans+DFAIT 61.80 (-0.89) 24.93 (+2.62)
Table 4: Performance of systems trained on a concatena-
tion of out-of-domain and in-domain data. All systems
were tuned on in-domain data. Absolute gains are shown
in parentheses, over the best performance achieved so far
(see Table 3).
form reasonably well without additional resources.
Unfortunately, it suggests that retraining a system is
required for better performance, which might hinder
the deployment of a standalone translation engine.
Further research needs to be carried out to determine
how many tweet pairs must be used in a parallel URL
corpus in order to get a sufficiently good in-domain
corpus.
4 Analysis
4.1 Translation output
Examples of translations produced by the best sys-
tem we trained are reported in Figure 3. The first
translation shows a case of an unknown French word
(soumissionnez). The second example illustrates
85
a typical example where the hashtags should have
been translated but were left unchanged. The third
example shows a correct translation, except that the
length of the translation (once the text is detok-
enized) is over the size limit allowed for a tweet.
Those problems are further analyzed in the remain-
ing subsections.
4.2 Unknown words
Unknown words negatively impact the quality of
MT output in several ways. First, they typically ap-
pear untranslated in the system?s output (we deemed
most appropriate this last resort strategy). Sec-
ondly, they perturb the language model, which often
causes other problems (such as dubious word order-
ing). Table 5 reports the main characteristics of the
words from all the tweets we collected that were not
present in the Hansard train corpus.
The out-of-vocabulary rate with respect to token
types hovers around 33% for both languages. No
less than 42% (resp. 37%) of the unknown English
(resp. French) token types are actually hashtags. We
defer their analysis to the next section. Also, 15%
(resp. 10%) of unknown English token types are
user names (@user), which do not require transla-
tion.
English French
tweet tokens 153 234 173 921
tweet types 13 921 15 714
OOV types 4 875 (35.0%) 5 116 (32.6%)
. hashtag types 2 049 (42.0%) 1 909 (37.3%)
. @user types 756 (15.5%) 521 (10.2%)
Table 5: Statistics on out-of-vocabulary token types.
We manually analyzed 100 unknown token types
that were not hashtags or usernames and that did not
contain any digit. We classified them into a num-
ber of broad classes whose distributions are reported
in Table 6 for the French unknown types. A simi-
lar distribution was observed for English unknown
types. While we could not decide of the nature of
21 types without their context of use (line ?type),
we frequently observed English types, as well as
acronyms and proper names. A few unknown types
result from typos, while many are indeed true French
types unseen at training time (row labeled french ),
some of which being very specific (term). Amus-
ingly, the French verbal neologism twitter (to tweet)
is unknown to the Hansard corpus we used.
french 26 sautez, perforateurs , twitter
english 22 successful , beauty
?types 21 bumbo , tra
name 11 absorbica , konzonguizi
acronym 7 hna , rnc
typo 6 gazouilli , pendan
term 3 apostasie , sibutramine
foreign 2 aanischaaukamikw, aliskiren
others 2 francophonesURL
Table 6: Distribution of 100 unknown French token types
(excluding hashtags and usernames).
4.3 Dealing with Hashtags
We have already seen that translating the text in
hashtags is often suitable, but not always. Typically,
hashtags in the middle of a sentence are to be trans-
lated, while those at the end typically should not be.
A model should be designed for learning when to
translate an hashtag or not. Also, some hashtags are
part of the sentence, while others are just (semantic)
tags. While a simple strategy for translating hash-
tags consists in removing the # sign at translation
time, then restoring it afterwards, this strategy would
fail in a number of cases that require segmenting the
text of the hashtag first. Table 7 reports the per-
centage of hashtags that should be segmented before
being translated, according to a manual analysis we
conducted over 1000 hashtags in both languages we
considered. While many hashtags are single words,
roughly 20% of them are not and require segmenta-
tion.
4.4 Translating under size constraints
The 140 character limit Twitter imposes on tweets is
well known and demands a certain degree of conci-
sion even human users find sometimes bothersome.
For machine output, this limit becomes a challeng-
ing problem. While there exists plain?but inelegant?
workarounds9, there may be a way to produce tweet
translations that are themselves Twitter-ready. (Jehl,
9The service eztweets.com splits long tweets into smaller
ones; twitlonger.com tweets the beginning of a long message,
86
SRC: vous soumissionnez pour obtenir de gros contrats ? voici 5 pratiques exemplaires a` suivre . URL
TRA: you soumissionnez big contracts for best practices ? here is 5 URL to follow .
REF: bidding on big contracts ? here are 5 best practices to follow . URL
SRC: avis de #sante?publique : maladies associe?es aux #salmonelles et a` la nourriture pour animaux de com-
pagnie URL #rappel
TRA: notice of #sante?publique : disease associated with the #salmonelles and pet food #rappel URL
REF: #publichealth notice : illnesses related to #salmonella and #petfood URL #recall
SRC: des ha??tiens de tous les a?ges , milieux et me?tiers te?moignent de l? aide qu? ils ont rec?ue depuis le se?isme
. URL #ha??ti
TRA: the haitian people of all ages and backgrounds and trades testify to the assistance that they have received
from the earthquake #ha??ti URL .
REF: #canada in #haiti : haitians of all ages , backgrounds , and occupations tell of the help they received .
URL
Figure 3: Examples of translations produced by an engine trained on a mix of in- and out-of-domain data.
w. en fr example
1 76.5 79.9 intelligence
2 18.3 11.9 gender equality
3 4.0 6.0 africa trade mission
4 1.0 1.4 closer than you think
5 0.2 0.6 i am making a difference
6 ? 0.2 fonds aide victime se?cheresse
afrique est
Table 7: Percentage of hashtags that require segmentation
prior to translation. w. stands for the number of words
into which the hashtag text should be segmented.
2010) pointed out this problem and reported that
3.4% of tweets produced were overlong, when trans-
lating from German to English. The reverse direc-
tions produced 17.2% of overlong German tweets.
To remedy this, she tried modifying the way BLEU
is computed to penalize long translation during the
tuning process, with BLEU scores worse than sim-
ply truncating the illegal tweets. The second strategy
the author tried consisted in generating n-best lists
and mining them to find legal tweets, with encour-
aging results (for n = 30 000), since the number
of overlong tweets was significantly reduced while
leaving BLEU scores unharmed.
In order to assess the importance of the problem
for our system, we measured the lengths of tweets
that a system trained like hans+canbusiness in Ta-
ble 4 (a mix of in- and out-of-domain data) could
produce. This time however, we used a larger test set
and provides a link to read the remainder. One could also simply
truncate an illegal tweet and hope for the best...
counting 498 tweets. To measure the lengths of their
translations, we first had to detokenize the transla-
tions produced, since the limitation applies to ?nat-
ural? text only. For each URL serialized token, we
counted 18 characters, the average length of a (short-
ened) url in a tweet. When translating from French
to English, the 498 translations had lengths ranging
from 45 to 138 characters; hence, they were all legal
tweets. From English to French, however, the trans-
lations are longer, and range from 32 characters to
223 characters, with 22.5% of them overlong.
One must recall that in our experiments, no tweets
were seen at training or tuning time, which explains
why the rate of translations that do not meet the
limit is high. This problem deserves a specific treat-
ment for a system to be deployed. One interest-
ing solution already described by (Jehl, 2010) is to
mine the n-best list produced by the decoder in or-
der to find the first candidate that constitutes a legal
tweet. This candidate is then picked as the trans-
lation. We performed this analysis on the canad-
abusiness output described earlier, from English to
French. We used n =1, 5, 10, 20, 50, 100, 200, 500,
1000, 5000, 10000, 30000 and computed the result-
ing BLEU scores and remaining percentage of over-
long tweets. The results are shown in Figure 4. The
results clearly show that the n-best list does contain
alternate candidates when the best one is too long.
Indeed, not only do we observe that the percentage
of remaining illegal tweets can fall steadily (from
22.4% to 6.6% for n = 30 000) as we dig deeper into
the list, but also the BLEU score stays unharmed,
showing even a slight improvement, from an ini-
87
tial 26.16 to 26.31 for n = 30 000. This counter-
intuitive result in terms of BLEU is also reported
in (Jehl, 2010) and is probably due to a less harsh
brevity penalty by BLEU on shorter candidates.
2013-03-01
nbest list
n wer ser BLEU
1 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.159.35 97.79 0.26 6
5 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.559.3 97.79 0.2622
10 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.1059.31 97.79 0.2623
20 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.2059.31 97.79 0.26 2
50 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.5059.34 97.79 0.2622
100 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.10059.27 97.79 0.2628
200 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.20059.23 97.79 0.2633
500 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.50059.24 97.79 0 263
1000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.100059.21 97.79 0.2634
5000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.500059.26 97.79 0.2635
10000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.1000059.26 97.79 0.2638
30000 /u/gottif/p j/nlptech2012/data/tweets/correction/nbest/trans.3000059.32 97.79 0.26 1
0%
5%
10%
15%
20%
25%
0.261
0.262
0.262
0.263
0.263
0.264
0.264
1 10 100 1000 10000 100000
%
 o
f o
ve
rlo
ng
 tw
ee
ts
 
BL
EU
 sc
or
e 
n-best list length (n) 
BLEU % overlong
Figure 4: BLEU scores and percentage of overlong
tweets when mining the n-best list for legal tweets, when
the first candidate is overlong. The BLEU scores (dia-
mond series) should be read off the left-hand vertical axis,
while the remaining percentage of illegal tweets (circle
series) should be read off the right-hand axis.
5 Discussion
We presented a number of experiments where we
translated tweets produced by Canadian govern-
ments institutions and organizations. Those tweets
have the distinguishing characteristic (in the Twitter-
sphere) of being written in proper English or French.
We show that mining the urls mentioned in those
tweets for parallel sentences can be a fruitful strat-
egy for adapting an out-of-domain translation engine
to this task, although further research could show
other ways of using this resource, whose quality
seems to be high according to our manual evalua-
tion. We also analyzed the main problems that re-
main to be addressed before deploying a useful sys-
tem.
While we focused here on acquiring useful cor-
pora for adapting a translation engine, we admit that
the adaptation scenario we considered is very sim-
plistic, although efficient. We are currently inves-
tigating the merit of different methods to adaptation
(Zhao et al, 2004; Foster et al, 2010; Daume III and
Jagarlamudi, 2011; Razmara et al, 2012; Sankaran
et al, 2012).
Unknown words are of concern, and should be
dealt with appropriately. The serialization of urls
was natural, but it could be extended to usernames.
The latter do not need to be translated, but reduc-
ing the vocabulary is always desirable when work-
ing with a statistical machine translation engine.
One interesting subcategories of out-of-vocabulary
tokens are hashtags. According to our analysis,
they require segmentation into words before being
translated in 20% of the cases. Even if they are
transformed into regular words (#radon?radon or
#genderequality?gender equality), however, it is
not clear at this point how to detect if they are used
like normally-occurring words in a sentence, as in
(#radon is harmful) or if they are simply tags added
to the tweet to categorize it.
We also showed that translating under size con-
straints can be handled easily by mining the n-best
list produced by the decoder, but only up to a point.
A remaining 6% of the tweets we analyzed in detail
could not find a shorter version. Numerous ideas
are possible to alleviate the problem. One could for
instance modify the logic of the decoder to penal-
ize hypotheses that promise to yield overlong trans-
lations. Another idea would be to manually in-
spect the strategies used by governmental agencies
on Twitter when attempting to shorten their mes-
sages, and to select those that seem acceptable and
implementable, like the suppression of articles or the
use of authorized abbreviations.
Adapting a translation pipeline to the very specific
world of governmental tweets therefore poses mul-
tiple challenges, each of which can be addressed in
numerous ways. We have reported here the results of
a modest but fertile subset of these adaptation strate-
gies.
Acknowledgments
This work was funded by a grant from the Natu-
ral Sciences and Engineering Research Council of
Canada. We also wish to thank Houssem Eddine
Dridi for his help with the Twitter API.
88
References
Eugene Agichtein, Carlos Castillo, Debora Donato, Aris-
tides Gionis, and Gilad Mishne. 2008. Finding high-
quality content in social media. In Proceedings of
the 2008 International Conference on Web Search and
Data Mining, WSDM ?08, pages 183?194.
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In 49th ACL, pages 407?412, Portland,
Oregon, USA, June.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP,
pages 451?459, Cambridge, MA, October.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL
(Short Papers), pages 42?47.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012. Twit-
ter translation using translation-based cross-lingual re-
trieval. In 7th Workshop on Statistical Machine Trans-
lation, pages 410?421, Montre?al, June.
Laura Jehl. 2010. Machine translation for twitter. Mas-
ter?s thesis, School of Philosophie, Psychology and
Language Studies, University of Edinburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondr?ej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. pages 177?
180.
William D. Lewis. 2010. Haitian creole: How to build
and ship an mt engine from scratch in 4 days, 17 hours,
& 30 minutes. In EAMT, Saint-Raphael.
Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation, Denver.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory Search and Topic
Summarization for Twitter. In William W. Cohen,
Samuel Gosling, William W. Cohen, and Samuel
Gosling, editors, ICWSM. The AAAI Press.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei J.
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th ACL, Jeju, Republic of Korea, jul.
Baskaran Sankaran, Majid Razmara, Atefeh Farzindar,
Wael Khreich, Fred Popowich, and Anoop Sarkar.
2012. Domain adaptation techniques for machine
translation and their evaluation in a real-world setting.
In Proceedings of 25th Canadian Conference on Arti-
ficial Intelligence, Toronto, Canada, may.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In 20th
COLING.
89
