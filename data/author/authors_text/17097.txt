Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1670?1681, Dublin, Ireland, August 23-29 2014.
Generating Supplementary Travel Guides from Social Media
Liu Yang
1,2
, Jing Jiang
2,?
, Lifu Huang
1,2
, Minghui Qiu
2
, Lizi Liao
2,3
1
Peking University / Beijing, China, 100871
2
Singapore Management University / Singapore, Singapore, 178902
3
Beijing Institute of Technology / Beijing, China, 100081
yang.liu@pku.edu.cn, jingjiang@smu.edu.sg
{warrior.fu, minghuiqiu, liaolizi.llz}@gmail.com
Abstract
In this paper we study how to summarize travel-related information in forum threads to gener-
ate supplementary travel guides. Such summaries presumably can provide additional and more
up-to-date information to tourists. Existing multi-document summarization methods have limita-
tions for this task because (1) they do not generate structured summaries but travel guides usually
follow a certain template, and (2) they do not put emphasis on named entities but travel guides
often recommend points of interest to travelers. To overcome these limitations, we propose to
use a latent variable model to align forum threads with the section structure of well-written travel
guides. The model also assigns section labels to named entities in forum threads. We then
propose to modify an ILP-based summarization method to generate section-specific summaries.
Evaluation on threads from Yahoo! Answers shows that our proposed method is able to generate
better summaries compared with a number of baselines based on ROUGE scores and coverage
of named entities.
1 Introduction
Online forums and community question answering (CQA) sites contain much useful information from
ordinary users, such as their personal experience, opinions, suggestions and recommendations. Extract-
ing and summarizing information from these rich information sources has a wide range of applications.
In this work, we study how to tap into user-generated content in forums such as Yahoo! Answers to
generate supplementary city travel guides. Travel guides published by well-known publishers such as
Lonely Planet are written by a small number of authors based on their travel experience. Presumably
if we could summarize the large amount of information given by ordinary users about a city, such a
summary could supplement the official travel guide and cover more up-to-date information.
However, social media content is diverse and noisy because it is contributed by many different au-
thors. Directly applying existing multi-document summarization methods to forum and CQA threads
may not produce good travel guides for the following reasons: (1) Summaries produced by standard
summarization methods are not structured, but travel guides usually follow a template structure. (2)
Travel guides put much emphasis on points of interest, which are usually location entities, but standard
text summarization methods are not entity-oriented.
To illustrate our points, in Table 1 we show (i) the overall structure of a travel guide for Sydney from
Lonely Planet, (ii) an excerpt from a summary generated by a state-of-the-art ILP-based summarization
method (Gillick and Favre, 2009) from a set of threads related to Sydney, and (iii) excerpts of a structured
summary generated by our proposed method. The comparison shows that the summary generated by
the standard ILP method mixes information on different topics together and does not mention many
* Corresponding author.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1670
Travel Guide from Lonely Planet (http://www.lonelyplanet.com/australia/sydney/)
Restaurants:
Sepia:There?s nothing washed out or brown-tinged about Sepia?s food: Martin Benn?s picture-perfect creations are presented in . . .
Icebergs Dining Room: Poised above the famous Icebergs swimming pool, Icebergs views sweep across the Bondi Beach arc to . . .
Shopping:
Strand Arcade: Constructed in 1891, the Strand rivals the QVB in the ornateness stakes. Three floors of designer fashions . . .
Westfield Sydney: The city?s newest shopping mall is a bafflingly large complex gobbling up Sydney Tower and a fair chunk of . . .
Transport:
Sydney Airport: Sydneys Kingsford Smith Airport , 10km south of the city centre, is Australias busiest airport, handling flights . . .
Water Taxis Combined:Fares based on up to four passengers; add $10 per person for additional passengers. Sample fares . . .
Yahoo! Answers Summary Generated by Standard ILP Method Yahoo! Answers Summary Generated by Our Method
It ?s not too far from Sydney . Sydney is the most expensive
place in Australia . They are a little lame ... Then you can go to
Darling Harbour, a beautiful habour which is a 10-minute walk
from town hall station . Make sure , if you are up to it to do the
bridge climb , this is a real treat . There are lots of interesting
things to see and do in and around Sydney . The suburbs-much
cheaper than the CBD. It was in the basement of a big shopping
mall . The only way to do that is to drive . Got to walk on top of
the Sydney harbour bridge and go up centre point tower ! Walk
around the street and see the beach . I would like to stay at a nice
hotel . My friend and I are wanting to take a trip to Sydney for
the summer . But you ?ll need to get there by taxi . Sydney is so
pretty, so you should be able to find stuff to do . And they have
many facilities . Good luck and have fun . Public transport is not
very good . Depending on what you ?re in Sydney to do it ?s hard
to say . . .
Restaurants:
Go to the two major restaurant areas close to the city Dar-
linghurst , along Oxford Street , and Newtown , along King
Street . Chinatown which is off George St. in the city look up
Dixon st. is a great place to get a cheap Chinese meal . . .
Shopping:
Queen Victoria Building and Pitt St Mall , World Square and
the Strand are good ideas to check out . Hair driers you can get
in many places , but the main places would be the department
stores such as Target , Big W , K-Mart , Myer, David Jones . . .
Transport:
The CBD is about 15 minutes by train from the airport and there
is a station at Circular Quay , right on the Harbour with access
to the bridge and the Opera House . You can catch an intercity
train with Cityrail from just about anywhere in Sydney . . .
Table 1: Comparison of different travel guides about Sydney. Top: excerpts from Lonely Planet. Bottom left: excerpt from
a summary generated by standard ILP. Bottom right: excerpts from summary generated by our method. Named entities are
highlighted in bold font.
interesting places to visit. The summary by our proposed method, in contrast, organizes the information
into sections and has a high coverage of places a tourist can visit.
To generate the kind of summaries as shown in the bottom right of Table 1, we propose to first leverage
the section structure of well-written travel guides and use a latent variable model to align forum threads
with the different sections from these travel guides. Moreover, observing that points of interest are orga-
nized by sections in these travel guides, we also identify location names from user-generated content and
try to uncover their underlying section labels. We then treat the remaining problem as a multi-document
summarization task. We modify an Integer Linear Programming (ILP)-based extractive summarization
framework (Gillick and Favre, 2009) to select sentences from forum threads to generate section-specific
summaries, where we specifically emphasize the inclusion of potential points of interest for each sec-
tion. Experiments using threads from Yahoo! Answers show that our proposed method generates better
summaries than a number of baselines in terms of ROUGE scores and coverage of named entities.
Our work makes the following contributions. First, we study a new problem of summarizing multiple
forum threads to generate city travel guides based on known template structure from well-written travel
guides. Second, we propose a principled approach based on latent variable models and Integer Linear
Programming. Third, we evaluate our method using real forum threads and human generated model
summaries, and the results are positive.
2 Overview of Our Method
Our task is to summarize travel-related information from forum threads for potential tourists. In order
to inject some structure into the generated summaries, we assume that we have a set of I well-written
travel guides that correspond to I different cities and have the same structure. We refer to these travel
guides as official travel guides. Each official travel guide consists of a fixed set of S sections such as
restaurants and shopping, and this section structure will be used to organize our generated summaries.
We further assume that each section of an official travel guide consists of a list of points of interest, each
with a name and a short description, as illustrated in Figure 1. We believe that this is a fairly common
structure followed by many if not all travel guides.
Given a target city, we assume that we can collect a set of threads about this city from travel-related
forums. In this paper we use threads from Yahoo! Answers, but our solution does not use any CQA
properties of the threads, so threads from other general forums can also be used. Our goal is to generate
a text summary with S sections from these threads, where each section has a length limit.
1671
As we have mentioned, we treat the problem as a multi-document summarization task. However,
different from standard text summarization, our generated summaries should contain S sections. To
achieve this goal, we first select a set of relevant threads for each section and then perform section-
specific summarization from the selected threads.
Thread selection: To select relevant threads given a section, a naive solution is to rank the threads based
on their relevance to the section, where relevance can be measured by, for example, cosine similarity
between a thread and all the text in the given travel guides belonging to the section. But we observe that
the language used in forum threads could be very different from that in the official travel guides, making
it hard to measure relevance purely based on lexical overlap. For example, in the entertainment section,
forum threads may contain words such as ?djs,? ?Xmas,? ?b?day? and ?anni.?, but these words do not
occur in the official travel guides. To overcome this difficulty, we propose to use a latent variable model
that jointly models official travel guides and forum threads. We treat the S sections as S latent factors
that govern the generation of the forum threads. With the latent factors observed in the official travel
guides, we receive some supervision; and yet by jointly modeling both the official travel guides and the
forum threads, we allow the latent factors to adapt to the lexical variations in user-generated content. In
the end, the learned latent factors can help us align forum threads with the sections and subsequently
select the most relevant ones for each section.
Section-specific summarization: Given the selected relevant threads for a section, we adopt an ILP-
based extractive summarization framework that has been shown to be effective (Gillick and Favre, 2009).
We modify the objective function in this framework to consider two factors: (1) Since not every sentence
in the selected threads is highly relevant to the section, we want to give preference to those more relevant
sentences in the objective function, where relevance can be measured using word distributions learned
by the latent variable model. (2) Since travel guides are expected to recommend points of interest to
readers, we try to maximize the coverage of section-specific location entities in the objective function.
3 Joint City Section Model
3.1 Model
In this section we present our Joint City Section Model (JCSM), which links official travel guides and
forum threads. The model is a typical extension of LDA, where a number of latent topics (i.e. latent
factors) are assumed to have generated the observed text. First of all, for each pre-defined section there
is a latent topic. These explain words such as ?food? and ?menu? for restaurants and ?store? and ?mall?
for shopping. In addition, in both travel guides and forum threads, some words are more related to the
city being discussed than any specific section. For example, when New York City is being discussed,
words such as ?NYC? and ?Manhattan? may frequently show up in any section. We therefore further
assume that for each city there is a city-specific topic. A switch variable is used to determine whether a
word comes from a city-specific or section-specific topic.
A special design of our model that differs from many existing LDA extensions is the treatment of
named entities. We first use a named entity recognizer to identify potential names of locations from
forum threads. We assume that each of these entities belongs to a section, which is indicated by a latent
variable. We then assume that the section labels of the non-entity words in forum threads are dependent
on the section labels of these entities. By doing so, we emphasize the importance of associating potential
points of interest with sections, which will be useful when we generate summaries.
We now formally present JCSM. To simplify the model description, we assume that we work with
I cities, each of which has a given, well-written travel guide and a set of forum threads. Note that in
practice this model can be easily extended such that a target city with forum threads does not need to have
a given travel guide to begin with. Let ?
i
denote the word distribution for the city-specific latent topic
associated with city i. Let ?
s
denote the word distribution for the section-specific latent topic for section
s. Let d
i,s,n
denote the n-th word in the s-th section of the i-th city?s travel guide. Here 1 ? d
i,s,n
? V is
an index into the vocabulary with size V . Let x
i,s,n
be a switch variable associated with d
i,s,n
to indicate
whether this word is city-specific or section-specific. For the j-th forum thread related to the i-th city, we
assume there is a distribution over sections, denoted as ?
i,j
. For the l-th location entity in the k-th post
1672
of this thread, we assume a latent variable c
i,j,k,l
(1 ? c
i,j,k,l
? S) that indicates the section label of this
entity. Then for the m-th word in this post, we first use a switch variable y
i,j,k,m
to determine whether
the word is city-specific or section-specific. If it is section-specific, we then choose one of the entities in
the same post, denoted as z
i,j,k,m
, and its corresponding section label as the section for this word.
All the binary switch variables follow a global Bernoulli distribution parameterized by pi. There are
hyperparameters ?, ?, ?
?
and ? that define the prior distributions. The complete model is depicted in
Figure 1. The generative process of JCSM is also described as follows.
S
N M L
K
J I
 d   w c ?
x y z
?
Figure 1: The plate notation of the Joint City Section Model (JCSM). Dashed variables will be integrated out in Gibbs sampling.
For clarity, the Dirichlet and Beta priors are omitted. The arrow pointing to z indicates that z is drawn from a uniform
distribution over the integers from 1 to L.
? For each city i, (i = 1, 2, ? ? ? , I), draw a city-specific word distribution ?
i
? Dir(?
?
)
? For each section s, (s = 1, 2, ? ? ? , S), draw a section-specific word distribution ?
s
? Dir(?)
? Draw a switch distribution pi ? Beta(?)
? For each city i (i = 1, 2, ? ? ? , I)
? For each section s (s = 1, 2, ? ? ? , S)
? For the n-th word in the given travel guide
- Draw x
i,s,n
? Bernoulli(pi)
- If x
i,s,n
= 1, draw d
i,s,n
? Multi(?
s
); otherwise, draw d
i,s,n
? Multi(?
i
).
? For the j-th thread
? Draw a thread specific section distribution ?
j
? Dir(?)
? For the k-th post
- For the l-th entity, draw c
i,j,k,l
? Multi(?
j
)
- For the m-th word, draw y
i,j,k,m
? Bernoulli(pi). If y
i,j,k,m
= 1, draw z
i,j,k,m
? Uniform(1, ? ? ? , L
i,j,k
)
and then draw w
i,j,k,m
? Multi(?
c
i,j,k,z
i,j,k,m
); otherwise, draw w
i,j,k,m
? Multi(?
i
).
3.2 Inference
We use collapsed Gibbs sampling to estimate the parameters in the model. The problem is to compute
the Gibbs update rules for sampling x
i,s,n
, c
i,j,k,l
, z
i,j,k,m
, y
i,j,k,m
.
Sample entity topic c
i,j,k,l
Let b denote {i, j, k, l} and u denote {i, j, k}. We can derive the Gibbs update rule for sampling entity
topic c
i,j,k,l
as follows:
p(c
b
= s|C
?
b
,W,D,X,Y,Z) =
n
s
i,j,?
b
+ ?
?
S
s
?
=1
n
s
?
i,j,?
b
+ S?
?
?
V
w=1
?
n
w
u,y=1,z=l
i
?
=1
(n
w
y=1,z=l,?
u
+ ? + i
?
? 1)
?
n
w
y=1,z=l,u
j
?
=1
(
?
V
w=1
n
w
y=1,z=l,?
u
+ V ? + j
?
? 1)
,
where n
s
i,j,?
b
denotes the number of entities whose topic assignments are s in thread {i, j} without
consideration of entity {i, j, k, l}. n
w
u,y=1,z=l
denotes the number of times term w occurs in the post
{i, j, k} with the constraint that y = 1 and z = l. n
w
y=1,z=l,?
u
is the number of times term w occurs in
all posts except the post {i, j, k} with the constraint that y = 1 and z = l.
Sample switch label x
i,s,n
We can derive the Gibbs update rule for sampling x
i,s,n
in a similar way. Note that the sampling of
x
i,s,n
is in travel guide word level. Let g denote{i, s, n}, the Gibbs update rule for sampling x
i,s,n
is as
follows:
1673
p(x
g
= 0|C,W,D
?
g
,X
?
g
,Y,Z) =
n
x=0
?
g
+ ?
?
1
x=0
n
x
?
g
+ 2?
?
n
w
g
x=0,i,?
g
+ ?
?
?
V
w=1
n
w
x=0,i,?
g
+ V ?
?
p(x
g
= 1|C,W,D
?
g
,X
?
g
,Y,Z) =
n
x=1
?
g
+ ?
?
1
x=0
n
x
?
g
+ 2?
?
n
w
g
x=1,s,?
g
+ ?
?
V
w=1
n
w
x=1,s,?
g
+ V ?
Sample post word topic z
i,j,k,m
and switch label y
i,j,k,m
For words in the thread posts, We can derive the Gibbs update rule for sampling post word topic z
i,j,k,m
and switch label y
i,j,k,m
. Note that the sampling of z
i,j,k,m
and y
i,j,k,m
is in post word level. Let f
denote{i, j, k,m}. The Gibbs update rule for sampling z
i,j,k,m
and y
i,j,k,m
is as follows:
p(z
f
= s|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
w
f
y=1,s
?
,?
f
+ ?
?
V
w=1
n
w
y=1,s
?
,?
f
+ V ?
?
1
L
i,j,k
p(y
f
= 0|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
y=0
?
f
+ ?
?
1
y=0
n
y
?
f
+ 2?
?
n
w
f
y=0,i,?
f
+ ?
?
?
V
w=1
n
w
y=0,i,?
f
+ V ?
?
p(y
f
= 1|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
y=1
?
f
+ ?
?
1
y=0
n
y
?
f
+ 2?
?
n
w
f
y=1,s
?
,?
f
+ ?
?
V
w=1
n
w
y=1,s
?
,?
f
+ V ?
where s
?
= c
i,j,k,l
which is the topic index of the associated entity of this word.
Parameter estimation
After Gibbs Sampling, we can make the following parameter estimation:
?
i,j,s
=
n
s
i,j
+ ?
?
S
s
?
=1
n
s
?
i,j
+ S?
. thread-section distribution.
?
s,w
=
n
w
s,y=1
+ ?
?
V
w
?
=1
n
w
?
s,y=1
+ V ?
. section-word distribution.
?
i,w
=
n
w
i,y=0
+ ?
?
?
V
w
?
=1
n
w
?
i,y=0
+ V ?
?
. city-word distribution.
pi
y
=
n
y
(.)
+ ?
?
1
y
?
=0
n
y
?
(.)
+ 2?
. switch distribution.
4 Generating Section-specific Summaries
With the JCSM model presented in the last section, we can learn a word distribution for each section,
which can help us find more relevant content for the section. For each section, we rank the forum
threads by how likely the words inside a thread is generated from the corresponding section-specific word
distribution. We select the top-K threads for each section to perform section-specific summarization.
Extractive summarization has been well studied and many algorithms have been proposed. We choose
to build our solution on top of an ILP-based framework proposed by Gillick and Favre (2009), partly
because our experiments comparing this ILP framework and other existing methods show its advantage
on our data sets (see Section 5). Below we first briefly review this ILP-based summarization framework
and then present our proposed improvements.
The idea behind the ILP framework by Gillick and Favre (2009) is to maximize the coverage of so-
called ?concepts? from the original corpus in the generated summary. In practice, bigrams are used as
concepts. Specifically, let us use i to index all the concepts from the original corpus. Let w
i
denote
the weight of the i-th concept computed based on its frequency and b
i
? {0, 1} denote the absence or
1674
presence of the concept. The framework aims to maximize
?
i
w
i
b
i
, i.e. the total weighted coverage of
the concepts, subject to the following constraints:
?
j
l
j
s
j
? L, (l
j
is the length of the j-th sentence in terms of words, and L is the length limit of the summary.)
?i, j : s
j
o
i,j
? b
i
, (s
j
? {0, 1} denotes the absence or presence of the j-th sentence.)
?i :
?
j
s
j
o
i,j
? b
i
. (o
i,j
? {0, 1} denotes whether concept i occurs in sentence j.)
Although this framework works well for standard summarization, our task is different. We propose the
following changes to this framework:
Favoring relevant sentences: Recall that although we select presumably the most relevant threads for
each section, we cannot guarantee that each sentence in these threads is related to the section. For
example, we observe that the things-to-do section is often mixed with content from restaurants, sights,
transport and entertainment sections. Also, some sentences are less relevant to the target city than
others. In order to select the more relevant sentences in the summary, we propose to add the second term
in Eqn. 1 below. Here j is used to index all the candidate sentences and u
j
is a weight for sentence j
based on its relevance.
We measure relevance with respect to both the city and the section. Let LL(j, ?) denote the log like-
lihood of generating sentence j from the section-specific topic ? and LL(j, ?) denote the log likelihood
of generating sentence j from the city-specific topic ?. We define u
j
as follows:
u
j
? exp (?LL(j, ?) + (1? ?)LL(j, ?)) .
u
j
are then normalized to be between 0 and 1. Note that here ? is a manually defined parameter used to
control the tradeoff between city-specific relevance and section-specific relevance. As we will show in
Section 5, both relevance factors turn out to be useful.
Covering section-specific points of interest: We hypothesize that a good summary travel guide should
mention potential points of interest to the reader. To this end, the last term in Eqn. 1 is added. Specifically,
k is an index for unique location names we find that have been labeled as belonging to section s according
to the JCSM model. e
k
? {0, 1} denotes whether the k-th entity is present in the selected sentences, and
v
k
denotes the weight for this entity based on its frequency.
Eventually, the summarization task is formulated as the following optimization problem:
Maximize: ?
1
?
i
w
i
b
i
+ ?
2
?
j
u
j
s
j
+ (1? ?
1
? ?
2
)
?
k
v
k
e
k
(1)
Subject to:
?
j
l
j
s
j
? L,
?i :
?
j
s
j
o
i,j
? b
i
, ?i, j : s
j
o
i,j
? b
i
,
?j :
?
k
s
j
p
j,k
? e
k
, ?j, k : s
j
p
j,k
? e
k
.
Here o
i,j
denotes whether concept i occurs in sentence j, and p
j,k
denotes whether entity k occurs in
sentence j. For the weights w
i
and v
k
, we normalize them using the total occurrences of bigrams/entities
to ensure their values are between 0 and 1. We solve the above optimization problem using the IBM
ILOG CPLEX Optimizer
1
.
5 Experiments
5.1 Data and Experimental Setup
We use real data from Yahoo! Answers and Lonely Planet for evaluation. We first crawl the travel guides
for 10 cities from Lonely Planet, where each travel guide has 8 sections. We then crawl the top 60000
Q&A threads ranked by number of posts related to these 10 cities (6000 for each city) from Yahoo!
Answers under the ?travel? category where all questions have been grouped by cities. We filter out
trivial factoid questions using features used by Tomasoni and Huang (2010). We then use the Stanford
1
http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/
1675
Method Singapore Sydney New York City Los Angeles Overall Average
R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4
Random 0.4091 0.1046 0.1576 0.4496 0.1100 0.1925 0.4442 0.1192 0.1858 0.4154 0.1130 0.1693 0.4309 0.1115 0.1771
Centroid 0.4029 0.0993 0.1484 0.4228 0.1100 0.1764 0.4235 0.1192 0.1722 0.3763 0.0787 0.1386 0.4133 0.1077 0.1640
LexRank 0.4396 0.1451 0.1891 0.4406 0.1296 0.1955 0.4304 0.1397 0.1859 0.4032 0.0992 0.1661 0.4350 0.1331 0.1894
DivRank 0.4534 0.1504 0.1888 0.4473 0.1161 0.1925 0.4391 0.1167 0.1804 0.4275 0.1180 0.1733 0.4487 0.1317 0.1888
GMDS 0.3918 0.0890 0.1415 0.4339 0.1066 0.1784 0.4064 0.0845 0.1576 0.3846 0.0809 0.1413 0.4045 0.0916 0.1553
ILP-BL 0.4635 0.1650 0.2000 0.4948 0.1731 0.2333 0.4691 0.1613 0.2073 0.4545 0.1445 0.1981 0.4755 0.1654 0.2136
Our Method 0.4723 0.1655 0.2035 0.5078 0.1787 0.2397 0.4716 0.1713 0.2086 0.4543 0.1565 0.1945 0.4804
?
0.1715
?
0.2144
?
Table 2: Comparison of the summarization results.
?
means the result is better than others except ILP-BL in the same column
at 5% significance level measured by Wilcoxon signed rank test. Note that only the average scores are tested for statistical
significance based on the 32 summarization tasks in total.
NER tool to recognize named entities in these threads. Since we notice that sometimes entities tagged as
PER are also possible points of interest, we include all entities of LOC, ORG and PER types. In order
to use higher quality threads for evaluation, for each city we pick the top 600 threads that have the most
overlapping points of interest with the Lonely Planet travel guides. On average, each thread contains 5.0
posts and 618.1 words. These 600? 10 threads are used to train the JCSM model.
We need human generated model summaries for evaluation. Since it is too time consuming to ask
human annotators to look through 600 threads and generate structured summaries, we instead opt to
first retrieve the top 30 relevant threads per section per city based on the JCSM results and then ask
human annotators to summarize these 30 threads to generate a section-specific summary. Our summa-
rization method as well as the baselines are also applied to these 30 threads per section per city for fair
comparison. We randomly select 4 cities for human annotation, giving us 8 ? 4 = 32 section-specific
summarization tasks. For each task, we ask four annotators to read all 30 threads and write a summary
as model summaries in our experiments
2
.
We use the following baseline algorithms for comparison: (1) Random, which randomly picks sum-
mary sentences. (2) Centroid (Radev et al., 2004), which selects sentences according to several features
like tfidf, cluster centroid and position. (3) LexRank (Erkan and Radev, 2004b)., which applies a graph-
based algorithm . (4) DivRank (Mei et al., 2010), which employs a time-variant random walk to enhance
diversity. (5) GMDS (Wan, 2008), which incorporates the document-level information and the sentence-
to-document relationship into the ranking process. (6) ILP-BL, which is the method proposed by Gillick
and Favre (2009).
We empirically set Dirichlet hyperparameters ? = 0.5, ? = 0.01, ? = 0.01, ?
?
= 0.1. We run JCSM
with 400 iterations of Gibbs sampling. For the weight parameters in the ILP model, we empirically set
?
1
= 0.7, ?
2
= 0.1, ? = 0.7 after we conduct multiple experiments to determine the best values of them
from 0.1 to 0.9.
5.2 Summarization Results
To compare the summaries generated by our method with those generated by the baselines, we first
compute their ROUGE scores against the human generated model summaries. ROUGE scores have
been widely used for evaluation of summarization systems (Lin and Hovy, 2003). We use the ROUGE
toolkit
3
, which provides multiple kinds of ROUGE metrics including ROUGE-N, ROUGE-L, ROUGE-
W and ROUGE-SU4. In the experiment results we report three ROUGE F-measure scores, namely,
ROUGE-1, ROUGE-2 and ROUGE-SU4. The higher the ROUGE scores, the better a summary is.
In Table 2 we show the summarization results of our method (with the optimal parameter setting) and
the baseline methods. For each city, the scores we show are averaged over the 8 sections. The overall
average scores on the right hand side are averaged over the 4 cities. We have the following findings from
the table: (1) Compared with the other baselines, the ILP-based baseline clearly shows its advantage,
justifying our our design choice of adopting an ILP-based framework as the basis of our method. (2)
Our method performs slightly better than ILP-BL based on the overall scores, but the difference is not
statistically significant.
2
The summary dataset can be found at https://sites.google.com/site/liuyang198908/code-data.
3
http://www.isi.edu/licensed-sw/see/rouge/
1676
section Singapore Sydney New York City Los Angeles
ILP-BL Our Method ILP-BL Our Method ILP-BL Our Method ILP-BL Our Method
restaurants 0.3750 0.5417 0.5714 0.7143 0.2500 0.3750 0.1053 0.2105
hotels 0.4091 0.4091 0.0000 0.5000 0.3636 0.5000 0.4500 0.5500
shopping 0.1429 0.5357 0.3750 0.3750 0.1905 0.1905 0.0455 0.1818
sights 0.5000 0.5789 0.3846 0.4615 0.3636 0.6364 0.1143 0.2571
entertainment 0.1304 0.2174 0.2500 0.7500 0.0909 0.2273 0.2500 0.4167
activities 0.4167 0.5833 0.2500 0.2500 0.1250 0.5000 0.2069 0.2759
transport 0.3889 0.5556 0.7500 0.7500 0.6000 0.8000 0.3158 0.7368
things-to-do 0.2105 0.2632 0.2500 0.5500 0.4583 0.5833 0.0000 0.2000
average 0.3217 0.4606 0.3539 0.5439 0.3052 0.4766 0.1860 0.3536
Table 3: Comparison of the recall of named entities of ILP-BL and our method.
Method Our Complete Model ?EC ?SR ?SecRel ?CityRel
R-1 0.4804 0.4520 0.4657 0.4672 0.4796
R-2 0.1715 0.1430 0.1669 0.1652 0.1685
RSU4 0.2144 0.1987 0.2028 0.2039 0.2120
Table 4: Summarization results of the degenerate versions of our method. ??? means removing this component from our
complete method. The table shows the average results over data sets of all cites.
Considering that an importance difference between our method and ILP-BL is our focus on points
of interest, we further compared ILP-BL and our method using a different metric. The objective is to
test the coverage of points of interest in our generated summaries versus the summaries generated by
ILP-BL. To this end, we first identify all the named entities in the model summaries using the Stanford
NER tool. We then check the percentage of these named entities covered in the generated summaries and
report these recall scores in Table 3. We can see that for majority of the 32 section-specific summaries,
our method clearly has a higher recall score than ILP-BL, showing that our method generates summaries
with more potential points of interest.
To further understand whether all the components of our improved ILP method have contributed to
the performance improvement, we compare our overall method with a few degenerate versions of our
method. In each degenerate version, we remove a single component of the objective function. The results
are shown in Table 4, where?EC removes the consideration of entity coverage (i.e. setting ?
1
+?
2
= 1),
?SR removes the consideration of sentence relevance (i.e. setting ?
2
= 0), ?SecRel removes only the
section-specific relevance of the sentences (i.e. setting ? = 0), and ?CityRel removes only the city-
specific relevance of the sentences (i.e. setting ? = 1). We can see that each degenerate version of our
method performs worse than the complete method, which shows that all components of the objective
function are useful. In particular, entity coverage and section-specific relevance seem to be the more
important components.
 0
 0.2
 0.4
 0.6
 0.8
 1 0
 0.2
 0.4
 0.6
 0.8
 1
 0.4
 0.5
 0.6
lambda2
lambda1
 0.445 0.45
 0.455 0.46
 0.465 0.47
 0.475 0.48
 0.485 0.49
 0.495
(a) R-1(?
1
, ?
2
)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
rho
R-1
R-2
RSU4
(b) ?
Figure 2: Summarization performance of our method by varying the value of the parameters ?
1
, ?
2
and ?.
1677
5.3 Analysis of Topic Words
We show some further analysis of our results. To begin with, we analyze the learning results of JCSM.
The top words in city-specific word distributions and section-specific word distributions learnt by JCSM
are presented in Table 5 and Table 6. Generally we observe clean top words for each city and each
section. For each city, city-specific words are those associated with the corresponding city. For example,
for Singapore, we see words such as ?s$? (Singapore dollars), ?sentosa? (an island resort in Singapore),
?orchard? (a boulevard that is the retail and entertainment hub of Singapore) and ?bugis? (a popular
shopping place). For New York City, we see ?square?, ?times? and ?manhattan?. For each section,
section-specific words are those words which frequently appear when people discuss about this section,
such as ?menu?, ?dishes? and ? seafood? for the restaurant section and ?train?, ?bus? and ?station? for
the transport section.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9 Topic 10
Singapore SFO Chicago Boston LA NYC Seattle Pairs London Sydney
singapore sf chicago boston beach york downtown paris london sydney
s$ san downtown end hollywood nyc seattle de tube harbour
centre francisco park north los park needle metro underground beach
food gate city downtown angeles central space eiffel central manly
shopping golden neighborhood fenway la square market french centre beaches
sentosa bay north bay downtown times rain la british house
road bart lake harvard drive manhattan place du palace opera
orchard union mile place california broadway pike tower thames quay
chinese wharf loop city miles city center des end australian
mrt muni ave college hills street waterfront rue kensington rocks
bugis square field subway long east area le station bridge
Table 5: Top city specific words discovered by JCSM.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8
restaurants hotels shopping sights entertainment activities transport thingsToDo
food hotel shop museum bar visit train bar
restaurant rooms store city music park bus place
menu free stores park club tour station tour
dishes wi-fi shopping art place fun airport city
place walk shops building night city time food
bar located find built dance walk line art
chicken offers clothes world beer day car day
fish station wear place clubs time walk including
fresh features mall house crowd shopping minutes music
seafood tv place area bars museum hours restaurant
Table 6: Top section specific words discovered by JCSM.
5.4 Parameter Sensitivity Analysis
We further give parameter sensitivity analysis for our proposed method. We show how sensitive our
results are with respect to the parameters ?
1
, ?
2
and ?. We choose the Sydney data set to perform
parameter sensitivity analysis. In Figure 2(a), we show how ROUGE-1 varies with respect to ?
1
and ?
2
.
We can see that the performance fluctuates within a limited range as we vary ?
1
and ?
2
. We find the
trend for ROUGE-2 and ROUGE-SU4 is similar so we leave out the figures for them. In Figure 2(b) we
see that the performance is pretty stable as we vary ?.
5.5 Sample Output and Case Study
Finally, we show a sample travel guide our method generates for Sydney in Table 7. We can see that first
of all the sentences selected by our method have high relevance to the corresponding sections. Second,
through observation we find that humans tend to select sentences containing more points of interest as
summary. Our summary sentences contain many points of interest as highlighted, showing the advantage
of our method.
1678
Sample Summary Sentences Generated from Yahoo! Answers by Our Method for Sydney
Hotel
Sorry can not recommend you a hotels as I have no idea of pricing , but if you want a nice area , check hotels in Bondi and Manly Beaches .
As for the Acer Arena , that is in the Homebush Olympic Park and you can choose to live in either Parramatta or the city .
You need to live in one of the surrounding residential suburbs , close to a train line . Try Alexandria , Newtown , Surry hills for inner suburbs . . . .
Sights
You can walk around the harbor area to the Opera House and you can see the beautiful Harbor Bridge .
All this is apart from the Opera House and the Botanical Gardens . Visit the Custom House Circular Quay and see a model of Sydney. You
must also do a day trip to the Blue Mountains . Harbour Wedding is one of the major attraction in Sydney . . . .
Entertainment
George Street has a number of bars . All the bars around the harbour are really good day and night . If you want to stay in a hotel where there is
entertainment at night , you could look at Woolloomooloo , Darlinghurst , Surry Hills , Kings Cross or Potts Point . Newtown is good for bars .
Get them to see a theatre show or something at the Opera House . . . .
Things-to-do
If you are going out for the day , starting with a walk to the city will be most enjoyable . Take a public ferry from Circular Quay to Darling
Harbour , about 15 minutes across the harbour and under the bridge , when you get to Darling Harbour go and see the Chinese Gardens . There
are lots of interesting things to see and do in and around Sydney . . . .
Activities
They have good markets at the weekend and great views of the Opera House . The Opera House is free to have a look at , if you like art then walk
through the Botanical Gardens and go and see the art gallery . If you ?re feeling brave , you can do a Harbour Bridge walk , though I think it may
be a little pricey . . . .
Table 7: Excerpts from the summary generated from Yahoo! Answers by our method for Sydney. We show summaries for the
5 sections other than the 3 sections shown in Table1. Named entities are highlighted in bold font.
6 Related Work
Multi-document summarization is a process to generate a text summary by reducing documents in size
while retaining the main points of the original documents. It has been extensively studied in the NLP
community, with most efforts on extractive summarization. Our work is also based on extractive sum-
marization. Extractive summarization essentially selects a set of sentences from the original documents
to form a summary.
To select sentences, different features and ranking strategies have been studied. Early work focuses
on finding good features to select summary sentences. Radev et al. (2004) proposed a centroid-based
summarizer which combines several pre-defined features like tfidf, cluster centroid and position to score
sentences. Lin and Hovy (2002) built the NeATS multi-document summarization system using term fre-
quency, sentence position, stigma words and simplified Maximal Marginal Relecvance (MMR). Nenkova
et al. (2006) proved that high-frequency words were significant in reflecting the focus of documents.
Ouyang et al. (2010) studied the influence of different word positions in summarization. Later, graph-
based ranking algorithms have been successfully applied to summarization. LexPageRank (Erkan and
Radev, 2004a) is a representative one based on the PageRank algorithm (Page et al., 1999). Later exten-
sions include ToPageRank (Pei et al., 2012), which incorporates topic information into the propagation
mechanism, the manifold-ranking based method for topic-focused summarization (Wan et al., 2007) and
DivRank (Mei et al., 2010), which introduces a time-variant matrix into a reinforced random walk to
balance prestige and diversity.
More recently, Integer Linear Programming (ILP) based framework was introduced as a global infer-
ence algorithm for multi-document summarization by McDonald (2007), which considers information
and redundancy at the sentence level. Gillick and Favre (2009) studied information and redundancy at a
sub-sentence, ?concept? level, modeling the value of a summary as a function of the concepts it covers.
In our work we also model concept level coverage of the summaries. Li et al. (2013) proposed a re-
gression model to estimate the frequency of bigrams in the reference summary and analyzed the impact
of bigram selection, weight estimation and ILP setup. Haghighi and Vanderwende (2009) constructed
a sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGE
gains along the way. Sauper and Barzilay (2009) investigated an approach for creating a comprehensive
textual overview of subject composed of information drawn from the Internet and applied ILP to opti-
mize both local fit of information into each topic and global coherence across the entire overview. Li
et al. (2011) developed an entity-aspect LDA model to cluster sentences into aspects and then extend
LexRank algorithm to rank sentences. Hu and Wan (2013) proposed to use SVR model and ILP method
to generate presentation slides for academic papers.
Our work is different from standard ILP-based multi-document summarization. We designed a latent
variable model to first separate the threads to be summarized into sections based on model gravel guides.
1679
We also emphasized the inclusion of potential points of interest in formulating the ILP optimization
problem.
Our work is also closely related to previous work on answer summarization in community-based
QA sites. Previous work on summarizing answers is mainly based on query focused multi-document
summarization techniques to summarize multiple answer documents given a single question. Liu et al.
(2008) proposed a CQA question taxonomy to classify questions in CQA and question-type oriented
answer summarization for better reuse of answers. Tomasoni and Huang (2010) proposed two concept-
scoring functions to combine quality, coverage, relevance and novelty measures for answer summary
in response to a question and showed that their summarized answers constitute a solid complement to
best answers voted by CQA users. Chan et al. (2012) presented an answer summarization method for
complex multi-sentence questions. For our work, we study a new problem of summarizing multiple
threads to automatically generate city travel guides based on known template structure from well-written
travel guides, which is different from the setting of single Q&A thread summarization in the previous
related studies.
7 Conclusion and Future Work
In this paper we proposed a summarization framework to generate well structured supplementary travel
guides from social media based on a latent variable model and integer linear programming. The la-
tent variable model could align forum threads with the section structure of well-written travel guides.
Compared to standard concept based ILP methods, our method additionally tries to cover more named
entities as points of interest and maximizes sentence relevance scores measured by section-specific and
city-specific word distributions learnt by the latent variable model. Extensive experiments with real data
from Yahoo! Answers show that our proposed method is able to generate better summaries compared
with a number of multi-document summarization baselines measured by ROUGE scores.
Currently our generated summaries may have overlap with the well-written model travel guides. In the
future, we plan to improve our method to emphasize the selection of additional information from social
media compared with the model travel guides. We will also look into the problem of how to summarize
information that does not fit into the template structure derived from model travel guides.
Acknowledgments
This work was done during Liu Yang?s visit to Singapore Management University. The authors would
like to thank the reviewers for their valuable comments on this work.
References
Wen Chan, Xiangdong Zhou, Wei Wang, and Tat-Seng Chua. 2012. Community answer summarization for multi-
sentence question with group l1 regularization. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Volume 1, ACL ?12, pages 582?591, Stroudsburg, PA, USA.
Association for Computational Linguistics.
G?unes Erkan and Dragomir R. Radev. 2004a. Lexpagerank: Prestige in multi-document text summarization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 4 of EMNLP
?04.
G?unes Erkan and Dragomir R. Radev. 2004b. Lexrank: Graph-based lexical centrality as salience in text summa-
rization. J. Artif. Int. Res., 22(1):457?479, December.
Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Langauge Processing, ILP ?09, pages 10?18, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, NAACL ?09, pages 362?370, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1680
Yue Hu and Xiaojun Wan. 2013. Ppsgen: Learning to generate presentation slides for academic papers. In
Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI ?13, pages 2099?2105.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011. Generating aspect-oriented multi-document summariza-
tion with event-aspect model. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, EMNLP ?11, pages 1137?1146, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In
Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, ACL ?13, pages
1004?1013, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2002. From single to multi-document summarization: A prototype system and
its evaluation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL
?02, pages 457?464, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, NAACL ?03, pages 71?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and sum-
marizing answers in community-based question answering services. In Proceedings of the 22Nd International
Conference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings
of the 29th European Conference on IR Research, ECIR?07, pages 557?564, Berlin, Heidelberg. Springer-
Verlag.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Divrank: The interplay of prestige and diversity in informa-
tion networks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ?10, pages 1009?1018, New York, NY, USA. ACM.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multi-
document summarizer: Exploring the factors that influence summarization. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ?06, pages
573?580, New York, NY, USA. ACM.
You Ouyang, Wenjie Li, Qin Lu, and Renxian Zhang. 2010. A study on position information in document
summarization. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,
COLING ?10, pages 919?927, Stroudsburg, PA, USA. Association for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bring-
ing order to the web. Technical Report 1999-66, Stanford InfoLab, November. Previous number = SIDL-WP-
1999-0120.
Yulong Pei, Wenpeng Yin, and Lian?en Huang. 2012. Generic multi-document summarization using topic-oriented
information. In Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelli-
gence, PRICAI?12, pages 435?446, Berlin, Heidelberg. Springer-Verlag.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty?s, and Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Inf. Process. Manage., 40(6):919?938, November.
Christina Sauper and Regina Barzilay. 2009. Automatically generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
208?216, Stroudsburg, PA, USA. Association for Computational Linguistics.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-aware measures for answer summarization in community
question answering. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA. Association for Computational Linguistics.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multi-document
summarization. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI?07,
pages 2903?2908, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Xiaojun Wan. 2008. An exploration of document impact on graph-based multi-document summarization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages
755?762, Stroudsburg, PA, USA. Association for Computational Linguistics.
1681
Proceedings of NAACL-HLT 2013, pages 401?410,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Mining User Relations from Online Discussions using Sentiment Analysis
and Probabilistic Matrix Factorization
Minghui Qiu?, Liu Yang?,?, Jing Jiang?
? School of Information Systems, Singapore Management University, Singapore
? School of Software and Microelectronics, Peking University, China
{minghui.qiu.2010,jingjiang}@smu.edu.sg, yang.liu@pku.edu.cn
Abstract
Advances in sentiment analysis have enabled
extraction of user relations implied in online
textual exchanges such as forum posts. How-
ever, recent studies in this direction only con-
sider direct relation extraction from text. As
user interactions can be sparse in online dis-
cussions, we propose to apply collaborative
filtering through probabilistic matrix factor-
ization to generalize and improve the opinion
matrices extracted from forum posts. Exper-
iments with two tasks show that the learned
latent factor representation can give good per-
formance on a relation polarity prediction task
and improve the performance of a subgroup
detection task.
1 Introduction
The fast growth of the social Web has led to a large
amount of interest in online social network analysis.
Most existing work on social network analysis re-
lies on explicit links among users such as undirected
friendship relations (Liben-Nowell and Kleinberg,
2003), directed following relations (Hopcroft et al,
2011) and trust/distrust relations (Leskovec et al,
2010). However, besides these explicit social rela-
tions, the various kinds of interactions between on-
line users often suggest other implicit relations. In
particular, in online discussion forums, users inter-
act through textual posts and these exchanged texts
often reveal whether two users are friends or foes, or
whether two users share the same viewpoint towards
a given issue.
To uncover such implicit relations requires text
analysis and particularly sentiment analysis. Re-
cently, Hassan et al (2012) studied predicting the
polarity of user interactions in online discussions
based on textual exchanges. They found that the au-
tomatically predicted signed relations had an accu-
racy above 80%. The extracted signed network was
further used to detect ideological subgroups. This is
a piece of pioneering work that extracts online social
relations based on text analysis.
In this paper, we further extend the idea of mining
social relations from online forum posts by incorpo-
rating collaborative filtering. Our work is motivated
by the observation that direct textual exchanges be-
tween users are sparse. For example, in the data set
we use, only around 13% of user-user pairs have di-
rect interactions. Collaborative filtering is a com-
monly used technique in recommender systems to
predict missing ratings. The key assumption is that
if two people have the same opinion on an item A,
they are likely to also have the same opinion on a
different item B. In online discussion forums, users
express their opinions about each other as well as
the various aspects of the topic under discussion, but
not every user comments on every aspect or every
other user. Collaborative filtering allows us to iden-
tify users with the same opinion even if they have not
directly interacted with each other or commented on
any common aspect.
Our method starts with extracting opinions on
users and topic aspects from online posts using sen-
timent analysis. The results are two matrices indi-
cating the sentiment polarity scores between pairs
of users and pairs of a user and an aspect. To in-
corporate collaborative filtering, we choose proba-
bilistic matrix factorization (PMF) (Salakhutdinov
401
and Mnih, 2008), a technique that has been success-
fully applied for collaborative filtering-based recom-
mendation problems. PMF automatically discovers
a low-rank representation for both users and items
based on observed rating data. In our problem, the
predicted sentiment polarity scores are treated as rat-
ing data, and the results of PMF are low-rank vectors
representing each user in online discussions.
We evaluate our method on two tasks. The first
is to predict the polarity of interactions between two
users not from their own textual exchanges but from
their interactions with other users or comments on
topic aspects. The second is to use the latent vectors
to group users based on viewpoints. We find that the
latent factor representation can produce good predic-
tion results for the first task and improve the cluster-
ing results of the second task compared with a num-
ber of baselines, showing the effectiveness of col-
laborative filtering for mining social relations from
online discussions.
2 Related Work
Our work is closely related to recent studies on
detecting subgroups from online discussions (Abu-
Jbara et al, 2012; Dasigi et al, 2012; Hassan et
al., 2012). Abu-Jbara et al (2012) proposed to
build discussant attitude profiles (DAP) from on-
line posts and use these profiles to cluster users into
subgroups. A DAP is a vector that contains the
attitudes of a discussant towards other discussants
and a set of opinion targets. We also extract opin-
ions of users towards other users and opinion tar-
gets from posts, which are similar to DAPs. The
difference is that we further apply probabilistic ma-
trix factorization to derive a low-rank representation
from the raw opinion scores. Our comparison with
DAP-based clustering shows that probabilistic ma-
trix factorization can improve subgroup detection.
Hassan et al (2012) proposed to predict the polar-
ity of interactions between users based on their tex-
tual exchanges. They defined a set of interaction
features using sentiment analysis and applied super-
vised learning for polarity prediction. In compari-
son, our work is unsupervised, that is, we do not use
any ground truth of interaction polarity for training.
Probabilistic matrix factorization was proposed
by Salakhutdinov and Mnih (2008) as a collabo-
rative filtering method for recommender systems.
It has attracted much attention and been extended
by Ma et al (2008) and Wang and Blei (2011).
In particular, Ma et al (2008) proposed a SocRec
model that combines social network information
with rating data using the PMF framework to per-
form social recommendation. Our model bears sim-
ilarity to SocRec in that we also consider two types
of interactions, i.e. user-user interactions and user-
aspect interactions. However, different from Ma et
al. (2008), we predict both the user-user and user-
aspect scores from textual posts using sentiment
analysis, and the user-user opinion polarity scores
are symmetric.
Part of our method uses sentiment analysis to ex-
tract opinions from text. This is built on top of a
large body of existing work on opinion extraction,
e.g. Choi et al (2006) and Wu et al (2009). As the
sentiment analysis component is not our main con-
tribution, we do not review existing work along this
direction in detail here. Interested readers can refer
to Pang and Lee (2008).
The idea of incorporating sentiment analysis into
collaborative filtering algorithms has been explored
by Kawamae (2011), Moshfeghi et al (2011) and
Leung et al (2011). While their work also com-
bines sentiment analysis with collaborative filtering,
the purpose is to improve the accuracy of item rec-
ommendation. In contrast, we borrow the idea and
technique of collaborative filtering to improve user
relation mining from online text.
3 Method Overview
In this section, we provide an overview of our
method. We first introduce some concepts.
User: We use user to refer to a discussant in an on-
line discussion. Each user has an online ID, which
can be used by other users to refer to him/her in a
post. Users are both opinion holders and opinion
targets. For example, User 1 below expresses a neg-
ative opinion towards another user in the following
snippet.
User 1: Actually, I have to disagree with you.
Aspect: We use topic aspect or aspect to refer to an
opinion target that is related to the topic under dis-
cussion. For example, when debating about whether
one should vote for Obama, people may express
402
opinions on targets such as ?President Obama? and
?Republican party,? as shown in the following snip-
pets. These aspects are all related to Obama?s pres-
idential campaign. As we will explain later, the as-
pects we consider are named entities and frequent
noun phrases.
User 2: Americans should vote for President Obama be-
cause he picks good corporations as winners.
User 3: I simply point out how absolutely terrible the Re-
publican party is.
Polarity Score: A sentiment polarity score is a
real number between 0 and 1, where 0 indicates a
completely negative opinion and 1 indicates a com-
pletely positive opinion.
User-User Opinion Matrix: The opinions ex-
tracted from posts between users are represented by
a user-user opinion matrix S, where entry si,j is a
polarity score between the i-th user and the j-th user.
We assume that the polarity scores are symmetric.
User-Aspect Opinion Matrix: The opinions held
by different users on the various topic aspects are
represented by a user-aspect opinion matrix R,
where entry ri,k is a polarity score indicating the i-th
user?s opinion towards the k-th aspect.
Given the matrices S and R, we perform proba-
bilistic matrix factorization to derive a low-rank vec-
tor representation for users and aspects such that if
the polarity score between two users or a user and
an aspect is high, the dot product between the corre-
sponding two vectors is also high.
In Section 4, we will explain in detail how we
identify topic aspects from a discussion thread and
how we obtain polarity scores from posts. In Sec-
tion 5, we will present the details of our probabilistic
matrix factorization model.
4 Construction of Opinion Matrices
The opinion matrices are constructed from a single
forum thread discussing some controversial topic.
4.1 Aspect Identification
As we have pointed out, there are two kinds of opin-
ion targets, namely users and aspects. Users are
clearly defined and can often be identified in posts
by their IDs or second person pronouns. For aspects,
however, there is not a pre-defined set. We observe
that these topic aspects are usually named entities
or noun phrases frequently mentioned. We therefore
use the OpenNLP toolkit1 to perform chunking and
obtain noun phrases and the Standford NER tagger2
to identify named entities from the posts.
Some of the candidate aspect phrases identified
above actually refer to the same actual aspect, e.g.
?Obama voter,? ?Obama voters? and ?the Obama
voter.? We remove stop words from each candidate
phrase and use the WordNet by Miller (1995) to ob-
tain the lemma of each word such that we can nor-
malize the candidate aspect phases to some extent.
Finally, to select salient aspects for a given discus-
sion topic, we count the number of times each candi-
date aspect has been expressed a positive or negative
opinion on by all users, and select those candidate
aspects which have opinion expressions from at least
M users. We set M to 2 in our experiments. Fig-
ure 1 shows the top salient aspects for the thread on
?Will you vote for Obama?? We acknowledge there
are still duplicate aspects in the results like ?Repub-
lican Party? and ?GOP?. To normalize these aspects,
some additional information such as Wikipedia en-
tries and Google snippets may be considered. We
will study this problem in our future work.
4.2 Opinion Expression Identification
Our next step is to identify candidate opinion expres-
sions. This problem has been studied in Hu and Liu
(2004), Popescu and Etzioni (2005), and Hassan
and Radev (2010). Based on previous work, we do
the following. We first combine three popular sen-
timent lexicons to form a single sentiment lexicon:
the lexicon used in Hu and Liu (2004), MPQA Sub-
jectivity Lexicon by Wilson et al (2005) and Senti-
WordNet by Baccianella et al (2010). Our final sen-
timent lexicon contains 15,322 negative expressions
and 10,144 positive expressions. We then identify
candidate opinion expressions by searching for oc-
currences of words in this lexicon in the posts.
4.3 Opinion Relation Extraction
Given a post that contains an aspect and an opin-
ion expression, we still need to determine whether
the opinion expression is used to describe the as-
pect. This is a relation extraction problem. We use a
supervised learning approach based on dependency
1http://opennlp.apache.org/
2http://nlp.stanford.edu/ner/index.shtml
403
0
20
40
60
80
100
OBAM
A BUSHAMER
ICA PALIN
REPU
BLICA
N
CONG
RESSTAX_
CUT
AME
RICA
N
CLIN
TONMCC
AIN
TEA_
PART
Y IRAQ
SARA
H_PA
LIN
PRES
IDEN
T_OB
AMAREAG
AN
RON_
PAUL
ECON
OMIC
_POL
ICY
AFGH
ANIS
TANCART
ER FOX
HEAL
TH_C
ARE
NATI
ONAL
_DEB
T
DEM
OCRA
T GOP
MIDD
LE_C
LASS
OBAM
A_AD
MINIS
TRAT
ION
REPU
BLICA
N_PA
RTY
TAX_
BREA
K
WAS
HING
TON
FEDE
RAL_
GOVE
RNM
ENT
HEAL
TH_C
ARE_
REFO
RMHITLE
R
IRAQ
_WAR
WAL
L_ST
REET
Figure 1: Salient aspects and number of users who express opinions on them in the thread ?Will you vote for Obama??
ID Dependency path rule Example
R1 ADJOP ? amod? NTR I simply point out how terrible REPUBLICAN PARTY is.
R2 ADJOP ? nsubj ? NTR BUSH is even more reasonable for tax hike than Obama.
R3 VOP ? dobj ? NTR I would never support OBAMA.
R4 VOP ? prep ? ? NTR I?ll vote for OBAMA.
R5 VOP ? nsubjpass? NTR DEMOCRATIC PARTY are ultimately corrupted by love of money.
R6 NOP ? dobj ? V ? nsubj ? NTR PAKISTAN is increasing terrorist threat.
R7 ADJOP ? amod? N ? nsubj ? NTR OBAMA was a top scorer for occidental college.
R8 ADVOP ? advmod? V ? nsubj ? NTR OBAMA is smarter than people.
Table 1: Examples of frequent dependency path rules in our training data. OP and TR refer to the opinion and the
target. The opinion words are in italic and the aspect words are in uppercase.
paths. Previous work by Mintz et al (2009), and Qiu
et al (2009) has shown that the shortest path be-
tween a candidate opinion aspect and a candidate
opinion expression in the dependency parse tree can
be effective in extracting opinion relations. We use
the Stanford Parser from Klein and Manning (2003)
to obtain the dependency parse trees for each sen-
tence in the posts and then get the dependency paths
between each pair of candidate aspect and opinion
expression. We use dependency relations and POS
tags of nodes along the path to represent a depen-
dency path. Given a set of training sentences (we
use the one from Wu et al (2009)), we can get a set
of dependency path rules based on their frequencies
in the training data. Table 1 shows the frequent de-
pendency path rules in our training data.
When a pair of aspect and opinion expression is
identified to be related, we use the polarity of the
opinion expression to label the relation. Finally,
given a pair of users, we use the percentage of pos-
itive interactions between them over all subjective
interactions (i.e. interactions with either positive or
negative opinions) as extracted from their exchanged
posts as the sentiment polarity score between the
two users, regardless of the reply-to direction of
the posts. Similarly, given a user and an aspect,
we also use the percentage of positive opinion re-
lations extracted as the sentiment polarity score be-
tween them. Thus the user-user opinion matrix and
the user-aspect opinion matrix are constructed. If
there is no subjective interaction detected between
two users or between a user and an aspect, the cor-
responding entry in the matrix is left empty. We will
see later that empty entries in the matrices are not
used in the probabilistic matrix factorization step.
5 Probabilistic Matrix Factorization
As we have pointed out earlier, a problem with the
matrices extracted as described in Section 4 is that
the matrices are sparse, i.e. many entries are empty.
For the data set we use, we find that around 87% of
entries in the user-user opinion matrix and around
90% of entries in the user-aspect opinion matrix are
empty. In this section, we describe how we use
Probabilistic Matrix Factorization (PMF) to repre-
sent users and aspects in a latent factor space and
thus generalize the user preferences.
Our model is almost a direct application of proba-
404
bilistic matrix factorization from Salakhutdinov and
Mnih (2008), originally proposed for recommender
systems. The main difference is that the user-user
opinion polarity scores are symmetric. Our model is
also similar to the one used by Ma et al (2008). We
describe our model as follows.
We assume that there are K latent factors with
which both users and aspects can be represented. Let
ui ? RK denote the vector in the latent factor space
for the i-th user, and ak the vector for the k-th aspect.
Recall that the opinions extracted from posts be-
tween users are represented by a user-user opinion
matrix S, and the opinions held by different users on
the various topic aspects are represented by a user-
aspect opinion matrix R. We assume that the polar-
ity scores si,j between the i-th and the j-th users and
ri,k between the i-th user and the k-th aspect in the
two matrices S and R are generated in the following
way:
p(si,j |ui, uj , ?
2
1) = N (si,j |g(u
T
i uj), ?
2
1),
p(ri,k|ui, ak, ?
2
2) = N (ri,k|g(u
T
i ak), ?
2
2),
where ?21 and ?
2
2 are variance parameters, g(?) the
logistic function, and N (?|?, ?2) is the normal dis-
tribution with mean ? and variance ?2.
We can see that with this generative assumption,
if two users are similar in terms of their dot product
in the latent factor space, then they are more likely
to have positive interactions as extracted from their
textual exchanges. Similarly, if a user and an aspect
are similar, then the user is more likely to express a
positive opinion on the aspect in his/her posts. The
latent factors can therefore encode user preferences
and similarity between two users in the latent factor
space reflects whether they share similar viewpoints.
We also place the following prior over ui and ak:
p(ui|?
2
U ) = N (ui|~0, ?
2
UI),
p(ak|?
2
A) = N (ak|~0, ?
2
AI),
where ?2U and ?
2
A are two variance parameters for
users and aspects, respectively, and I is the identify
matrix.
Figure 2 shows the plate notation for the genera-
tive model.
Let U be aK?U matrix containing the vectors ui
for allU users, andA be anK?Amatrix containing
Figure 2: Probabilistic matrix factorization model on
opinion matrices.
the vectors ak for all A aspects. To automatically
learn U andA, we minimize the following objective
function:
L(U ,A,S,R)
=
1
2
U?
i=1
A?
k=1
I(ri,k)(ri,k ? g(uTi ak))2
+
?1
2
U?
i=1
U?
j=1
I(si,j)(si,j ? g(uTi uj))2
+
?U
2
||U||2F +
?A
2
||A||2F , (1)
where ? = ?
2
1
?22
, ?U =
?21
?2U
, and ?A =
?21
?2A
, I(s) is
an indicator function which equals 1 when s is not
empty and otherwise 0.
To optimize the objective function above, we can
perform gradient descent on U and A to find a local
optimum point. The derivation is similar to Ma et al
(2008).
Degenerate Versions of the Model
We refer to the complete model described above
as PMF-UOM (PMF model based on User Opinion
Matrices). PMF-UOM has the following two degen-
erate versions by considering either only the user-
user opinion matrix or only the user-aspect opinion
matrix.
PMF-UU: In this degenerate version of the model,
we use only the user-user opinion matrix to learn the
latent factor representation. Specifically, the objec-
tive function is modified such that we drop the sum
405
of the square errors involving R and the regularizer
on A.
PMF-UA: In this degenerate version of the model,
we use only the user-aspect opinion matrix to learn
the latent factor representation. Specifically, the ob-
jective function is modified such that we drop the
sum of the square errors involving S.
6 Experiments
In this section, we present our experiments that eval-
uate our model.
6.1 Data Set and Experiment Settings
The data set we use comes from Abu-Jbara et al
(2012) and Hassan et al (2012). The data set
contains a set of discussion threads collected from
two political forums (Createdebate3 and Politicalfo-
rum4) and one Wikipedia discussion session. We
randomly select 6 threads from the original data set
to evaluate our model. Some details of the data we
use are listed in Table 2.
ID topic #sides #sentences #users
DS1 Vote for Obama 2 12492 197
DS2 Abortion Banned 6 3844 70
DS3 Profile Muslims 4 2167 69
DS4 England and USA 6 2030 62
DS5 Tax Cuts 2 1193 26
DS6 Political Spectrum 7 1130 50
Table 2: Some statistics of the data sets.
In our experiments, for the PMF-based methods,
we set the number of latent factors to be 10 as we
do not observe big difference when vary the latent
factor size from 10 to 50. For the other parame-
ters, we select the optimal setting for each thread
based on the average of 50 runs. ?U is chosen
from {0.1, 0.01}, ?A from {0.01, 0.001} and ? from
{1, 0.1}.
6.2 Relation Polarity Prediction
The first task we use to evaluate our model is to pre-
dict the polarity of interactions between two users.
Different from Hassan et al (2012), however, we
are not using this task to evaluate the accuracy of
sentiment analysis from text. Our experimental set-
ting is completely different in that we do not make
3www.createdebate.com
4www.politicalforum.com
use of the text exchanges between the two users but
instead use their interactions with other users or as-
pects. The purpose is to test the effectiveness of col-
laborative filtering.
Experimental Setting: The experiments are set up
in the following way. Given a pair of users i and j
who have directly exchanged posts, i.e. si,j is not
empty, we first hide the value of si,j in the matrix S.
Let the altered matrix be S?(i,j). We then use S?(i,j)
instead of S in the learning process as described in
Section 5 to learn the latent factor representation.
Let u?i and u?j denote the learned latent vectors for
user i and user j. We predict the polarity of relation
between i and j as follows:
s?i,j =
{
1 if g(u?Ti u?j) > 0.5,
0 otherwise,
where g(?) is the logistic function to convert the dot
product into a value between 0 and 1.
To judge the quality of the predicted polarity s?i,j ,
we could compare it with si,j . But since si,j itself is
predicted from the textual exchanges between i and
j, it is not the ground truth. Instead, we ask two hu-
man annotators to assign the true polarity label for
user i and user j by reading the textual exchanges
between them and judging whether they are friends
or foes in the discussion thread. The annotators are
asked to assign a score of 0 (indicating a negative
relation), 0.5 (indicating a neutral relation) or 1 (in-
dicating a positive relation). The lowest agreement
score based on Cohen?s kappa coefficient among the
6 threads we use is 0.56, showing fair to good agree-
ment. As ground truth, we set the final polarity score
to 1 if the average score of the two annotators is
larger than 0.5 and 0 otherwise.
We compare the PMF-based methods with two
majority baselines: MBL-0 always predicts negative
relations for all the user pairs (assuming most rela-
tions are negative) and MBL-1 always predicts posi-
tive relations (assuming most relations are positive).
We use MAE (mean absolute error) and RMSE
(root mean square error) as defined below as perfor-
mance metrics:
MAE =
?
i,j |s?i,j ? li,j |
N
,
RMSE =
??
i,j(s?i,j ? li,j)
2
N
,
406
0.2
0.4
0.6
0.8
1.0
DS1 DS2 DS3 DS4 DS5 DS6
MA
E
MB-1
MB-0
PMF-UU
PMF-UA
PMF-UOM
Figure 3: Comparing all the methods in terms of MAE.
0.2
0.4
0.6
0.8
1.0
DS1 DS2 DS3 DS4 DS5 DS6
RM
SE
MB-1
MB-0
PMF-UU
PMF-UA
PMF-UOM
Figure 4: Comparing all the methods in terms of RMSE.
where N is the total number of user pairs we test,
and li,j is the ground truth polarity score between
user i and user j.
Results: We show the results of our model and of
PMF-UU and PMF-UA in terms of MAE in Figure 3
and RMSE in Figure 4. The MAE values range be-
tween 0.31 and 0.44 except for DS5, which has a
higher error rate of 0.53. The results show that even
without knowing the textual exchanges between two
users, from their interactions with other users and/or
with topic aspects, we can still infer the polarity of
their relation with decent accuracy most of the time.
The results also show the comparison between our
model and the competing methods. We can see that
overall the complete model (PMF-UOM) performs
better than the two degenerate models (PMF-UU
and PMF-UA). The differences are statistically sig-
nificant at the 5% level without considering DS5, as
indicated by a 2-tailed paired t-test. Comparing to
the majority baselines, our model significantly out-
performs MBL-1 at 1% significance level while out-
performs MBL-0 on all the data sets except DS5. A
close examinations shows DS5 has very unbalanced
relations (around 83% of relations are negative). Ex-
cept for the unbalanced data set, our model has rea-
sonably good performance.
6.3 Subgroup Detection
The second task we study is the problem of detecting
ideological subgroups from discussion threads. The
original data set has been labeled with the ground
truth for this problem, that is, for each thread the
number of viewpoints is known and the viewpoint
held by each user is labeled. A subgroup is defined
as a set of users holding the same viewpoint.
Experimental Setting: Through this second exper-
iment, we would like to verify the hypothesis that
using the learned latent factor representation U for
users, we can better detect subgroups than directly
using the opinion matrices S and R. For all the
methods we compare, we first construct a feature
vector representation for each user. We then apply
K-means clustering to group users. The number of
clusters is set to be the true number of viewpoints
for each thread. The different methods are described
below:
? PMF-based methods: We simply use the
learned latent vectors u?i after optimizing the
objective function as the feature vectors to rep-
resent each user.
? BL-1: This is our own implementation to sim-
ulate the method by Abu-Jbara et al (2012).
Here each user is represented by a (3 ? (U +
A))-dimensional vector, where U is the num-
ber of users and A is the number of aspects,
i.e. (U +A) is the total number of opinion tar-
gets. For each opinion target, there are 3 di-
mensions in the feature vector, corresponding
to the number of positive, neutral and negative
opinion expressions towards the target from the
online posts.
? BL-2: BL-2 is similar to BL-1 except that we
only use a (U+A)-dimensional vector to repre-
407
sent each user. Here for each opinion target, we
directly use the corresponding sentiment polar-
ity score si,j or ri,j from the matrix S orR. For
empty entries in S andR, we use a score of 0.5.
We use Purity (the higher the better), Entropy (the
lower the better) and Rand Index (the higher the bet-
ter) to evaluate the performance of subgroup detec-
tion (Manning et al, 2008). We further use Accuracy
obtained by choosing the best alignment of clusters
with the ground truth class labels and computing the
percentage of users that are ?classified? correctly.
Results: We first give an overview of the perfor-
mance of all the methods on the task. We show the
average performance of the methods on all the data
sets in Figure 5. Overall, our model has a better per-
formance than all the competing methods.
0.4
0.6
0.8
1.0
Purity Entropy Accuracy RandIndex
BL-1
BL-2
PMF-UU
PMF-UA
PMF-UOM
Figure 5: An overview of the average performance of all
the methods on the 6 threads.
We present all the results in Table 3. We per-
form 2-tailed paired t-test on the results. We find
that PMF-UOM outperforms all the other methods
in terms of RandIndex at 5% significance level and
outperforms other methods in terms of Purity and
Entropy at 10% significance level. Furthermore,
the PMF-UOM model outperforms its degenerative
models PMF-UU and PMF-UA at 10% significance
level in terms of all the measures.
We observe that PMF-UOM achieves the best per-
formance in terms of all the measures for almost
all threads. In particular, comparison with BL-1
and BL-2 shows that collaborative filtering can gen-
eralize the user preferences and help better group
the users based on their viewpoints. The fact that
PMF-UOM outperforms both PMF-UU and PMF-
UA shows that it is important to consider both user-
user interactions and user-aspect interactions.
The Effects of Cluster Size: To test the effect of the
number of clusters on the experiment result, we vary
the number of clusters from 2 to 10 in all methods.
We find that all methods tend to achieve better re-
sults when the number of clusters equals the ground
truth cluster size. Overall, our method PMF-UOM
shows a better performance than the other four meth-
ods when the number of clusters changes, which in-
dicates the robustness of our method.
BL-1 BL-2 PMF-UU PMF-UA PMF-UOM
DS1
P 0.61 0.61 0.61 0.61 0.62
E 0.96 0.96 0.94 0.95 0.94
A 0.59 0.59 0.55 0.57 0.60
R 0.51 0.51 0.50 0.51 0.52
DS2
P 0.53 0.63 0.64 0.61 0.68
E 1.17 1.22 1.14 1.09 0.99
A 0.47 0.53 0.48 0.47 0.50
R 0.50 0.50 0.56 0.56 0.58
DS3
P 0.66 0.68 0.62 0.60 0.68
E 1.05 1.01 1.06 1.07 0.94
A 0.61 0.63 0.48 0.47 0.58
R 0.50 0.52 0.53 0.53 0.57
DS4
P 0.64 0.64 0.66 0.65 0.70
E 0.92 0.94 0.90 0.91 0.85
A 0.59 0.64 0.62 0.62 0.68
R 0.49 0.52 0.52 0.51 0.56
DS5
P 0.86 0.86 0.86 0.86 0.86
E 0.56 0.56 0.49 0.48 0.38
A 0.70 0.70 0.57 0.60 0.71
R 0.52 0.52 0.43 0.45 0.56
DS6
P 0.50 0.50 0.60 0.60 0.68
E 1.35 1.35 1.03 1.04 0.79
A 0.40 0.30 0.53 0.54 0.64
R 0.53 0.53 0.68 0.68 0.74
Table 3: Results on subgroup detection on all the 6
threads. P, E, A and R refer to Purity, Entropy, Accuracy
and RandIndex, respectively.
7 Conclusions
In this paper, we studied how to use probabilistic
matrix factorization, a common technique for col-
laborative filtering, to improve relation mining from
online discussion forums. We first applied senti-
ment analysis to extract user-user opinions and user-
aspect opinions from forum posts. The extracted
opinions form two opinion matrices. We then ap-
plied probabilistic matrix factorization using these
408
two matrices to discover a low-rank latent factor
space which aims to better generalize the users? un-
derlying preferences and indicate user similarities
based on their viewpoints. Using a data set with 6
discussion threads, we showed that the learned la-
tent vectors can be used to predict the polarity of
user relations well without using the users? direct
interaction data, demonstrating the effectiveness of
collaborative filtering. We further found that for the
task of subgroup detection, the latent vectors gave
better performance than using the directly extracted
opinion data, again showing that collaborative fil-
tering through probabilistic matrix factorization can
help address the sparseness problem in the extracted
opinion matrices and help improve relation mining.
Our current work mainly focuses on the user opin-
ion matrices. As future work, we would like to ex-
plore how to incorporate textual contents without
opinionated expressions. One possible way is to
consider the combination of matrix factorization and
topic modeling as studied by Wang and Blei (2011)
where we can use topic modeling to study textual
contents.
Acknowledgments
We thank the reviewers for their valuable comments
on this work.
References
Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and
Dragomir R. Radev. 2012. Subgroup detection in
ideological discussions. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 399?409.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?06, pages 431?439, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Pradeep Dasigi, Weiwei Guo, and Mona T. Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude using
textual latent semantics. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 65?69.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 395?403,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Detecting subgroups in online discussions by
modeling positive and negative relations among par-
ticipants. In Proceedings of the 2012 EMNLP, pages
59?70.
John Hopcroft, Tiancheng Lou, and Jie Tang. 2011. Who
will follow you back?: reciprocal relationship predic-
tion. In Proceedings of the 20th ACM international
conference on Information and knowledge manage-
ment, pages 1137?1146.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the 10th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177.
Noriaki Kawamae. 2011. Predicting future reviews: sen-
timent analysis models for collaborative filtering. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM ?11,
pages 605?614.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010. Predicting positive and negative links in online
social networks. In Proceedings of the 19th interna-
tional conference on World wide web, pages 641?650.
Cane Wing-Ki Leung, Stephen Chi-Fai Chan, Fu-Lai
Chung, and Grace Ngai. 2011. A probabilistic rat-
ing inference framework for mining user preferences
from reviews. World Wide Web, 14(2):187?215.
David Liben-Nowell and Jon Kleinberg. 2003. The link
prediction problem for social networks. In Proceed-
ings of the twelfth international conference on Infor-
mation and knowledge management.
Hao Ma, Haixuan Yang, Michael R. Lyu, and Irwin King.
2008. Sorec: Social recommendation using proba-
bilistic matrix factorization. In Proc. of ACM interna-
tional conference on Information and knowledge man-
agement.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, July.
409
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, Vol. 38, No.
11:39?41.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2, ACL
?09, pages 1003?1011, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Yashar Moshfeghi, Benjamin Piwowarski, and Joe-
mon M. Jose. 2011. Handling data sparsity in collabo-
rative filtering using emotion and semantic based fea-
tures. In Proceedings of the 34th international ACM
SIGIR conference on Research and development in In-
formation Retrieval, SIGIR ?11, pages 625?634.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI?09,
pages 1199?1204, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Ruslan Salakhutdinov and Andriy Mnih. 2008. Prob-
abilistic matrix factorization. In Advances in Neural
Information Processing Systems, volume 20.
Chong Wang and David M. Blei. 2011. Collaborative
topic modeling for recommending scientific articles.
In Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 448?456.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3
- Volume 3, EMNLP ?09, pages 1533?1541, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
410
