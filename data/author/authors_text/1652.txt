Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 145?153,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic training of lemmatization rules that handle morphological 
changes in pre-, in- and suffixes alike 
 
 
Bart Jongejan 
CST-University of Copenhagen 
Njalsgade 140-142 2300 K?benhavn S 
Denmark 
bartj@hum.ku.dk 
Hercules Dalianis? ? 
?DSV, KTH - Stockholm University  
Forum 100, 164 40 Kista, Sweden  
?Euroling AB, SiteSeeker 
Igeldammsgatan 22c  
112 49 Stockholm, Sweden  
hercules@dsv.su.se 
 
  
 
Abstract 
We propose a method to automatically train 
lemmatization rules that handle prefix, infix 
and suffix changes to generate the lemma from 
the full form of a word. We explain how the 
lemmatization rules are created and how the 
lemmatizer works. We trained this lemmatizer 
on Danish, Dutch, English, German, Greek, 
Icelandic, Norwegian, Polish, Slovene and 
Swedish full form-lemma pairs respectively. 
We obtained significant improvements of 24 
percent for Polish, 2.3 percent for Dutch, 1.5 
percent for English, 1.2 percent for German 
and 1.0 percent for Swedish compared to plain 
suffix lemmatization using a suffix-only lem-
matizer. Icelandic deteriorated with 1.9 per-
cent. We also made an observation regarding 
the number of produced lemmatization rules as 
a function of the number of training pairs. 
1 Introduction 
Lemmatizers and stemmers are valuable human 
language technology tools to improve precision 
and recall in an information retrieval setting. For 
example, stemming and lemmatization make it 
possible to match a query in one morphological 
form with a word in a document in another mor-
phological form. Lemmatizers can also be used 
in lexicography to find new words in text mate-
rial, including the words? frequency of use. Other 
applications are creation of index lists for book 
indexes as well as key word lists 
Lemmatization is the process of reducing a 
word to its base form, normally the dictionary 
look-up form (lemma) of the word. A trivial way 
to do this is by dictionary look-up.  More ad-
vanced systems use hand crafted or automatically 
generated transformation rules that look at the 
surface form of the word and attempt to produce 
the correct base form by replacing all or parts of 
the word. 
Stemming conflates a word to its stem. A stem 
does not have to be the lemma of the word, but 
can be any trait that is shared between a group of 
words, so that even the group membership itself 
can be regarded as the group?s stem.  
The most famous stemmer is the Porter Stem-
mer for English (Porter 1980). This stemmer re-
moves around 60 different suffixes, using rewrit-
ing rules in two steps. 
The paper is structured as follows: section 2 
discusses related work, section 3 explains what 
the new algorithm is supposed to do, section 4 
describes some details of the new algorithm, sec-
tion 5 evaluates the results, conclusions are 
drawn in section 6, and finally in section 7 we 
mention plans for further tests and improve-
ments. 
2 Related work  
There have been some attempts in creating 
stemmers or lemmatizers automatically. Ek-
mek?ioglu et al (1996) have used N-gram 
matching for Turkish that gave slightly better 
results than regular rule based stemming. Theron 
and Cloete (1997) learned two-level rules for 
English, Xhosa and Afrikaans, but only single 
character insertions, replacements and additions 
were allowed. Oard et al (2001) used a language 
independent stemming technique in a dictionary 
based cross language information retrieval ex-
periment for German, French and Italian where 
English was the search language. A four stage 
backoff strategy for improving recall was intro-
145
duced. The system worked fine for French but 
not so well for Italian and German. Majumder et 
al. (2007) describe a statistical stemmer, YASS 
(Yet Another Suffix Stripper), mainly for Ben-
gali and French, but they propose it also for 
Hindi and Gujarati. The method finds clusters of 
similar words in a corpus. The clusters are called 
stems. The method works best for languages that 
are basically suffix based. For Bengali precision 
was 39.3 percent better than without stemming, 
though no absolute numbers were reported for 
precision. The system was trained on a corpus 
containing 301 562 words.  
Kanis & M?ller (2005) used an automatic 
technique called OOV Words Lemmatization to 
train their lemmatizer on Czech, Finnish and 
English data. Their algorithm uses two pattern 
tables to handle suffixes as well as prefixes. Plis-
son et al (2004) presented results for a system 
using Ripple Down Rules (RDR) to generate 
lemmatization rules for Slovene, achieving up to 
77 percent accuracy. Matja? et al (2007) present 
an RDR system producing efficient suffix based 
lemmatizers for 14 languages, three of which 
(English, German and Slovene) our algorithm 
also has been tested with. 
Stempel (Bia?ecki 2004) is a stemmer for Pol-
ish that is trained on Polish full form ? lemma 
pairs. When tested with inflected out-of-
vocabulary (OOV) words Stempel produces 95.4 
percent correct stems, of which about 81 percent  
also happen to be correct lemmas.  
Hedlund (2001) used two different approaches 
to automatically find stemming rules from a cor-
pus, for both Swedish and English. Unfortunately 
neither of these approaches did beat the hand 
crafted rules in the Porter stemmer for English 
(Porter 1980) or the Euroling SiteSeeker stem-
mer for Swedish, (Carlberger et al 2001).  
Jongejan & Haltrup (2005) constructed a 
trainable lemmatizer for the lexicographical task 
of finding lemmas outside the existing diction-
ary, bootstrapping from a training set of full form 
? lemma pairs extracted from the existing dic-
tionary. This lemmatizer looks only at the suffix 
part of the word. Its performance was compared 
with a stemmer using hand crafted stemming 
rules, the Euroling SiteSeeker stemmer for 
Swedish, Danish and Norwegian, and also with a 
stemmer for Greek, (Dalianis & Jongejan 2006). 
The results showed that lemmatizer was as good 
as the stemmer for Swedish, slightly better for 
Danish and Norwegian but worse for Greek. 
These results are very dependent on the quality 
(errors, size) and complexity (diacritics, capitals) 
of the training data. 
In the current work we have used Jongejan & 
Haltrup?s lemmatizer as a reference, referring to 
it as the ?suffix lemmatizer?. 
3 Delineation 
3.1 Why affix rules? 
German and Dutch need more advanced methods 
than suffix replacement since their affixing of 
words (inflection of words) can include both pre-
fixing, infixing and suffixing. Therefore we cre-
ated a trainable lemmatizer that handles pre- and 
infixes in addition to suffixes. 
Here is an example to get a quick idea of what 
we wanted to achieve with the new training algo-
rithm. Suppose we have the following Dutch full 
form ? lemma pair: 
afgevraagd ? afvragen 
(Translation: wondered, to wonder) 
If this were the sole input given to the training 
program, it should produce a transformation rule 
like this: 
*ge*a*d ? ***en 
The asterisks are wildcards and placeholders. 
The pattern on the left hand side contains three 
wildcards, each one corresponding to one place-
holder in the replacement string on the right hand 
side, in the same order. The characters matched 
by a wildcard are inserted in the place kept free 
by the corresponding placeholder in the replace-
ment expression. 
With this ?set? of rules a lemmatizer would be 
able to construct the correct lemma for some 
words that had not been used during the training, 
such as the word verstekgezaagd (Transla-
tion: mitre cut): 
 
Word verstek ge z a ag d 
Pattern * ge * a * d 
Replacement *  *  * en
Lemma verstek  z  ag en
 
Table 1. Application of a rule to an OOV word. 
 
 
For most words, however, the lemmatizer would 
simply fail to produce any output, because not all 
words do contain the literal strings ge and a and 
a final d.  We remedy this by adding a one-size-
fits-all rule that says ?return the input as output?: 
 
* ? * 
146
So now our rule set consists of two rules: 
*ge*a*d ? ***en 
* ? * 
The lemmatizer then finds the rule with the most 
specific pattern (see 4.2) that matches and ap-
plies only this rule. The last rule?s pattern 
matches any word and so the lemmatizer cannot 
fail to produce output. Thus, in our toy rule set 
consisting of two rules, the first rule handles 
words like gevraagd, afgezaagd, 
geklaagd, (all three correctly) and getalmd 
(incorrectly) while the second rule handles words 
like directeur (correctly) and zei (incor-
rectly). 
3.2 Inflected vs. agglutinated languages 
A lemmatizer that only applies one rule per word 
is useful for inflected languages, a class of lan-
guages that includes all Indo-European lan-
guages. For these languages morphological 
change is not a productive process, which means 
that no word can be morphologically changed in 
an unlimited number of ways. Ideally, there are 
only a finite number of inflection schemes and 
thus a finite number of lemmatization rules 
should suffice to lemmatize indefinitely many 
words.  
In agglutinated languages, on the other hand, 
there are classes of words that in principle have 
innumerous word forms. One way to lemmatize 
such words is to peel off all agglutinated mor-
phemes one by one. This is an iterative process 
and therefore the lemmatizer discussed in this 
paper, which applies only one rule per word, is 
not an obvious choice for agglutinated lan-
guages. 
3.3 Supervised training 
An automatic process to create lemmatization 
rules is described in the following sections. By 
reserving a small part of the available training 
data for testing it is possible to quite accurately 
estimate the probability that the lemmatizer 
would produce the right lemma given any un-
known word belonging to the language, even 
without requiring that the user masters the lan-
guage (Kohavi 1995). 
On the downside, letting a program construct 
lemmatization rules requires an extended list of 
full form ? lemma pairs that the program can 
exercise on ? at least tens of thousands and pos-
sibly over a million entries (Dalianis and Jonge-
jan 2006). 
3.4 Criteria for success 
The main challenge for the training algorithm is 
that it must produce rules that accurately lemma-
tize OOV words. This requirement translates to 
two opposing tendencies during training. On the 
one hand we must trust rules with a wide basis of 
training examples more than rules with a small 
basis, which favours rules with patterns that fit 
many words. On the other hand we have the in-
compatible preference for cautious rules with 
rather specific patterns, because these must be 
better at avoiding erroneous rule applications 
than rules with generous patterns. The envisaged 
expressiveness of the lemmatization rules ? al-
lowing all kinds of affixes and an unlimited 
number of wildcards ? turns the challenge into a 
difficult balancing act. 
In the current work we wanted to get an idea 
of the advantages of an affix-based algorithm 
compared to a suffix-only based algorithm. 
Therefore we have made the task as hard as pos-
sible by not allowing language specific adapta-
tions to the algorithms and by not subdividing 
the training words in word classes.  
4 Generation of rules and look-up data 
structure  
4.1 Building a rule set from training pairs 
The training algorithm generates a data structure 
consisting of rules that a lemmatizer must trav-
erse to arrive at a rule that is elected to fire.  
Conceptually the training process is as fol-
lows. As the data structure is being built, the full 
form in each training pair is tentatively lemma-
tized using the data structure that has been cre-
ated up to that stage. If the elected rule produces 
the right lemma from the full form, nothing 
needs to be done. Otherwise, the data structure 
must be expanded with a rule such that the new 
rule a) is elected instead of the erroneous rule 
and b) produces the right lemma from the full 
form. The training process terminates when the 
full forms in all pairs in the training set are trans-
formed to their corresponding lemmas.  
After training, the data structure of rules is 
made permanent and can be consulted by a lem-
matizer. The lemmatizer must elect and fire rules 
in the same way as the training algorithm, so that 
all words from the training set are lemmatized 
correctly. It may however fail to produce the cor-
rect lemmas for words that were not in the train-
ing set ? the OOV words. 
147
4.2 Internal structure of rules: prime and 
derived rules 
During training the Ratcliff/Obershelp algorithm 
(Ratcliff & Metzener 1988) is used to find the 
longest non-overlapping similar parts in a given 
full form ? lemma pair. For example, in the pair 
afgevraagd ? afvragen 
the longest common substring is vra, followed 
by af and g. These similar parts are replaced 
with wildcards and placeholders: 
*ge*a*d ? ***en 
Now we have the prime rule for the training pair, 
the least specific rule necessary to lemmatize the 
word correctly. Rules with more specific patterns 
? derived rules ? can be created by adding char-
acters and by removing or adding wildcards. A 
rule that is derived from another rule (derived or 
prime) is more specific than the original rule: 
Any word that is successfully matched by the 
pattern of a derived rule is also successfully 
matched by the pattern of the original rule, but 
the converse is not the case. This establishes a 
partial ordering of all rules. See Figures 1 and 2, 
where the rules marked ?p? are prime rules and 
those marked ?d? are derived. 
Innumerous rules can be derived from a rule 
with at least one wildcard in its pattern, but only 
a limited number can be tested in a finite time. 
To keep the number of candidate rules within 
practical limits, we used the strategy that the pat-
tern of a candidate is minimally different from its 
parent?s pattern: it can have one extra literal 
character or one wildcard less or replace one 
wildcard with one literal character. Alternatively, 
a candidate rule (such as the bottom rule in Fig-
ure 4) can arise by merging two rules. Within 
these constraints, the algorithm creates all possi-
ble candidate rules that transform one or more 
training words to their corresponding lemmas. 
4.3 External structure of rules: partial or-
dering in a DAG and in a tree 
We tried two different data structures to store 
new lemmatizer rules, a directed acyclic graph 
(DAG) and a plain tree structure with depth first, 
left to right traversal. 
The DAG (Figure 1) expresses the complete 
partial ordering of the rules. There is no prefer-
ential order between the children of a rule and all 
paths away from the root must be regarded as 
equally valid. Therefore the DAG may lead to 
several lemmas for the same input word. For ex-
ample, without the rule in the bottom part of Fig-
ure 1, the word gelopen would have been lem-
matized to both lopen (correct) and gelopen 
(incorrect): 
gelopen: 
*ge* ? **   lopen 
*pen ? *pen  gelopen 
By adding a derived rule as a descendent of both 
these two rules, we make sure that lemmatization 
of the word gelopen is only handled by one 
rule and only results in the correct lemma: 
gelopen: 
*ge*pen ? **pen  lopen 
 
 
Figure 1. Five training pairs as supporters for 
five rules in a DAG. 
 
 
The tree in Figure 2 is a simpler data structure 
and introduces a left to right preferential order 
between the children of a rule. Only one rule 
fires and only one lemma per word is produced. 
For example, because the rule *ge* ? ** pre-
cedes its sibling rule *en ? *, whenever the 
former rule is applicable, the latter rule and its 
descendents are not even visited, irrespective of 
their applicability. In our example, the former 
rule ? and only the former rule ? handles the 
lemmatization of gelopen, and since it pro-
duces the correct lemma an additional rule is not 
necessary.  
In contrast to the DAG, the tree implements 
negation: if the Nth sibling of a row of children 
fires, it not only means that the pattern of the Nth 
rule matches the word, it also means that the pat-
terns of the N-1 preceding siblings do not match 
the word. Such implicit negation is not possible 
in the DAG, and this is probably the main reason 
why the experiments with the DAG-structure 
lead to huge numbers of rules, very little gener-
* ? * 
ui ? ui 
*ge* ? ** 
overgegaan ? overgaan 
*en ? * 
uien? ui 
*pen ?*pen 
lopen ? lopen 
*ge*pen ? **pen 
gelopen ? lopen 
p 
p p 
d 
d 
148
alization, uncontrollable training times (months, 
not minutes!) and very low lemmatization qual-
ity. On the other hand, the experiments with the 
tree structure were very successful. The building 
time of the rules is acceptable, taking small re-
cursive steps during the training part. The mem-
ory use is tractable and the quality of the results 
is good provided good training material. 
  
 
Figure 2. The same five training pairs as sup-
porters for only four rules in a tree. 
 
4.4 Rule selection criteria 
This section pertains to the training algorithm 
employing a tree. 
The typical situation during training is that a 
rule that already has been added to the tree 
makes lemmatization errors on some of the train-
ing words. In that case one or more corrective 
children have to be added to the rule1.  
If the pattern of a new child rule only matches 
some, but not all training words that are lemma-
tized incorrectly by the parent, a right sibling 
rule must be added. This is repeated until all 
training words that the parent does not lemmatize 
correctly are matched by the leftmost child rule 
or one of its siblings. 
A candidate child rule is faced with training 
words that the parent did not lemmatize correctly 
and, surprisingly, also supporters of the parent, 
because the pattern of the candidate cannot dis-
criminate between these two groups. 
On the output side of the candidate appear the 
training pairs that are lemmatized correctly by 
the candidate, those that are lemmatized incor-
                                                 
1 If the case of a DAG, care must be taken that the 
complete representation of the partial ordering of 
rules is maintained. Any new rule not only becomes a 
child of the rule that it was aimed at as a corrective 
child, but often also of several other rules. 
rectly and those that do not match the pattern of 
the candidate.  
For each candidate rule the training algorithm 
creates a 2?3 table (see Table 2) that counts the 
number of training pairs that the candidate lem-
matizes correctly or incorrectly or that the candi-
date does not match. The two columns count the 
training pairs that, respectively, were lemmatized 
incorrectly and correctly by the parent. These six 
parameters Nxy can be used to select the best can-
didate. Only four parameters are independent, 
because the numbers of training words that the 
parent lemmatized incorrectly (Nw) and correctly 
(Nr) are the same for all candidates. Thus, after 
the application of the first and most significant 
selection criterion, up to three more selection 
criteria of decreasing significance can be applied 
if the preceding selection ends in a tie. 
 
           Parent 
Child 
Incorrect Correct 
(supporters) 
Correct  Nwr Nrr 
Incorrect  Nww Nrw 
Not matched Nwn Nrn 
Sum Nw Nr 
 
Table 2. The six parameters for rule selection 
among candidate rules. 
 
A large Nwr and a small Nrw are desirable. Nwr is a 
measure for the rate at which the updated data 
structure has learned to correctly lemmatize 
those words that previously were lemmatized 
incorrectly. A small Nrw indicates that only few 
words that previously were lemmatized correctly 
are spoiled by the addition of the new rule. It is 
less obvious how the other numbers weigh in.  
We have obtained the most success with crite-
ria that first select for highest Nwr + Nrr - Nrw . If 
the competition ends in a tie, we select for lowest 
Nrr among the remaining candidates. If the com-
petition again ends in a tie, we select for highest 
Nrn ? Nww . Due to the marginal effect of a fourth 
criterion we let the algorithm randomly select 
one of the remaining candidates instead. 
The training pairs that are matched by the pat-
tern of the winning rule become the supporters 
and non-supporters of that new rule and are no 
longer supporters or non-supporters of the par-
ent. If the parent still has at least one non-
supporter, the remaining supporters and non-
supporters ? the training pairs that the winning 
* ? * 
ui ? ui 
*ge* ? ** 
overgegaan ? overgaan 
gelopen ? lopen 
*en ? * 
uien? ui 
*pen ?*pen 
lopen ? lopen 
p 
p p 
d 
149
candidate does not match ? are used to select the 
right sibling of the new rule. 
5 Evaluation 
We trained the new lemmatizer using training 
material for Danish (STO), Dutch (CELEX), 
English (CELEX), German (CELEX), Greek 
(Petasis et al 2003), Icelandic (IFD), Norwegian 
(SCARRIE), Polish (Morfologik), Slovene 
(Jur?i? et al 2007) and Swedish (SUC).  
The guidelines for the construction of the 
training material are not always known to us. In 
some cases, we know that the full forms have 
been generated automatically from the lemmas. 
On the other hand, we know that the Icelandic 
data is derived from a corpus and only contains 
word forms occurring in that corpus. Because of 
the uncertainties, the results cannot be used for a 
quantitative comparison of the accuracy of lem-
matization between languages. 
Some of the resources were already disam-
biguated (one lemma per full form) when we re-
ceived the data. We decided to disambiguate the 
remaining resources as well. Handling homo-
graphs wisely is important in many lemmatiza-
tion tasks, but there are many pitfalls. As we 
only wanted to investigate the improvement of 
the affix algorithm over the suffix algorithm, we 
decided to factor out ambiguity. We simply 
chose the lemma that comes first alphabetically 
and discarded the other lemmas from the avail-
able data. 
The evaluation was carried out by dividing the 
available material in training data and test data in 
seven different ratios, setting aside between 
1.54% and 98.56% as training data and the re-
mainder as OOV test data. (See section 7). To 
keep the sample standard deviation s for the ac-
curacy below an acceptable level we used the 
evaluation method repeated random subsampling 
validation that is proposed in Voorhees (2000) 
and Bouckaert & Frank (2000). We repeated the 
training and evaluation for each ratio with sev-
eral randomly chosen sets, up to 17 times for the 
smallest and largest ratios, because these ratios 
lead to relatively small training sets and test sets 
respectively. The same procedure was followed 
for the suffix lemmatizer, using the same training 
and test sets. Table 3 shows the results for the 
largest training sets. 
For some languages lemmatization accuracy 
for OOV words improved by deleting rules that 
are based on very few examples from the training 
data. This pruning was done after the training of 
the rule set was completed. Regarding the affix 
algorithm, the results for half of the languages 
became better with mild pruning, i.e. deleting 
rules with only one example. For Danish, Dutch, 
German, Greek and Icelandic pruning did not 
improve accuracy. Regarding the suffix algo-
rithm, only English and Swedish profited from 
pruning. 
 
Language 
Suffix  
% 
Affix 
% ? %  
N ? 
1000 n 
Icelandic 73.2?1.4 71.3?1.5 -1.9 58 17
Danish 93.2?0.4 92.8?0.2 -0.4 553 5
Norwegian 87.8?0.4 87.6?0.3 -0.2 479 6
Greek 90.2?0.3 90.4?0.4 0.2 549 5
Slovene 86.0?0.6 86.7?0.3 0.7 199 9
Swedish 91.24?0.18 92.3?0.3 1.0 478 6
German 90.3?0.5 91.46?0.17 1.2 315 7
English 87.5?0.9 89.0?1.3 1.5 76 15
Dutch 88.2?0.5 90.4?0.5 2.3 302 7
Polish 69.69?0.06 93.88?0.08 24.2 3443 2
 
Table 3. Accuracy for the suffix and affix algo-
rithms. The fifth column shows the size of the 
available data. Of these, 98.56% was used for 
training and 1.44% for testing. The last column 
shows the number n of performed iterations, 
which was inversely proportional to ?N with a 
minimum of two. 
6 Some language specific notes 
For Polish, the suffix algorithm suffers from 
overtraining. The accuracy tops at about 100 000 
rules, which is reached when the training set 
comprises about 1 000 000 pairs.  
 
  
Figure 3. Accuracy vs. number of rules for Polish 
Upper swarm of data points: affix algorithm. 
Lower swarm of data points: suffix algorithm. 
Each swarm combines results from six rule sets 
with varying amounts of pruning (no pruning and 
pruning with cut-off = 1..5). 
 
If more training pairs are added, the number of 
rules grows, but the accuracy falls. The affix al-
gorithm shows no sign of overtraining, even 
150
though the Polish material comprised 3.4 million 
training pairs, more than six times the number of 
the second language on the list, Danish. See Fig-
ure 3. 
The improvement of the accuracy for Polish 
was tremendous. The inflectional paradigm in 
Polish (as in other Slavic languages) can be left 
factorized, except for the superlative. However, 
only 3.8% of the words in the used Polish data 
have the superlative forming prefix naj, and 
moreover this prefix is only removed from ad-
verbs and not from the much more numerous 
adjectives.  
The true culprit of the discrepancy is the great 
number (> 23%) of words in the Polish data that 
have the negative prefix nie, which very often 
does not recur in the lemma. The suffix algo-
rithm cannot handle these 23% correctly. 
The improvement over the suffix lemmatizer 
for the case of German is unassuming. To find 
out why, we looked at how often rules with infix 
or prefix patterns fire and how well they are do-
ing. We trained the suffix algorithm with 9/10 of 
the available data and tested with the remaining 
1/10, about 30 000 words. Of these, 88% were 
lemmatized correctly (a number that indicates the 
smaller training set than in Table 3). 
  
 German Dutch 
Acc. 
% Freq % Acc. % Freq % 
all 88.1  100.0 87.7 100.0
suffix-
only 88.7 94.0 88.1 94.9
prefix 79.9 4.4 80.9 2.4
infix 83.3 2.3 77.4 3.0
? ? ?  92.8 0.26 N/A 0.0
ge infix 68.6 0.94 77.9 2.6
 
Table 4. Prevalence of suffix-only rules, rules 
specifying a prefix, rules specifying an infix and 
rules specifying infixes containing either ?, ? or 
? or the letter combination ge. 
 
Almost 94% of the lemmas were created using 
suffix-only rules, with an accuracy of almost 
89%. Less than 3% of the lemmas were created 
using rules that included at least one infix sub-
pattern. Of these, about 83% were correctly 
lemmatized, pulling the average down. We also 
looked at two particular groups of infix-rules: 
those including the letters ?, ? or ? and those 
with the letter combination ge. The former 
group applies to many words that display umlaut, 
while the latter applies to past participles. The 
first group of rules, accounting for 11% of all 
words handled by infix rules, performed better 
than average, about 93%, while the latter group, 
accounting for 40% of all words handled by infix 
rules, performed poorly at 69% correct lemmas. 
Table 4 summarizes the results for German and 
the closely related Dutch language. 
7 Self-organized criticality 
Over the whole range of training set sizes the 
number of rules goes like dNC.  with C<0 , and N 
the number of training pairs. The value of C and 
d not only depended on the chosen algorithm, but 
also on the language. Figure 4 shows how the 
number of generated lemmatization rules for Pol-
ish grows as a function of the number of training 
pairs.  
  
Figure 4.  Number of rules vs. number of training 
pairs for Polish (double logarithmic scale). 
Upper row: unpruned rule sets 
Lower row: heavily pruned rule sets (cut-off=5) 
 
There are two rows of data, each row containing 
seven data points. The rules are counted after 
training with 1.54 percent of the available data 
and then repeatedly doubling to 3.08, 6.16, 
12.32, 24.64, 49.28 and 98.56 percent of the 
available data. The data points in the upper row 
designate the number of rules resulting from the 
training process. The data points in the lower 
row arise by pruning rules that are based on less 
than six examples from the training set. 
The power law for the upper row of data points 
for Polish in Figure 4 is 
87.080.0 trainingrules NN =
 
151
As a comparison, for Icelandic the power law for 
the unpruned set of rules is 
90.032.1 trainingrules NN =
 
These power law expressions are derived for the 
affix algorithm. For the suffix algorithm the ex-
ponent in the Polish power law expression is 
very close to 1 (0.98), which indicates that the 
suffix lemmatizer is not good at all at generaliz-
ing over the Polish training data: the number of 
rules grows almost proportionally with the num-
ber of training words. (And, as Figure 3 shows, 
to no avail.) On the other hand, the suffix lem-
matizer fares better than the affix algorithm for 
Icelandic data, because in that case the exponent 
in the power law expression is lower: 0.88 versus 
0.90.  
The power law is explained by self-organized 
criticality (Bak et al 1987, 1988). Rule sets that 
originate from training sets that only differ in a 
single training example can be dissimilar to any 
degree depending on whether and where the dif-
ference is tipping the balance between competing 
rule candidates. Whether one or the other rule 
candidate wins has a very significant effect on 
the parts of the tree that emanate as children or as 
siblings from the winning node. If the difference 
has an effect close to the root of the tree, a large 
expanse of the tree is affected. If the difference 
plays a role closer to a leaf node, only a small 
patch of the tree is affected. The effect of adding 
a single training example can be compared with 
dropping a single rice corn on top of a pile of 
rice, which can create an avalanche of unpredict-
able size. 
8 Conclusions 
Affix rules perform better than suffix rules if the 
language has a heavy pre- and infix morphology 
and the size of the training data is big. The new 
algorithm worked very well with the Polish Mor-
fologik dataset and compares well with the 
Stempel algorithm (Bia?ecki 2008).  
Regarding Dutch and German we have ob-
served that the affix algorithm most often applies 
suffix-only rules to OOV words. We have also 
observed that words lemmatized this way are 
lemmatized better than average. The remaining 
words often need morphological changes in more 
than one position, for example both in an infix 
and a suffix. Although these changes are corre-
lated by the inflectional rules of the language, the 
number of combinations is still large, while at 
the same time the number of training examples 
exhibiting such combinations is relatively small. 
Therefore the more complex rules involving infix 
or prefix subpatterns or combinations thereof are 
less well-founded than the simple suffix-only 
rules. The lemmatization accuracy of the com-
plex rules will therefore in general be lower than 
that of the suffix-only rules. The reason why the 
affix algorithm is still better than the algorithm 
that only considers suffix rules is that the affix 
algorithm only generates suffix-only rules from 
words with suffix-only morphology. The suffix-
only algorithm is not able to generalize over 
training examples that do not fulfil this condition 
and generates many rules based on very few ex-
amples. Consequently, everything else being 
equal, the set of suffix-only rules generated by 
the affix algorithm must be of higher quality than 
the set of rules generated by the suffix algorithm. 
The new affix algorithm has fewer rules sup-
ported by only one example from the training 
data than the suffix algorithm. This means that 
the new algorithm is good at generalizing over 
small groups of words with exceptional mor-
phology. On the other hand, the bulk of ?normal? 
training words must be bigger for the new affix 
based lemmatizer than for the suffix lemmatizer. 
This is because the new algorithm generates im-
mense numbers of candidate rules with only 
marginal differences in accuracy, requiring many 
examples to find the best candidate. 
When we began experimenting with lemmati-
zation rules with unrestricted numbers of affixes, 
we could not know whether the limited amount 
of available training data would be sufficient to 
fix the enormous amount of free variables with 
enough certainty to obtain higher quality results 
than obtainable with automatically trained lem-
matizers allowing only suffix transformations. 
However, the results that we have obtained 
with the new affix algorithm are on a par with or 
better than those of the suffix lemmatizer. There 
is still room for improvements as only part of the 
parameter space of the new algorithm has been 
searched. The case of Polish shows the superior-
ity of the new algorithm, whereas the poor re-
sults for Icelandic, a suffix inflecting language 
with many inflection types, were foreseeable, 
because we only had a small training set. 
9 Future work  
Work with the new affix lemmatizer has until 
now focused on the algorithm. To really know if 
the carried out theoretical work is valuable we 
would like to try it out in a real search setting in 
a search engine and see if the users appreciate 
the new algorithm?s results. 
152
References  
Per Bak, Chao Tang and Kurt Wiesenfeld. 1987. Self-
Organized Criticality: An Explanation of 1/f Noise,  
Phys. Rev. Lett., vol. 59,. pp. 381-384, 1987 
Per Bak, Chao Tang and Kurt Wiesenfeld . 1988. 
Phys. Rev. A38, (1988), pp. 364-374 
Andrzej Bia?ecki, 2004, Stempel - Algorithmic 
Stemmer for Polish Language 
http://www.getopt.org/stempel/ 
Remco R. Bouckaert and Eibe Frank. 2000. Evaluat-
ing the Replicability of Significance Tests for 
Comparing Learning Algorithms. In H. Dai, R. 
Srikant, & C. Zhang (Eds.), Proc. 8th Pacific-Asia 
Conference, PAKDD 2004, Sydney, Australia, 
May 26-28, 2004 (pp. 3-12). Berlin: Springer. 
Johan Carlberger, Hercules Dalianis, Martin Hassel, 
and Ola Knutsson. 2001. Improving Precision in 
Information Retrieval for Swedish using Stem-
ming. In the Proceedings of NoDaLiDa-01 - 13th 
Nordic Conference on Computational Linguistics, 
May 21-22, Uppsala, Sweden. 
Celex: http://celex.mpi.nl/ 
Hercules Dalianis and Bart Jongejan 2006. Hand-
crafted versus Machine-learned Inflectional Rules: 
the Euroling-SiteSeeker Stemmer and CST's Lem-
matiser, in Proceedings of the International Con-
ference on Language Resources and Evaluation, 
LREC 2006. 
F. ?una Ekmek?ioglu, Mikael F. Lynch, and Peter 
Willett. 1996. Stemming and N-gram matching for 
term conflation in Turkish texts. Information Re-
search, 7(1) pp 2-6. 
Niklas Hedlund 2001. Automatic construction of 
stemming rules, Master Thesis, NADA-KTH, 
Stockholm, TRITA-NA-E0194. 
IFD: Icelandic Centre for Language Technology, 
http://tungutaekni.is/researchsystems/rannsoknir_1
2en.html 
Bart Jongejan and Dorte Haltrup. 2005. The CST 
Lemmatiser. Center for Sprogteknologi, University 
of Copenhagen version 2.7 (August, 23 2005) 
http://cst.dk/online/lemmatiser/cstlemma.pdf 
Jakub Kanis  and  Ludek M?ller. 2005. Automatic 
Lemmatizer Construction with Focus on OOV 
Words Lemmatization in Text, Speech and Dia-
logue, Lecture Notes in Computer Science, Berlin / 
Heidelberg, pp 132-139 
Ron Kohavi. 1995. A study of cross-validation and 
bootstrap for accuracy estimation and model selec-
tion. Proceedings of the Fourteenth International 
Joint Conference on Artificial Intelligence 2 (12): 
1137?1143, Morgan Kaufmann, San Mateo. 
 
Prasenjit Majumder, Mandar Mitra, Swapan K. Parui, 
Gobinda Kole, Pabitra Mitra, and Kalyankumar 
Datta. 2007. YASS: Yet another suffix stripper. 
ACM Transactions on Information Systems , Vol-
ume 25 ,  Issue 4, October 2007.  
Jur?i? Matja?, Igor Mozeti?, and Nada Lavra?. 2007. 
Learning ripple down rules for efficient lemmatiza-
tion In proceeding of the Conference on Data Min-
ing and Data Warehouses (SiKDD 2007), October 
12, 2007, Ljubljana, Slovenia 
Morfologik: Polish morphological analyzer 
http://mac.softpedia.com/get/Word-
Processing/Morfologik.shtml 
Douglas W. Oard, Gina-Anne Levow, and Clara I. 
Cabezas. 2001. CLEF experiments at Maryland: 
Statistical stemming and backoff translation. In 
Cross-language information retrieval and evalua-
tion: Proceeding of the Clef 2000 workshops Carol 
Peters Ed. Springer Verlag pp. 176-187. 2001. 
Georgios Petasis, Vangelis Karkaletsis , Dimitra Far-
makiotou , Ion Androutsopoulos  and Constantine 
D. Spyropoulo. 2003. A Greek Morphological 
Lexicon and its Exploitation by Natural Language 
Processing Applications. In Lecture Notes on 
Computer Science (LNCS), vol.2563, "Advances 
in Informatics - Post-proceedings of the 8th Pan-
hellenic Conference in Informatics", Springer Ver-
lag. 
Jo?l Plisson, Nada Lavra?, and Dunja Mladenic. 2004, 
A rule based approach to word lemmatization, 
Proceedings of the 7th International Multi-
conference Information Society, IS-2004, Institut 
Jozef Stefan, Ljubljana, pp.83-6. 
Martin F. Porter 1980. An algorithm for suffix strip-
ping. Program, vol 14, no 3, pp 130-130. 
John W. Ratcliff and David Metzener, 1988. Pattern 
Matching: The Gestalt Approach, Dr. Dobb's 
Journal, page 46, July 1988. 
SCARRIE 2009. Scandinavian Proofreading Tools 
http://ling.uib.no/~desmedt/scarrie/ 
STO: http://cst.ku.dk/sto_ordbase/ 
SUC 2009. Stockholm Ume? corpus, 
http://www.ling.su.se/staff/sofia/suc/suc.html 
Pieter Theron and Ian Cloete 1997 Automatic acquisi-
tion of two-level morphological rules, Proceedings 
of the fifth conference on Applied natural language 
processing, p.103-110, March 31-April 03, 1997, 
Washington, DC. 
Ellen M. Voorhees. 2000. Variations in relevance 
judgments and the measurement of retrieval effec-
tiveness, J. of Information Processing and Man-
agement 36 (2000) pp 697-716 
153
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 10?16
Manchester, August 2008
Abstract 
Hall? Norden is a web site with information 
regarding mobility between the Nordic coun-
tries in five different languages; Swedish, 
Danish, Norwegian, Icelandic and Finnish.  
We wanted to create a Nordic cross-language 
dictionary for the use in a cross-language 
search engine for Hall? Norden. The entire 
set of texts on the web site was treated as one 
multilingual parallel corpus. From this we 
extracted parallel corpora for each language 
pair. The corpora were very sparse, contain-
ing on average less than 80 000 words per 
language pair. We have used the Uplug word 
alignment system (Tiedemann 2003a), for the 
creation of the dictionaries. The results gave 
on average 213 new dictionary words (fre-
quency > 3) per language pair. The average 
error rate was 16 percent. Different combina-
tions with Finnish had a higher error rate, 33 
percent, whereas the error rate for the re-
maining language pairs only yielded on aver-
age 9 percent errors. The high error rate for 
Finnish is possibly due to the fact that the 
Finnish language belongs to a different lan-
guage family. Although the corpora were 
very sparse the word alignment results for the 
combinations of Swedish, Danish, Norwe-
gian and Icelandic were surprisingly good 
compared to other experiments with larger 
corpora.   
                                                 
 ? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1 Introduction 
Hall? Norden (Hello Scandinavia) is a web site 
with information regarding mobility between the 
Nordic countries and is maintained by the Nordic 
Council.  Mobility information concerns issues 
such as how employment services, social ser-
vices, educational systems etc. work in the dif-
ferent countries. The web site has information in 
five different languages; Swedish, Danish, Nor-
wegian, Icelandic and Finnish.  In this paper 
Nordic languages are defined as Swedish, Danish, 
Norwegian, Icelandic and Finnish. Scandinavian 
languages are defined as the Nordic languages 
excluding Finnish. 
The texts on the web site were almost parallel 
and there were also ten minimal dictionaries with 
on average 165 words available for the different 
languages. The dictionaries consisted of domain-
specific words regarding mobility information in 
the Nordic countries. The Nordic Council wanted 
to extend the dictionaries so they would cover a 
larger part of the specific vocabulary, in order to 
help the people in the Nordic countries to find 
and learn the concepts in their neighboring coun-
tries. 
The entire set of texts on the web site was 
treated as one multilingual parallel corpus. From 
this we extracted parallel corpora for each lan-
guage pair.  We discovered, as expected, that the 
corpora were very sparse, containing on average 
less than 80 000 words per language pair. We 
needed to construct 10 different dictionaries and 
therefore we processed 10 pairs of parallel text 
sets. We have used the Uplug word alignment 
system (Tiedemann 2003a), for the creation of 
the dictionaries. The system and motivation for 
the choice of system is further discussed in Sec-
tion 2.1. 
Automatic Construction of Domain-specific Dictionaries on  
Sparse Parallel Corpora in the Nordic Languages 
Sumithra Velupillai 
DSV/KTH-Stockholm University 
SE-164 40 Kista 
Sweden 
sumithra@dsv.su.se 
 
Hercules Dalianis 1, 2 
1) DSV/KTH-Stockholm University 
SE-164 40 Kista 
Sweden 
2) Euroling AB 
Igeldammsgatan 22c 
112 49 Stockholm, Sweden 
hercules@dsv.su.se 
10
We also discovered that the texts were not 
completely parallel. Therefore, we made a small 
experiment on attempting to enhance the results 
by deleting texts that were not parallel. Multilin-
gual parallel corpora covering all Nordic lan-
guages are very rare. Although the corpora cre-
ated in this work are domain-specific, they are an 
important contribution for further research on 
Nordic multilingual issues. Moreover, many 
large governmental, industrial or similar web 
sites that contain information in several lan-
guages may profit from compiling multilingual 
dictionaries automatically in order to enhance 
their search engines and search results. 
In this project, our two main goals were to 
compile parallel corpora covering the Nordic 
languages, and to evaluate the results of auto-
matically creating dictionaries using an existing 
tool with basic settings, in order to find out 
where more work would need to be done and 
where performance is actually acceptable. We 
have limited the work by only testing one system 
(Uplug) with basic settings. Our experiments and 
results are described in further detail in the fol-
lowing sections. Conclusions and future work are 
discussed in the final section. 
2 Related Work 
Word alignment systems have been used in pre-
vious research projects for automatically creating 
dictionaries. In Charitakis (2007) Uplug was 
used for aligning words in a Greek-English paral-
lel corpus. The corpus was relatively sparse, con-
taining around 200 000 words for each language, 
downloaded from two different bilingual web 
sites. A sample of 498 word pairs from Uplug 
were evaluated by expert evaluators and the re-
sult was 51 percent correctly translated words 
(frequency > 3). When studying high frequent 
word pairs (>11), there were 67 percent correctly 
translated words. In Megyesi & Dahlqvist (2007) 
an experiment is described where they had 150 
000 words in Swedish and 126 000 words in 
Turkish that gave 69 percent correct translations 
(Uplug being one of the main tools used). In this 
work the need for parallel corpora in different 
language combinations is also discussed. 
The ITools? suite for word alignment that was 
used in Nystr?m et al(2006) on a medical paral-
lel corpus, containing 174 000 Swedish words 
and 153 000 English words, created 31 000 word 
pairs with 76 percent precision and 77 percent 
recall. In this work the word alignment was pro-
duced interactively.  
A shared task on languages with sparse re-
sources is described in Martin et al(2005). The 
language pairs processed were English-Inuktitut, 
Romanian-English and English-Hindi, where the 
English-Inuktitut parallel corpus contained 
around 4 million words for English and 2 mil-
lions words for Inuktitut. English-Hindi had less 
words, 60 000 words and 70 000 words respec-
tively. The languages with the largest corpora 
obtained best word alignment results, for Eng-
lish-Inuktitut over 90 percent precision and recall 
and for English-Hindi 77 percent precision and 
68 percent recall. One conclusion from the 
shared task was that it is worth using additional 
resources for languages with very sparse corpora 
improving results with up to 20 percent but not 
for the languages with more abundant corpora 
such as for instance English-Inuktitut.  
2.1 Word Alignment: Uplug 
We have chosen to use the Uplug word align-
ment system since it is a non-commercial system 
which does not need a pre-trained model and is 
easy to use. It is also updated continuously and 
incorporates other alignment models, such as 
GIZA++ (Och & Ney 2003). We did not want to 
evaluate the performance of different systems in 
the work presented here, but rather evaluate the 
performance of only one system applied on dif-
ferent language combinations and on sparse cor-
pora. Evaluating the performance of different 
systems is an important and interesting research 
problem, but is left for future work. An evalua-
tion of two word alignment systems Plug (Uplug) 
and Arcade is described in Ahrenberg et al
(2000). 
The Uplug system implements a word align-
ment process that combines different statistical 
measures for finding word alignment candidates 
and is fully automatic. It is also possible to com-
bine statistical measures with linguistic informa-
tion, such as part-of-speech tags. In the preproc-
essing steps the corpora are converted to an xml-
format and they are also sentence aligned. 
We have chosen to use basic settings for all 
corpora in the different language pairs, in order 
to evaluate the effect of this. The default word 
alignment settings in Uplug works in the follow-
ing way:  
 
? create basic clues (Dice and LCSR) 
? run GIZA++ with standard settings 
(trained on plain text) 
11
? learn clues from GIZA's Viterbi align-
ments 
? "radical stemming" (take only the 3 initial 
characters of each token) and run GIZA++ 
again 
? align words with existing clues 
? learn clues from previous alignment 
? align words again with all existing clues1 
 
This approach is called the clue alignment ap-
proach and is described further in Tiedemann 
(2003b). In the work presented here, we have not 
included any linguistic information, as we 
wanted to evaluate the performance of applying 
the system on sparse, raw, unprocessed corpora 
for different (Nordic) language pairs, using de-
fault settings. 
 
3 Experiments and Results 
For the project presented in this paper we wanted 
to see if it was possible to create domain-specific 
dictionaries on even smaller corpora. (compared 
to the ones described in Section 2) for all the 
Nordic language pairs. We did not have the pos-
sibility to evaluate the results for Icelandic-
Finnish, since we did not find any evaluator hav-
ing knowledge in both Icelandic and Finnish. 
Therefore we present the results for the remain-
ing nine language pairs. In total we had four 
evaluators for the other language combinations. 
Each evaluator evaluated those language pairs 
                                                 
                                                
1 Steps taken from the Quickstart guidelines for the Uplug 
system, which can be downloaded here: 
http://uplug.sourceforge.net/ 
she or he had fluent or near-fluent knowledge in. 
The domain was very restricted containing only 
words about mobility between the Nordic coun-
tries. 
The Scandinavian languages are closely re-
lated. Swedish, Danish, and Norwegian are com-
prehensible for Scandinavians. A typical Swede 
will for instance understand written and to a cer-
tain degree spoken Danish, but is not able to 
speak Danish. Typical Swedes will, for instance, 
have a passive understanding of Danish (and vice 
versa for the other languages). Finnish on the 
other hand belongs to the Finno-Ugric group of 
the Uralic languages, while the Scandinavian 
languages are North-Germanic Indo-European 
languages. We wanted to investigate if, and how, 
these differences affect the word alignment re-
sults. We also wanted to experiment with differ-
ent frequency thresholds, in order to see if this 
would influence the results. 
The first step was to extract the web pages 
from the web site and obtain the web pages in 
plain text format. We obtained help for that work 
from Euroling AB,2 our contractor.  
In Table 1 we show general information about 
the corpora. We see that the distribution of words 
is even for the Scandinavian languages, but not 
for the combinations with Finnish. It is interest-
ing to observe that Finnish has fewer word to-
kens than the Scandinavian languages.  
All Nordic languages, both Scandinavian and 
Finnish, have very productive word compound-
ing. In Finnish word length is longer, on average, 
 
2 See: http://www.euroling.se/ 
Language pair No. texts No. words Word distribution, first language in language pair, %
sw-da 191 83871 49.2
sw-no 133 62554 49.7
sw-fi 196 73933 57.6
sw-ice 187 82711 48.5
da-no 156 68777 50.2
da-fi 239 84194 58.4
da-ice 232 97411 49.5
no-fi 156 58901 58.2
no-ice 145 64931 49.6
Average 182 75254 52.3
Table 1: General corpora information, initial corpora 
12
and the number of words per clause lower, on 
average, due to its extensive morphology. 
In Dalianis et al(2007) lemmatizing the text 
set before the alignment process did not improve 
results. In the work presented here, we have also 
made some experiments on lemmatizing the cor-
pora before the alignment process. We have used 
the CST lemmatizer3 for the Scandinavian lan- 
guages and Fintwol4 for Finnish. Unfortunately, 
the results were not improved. The main reason 
for the decrease in performance is probably due 
to the loss of sentence formatting during the 
lemmatization process. The sentence alignment 
is a crucial preprocessing step for the word 
alignment process, and a lot of the sentence 
boundaries were lost in the lemmatization proc-
ess. However, the resulting word lists from 
Uplug have been lemmatized using the same 
lemmatizers, in order to obtain normalized dic-
tionaries. 
The corpora were to some extent non-parallel 
containing some extra non-parallel paragraphs. 
We found that around five percent of the corpora 
were non-parallel. In order to detect non-parallel 
sections we have used a simpler algorithm than 
in for instance Munteanu & Marcu (2006). The 
total number of paragraphs and sentences in each 
                                                 
                                                
3 See: http://cst.dk/download/cstlemma/current/doc/ 
4 See: http://www2.lingsoft.fi/cgi-bin/fintwol 
parallel text pair were counted. If the total num-
ber for each language in some language pair dif-
fered more than 20 percent these files were de-
leted. The refined corpora have been re-aligned 
with Uplug and evaluated. In Table 2 we show 
the general information for the refined corpora. 
3.1 Evaluation 
Our initial plan was to use the manually con-
structed dictionaries from the web site as an 
evaluation resource, but the words in these dic-
tionaries were rare in the corpus. Therefore we 
used human evaluators to evaluate the results 
from Uplug.  
The results from the Uplug execution gave on 
average 213 new dictionary words (frequency > 
3) per language, see Table 3. The average error 
rate 5  was 16 percent. We delimited the word 
amount by removing words shorter than six char-
acters, and also multiword expressions6 from the 
resulting word lists. The six character strategy is 
efficient for the Scandinavian languages as an 
alternative to stop word removal (Dalianis et al
2003) since the Scandinavian languages, as well 
 
5 The error rate is in this paper defined as the percentage of 
wrongly generated entries compared to the total number of 
generated entries. 
6 A multiword expression is in this paper defined as words 
(sequences of characters, letters or digits) separated by a 
blank or a hyphen. 
Language pair No. parallel texts Deleted files, % No. words, parallel 
Word distribution, 
first language in 
language pair, %
sw-da 179 6.3 78356 49.7
sw-no 128 3.8 59161 49.8
sw-fi 189 3.6 69525 58.1
sw-ice 175 5.9 76056 48.3
da-no 147 5.8 64946 50.2
da-fi 222 7.1 77849 58.6
da-ice 210 3.4 89093 49.0
no-fi 145 7.1 55409 58.3
no-ice 130 2.1 59622 49.0
Average 169 5.0 70002 52.3
Table 2: General corpora information, refined parallel corpora (non-parallel texts deleted) 
 
13
as Finnish, mostly produce compounds that are 
formed into one word (i.e. without blanks or hy-
phens). In Tiedemann (2008), a similar strategy 
of removing words with a word length shorter 
than five characters was carried out but in that 
case for English, Dutch and German. 
Different combinations with Finnish had a 
higher error rate, 30 percent, whereas the error 
rate for the combinations of the Scandinavian 
languages only yielded on average 9 percent  
errors. 
The high error rate for Finnish is possibly due 
to the fact that the Finnish language belongs to a 
different language family. We can see the same 
phenomena for Greek (Charitakis, 2007) and 
Turkish (Megyesi & Dahlqvist, 2007) combined 
with English and Swedish respectively, with 33 
and 31 percent erroneously translated words. 
However, one might expect even higher error 
rates due to the differences in the different lan-
guage pairs (and the sparseness of the data). Fin-
nish has free word order and is typologically 
very different from the Scandinavian languages, 
and the use of form words differs between the 
languages. On the other hand, both Finnish and 
the Scandinavian languages produce long, com-
plex compounds somewhat similarly, and the 
word order in Finnish share many features with 
the word order in the Scandinavian languages. 
One important aspect is the cultural similarities 
that the languages share.  
The main errors that were produced for the 
combinations of Finnish and the Scandinavian 
languages consisted of either errors with particles 
or compounds where the head word or attribute 
were missing in the Finnish alignment. For in-
stance, the Swedish word inv?nare (inhabitant) 
was aligned with the Finnish word asukasluku 
(number of inhabitants). Another error which 
was produced for all combinations with Finnish 
was lis?tieto (more information) which was 
aligned with ytterligere (additional, more) in 
Norwegian (and equivalent words in Swedish 
and Danish), an example of an error where the 
head word is missing. Many texts had sentences 
pointing to further information, which might ex-
plain this type of error. 
The lemmatizers produced some erroneous 
word forms. In Dalianis & Jongejan (2006) the 
CST lemmatizer was evaluated and reported an 
average error rate of nine percent. Moreover, 
since the lemmatization process is performed on 
the resulting word lists, and not within the origi-
nal context in which the words occur, the auto-
matic lemmatization is more difficult for the two 
lemmatizers used in this project. These errors 
have not been included in our evaluation since 
they are not produced by the Uplug alignment 
procedure. 
We can also see in Table 3 that deleting non-
parallel texts using our simple algorithm did not 
improve the overall results significantly. Perhaps 
our simple algorithm was too coarse for these 
corpora. The texts were in general very short and 
simple frequency information on paragraph and 
sentence amounts might not have captured non-
parallel fragments on such texts. 
 Initial   Deleting non-parallel 
Language 
pair 
No. dictionary 
words  
Erroneous 
translations, %
No. dictionary 
words  Erroneous translations, % 
sw-da 322 7.1 305 7.2
sw-no 269 6.3 235 9.4
sw-fi 138 29.0 133  34.6
sw-ice 151 18.5 173 16.2
da-no 322 3.7 304 4.3
da-fi 169 34.3 244  33.2
da-ice 206 6.8 226 10.2
no-fi 185 27.6 174  30.0
no-ice 159 14.5 181 14.4
Average  213 16.4  219  16.1
Table 3: Produced dictionary words and error rate 
14
The produced dictionary words were of high 
domain-specific quality. The majority of the cor-
rect and erroneous word pairs were covered by 
both the initial and the refined corpus. Deleting 
non-parallel texts produced some new, valuable 
words that were not included in the initial results. 
However, since these dictionaries were generally 
smaller, this did not improve the overall results, 
and the error rate was somewhat higher for most 
language pairs. Improved dictionary in this work 
means as many word pairs as possible with do-
main-specific significance. 
Since the texts were about different country-
specific issues they could contain sections in an-
other language (names of ministries, offices etc). 
This produced some errors in the alignment re-
sults. These errors might have been avoided by 
applying a language checker while processing 
the texts. 
The errors for the Scandinavian languages 
were also mainly of the same type, and mostly 
due to the fact that the texts were not completely 
parallel, or due to form words or compounds. For 
instance, the Swedish word exempelvis (for ex-
ample) was aligned with the Norwegian word 
eksempel (example), which was counted as an 
error, but which, in its context, is not completely 
erroneous. 
Even at a relatively low frequency threshold 
the results were very good for the Scandinavian 
languages. We tried to increase the frequency 
threshold in order to see if this would improve 
the results for Finnish, which it unfortunately did 
not. However, as stated above, the errors were 
mainly of the same type, and probably constant 
over different frequencies. We also see that for 
Icelandic, unlike the other languages, deleting 
non-parallel fragments yielded larger dictionar-
ies. Uplug produced more multiword units for 
the initial corpora containing Icelandic, single 
word pairs were more frequent in the refined 
corpus. However, the overall results were not 
improved. 
4 Conclusions and Future Work 
Although the corpora were very sparse the word 
alignment results for Swedish-Danish, Swedish-
Norwegian and Danish-Norwegian were surpris-
ingly good with on average 93.1 percent correct 
results. The results for Finnish were worse with 
on average only 67.4 percent correct results. 
However, as discussed above, the main errors 
were of the same type. Creating dictionaries for 
non-related languages might need more elaborate 
alignment approaches. In the special case of Fin-
nish combined with one (or several) of the Scan-
dinavian languages, simple preprocessing steps 
might improve the results. For instance, remov-
ing stop words before running the corpora 
through a word alignment system might handle 
the errors where particles and form words are 
included. Also, tagging the corpora with part-of-
speech tags and lemmatizing as a preprocessing 
step might improve results. 
An important aspect of automatically creating 
multilingual dictionaries is the need for preproc-
essing tools covering all languages. This is often 
difficult to obtain, and different tools use differ-
ent formatting and tagging schemes. Moreover, 
they might differ in robustness, which also af-
fects the end results. In this project, we encoun-
tered such problems during the lemmatization 
process for instance, but we did not have the op-
portunity to explore and evaluate alternative 
tools. In the future, evaluating the performance 
of the preprocessing steps might be desirable. 
Evaluating translated words is not easy. Many 
words may be related without being direct trans-
lations. Manual evaluation has the advantage of 
taking such issues into account, but this also 
means that the results might differ depending on 
the evaluator. Furthermore, evaluating transla-
tions without contextual information is problem-
atic. Also, the criteria for judging a translation as 
correct or not depend on the goal for the use of 
the word lists. For instance, the errors for the 
combinations with Finnish might not be prob-
lematic in a real-world search engine setting, de-
pending on which demands there are on the 
search results. The errors produced in the work 
presented here would probably yield acceptable 
search results. Such user and search engine result 
aspects have not been evaluated here, but are 
interesting research questions for future work. 
The Nordic languages are highly inflectional. 
Combining compound splitting and lemmatizing 
before the alignment process might improve the 
results. Especially compound splitting could 
probably handle the errors produced for the com-
binations of Finnish with the Scandinavian lan-
guages. Cross-combining the different language 
pairs might enhance the results and create more 
specific and errorless dictionaries. Other word 
alignment systems should also be tested, in order 
to compare different approaches and their results. 
Perhaps results from different systems could also 
be combined, in order to produce more extensive 
dictionaries. Furthermore, other approaches to 
15
detect non-parallel fragments should be investi-
gated. 
Finding the boundary for the minimum size of 
parallel corpora in order to obtain acceptable dic-
tionaries is also an interesting research issue 
which should be explored. 
Automatically creating multilingual dictionar-
ies is not trivial. Many aspects need to be consid-
ered. Especially, the final use of the produced 
results influences both the preprocessing steps 
required and the evaluation of the results. Also, 
the languages in consideration affect the steps 
that need to be made. However, in this paper we 
have shown that using state-of-the-art tools on 
sparse, raw, unprocessed domain-specific cor-
pora in both related and non-related languages 
yield acceptable and even commendable results. 
Depending on the purposes for the use of the dic-
tionaries, simple adjustments would probably 
yield even better results. 
In a real-world setting, parallel (or near-
parallel) corpora covering several (small) lan-
guages are difficult to obtain and compile. Most 
resources are found on the Internet, and the qual-
ity of the corpora may vary depending on many 
aspects. Formatting, translations, text length and 
style may differ considerably depending on the 
type of texts. Freely available text sets for small 
languages are often sparse. Despite this, we have 
shown that it is possible to compile valuable re-
sources from available data.   
There are very few sources of dictionaries 
covering the Nordic language pairs. The created 
corpora will be made publicly available for fur-
ther research and evaluation. 
References 
Ahrenberg, L., M. Merkel, A. S?gvall Hein and J. 
Tiedemann 2000. Evaluation of word alignment 
systems. Lars Ahrenberg, Magnus Merkel, Anna 
S?gvall Hein and J?rg Tiedemann. Proceedings of 
the Second International Conference on Linguistic 
Resources and Evaluation (LREC-2000), Athens, 
Greece, 31 May - 2 June, 2000, Volume III: 1255-
1261. 
Charitakis, K. 2007. Using parallel corpora to create a 
Greek-English dictionary with Uplug, in Proc. 16th 
Nordic Conference on Computational Linguistics - 
NODALIDA ?07. 
Dalianis, H. and B. Jongejan 2006. Hand-crafted ver-
sus Machine-learned Inflectional Rules: the Eurol-
ing-SiteSeeker Stemmer and CST's Lemmatiser, in 
Proc. of the International Conference on Language 
Resources and Evaluation, LREC 2006. 
Dalianis, H., M. Rimka and V. Kann 2007. Using 
Uplug and SiteSeeker to construct a cross language 
search engine for Scandinavian. Workshop: The 
Automatic Treatment of Multilinguality in Re-
trieval, Search and Lexicography, Copenhagen, 
April 2007. 
Dalianis, H., M. Hassel, J. Wedekind, D. Haltrup, K. 
de Smedt and T.C. Lech. 2003. Automatic text 
summarization for the Scandinavian languages. In 
Holmboe, H. (ed.) Nordisk Sprogteknologi 2002: 
?rbog for Nordisk Spr?kteknologisk Forsknings-
program 2000-2004, pp. 153-163. Museum Tuscu-
lanums Forlag. 
Martin, J and R. Mihalcea and T. Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. Proceedings of the ACL 2005 Workshop 
on Building and Using Parallel Texts: Data Driven 
Machine Translation and Beyond, Ann Arbor, MI, 
June 2005.  
Megyesi, B. and B. Dahlqvist, 2007. The Swedish-
Turkish Parallel Corpus and Tools for its Creation, 
in Proc. 16th Nordic Conference on Computational 
Linguistics - NODALIDA ?07. 
Munteanu, D.S. and D. Marcu 2006. Extracting Paral-
lel Sub-sentential Fragments from Non-parallel 
Corpora. ACL ?06: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics, 
pp. 81-88, Sydney, Australia. 
Nystr?m, M., M. Merkel, L. Ahrenberg, P. Zweigen-
baum, H. Petersson and H. ?hlfeldt. 2006. Creat-
ing a Medical English-Swedish Dictionary using 
Interactive Word Alignment, in BMC medical in-
formatics and decision making, 6:35.  
Franz Josef Och, Hermann Ney. A Systematic Com-
parison of Various Statistical Alignment Models, 
Computational Linguistics, volume 29, number 1, 
pp. 19-51 March 2003. 
Tiedemann, J. 2003a. Recycling Translations: Extrac-
tion of Lexical Data from Parallel Corpora and 
their Application in Natural Language Processing. 
Acta Universitatis Upsaliensis: Studia linguistica 
upsaliensia, ISSN 1652-1366, ISBN 91-554-5815-
7. 
Tiedemann, J. 2003b. Combining clues for word 
alignment. In Proceedings of the Tenth Conference 
on European Chapter of the Association For Com-
putational Linguistics - Volume 1 (Budapest, Hun-
gary, April 12 - 17, 2003). European Chapter Meet-
ing of the ACL. Association for Computational 
Linguistics, Morristown, NJ, 339-346. DOI= 
http://dx.doi.org/10.3115/1067807.1067852. 
Tiedemann, J. 2008. Synchronizing Translated Movie 
Subtitles. In the Proceedings of the Sixth Interna-
tional Conference on Language Resources and 
Evaluation, LREC 2008, Marrakech, Morocco, 
May 28-30, 2008. 
16
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 53?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Characteristics and Analysis of Finnish and Swedish  Clinical Intensive Care Nursing Narratives   Helen Allvinf, Elin Carlssonf, Hercules Dalianisf, Riitta Danielsson-Ojalaa, Vidas Daudaravi?iusb, Martin Hasself, Dimitrios Kokkinakisc, Helj? Lund-gren-Lainea, Gunnar Nilssonf, ?ystein Nytr?d, Sanna Salanter?a, Maria Skeppstedtf, Hanna Suominene, Sumithra Velupillaif aDepartment of Nursing Science, University of Turku, VSSHP, Turku, Finland, bVytautas Magnus University, Lithuania, cDepartment of Swedish, University of Gothenburg, Sweden, dIDI, The Norwegian University of Science and Technology, Norway, eNICTA Canberra Research Laboratory and Australian National University, Australia fDepartment of Computer and Systems Sciences/Stockholm University Forum 100 SE-164 40 Kista, Sweden http://www.dsv.su.se/hexanord 
Abstract 
We present a comparative study of Finnish and Swedish free-text nursing narratives from intensive care. Although the two languages are linguistically very dissimilar, our hypothe-sis is that there are similarities that are impor-tant and interesting from a language technology point of view. This may have im-plications when building tools to support pro-ducing and using health care documentation. We perform a comparative qualitative analysis based on structure and content, as well as a comparative quantitative analysis on Finnish and Swedish Intensive Care Unit (ICU) nurs-ing narratives. Our findings are that ICU nurs-ing narratives in Finland and Sweden have many properties in common, but that many of these are challenging when it comes to devel-oping language technology tools. 
1 Introduction The purpose of this study1 is to do content and lexical analysis of nursing narratives written in an                                                            1 Our research on the Stockholm EPR Corpus (Dalianis et al, 2009) has been approved by Etikpr?vningsn?mnden i Stock-holm, (the Regional Vetting Board), reference number 
Intensive Care Unit (ICU). The ultimate goal of our research is to define linguistic similarities and language-specific aspects that differentiate clinical narratives in Finnish and Swedish in order to lay groundwork for developing internationally appli-cable language technology solutions and create a framework for characterising and comparing clini-cal narratives. Free text is handy for information entry but a challenge for information extraction, care handover and other uses of gathered informa-tion. Language technology can alleviate some of these problems in retrospective analysis by offer-ing a more semantically informed interpretation and abstraction. However, the most promising po-tential of language technology is to interactively improve, interpret and code during text entry so that the resulting structured, coded, free text can be validated immediately. The critical bottleneck to-day is namely information handover and reuse, and extensive text is simply not used nor is useful. In-teractively validated, semantically processed text could be more usable and support abstraction, visualization and query tools for the benefit of cli-nicians, patients, researchers and quality adminis-trators.                                                                                               2009/1742-31/5. Our research on the Finnish Corpus (Salan-ter? et al, 2009) has been approved by the Ethical committee of the Hospital District of South West Finland, reference number 2/2009, ?66. 
53
In this paper, we analyze Finnish and Swedish ICU nursing narratives from both qualitative and quantitative perspectives. Our data includes textual nursing documentation of adult patients with a protracted inpatient period. We have chosen ICUs because of their international similarity in decision making (Lauri & Salanter? 2002) and nursing documentation because it covers the entire inpa-tient period. 2 Background 
2.1 Clinical text Clinical text covers the text documents produced for clinical work by clinicians and occurs in clini-cal information systems. It is written by clinicians, that is, professionals (physicians, nurses, therapists and other specialist) responsible for patient care. Its primary purpose is to serve patient care as a summary or hand-over note. However, clinical text is also written for legal requirements, care continu-ity and purposes of reimbursement, management and research. Clinical text covers every care phase and, depending on the purpose, documents differ. Documents that describe the patient?s state, current health problems and socio-medical history are very different from those describing a care plan, its ac-tualization and evaluation of care outcomes. Again, these differ from diagnostic notes, lab results, ra-diography readings, pathology reports and dis-charge documents that plan further care at discharge.  Finally, clinical text may have been entered in ?real time?, in retrospect, or as a sum-mary, by the bedside or elsewhere. The enterer can be a clinician, secretary who transcribes a dictate, speech recognition software or another system that generates or synthesizes text, (McDonald 1997, Thoroddsen et al, 2009.).  2.2 Legal requirements for clinical documen-tation in different countries In several countries clinical documentation is based on law. In Finland, the Ministry of Social Affairs and Health (Statutes of Finland, 298/2009) defines that to ensure good care, all necessary and wide-ranging information has to be registered in patient records. In Sweden, the National Board of Health and Welfare has a similar approach (Pa-tientdatalagen 2008:355). Clinical text should be 
explicit and intelligible, and only generally well-known, accepted concepts and abbreviations are allowed to be used. It should detail adequately the patient?s conditions, care and recovery.  2.3 Special features of ICU and nursing An ICU is an essential component of most large hospitals with high quality care.  ICUs provide care for critically ill patients and focus on condi-tions that are life-threatening and require compre-hensive care and constant monitoring (Webster's 2010). This task is fairly similar universally. It is based on optional, international guidelines focus-ing on triage, admission, discharge and education. This international similarity was evident when nurses? decision making was studied in Canada, Finland, Northern Ireland, Switzerland, Norway and the USA (Lauri & Salanter? 2002); the study showed that decision making of ICU nurses was the most uniform in different countries when com-pared with nurses working in public health care, psychiatric care, and short and long term care. Clinical text written by nurses, that is, nursing narratives, both in Finland and in Sweden is based on the care process which stands for gathering in-formation from the patient, setting goals for care, implementing nursing interventions, and evaluat-ing the results of given care. In Finland, the na-tional standardized documentation model has been implemented with the Finnish care classification (assessment, interventions and outcomes of care) (Tanttu & Ikonen 2007). The Swedish VIPS model provides a structure for the documentation process with key words that reflect the nursing process (Ehrenberg et al, 1996).  ICU nursing narratives can be lengthy, espe-cially when the patient stay in the ICU is pro-longed. As much as 60 A4 pages equivalents of written text may be gathered during one period of care. However, clinicians have somewhat different opinion on how to organize the information they write. For example, headings are often inconsistent and text under headings can cover a lot of other issues than those directly concerning the given heading. (Suominen et al, 2009.)  2.4 Related studies Since most of the available clinical documents are in free-text form, a number of stylistically oriented efforts to characterize the data from various angles 
54
have taken place. This may include various topics, from viewing detailed information about specific items (e.g. readability, Kim et al, 2007) to identi-fying patterns and structures in order to provide better technology to automatically process the sublanguage (Pakhomov et al, 2006). The majority of such efforts investigate different aspects of lin-guistic features at a monolingual level, for in-stance, Hahn & Wermter (2004); Tomanek et al, (2007); Chung (2009); Harkema et al, (2009); while for a thorough review of various related is-sues see Meystre et al, (2008). In the Nordic con-text, Josefsson (1999) discusses Swedish clinical language and shows examples on how verb con-structions in a clinical setting differ from a non clinical setting.  One claim is that the physician unmarks the verb forms for agentivity when writ-ing about the patient and what actions she takes, for example, Patienten hallucinerar [The patient hallucinates] instead of the normal form  Patienten f?r hallucinationer [The patient experiences hallu-cinations].   Helles? (2005) describes nurses' general use of the language function in the nursing discharge notes. She finds that the text in the nursing dis-charge notes is information-dense and character-ized by technical terms, and that the use of standardised templates helped nurses improve the completeness, structure and content of the informa-tion. Comparisons at a monolingual level between written clinical text and lay text has been carried out by Dalianis et al, (2009). A contrastive com-putational linguistics study was carried out be-tween the Stockholm EPR Corpus (SEPR) and a general language corpus, both written Swedish text. The findings showed that SEPR contained longer words and that the vocabulary was highly domain-specific. Other work is described in Ownby, (2005). Comparing clinical text at a crosslingual level has, to our knowledge, only been done by Borin et al, (2007).  3 Analysis of Finnish and Swedish ICU nursing narratives  The analyzed nursing narratives origin from one ICU in a university-affiliated hospital both in Fin-land and Sweden. Our inclusion criterion was an ICU inpatient period of at least 5 days and patient's age of at least 16 years. The Finnish data includes nursing narratives from 514 patient records (496 
unique patients, 18 rebounds, a patient record is defined as each inpatient period of at least 5 days per patient) between January 2005 and August 2006. The Swedish data includes nursing narra-tives from 379 patient records (333 unique pa-tients, 46 rebounds) between January 2006 and May 2008. Since we did not have complete admis-sion and discharge documents from both countries, our analysis is performed on daily nursing narra-tives. These documents are written by ICU nurses during the actual inpatient period from the patient admission to the discharge.    3.1 Qualitative analysis A manual content analysis was performed by four health care professionals (i.e., three native Finnish speakers knowing Swedish, one Swedish native speaker) and one native Swedish speaking lan-guage consultant. Three average-sized patient re-cords each from Finland and Sweden were chosen for our analysis (average size 2,389 words for Fin-land and 5,169 words for Sweden). In the analysis, we considered special features (Table 1) of daily notes both from the structural and content related points of view.  The style and context of both Finnish and Swed-ish text is very similar. For health care profession-als, and especially with an ICU background, all the texts are intelligible and the meaning of a writer becomes evident from the context even in the pres-ence of numerous linguistic and grammatical mis-takes; almost all the sentences are lacking both grammatical subjects and objects. It is evident that in both countries, the narratives are written from a professional to a professional in order to support information transfer, remind about important facts, and supplement numerical data.  A feature common for all the six records is that they rarely contain any subjects or objects when nurses are writing about patients. However, in the Swedish nursing narratives the word patient is used as a subject or object much more often than in the Finnish narratives. The abbreviation pat. is mostly used for this reference and she/he is never used for this purpose. In the whole data, pat. is 40 percent more common than she/he, which is the most common personal pronoun. It seems that the word patient or pat. is used more when the profes-sionals are writing about relatives. In general, pro-nouns are used infrequently in the narratives, and  
55
  Table 1. Special structural and contextual features of Finnish and Swedish daily ICU nursing narratives. The original examples are added in ().  I very rarely. If the reader is not a health care pro-fessional, a risk for confusing the subject (i.e., the patient or nurse) arises. However, the context makes it almost always clear who is referred to.  Approximately half of the narratives do not contain any verb. The most common tense is perfect, but 
without the auxiliary has. When the meaning does not contain a subject it becomes ?unnatural? to use has. Instead, the supine form is used, for example slept, lain, and eaten. Both present and past parti- ciples without be-verb are common, for example, Breathing: Ventilator parameters unchanged. 
Special features of Finnish narratives   Special features of Swedish narratives   Structure Examples Structure Examples 
Headings are used in 2 out of 3 patient records. Headings are typi-cally used as subjects or subjects are partially used.  
Diuresis: occasionally profuse. (Diureesi: ajoittain runsasta.)  Pupils move under eyelids but does not open eyes. (Pupillit liikkuvat luomien alla, mutta ei avaa silmi??n.)  
Headings are used in all daily narra-tives. In Swedish daily narratives, the structure of headings seems to be obligatory. The headings are used typically as subjects.  
Circulation: Stable with ino-trop. (Cirkulation: Stabil med ino-tropi.)  Reacts only for pain stimula-tion during the suction of intu-bation tube. (Reagerar enbart vid sm?rt-stimuli vid sugning i tuben.) 
Present and past participles are typical but verbs of be, is and are are not used.  
Consciousness remained un-changed. (Tajunta pysynyt ennallaan.)  Blood pressure low. (Verenpaine matala.) 
Present and past participles are typical but verbs of be, is and are are not used.  
Breathing: Ventilator parame-ters unchanged. (Andning: Ventilator paramet-rarna of?r?ndrade.) 
Complete sentences are rarely used.  
No spontaneous movements, rigidifies. (Ei spontaania liikett?, j?ykiste-lee.)  Husband and daughter have been staying a long time beside the patient. (Mies ja tyt?r olleet pitk??n potilaan vierell?.) 
Complete sentences are rarely used.  
Light sedation, looks up now and then. (L?tt sederad, tittar upp ibl-and.)  She took the wedding ring and the watch home. (Hon tog med sig vigselring och klocka hem.) 
Misspellings are found but the content or meaning is still clear.  Hemodynamic ? hemodynamic (Hemodynamiikka ? henody-namiikka) Misspellings are found but the content or meaning is still clear.  
The motther is informed. (Mammman ?r informered.)  Magnesium is addded. (Magnesium har tilllsatts.) Content Examples Content  Examples 
The word patient as a subject or object is infrequently mentioned. If this word is mentioned it is not abbreviated.  
Oxidates well or ventilates well. (Happeutuu hyvin tai ventiloituu hyvin.)  
The word patient is used more often than in Finnish narratives as a subject or object. It is also replaced with abbreviations of Pat or Pt.  
Patient got a percutanous tracheostomy today. (Patienten har f?tt en perkutan trakeostomi idag.)  Very worried about patient?s condition. (Mycket oroliga ?ver patien-tens tillst?nd.)  Pt. wakes up for talking and appears to be adequate. (Pt. vakner p? tilltal och up-plevs som adekvat.) 
Signs are typically used: e.g., >, <, -->, +, -.  
The height for the drain raised from 10 --> 20 mmHg. (Dreneerausrajaa nostettu 10  --> 20 mmHg.)  Got medicine --> good response. (Sai l??kett? -->hyv? vaste.) 
Many different abbreviations are used. The origin of entire word is Swedish, English, Latin, professional or ICU typical.  
em. [eftermiddag, afternoon], HR [heart rate], VF [Ven-tricula/Fibrillation, Ventricular Fibrillation]  
56
The use of headings is frequent and good ? most of the time the content matches the headings (Tables 1 and 3). In addition, headings are used similarly in the Swedish and Finnish documents. Most of the time the headings are considered as subjects of the sentence, for example, Consciousness: Unchanged. Liquor brighter than yesterday. However, in the use of headings there are two interesting findings: If the headings are to be cho-sen freely, as in the Finnish narratives, nurses tend to use their own headings and hence many syno-nyms or closely related concepts are used; for ex-ample, hemodynamics versus blood pressure and pulse or breathing versus oxidation. If the headings are obligatory, as in the Swedish narratives, nurses tend to write their observations under the heading which is somehow closest to the subject; for exam-ple, body temperature under circulation or level of sedation under sleep. For both languages the use of different abbrevia-tions is very common. Almost every daily nursing narrative included several abbreviations. Most of the abbreviations are typical for an ICU domain: CVP [central venous pressure], PEEP [Positive End-Expiratory Pressure], EN [Enteral Nutrition], TPN [Total Parenteral Nutrition], pO2 [partial pressure of oxygen], pCO2 [partial pressure of carbon dioxide], MV [Minute Ventilation] and MAP [Mean Arterial Pressure]. From a language technology point of view this means that ICU nurs-ing narratives contain language-independent vo-cabulary. However, nurses in both countries also use many language dependent abbreviations. 
3.2 Quantitative analysis The Finnish data set (n=514) was quantitatively analyzed using the morphological analyser FinT-WOL and the disambiguator FinCG, (Lingsoft 2010), and the Swedish data set (n=379) using the GTA, Granska Text Analyzer (Knutsson et al, 2003). Both data sets are rich in terms of amount of text and vocabulary (Table 2). It is also clear that the amount of text written per day and patient varies a lot in both data sets. More complex words were spelled in numerous ways. For example, the pharmacological substance Noradrenalin had ap-proximately 350 and 60 different spellings in the Finnish and Swedish data sets, respectively. This problem is part of a more general issue of refer-
ence resolution e.g. when mapping different lexical terms referring to the same concept. In our quantitative analysis, we have included punctuation characters. In the Swedish data there was a large amount of html-tags and other format-ting characters, which has a high impact on the total number of tokens (see Table 2). Moreover, as Finnish is highly inflective, FinCG produces alter-native lemmas, hence it is possible to reduce the sparseness of the data by processing the output by choosing only one alternative lemma (see total number of types in Table 2).  To further illustrate the richness of ICU nursing language, the number of unique bigrams (e.g., ?is not?, ?oxidate well? and ?night time? (note: a mis-spelled compound) are the most common ones for Finnish) and trigrams (e.g., ?oxygeneated and ven-tilated?, ?and ventilate well? are among the most common ones for Finnish) were 368,166 (275,205 after FinCG) and 745,407 (356,307 after FinCG) for Finnish patient records. For the Swedish data, the number of unique bigrams was 469,455 (344,127 after GTA) and 1,064,944 (905,539 after GTA). Examples of common Swedish ICU bi-grams and trigrams include ?circulation stabile?, ?during night?, ?in connection with?, and ?with good effect?. Of the content of Finnish nursing narratives, 11% are verbs, 7% nouns and less than 1% pronouns. For Swedish nursing narratives, the respective percentages are 11%, 27% and 2%. One reason for the high numbers for nouns in the Swed-ish data might be due to the large amount of (obligatory) headings relative to the Finnish data (see Table 3).  To support fluent information flow, language technology is needed to strengthen referential con-gruence. Much of this richness of vocabulary is explained by abbreviations and personal differ-ences in professional jargon. In particular, abbre-viations were common. Based on the analysis of the most common words, abbreviations were rela-tively established in Swedish data. For the Finnish data, abbreviations were less standard but RR, SR, CVP, h, ad, ml, ok, vas. [vasen, left] and oik. [oikea, right] were extremely common. Thus, ref-erential congruence can be strengthened by spell-ing out the most common abbreviations automatically. Adding topical content headings is another way to support information flow. Topical content head-ings were mandatory for Swedish data, but no de-
57
fault headings for Finnish existed. However, the headings for Finnish were established in terms of content. In Table 3, we see that the headings for both languages cover similar topics, which indi-cates that the clinical information need is similar for professionals in both countries (and languages). Thus, we recommend forming a standardized set of headings from which the user can voluntarily se-lect the ones to be used. This does not exclude add-ing other headings. Another alternative is to develop language technology for topic segmenta-tion and labeling. We have promising results from this approach (see, e.g., Suominen 2009).  Temporal expressions (e.g., time, evening, night) were often used in both data sets. This poses the question of tense analysis of verbs being unneces-sary and the time-related words being enough to imply the needed temporal information. It is also interesting to note that the negations inte [not, Swe], ingen [none, Swe], ej [not, Swe] and ei [no/not, Fin] are all among the most common types, which is an important property to take into account in information extraction applications. Furthermore, words regarding the oral cavity, such as breathing and mucus, as well as relations, such as daughter, son, wife, and husband are very com-mon in both data sets.  Inspired by the tf?idf-measure from information retrieval, we also analyzed the most common words in terms of a) the number of patients in whose documents the word was used and b) the number of daily nursing narratives in which the word was used. Here, we found, in both data sets, that those words that were used for all patients as well as all daily narratives, were very similar in both data sets, and were related to the most com-mon headings, temporal expressions, negations and monitoring (e.g., increase, continue, begin).  The amount of Protected Health Information (PHI) in form of person names was equal in both of the data sets: 1.5 person names per thousand tokens. This is notable, since this has implications when it comes to integrity issues and reuse of data for research purposes. FinCG did not recognize 36% of the content of Finnish nursing narratives. However, words marked as unrecognized by FinCG also included punctuation marks. In our previous study (see Suominen 2009 and references therein), we tai-lored FinCG by extending approximately 35,000 clinical terms. The extension not only substantially 
improved the applicability of FinCG to the health domain but also initiated piloting of our language technology components in an authentic healthcare environment in the fall 2008. This lead to the re-lease of commercial language technology for Fin-nish health records (Lingsoft 2010).   Data Finnish  Swedish  Total number of patients  514  379  Total number of  tokens,  types (unique tokens) and types after processing 
 1,227,909 63,328 38,649 
 1,959,271 - 41,883 Number of tokens per patient: Minimum Maximum Average Standard deviation 
 540 14,118 2,389 1,635   
 92 36,830 5,169 5,271 Total number of  daily documents and shifts  5,915 17,103  4,700 ? Number of tokens per daily document: Minimum Maximum Average Standard deviation 
  0 915 208 87 
  5 9,389 417 239  Table 2. Comparison of Finnish and Swedish ICU data sets: total amount of text per patient. A daily document, i.e. nursing narrative, contains all text written about a given patient during a calendar day.  Finnish n ? Swedish  n = Hemodynamics  7,800  Respiratory  11,301  Consciousness  6,900  Circulation 10,630  Relatives  5,700  Elimination  10,041  Diuresis  5,400  Nutrition  8,258  Breathing  4,500  Communication  5,880  Oxygenation  3,600  Event Time  5,681  Other  3,200  Pain  4,732  Excretion  590  Psychosocial  4,682  Hemodialysis  370  Sleep  4,438  Pulse  160  Skin  4,402  Skin  160  Activity  3,794   Table 3. Comparison of Finnish and Swedish ICU data sets: the most common headings. For the Finnish data, where default headings were not given, we approxi-mated the amount of heading by using an automated heuristics followed by manual combination of headings with the same meaning. 
58
For Swedish, GTA handles unknown words dif-ferently than FinCG. However, by comparing the ICU words with a Swedish general language cor-pus (PAROLE, Gellerstam et al (2000)), we found that 69% of the types are not included in PAROLE, which indicates a need for tailoring GTA (or simi-lar tools for Swedish) with domain-specific ICU terms.  4 Conclusions The purpose of this study was to do content and lexical analysis of nursing narratives written in an ICU. Our findings are that, even though the Fin-nish and Swedish languages are not linguistically closely related, the way of writing clinical nursing ICU narratives in both countries is very similar. Moreover, the written context made sentences clear for content experts, even though the texts were full of specialized jargon, misspellings, ab-breviations, and missing subjects and objects. However, these characteristics make clinical text challenging for language technology. For example coreference resolution as in the case of noradrena-lin. We have also shown that the content characteris-tics of Finnish and Swedish ICU nursing narratives are very similar. This implies that developing tools for documentation support in ICUs is not country or language dependant in that respect. Developing such tools may improve possibilities for informa-tion extraction and text mining, enabling the possi-bilities to reuse the vast amounts of important practice-based information and evidence captured in clinical narratives. The framework we have in-troduced here could easily be employed in other studies of clinical texts. 6 Future work In the future, we will use the results of this study in developing language technology for Finnish, Swe-dish and other Nordic ICU narratives. We will study how to identify abbreviations, misspellings and normalize and correct them, by using various distance measures and concept management tech-niques. We will also study how to automatically identify important parts of text and highlight them. Furthermore, we are interested in studying text provenance and pragmatics in this particular set-ting. In addition, we will evaluate the influence of 
these technology components in clinical practice. We will also address similarities and differences in clinical text written by various professional groups or at other hospital wards and health care units. Finally, we are eager to seek possibilities to incor-porate laymen's information needs and their inter-action with health care providers into our study. Acknowledgments We would like to thank Nordforsk and the Nordic Council of Ministers for the funding of our research network HEXAnord ? HEalth teXt Analysis network in the Nordic and Baltic countries and NICTA, funded by the Australian Government as represented by the De-partment of Broadband, Communications and the Digi-tal Economy and the Australian Research Council through the ICT Centre of Excellence program. We would also like to thank the Department of Information Technology and TUCS, University of Turku, Finland. References Lars Borin, Natalia Grabar, Catalina Hallett, Davis Hardcastle, Maria Toporowska Gronostaj, Dimitrios Kokkinakis, Sandra Williams and Alistair Willis. 2007. Empowering the patient with language tech-nology. SemanticMining NoE 507505: Deliverable D27.2. <http://gup.ub.gu.se/gup/record/index.xsql?pubid=53590>  Grace Yuet-Chee Chung. 2009. Towards identifying intervention arms in randomized controlled trials: ex-tracting coordinating constructions.  Journal of Bio-medical Informatics. 42(5):790?800  Hercules Dalianis, Martin Hassel and Sumithra Velupil-lai. 2009. The Stockholm EPR Corpus - Characteris-tics and Some Initial Findings. Proceedings of ISHIMR 2009, Evaluation and implementation of e-health and health information initiatives: interna-tional perspectives. 14th International Symposium for Health Information Management Research, Kal-mar, Sweden, 14-16 October, 2009, pp 243-249, pdf. Awarded best paper.  Anna Ehrenberg, Margareta Ehnfors and Ingrid Thorell-Ekstrand. 1996. Nursing documentation in patient re-cords: experience of the use of the VIPS model. Journal of Advanced Nursing 24, 853?867.  Martin Gellerstam, Yvonne Cederholm, and Torgny Rasmark. The bank of Swedish. In: Proceedings of LREC 2000 -- The 2nd International Conference on Language Resources and Evaluation, pages 329?333, Athens, Greece.  Udo Hahn and Joachim Wermter. 2004. High-performance tagging on medical texts. Proceedings of the 20th international conference on Computa-tional Linguistics. Geneva, Switzerland.  
59
Henk Harkema, Dowling JN, Thornblade T, Chapman WW. 2009. ConText: an algorithm for determining negation, experiencer, and temporal status from clin-ical reports. Journal of Biomedical Informatics 2009;42(5):839?51.  Ragnhild Helles?. 2005. Information handling in the nursing discharge notes, Journal of Clinical Nursing, Volume 15 Issue 1, 11 - 21. Blackwell publishing  Gunl?g Josefsson. 1999. F? feber eller tempa? N?gra tankar om agentivitet i medicinskt fackspr?k, Alla tiders spr?k: en v?nskrift till Gertrud Pettersson. Pages 127. Institutionen f?r nordiska spr?k. Lund. (In Swedish)  Hyeoneui Kim, Sergey Goryachev, Craciela Rosemblat, Allen Browne, Alla Keselman and Qing Zeng-Treitler. 2007. Beyond surface characteristics: a new health text-specific readability measurement. AMIA Annual Symp. 11:418-22.  Ola Knutsson, Johnny Bigert, and Vigg Kann. 2003. A robust shallow parser for Swedish. In Proceedings 14th Nordic Conf. on Comp. Ling. NODALIDA. Sirkka Lauri and Sanna Salanter?;. 2002. Developing an instrument to measure and describe clinical decision making in different nursing fields. Journal of Profes-sional Nursing. Mar-Apr;18(2), 93-100.  Lingsoft. 2010, Lingsoft Oy, http://www.lingsoft.fi/  Clement J. McDonald. 1997. The Barriers to Electronic Medical Record Systems and How to Overcome Them. JAMIA. 1997;4:213?221.   St?phane M. Meystre, Guergana K. Savova, Karin C. Kipper-Schuler and John E. Hurdle. 2008. Extracting Information from Textual Documents in the Elec-tronic Health Record: a Review of Recent Research. Yearbook Med Inform. 2008:128-44.  Raymond L. Ownby 2005. Influence of Vocabulary and Sentence Complexity and Passive Voice on the Readability of Consumer-Oriented Mental Health In-formation on the Internet. AMIA Annual Symposium Proceedings. 2005: 585?588. Serguei V. S. Pakhomov, Anni Coden and Christopher G. Chute. 2006. Developing a corpus of clinical notes manually annotated for part-of-speech. International Journal of Medical Informatics. 2006 Jun;75(6):418-29. Epub 2005 Sep 19. Patientdatalagen (2008:355) Svensk f?rfattnings-samling, Socialdepartementet, 2008, Stockholm. (In Swedish)  Hanna Suominen. 2009. Machine Learning and Clinical Text: Supporting Health Information Flow. TUCS Dissertations No 125, Turku Centre for Computer Science, 2009, Turku, Finland.  Hanna Suominen, Helj? Lundgr?n-Laine, Sanna Salan-ter?, Helena Karsten, and Tapio Salakoski. 2009. In-formation flow in intensive care narratives. In Chen J, Chen C, Ely J, Hakkani-Tr D, He J, Hsu H.-H, Liao L, Liu C, Pop  M, Ranganathan S, Reddy C.K, 
Ruan J, Song Y, Tseng V.S, Ungar L, Wu D, Wu Z, Xu K, Yu H, Zelikovsky A, editors. Proceedings IEEE International Conference on Bioinformatics and Biomedicine Workshops, BIBM 2009, pages 325?330. Institute of Electrical and Electronics Engi-neers, Los Alamitos, California, USA.  Kaarina Tanttu and Helena Ikonen. 2007. Nationally standardized electronic nursing documentation in Finland by the year 2007. Stud Health Technol In-form.122:540-1.  Asta Thoroddsen, Kaija Saranto, Anna Ehrenberg, Wal-ter Sermeus. 2009. Models, standards and structures of nursing documentation in European countries. Stud Health Technol Inform.146:327-31.  Katrin Tomanek, Joachim Wermter and Udo Hahn. 2007. A Reappraisal of sentence and token splitting for life sciences documents. Stud Health Technol In-form. 129 (Pt 1):524-8. Webster?s 2010. Webster?s New World Medical Dic-tionary. http://www.medterms.com/script/main/hp.asp,last visited February 2, 2010. 
60
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 84?91,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Uncertainty Detection as Approximate Max-Margin Sequence Labelling
Oscar Ta?ckstro?m
SICS / Uppsala University
Kista / Uppsala, Sweden
oscar@sics.se
Gunnar Eriksson
SICS
Kista, Sweden
guer@sics.se
Sumithra Velupillai
DSV, Stockholm University
Kista, Sweden
sumithra@dsv.su.se
Hercules Dalianis
DSV, Stockholm University
Kista, Sweden
hercules@dsv.su.se
Martin Hassel
DSV, Stockholm University
Kista, Sweden
xmartin@dsv.su.se
Jussi Karlgren
SICS
Kista, Sweden
jussi@sics.se
Abstract
This paper reports experiments for the
CoNLL-2010 shared task on learning to
detect hedges and their scope in natu-
ral language text. We have addressed
the experimental tasks as supervised lin-
ear maximum margin prediction prob-
lems. For sentence level hedge detection
in the biological domain we use an L1-
regularised binary support vector machine,
while for sentence level weasel detection
in the Wikipedia domain, we use an L2-
regularised approach. We model the in-
sentence uncertainty cue and scope de-
tection task as an L2-regularised approxi-
mate maximum margin sequence labelling
problem, using the BIO-encoding. In ad-
dition to surface level features, we use a
variety of linguistic features based on a
functional dependency analysis. A greedy
forward selection strategy is used in ex-
ploring the large set of potential features.
Our official results for Task 1 for the bio-
logical domain are 85.2 F1-score, for the
Wikipedia set 55.4 F1-score. For Task 2,
our official results are 2.1 for the entire
task with a score of 62.5 for cue detec-
tion. After resolving errors and final bugs,
our final results are for Task 1, biologi-
cal: 86.0, Wikipedia: 58.2; Task 2, scopes:
39.6 and cues: 78.5.
1 Introduction
This paper reports experiments to detect uncer-
tainty in text. The experiments are part of the two
shared tasks given by CoNLL-2010 (Farkas et al,
2010). The first task is to identify uncertain sen-
tences; the second task is to detect the cue phrase
which makes the sentence uncertain and to mark
its scope or span in the sentence.
Uncertainty as a target category needs to be ad-
dressed with some care. Sentences, utterances,
statements are not uncertain ? their producer, the
speaker or author, is. Statements may explicitly
indicate this uncertainty, employing several differ-
ent linguistic and textual mechanisms to encode
the speaker?s attitude with respect to the verac-
ity of an utterance. The absence of such markers
does not necessarily indicate certainty ? the oppo-
sition between certain and uncertain is not clearly
demarkable, but more of a dimensional measure.
Uncertainty on the part of the speaker may be dif-
ficult to differentiate from a certain assessment of
an uncertain situation, It is unclear whether this
specimen is an X or a Y vs. The difference between
X and Y is unclear.
In this task, the basis for identifying uncertainty
in utterances is almost entirely lexical. Hedges,
the main target of this experiment, are an estab-
lished category in lexical grammar analyses - see
e.g. Quirk et al (1985), for examples of English
language constructions. Most languages use vari-
ous verbal markers or modifiers for indicating the
speaker?s beliefs in what is being said, most proto-
typically using conditional or optative verb forms,
Six Parisiens seraient morts, or auxiliaries, This
mushroom may be edible, but aspectual markers
may also be recruited for this purpose, more indi-
rectly, I?m hoping you will help vs. I hope you will
help; Do you want to see me now vs. Did you want
to see me now. Besides verbs, there are classes
of terms that through their presence, typically in
an adverbial role, in an utterance make explicit
its tentativeness: possibly, perhaps... and more
complex constructions with some reservation, es-
pecially such that explicitly mention the speaker
and the speaker?s beliefs or doubts, I suspect that
X.
Weasels, the other target of this experiment,
on the other hand, do not indicate uncertainty.
84
Weasels are employed when speakers attempt to
convince the listener of something they most likely
are certain of themselves, by anchoring the truth-
fulness of the utterance to some outside fact or au-
thority (Most linguists believe in the existence of
an autonomous linguistic processing component),
but where the authority in question is so unspecific
as not to be verifiable when scrutinised.
We address both CoNLL-2010 shared tasks
(Farkas et al, 2010). The first, detecting uncer-
tain information on a sentence level, we solve by
using an L1-regularised support vector machine
with hinge loss for the biological domain, and
an L2-regularised maximum margin model for the
Wikipedia domain. The second task, resolution of
in-sentence scopes of hedge cues, we approach as
an approximate L2-regularized maximum margin
structured prediction problem. Our official results
for Task 1 for the biological domain are 85.2 F1-
score, for the Wikipedia set 55.4 F1-score. For
Task 2, our official results were 2.1 for the entire
task with a score of 62.5 for cue detection. After
resolving errors and unfortunate bugs, our final re-
sults are for Task 1, biological: 86.0, Wikipedia:
58.2; Task 2: 39.6 and 78.5 for cues.
2 Detecting Sentence Level Uncertainty
On the sentence level, word- and lemma-based
features have been shown to be useful for uncer-
tainty detection (see e.g. Light et al (2004), Med-
lock and Briscoe (2007), Medlock (2008), and
Szarvas (2008)). Medlock (2008) and Szarvas
(2008) employ probabilistic, weakly supervised
methods, where in the former, a stemmed single
term and bigram representation achieved best re-
sults (0.82 BEP), and in the latter, a more complex
n-gram feature selection procedure was applied
using a Maximum Entropy classifier, achieving
best results when adding reliable keywords from
an external hedge keyword dictionary (0.85 BEP,
85.08 F1-score on biomedical articles). More lin-
guistically motivated features are used by Kil-
icoglu and Bergler (2008), such as negated ?un-
hedging? verbs and nouns and that preceded by
epistemic verbs and nouns. On the fruit-fly dataset
(Medlock and Briscoe, 2007) they achieve 0.85
BEP, and on the BMC dataset (Szarvas, 2008) they
achieve 0.82 BEP. Light et al (2004) also found
that most of the uncertain sentences appeared to-
wards the end of the abstract, indicating that the
position of an uncertain sentence might be a use-
ful feature.
Ganter and Strube (2009) consider weasel tags
in Wikipedia articles as hedge cues, and achieve
results of 0.70 BEP using word- and distance
based features on a test set automatically derived
from Wikipedia, and 0.69 BEP on a manually an-
notated test set using syntactic patterns as fea-
tures. These results suggest that syntactic features
are useful for identifying weasels that ought to be
tagged. However, evaluation is performed on bal-
anced test sets, which gives a higher baseline.
2.1 Learning and Optimization Framework
A guiding principle in our approach to this shared
task has been to focus on highly computationally
efficient models, both in terms of training and pre-
diction times. Although kernel based non-linear
separators may sometimes obtain better predic-
tion performance, compared to linear models, the
speed penalty at prediction time is often substan-
tial, since the number of support patterns often
grows linearly with the size of the training set. We
therefore restrict ourselves to linear models, but
allow for a restricted family of explicit non-linear
mappings by feature combinations.
For sentence level hedge detection in the bio-
logical domain, we employ an L1-regularised sup-
port vector machine with hinge loss, as provided
by the library implemented by Fan et al (2008),
while for weasel detection in the Wikipedia do-
main, we instead use the L2-regularised maximum
margin model described in more detail in section
3.1. In both cases, we approximately optimise the
F1-measure by weighting each class by the inverse
of its proportion in the training data.
The reason for using L1-regularisation in the bi-
ological domain is that the annotation is heavily
biased towards a rather small number of lexical
cues, making most of the potential surface features
irrelevant. The Wikipedia weasel annotation, on
the other hand, is much more noisy and less de-
termined by specific lexical markers. Regularising
with respect to the L1-norm is known to give pref-
erence to sparse models and for the special case
of logistic regression, Ng (2004) proved that the
sample complexity grows only logarithmically in
the number of irrelevant features, instead of lin-
early as when regularising with respect to the L2-
norm. Our preliminary experiments indicated that
L1-regularisation is superior to L2-regularisation
in the biological domain, while slightly inferior in
85
the Wikipedia domain.
2.2 Feature Definitions
The asymmetric relationship between certain and
uncertain sentences becomes evident when one
tries to learn this distinction based on surface level
cues. While the UNCERTAIN category is to a large
extent explicitly anchored in lexical markers, the
CERTAIN category is more or less defined implic-
itly as the complement of the UNCERTAIN cate-
gory. To handle this situation, we use a bias fea-
ture to model the weight of the CERTAIN category,
while explicit features are used to model the UN-
CERTAIN category.
The following list describes the feature tem-
plates explored for sentence level uncertainty de-
tection. Some features are based on a linguistic
analysis by the Connexor Functional Dependency
(FDG) parser (Tapanainen and Ja?rvinen, 1997).
SENLEN Preliminary experiments indicated that taking sen-
tence length into account is beneficial. We incorporate
this by using three different bias terms, according to the
length (in tokens) of the sentences. This feature takes
the following values: S < 18 ? M ? 32 < L.
DOCPT Document part, e.g., TITLE, ABSTRACT and BODY
TEXT, allowing for different models for different docu-
ment parts.
TOKEN, LEMMA Tokens in most cases equals words, but
may in some special cases also be multiword units, e.g.
of course, as defined by the FDG tokenisation. Lemmas
are base forms of words, with some special features
introduced for numeric tokens, e.g., year, short number,
and long number.
QUANT Syntactic function of a noun phrase with a quanti-
fier head (at least some of the isoforms are conserved
between mouse and humans), or a modifying quantifier
(Recently, many investigators have been interested in
the study on eosinophil biology).
HEAD, DEPREL Functional dependency head of the token,
and the type of dependency relation between the head
and the token, respectively.
SYN Phrase-level and clause-level syntactic functions of a
word.
MORPH Part-of-speech and morphological traits of a word.
Each feature template defines a set of features
when applied to data. The TOKEN, LEMMA,
QUANT, HEAD, DEPREL templates yield single-
ton sets of features for each token, while the SYN
and MORPH templates extends to sets consisting
of several features for each token. A sentence is
represented as the union of all active token level
features and the SENLEN and DOCPT, if active.
In addition to the linear combination of concrete
features, we allow combined features by the Carte-
sian product of the feature set extensions of two or
more feature templates.
2.3 Feature Template Selection
Although regularised maximum margin models
often cope well even in the presence of irrelevant
features, it is a good idea to search the large set of
potential features for an optimal subset.
In order to make this search feasible we make
two simplifications. First, we do not explore the
full set of individual features, but instead the set of
feature templates, as defined above. Second, we
perform a greedy search in which we iteratively
add the feature template that gives the largest per-
formance improvement, when added to the cur-
rent optimal set of templates. The performance of
a feature set for sentence level detection is mea-
sured as the mean F1-score, with respect to the
UNCERTAIN class, minus one standard deviation
? the mean and standard deviation are computed
by three fold cross-validation on the training set.
We subtract one standard deviation from the mean
in order to promote stable solutions over unstable
ones.
Of course, these simplifications do not come for
free. The solution of the optimisation problem
might be quite unstable with respect to the optimal
hyper-parameters of the learning algorithm, which
in turn may depend on the feature set used. This
risk could be reduced by conducting a more thor-
ough parameter search for each candidate feature
set, however, this was simply too time consuming
for the present work. A further risk of using for-
ward selection is that feature interactions are ig-
nored. This issue is handled better with backward
elimination, but that is also more time consuming.
The full set of explored feature templates is too
large to be listed here; instead we list the features
selected in each iteration of the search, together
with their corresponding scores, in Table 1.
3 Detecting In-sentence Uncertainty
When it comes to the automatic identification of
hedge cues and their linguistic scopes, Morante
and Daelemans (2009) and O?zgu?r and Radev
(2009) report experiments on the BioScope cor-
pus (Vincze et al, 2008), achieving best results
(10-fold cross evaluation) on the identification of
hedge cues of 71.59 F-score (using IGTree with
current, preceding and subsequent word and cur-
86
Task Template set Dev F1 Test F1
Bio
SENLEN - -
? LEMMA 88.9 (.25) 78.79
? LEMMABI 90.3 (.19) 85.86
? LEMMA?QUANT 90.3 (.07) 85.97
Wiki
SENLEN - -
? TOKEN?DOCPT 59.0 (.76) 60.12
? TOKENBI?SENLEN 59.9 (.09) 58.26
Table 1: Top feature templates for sentence level
hedge and weasel detection.
rent lemma as features) and 82.82 F-score (using a
Support Vector Machine classifier and a complex
feature set including keyword and dependency re-
lation information), respectively. On the task of
automatic scope resolution, best results are re-
ported as 59.66 (F-score) and 61.13 (accuracy),
respectively, on the full paper subset. O?zgu?r and
Radev (2009) use a rule-based method for this sub-
task, while Morante and Daelemans (2009) use
three different classifiers as input to a CRF-based
meta-learner, with a complex set of features, in-
cluding hedge cue information, current and sur-
rounding token information, distance information
and location information.
3.1 Learning and Optimisation Framework
In recent years, a wide range of different ap-
proaches to general structured prediction prob-
lems, of which sequence labelling is a special
case, have been suggested. Among others, Con-
ditional Random Fields (Lafferty et al, 2001),
Max-Margin Markov Networks (Taskar et al,
2003), and Structured Support Vector Machines
(Tsochantaridis et al, 2005). A drawback of
these approaches is that they are all quite com-
putationally demanding. As an alternative, we
propose a much more computationally lenient ap-
proach based on the regularised margin-rescaling
formulation of Taskar et al (2003), which we in-
stead optimise by stochastic subgradient descent
as suggested by Ratliff et al (2007). In addi-
tion we only perform approximate decoding, us-
ing beam search, which allows arbitrary complex
joint feature maps to be employed, without sacri-
ficing speed.
3.1.1 Technical Details
Let X denote the pattern set and let Y denote the
set of structured labels. Let A denote the set of
atomic labels and let each label y ? Y consist of
an indexed sequence of atomic labels yi ? A. De-
note by Yx ? Y the set of possible label assign-
ments to pattern x ? X and by yx ? Yx its cor-
rect label. In the specific case of BIO-sequence
labelling, A = {BEGIN, INSIDE, OUTSIDE} and
Yx = A|x|, where |x| is the length of the sequence
x ? X .
A structured classification problem amounts
to learning a mapping from patterns to labels,
f : X 7? Y , such that the expected loss
EX?Y [?(yx, f(x))] is minimised. The prediction
loss, ? : Y ? Y 7? <+, measures the loss of
predicting label y = f(x) when the correct la-
bel is yx, with ?(yx, yx) = 0. Here we assume
the Hamming loss, ?H(y, y?) = ?|y|i=1 ?(yi, y?i),
where ?(yi, y?i) = 1 if yi 6= y?i and 0 otherwise.
The idea of the margin-rescaling approach is to
let the structured margin between the correct label
yx and a hypothesis y ? Yx scale linearly with the
prediction loss ?(yx, y) (Taskar et al, 2003). The
structured margin is defined in terms of a score
function S : X ? Y 7? <, in our case the linear
score function S(x, y) = wT?(x, y), where w ?
<m is a vector of parameters and? : X?Y 7? <m
is a joint feature function. The learning problem
then amounts to finding parameters w such that
S(x, yx) ? S(x, y) + ?(yx, y) for all y ? Yx \
{yx} over the training data D. In other words, we
want the score of the correct label to be higher than
the score plus the loss, of all other labels, for each
instance. In order to balance margin maximisation
and margin violation, we add theL2-regularisation
term ?w?2.
By making use of the loss augmented decoding
function
f?(x, yx) = argmax
y?Yx
[S(x, y) + ?(yx, y)] , (1)
we get the following regularised risk functional:
Q?,D(w) =
|D|?
i=1
S?(x(i), yx(i)) + ?2 ?w?
2, (2)
where
S?(x, yx) = maxy?Yx [S(x, y) + ?(yx, y)]?S(x, yx)
(3)
We optimise (2) by stochastic approximate subgra-
dient descent with step size sequence [?0/?t]?t=1
(Ratliff et al, 2007). The initial step size ?0
and the regularisation factor ? are data depen-
dent hyper-parameters, which we tune by cross-
validation.
87
This framework is highly efficient both at learn-
ing and prediction time. Training cues and scopes
on the biological data, takes about a minute, while
prediction times are in the order of seconds, using
a Java based implementation on a standard laptop;
the absolute majority of that time is spent on read-
ing and extracting features from an inefficient in-
ternal JSON-based format.
3.1.2 Hashed Feature Functions
Joint feature functions enable encoding of depen-
dencies between labels and relations between pat-
tern and label. Most feature templates are de-
fined based on input only, while some are de-
fined with respect to output features as well. Let
?(x, y1:i?1, i) ? <m denote the joint feature func-
tion corresponding to the application of all active
feature templates to pattern x ? X and partially
decoded label y1:i?1 ? Ai?1 when decoding at
position i. The feature mapping used in scoring
candidate label yi ? A is then computed as the
Cartesian product ?(x, y, i) = ?(x, y1:i?1, i) ?
?(yi), where ?(yi) ? <m is a unique unitary fea-
ture vector representation of label yi. The feature
representation for a complete sequence x and its
associated label y is then computed as
?(x, y) =
|x|?
i=1
?(x, y, i)
When employing joint feature functions and com-
bined features, the number of unique features may
grow very large. This is a problem when the
amount of internal memory is limited. Feature
hashing, as described by Weinberger et al (2009),
is a simple trick to circumvent this problem. As-
sume that we have an original feature function
? : X ? Y 7? <m, where m might be arbitrar-
ily large. Let h : N+ 7? [1, n] be a hash function
and let h?1(i) ? [1,m] be the set of integers such
that j ? h?1(i) iff h(j) = i. We now use this
hash function to map the index of each feature in
?(x, y) to its corresponding index in ?(x, y), as
?i(x, y) =?j?h?1(i) ?j(x, y). The features in ?
are thus unions of multisets of features in ?. Given
a hash function with good collision properties, we
can expect that the subset of features mapped to
any index in?(x, y) is small and composed of ele-
ments drawn at random from ?(x, y). Weinberger
et al (2009) contains proofs of bounds on these
distributions. Furthermore, by using a k-valued
hash function h : Nk 7? [1, n], the Cartesian prod-
uct of k feature sets can be computed much more
efficiently, compared to using a dictionary.
3.2 Position Based Feature Definitions
For in-sentence cue and scope prediction we make
use of the same token level feature templates as
for sentence level detection. An additional level
of expressivity is added in that each token level
template is associated with a token position. A
template is addressed either relative to the token
currently being decoded, or by the dependency arc
of a token, which in turn is addressed by a relative
position. The addressing can be either to a single
position, or a range of positions. Feature templates
may further be defined with respect to features of
the input pattern, the token level labels predicted
so far, or with respect to combinations of input
and label features. Joint features, just as complex
feature combinations, are created by forming the
Cartesian product of an input feature set and a la-
bel feature set.
The feature templates are instantiated by pre-
fixing the template name to each member of the
feature set. To exemplify, the single position tem-
plate TOKENi, given that the token currently be-
ing decoded at position i is suggests, is instanti-
ated as the singleton set {TOKENi = suggests}.
The range template TOKENi,i+1, given that the
current token is suggests and the next token is
that, is instantiated as the set {TOKENi,i+1 =
suggests, TOKENi,i+1 = that}; i.e. each member
of the set is prefixed by the range template name.
In addition to the token level templates used for
sentence level prediction, the following templates
were explored:
LABEL Label predicted so far at the addressed position(s).
HEAD.X An arbitrary feature, X, addressed by follow-
ing the dependency arc(s) from the addressed posi-
tion(s). For example, HEAD.LEMMAi corresponds to
the lemma found by looking at the dependency head of
the current token.
CUE, CUESCOPE Whether the token(s) addressed is re-
spectively, a cue marker, or within the syntactic scope
of the current cue, following the definition of scope
provided by Vincze et al (2008).
3.3 Feature Template Selection
Just as with sentence level detection, we used a
greedy forward selection strategy when searching
for the optimal subset of feature templates. The
cue and scope detection subtasks were optimised
separately.
88
The scoring measures used in the search for
cue and scope detection features differ. In order
to match the official scoring measure for cue de-
tection, we optimise the F1-score of labels cor-
responding to cue tags, i.e. we treat the BEGIN
and INSIDE cue tags as an equivalence class. The
official scoring measure for scope prediction, on
the other hand, corresponds to the exact match
of scope boundaries. Unfortunately using exact
match performance turned out to be not very well
suited for use in greedy forward selection. This
is because before a sufficient per token accuracy
has been reached, and even when it has, the ex-
act match score may fluctuate wildly. Therefore,
as a substitute, we instead guide the search by to-
ken level accuracy. This discrepancy between the
search criterion and the official scoring metric is
unfortunate.
Again, when taking into account position ad-
dressing, joint features and combined features, the
complete set of explored templates is too large to
fit in the current experiment. The selected features
together with their corresponding scores are found
in Table 2.
Task Template set Dev F1 Test F1
Cue
TOKENi 74.0 (1.5) -
? TOKENi?1 81.0 (.30) 68.78
? MORPHi 83.6 (.10) 74.06
? LEMMAi ? LEMMAi+1 85.6 (.20) 78.41
? SYNi 86.5 (.41) 78.28
? LEMMAi?1 ? LEMMAi 86.7 (.42) 78.52
Scope
CueScopei 66.9 (.92) -
? LABELi?2,i?1 79.5 (.67) 34.80
? LEMMAi 82.4 (1.1) 33.18
? MORPHi 83.1 (.35) 35.70
? CUEi?2,i?1 83.4 (.13) 40.14
? CUEi,i+1,i+2 83.6 (.11) 41.15
? LEMMAi?1 84.1 (.16) 40.04
? MORPHi 84.4 (.33) 40.04
? TOKENi+1 84.5 (.09) 39.64
Table 2: Top feature templates for in-sentence de-
tection of hedge cues and scopes.
4 Discussion
Our final F1-score results for the corrected system
are, in Task 1 for the biological domain 85.97, for
the Wikipedia domain 58.25; for Task 2, our re-
sults are 39.64 for the entire task with a score of
78.52 for cue detection.
Any gold standard-based shared experiment un-
avoidably invites discussion on the reliability of
the gold standard. It is easy to find borderline ex-
amples in the evaluation corpus, e.g. sentences
that may just as well be labeled ?certain? rather
than ?uncertain?. This gives an indication of the
true complexity of assessing the hidden variable of
uncertainty and coercing it to a binary judgment
rather than a dimensional one. It is unlikely that
everyone will agree on a binary judgment every
time.
To improve experimental results and the gen-
eralisability of the results for the task of detect-
ing uncertain information on a sentence level, we
would need to break reliance on the purely lexical
cues. For instance, we now have identified possi-
ble and putative as markers for uncertainty, but in
many instances they are not (Finally, we wish to
ensure that others can use and evaluate the GREC
as simply as possible). This would be avoidable
through either a deeper analysis of the sentence
to note that possible in this case does not modify
anything of substance in the sentence, or alterna-
tively through a multi-word term preprocessor to
identify as simply as possible as an analysis unit.
In the Wikipedia experiment, where the objec-
tive is to identify weasel phrases, the judicious en-
coding of quantifiers such as ?some of the most
well-known researchers say that X? would be
likely to identify the sought-for sentences when
the quantified NP is in subject position. In our
experiment we find that our dependency analysis
did not distinguish between the various syntactic
roles of quantified NPs. As a result, we marked
several sentences with a quantifier as a ?weasel?
sentence, even where the quantified NP was in a
non-subject role ? leading to overly many weasel
sentences. An example is given in Table 3.
If certainty can be identified separately, not as
absence of overt uncertainty, identifying uncer-
tainty can potentially be aided through the iden-
tification of explicit certainty together with nega-
tion, as found by Kilicoglu and Bergler (2008). In
keeping with their results, we found negations in a
sizeable proportion of the annotated training mate-
rial. Currently we capture negation as a lexical cue
in immediate bigrams, but with longer range nega-
tions, we will miss some clear cases: Table 3 gives
two examples. To avoid these misses, we will both
need to identify overt expressions of certainty and
to identify and track the scope of negation ? the
first challenge is unexplored but would not seem
to be overly complex; the second is a well-known
89
and established challenge for NLP systems in gen-
eral.
In the task of detecting in-sentence uncertainty
? identification of hedge cues and their scopes ?
we find that an evaluation method based on ex-
act match of a token sequence is overly unforgiv-
ing. There are many cases where the marginal to-
kens of a sequence are less than central or irrele-
vant for the understanding of the hedge cue and its
scope: moving the boundary by one position over
an uninteresting token may completely invalidate
an otherwise arguably correct analysis. A token-
by-token scoring would be a more functional eval-
uation criterion, or perhaps a fuzzy match, allow-
ing for a certain amount of erroneous characters.
For our experiments, this has posed some chal-
lenges. While we model the in-sentence un-
certainty detection as a sequence labelling prob-
lem in the BIO-representation (BEGIN, INSIDE,
OUTSIDE), the provided corpus uses an XML-
representation. Moreover, the official scoring tool
requires that the predictions are well formed XML,
necessitating a conversion from XML to BIO prior
to training and from BIO to XML after prediction.
Consistent tokenisation is important, but the syn-
tactic analysis components used by us distorted the
original tokenisation and restoring the exact same
token sequence proved problematic.
Conversion from BIO to XML is straightforward
for cues, while some care must be taken when an-
notating scopes, since erroneous scope predictions
may result in malformed XML. When adding the
scope annotation, we use a stack based algorithm.
For each sentence, we simultaneously traverse the
scope-sequence corresponding to each cue, left to
right, token by token. The stack is used to en-
sure that scopes are either separated or nested and
an additional restriction ensures that scopes may
never start or end inside a cue. In case the al-
gorithm fails to place a scope according to these
restrictions, we fall back and let the scope cover
the whole sentence. Several of the more frequent
errors in our analyses are scoping errors, many
likely to do with the fallback solution. Our analy-
sis quite frequently fails also to assign the subject
of a sentence to the scope of a hedging verb. Ta-
ble 3 shows one example each of these errors ?
overextended scope and missing subject.
Unfortunately, the tokenisation output by our
analysis components is not always consistent with
the tokenisation assumed by the BioScope annota-
tion. A post-processing step was therefore added
in which each, possibly complex, token in the pre-
dicted BIO-sequence is heuristically mapped to its
corresponding position in the XML structure. This
post-processing is not perfect and scopes and cues
at non-word token boundaries, such as parenthe-
ses, are quite often misplaced with respect to the
BioScope annotation. Table 3 gives one example
which is scored ?erroneous? since the token ?(63)?
is in scope, where the ?correct? solution has it out-
side the scope. These errors are not important to
address, but are quite frequent in our results ? ap-
proximately 80 errors are of this type.
To achieve more general and effective methods
to detect uncertainty in an argument, we should
note that uncertainty is signalled in a text through
many mechanisms, and that the purely lexical and
explicit signal found through the present experi-
ments in hedge identification is effective and use-
ful, but will not catch everything we might want to
find. Lexical approaches are also domain depen-
dent. For instance, Szarvas (2008) and Morante
and Daelemans (2009) report loss in performance,
when applying the same methods developed on bi-
ological data, on clinical text. Using the systems
developed for scientific text elsewhere poses a mi-
gration challenge. It would be desirable both to
automatically learn a hedging lexicon from a gen-
eral seed set and to have features on a higher level
of abstraction.
Our main result is that casting this task as a se-
quence labelling problem affords us the possibility
to combine linguistic analyses with a highly effi-
cient implementation of a max-margin prediction
algorithm. Our framework processes the data sets
in minutes for training and seconds for prediction
on a standard personal computer.
5 Acknowledgements
The authors would like to thank Joakim Nivre
for feedback in earlier stages of this work. This
work was funded by The Swedish National Grad-
uate School of Language Technology and by the
Swedish Research Council.
References
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learn-
ing Research, 9:1871?1874.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
90
Neg + certain However, how IFN-? and IL-4 inhibit IL-17 production is not yet known.
Neg + certain The mechanism by which Tregs preserve peripheral tolerance is still not entirely clear.
?some?: not weasel Tourist folks usually visit this peaceful paradise to enjoy some leisurenonsubj .
?some?: weasel Somesubj suggest that the origin of music likely stems from naturally occurring sounds and rhythms.
Prediction dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR? because
dRas85DV12 can activate endogenous PI3K signaling [16]</xcope>.
Gold standard dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR?</xcope> because
dRas85DV12 can activate endogenous PI3K signaling [16].
Prediction However, the precise molecular mechanisms of Stat3-mediated expression of ROR?t
<xcope .1>are still <cue .1>unclear</cue></xcope>.
Gold standard However, <xcope .1>the precise molecular mechanisms of Stat3-mediated expression of ROR?t
are still <cue .1>unclear</cue></xcope>.
Prediction Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit ROR?t
activity on its target genes, at least in par,t through direct interaction with ROR?t (63)</xcope>.
Gold standard Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit RORt
activity on its target genes, at least in par,t through direct interaction with RORt</xcope> (63).
Table 3: Examples of erroneous analyses.
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their Scope
in Natural Language Text. In Proceedings of the 14th
Conference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 1?12, Uppsala,
Sweden, July. Association for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding hedges
by chasing weasels: hedge detection using Wikipedia tags
and shallow linguistic features. In ACL-IJCNLP ?09: Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Halil Kilicoglu and Sabine Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a linguis-
tically motivated perspective. BMC Bioinformatics, 9.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data. In
Proc. 18th Int. Conf. on Machine Learning. Morgan Kauf-
mann Publishers.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and state-
ments in between. In Lynette Hirschman and James
Pustejovsky, editors, HLT-NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, Ontologies
and Databases, Boston, USA. ACL.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature. In
Proceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Informatics,
41(4):636?654.
Roser Morante and Walter Daelemans. 2009. Learning the
scope of hedge cues in biomedical texts. In BioNLP ?09:
Proceedings of Workshop on BioNLP, Morristown, NJ,
USA. ACL.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regulariza-
tion, and rotational invariance. In ICML ?04: Proceedings
of the 21st International Conference on Machine learning,
page 78, New York, NY, USA. ACM.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detecting
speculations and their scopes in scientific text. In Pro-
ceedings of 2009 Conference on Empirical Methods in
Natural Language Processing, Singapore. ACL.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and
Jan Svartvik. 1985. A comprehensive grammar of the
English language. Longman.
Nathan D. Ratliff, Andrew J. Bagnell, and Martin A. Zinke-
vich. 2007. (Online) subgradient methods for structured
prediction. In Eleventh International Conference on Arti-
ficial Intelligence and Statistics (AIStats).
Gyo?rgy Szarvas. 2008. Hedge classification in biomedical
texts with a weakly supervised selection of keywords. In
Proceedings of ACL-08: HLT, Columbus, Ohio. ACL.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Conference
on Applied Natural Language Processing.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard Scho?lkopf, editors,
NIPS. MIT Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Jour-
nal of Machine Learning Research, 6:1453?1484.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas, Gyo?rgy
Mo?ra, and Ja?nos Csirik. 2008. The BioScope corpus:
biomedical texts annotated for uncertainty, negation and
their scopes. BMC Bioinformatics, 9(S-11).
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In ICML ?09: Proceedings
of the 26th Annual International Conference on Machine
Learning, New York, NY, USA. ACM.
91
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 5?13,
Uppsala, July 2010.
Creating and Evaluating a Consensus for Negated and Speculative Words
in a Swedish Clinical Corpus
Hercules Dalianis, Maria Skeppstedt
Department of Computer and Systems Sciences (DSV)
Stockholm University
Forum 100
SE-164 40 Kista, Sweden
{hercules, mariask}@dsv.su.se
Abstract
In this paper we describe the creation
of a consensus corpus that was obtained
through combining three individual an-
notations of the same clinical corpus in
Swedish. We used a few basic rules that
were executed automatically to create the
consensus. The corpus contains nega-
tion words, speculative words, uncertain
expressions and certain expressions. We
evaluated the consensus using it for nega-
tion and speculation cue detection. We
used Stanford NER, which is based on the
machine learning algorithm Conditional
Random Fields for the training and detec-
tion. For comparison we also used the
clinical part of the BioScope Corpus and
trained it with Stanford NER. For our clin-
ical consensus corpus in Swedish we ob-
tained a precision of 87.9 percent and a re-
call of 91.7 percent for negation cues, and
for English with the Bioscope Corpus we
obtained a precision of 97.6 percent and a
recall of 96.7 percent for negation cues.
1 Introduction
How we use language to express our thoughts, and
how we interpret the language of others, varies be-
tween different speakers of a language. This is
true for various aspects of a language, and also
for the topic of this article; negations and spec-
ulations. The differences in interpretation are of
course most relevant when a text is used for com-
munication, but it also applies to the task of anno-
tation. When the same text is annotated by more
than one annotator, given that the annotating task
is non-trivial, the resulting annotated texts will not
be identical. This will be the result of differences
in how the text is interpreted, but also of differ-
ences in how the instructions for annotation are
interpreted. In order to use the annotated texts,
it must first be decided if the interpretations by the
different annotators are similar enough for the pur-
pose of the text, and if so, it must be decided how
to handle the non-identical annotations.
In the study described in this article, we have
used a Swedish clinical corpus that was anno-
tated for certainty and uncertainty, as well as for
negation and speculation cues by three Swedish-
speaking annotators. The article describes an eval-
uation of a consensus annotation obtained through
a few basic rules for combining the three different
annotations into one annotated text.
1
2 Related research
2.1 Previous studies on detection of negation
and speculation in clinical text
Clinical text often contains reasoning, and thereby
many uncertain or negated expressions. When,
for example, searching for patients with a specific
symptom in a clinical text, it is thus important to
be able to detect if a statement about this symptom
is negated, certain or uncertain.
The first approach to identifying negations in
Swedish clinical text was carried out by Skeppst-
edt (2010), by whom the well-known NegEx algo-
rithm (Chapman et al, 2001), created for English
clinical text, was adapted to Swedish clinical text.
Skeppstedt obtained a precision of 70 percent and
a recall of 81 percent in identifying negated dis-
eases and symptoms in Swedish clinical text. The
NegEx algorithm is purely rule-based, using lists
of cue words indicating that a preceding or follow-
ing disease or symptom is negated. The English
version of NegEx (Chapman et al, 2001) obtained
a precision of 84.5 percent and a recall of 82.0 per-
cent.
1
This research has been carried out after approval from
the Regional Ethical Review Board in Stockholm (Etikprvn-
ingsnmnden i Stockholm), permission number 2009/1742-
31/5.
5
Another example of negation detection in En-
glish is the approach used by Huang and Lowe
(2007). They used both parse trees and regu-
lar expressions for detecting negated expressions
in radiology reports. Their approach could de-
tect negated expressions both close to, and also
at some distance from, the actual negation cue (or
what they call negation signal). They obtained a
precision of 98.6 percent and a recall of 92.6 per-
cent.
Elkin et al (2005) used the terms in SNOMED-
CT (Systematized Nomenclature of Medicine-
Clinical Terms), (SNOMED-CT, 2010) and
matched them to 14 792 concepts in 41 health
records. Of these concepts, 1 823 were identified
as negated by humans. The authors used Mayo
Vocabulary Server Parsing Engine and lists of cue
words triggering negation as well as words in-
dicating the scope of these negation cues. This
approach gave a precision of 91.2 percent and
a recall of 97.2 percent in detecting negated
SNOMED-CT concepts.
In Rokach et al (2008), they used clinical nar-
rative reports containing 1 766 instances annotated
for negation. The authors tried several machine
learning algorithms for detecting negated findings
and diseases, including hidden markov models,
conditional random fields and decision trees. The
best results were obtained with cascaded decision
trees, with nodes consisting of regular expressions
for negation patterns. The regular expressions
were automatically learnt, using the LCS (longest
common subsequence) algorithm on the training
data. The cascaded decision trees, built with LCS,
gave a precision of 94.4 percent, a recall of 97.4
percent and an F-score of 95.9 percent.
Szarvas (2008) describes a trial to automatically
identify speculative sentences in radiology reports,
using Maximum Entropy Models. Advanced fea-
ture selection mechanisms were used to automat-
ically extract cue words for speculation from an
initial seed set of cues. This, combined with man-
ual selection of the best extracted candidates for
cue words, as well as with outer dictionaries of
cue words, yielded an F-score of 82.1 percent for
detecting speculations in radiology reports. An
evaluation was also made on scientific texts, and
it could be concluded that cue words for detecting
speculation were domain-specific.
Morante and Daelemans (2009) describe a ma-
chine learning system detecting the scope of nega-
tions, which is based on meta-learning and is
trained and tested on the annotated BioScope Cor-
pus. In the clinical part of the corpus, the au-
thors obtained a precision of 100 percent, a re-
call of 97.5 percent and finally an F-score of 98.8
percent on detection of cue words for negation.
The authors used TiMBL (Tilburg Memory Based
Learner), which based its decision on features
such as the words annotated as negation cues and
the two words surrounding them, as well as the
part of speech and word forms of these words.
For detection of the negation scope, the task was
to decide whether a word in a sentence contain-
ing a negation cue was either the word starting
or ending a negation scope, or neither of these
two. Three different classifiers were used: sup-
port vector machines, conditional random fields
and TiMBL. Features that were used included the
word and the two words preceding and following
it, the part of speech of these words and the dis-
tance to the negation cue. A fourth classifier, also
based on conditional random fields, used the out-
put of the other three classifiers, among other fea-
tures, for the final decision. The result was a pre-
cision of 86.3 percent and a recall of 82.1 percent
for clinical text. It could also be concluded that the
system was portable to other domains, but with a
lower result.
2.2 The BioScope Corpus
Annotated clinical corpora in English for nega-
tion and speculation are described in Vincze et al
(2008), where clinical radiology reports (a sub-
set of the so called BioScope Corpus) encompass-
ing 6 383 sentences were annotated for negation,
speculation and scope. Henceforth, when refer-
ring to the BioScope Corpus, we only refer to the
clinical subset of the BioScope Corpus. The au-
thors found 877 negation cues and 1 189 specu-
lation cues, (or what we call speculative cues) in
the corpora in 1 561 sentences. This means that
fully 24 percent of the sentences contained some
annotation for negation or uncertainty. However,
of the original 6 383 sentences, 14 percent con-
tained negations and 13 percent contained spec-
ulations. Hence some sentences contained both
negations and speculations. The corpus was anno-
tated by two students and their work was led by a
chief annotator. The students were not allowed to
discuss their annotations with each other, except at
regular meetings, but they were allowed to discuss
6
with the chief annotator. In the cases where the
two student annotators agreed on the annotation,
that annotation was chosen for the final corpus. In
the cases where they did not agree, an annotation
made by the chief annotator was chosen.
2.3 The Stanford NER based on CRF
The Stanford Named Entity Recognizer (NER) is
based on the machine learning algorithm Condi-
tional Random Fields (Finkel et al, 2005) and has
been used extensively for identifying named enti-
ties in news text. For example in the CoNLL-2003,
where the topic was language-independent named
entity recognition, Stanford NER CRF was used
both on English and German news text for train-
ing and evaluation. Where the best results for En-
glish with Stanford NER CRF gave a precision of
86.1 percent, a recall of 86.5 percent and F-score
of 86.3 percent, for German the best results had
a precision of 80.4 percent, a recall of 65.0 per-
cent and an F-score of 71.9 percent, (Klein et al,
2003). We have used the Stanford NER CRF for
training and evaluation of our consensus.
2.4 The annotated Swedish clinical corpus
for negation and speculation
A process to create an annotated clinical corpus
for negation and speculation is described in Dalia-
nis and Velupillai (2010). A total of 6 740 ran-
domly extracted sentences from a very large clin-
ical corpus in Swedish were annotated by three
non-clinical annotators. The sentences were ex-
tracted from the text field Assessment (Bed?omning
in Swedish). Each sentence and its context from
the text field Assessment were presented to the an-
notators who could use five different annotation
classes to annotate the corpora. The annotators
had discussions every two days on the previous
days? work led by the experiment leader.
As described in Velupillai (2010), the anno-
tation guidelines were inspired by the BioScope
Corpus guidelines. There were, however, some
differences, such as the scope of a negation or of
an uncertainty not being annotated. It was instead
annotated if a sentence or clause was certain, un-
certain or undefined. The annotators could thus
choose to annotate the entire sentence as belong-
ing to one of these three classes, or to break up the
sentence into subclauses.
Pairwise inter-annotator agreement was also
measured in the article by Dalianis and Velupillai
(2010) . The average inter-annotator agreement in-
creased after the first annotation rounds, but it was
lower than the agreement between the annotators
of the BioScope Corpus.
The annotation classes used were thus negation
and speculative words, but also certain expression
and uncertain expression as well as undefined. The
annotated subset contains a total of 6 740 sen-
tences or 71 454 tokens, including its context.
3 Method for constructing the consensus
We constructed a consensus annotation out of the
three different annotations of the same clinical cor-
pus that is described in Dalianis and Velupillai
(2010). The consensus was constructed with the
general idea of choosing, as far as possible, an an-
notation for which there existed an identical anno-
tation performed by at least two of the annotators,
and thus to find a majority annotation. In the cases
where no majority was found, other methods were
used.
Other options would be to let the annotators dis-
cuss the sentences that were not identically an-
notated, or to use the method of the BioScope
Corpus, where the sentences that were not iden-
tically annotated were resolved by a chief annota-
tor (Vincze et al, 2008). A third solution, which
might, however, lead to a very biased corpus,
would be to not include the sentences for which
there was not a unanimous annotation in the re-
sulting consensus corpus.
3.1 The creation of a consensus
The annotation classes that were used for annota-
tion can be divided into two levels. The first level
consisted of the annotation classes for classifying
the type of sentence or clause. This level thus in-
cluded the annotation classes uncertain, certain
and undefined. The second level consisted of
the annotation classes for annotating cue words
for negation and speculation, thus the annotation
classes negation and speculative words. The an-
notation classes on the first level were considered
as more important for the consensus, since if there
was no agreement on the kind of expression, it
could perhaps be said to be less important which
cue phrases these expressions contained. In the
following constructed example, the annotation tag
Uncertain is thus an annotation on the first level,
while the annotation tags Negation and Specula-
tive words are on the second level.
7
<Sentence>
<Uncertain>
<Speculative_words>
<Negation>Not</Negation>
really
</Speculative_words>
much worse than before
</Uncertain>
<Sentence>
When constructing the consensus corpus, the
annotated sentences from the first rounds of an-
notation were considered as sentences annotated
before the annotators had fully learnt to apply
the guidelines. The first 1 099 of the annotated
sentences, which also had a lower inter-annotator
agreement, were therefore not included when con-
structing the consensus. Thereby, 5 641 sentences
were left to compare.
The annotations were compared on a sentence
level, where the three versions of each sentence
were compared. First, sentences for which there
existed an identical annotation performed by at
least two of the annotators were chosen. This was
the case for 5 097 sentences, thus 90 percent of the
sentences.
For the remaining 544 sentences, only annota-
tion classes on the first level were compared for a
majority. For the 345 sentences where a majority
was found on the first level, a majority on the sec-
ond level was found for 298 sentences when the
scope of these tags was disregarded. The annota-
tion with the longest scope was then chosen. For
the remaining 47 sentences, the annotation with
the largest number of annotated instances on the
second level was chosen.
The 199 sentences that were still not resolved
were then once again compared on the first level,
this time disregarding the scope. Thereby, 77 sen-
tences were resolved. The annotation with the
longest scopes on the first-level annotations was
chosen.
The remaining 122 sentences were removed
from the consensus. Thus, of the 5 641 sentences,
2 percent could not be resolved with these basic
rules. In the resulting corpus, 92 percent of the
sentences were identically annotated by at least
two persons.
3.2 Differences between the consensus and
the individual annotations
Aspects of how the consensus annotation differed
from the individual annotations were measured.
The number of occurrences of each annotation
class was counted, and thereafter normalised on
the number of sentences, since the consensus an-
notation contained fewer sentences than the origi-
nal, individual annotations.
The results in Table 1 show that there are fewer
uncertain expressions in the consensus annotation
than in the average of the individual annotations.
The reason for this could be that if the annotation
is not completeley free of randomness, the class
with a higher probability will be more frequent in
a majority consensus, than in the individual anno-
tations. In the cases where the annotators are un-
sure of how to classify a sentence, it is not unlikely
that the sentence has a higher probability of being
classified as belonging to the majority class, that
is, the class certain.
The class undefined is also less common in
the consensus annotation, and the same reasoning
holds true for undefined as for uncertain, perhaps
to an even greater extent, since undefined is even
less common.
Also the speculative words are fewer in the con-
sensus. Most likely, this follows from the uncer-
tain sentences being less common.
The words annotated as negations, on the other
hand, are more common in the consensus anno-
tation than in the individual annotations. This
could be partly explained by the choice of the 47
sentences with an annotation that contained the
largest number of annotated instances on the sec-
ond level, and it is an indication that the consensus
contains some annotations for negation cues which
have only been annotated by one person.
Type of Annot. class Individ. Consens.
Negation 853 910
Speculative words 1 174 1 077
Uncertain expression 697 582
Certain expression 4 787 4 938
Undefined expression 257 146
Table 1: Comparison of the number of occurrences
of each annotation class for the individual annota-
tions and the consensus annotation. The figures
for the individual annotations are the mean of the
three annotators, normalised on the number of sen-
tences in the consensus.
Table 2 shows how often the annotators have
divided the sentences into clauses and annotated
each clause with a separate annotation class. From
the table we can see that annotator A and also an-
8
notator H broke up sentences into more than one
type of the expressions Certain, Uncertain or Un-
defined expressions more often than annotator F.
Thereby, the resulting consensus annotation has a
lower frequency of sentences that contained these
annotations than the average of the individual an-
notations. Many of the more granular annotations
that break up sentences into certain and uncertain
clauses are thus not included in the consensus an-
notation. There are instead more annotations that
classify the entire sentence as either Certain, Un-
certain or Undefined.
Annotators A F H Cons.
No. sentences 349 70 224 147
Table 2: Number of sentences that contained more
than one instance of either one of the annotation
classes Certain, Uncertain or Undefined expres-
sions or a combination of these three annotation
classes.
3.3 Discussion of the method
The constructed consensus annotation is thus dif-
ferent from the individual annotations, and it could
at least in some sense be said to be better, since 92
percent of the sentences have been identically an-
notated by at least two persons. However, since for
example some expressions of uncertainty, which
do not have to be incorrect, have been removed, it
can also be said that some information containing
possible interpretations of the text, has also been
lost.
The applied heuristics are in most cases specific
to this annotated corpus. The method is, however,
described in order to exemplify the more general
idea to use a majority decision for selecting the
correct annotations. What is tested when using the
majority method described in this article for de-
ciding which annotation is correct, is the idea that
a possible alternative to a high annotator agree-
ment would be to ask many annotators to judge
what they consider to be certain or uncertain. This
could perhaps be based on a very simplified idea
of language, that the use and interpretation of lan-
guage is nothing more than a majority decision by
the speakers of that language.
A similar approach is used in Steidl et al
(2005), where they study emotion in speech. Since
there are no objective criteria for deciding with
what emotion something is said, they use manual
classification by five labelers, and a majority vot-
ing for deciding which emotion label to use. If less
than three labelers agreed on the classification, it
was omitted from the corpus.
It could be argued that this is also true for un-
certainty, that if there is no possibility to ask the
author of the text, there are no objective criteria
for deciding the level of certainty in the text. It is
always dependent on how it is perceived by the
reader, and therefore a majority method is suit-
able. Even if the majority approach can be used for
subjective classifications, it has some problems.
For example, to increase validity more annotators
are needed, which complicates the process of an-
notation. Also, the same phenomenon that was
observed when constructing the consensus would
probably also arise, that a very infrequent class
such as uncertain, would be less frequent in the
majority consensus than in the individual annota-
tions. Finally, there would probably be many cases
where there is no clear majority for either com-
pletely certain or uncertain: in these cases, having
many annotators will not help to reach a decision
and it can only be concluded that it is difficult to
classify this part of a text. Different levels of un-
certainty could then be introduced, where the ab-
sence of a clear majority could be an indication of
weak certainty or uncertainty, and a very weak ma-
jority could result in an undefined classification.
However, even though different levels of cer-
tainty or uncertainty are interesting when study-
ing how uncertainties are expressed and perceived,
they would complicate the process of information
extraction. Thus, if the final aim of the annota-
tion is to create a system that automatically detects
what is certain or uncertain, it would of course be
more desirable to have an annotation with a higher
inter-annotator agreement. One way of achieving
a this would be to provide more detailed annota-
tion guidelines for what to define as certainty and
uncertainty. However, when it comes to such a
vague concept as uncertainty, there is always a thin
line between having guidelines capturing the gen-
eral perception of uncertainty in the language and
capturing a definition of uncertainty that is specific
to the writers of the guidelines. Also, there might
perhaps be a risk that the complex concept of cer-
tainty and uncertainty becomes overly simplified
when it has to be formulated as a limited set of
guidelines. Therefore, a more feasible method of
achieving higher agreement is probably to instead
9
Class Neg-Spec Relevant Retrieved Corpus Precision Recall F-score
Negation 782 890 853 0.879 0.917 0.897
Speculative words 376 558 1061 0.674 0.354 0.464
Total 1 158 1 448 1 914 0.800 0.605 0.687
Table 3: The results for negation and speculation on consensus when executing Stanford NER CRF using
ten-fold cross validation.
Class Cert-Uncertain Relevant Retrieved Corpus Precision Recall F-score
Certain expression 4 022 4 903 4 745 0.820 0.848 0.835
Uncertain expression 214 433 577 0.494 0.371 0.424
Undefined expression 2 5 144 0.400 0.014 0.027
Total 4 238 5 341 5 466 0.793 0.775 0.784
Table 4: The results for certain and uncertain on consensus when executing Stanford NER CRF using
ten-fold cross validation.
simplify what is being annotated, and not annotate
for such a broad concept as uncertainty in general.
Among other suggestions for improving the an-
notation guidelines for the corpus that the consen-
sus is based on, Velupillai (2010) suggests that the
guidelines should also include instructions on the
focus of the uncertainties, that is, what concepts
are to be annotated for uncertainty.
The task could thus, for example, be tailored to-
wards the information that is to be extracted, and
thereby be simplified by only annotating for un-
certainty relating to a specific concept. If diseases
or symptoms that are present in a patient are to be
extracted, the most relevant concept to annotate is
whether a finding is present or not present in the
patient, or whether it is uncertain if it is present or
not. This approach has, for example, achieved a
very high inter-annotator agreement in the anno-
tation of the evaluation data used by Chapman et
al. (2001). Even though this approach is perhaps
linguistically less interesting, not giving any infor-
mation on uncertainties in general, if the aim is to
search for diseases and symptoms in patients, it
should be sufficient.
In light of the discussion above, the question to
what extent the annotations in the constructed con-
sensus capture a general perception of certainty or
uncertainty must be posed. Since it is constructed
using a majority method with three annotators,
who had a relatively low pairwise agreement, the
corpus could probably not be said to be a precise
capture of what is a certainty or uncertainty. How-
ever, as Artstein and Poesio (2008) point out, it
cannot be said that there is a fixed level of agree-
ment that is valid for all purposes of a corpus, but
the agreement must be high enough for a certain
purpose. Therefore, if the information on whether
there was a unanimous annotation of a sentence or
not is retained, serving as an indicator of how typ-
ical an expression of certainty or uncertainty is,
the constructed corpus can be a useful resource.
Both for studying how uncertainty in clinical text
is constructed and perceived, and as one of the re-
sources that is used for learning to automatically
detect certainty and uncertainty in clinical text.
4 Results of training with Stanford NER
CRF
As a first indication of whether it is possible to use
the annotated consensus corpus for finding nega-
tion and speculation in clinical text, we trained the
Stanford NER CRF, (Finkel et al, 2005) on the an-
notated data. Artstein and Poesio (2008) write that
the fact that annotated data can be generalized and
learnt by a machine learning system is not an in-
dication that the annotations capture some kind of
reality. If it would be shown that the constructed
consensus is easily generalizable, this can thus not
be used as an evidence of its quality. However, if it
would be shown that the data obtained by the an-
notations cannot be learnt by a machine learning
system, this can be used as an indication that the
data is not easily generalizable and that the task
to learn perhaps should, if possible, be simplified.
Of course, it could also be an indication that an-
other learning algorithm should be used or other
features selected.
We created two training sets of annotated con-
sensus material.
The first training set contained annotations on
the second level, thus annotations that contained
the classes Speculative words and Negation. In 76
cases, the tag for Negation was inside an annota-
tion for Speculative words, and these occurrences
10
Class Neg-Spec Bio Relevant Retrieved Corpus Precision Recall F-score
Negation 843 864 872 0.976 0.967 0.971
Speculative words 1 021 1 079 1 124 0.946 0.908 0.927
Scope
1
1 295 1 546 1 595
2
0.838 0.812 0.825
Table 5: The results for negations, speculation cues and scopes on the BioScope Corpus when executing
Stanford NER CRF using ten-fold cross validation.
Class Neg-Spec Relevant Retrieved Corpus Precision Recall F-score
Negation A 791 1 005 896 0.787 0.883 0.832
Speculative words 684 953 1 699 0.718 0.403 0.516
Negation F 938 1097 1023 0.855 0.916 0.884
Speculative words 464 782 1 496 0.593 0.310 0.407
Negation H 722 955 856 0.756 0.843 0.797
Speculative words 552 853 1 639 0.647 0.336 0.443
Table 6: The results for negations and speculation cues and scopes for annotator A, F and H respectively
when executing Stanford NER CRF using ten-fold cross validation.
of the tag Negation were removed. It is detecting
this difference between a real negation cue and a
negation word inside a cue for speculation that is
one of the difficulties that distinguishes the learn-
ing task from a simple string matching.
The second training set only contained the con-
sensus annotations on the first level, thus the anno-
tation classes Certain, Uncertain and Undefined.
We used the default settings on Stanford NER
CRF. The results of the evaluation using ten-fold
cross validation (Kohavi, 1995) are shown in Table
3 and Table 4.
As a comparison, and to verify the suitabil-
ity of the chosen machine learning method, we
also trained and evaluated the BioScope Corpus
using Stanford NER CRF for negation, specula-
tion and scope. The results can be seen in Ta-
ble 5. When training the detection of scope, only
BioScope sentences that contained an annotation
for negation and speculation were selected for the
training and evaluation material for the Stanford
NER CRF. This division into two training sets fol-
lows the method used by Morante and Daelemans
(2009), where sentences containing a cue are first
detected, and then, among these sentences, the
scope of the cue is determined.
We also trained and evaluated the annotations
that were carried out by each annotator A, F and
H separately, i.e. the source of consensus. The re-
sults can be seen in Table 6.
We also compared the distribution of Negation
and Speculative words in the consensus versus the
BioScope Corpus and we found that the consen-
sus, in Swedish, used about the same number of
(types) for negation as the BioScope Corpus in
English (see Table 7), but for speculative words
the consensus contained many more types than the
BioScope Corpus. In the constructed consensus,
72 percent of the Speculative words occurred only
once, whereas in the BioScope Corpus this was the
case for only 24 percent of the Speculative words.
Type of word Cons. Bio
Unique words (Types)
annotated as Negation 13 19
Negations that
occurred only once 5 10
Unique words (Types)
annotated as Speculative 408 79
Speculative words that
occurred only once 294 19
Table 7: Number of unique words both in the Con-
sensus and in the BioScope Corpus that were an-
notated as Negation and as Speculative words, and
how many of these that occurred only once.
5 Discussion
The training results using our clinical consensus
corpus in Swedish gave a precision of 87.9 percent
and a recall of 91.7 percent for negation cues and a
precision of 67.4 percent and a recall of 35.4 per-
cent for speculation cues. The results for detecting
negation cues are thus much higher than for de-
tecting cues for speculation using Stanford NER
CRF. This difference is not very surprising, given
1
The scopes were trained and evaluated separetely from
the negations and speculations.
2
The original number of annotated scopes in the BioScope
Corpus is 1 981. Of these, 386 annotations for nested scopes
were removed.
11
the data in Table 7, which shows that there are only
a very limited number of negation cues, whereas
there exist over 400 different cue words for spec-
ulation. One reason why the F-score for negation
cues is not even higher, despite the fact that the
number of cues for negations is very limited, could
be that a negation word inside a tag for speculative
words is not counted as a negation cue. There-
fore, the word not in, for example, not really could
have been classified as a negation cue by Stanford
NER CRF, even though it is a cue for speculation
and not for negation. Another reason could be that
the word meaning without in Swedish (utan) also
means but, which only sometimes makes it a nega-
tion cue.
We can also observe in Table 4, that the results
for detection of uncertain expressions are very low
(F-score 42 percent). For undefined expressions,
due to scarce training material, it is not possible
to interpret the results. For certain expressions the
results are acceptable, but since the instances are
in majority, the results are not very useful.
Regarding the BioScope Corpus we can ob-
serve (see Table 5) that the training results both
for detecting cues for negation and for specula-
tions are very high, with an F-score of 97 and 93
percent, respectively. For scope detection, the re-
sult is lower but acceptable, with an F-score of
83 percent. These results indicate that the chosen
method is suitable for the learning task.
The main reason for the differences in F-score
between the Swedish consensus corpus and the
BioScope Corpus, when it comes to the detection
of speculation cues, is probably that the variation
of words that were annotated as Speculative word
is much larger in the constructed consensus than
in the BioScope Corpus.
As can be seen in Table 7, there are many more
types of speculative words in the Swedish consen-
sus than in the BioScope Corpus. We believe that
one reason for this difference is that the sentences
in the constructed consensus are extracted from
a very large number of clinics (several hundred),
whereas the BioScope Corpus comes from one ra-
diology clinic. This is supported by the findings of
Szarvas (2008), who writes that cues for specula-
tion are domain-specific. In this case, however, the
texts are still within the domain of clinical texts.
Another reason for the larger variety of cues for
speculation in the Swedish corpus could be that
the guidelines for annotating the BioScope Cor-
pus and the method for creating a consensus were
different.
When comparing the results for the individual
annotators with the constructed consensus, the fig-
ures in Tables 3 and 6 indicate that there are no
big differences in generalizability. When detecting
cues for negation, the precision for the consensus
is better than the precision for the individual an-
notations. However, the results for the recall are
only slightly better or equivalent for the consensus
than for the individual annotations. If we analyse
the speculative cues we can observe that the con-
sensus and the individual annotations have similar
results.
The low results for learning to detect cues for
speculation also serve as an indicator that the task
should be simplified to be more easily generaliz-
able. For example, as previously suggested for
increasing the inter-annotator agreement, the task
could be tailored towards the specific information
that is to be extracted, such as the presence of a
disease in a patient.
6 Future work
To further investigate if a machine learning algo-
rithm such as Conditional Random Fields can be
used for detecting speculative words, more infor-
mation needs to be provided for the Conditional
Random Fields, such as part of speech or if any
of the words in the sentence can be classified as a
symptom or a disease. One Conditional Random
Fields system that can treat nested annotations is
CRF++ (CRF++, 2010). CRF++ is used by several
research groups and we are interested in trying it
out for the negation and speculation detection as
well as scope detection.
7 Conclusion
A consensus clinical corpus was constructed by
applying a few basic rules for combining three in-
dividual annotations into one. Compared to the
individual annotations, the consensus contained
fewer annotations of uncertainties and fewer an-
notations that divided the sentences into clauses.
It also contained fewer annotations for speculative
words, and more annotations for negations. Of
the sentences in the constructed corpus, 92 percent
were identically annotated by at least two persons.
In comparison with the BioScope Corpus, the
constructed consensus contained both a larger
number and a larger variety of speculative cues.
12
This might be one of the reasons why the results
for detecting cues for speculative words using the
Stanford NER CRF are much better for the Bio-
Scope Corpus than for the constructed consensus
corpus; the F-scores are 93 percent versus 46 per-
cent.
Both the BioScope Corpus and the constructed
consensus corpus had high values for detection of
negation cues, F-scores 97 and 90 percent, respec-
tively.
As is suggested by Velupillai (2010), the guide-
lines for annotation should include instructions
on the focus of the uncertainties. To focus the
decision of uncertainty on, for instance, the dis-
ease of a patient, might improve both the inter-
annotator agreement and the possibility of auto-
matically learning to detect the concept of uncer-
tainty.
Acknowledgments
We are very grateful for the valuable comments by
the three anonymous reviewers.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Wendy W. Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001.
A simple algorithm for identifying negated findings
and diseases in discharge summaries. Journal of
biomedical informatics, 34(5):301?310.
CRF++. 2010. CRF++: Yet another CRF toolkit, May
8. http://crfpp.sourceforge.net//.
Hercules Dalianis and Sumithra Velupillai. 2010.
How certain are clinical assessments? Annotating
Swedish clinical text for (un)certainties, specula-
tions and negations. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC?10), Valletta, Malta, May.
Peter L. Elkin, Steven H. Brown, Brent A. Bauer,
Casey S. Husser, William Carruth, Larry R.
Bergstrom, and Dietlind L. Wahner-Roedler. 2005.
A controlled trial of automated classification of
negation from clinical notes. BMC Medical Infor-
matics and Decision Making, 5(1):13.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 363?370.
Yang Huang and Henry J. Lowe. 2007. A novel hybrid
approach to automated negation detection in clinical
radiology reports. Journal of the American Medical
Informatics Association, 14(3):304.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003, pages 180?183. Association for
Computational Linguistics.
Ron Kohavi. 1995. A study of cross-validation and
bootstrap for accuracy estimation and model selec-
tion. In International Joint Conference on Artificial
Intelligence, volume 14, pages 1137?1145.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In CoNLL ?09: Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning, pages 21?29. Association for Com-
putational Linguistics.
Lior Rokach, Roni Romano, and Oded Maimo. 2008.
Negation recognition in medical narrative reports.
Information Retrieval, 11(6):499?538.
Maria Skeppstedt. 2010. Negation detection in
Swedish clinical text. In Louhi?10 - Second Louhi
Workshop on Text and Data Mining of Health Doc-
uments, held in conjunction with NAACL HLT 2010,
Los Angeles, June.
SNOMED-CT. 2010. Systematized nomen-
clature of medicine-clinical terms, May 8.
http://www.ihtsdo.org/snomed-ct/.
Stefan Steidl, Michael Levit, Anton Batliner, Elmar
N?oth, and Heinrich Niemann. 2005. ?Off all
things the measure is man? Automatic classification
of emotions and inter-labeler consistency. In Pro-
ceeding of the IEEE ICASSP,2005, pages 317?320.
Gy?orgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selec-
tion of keywords. In Proceedings of ACL-08: HLT,
pages 281?289, Columbus, Ohio, June. Association
for Computational Linguistics.
Sumithra Velupillai. 2010. Towards a better un-
derstanding of uncertainties and speculations in
swedish clinical text ? analysis of an initial anno-
tation trial. To be published in the proceedings of
the Negation and Speculation in Natural Language
Processing Workshop, July 10, 2010, Uppsala, Swe-
den.
Veronika Vincze, Gy?orgy Szarvas, Rich?ard Farkas,
Gy?orgy M?ora, and J?anos Csirik. 2008. The bio-
scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(S-11).
13
