Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?36,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
CHISPA on the GO
A mobile Chinese-Spanish translation service for travelers in trouble
Jordi Centelles
1,2
, Marta R. Costa-juss
`
a
1,2
and Rafael E. Banchs
2
1
Universitat Polit`ecnica de Catalunya, Barcelona
2
Institute for Infocomm Research, Singapore
{visjcs,vismrc,rembanchs}@i2r.a-star.edu.sg
Abstract
This demo showcases a translation service that
allows travelers to have an easy and convenient
access to Chinese-Spanish translations via a mo-
bile app. The system integrates a phrase-based
translation system with other open source compo-
nents such as Optical Character Recognition and
Automatic Speech Recognition to provide a very
friendly user experience.
1 Introduction
During the last twenty years, Machine Transla-
tion technologies have matured enough to get out
from the academic world and jump into the com-
mercial area. Current commercially available ma-
chine translation services, although still not good
enough to replace human translations, are able to
provide useful and reliable support in certain ap-
plications such as cross-language information re-
trieval, cross-language web browsing and docu-
ment exploration.
On the other hand, the increasing use of smart-
phones, their portability and the availability of in-
ternet almost everywhere, have allowed for lots of
traditional on-line applications and services to be
deployed on these mobile platforms.
In this demo paper we describe ?CHISPA on the
GO? a Chinese-Spanish translation service that in-
tends to provide a portable and easy to use lan-
guage assistance tool for travelers between Chi-
nese and Spanish speaking countries.
The main three characteristics of the presented
demo system are as follows:
? First, the system uses a direct translation be-
tween Chinese and Spanish, rather than using
a pivot language as intermediate step as most
of the current commercial systems do when
dealing with distant languages.
? Second, in addition to support on-line trans-
lations, as other commercial systems, our
system also supports access from mobile
platforms, Android and iOS, by means of na-
tive mobile apps.
? Third, the mobile apps combine the base
translation technology with other supporting
technologies such as Automatic Speech
Recognition (ASR), Optical Character
Recognition (OCR), Image retrieval and
Language detection in order to provide a
friendly user experience.
2 SMT system description
The translation technology used in our system
is based on the well-known phrase-based trans-
lation statistical approach (Koehn et al., 2003).
This approach performs the translation splitting
the source sentence in segments and assigning to
each segment a bilingual phrase from a phrase-
table. Bilingual phrases are translation units that
contain source words and target words, and have
different scores associated to them. These bilin-
gual phrases are then selected in order to max-
imize a linear combination of feature functions.
Such strategy is known as the log-linear model
(Och and Ney, 2002). The two main feature func-
tions are the translation model and the target lan-
guage model. Additional models include lexical
weights, phrase and word penalty and reordering.
2.1 Experimental details
Generally, Chinese-Spanish translation follows
pivot approaches to be translated (Costa-juss`a et
al., 2012) because of the lack of parallel data to
train the direct approach. The main advantage
of our system is that we are using the direct ap-
proach and at the same time we rely on a pretty
large corpus. For Chinese-Spanish, we use (1) the
Holy Bible corpus (Banchs and Li, 2008), (2) the
33
United Nations corpus, which was released for re-
search purposes (Rafalovitch and Dale, 2009), (3)
a small subset of the European Parliament Plenary
Speeches where the Chinese part was syntheti-
cally produced by translating from English, (4) a
large TAUS corpus (TausData, 2013) which comes
from technical translation memories, and (5) an in-
house developed small corpus in the transportation
and hospitality domains. In total we have 70 mil-
lion words.
A careful preprocessing was developed for all
languages. Chinese was segmented with Stanford
segmenter (Tseng et al., 2005) and Spanish was
preprocessed with Freeling (Padr?o et al., 2010).
When Spanish is used as a source language, it is
preprocessed by lower-casing and unaccented the
input. Finally, we use the MOSES decoder (Koehn
et al., 2007) with standard configuration: align-
grow-final-and alignment symmetrization, 5-gram
language model with interpolation and kneser-ney
discount and phrase-smoothing and lexicalized re-
ordering. We use our in-house developed corpus
to optimize because our application is targeted to
the travelers-in-need domain.
3 Web Translator and Mobile
Application
This section describes the main system architec-
ture and the main features of web translator and
the mobile applications.
3.1 System architecture
Figure 1 shows a block diagram of the system ar-
chitecture. Below, we explain the main compo-
nents of the architecture, starting with the back-
end and ending with the front-end.
3.1.1 Back-end
As previously mentioned, our translation system
uses MOSES. More specifically, we use the open
source MOSES server application developed by
Saint-Amand (2013). Because translation tables
need to be kept permanently in memory, we use bi-
nary tables to reduce the memory space consump-
tion. The MOSES server communicates with a PHP
script that is responsible for receiving the query to
be translated and sending the translation back.
For the Chinese-Spanish language pair, we
count with four types of PHP scripts. Two of them
communicate with the web-site and the other two
with the mobile applications. In both cases, one
Figure 1: Block diagram of the system architec-
ture
of the two PHP scripts supports Chinese to Span-
ish translations and the other one the Spanish to
Chinese translations.
The functions of the PHP scripts responsible
for supporting translations are: (1) receive the
Chinese/Spanish queries from the front-end; (2)
preprocess the Chinese/Spanish queries; (3) send
these preprocessed queries to the Chinese/Spanish
to Spanish/Chinese MOSES servers; (4) receive the
translated queries; and (5) send them back to the
front-end.
3.1.2 Front-end
HTML and Javascript constitute the main code
components of the translation website.Another
web development technique used was Ajax, which
allows for asynchronous communication between
the MOSES server and the website. This means that
the website does not need to be refreshed after ev-
ery translation.
The HTTP protocol is used for the communica-
tions between the web and the server. Specifically,
34
we use the POST method, in which the server re-
ceives data through the request message?s body.
The Javascript is used mainly to implement the
input methods of the website, which are a Spanish
keyboard and a Pinyin input method, both open
source and embedded into our code. Also, using
Javascript, a small delay was programmed in order
to automatically send the query to the translator
each time the user stops typing.
Another feature that is worth mentioning is the
support of user feedback to suggest better transla-
tions. Using MYSQL, we created a database in
the server where all user suggestions are stored.
Later, these suggestions can be processed off-line
and used in order to improve the system.
Additionally, all translations processed by the
system are stored in a file. This information is to
be exploited in the near future, when a large num-
ber of translations has been collected, to mine for
the most commonly requested translations. The
most common translation set will be used to im-
plement an index and search engine so that any
query entered by a user, will be first checked
against the index to avoid overloading the trans-
lation engine.
3.2 Android and iphone applications
The android app was programmed with the An-
droid development tools (ADT). It is a plug-in for
the Eclipse IDE that provides the necessary envi-
ronment for building an app.
The Android-based ?CHISPA on the GO? app
is depicted in Figure 2.
For the communication between the Android
app and the server we use the HTTPClient inter-
face. Among other things, it allows a client to
send data to the server via, for instance, the POST
method, as used on the website case.
For the Iphone app we use the xcode software
provided by apple and the programming language
used is Objective C.
In addition to the base translation system, the
app also incorporates Automatic Speech Recogni-
tion (ASR), Optical Character Recognition tech-
nologies as input methods (OCR), Image retrieval
and Language detection.
3.2.1 ASR and OCR
In the case of ASR, we relay on the native ASR
engines of the used mobile platforms: Jelly-bean
in the case of Android
1
and Siri in the case of
1
http://www.android.com/about/jelly-bean/
Figure 2: Android application
iOS
2
. Regarding the OCR implemented technol-
ogy, this is an electronic conversion of scanned
images into machine-encoded text. We adapted
the open-source OCR Tesseract (released under the
Apache license) (Tesseract, 2013).
3.2.2 Image retrieval
For image retrieving, we use the popular website
flickr (Ludicorp, 2004). The image retrieving is
activated with an specific button ?search Image?
button in the app (see Figure 2). Then, an URL
(using the HTTPClient method) is sent to a flickr
server. In the URL we specify the tag (i.e. the
topic of the images we want), the number of im-
ages, the secret key (needed to interact with flickr)
and also the type of object we expect (in our case,
a JSON object). When the server response is re-
ceived, we parse the JSON object. Afterwards,
with the HTTPConnection method and the infor-
mation parsed, we send the URL back to the server
and we retrieve the images requested. Also, the
JAVA class that implements all these methods ex-
tends an AsyncTask in order to not block the
user interface meanwhile is exchanging informa-
tion with the flickr servers.
3.2.3 Language detection
We have also implemented a very simple but ef-
fective language detection system, which is very
suitable for distinguishing between Chinese and
Spanish. Given the type of encoding we are using
2
http://www.apple.com/ios/siri/
35
(UTF-8), codes for most characters used in Span-
ish are in the range from 40 to 255, and codes for
most characters used in Chinese are in the range
from 11,000 and 30,000. Accordingly, we have
designed a simple procedure which computes the
average code for the sequence of characters to be
translated. This average value is compared with a
threshold to determine whether the given sequence
of characters represents a Chinese or a Spanish in-
put.
4 Conclusions
In this demo paper, we described ?CHISPA on
the GO? a translation service that allows travelers-
in-need to have an easy and convenient access to
Chinese-Spanish translations via a mobile app.
The main characteristics of the presented sys-
tem are: the use direct translation between Chi-
nese and Spanish, the support of both website as
well as mobile platforms, and the integration of
supporting input technologies such as Automatic
Speech Recognition, Optical Character Recogni-
tion, Image retrieval and Language detection.
As future work we intend to exploit collected
data to implement an index and search engine for
providing fast access to most commonly requested
translations. The objective of this enhancement is
twofold: supporting off-line mode and alleviating
the translation server load.
Acknowledgments
The authors would like to thank the Universitat
Polit`ecnica de Catalunya and the Institute for In-
focomm Research for their support and permission
to publish this research. This work has been par-
tially funded by the Seventh Framework Program
of the European Commission through the Inter-
national Outgoing Fellowship Marie Curie Action
(IMTraP-2011-29951) and the HLT Department of
the Institute for Infocomm Reseach.
References
R. E. Banchs and H. Li. 2008. Exploring Span-
ish Morphology effects on Chinese-Spanish SMT.
In MATMT 2008: Mixing Approaches to Machine
Translation, pages 49?53, Donostia-San Sebastian,
Spain, February.
M. R. Costa-juss`a, C. A. Henr??quez Q, and R. E.
Banchs. 2012. Evaluating indirect strategies for
chinese-spanish statistical machine translation. J.
Artif. Int. Res., 45(1):761?780, September.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL?03).
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?07), pages
177?180, Prague, Czech Republic, June.
Ludicorp. 2004. Flickr. accessed online May 2013
http://www.flickr.com/.
F.J. Och and H. Ney. 2002. Dicriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL?02), pages 295?302, Philadelphia,
PA, July.
L. Padr?o, M. Collado, S. Reese, M. Lloberes, and
I. Castell?on. 2010. FreeLing 2.1: Five Years of
Open-Source Language Processing Tools. In Pro-
ceedings of 7th Language Resources and Evaluation
Conference (LREC 2010), La Valleta, Malta, May.
A. Rafalovitch and R. Dale. 2009. United Nations
General Assembly Resolutions: A Six-Language
Parallel Corpus. In Proceedings of the MT Summit
XII, pages 292?299, Ottawa.
H. Saint-Amand. 2013. Moses server. accessed
online May 2013 http://www.statmt.org/
moses/?n=Moses.WebTranslation.
TausData. 2013. Taus data. accessed online May 2013
http://www.tausdata.org.
Tesseract. 2013. Ocr. accessed online
May 2013 https://code.google.com/p/
tesseract-ocr/.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and
C. Manning. 2005. A conditional random field
word segmenter. In Fourth SIGHAN Workshop on
Chinese Language Processing.
Appendix: Demo Script Outline
The presenter will showcase the ?CHISPA on the
GO? app by using the three different supported in-
put methods: typing, speech and image. Trans-
lated results will be displayed along with related
pictures of the translated items and/or locations
when available. A poster will be displayed close
to the demo site, which will illustrate the main ar-
chitecture of the platform and will briefly explain
the technology components of it.
36
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 203?207,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Movie-DiC: a Movie Dialogue Corpus for Research and Development 
 
 
Rafael E. Banchs 
Human Language Technology 
Institute for Infocomm Research 
Singapore 138632 
rembanchs@i2r.a-star.edu.sg 
 
 
 
 
Abstract 
This paper describes Movie-DiC a Movie 
Dialogue Corpus recently collected for re-
search and development purposes. The col-
lected dataset comprises 132,229 dialogues 
containing a total of 764,146 turns that 
have been extracted from 753 movies. De-
tails on how the data collection has been 
created and how it is structured are pro-
vided along with its main statistics and cha-
racteristics. 
1 Introduction 
Data driven applications have proliferated in Com-
putational Linguistics during the last decade. Seve-
ral factors, such as the availability of more power-
ful computers, an almost unlimited storage ca-
pacity, the availability of large volumes of data in 
digital format, as well as the recent advances in 
machine learning theory, have significantly con-
tributed to such a proliferation.   
Among the many applications that have benefi-
ted from this data-driven boom, probably the most 
representative examples are: information retrieval 
(Qin et al, 2008), machine translation (Brown et 
al., 1993), question answering (Molla-Aliod and 
Vicedo, 2010) and dialogue systems (Rieser and 
Lemon, 2011). 
In the specific case of dialogue systems, data 
acquisition can impose some challenges depending 
on the specific domain and task the dialogue sys-
tem is targeted for. In some specific domains, in 
which human-human dialogue applications already 
exists, data collection is generally straight forward, 
while in some other cases, data design and collec-
tion can constitute a complex problem (Williams 
and Young, 2003; Zue, 2007; Misu et al, 2009).  
Depending on the objective being pursued, dia-
logue systems can be grouped into two major cate-
gories: task-oriented and chat-oriented systems. In 
the first case, the system is required to help the 
user to accomplish a specific goal or objective 
(Busemann et al, 1997; Stallard, 2000). In the se-
cond case, the system objective is mainly entertain-
ment oriented. Systems in this category are re-
quired to play, chitchat or just accompany the user 
(Weizenbaum, 1966; Wallis, 2010).  
In this work, we focus our attention on dialogue 
data which is suitable for training chat-oriented 
dialogue systems. Different from task-oriented dia-
logue collections (Mann, 2003), instead of being 
concentrated on a specific domain or area of 
knowledge, the training dataset for a chat-oriented 
dialogue system must cover a wide variety of do-
mains, as well as be able to provide a fair represen-
tation of world-knowledge semantics and prag-
matics (Bunt, 2000). To this end, we have col-
lected dialogues from movie scripts aiming at 
constructing a dialogue corpus which should pro-
vide a good sample of domains, styles and world 
knowledge, as well as constitute a valuable re-
source for research and development purposes. 
The rest of the paper is structured as follows. 
Section 2 describes in detail the implemented col-
lection process and the structure of the generated 
database. Section 3 presents the main statistics, as 
well as the main characteristics of the resulting 
corpus. Finally, section 4 presents our conclusions 
and future work plans.  
203
2 Collecting Dialogues from Movies  
As already stated in the introduction, our presented 
dialogue corpus has been extracted from movie 
scripts. More specifically, scripts freely available 
from The Internet Movie Script Data Collection 
(http://www.imsdb.com/) have been used. In this 
section we describe the implemented data collec-
tion process and the data structure finally used for 
the generated corpus. 
As a first step of the collection construction, 
dialogues have to be identified and extracted from 
the crawled html files. Three basic types of infor-
mation elements are extracted from the scripts: 
speakers, utterances and context.  
The utterance and speaker information elements 
contain what is said at each dialogue turn and the 
corresponding character who says it, respectively. 
Context information elements, on the other hand, 
contain all additional information/texts appearing 
in the scripts, which are typically of narrative 
nature and explain what is happening in the scene.  
Figure 1 depicts a browser snapshot illustrating 
the typical layout of a movie script and the most 
common spatial distribution of the aforementioned 
information elements. 
It is important to mention that a lot of different 
variants to the format presented in Figure 1 can be 
actually encountered in The Internet Movie Script 
Data Collection. Because of this, our parsing al-
gorithms had to be revised and adjusted several 
times in order to achieve a reasonable level of 
robustness that allowed for processing the largest 
possible amount of movie scripts.  
Another important problem was the identifica-
tion of dialogue boundaries. Some heuristics were 
implemented by taking into account the size and 
number of context elements between speaker turns.  
A post-processing step was also implemented to 
either filter out or amend some of the most com-
mon parsing errors occurring during the extraction 
phase. Some of these errors include: corrupted for-
mats, turn continuations, notes inserted within the 
turn, misspelling of speaker names, etc. 
In addition to this, a semi-automatic process was 
still necessary to filter out movie scripts exhibiting 
extremely different layouts or invalid file formats. 
Approximately, 17% of the movie scripts crawled 
from The Internet Movie Script Data Collection 
had to be discarded. From a total of 911 crawled 
scripts, only 753 were successfully processed.   
  
Figure 1: Typical layout of a movie script 
 
The extracted information was finally organized 
in dialogical units, in which the information regar-
ding turn sequences inside each dialogue, as well 
as dialogue sequences within each movie script 
was preserved. Figure 2 illustrates an example of 
the XML representation for one of the dialogues 
extracted from Who Framed Roger Rabbit.  
 
<dialogue id="47" n_utterances="4"> 
   <speaker>VALIANT</speaker> 
      <context></context> 
      <utterance>You shot Roger.</utterance> 
   <speaker>JESSICA RABBIT</speaker> 
      <context>Jessica moves the box aside and 
tugs on the rabbit ears. The rabbit head pops 
off. Underneath is a Weasle. In his hand is the 
Colt .45 Buntline.</context> 
      <utterance>That's not Roger. It's one of 
Doom's men. He killed R.K. Maroon.</utterance> 
   <speaker>VALIANT</speaker> 
      <context></context> 
      <utterance>Lady, I guess I had you pegged 
wrong.</utterance> 
   <speaker>JESSICA RABBIT</speaker> 
      <context>As they run down the 
alley...</context> 
      <utterance>Don't worry, you're not the 
first. We better get out of here.</utterance> 
</dialogue> 
 
Figure 2: An example of a dialogue unit  
204
3 Movie Dialogue Corpus Statistics 
In this section we present the main statistics of the 
resulting dialogue corpus and study some of its 
more important properties. The final dialogue col-
lection was the result of successfully processing 
753 movie scripts. Table 1 summarizes the main 
statistics of the resulting dialogue collection.  
 
Total number of scripts collected 911 
Total number of scripts processed 753 
Total number of  dialogues 132,229 
Total number of  speaker turns 764,146 
Average amount of dialogues per movie 175.60 
Average amount of turns per movie 1,014.80 
Average amount of turns per dialogue 5.78 
 
Table 1: Main statistics of the collected movie 
dialogue dataset 
 
Movies were mainly crawled from the action, 
crime, drama and thriller genres. However, as each 
movie commonly belongs to more than one single 
genre, much more genres are actually represented 
in the dataset. Table 2 summarizes the distribution 
of movies by genre (notice that, as most of the 
movies belong to more than one genre, the total 
summation of percentages exceeds 100%). 
 
Genre Movies Percentage
Action 258 34.26 
Adventure 133 17.66 
Animation 22 2.92 
Comedy 149 19.79 
Crime 163 21.65 
Drama 456 60.56 
Family 31 4.12 
Fantasy 82 10.89 
Horror 104 13.81 
Musical 18 2.39 
Mystery 95 12.62 
Romance 123 16.33 
Sci-Fi 129 17.13 
Thriller 329 43.69 
War 25 3.32 
Western 11 1.46 
 
Table 2: Distribution of movies per genre 
 
The first characteristic of the corpus to be ana-
lyzed is the distribution of dialogues per movie. 
This distribution is shown in Figure 3. As seen 
from the figure, the distribution of dialogues per 
movie is clearly symmetric around its mean value 
of 175 dialogues per movie. For most of the mo-
vies in the collection, a number of dialogues ran-
ging from about 100 to 250 were extracted.    
 
  
Figure 3: Distribution of dialogues per movie  
 
The second property of the corpus to be studied 
is the distribution of turns per dialogue. This distri-
bution is shown in Figure 4. As seen from the 
figure, this distribution approximates a power law 
behavior, with a large number of very short dia-
logues (about 50K two-turn dialogues) and a small 
amount of long dialogues (only six dialogues with 
more than 200 turns). The median of the distribu-
tion is 5.63 turns per dialogue.   
 
  
Figure 4: Distribution of turns per dialogue  
 
The third property of the corpus to be described 
is the distribution of number of speakers per dia-
205
logue. This distribution is shown in Figure 5. As 
seen from the bar-plot depicted in the figure, the 
largest proportion of dialogues (around 60K) in-
volves two speakers. The second largest proportion 
of ?dialogues? (about 35K) involves only a single 
speaker, which means that this subset of the data 
collection is actually composed by monologues or 
single speaker interventions. The third and fourth 
larger proportions are those involving three and 
four speakers, respectively.      
  
Figure 5: Distribution of number of speakers per 
dialogue 
 
Finally, in Figure 6, we present a cross-plot be-
tween the number of dialogues and the number of 
turns within each movie script. 
 
  
Figure 6: Cross-plot between the number of 
dialogues and turns within each movie script 
As seen from the cross-plot, an average movie 
has between 150 and 200 dialogues comprising 
between 1000 and 1200 turns in total. The cross-
plot also reveals some interesting extreme cases in 
the data collection.  
For instance, movies with few dialogues but ma-
ny turns are located towards the upper-left corner 
of the figure. In this zone we can find movies as: 
Happy Birthday Wanda June, Hannah and Her 
Sisters and All About Eve. In the lower-left corner 
of the figure we can find movies with few dia-
logues and few turns, as for instance: 1492 Con-
quest of Paradise and The Cooler.  
In the right side of the figure we find the lots-of-
dialogues region. There we can find movies with 
lots of very short dialogues (lower-right corner), 
such as Jimmy and Judy and Walking Tall; or mo-
vies with lots of dialogues and turns (upper-right 
corner), such as The Curious Case of Benjamin 
Button and Jennifer?s Body. 
4 Conclusions and Future Work  
In this paper, we have described Movie-DiC a 
Movie Dialogue Corpus that has been collected for 
research and development purposes. The data col-
lection comprises 132,229 dialogues containing a 
total of 764,146 turns/utterances that have been 
extracted from 753 movies. Details on how the 
data collection has been created and how the 
corpus is structured were provided along with the 
main statistics and characteristics of the corpus. 
Although strictly speaking, and by its particular 
nature, Movie-DiC does not constitute a corpus of 
real human-to-human dialogues, it does constitute 
an excellent dataset for studying the semantic and 
pragmatic aspects of human communication within 
a wide variety of contexts, scenarios, styles and 
socio-cultural settings.  
Specific technologies and applications that can 
exploit a resource like this include, but are not res-
tricted to: example-based chat bots (Banchs and Li, 
2012), question answering systems, discourse and 
pragmatics analysis, narrative vs. colloquial style 
classification, genre classification, etc.  
As future work, we intend to expand the current 
size of the collection from 0.7K to 2K movies, as 
well as to improve some of our parsing and post-
processing algorithms for reducing the amount of 
noise still present in the collection and enhance the 
quality of the current version of the dataset. 
206
Acknowledgments 
The author would like to thank the Institute for 
Infocomm Research for its support and permission 
to publish this work. 
References  
Banchs R E, Li H (2012) IRIS: a chat-oriented dialogue 
system based on the vector space model. In Procee-
dings of the 50th Annual Meeting of the ACL, demo 
session.  
Brown P, Della Pietra S, Della Pietra V, Mercer R 
(1993) The mathematics of statistical machine trans-
lation: parameter estimation. Computational Linguis-
tics 19(2):263-311. 
Bunt H (ed) (2000) Abduction, belief, and context in 
dialogue: studies in computational pragmatics. J. 
Benjamins. 
Busemann S, Declerck T, Diagne A, Dini L, Klein J, 
Schmeier S (1997) Natural language dialogue service 
for appointment scheduling agents. In Proceedings of 
the 5th Conference on Applied Natural Language Pro-
cessing, pp 25-32. 
Mann W (2003) The Dialogue Diversity Corpus. Acces-
sed online on 16 March 2012 from: http://www-
bcf.usc.edu/~billmann/diversity/DDivers-site.htm  
Misu T, Ohtake K, Hori C, Kashioka H, Nakamura S 
(2009) Annotating communicative function and se-
mantic content in dialogue act for construction of 
consulting dialogue systems. In Proceedings of the 
Int. Conf. of Spoken Language Processing 
 
 
 
 
 
 
 
 
 
 
 
 
 
Molla-Aliod D, Vicedo J (2010) Question answering. In 
Indurkhya and Damerau (eds) Handbook of Natural 
Language Processing, pp 485-510. Chapman & Hall. 
Qin T, Liu T, Zhang X, Wang D, Xiong W, Li H (2008) 
Learning to rank relational objects and its application 
to Web search. In Proceedings of the 17th Interna-
tional Conference on World Wide Web, pp 407-416. 
Rieser V, Lemon O (2011) Reinforcement learning for 
adaptive dialogue systems: a data-driven methodolo-
gy for dialogue management and natural language 
generation. Springer. 
Stallard D (2000) Talk?n?travel: a conversational system 
for air travel planning. In Proceedings of the 6th 
Conference on Applied Natural Language Proces-
sing, pp 68-75.  
Wallis P (2010) A robot in the kitchen. In Proceedings 
of the ACL 2010 Workshop on Companionable Dia-
logue Systems, pp 25-30. 
Weizenbaum J (1966) ELIZA ? A computer program 
for the study of natural language communication be-
tween man and machine. Communications of the 
ACM 9(1):36-45. 
Williams J, Young S (2003) Using Wizard-of-Oz 
simulations to bootstrap Reinforcement-Learning-
based dialog management systems. In Proceedings of 
the 4th SIGDIAL Workshop on Discourse and Dia-
logue. 
Zue V (2007) On organic interfaces. In Proceedings of 
the International Conference of Spoken Language 
Processing. 
 
207
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 37?42,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
IRIS: a Chat-oriented Dialogue System based on the Vector Space Model 
 
 
Rafael E. Banchs Haizhou Li 
Human Language Technology Human Language Technology 
Institute for Infocomm Research Institute for Infocomm Research 
Singapore 138632 Singapore 138632 
rembanchs@i2r.a-star.edu.sg hli@i2r.a-star.edu.sg 
 
 
 
Abstract 
This system demonstration paper presents 
IRIS (Informal Response Interactive Sys-
tem), a chat-oriented dialogue system based 
on the vector space model framework. The 
system belongs to the class of example-
based dialogue systems and builds its chat 
capabilities on a dual search strategy over a 
large collection of dialogue samples. Addi-
tional strategies allowing for system adap-
tation and learning implemented over the 
same vector model space framework are 
also described and discussed.    
1 Introduction 
Dialogue systems have been gaining popularity re-
cently as the demand for such kind of applications 
have increased in many different areas. Addition-
ally, recent advances in other related language 
technologies such as speech recognition, discourse 
analysis and natural language understanding have 
made possible for dialogue systems to find practi-
cal applications that are commercially exploitable 
(Pieraccini et al, 2009; Griol et al, 2010).   
From the application point of view, dialogue 
systems can be categorized into two major classes: 
task-oriented and chat-oriented. In the case of task-
oriented dialogue systems, the main objective of 
such a system is to help the user to complete a task, 
which typically includes booking transportation or 
accommodation services, requesting specific infor-
mation from a service facility, etc. (Busemann et 
al., 1997; Seneff and Polifroni, 2000; Stallard, 
2000). On the other hand, chat-oriented systems 
are not intended to help the user completing any 
specific task, but to provide a means for participa-
ting in a game, or just for chitchat or entertain-
ment. Typical examples of chat-oriented dialogue 
systems are the so called chat bots (Weizenbaum, 
1966; Ogura et al, 2003, Wallis, 2010).     
In this paper, we introduce IRIS (Informal Res-
ponse Interactive System), a chat-oriented dialogue 
system that is based on the vector space model 
framework (Salton et al, 1975; van Rijsbergen, 
2005). From the operational point of view, IRIS 
belongs to the category of example-based dialogue 
systems (Murao et al, 2003). Its dialogue strategy 
is supported by a large database of dialogues that is 
used to provide candidate responses to a given user 
input. The search for candidate responses is per-
formed by computing the cosine similarity metric 
into the vector space model representation, in 
which each utterance in the dialogue database is 
represented by a vector. 
Different from example-based question answer-
ing systems (Vicedo, 2002; Xue et al, 2008), IRIS 
uses a dual search strategy. In addition to the cur-
rent user input, which is compared with all existent 
utterances in the database, a vector representation 
of the current dialogue history is also compared 
with vector representations of full dialogues in the 
database. Such a dual search strategy allows for in-
corporating information about the dialogue context 
into the response selection process. 
The rest of the paper is structured as follows. 
Section 2 presents the architecture of IRIS as well 
as provides a general description of the dataset that 
has been used for its implementation. Section 3 
presents some illustrative examples of dialogues 
generated by IRIS, and Section 4 presents the main 
conclusions of this work. 
37
2 The IRIS Implementation  
In this section we first provide a detailed descrip-
tion of the IRIS architecture along with the most 
relevant issues behind its implementation. Then, 
we describe the specific dialogue dataset that sup-
ports the IRIS implementation.  
2.1 Architecture 
As already mentioned, IRIS architecture is heavily 
based on a vector space model framework, which 
includes a standard similarity search module from 
vector-based information retrieval systems (Salton 
and McGill, 1983). However, it also implements 
some additional modules that provide the system 
with capabilities for automatic chatting.   
Figure 1 depicts a block diagram that illustrates 
the main modules in the IRIS architecture. As seen 
from the picture, the whole system comprises se-
ven processing modules and three repositories.     
  
Figure 1: General block diagram for IRIS          
The main operation of IRIS can be described as 
follows. When a new dialogue starts, the control of 
the dialogue is passed from the dialogue manage-
ment module to the initiation/ending module. This 
module implements a two-state dialogue strategy 
which main objectives are: first, to greet the user 
and self-introduce IRIS and, second, to collect the 
name of the user. This module uses a basic parsing 
algorithm that is responsible for extracting the 
user?s name from the provided input. The name is 
the first vocabulary term learned by IRIS, which is 
stored in the vocabulary learning repository. 
Once the dialogue initiation has been concluded 
the dialogue management system gains back the 
control of the dialogue and initializes the current 
history vector. Two types of vector initializations 
are possible here. If the user is already know by 
IRIS, it will load the last stored dialogue history 
for that user; otherwise, IRIS will randomly select 
one dialogue history vector from the dialogue data-
base. After this initialization, IRIS prompts the 
user for what he desires to do. From this moment, 
the example-based chat strategy starts.  
For each new input from the user, the dialogue 
management module makes a series of actions that, 
after a decision process, can lead to different types 
of responses. In the first action, the dynamic repla-
cement module searches for possible matches bet-
ween the terms within the vocabulary learning 
repository and the input string. In a new dialogue, 
the only two terms know by IRIS are its own name 
and the user name. If any of this two terms are 
identified, they are automatically replaced by the 
placeholders <self-name> and <other-name>, res-
pectively. 
In the case of a mature dialogue, when there are 
more terms into the vocabulary learning repository, 
every term matched in the input is replaced by its 
corresponding definition stored in the vocabulary 
learning database.  
Just after the dynamic replacement is conducted, 
tokenization and vectorization of the user input is 
carried out. During tokenization, an additional 
checking is conducted by the dialogue manager. It 
looks for any adaptation command that could be 
possibly inserted at the beginning of the user input. 
More details on adaptation commands will be 
given when describing the style/manner adaptation 
module. Immediately after tokenization, unknown 
vocabulary terms (OOVs) are identified. IRIS will 
consider as OOV any term that is not contained in 
either the dialogue or vocabulary learning data-
bases. In case an OOV is identified, a set of heuris-
tics (aiming at avoiding confusing misspellings 
with OOVs) are applied to decide whether IRIS 
should ask the user for the meaning of such a term. 
38
If IRIS decides to ask for the meaning of the 
term, the control of the dialogue is passed to the 
vocabulary learning module which is responsible 
for collecting the meaning of the given term from 
the user or, alternatively, from an external source 
of information. Once the definition is collected and 
validated, it is stored along with the OOV term into 
the vocabulary learning repository. After comple-
ting a learning cycle, IRIS acknowledges the user 
about having ?understood? the meaning of the term 
and control is passed back to the dialogue manage-
ment module, which waits for a new user input.  
If IRIS decides not to ask for the meaning of the 
OOV term, or if no OOV term has been identified, 
vectorization of the user input is completed by the 
vector similarity modules and similarity scores are 
computed for retrieving best matches from the 
dialogue database. Two different similarity scores 
are actually used by IRIS. The first score is applied 
at the utterance level. It computes the cosine 
similarities between the current user input vector 
and all single utterances stored in the database. 
This score is used for retrieving a large amount of 
candidate utterances from the dialogue database, 
generally between 50 and 100, depending on the 
absolute value of the associated scores. 
The second score is computed over history 
vectors. The current dialogue history, which is 
available from the current history repository, inclu-
des all utterances interchanged by the current user 
and IRIS. In other to facilitate possible topic chan-
ges along the dialogue evolution, a damping or 
?forgetting? factor is used for giving more impor-
tance to the most recent utterances in the dialogue 
history. A single vector representation is then com-
puted for the currently updated dialogue history 
after applying the damping factor. The cosine 
similarity between this vector and the vector repre-
sentations for each full dialogue stored in the dia-
logue database are computed and used along with 
the utterance-level score for generating a final rank 
of candidate utterances. A log-linear combination 
scheme is used for combining the two scores. The 
dialogue management module randomly selects 
one of the top ranked utterances and prompts back 
to the user the corresponding reply (from the dia-
logue database) to the wining utterance. 
Just immediately before prompting back the res-
ponse to the user, the dynamic replacement module 
performs an inverse operation for replacing the two 
placeholders <self-name> and <other-name>, in 
case they occur in the response, by their actual 
values. 
The final action taken by IRIS is related to the 
style/manner adaptation module. For this action to 
take place the user has to include one of three pos-
sible adaptations commands at the beginning of 
her/his new turn. The three adaptation commands 
recognized by IRIS are: ban (*), reinforce (+), and 
discourage (?). By using any of these three charac-
ters as the first character in the new turn, the user is 
requesting IRIS to modify the vector space repre-
sentation of the previous selected response as 
follows: 
? Ban (*): IRIS will mark its last response as a 
prohibited response and will not show such 
response ever again. 
? Reinforce (+): IRIS will pull the vector space 
representation of its last selected utterance 
towards the vector space representation of the 
previous user turn, so that the probability of 
generating the same response given a similar 
user input will be increased. 
? Discourage (?): IRIS will push the vector 
space representation of its last selected utter-
ance apart from the vector space represen-
tation of the previous user turn, so that the 
probability of generating the same response 
given a similar user input will be decreased. 
2.2 Dialogue Data Collection 
For the current implementation of IRIS, a subset of 
the Movie-DiC dialogue data collection has been 
used (Banchs, 2012). Movie-DiC is a dialogue 
corpus that has been extracted from movie scripts 
which are freely available at The Internet Movie 
Script Data Collection (http://www.imsdb.com/). 
In this subsection, we present a brief description on 
the specific data subset used for the implementa-
tion of IRIS, as well as we briefly review the 
process followed for collecting the data and ex-
tracting the dialogues. 
First of all, dialogues have to be identified and 
parsed from the collected html files. Three basic 
elements are extracted from the scripts: speakers, 
utterances and context. The speaker and utterance 
elements contain information about the characters 
who speak and what they said at each dialogue 
turn. On the other hand, context elements contain 
all the additional information (explanations and 
descriptions) appearing in the scripts. 
39
The extracted dialogues are stored into a data 
structure such that the information about turn se-
quences within the dialogues and dialogue sequen-
ces within the scripts are preserved. 
Some post-processing is also necessary to filter 
out and/or repair the most common parsing errors 
occurring during the dialogue extraction phase. 
Some of these errors include: bad script formatting, 
same-speaker turn continuations, explanatory notes 
inserted within the turns, misspelling of names in 
the speaker headers, changes in the encoding for-
mat, etc. 
The final dialogue collection used in the IRIS 
implementation consists of dialogues from 153 
movie scripts, mainly belonging to the comedy, 
action and family genres. Table 1 summarizes the 
main statistics of the resulting dataset. 
 
Total number of movie scripts 153 
Total number of  dialogues 24,265 
Total number of  speaker turns 159,182 
Average amount of dialogues per movie 158.59 
Average amount of turns per dialogue 6.56 
Total number of running words 1,670,879
Overall vocabulary size 34,903 
 
Table 2: Main statistics of the movie dialogue 
dataset used in the IRIS implementation 
 
For each turn in the dialogue collection, a vector 
space model representation was constructed. For 
this, the standard bag-of-words weighting scheme 
known as TF-IDF was used (Spark, 1972; Salton 
and Buckley, 1988).  
Before performing the vectorization, word toke-
nization was conducted. In this step, all punctua-
tion marks were removed, with the exception of 
the question ??? and exclamation ?!? marks. Simi-
larly, all other non-alphanumeric characters occur-
ring in the utterances were removed as well. Also 
during the tokenization phase, all self-references to 
current speaker names in the utterances were re-
placed by the <self-name> placeholder, as well as 
all references to the names of other speakers 
participating in the same dialogue were replaced 
by the <other-name> place-holder. 
Finally, a vector space model representation was 
also computed for each full dialogue in the collec-
tion. For this bag-of-words model at the dialogue 
level, both utterance and context information were 
taken into account. Again, the TF-IDF weighting 
scheme was used.  
3 Some Dialogue Examples 
In this section we show some real examples of 
interactions between IRIS and human users. First, 
we present some interesting examples of good per-
formance, as well as illustrate some of the learning 
capabilities of IRIS. Then, we present some of the 
common failures which identify specific points of 
attention for further improvements.  
3.1 Good Performance Examples 
Our first example illustrates the beginning of a 
typical chat session between IRIS and a new user. 
This example is depicted in Table 2.   
 
# Spkr. Turn 
1 IRIS Hi, I?m IRIS? What?s your name? 
2 USER Hi Iris, I am Peter 
3 IRIS Nice name Peter... What do you want to talk 
about? 
4 USER Sports? 
5 IRIS How come in all that time I was trying to get 
you a date, you never asked me out? 
6 USER ok, do you want to go for dinner tonight? 
7 IRIS Yes. 
8 USER What kind of food do you like? 
9 IRIS Anything but guts and mushrooms. 
10 USER Ok pick you up at seven! 
11 IRIS Ok. Pick me up at seven. 
 
Table 2: Beginning of a chat session between IRIS 
and a new user 
 
For the dialogue depicted in Table 2, turn num-
bers 1, 2 and 3 are processed by the dialogue intia-
tion/ending module. The example-based dialogue 
management strategy starts from turn 4 onwards. 
Notice that as far as this is a new user, not previous 
dialogue history exists, so in this case a random 
history vector has been selected and instead of 
focusing in the sports topic suggested by the user, 
IRIS ?takes the initiative? of asking for a date.  
In our second example, which is presented in 
Table 3, we illustrate the beginning of a typical 
chat session between IRIS and a returning user. 
For this particular user, her last interaction with 
IRIS was about sports. 
Similar to the previous example, turn 1 is pro-
cessed by the dialogue intiation/ending module and 
the example-based dialogue management strategy 
starts from turn 2 onwards. In this particular case, 
IRIS is much more centered on the sports topic as 
this context information has been already provided 
40
by the stored dialogue history of the previous chat 
session with this particular user. 
 
# Spkr. Turn 
1 IRIS Hey, Helen... welcome back! How are you 
doing? 
2 USER fine and you? 
3 IRIS Is there more? 
4 USER Yes, I finally went to that tennis match I told 
you the last time. 
5 IRIS The match! 
6 USER Yes, have you seen some sports recently? 
7 IRIS I hate sports. 
 
Table 3: Beginning of a chat session between IRIS 
and a returning user 
 
In our third example, which is presented in 
Table 4, we illustrate the learning of a new vocabu-
lary word by IRIS. In this example, when the un-
known term paella is detected, the control of the 
dialogue is passed to the vocabulary learning mo-
dule, which takes care of turns 7, 8, 9 and 10. 
 
# Spkr. Turn 
1 USER do you cook anything Iris? 
2 IRIS Eggs. 
3 USER ha ha ha! 
4 IRIS Ha-ha-ha! Ha-ha-ha! 
5 USER Can you cook anything else? 
6 IRIS Do you cook? 
7 USER Paella 
8 IRIS What the hell is that???? 
9 USER It is a Spanish food. Yellow rice with some 
seafood on it. 
10 IRIS Ok, I see... thanks! 
11 USER So, do you want some paella today? 
12 IRIS Well, it beats working in a seafood restaurant...
 
Table 4: Chat segment in which IRIS learns the 
new vocabulary term paella 
 
Notice that when the user asks IRIS about 
having some paella today, IRIS is already able to 
associate it with seafood as it was stated in the 
user?s provided definition. The process actually 
occurs as follows: after tokenization, but before 
vectorization of the user input in turn 11, the 
dynamic replacement module substitutes the term 
paella by its definition, which has been previously 
stored in the vocabulary learning repository when 
turn 9 was processed. The actual user input that is 
finally vectorized in turn 11 is the following one: 
so do you want some it is a spanish food yellow 
rice with some seafood on it today ?, which is the 
utterance used by IRIS to retrieve and select the 
response it provides in turn 12. 
3.2 Common Failure Examples 
In this subsection we focus our attention in the 
most common failures exhibited by IRIS. Some of 
these failures put in evidence specific points of 
attention that should be taken into account for 
further improvements of the system.    
Our first example illustrates the problem of IRIS 
lack of consistency in issues for which consistent 
answers are required. Two specific chat segments 
in which IRIS provides inconsistent responses are 
presented in Table 5.  
 
# Spkr. Turn 
1 USER how old are you Iris? 
2 IRIS I'm thirteen, how old are you? 
3 USER I am 40 
4 IRIS Only I did better... 40 crates a day. 
5 USER how old are you Iris? 
6 IRIS Ten.
# Spkr. Turn 
1 USER Yes, have you seen some sports recently? 
2 IRIS I hate sports. 
3 USER Really, what about soccer? 
4 IRIS I'd sooner watch soccer. 
5 USER what do you prefer soccer or football? 
6 IRIS Football is my life. 
 
Table 5: IRIS incapability to being consistent 
 
The first example presented in Table 5 constitu-
tes a serious consistency problem. In this case IRIS 
has reported two different ages in the same chat 
session. The second case, although not so serious 
as the previous one, also constitutes a consistency 
failure. In this case IRIS states Football is my life 
just two turns after saying I hate sports. 
Our second example, which is presented in Ta-
ble 6, illustrates a problem derived from the noise 
that is still present in the dataset.  
       
# Spkr. Turn 
1 IRIS You watching the ballgame? 
2 USER No 
3 IRIS Bianca stares at him for a moment. 
 
Table 6: Example of noise in the dialogue dataset 
 
In the particular example illustrated in Table 6, 
as seen from turn 3, a context element has been 
41
mistakenly stored in the data collection as an 
utterance during the dataset preparation phase. 
Several problems similar to this one, which are 
related to noise in the database, have been detected 
already. To tackle this problem we need to refine 
the parsing and post-processing algorithms used 
during the dialogue dataset construction phase. 
4 Conclusions and Future Work  
In this paper, we have presented IRIS (Informal 
Response Interactive System), a chat-oriented dia-
logue system that is based on the vector space 
model framework. The system belongs to the class 
of example-based dialogue systems and builds its 
chat capabilities on a dual search strategy over a 
large collection of movie dialogues.  
Additional strategies allowing for system adap-
tation and learning have been also implemented 
over the same vector space model framework. 
More specifically, IRIS is capable of learning new 
vocabulary terms and semantically relating them to 
previous knowledge, as well as adapting its dia-
logue decisions to some stated user preferences.  
We have also described the main characteristics 
of the architecture of IRIS and the most important 
functions performed by each of its constituent 
modules. Finally, we have provided some exam-
ples of good chat performance and some examples 
of the common failures exhibited by IRIS.  
As future work, we intend to improve IRIS per-
formance by addressing some of the already identi-
fied common failures. Similarly, we intend to aug-
ment IRIS chatting capabilities by extending the 
size of the current dialogue database and integra-
ting a strategy for group chatting. 
Acknowledgments 
The authors would like to thank the Institute for 
Infocomm Research for its support and permission 
to publish this work. 
References  
Banchs R E (2012) Movie-DiC: a movie dialogue cor-
pus for research and development. In Proc. of the 50th 
Annual Meeting of the ACL. 
Busemann S, Declerck T, Diagne A, Dini L, Klein J, 
Schmeier S (1997) Natural language dialogue service 
for appointment scheduling agents. In Proc. of the 5th 
Conference on Applied NLP, pp 25-32. 
Griol D, Callejas Z, Lopez-Cozar R (2010) Statistical 
dialog management methodologies for real applica-
tions. In Proc. of SIGDIAL?10, pp 269-272. 
Murao H, Kawaguchi N, Matsubara S, Yamaguchi Y, 
Inagaki Y (2003) Example-based spoken dialogue 
system using WOZ system log. In Proc. of the 4th 
SIGDIAL, pp 140-148. 
Ogura K, Masuda T, Ishizaki M (2003) Building a new 
Internet chat system for sharing timing information. 
In Proc. of the 4th SIGDIAL, pp 97-104. 
Pieraccini R, Suendermann D, Dayanidhi K, Liscombe J 
(2009) Are we there yet? Research in commercial 
spoken dialog systems. In Proc. of TSD?09, pp 3-13. 
Salton G, Wong A, Yang C (1975) A vector space mo-
del for automatic indexing. Communications of the 
ACM 18(11):613-620. 
Salton G, McGill M (1983) Introduction to modern 
information retrieval. McGraw-Hill.  
Salton G, Buckley C (1988) Term-weighting approa-
ches in automatic text retrieval. Information Proces-
sing & Management 24(5):513-523 
Seneff S, Polifroni J (2000) Dialogue management in 
the Mercury flight reservation system. In Proc. of the 
ANLP-NAACL 2000 Workshop on Conversational 
Systems, pp 11-16. 
Spark K (1972) A statistical interpretation of term speci-
ficity and its application in retrieval. Journal of Do-
cumentation 28(1):11-21  
Stallard D (2000) Talk?n?travel: a conversational system 
for air travel planning. In Proc. of the 6th Conference 
on Applied NLP, pp 68-75. 
van Rijsbergen C (2005) A probabilistic logic for infor-
mation retrieval. In Advances in Information Retrie-
val, Lecture Notes in Computer Science 3408:1-6.   
Vicedo J (2002) SEMQA: A semantic model applied to 
question answering systems. PhD Thesis, University 
of Alicante. 
Wallis P (2010) A robot in the kitchen. In Proceedings 
of the ACL 2010 Workshop on Companionable Dia-
logue Systems, pp 25-30. 
Weizenbaum J (1966) ELIZA ? A computer program 
for the study of natural language communication be-
tween man and machine. Communications of the 
ACM 9(1):36-45.  
Xue X, Jeon J, Croft W (2008) Retrieval models for 
question and answer archives. In Proc. of the 31st 
Annual International ACM SIGIR Conference on 
R&D in Information Retrieval, pp 475-482. 
42
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 19?23,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Composite Kernel Approach for Dialog Topic Tracking with
Structured Domain Knowledge from Wikipedia
Seokhwan Kim, Rafael E. Banchs, Haizhou Li
Human Language Technology Department
Institute for Infocomm Research
Singapore 138632
{kims,rembanchs,hli}@i2r.a-star.edu.sg
Abstract
Dialog topic tracking aims at analyzing
and maintaining topic transitions in on-
going dialogs. This paper proposes a com-
posite kernel approach for dialog topic
tracking to utilize various types of do-
main knowledge obtained fromWikipedia.
Two kernels are defined based on history
sequences and context trees constructed
based on the extracted features. The ex-
perimental results show that our compos-
ite kernel approach can significantly im-
prove the performances of topic tracking
in mixed-initiative human-human dialogs.
1 Introduction
Human communications in real world situations
interlace multiple topics which are related to each
other in conversational contexts. This fact sug-
gests that a dialog system should be also capable
of conducting multi-topic conversations with users
to provide them a more natural interaction with the
system. However, the majority of previous work
on dialog interfaces has focused on dealing with
only a single target task. Although some multi-
task dialog systems have been proposed (Lin et al,
1999; Ikeda et al, 2008; Celikyilmaz et al, 2011),
they have aimed at just choosing the most proba-
ble one for each input from the sub-systems, each
of which is independently operated from others.
To analyze and maintain dialog topics from a
more systematic perspective in a given dialog flow,
some researchers (Nakata et al, 2002; Lagus and
Kuusisto, 2002; Adams and Martell, 2008) have
considered this dialog topic identification as a sep-
arate sub-problem of dialog management and at-
tempted to solve it with text categorization ap-
proaches for the recognized utterances in a given
turn. The major obstacle to the success of these
approaches results from the differences between
written texts and spoken utterances. In most text
categorization tasks, the proper category for each
textual unit can be assigned based only on its own
content. However, the dialog topic at each turn
can be determined not only by the user?s inten-
tions captured from the given utterances, but also
by the system?s decisions for dialog management
purposes. Thus, the text categorization approaches
can only be effective for the user-initiative cases
when users tend to mention the topic-related ex-
pressions explicitly in their utterances.
The other direction of dialog topic tracking ap-
proaches made use of external knowledge sources
including domain models (Roy and Subramaniam,
2006), heuristics (Young et al, 2007), and agen-
das (Bohus and Rudnicky, 2003; Lee et al, 2008).
These knowledge-based methods have an advan-
tage of dealing with system-initiative dialogs, be-
cause dialog flows can be controlled by the sys-
tem based on given resources. However, this as-
pect can limit the flexibility to handle the user?s
responses which are contradictory to the system?s
suggestions. Moreover, these approaches face cost
problems for building a sufficient amount of re-
sources to cover broad states of complex dialogs,
because these resources should be manually pre-
pared by human experts for each specific domain.
In this paper, we propose a composite kernel
to explore various types of information obtained
from Wikipedia for mixed-initiative dialog topic
tracking without significant costs for building re-
sources. Composite kernels have been success-
fully applied to improve the performances in other
NLP problems (Zhao and Grishman, 2005; Zhang
et al, 2006) by integrating multiple individual ker-
nels, which aim to overcome the errors occurring
at one level by information from other levels. Our
composite kernel consists of a history sequence
and a domain context tree kernels, both of which
are composed based on similar textual units in
Wikipedia articles to a given dialog context.
19
t Speaker Utterance Topic Transition
0 Guide How can I help you? NONE?NONE
1
Tourist Can you recommend some good places to visit
in Singapore?
NONE?ATTR
Guide Well if you like to visit an icon of Singapore,
Merlion park will be a nice place to visit.
2
Tourist Merlion is a symbol for Singapore, right?
ATTR?ATTR
Guide Yes, we use that to symbolise Singapore.
3
Tourist Okay.
ATTR?ATTR
Guide The lion head symbolised the founding of the is-
land and the fish body just symbolised the hum-
ble fishing village.
4
Tourist How can I get there from Orchard Road?
ATTR?TRSP
Guide You can take the north-south line train from Or-
chard Road and stop at Raffles Place station.
5
Tourist Is this walking distance from the station to the
destination?
TRSP?TRSP
Guide Yes, it?ll take only ten minutes on foot.
6
Tourist Alright.
TRSP?FOOD
Guide Well, you can also enjoy some seafoods at the
riverside near the place.
7
Tourist What food do you have any recommendations
to try there?
FOOD?FOOD
Guide If you like spicy foods, you must try chilli crab
which is one of our favourite dishes here in Sin-
gapore.
8 Tourist Great! I?ll try that. FOOD?FOOD
Figure 1: Examples of dialog topic tracking on
Singapore tour guide dialogs
2 Dialog Topic Tracking
Dialog topic tracking can be considered as a clas-
sification problem to detect topic transitions. The
most probable pair of topics at just before and after
each turn is predicted by the following classifier:
f(x
t
) = (y
t?1
, y
t
), where x
t
contains the input
features obtained at a turn t, y
t
? C , and C is a
closed set of topic categories. If a topic transition
occurs at t, y
t
should be different from y
t?1
. Oth-
erwise, both y
t
and y
t?1
have the same value.
Figure 1 shows an example of dialog topic
tracking in a given dialog fragment on Singapore
tour guide domain between a tourist and a guide.
This conversation is divided into three segments,
since f detects three topic transitions at t
1
, t
4
and
t
6
. Then, a topic sequence of ?Attraction?, ?Trans-
portation?, and ?Food? is obtained from the results.
3 Wikipedia-based Composite Kernel for
Dialog Topic Tracking
The classifier f can be built on the training exam-
ples annotated with topic labels using supervised
machine learning techniques. Although some fun-
damental features extracted from the utterances
mentioned at a given turn or in a certain number of
previous turns can be used for training the model,
this information obtained solely from an ongoing
dialog is not sufficient to identify not only user-
initiative, but also system-initiative topic transi-
tions.
To overcome this limitation, we propose to
leverage on Wikipedia as an external knowledge
source that can be obtained without significant
effort toward building resources for topic track-
ing. Recently, some researchers (Wilcock, 2012;
Breuing et al, 2011) have shown the feasibility
of using Wikipedia knowledge to build dialog sys-
tems. While each of these studies mainly focuses
only on a single type of information including cat-
egory relatedness or hyperlink connectedness, this
work aims at incorporating various knowledge ob-
tained from Wikipedia into the model using a com-
posite kernel method.
Our composite kernel consists of two different
kernels: a history sequence kernel and a domain
context tree kernel. Both represent the current di-
alog context at a given turn with a set of relevant
Wikipedia paragraphs which are selected based on
the cosine similarity between the term vectors of
the recently mentioned utterances and each para-
graph in the Wikipedia collection as follows:
sim (x, p
i
) =
?(x) ? ?(p
i
)
|?(x)||?(p
i
)|
,
where x is the input, p
i
is the i-th paragraph in
the Wikipedia collection, ?(p
i
) is the term vector
extracted from p
i
. The term vector for the input x,
?(x), is computed by accumulating the weights in
the previous turns as follows:
?(x) =
(
?
1
, ?
2
, ? ? ? , ?
|W |
)
? R
|W |
,
where ?
i
=
?
h
j=0
(
?
j
? tf idf(w
i
, u
(t?j)
)
)
, u
t
is
the utterance mentioned in a turn t, tf idf(w
i
, u
t
)
is the product of term frequency of a word w
i
in
u
t
and inverse document frequency of w
i
, ? is a
decay factor for giving more importance to more
recent turns, |W | is the size of word dictionary,
and h is the number of previous turns considered
as dialog history features.
After computing this relatedness between the
current dialog context and every paragraph in the
Wikipedia collection, two kernel structures are
constructed using the information obtained from
the highly-ranked paragraphs in the Wikipedia.
3.1 History Sequence Kernel
The first structure to be constructed for our com-
posite kernel is a sequence of the most similar
paragraph IDs of each turn from the beginning of
the session to the current turn. Formally, the se-
quence S at a given turn t is defined as:
S = (s
0
, ? ? ? , s
t
),
where s
j
= argmax
i
(sim (x
j
, p
i
)).
20
Since our hypothesis is that the more similar the
dialog histories of the two inputs are, the more
similar aspects of topic transtions occur for them,
we propose a sub-sequence kernel (Lodhi et al,
2002) to map the data into a new feature space de-
fined based on the similarity of each pair of history
sequences as follows:
K
s
(S
1
, S
2
) =
?
u?A
n
?
i:u=S
1
[i]
?
j:u=S
2
[j]
?
l(i)+l(j)
,
where A is a finite set of paragraph IDs, S is a fi-
nite sequence of paragraph IDs, u is a subsequence
of S, S[j] is the subsequence with the i-th charac-
ters ?i ? j, l(i) is the length of the subsequence,
and ? ? (0, 1) is a decay factor.
3.2 Domain Context Tree Kernel
The other kernel incorporates more various types
of domain knowledge obtained from Wikipedia
into the feature space. In this method, each in-
stance is encoded in a tree structure constructed
following the rules in Figure 2. The root node of
a tree has few children, each of which is a subtree
rooted at each paragraph node in:
P
t
= {p
i
|sim (x
t
, p
i
) > ?},
where ? is a threshold value to select the relevant
paragraphs. Each subtree consists of a set of fea-
tures from a given paragraph in the Wikipedia col-
lection in a hierarchical structure. Figure 3 shows
an example of a constructed tree.
Since this constructed tree structure represents
semantic, discourse, and structural information
extracted from the similar Wikipedia paragraphs
to each given instance, we can explore these more
enriched features to build the topic tracking model
using a subset tree kernel (Collins and Duffy,
2002) which computes the similarity between each
pair of trees in the feature space as follows:
K
t
(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
? (n
1
, n
2
) ,
where N
T
is the set of T ?s nodes, ? (n
1
, n
2
) =
?
i
I
i
(n
i
) ? I
i
(n
2
), and I
i
(n) is a function that is
1 iff the i-th tree fragment occurs with root at node
n and 0 otherwise.
3.3 Kernel Composition
In this work, a composite kernel is defined by com-
bining the individual kernels including history se-
quence and domain context tree kernels, as well as
<TREE>:=(ROOT <PAR>...<PAR>)
<PAR>:=(PAR_ID <PARENTS>
<PREV_PAR><NEXT_PAR><LINKS>)
<PARENTS>:=(?PARENTS? <ART><SEC>)
<ART>:=(ART_ID <ART_NAME><CAT_LIST>)
<ART_NAME>:=(?ART_NAME? ART_NAME)
<CAT_LIST>:=(?CAT? <CAT>...<CAT>)
<CAT>:=(CAT_ID
*
)
<SEC>:=(SEC_ID <SEC_NAME><PARENT_SEC>
<PREV_SEC><NEXT_SEC>)
<SEC_NAME>:=(?SEC_NAME? SEC_NAME)
<PARENT_SEC>:=(?PRN_SEC?, PRN_SEC_ID)
<PREV_SEC>:=(?PREV_SEC?, PREV_SEC_NAME)
<NEXT_SEC>:=(?NEXT_SEC?, NEXT_SEC_NAME)
<PREV_PAR>:=(?PREV_PAR?, PREV_PAR_ID)
<NEXT_PAR>:=(?NEXT_PAR?, NEXT_PAR_ID)
<LINKS>:=(?LINKS? <LINK>...<LINK>)
<LINK>:=(LINK_NAME
*
)
Figure 2: Rules for constructing a domain context
tree from Wikipedia: PAR, ART, SEC, and CAT
are acronyms for paragraph, article, section, and
category, respectively
Figure 3: An example of domain context tree
the linear kernel between the vectors representing
fundamental features extracted from the utterances
themselves and the results of linguistic preproces-
sors. The composition is performed by linear com-
bination as follows:
K(x
1
, x
2
) =? ?K
l
(V
1
, V
2
) + ? ?K
s
(S
1
, S
2
)
+ ? ?K
t
(T
1
, T
2
),
where V
i
, S
i
, and T
i
are the feature vector, his-
tory sequence, and domain context tree of x
i
, re-
spectively, K
l
is the linear kernel computed by in-
ner product of the vectors, ?, ?, and ? are coeffi-
cients for linear combination of three kernels, and
? + ? + ? = 1.
4 Evaluation
To demonstrate the effectiveness of our proposed
kernel method for dialog topic tracking, we per-
formed experiments on the Singapore tour guide
dialogs which consists of 35 dialog sessions col-
lected from real human-human mixed initiative
conversations related to Singapore between guides
21
and tourists. All the recorded dialogs with the total
length of 21 hours were manually transcribed, then
these transcribed dialogs with 19,651 utterances
were manually annotated with the following nine
topic categories: Opening, Closing, Itinerary, Ac-
commodation, Attraction, Food, Transportation,
Shopping, and Other.
Since we aim at developing the system which
acts as a guide communicating with tourist users,
an instance for both training and prediction of
topic transition was created for each turn of
tourists. The annotation of an instance is a pair of
previous and current topics, and the actual number
of labels occurred in the dataset is 65.
For each instance, the term vector was gener-
ated from the utterances in current user turn, previ-
ous system turn, and history turns within the win-
dow sizes h = 10. Then, the history sequence and
tree context structures for our composite kernel
were constructed based on 3,155 articles related
to Singapore collected from Wikipedia database
dump as of February 2013. For the linear ker-
nel baseline, we used the following features: n-
gram words, previous system actions, and current
user acts which were manually annotated. Finally,
8,318 instances were used for training the model.
We trained the SVM models using
SVM
light 1
(Joachims, 1999) with the follow-
ing five different combinations of kernels: K
l
only, K
l
withP as features, K
l
+K
s
,K
l
+K
t
, and
K
l
+K
s
+K
t
. The threshold value ? for selecting
P was 0.5, and the combinations of kernels were
performed with the same ?, ?, or ? coefficient
values for all sub-kernels. All the evaluations
were done in five-fold cross validation to the man-
ual annotations with two different metrics: one
is accuracy of the predicted topic label for every
turn, and the other is precision/recall/F-measure
for each event of topic transition occurred either
in the answer or the predicted result.
Table 1 compares the performances of the five
combinations of kernels. When just the para-
graph IDs were included as additional features,
it failed to improve the performances from the
baseline without any external features. However,
our proposed kernels using history sequences and
domain context trees achieved significant perfor-
mances improvements for both evaluation metrics.
While the history sequence kernel enhanced the
coverage of the model to detect topic transitions,
1
http://svmlight.joachims.org/
Turn-level Transition-level
Accuracy P R F
K
l
62.45 42.77 24.77 31.37
K
l
+ P 62.44 42.76 24.77 31.37
K
l
+ K
s
67.19 39.94 40.59 40.26
K
l
+ K
t
68.54 45.55 35.69 40.02
All 69.98 44.82 39.83 42.18
Table 1: Experimental Results
0
500
1000
1500
2000
2500
3000
K
l
K
l
+ P K
l
+K
s
K
l
+K
t
ALL
N
um
be
r
of
T
ra
ns
it
io
n
E
rr
or
s FP(SYS)
FN(SYS)
FP(USR)
FN(USR)
Figure 4: Error distibutions of topic transitions:
FN and FP denotes false negative and false posi-
tive respectively. USR and SYS in the parentheses
indicate the initiativity of the transitions.
the domain context tree kernel contributed to pro-
duce more precise outputs. Finally, the model
combining all the kernels outperformed the base-
line by 7.53% in turn-level accuracy and 10.81%
in transition-level F-measure.
The error distributions in Figure 4 indicate that
these performance improvements were achieved
by resolving the errors not only on user-initiative
topic transitions, but also on system-initiative
cases, which implies the effectiveness of the struc-
tured knowledge from Wikipedia to track the top-
ics in mixed-initiative dialogs.
5 Conclusions
This paper presented a composite kernel approach
for dialog topic tracking. This approach aimed to
represent various types of domain knowledge ob-
tained from Wikipedia as two structures: history
sequences and domain context trees; then incor-
porate them into the model with kernel methods.
Experimental results show that the proposed ap-
proaches helped to improve the topic tracking per-
formances in mixed-initiative human-human di-
alogs with respect to the baseline model.
22
References
P. H. Adams and C. H. Martell. 2008. Topic detection
and extraction in chat. In Proceedings of the 2008
IEEE International Conference on Semantic Com-
puting, pages 581?588.
D. Bohus and A. Rudnicky. 2003. Ravenclaw: dia-
log management using hierarchical task decomposi-
tion and an expectation agenda. In Proceedings of
the European Conference on Speech, Communica-
tion and Technology, pages 597?600.
A. Breuing, U. Waltinger, and I. Wachsmuth. 2011.
Harvesting wikipedia knowledge to identify topics
in ongoing natural language dialogs. In Proceedings
of the IEEE/WIC/ACM International Conference on
Web Intelligence and Intelligent Agent Technology
(WI-IAT), pages 445?450.
A. Celikyilmaz, D. Hakkani-Tu?r, and G. Tu?r. 2011.
Approximate inference for domain detection in
spoken language understanding. In Proceedings
of the 12th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 713?716.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association
for computational linguistics, pages 263?270.
S. Ikeda, K. Komatani, T. Ogata, H. G. Okuno, and
H. G. Okuno. 2008. Extensibility verification of ro-
bust domain selection against out-of-grammar utter-
ances in multi-domain spoken dialogue system. In
Proceedings of the 9th INTERSPEECH, pages 487?
490.
T. Joachims. 1999. Making large-scale SVM learn-
ing practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, chapter 11, pages 169?
184. MIT Press, Cambridge, MA.
K. Lagus and J. Kuusisto. 2002. Topic identification
in natural language dialogues using neural networks.
In Proceedings of the 3rd SIGdial workshop on Dis-
course and dialogue, pages 95?102.
C. Lee, S. Jung, and G. G. Lee. 2008. Robust dia-
log management with n-best hypotheses using di-
alog examples and agenda. In Proceedings of the
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 630?637.
B. Lin, H. Wang, and L. Lee. 1999. A distributed
architecture for cooperative spoken dialogue agents
with coherent dialogue state and history. In Pro-
ceedings of the IEEE Automatic Speech Recognition
and Understanding Workshop (ASRU).
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. The Journal of
Machine Learning Research, 2:419?444.
T. Nakata, S. Ando, and A. Okumura. 2002. Topic de-
tection based on dialogue history. In Proceedings of
the 19th international conference on Computational
linguistics (COLING), pages 1?7.
S. Roy and L. V. Subramaniam. 2006. Automatic gen-
eration of domain models for call centers from noisy
transcriptions. In Proceedings of COLING/ACL,
pages 737?744.
G. Wilcock. 2012. Wikitalk: a spoken wikipedia-
based open-domain knowledge access system. In
Proceedings of the Workshop on Question Answer-
ing for Complex Domains, page 5770.
S. Young, J. Schatzmann, K. Weilhammer, and H. Ye.
2007. The hidden information state approach to di-
alog management. In Proceedings of the Interna-
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 149?152.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 419?426.
23
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 30?37,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
An Empirical Evaluation of Stop Word Removal                                  
in Statistical Machine Translation 
Chong Tze Yuang 
School of Computer Engi-
neering, Nanyang Technolo-
gical University, 639798 
Singapore 
tychong@ntu.edu.sg 
Rafael E. Banchs 
Institute for Infocomm Re-
search, A*STAR, 138632, 
Singapore 
rembanchs@i2r.a-
star.edu.sg 
Chng Eng Siong 
School of Computer Engi-
neering Nanyang Technolo-
gical University, 639798 
Singapore. 
aseschng@ntu.edu.sg 
 
 
 
 
 
Abstract 
In this paper we evaluate the possibility of 
improving the performance of a statistical 
machine translation system by relaxing the 
complexity of the translation task by remov-
ing the most frequent and predictable terms 
from the target language vocabulary. After-
wards, the removed terms are inserted back 
in the relaxed output by using an n-gram 
based word predictor. Empirically, we have 
found that when these words are omitted 
from the text, the perplexity of the text de-
creases, which may imply the reduction of 
confusion in the text. We conducted some 
machine translation experiments to see if 
this perplexity reduction produced a better 
translation output. While the word predic-
tion results exhibits 77% accuracy in pre-
dicting 40% of the most frequent words in 
the text, the perplexity reduction did not 
help to produce better translations. 
1 Introduction 
It is a characteristic of natural language that a 
large proportion of running words in a corpus 
corresponds to a very small fraction of the voca-
bulary. An analysis of the Brown Corpus has 
shown that the hundred most frequent words 
account for 42% of the corpus, while only 0.1% 
in the vocabulary. On the other hand, words oc-
curring only once account merely 5.7% in the 
corpus but 58% in the vocabulary (Bell et al 
1990). This phenomenon can be explained in 
terms of Zipf?s Law, which states that the prod-
uct of word ranks and their frequencies approx-
imates a constant, i.e. word-frequency plot is 
close to a hyperbolic function, and hence the few 
top ranked words would account for a great por-
tion of the corpus. Also, it appears that the top 
ranked words are mainly function words. For 
instance, the eight most frequent words in the 
Brown Corpus are the, of, and, to, a, in, that and 
is (Bell et al 1990). 
It is a common practice in Information Re-
trieval (IR) to filter the most frequent words out 
from processed documents (which are referred to 
as stop words), as these function words are se-
mantically non-informative and constitute weak 
indexing terms. By removing this great amount 
of stop words, not only space and time complexi-
ties can be reduced, but document content can be 
better discriminated by the remaining content 
words (Fox, 1989; Rijsbergen, 1979; Zou et al, 
2006; Dolamic & Savoy 2009). 
Inspired by the concept of stop word removal 
in Information Retrieval, in this work we study 
the feasibility of stop word removal in Statistical 
Machine Translation (SMT). Different from In-
formation Retrieval, that ranks or classifies doc-
uments; SMT hypothesizes sentences in target 
language. Therefore, without explicitly removing 
frequent words from the documents, we proposed 
to ignore such words in the target language vo-
cabulary, i.e. by replacing those words with a 
null token. We term this process as ?relaxation? 
and the omitted words as ?relaxed words?.  
Relaxed SMT here refers to a translation task 
in which target vocabulary words are intentional-
ly omitted from the training dataset for reducing 
translation complexity. Since the most frequent 
words are targeted to be relaxed, as a result, there 
will be vast amount of null tokens in the output 
text, which later shall be recovered in a post 
processing stage. The idea of relaxation in SMT 
is motivated by one of our experimental findings, 
in which the perplexity measured over a test set 
decreases when most frequent words are relaxed. 
For instance, a 15% of perplexity reduction is 
observed when the twenty most frequent words 
are relaxed in the English EPPS dataset. The 
reduction of perplexity allows us to conjecture 
30
about the decrease of confusion in the text, from 
which a SMT system might be benefited. 
After applying relaxed SMT, the resulting 
null tokens in the translated sentences have to be 
replaced by the corresponding words from the set 
of relaxed words. As relaxed words are chosen 
from the top ranked words, which possess high 
occurrences in the corpus, their n-gram probabili-
ties could be reliably trained to serve for word 
prediction. Also, these words are mainly function 
words and, from the human perspective, function 
words are usually much easier to predict from 
their neighbor context than content words. Con-
sider for instance the sentence the house of the 
president is very nice. Function words like the, 
of, and is, are certainly easier to be predicted than 
content words such as house, president, and nice. 
The rest of the paper is organized into four 
sections. In section 2, we discuss the relaxation 
strategy implemented for a SMT system, which 
generates translation outputs that contain null to-
kens. In section 3, we present the word predic-
tion mechanism used to recover the null tokens 
occurring in the relaxed translation outputs. In 
section 4, we present and discuss the experimen-
tal results. Finally, in section 5 we present the 
most relevant conclusion of this work. 
2 Relaxation for Machine Translation 
In this paper, relaxation refers to the replacement 
of the most frequent words in text by a null to-
ken. In the practice, a set of frequent words is 
defined and the cardinality of such set is referred 
to as the relaxation order. For example, lets the 
relaxation order be two and the two words on the 
top rank are the and is. By relaxing the sample 
sentence previously presented in the introduc-
tion, the following relaxed sentence will be ob-
tained: NULL house of NULL President NULL 
very beautiful. 
From some of our preliminary experimental 
results with the EPPS dataset, we did observe 
that a relaxation order of twenty leaded to a per-
plexity reduction of about a 15%. To see whether 
this contributes to improving the translation per-
formance, we trained a translation system by 
relaxing the top ranked words in the vocabulary 
of the target language. In this way, there will be a 
large number of words in the source language 
that will be translated to a null token. For exam-
ple: la (the in Spanish) and es (is in Spanish) will 
be both translated to a null token in English. 
This relaxation of terms is only applied to the 
target language vocabulary, and it is conducted 
after the word alignment process but before the 
extraction of translation units and the computa-
tion of model probabilities. The main objective 
of this relaxation procedure is twofold: on the 
one hand, it attempts to reduce the complexity of 
the translation task by reducing the size of the 
target vocabulary while affecting a large propor-
tion of the running words in the text; on the other 
hand, it should also help to reduce model sparse-
ness and improve model probability estimates. 
Of course, different from the Information Re-
trieval case, in which stop words are not used at 
all along the search process, in the considered 
machine translation scenario, the removed words 
need to be recovered after decoding in order to 
produce an acceptable translation. The relaxed 
word replacement procedure, which is based on 
an n-gram based predictor, is implemented as a 
post-processing step and applied to the relaxed 
machine translation output in order to produce 
the final translation result.  
Our bet here is that the gain in the translation 
step, which is derived from the relaxation strate-
gy, should be enough to compensate the error 
rate of the word prediction step, producing, in 
overall, a significant improvement in translation 
quality with respect to the non-relaxed baseline 
procedure. 
The next section describes the implemented 
word prediction model in detail. It constitutes a 
fundamental element of our proposed relaxed 
SMT approach. 
3 Frequent Word Prediction 
Word prediction has been widely studied and 
used in several different tasks such as, for exam-
ple, augmented and alternative communication 
(Wandmacher and Antoine, 2007) and spelling 
correction (Thiele et al, 2000). In addition to the 
commonly used word n-gram, various language 
modeling techniques have been applied, such as 
the semantic model (Lu?s and Rosa, 2002; Wand-
macher and Antoine, 2007) and the class-based 
model (Thiele et al, 2000; Zohar and Roth, 
2000; Ruch et al, 2001). 
The role of such a word predictor in our con-
sidered problem is to recover the null tokens in 
the translation output by replacing them with the 
words that best fit the sentence. This task is es-
sentially a classification problem, in which the 
most suitable relaxed word for recovering a giv-
en null token must be selected. In other words, 
  max	
 sentence??, where 
sentence?  is the probabilistic model, e.g. n-
31
gram, that estimates the likelihood of a sentence 
when a null token is recovered with word  , 
drawn from the set of relaxed words . The car-
dinality || is referred to as the relaxation order, 
e.g. ||  5 implies that the five most frequent 
words have been relaxed and are candidates to be 
recovered. 
Notice that the word prediction problem in 
this task is quite different from other works in the 
literature. This is basically because the relaxed 
words to be predicted in this case are mainly 
function words. Firstly, it may not be effective to 
predict a function word semantically. For exam-
ple, we are more certain in predicting equity than 
for given the occurrence of share in the sentence. 
Secondly, although class-based modeling is com-
monly used for prediction, its original intention 
is to tackle the sparseness problem, whereas our 
task focuses only on the most frequent words. 
In this preliminary work, our predicting me-
chanism is based on an n-gram model. It predicts 
the word that yields the maximum a posteriori 
probability, conditioned on its predecessors. For 
the case of the trigram model, it can be expressed 
as follows: 
 
  max	
 |  (1) 
 
Often, there are cases in which more than one 
null token occur consecutively. In such cases 
predicting a null token is conditioned on the pre-
vious recovered null tokens. To prevent a predic-
tion error from being propagated, one possibility 
is to consider the marginal probability (summed 
over the relaxed word set) over the words that 
were previously null tokens. For example, if  
is a relaxed word, which has been recovered 
from earlier predictions, then the prediction of  
should no longer be conditioned by  . This 
can be computed as follows: 
 
  max	
 |Proceedings of the SIGDIAL 2013 Conference, pages 145?147,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
AIDA: Artificial Intelligent Dialogue Agent 
 
Rafael E. Banchs, Ridong Jiang, Seokhwan Kim, Arthur Niswar, Kheng Hui Yeo 
Natural Language Understanding Lab, Human Language Technology Department  
Institute for Infocomm Research, Singapore 138632 
{rembanchs,rjiang,kims,aniswar,yeokh}@i2r.a-star.edu.sg 
 
 
Abstract 
This demo paper describes our Artificial Intel-
ligent Dialogue Agent (AIDA), a dialogue 
management and orchestration platform under 
development at the Institute for Infocomm Re-
search. Among other features, it integrates dif-
ferent human-computer interaction engines 
across multiple domains and communication 
styles such as command, question answering, 
task-oriented dialogue and chat-oriented dia-
logue. The platform accepts both speech and 
text as input modalities by either direct micro-
phone/keyboard connections or by means of 
mobile device wireless connection. The output 
interface, which is supported by a talking ava-
tar, integrates speech and text along with other 
visual aids. 
1 Introduction 
Some recent efforts towards the development of 
a more comprehensive framework for dialogue 
supported applications include research on multi-
domain or multi-task dialogue agents (Komatani 
et. al 2006, Lee et. al 2009, Nakano et. al 2011, 
Lee et. al 2012). With this direction in mind, our 
Artificial Intelligent Dialogue Agent (AIDA) has 
been created aiming the following two objec-
tives: (1) serving as a demonstrator platform for 
showcasing different dialogue systems and relat-
ed technologies, and (2) providing an experi-
mental framework for conducting research in the 
area of dialogue management and orchestration. 
The main objective of this paper is to present 
and describe the main characteristics of AIDA. 
The rest of the paper is structured as follows. 
First, in section 2, a description of APOLLO, the 
software integration platform supporting AIDA 
is presented. Then, in section 3, the main features 
of AIDA as a dialogue management and orches-
tration platform are described, and a real exam-
ple of human interaction with AIDA is reported. 
Finally, in section 4, our conclusions and future 
work plans are presented.  
2 The APOLLO Integration Platform 
APOLLO (Jiang et al 2012) is a component 
pluggable dialogue framework, which allows for 
the interconnection and control of the different 
components required for the implementation of 
dialogue systems. This framework allows for the 
interoperability of four different classes of com-
ponents: dialogue (ASR, NLU, NLG, TTS, etc.), 
managers (vertical domain-dependent task man-
agers), input/output (speech, text, image and vid-
eo devices), and backend (databases, web crawl-
ers and indexes, rules and inference engines). 
 The different components can be connected to 
APOLLO either by means of specifically created 
plug-ins or by using TCP-IP based socket com-
munications. All component interactions are con-
trolled by using XML scripts. Figure 1 presents a 
general overview of the APOLLO framework. 
  
 
Figure 1: The APOLLO framework 
145
3 Main Features of AIDA  
AIDA (Artificial Intelligent Dialogue Agent) is a 
dialogue management and orchestration plat-
form, which is implemented over the APOLLO 
framework. In AIDA, different communication 
task styles (command, question answering, task-
oriented dialogue and chatting) are hierarchically 
organized according to their atomicity; i.e. more 
atomic (less interruptible) tasks are given prefer-
ence over less atomic (more interruptible) tasks.  
In the case of the chatting engine, as it is the 
least atomic task of all, it is located in the bottom 
of the hierarchy. This engine also behaves as a 
back-off system, which is responsible for taking 
care of all the user interactions that other engines 
fail to resolve properly.    
In AIDA, a dialogue orchestration mechanism 
is used to simultaneously address the problems 
of domain switching and task selection. One of 
the main components of this mechanism is the 
user intention inference module, which makes 
informed decisions for selecting and assigning 
turns across the different individual engines in 
the platform.  
Domain and task selection decisions are made 
based on three different sources of information: 
the current user utterance, which includes stand-
ard semantic and pragmatic features extracted 
from the user utterance; engine information 
states, which takes into account individual in-
formation states from all active engines in the 
platform; and system expectations, which is con-
structed based on the most recent history of user-
system interactions, the task hierarchy previously 
described and the archived profile of the current 
user interacting with the system. 
Our current implementation of AIDA inte-
grates six different dialogue engines: (BC) a basic 
command application, which is responsible for 
serving basic requests such as accessing calendar 
and clock applications, interfacing with search 
engines, displaying maps, etc.; (RA) a reception-
ist application, which consists of a question an-
swering system for providing information about 
the Fusionopolis Complex; (IR) I2R information 
system, which implements as question answering 
system about our institute; (FR) a flight reserva-
tion system, which consists of a frame-based dia-
logue engine that uses statistical natural language 
understanding; (RR) a restaurant recommenda-
tion system, which implements a three-stage 
frame-based dialogue system that uses rule-base 
natural language understanding, and (CH) our 
IRIS chatting agent (Banchs and Li, 2012). 
Regarding input/output modalities, speech and 
text can be used as input channels for user utter-
ances. Direct connections via microphone and 
keyboard are supported, as well as remote con-
nections via mobile devices.  
Additionally, audio and video inputs are used 
to provide AIDA with user identification and 
tracking capabilities. In the first case, speaker 
identification techniques are used to compare the 
voice profile of the current speaker with a set of 
users already known by the system. In the second 
case, face detection and tracking are used in 
combination with sound localization to deter-
mine what the current speaker?s location is when 
dealing with multi-party dialogue scenarios. 
The main output of AIDA is composed of a 
browser interface in which several frames, in-
cluding different visual elements, are presented 
along with a talking avatar. The different visual 
elements include a banner, a text display and a 
general purpose frame for displaying different 
html files, images or URL?s as required.  
For avatar and text-to-speech, AIDA relies on 
the SitePal API (SitePal 2013). This API allows 
for controlling both the avatar speech and gaze. 
Speech functionalities are used for generating a 
spoken form of the system?s output (which is 
also displayed as text in one of the frames of the 
interface) and gaze functionalities are used in 
combination with face tracking to direct the ava-
tar attention to the user who is speaking.    
Figure 2 illustrates the main output interface 
of AIDA. 
 
 
Figure 2: AIDA?s output interface. 
146
Finally, an actual example of a user interaction 
with the AIDA platform is presented in Table 1. 
In the table, the following information is provid-
ed for each of the turns composing the dialogue: 
the user utterance, the corresponding system re-
sponse, the engine that was responsible for gen-
erating the response, and the action taken by the 
system (if any action was taken).  
 
Turn Speaker Utterance/Action 
1 
USER Good morning, how are you? 
AIDA-CH How can I help you? 
2 
USER Can you help me to book a flight? 
AIDA-FR Flight reservation, how can I help you? 
(action) Displays flight reservation main screen 
3 
USER I want to fly from Singapore to Tokyo 
AIDA-FR What day do you want to leave? 
4 
USER Next Monday in the evening 
AIDA-FR What day do you want to return? 
5 
USER The second of April in the morning 
AIDA-FR Here is your itinerary 
(action) Displays flight schedule on the screen 
6 
USER Is there any coffee shop nearby? 
AIDA-RA 
There are three coffee shops in this buil-
ding: Jass Caf?, Starbucks and Ya Kun 
(action) Shows logos of retrieved coffee shops  
7 
USER Do you drink coffee? 
AIDA-CH Coffee? 
8 
USER Yes, coffee 
AIDA-CH No 
9 
USER Why not? 
AIDA-CH Well, for one thing you can?t spell.  
10 
USER Please suggest a good Italian restaurant 
AIDA-RR 
What area of the city do you want me to 
search for? 
(action) Displays restaurant system main screen 
11 
USER Near Sentosa 
AIDA-RR 
Ok, I have found one restaurant for you: 
Alkaff Mansion Ristorante. Do you 
want more information or do you want 
to make a reservation? 
(action) Displays http://www.alkaff.com.sg/  
12 
USER Yes please 
AIDA-RR 
Ok, I will do a reservation for you! And, 
how many guests is this reservation for? 
13 
USER Only two persons 
AIDA-RR 
And, will you be interested in dinner or 
lunch?  
14 
USER Please make it for tonight 
AIDA-RR Your reservation has been made. 
(action) Displays reservation details 
15 
USER Great, thank you 
AIDA-CH You are welcome 
Table 2: Example of a user interaction with AIDA 
4 Conclusions and Future Work 
In this demo paper we have presented AIDA (Ar-
tificial Intelligent Dialogue Agent), the dialogue 
management and orchestration platform currently 
under development at the Institute for Infocomm 
Research in Singapore.  
AIDA integrates different communication task 
styles (command, question answering, dialogue 
and chat) into a single conversational agent. It 
also combines speech recognition and synthesis, 
image processing and avatar technologies to im-
prove the user-system interaction experience. 
Our future plans include the incorporation of 
more vertical applications into the platform, as 
well as the integration of machine translation 
engines to develop multilingual capabilities. 
Demonstration Plan 
During the SIGDIAL demo presentation, the fol-
lowing functionalities will be demonstrated: text 
and speech input; dialogue orchestration among 
receptionist, flight reservation, I2R information 
system, restaurant booking and chatting agent; 
and avatar-supported speech and visual output 
interface. For the case of speech input and ava-
tar-supported output, the use of these technolo-
gies is subject to the availability of internet con-
nection at the location of the demo.  
References  
R. E. Banchs and H. Li. 2012. IRIS: a chat-oriented 
dialogue system based on the vector space model, 
in Demo Session of Association of Computational 
Linguistics, pp. 37?42. 
R. Jiang, Y. K. Tan, D. K. Limbu and H. Li. 2012. 
Component pluggable dialogue framework and its 
application to social robots. In Proc. Int?l Work-
shop on Spoken Language Dialog Systems. 
K. Komatani, N. Kanda, M. Nakano, K. Nakadai, H. 
Tsujino, T. Ogata and H. G. Okuno. 2006. Multi-
domain spoken dialogue system with extensibility 
and robustness against speech recognition errors. In 
Proc. SIGdial Workshop on Discourse and Dia-
logue, pp. 9?17.  
C. Lee, S. Jung, S. Kim and G. G. Lee. 2009. Exam-
ple-based dialog modeling for practical multi-
domain dialog system. Speech Communication, 51, 
pp. 466?484. 
I. Lee, S. Kim, K. Kim, D. Lee, J. Choi, S. Ryu and 
G. G. Lee. 2012. A two step approach for efficient 
domain selection in multi-domain dialog systems. 
In Proc. Int?l Workshop on Spoken Dialogue Sys-
tems.  
M. Nakano, S. Sato, K. Komatani, K. Matsutama, K. 
Funakoshi and H. G. Okuno. 2011. A two stage 
domain selection framework for extensible multi-
domain spoken dialogue systems. In Proc. SIGdial 
Workshop on Discourse and Dialogue. 
SitePal API & Programmer Information, accessed on 
June 27th, 2013 http://www.sitepal.com/support/  
147
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 70?74,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
A Principled Approach to Context-Aware Machine Translation 
Rafael E. Banchs 
Institute for Infocomm Research 
1 Fusionopolis Way, #21-01, Singapore 138632 
rembanchs@i2r.a-star.edu.sg 
 
Abstract 
This paper presents a new principled approach to 
context-aware machine translation. The proposed 
approach reformulates the posterior probability of a 
translation hypothesis given the source input by 
incorporating the source-context information as an 
additional conditioning variable. As a result, a new 
model component, which is referred to as the 
context-awareness model, is added into the original 
noisy channel framework. A specific computation-
al implementation for the new model component is 
also described along with its main properties and 
limitations.     
1 Introduction 
It is well known that source-context information 
plays a significant role in human-based language 
translation (Padilla and Bajo, 1998). A similar 
claim can be supported for the case of Machine 
Translation on the grounds of the Distributional 
Hypothesis (Firth, 1957). According to the Dis-
tributional Hypothesis, much of the meaning of a 
given word is implied by its context rather than 
by the word itself.  
In this work, we first focus our attention on the 
fact that the classical formulation of the statis-
tical machine translation framework, implicitly 
disregards the role of source-context information 
within the translation generation process. Based 
on this, we propose a principled reformulation 
that allows for introducing context-awareness 
into the statistical machine translation frame-
work. Then, a specific computational implement-
ation for the newly proposed model is derived 
and described, along with its main properties and 
limitations. 
The remainder of the paper is structured as 
follows. First, in section 2, the theoretical back-
ground and motivation for this work are present-
ed. Then, in section 3, the proposed model 
derivation is described. In section 4, a specific 
computational implementation for the model is 
provided. And, finally in section 5, main conclu-
sions and future research work are presented. 
2 Theoretical Background 
According to the original formulation of the 
translation problem within the statistical frame-
work, the decoding process is implemented by 
means of a probability maximization mechanism:  
?? =  argmax? ?(?|?)        (1) 
which means that the most likely translation ?? 
for a source sentence ?  is provided by the 
hypothesis ?  that maximizes the conditional 
probability of ? given ?. 
Furthermore, by considering the noisy channel 
approach introduced in communications theory, 
the formulation in (1) can be rewritten as:  
?? =  argmax? ?(?|?) ?(?)        (2) 
where the likelihood ?(?|?) is referred to as the 
translation model and the prior ?(?) is referred 
to as the language model.  
Notice from the resulting formulation in (2) 
that, as the maximization runs over the trans-
lation hypothesis space {?}, the evidence ?(?) is 
not accounted for.  
This particular consequence of the mathema-
tical representation in (2) is counterintuitive to 
the notion of source-context information being 
useful for selecting appropriate translations.  
This problem becomes more relevant when the 
probability models in (2) are decomposed into 
sub-sentence level probabilities for operational 
purposes. Indeed, the computational implement-
ation of (2) requires the decomposition of senten-
ce-level probabilities ?(?|?) and ?(?) into sub-
sentence level probabilities ?(?|?)  and ?(?) , 
were ? and ? refer to sub-sentence units, such as 
words or groups of words. 
In the original problem formulation (Brown et 
al., 1993), the sentence-level translation model 
?(?|?) in (2) is approximated by means of word-
level probabilities, and the sentence-level langua-
ge model ?(?)  is approximated by means of 
word n-gram probabilities.  
70
Within this framework, translation probabilities 
at the sentence-level are estimated from word-
level probabilities as follows1
?(?|?) =  ? ? ?(??|??)??         (3) 
:  
where ??  and ??  refer to individual words 
occurring in ?  and ? , respectively. The proba-
bilities ?(??|??) are referred to as lexical models 
and they represent the probability of an indi-
vidual source word ?? to be the translation of a 
given target word ?? . These lexical models are 
estimated by using word alignment probabilities.  
In statistical phrase-based translation (Koehn et 
al., 2003), the translation model is approximated 
by means of phrase-level probabilities (a phrase 
is a bilingual pair of sub-sentence units that is 
consistent with the word alignments). 
Within this framework, translation probabilities 
at the sentence-level are computed from phrase-
level probabilities as follows:  
?(?|?) =  ? ?(??|??)?         (4) 
where ??  and ??  refer to phrases (i.e. groups of 
words) occurring in ?  and ? , respectively. The 
probabilities ?(??|??) are estimated by means of 
relative frequencies and, accordingly, they are 
referred to as relative frequency models. 
Finally, in (Och and Ney, 2002), the maximum 
entropy framework was introduced into machine 
translation and the two-model formulation in the 
noisy channel approach (2) was extended to the 
log-linear combination of as many relevant 
models as can be reasonably derived from the 
training data. In addition, the maximum entropy 
framework also allows for tuning the weights in 
the log-linear combinations of models by means 
of discriminative training. 
Within this framework, translation probabilities 
at the sentence-level are estimated from phrase-
level probabilities as follows:  
?(?|?) =
1
?
exp{? ? ?? ??(??, ??)?? }        (5) 
where ??(??, ??) are referred to as feature models 
or functions, ??  are the feature weights of the 
log-linear combination, and ? is a normalization 
factor. Notice from (5) that in the maximum 
entropy framework the posterior probability 
?(?|?) is modeled rather than the likelihood. 
                                                          
1 For the sake of clarity additional model components 
such as fertility, reordering and distortion are omitted 
in both (3) and (4). 
From (3) and (4), it is clear that source-context 
information is not taken into account during 
translation hypothesis generation. In such cases, 
the individual sub-sentence unit probabilities 
depend only on the restricted context provided 
by the same sub-sentence unit level as observed 
from the training data.  
In the case of (5), on the other hand, some 
room is left for incorporating source-context 
information in the hypothesis generation process 
by means of context-aware feature models. This 
is basically done by using features that relate the 
occurrences of sub-sentence units with relevant 
source-context information of lager extension.  
Several research works have already addressed 
the problem of incorporating source context 
information into the translation process within 
the maximum entropy framework (Carpuat and 
Wu, 2007; Carpuat and Wu 2008; Haque et al. 
2009; Espa?a-Bonet et al. 2009; Costa-juss? and 
Banchs 2010; Haque et al. 2010; Banchs and 
Costa-juss? 2011). 
In the following section, we will reformulate 
the translation problem, as originally described 
in (1), in order to provide a principled approach 
to context-aware machine translation for both the 
noisy channel and the phrase-based approaches. 
As seen later, this will result in the incorporation 
of a new model component, which can be also 
used as a feature function within the context of 
the maximum entropy framework.  
3 Model Derivation  
In our proposed formulation for context-aware 
machine translation, we assume that the most 
likely translation ??  for a source sentence ? does 
not depends on ? only, but also on the context ? 
in which ? occurs. While this information might 
be not too relevant when estimating probabilities 
at the sentence level, it certainly becomes a very 
useful evidence support at the sub-sentence level.  
Based on this simple idea, we can reformulate 
the mathematical representation of the translation 
problem presented in (1) as follows:  
?? =  argmax? ?(?|?,?)        (6) 
where ?(?|?,?) is the conditional probability of 
a translation hypothesis ?  given the source 
sentence ? and the context ?  in which ? occurs. 
This means that the most likely translation ?? for 
a source sentence ? is provided by the hypothesis 
? that maximizes the conditional probability of ? 
given ? and ?. 
71
For now, let us just consider the context to be 
any unit of source language information with 
larger span than the one of the units used to 
represent ? . For instance, if ?  is a sentence, ? 
can be either a paragraph or a full document; if ? 
is a sub-sentence unit, ? can be a sentence; and 
so on. 
From the theoretical point of view, the formula-
tion in (6) is supported by the assumptions of the 
Distributional Hypothesis, which states that 
meaning is mainly derived from context rather 
than from individual language units. According 
to this, the formulation in (6) allows for incor-
porating context information into the translation 
generation process, in a similar way humans take 
source-context information into account when 
producing a translation.  
After some mathematical manipulations, the 
conditional probability in (6) can be rewritten as 
follows:  
?(?|?,?) =   
?(?|?,?) ?(?|?) ?(?)
?(?|?) ?(?)
        (7) 
where ?(?|?) and ?(?) are the same translation 
and language model probabilities as in (2), and  
?(?|?,?) is the conditional probability of the 
source-context ? given the translation pair ??,??. 
Notice that if the translation pair is independent 
of the context, i.e. ??,?? ? ?, then (7) reduces to:  
?(?|?,?) =   
 ?(?|?) ?(?)
?(?)
         (8) 
and the context-aware formulation in (6) reduces 
to the noisy channel formulation presented 
earlier in (2). 
If we assume, on the other hand, that the 
translation pair is not independent of the context, 
the formulation in (6) can be rewritten in terms 
of (7) as follows:  
?? =  argmax? ?(?|?,?) ?(?|?) ?(?)        (9) 
As seen from (2) and (9), the proposed context-
aware machine translation formulation is similar 
to the noisy channel approach formulation with 
the difference that a new probability model has 
been introduced: ?(?|?,?). This new model will 
be referred to as the context-awareness model, 
and it acts as a complementary model, which 
favors those translation hypotheses ? for which 
the current source context ?  is highly probable 
given the translation pair ??,??. 
In the same way translation probabilities 
?(?|?)  at the sentence-level can be estimated 
from lower-level unit probabilities, such as word 
or phrases, context-awareness probabilities at the 
sentence-level can be also estimated from lower-
level unit probabilities. For instance, ?(?|?,?) 
can be approximated by means of phrase-level 
probabilities according to the following equation:  
?(?|?,?) =  ? ?(?|??, ??)?         (10) 
where ?? and ?? refer to phrase pairs occurring in 
? and ?, respectively, and ? is the source-context 
for the translation under consideration. 
In the following section we develop a specific 
computational implementation for estimating the 
probabilities of the context-awareness model.   
4 Model Implementation 
Before developing a specific implementation for 
the context-awareness model in (10), we need to 
define what type of units ??  and ??  will be used 
and what kind of source-context information ? 
will be taken into account. 
Here, we will consider the phrase-based 
machine translation scenario, where phrase pairs 
<??, ??> are used as the building blocks of the 
translation generation process. Accordingly, and 
in order to be relevant, the span of the context 
information to be used must be larger than the 
one implicitly accounted for by the phrases.     
Typically, phrases span vary from one to 
several words, but most of the time they remain 
within the sub-sentence level. Then, a context 
definition at the sentence-level should be appro-
priate for the purpose of estimating context-
awareness probabilities at the phrase-level. In 
this way, we can consider the context evidence ? 
to be the same sentence being translated ?.   
With these definitions on place, we can now 
propose a maximum likelihood approach for 
estimating context-awareness probabilities at the 
phrase-level. According to this, the probabilities 
can be computed by using relative frequencies as 
follows:  
?(?|??, ??) =  
????? (?,??,??)
????? (??,??)
        (11) 
where the numerator accounts for the number of 
times the phrase pair <??, ??> has been seen along 
with context ?  in the training data, and the 
denominator accounts for the number of times 
the phrase pair <??, ??> has been seen along with 
any context in the training data. 
While the computation of the denominator in 
(11) is trivial, i.e. it just needs to count the 
72
number of times <??, ??> occurs in the parallel 
text, the computation of the numerator requires 
certain consideration.  
Indeed, if we consider the context to be the 
source sentence being translated ?, counting the 
number of times a phrase pair <??, ??> has been 
seen along with context ?  implies that ?  is 
expected to appear several times in the training 
data. In practice, this rarely occurs! According to 
this, the counts for the numerator in (11) will be 
zero most of the time (when the sentence being 
translated is not contained in the training data) 
or, eventually, one (when the sentence being 
translated is contained in the training data). 
Moreover, if the sentence being translated is 
contained in the training data, then its translation 
is already known! So, why do we need to 
generate any translation at all? 
To circumvent this apparent inconsistency of 
the model, and to compute proper estimates for 
the values of ????? (?, ??, ??) , our proposed 
model implementation uses fractional counts. 
This means that, instead of considering integer 
counts of exact occurrences of the context ? 
within the training data, we will consider frac-
tional counts to account for the occurrences of 
contexts that are similar to ?. In order to serve 
this purpose, a similarity metric within the range 
from zero (no similarity at all) to one (maximum 
similarity) is required. 
In this way, for each source sentence ??,? in the 
training data that is associated to the phrase pair 
<??, ??>, its corresponding fractional count would 
be given by the similarity between ??,?  and the 
input sentence being translated ?.  
??????(??,?) =  ???(?, ??,?)        (12) 
According to this, the numerator in (11) can be 
expressed in terms of (12) as: 
????? (?, ??, ??) =  ? ???(?, ??,?)?         (13) 
and the context-awareness probability estimates 
can be computed as:  
?(?|??, ??) =  
? ???(?,??,?)?
? ???(??,? ,??,?)?
        (14) 
Notice that in (14) it is assumed that the 
number of times the phrase pair <??, ??> occurs in 
the parallel text, i.e. ????? (??, ??), is equal to the 
number of sentence pairs containing <??, ??>. In 
other words, multiple occurrences of the same 
phrase pair within a bilingual sentence pair are 
accounted for only once. 
Finally, two important differences between the 
context-awareness model presented here and 
other conventional models used in statistical 
machine translation must be highlighted.  
First, notice that the context-awareness model 
is a dynamic model, in the sense that it has to be 
estimated at run-time. In fact, as the model 
probabilities depend on the input sentence to be 
translated, such probabilities cannot be computed 
beforehand as in the case of other models. 
Second, different from the lexical models and 
relative frequencies that can be computed on 
both directions (source-to-target and target-to-
source), a symmetric version of the context-
awareness model cannot be implemented for 
decoding. This is basically because estimating 
probabilities of the form ?(?|??, ??) requires the 
knowledge of the translation output ?, which is 
not known until decoding is completed. 
However, the symmetric version of the context-
awareness model can be certainly used at a post-
processing stage, such as in n-best rescoring; or, 
alternatively, an incremental implementation can 
be devised for its use during decoding. 
5 Conclusions and Future Work  
We have presented a new principled approach to 
context-aware machine translation. The proposed 
approach reformulates the posterior probability 
of a translation hypothesis given the source input 
by incorporating the source-context information 
as and additional conditioning variable. As a 
result, a new probability model component, the 
context-awareness model, has been introduced 
into the noisy channel approach formulation. 
We also presented a specific computational 
implementation of the context-awareness model, 
in which likelihoods are estimated for the context 
evidence at the phrase-level based on the use of 
fractional counts, which can be computed by 
means of a similarity metric. 
Future work in this area includes efficient run-
time implementations and comparative evalua-
tions of different similarity metrics to be used for 
computing the fractional counts. Similarly, a 
comparative evaluation between an incremental 
implementation of the symmetric version of the 
context-awareness model and its use in a post-
processing stage should be also conducted. 
Acknowledgments 
The author wants to thank I2R for its support and 
permission to publish this work, as well as the 
reviewers for their insightful comments. 
73
References  
Banchs, R.E., Costa-juss?, M. R. 2011. A Semantic 
Feature for Statistical Machine Translation. In 
Proceedings of the Fifth Workshop on Syntax,  
Semantics and Structure in Statistical Translation, 
ACL HLT 2011, pp. 126-134. 
Brown, P., Della-Pietra, S., Della-Pietra, V., Mercer, 
R. 1993. The Mathematics of Statistical Machine 
Translation: Computational Linguistics 19(2), 263-
- 311 
Carpuat, M., Wu, D. 2007. How Phrase Sense 
Disambiguation Outperforms Word Sense Disam-
biguation for Statistical Machine Translation. In: 
11th International Conference on Theoretical and 
Methodological Issues in Machine Translation. 
Skovde 
Carpuat, M., Wu, D. 2008. Evaluation of Context-
Dependent Phrasal Translation Lexicons for 
Statistical Machine Translation. In: 6th Interna-
tional Conference on Language Resources and 
Evaluation (LREC). Marrakech 
Costa-juss?, M. R., Banchs, R.E. 2010. A Vector-
Space Dynamic Feature for Phrase-Based 
Statistical Machine Translation. Journal of 
Intelligent Information Systems 
Espa?a-Bonet, C., Gimenez, J., Marquez, L. 2009. 
Discriminative Phrase-Based Models for Arabic 
Machine Translation. ACM Transactions on Asian 
Language Information Processing Journal (Special 
Issue on Arabic Natural Language Processing) 
Firth, J.R. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis, 51: 1-31 
Haque, R., Naskar, S. K., Ma, Y., Way, A. 2009. 
Using Supertags as Source Language Context in 
SMT. In: 13th Annual Conference of the European 
Association for Machine Translation, pp. 234--241. 
Barcelona 
Haque, R., Naskar, S. K., van den Bosh, A., Way, A. 
2010. Supertags as Source Language Context in 
Hierarchical Phrase-Based SMT. In: 9th Con-
ference of the Association for Machine Translation 
in the Americas (AMTA) 
Koehn, P., Och, F. J., Marcu, D. 2003. Statistical 
Phrase-Based Translation. In: Human Language 
Technology Conference and Conference on 
Empirical Methods in Natural Language Proces-
sing (HLTEMNLP), pp. 48--54. Edmonton 
Och, F. J., Ney, H. (2002) Discriminative Training 
and Maximum Entropy Models for Statistical 
Machine Translation. In: 40th Annual Meeting of 
the Association for Computational Linguistics, pp. 
295--302 
Padilla, P., Bajo, T. (1998) Hacia un Modelo de 
Memoria y Atenci?n en la Interpretaci?n Simul-
t?nea. Quaderns: Revista de Traducci? 2, 107--117 
 
74
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 79?83,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
English-to-Hindi system description for WMT 2014:
Deep Source-Context Features for Moses
Marta R. Costa-juss
`
a
1
, Parth Gupta
2
, Rafael E. Banchs
3
and Paolo Rosso
2
1
Centro de Investigaci?on en Computaci?on, Instituto Polit?ecnico Nacional, Mexico
2
NLE Lab, PRHLT Research Center, Universitat Polit`ecnica de Val`encia
3
Human Language Technology, Institute for Infocomm Research, Singapore
1
marta@nlp.cic.ipn.mx,
2
{pgupta,prosso}@dsic.upv.es,
3
rembanchs@i2r.a-star.edu.sg
Abstract
This paper describes the IPN-UPV partici-
pation on the English-to-Hindi translation
task from WMT 2014 International Evalu-
ation Campaign. The system presented is
based on Moses and enhanced with deep
learning by means of a source-context fea-
ture function. This feature depends on the
input sentence to translate, which makes
it more challenging to adapt it into the
Moses framework. This work reports the
experimental details of the system putting
special emphasis on: how the feature func-
tion is integrated in Moses and how the
deep learning representations are trained
and used.
1 Introduction
This paper describes the joint participation of the
Instituto Polit?ecnico Nacional (IPN) and the Uni-
versitat Polit`ecnica de Valencia (UPV) in cooper-
ation with Institute for Infocomm Research (I2R)
on the 9th Workshop on Statistical Machine Trans-
lation (WMT 2014). In particular, our participa-
tion was in the English-to-Hindi translation task.
Our baseline system is an standard phrase-
based SMT system built with Moses (Koehn et al.,
2007). Starting from this system we propose to in-
troduce a source-context feature function inspired
by previous works (R. Costa-juss`a and Banchs,
2011; Banchs and Costa-juss`a, 2011). The main
novelty of this work is that the source-context fea-
ture is computed in a new deep representation.
The rest of the paper is organized as follows.
Section 2 presents the motivation of this seman-
tic feature and the description of how the source
context feature function is added to Moses. Sec-
tion 3 explains how both the latent semantic in-
dexing and deep representation of sentences are
used to better compute similarities among source
contexts. Section 4 details the WMT experimental
framework and results, which proves the relevance
of the technique proposed. Finally, section 5 re-
ports the main conclusions of this system descrip-
tion paper.
2 Integration of a deep source-context
feature function in Moses
This section reports the motivation and descrip-
tion of the source-context feature function, to-
gether with the explanation of how it is integrated
in Moses.
2.1 Motivation and description
Source context information in the phrase-based
system is limited to the length of the translation
units (phrases). Also, all training sentences con-
tribute equally to the final translation.
We propose a source-context feature func-
tion which measures the similarity between
the input sentence and all training sen-
tences. In this way, the translation unit
should be extended from source|||target to
source|||target|||trainingsentence, with the
trainingsentence the sentence from which
the source and target phrases were extracted.
The measured similarity is used to favour those
translation units that have been extracted from
training sentences that are similar to the current
sentence to be translated and to penalize those
translation units that have been extracted from
unrelated or dissimilar training sentences as
shown in Figure 2.1.
In the proposed feature, sentence similarity is
measured by means of the cosine distance in a
reduced dimension vector-space model, which is
constructed either by means of standard latent se-
mantic analysis or using deep representation as de-
cribed in section 3.
79
S1: we could not book the room in time
T1: hm smy m\ EVkV aArE?fta nhF\ kr sk\
S2: I want to write the book in time
T2: m{\ smy m\ EktaAb ElKnA cAhtaA h 
Input: i am reading a nice book
book : EktaAb
book : aArE?fta krnA
S2
S1
Input
Figure 1: Illustration of the method
2.2 Integration in Moses
As defined in the section above and, previously,
in (R. Costa-juss`a and Banchs, 2011; Banchs
and Costa-juss`a, 2011), the value of the proposed
source context similarity feature depends on each
individual input sentence to be translated by the
system. We are computing the similarity between
the source input sentence and all the source train-
ing sentences.
This definition implies the feature function de-
pends on the input sentence to be translated. To
implement this requirement, we followed our pre-
vious implementation of an off-line version of the
proposed methodology, which, although very in-
efficient in the practice, allows us to evaluate the
impact of the source-context feature on a state-of-
the-art phrase-based translation system. This prac-
tical implementation follows the next procedure:
1. Two sentence similarity matrices are com-
puted: one between sentences in the develop-
ment and training sets, and the other between
sentences in the test and training datasets.
2. Each matrix entry m
ij
should contain the
similarity score between the i
th
sentence in
the training set and the j
th
sentence in the de-
velopment (or test) set.
3. For each sentence s in the test and develop-
ment sets, a phrase pair list L
S
of all poten-
tial phrases that can be used during decoding
is extracted from the aligned training set.
4. The corresponding source-context similarity
values are assigned to each phrase in lists L
S
according to values in the corresponding sim-
ilarity matrices.
5. Each phrase list L
S
is collapsed into a phrase
table T
S
by removing repetitions (when re-
moving repeated entries in the list, the largest
value of the source-context similarity feature
is retained).
6. Each phrase table is completed by adding
standard feature values (which are computed
in the standard manner).
7. Moses is used on a sentence-per-sentence ba-
sis, using a different translation table for each
development (or test) sentence.
3 Representation of Sentences
We represent the sentences of the source language
in the latent space by means of linear and non-
linear dimensionality reduction techniques. Such
models can be seen as topic models where the low-
dimensional embedding of the sentences represent
conditional latent topics.
3.1 Deep Autoencoders
The non-linear dimensionality reduction tech-
nique we employ is based on the concept of deep
learning, specifically deep autoencoders. Autoen-
coders are three-layer networks (input layer, hid-
den layer and output layer) which try to learn an
identity function. In the neural network represen-
tation of autoencoder (Rumelhart et al., 1986), the
visible layer corresponds to the input layer and
hidden layer corresponds to the latent features.
The autoencoder tries to learn an abstract repre-
sentation of the data in the hidden layer in such
a way that minimizes reconstruction error. When
the dimension of the hidden layer is sufficiently
small, autoencoder is able to generalise and derive
powerful low-dimensional representation of data.
We consider bag-of-words representation of text
sentences where the visible layer is binary feature
vector (v) where v
i
corresponds to the presence
or absence of i
th
word. We use binary restricted
Boltzmann machines to construct an autoencoder
as shown in (Hinton et al., 2006). Latent repre-
sentation of the input sentence can be obtained as
shown below:
p(h|v) = ?(W ? v + b) (1)
where W is the symmetric weight matrix between
visible and hidden layer and b is hidden layer
bias vector and ?(x) is sigmoid logistic function
1/(1 + exp(?x)).
80
Autoencoders with single hidden layer do not
have any advantage over linear methods like
PCA (Bourlard and Kamp, 1988), hence we
consider deep autoencoder by stacking multiple
RBMs on top of each other (Hinton and Salakhut-
dinov, 2006). The autoencoders have always been
difficult to train through back-propagation until
greedy layerwise pre-training was found (Hinton
and Salakhutdinov, 2006; Hinton et al., 2006; Ben-
gio et al., 2006). The pre-training initialises the
network parameters in such a way that fine-tuning
them through back-propagation becomes very ef-
fective and efficient (Erhan et al., 2010).
3.2 Latent Semantic Indexing
Linear dimensionality reduction technique, latent
semantic indexing (LSI) is used to represent sen-
tences in abstract space (Deerwester et al., 1990).
The term-sentence matrix (X) is created where x
ij
denotes the occurrence of i
th
term in j
th
sentence.
Matrix X is factorized using singular value decom-
position (SVD) method to obtain top m principle
components and the sentences are represented in
this m dimensional latent space.
4 Experiments
This section describes the experiments carried out
in the context of WMT 2014. For English-Hindi
the parallel training data was collected by Charles
University and consisted of 3.6M English words
and 3.97M Hindi words. There was a monolingual
corpus for Hindi comming from different sources
which consisted of 790.8M Hindi words. In ad-
dition, there was a development corpus of news
data translated specifically for the task which con-
sisted of 10.3m English words and 10.1m Hindi
words. For internal experimentation we built a
test set extracted from the training set. We se-
lected randomly 429 sentences from the training
corpus which appeared only once, removed them
from training and used them as internal test set.
Monolingual Hindi corpus was used to build a
larger language model. The language model was
computed doing an interpolation of the language
model trained on the Hindi part of the bilingual
corpus (3.97M words) and the language model
trained on the monolingual Hindi corpus (790.8M
words). Interpolation was optimised in the de-
velopment set provided by the organizers. Both
language models interpolated were 5-grams using
Kneser-Ney smoothing.
The preprocessing of the corpus was done with
the standard tools from Moses. English was low-
ercased and tokenized. Hindi was tokenized with
the simple tokenizer provided by the organizers.
We cleaned the corpus using standard parameters
(i.e. we keep sentences between 1 and 80 words
of length).
For training, we used the default Moses op-
tions, which include: the grow-diag-final and
word alignment symmetrization, the lexicalized
reordering, relative frequencies (conditional and
posterior probabilities) with phrase discounting,
lexical weights and phrase bonus for the trans-
lation model (with phrases up to length 10), a
language model (see details below) and a word
bonus model. Optimisation was done using the
MERT algorithm available in Moses. Optimisa-
tion is slow because of the way integration of the
feature function is done that it requires one phrase
table for each input sentence.
During translation, we dropped unknown words
and used the option of minimum bayes risk de-
coding. Postprocessing consisted in de-tokenizing
Hindi using the standard detokenizer of Moses
(the English version).
4.1 Autoencoder training
The architecture of autoencoder we consider was
n-500-128-500-n where n is the vocabulary size.
The training sentences were stemmed, stopwords
were removed and also the terms with sentences
frequency
1
less than 20 were also removed. This
resulted in vocabulary size n=7299.
The RBMs were pretrained using Contrastive
Divergence (CD) with step size 1 (Hinton, 2002).
After pretraining, the RBMs were stacked on top
of each other and unrolled to create deep autoen-
coder (Hinton and Salakhutdinov, 2006). During
the fine-tuning stage, we backpropagated the re-
construction error to update network parameters.
The size of mini-batches during pretraining and
fine-tuning were 25 and 100 respectively. Weight
decay was used to prevent overfitting. Addition-
ally, in order to encourage sparsity in the hid-
den units, Kullback-Leibler sparsity regularization
was used. We used GPU
2
based implementation of
autoencoder to train the models which took around
4.5 hours for full training.
1
total number of training sentences in which the term ap-
pears
2
NVIDIA GeForce GTX Titan with Memory 5 GiB and
2688 CUDA cores
81
4.2 Results
Table 1 shows the improvements in terms of
BLEU of adding deep context over the baseline
system for English-to-Hindi (En2Hi) over devel-
opment and test sets. Adding source-context infor-
mation using deep learning outperforms the latent
semantic analysis methodology.
En2Hi
Dev Test
baseline 9.42 14.99
+lsi 9.83 15.12
+deep context 10.40
?
15.43
?
Table 1: BLEU scores for En2Hi translation task..
?
depicts statistical significance (p-value<0.05).
Our source-context feature function may be
more discriminative in a task like English-to-Hindi
where the target language has a larger vocabulary
than the source one.
Table 2 shows an example of how the translation
is improving in terms of lexical semantics which is
the goal of the methodology presented in the pa-
per. The most frequent sense of word cry is ronA,
which literally means ?to cry? while the example
in Table 2 refers to the sense of cry as cFK, which
means to scream. Our method could identify the
context and hence the source context feature (scf )
of the unit cry|||cFK is higher than for the unit
scf (cry|||ronA) as shown in Table 3 and for this
particular input sentence.
5 Conclusion
This paper reports the IPN-UPV participation in
the WMT 2014 Evaluation Campaign. The system
is Moses-based with an additional feature function
based on deep learning. This feature function in-
troduces source-context information in the stan-
dard Moses system by adding the information of
how similar is the input sentence to the different
training sentences. Significant improvements over
System Translation
Source soft cry from the depth
Baseline ghrAiyo\ s m lAym ron lgta
+deep context ghrAiyo\ s m lAym cFK
Reference ghrAiyo\ s koml cFK
Table 2: Manual analysis of a translation output.
cp pp scf
cry|||ronA 0.23 0.06 0.85
cry|||cFK 0.15 0.04 0.90
Table 3: Probability values (conditional, cp, and
posterior, pp, as standard features in a phrase-
based system) for the word cry and two Hindi
translations.
the baseline system are reported in the task from
English to Hindi.
As further work, we will implement our feature
function in Moses using suffix arrays in order to
make it more efficient.
Acknowledgements
This work has been supported in part by
Spanish Ministerio de Econom??a y Compet-
itividad, contract TEC2012-38939-C03-02 as
well as from the European Regional Develop-
ment Fund (ERDF/FEDER). The work of the
second and fourth authors is also supported
by WIQ-EI (IRSES grant n. 269180) and
DIANA-APPLICATIONS (TIN2012-38603-C02-
01) project and VLC/CAMPUS Microcluster on
Multimodal Interaction in Intelligent Systems.
References
Rafael E. Banchs and Marta R. Costa-juss`a. 2011. A
semantic feature for statistical machine translation.
In Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
SSST-5, pages 126?134.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2006. Greedy layer-wise training
of deep networks. In NIPS, pages 153?160.
Herv?e Bourlard and Yves Kamp. 1988. Auto-
association by multilayer perceptrons and singu-
lar value decomposition. Biological Cybernetics,
59(4):291?294, September.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Dumitru Erhan, Yoshua Bengio, Aaron C. Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research, 11:625?660.
82
Geoffrey Hinton and Ruslan Salakhutdinov. 2006. Re-
ducing the dimensionality of data with neural net-
works. Science, 313(5786):504 ? 507.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Computation, 18(7):1527?1554.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Computation, 14(8):1771?1800.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180.
Marta R. Costa-juss`a and Rafael E. Banchs. 2011. The
bm-i2r haitian-cr?eole-to-english translation system
description for the wmt 2011 evaluation campaign.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 452?456, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533?536.
83
Proceedings of the SIGDIAL 2014 Conference, pages 332?336,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Sequential Labeling for Tracking Dynamic Dialog States
Seokhwan Kim, Rafael E. Banchs
Human Language Technology Department
Institute for Infocomm Research
Singapore 138632
{kims,rembanchs}@i2r.a-star.edu.sg
Abstract
This paper presents a sequential labeling
approach for tracking the dialog states for
the cases of goal changes in a dialog ses-
sion. The tracking models are trained us-
ing linear-chain conditional random fields
with the features obtained from the results
of SLU. The experimental results show
that our proposed approach can improve
the performances of the sub-tasks of the
second dialog state tracking challenge.
1 Introduction
A dialog manager is one of the key components
of a dialog system, which aims at determining the
system actions to generate appropriate responses
to users. To make the system capable of conduct-
ing a dialog in a more natural and effective man-
ner, the dialog manager should take into account
not only a given user utterance itself, but also
the dialog state which represents various conver-
sational situations obtained from the dialog ses-
sion progress. Dialog state tracking is a sub-task
of dialog management that analyzes and maintains
this dialog state at each moment. The major ob-
stacle to dialog state tracking is that the inputs to
the tracker are likely to be noisy because of the
errors produced by automatic speech recognition
(ASR) and spoken language understanding (SLU)
processes which are required to be performed prior
to the tracking.
Thus, many researchers have focused on im-
proving the robustness of dialog state trackers
against ASR and SLU errors. The simplest ways
to tackle this problem have been based on hand-
crafted rules mainly on the confidence scores ob-
tained from ASR and SLUmodules (Nakano et al.,
1999; Wang and Lemon, 2013). However, these
approaches have the limitation that building the
quality rules manually is expensive and, what is
worse, the confidence scores could be unreliable
and inconsistent in some cases.
The other direction of dialog state tracking ap-
proaches have utilized statistical machine learn-
ing techniques to obtain the distribution over a set
of hypotheses. Although the most widely studied
approaches have been based on generative mod-
els (Williams and Young, 2007; Williams, 2010;
Young et al., 2010; Thomson and Young, 2010;
Gas?ic? and Young, 2011; Raux and Ma, 2011), re-
cently, some researchers have reported that dis-
criminative models (Bohus and Rudnicky, 2006;
Lee, 2013; Zilka et al., 2013) achieved compara-
ble, or even better, performances than generative
models, especially in the tasks of the first dialog
state tracking challenge (DSTC) (Williams et al.,
2013).
This work focuses on the second phase of
DSTC (Henderson et al., 2014). The major dif-
ference of DSTC 2 from the previous challenge is
that user goals can be changed even in a single di-
alog session. This aspect can cause the limitations
of the previous approaches assuming the fixed user
goal for each session. To solve this dynamic state
tracking problem, we propose a sequential label-
ing approach using linear-chain conditional ran-
dom fields (CRFs) (Lafferty et al., 2001). This
approach aims to improve the performances of
the tracker in the case of goal changes by jointly
performing prediction and segmentation of dialog
states.
2 Problem Definition
A dialog state defined in DSTC 2 consists of the
following three components: goals, method, and
requested slots.
2.1 Goals Tracking
Goals represent the constraint values which are
truly intended by a user at each moment. These
values can be represented by using a slot filling
332
Utterance
Goals
Food Area
S
1
Hello, How may I help you?
U
1
I need a Persian restaurant in the south part of
town.
Persian South
S
2
What kind of food would you like?
U
2
Persian. Persian South
S
3
I?m sorry but there is no restaurant serving persian
food
U
3
How about Portuguese food? Portuguese South
S
4
Peking restaurant is a nice place in the south of
town.
U
4
Is that Portuguese? Portuguese South
S
5
Nandos is a nice place in the south of town serving
tasty Portuguese food.
U
5
Alright. Whats the phone number? Portuguese South
S
6
The phone number of nandos is 01223 327908 .
U
6
And the address? Portuguese South
S
7
Sure, nandos is on Cambridge Leisure Park Clifton
Way.
U
7
Thank you good bye.
Figure 1: Examples of user goals tracking on a
dialog in the restaurant information domain
over the following four categories: area, food,
name, and price range. Assuming the possible
value set for each slot is fixed, this task can be
considered to be a problem of finding the distri-
butions over these hypotheses. While the previous
challenge aims at identifying a single fixed goal
for each session, the models for DSTC 2 should
be able to handle goal changes during a session,
as shown in Figure 1.
2.2 Method Tracking
Method tracking is performed by classifying the
way of requesting information by a user into the
following four categories: ?by constraints?, ?by al-
ternatives?, ?by name?, and ?finished?. The prob-
ability distribution over these four hypotheses is
computed for each turn. For example, a meth-
ods sequence {byconstraints, byconstraints, byal-
ternatives, byalternatives, byalternatives, byalter-
natives, finished} can be obtained for the dialog
session in Figure 1.
2.3 Requested Slots Tracking
The other component for dialog state tracking is to
specify the slots requested by a user. The tracker
should output the binary distributions with the
probabilities whether each slot is requested or not.
Since the requestable slots are area, food, name,
pricerange, addr, phone, postcode, and signature,
eight different distributions are obtained at each
turn. In the previous example dialog, ?phone? and
?addr? are requested in the 5th and 6th turns re-
spectively.
(a) Goal chain on the food slot
(b) Method chain
(c) Requested chain on the phone slot
Figure 2: Examples of dialog state tracking as se-
quential labeling with liner-chain CRFs
3 Method
Although some discriminative approaches (Lee,
2013; Zilka et al., 2013; Lee and Eskenazi, 2013;
Ren et al., 2013) have successfully applied to the
dialog state tracking tasks of DSTC 1 by explor-
ing various features, they have limited ability to
perform the DSTC 2 tasks, because the previous
models trained based on the features mostly ex-
tracted under the assumption that the user goal in
a session is unchangeable. To overcome this limi-
tation, we propose a sequential labeling approach
using linear-chain CRFs for dynamic dialog state
tracking.
3.1 Sequential Labeling of Dialog States
The goal of sequential labeling is to produce the
most probable label sequence y = {y
1
, ? ? ? , y
n
}
of a given input sequence x = {x
1
, ? ? ? , x
n
},
where n is the length of the input sequence, x
i
?
X , X is the finite set of the input observation,
y
i
? Y , and Y is the set of output labels. The
input sequence for dialog state tracking at a given
turn t is defined as x
t
= {x
1
, ? ? ? , x
t
}, where x
i
denotes the i-th turn in a given dialog session, then
a tracker should be able to output a set of label se-
quences for every sub-task.
333
For the goals and requested slots tasks, a la-
bel sequence is assigned to each target slot, which
means the number of output sequences for these
sub-tasks are four and eight in total, respectively.
On the other hand, only a single label sequence is
defined for the method tracking task.
Due to discourse coherences in conversation,
the same labels are likely to be located contigu-
ously in a label sequence. To detect the bound-
aries of these label chunks, the BIO tagging
scheme (Ramshaw and Marcus, 1999) is adopted
for all the label sequences, which marks beginning
of a chunk as ?B?, continuing of a chunk as ?I?, and
outside a chunk as ?O?. Figure 2 shows the exam-
ples of label sequences according to this scheme
for the input dialog session in Figure 1.
3.2 Linear Chain CRFs
In this work, all the sequential labeling tasks were
performed by the tracking models trained using
first-order linear-chain CRFs. Linear-chain CRFs
are conditional probability distributions over the
label sequences y conditioned on the input se-
quence x, which are defined as follows:
p (y|x) =
1
Z (x)
n
?
t=1
?(y
t
, y
t?1
,x),
?(y
t
, y
t?1
,x) = ?
1
(y
t
,x) ? ?
2
(y
t
, y
t?1
),
?
1
(y
t
,x) = exp
(
?
k
?
k
f
k
(y
t
,x)
)
,
?
2
(y
t
, y
t?1
) = exp
(
?
k
?
k
f
k
(y
t
, y
t?1
)
)
,
where Z(x) is the normalization function which
makes that the distribution sums to 1, {f
k
} is a set
of feature functions for observation and transition,
and {?
k
} is a set of weight parameters which are
learnt from data.
3.3 Features
To train the tracking models, a set of feature func-
tions were defined based on the n-best list of user
actions obtained from the live SLU results at a
given turn and the system actions corresponding
to the previous system output.
The most fundamental information to capture a
user?s intentions can be obtained from the SLU hy-
potheses with ?inform? action type. For each ?in-
form? action in the n-best SLU results, a feature
function is defined as follows:
f
i
(inf, s, v) =
{
S
i
(inf, s, v), if inf(s, v) ? UA
i
,
0, otherwise,
where S
i
(a, s, v) is the confidence score of the
hypothesis (a, s, v) assigned by SLU for the i-th
turn, a is the action type, s is the target slot, v is
its value, and UA
i
is the n-best list of SLU results.
Similarly, the actions with ?confirm? and ?deny?
types derive the corresponding feature functions
defined as:
f
i
(con, s, v) =
{
S
i
(con, s, v), if con(s, v) ? UA
i
,
0, otherwise,
f
i
(den, s, v) =
{
S
i
(den, s, v), if den(s, v) ? UA
i
,
0, otherwise.
In contrast with the above action types, both ?af-
firm? and ?negate? don?t specify any target slot and
value information on the SLU results. The feature
functions for these types are defined with (s, v)
derived from the previous ?expl-conf? and ?impl-
conf? system actions as follows:
f
i
(aff, s, v) =
?
?
?
?
?
max
j
(S
ij
(aff)) , if expl-conf(s, v) ? SA
i
,
or impl-conf(s, v) ? SA
i
0, otherwise,
f
i
(neg, s, v) =
?
?
?
?
?
max
j
(S
ij
(neg)) , if expl-conf(s, v) ? SA
i
,
or impl-conf(s, v) ? SA
i
0, otherwise,
where SA
i
is the system actions at the i-th turn.
The user actions with ?request? and ?reqalts?
could be able to play a crucial role to track the
requested slots with the following functions:
f
i
(req, s) =
{
S
i
(req, s), if req(s) ? UA
i
,
0, otherwise,
f
i
(reqalts, s) =
{
S
i
(reqalts, s), if reqalts ? UA
i
,
0, otherwise.
The other function is to indicate whether the
system is able to provide the information on (s, v)
using the ?canthelp? actions as follows:
f
i
(canthelp, s, v) =
{
1, if canthelp(s, v) ? SA
i
,
0, otherwise.
334
Dev set Test set
Acc L2 ROC Acc L2 ROC
Joint Goals
ME 0.638 0.551 0.144 0.596 0.671 0.036
CRF 0.644 0.545 0.103 0.601 0.649 0.064
Method
ME 0.839 0.260 0.398 0.877 0.204 0.397
CRF 0.875 0.202 0.181 0.904 0.155 0.187
Requested Slots
ME 0.946 0.099 0.000 0.957 0.081 0.000
CRF 0.942 0.107 0.000 0.960 0.073 0.000
Table 1: Comparisons of dialog state tracking performances
4 Experiment
To demonstrate the effectiveness of our proposed
sequential labeling approach for dialog state track-
ing, we performed experiments on the DSTC 2
dataset which consists of 3,235 dialog sessions
on restaurant information domain which were col-
lected using Amazon Mechanical Turk. The re-
sults of ASR and SLU are annotated for every
turn in the dataset, as well as the gold standard
annotations are also provided for evaluation. We
used this dataset following the original division
into training/development/test sets, which have
1,612/506/1,117 sessions, respectively.
Using this dataset, we trained two different
types of models: one is based on CRFs for our pro-
posed sequential labeling approach; and the other
is a baseline using maximum entropy (ME) that
performs the prediction for each individual turn
separately from others in a given session. All the
models for both approaches were trained on the
training set with the same feature functions de-
fined in Section 3.3 using MALLET
1
toolkit.
The trained models were used for predicting
goals, method, and requested slots of each turn in
the development and test sets, the results of which
were then organized into a tracker output object
defined as the input format to the evaluation script
of DSTC 2. Since we omitted the joint goals dis-
tributions in the output, the evaluations on the joint
goals were performed on the independent combi-
nations of the slot distributions.
Among the various combinations of evaluation
variables listed in the results of the evaluation
script, the following three featured metrics were
selected to report the performances of the tracker
in this paper: Accuracy, L2 norm, and ROC CA 5.
All these metrics were computed for the predicted
joint goals, method and requested slots.
1
http://mallet.cs.umass.edu/
Table 1 compares the performances of our pro-
posed approach (CRF) and the baseline method
(ME) for three sub-tasks on the development and
test sets. The results indicate that our proposed
sequential labeling approach achieved better per-
formances than the baseline for most cases. Es-
pecially, CRF models produced better joint goals
and method predictions in terms of accuracy and
L2 norm on both development and test sets. For
the requested slots task, our proposed approach
failed to generate better results than the baseline
on the development set. However, this situation
was reversed on the test set, which means our pro-
posed approach achieved better performances on
all three sub-tasks on the test set in two of the three
evaluation metrics.
5 Conclusions
This paper presented a sequential labeling ap-
proach for dialog state tracking. This approach
aimed to solve the cases of goal changes using
linear-chain CRFs. Experimental results show
the merits of our proposed approach with the im-
proved performances on all the sub-tasks of DSTC
2 compared to the baseline which doesn?t consider
sequential aspects.
However, these results are still not enough to
be competitive with the other participants in the
challenge. One possible reason is that our trackers
were trained only on the very basic features in this
work. If we discover more advanced features that
help to track the proper dialog states, they can raise
the overall performances further.
The other direction of our future work is to inte-
grate these dialog state trackers with our existing
dialog systems which accept the 1-best results of
ASR and SLU as they are, then to see their impacts
on the whole system level.
335
References
Dan Bohus and Alex Rudnicky. 2006. A k-
hypotheses+ other belief updating model. In Proc.
of the AAAI Workshop on Statistical and Empirical
Methods in Spoken Dialogue Systems.
Milica Gas?ic? and Steve Young. 2011. Effective
handling of dialogue state in the hidden informa-
tion state pomdp-based dialogue manager. ACM
Transactions on Speech and Language Processing
(TSLP), 7(3):4.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the SIGdial 2014 Con-
ference, Baltimore, U.S.A., June.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, pages
414?422.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the SIG-
DIAL 2013 Conference, pages 442?451.
Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999. Un-
derstanding unsegmented user utterances in real-
time spoken dialogue systems. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 200?207.
Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157?176. Springer.
Antoine Raux and Yi Ma. 2011. Efficient probabilistic
tracking of user goal and dialog history for spoken
dialog systems. In Proceedings of INTERSPEECH,
pages 801?804.
Hang Ren,Weiqun Xu, Yan Zhang, and YonghongYan.
2013. Dialog state tracking using conditional ran-
dom fields. In Proceedings of the SIGDIAL 2013
Conference, pages 457?461.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A pomdp framework for
spoken dialogue systems. Computer Speech & Lan-
guage, 24(4):562?588.
Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the dia-
log state tracking challenge: On the believability of
observed information. In Proceedings of the SIG-
DIAL 2013 Conference, pages 423?432.
Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language,
21(2):393?422.
Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL 2013
Conference, pages 404?413.
Jason D Williams. 2010. Incremental partition re-
combination for efficient tracking of multiple dialog
states. In Acoustics Speech and Signal Processing
(ICASSP), 2010 IEEE International Conference on,
pages 5382?5385. IEEE.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for pomdp-based spoken dia-
logue management. Computer Speech & Language,
24(2):150?174.
Lukas Zilka, David Marek, Matej Korvas, and Filip Ju-
rcicek. 2013. Comparison of bayesian discrimina-
tive and generative models for dialogue state track-
ing. In Proceedings of the SIGDIAL 2013 Confer-
ence, pages 452?456.
336
