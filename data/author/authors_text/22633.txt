Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 87?93,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Language Identification in Code-Switching Scenario
Naman Jain
LTRC, IIIT-H, Hyderabad, India
naman.jain@research.iiit.ac.in
Riyaz Ahmad Bhat
LTRC, IIIT-H, Hyderabad, India
riyaz.bhat@research.iiit.ac.in
Abstract
This paper describes a CRF based token
level language identification system en-
try to Language Identification in Code-
Switched (CS) Data task of CodeSwitch
2014. Our system hinges on using con-
ditional posterior probabilities for the in-
dividual codes (words) in code-switched
data to solve the language identification
task. We also experiment with other lin-
guistically motivated language specific as
well as generic features to train the CRF
based sequence labeling algorithm achiev-
ing reasonable results.
1 Introduction
This paper describes our participation in the Lan-
guage Identification in Code-Switched Data task
at CodeSwitch 2014 (Solorio et al., 2014). The
workshop focuses on NLP approaches for the
analysis and processing of mixed-language data
with a focus on intra sentential code-switching,
while the shared task focuses on the identifica-
tion of the language of each word in a code-
switched data, which is a prerequisite for ana-
lyzing/processing such data. Code-switching is a
sociolinguistics phenomenon, where multilingual
speakers switch back and forth between two or
more common languages or language-varieties,
in the context of a single written or spoken
conversation. Natural language analysis of code-
switched (henceforth CS) data for various NLP
tasks like Parsing, Machine Translation (MT), Au-
tomatic Speech Recognition (ASR), Information
Retrieval (IR) and Extraction (IE) and Semantic
Processing, is more complex than monolingual
data. Traditional NLP techniques perform miser-
ably when processing mixed language data. The
performance degrades at a rate proportional to the
amount and level of code-switching present in the
data. Therefore, in order to process such data,
a separate language identification component is
needed, to first identify the language of individual
words.
Language identification in code-switched data
can be thought of as a sub-task of a document
level language identification task. The latter aims
to identify the language a given document is writ-
ten in (Baldwin and Lui, 2010), while the former
addresses the same problem, however at the token
level. Although, both the problems have separate
goals, they can fundamentally be modeled with a
similar set of features and techniques. However,
language identification at the word level is more
challenging than a typical document level lan-
guage identification problem. The number of fea-
tures available at document level is much higher
than at word level. The available features for word
level identification are word morphology, syllable
structure and phonemic (letter) inventory of the
language(s). Since these features are related to the
structure of a word, letter based n-gram models
have been reported to give reasonably accurate and
comparable results (Dunning, 1994; Elfardy and
Diab, 2012; King and Abney, 2013; Nguyen and
Dogruoz, 2014; Lui et al., 2014). In this work, we
present a token level language identification sys-
tem which mainly hinges on the posterior prob-
abilities computed using n-gram based language
models.
The rest of the paper is organized as follows: In
Section 2, we discuss about the data of the shared
task. In Section 3, we discuss the methodology
we adapted to address the problem of language
identification, in detail. Experiments based on our
methodology are discussed in Section 4. In Sec-
tion 5, we present the results obtained, with a brief
discussion. Finally we conclude in Section 6 with
some future directions.
87
2 Data
The Language Identification in the Code-Switched
(CS) data shared task is meant for language
identification in 4 language pairs (henceforth
LP) namely, Nepali-English (N-E), Spanish-
English (S-E), Mandarin-English (M-E) and Mod-
ern Standard Arabic-Arabic dialects (MSA-A). So
as to get familiar with the training and testing data,
trial data sets consisting of 20 tweets each, corre-
sponding to all the language-pairs, were first re-
leased. Additional test data as ?surprise genre? for
S-E, N-E and MSA-A were also released, which
comprised of data from Facebook, blogs and Ara-
bic commentaries.
2.1 Tag Description
Each word in the training data is classified into
one of the 6 different classes which are, Lang1,
Lang2, Mixed, Other, Ambiguous and NE.
?Lang1? and ?Lang2? tags correspond to words
specific to the languages in an LP. ?Mixed? words
are those words that are partially in both the lan-
guages. ?Ambiguous? words are the ones that
could belong to either of the language. All gib-
berish and unintelligible words and words that
do not belong to any of the languages fall under
?Other? category. ?Named Entities? (NE) com-
prise of proper names that refer to people, places,
organizations, locations, movie titles and song ti-
tles etc.
2.2 Data Format and Data Crawling
Due to Twitter policies, distributing the data di-
rectly is not possible in the shared task and thus the
trial, training and testing data are provided as char
offsets with label information along with tweetID
1
and userID
2
. We use twitter
3
python script to crawl
the tweets and our own python script to further to-
kenize and synchronize the tags in the data.
Since the data for ?surprise genre? comes from
different social media sources, the ID format
varies from file to file but all the other details are
kept as is. In addition to the details, the tokens ref-
erenced by the offsets are provided unlike Twitter
data. (1) and (2) below, show the format of tweets
in train and test data respectively, while (3) shows
a typical tweet in the surprise genre data.
1
Each tweet on Twitter has a unique tweetID
2
Each user on Twitter carries a userID
3
http://emnlp2014.org/workshops/CodeSwitch/scripts/
twitter.zip
(1) TweetID UserID startIndex endIndex Tag
(2) TweetID UserID startIndex endIndex
(3) SocialMediaID UserID startIndex endIn-
dex Word
2.3 Data Statistics
The CS data is divided into two types of
tweets (henceforth posts)
4
namely, Code-switched
posts and Monolingual posts. Table 1 shows the
original number of posts that are released for the
shared task for all LPs, along with their tag counts.
Due to the dynamic nature of social media, the
posts can be either deleted or updated and thus
different participants would have crawled different
number of posts. Thus, to come up with a compa-
rable platform for all the teams, the intersection of
data from all the users is used as final testing data
to report the results. Table 1 shows the number of
tweets or posts in testing data that are finally used
for the evaluation.
3 Methodology
We divided the language identification task into
a pipeline of 3 sub-tasks namely Pre-Processing,
Language Modeling, and Sequence labeling using
CRF
5
. The pipeline is followed for all the LPs with
some LP specific variations in selecting the most
relevant features to boost the results.
3.1 Pre-Processing
In the pre-processing stage, we crawl the tweets
from Twitter given their offsets in the training data
and then tokenize and synchronize the words with
their tags as mentioned in Section 2.2. For each
LP we separate out the tokens into six classes to
use the data for Language Modeling and also to
manually analyze the language specific properties
to be used as features further in sequence labeling
. While synchronizing the words in a tweet with
their tags, we observed that some offsets do not
match with the words and this would lead to mis-
match of labels with tokens and thus degrade the
quality of training data.
To filter out the incorrect instances from the
training data, we frame pattern matching rules
which are specific to the languages present. But
this filtering is done only for the words present in
4
In case of twitter data, we have tweets but in case of sur-
prise genre data we have posts
5
Conditional Random Field
88
Language Pairs # Tweets # Tokens
CodeSwitched Monolingual Ambiguous Lang1 Lang2 Mixed NE Other
T
r
a
i
n
MSA-A dialects 774 5,065 1,066 79,134 16,291 15 14,112 8,699
Mandarin-English 521 478 0 12,114 2,431 12 1,847 1,025
Nepali-English 7,203 2,790 126 45,483 60,697 117 3982 35,651
Spanish-English 3,063 8,337 344 77,107 33,099 51 2,918 27,227
T
e
s
t
MSA-A dialects I 32 2,300 11 44,314 141 0 5,939 3,902
Mandarin-English 247 66 0 4,703 881 1 254 442
Nepali-English 2,665 209 0 12,286 17,216 60 1,071 9,635
Spanish-English 471 1,155 43 7,040 5,549 12 464 4,311
MSA-A dialects II 293 1,484 119 10,459 14,800 2 4,321 2,940
S
u
r
p
r
i
s
e
MSA-A dialects - - 110 2,687 6,930 3 1,097 1,190
Nepali-English 20 82 0 173 699 0 127 88
Spanish-English 22 27 1 636 306 1 38 120
Table 1: Data Statistics
?Lang1? and ?Lang2? classes. There are two rea-
sons to consider these labels. First, ?Lang1? and
?Lang2? classes hold maximum share of words in
any LP as shown in Table 1, and thus have a higher
impact on the overall accuracy of the language
identification system. In addition to the above,
these categories correspond to the focus point of
the shared task. Second, for ?Ambiguous?, ?NE?
and ?Other? categories, it is difficult to find the
patterns according to their definitions. Although
rules can be framed for ?Mixed? category, since
their count is too less as compared to the other
categories (Table 1), it is of no use to train a sepa-
rate language model with very less number of in-
stances.
For Mandarin and Arabic data sets, any word
present in Roman script is excluded from the data.
Similarly for English and Nepali, if any word con-
tains characters other than Roman or numeral they
are excluded from the data. In addition to the
rule for English and Nepali, the additional alpha-
bets in Spanish are also included in the set of Ro-
man and numeral entries. Table 2 shows the num-
ber of words that remained in each of the lan-
guages/dialects, after the preprocessing.
One of the bonus points in the shared task is
that 3 out of 4 LPs share ?English? as their sec-
ond language. In order to increase the training size
for English, we merged all the English words into
a single file and thus reduced the number of lan-
guage models to be trained from 8 to 6, one for
each language (or dialect).
Language Data Size Average Token Length
Arabic 10,380 8.14
English 105,014 3.83
Mandarin 12,874 4.99
MSA 53,953 8.93
Nepali 35,620 4.26
Spanish 32,737 3.96
Table 2: Data Statistics after Filtering
3.2 Language Modeling
In this stage, we train separate smoothed n-gram
based language models for each language in an LP.
We compute the conditional probability for each
word using these language models, which is then
used as a feature, among others for sequence la-
beling to finally predict the tags.
3.2.1 N-gram Language Models
Given a word w, we compute the conditional prob-
ability corresponding to k
6
classes c
1
, c
2
, ... , c
k
as:
p(c
i
|w) = p(w|c
i
) ? p(c
i
) (1)
The prior distribution p(c) of a class is es-
timated from the respective training sets shown
in Table 2. Each training set is used to train a
separate letter-based language model to estimate
the probability of word w. The language model
p(w) is implemented as an n-gram model using
the IRSTLM-Toolkit (Federico et al., 2008) with
Kneser-Ney smoothing. The language model is
6
In our case value of k is 2 as there are 2 languages in an
LP
89
defined as:
p(w) =
n
?
i=1
p(l
i
|l
i?1
i?k
) (2)
where l is a letter and k is a parameter indicating
the amount of context used (e.g., k=4 means 5-
gram model).
3.3 CRF based Sequence Labeling
After Language Modeling, we use CRF-based
(Conditional Random Fields (Lafferty et al.,
2001)) sequence labeling to predict the labels of
words in their surrounding context. The CRF algo-
rithm predicts the class of a word in its surround-
ing context taking into account other features not
explicitly represented in its structure.
3.3.1 Feature Set
In order to train CRF models, we define a feature
set which is a hybrid combination of three sub-
types of features namely, Language Model Fea-
tures (LMF), Language Specific Features (LSF)
and Morphological Features (MF).
LMF: This sub-feature set consists of poste-
rior probability scores calculated using language
models for each language in an LP. Although
we trained language models only for ?Lang1?
and ?Lang2? classes, we computed the probabil-
ity scores for all the words belonging to any of the
categories.
LSF: Each language carries some specific traits
that could assist in language identification. In
this sub-feature set we exploited some of the lan-
guage specific features exclusively based on the
description of the tags provided. The common fea-
tures for all the LPs are HAS NUM (Numeral is
present in the word), HAS PUNC (Punctuation is
present in the word), IS NUM (Word is a numeral),
IS PUNC (word is a punctuation or a collection of
punctuations), STARTS NUM (word starts with a
numeral) and STARTS PUNC (word starts with a
punctuation). All these features are used to gener-
ate variations to distinguish ?Other? class from rest
of the classes during prediction.
Two features exclusively used for the English
sharing LPs are HAS CAPITAL (capital letters are
present in the word) and IS ENGLISH (word be-
longs to English or not). HAS CAPITAL is used
to capture the capitalization property of the En-
glish writing system. This feature is expected to
help in the identification of ?NEs?. IS ENGLISH is
used to indicate whether a word is an valid English
word or not, based on its presence in English dic-
tionaries. We used dictionaries available in PyEn-
chant
7
.
For the M-E LP, we are using ?TYPE?
8
as a
feature with possible values as ENGLISH, MAN-
DARIN, NUM, PUNC and OTHER. If all the
characters in the word are English alphabets EN-
GLISH is taken as the value and Mandarin oth-
erwise. Similar checks are used for NUM and
PUNC types. But if no case is satisfied, OTHER
is taken as the value.
We observed that the above features did not con-
tribute much to distinguish between any of the tags
in case of the MSA-A LP. Since this pair consists
of two different dialects of a language rather than
two different languages, the posterior probabilities
would be close to each other as compared to other
LPs. Thus we use the difference of these probabil-
ities as a feature in order to discriminate ambigu-
ous words or NEs that are spelled similarly.
MF: This sub-feature set comprises of the mor-
phological features corresponding to a word. We
automatically extracted these features using a
python script. The first feature of this set is a bi-
nary length variable (MORE/LESS) depending on
the length of the word with threshold value 4. The
other 8 features capture the prefix and suffix prop-
erties of a word, 4 for each type. In prefix type,
4, 3, 2 and 1 characters, if present, are taken from
the beginning of a word as 4 features. Similarly
for the suffix type, 1, 2, 3 and 4 characters, again
if present, are taken from the end of a word as 4
features. In both the cases if any value is miss-
ing, it is kept as NULL (LL). (4) below, shows
a typical example from English data with the MF
sub-feature set for the word ?one?, where F1 rep-
resents the value of binary length variable, F2-F5
and F6-F9 represent the prefix and suffix features
respectively.
(4) one
Word
Less
F1
LL
F2
one
F3
on
F4
o
F5
LL
F6
one
F7
ne
F8
e
F9
3.3.2 Context Window
Along with the above mentioned features, we
chose an optimal context template to train the CRF
7
PyEnchant is a spell checking library in Python
(http://pythonhosted.org/pyenchant/)
8
Since it captures the properties of IS NUM and
IS PUNC, these features are not used again
90
models. We selected the window size to be 5, with
2 words before and after the target word. Furnish-
ing the training, testing and surprise genre data
with the features discussed in 3.3.1, we trained 4
CRF models on training data using feature tem-
plates based on the context decided. These mod-
els are used to finally predict the tags on the testing
and surprise genre data.
4 Experiments
The pipeline mentioned in Section 3 was used for
the language identification task for all the LPs.
We carried out a series of experiments with pre-
processing to clean the training data and also to
synchronize the testing data. We also did some
post-processing to handle language and tag spe-
cific cases.
In order to generate language model scores,
we trained 6 language models (one for each lan-
guage/dialect) on the filtered-out training data as
mentioned in Table 2. We experimented with dif-
ferent values of n-gram to select the optimal value
based on the F1-measure. Table 3 shows the opti-
mal order of n-gram, selected corresponding to the
highest value of F1-score. Using the optimal value
of n-gram, language models have been trained and
then posterior probabilities have been calculated
using equation (1).
Finally, we trained separate CRF models for
each LP, using the CRF++
9
tool kit based on the
features described in Section 3.3.1 and the feature
template in Section 3.3.2. To empirically find the
relevance of features we also performed leave-one
out experiments so as to decide the optimal fea-
tures for the language identification task (more de-
tails in Section 4.1). Then, using these CRF mod-
els, tags were predicted on the testing and surprise
genre datasets.
Language-Pair N-gram
MSA-A 5
M-E 5
N-E 6
S-E 5
Table 3: Optimal Value of N-gram
4.1 Feature Ranking
We expect that some features would be more im-
portant than others and would impact the task
9
http://crfpp.googlecode.com/svn/trunk/doc/index.html?
source=navbar
of language identification irrespective of the lan-
guage pair. In order to identify such optimal fea-
tures for the language identification task, we rank
them based on their information gain scores.
4.1.1 Information Gain
We used information gain to score features ac-
cording to their expected usefulness for the task at
hand. Information gain is an information theoretic
concept that measures the amount of knowledge
that is gained about a given class by having access
to a particular feature. If f is the occurrence an
individual feature and
?
f the non-occurrence of a
feature, information gain can be measured by the
following formula:
G(x) = P (f)
?
P (y|f)logP (y|f)
+ P (
?
f)
?
logP (y|
?
f)logP (y|
?
f)
(3)
For each language pair, the importance of fea-
ture types are represented by the following order:
? MSA-A dialects: token > word morphology
> posterior probabilities > others
? Mandarin-English: token > posterior prob-
abilities > word morphology > language
type > others
? Nepali-English: token > posterior probabil-
ities > word morphology > dictionary > oth-
ers
? Spanish-English: token > posterior proba-
bilities > word morphology > others > dic-
tionary
Apart from MSA-A dialects, top 3 features sug-
gested by information gain are token and its sur-
rounding context, posterior probabilities and word
morphology. For Arabic dialects word morphol-
ogy is more important than posterior probabilities.
It could be due to the fact that Arabic dialects share
a similar phonetic inventory and thus have similar
posterior probabilities. However, they differ sig-
nificantly in their morphological structure (Zaidan
and Callison-Burch, 2013).
We also carried out leave-one-out experiments
over all the features to ascertain their impact on the
classification performance. The results of these
experiments are shown in Table (5). Accuracies
are averaged over 5-fold cross-validation.
91
Token Level
Language Pairs Ambiguous Lang1 Lang2 Mixed NE Other
R P F1 R P F1 R P F1 R P F1 R P F1 R P F1 Overall Accuracy
T
e
s
t
MSA-A I 0.00 0.00 0.00 0.92 0.95 0.94 0.40 0.03 0.06 - - - 0.70 0.77 0.73 0.90 0.85 0.87 0.90
M-E - - - 0.98 0.98 0.98 0.67 0.66 0.67 0.00 1.00 0.00 0.84 0.38 0.53 0.22 0.71 0.33 0.88
N-E - - - 0.95 0.93 0.94 0.98 0.96 0.97 0.00 1.00 0.00 0.39 0.79 0.52 0.94 0.96 0.95 0.95
S-E 0.00 1.00 0.00 0.88 0.81 0.84 0.83 0.90 0.86 0.00 1.00 0.00 0.16 0.40 0.23 0.83 0.80 0.82 0.83
MSA-A II 0.00 0.00 0.00 0.91 0.47 0.62 0.36 0.84 0.51 0.00 1.00 0.00 0.59 0.80 0.68 0.80 0.71 0.75 0.60
S
u
r
p
r
i
s
e
MSA-A 0.00 0.00 0.00 0.94 0.38 0.54 0.46 0.93 0.61 0.00 1.00 0.00 0.52 0.78 0.62 0.96 0.96 0.96 0.62
N-E - - - 0.92 0.76 0.84 0.95 0.89 0.91 - - - 0.35 0.92 0.50 0.85 0.89 0.87 0.86
S-E 0.00 1.00 0.00 0.86 0.81 0.83 0.82 0.87 0.85 0.00 1.00 0.00 0.15 0.40 0.22 0.82 0.78 0.80 0.94
Table 4: Token Level Results
Left Out Feature MSA-A M-E N-E S-E
Context 76.32 94.07 93.97 92.30
Morphology 79.29 93.67 93.98 93.51
Probability 79.24 89.16 93.86 93.28
Dictionary - 87.75 93.73 92.99
Language Type - 87.97 - -
Others 78.80 83.84 92.10 92.20
All Features 79.37 95.11 94.52 93.54
Table 5: Leave-one-out Experiments
5 Results and Discussion
Each language identification system is evaluated
against two data tracks namely, ?Testing? and ?Sur-
prise Genre? data as mentioned in Section 2. Sur-
prise genre data of Mandarin-English LP was not
provided, so no results are available. All the results
are provided on two levels, comment/post/tweet
and token level. Tables 4 and 6 show results of our
language identification system on both the levels
respectively.
In case of Tweets, systems are evaluated using
the following measures: Accuracy, Recall, Preci-
sion and F-Score. However at token level, sys-
tems are evaluated separately for each tag in an
LP using Recall, Precision and F1-Score as the
measures. Table 4 shows that the results for ?Am-
biguous? and ?Mixed? categories are either miss-
ing (due to absence of tokens in that category), or
have 0.00 F1-Score. One obvious reason could be
the sparsity of data for these categories.
6 Conclusion and Future Work
In this paper, we have described a CRF based to-
ken level language identification system that uses a
set of naive easily computable features guarantee-
ing reasonable accuracies over multiple language
pairs. Our analysis showed that the most important
Language Pairs Tweet Level
Accuracy Recall Precision F-score
T
e
s
t
MSA-A I 0.605 0.719 0.025 0.048
M-E 0.751 0.814 0.863 0.838
N-E 0.948 0.979 0.966 0.972
S-E 0.835 0.773 0.692 0.730
MSA-A II 0.469 0.823 0.213 0.338
S
u
r
p
r
i
s
e
MSA-A 0.457 0.833 0.128 0.222
N-E 0.735 0.900 0.419 0.571
S-E 0.830 0.765 0.689 0.725
Table 6: Comment/Post/Tweet Level Results
feature is the word structure which in our system
is captured by n-gram posterior probabilities and
word morphology. Our analysis of Arabic dialects
shows that word morphology plays an important
role in the identification of mixed codes of closely
related languages.
7 Acknowledgement
We would like to thank Himani Chaudhry for her
valuable comments and suggestions that helped us
to improve the quality of the paper.
References
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Heba Elfardy and Mona T Diab. 2012. Token level
identification of linguistic code switching. In COL-
ING (Posters), pages 287?296.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
92
dling large scale language models. In Interspeech,
pages 1618?1621.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. volume 2, pages 27?40.
Dong Nguyen and A Seza Dogruoz. 2014. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Octobe, 2014, Doha,
Qatar.
Omar F Zaidan and Chris Callison-Burch. 2013. Ara-
bic dialect identification. Computational Linguis-
tics, 40(1):171?202.
93
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 47?55,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Adapting Predicate Frames for Urdu PropBanking
Riyaz Ahmad Bhat
?
, Naman Jain
?
, Dipti Misra Sharma
?
, Ashwini Vaidya
?
,
Martha Palmer
?
, James Babani
?
and Tafseer Ahmed
?
LTRC, IIIT-H, Hyderabad, India
?
University of Colorado, Boulder, CO 80309 USA
?
DHA Suffa University, Karachi, Pakistan
?
{riyaz.bhat, naman.jain}@research.iiit.ac.in, dipti@iiit.ac.in,
{vaidyaa,mpalmer, james.babani}@colorado.edu, tafseer@dsu.edu.pk
Abstract
Hindi and Urdu are two standardized reg-
isters of what has been called the Hindus-
tani language, which belongs to the Indo-
Aryan language family. Although, both
the varieties share a common grammar,
they differ significantly in their vocabulary
to an extent where both become mutually
incomprehensible (Masica, 1993). Hindi
draws its vocabulary from Sanskrit while
Urdu draws its vocabulary from Persian,
Arabic and even Turkish. In this paper,
we present our efforts to adopt frames of
nominal and verbal predicates that Urdu
shares with either Hindi or Arabic for
Urdu PropBanking. We discuss the fea-
sibility of porting such frames from either
of the sources (Arabic or Hindi) and also
present a simple and reasonably accurate
method to automatically identify the ori-
gin of Urdu words which is a necessary
step in the process of porting such frames.
1 Introduction
Hindi and Urdu, spoken primarily in northern In-
dia and Pakistan, are socially and even officially
considered two different language varieties. How-
ever, such a division between the two is not es-
tablished linguistically. They are two standard-
ized registers of what has been called the Hindus-
tani language, which belongs to the Indo-Aryan
language family. Masica (1993) explains that,
while they are different languages officially, they
are not even different dialects or sub-dialects in
a linguistic sense; rather, they are different liter-
ary styles based on the same linguistically defined
sub-dialect. He further explains that at the collo-
quial level, Hindi and Urdu are nearly identical,
both in terms of core vocabulary and grammar.
However, at formal and literary levels, vocabu-
lary differences begin to loom much larger (Hindi
drawing its higher lexicon from Sanskrit and Urdu
from Persian and Arabic) to the point where the
two styles/languages become mutually unintelligi-
ble. In written form, not only the vocabulary but
the way Urdu and Hindi are written makes one be-
lieve that they are two separate languages. They
are written in separate orthographies, Hindi be-
ing written in Devanagari, and Urdu in a modi-
fied Persio-Arabic script. Given such (apparent)
divergences between the two varieties, two paral-
lel treebanks are being built under The Hindi-Urdu
treebanking Project (Bhatt et al., 2009; Xia et al.,
2009). Both the treebanks follow a multi-layered
and multi-representational framework which fea-
tures Dependency, PropBank and Phrase Structure
annotations. Among the two treebanks the Hindi
treebank is ahead of the Urdu treebank across all
layers. In the case of PropBanking, the Hindi tree-
bank has made considerable progress while Urdu
PropBanking has just started.
The creation of predicate frames is the first step
in PropBanking, which is followed by the actual
annotation of verb instances in corpora. In this
paper, we look at the possibility of porting re-
lated frames from Arabic and Hindi PropBanks for
Urdu PropBanking. Given that Urdu shares its vo-
cabulary with Arabic, Hindi and Persian, we look
at verbal and nominal predicates that Urdu shares
with these languages and try to port and adapt their
frames from the respective PropBanks instead of
creating them afresh. This implies that identifi-
cation of the source of Urdu predicates becomes
a necessary step in this process. Thus, in order
to port the relevant frames, we need to first iden-
tify the source of Urdu predicates and then extract
their frames from the related PropBanks. To state
briefly, we present the following as contributions
of this paper:
? Automatic identification of origin or source
of Urdu vocabulary.
47
? Porting and adapting nominal and verbal
predicate frames from the PropBanks of re-
lated languages.
The rest of the paper is organised as follows: In
the next Section we discuss the Hindi-Urdu tree-
banking project with the focus on PropBanking.
In Section 3, we discuss our efforts to automati-
cally identify the source of Urdu vocabulary and
in Section 4, we discuss the process of adapting
and porting Arabic and Hindi frames for Urdu
PropBanking. Finally we conclude with some
future directions in Section 5.
2 A multi-layered,
multi-representational treebank
Compared to other existing treebanks, Hindi/Urdu
Treebanks (HTB/UTB) are unusual in that they are
multi-layered. They contain three layers of anno-
tation: dependency structure (DS) for annotation
of modified-modifier relations, PropBank-style
annotation (PropBank) for predicate-argument
structure, and an independently motivated phrase-
structure (PS). Each layer has its own framework,
annotation scheme, and detailed annotation guide-
lines. Due to lack of space and relevance to our
work, we only look at PropBanking with reference
to Hindi PropBank, here.
2.1 PropBank Annotation
The first PropBank, the English PropBank (Kings-
bury and Palmer, 2002), originated as a one-
million word subset of the Wall Street Journal
(WSJ) portion of Penn Treebank II (an English
phrase structure treebank). The verbs in the Prop-
Bank are annotated with predicate-argument struc-
tures and provide semantic role labels for each
syntactic argument of a verb. Although these
were deliberately chosen to be generic and theory-
neutral (e.g., ARG0, ARG1), they are intended
to consistently annotate the same semantic role
across syntactic variations. For example, in both
the sentences John broke the window and The win-
dow broke, ?the window? is annotated as ARG1
and as bearing the role of ?Patient?. This reflects
the fact that this argument bears the same seman-
tic role in both the cases, even though it is realized
as the structural subject in one sentence and as the
object in the other. This is the primary difference
between PropBank?s approach to semantic role la-
bels and the Paninian approach to karaka labels,
which it otherwise resembles closely. PropBank?s
ARG0 and ARG1 can be thought of as similar
to Dowty?s prototypical ?Agent? and ?Patient?
(Dowty, 1991). PropBank provides, for each sense
of each annotated verb, its ?roleset?, i.e., the possi-
ble arguments of the predicate, their labels and all
possible syntactic realizations. The primary goal
of PropBank is to supply consistent, simple, gen-
eral purpose labeling of semantic roles for a large
quantity of coherent text that can provide training
data for supervised machine learning algorithms,
in the same way that the Penn Treebank supported
the training of statistical syntactic parsers.
2.1.1 Hindi PropBank
The Hindi PropBank project has differed signif-
icantly from other PropBank projects in that the
semantic role labels are annotated on dependency
trees rather than on phrase structure trees. How-
ever, it is similar in that semantic roles are defined
on a verb-by-verb basis and the description at
the verb-specific level is fine-grained; e.g., a
verb like ?hit? will have ?hitter? and ?hittee?.
These verb-specific roles are then grouped into
broader categories using numbered arguments
(ARG). Each verb can also have a set of modifiers
not specific to the verb (ARGM). In Table 1,
PropBank-style semantic roles are listed for
the simple verb de ?to give?. In the table, the
numbered arguments correspond to the giver,
thing given and recipient. Frame file definitions
are created manually and include role information
as well as a unique roleset ID (e.g. de.01 in Table
1), which is assigned to every sense of a verb. In
addition, for Hindi the frame file also includes the
transitive and causative forms of the verb (if any).
Thus, the frame file for de ?give? will include
dilvaa ?cause to give?.
de.01 to give
Arg0 the giver
Arg1 thing given
Arg2 recipient
Table 1: A Frame File
The annotation process for the PropBank takes
place in two stages: the creation of frame files for
individual verb types, and the annotation of pred-
icate argument structures for each verb instance.
The annotation for each predicate in the corpus
is carried out based on its frame file definitions.
48
The PropBank makes use of two annotation tools
viz. Jubilee (Choi et al., 2010b) and Cornerstone
(Choi et al., 2010a) for PropBank instance annota-
tion and PropBank frame file creation respectively.
For annotation of the Hindi and Urdu PropBank,
the Jubilee annotation tool had to be modified to
display dependency trees and also to provide ad-
ditional labels for the annotation of empty argu-
ments.
3 Identifying the source of Urdu
Vocabulary
Predicting the source of a word is similar to lan-
guage identification where the task is to identify
the language a given document is written in. How-
ever, language identification at word level is more
challenging than a typical document level lan-
guage identification problem. The number of fea-
tures available at document level is much higher
than at word level. The available features for word
level identification are word morphology, syllable
structure and phonemic (letter) inventory of the
language(s).
In the case of Urdu, the problem is even more
complex as the borrowed words don?t necessarily
carry the inflections of their source language and
don?t retain their identity as such (they undergo
phonetic changes as well). For example, khabar
?news? which is an Arabic word declines as per
the morphological paradigm of feminine nom-
inals in Hindi and Urdu as shown in Table (2).
However, despite such challenges, if we look at
the character histogram in Figure (1), we can still
identify the source of a sufficiently large portion
of Urdu vocabulary just by using letter-based
heuristics. For example neither Arabic nor Persian
has aspirated consonants like bH, ph Aspirated
Bilabial Plosives; tSh, dZH Aspirated Alveolar
Fricatives; ?H Aspirated Retroflex Plosive; gH, kh
Aspirated Velar Plosives etc. while Hindi does.
Similarly, the following sounds occur only in
Arabic and Persian: Z Fricative Postalveolar; T,
D Fricative Dental; ? Fricative Pharyngeal; X
Fricative Uvular etc. Using these heuristics we
could identify 2,682 types as Indic, and 3,968
as either Persian or Arabic out of 12,223 unique
types in the Urdu treebank (Bhat and Sharma,
2012).
Singular Plural
Direct khabar khabarain
Oblique khabar khabaron
Table 2: Morphological Paradigm of khabar
This explains the efficiency of n-gram based ap-
proaches to either document level or word level
language identification tasks as reported in the re-
cent literature on the problem (Dunning, 1994;
Elfardy and Diab, 2012; King and Abney, 2013;
Nguyen and Dogruoz, 2014; Lui et al., 2014).
In order to predict the source of an Urdu word,
we frame two classification tasks: (1) binary clas-
sification into Indic and Persio-Arabic and, (2) tri-
class classification into Arabic, Indic and Persian.
Both the problems are modeled using smoothed n-
gram based language models.
3.1 N-gram Language Models
Given a word w to classify into one of k classes
c
1
, c
2
, ... , c
k
, we will choose the class with the
maximum conditional probability:
c
?
= argmax
c
i
p(c
i
|w)
= argmax
c
i
p(w|c
i
) ? p(c
i
)
(1)
The prior distribution p(c) of a class is esti-
mated from the respective training sets shown in
Table (3). Each training set is used to train a
separate letter-based language model to estimate
the probability of word w. The language model
p(w) is implemented as an n-gram model using
the IRSTLM-Toolkit (Federico et al., 2008) with
Kneser-Ney smoothing. The language model is
defined as:
p(w) =
n
?
i=1
p(l
i
|l
i?1
i?k
) (2)
where, l is a letter and k is a parameter indicat-
ing the amount of context used (e.g., k = 4 means
5-gram model).
3.2 Etymological Data
In order to prepare training and testing data
marked with etymological information for our
classification experiments, we used the Online
1
http://www.langsci.ucl.ac.uk/ipa/IPA chart %28C%
292005.pdf
49
bH Z T ? X D sQ tQ dQ Q DQ G f tSh q ?H gH khdZH N ? ph S ? th t?h d?H tS b d g H k dZ m l n p s r t t? V j d?
0
5 ? 10
?2
0.1
0.15
0.2
0.25
0.3
0.35
Alphabets in IPA
1
R
e
l
a
t
i
v
e
F
r
e
q
u
e
n
c
y
Arabic
Hindi
Persian
Urdu
Figure 1: Relative Distribution of Arabic, Hindi, Persian and Urdu Alphabets (Consonants only)
Urdu Dictionary
2
(henceforth OUD). OUD has
been prepared under the supervision of the e-
government Directorate of Pakistan
3
. Apart from
basic definition and meaning, it provides etymo-
logical information for more than 120K Urdu
words. Since the dictionary is freely
4
available
and requires no expertise for extraction of word
etymology which is usually the case with manual
annotation, we could mark the etymological infor-
mation on a reasonably sized word list in a limited
time frame. The statistics are provided in Table
(3). We use Indic as a cover term for all the words
that are either from Sanskrit, Prakrit, Hindi or lo-
cal languages.
Language Data Size Average Token Length
Arabic 6,524 6.8
Indic 3,002 5.5
Persian 4,613 6.5
Table 3: Statistics of Etymological Data
2
http://182.180.102.251:8081/oud/default.aspx
3
www.e-government.gov.pk
4
We are not aware of an offline version of OUD.
3.3 Experiments
We carried out a number of experiments in order
to explore the effect of data size and the order of
n-gram models on the classification performance.
By varying the size of training data, we wanted to
identify the lower bound on the training size with
respect to the classification performance. We var-
ied the training size per training iteration by 1%
for n-grams in the order 1-5 for both the classifi-
cation problems. For each n-gram order 100 ex-
periments were carried out, i.e overall 800 exper-
iments for binary and tri-class classification. The
impact of training size on the classification perfor-
mance is shown in Figures (2) and (3) for binary
and tri-class classification respectively. As ex-
pected, at every iteration the additional data points
introduced into the training data increased the per-
formance of the model. With a mere 3% of the
training data, we could reach a reasonable accu-
racy of 0.85 in terms of F-score for binary classi-
fication and for tri-class classification we reached
the same accuracy with 6% of the data.
Similarly, we tried different order n-gram mod-
els to quantify the effect of character context on
50
the classification performance. As with the in-
crease in data size, increasing the n-gram order
profoundly improved the results. In both the clas-
sification tasks, unigram based models converge
faster than the higher order n-gram based models.
The obvious reason for it is the small, finite set of
characters that a language operates with (? 37 in
Arabic, ? 39 in Persian and ? 48 in Hindi). A
small set of words (unique in our case) is probably
enough to capture at least a single instance of each
character. As no new n-gram is introduced with
subsequent additions of new tokens in the training
data, the accuracy stabilizes. However, the accu-
racy with higher order n-grams kept on increas-
ing with an increase in the data size, though it was
marginal after 5-grams. The abrupt increase after
8,000 training instances is probably due to the ad-
dition of an unknown bigram sequence(s) to the
training data. In particular, the Recall of Persio-
Arabic increased by 2.2%.
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 2: Learning Curves for Binary Classifica-
tion of Urdu Vocabulary
3.4 Results
We performed 10-fold cross validation over all the
instances of the etymological data for both the bi-
nary and tri-class classification tasks. We split the
data into training and testing sets with a ratio of
80:20 using the stratified sampling. Stratified sam-
pling distributes the samples of each class in train-
ing and testing sets with the same percentage as in
the complete set. For all the 10-folds, the order of
0 0.2 0.4 0.6 0.8 1 1.2
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Data Size
F
-
S
c
o
r
e
1-gram
2-gram
3-gram
4-gram
Figure 3: Learning Curves for Tri-class Classifi-
cation of Urdu Vocabulary
n-gram was varied again from 1-5. Tables (4) and
(5) show the consolidated results for these tasks
with a frequency based baseline to evaluate the
classification performance. In both the tasks, we
achieved highest accuracy with language models
trained with 5-gram letter sequence context. The
best results in terms of F-score are 0.96 and 0.93
for binary and tri-class classification respectively.
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.40 0.50 0.40
1-gram 0.89 0.89 0.89
2-gram 0.95 0.95 0.95
3-gram 0.96 0.96 0.96
4-gram 0.96 0.96 0.96
5-gram 0.96 0.96 0.96
Table 4: Results of 10-fold Cross Validation on
Binary Classification
Although, we have achieved quite reasonable
accuracies in both the tasks, a closer look at the
confusion matrices shown in Tables (6) and (7)
show that we can still improve the accuracies by
balancing the size of data across classes. In binary
classification our model is more biased towards
Persio-Arabic as the data is highly imbalanced.
Our binary classifier misclassifies 0.86% of Indic
tokens as Persio-Arabic since the prior probability
of the latter is much higher than that of the former.
While in the case of tri-class classification, using
51
Type Precision (P) Recall (R) F1-Score (F)
Baseline 0.15 0.33 0.21
1-gram 0.83 0.83 0.83
2-gram 0.89 0.89 0.89
3-gram 0.91 0.91 0.91
4-gram 0.93 0.93 0.93
5-gram 0.93 0.93 0.93
Table 5: Results of 10-fold Cross Validation on
Tri-Class Classification
higher order n-gram models can resolve the
prominent confusion between Arabic and Persian.
Since both Arabic and Persian share almost the
same phonetic inventory, working with lower
order n-gram models doesn?t seem ideal.
Class Indic Persio-Arabic
Indic 235 60
Persio-Arabic 15 1,057
Table 6: Confusion Matrix of Binary Classifica-
tion
Class Arabic Indic Persian
Arabic 605 5 26
Indic 11 268 18
Persian 22 9 415
Table 7: Confusion Matrix of Tri-class Classifica-
tion
4 Adapting Frames from Arabic and
Hindi PropBanks
As discussed in Section 2.1.1, the creation of pred-
icate frames precedes the actual annotation of verb
instances in a given corpus. In this section, we de-
scribe our approach towards the first stage of Urdu
PropBanking by adapting related predicate frames
from Arabic and Hindi PropBanks (Palmer et al.,
2008; Vaidya et al., 2011). Since a PropBank
is not available for Persian, we could only adapt
those predicate frames which are shared with Ara-
bic and Hindi.
Although, Urdu shares or borrows most of its
literary vocabulary from Arabic and Persian, it re-
tains its simple verb (as opposed to compound or
complex verbs) inventory from Indo-Aryan ances-
try. Verbs from Arabic and Persian are borrowed
less frequently, although there are examples such
as ?khariid? buy, ?farma? say etc.
5
This over-
lap in the verb inventory between Hindi and Urdu
might explain the fact that they share the same
grammar.
The fact that Urdu shares its lexicon with these
languages, prompted us towards exploring the
possibility of using their resources for Urdu Prop-
Banking. We are in the process of adapting frames
for those Urdu predicates that are shared with ei-
ther Arabic or Hindi.
Urdu frame file creation must be carried out for
both simple verbs and complex predicates. Since
Urdu differs very little in simple verb inventory
from Hindi, this simplifies the development pro-
cess as the frames could be ported easily. How-
ever, this is not the case with nominal predicates.
In Urdu, many nominal predicates are borrowed
from Arabic or Persian as shown in Table (8).
Given that a PropBank for Persian is not available,
the task of creating the frames for nominal predi-
cates in Urdu would have been fairly daunting in
the paucity of the Arabic PropBank, as well.
Simple Verbs Nominal Predicates
Language Total Unique Total Unique
Arabic 12 1 6,780 765
Hindi 7,332 441 1,203 258
Persian 69 3 2,276 352
Total 7,413 445 10,259 1,375
Table 8: Urdu Treebank Predicate Statistics
4.1 Simple Verbs
The simple verb inventory of Urdu and Hindi is
almost similar, so the main task was to locate and
extract the relevant frames from Hindi frame files.
Fortunately, with the exception of farmaa ?say?,
all the other simple verbs which Urdu borrows
from Persian or Arabic (cf. Table (8)) were also
borrowed by Hindi. Therefore, the Hindi sim-
ple verb frame files sufficed for porting frames for
Urdu simple verbs.
There were no significant differences found be-
tween the Urdu and Hindi rolesets, which describe
either semantic variants of the same verb or its
causative forms. Further, in order to name the
frame files with their corresponding Urdu lemmas,
we used Konstanz?s Urdu transliteration scheme
5
Borrowed verbs often do not function as simple verbs
rather they are used like nominals in complex predicate con-
structions such as mehsoos in ?mehsoos karnaa? to feel.
52
(Malik et al., 2010) to convert a given lemma into
its romanized form. Since the Hindi frame files
use the WX transliteration scheme
6
, which is not
appropriate for Urdu due to lack of coverage for
Persio-Arabic phonemes or sounds like dQ ?pha-
ryngealized voiced alveolar stop?. The frame files
also contain example sentences for each predicate,
in order to make the PropBank annotation task eas-
ier. While adapting the frame files from Hindi
to Urdu, simply transliterating such examples for
Urdu predicates was not always an option, because
sentences consisting of words with Sanskrit origin
may not be understood by Urdu speakers. Hence,
all the examples in the ported frames have been
replaced with Urdu sentences by an Urdu expert.
In general we find that the Urdu verbs are quite
similar to Hindi verbs, and this simplified our task
of adapting the frames for simple verbs. The
nouns, however, show more variation. Since a
large proportion (up to 50%) of Urdu predicates
are expressed using verb-noun complex predi-
cates, nominal predicates play a crucial role in our
annotation process and must be accounted for.
4.2 Complex Predicates
In the Urdu treebank, there are 17,672 predicates,
of which more than half have been identified as
noun-verb complex predicates (NVC) at the de-
pendency level. Typically, a noun-verb complex
predicate chorii ?theft? karnaa ?to do? has two
components: a noun chorii and a light verb karnaa
giving us the meaning ?steal?. The verbal compo-
nent in NVCs has reduced predicating power (al-
though it is inflected for person, number, and gen-
der agreement as well as tense, aspect and mood)
and its nominal complement is considered the true
predicate. In our annotation of NVCs, we fol-
low a procedure common to all PropBanks, where
we create frame files for the nominal or the ?true?
predicate (Hwang et al., 2010). An example of a
frame file for a noun such as chorii is described in
Table (9).
The creation of a frame file for the set of
true predicates that occur in an NVC is impor-
tant from the point of view of linguistic annota-
tion. Given the large number of NVCs, a semi-
automatic method has been proposed for creating
Hindi nominal frame files, which saves the man-
ual effort required for creating frames for nearly
6
http://en.wikipedia.org/wiki/WX notation
Frame file for chorii-n(oun)
chorii.01: theft-n light verb: kar?do; to steal?
Arg0 person who steals
Arg1 thing stolen
chorii.02 : theft-n light verb: ho ?be/become; to
get stolen?
Arg1 thing stolen
Table 9: Frame file for predicate noun chorii
?theft? with two frequently occurring light verbs
ho and kar. If other light verbs are found to occur,
they are added as additional rolesets as chorii.03,
chorii.04 and so on.
3,015 unique Hindi noun and light verb combina-
tions (Vaidya et al., 2013).
For Urdu, the process of nominal frame file cre-
ation is preceded by the identification of the ety-
mological origin for each nominal. If that nomi-
nal has an Indic or Arabic origin, relevant frames
from Arabic or Hindi PropBanks were adapted for
Urdu. On the other hand, if the Urdu nominal orig-
inates from Persian, then frame creation will be
done either manually or using other available Per-
sian language resources, in the future.
In Table (8), there are around 258 nominal pred-
icates that are common in Hindi and Urdu, so we
directly ported their frames from Hindi PropBank
with minor changes as was done for simple verb
frames. Out of 765 nominal predicates shared with
Arabic, 308 nominal predicate frames have been
ported to Urdu. 98 of these nominal predicate
frames were already present in the Arabic Prop-
Bank and were ported as such. However, for the
remaining 667 unique predicates, frames are be-
ing created manually by Arabic PropBanking ex-
perts and will be ported to Urdu once they become
available.
Porting of Arabic frames to Urdu is not that triv-
ial. We observed that while Urdu borrows vocabu-
lary from Arabic it does not borrow all the senses
for some words. In such cases, the rolesets that are
irrelevant to Urdu have to be discarded manually.
The example sentences for all the frames ported
from Arabic PropBank have to be sourced from
either the web or manually created by an Urdu ex-
pert, as was the case with Hindi simple verbs.
5 Conclusion
In this paper we have exploited the overlap be-
tween the lexicon of Urdu, Arabic and Hindi for
the creation of predicate frames for Urdu Prop-
53
Banking. We presented a simple and accurate clas-
sifier for the identification of source or origin of
Urdu vocabulary which is a necessary step in the
overall process of extraction of predicate frames
from the related PropBanks. In the case of sim-
ple verbs that occur in the Urdu treebank, we have
extracted all the frames from the Hindi PropBank
and adapted them for Urdu PropBanking. Simi-
larly for complex predicates, frames for Urdu tree-
bank nominal predicates are extracted from Hindi
as well as from Arabic PropBanks. Since a Prop-
Bank is not available for Persian, the creation
of frames for shared predicates with Persian is a
prospect for future work. We plan to create these
frames either manually or semi-automatically, us-
ing the available Persian Dependency treebanks
(Rasooli et al., 2011; Rasooli et al., 2013).
Acknowledgments
We would like to thank Himani Chaudhry for her
valuable comments that helped to improve the
quality of this paper.
The work reported in this paper is supported by
the NSF grant (Award Number: CNS 0751202;
CFDA Number: 47.070)
7
.
References
Riyaz Ahmad Bhat and Dipti Misra Sharma. 2012.
A dependency treebank of urdu and its evaluation.
In Proceedings of the Sixth Linguistic Annotation
Workshop, pages 157?165. Association for Compu-
tational Linguistics.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for hindi/urdu. In Proceedings of the Third
Linguistic Annotation Workshop, pages 186?189.
Association for Computational Linguistics.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010a. Propbank frameset annotation guidelines us-
ing a dedicated editor, cornerstone. In LREC.
Jinho D Choi, Claire Bonial, and Martha Palmer.
2010b. Propbank instance annotation guidelines us-
ing a dedicated editor, jubilee. In LREC.
David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547?619.
7
Any opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the author(s)
and do not necessarily reflect the views of the National Sci-
ence Foundation.
Ted Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Heba Elfardy and Mona T Diab. 2012. Token level
identification of linguistic code switching. In COL-
ING (Posters), pages 287?296.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
dling large scale language models. In Interspeech,
pages 1618?1621.
Jena D Hwang, Archna Bhatia, Clare Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and
Martha Palmer. 2010. Propbank annotation of mul-
tilingual light verb constructions. In Proceedings of
the Fourth Linguistic Annotation Workshop, pages
82?90. Association for Computational Linguistics.
Ben King and Steven P Abney. 2013. Labeling the
languages of words in mixed-language documents
using weakly supervised methods. In HLT-NAACL,
pages 1110?1119.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In LREC. Citeseer.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation for Computational Linguistics, 2:27?40.
Muhammad Kamran Malik, Tafseer Ahmed, Sebastian
Sulger, Tina B?ogel, Atif Gulzar, Ghulam Raza, Sar-
mad Hussain, and Miriam Butt. 2010. Transliterat-
ing urdu for a broad-coverage urdu/hindi lfg gram-
mar. In LREC.
Colin P Masica. 1993. The Indo-Aryan Languages.
Cambridge University Press.
Dong Nguyen and A Seza Dogruoz. 2014. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Martha Palmer, Olga Babko-Malaya, Ann Bies,
Mona T Diab, Mohamed Maamouri, Aous Man-
souri, and Wajdi Zaghouani. 2008. A pilot arabic
propbank. In LREC.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
persian verbs: The first steps towards persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of a
persian syntactic dependency treebank. In Proceed-
ings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
54
Linguistics: Human Language Technologies, pages
306?314.
Ashwini Vaidya, Jinho D Choi, Martha Palmer, and
Bhuvana Narasimhan. 2011. Analysis of the hindi
proposition bank using dependency structure. In
Proceedings of the 5th Linguistic Annotation Work-
shop, pages 21?29. Association for Computational
Linguistics.
Ashwini Vaidya, Martha Palmer, and Bhuvana
Narasimhan. 2013. Semantic roles for nominal
predicates: Building a lexical resource. NAACL
HLT 2013, 13:126.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In The 7th International
Workshop on Treebanks and Linguistic Theories.
Groningen, Netherlands.
55
