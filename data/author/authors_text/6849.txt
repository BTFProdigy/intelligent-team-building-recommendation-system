Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 915?932,
Prague, June 2007. c?2007 Association for Computational Linguistics
The CoNLL 2007 Shared Task on Dependency Parsing
Joakim Nivre?? Johan Hall? Sandra Ku?bler? Ryan McDonald??
Jens Nilsson? Sebastian Riedel?? Deniz Yuret??
?Va?xjo? University, School of Mathematics and Systems Engineering, first.last@vxu.se
?Uppsala University, Dept. of Linguistics and Philology, joakim.nivre@lingfil.uu.se
?Indiana University, Department of Linguistics, skuebler@indiana.edu
??Google Inc., ryanmcd@google.com
??University of Edinburgh, School of Informatics, S.R.Riedel@sms.ed.ac.uk
??Koc? University, Dept. of Computer Engineering, dyuret@ku.edu.tr
Abstract
The Conference on Computational Natural
Language Learning features a shared task, in
which participants train and test their learn-
ing systems on the same data sets. In 2007,
as in 2006, the shared task has been devoted
to dependency parsing, this year with both a
multilingual track and a domain adaptation
track. In this paper, we define the tasks of the
different tracks and describe how the data
sets were created from existing treebanks for
ten languages. In addition, we characterize
the different approaches of the participating
systems, report the test results, and provide
a first analysis of these results.
1 Introduction
Previous shared tasks of the Conference on Compu-
tational Natural Language Learning (CoNLL) have
been devoted to chunking (1999, 2000), clause iden-
tification (2001), named entity recognition (2002,
2003), and semantic role labeling (2004, 2005). In
2006 the shared task was multilingual dependency
parsing, where participants had to train a single
parser on data from thirteen different languages,
which enabled a comparison not only of parsing and
learning methods, but also of the performance that
can be achieved for different languages (Buchholz
and Marsi, 2006).
In dependency-based syntactic parsing, the task is
to derive a syntactic structure for an input sentence
by identifying the syntactic head of each word in the
sentence. This defines a dependency graph, where
the nodes are the words of the input sentence and the
arcs are the binary relations from head to dependent.
Often, but not always, it is assumed that all words
except one have a syntactic head, which means that
the graph will be a tree with the single independent
word as the root. In labeled dependency parsing, we
additionally require the parser to assign a specific
type (or label) to each dependency relation holding
between a head word and a dependent word.
In this year?s shared task, we continue to explore
data-driven methods for multilingual dependency
parsing, but we add a new dimension by also intro-
ducing the problem of domain adaptation. The way
this was done was by having two separate tracks: a
multilingual track using essentially the same setup
as last year, but with partly different languages, and
a domain adaptation track, where the task was to use
machine learning to adapt a parser for a single lan-
guage to a new domain. In total, test results were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). Not everyone submitted papers describ-
ing their system, and some papers describe more
than one system (or the same system in both tracks),
which explains why there are only (!) twenty-one
papers in the proceedings.
In this paper, we provide task definitions for the
two tracks (section 2), describe data sets extracted
from available treebanks (section 3), report results
for all systems in both tracks (section 4), give an
overview of approaches used (section 5), provide a
first analysis of the results (section 6), and conclude
with some future directions (section 7).
915
2 Task Definition
In this section, we provide the task definitions that
were used in the two tracks of the CoNLL 2007
Shard Task, the multilingual track and the domain
adaptation track, together with some background
and motivation for the design choices made. First
of all, we give a brief description of the data format
and evaluation metrics, which were common to the
two tracks.
2.1 Data Format and Evaluation Metrics
The data sets derived from the original treebanks
(section 3) were in the same column-based format
as for the 2006 shared task (Buchholz and Marsi,
2006). In this format, sentences are separated by a
blank line; a sentence consists of one or more to-
kens, each one starting on a new line; and a token
consists of the following ten fields, separated by a
single tab character:
1. ID: Token counter, starting at 1 for each new
sentence.
2. FORM: Word form or punctuation symbol.
3. LEMMA: Lemma or stem of word form, or an
underscore if not available.
4. CPOSTAG: Coarse-grained part-of-speech tag,
where the tagset depends on the language.
5. POSTAG: Fine-grained part-of-speech tag,
where the tagset depends on the language, or
identical to the coarse-grained part-of-speech
tag if not available.
6. FEATS: Unordered set of syntactic and/or mor-
phological features (depending on the particu-
lar language), separated by a vertical bar (|), or
an underscore if not available.
7. HEAD: Head of the current token, which is
either a value of ID or zero (0). Note that,
depending on the original treebank annotation,
there may be multiple tokens with HEAD=0.
8. DEPREL: Dependency relation to the HEAD.
The set of dependency relations depends on
the particular language. Note that, depending
on the original treebank annotation, the depen-
dency relation when HEAD=0 may be mean-
ingful or simply ROOT.
9. PHEAD: Projective head of current token,
which is either a value of ID or zero (0), or an
underscore if not available.
10. PDEPREL: Dependency relation to the
PHEAD, or an underscore if not available.
The PHEAD and PDEPREL were not used at all
in this year?s data sets (i.e., they always contained
underscores) but were maintained for compatibility
with last year?s data sets. This means that, in prac-
tice, the first six columns can be considered as input
to the parser, while the HEAD and DEPREL fields
are the output to be produced by the parser. Labeled
training sets contained all ten columns; blind test
sets only contained the first six columns; and gold
standard test sets (released only after the end of the
test period) again contained all ten columns. All data
files were encoded in UTF-8.
The official evaluation metric in both tracks was
the labeled attachment score (LAS), i.e., the per-
centage of tokens for which a system has predicted
the correct HEAD and DEPREL, but results were
also reported for unlabeled attachment score (UAS),
i.e., the percentage of tokens with correct HEAD,
and the label accuracy (LA), i.e., the percentage of
tokens with correct DEPREL. One important differ-
ence compared to the 2006 shared task is that all to-
kens were counted as ?scoring tokens?, including in
particular all punctuation tokens. The official eval-
uation script, eval07.pl, is available from the shared
task website.1
2.2 Multilingual Track
The multilingual track of the shared task was orga-
nized in the same way as the 2006 task, with an-
notated training and test data from a wide range of
languages to be processed with one and the same
parsing system. This system must therefore be able
to learn from training data, to generalize to unseen
test data, and to handle multiple languages, possi-
bly by adjusting a number of hyper-parameters. Par-
ticipants in the multilingual track were expected to
submit parsing results for all languages involved.
1http://depparse.uvt.nl/depparse-wiki/SoftwarePage
916
One of the claimed advantages of dependency
parsing, as opposed to parsing based on constituent
analysis, is that it extends naturally to languages
with free or flexible word order. This explains the
interest in recent years for multilingual evaluation
of dependency parsers. Even before the 2006 shared
task, the parsers of Collins (1997) and Charniak
(2000), originally developed for English, had been
adapted for dependency parsing of Czech, and the
parsing methodology proposed by Kudo and Mat-
sumoto (2002) and Yamada and Matsumoto (2003)
had been evaluated on both Japanese and English.
The parser of McDonald and Pereira (2006) had
been applied to English, Czech and Danish, and the
parser of Nivre et al (2007) to ten different lan-
guages. But by far the largest evaluation of mul-
tilingual dependency parsing systems so far was the
2006 shared task, where nineteen systems were eval-
uated on data from thirteen languages (Buchholz and
Marsi, 2006).
One of the conclusions from the 2006 shared task
was that parsing accuracy differed greatly between
languages and that a deeper analysis of the factors
involved in this variation was an important problem
for future research. In order to provide an extended
empirical foundation for such research, we tried to
select the languages and data sets for this year?s task
based on the following desiderata:
? The selection of languages should be typolog-
ically varied and include both new languages
and old languages (compared to 2006).
? The creation of the data sets should involve as
little conversion as possible from the original
treebank annotation, meaning that preference
should be given to treebanks with dependency
annotation.
? The training data sets should include at least
50,000 tokens and at most 500,000 tokens.2
The final selection included data from Arabic,
Basque, Catalan, Chinese, Czech, English, Greek,
Hungarian, Italian, and Turkish. The treebanks from
2The reason for having an upper bound on the training set
size was the fact that, in 2006, some participants could not train
on all the data for some languages because of time limitations.
Similar considerations also led to the decision to have a smaller
number of languages this year (ten, as opposed to thirteen).
which the data sets were extracted are described in
section 3.
2.3 Domain Adaptation Track
One well known characteristic of data-driven pars-
ing systems is that they typically perform much
worse on data that does not come from the train-
ing domain (Gildea, 2001). Due to the large over-
head in annotating text with deep syntactic parse
trees, the need to adapt parsers from domains with
plentiful resources (e.g., news) to domains with lit-
tle resources is an important problem. This prob-
lem is commonly referred to as domain adaptation,
where the goal is to adapt annotated resources from
a source domain to a target domain of interest.
Almost all prior work on domain adaptation as-
sumes one of two scenarios. In the first scenario,
there are limited annotated resources available in the
target domain, and many studies have shown that
this may lead to substantial improvements. This in-
cludes the work of Roark and Bacchiani (2003), Flo-
rian et al (2004), Chelba and Acero (2004), Daume?
and Marcu (2006), and Titov and Henderson (2006).
Of these, Roark and Bacchiani (2003) and Titov and
Henderson (2006) deal specifically with syntactic
parsing. The second scenario assumes that there are
no annotated resources in the target domain. This is
a more realistic situation and is considerably more
difficult. Recent work by McClosky et al (2006)
and Blitzer et al (2006) have shown that the exis-
tence of a large unlabeled corpus in the new domain
can be leveraged in adaptation. For this shared-task,
we are assuming the latter setting ? no annotated re-
sources in the target domain.
Obtaining adequate annotated syntactic resources
for multiple languages is already a challenging prob-
lem, which is only exacerbated when these resources
must be drawn from multiple and diverse domains.
As a result, the only language that could be feasibly
tested in the domain adaptation track was English.
The setup for the domain adaptation track was as
follows. Participants were provided with a large an-
notated corpus from the source domain, in this case
sentences from the Wall Street Journal. Participants
were also provided with data from three different
target domains: biomedical abstracts (development
data), chemical abstracts (test data 1), and parent-
child dialogues (test data 2). Additionally, a large
917
unlabeled corpus for each data set (training, devel-
opment, test) was provided. The goal of the task was
to use the annotated source data, plus any unlabeled
data, to produce a parser that is accurate for each of
the test sets from the target domains.3
Participants could submit systems in either the
?open? or ?closed? class (or both). The closed class
requires a system to use only those resources pro-
vided as part of the shared task. The open class al-
lows a system to use additional resources provided
those resources are not drawn from the same domain
as the development or test sets. An example might
be a part-of-speech tagger trained on the entire Penn
Treebank and not just the subset provided as train-
ing data, or a parser that has been hand-crafted or
trained on a different training set.
3 Treebanks
In this section, we describe the treebanks used in the
shared task and give relevant information about the
data sets created from them.
3.1 Multilingual Track
Arabic The analytical syntactic annotation
of the Prague Arabic Dependency Treebank
(PADT) (Hajic? et al, 2004) can be considered a
pure dependency annotation. The conversion, done
by Otakar Smrz, from the original format to the
column-based format described in section 2.1 was
therefore relatively straightforward, although not all
the information in the original annotation could be
transfered to the new format. PADT was one of the
treebanks used in the 2006 shared task but then only
contained about 54,000 tokens. Since then, the size
of the treebank has more than doubled, with around
112,000 tokens. In addition, the morphological
annotation has been made more informative. It
is also worth noting that the parsing units in this
treebank are in many cases larger than conventional
sentences, which partly explains the high average
number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
3Note that annotated development data for the target domain
was only provided for the development domain, biomedical ab-
stracts. For the two test domains, chemical abstracts and parent-
child dialogues, the only annotated data sets were the gold stan-
dard test sets, released only after test runs had been submitted.
Basque For Basque, we used the 3LB Basque
treebank (Aduriz et al, 2003). At present, the tree-
bank consists of approximately 3,700 sentences, 334
of which were used as test data. The treebank com-
prises literary and newspaper texts. It is annotated
in a dependency format and was converted to the
CoNLL format by a team led by Koldo Gojenola.
Catalan The Catalan section of the CESS-ECE
Syntactically and Semantically Annotated Cor-
pora (Mart?? et al, 2007) is annotated with, among
other things, constituent structure and grammatical
functions. A head percolation table was used for
automatically converting the constituent trees into
dependency trees. The original data only contains
functions related to the verb, and a function table
was used for deriving the remaining syntactic func-
tions. The conversion was performed by a team led
by Llu??s Ma`rquez and Anto`nia Mart??.
Chinese The Chinese data are taken from the
Sinica treebank (Chen et al, 2003), which con-
tains both syntactic functions and semantic func-
tions. The syntactic head was used in the conversion
to the CoNLL format, carried out by Yu-Ming Hsieh
and the organizers of the 2006 shared task, and the
syntactic functions were used wherever it was pos-
sible. The training data used is basically the same
as for the 2006 shared task, except for a few correc-
tions, but the test data is new for this year?s shared
task. It is worth noting that the parsing units in this
treebank are sometimes smaller than conventional
sentence units, which partly explains the low aver-
age number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
Czech The analytical syntactic annotation of the
Prague Dependency Treebank (PDT) (Bo?hmova? et
al., 2003) is a pure dependency annotation, just as
for PADT. It was also used in the shared task 2006,
but there are two important changes compared to
last year. First, version 2.0 of PDT was used in-
stead of version 1.0, and a conversion script was
created by Zdenek Zabokrtsky, using the new XML-
based format of PDT 2.0. Secondly, due to the upper
bound on training set size, only sections 1?3 of PDT
constitute the training data, which amounts to some
450,000 tokens. The test data is a small subset of the
development test set of PDT.
918
English For English we used the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993). In particular, we used sections 2-11 for train-
ing and a subset of section 23 for testing. As a pre-
processing stage we removed many functions tags
from the non-terminals in the phrase structure repre-
sentation to make the representations more uniform
with out-of-domain test sets for the domain adapta-
tion track (see section 3.2). The resulting data set
was then converted to dependency structures using
the procedure described in Johansson and Nugues
(2007a). This work was done by Ryan McDonald.
Greek The Greek Dependency Treebank
(GDT) (Prokopidis et al, 2005) adopts a de-
pendency structure annotation very similar to those
of PDT and PADT, which means that the conversion
by Prokopis Prokopidis was relatively straightfor-
ward. GDT is one of the smallest treebanks in
this year?s shared task (about 65,000 tokens) and
contains sentences of Modern Greek. Just like PDT
and PADT, the treebank contains more than one
level of annotation, but we only used the analytical
level of GDT.
Hungarian For the Hungarian data, the Szeged
treebank (Csendes et al, 2005) was used. The tree-
bank is based on texts from six different genres,
ranging from legal newspaper texts to fiction. The
original annotation scheme is constituent-based, fol-
lowing generative principles. It was converted into
dependencies by Zo?ltan Alexin based on heuristics.
Italian The data set used for Italian is a subset
of the balanced section of the Italian Syntactic-
Semantic Treebank (ISST) (Montemagni et al,
2003) and consists of texts from the newspaper Cor-
riere della Sera and from periodicals. A team led
by Giuseppe Attardi, Simonetta Montemagni, and
Maria Simi converted the annotation to the CoNLL
format, using information from two different anno-
tation levels, the constituent structure level and the
dependency structure level.
Turkish For Turkish we used the METU-Sabanc?
Turkish Treebank (Oflazer et al, 2003), which was
also used in the 2006 shared task. A new test set of
about 9,000 tokens was provided by Gu?ls?en Eryig?it
(Eryig?it, 2007), who also handled the conversion to
the CoNLL format, which means that we could use
all the approximately 65,000 tokens of the original
treebank for training. The rich morphology of Turk-
ish requires the basic tokens in parsing to be inflec-
tional groups (IGs) rather than words. IGs of a single
word are connected to each other deterministically
using dependency links labeled DERIV, referred to
as word-internal dependencies in the following, and
the FORM and the LEMMA fields may be empty
(they contain underscore characters in the data files).
Sentences do not necessarily have a unique root;
most internal punctuation and a few foreign words
also have HEAD=0.
3.2 Domain Adaptation Track
As mentioned previously, the source data is drawn
from a corpus of news, specifically the Wall Street
Journal section of the Penn Treebank (Marcus et al,
1993). This data set is identical to the English train-
ing set from the multilingual track (see section 3.1).
For the target domains we used three different
labeled data sets. The first two were annotated
as part of the PennBioIE project (Kulick et al,
2004) and consist of sentences drawn from either
biomedical or chemical research abstracts. Like the
source WSJ corpus, this data is annotated using the
Penn Treebank phrase structure scheme. To con-
vert these sets to dependency structures we used the
same procedure as before (Johansson and Nugues,
2007a). Additional care was taken to remove sen-
tences that contained non-WSJ part-of-speech tags
or non-terminals (e.g., HYPH part-of-speech tag in-
dicating a hyphen). Furthermore, the annotation
scheme for gaps and traces was made consistent with
the Penn Treebank wherever possible. As already
mentioned, the biomedical data set was distributed
as a development set for the training phase, while
the chemical data set was only used for final testing.
The third target data set was taken from the
CHILDES database (MacWhinney, 2000), in partic-
ular the EVE corpus (Brown, 1973), which has been
annotated with dependency structures. Unfortu-
nately the dependency labels of the CHILDES data
were inconsistent with those of the WSJ, biomedi-
cal and chemical data sets, and we therefore opted
to only evaluate unlabeled accuracy for this data
set. Furthermore, there was an inconsistency in how
main and auxiliary verbs were annotated for this data
set relative to others. As a result of this, submitting
919
Multilingual Domain adaptation
Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES
Language family Sem. Isol. Rom. Sin. Sla. Ger. Hel. F.-U. Rom. Tur. Ger.
Annotation d d c+f c+f d c+f d c+f c+f d c+f d
Training data Development data
Tokens (k) 112 51 431 337 432 447 65 132 71 65 5
Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2
Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1
LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No
No. CPOSTAG 15 25 17 13 12 31 18 16 14 14 25
No. POSTAG 21 64 54 294 59 45 38 43 28 31 37
No. FEATS 21 359 33 0 71 0 31 50 21 78 0
No. DEPREL 29 35 42 69 46 20 46 49 22 25 18
No. DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1
% HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0
% HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0
% HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0
HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0
% Non-proj. arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4
% Non-proj. sent. 10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0
Punc. attached S S A S S A S A A S A
DEPRELS for punc. 10 13 6 29 16 13 15 1 10 12 8
Test data PCHEM CHILDES
Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999
Sentences 131 334 167 690 286 214 197 390 249 300 195 666
Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9
% New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10
% New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a
Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development
set and the two test sets of the domain adaptation track.
920
results for the CHILDES data was considered op-
tional. Like the chemical data set, this data set was
only used for final testing.
Finally, a large corpus of unlabeled in-domain
data was provided for each data set and made avail-
able for training. This data was drawn from theWSJ,
PubMed.com (specific to biomedical and chemical
research literature), and the CHILDES data base.
The data was tokenized to be as consistent as pos-
sible with the WSJ training set.
3.3 Overview
Table 1 describes the characteristics of the data sets.
For the multilingual track, we provide statistics over
the training and test sets; for the domain adaptation
track, the statistics were extracted from the develop-
ment set. Following last year?s shared task practice
(Buchholz and Marsi, 2006), we use the following
definition of projectivity: An arc (i, j) is projective
iff all nodes occurring between i and j are dominated
by i (where dominates is the transitive closure of the
arc relation).
In the table, the languages are abbreviated to their
first two letters. Language families are: Semitic,
Isolate, Romance, Sino-Tibetan, Slavic, Germanic,
Hellenic, Finno-Ugric, and Turkic. The type of the
original annotation is either constituents plus (some)
functions (c+f) or dependencies (d). For the train-
ing data, the number of words and sentences are
given in multiples of thousands, and the average
length of a sentence in words (including punctua-
tion tokens). The following rows contain informa-
tion about whether lemmas are available, the num-
ber of coarse- and fine-grained part-of-speech tags,
the number of feature components, and the number
of dependency labels. Then information is given on
how many different dependency labels can co-occur
with HEAD=0, the percentage of HEAD=0 depen-
dencies, and the percentage of heads preceding (left)
or succeeding (right) a token (giving an indication of
whether a language is predominantly head-initial or
head-final). This is followed by the average number
of HEAD=0 dependencies per sentence and the per-
centage of non-projective arcs and sentences. The
last two rows show whether punctuation tokens are
attached as dependents of other tokens (A=Always,
S=Sometimes) and specify the number of depen-
dency labels that exist for punctuation tokens. Note
that punctuation is defined as any token belonging to
the UTF-8 category of punctuation. This means, for
example, that any token having an underscore in the
FORM field (which happens for word-internal IGs
in Turkish) is also counted as punctuation here.
For the test sets, the number of words and sen-
tences as well as the ratio of words per sentence are
listed, followed by the percentage of new words and
lemmas (if applicable). For the domain adaptation
sets, the percentage of new words is computed with
regard to the training set (Penn Treebank).
4 Submissions and Results
As already stated in the introduction, test runs were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). In the result tables below, systems are
identified by the last name of the teammember listed
first when test runs were uploaded for evaluation. In
general, this name is also the first author of a paper
describing the system in the proceedings, but there
are a few exceptions and complications. First of all,
for four out of twenty-seven systems, no paper was
submitted to the proceedings. This is the case for the
systems of Jia, Maes et al, Nash, and Zeman, which
is indicated by the fact that these names appear in
italics in all result tables. Secondly, two teams sub-
mitted two systems each, which are described in a
single paper by each team. Thus, the systems called
?Nilsson? and ?Hall, J.? are both described in Hall et
al. (2007a), while the systems called ?Duan (1)? and
?Duan (2)? are both described in Duan et al (2007).
Finally, please pay attention to the fact that there
are two teams, where the first author?s last name is
Hall. Therefore, we use ?Hall, J.? and ?Hall, K.?,
to disambiguate between the teams involving Johan
Hall (Hall et al, 2007a) and Keith Hall (Hall et al,
2007b), respectively.
Tables 2 and 3 give the scores for the multilingual
track in the CoNLL 2007 shared task. The Average
column contains the average score for all ten lan-
guages, which determines the ranking in this track.
Table 4 presents the results for the domain adapta-
tion track, where the ranking is determined based on
the PCHEM results only, since the CHILDES data
set was optional. Note also that there are no labeled
921
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2)
Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5)
Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1)
Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10)
Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3)
Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11)
Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7)
Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9)
Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12)
Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6)
Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15)
Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4)
Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8)
Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14)
Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17)
Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13)
Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16)
Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18)
Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19)
Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20)
Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6)
Nash 8.65(22)* 86.49(8)
Shimizu 7.20(23) 72.02(20)
Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task. Teams
are denoted by the last name of their first member, with italics indicating that there is no corresponding
paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a score
in the Average column indicates a statistically significant difference with the next lower rank.
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3)
Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2)
Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1)
Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9)
Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10)
Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5)
Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7)
Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12)
Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4)
Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13)
Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15)
Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6)
Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8)
Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14)
Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17)
Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16)
Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11)
Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18)
Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19)
Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20)
Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8)
Nash 8.77(22)* 87.71(9)
Shimizu 7.79(23) 77.91(20)
Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.
Teams are denoted by the last name of their first member, with italics indicating that there is no correspond-
ing paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a
score in the Average column indicates a statistically significant difference with the next lower rank.
922
LAS UAS
Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o
Sagae 81.06(1) 83.42(1)
Attardi 80.40(2) 83.08(3) 58.67(3)
Dredze 80.22(3) 83.38(2) 61.37(1)
Nguyen 79.50(4)* 82.04(4)*
Jia 76.48(5)* 78.92(5)* 57.43(5)
Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1)
Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)*
Zeman 50.61(8) 54.57(8) 58.89(2)
Schneider 63.01(3)* 66.53(3)* 60.27(2)
Watson 55.47(4) 62.79(4) 45.61(3)
Wu 52.89(6)
Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of
the domain adaptation track in the CoNLL 2007 shared task. Teams are denoted by the last name of their
first member, with italics indicating that there is no corresponding paper in the proceedings. The number
in parentheses next to each score gives the rank. A star next to a score in the PCHEM columns indicates a
statistically significant difference with the next lower rank.
attachment scores for the CHILDES data set, for rea-
sons explained in section 3.2. The number in paren-
theses next to each score gives the rank. A star next
to a score indicates that the difference with the next
lower rank is significant at the 5% level using a z-
test for proportions. A more complete presentation
of the results, including the significance results for
all the tasks and their p-values, can be found on the
shared task website.4
Looking first at the results in the multilingual
track, we note that there are a number of systems
performing at almost the same level at the top of the
ranking. For the average labeled attachment score,
the difference between the top score (Nilsson) and
the fifth score (Hall, J.) is no more than half a per-
centage point, and there are generally very few sig-
nificant differences among the five or six best sys-
tems, regardless of whether we consider labeled or
unlabeled attachment score. For the closed class of
the domain adaptation track, we see a very similar
pattern, with the top system (Sagae) being followed
very closely by two other systems. For the open
class, the results are more spread out, but then there
are very few results in this class. It is also worth not-
ing that the top scores in the closed class, somewhat
unexpectedly, are higher than the top scores in the
4http://nextens.uvt.nl/depparse-wiki/AllScores
open class. But before we proceed to a more detailed
analysis of the results (section 6), we will make an
attempt to characterize the approaches represented
by the different systems.
5 Approaches
In this section we give an overview of the models,
inference methods, and learning methods used in the
participating systems. For obvious reasons the dis-
cussion is limited to systems that are described by
a paper in the proceedings. But instead of describ-
ing the systems one by one, we focus on the basic
methodological building blocks that are often found
in several systems although in different combina-
tions. For descriptions of the individual systems, we
refer to the respective papers in the proceedings.
Section 5.1 is devoted to system architectures. We
then describe the two main paradigms for learning
and inference, in this year?s shared task as well as in
last year?s, which we call transition-based parsers
(section 5.2) and graph-based parsers (section 5.3),
adopting the terminology of McDonald and Nivre
(2007).5 Finally, we give an overview of the domain
adaptation methods that were used (section 5.4).
5This distinction roughly corresponds to the distinction
made by Buchholz and Marsi (2006) between ?stepwise? and
?all-pairs? approaches.
923
5.1 Architectures
Most systems perform some amount of pre- and
post-processing, making the actual parsing compo-
nent part of a sequential workflow of varying length
and complexity. For example, most transition-
based parsers can only build projective dependency
graphs. For languages with non-projective depen-
dencies, graphs therefore need to be projectivized
for training and deprojectivized for testing (Hall et
al., 2007a; Johansson and Nugues, 2007b; Titov and
Henderson, 2007).
Instead of assigning HEAD and DEPREL in a
single step, some systems use a two-stage approach
for attaching and labeling dependencies (Chen et al,
2007; Dredze et al, 2007). In the first step unlabeled
dependencies are generated, in the second step these
are labeled. This is particularly helpful for factored
parsing models, in which label decisions cannot be
easily conditioned on larger parts of the structure
due to the increased complexity of inference. One
system (Hall et al, 2007b) extends this two-stage ap-
proach to a three-stage architecture where the parser
and labeler generate an n-best list of parses which in
turn is reranked.6
In ensemble-based systems several base parsers
provide parsing decisions, which are added together
for a combined score for each potential dependency
arc. The tree that maximizes the sum of these com-
bined scores is taken as the final output parse. This
technique is used by Sagae and Tsujii (2007) and in
the Nilsson system (Hall et al, 2007a). It is worth
noting that both these systems combine transition-
based base parsers with a graph-based method for
parser combination, as first described by Sagae and
Lavie (2006).
Data-driven grammar-based parsers, such as Bick
(2007), Schneider et al (2007), and Watson and
Briscoe (2007), need pre- and post-processing in or-
der to map the dependency graphs provided as train-
ing data to a format compatible with the grammar
used, and vice versa.
5.2 Transition-Based Parsers
Transition-based parsers build dependency graphs
by performing sequences of actions, or transitions.
Both learning and inference is conceptualized in
6They also flip the order of the labeler and the reranker.
terms of predicting the correct transition based on
the current parser state and/or history. We can fur-
ther subclassify parsers with respect to the model (or
transition system) they adopt, the inference method
they use, and the learning method they employ.
5.2.1 Models
The most common model for transition-based
parsers is one inspired by shift-reduce parsing,
where a parser state contains a stack of partially
processed tokens and a queue of remaining input
tokens, and where transitions add dependency arcs
and perform stack and queue operations. This type
of model is used by the majority of transition-based
parsers (Attardi et al, 2007; Duan et al, 2007; Hall
et al, 2007a; Johansson and Nugues, 2007b; Man-
nem, 2007; Titov and Henderson, 2007; Wu et al,
2007). Sometimes it is combined with an explicit
probability model for transition sequences, which
may be conditional (Duan et al, 2007) or generative
(Titov and Henderson, 2007).
An alternative model is based on the list-based
parsing algorithm described by Covington (2001),
which iterates over the input tokens in a sequen-
tial manner and evaluates for each preceding token
whether it can be linked to the current token or not.
This model is used by Marinov (2007) and in com-
ponent parsers of the Nilsson ensemble system (Hall
et al, 2007a). Finally, two systems use models based
on LR parsing (Sagae and Tsujii, 2007; Watson and
Briscoe, 2007).
5.2.2 Inference
The most common inference technique in transition-
based dependency parsing is greedy deterministic
search, guided by a classifier for predicting the next
transition given the current parser state and history,
processing the tokens of the sentence in sequen-
tial left-to-right order7 (Hall et al, 2007a; Mannem,
2007; Marinov, 2007; Wu et al, 2007). Optionally
multiple passes over the input are conducted until no
tokens are left unattached (Attardi et al, 2007).
As an alternative to deterministic parsing, several
parsers use probabilistic models and maintain a heap
or beam of partial transition sequences in order to
pick the most probable one at the end of the sentence
7For diversity in parser ensembles, right-to-left parsers are
also used.
924
(Duan et al, 2007; Johansson and Nugues, 2007b;
Sagae and Tsujii, 2007; Titov and Henderson, 2007).
One system uses as part of their parsing pipeline a
?neighbor-parser? that attaches adjacent words and
a ?root-parser? that identifies the root word(s) of a
sentence (Wu et al, 2007). In the case of grammar-
based parsers, a classifier is used to disambiguate
in cases where the grammar leaves some ambiguity
(Schneider et al, 2007; Watson and Briscoe, 2007)
5.2.3 Learning
Transition-based parsers either maintain a classifier
that predicts the next transition or a global proba-
bilistic model that scores a complete parse. To train
these classifiers and probabilitistic models several
approaches were used: SVMs (Duan et al, 2007;
Hall et al, 2007a; Sagae and Tsujii, 2007), modified
finite Newton SVMs (Wu et al, 2007), maximum
entropy models (Sagae and Tsujii, 2007), multiclass
averaged perceptron (Attardi et al, 2007) and max-
imum likelihood estimation (Watson and Briscoe,
2007).
In order to calculate a global score or probabil-
ity for a transition sequence, two systems used a
Markov chain approach (Duan et al, 2007; Sagae
and Tsujii, 2007). Here probabilities from the output
of a classifier are multiplied over the whole sequence
of actions. This results in a locally normalized
model. Two other entries used MIRA (Mannem,
2007) or online passive-aggressive learning (Johans-
son and Nugues, 2007b) to train a globally normal-
ized model. Titov and Henderson (2007) used an in-
cremental sigmoid Bayesian network to model the
probability of a transition sequence and estimated
model parameters using neural network learning.
5.3 Graph-Based Parsers
While transition-based parsers use training data to
learn a process for deriving dependency graphs,
graph-based parsers learn a model of what it means
to be a good dependency graph given an input sen-
tence. They define a scoring or probability function
over the set of possible parses. At learning time
they estimate parameters of this function; at pars-
ing time they search for the graph that maximizes
this function. These parsers mainly differ in the
type and structure of the scoring function (model),
the search algorithm that finds the best parse (infer-
ence), and the method to estimate the function?s pa-
rameters (learning).
5.3.1 Models
The simplest type of model is based on a sum of
local attachment scores, which themselves are cal-
culated based on the dot product of a weight vector
and a feature representation of the attachment. This
type of scoring function is often referred to as a first-
order model.8 Several systems participating in this
year?s shared task used first-order models (Schiehlen
and Spranger, 2007; Nguyen et al, 2007; Shimizu
and Nakagawa, 2007; Hall et al, 2007b). Canisius
and Tjong Kim Sang (2007) cast the same type of
arc-based factorization as a weighted constraint sat-
isfaction problem.
Carreras (2007) extends the first-order model to
incorporate a sum over scores for pairs of adjacent
arcs in the tree, yielding a second-order model. In
contrast to previous work where this was constrained
to sibling relations of the dependent (McDonald and
Pereira, 2006), here head-grandchild relations can
be taken into account.
In all of the above cases the scoring function is
decomposed into functions that score local proper-
ties (arcs, pairs of adjacent arcs) of the graph. By
contrast, the model of Nakagawa (2007) considers
global properties of the graph that can take multi-
ple arcs into account, such as multiple siblings and
children of a node.
5.3.2 Inference
Searching for the highest scoring graph (usually a
tree) in a model depends on the factorization cho-
sen and whether we are looking for projective or
non-projective trees. Maximum spanning tree al-
gorithms can be used for finding the highest scor-
ing non-projective tree in a first-order model (Hall
et al, 2007b; Nguyen et al, 2007; Canisius and
Tjong Kim Sang, 2007; Shimizu and Nakagawa,
2007), while Eisner?s dynamic programming algo-
rithm solves the problem for a first-order factoriza-
tion in the projective case (Schiehlen and Spranger,
2007). Carreras (2007) employs his own exten-
sion of Eisner?s algorithm for the case of projective
trees and second-order models that include head-
grandparent relations.
8It is also known as an edge-factored model.
925
The methods presented above are mostly efficient
and always exact. However, for models that take
global properties of the tree into account, they can-
not be applied. Instead Nakagawa (2007) uses Gibbs
sampling to obtain marginal probabilities of arcs be-
ing included in the tree using his global model and
then applies a maximum spanning tree algorithm to
maximize the sum of the logs of these marginals and
return a valid cycle-free parse.
5.3.3 Learning
Most of the graph-based parsers were trained using
an online inference-based method such as passive-
aggressive learning (Nguyen et al, 2007; Schiehlen
and Spranger, 2007), averaged perceptron (Carreras,
2007), or MIRA (Shimizu and Nakagawa, 2007),
while some systems instead used methods based on
maximum conditional likelihood (Nakagawa, 2007;
Hall et al, 2007b).
5.4 Domain Adaptation
5.4.1 Feature-Based Approaches
One way of adapting a learner to a new domain with-
out using any unlabeled data is to only include fea-
tures that are expected to transfer well (Dredze et
al., 2007). In structural correspondence learning a
transformation from features in the source domain
to features of the target domain is learnt (Shimizu
and Nakagawa, 2007). The original source features
along with their transformed versions are then used
to train a discriminative parser.
5.4.2 Ensemble-Based Approaches
Dredze et al (2007) trained a diverse set of parsers
in order to improve cross-domain performance by
incorporating their predictions as features for an-
other classifier. Similarly, two parsers trained with
different learners and search directions were used
in the co-learning approach of Sagae and Tsujii
(2007). Unlabeled target data was processed with
both parsers. Sentences that both parsers agreed on
were then added to the original training data. This
combined data set served as training data for one of
the original parsers to produce the final system. In
a similar fashion, Watson and Briscoe (2007) used a
variant of self-training to make use of the unlabeled
target data.
5.4.3 Other Approaches
Attardi et al (2007) learnt tree revision rules for the
target domain by first parsing unlabeled target data
using a strong parser; this data was then combined
with labeled source data; a weak parser was applied
to this new dataset; finally tree correction rules are
collected based on the mistakes of the weak parser
with respect to the gold data and the output of the
strong parser.
Another technique used was to filter sentences of
the out-of-domain corpus based on their similarity
to the target domain, as predicted by a classifier
(Dredze et al, 2007). Only if a sentence was judged
similar to target domain sentences was it included in
the training set.
Bick (2007) used a hybrid approach, where a data-
driven parser trained on the labeled training data was
given access to the output of a Constraint Grammar
parser for English run on the same data. Finally,
Schneider et al (2007) learnt collocations and rela-
tional nouns from the unlabeled target data and used
these in their parsing algorithm.
6 Analysis
Having discussed the major approaches taken in the
two tracks of the shared task, we will now return to
the test results. For the multilingual track, we com-
pare results across data sets and across systems, and
report results from a parser combination experiment
involving all the participating systems (section 6.1).
For the domain adaptation track, we sum up the most
important findings from the test results (section 6.2).
6.1 Multilingual Track
6.1.1 Across Data Sets
The average LAS over all systems varies from 68.07
for Basque to 80.95 for English. Top scores vary
from 76.31 for Greek to 89.61 for English. In gen-
eral, there is a good correlation between the top
scores and the average scores. For Greek, Italian,
and Turkish, the top score is closer to the average
score than the average distance, while for Czech, the
distance is higher. The languages that produced the
most stable results in terms of system ranks with re-
spect to LAS are Hungarian and Italian. For UAS,
Catalan also falls into this group. The language that
926
Setup Arabic Chinese Czech Turkish
2006 without punctuation 66.9 90.0 80.2 65.7
2007 without punctuation 75.5 84.9 80.0 71.6
2006 with punctuation 67.0 90.0 80.2 73.8
2007 with punctuation 76.5 84.7 80.2 79.8
Table 5: A comparison of the LAS top scores from 2006 and 2007. Official scoring conditions in boldface.
For Turkish, scores with punctuation also include word-internal dependencies.
produced the most unstable results with respect to
LAS is Turkish.
In comparison to last year?s languages, the lan-
guages involved in the multilingual track this year
can be more easily separated into three classes with
respect to top scores:
? Low (76.31?76.94):
Arabic, Basque, Greek
? Medium (79.19?80.21):
Czech, Hungarian, Turkish
? High (84.40?89.61):
Catalan, Chinese, English, Italian
It is interesting to see that the classes are more easily
definable via language characteristics than via char-
acteristics of the data sets. The split goes across
training set size, original data format (constituent
vs. dependency), sentence length, percentage of un-
known words, number of dependency labels, and ra-
tio of (C)POSTAGS and dependency labels. The
class with the highest top scores contains languages
with a rather impoverished morphology. Medium
scores are reached by the two agglutinative lan-
guages, Hungarian and Turkish, as well as by Czech.
The most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection. Based on these characteristics, one would
expect to find Czech in the last class. However, the
Czech training set is four times the size of the train-
ing set for Arabic, which is the language with the
largest training set of the difficult languages.
However, it would be wrong to assume that train-
ing set size alone is the deciding factor. A closer
look at table 1 shows that while Basque and Greek
in fact have small training data sets, so do Turk-
ish and Italian. Another factor that may be asso-
ciated with the above classification is the percent-
age of new words (PNW) in the test set. Thus, the
expectation would be that the highly inflecting lan-
guages have a high PNW while the languages with
little morphology have a low PNW. But again, there
is no direct correspondence. Arabic, Basque, Cata-
lan, English, and Greek agree with this assumption:
Catalan and English have the smallest PNW, and
Arabic, Basque, and Greek have a high PNW. But
the PNW for Italian is higher than for Arabic and
Greek, and this is also true for the percentage of
new lemmas. Additionally, the highest PNW can be
found in Hungarian and Turkish, which reach higher
scores than Arabic, Basque, and Greek. These con-
siderations suggest that highly inflected languages
with (relatively) free word order need more training
data, a hypothesis that will have to be investigated
further.
There are four languages which were included in
the shared tasks on multilingual dependency pars-
ing both at CoNLL 2006 and at CoNLL 2007: Ara-
bic, Chinese, Czech, and Turkish. For all four lan-
guages, the same treebanks were used, which allows
a comparison of the results. However, in some cases
the size of the training set changed, and at least one
treebank, Turkish, underwent a thorough correction
phase. Table 5 shows the top scores for LAS. Since
the official scores excluded punctuation in 2006 but
includes it in 2007, we give results both with and
without punctuation for both years.
For Arabic and Turkish, we see a great improve-
ment of approximately 9 and 6 percentage points.
For Arabic, the number of tokens in the training
set doubled, and the morphological annotation was
made more informative. The combined effect of
these changes can probably account for the substan-
tial improvement in parsing accuracy. For Turkish,
the training set grew in size as well, although only by
600 sentences, but part of the improvement for Turk-
ish may also be due to continuing efforts in error cor-
927
rection and consistency checking. We see that the
choice to include punctuation or not makes a large
difference for the Turkish scores, since non-final IGs
of a word are counted as punctuation (because they
have the underscore character as their FORM value),
which means that word-internal dependency links
are included if punctuation is included.9 However,
regardless of whether we compare scores with or
without punctuation, we see a genuine improvement
of approximately 6 percentage points.
For Chinese, the same training set was used.
Therefore, the drop from last year?s top score to this
year?s is surprising. However, last year?s top scor-
ing system for Chinese (Riedel et al, 2006), which
did not participate this year, had a score that was
more than 3 percentage points higher than the sec-
ond best system for Chinese. Thus, if we compare
this year?s results to the second best system, the dif-
ference is approximately 2 percentage points. This
final difference may be attributed to the properties of
the test sets. While last year?s test set was taken from
the treebank, this year?s test set contains texts from
other sources. The selection of the textual basis also
significantly changed average sentence length: The
Chinese training set has an average sentence length
of 5.9. Last year?s test set alo had an average sen-
tence length of 5.9. However, this year, the average
sentence length is 7.5 tokens, which is a significant
increase. Longer sentences are typically harder to
parse due to the increased likelihood of ambiguous
constructions.
Finally, we note that the performance for Czech
is almost exactly the same as last year, despite the
fact that the size of the training set has been reduced
to approximately one third of last year?s training set.
It is likely that this in fact represents a relative im-
provement compared to last year?s results.
6.1.2 Across Systems
The LAS over all languages ranges from 80.32 to
54.55. The comparison of the system ranks aver-
aged over all languages with the ranks for single lan-
9The decision to include word-internal dependencies in this
way can be debated on the grounds that they can be parsed de-
terministically. On the other hand, they typically correspond to
regular dependencies captured by function words in other lan-
guages, which are often easy to parse as well. It is therefore
unclear whether scores are more inflated by including word-
internal dependencies or deflated by excluding them.
guages show considerably more variation than last
year?s systems. Buchholz and Marsi (2006) report
that ?[f]or most parsers, their ranking differs at most
a few places from their overall ranking?. This year,
for all of the ten best performing systems with re-
spect to LAS, there is at least one language for which
their rank is at least 5 places different from their
overall rank. The most extreme case is the top per-
forming Nilsson system (Hall et al, 2007a), which
reached rank 1 for five languages and rank 2 for
two more languages. Their only outlier is for Chi-
nese, where the system occupies rank 14, with a
LAS approximately 9 percentage points below the
top scoring system for Chinese (Sagae and Tsujii,
2007). However, Hall et al (2007a) point out that
the official results for Chinese contained a bug, and
the true performance of their system was actually
much higher. The greatest improvement of a sys-
tem with respect to its average rank occurs for En-
glish, for which the system by Nguyen et al (2007)
improved from the average rank 15 to rank 6. Two
more outliers can be observed in the system of Jo-
hansson and Nugues (2007b), which improves from
its average rank 12 to rank 4 for Basque and Turkish.
The authors attribute this high performance to their
parser?s good performance on small training sets.
However, this hypothesis is contradicted by their re-
sults for Greek and Italian, the other two languages
with small training sets. For these two languages,
the system?s rank is very close to its average rank.
6.1.3 An Experiment in System Combination
Having the outputs of many diverse dependency
parsers for standard data sets opens up the interest-
ing possibility of parser combination. To combine
the outputs of each parser we used the method of
Sagae and Lavie (2006). This technique assigns to
each possible labeled dependency a weight that is
equal to the number of systems that included the de-
pendency in their output. This can be viewed as
an arc-based voting scheme. Using these weights
it is possible to search the space of possible depen-
dency trees using directed maximum spanning tree
algorithms (McDonald et al, 2005). The maximum
spanning tree in this case is equal to the tree that on
average contains the labeled dependencies that most
systems voted for. It is worth noting that variants
of this scheme were used in two of the participating
928
5 10 15 20Number of Systems
80
82
84
86
88
Accu
racy
Unlabeled AccuracyLabeled Accuracy
Figure 1: System Combination
systems, the Nilsson system (Hall et al, 2007a) and
the system of Sagae and Tsujii (2007).
Figure 1 plots the labeled and unlabeled accura-
cies when combining an increasing number of sys-
tems. The data used in the plot was the output of all
competing systems for every language in the mul-
tilingual track. The plot was constructed by sort-
ing the systems based on their average labeled accu-
racy scores over all languages, and then incremen-
tally adding each system in descending order.10 We
can see that both labeled and unlabeled accuracy are
significantly increased, even when just the top three
systems are included. Accuracy begins to degrade
gracefully after about ten different parsers have been
added. Furthermore, the accuracy never falls below
the performance of the top three systems.
6.2 Domain Adaptation Track
For this task, the results are rather surprising. A look
at the LAS and UAS for the chemical research ab-
stracts shows that there are four closed systems that
outperform the best scoring open system. The best
system (Sagae and Tsujii, 2007) reaches an LAS of
81.06 (in comparison to their LAS of 89.01 for the
English data set in the multilingual track). Consider-
ing that approximately one third of the words of the
chemical test set are new, the results are noteworthy.
The next surprise is to be found in the relatively
low UAS for the CHILDES data. At a first glance,
this data set has all the characteristics of an easy
10The reason that there is no data point for two parsers is
that the simple voting scheme adopted only makes sense with at
least three parsers voting.
set; the average sentence is short (12.9 words), and
the percentage of new words is also small (6.10%).
Despite these characteristics, the top UAS reaches
62.49 and is thus more than 10 percentage points
below the top UAS for the chemical data set. One
major reason for this is that auxiliary and main
verb dependencies are annotated differently in the
CHILDES data than in the WSJ training set. As a
result of this discrepancy, participants were not re-
quired to submit results for the CHILDES data. The
best performing system on the CHILDES corpus is
an open system (Bick, 2007), but the distance to
the top closed system is approximately 1 percent-
age point. In this domain, it seems more feasible to
use general language resources than for the chemi-
cal domain. However, the results prove that the extra
effort may be unnecessary.
7 Conclusion
Two years of dependency parsing in the CoNLL
shared task has brought an enormous boost to the
development of dependency parsers for multiple lan-
guages (and to some extent for multiple domains).
But even though nineteen languages have been cov-
ered by almost as many different parsing and learn-
ing approaches, we still have only vague ideas about
the strengths and weaknesses of different methods
for languages with different typological characteris-
tics. Increasing our knowledge of the multi-causal
relationship between language structure, annotation
scheme, and parsing and learning methods probably
remains the most important direction for future re-
search in this area. The outputs of all systems for all
data sets from the two shared tasks are freely avail-
able for research and constitute a potential gold mine
for comparative error analysis across languages and
systems.
For domain adaptation we have barely scratched
the surface so far. But overcoming the bottleneck
of limited annotated resources for specialized do-
mains will be as important for the deployment of
human language technology as being able to handle
multiple languages in the future. One result from
the domain adaptation track that may seem surpris-
ing at first is the fact that closed class systems out-
performed open class systems on the chemical ab-
stracts. However, it seems that the major problem in
929
adapting pre-existing parsers to the new domain was
not the domain as such but the mapping from the
native output of the parser to the kind of annotation
provided in the shared task data sets. Thus, find-
ing ways of reusing already invested development
efforts by adapting the outputs of existing systems
to new requirements, without substantial loss in ac-
curacy, seems to be another line of research that may
be worth pursuing.
Acknowledgments
First and foremost, we want to thank all the peo-
ple and organizations that generously provided us
with treebank data and helped us prepare the data
sets and without whom the shared task would have
been literally impossible: Otakar Smrz, Charles
University, and the LDC (Arabic); Maxux Aranz-
abe, Kepa Bengoetxea, Larraitz Uria, Koldo Go-
jenola, and the University of the Basque Coun-
try (Basque); Ma. Anto`nia Mart?? Anton??n, Llu??s
Ma`rquez, Manuel Bertran, Mariona Taule?, Difda
Monterde, Eli Comelles, and CLiC-UB (Cata-
lan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming
Hsieh, and Academia Sinica (Chinese); Jan Hajic?,
Zdenek Zabokrtsky, Charles University, and the
LDC (Czech); Brian MacWhinney, Eric Davis, the
CHILDES project, the Penn BioIE project, and
the LDC (English); Prokopis Prokopidis and ILSP
(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun-
garian); Giuseppe Attardi, Simonetta Montemagni,
Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril
Ribarov, Alessandro Lenci, Nicoletta Calzolari,
ILC, and ELRA (Italian); Gu?ls?en Eryig?it, Kemal
Oflazer, and Ruket C?ak?c? (Turkish).
Secondly, we want to thank the organizers of last
year?s shared task, Sabine Buchholz, Amit Dubey,
Erwin Marsi, and Yuval Krymolowski, who solved
all the really hard problems for us and answered all
our questions, as well as our colleagues who helped
review papers: Jason Baldridge, Sabine Buchholz,
James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju-
lia Hockenmaier, Yuval Krymolowski, Erwin Marsi,
Bea?ta Megyesi, Yannick Versley, and Alexander
Yeh. Special thanks to Bertjan Busser and Erwin
Marsi for help with the CoNLL shared task website
and many other things, and to Richard Johansson for
letting us use his conversion tool for English.
Thirdly, we want to thank the program chairs
for EMNLP-CoNLL 2007, Jason Eisner and Taku
Kudo, the publications chair, Eric Ringger, the
SIGNLL officers, Antal van den Bosch, Hwee Tou
Ng, and Erik Tjong Kim Sang, and members of the
LDC staff, Tony Castelletto and Ilya Ahtaridis, for
great cooperation and support.
Finally, we want to thank the following people,
who in different ways assisted us in the organi-
zation of the CoNLL 2007 shared task: Giuseppe
Attardi, Eckhard Bick, Matthias Buch-Kromann,
Xavier Carreras, Tomaz Erjavec, Svetoslav Mari-
nov, Wolfgang Menzel, Xue Nianwen, Gertjan van
Noord, Petya Osenova, Florian Schiel, Kiril Simov,
Zdenka Uresova, and Heike Zinsmeister.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT).
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
E. Bick. 2007. Hybrid ways to improve domain inde-
pendence in an ML dependency parser. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
the Tenth Conf. on Computational Natural Language
Learning (CoNLL).
S. Canisius and E. Tjong Kim Sang. 2007. A constraint
satisfaction approach to dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
930
X. Carreras. 2007. Experiments with a high-order pro-
jective dependency parser. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of the First Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
W. Chen, Y. Zhang, and H. Isahara. 2007. A two-stage
parser for multilingual dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
H. Daume? and D. Marcu. 2006. Domain adaptation for
statistical classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
M. Dredze, J. Blitzer, P. P. Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly hard do-
main adaptation for dependency parsing. In Proc. of
the CoNLL 2007 Shared Task. EMNLP-CoNLL.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
G. Eryig?it. 2007. ITU validation set for Metu-Sabanc?
Turkish Treebank. URL: http://www3.itu.edu.tr/
?gulsenc/papers/validationset.pdf.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, A. Luo, N. Nicolov, and S. Roukos. 2004. A
statisical model for multilingual entity detection and
tracking. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP).
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools.
J. Hall, J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007a. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
K. Hall, J. Havelka, and D. Smith. 2007b. Log-linear
models of non-projective trees, k-best MST parsing
and tree-ranking. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Johansson and P. Nugues. 2007a. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conf. on Computational Lin-
guistics (NODALIDA).
R. Johansson and P. Nugues. 2007b. Incremental depen-
dency parsing using online learning. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of the Sixth
Conf. on Computational Language Learning (CoNLL).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
P. R. Mannem. 2007. Online learning for determinis-
tic dependency parsing. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
S. Marinov. 2007. Covington variations. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
931
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of the 11th Conf. of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Methods
in Natural Language Processing (HLT/EMNLP).
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
T. Nakagawa. 2007. Multilingual dependency parsing
using Gibbs sampling. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
L.-M. Nguyen, T.-P. Nguyen, and A. Shimazu. 2007. A
multilingual dependency analysis system using online
passive-aggressive learning. In Proc. of the CoNLL
2007 Shared Task. EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13:95?135.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT).
S. Riedel, R. C?ak?c?, and I. Meza-Ruiz. 2006. Multi-
lingual dependency parsing with incremental integer
linear programming. In Proc. of the Tenth Conf. on
Computational Natural Language Learning (CoNLL).
B. Roark and M. Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In
Proc. of the Human Language Technology Conf. and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conf. of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT/NAACL).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
M. Schiehlen and Kristina Spranger. 2007. Global learn-
ing of labelled dependency trees. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
G. Schneider, K. Kaljurand, F. Rinaldi, and T. Kuhn.
2007. Pro3Gres parser in the CoNLL domain adap-
tation shared task. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
N. Shimizu and H. Nakagawa. 2007. Structural corre-
spondence learning for dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL).
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Watson and T. Briscoe. 2007. Adapting the RASP
system for the CoNLL07 domain-adaptation task. In
Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
Y.-C. Wu, J.-C. Yang, and Y.-S. Lee. 2007. Multi-
lingual deterministic dependency parsing framework
using modified finite Newton method support vector
machines. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT).
932
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 129?137,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Incremental Integer Linear Programming for Non-projective Dependency
Parsing
Sebastian Riedel and James Clarke
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
s.r.riedel@sms.ed.ac.uk, jclarke@ed.ac.uk
Abstract
Integer Linear Programming has recently
been used for decoding in a number of
probabilistic models in order to enforce
global constraints. However, in certain ap-
plications, such as non-projective depen-
dency parsing and machine translation,
the complete formulation of the decod-
ing problem as an integer linear program
renders solving intractable. We present an
approach which solves the problem in-
crementally, thus we avoid creating in-
tractable integer linear programs. This ap-
proach is applied to Dutch dependency
parsing and we show how the addition
of linguistically motivated constraints can
yield a significant improvement over state-
of-the-art.
1 Introduction
Many inference algorithms require models to
make strong assumptions of conditional indepen-
dence between variables. For example, the Viterbi
algorithm used for decoding in conditional ran-
dom fields requires the model to be Markovian.
Strong assumptions are also made in the case of
McDonald et al?s (2005b) non-projective depen-
dency parsing model. Here attachment decisions
are made independently of one another1. However,
often such assumptions can not be justified. For
example in dependency parsing, if a subject has
already been identified for a given verb, then the
probability of attaching a second subject to the
verb is zero. Similarly, if we find that one coor-
dination argument is a noun, then the other argu-
1If we ignore the constraint that dependency trees must be
cycle-free (see sections 2 and 3 for details).
ment cannot be a verb. Thus decisions are often
co-dependent.
Integer Linear Programming (ILP) has recently
been applied to inference in sequential condi-
tional random fields (Roth and Yih, 2004), this
has allowed the use of truly global constraints
during inference. However, it is not possible to
use this approach directly for a complex task like
non-projective dependency parsing due to the ex-
ponential number of constraints required to pre-
vent cycles occurring in the dependency graph.
To model all these constraints explicitly would re-
sult in an ILP formulation too large to solve effi-
ciently (Williams, 2002). A similar problem also
occurs in an ILP formulation for machine transla-
tion which treats decoding as the Travelling Sales-
man Problem (Germann et al, 2001).
In this paper we present a method which extends
the applicability of ILP to a more complex set of
problems. Instead of adding all the constraints we
wish to capture to the formulation, we first solve
the program with a fraction of the constraints. The
solution is then examined and, if required, addi-
tional constraints are added. This procedure is re-
peated until all constraints are satisfied. We apply
this dependency parsing approach to Dutch due
to the language?s non-projective nature, and take
the parser of McDonald et al (2005b) as a starting
point for our model.
In the following section we introduce depen-
dency parsing and review previous work. In Sec-
tion 3 we present our model and formulate it as
an ILP problem with a set of linguistically mo-
tivated constraints. We include details of an in-
cremental algorithm used to solve this formula-
tion. Our experimental set-up is provided in Sec-
tion 4 and is followed by results in Section 5 along
with runtime experiments. We finally discuss fu-
129
Figure 1: A Dutch dependency tree for ?I?ll come
at twelve and then you?ll get what you deserve?
ture research and potential improvements to our
approach.
2 Dependency Parsing
Dependency parsing is the task of attaching words
to their arguments. Figure 1 shows a dependency
graph for the Dutch sentence ?I?ll come at twelve
and then you?ll get what you deserve? (taken from
the Alpino Corpus (van der Beek et al, 2002)). In
this dependency graph the verb ?kom? is attached
to its subject ?ik?. ?kom? is referred to as the head
of the dependency and ?ik? as its child. In labelled
dependency parsing edges between words are la-
belled with the relation captured. In the case of
the dependency between ?kom? and ?ik? the label
would be ?subject?.
In a dependency tree every token must be the
child of exactly one other node, either another to-
ken or the dummy root node. By definition, a de-
pendency tree is free of cycles. For example, it
must not contain dependency chains such as ?en?
? ?kom?? ?ik?? ?en?. For a more formal def-
inition see previous work (Nivre et al, 2004).
An important distinction between dependency
trees is whether they are projective or non-
projective. Figure 1 is an example of a projec-
tive dependency tree, in such trees dependencies
do not cross. In Dutch and other flexible word or-
der languages such as German and Czech we also
encounter non-projective trees, in these cases the
trees contain crossing dependencies.
Dependency parsing is useful for applications
such as relation extraction (Culotta and Sorensen,
2004) and machine translation (Ding and Palmer,
2005). Although less informative than lexicalised
phrase structures, dependency structures still cap-
ture most of the predicate-argument information
needed for applications. It has the advantage of be-
ing more efficient to learn and parse.
McDonald et al (2005a) introduce a depen-
dency parsing framework which treats the task as
searching for the projective tree that maximises
the sum of local dependency scores. This frame-
Figure 2: An incorrect partial dependency tree.
The verb ?krijg? is incorrectly coordinated with
the preposition ?om?.
work is efficient and has also been extended to
non-projective trees (McDonald et al, 2005b). It
provides a discriminative online learning algo-
rithm which when combined with a rich feature set
reaches state-of-the-art performance across multi-
ple languages.
However, within this framework one can only
define features over single attachment decisions.
This leads to cases where basic linguistic con-
straints are not satisfied (e.g. verbs with two sub-
jects or incompatible coordination arguments). An
example of this for Dutch is illustrated in Figure 2
which was produced by the parser of McDonald
et al (2005b). Here the parse contains a coordi-
nation of incompatible word classes (a preposition
and a verb).
Our approach is able to include additional con-
straints which forbid configurations such as those
in Figure 2. While McDonald and Pereira (2006)
address the issue of local attachment decisions by
defining scores over attachment pairs, our solution
is more general. Furthermore, it is complementary
in the sense that we could formulate their model
using ILP and then add constraints.
The method we present is not the only one that
can take global constraints into account. Deter-
ministic dependency parsing (Nivre et al, 2004;
Yamada and Matsumoto, 2003) can apply global
constraints by conditioning attachment decisions
on the intermediate parse built. However, for effi-
ciency a greedy search is used which may produce
sub-optimal solutions. This is not the case when
using ILP.
3 Model
Our underlying model is a modified labelled ver-
sion2 of McDonald et al (2005b):
s(x,y) =
?
(i,j,l)?y
s(i, j, l)
=
?
(i,j,l)?y
w ? f(i, j, l)
2Note that this is not described in the McDonald papers
but implemented in his software.
130
where x is a sentence, y is a set of labelled de-
pendencies, f(i, j, l) is a multidimensional fea-
ture vector representation of the edge from token
i to token j with label l and w the correspond-
ing weight vector. For example, a feature f101 in fcould be:
f101(i, j, l) =
?
?
?
?
?
1 if t(i) = ?en? ? p(j) = V
?l = ?coordination?
0 otherwise
where t(i) is the word at token i and p(j) the part-
of-speech tag at token j.
Decoding in this model amounts to finding the
y for a given x that maximises s(x,y):
y? = arg max
y
s(x,y)
while fulfilling the following constraints:
T1 For every non-root token in x there exists ex-
actly one head; the root token has no head.
T2 There are no cycles.
Thus far, the formulation follows McDonald
et al (2005b) and corresponds to the Maximum
Spanning Tree (MST) problem. In addition to T1
and T2, we include a set of linguistically moti-
vated constraints:
A1 Heads are not allowed to have more than one
outgoing edge labelled l for all l in a set of
labels U .
C1 In a symmetric coordination there is exactly
one argument to the right of the conjunction
and at least one argument to the left.
C2 In an asymmetric coordination there are no ar-
guments to the left of the conjunction and at
least two arguments to the right.
C3 There must be at least one comma between
subsequent arguments to the left of a sym-
metric coordination.
C4 Arguments of a coordination must have com-
patible word classes.
P1 Two dependencies must not cross if one of
their labels is in a set of labels P .
A1 covers constraints such as ?there can only
be one subject? if U contains ?subject? (see Sec-
tion 4.4 for more details of U ). C1 applies to
configurations which contain conjunctions such as
?en?,?of? or ?maar? (?and?, ?or? and ?but?). C2
will rule-out settings where a conjunction such as
?zowel? (translates as ?both?) having arguments
to its left. C3 forces coordination arguments to
the left of a conjunction to have commas in be-
tween. C4 avoids parses in which incompatible
word classes are coordinated, such as nouns and
verbs. Finally, P1 allows selective projective pars-
ing: we can, for instance, forbid the crossing of
?Noun-Determiner? dependencies if we add the
corresponding label type to P (see Section 4.4 for
more details of P ) . If we extend P to contain all
labels we forbid any type of crossing dependen-
cies. This corresponds to projective parsing.
3.1 Decoding
McDonald et al (2005b) use the Chu-Liu-
Edmonds (CLE) algorithm to solve the maxi-
mum spanning tree problem. However, global con-
straints cannot be incorporated into the CLE algo-
rithm (McDonald et al, 2005b). We alleviate this
problem by presenting an equivalent Integer Lin-
ear Programming formulation which allows us to
incorporate global constraints naturally.
Before giving full details of our formulation
we first introduce some of the concepts of lin-
ear and integer linear programming (for a more
thorough introduction see Winston and Venkatara-
manan (2003)).
Linear Programming (LP) is a tool for solving
optimisation problems in which the aim is to max-
imise (or minimise) a given linear function with
respect to a set of linear constraints. The func-
tion to be maximised (or minimised) is referred
to as the objective function. A number of decision
variables are under our control which exert influ-
ence on the objective function. Specifically, they
have to be optimised in order to maximise (or min-
imise) the objective function. Finally, a set of con-
straints restrict the values that the decision vari-
ables can take. Integer Linear Programming is an
extension of linear programming where all deci-
sion variables must take integer values.
There are several explicit formulations of the
MST problem as an integer linear program in the
literature (Williams, 2002). They are based on
the concept of eliminating subtours (cycles), cuts
(disconnections) or requiring intervertex flows
(paths). However, in practice these formulations
cause long solve times ? as the first two meth-
131
Algorithm 1 Incremental Integer Linear Program-
ming
C ? Bx
repeat
y? solve(C, Ox, Vx)
W ? violated(y, Ix)
C ? C ?W
until V = ?
return y
ods yield an exponential number of constraints.
Although the latter scales cubically, it produces
non-fractional solutions in its relaxed version; this
causes long runtimes for the branch and bound al-
gorithm (Williams, 2002) commonly used in inte-
ger linear programming. We found out experimen-
tally that dependency parsing models of this form
do not converge on a solution after multiple hours
of solving, even for small sentences.
As a workaround for this problem we follow an
incremental approach akin to the work of Warme
(1998). Instead of adding constraints which forbid
all possible cycles in advance (this would result
in an exponential number of constraints) we first
solve the problem without any cycle constraints.
The solution is then examined for cycles, and if
cycles are found we add constraints to forbid these
cycles; the solver is then run again. This process
is repeated until no more violated constraints are
found. The same procedure is used for other types
of constraints which are too expensive to add in
advance (e.g. the constraints of P1).
Algorithm 1 outlines our approach. For a sen-
tence x, Bx is the set of constraints that we addin advance and Ix are the constraints we add in-crementally. Ox is the objective function and Vxis a set of variables including integer declarations.
solve(C, O, V ) maximises the objective function
O with respect to the set of constraints C and vari-
ables V . violated(y, I) inspects the proposed so-
lution (y) and returns all constraints in I which are
violated.
The number of iterations required in this ap-
proach is at most polynomial with respect to the
number of variables (Gro?tschel et al, 1981). In
practice, this technique converges quickly (less
than 20 iterations in 99% of approximately 12,000
sentences), yielding average solve times of less
than 0.5 seconds.
Our approach converges quickly due to the
quality of the scoring function. Its weights have
been trained on cycle free data, thus it is more
likely to guide the search to a cycle free solution.
In the following section we present the objec-
tive function Ox, variables Vx and linear con-straints Bx and Ix needed for parsing x using Al-gorithm 1.
3.1.1 Variables
Vx contains a set of binary variables to representlabelled edges:
ei,j,l ?i ? 0..n, j ? 1..n,
l ? bestk(i, j)
where n is the number of tokens and the index 0
represents the root token. bestk(i, j) is the set of klabels with highest s(i, j, l). ei,j,l equals 1 if thereis a edge (i.e., a dependency) with the label l be-
tween token i (head) and j (child), 0 otherwise. k
depends on the type of constraints we want to add.
For the plain MST problem it is sufficient to set
k = 1 and only take the best scoring label for each
token pair. However, if we want a constraint which
forbids duplicate subjects we need to provide ad-
ditional labels to fall back on.
Vx also contains a set of binary auxiliary vari-ables:
di,j ?i ? 0..n, j ? 1..n
which represent the existence of a dependency be-
tween tokens i and j. We connect these to the ei,j,lvariables by the constraint:
di,j =
?
l?bestk(i,j)
ei,j,l
3.1.2 Objective Function
Given the above variables our objective function
Ox can be represented as (using a suitable k):
?
i,j
?
l?bestk(i,j)
s(i, j, l) ? ei,j,l
3.1.3 Base Constraints
We first introduce a set of base constraints Bxwhich we add in advance.
Only One Head (T1) Every token has exactly
one head:
?
i
di,j = 1
for non-root tokens j > 0 in x. An exception is
made for the artificial root node:
?
i
di,0 = 0
132
Label Uniqueness (A1) To enforce uniqueness
of children with labels in U we augment our model
with the constraint:
?
j
ei,j,l ? 1
for every token i in x and label l in U .
Symmetric Coordination (C1) For each con-
junction token i which forms a symmetric coor-
dination we add:
?
j<i
di,j ? 1
and
?
j>i
di,j = 1
Asymmetric Coordination (C2) For each con-
junction token i which forms an asymmetric coor-
dination we add:
?
j<i
di,j = 0
and
?
j>i
di,j ? 2
3.1.4 Incremental Constraints
Now we present the set of constraints Ix we addincrementally. The constraints are chosen based on
the two criteria: (1) adding them to the base con-
straints (those added in advance) would result in
an extremely large program, and (2) it must be ef-
ficient to detect whether the constraint is violated
in y.
No Cycles (T2) For every possible cycle c for
the sentence x we have a constraint which forbids
the case where all edges in c are active simultane-
ously:
?
(i,j)?c
di,j ? |c| ? 1
Comma Coordination (C3) For each symmet-
ric conjunction token i which forms a symmetric
coordination and each set of tokens A in x to the
left of i with no comma between each pair of suc-
cessive tokens we add:
?
a?A
di,a ? |A| ? 1
which forbids configurations where i has the argu-
ment tokens A.
Compatible Coordination Arguments (C4)
For each conjunction token i and each set of to-
kens A in x with incompatible POS tags, we add a
constraint to forbid configurations where i has the
argument tokens A.
?
a?A
di,a ? |A| ? 1
Selective Projective Parsing (P1) For each pair
of triplets (i, j, l1) and (m, n, l2) we add the con-straint:
ei,j,l1 + em,n,l2 ? 1
if l1 or l2 is in P .
3.2 Training
For training we use single-best MIRA (McDon-
ald et al, 2005a). This is an online algorithm that
learns by parsing each sentence and comparing
the result with a gold standard. Training is com-
plete after multiple passes through the whole cor-
pus. Thus we decode using the Chu-Liu-Edmonds
(CLE) algorithm due to its speed advantage over
ILP (see Section 5.2 for a detailed comparison of
runtimes).
The fact that we decode differently during train-
ing (using CLE) and testing (using ILP) may de-
grade performance. In the presence of additional
constraints weights may be able to capture other
aspects of the data.
4 Experimental Set-up
Our experiments were designed to answer the fol-
lowing questions:
1. How much do our additional constraints help
improve accuracy?
2. How fast is our generic inference method in
comparison with the Chu-Liu-Edmonds algo-
rithm?
3. Can approximations be used to increase the
speed of our method while remaining accu-
rate?
Before we try to answer these questions we briefly
describe our data, features used, settings for U and
P in our parametric constraints, our working envi-
ronment and our implementation.
133
4.1 Data
We use the Alpino treebank (van der Beek et al,
2002), taken from the CoNLL shared task of mul-
tilingual dependency parsing3. The CoNLL data
differs slightly from the original Alpino treebank
as the corpus has been part-of-speech tagged using
a Memory-Based-Tagger (Daelemans et al, 1996).
It consists of 13,300 sentences with an average
length of 14.6 tokens. The data is non-projective,
more specifically 5.4% of all dependencies are
crossed by at least one other dependency. It con-
tains approximately 6000 sentences more than the
Alpino corpus used by Malouf and van Noord
(2004).
The training set was divided into a 10% devel-
opment set (dev) while the remaining 90% is used
as a training and cross-validation set (cross). Fea-
ture sets, constraints and training parameters were
selected through training on cross and optimising
against dev.
Our final accuracy scores and runtime eval-
uations were acquired using a nine-fold cross-
validation on cross
4.2 Environment and Implementation
All our experiments were conducted on a Intel
Xeon with 3.8 Ghz and 4Gb of RAM. We used
the open source Mixed Integer Programming li-
brary lp solve4 to solve the Integer Linear Pro-
grams. Our code ran in Java and called the JNI-
wrapper around the lp solve library.
4.3 Feature Sets
Our feature set was determined through experi-
mentation with the development set. The features
are based upon the data provided within the Alpino
treebank. Along with POS tags the corpus contains
several additional attributes such as gender, num-
ber and case.
Our best results on the development set were
achieved using the feature set of McDonald et al
(2005a) and a set of features based on the addi-
tional attributes. These features combine the at-
tributes of the head with those of the child. For
example, if token i has the attributes a1 and a2,and token j has the attribute a3 then we createdthe features (a1 ? a3) and (a2 ? a3).
3For details see http://nextens.uvt.nl/
?conll.
4The software is available from http://www.
geocities.com/lpsolve.
4.4 Constraints
All the constraints presented in Section 3 were
used in our model. The set U of unique labels
constraints contained su, obj1, obj2, sup, ld, vc,
predc, predm, pc, pobj1, obcomp and body. Here
su stands for subject and obj1 for direct object (for
full details see Moortgat et al (2000)).
The set of projective labels P contained cnj,
for coordination dependencies; and det, for de-
terminer dependencies. One exception was added
for the coordination constraint: dependencies can
cross when coordinated arguments are verbs.
One drawback of hard deterministic constraints
is the undesirable effect noisy data can cause. We
see this most prominently with coordination argu-
ment compatibility. Words ending in ?en? are typ-
ically wrongly tagged and cause our coordination
argument constraint to discard correct coordina-
tions. As a workaround we assigned words ending
in ?en? a wildcard POS tag which is compatible
with all POS tags.
5 Results
In this section we report our results. We not only
present our accuracy but also provide an empiri-
cal evaluation of the runtime behaviour of this ap-
proach and show how parsing can be accelerated
using a simple approximation.
5.1 Accuracy
An important question to answer when using
global constraints is: How much of a performance
boost is gained when using global constraints?
We ran the system without any linguistic con-
straints as a baseline (bl) and compared it to a
system with the additional constraints (cnstr). To
evaluate our systems we use the accuracy over la-
belled attachment decisions:
LAC = NlNt
where Nl is the number of tokens with correcthead and label and Nt is the total number of to-kens. For completeness we also report the unla-
belled accuracy:
UAC = NuNt
where Nu is the number of tokens with correcthead.
134
LAC UAC LC UC
bl 84.6% 88.9% 27.7% 42.2%
cnstr 85.1% 89.4% 29.7% 43.8%
Table 1: Labelled (LAC) and unlabelled (UAC) ac-
curacy using nine-fold cross-validation on cross
for baseline (bl) and constraint-based (constr) sys-
tem. LC and UC are the percentages of sentences
with 100% labelled and unlabelled accuracy, re-
spectively.
Table 1 shows our results using nine-fold cross-
validation on the cross set. The baseline system
(no additional constraints) gives an unlabelled ac-
curacy of 84.6% and labelled accuracy of 88.9%.
When we add our linguistic constraints the per-
formance increases by 0.5% for both labelled and
unlabelled accuracy. This increase is significant
(p < 0.001) according to Dan Bikel?s parse com-
parison script and using the Sign test (p < 0.001).
Now we give a little insight into how our results
compare with the rest of the community. The re-
ported state-of-the-art parser of Malouf and van
Noord (2004) achieves 84.4% labelled accuracy
which is very close numerically to our baseline.
However, they use a subset of the CoNLL Alpino
treebank with a higher average number of tokens
per sentences and also evaluate control relations,
thus results are not directly comparable. We have
also run our parser on the relatively small (approx-
imately 400 sentences) CoNNL test data. The best
performing system (McDonald et al 2006; note:
this system is different to our baseline) achieves
79.2% labelled accuracy while our baseline sys-
tem achieves 78.6% and our constrained version
79.8%. However, a significant difference is only
observed between our baseline and our constraint-
based system.
Examining the errors produced using the dev
set highlight two key reasons why we do not see
a greater improvement using our constraint-based
system. Firstly, we cannot improve on coordina-
tions that include words ending with ?en? based on
the workaround present in Section 4.4. This prob-
lem can only be solved by improving POS taggers
for Dutch or by performing POS tagging within
the dependency parsing framework.
Secondly, our system suffers from poor next
best solutions. That is, if the best solution violates
some constraints, then we find the next best solu-
tion is typically worse than the best solution with
violated constraints. This appears to be a conse-
quence of inaccurate local score distributions (as
opposed to inaccurate best local scores). For ex-
ample, suppose we attach two subjects, t1 and t2,to a verb, where t1 is the actual subject while t2is meant to be labelled as object. If we forbid this
configuration (two subjects) and if the score of la-
belling t1 object is higher than that for t2 beinglabelled subject, then the next best solution will
label t1 incorrectly as object and t2 incorrectly assubject. This is often the case, and thus results in a
drop of accuracy.
5.2 Runtime Evaluation
We now concentrate on the runtime of our method.
While we expect a longer runtime than using the
Chu-Liu-Edmonds as in previous work (McDon-
ald et al, 2005b), we are interested in how large
the increase is.
Table 2 shows the average solve time (ST) for
sentences with respect to the number of tokens in
each sentence for our system with constraints (cn-
str) and the Chu-Liu-Edmonds (CLE) algorithm.
All solve times do not include feature extraction
as this is identical for all systems. For cnstr we
also show the number of sentences that could not
be parsed after two minutes, the average number
of iterations and the average duration of the first
iteration.
The results show that parsing using our generic
approach is still reasonably fast although signifi-
cantly slower than using the Chu-Liu-Edmonds al-
gorithm. Also, only a small number of sentences
take longer than two minutes to parse. Thus, in
practice we would not see a significant degrada-
tion in performance if we were to fall back on the
CLE algorithm after two minutes of solving.
When we examine the average duration of the
first iteration it appears that the majority of the
solve time is spent within this iteration. This could
be used to justify using the CLE algorithm to find
a initial solution as starting point for the ILP solver
(see Section 6).
5.3 Approximation
Despite the fact that our parser can parse all sen-
tences in a reasonable amount of time, it is still sig-
nificantly slower than the CLE algorithm. While
this is not crucial during decoding, it does make
discriminative online training difficult as training
requires several iterations of parsing the whole
corpus.
135
Tokens 1-10 11-20 21-30 31-40 41-50 >50
Count 5242 4037 1835 650 191 60
Avg. ST (CLE) 0.27ms 0.98ms 3.2ms 7.5ms 14ms 23ms
Avg. ST (cnstr) 5.6ms 52ms 460ms 1.5s 7.2s 33s
ST > 120s (cnstr) 0 0 0 0 3 3
Avg. # iter. (cnstr) 2.08 2.87 4.48 5.82 8.40 15.17
Avg. ST 1st iter. (cnstr) 4.2ms 37ms 180ms 540ms 1.3s 2.6s
Table 2: Runtime evaluation for different sentence lengths. Average solve time (ST) for our system
with constraints (constr), the Chu-Liu-Edmonds algorithm (CLE), number of sentences with solve times
greater than 120 seconds, average number of iterations and first iteration solve time.
q=5 q=10 all CLE
LAC 84.90% 85.11% 85.14% 85.14%
ST 351s 760s 3640s 20s
Table 3: Labelled accuracy (LAC) and total solve
time (ST) for the cross dataset using varying q val-
ues and the Chu-Liu-Edmonds algorithm (CLE)
Thus we investigate if it is possible to speed up
our inference using a simple approximation. For
each token we now only consider the q variables
in Vx with the highest scoring edges. For exam-ple, if we set q = 2 the set of variables for a to-
ken j will contain two variables, either both for
the same head i but with different labels (variables
ei,j,l1 and ei,j,l2) or two distinct heads i1 and i2(variables ei1,j,l1 and ei2,j,l2) where labels l1 and
l2 may be identical.
Table 3 shows the effect of different q values
on solve time for the full corpus cross (roughly
12,000 sentences) and overall accuracy. We see
that solve time can be reduced by 80% while only
losing a marginal amount of accuracy when we set
q to 10. However, we are unable to reach the 20
seconds solve time of the CLE algorithm. Despite
this, when we add the time for preprocessing and
feature extraction, the CLE system parses a cor-
pus in around 15 minutes whereas our system with
q = 10 takes approximately 25 minutes5. When
we view the total runtime of each system we see
our system is more competitive.
6 Discussion
While we have presented significant improve-
ments using additional constraints, one may won-
5Even when caching feature extraction during training
McDonald et al (2005a) still takes approximately 10 minutes
to train.
der whether the improvements are large enough
to justify further research in this direction; espe-
cially since McDonald and Pereira (2006) present
an approximate algorithm which also makes more
global decisions. However, we believe that our ap-
proach is complementary to their model. We can
model higher order features by using an extended
set of variables and a modified objective function.
Although this is likely to increase runtime, it may
still be fast enough for real world applications. In
addition, it will allow exact inference, even in the
case of non-projective parsing. Also, we argue that
this approach has potential for interesting exten-
sions and applications.
For example, during our runtime evaluations we
find that a large fraction of solve time is spent in
the first iteration of our incremental algorithm. Af-
ter the first iteration the solver uses its last state to
efficiently search for solutions in the presence of
new constraints. Some solvers allow the specifica-
tion of an initial solution as a starting point, thus it
is expected that significant improvements in terms
of speed can be made by using the CLE algorithm
to provide an initial solution.
Our approach uses a generic algorithm to solve
a complex task. Thus other applications may ben-
efit from it. For instance, Germann et al (2001)
present an ILP formulation of the Machine Trans-
lation (MT) decoding task in order to conduct ex-
act inference. However, their model suffers from
the same type of exponential blow-up we observe
when we add all our cycle constraints in advance.
In fact, the constraints which cause the exponential
explosion in their graphically formulation are of
the same nature as our cycle constraints. We hope
that the incremental approach will allow exact MT
decoding for longer sentences.
136
7 Conclusion
In this paper we have presented a novel ap-
proach for inference using ILP. While previous ap-
proaches which use ILP for decoding have solved
each integer linear program in one run, we incre-
mentally add constraints and solve the resulting
program until no more constraints are violated.
This allows us to efficiently use ILP for depen-
dency parsing and add constraints which provide
a significant improvement over the current state-
of-the-art parser (McDonald et al, 2005b) on the
Dutch Alpino corpus (see bl row in Table 1).
Although slower than the baseline approach,
our method can still parse large sentences (more
than 50 tokens) in a reasonable amount of time
(less than a minute). We have shown that pars-
ing time can be significantly reduced using a
simple approximation which only marginally de-
grades performance. Furthermore, we believe that
the method has potential for further extensions and
applications.
Acknowledgements
Thanks to Ivan Meza-Ruiz, Ruken C?ak?c?, Beata
Kouchnir and Abhishek Arun for their contribu-
tion during the CoNLL shared task and to Mirella
Lapata for helpful comments and suggestions.
References
Culotta, Aron and Jeffery Sorensen. 2004. Dependency tree
kernels for relation extraction. In 42nd Annual Meeting of
the Association for Computational Linguistics. Barcelona,
Spain, pages 423?429.
Daelemans, W., J. Zavrel, and S. Berck. 1996. MBT: A
memory-based part of speech tagger-generator. In Pro-
ceedings of the Fourth Workshop on Very Large Corpora.
pages 14?27.
Ding, Yuan and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency insertion
grammars. In The 43rd Annual Meeting of the Association
of Computational Linguistics. Ann Arbor, MI, USA, pages
541?548.
Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2001. Fast decoding and optimal de-
coding for machine translation. In Meeting of the Asso-
ciation for Computational Linguistics. Toulouse, France,
pages 228?235.
Gro?tschel, M., L. Lova?sz, and A. Schrijver. 1981. The ellip-
soid method and its consequences in combina- torial opti-
mization. Combinatorica I:169? 197.
Malouf, Robert and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
Proc. of IJCNLP-04 Workshop ?Beyond Shallow Analy-
ses?. Sanya City, Hainan Island, China.
McDonald, R., K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In 43rd An-
nual Meeting of the Association for Computational Lin-
guistics. Ann Arbor, MI, USA, pages 91?98.
McDonald, R. and F. Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In 11th Con-
ference of the European Chapter of the Association for
Computational Linguistics. Trento, Italy, pages 81?88.
McDonald, R., F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning tree
algorithms. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Methods in
Natural Language Processing. Association for Computa-
tional Linguistics, Vancouver, British Columbia, Canada,
pages 523?530.
McDonald, Ryan, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-stage
discriminative parser. In Proceedings of CoNLL-2006.
New York, USA.
Moortgat, M., I. Schuurman, and T. van der Wouden.
2000. Cgn syntactische annotatie. Internal report Corpus
Gesproken Nederlands.
Nivre, J., J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proceedings of CoNLL-2004. Boston,
MA, USA, pages 49?56.
Roth, D. and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of CoNLL-2004,. Boston, MA, USA, pages
1?8.
van der Beek, L., G. Bouma, R. Malouf, G. van Noord,
Leonoor van der Beek, Gosse Bouma, Robert Malouf, and
Gertjan van Noord. 2002. The Alpino dependency tree-
bank. In Computational Linguistics in the Netherlands
(CLIN). Rodopi.
Warme, David Michael. 1998. Spanning Trees in Hyper-
graphs with Application to Steiner Trees. Ph.D. thesis,
University of Virginia.
Williams, Justin C. 2002. A linear-size zero - one program-
ming model for the minimum spanning tree problem in
planar graphs. Networks 39:53?60.
Winston, Wayne L. and Munirpallam Venkataramanan.
2003. Introduction to Mathematical Programming.
Brooks/Cole.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In Pro-
ceedings of IWPT . pages 195?206.
137
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 226?230, New York City, June 2006. c?2006 Association for Computational Linguistics
Multi-lingual Dependency Parsing with Incremental Integer Linear
Programming
Sebastian Riedel and Ruket C?ak?c? and Ivan Meza-Ruiz
ICCS
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, UK
S.R.Riedel,R.Cakici,I.V.Meza-Ruiz@sms.ed.ac.uk
Abstract
Our approach to dependency parsing is
based on the linear model of McDonald
et al(McDonald et al, 2005b). Instead of
solving the linear model using the Max-
imum Spanning Tree algorithm we pro-
pose an incremental Integer Linear Pro-
gramming formulation of the problem that
allows us to enforce linguistic constraints.
Our results show only marginal improve-
ments over the non-constrained parser. In
addition to the fact that many parses did
not violate any constraints in the first place
this can be attributed to three reasons: 1)
the next best solution that fulfils the con-
straints yields equal or less accuracy, 2)
noisy POS tags and 3) occasionally our
inference algorithm was too slow and de-
coding timed out.
1 Introduction
This paper presents our submission for the CoNLL
2006 shared task of multilingual dependency pars-
ing. Our parser is inspired by McDonald et
al.(2005a) which treats the task as the search for the
highest scoring Maximum Spanning Tree (MST) in
a graph. This framework is efficient for both pro-
jective and non-projective parsing and provides an
online learning algorithm which combined with a
rich feature set creates state-of-the-art performance
across multiple languages (McDonald and Pereira,
2006).
However, McDonald and Pereira (2006) mention
the restrictive nature of this parsing algorithm. In
their original framework, features are only defined
over single attachment decisions. This leads to cases
where basic linguistic constraints are not satisfied
(e.g. verbs with two subjects). In this paper we
present a novel way to implement the parsing al-
gorithms for projective and non-projective parsing
based on a more generic incremental Integer Linear
Programming (ILP) approach. This allows us to in-
clude additional global constraints that can be used
to impose linguistic information.
The rest of the paper is organised in the following
way. First we give an overview of the Integer Linear
Programming model and how we trained its param-
eters. We then describe our feature and constraint
sets for the 12 different languages of the task (Hajic?
et al, 2004; Chen et al, 2003; Bo?hmova? et al, 2003;
Kromann, 2003; van der Beek et al, 2002; Brants
et al, 2002; Kawata and Bartels, 2000; Afonso et
al., 2002; Dz?eroski et al, 2006; Civit Torruella and
Mart?? Anton??n, 2002; Nilsson et al, 2005; Oflazer et
al., 2003; Atalay et al, 2003). Finally, our results are
discussed and error analyses for Chinese and Turk-
ish are presented.
2 Model
Our model is based on the linear model presented in
McDonald et al (2005a),
s (x,y) =
?
(i,j)?y
s (i, j) =
?
w ? f (i, j)(1)
where x is a sentence, y a parse and s a score func-
tion over sentence-parse pairs. f (i, j) is a multidi-
226
mensional feature vector representation of the edge
from token i to token j and w the corresponding
weight vector. Decoding in this model amounts to
finding the y for a given x that maximises s (x,y)
y? = argmaxys (x,y)
and y contains no cycles, attaches exactly one head
to each non-root token and no head to the root node.
2.1 Decoding
Instead of using the MST algorithm (McDonald et
al., 2005b) to maximise equation 1, we present an
equivalent ILP formulation of the problem. An ad-
vantage of a general purpose inference technique is
the addition of further linguistically motivated con-
straints. For instance, we can add constraints that
enforce that a verb can not have more than one sub-
ject argument or that coordination arguments should
have compatible types. Roth and Yih (2005) is
similarly motivated and uses ILP to deal with ad-
ditional hard constraints in a Conditional Random
Field model for Semantic Role Labelling.
There are several explicit formulations of the
MST problem as integer programs in the literature
(Williams, 2002). They are based on the concept of
eliminating subtours (cycles), cuts (disconnections)
or requiring intervertex flows (paths). However, in
practice these cause long solving times. While the
first two types yield an exponential number of con-
straints, the latter one scales cubically but produces
non-fractional solutions in its relaxed version, caus-
ing long runtime of the branch and bound algorithm.
In practice solving models of this form did not con-
verge after hours even for small sentences.
To get around this problem we followed an incre-
mental approach akin to Warme (1998). Instead of
adding constraints that forbid all possible cycles in
advance (this would result in an exponential num-
ber of constraints) we first solve the problem without
any cycle constraints. Only if the result contains cy-
cles we add constraints that forbid these cycles and
run the solver again. This process is repeated un-
til no more violated constraints are found. Figure 1
shows this algorithm.
Groetschel et al (1981) showed that such an ap-
proach will converge after a polynomial number of
iterations with respect to the number of variables.
1. Solve IP Pi
2. Find violated constraints C in the solution of Pi
3. if C = ? we are done
4. Pi+1 = Pi ? C
5. i = i + 1
6. goto (1)
Figure 1: Incremental Integer Linear Programming
In practice, this technique showed fast convergence
(less than 10 iterations) in most cases, yielding solv-
ing times of less than 0.5 seconds. However, for
some sentences in certain languages, such as Chi-
nese or Swedish, an optimal solution could not be
found after 500 iterations.
In the following section we present the bjective
function, variables and linear constraints that make
up the Integer Linear Program.
2.1.1 Variables
In the implementation1 of McDonald et al
(2005b) dependency labels are handled by finding
the best scoring label for a given token pair so that
s (i, j) = max s (i, j, label)
goes into Equation 1. This is only exact as long as no
further constraints are added. Since our aim is to add
constraints our variables need to explicitly model la-
bel decisions. Therefore, we introduce binary vari-
ables
li,j,label?i ? 0..n, j ? 1..n, label ? bestb (i, j)
where n is the number of tokens and the index 0
represents the root token. bestb (i, j) is the set of b
labels with maximal s (i, j, label). li,j,label equals 1
if there is a dependency with the label label between
token i (head) and j (child), 0 otherwise.
Furthermore, we introduce binary auxiliary vari-
ables
di,j?i ? 0..n, j ? 1..n
representing the existence of a dependency between
tokens i and j. We connect these to the li,j,label vari-
ables by a constraint
di,j =
?
label
li,j,label
.
1Note, however, that labelled parsing is not described in the
publication.
227
2.1.2 Objective Function
Given the above variables our objective function
can be represented as
?
i,j
?
label?bestk(i,j)
s (i, j, label) ? li,j,label
with a suitable k.
2.1.3 Constraints Added in Advance
Only One Head In all our languages every token
has exactly one head. This yields
?
i>0
di,j = 1
for non-root tokens j > 0 and
?
i
di,0 = 0
for the artificial root node.
Typed Arity Constraints We might encounter so-
lutions of the basic model that contain, for instance,
verbs with two subjects. To forbid these we simply
augment our model with constraints such as
?
j
li,j,subject ? 1
for all verbs i in a sentence.
2.1.4 Incremental Constraints
No Cycles If a solution contains one or more cy-
cles C we add the following constraints to our IP:
For every c ? C we add
?
(i,j)?c
di,j ? |c| ? 1
to forbid c.
Coordination Argument Constraints In coordi-
nation conjuncts have to be of compatible types. For
example, nouns can not coordinate with verbs. We
implemented this constraint by checking the parses
for occurrences of incompatible arguments. If we
find two arguments j, k for a conjunction i: di,j and
di,k and j is a noun and k is a verb then we add
di,j + di,k ? 1
to forbid configurations in which both dependencies
are active.
Projective Parsing In the incremental ILP frame-
work projective parsing can be easily implemented
by checking for crossing dependencies after each it-
eration and forbidding them in the next. If we see
two dependencies that cross, di,j and dk,l, we add
the constraint
di,j + dk,l ? 1
to prevent this in the next iteration. This can also
be used to prevent specific types of crossings. For
instance, in Dutch we could only allow crossing de-
pendencies as long as none of the dependencies is a
?Determiner? relation.
2.2 Training
We used single-best MIRA(Crammer and Singer,
2003).For all experiments we used 10 training iter-
ations and non-projective decoding. Note that we
used the original spanning tree algorithm for decod-
ing during training as it was faster.
3 System Summary
We use four different feature sets. The first fea-
ture set, BASELINE, is taken from McDonald and
Pereira (2005b). It uses the FORM and the POSTAG
fields. This set alo includes features that combine
the label and POS tag of head and child such as
(Label, POSHead) and (Label, POSChild?1). For
our Arabic and Japanese development sets we ob-
tained the best results with this configuration. We
also use this configuration for Chinese, German and
Portuguese because training with other configura-
tions took too much time (more than 7 days).
The BASELINE also uses pseudo-coarse-POS tag
(1st character of the POSTAG) and pseudo-lemma
tag (4 characters of the FORM when the length
is more than 3). For the next configuration we
substitute these pseudo-tags by the CPOSTAG and
LEMMA fields that were given in the data. This con-
figuration was used for Czech because for other con-
figurations training could not be finished in time.
The third feature set tries to exploit the generic
FEATS field, which can contain a list features such
as case and gender. A set of features per depen-
dency is extracted using this information. It con-
sists of cross product of the features in FEATS. We
used this configuration for Danish, Dutch, Spanish
228
and Turkish where it showed the best results during
development.
The fourth feature set uses the triplet of la-
bel, POS child and head as a feature such as
(Label, POSHead, POSChild). It also uses the
CPOSTAG and LEMMA fields for the head. This
configuration is used for Slovene and Swedish data
where it performed best during development.
Finally, we add constraints for Chinese, Dutch,
Japanese and Slovene. In particular, arity constraints
to Chinese and Slovene, coordination and arity con-
straints to Dutch, arity and selective projectivity
constraints for Japanese2. For all experiments b was
set to 2. We did not apply additional constraints to
any other languages due to lack of time.
4 Results
Our results on the test set are shown in Table 1.
Our results are well above the average for all lan-
guages but Czech. For Chinese we perform signif-
icantly better than all other participants (p = 0.00)
and we are in the top three entries for Dutch, Ger-
man, Danish. Although Dutch and Chinese are lan-
guages were we included additional constraints, our
scores are not a result of these. Table 2 compares the
result for the languages with additional constraints.
Adding constraints only marginally helps to improve
the system (in the case of Slovene a bug in our im-
plementation even degraded accuracy). A more de-
tailed explanation to this observation is given in the
following section. A possible explanation for our
high accuracy in Chinese could be the fact that we
were not able to optimise the feature set on the de-
velopment set (see the previous section). Maybe this
prevented us from overfitting. It should be noted that
we did use non-projective parsing for Chinese, al-
though the corpus was fully projective. Our worst
results in comparison with other participants can be
seen for Czech. We attribute this to the reduced
training set we had to use in order to produce a
model in time, even when using the original MST
algorithm.
2This is done in order to capture the fact that crossing de-
pendencies in Japanese could only be introduced through dis-
fluencies.
4.1 Chinese
For Chinese the parser was augmented with a set of
constraints that disallowed more than one argument
of the types head, goal, nominal, range, theme, rea-
son, DUMMY, DUMMY1 and DUMMY2.
By enforcing arity constraints we could either turn
wrong labels/heads into right ones and improve ac-
curacy or turn right labels/heads into wrong ones and
degrade accuracy. For the test set the number of im-
provements (36) was higher than the number of er-
rors (22). However, this margin was outweighed by
a few sentences we could not properly process be-
cause our inference method timed out. Our overall
improvement was thus unimpressive 7 tokens.
In the context of duplicate ?head? dependencies
(that is, dependencies labelled ?head?) the num-
ber of sentences where accuracy dropped far out-
weighed the number of sentences where improve-
ments could be gained. Removing the arity con-
straints on ?head? labels therefore should improve
our results.
This shows the importance of good second best
dependencies. If the dependency with the second
highest score is the actual gold dependency and its
score is close to the highest score, we are likely to
pick this dependency in the presence of additional
constraints. On the other hand, if the dependency
with the second highest score is not the gold one and
its score is too high, we will probably include this
dependency in order to fulfil the constraints.
There may be some further improvement to be
gained if we train our model using k-best MIRA
with k > 1 since it optimises weights with respect
to the k best parses.
4.2 Turkish
There is a considerable gap between the unlabelled
and labelled results for Turkish. And in terms of la-
bels the POS type Noun gives the worst performance
because many times a subject was classified as ob-
ject or vice a versa.
Case information in Turkish assigns argument
roles for nouns by marking different semantic roles.
Many errors in the Turkish data might have been
caused by the fact that this information was not ad-
equately used. Instead of fine-tuning our feature set
to Turkish we used the feature cross product as de-
229
Model AR CH CZ DA DU GE JP PO SL SP SW TU
OURS 66.65 89.96 67.64 83.63 78.59 86.24 90.51 84.43 71.20 77.38 80.66 58.61
AVG 59.94 78.32 67.17 78.31 70.73 78.58 85.86 80.63 65.16 73.53 76.44 55.95
TOP 66.91 89.96 80.18 84.79 79.19 87.34 91.65 87.60 73.44 82.25 84.58 65.68
Table 1: Labelled accuracy on the test sets.
Constraints DU CH SL JA
with 3927 4464 3612 4526
without 3928 4471 3563 4528
Table 2: Number of tokens correctly classified with
and without constraints.
scribed in Section 3. Some of the rather meaning-
less combinations might have neutralised the effect
of sensible ones. We believe that using morpho-
logical case information in a sound way would im-
prove both the unlabelled and the labelled dependen-
cies. However, we have not performed a separate ex-
periment to test if using the case information alone
would improve the system any better. This could be
the focus of future work.
5 Conclusion
In this work we presented a novel way of solving the
linear model of McDonald et al (2005a) for projec-
tive and non-projective parsing based on an incre-
mental ILP approach. This allowed us to include
additional linguistics constraints such as ?a verb can
only have one subject.?
Due to time constraints we applied additional
constraints to only four languages. For each one
we gained better results than the baseline without
constraints, however, this improvement was only
marginal. This can be attributed to 4 main rea-
sons: Firstly, the next best solution that fulfils the
constraints was even worse (Chinese). Secondly,
noisy POS tags caused coordination constraints to
fail (Dutch). Thirdly, inference timed out (Chinese)
and fourthly, constraints were not violated that often
in the first place (Japanese).
However, the effect of the first problem might be
reduced by training with a higher k. The second
problem could partly be overcome by using a bet-
ter tagger or by a special treatment within the con-
straint handling for word types which are likely to
be mistagged. The third problem could be avoidable
by adding constraints during the branch and bound
algorithm, avoiding the need to resolve the full prob-
lem ?from scratch? for every constraint added. With
these remedies significant improvements to the ac-
curacy for some languages might be possible.
6 Acknowledgements
We would like to thank Beata Kouchnir, Abhishek
Arun and James Clarke for their help during the
course of this project.
References
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach. Learn.
Res., 3:951?991.
M. Groetschel, L. Lovasz, and A. Schrijver. 1981. The ellipsoid
method and its consequences in combinatorial optimization.
Combinatorica, I:169? 197.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. of the 11th
Annual Meeting of the EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In Proc. of the
43rd Annual Meeting of the ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Ha-
jic. 2005b. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT/EMNLP 2005,
Vancouver, B.C., Canada.
D. Roth and W. Yih. 2005. Integer linear programming in-
ference for conditional random fields. In Proc. of the In-
ternational Conference on Machine Learning (ICML), pages
737?744.
David Michael Warme. 1998. Spanning Trees in Hypergraphs
with Application to Steiner Trees. Ph.D. thesis, University of
Virginia.
Justin C. Williams. 2002. A linear-size zero - one program-
ming model for the minimum spanning tree problem in pla-
nar graphs. Networks, 39:53?60.
230
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 405?413,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Jointly Identifying Temporal Relations with Markov Logic
Katsumasa Yoshikawa
NAIST, Japan
katsumasa-y@is.naist.jp
Sebastian Riedel
University of Tokyo, Japan
sebastian.riedel@gmail.com
Masayuki Asahara
NAIST, Japan
masayu-a@is.naist.jp
Yuji Matsumoto
NAIST, Japan
matsu@is.naist.jp
Abstract
Recent work on temporal relation iden-
tification has focused on three types of
relations between events: temporal rela-
tions between an event and a time expres-
sion, between a pair of events and between
an event and the document creation time.
These types of relations have mostly been
identified in isolation by event pairwise
comparison. However, this approach ne-
glects logical constraints between tempo-
ral relations of different types that we be-
lieve to be helpful. We therefore propose a
Markov Logic model that jointly identifies
relations of all three relation types simul-
taneously. By evaluating our model on the
TempEval data we show that this approach
leads to about 2% higher accuracy for all
three types of relations ?and to the best
results for the task when compared to those
of other machine learning based systems.
1 Introduction
Temporal relation identification (or temporal or-
dering) involves the prediction of temporal order
between events and/or time expressions mentioned
in text, as well as the relation between events in a
document and the time at which the document was
created.
With the introduction of the TimeBank corpus
(Pustejovsky et al, 2003), a set of documents an-
notated with temporal information, it became pos-
sible to apply machine learning to temporal order-
ing (Boguraev and Ando, 2005; Mani et al, 2006).
These tasks have been regarded as essential for
complete document understanding and are useful
for a wide range of NLP applications such as ques-
tion answering and machine translation.
Most of these approaches follow a simple
schema: they learn classifiers that predict the tem-
poral order of a given event pair based on a set of
the pair?s of features. This approach is local in the
sense that only a single temporal relation is consid-
ered at a time.
Learning to predict temporal relations in this iso-
lated manner has at least two advantages over any
approach that considers several temporal relations
jointly. First, it allows us to use off-the-shelf ma-
chine learning software that, up until now, has been
mostly focused on the case of local classifiers. Sec-
ond, it is computationally very efficient both in
terms of training and testing.
However, the local approach has a inherent
drawback: it can lead to solutions that violate logi-
cal constraints we know to hold for any sets of tem-
poral relations. For example, by classifying tempo-
ral relations in isolation we may predict that event
A happened before, and event B after, the time
of document creation, but also that event A hap-
pened after event B?a clear contradiction in terms
of temporal logic.
In order to repair the contradictions that the local
classifier predicts, Chambers and Jurafsky (2008)
proposed a global framework based on Integer Lin-
ear Programming (ILP). They showed that large
improvements can be achieved by explicitly incor-
porating temporal constraints.
The approach we propose in this paper is similar
in spirit to that of Chambers and Jurafsky: we seek
to improve the accuracy of temporal relation iden-
tification by predicting relations in a more global
manner. However, while they focused only on the
temporal relations between events mentioned in a
document, we also jointly predict the temporal or-
der between events and time expressions, and be-
tween events and the document creation time.
Our work also differs in another important as-
pect from the approach of Chambers and Jurafsky.
Instead of combining the output of a set of local
classifiers using ILP, we approach the problem of
joint temporal relation identification using Markov
Logic (Richardson and Domingos, 2006). In this
405
framework global correlations can be readily cap-
tured through the addition of weighted first order
logic formulae.
Using Markov Logic instead of an ILP-based ap-
proach has at least two advantages. First, it allows
us to easily capture non-deterministic (soft) rules
that tend to hold between temporal relations but do
not have to. 1 For example, if event A happens be-
fore B, and B overlaps with C, then there is a good
chance that A also happens before C, but this is not
guaranteed.
Second, the amount of engineering required to
build our system is similar to the efforts required
for using an off-the-shelf classifier: we only need
to define features (in terms of formulae) and pro-
vide input data in the correct format. 2 In particu-
lar, we do not need to manually construct ILPs for
each document we encounter. Moreover, we can
exploit and compare advanced methods of global
inference and learning, as long as they are imple-
mented in our Markov Logic interpreter of choice.
Hence, in our future work we can focus entirely
on temporal relations, as opposed to inference or
learning techniques for machine learning.
We evaluate our approach using the data of the
?TempEval? challenge held at the SemEval 2007
Workshop (Verhagen et al, 2007). This challenge
involved three tasks corresponding to three types
of temporal relations: between events and time ex-
pressions in a sentence (Task A), between events of
a document and the document creation time (Task
B), and between events in two consecutive sen-
tences (Task C).
Our findings show that by incorporating global
constraints that hold between temporal relations
predicted in Tasks A, B and C, the accuracy for
all three tasks can be improved significantly. In
comparison to other participants of the ?TempE-
val? challenge our approach is very competitive:
for two out of the three tasks we achieve the best
results reported so far, by a margin of at least 2%. 3
Only for Task Bwe were unable to reach the perfor-
mance of a rule-based entry to the challenge. How-
ever, we do perform better than all pure machine
1It is clearly possible to incorporate weighted constraints
into ILPs, but how to learn the corresponding weights is not
obvious.
2This is not to say that picking the right formulae in
Markov Logic, or features for local classification, is always
easy.
3To be slightly more precise: for Task C we achieve this
margin only for ?strict? scoring?see sections 5 and 6 for more
details.
learning-based entries.
The remainder of this paper is organized as fol-
lows: Section 2 describes temporal relation identi-
fication including TempEval; Section 3 introduces
Markov Logic; Section 4 explains our proposed
Markov Logic Network; Section 5 presents the set-
up of our experiments; Section 6 shows and dis-
cusses the results of our experiments; and in Sec-
tion 7 we conclude and present ideas for future re-
search.
2 Temporal Relation Identification
Temporal relation identification aims to predict
the temporal order of events and/or time expres-
sions in documents, as well as their relations to the
document creation time (DCT). For example, con-
sider the following (slightly simplified) sentence of
Section 1 in this paper.
With the introduction of the TimeBank cor-
pus (Pustejovsky et al, 2003), machine
learning approaches to temporal ordering
became possible.
Here we have to predict that the ?Machine learn-
ing becoming possible? event happened AFTER
the ?introduction of the TimeBank corpus? event,
and that it has a temporal OVERLAP with the year
2003. Moreover, we need to determine that both
events happened BEFORE the time this paper was
created.
Most previous work on temporal relation iden-
tification (Boguraev and Ando, 2005; Mani et al,
2006; Chambers and Jurafsky, 2008) is based on
the TimeBank corpus. The temporal relations in
the Timebank corpus are divided into 11 classes;
10 of them are defined by the following 5 relations
and their inverse: BEFORE, IBEFORE (immedi-
ately before), BEGINS, ENDS, INCLUDES; the re-
maining one is SIMULTANEOUS.
In order to drive forward research on temporal
relation identification, the SemEval 2007 shared
task (Verhagen et al, 2007) (TempEval) included
the following three tasks.
TASK A Temporal relations between events and
time expressions that occur within the same
sentence.
TASK B Temporal relations between the Docu-
ment Creation Time (DCT) and events.
TASK C Temporal relations between the main
events of adjacent sentences.4
4The main event of a sentence is expressed by its syntacti-
cally dominant verb.
406
To simplify matters, in the TempEval data, the
classes of temporal relations were reduced from
the original 11 to 6: BEFORE, OVERLAP, AFTER,
BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER,
and VAGUE.
In this work we are focusing on the three tasks of
TempEval, and our running hypothesis is that they
should be tackled jointly. That is, instead of learn-
ing separate probabilistic models for each task, we
want to learn a single one for all three tasks. This
allows us to incorporate rules of temporal consis-
tency that should hold across tasks. For example, if
an event X happens before DCT, and another event
Y after DCT, then surely X should have happened
before Y. We illustrate this type of transition rule in
Figure 1.
Note that the correct temporal ordering of events
and time expressions can be controversial. For in-
stance, consider the example sentence again. Here
one could argue that ?the introduction of the Time-
Bank? may OVERLAP with ?Machine learning be-
coming possible? because ?introduction? can be
understood as a process that is not finished with
the release of the data but also includes later adver-
tisements and announcements. This is reflected by
the low inter-annotator agreement score of 72% on
Tasks A and B, and 68% on Task C.
3 Markov Logic
It has long been clear that local classification
alone cannot adequately solve all prediction prob-
lems we encounter in practice.5 This observa-
tion motivated a field within machine learning,
often referred to as Statistical Relational Learn-
ing (SRL), which focuses on the incorporation
of global correlations that hold between statistical
variables (Getoor and Taskar, 2007).
One particular SRL framework that has recently
gained momentum as a platform for global learn-
ing and inference in AI is Markov Logic (Richard-
son and Domingos, 2006), a combination of first-
order logic and Markov Networks. It can be under-
stood as a formalism that extends first-order logic
to allow formulae that can be violated with some
penalty. From an alternative point of view, it is an
expressive template language that uses first order
logic formulae to instantiate Markov Networks of
repetitive structure.
From a wide range of SRL languages we chose
Markov Logic because it supports discriminative
5It can, however, solve a large number of problems surpris-
ingly well.
Figure 1: Example of Transition Rule 1
training (as opposed to generative SRL languages
such as PRM (Koller, 1999)). Moreover, sev-
eral Markov Logic software libraries exist and are
freely available (as opposed to other discrimina-
tive frameworks such as Relational Markov Net-
works (Taskar et al, 2002)).
In the following we will explain Markov Logic
by example. One usually starts out with a set
of predicates that model the decisions we need to
make. For simplicity, let us assume that we only
predict two types of decisions: whether an event e
happens before the document creation time (DCT),
and whether, for a pair of events e1 and e2, e1
happens before e2. Here the first type of deci-
sion can be modeled through a unary predicate
beforeDCT(e), while the latter type can be repre-
sented by a binary predicate before(e1, e2). Both
predicates will be referred to as hidden because we
do not know their extensions at test time. We also
introduce a set of observed predicates, representing
information that is available at test time. For ex-
ample, in our case we could introduce a predicate
futureTense(e) which indicates that e is an event
described in the future tense.
With our predicates defined, we can now go on
to incorporate our intuition about the task using
weighted first-order logic formulae. For example,
it seems reasonable to assume that
futureTense (e) ? ?beforeDCT (e) (1)
often, but not always, holds. Our remaining un-
certainty with regard to this formula is captured
by a weight w we associate with it. Generally
we can say that the larger this weight is, the more
likely/often the formula holds in the solutions de-
scribed by our model. Note, however, that we do
not need to manually pick these weights; instead
they are learned from the given training corpus.
The intuition behind the previous formula can
also be captured using a local classifier.6 However,
6Consider a log-linear binary classifier with a ?past-tense?
407
Markov Logic also allows us to say more:
beforeDCT (e1) ? ?beforeDCT (e2)
? before (e1, e2) (2)
In this case, we made a statement about more
global properties of a temporal ordering that can-
not be captured with local classifiers. This formula
is also an example of the transition rules as seen in
Figure 2. This type of rule forms the core idea of
our joint approach.
A Markov Logic Network (MLN) M is a set of
pairs (?,w) where ? is a first order formula and w
is a real number (the formula?s weight). It defines a
probability distribution over sets of ground atoms,
or so-called possible worlds, as follows:
p (y) = 1
Z
exp
?
?
?
(?,w)?M
w
?
c?C?
f?c (y)
?
? (3)
Here each c is a binding of free variables in ? to
constants in our domain. Each f?c is a binary fea-
ture function that returns 1 if in the possible world
y the ground formula we get by replacing the free
variables in ? with the constants in c is true, and
0 otherwise. C? is the set of all bindings for the
free variables in ?. Z is a normalisation constant.
Note that this distribution corresponds to a Markov
Network (the so-called Ground Markov Network)
where nodes represent ground atoms and factors
represent ground formulae.
Designing formulae is only one part of the game.
In practice, we also need to choose a training
regime (in order to learn the weights of the formu-
lae we added to the MLN) and a search/inference
method that picks the most likely set of ground
atoms (temporal relations in our case) given our
trained MLN and a set of observations. How-
ever, implementations of these methods are often
already provided in existing Markov Logic inter-
preters such as Alchemy 7 and Markov thebeast. 8
4 Proposed Markov Logic Network
As stated before, our aim is to jointly tackle
Tasks A, B and C of the TempEval challenge. In
this section we introduce the Markov Logic Net-
work we designed for this goal.
We have three hidden predicates, corresponding
to Tasks A, B, and C: relE2T(e, t, r) represents the
temporal relation of class r between an event e
feature: here for every event e the decision ?e happens be-
fore DCT? becomes more likely with a higher weight for this
feature.
7http://alchemy.cs.washington.edu/
8http://code.google.com/p/thebeast/
Figure 2: Example of Transition Rule 2
and a time expression t; relDCT(e, r) denotes the
temporal relation r between an event e and DCT;
relE2E(e1, e2, r) represents the relation r between
two events of the adjacent sentences, e1 and e2.
Our observed predicates reflect information we
were given (such as the words of a sentence), and
additional information we extracted from the cor-
pus (such as POS tags and parse trees). Note that
the TempEval data also contained temporal rela-
tions that were not supposed to be predicted. These
relations are represented using two observed pred-
icates: relT2T(t1, t2, r) for the relation r between
two time expressions t1 and t2; dctOrder(t, r) for
the relation r between a time expression t and a
fixed DCT.
An illustration of all ?temporal? predicates, both
hidden and observed, can be seen in Figure 3.
4.1 Local Formula
Our MLN is composed of several weighted for-
mulae that we divide into two classes. The first
class contains local formulae for the Tasks A, B
and C. We say that a formula is local if it only
considers the hidden temporal relation of a single
event-event, event-time or event-DCT pair. The
formulae in the second class are global: they in-
volve two or more temporal relations at the same
time, and consider Tasks A, B and C simultane-
ously.
The local formulae are based on features em-
ployed in previous work (Cheng et al, 2007;
Bethard andMartin, 2007) and are listed in Table 1.
What follows is a simple example in order to illus-
trate how we implement each feature as a formula
(or set of formulae).
Consider the tense-feature for Task C. For this
feature we first introduce a predicate tense(e, t)
that denotes the tense t for an event e. Then we
408
Figure 3: Predicates for Joint Formulae; observed
predicates are indicated with dashed lines.
Table 1: Local Features
Feature A B C
EVENT-word X X
EVENT-POS X X
EVENT-stem X X
EVENT-aspect X X X
EVENT-tense X X X
EVENT-class X X X
EVENT-polarity X X
TIMEX3-word X
TIMEX3-POS X
TIMEX3-value X
TIMEX3-type X
TIMEX3-DCT order X X
positional order X
in/outside X
unigram(word) X X
unigram(POS) X X
bigram(POS) X
trigram(POS) X X
Dependency-Word X X X
Dependency-POS X X
add a set of formulae such as
tense(e1, past) ? tense(e2, future)
? relE2E(e1, e2, before) (4)
for all possible combinations of tenses and tempo-
ral relations.9
4.2 Global Formula
Our global formulae are designed to enforce con-
sistency between the three hidden predicates (and
the two observed temporal predicates we men-
tioned earlier). They are based on the transition
9This type of ?template-based? formulae generation can be
performed automatically by the Markov Logic Engine.
rules we mentioned in Section 3.
Table 2 shows the set of formula templates we
use to generate the global formulae. Here each
template produces several instantiations, one for
each assignment of temporal relation classes to the
variables R1, R2, etc. One example of a template
instantiation is the following formula.
dctOrder(t1, before) ? relDCT(e1, after)
? relE2T(e1, t1, after) (5a)
This formula is an expansion of the formula tem-
plate in the second row of Table 2. Note that it
utilizes the results of Task B to solve Task A.
Formula 5a should always hold,10 and hence we
could easily implement it as a hard constraint in
an ILP-based framework. However, some transi-
tion rules are less determinstic and should rather
be taken as ?rules of thumb?. For example, for-
mula 5b is a rule which we expect to hold often,
but not always.
dctOrder(t1, before) ? relDCT(e1, overlap)
? relE2T(e1, t1, after) (5b)
Fortunately, this type of soft rule poses no prob-
lem for Markov Logic: after training, Formula 5b
will simply have a lower weight than Formula 5a.
By contrast, in a ?Local Classifier + ILP?-based
approach as followed by Chambers and Jurafsky
(2008) it is less clear how to proceed in the case
of soft rules. Surely it is possible to incorporate
weighted constraints into ILPs, but how to learn the
corresponding weights is not obvious.
5 Experimental Setup
With our experiments we want to answer two
questions: (1) does jointly tackling Tasks A, B,
and C help to increase overall accuracy of tempo-
ral relation identification? (2) How does our ap-
proach compare to state-of-the-art results? In the
following we will present the experimental set-up
we chose to answer these questions.
In our experiments we use the test and training
sets provided by the TempEval shared task. We
further split the original training data into a training
and a development set, used for optimizing param-
eters and formulae. For brevity we will refer to the
training, development and test set as TRAIN, DEV
and TEST, respectively. The numbers of temporal
relations in TRAIN, DEV, and TEST are summa-
rized in Table 3.
10However, due to inconsistent annotations one will find vi-
olations of this rule in the TempEval data.
409
Table 2: Joint Formulae for Global Model
Task Formula
A? B dctOrder(t, R1) ? relE2T(e, t, R2) ? relDCT(e,R3)
B ? A dctOrder(t, R1) ? relDCT(e,R2) ? relE2T(e, t, R3)
B ? C relDCT(e1, R1) ? relDCT(e2, R2) ? relE2E(e1, e2, R3)
C ? B relDCT(e1, R1) ? relE2E(e1, e2, R2) ? relDCT(e2, R3)
A? C relE2T(e1, t1, R1) ? relT2T(t1, t2, R2) ? relE2T(e2, t2, R3) ? relE2E(e1, e2, R4)
C ? A relE2T(e2, t2, R1) ? relT2T(t1, t2, R2) ? relE2E(e1, e2, R3) ? relE2T(e1, t1, R4)
Table 3: Numbers of Labeled Relations for All
Tasks
TRAIN DEV TEST TOTAL
Task A 1359 131 169 1659
Task B 2330 227 331 2888
Task C 1597 147 258 2002
For feature generation we use the following
tools. 11 POS tagging is performed with TnT
ver2.2;12 for our dependency-based features we use
MaltParser 1.0.0.13 For inference in our models
we use Cutting Plane Inference (Riedel, 2008) with
ILP as a base solver. This type of inference is ex-
act and often very fast because it avoids instantia-
tion of the complete Markov Network. For learning
we apply one-best MIRA (Crammer and Singer,
2003) with Cutting Plane Inference to find the cur-
rent model guess. Both training and inference algo-
rithms are provided by Markov thebeast, a Markov
Logic interpreter tailored for NLP applications.
Note that there are several ways to manually op-
timize the set of formulae to use. One way is to
pick a task and then choose formulae that increase
the accuracy for this task on DEV. However, our
primary goal is to improve the performance of all
the tasks together. Hence we choose formulae with
respect to the total score over all three tasks. We
will refer to this type of optimization as ?averaged
optimization?. The total scores of the all three tasks
are defined as follows:
Ca + Cb + Cc
Ga +Gb +Gc
where Ca, Cb, and Cc are the number of the cor-
rectly identified labels in each task, and Ga, Gb,
and Gc are the numbers of gold labels of each task.
Our system necessarily outputs one label to one re-
lational link to identify. Therefore, for all our re-
11Since the TempEval trial has no restriction on pre-
processing such as syntactic parsing, most participants used
some sort of parsers.
12http://www.coli.uni-saarland.de/
?thorsten/tnt/
13http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
sults, precision, recall, and F-measure are the exact
same value.
For evaluation, TempEval proposed the two scor-
ing schemes: ?strict? and ?relaxed?. For strict scor-
ing we give full credit if the relations match, and no
credit if they do not match. On the other hand, re-
laxed scoring gives credit for a relation according
to Table 4. For example, if a system picks the re-
lation ?AFTER? that should have been ?BEFORE?
according to the gold label, it gets neither ?strict?
nor ?relaxed? credit. But if the system assigns
?B-O (BEFORE-OR-OVERLAP)? to the relation,
it gets a 0.5 ?relaxed? score (and still no ?strict?
score).
6 Results
In the following we will first present our com-
parison of the local and global model. We will then
go on to put our results into context and compare
them to the state-of-the-art.
6.1 Impact of Global Formulae
First, let us show the results on TEST in Ta-
ble 5. You will find two columns, ?Global? and
?Local?, showing scores achieved with and with-
out joint formulae, respectively. Clearly, the global
models scores are higher than the local scores for
all three tasks. This is also reflected by the last row
of Table 5. Here we see that we have improved
the averaged performance across the three tasks by
approximately 2.5% (? < 0.01, McNemar?s test 2-
tailed). Note that with 3.5% the improvements are
particularly large for Task C.
The TempEval test set is relatively small (see Ta-
ble 3). Hence it is not clear how well our results
would generalize in practice. To overcome this is-
sue, we also evaluated the local and global model
using 10-fold cross validation on the training data
(TRAIN + DEV). The corresponding results can be
seen in Table 6. Note that the general picture re-
mains: performance for all tasks is improved, and
the averaged score is improved only slightly less
than for the TEST results. However, this time the
score increase for Task B is lower than before. We
410
Table 4: Evaluation Weights for Relaxed Scoring
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
B: BEFORE O: OVERLAP
A: AFTER B-O: BEFORE-OR-OVERLAP
O-A: OVERLAP-OR-AFTER V: VAGUE
Table 5: Results on TEST Set
Local Global
task strict relaxed strict relaxed
Task A 0.621 0.669 0.645 0.687
Task B 0.737 0.753 0.758 0.777
Task C 0.531 0.599 0.566 0.632
All 0.641 0.682 0.668 0.708
Table 6: Results with 10-fold Cross Validation
Local Global
task strict relaxed strict relaxed
Task A 0.613 0.645 0.662 0.691
Task B 0.789 0.810 0.799 0.819
Task C 0.533 0.608 0.552 0.623
All 0.667 0.707 0.689 0.727
see that this is compensated by much higher scores
for Task A and C. Again, the improvements for all
three tasks are statistically significant (? < 10?8,
McNemar?s test, 2-tailed).
To summarize, we have shown that by tightly
connecting tasks A, B and C, we can improve tem-
poral relation identification significantly. But are
we just improving a weak baseline, or can joint
modelling help to reach or improve the state-of-the-
art results? We will try to answer this question in
the next section.
6.2 Comparison to the State-of-the-art
In order to put our results into context, Table 7
shows them along those of other TempEval par-
ticipants. In the first row, TempEval Best gives
the best scores of TempEval for each task. Note
that all but the strict scores of Task C are achieved
by WVALI (Puscasu, 2007), a hybrid system that
combines machine learning and hand-coded rules.
In the second row we see the TempEval average
scores of all six participants in TempEval. The
third row shows the results of CU-TMP (Bethard
and Martin, 2007), an SVM-based system that
achieved the second highest scores in TempEval for
all three tasks. CU-TMP is of interest because it is
the best pure Machine-Learning-based approach so
far.
The scores of our local and global model come
in the fourth and fifth row, respectively. The last
row in the table shows task-adjusted scores. Here
we essentially designed and applied three global
MLNs, each one tailored and optimized for a dif-
ferent task. Note that the task-adjusted scores are
always about 1% higher than those of the single
global model.
Let us discuss the results of Table 7 in detail. We
see that for task A, our global model improves an
already strong local model to reach the best results
both for strict scores (with a 3% points margin) and
relaxed scores (with a 5% points margin).
For Task C we see a similar picture: here adding
global constraints helped to reach the best strict
scores, again by a wide margin. We also achieve
competitive relaxed scores which are in close range
to the TempEval best results.
Only for task B our results cannot reach the best
TempEval scores. While we perform slightly better
than the second-best system (CU-TMP), and hence
report the best scores among all pure Machine-
Learning based approaches, we cannot quite com-
pete with WVALI.
6.3 Discussion
Let us discuss some further characteristics and
advantages of our approach. First, notice that
global formulae not only improve strict but also re-
laxed scores for all tasks. This suggests that we
produce more ambiguous labels (such as BEFORE-
OR-OVERLAP) in cases where the local model has
been overconfident (and wrongly chose BEFORE
or OVERLAP), and hence make less ?fatal errors?.
Intuitively this makes sense: global consistency is
easier to achieve if our labels remain ambiguous.
For example, a solution that labels every relation
as VAGUE is globally consistent (but not very in-
formative).
Secondly, one could argue that our solution to
joint temporal relation identification is too com-
plicated. Instead of performing global inference,
one could simply arrange local classifiers for the
tasks into a pipeline. In fact, this has been done by
Bethard and Martin (2007): they first solve task B
and then use this information as features for Tasks
A and C.While they do report improvements (0.7%
411
Table 7: Comparison with Other Systems
Task A Task B Task C
strict relaxed strict relaxed strict relaxed
TempEval Best 0.62 0.64 0.80 0.81 0.55 0.64
TempEval Average 0.56 0.59 0.74 0.75 0.51 0.58
CU-TMP 0.61 0.63 0.75 0.76 0.54 0.58
Local Model 0.62 0.67 0.74 0.75 0.53 0.60
Global Model 0.65 0.69 0.76 0.78 0.57 0.63
Global Model (Task-Adjusted) (0.66) (0.70) (0.76) (0.79) (0.58) (0.64)
on Task A, and about 0.5% on Task C), generally
these improvements do not seem as significant as
ours. What is more, by design their approach can
not improve the first stage (Task B) of the pipeline.
On the same note, we also argue that our ap-
proach does not require more implementation ef-
forts than a pipeline. Essentially we only have to
provide features (in the form of formulae) to the
Markov Logic Engine, just as we have to provide
for a SVM or MaxEnt classifier.
Finally, it became more clear to us that there are
problems inherent to this task and dataset that we
cannot (or only partially) solve using global meth-
ods. First, there are inconsistencies in the training
data (as reflected by the low inter-annotator agree-
ment) that often mislead the learner?this prob-
lem applies to learning of local and global formu-
lae/features alike. Second, the training data is rela-
tively small. Obviously, this makes learning of re-
liable parameters more difficult, particularly when
data is as noisy as in our case. Third, the tempo-
ral relations in the TempEval dataset only directly
connect a small subset of events. This makes global
formulae less effective.14
7 Conclusion
In this paper we presented a novel approach to
temporal relation identification. Instead of using
local classifiers to predict temporal order in a pair-
wise fashion, our approach uses Markov Logic to
incorporate both local features and global transi-
tion rules between temporal relations.
We have focused on transition rules between
temporal relations of the three TempEval subtasks:
temporal ordering of events, of events and time ex-
pressions, and of events and the document creation
time. Our results have shown that global transition
rules lead to significantly higher accuracy for all
three tasks. Moreover, our global Markov Logic
14See (Chambers and Jurafsky, 2008) for a detailed discus-
sion of this problem, and a possible solution for it.
model achieves the highest scores reported so far
for two of three tasks, and very competitive results
for the remaining one.
While temporal transition rules can also be cap-
tured with an Integer Linear Programming ap-
proach (Chambers and Jurafsky, 2008), Markov
Logic has at least two advantages. First, handling
of ?rules of thumb? between less specific tempo-
ral relations (such as OVERLAP or VAGUE) is
straightforward?we simply let the Markov Logic
Engine learn weights for these rules. Second, there
is less engineering overhead for us to perform, be-
cause we do not need to generate ILPs for each doc-
ument.
However, potential for further improvements
through global approaches seems to be limited by
the sparseness and inconsistency of the data. To
overcome this problem, we are planning to use ex-
ternal or untagged data along with methods for un-
supervised learning in Markov Logic (Poon and
Domingos, 2008).
Furthermore, TempEval-2 15 is planned for 2010
and it has challenging temporal ordering tasks in
five languages. So, we would like to investigate the
utility of global formulae for multilingual tempo-
ral ordering. Here we expect that while lexical and
syntax-based features may be quite language de-
pendent, global transition rules should hold across
languages.
Acknowledgements
This work is partly supported by the Integrated
Database Project, Ministry of Education, Culture,
Sports, Science and Technology of Japan.
References
Steven Bethard and James H. Martin. 2007. Cu-tmp:
Temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Interna-
tional Workshop on SemEval-2007., pages 129?132.
15http://www.timeml.org/tempeval2/
412
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal reason-
ing. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence, pages 997?
1003.
Nathanael Chambers and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves tem-
poral ordering. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 698?706, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. Naist.japan: Temporal relation iden-
tification using dependency parsed tree. In Proceed-
ings of the 4th International Workshop on SemEval-
2007., pages 245?248.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning (Adaptive Computation
and Machine Learning). The MIT Press.
Daphne Koller, 1999. Probabilistic Relational Models,
pages 3?13. Springer, Berlin/Heidelberg, Germany.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006.
Machine learning of temporal relations. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 753?760, Morristown, NJ, USA.
Association for Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
650?659, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Georgiana Puscasu. 2007. Wvali: Temporal relation
identification by syntactico-semantic analysis. In
Proceedings of the 4th International Workshop on
SemEval-2007., pages 484?487.
James Pustejovsky, Jose Castano, Robert Ingria, Reser
Sauri, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. The timebank corpus. In Proceed-
ings of Corpus Linguistics 2003, pages 647?656.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. In Machine Learning.
Sebastian Riedel. 2008. Improving the accuracy and
efficiency of map inference for markov logic. In Pro-
ceedings of UAI 2008.
Ben Taskar, Abbeel Pieter, and Daphne Koller. 2002.
Discriminative probabilistic models for relational
data. In Proceedings of the 18th Annual Conference
on Uncertainty in Artificial Intelligence (UAI-02),
pages 485?492, San Francisco, CA. Morgan Kauf-
mann.
Marc Verhagen, Robert Gaizaukas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proceedings of the 4th Inter-
national Workshop on SemEval-2007., pages 75?80.
413
Proceedings of NAACL HLT 2009: Short Papers, pages 5?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Revisiting Optimal Decoding for Machine Translation IBM Model 4
Sebastian Riedel?? James Clarke?
?Department of Computer Science, University of Tokyo, Japan
?Database Center for Life Science, Research Organization of Information and System, Japan
?Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801
?sebastian.riedel@gmail.com ?clarkeje@gmail.com
Abstract
This paper revisits optimal decoding for statis-
tical machine translation using IBM Model 4.
We show that exact/optimal inference using
Integer Linear Programming is more practical
than previously suggested when used in con-
junction with the Cutting-Plane Algorithm. In
our experiments we see that exact inference
can provide a gain of up to one BLEU point
for sentences of length up to 30 tokens.
1 Introduction
Statistical machine translation (MT) systems typ-
ically contain three essential components: (1) a
model, specifying how the process of translation oc-
curs; (2) learning regime, dictating the estimation of
model?s parameters; (3) decoding algorithm which
provides the most likely translation of an input sen-
tence given a model and its parameters.
The search space in statistical machine transla-
tion is vast which can make it computationally pro-
hibitively to perform exact/optimal decoding (also
known as search and MAP inference) especially
since dynamic programming methods (such as the
Viterbi algorithm) are typically not applicable. Thus
greedy or heuristic beam-based methods have been
prominent (Koehn et al, 2007) due to their effi-
ciency. However, the efficiency of such methods
have two drawbacks: (1) they are approximate and
give no bounds as to how far their solution is
away from the true optimum; (2) it can be difficult
to incorporate additional generic global constraints
into the search. The first point may be especially
problematic from a research perspective as without
bounds on the solutions it is difficult to determine
whether the model or the search algorithm requires
improvement for better translations.
Similar problems exist more widely throughout
natural language processing where greedy based
methods and heuristic beam search have been used
in lieu of exact methods. However, recently there has
been an increasing interest in using Integer Linear
Programming (ILP) as a means to find MAP solu-
tions. ILP overcomes the two drawbacks mentioned
above as it is guaranteed to be exact, and has the
ability to easily enforce global constraints through
additional linear constraints. However, efficiency is
usually sacrificed for these benefits.
Integer Linear Programming has previously been
used to perform exact decoding for MT using IBM
Model 4 and a bigram language model. Germann
et al (2004) view the translation process akin to the
travelling salesman problem; however, from their re-
ported results it is clear that using ILP naively for de-
coding does not scale up beyond short sentences (of
eight tokens). This is due to the exponential num-
ber of constraints required to represent the decod-
ing problem as an ILP program. However, work in
dependency parsing (Riedel and Clarke, 2006) has
demonstrated that it is possible to use ILP to perform
efficient inference for very large programs when
used in an incremental manner. This raises the ques-
tion as to whether incremental (or Cutting-Plane)
ILP can also be used to decode IBM Model 4 on
real world sentences.
In this work we show that it is possible. Decod-
ing IBM Model 4 (in combination with a bigram
language model) using Cutting-Plane ILP scales to
much longer sentences. This affords us the oppor-
tunity to finally analyse the performance of IBM
Model 4 and the performance of its state-of-the-
5
art ReWrite decoder. We show that using exact in-
ference provides an increase of up to one BLEU
point on two language pairs (French-English and
German-English) in comparison to decoding using
the ReWrite decoder. Thus the ReWrite decoder per-
forms respectably but can be improved slightly, al-
beit at the cost of efficiency.
Although the community has generally moved
away from word-based models, we believe that dis-
playing optimal decoding in IBM Model 4 lays the
foundations of future work. It is the first step in pro-
viding a method for researchers to gain greater in-
sight into their translation models by mapping the
decoding problem of other models into an ILP rep-
resentation. ILP decoding will also allow the incor-
poration of global linguistic constraints in a manner
similar to work in other areas of natural language
processing.
The remainder of this paper is organised as fol-
lows: Sections 2 and 3 briefly recap IBM Model 4
and its ILP formulation. Section 4 reviews the
Cutting-Plane Algorithm. Section 5 outlines our ex-
periments and we end the paper with conclusions
and a discussion of open questions for the commu-
nity.
2 IBM Model 4
In this paper we focus on the translation model de-
fined by IBM Model 4 (Brown et al, 1993). Transla-
tion using IBM Model 4 is performed by treating the
translation process a noisy-channel model where the
probability of the English sentence given a French
sentence is, P (e|f) = P (f |e) ? P (e), where P (e) is
a language model of English. IBM Model 4 defines
P (f |e) and models the translation process as a gen-
erative process of how a sequence of target words
(in our case French or German) is generated from a
sequence of source words (English).
The generative story is as follows. Imagine we
have an English sentence, e = e1, . . . , el and along
with a NULL word (eo) and French sentence, f =
f1, . . . , fm. First a fertility is drawn for each English
word (including the NULL symbol). Then, for each
ei we then independently draws a number of French
words equal to ei?s fertility. Finally we process the
English source tokens in sequence to determine the
positions of their generated French target words. We
refer the reader to Brown et al (1993) for full details.
3 Integer Linear Programming
Formulation
Given a trained IBM Model 4 and a French sentence
f we need to find the English sentence e and align-
ment a with maximal p (a, e|f) w p (e) ? p (a, f |e).1
Germann et al (2004) present an ILP formula-
tion of this problem. In this section we will give a
very high-level description of the formulation.2 For
brevity we refer the reader to the original work for
more details.
In the formulation of Germann et al (2004) an
English translation is represented as the journey of
a travelling salesman that visits one English token
(hotel) per French token (city). Here the English to-
ken serves as the translation of the French one. A
set of binary variables denote whether or not cer-
tain English token pairs are directly connected in
this journey. A set of constraints guarantee that for
each French token exactly one English token is vis-
ited. The formulation also contains an exponential
number of constraints which forbid the possible cy-
cles the variables can represent. It is this set of con-
straints that renders MT decoding with ILP difficult.
4 Cutting Plane Algorithm
The ILP program above has an exponential number
of (cycle) constraints. Hence, simply passing the ILP
to an off-the-shelf ILP solver is not practical for all
but the smallest sentences. For this reason Germann
et al (2004) only consider sentences of up to eight
tokens. However, recent work (Riedel and Clarke,
2006) has shown that even exponentially large de-
coding problems may be solved efficiently using ILP
solvers if a Cutting-Plane Algorithm (Dantzig et al,
1954) is used.3
A Cutting-Plane Algorithm starts with a subset of
the complete set of constraints. In our case this sub-
set contains all but the (exponentially many) cycle
constraints. The corresponding ILP is solved by a
1Note that in theory we should be maximizing p (e|f). How-
ever, this requires summation over all possible alignments and
hence the problem is usually simplified as described here.
2Note that our actual formulation differs slightly from the
original work because we use a first order modelling language
that imposed certain restrictions on the type of constraints al-
lowed.
3It is worth mentioning that Cutting Plane Algorithms have
been successfully applied for solving very large instances of the
Travelling Salesman Problem, a problem essentially equivalent
to the decoding in IBM Model 4.
6
standard ILP solver, and the solution is inspected
for cycles. If it contains no cycles, we have found
the true optimum: the solution with highest score
that does not violate any constraints. If the solution
does contain cycles, the corresponding constraints
are added to the ILP which is in turn solved again.
This process is continued until no more cycles can
be found.
5 Evaluation
In this section we describe our experimental setup
and results.
5.1 Experimental setup
Our experimental setup is designed to answer sev-
eral questions: (1) Is exact inference in IBM Model 4
possible for sentences of moderate length? (2) How
fast is exact inference using Cutting-Plane ILP?
(3) How well does the ReWrite Decoder4 perform
in terms of finding the optimal solution? (4) Does
optimal decoding produce better translations?
In order to answer these questions we obtain
a trained IBM Model 4 for French-English and
German-English on Europarl v3 using GIZA++. A
bigram language model with Witten-Bell smooth-
ing was estimated from the corpus using the CMU-
Cambridge Language Modeling Toolkit.
For exact decoding we use the two models to gen-
erate ILP programs for sentences of length up to
(and including) 30 tokens for French and 25 tokens
for German.5 We filter translation candidates follow-
ing Germann et al (2004) by using only the top ten
translations for each word6 and a list of zero fertil-
ity words.7 This resulted in 1101 French and 1062
German sentences for testing purposes. The ILP pro-
grams were then solved using the method described
in Section 3. This was repeated using the ReWrite
Decoder using the same models.
5.2 Results
The Cutting-Plane ILP decoder (which we will refer
to as ILP decoder) produced output for 986 French
sentences and 954 German sentences. From this we
can conclude that it is possible to solve 90% of our
4Available at http://www.isi.edu/
licensed-sw/rewrite-decoder/
5These limits were imposed to ensure the Python script gen-
erating the ILP programs did not run out of memory.
6Based on t(e|f).
7Extracted using the rules in the filter script
rewrite.mkZeroFert.perl
sentences exactly using ILP. For the remaining 115
and 108 sentences we did not produce a solution due
to: (1) the solver not completing within 30 minutes,
or (2) the solver running out of memory.8
Table 1 shows a comparison of the results, bro-
ken down by input sentence length, obtained on the
986 French and 954 German sentences using the ILP
and ReWrite decoders. First we turn our attention to
the solve times obtained using ILP (for the sentences
for which the solution was found within 30 min-
utes). The table shows that the average solve time
is under one minute per sentence. As we increase
the sentence length we see the solve time increases,
however, we never see an order of magnitude in-
crease between brackets as witnessed by Germann
et al (2004) thus optimal decoding is more practi-
cal than previously suggested. The average number
of Cutting-Plane iterations required was 4.0 and 5.6
iterations for French and German respectively with
longer sentences requiring more on average.
We next examine the performance of the two de-
coders. Following Germann et al (2004) we define
the ReWrite decoder as finding the optimal solution
if the English sentence is the same as that produced
by the ILP decoder. Table 1 shows that the ReWrite
decoder finds the optimal solution 40.1% of the time
for French and 29.1% for German. We also see the
ReWrite decoder is less likely to find the optimal so-
lution of longer sentences. We now look at the model
scores more closely. The average log model error
per token shows that the ReWrite decoder?s error is
proportional to sentence length and on average the
ReWrite decoder is 2.2% away from the optimal so-
lution in log space and 60.6% in probability space9
for French, and 4.7% and 60.9% for German.
Performing exact decoding increases the BLEU
score by 0.97 points on the French-English data set
and 0.61 points on the German-English data set with
similar performance increases observed for all sen-
tence lengths.
6 Discussion and Conclusions
In this paper we have demonstrated that optimal de-
coding of IBM Model 4 is more practical than previ-
ously suggested. Our results and analysis show that
8All experiments were run on 3.0GHz Intel Core 2 Duo with
4GB RAM using a single core.
9These high error rates are an artefact of the extremely small
probabilities involved.
7
Len # Solve Stats BLEU%Eq Err Time ReW ILP Diff
1?5 21 85.7 15.0 0.7 56.5 56.2 -0.32
6?10 121 64.5 7.8 1.4 26.1 28.0 1.90
11?15 118 47.9 5.9 2.7 22.9 23.7 0.85
16?20 238 37.4 6.3 13.9 20.4 20.8 0.41
21?25 266 30.5 6.6 70.1 20.9 22.5 1.62
26?30 152 25.7 5.3 162.6 20.9 22.3 1.38
1?30 986 40.1 6.5 48.1 21.7 22.6 0.97
(a) French-English
Len # Solve Stats BLEU%Eq Err Time ReW ILP Diff
1?5 31 83.9 27.4 0.8 40.7 41.1 0.44
6?10 175 51.4 19.7 1.7 19.2 20.9 1.72
11?15 242 30.6 17.4 5.5 16.0 16.7 0.72
16?20 257 19.1 14.4 23.9 15.8 15.9 0.16
21?25 249 15.7 14.0 173.4 15.3 15.9 0.61
1?25 954 29.1 16.4 53.5 16.1 16.7 0.61
(b) German-English
Table 1: Results on the two corpora. Len: range of sentence lengths; #: number of sentences in this range; %Eq: percentage of
times ILP decoder returned same English sentence; Err: average difference between decoder scores per token (?10?2) in log space;
Time: the average solve time per sentence of ILP decoder in seconds; BLEU ReW, BLEU ILP, BLEU Diff: the BLEU scores of the
output and difference between BLEU scores.
exact decoding has a practical purpose. It has al-
lowed us to investigate and validate the performance
of the ReWrite decoder through comparison of the
outputs and model scores from the two decoders.
Exact inference also provides an improvement in
translation quality as measured by BLEU score.
During the course of this research we have en-
countered numerous challenges that were not appar-
ent at the start. These challenges raise some interest-
ing research questions and practical issues one must
consider when embarking on exact inference using
ILP. The first issue is that the generation of the ILP
programs can take a long time. This leads us to won-
der if there may be a way to provide tighter integra-
tion of program generation and solving. Such an in-
tegration would avoid the need to query the models
in advance for all possible model components the
solver may require.
Related to this issue is how to tackle the incor-
poration of higher order language models. Currently
we use our bigram language model in a brute-force
manner: in order to generate the ILP we evaluate
the probability of all possible bigrams of English
candidate tokens in advance. It seems clear that
with higher order models this process will become
prohibitively expensive. Moreover, even if the ILP
could be generated efficiently, they will obviously be
larger and harder to solve than our current ILPs. One
possible solution may be the use of so-called de-
layed column generation strategies which incremen-
tally add parts of the objective function (and hence
the language model), but only when required by the
ILP solver.10
10Note that delayed column generation is dual to performing
cutting planes.
The use of ILP in other NLP tasks has provided
a principled and declarative manner to incorporate
global linguistic constraints on the system output.
This work lays the foundations for incorporating
similar global constraints for translation. We are cur-
rently investigating linguistic constraints for IBM
Model 4 and other word-based models in general. A
further extension is to reformulate higher-level MT
models (phrase- and syntax-based) within the ILP
framework. These representations could be more de-
sirable from a linguistic constraint perspective as the
formulation of constraints may be more intuitive.
Acknowledgements
We would like to thank Ulrich Germann and Daniel
Marcu for their help with the ISI ReWrite Decoder.
References
Brown, Peter F., Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics of sta-
tistical machine translation: parameter estimation. Compu-
tational Linguistics 19(2):263?311.
Dantzig, George B., Ray Fulkerson, and Selmer M. Johnson.
1954. Solution of a large-scale traveling salesman problem.
Operations Research 2:393?410.
Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2004. Fast and optimal decoding for ma-
chine translation. Artificial Intelligence 154(1-2):127?143.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In ACL 2009 Demos. Prague, Czech Republic, pages
177?180.
Riedel, Sebastian and James Clarke. 2006. Incremental integer
linear programming for non-projective dependency parsing.
In EMNLP 2006. pages 129?137.
8
Proceedings of the Workshop on BioNLP: Shared Task, pages 41?49,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Markov Logic Approach to Bio-Molecular Event Extraction
Sebastian Riedel
??
Hong-Woo Chun
??
Toshihisa Takagi
??
Jun'ichi Tsujii
???
?
Database Center for Life Science, Research Organization of Information and System, Japan
?
Department of Computer Science, University of Tokyo, Japan
?
Department of Computational Biology, University of Tokyo, Japan
?
School of Informatics, University of Manchester, UK
?
National Centre for Text Mining, UK
{sebastian,chun,takagi}@dbcls.rois.ac.jp
tsujii@is.s.u-tokyo.ac.jp
Abstract
In this paper we describe our entry to the
BioNLP 2009 Shared Task regarding bio-
molecular event extraction. Our work can
be described by three design decisions: (1)
instead of building a pipeline using local
classifier technology, we design and learn
a joint probabilistic model over events in
a sentence; (2) instead of developing spe-
cific inference and learning algorithms for
our joint model, we apply Markov Logic, a
general purpose Statistical Relation Learn-
ing language, for this task; (3) we represent
events as relational structures over the to-
kens of a sentence, as opposed to structures
that explicitly mention abstract event en-
tities. Our results are competitive: we
achieve the 4th best scores for task 1 (in
close range to the 3rd place) and the best
results for task 2 with a 13 percent point
margin.
1 Introduction
The continuing rapid development of the Inter-
net makes it very easy to quickly access large
amounts of data online. However, it is impossi-
ble for a single human to read and comprehend a
significant fraction of the available information.
Genomics is not an exception, with databases
such as MEDLINE storing a vast amount of
biomedical knowledge.
A possible way to overcome this is informa-
tion extraction (IE) based on natural language
processing (NLP) techniques. One specific IE
sub-task concerns the extraction of molecular
events that are mentioned in biomedical liter-
ature. In order to drive forward research in this
domain, the BioNLP Shared task 2009 (Kim
et al, 2009) concerned the extraction of such
events from text. In the course of the shared task
the organizers provided a training/development
set of abstracts for biomedical papers, annotated
with the mentioned events. Participants were
required to use this data in order to engineer
a event predictor which was then evaluated on
unseen test data.
The shared task covered three sub-tasks. The
first task concerned the extraction of events
along with their clue words and their main argu-
ments. Figure 1 shows a typical example. The
second task was an extension of the first one,
requiring participants to not only predict the
core arguments of each event, but also the cel-
lular locations the event is associated with in
the text. The events in this task were simi-
lar in nature to those in figure 1, but would
also contain arguments that are neither events
nor proteins but cellular location terms. In con-
trast to the protein terms, cellular location terms
were not given as input and had to be predicted,
too. Finally, for task 3 participants were asked
to extract negations and speculations regarding
events. However, in our work we only tackled
Task 1 and Task 2, and hence we omit further
details on Task 3 for brevity.
Our approach to biomedical event extraction
is inspired by recent work on Semantic Role La-
belling (Meza-Ruiz and Riedel, 2009; Riedel and
Meza-Ruiz, 2008) and can be characterized by
three decisions that we will illustrate in the fol-
lowing. First, we do not build a pipelined sys-
tem that first predicts event clues and cellular
locations, and then relations between these; in-
41
stead, we design and learn a joint discrimina-
tive model of the complete event structure for
a given sentence. This allows us to incorporate
global correlations between decisions in a prin-
cipled fashion. For example, we know that any
event that has arguments which itself are events
(such as the positive regulation event in figure
1) has to be a regulation event. This means that
when we make the decision about the type of
an event (e.g., in the first step of a classifica-
tion pipeline) independently from the decisions
about its arguments and their type, we run the
risk of violating this constraint. However, in a
joint model this can be easily avoided.
Our second design choice is the following: in-
stead of designing and implementing specific in-
ference and training methods for our structured
model, we useMarkov Logic, a Statistical Re-
lational Learning language, and define our global
model declaratively. This simplified the imple-
mentation of our system significantly, and al-
lowed us to construct a very competitive event
extractor in three person-months. For example,
the above observation is captured by the simple
formula:
eventType (e, t) ? role (e, a, r) ? event (a) ?
regType (t) (1)
Finally, we represent event structures as rela-
tional structures over tokens of a sentence,
as opposed to structures that explicitly mention
abstract event entities (compare figure 1 and 2).
The reason is as follows. Markov Logic, for now,
is tailored to link prediction problems where we
may make inferences about the existence of rela-
tions between given entities. However, when the
identity and number of objects of our domain is
unknown, things become more complicated. By
mapping to relational structure over grounded
text, we also show a direct connection to recent
formulations of Semantic Role Labelling which
may be helpful in the future.
The remainder of this paper is organized as
follows: we will first present the preprocessing
steps we perform (section 2), then the conversion
to a link prediction problem (section 3). Subse-
quently, we will describe Markov Logic (section
4) and our Markov Logic Network for event ex-
!"# !"$ !"%
&'()*
+,*-*+,*-*
+,*-*
1 2 3 4 5 6 7 8 9
Figure 1: Example gold annotation for task 1 of the
shared task.
1 2 3 4 5 6 7 8 9
Figure 2: Link Prediction version of the events in
figure 1.
traction (section 5). Finally, we present our re-
sults (in section 6) and conclude (section 7).
2 Preprocessing
The original data format provided by the shared
task organizers consists of (a) a collection
biomedical abstracts, and (b) standoff anno-
tation that describes the proteins, events and
sites mentioned in these abstracts. The organiz-
ers also provided a set of dependency and con-
stituent parses for the abstracts. Note that these
parses are based on a different tokenisation of the
text in the abstracts.
In our first preprocessing step we convert the
standoff annotation in the original data to stand-
off annotation for the tokenisation used in the
parses. This allows us to formulate our proba-
bilistic model in terms of one consistent tokeni-
sation (and be able to speak of token instead of
character offsets). Then we we retokenise the
input text (for the parses) according the protein
boundaries that were given in the shared task
data (in order to split strings such as p50/p55).
Finally, we use this tokenisation to once again
adapt the stand-off annotation (using the previ-
ously adapted version as input).
3 Link Prediction Representation
As we have mentioned earlier, before we learn
and apply our Statistical Relational Model, we
convert the task to link prediction over a se-
quence of tokens. In the following we will present
this transformation in detail.
42
To simplify our later presentation we will first
introduce a formal representation of the events,
proteins and locations mentioned in a sentence.
Let us simply identify both proteins and cellular
location entities with their token position in the
sentence. Furthermore, let us describe an event e
as a tuple (i, t, A) where i is the token position of
the clue word of e and t is the event type of e; A
is a set of labelled arguments (a, r) where each a
is either a protein, location or event, and r is the
role a plays with respect to e. We will identify
the set of all proteins, locations and events for a
sentence with P , L and E, respectively.
For example, in figure 1 we have P =
{4, 7} , L = ? and E = {e13, e14, e15} with
e15 = (5, gene_expr, {(4,Theme)})
e14 = (2, pos_reg, {(e15,Theme) , (7,Cause)})
e13 = (1, neg_reg, {(e14,Theme)})
3.1 Events to Links
As we mentioned in section 1, Markov Logic (or
its interpreters) are not yet able to deal with
cases where the number and identity of entities is
unknown, while relations/links between known
objects can be readily modelled. In the follow-
ing we will therefore present a mapping of an
event structure E to a labelled relation over to-
kens. Essentially, we project E to a pair (L,C)
where L is a set of labelled token-to-token links
(i, j, r), and C is a set of labelled event clues
(i, t). Note that this mapping has another ben-
efit: it creates a predicate-argument structure
very similar to most recent formulations of Se-
mantic Role Labelling (Surdeanu et al, 2008).
Hence it may be possible to re-use or adapt the
successful approaches in SRL in order to improve
bio-molecular event extraction. Since our ap-
proach is inspired by the Markov Logic role la-
beller in (Riedel and Meza-Ruiz, 2008), this work
can be seen as an attempt in this direction.
For a sentence with given P , L and E, algo-
rithm 1 presents our mapping from E to (L,C).
For brevity we omit a more detailed description
of the algorithm. Note that for our running ex-
ample eventsToLinks would return
C = {(1, neg_reg) , (2, pos_reg) , (5, gene_expr)}
(2)
Algorithm 1 Event to link conversion
/* returns all clues C and links L given
by the events in E */
1 function eventsToLinks (E):
2 C ? ?, L? ?
3 for each event (i, t, A) ? E do
4 C ? C?{(i, t)}
5 for each argument (a, r) ? A do
6 if a is an event (i?, t?, A?) do
7 L? L?{(i, i?, r)} with a = (i?, t?, A?)
8 else
9 L? L ? {(i, a, r)}
10 return (C,L)
and
L = {(1, 2,Theme) , (2, 5,Theme) ,
(2, 7,Cause) , (5, 4,Theme)} . (3)
3.2 Links to Events
The link-based representation allows us to sim-
plify the design of our Markov Logic Network.
However, after we applied the MLN to our data,
we still need to transform this representation
back to an event structure (in order to use or
evaluate it). This mapping is presented in al-
gorithm 2 and discussed in the following. Note
that we expect the relational structure L to be
cycle free. We again omit a detailed discussion of
this algorithm. However, one thing to notice is
the special treatment we give to binding events.
Roughly speaking, for the binding event clue c
we create an event with all arguments of c in
L. For a non-binding event clue c we first col-
lect all roles for c, and then create one event per
assignment of argument tokens to these roles.
If we would re-convert C and L from equation
2 and 3, respectively, we could return to our orig-
inal event structure in figure 1. However, con-
verting back and forth is not loss-free in general.
For example, if we have a non-binding event in
the original E set with two arguments A and B
with the same role Theme, the round-trip con-
version would generate two events: one with A
as Theme and one with B as Theme.
4 Markov Logic
Markov Logic (Richardson and Domingos, 2006)
is a Statistical Relational Learning language
43
Algorithm 2 link to event conversion. Assume:
no cycles; tokens can only be one of protein, site
or event; binding events have only protein argu-
ments.
/* returns all events E specified
by clues C and links L */
1 function linksToEvents (C,L)
2 return S(i,t)?C resolve (i, C, L)
/* returns all events for
the given token i */
1 function resolve (i, C, L)
2 if no t with (i, t) ? C return {i}
3 t? type (i, C)
4 if t = binding return {(i, t, A)} with
5 A = {(a, r) | (i, a, r) ? L}
6 Ri ? {r?|?a : (i, a, r) ? L}
7 for each role r ? Ri do
8 Ar ? {a| (i, a, r) ? L}
9 Br ?
S
a?Ar {(resolve (a) , r)}
10 return SA?expand(Br1 ,...,Brn ) {(i, t, A)}
/* returns all possible argument
sets for Br1 , . . . , Brn */
1 function expand (Br1 , . . . , Brn )
2 if n = 1 return Brn
3 return
S
a?Br1
S
A?expand(Br2 ,...,Brn ) {(a, r1)} ?A
based on First Order Logic and Markov Net-
works. It can be seen as a formalism that ex-
tends First Order Logic to allow formulae that
can be violated with some penalty. From an al-
ternative point of view, it is an expressive tem-
plate language that uses First Order Logic for-
mulae to instantiate Markov Networks of repet-
itive structure.
Let us introduce Markov Logic by considering
the event extraction task (as relational structure
over tokens as generated by algorithm 1). In
Markov Logic we can model this task by first
introducing a set of logical predicates such as
eventType(Token,Type), role(Token,Token,Role)
and word(Token,Word). Then we specify a set of
weighted first order formulae that define a distri-
bution over sets of ground atoms of these pred-
icates (or so-called possible worlds). Note that
we will refer predicates such as word as observed
because they are known in advance. In contrast,
role is hidden because we need to infer its ground
atoms at test time.
Ideally, the distribution we define with these
weighted formulae assigns high probability to
possible worlds where events are correctly iden-
tified and a low probability to worlds where this
is not the case. For example, in our running ex-
ample a suitable set of weighted formulae would
assign a higher probability to the world
{word (1, prevented) , eventType (1, neg_reg) ,
role(1, 2,Theme), event(2), . . .}
than to the world
{word (1, prevented) , eventType (1, binding) ,
role(1, 2,Theme), event(2), . . .}
In Markov Logic a set of weighted first order for-
mulae is called a Markov Logic Network (MLN).
Formally speaking, an MLN M is a set of pairs
(?,w) where ? is a first order formula and w a
real weigh t. M assigns the probability
p (y) = 1Z exp
?
? ?
(?,w)?M
w
?
c?C?
f?c (y)
?
?
(4)
to the possible world y. Here C? is the set of all
possible bindings of the free variables in ? with
the constants of our domain. f?c is a feature
function that returns 1 if in the possible world y
the ground formula we get by replacing the free
variables in ? by the constants in the binding
c is true and 0 otherwise. Z is a normalisation
constant.
4.1 Inference and Learning
Assuming that we have an MLN, a set of weights
and a given sentence, we need to predict the
choice of event clues and roles with maximal
a posteriori probability (MAP). To this end
we apply a method that is both exact and ef-
ficient: Cutting Plane Inference Riedel (2008,
CPI) with Integer Linear Programming (ILP) as
base solver.
In order to learn the weights of the MLN
we use the 1-best MIRA Crammer and Singer
(2003) Online Learning method. As MAP infer-
ence method that is applied in the inner loop of
the online learner we apply CPI, again with ILP
as base solver. The loss function for MIRA is a
44
weighted sum FP +?FN where FP is the num-
ber of false positives, FN the number of false
negatives and ? = 0.01.
5 Markov Logic Network for Event
Extraction
We define four hidden predicates our task:
event(i) indicates that there is an event with
clue word i; eventType(i,t) denotes that at token
i there is an event with type t; site(i) denotes a
cellular location mentioned at token i; role(i,j,r)
indicates that token i has the argument j with
role r. In other words, the four hidden predicates
represent the set of sites L (via site), the set of
event clues C (via event and eventType) and the
set of links L (via role) presented in section 3.
There are numerous observed predicates we
use. Firstly, the provided information about
protein mentions is captured by the predicate
protein(i), indicating there is a protein mention
ending at token i. We also describe event types
and roles in more detail: regType( t) holds for
an event type t iff it is a regulation event type;
task1Role(r) and task2Role(r) hold for a role r
if is a role of task 1 (Theme, Cause) or task 2
(Site, CSite, etc.).
Furthermore, we use predicates that de-
scribe properties of tokens (such as the word
or stem of a token) and token pairs (such
as the dependency between two tokens); this
set is presented in table 1. Here the path
and pathNL predicates may need some fur-
ther explanation. When path(i,j,p,parser) is
true, there must be a labelled dependency
path p between i and j according to the
parser parser. For example, in figure 1 we
will observe path(1,5,dobj?prep_of?,mcclosky-
charniak). pathNL just omits the depen-
dency labels, leading to path(1,5,??,mcclosky-
charniak) for the same example.
We use two parses per sentence: the outputs
of a self-trained reranking parser Charniak and
Johnson (2005); McClosky and Charniak (2008)
and a CCG parser (Clark and Curran, 2007),
provided as part of the shared task dataset. As
dictionaries we use a collection of cellular lo-
cation terms taken from the Genia event cor-
pus (Kim et al, 2008), a small handpicked set of
event triggers and a list of English stop words.
Predicate Description
word(i,w) Token i has word w.
stem(i,s) i has (Porter) stem s.
pos(i,p) i has POS tag p.
hyphen(i,w) i has word w after last hyphen.
hyphenStem(i,s) i has stem s after last hyphen.
dict(i,d) i appears in dictionary d.
genia(i,p) i is event clue in the Genia
corpus with precision p.
dep(i,j,d,parser) i is head of token j with
dependency d according to
parser parser.
path(i,j,p,parser) Labelled Dependency path
according to parser parser
between tokens i and j is p.
pathNL(i,j,p,parser) Unlabelled dependency path
according to parser p between
tokens i and j is path.
Table 1: Observable predicates for token and token
pair properties.
5.1 Local Formulae
A formula is local if its groundings relate any
number of observed ground atoms to exactly one
hidden ground atom. For example, the ground-
ing
dep (1, 2, dobj, ccg) ? word (1, prevented) ?
eventType (2, pos_reg) (5)
of the local formula
dep(h, i, d, parser) ? word (h,+w) ?
eventType(i,+t) (6)
connects a single hidden eventType ground atom
with an observed word and dep atom. Note that
the + prefix for variables indicates that there is
a different weight for each possible pair of word
and event type (w, t).
5.1.1 Local Entity Formulae
The local formulae for the hidden event/1
predicate can be summarized as follows. First,
we add a event (i) formula that postulates the
existence of an event for each token. The weight
of this formulae serves as a general bias for or
against the existence of events.
45
Next, we add one formula
T (i,+t) ? event (i) (7)
for each simple token property predicate T in
table 1 (those in the first section of the table).
For example, when we plug in word for T we get
a formula that encourages or discourages the ex-
istence of an event token based on the word form
of the current token: word (i,+t) ? event (i).
We also add the formula
genia (i, p) ? event (i) (8)
and multiply the feature-weight product for each
of its groundings with the precision p. This is
corresponds to so-called real-valued feature func-
tions, and allows us to incorporate probabili-
ties and other numeric quantities in a principled
fashion.
Finally, we add a version of formula 6 where
we replace eventType(i,t) with event(i).
For the cellular location site predicate we
use exactly the same set of formulae but re-
place every occurrence of event(i) with site(i).
This demonstrates the ease with which we could
tackle task 2: apart from a small set of global
formulae we introduce later, we did not have to
do more than copy one file (the event model file)
and perform a search-and-replace. Likewise, in
the case of the eventType predicate we simply
replace event(i) with eventType(i,+t).
5.1.2 Local Link Formulae
The local formulae for the role/3 predicate
are different in nature because they assess two
tokens and their relation. However, the first for-
mula does look familiar: role (i, j,+r). This for-
mula captures a (role-dependent) bias for the ex-
istence of a role between any two tokens.
The next formula we add is
dict (i,+di) ? dict (j,+dj) ? role (i, j,+r) (9)
and assesses each combination of dictionaries
that the event and argument token are part of.
Furthermore, we add the formula
path (i, j,+p,+parser) ? role (i, j,+r) (10)
that relates the dependency path between two
tokens i and j with the role that j plays with
respect to i. We also add an unlabelled version
of this formula (using pathNL instead of path).
Finally, we add a formula
P (i, j,+p,+parser) ? T (i,+t) ?
role (i, j,+r) (11)
for each P in {path,pathNL} and T in
{word,stem,pos,dict,protein}. Note that for
T=protein we replace T (i,+t) with T (i).
5.2 Global Formulae
Global formulae relate two or more hidden
ground atoms. For example, the formula in
equation 1 is global. While local formulae can be
used in any conventional classifier (in the form
of feature functions conditioned only on the in-
put data) this does not hold for global ones.
We could enforce global constraints such as the
formula in equation 1 by building up structure
incrementally (e.g. start with one classifier for
events and sites, and then predict roles between
events and arguments with another). However,
this does not solve the typical chicken-and-egg
problem: evidence for possible arguments could
help us to predict the existence of event clues,
and evidence for events help us to predict argu-
ments. By contrast, global formulae can capture
this type of correlation very naturally.
Table 2 shows the global formulae we use. We
divide them into three parts. The first set of for-
mulae (CORE) ensures that event and eventType
atoms are consistent. In all our experiments we
will always include all CORE formulae; without
them we might return meaningless solutions that
have events with no event types, or types with-
out events.
The second set of formulae (VALID) consist
of CORE and formulae that ensure that the link
structure represents a valid set of events. For
example, this includes formula 12 that enforces
each event to have at least one theme.
Finally, FULL includes VALID and two con-
straints that are not strictly necessary to enforce
valid event structures. However, they do help us
to improve performance. Formula 14 forbids a
token to be argument of more than one event. In
fact, this formula does not hold all the time, but
46
# Formula Description
1 event (i)? ?t.eventType (i, t) If there is an event there should be an event type.
2 eventType (i, t)? event (i) If there is an event type there should be an event.
3 eventType (i, t) ? t 6= o? ?eventType (i, o) There cannot be more than one event type per token.
4 ?site (i) ? ?event (i) A token cannot be both be event and site.
5 role (i, j, r)? event (i) If j plays the role r for i then i has to be an event.
6 role (i, j, r1) ? r1 6= r2 ? ?role (i, j, r2) There cannot be more than one role per argument.
7 eventType (e, t) ? role (e, a, r) ? event (a)? regType (t) Only reg. type events can have event arguments.
9 role (i, j, r) ? taskOne (r)? event (j) ? protein (j) For task 1 roles arguments must be proteins or events
10 role (i, j, r) ? taskTwo (r)? site (j) Task 2 arguments must be cellular locations (site).
11 site (j)? ?i, r.role (i, j, r) ? taskTwo (r) Sites are always associated with an event.
12 event (i)? ?j.role (i, j,Theme) Every events need a theme.
13 eventType (i, t) ? ?allowed (t, r)? ?role (i, j, r) Certain events may not have certain roles.
14 role (i, j, r1) ? k 6= i? ?role (k, j, r2) A token cannot be argument of more than one event.
15 j < k ? i < j ? role (i, j, r1)? ?role (i, k, r2) No inside outside chains.
Table 2: All three sets of global formulae used: CORE (1-3), VALID (1-13), FULL (1-15).
by adding it we could improve performance. For-
mula 15 is our answer to a type of event chain
that earlier models would tend to produce.
Note that all formulae but formula 15 are de-
terministic. This amounts to giving them a very
high/infinite weight in advance (and not learn-
ing it during training).
6 Results
In table 3 we can see our results for task 1 and
2 of the shared task. The measures we present
here correspond to the approximate span, ap-
proximate recursive match criterion that counts
an event as correctly predicted if all arguments
are extracted and the event clue tokens approx-
imately match the gold clue tokens. For more
details on this metric we refer the reader to the
shared task overview paper.
To put our results into context: for task 1 we
reached the 4th place among 20 participants, are
in close range to place 2 and 3, and significantly
outperform the 5th best entry. Moreover, we
had highest scoring scores for task 2 with a 13%
margin to the runner-up. Using both training
and development set for training (as allowed by
the task organisers), our task 1 score rises to
45.1, slightly higher than the score of the current
third.
In terms of accuracy across different event
types our model performs worse for binding, reg-
ulation type and transcription events. Binding
events are inherently harder to correctly extract
because they often have multiple core arguments
while other non-regulation events have only one;
just missing one of the binding arguments will
lead to an event that is considered as error with
no partial credit given. If we would give credit
for binding with partially correct arguments our
F-score for binding events would rise to 49.8.
One reason why regulation events are difficult
to extract is the fact that they often have argu-
ments which themselves are events, too. In this
case our recall is bound by the recall for argu-
ment events because we can never find a regu-
lation event if we cannot predict the argument
event. Note that we are still unsure about tran-
scription events, in particular because we ob-
serve 49% F-score for such events in the devel-
opment set.
How does our model benefit from the global
formulae we describe in section 5 (and which
represent one of the core benefits of a Markov
Logic approach)? To evaluate this we compare
our FULL model with CORE and VALID from
table 2. Note that because the evaluation inter-
face rejects invalid event structures, we cannot
use the evaluation metrics of the shared task.
Instead we use table 4 to present an evaluation
in terms of ground atom F1-score for the hidden
predicates of our model. This amounts to a per-
47
Task 1 Task 2
R P F R P F
Loc 37.9 88.0 53.0 32.8 76.0 45.8
Bind 23.1 48.2 31.2 22.4 47.0 30.3
Expr 63.0 75.1 68.5 63.0 75.1 68.5
Trans 16.8 29.9 21.5 16.8 29.9 21.5
Cata 64.3 81.8 72.0 64.3 81.8 72.0
Phos 78.5 77.4 77.9 69.1 70.1 69.6
Total 48.3 68.9 56.8 46.8 67.0 55.1
Reg 23.7 40.8 30.0 22.3 38.5 28.2
Pos 26.8 42.8 32.9 26.7 42.3 32.7
Neg 27.2 40.2 32.4 26.1 38.6 31.2
Total 26.3 41.8 32.3 25.8 40.8 31.6
Total 36.9 55.6 44.4 35.9 54.1 43.1
Table 3: (R)ecall, (P)recision, and (F)-Score for task
1 and 2 in terms of event types.
role, per-site and per-event-clue evaluation. The
numbers here will not directly correspond to ac-
tual scores, but generally we can assume that if
we do better in our metrics, we will likely have
better scores.
In table 4 we notice that ensuring consistency
between all predicates has a significant impact
on the performance across the board (see the
VALID results). Furthermore, when adding ex-
tra formulae that are not strictly necessary for
consistency, but which encourage more likely
event structure, we again see significant improve-
ments (see FULL results). Interestingly, al-
though the extra formulae only directly consider
role atoms, they also have a significant impact
on event and particularly site extraction perfor-
mance. This reflects how in a joint model deci-
sions which would appear in the end of a tradi-
tional pipeline (e.g., extracting roles for events)
can help steps that would appear in the begin-
ning (extracting events and sites).
For the about 7500 sentences in the training
set we need about 3 hours on a MacBook Pro
with 2.8Ghz and 4Gb RAM to learn the weights
of our MLN. This allowed us to try different sets
of formulae in relatively short time.
7 Conclusion
Our approach the BioNLP Shared Task 2009 can
be characterized by three decisions: (a) jointly
CORE VALID FULL
eventType 52.8 63.2 64.3
role 44.0 53.5 55.7
site 42.0 46.0 51.5
Total 50.7 60.1 61.9
Table 4: Ground atom F-scores for global formulae.
modelling the complete event structure for a
given sentence; (b) using Markov Logic as gen-
eral purpose-framework in order to implement
our joint model; (c) framing the problem as a
link prediction problem between tokens of a sen-
tence.
Our results are competitive: we reach the 4th
place in task 1 and the 1st place for task 2 (with
a 13% margin). Furthermore, the declarative na-
ture of Markov Logic helped us to achieve these
results with a moderate amount of engineering.
In particular, we were able to tackle task 2 by
copying the local formulae for event prediction,
and adding three global formulae (4, 10 and 11
in table 2). Finally, our system was fast to train
(3 hours) . This greatly simplified the search for
good sets of formulae.
We have also shown that global formulae sig-
nificantly improve performance in terms of event
clue, site and argument prediction. While a sim-
ilar effect may be possible with reranking archi-
tectures, we believe that in terms of implemen-
tation efforts our approach is at least as simple.
In fact, our main effort lied in the conversion to
link prediction, not in learning or inference. In
future work we will therefore investigate means
to extend Markov Logic (interpreter) in order to
directly model event structure.
Acknowledgements
We thank Dr. Chisato Yamasaki and Dr.
Tadashi Imanishi, BIRC, AIST, for their help.
This work is supported by the Integrated
Database Project (MEXT, Japan), the Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan) and the Genome Network Project
(MEXT, Japan).
48
References
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent dis-
criminative reranking. In Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics (ACL' 05). pages
173180.
Clark, Stephen and James R. Curran. 2007.
Wide-coverage efficient statistical parsing
with ccg and log-linear models. Comput. Lin-
guist. 33(4):493552.
Crammer, Koby and Yoram Singer. 2003. Ultra-
conservative online algorithms for multiclass
problems. Journal of Machine Learning Re-
search 3:951991.
Kim, Jin D., Tomoko Ohta, and Jun'ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC Bioinformat-
ics 9(1).
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo,
Yoshinobu Kano, and Jun'ichi Tsujii. 2009.
Overview of bionlp'09 shared task on event ex-
traction. In Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL
2009 Workshop. To appear.
McClosky, David and Eugene Charniak. 2008.
Self-training for biomedical parsing. In
Proceedings of the 46rd Annual Meeting of
the Association for Computational Linguistics
(ACL' 08).
Meza-Ruiz, Ivan and Sebastian Riedel. 2009.
Jointly identifying predicates, arguments and
senses using markov logic. In Joint Human
Language Technology Conference/Annual
Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL '09).
Richardson, Matt and Pedro Domingos. 2006.
Markov logic networks. Machine Learning
62:107136.
Riedel, Sebastian. 2008. Improving the accuracy
and efficiency of map inference for markov
logic. In Proceedings of the 24th Annual Con-
ference on Uncertainty in AI (UAI '08).
Riedel, Sebastian and Ivan Meza-Ruiz. 2008.
Collective semantic role labelling with markov
logic. In Proceedings of the 12th Conference
on Computational Natural Language Learning
(CoNLL' 08). pages 193197.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Llu?s M?rquez, and Joakim Nivre.
2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependen-
cies. In Proceedings of the 12th Conference
on Computational Natural Language Learning
(CoNLL-2008).
49
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 155?163,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Jointly Identifying Predicates, Arguments and Senses using Markov Logic
Ivan Meza-Ruiz? Sebastian Riedel??
?School of Informatics, University of Edinburgh, UK
?Department of Computer Science, University of Tokyo, Japan
?Database Center for Life Science, Research Organization of Information and System, Japan
?I.V.Meza-Ruiz@sms.ed.ac.uk ? sebastian.riedel@gmail.com
Abstract
In this paper we present a Markov Logic Net-
work for Semantic Role Labelling that jointly
performs predicate identification, frame dis-
ambiguation, argument identification and ar-
gument classification for all predicates in a
sentence. Empirically we find that our ap-
proach is competitive: our best model would
appear on par with the best entry in the
CoNLL 2008 shared task open track, and at
the 4th place of the closed track?right be-
hind the systems that use significantly better
parsers to generate their input features. More-
over, we observe that by fully capturing the
complete SRL pipeline in a single probabilis-
tic model we can achieve significant improve-
ments over more isolated systems, in particu-
lar for out-of-domain data. Finally, we show
that despite the joint approach, our system is
still efficient.
1 Introduction
Semantic Role Labelling (SRL, Ma?rquez et al,
2008) is generally understood as the task of iden-
tifying and classifying the semantic arguments and
modifiers of the predicates mentioned in a sentence.
For example, in the case of the following sentence:
we are to find out that for the predicate token ?plays?
with sense ?play a role? (play.02) the phrase headed
by the token ?Haag? is referring to the player (A0)
of the play event, and the phrase headed by the token
?Elianti? is referring to the role (A1) being played.
SRL is considered as a key task for applications that
require to answer ?Who?, ?What?, ?Where?, etc.
questions, such as Information Extraction, Question
Answering and Summarization.
Any real-world SRL system needs to make sev-
eral decisions, either explicitly or implicitly: which
are the predicate tokens of a sentence (predicate
identification), which are the tokens that have se-
mantic roles with respect to these predicates (argu-
ment identification), which are the roles these to-
kens play (argument classification), and which is the
sense of the predicate (sense disambiguation).
In this paper we use Markov Logic (ML), a Statis-
tical Relational Learning framework that combines
First Order Logic and Markov Networks, to develop
a joint probabilistic model over all decisions men-
tioned above. The following paragraphs will moti-
vate this choice.
First, it allows us to readily capture global cor-
relations between decisions, such as the constraint
that a predicate can only have one agent. This type
of correlations has been successfully exploited in
several previous SRL approaches (Toutanova et al,
2005; Punyakanok et al, 2005).
Second, we can use the joint model to evaluate
the benefit of incorporating decisions into the joint
model that either have not received much attention
within the SRL community (predicate identification
and sense disambiguation), or been largely made in
isolation (argument identification and classification
for all predicates of a sentence).
Third, our ML model is essentially a template that
describes a class of Markov Networks. Algorithms
can perform inference in terms of this template with-
155
out ever having to fully instantiate the complete
Markov Network (Riedel, 2008; Singla and Domin-
gos, 2008). This can dramatically improve the effi-
ciency of an SRL system when compared to a propo-
sitional approach such as Integer Linear Program-
ming (ILP).
Finally, when it comes to actually building an
SRL system with ML there are ?only? four things
to do: preparing input data files, converting out-
put data files, and triggering learning and inference.
The remaining work can be done by an off-the-
shelf Markov Logic interpreter. This is to be con-
trasted with pipeline systems where several compo-
nents need to be trained and connected, or Integer
Linear Programming approaches for which we need
to write additional wrapper code to generate ILPs.
Empirically we find that our system is
competitive?our best model would appear on
par with the best entry in the CoNLL 2008 shared
task open track, and at the 4th place of the closed
track?right behind systems that use significantly
better parsers1 to generate their input features.
We also observe that by integrating frame disam-
biguation into the joint SRL model, and by extract-
ing all arguments for all predicates in a sentence
simultaneously, significant improvements compared
to more isolated systems can be achieved. These
improvements are particularly large in the case of
out-of-domain data, suggesting that a joint approach
helps to increase the robustness of SRL. Finally, we
show that despite the joint approach, our system is
still efficient.
Our paper is organised as follows: we first intro-
duce ML (section 2), then we present our model in
terms of ML (section 3) and illustrate how to per-
form learning and inference with it (section 4). How
this model will be evaluated is explained in section 5
with the corresponding evaluation presented in sec-
tion 6. We conclude in section 7.
2 Markov Logic
Markov Logic (ML, Richardson and Domingos,
2005) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First Or-
der Logic to allow formulae that can be violated with
1Our unlabelled accuracy for syntactic dependencies is at
least 3% points under theirs.
some penalty. From an alternative point of view, it is
an expressive template language that uses First Or-
der Logic formulae to instantiate Markov Networks
of repetitive structure.
Let us describe ML by considering the predicate
identification task. In ML we can model this task by
first introducing a set of logical predicates2 such as
isPredicate(Token) or word(Token,Word). Then we
specify a set of weighted first order formulae that
define a distribution over sets of ground atoms of
these predicates (or so-called possible worlds).
Ideally, the distribution we define with these
weighted formulae assigns high probability to possi-
ble worlds where SRL predicates are correctly iden-
tified and a low probability to worlds where this is
not the case. For example, a suitable set of weighted
formulae would assign a high probability to the
world3
{word (1,Haag) , word(2, plays),
word(3,Elianti), isPredicate(2)}
and a low one to
{word (1,Haag) , word(2, plays),
word(3,Elianti), isPredicate(3)}
In Markov Logic a set of weighted formulae is called
a Markov Logic Network (MLN). Formally speak-
ing, an MLN M is a set of pairs (?,w) where ? is a
first order formula and w a real weight. M assigns
the probability
p (y) = 1Z exp
?
? ?
(?,w)?M
w
?
c?C?
f?c (y)
?
? (1)
to the possible world y. Here C? is the set of all
possible bindings of the free variables in ? with the
constants of our domain. f?c is a feature function
that returns 1 if in the possible world y the ground
formula we get by replacing the free variables in ?
by the constants in c is true and 0 otherwise. Z
is a normalisation constant. Note that this distri-
bution corresponds to a Markov Network (the so-
called Ground Markov Network) where nodes repre-
sent ground atoms and factors represent ground for-
mulae.
2In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
3?Haag plays Elianti? is a segment of a sentence in the train-
ing corpus.
156
For example, if M contains the formula ?
word (x, take) ? isPredicate (x)
then its corresponding log-linear model has, among
others, a feature f?t1 for which x in ? has been re-
placed by the constant t1 and that returns 1 if
word (1, take) ? isPredicate (1)
is true in y and 0 otherwise.
We will refer predicates such as word as observed
because they are known in advance. In contrast, is-
Predicate is hidden because we need to infer it at test
time.
3 Model
Conceptually we divide our SRL system into three
stages: one stage that identifies the predicates of
a sentence, one stage that identifies and classifies
the arguments of these predicates, and a final stage
that predicts the sense of each predicate. We should
stress that this architecture is intended to illustrate
a typical SRL system, and to describe the pipeline-
based approach we will compare our models to.
However, it does not correspond to the way in-
ference is performed in our proposed model?we
jointly infer all decisions described above.
Note that while the proposed division into con-
ceptual stages seems somewhat intuitive, it is by no
means uncontroversial. In fact, for the CoNLL 2008
shared task slightly more than one half of the par-
ticipants performed sense disambiguation before ar-
gument identification and classification; most other
participants framed the problem in the reverse or-
der.4
We define five hidden predicates for the three
stages of the task. Figure 1 illustrates these pred-
icates and the stage they belong to. For predicate
identification, we use the predicate isPredicate. is-
Predicate(p) indicates that the word in the position
p is an SRL predicate. For argument identifica-
tion and classification, we use the predicates isAr-
gument, hasRole and role. The atom isArgument(a)
signals that the word in the position a is a SRL ar-
gument of some (unspecified) SRL predicate while
hasRole(p,a) indicates that the token at position a is
4However, for almost all pipeline based systems, predicate
identification was the first stage of the role labelling process.
isPredicate
sense
isArgument
hasRole
role
PredicateIdentification
ArgumentIdentification &clasification
SenseDisambiguation
Bo
tto
m-
up
Top
-Do
wn
Pip
elin
e d
ire
ctio
n
Figure 1: MLN hidden predicates divided in stages
an argument of the predicate in position p. The pred-
icate role(p,a,r) corresponds to the decision that the
argument at position a has the role r with respect to
the predicate in position p. Finally, for sense disam-
biguation we define the predicate sense(p,e) which
signals that the predicate in position p has the sense
e.
Before we continue to describe the formulae of
our Markov Logic Network we would like to high-
light the introduction of the isArgument predicate
mentioned above. This predicate corresponds to a
decision that is usually made implicitly: a token is
an argument if there exists a predicate for which it
plays a semantic role. Here we model this decision
explicitly, assuming that there exist cases where a
token clearly has to be an argument of some pred-
icate, regardless of which predicate in the sentence
this might be. It is this assumption that requires us to
infer the arguments for all predicates of a sentence
at once?otherwise we cannot make sure that for a
marked argument there exists at least one predicate
for which the argument plays a semantic role.
In addition to the hidden predicates, we define
observable predicates to represent the information
available in the corpus. Table 1 presents these pred-
icates.
3.1 Local formulae
A formula is local if its groundings relate any num-
ber of observed ground atoms to exactly one hidden
ground atom. For example, two groundings of the
local formula
lemma(p,+l1)?lemma(a,+l2) ? hasRole(p, a)
can be seen in the Factor Graph of Figure 2. Both
connect a single hidden hasRole ground atom with
157
word(i,w) Token i has word w
lemma(i,l) Token i has lemma l
ppos(i,p) Token i has POS tag p
cpos(i,p) Token i has coarse POS tag p
voice(i,v) Token i is verb and has voice v
(Active/Passive).
subcat(i,f) Token i has subcategorization
frame f
dep(i,j,d) Token h is head of token m and
has dependency label d
palmer(i,j) Token j can be semantic argu-
ment for token i according to
high recall heuristic?
depPath(i,j,p) Dependency path between to-
kens i and j is p?
depFrame(i,j,f) f is a syntactic (dependency)
frame in which tokens i and j
are designated as ?pivots??
Table 1: Observable predicates; predicates marked with
? are dependency parsing-based versions for features of
Xue and Palmer (2004).
two observed lemma ground atoms. The + notation
indicates that the MLN contains one instance of the
rule, with a separate weight, for each assignment of
the variables with a plus sign (?).
The local formulae for isPredicate, isArgument
and sense aim to capture the relation of the tokens
with their lexical and syntactic surroundings. This
includes formulae such as
subcat(p,+f) ? isPredicate(p)
which implies that a certain token is a predicate
with a weight that depends on the subcategorization
frame of the token. Further local formulae are con-
structed using those observed predicates in table 1
that relate single tokens and their properties.
The local formulae for role and hasRole focus on
properties of the predicate and argument token?the
formula illustrated in figure 2 is an example of this?
and on the relation between the two tokens. An ex-
ample of the latter type is the formula
depPath(p, a,+d) ? role(p, a,+r)
which implies that token a plays the semantic role r
with respect to token p, and for which the weight de-
pends on the syntactic (dependency) path d between
p and a and on the actual role to assign. Again,
further formulae are constructed using the observed
Figure 2: Factor graph for the first local formula in sec-
tion 3.1. Here round nodes represent variables (corre-
sponding to the states of ground atoms) and the rectan-
gular nodes represent the factor and their parameters at-
tached to the ground formulae.
predicates in table 1; however, this time we consider
both predicates that relate tokens to their individual
properties and predicates that describe the relation
between tokens.
Unfortunately, the complete set of local formulae
is too large to be exhaustively described in this pa-
per. Its size results from the fact that we also con-
sider conjunctions of several atoms as conditions,
and lexical windows around tokens. Hence, instead
of describing all local formulae we refer the reader
to our MLN model files.5 They can be used both as
a reference and as input to our Markov Logic En-
gine,6 and thus allow the reader to easily reproduce
our results.
3.2 Global formulae
Global formulae relate several hidden ground atoms.
We use this type of formula for two purposes: to
ensure consistency between the predicates of all
SRL stages, and to capture some of our background
knowledge about SRL. We will refer to formulae
that serve the first purpose as structural constraints.
For example, a structural constraint is given by the
(deterministic) formula
role(p, a, r) ? hasRole(p, a)
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a). Note that this formula by itself models
the traditional ?bottom-up? argument identification
and classification pipeline (Xue and Palmer, 2004):
5http://code.google.com/p/thebeast/
source/browse/#svn/mlns/naacl-hlt
6http://code.google.com/p/thebeast
158
it is possible to not assign a role r to an predicate-
argument pair (p, a) proposed by the identification
stage; however, it is impossible to assign a role r
to token pairs (p, a) that have not been proposed as
potential arguments.
An example of another class of structural con-
straints is
hasRole(p, a) ? ?r.role(p, a, r)
which, by itself, models an inverted or ?top-down?
pipeline. In this architecture the argument classifi-
cation stage can assign roles to tokens that have not
been proposed by the argument identification stage.
However, it must assign a label to any token pair the
previous stage proposes.
For the SRL predicates that perform a labelling
task (role and sense) we also need a structural con-
straint which ensures that not more than one label is
assigned. For instance,
(role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2))
forbids two different semantic roles for a pair of
words.
There are three global formulae that capture our
linguistic background knowledge. The first one is
a deterministic constraint that had been frequently
applied in the SRL literature. It forbids cases where
distinct arguments of a predicate have the same role
unless the role describes a modifier:
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ?
?role (p, a2, r)
The second ?linguistic? global formula is
role(p, a,+r) ? lemma(p,+l) ? sense(p,+s)
which implies that when a predicate p with lemma l
has an argument awith role r it has to have the sense
s. Here the weight depends on the combination of
role r, lemma l and sense s.
The third and final ?linguistic? global formula is
lemma(p,+l) ? ppos(a,+p)
?hasRole(p, a) ? sense(p,+f)
It implies that if a predicate p has the lemma l and an
argument a with POS tag p it has to have the sense
s. This time the weight depends on the combination
of POS tag p, lemma l and sense s.
Note that the final two formulae evaluate the se-
mantic frame of a predicate and become local for-
mulae in a pipeline system that performs sense dis-
ambiguation after argument identification and clas-
sification.
Table 2 summarises the global formulae we use in
this work.
4 Inference and Learning
Assuming that we have an MLN, a set of weights
and a given sentence then we need to predict the
choice of predicates, frame types, arguments and
role labels with maximal a posteriori probabil-
ity (MAP). To this end we apply a method that
is both exact and efficient: Cutting Plane Infer-
ence (CPI, Riedel, 2008) with Integer Linear Pro-
gramming (ILP) as base solver.
Instead of fully instantiating the Markov Network
that a Markov Logic Network describes, CPI begins
with a subset of factors/edges?in our case we use
the factors that correspond to the local formulae of
our model?and solves the MAP problem for this
subset using the base solver. It then inspects the
solution for ground formulae/features that are not
yet included but could, if added, lead to a different
solution?this process is usually referred to as sep-
aration. The ground formulae that we have found
are added and the network is solved again. This pro-
cess is repeated until the network does not change
anymore.
This type of algorithm could also be realised for
an ILP formulation of SRL. However, it would re-
quire us to write a dedicated separation routine for
each type of constraint we want to add. In Markov
Logic, on the other hand, separation can be gener-
ically implemented as the search for variable bind-
ings that render a weighted first order formulae true
(if its weight is negative) or false (if its weight is
positive). In practise this means that we can try new
global formulae/constraints without any additional
implementation overhead.
We learn the weights associated with each MLN
using 1-best MIRA (Crammer and Singer, 2003)
Online Learning method. As MAP inference
method that is applied in the inner loop of the on-
line learner we apply CPI, again with ILP as base
159
Bottom-up
sense(p, s) ? isPredicate(p)
hasRole(p, a) ? isPredicate(p)
hasRole(p, a) ? isArgument(a)
role(p, a, r) ? hasLabel(p, a)
Top-Down
isPredicate(p) ? ?s.sense(p, s)
isPredicate(p) ? ?a.hasRole(p, a)
isArgument(a) ? ?p.hasRole(p, a)
hasLabel(p, a) ? ?r.role(p, a, r)
Unique Labels role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)sense(p, s1) ? s1 6= s2 ? ?sense(p, r2)
Linguistic
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ? ?role (p, a2, r)
lemma(p,+l) ? ppos(a,+p) ? hasRole(p, a) ? sense(p,+f)
lemma(p,+l) ? role(p, a,+r) ? sense(p,+f)
Table 2: Global formulae for ML model
solver.
5 Experimental Setup
For training and testing our SRL systems we used a
version of the CoNLL 2008 shared task (Surdeanu
et al, 2008) dataset that only mentions verbal predi-
cates, disregarding the nominal predicates available
in the original corpus.7 While the original (open
track) corpus came with MALT (Nivre et al, 2007)
dependencies, we observed slightly better results
when using the dependency parses generated with
a Charniak parser (Charniak, 2000). Hence we used
the latter for all our experiments.
To assess the performance of our model, and it to
evaluate the possible gains to be made from consid-
ering a joint model of the complete SRL pipeline,
we set up several systems. The full system uses a
Markov Logic Network with all local and global for-
mulae described in section 3. For the bottom-up sys-
tem we removed the structural top-down constraints
from the complete model?previous work Riedel
and Meza-Ruiz (2008) has shown that this can lead
to improved performance. The bottom-up (-arg) sys-
tem is equivalent to the bottom-up system, but it
does not include any formulae that mention the hid-
den isArgument predicate.
For the systems presented so far we perform joint
inference and learning. The pipeline system dif-
fers in this regard. For this system we train a sep-
arate model for each stage in the pipeline of figure
1. The predicate identification stage identifies the
predicates (using all local isPredicate formulae) of
7The reason for this choice where license problems.
a sentence. The next stage predicts arguments and
their roles for the identified predicates. Here we in-
clude all local and global formulae that involve only
the predicates of this stage. In the last stage we pre-
dict the sense of each identified predicate using all
formulae that involve the sense, without the struc-
tural constraints that connect the sense predicate to
the previous stages of the pipeline (these constraints
are enforced by architecture).
6 Results
Table 3 shows the results of our systems for the
CoNLL 2008 development set and the WSJ and
brown test sets. The scores are calculated using the
semantic evaluation metric of the CoNLL-08 shared
task (Surdeanu et al, 2008). This metric measures
the precision, recall and F1 score of the recovered
semantic dependencies. A semantic dependency is
created for each predicate and its arguments, the
label of such dependency is the role of the argu-
ment. Additionally, there is a semantic dependency
for each predicate and aROOT argument which has
the sense of the predicate as label.
To put these results into context, let us compare
them to those of the participants of the CoNLL 2008
shared task (see the last three rows of table 3).8 Our
best model, Bottom-up, would reach the highest F1
WSJ score, and second highest Brown score, for
the open track. Here the best-performing participant
was Vickrey and Koller (2008).
Table 3 also shows the results of the best (Jo-
hansson and Nugues, 2008) and fourth best sys-
8Results of other systems were extracted from Table 16 of
the shared task overview paper (Surdeanu et al, 2008).
160
tem (Zhao and Kit, 2008) of the closed track. We
note that we do significantly worse than Johansson
and Nugues (2008), and roughly equivalent to Zhao
and Kit (2008); this places us on the fourth rank of
19 participants. However, note that all three sys-
tems above us, as well as Zhao and Kit (2008), use
parsers with at least about 90% (unlabelled) accu-
racy on the WSJ test set (Johansson?s parser has
about 92% unlabelled accuracy).9 By contrast, with
about 87% unlabelled accuracy our parses are sig-
nificantly worse.
Finally, akin to Riedel and Meza-Ruiz (2008) we
observe that the bottom-up joint model performs
better than the full joint model.
System Devel WSJ Brown
Full 76.93 79.09 67.64
Bottom-up 77.96 80.16 68.02
Bottom-up (-arg) 77.57 79.37 66.70
Pipeline 75.69 78.19 64.66
Vickrey N/A 79.75 69.57
Johansson N/A 86.37 71.87
Zhao N/A 79.40 66.38
Table 3: Semantic F1 scores for our systems and three
CoNLL 2008 shared task participants. The Bottom-up
results are statistically significantly different to all others
(i.e., ? ? 0.05 according to the sign test).
6.1 Joint Model vs. Pipeline
Table 3 suggests that by including sense disam-
biguation into the joint model (as is the case for all
systems but the pipeline) significant improvements
can be gained. Where do these improvements come
from? We tried to answer this question by taking a
closer look at how accurately the pipeline predicts
the isPredicate, isArgument, hasRole, role and
sense relations, and how this compares to the result
of the joint full model.
Table 4 shows that the joint model mainly does
better when it comes to predicting the right predi-
cate senses. This is particularly true for the case of
the Brown corpus?here we gain about 10% points.
These results suggest that a more joint approach may
be particularly useful in order to increase the robust-
ness of an SRL system in out-of-domain scenarios.10
9Since our parses use a different label set we could not com-
WSJ Brown
Pipe. Fu. Pipe. Fu.
isPredicate 96.6 96.5 92.2 92.5
isArgument 90.3 90.6 85.9 86.9
hasRole 88.0 87.9 83.6 83.8
role 75.4 75.5 64.2 64.6
sense 85.5 88.5 67.3 77.1
Table 4: F1 scores for M predicates; Pipe. refers to the
Pipeline system, Fu. to the full system.
6.2 Modelling if a Token is an Argument
In table 3 we also observe that improvements can be
made if we explicitly model the decision whether a
token is a semantic argument of some predicate or
not. As we mentioned in section 3, this aspect of our
model requires us to jointly perform inference for
all predicates of a sentence, and hence our results
justify the per-sentence SRL approach proposed in
this paper.
In order to analyse where these improvements
come from, we again list our results on a per-SRL-
predicate basis. Table 5 shows that by including the
isArgument predicate and the corresponding for-
mulae we gain around 0.6% and 1.0% points across
the board for WSJ and Brown, respectively.11 As
shown in table 3, these improvements result in about
1.0% improvements for both WSJ and Brown in
terms of the CoNLL 2008 metric. Hence, an ex-
plicit model of the ?is an argument? decision helps
the SRL at all levels.
How the isArgument helps to improve the over-
all role labelling score can be illustrated with the
example in figure 3. Here the model without a
hidden isArgument predicate fails to attach the
preposition ?on? to the predicate ?start.01? (here 01
refers to the sense of the predicate). Apparently
the model has not enough confidence to assign the
preposition to either ?start.01? or ?get.03?, so it just
drops the argument altogether. However, because
the isArgument model knows that most preposi-
tions have to be modifying some predicate, pres-
pare labelled accuracy.
10The differences between results of the full and joint model
are statistically significant with the exception of the results for
the isPredicate predicate for the WSJ test set.
11The differences between results of the w/ and w/o model
are statistically significant with the exception of the results for
the sense predicate for the Brown test set.
161
Figure 3: Segment of the CoNLL 2008 development set
for which the bottom-up model w/o isArgument predi-
cate fails to attach the preposition ?on? as an ?AM-LOC?
for ?started?. The joint bottom-up model attaches the
preposition correctly.
sure is created that forces a decision between the
two predicates. And because for the role model
?start.01? looks like a better fit than ?get.03?, the
correct attachment is found.
WSJ Brown
w/o w/ w/o w/
isPredicate 96.3 96.5 91.4 92.5
hasRole 87.1 87.7 82.5 83.6
role 76.9 77.5 65.2 66.2
sense 88.3 89.0 76.1 77.5
Table 5: F1 scores for ML predicates; w/o refers to
a Bottom-up system without isArgument predicate, w/
refers to a Bottom-up system with isArgument predicate.
6.3 Efficiency
In the previous sections we have shown that our joint
model indeed does better than an equivalent pipeline
system. However, usually most joint approaches
come at a price: efficiency. Interestingly, in our case
we observe the opposite: our joint model is actually
faster than the pipeline. This can be seen in table 6,
where we list the time it took for several different
system to process the WSJ and Brown test corpus,
respectively. When we compare the times for the
bottom-up model to those of the pipeline, we note
that the joint model is twice as fast. While the indi-
vidual stages within the pipeline may be faster than
the joint system (even when we sum up inference
times), extracting results from one system and feed-
ing them into another creates overhead which offsets
this potential reduction.
Table 6 also lists the run-time of a bottom-up
system that solves the inference problem by fully
grounding the Markov Network that the Markov
Logic (ML) model describes, mapping this network
to an Integer Linear Program, and finding the most
likely assignment using an ILP solver. This sys-
tem (Bottom-up (-CPI)) is four times slower than the
equivalent system that uses Cutting Plane Inference
(Bottom-up). This suggests that if we were to imple-
ment the same joint model using ILP instead of ML,
our system would either be significantly slower, or
we would need to implement a Cutting Plane algo-
rithm for the corresponding ILP formulation?when
we use ML this algorithm comes ?for free?.
System WSJ Brown
Full 9.2m 1.5m
Full (-CPI) 38.4m 7.47m
Bottom-up 9.5m 1.6m
Bottom-up (-CPI) 38.8m 6.9m
Pipeline 18.9m 2.9m
Table 6: Testing times for full model and bottom-up when
CPI algorithm is not used. TheWSJ test set contains 2414
sentences, the Brown test set 426. Our best systems thus
takes on average 230ms per WSJ sentence (on a 2.4Ghz
system).
7 Conclusion
In this paper we have presented aMarkov Logic Net-
work that jointly models all predicate identification,
argument identification and classification and sense
disambiguation decisions for a sentence. We have
shown that this approach is competitive, in particular
if we consider that our input parses are significantly
worse than those of the top CoNLL 2008 systems.
We demonstrated the benefit of jointly predicting
senses and semantic arguments when compared to a
pipeline system that first picks arguments and then
senses. We also showed that by modelling whether
a token is an argument of some predicate and jointly
picking arguments for all predicates of a sentence,
further improvements can be achieved.
Finally, we demonstrated that our system is effi-
cient, despite following a global approach. This ef-
ficiency was also shown to stem from the first order
inference method our Markov Logic engine applies.
Acknowledgements
The authors are grateful to Mihai Surdeanu for pro-
viding the version of the corpus used in this work.
162
References
Eugene Charniak. A maximum-entropy-inspired
parser. In Proceedings of NAACL-2000, 2000.
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?
991, 2003.
Richard Johansson and Pierre Nugues. Dependency-
based semantic role labeling of propbank. In Pro-
ceedings of EMNLP-2008., 2008.
Llu??s Ma?rquez, Xavier Carreras, Ken Litkowski, and
Suzanne Stevenson. Semantic role labeling. Com-
putational Linguistics, 34(2), 2008. Introduction
to the Special Issue on Semantic Role Labeling.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kuebler, S. Marinov, and E. Marsi. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language
Engineering, 13(02):95?135, 2007.
V. Punyakanok, D. Roth, and W. Yih. General-
ized inference with multiple semantic role label-
ing systems. In Ido Dagan and Dan Gildea, ed-
itors, CoNLL ?05: Proceedings of the Annual
Conference on Computational Natural Language
Learning, pages 181?184, 2005.
Matthew Richardson and Pedro Domingos. Markov
logic networks. Technical report, University of
Washington, 2005.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
UAI ?08: Proceedings of the Annual Conference
on Uncertainty in AI, 2008.
Sebastian Riedel and Ivan Meza-Ruiz. Collective
semantic role labelling with markov logic. In
Conference on Computational Natural Language
Learning, 2008.
P. Singla and P. Domingos. Lifted First-Order Belief
Propagation. Association for the Advancement of
Artificial Intelligence (AAAI), 2008.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the
12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), 2008.
Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. Joint learning improves seman-
tic role labeling. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Compu-
tational Linguistics, Morristown, NJ, USA, 2005.
David Vickrey and Daphne Koller. Applying sen-
tence simplification to the conll-2008 shared task.
In Proceedings of CoNLL-2008., 2008.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In EMNLP ?04:
Proceedings of the Annual Conference on Em-
pirical Methods in Natural Language Processing,
2004.
Hai Zhao and Chunyu Kit. Parsing syntactic and se-
mantic dependencies with two single-stage max-
imum entropy models. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, Manchester,
England, 2008.
163
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 85?90,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Semantic Role Labelling with Markov Logic
Ivan Meza-Ruiz? Sebastian Riedel??
?School of Informatics, University of Edinburgh, UK
?Department of Computer Science, University of Tokyo, Japan
?Database Center for Life Science, Research Organization of Information and System, Japan
?I.V.Meza-Ruiz@sms.ed.ac.uk ? sebastian.riedel@gmail.com
Abstract
This paper presents our system for the CoNLL
2009 Shared Task on Syntactic and Semantic
Dependencies in Multiple Languages (Hajic?
et al, 2009). In this work we focus only on the
Semantic Role Labelling (SRL) task. We use
Markov Logic to define a joint SRL model and
achieve the third best average performance in
the closed Track for SRLOnly systems and the
sixth including for both SRLOnly and Joint
systems.
1 Markov Logic
Markov Logic (ML, Richardson and Domingos,
2006) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First Or-
der Logic to allow formulae that can be violated with
some penalty. From an alternative point of view, it is
an expressive template language that uses First Or-
der Logic formulae to instantiate Markov Networks
of repetitive structure.
In the ML framework, we model the SRL task
by first introducing a set of logical predicates1 such
as word(Token,Ortho) or role(Token,Token,Role). In
the case of word/2 the predicate represents a word
of a sentence, the type Token identifies the position
of the word and the type Ortho its orthography. In
the case of role/3, the predicate represents a seman-
tic role. The first token identifies the position of the
predicate, the second the syntactic head of the argu-
ment and finally the type Role signals the semantic
role label. We will refer to predicates such as word/2
1In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
as observed because they are known in advance. In
contrast, role/3 is hidden because we need to infer it
at test time.
With the ML predicates we specify a set of
weighted first order formulae that define a distribu-
tion over sets of ground atoms of these predicates (or
so-called possible worlds). A set of weighted formu-
lae is called a Markov Logic Network (MLN). For-
mally speaking, an MLN M is a set of pairs (?,w)
where ? is a first order formula and w a real weight.
M assigns the probability
p (y) = 1Z exp
?
? ?
(?,w)?M
w
?
c?C?
f?c (y)
?
? (1)
to the possible world y. Here C? is the set of all
possible bindings of the free variables in ? with the
constants of our domain. f?c is a feature function
that returns 1 if in the possible world y the ground
formula we get by replacing the free variables in ?
by the constants in c is true and 0 otherwise. Z
is a normalisation constant. Note that this distri-
bution corresponds to a Markov Network (the so-
called Ground Markov Network) where nodes repre-
sent ground atoms and factors represent ground for-
mulae.
In this work we use 1-best MIRA (Crammer and
Singer, 2003) Online Learning in order to train the
weights of an MLN. To find the SRL assignment
with maximal a posteriori probability according to
an MLN and observed sentence, we use Cutting
Plane Inference (CPI, Riedel, 2008) with ILP base
solver. This method is used during both test time
and the MIRA online learning process.
85
2 Model
In order to model the SRL task in the ML frame-
work, we propose four hidden predicates. Consider
the example of the previous section:
argument/1 indicates the phrase for which its head
is a specific position is an SRL argument.
In our example argument(2) signals that the
phrase for which the word in position 2 is its
head is an argument (i.e., Ms. Haag).
hasRole/2 relates a SRL predicate to a SRL argu-
ment. For example, hasRole(3,2) relates the
predicate in position 3 (i.e., play) to the phrase
which head is in position 2 (i.e., Ms. Haag).
role/3 identifies the role for a predicate-argument
pair. For example, role(3,2,ARG0) denotes the
role ARG0 for the SRL predicate in the posi-
tion 2 and the SRL argument in position 3.
sense/2 denotes the sense of a predicate at a specific
position. For example, sense(3,02) signals that
the predicate in position 3 has the sense 02.
We also define three sets of observable predicates.
The first set represents information about each token
as provided in the shared task corpora for the closed
track: word for the word form (e.g. word(3,plays));
plemma/2 for the lemma; ppos/2 for the POS tag;
feat/3 for each feature-value pair; dependency/3 for
the head dependency and relation; predicate/1 for
tokens that are predicates according to the ?FILL-
PRED? column. We will refer to these predicates as
the token predicates.
The second set extends the information provided
in the closed track corpus: cpos/2 is a coarse POS
tag (first letter of actual POS tag); possibleArg/1 is
true if the POS tag the token is a potential SRL argu-
ment POS tag (e.g., PUNC is not); voice/2 denotes
the voice for verbal tokens based on heuristics that
use syntactic information, or based on features in the
FEAT column of the data. We will refer to these
predicates as the extended predicates.
Finally, the third set represents dependency infor-
mation inspired by the features proposed by Xue and
Palmer (2004). There are two types of predicates
in this set: paths and frames. Paths capture the de-
pendency path between two tokens, and frames the
subcategorisation frame for a token or a pair of to-
kens. There are directed and undirected versions of
paths, and labelled (with dependency relations) and
unlabelled versions of paths and frames. Finally, we
have a frame predicate with the distance from the
predicate to its head. We will refer to the paths and
most of the frames predicates as the path predicates,
while we will consider the frame predicates for a
unique token part token predicates.
The ML predicates here presented are used within
the formulae of our MLN. We distinguish between
two types of formula: local and global.
2.1 Local formulae
A formula is local if its groundings relate any num-
ber of observed ground atoms to exactly one hidden
ground atom. For example, a grounding of the local
formula
lemma(p,+l1)?lemma(a,+l2) ? hasRole(p, a)
connects a hidden hasRole/2 ground atom to two ob-
served plemma/2 ground atoms. This formula can be
interpreted as the feature for the predicate and argu-
ment lemmas in the argument identification stage of
a pipeline SRL system. Note that the ?+? prefix indi-
cates that there is a different weight for each possible
pair of lemmas (l1, l2).
We divide our local formulae into four sets, one
for each hidden predicate. For instance, the set for
argument/1 only contains formulae in which the hid-
den predicate is argument/1.
The sets for argument/1 and sense/2 predicates
have similar formulae since each predicate only in-
volves one token at time: the SRL argument or the
SRL predicate token. The formulae in these sets are
defined using only token or extended observed pred-
icates.
There are two differences between the argument/1
and sense/2 formulae. First, the argument/1 for-
mulae use the possibleArg/1 predicate as precondi-
tion, while the sense formulae are conditioned on
the predicate/1 predicate. For instance, consider the
argument/1 formula based on word forms:
word(a,+w) ? possibleArg(a) ? argument(a),
and the equivalent version for the sense/2 predicate:
word(p,+w) ? predicate(p) ? sense(p,+s).
This means we only apply the argument/1 formulae
if the token is a potential SRL argument, and the
sense/2 formulae if the token is a SRL predicate.
86
The second difference is the fact that for the
sense/2 formulae we have different weights for each
possible sense (as indicated by the +s term in the
second formula above), while for the argument/1
formulae this is not the case. This follows naturally
from the fact that argument/1 do not explicitly con-
sider senses.
Table 1 presents templates for the local formuale
of argument/1 and sense/2. Templates allow us to
compactly describe the FOL clauses of a ML. The
template column shows the body of a clause. The
last two columns of the table indicate if there is a
clause with the given body and argument(i) (I) or
sense(i,+s) (S) head, respectively. For example,
consider the first row: since the last two columns
of the row are marked, this template expands into
two formulae: word(i,+w) ? argument(i) and
word(i,+w) ? sense(i,+s). Including the pre-
conditions for each hidden predicate we obtain the
following formulae:
possibleArg(i) ? word(i,+w) ? argument(i)
and
predicate(i) ? word(i,+w) ? sense(i,+s).
In the case of the template marked with a ?*?
sign, the parameters P and I, where P ?
{ppos, plemma} and I ? {?2,?1, 0, 1, 2}, have to
be replaced by any combination of possible values.
Since we generate argument and sense formulae
for this template, the row corresponds to 20 formu-
lae in total.
Table 2 shows the local formuale for hasRole/2
and role/3 predicates, for these formulae we use to-
ken, extended and path predicates. In this case,
these templates have as precondition the formula
predicate(p) ? possibleArg(a). This ensures that
the formulae are only applied for SRL predicates
and potential SRL arguments. In the table we in-
clude the values to replace the template parame-
ters with. Some of these formulae capture a no-
tion of distance between SRL predicate and SRL
argument and are implicitely conjoined with a
distance(p, a,+d) atom. If a formulae exists both
with and without distance atom, we write Both in
the ?Dist? column; if it only exists with the distance
atom, we write Only, otherwise No.
Note that Tables 1 and 2 do not mention
the feature information provided in the cor-
Template I S
word(i,+w) X X
P(i+ I,+v)* X X
cpos(i+ 1,+c1) ? cpos(i? 1,+c2) X X
cpos(i+ 1,+c1) ? cpos(i? 1,+c2) ?
cpos(i+ 2,+c3) ? cpos(i? 2,+c4)
X X
dep(i, ,+d) X X
dep( , i,+d) X X
ppos(i,+o) ? dep(i, j,+d) X X
ppos(i,+o1) ? ppos(j,+o2) ?
dep(i, j,+d)
X X
ppos(j,+o1) ? ppos(k,+o2) ?
dep(j, k, ) ? dep(k, i,+d)
X X
plemma(i,+l) ? dep(j, i,+d) X X
frame(i,+f) X X
(Empty Body) X
Table 1: Templates of the local formulae for argument/1
and sense/2. I: head of clause is argument(i), S: head of
clause is sense(i,+s)
pora because this information was not avail-
able for every language. We therefore group
the formulae which consider the feature/3 pred-
icate into another a set we call feature formu-
lae. This is the summary of these formulae:
feat(p,+f,+v) ? sense(p,+s)
feat(p,+f,+v) ? argument(a)
feat(p,+f,+v1) ? feat(p, f,+v2) ?
hasRole(p, a)
feat(p,+f,+v1) ? feat(p, f,+v2) ?
role(p, a,+r)
Additionally, we define a set of language spe-
cific formulae. They are aimed to capture the re-
lations between argument and its siblings for the
hasRole/2 and role/3 predicates. In practice in
turned out that these formulae were only beneficial
for the Japanese language. This is a summary of
such formulae which we called argument siblings:
dep(a, h, ) ? dep(h, c, ) ? ppos(a,+p1)?
ppos(c,+p2) ? hasRole(p, a)
dep(a, h, ) ? dep(h, c, ) ? ppos(a,+p1)?
ppos(c,+p2) ? role(p, a,+r)
dep(a, h, ) ? dep(h, c, ) ? plemma(a,+p1)?
ppos(c,+p2) ? hasRole(p, a)
dep(a, h, ) ? dep(h, c, ) ? plemma(a,+p1)?
ppos(c,+p2) ? role(p, a,+r)
With these sets of formulae we can build specific
MLNs for each language in the shared task. We
group the formulae into the modules: argument/1,
87
Template Parameters Dist. H R
P(p,+v) P ? S1 Both X X
plemma(p,+l) ? ppos(a,+o) No X
ppos(p,+o) ? plemma(a,+l) No X
plemma(p,+l1) ? plemma(a,+l2) Only X X
ppos(p,+o1) ? ppos(a,+o2) Only X
ppos(p,+o1) ? ppos(a+ I,+o2) I ? {?1, 0, 1} Only X
plemma(p,+l) Only X
voice(p,+e) ? lemma(a,+l) Only X
cpos(p,+c1) ? cpos(p+ I,+c2) ? cpos(a,+c3) ? cpos(a+ J, c4) I,J ? {?1, 1}2 No X X
ppos(p,+v1) ? ppos(a, IN) ? dep(a,m, ) ?P(m,+v2) P ? S1 No X X
plemma(p,+v1) ? ppos(a, IN) ? dep(a,m, ) ? ppos(m,+v2) No X X
P(p, a,+v) P ? S2 No X X
P(p, a,+v) ? plemma(p,+l) P ? S3 No X X
P(p, a,+v) ? plemma(p,+l1) ? plemma(a,+l2) P ? S4 No X X
pathFrame(p, a,+t) ? plemma(p,+l) ? voice(p,+e) No X X
pathFrameDist(p, a,+t) Only X X
pathFrameDist(p, a,+t) ? voice(p,+e) Only X X
pathFrameDist(p, a,+t) ? plemma(p,+l) Only X X
P(p, a,+v) ? plemma(a,+l) P ? S5 Only X X
P(p, a,+v) ? ppos(p,+o) P ? S5 Only X X
pathFrameDist(p, a,+t) ? ppos(p,+o1) ? ppos(a,+o2) Only X X
path(p, a,+t) ? plemma(p,+l) ? cpos(a,+c) Only X X
dep( , a,+d) Only X X
dep( , a,+) ? voice(p,+e) Only X X
dep( , a,+d1) ? dep( , p,+d2) Only X X
(EmptyBody) No X X
Table 2: Templates of the local formulae for hasRole/2 and role/3. H: head of clause is hasRole(p, a), R:
head of clause is role(p, a,+r) and S1 = {ppos, plemma}, S2 = {frame, unlabelFrame, path}, S3 =
{frame, pathFrame}, S4 = {frame, pathFrame, path}, S5 = {pathFrameDist, path}
hasRole/2, role/3, sense/3, feature and argument sib-
lings. Table 3 shows the different configurations of
such modules that we used for the individual lan-
guages. We omit to mention the argument/1, has-
Role/2 and role/3 modules because they are present
for all languages.
A more detailed description of the formulae can
be found in our MLN model files.2 They can be
used both as a reference and as input to our Markov
Logic Engine,3 and thus allow the reader to easily
reproduce our results.
2.2 Global formulae
Global formulae relate several hidden ground atoms.
We use them for two purposes: to ensure consis-
2http://thebeast.googlecode.com/svn/
mlns/conll09
3http://thebeast.googlecode.com
Set Feature sense/2 Argument
siblings
Catalan Yes Yes No
Chinese No Yes No
Czech Yes No No
English No Yes No
German Yes Yes No
Japanese Yes No Yes
Spanish Yes Yes No
Table 3: Different configuration of the modules for the
formulae of the languages.
88
tency between the decisions of all SRL stages and
to capture some of our intuition about the task. We
will refer to formulae that serve the first purpose
as structural constraints. For example, a structural
constraint is given by the (deterministic) formula
role(p, a, r) ? hasRole(p, a)
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a).
The global formulae that capture our intuition
about the task itself can be further divided into two
classes. The first one uses deterministic or hard con-
straints such as
role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)
which forbids cases where distinct arguments of a
predicate have the same role unless the role de-
scribes a modifier.
The second class of global formulae is soft or non-
deterministic. For instance, the formula
lemma(p,+l) ? ppos(a,+p)
?hasRole(p, a) ? sense(p,+f)
is a soft global formula. It captures the observation
that the sense of a verb or noun depends on the type
of its arguments. Here the type of an argument token
is represented by its POS tag.
Table 4 presents the global formulae used in this
model.
3 Results
For our experiments we use the corpora provided in
the SRLonly track of the shared task. Our MLN
is tested on the following languages: Catalan and
Spanish (Taule? et al, 2008) , Chinese (Palmer and
Xue, 2009), Czech (Hajic? et al, 2006),4 English
(Surdeanu et al, 2008), German (Burchardt et al,
2006), Japanese (Kawahara et al, 2002).
Table 5 presents the F1-scores and training/test
times for the development and in-domain corpora.
Clearly, our model does better for English. This is
4For training we use only sentences shorter than 40 words in
this corpus.
Structural constraints
hasRole(p, a) ? argument(a)
role(p, a, r) ? hasRole(p, a)
argument(a) ? ?p.hasRole(p, a)
hasRole(p, a) ? ?r.role(p, a, r)
Hard constraints
role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)
sense(p, s1) ? s1 6= s2 ? ?sense(p, r2)
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ?
?role (p, a2, r)
Soft constraints
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ?
?role (p, a2, r)
plemma(p,+l)?ppos(a,+p)?hasRole(p, a) ?
sense(p,+f)
plemma(p,+l)? role(p, a,+r) ? sense(p,+f)
Table 4: Global formulae for ML model
Language Devel Test Train Test
time time
Average 77.25% 77.46% 11h 29m 23m
Catalan 78.10% 78.00% 6h 11m 14m
Chinese 77.97% 77.73% 36h 30m 34m
Czech 75.98% 75.75% 14h 21m 1h 7m
English 82.28% 83.34% 12h 26m 16m
German 72.05% 73.52% 2h 28m 7m
Japanese 76.34% 76.00% 2h 17m 4m
Spanish 78.03% 77.91% 6h 9m 16m
Table 5: F-scores for in-domain in corpora for each lan-
guage.
in part because the original model was developed for
English.
To put these results into context: our SRL system
is the third best in the SRLOnly track of the Shared
Task, and it is the sixth best on both Joint and SR-
LOnly tracks. For five of the languages the differ-
ence to the F1 scores of the best system is 3%. How-
ever, for German it is 6.19% and for Czech 10.76%.
One possible explanation for the poor performance
on Czech data will be given below. Note that in com-
parison our system does slightly better in terms of
precision than in terms of recall (we have the fifth
best average precision and the eighth average recall).
Table 6 presents the F1 scores of our system for
the out of domain test corpora. We observe a similar
tendency: our system is the sixth best for both Joint
and SRLOnly tracks. We also observe similar large
differences between our scores and the best scores
for German and Czech (i.e., > 7.5%), while for En-
glish the difference is relatively small (i.e., < 3%).
89
Language Czech English German
F-score 77.34% 71.86% 62.37%
Table 6: F-scores for out-domain in corpora for each lan-
guage.
Finally, we evaluated the effect of the argument
siblings set of formulae introduced for the Japanese
MLN. Without this set the F-score is 69.52% for the
Japanese test set. Hence argument siblings formulae
improve performance by more than 6%.
We found that the MLN for Czech was the one
with the largest difference in performance when
compared to the best system. By inspecting our
results for the development set, we found that for
Czech many of the errors were of a rather techni-
cal nature. Our system would usually extract frame
IDs (such as ?play.02?) by concatenating the lemma
of the token and outcome of the sense/2 prediction
(for the ?02? part). However, in the case of Czech
some frame IDs are not based on the lemma of the
token, but on an abstract ID in a vocabulary (e.g.,
?v-w1757f1?). In these cases our heuristic failed,
leading to poor results for frame ID extraction.
4 Conclusion
We presented a Markov Logic Network that per-
forms joint multi-lingual Semantic Role Labelling.
This network achieves the third best semantic F-
scores in the closed track among the SRLOnly sys-
tems of the CoNLL-09 Shared Task, and sixth best
semantic scores among SRLOnly and Joint systems
for the closed task.
We observed that the inclusion of features which
take into account information about the siblings of
the argument were beneficial for SRL performance
on the Japanese dataset. We also noticed that our
poor performance with Czech are caused by our
frame ID heuristic. Further work has to be done in
order to overcome this problem.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado?, and Manfred
Pinkal. The SALSA corpus: a German corpus
resource for lexical semantics. In Proceedings of
LREC-2006, Genoa, Italy, 2006.
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?
991, 2003. ISSN 1533-7928.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, and Zdene?k Z?abokrtsky?. Prague
dependency treebank 2.0, 2006.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??,
Llu??s Ma`rquez, Adam Meyers, Joakim Nivre, Se-
bastian Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mi-
ahi Surdeanu, Nianwen Xue, and Yi Zhang. The
CoNLL-2009 shared task: Syntactic and semantic
dependencies in multiple languages. In Proceed-
ings of CoNLL-2009), Boulder, Colorado, USA,
2009.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. Construction of a Japanese relevance-
tagged corpus. In Proceedings of the LREC-2002,
pages 2008?2013, Las Palmas, Canary Islands,
2002.
Martha Palmer and Nianwen Xue. Adding semantic
roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172, 2009.
Matt Richardson and Pedro Domingos. Markov
logic networks. Machine Learning, 62:107?136,
2006.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
UAI ?08: Proceedings of the Annual Conference
on Uncertainty in AI, 2008.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. The CoNLL-
2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of
CoNLL-2008, 2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta
Recasens. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. In Proceedings of
LREC-2008, Marrakesh, Morroco, 2008.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In EMNLP ?04:
Proceedings of the Annual Conference on Em-
pirical Methods in Natural Language Processing,
2004.
90
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013?1023,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Collective Cross-Document Relation Extraction Without Labelled Data
Limin Yao Sebastian Riedel Andrew McCallum
University of Massachusetts, Amherst
{lmyao,riedel,mccallum}@cs.umass.edu
Abstract
We present a novel approach to relation ex-
traction that integrates information across doc-
uments, performs global inference and re-
quires no labelled text. In particular, we
tackle relation extraction and entity identifi-
cation jointly. We use distant supervision to
train a factor graph model for relation ex-
traction based on an existing knowledge base
(Freebase, derived in parts from Wikipedia).
For inference we run an efficient Gibbs sam-
pler that leads to linear time joint inference.
We evaluate our approach both for an in-
domain (Wikipedia) and a more realistic out-
of-domain (New York Times Corpus) setting.
For the in-domain setting, our joint model
leads to 4% higher precision than an isolated
local approach, but has no advantage over a
pipeline. For the out-of-domain data, we ben-
efit strongly from joint modelling, and observe
improvements in precision of 13% over the
pipeline, and 15% over the isolated baseline.
1 Introduction
Relation Extraction is the task of predicting seman-
tic relations over entities expressed in structured or
semi-structured text. This includes, for example,
the extraction of employer-employee relations men-
tioned in newswire, or protein-protein interactions
expressed in biomedical papers. It also includes the
prediction of entity types such as country, citytown
or person, if we consider entity types as unary rela-
tions.
A particularly attractive approach to relation ex-
traction is based on distant supervision.1 Here in
1Also called self training, or weak supervision.
place of annotated text, only an existing knowl-
edge base (KB) is needed to train a relation extrac-
tor (Mintz et al, 2009; Bunescu and Mooney, 2007;
Riedel et al, 2010). The facts in the KB are heuris-
tically aligned to an unlabelled training corpus, and
the resulting alignment is the basis for learning the
extractor.
Naturally, the predictions of a distantly supervised
relation extractor will be less accurate than those of
a supervised one. While facts of existing knowledge
bases are inexpensive to come by, the heuristic align-
ment to text will often lead to noisy patterns in learn-
ing. When applied to unseen text, these patterns will
produce noisy facts. Indeed, we find that extraction
precision still leaves much room for improvement.
This room is not as large as in previous work (Mintz
et al, 2009) where target text and training KB are
closely related. However, when we use the knowl-
edge base Freebase (Bollacker et al, 2008) and the
New York Times corpus (Sandhaus, 2008), we ob-
serve very low precision. For example, the preci-
sion of the top-ranked 50 nationality relation
instances is only 28%.
On inspection, it turns out that many of the errors
can be easily identified: they amount to violations
of basic compatibility constraints between facts. In
particular, we observe unsatisfied selectional pref-
erences of relations towards particular entity types
as types of their arguments. An example is the fact
that the first argument of nationality is always
a person while the second is a country. A sim-
ple way to address this is a pipeline: first predict
entity types, and then condition on these when pre-
dicting relations. However, this neglects the fact that
relations could as well be used to help entity type
prediction.
1013
While there is some existing work on enforcing
such constraints in a joint fashion (Roth and Yih,
2007; Kate and Mooney, 2010; Riedel et al, 2009),
they are not directly applicable here. The difference
is the amount of facts they take into account at the
same time. They focus on single sentence extrac-
tions, and only consider very few interacting facts.
This allows them to work with exact optimization
techniques such as (Integer) Linear Programs and
still remain efficient.2 However, when working on
a sentence level they fail to exploit the redundancy
present in a corpus. Moreover, the fewer facts they
consider at the same time, the lower the chance that
some of these will be incompatible, and that mod-
elling compatibility will make a difference.
In this work we present a novel approach that
performs relation extraction across documents, en-
forces selectional preferences, and needs no labelled
data. It is based on an undirected graphical model
in which variables correspond to facts, and factors
between them measure compatibility. In order to
scale up, we run an efficient Gibbs-Sampler at in-
ference time, and train our model using SampleR-
ank (Wick et al, 2009). In practice this leads to a
runtime behaviour that is linear in the size of the cor-
pus. For example, 200,000 documents take less than
three hours for training and testing.
For evaluation we consider two scenarios. First
we follow Mintz et al (2009), use Freebase as
source of distant supervision, and employ Wikipedia
as source of unlabelled text?we will call this an
in-domain setting. This scenario is somewhat arti-
ficial in that Freebase itself is partially derived from
Wikipedia, and in practice we cannot expect text and
training knowledge base to be so close. Hence we
also evaluate our approach on the New York Times
corpus (out-of-domain setting).
For in-domain data we make the following find-
ing. When we compare to an isolated baseline that
makes no use of entity types, our joint model im-
proves average precision by 4%. However, it does
not outperform a pipelined system. In the out-of-
domain setting, our collective model substantially
outperforms both other approaches. Compared to
the isolated baseline, we achieve a 15% increase in
2The pyramid algorithm of Kate and Mooney (2010) may
scale well, but it is not clear how to apply their scheme to cross-
document extraction.
precision. With respect to the pipeline approach, the
increase is 13%.
In the following we will first give some back-
ground information on relation extraction with dis-
tant supervision. Then we will present our graphi-
cal model as well as the inference and learning tech-
niques we apply. After discussing related work, we
present our empirical results and conclude.
2 Background
In this section we will introduce the terminology and
concepts we use throughout the paper. We will also
give a brief introduction to relation extraction, in
particular in the context of distant supervision.
2.1 Relations
We seek to extract facts about entities. Example en-
tities would be the company founder BILL GATES,
the company MICROSOFT, and the country USA.
A relation R is a set of tuples c over entities. We
will follow (Mintz et al, 2009) and call the term
R (c1, . . . cn) with c ? R a relation instance.3 It
denotes the membership of the tuple c in the re-
lation R. For example, founded (BILL GATES,
MICROSOFT) is a relation instance denoting that
BILL GATES and MICROSOFT are related in the
founded relation.
In the following we will always consider some set
of candidate tuples C that may or may not be re-
lated. We define Cn ? C to be set of all n-ary tu-
ples in C. Note that while our definition considers
general n-nary relations, in practice we will restrict
us to unary and binary relations C1 and C2.
Following previous work (Mintz et al, 2009; Ze-
lenko et al, 2003; Culotta and Sorensen, 2004) we
make one more simplifying assumption: every can-
didate tuple can be member of at most one relation.
2.2 Entity Types
An entity can be of one or several entity types. For
example, BILL GATES is a person, and a company
founder. Entity types correspond to the special
case of relations with arity one, and will be treated
as such in the following.
3Other commonly used terms are relational facts, ground
facts, ground atoms, and assertions.
1014
We care about entity types for two reasons. First,
they can be important for downstream applications:
if consumers of our extracted facts know the type
of entities, they can find them more easily, visu-
alize them more adequately, and perform opera-
tions specific to these types (write emails to persons,
book a hotel in a city, etc.). Second, they are use-
ful for extracting binary relations due to selectional
preferences?see section 2.6.
2.3 Mentions
In natural language text spans of tokens are used to
refer to entities. We call such spans entity mentions.
Consider, for example, the following sentence snip-
pet:
(1) Political opponents of President Evo Morales
of Bolivia have in recent days stepped up...
Here ?Evo Morales? is an entity mention of pres-
ident EVO MORALES, and ?Bolivia? a mention of
the country BOLIVIA he is the president of.
People often express relations between entities in
natural language texts by mentioning the participat-
ing entities in specific syntactic and lexical patterns.
We will refer to any tuple of mentions of entities
(e1, . . . en) in a sentence as candidate mention tu-
ple. If such a candidate expresses the relation R,
then it is a relation mention of the relation instance
R (e1, . . . , en).
Consider again example 1. Here the pair of en-
tity mentions (?Evo Morales?, ?Bolivia?) is a candi-
date mention tuple. In fact, in this case the candidate
is indeed a relation mention of the relation instance
nationality (EVO MORALES, BOLIVIA).
2.4 Relation Extraction
We define the task of relation extraction as follows.
We are given a corpus of documents and a set of
target relations. Then we are asked to predict all re-
lation instances I so that for each R (c) ? I there
exists at least one relation mention in the given cor-
pus.
The above definition covers a range of existing
approaches by varying over what we define as tar-
get corpus. On one end, we have extractors that
process text on a per sentence basis (Zelenko et al,
2003; Culotta and Sorensen, 2004). On the other
end, we have methods that take relation mentions
from several documents and use these as input fea-
tures (Mintz et al, 2009; Bunescu and Mooney,
2007).
There is a compelling reason for performing re-
lation extraction within a larger scope that consid-
ers mentions across documents: redundancy. Often
facts are mentioned in several sentences and doc-
uments. Some of these mentions may be difficult
to parse, or they use unseen patterns. But the more
mentions we consider, the higher the probability that
one does parse, and fits a pattern we have seen in the
training data.
Note that for relation extraction that considers
more than a single mention we have to solve the
coreference problem in order to determine which
mentions refer to the same entity. In the follow-
ing we will assume that coreference clusters are pro-
vided by a preprocessing step.
2.5 Distant Supervision
In relation extraction we often encounter a lack of
explicitly annotated text, but an abundance of struc-
tured data sources such as company databases or col-
laborative knowledge bases like Freebase. In order
to exploit this, many approaches use simple but ef-
fective heuristics to align existing facts with unla-
belled text. This labelled text can then be used as
training material of a supervised learner.
One heuristic is to assume that each candidate
mention tuple of a training fact is indeed expressing
the corresponding relation (Bunescu and Mooney,
2007). Mintz et al (2009) refer to this as the dis-
tant supervision assumption.
Clearly, this heuristic can fail. Let us again
consider the nationality relation between EVO
MORALES and BOLIVIA. In an 2007 article of the
New York Times we find this relation mention can-
didate:
(2) ...the troubles faced by Evo Morales in
Bolivia...
This sentence does not directly express that EVO
MORALES is a citizen of BOLIVIA, and hence vi-
olates the distant supervision assumption. The prob-
lem with this observation is that at training time
we may learn a relatively large weight for the
feature ?<Entity1> in <Entity2>? associated with
1015
nationality. When testing our model we then
encounter a sentence such as
(3) Arrest Warrant Issued for Richard Gere in
India.
that leads us to extract that RICHARD GERE is a cit-
izen of INDIA.
2.6 Global Consistency of Facts
As discussed above, distant supervision can lead to
noisy extractions. However, such noise can often be
easily identified by testing how compatible the ex-
tracted facts are to each other. In this work we are
concerned with a particular type of compatibility:
selectional preferences.
Relations require, or prefer, their arguments to be
of certain types. For example, the nationality
relation requires the first argument to be a person,
and the second to be a country. On inspection,
we find that these preferences are often not satis-
fied in a baseline distant supervision system akin to
Mintz et al (2009). This often results from patterns
such as ?<Entity1> in <Entity2>? that fire in many
cases where <Entity2> is a location, but not a
country.
3 Model
Our observations in the previous section suggest
that we should (a) explicitly model compatibil-
ity between extracted facts, and (b) integrate ev-
idence from several documents to exploit redun-
dancy. In this work we choose a Conditional Ran-
dom Field (CRF) to achieve this. CRFs are a natural
fit for this task: They allow us to capture correlations
in an explicit fashion, and to incorporate overlapping
input features from multiple documents.
The hidden output variables of our model areY =
(Yc)c?C . That is, we have one variable Yc for each
candidate tuple c ? C . This variable can take as
value any relation in C with the same arity as c. See
example relation variables in figure 1.
The observed input variablesX consists of a fam-
ily of variables Xc =
(
X1c, . . .X
m
c
)
m?M for each
candidate tuple c. Here Xic stores relevant observa-
tions we make for the i-th candidate mention tuple of
c in the corpus. For example, X1BILL GATES,MICROSOFT
in figure 1 would contain, among others, the pattern
?[M2] was founded by [M1]?.
3.1 Factor Templates
Our conditional probability distribution over vari-
ables X and Y is defined using using a set T of
factor templates. Each template Tj ? T defines
a set of factors {(yi,xi)}, a set Kj of feature in-
dices, parameters
{
?jk
}
k?Kj
and feature functions
{
f jk
}
k?Kj
. Together they define the following con-
ditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
P
k?Kj
?jkf
j
k(yi,xi)
(4)
In our case the set T consists of four templates
we will describe below. We construct this graphical
model using FACTORIE (McCallum et al, 2009), a
probabilistic programming language that simplifies
the construction process, as well as inference and
learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template
is unrolled, it creates one factor per variable Yc for
candidate tuple c ? C. The template also consists of
one weight ?Biasr and feature function f
Bias
r for each
possible relation r. fBiasr fires if the relation associ-
ated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need
to model the correlation between relation instances
and their mentions in text. For this purpose we de-
fine the template TMen that connects each relation
instance variable Yc with its observed mention vari-
ables Xc. Crucially, this template gathers mentions
from multiple documents, and enables us to exploit
redundancy.
The feature functions of this template are taken
from Mintz et al (2009). This includes features that
inspect the lexical content between entity mentions
in the same sentence, and the syntactic path between
them. One example is
fMen101 (yc,xc)
def
=
?
??
??
1 yc = founded ? ?i with
"M2 was founded by M1" ? xic
0 otherwise
.
1016
founder
Microsoft was 
founded by Bill Gates...
person
company
nationality
country
With Microsoft chairman 
Bill Gates soon relinquishing...
Bill Gates was 
born in the USA  in 1955
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1
?
j
k
f
j
k
(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need to
model the correlation between relation instances and
their mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
from (Mintz et al, 2009b) (with minor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Crucially, these templates function on a cross-
document level. They gather all mentions of the can-
didate tuple c and extract features from all of these.
3.1.3 Se ectional Preference Templates
To capture the correlations between entity types
and the relations the entities participate in, we in-
troduce the joint template TJoint. It connects a re-
lation instance variable Ye1,...,ea to the entity type
variables Ye1 , . . . , Yen . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The feature fires when the variables are in the
state r, t1 . . . ta. After training we would expect
a weight ?Joint
founder,person,company to be larger than
?Joint
founder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t that fire if ei is
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1 ?
j
kf
j
k(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need to
model the correlation between relation instances and
t ir mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
fr m (Mintz et al, 2009b) (with inor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Crucially, these templates function on a cross-
document level. They gather all mentions of the can-
didate tuple c and extract features from all of these.
3.1.3 Selectional Preference Templates
To capture the correlations between entity types
and the relations the entities participate in, we in-
troduce th j int tem late TJoint. It connects a re-
la ion instance variable Ye1,...,ea to the entity type
variables Ye1 , . . . , Ye . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The feature fires when the variables are in the
state r, t1 . . . ta. After training we would expect
a weight ?Jointfounder,person,company to be larger than
?Jointfounder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t that fire if ei is
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1 ?
j
kf
j
k(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we ne d to
model the correlation between relation instances and
their mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
from (Mintz et al, 2009b) (with minor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Cruci lly, these templates function on a cross-
document level. Th y gather all mentions of the can-
didate tupl c and extract features from all o these.
3.1.3 Selectional Preference Templates
To capture the correlations between entity types
and the relations he entities participate in, we in-
troduce the joint template TJoint. It connects a re-
lation instance variable Ye1,...,e to the entity type
variables Ye1 , . . . , Yen . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r, 1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The featur fires when the variables are i the
state r, t1 . . . ta. After training we would xpect
a weight ?Jointfounder,person,company to be larger than
?Jointfounder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t tha fire if ei is
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i )
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,; exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
Figure 1: Factor Graph of our model that captures selectional preferences and functionality constraints. For
readability we only label a subsets of equivalent variables and factors. Note that the graph shows an example
assignment to variables.
It tests whether for any mentions of the candidate
tuple the phrase "founded by" appears between the
mentions of the argument entities.
3.1.3 S lectional Preference T mplates
To capture the correlations between entity types
and relations the entities participate in, we introduce
the template TJoint. It connects a relation instance
variable Ye1,...,en to the individual entity type vari-
ables Ye1 , . . . , Yen . To measure the compatibility
between relation and entity variables, we use one
feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta) for each com-
bination of relation and entity types r, t1 . . . ta.
f Jointr,t1...ta fires when the factor variables are in the
state r, t1 . . . ta. For example, f Jointfounded,person,company
fires if Ye1 is in state person, Ye2 in state company,
and Ye1,e2 in state founded.
We also add a template TPair that measures the
pairwise compatibility between the relation variable
Ye1,...,ea and each entity variable Yei in isolation.
Here we use features fPairi,r,t that fire if ei is the i-th ar-
gument of c, has the entity type t and the candidate
tuple c is labelled as instance of relation r. For ex-
ample, fPair1,founded,person fires if Ye1(argument i = 1)
is in state person, and Ye1,e2 in state founded, re-
gardless of the state of Ye2 .
3.2 Inference
There are two types of inference we have to perform:
sampling from the posterior during training (see sec-
tion 3.3), and finding the most likely configuration
(aka MAP inference). In both settings we employ a
Gibbs sampler (Geman and Geman, 1990) that ran-
domly picks a variable Yc and samples its relation
value conditioned on its Markov Blanket. At test
time we decrease the temperature of our sampler in
order to find an approximation of the MAP solution.
3.3 Training
Most learning methods need to calculate the model
expectations (Lafferty et al, 2001) or the MAP con-
figuration (Collins, 2002) before making an update
to the parameters. This step of inference is usually
the bottleneck for learning, even when performed
approximately.
SampleRank (Wick et al, 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within MCMC infer-
ence. Every pair of consecutive samples in the
MCMC chain is ranked according to the model and
the ground truth, and the parameters are updated
when the rankings disagree. This update can fol-
low different schemes, here we use MIRA (Cram-
mer and Singer, 2003). This allows the learner to
acquire more supervision per instance, and has led
to efficient training for models in which inference
1017
is expensive and generally intractable (Singh et al,
2009).
4 Related Work
Distant Supervision Learning to extract relations
by using distant supervision has raised much interest
in recent years. Our work is inspired by Mintz et al
(2009) who also use Freebase as distant supervision
source. We also heuristically align our knowledge
base to text by making the distant supervision as-
sumption (Bunescu and Mooney, 2007; Mintz et al,
2009). However, in contrast to these previous ap-
proaches, and other related distant supervision meth-
ods (Craven and Kumlien, 1999; Weld et al, 2009;
Hoffmann et al, 2010), we perform relation extrac-
tion collectively with entity type prediction.
Schoenmackers et al (2008) use entailment rules
on assertion extracted by TextRunner to increase re-
call. They also perform cross-document probabilis-
tic inference based on Markov Networks. However,
they do not infer the types of entities and work in an
open IE setting.
Selectional Preferences In the context of super-
vised relation extraction, selectional preferences
have been applied. For example, Roth and Yih
(2007) have used Linear Programming to enforce
consistency between entity types and extracted re-
lations. Kate and Mooney (2010) use a pyramid
parsing scheme to achieve the same. Riedel et al
(2009) use Markov Logic to model interactions be-
tween event-argument relations for biomedical event
extraction. However, their work is (a) supervised,
and (b) performs extraction on a per-sentence basis.
Carlson et al (2010) also use selectional prefer-
ences. However, instead of exploiting them for train-
ing a graphical model using distant supervision, they
use selectional preferences to improve a bootstrap-
ping process. Here in each iteration of bootstrap-
ping, extracted facts that violate compatibility con-
straints will not be used to generate additional pat-
terns in the next iteration.
5 Experiments
We set up experiments to answer the following ques-
tions: (i) Does the explicit modelling of selectional
preferences improve accuracy? (ii) Can we also per-
form joint entity and relation extraction in a pipeline
and achieve similar results? (iii) How does our
cross-document approach scale?
To answer these questions we carry out experi-
ments on two data sets, Wikipedia and New York
Times articles, and use Freebase as distant supervi-
sion source for both.
5.1 Experimental Setup
We follow Mintz et al (2009) and perform two types
of evaluation: held-out and manual. In both cases
we have a training and a test corpus of documents,
and training and test sets of entities. For held-out
evaluation we split the set of entities in Freebase into
training and test sets. For manual evaluation we use
all Freebase entities during training. For testing we
use all entities that appear in the test document cor-
pus.
For both training and testing we then choose the
candidate tuples C that may or may not be relation
instances. To pick the entities C1 we want to predict
entity types for, we choose all entities that are men-
tioned at least once in the train/test corpus. To pick
the entity pairs C2 that we want to predict the rela-
tions of, we choose those that appear at least once
together in a sentence.
The set of candidates C will contain many tuples
which are not related in any Freebase relations. For
efficiency, we filter out a large fraction of these neg-
ative candidates for training. The number of neg-
ative examples we keep is chosen to be about 10
times the number of positive candidates. This num-
ber stems from trading-off the accuracy it leads to
and the increased training time it requires.
For both manual and held-out evaluation we rank
extracted test relation instances in the MAP state of
the network. This state is found by sampling 20 iter-
ations with a low temperature of 0.00001. The rank-
ing is done according to the log linear score that the
assigned relation for a candidate tuple gets from the
factors in its Markov Blanket. For optimal perfor-
mance, the score is normalized by the number of re-
lation mentions.
For manual evaluation we pick the top ranked 50
relation instances for the most frequent relations.
We ask three annotators to inspect the mentions of
these relation instances to decide whether they are
correct. Upon disagreement, we use majority vote.
To summarize precisions across relations, we take
1018
their average, and their average weighted by the pro-
portion of predicted instances for the given relation.
5.1.1 Data preprocessing
We preprocess our textual data as follows:
We first use the Stanford named entity recog-
nizer (Finkel et al, 2005) to find entity mentions in
the corpus. The NER tagger segments each docu-
ment into sentences and classifies each token into
four categories: PERSON, ORGANIZATION, LO-
CATION and NONE. We treat consecutive tokens
which share the same category as single entity men-
tion. Then we associate these mentions with Free-
base entities. This is achieved by performing a
string match between entity mention phrases and the
canonical names of entities as present in Freebase.
For each candidate tuple c with arity 2 and each
of its mention tuples iwe extract a set of featuresXic
similar to those used in (Mintz et al, 2009): lexical,
Part-Of-Speech (POS), named entity and syntactic
features, i.e. features obtained from the dependency
parsing tree of a sentence. We use the openNLP POS
tagger4 to obtain POS tags and employ the Malt-
Parser (Nivre et al, 2004) for dependency parsing.
For candidate tuples with arity 1 (entity types) we
use the following features: the entity?s word form,
the POS sequence, the head of the entity in the de-
pendency parse tree, the Stanford named entity tag,
and the left and right words to the current entity
mention phrase.
5.1.2 Configurations
We apply the following configurations of our fac-
tor graphs. As our baseline, and roughly equivalent
to previous work (Mintz et al, 2009), we pick the
templates TBias and TMen. These describe a fully dis-
connected graph, and we will refer to this configu-
ration as isolated. Next, we add the templates TJoint
and TPair to model selectional preferences, and refer
to this setting as joint.
In addition, we evaluate howwell selectional pref-
erences can be captured with a simple pipeline. For
this pipeline we first train an isolated system for en-
tity type prediction. Then we use the output of the
entity type prediction system as input for the relation
extraction system.
4available at http://opennlp.sourceforge.net/
5.1.3 Entity types and Relation types
Freebase contains many relation types and only
a subset of those relation types occur frequently
in the corpus. Since classes with very few
training instances are generally hard to learn,
we restrict ourselves to the 54 most frequently
mentioned relations. These include, for ex-
ample, nationality, contains, founded
and place_of_birth. Note that we con-
vert two Freebase non-binary temporal relations
to binary relations: employment_tenure and
place_lived. In both cases we simply disregard
the temporal information in the Freebase data.
As our main focus is relation extraction, we re-
strict ourselves to entity types compatible with our
selected relations. To this end we inspect the Free-
base schema information provided for each relation,
and include those entity types that are declared as
arguments of our relations. This leads to 10 entity
types including person, citytown, country,
and company.
Note that a Freebase entity can have several types.
We pick one of these by choosing the most specific
one that is a member of our entity type subset, or
MISC if no such member exists.
5.2 Wikipedia
In our first set of experiments we train and test using
Wikipedia as the text corpus. This is a comparatively
easy scenario because the facts in Freebase are partly
derived from Wikipedia, hence there is an increased
chance of properly aligning training facts and text.
This is similar to the setting of Mintz et al (2009).
5.2.1 Held Out Evaluation
We split 1,300,000 Wikipedia articles into train-
ing and test sets. Table 1 shows the statistics for this
split. The last row provides the number of negative
relation instances (candidates which are not related
according to Freebase) associated with each data set.
Figure 2 shows the precision-recall curves of re-
lation extraction for held-out data of various config-
urations. We notice a slight advantage of the joint
approach in the low recall area. Moreover, the joint
model predicts more relation instances, as can be
seen by its longer line in the graph.
For higher recall, the joint model performs
slightly worse. On closer inspection, we find that
1019
Wikipedia NYT
Train Test Train Test
#Documents 900K 400K 177K 39K
#Entities 213K 137K 56K 27K
#Positive 36K 24K 5K 2K
#Negative 219K 590K 64K 94K
Table 1: The statistics of held-out evaluation on
Wikipedia and New York Times.
0.0 0.1 0.2 0.3 0.4
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Recall
Pre
cisio
n
joint
pipe
isolated
Figure 2: Precision-recall curves for various setups
in Wikipedia held-out setting.
this observation is somewhat misleading. Many of
the predictions of the joint model are not in the
held-out test set derived from Freebase, but never-
theless correct. Hence, to understand if one system
really outperforms another, we need to rely on man-
ual evaluation.
Note that the figure only considers binary
relations?for entity types all configurations per-
form similarly.
5.2.2 Manual Evaluation
As mentioned above, held-out evaluation in this
context suffers from false negatives in Freebase. Ta-
ble 2 therefore shows the results of our manual eval-
uation. They are based on the average, and weighted
average, of the precisions for the relation instances
of the most frequent relations. We notice that here
Isolated Pipeline Joint
Wikipedia 0.82 0.87 0.86
Wikipedia (w) 0.95 0.94 0.95
NYT 0.63 0.65 0.78
NYT (w) 0.78 0.82 0.94
Table 2: Average and weighted (w) average preci-
sion over frequent relations for New York Times and
Wikipedia data, based on manual evaluation.
all systems perform comparably for weighted aver-
age precision. For average precision we see an ad-
vantage for both the pipeline and the joint model
over the isolated system.
One reason for similar weighted average preci-
sions is the fact that all approaches accurately pre-
dict a large number of contains instances. This is
due to very regular and simple patterns inWikipedia.
For example, most articles on towns start with ?A is
a municipality in the district of B in C, D.? For these
sentences, the relative position of two location men-
tions is a very good predictor of contains. When
used as a feature, it leads to high precision for all
models. And since contains instances are most
frequent, and we take the weighted average, results
are generally close to each other.
To summarize: in this in-domain setting, mod-
elling compatibility between entity types and rela-
tions helps to improve average precision, but not
weighted average precision. This holds for both the
joint and the pipeline model. However, we will see
how this changes substantially when moving to an
out-of-domain scenario.
5.3 New York Times
For our second set of experiments we use New
York Times data as training and test corpora. As
we argued before, this is expected to be the more
difficult?and more realistic?scenario.
5.3.1 Held-out Evaluation
We choose all articles of the New York times dur-
ing 2005 and 2006 as training corpus. As test corpus
we use the first 6 months of 2007.
Figure 3 shows precision-recall curves for our var-
ious setups. We see that jointly modelling entity
1020
0.00 0.05 0.10 0.15 0.20
0.2
0.4
0.6
0.8
1.0
Recall
Pre
cisio
n
joint
pipe
isolated
Figure 3: Precision-recall curves for various setups
in New York Times held-out setting.
types and relations helps to improve precision.
Due to the smaller overlap between Freebase and
NYT data, figure 3 also has to be taken with more
caution. The systems may predict correct relation
instances that just do not appear in Freebase. Hence
manual evaluation is even more important.
When evaluating entity precision we find that for
both models it is about 84%. This raises the ques-
tion why the joint entity type and relation extrac-
tion model outperforms the pipeline on relations.
We take a close look at the entities which partici-
pate in relations and find that joint model performs
better on most entity types, for example, country
and citytown. We also look at the relation in-
stances which are predicted by both systems and find
that the joint model does predict correct entity types
when the pipeline mis-predicts. And exactly these
mis-predictions lead the pipeline astray. Consider-
ing binary relation instances where the pipeline fails
but the joint model does not, we observe an entity
precision of 76% for the pipeline and 86% for our
joint approach. The joint model fails to correctly
predict some entity types that the pipeline gets right,
but these tend to appear in contexts where relation
instances are easy to extract without considering en-
Relation Type Iso. Pipe Joint
contains 0.92 0.98 0.96
nationality 0.28 0.64 0.82
plc_lived 0.88 0.70 0.96
plc_of_birth 0.32 0.20 0.25
works_for 0.96 0.98 0.98
plc_of_death 0.24 0.40 0.42
children 1.00 0.92 0.98
founded 0.42 0.34 0.71
Table 3: Precision at 50 for the most frequent rela-
tions on New York Times
tity types.5
5.3.2 Manual Evaluation
Manually evaluated precision for New York
Times data can be seen in table 2. In contrast to the
Wiki setting, here modelling entity types and rela-
tions jointly makes a substantial difference. For av-
erage precision, our joint model improves over the
isolated baseline by 15%, and over the pipeline by
13%. Similar improvements can be observed for
weighted average precision.
Let us look at a break-down of precisions with
respect to different relations shown in table 3. We
see dramatic improvements for nationality and
founded when applying the joint model. Note that
the nationality relation takes a larger part in
the predicted relation instances of the joint model
and hence contributes significantly to the weighted
average precision.
5.4 Scalability
We propose to perform joint inference for large scale
information extraction. An obvious concern in this
scenario is scalability. In practice we find that infer-
ence (and hence learning) in our model scales lin-
early with the number of candidate tuples. This can
be seen in figure 4a. It is to be expected since the
number of candidates equals the number of variables
the sampler has to process in each iteration.
The above observation also means that our ap-
proach scales linearly with corpus size. To illustrate
5Note that our learned preferences are soft, and hence can
be violated in case of wrong entity type predictions.
1021
1e+05 2e+05 3e+05 4e+05 5e+05
200
300
400
500
600
700
800
Number of Candidate Tuples
Time pe
r iteratio
n (seco
nds)
(a) CPU time
0 5000 10000 15000 20000 25000
1e+05
2e+05
3e+05
4e+05
5e+05
6e+05
Number of Documents
Numbe
r of Can
didate T
uples
(b) Candidate tuples
Figure 4: CPU time for one iteration per candidate
tuple, and candidate tuples per document.
this, figure 4b shows how the number of candidates
scales with the number of documents. Again we ob-
serve a linear behavior. Since both are linear, we can
say that our joint approach is linear in the number of
documents.
Total training and test times are moderate, too.
For example, the held-out experiments with 200,000
NYT documents finish within three hours.
6 Conclusion
This paper presents a novel approach to extracting
relational facts from text. Akin to previous work in
relation extraction with distant supervision, we re-
quire no annotated text. However, instead extract-
ing facts in isolation, we model interactions between
facts in order to improve precision. In particular, we
capture selectional preferences of relations. These
preferences are modelled in a cross-document fash-
ion using a large scale factor graph. We show in-
ference and learning can be efficiently performed
in linear time by Gibbs Sampling and SampleRank.
When applied to out-of-domain text, this approach
leads to a 15% increase in precision over an isolated
baseline, and a 13% improvement over a pipelined
system.
A crucial aspect of our approach is its extensibil-
ity. Since it is exclusively framed in terms of an
undirected graphical model, it is conceptually easy
to extend it to other types of compatibilities, such
as functionality constraints. It could also be ex-
tended to tackle coreference resolution. Eventually
we seek to model the complete process of the au-
tomatic construction of KB within this framework,
and capture dependencies between extractions in a
joint and principled fashion. As we have seen here,
in particular when learning is less supervised and
extractions are noisy, capturing such interactions is
paramount.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation un-
der NSF grant #IIS-0326249, and in part by UPenn
NSF medium IIS-0803847. The University of Mas-
sachusetts also gratefully acknowledges the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD ?08: Proceedings of the 2008
ACM SIGMOD international conference on Manage-
ment of data, pages 1247?1250, New York, NY, USA.
ACM.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Andrew Carlson, Justin Betteridge, Richard Wang, Es-
tevam Hruschka, and Tom Mitchell. 2010. Cou-
pled semi-supervised learning for information extrac-
tion. In Third ACM International Conference on Web
Search and Data Mining (WSDM ?10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical methods in natural lan-
guage processing (EMNLP ?02), volume 10, pages 1?
8.
1022
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge-bases by extracting information from
text sources. In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for Molecular
Biology, pages 77?86, Germany.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
S. Geman and D. Geman. 1990. Stochastic relaxation,
gibbs distributions, and the bayesian restoration of im-
ages. pages 452?472.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In ACL.
Rohit J. Kate and Raymond J. Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proceedings of the 12th Conference on Com-
putational Natural Language Learning (CoNLL? 10).
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In International Conference on Machine Learn-
ing (ICML).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Y. Bengio, D. Schuur-
mans, J. Lafferty, C. K. I. Williams, and A. Culotta, ed-
itors, Advances in Neural Information Processing Sys-
tems 22, pages 1249?1257.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ?09),
pages 1003?1011. Association for Computational Lin-
guistics.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ?09), pages 41?49.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ?08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Sameer Singh, Karl Schultz, and Andrew McCallum.
2009. Bi-directional joint inference for entity res-
olution and segmentation using imperatively-defined
factor graphs. In European Conference on Machine
Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML PKDD), pages 414?
429.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using wikipedia to bootstrap open information extrac-
tion. In ACM SIGMOD Record.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009. Samplerank: Learning
preferences from atomic gradients. In Neural Infor-
mation Processing Systems (NIPS), Workshop on Ad-
vances in Ranking.
Dimitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. JMLR, 3(6):1083 ? 1106.
1023
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1?12,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Fast and Robust Joint Models for Biomedical Event Extraction
Sebastian Riedel Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{riedel,mccallum}@cs.umass.edu
Abstract
Extracting biomedical events from literature
has attracted much recent attention. The best-
performing systems so far have been pipelines
of simple subtask-specific local classifiers. A
natural drawback of such approaches are cas-
cading errors introduced in early stages of the
pipeline. We present three joint models of
increasing complexity designed to overcome
this problem. The first model performs joint
trigger and argument extraction, and lends it-
self to a simple, efficient and exact infer-
ence algorithm. The second model captures
correlations between events, while the third
model ensures consistency between arguments
of the same event. Inference in these models
is kept tractable through dual decomposition.
The first two models outperform the previous
best joint approaches and are very competi-
tive with respect to the current state-of-the-
art. The third model yields the best results re-
ported so far on the BioNLP 2009 shared task,
the BioNLP 2011 Genia task and the BioNLP
2011 Infectious Diseases task.
1 Introduction
Whenever we advance our scientific understanding
of the world, we seek to publish our findings. The
result is a vast and ever-expanding body of natural
language text that is becoming increasingly difficult
to leverage. This is particularly true in the context
of life sciences, where large quantities of biomedi-
cal articles are published on a daily basis. To sup-
port tasks such data mining, search and visualiza-
tion, there is a clear need for structured representa-
tions of the knowledge these articles convey. This is
indicated by a large number of public databases with
content ranging from simple protein-protein interac-
tions to complex pathways. To increase coverage of
such databases, and to keep up with the rate of pub-
lishing, we need to automatically extract structured
representations from biomedical text?a process of-
ten referred to as biomedical text mining.
One major focus of biomedical text mining has
been the extraction of named entities, such genes
or gene products, and of flat binary relations be-
tween such entities, such as protein-protein interac-
tions. However, in recent years there has also been
an increasing interest in the extraction of biomedi-
cal events and their causal relations. This gave rise
to the BioNLP 2009 and 2011 shared tasks which
challenged participants to gather such events from
biomedical text (Kim et al, 2009; Kim et al, 2011).
Notably, these events can be complex and recursive:
they may have several arguments, and some of the
arguments may be events themselves.
Current state-of-the-art event extractors fol-
low the same architectural blueprint and divide
the extraction process into a pipeline of three
stages (Bj?rne et al, 2009; Miwa et al, 2010c). First
they predict a set of candidate event trigger words
(say, tokens 2, 5 and 6 in figure 1), then argument
mentions are attached to these triggers (say, token
4 for trigger 2). The final stage decides how ar-
guments are shared between events?compare how
one event subsumes all arguments of trigger 6 in fig-
ure 1, while two events share the three arguments
of trigger 4 in figure 2. This architecture is prone
to cascading errors: If we miss a trigger in the first
stage, we will never be able to extract the full event
1
... the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain ...
E1:Phosphorylation
E2:Regulation
E3:Binding
Theme
Cause
Theme
Theme
Theme
Regulation BindingPhosphorylation
Theme
Cause
Theme
Theme
Theme
Same Binding
1 2 3
4
5 6 7 8 9 10 11
b
4,9
e
2,Phos.
a
6,9,Theme
(a)
(b)
Figure 1: (a) sentence with target event structure to extract; (b) projection to a set of labelled graph over tokens.
it concerns. Current systems attempt to tackle this
problem by passing several candidates to the next
stage. However, this tends to increase the false pos-
itive rate. In fact, Miwa et al (2010c) observe that
30% of their errors stem from this type of ad-hoc
module communication.
Joint models have been proposed to overcome this
problem (Poon and Vanderwende, 2010; Riedel et
al., 2009). However, besides not being as accurate
as their pipelined competitors, mostly because they
do not yet exploit the rich set of features used by
Miwa et al (2010b) and Bj?rne et al (2009), they
also suffer from the complexity of inference. For
example, to remain tractable, the best joint system
so far (Poon and Vanderwende, 2010) works with
a simplified representation of the problem in which
certain features are harder to capture, employs local
search without certificates of optimality, and further-
more requires a 32-core cluster for quick train-test
cycles. Existing joint models also rely on heuristics
when it comes to deciding which arguments share
the same event. Contrast this with the best current
pipeline (Miwa et al, 2010c; Miwa et al, 2010b)
which uses a classifier for this task.
We present a family of event extraction mod-
els that address the aforementioned problems. The
first model jointly predicts triggers and arguments.
Notably, the highest scoring event structure under
this model can be found efficiently in O (mn) time
where m is the number of trigger candidates, and
n the number of argument candidates. This is
only slightly slower than the O (m?n) runtime of a
pipeline, where m? is the number of trigger candi-
dates as filtered by the first stage. We achieve these
guarantees through a novel algorithm that jointly
picks best trigger label and arguments on a per-token
basis. Remarkably, it takes roughly as much time to
train this model on one core as the model of Poon
and Vanderwende (2010) on 32 cores, and leads to
better results.
The second model enforces additional constraints
that ensure consistency between events in hierarchi-
cal regulation structures. While inference in this
model is more complicated, we show how dual de-
composition (Komodakis et al, 2007; Rush et al,
2010) can be used to efficiently find exact solutions
for a large fraction of problems.
Our third model includes the first two, and explic-
itly captures which arguments are part in the same
event?the third stage of existing pipelines. Due to
a complex coupling between this model and the first
two, inference here requires a projected version of
the sub-gradient technique demonstrated by Rush et
al. (2010).
When evaluated on the BioNLP 2009 shared task,
the first two models outperform the previous best
joint approaches and are competitive when com-
pared to current state-of-the-art. With 57.4 F1 on
the test set, the third model yields the best results
reported so far with a 1.1 F1 margin to the results
of Miwa et al (2010b). For the BioNLP 2011 Ge-
nia task 1 and the BioNLP 2011 Infectious Diseases
task, Model 3 yields the second-best and best results
reported so far. The second-best results are achieved
with Model 3 as is (Riedel and McCallum, 2011),
the best results when using Stanford event predic-
tions as input features (Riedel et al, 2011). The
margins between Model 3 and the best runner-ups
range from 1.9 F1 to 2.8 F1.
In the following we will first introduce biomedical
event extraction and our notation. Then we go on to
present our models and their inference routines. We
present related work, show our empirical evaluation,
and conclude.
2
Grb2 can be coimmunoprecipitated with Sos1 and Sos2
Binding Binding
Theme
Theme
Theme
Theme
Theme
Theme
Theme
1
2 3 4 5 6 7 8
Figure 2: Two binding events with identical trigger. The
projection graph does not change even if both events are
merged.
2 Biomedical Event Extraction
By bio-molecular event we mean a change of state
of one or more bio-molecules. Our task is to extract
structured information about such events from nat-
ural language text. More concretely, let us consider
part (a) of figure 1. We see a snippet of text from a
biomedical abstract, and the three events that can be
extracted from it. We will use these to characterize
the types of events we ought to extract, as defined
by the 2009 BioNLP shared task. Note that for the
shared task, protein mentions are given by the task
organizers and hence do not need to be extracted.
The event E1 in the figure refers to a Phosphory-
lation of the TRAF2 protein. It is an instance of a
set of simple events that describe changes to a sin-
gle gene or gene product. Other members of this
set are: Expression, Transcription, Localization, and
Catabolism. Each of these events has to have exactly
one theme, the protein of which a state change is de-
scribed. A labelled edge in figure 1a) shows that
TRAF2 is the theme of E1.
Event E3 is a Binding of TRAF2 and CD40.
Binding events are particular in that they may have
more than one theme, as there can be several bio-
molecules associated in a binding structure. This is
in fact the case for E3.
In the top-center of figure 1a) we see the Regu-
lation event E2. Such events describe regulatory or
causal relations between events. Other instances of
this type of events are: Positive Regulation and Neg-
ative Regulation. Regulations have to have exactly
one theme; this theme can a be protein or, as in our
case, another event. Regulations may also have zero
or one cause arguments that denote events or pro-
teins which trigger the regulation.
In the BioNLP shared task, we are also asked to
find a trigger (or clue) token for each event. This
token grounds the event in text and allows users to
quickly validate extracted events. For example, the
trigger for event E2 is ?inhibit?, as indicated by a
dashed line.
2.1 Event Projection
To formulate the search for event structures of the
form shown in figure 1a) as an optimization prob-
lem, it will be convenient to represent them through
a set of binary variables. We introduce such a rep-
resentation, inspired by previous work (Riedel et al,
2009; Bj?rne et al, 2009) and based on a projection
of events to a graph structure over tokens, as seen
figure 1b).
Consider sentence x and a set of candidate trig-
ger tokens, denoted by Trig (x). We label each can-
didate i with the event type it is a trigger for, or
None if it is not a trigger. This decision is rep-
resented through a set of binary variables ei,t, one
for each possible event type t. In our example we
have e6,Binding = 1. The set of possible event types
will be denoted as T , the regulation event types as
TReg def= {PosReg, NegReg, Reg} and its complement
as T?reg def= T \ TReg.
For each candidate trigger i we consider the argu-
ments of all events that have i as trigger. Each ar-
gument a will either be an event itself, or a protein.
For events we add a labelled edge between i and the
trigger j of a. For proteins we add an edge between
i and the syntactic head j of the protein mention. In
both cases we label the edge i ? j with the role
of the argument a. The edge is represented through
a binary variable ai,j,r, where r ? R is the argu-
ment role and R def= {Theme, Cause, None}. The
role None is active whenever no Theme or Cause
role is present. In our example we get, among oth-
ers, a2,4,Theme = 1.
So far our representation is equivalent to map-
pings in previous work (Riedel et al, 2009; Bj?rne et
al., 2009) and hence shares their main shortcoming:
we cannot differentiate between two (or more) bind-
ing events with the same trigger but different argu-
ments, or one binding event with several arguments.
Consider, for example, the arguments of trigger 6 in
figure 1b) that are all subsumed in a single event. By
contrast, the arguments of trigger 4 shown in figure
2 are split between two events.
Previous work has resolved this ambiguity
3
through ad-hoc rules (Bj?rne et al, 2009) or with
a post-processing classifier (Miwa et al, 2010c).
We propose to augment the graph representation
through edges between pairs of proteins that are
themes in the same binding event. For two protein
tokens p and q we represent this edge through the
binary variable bp,q. Hence, in figure 1b) we have
b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1
but b6,8 = 0. By explicitly modeling such ?sib-
ling? edges we not only minimize the need for post-
processing. We can also improve attachment deci-
sions akin to second order models in dependency
parsing (McDonald and Pereira, 2006). Note that
while merely introducing such variables is easy, en-
forcing consistency between them and the ei,t and
ai,j,r variables is not. We address this in section
3.3.1.
Reconstruction of events from solutions (e,a,b)
can be done almost exactly as described by Bj?rne
et al (2009). However, while they group binding
arguments according to ad-hoc rules based on de-
pendency paths from trigger to argument, we simply
query the variables bp,q.
To simplify our exposition we introduce addi-
tional notation. We denote the set of protein head
tokens with Prot (x); the set of a possible targets
for outgoing edges from a trigger is Cand(x) def=
Trig (x) ? Prot (x). We will often omit the do-
mains of indices and instead assign them a fixed do-
main in advance: i, l ? Trig (x), j, k ? Cand (x),
p, q ? Prot (x), r ? R and t ? T . Bold face
letters are used to denote composite vectors e, a
and b of variables ei,t, ai,j,r and bp,q. The vector
y is the joint vector of e,a and b. The short-form
ei ? t will mean ?t? : ei,t? ? ?t,t? where ?t,t? is
the Kronecker Delta. Likewise, ai,j ? r means
?r? : ai,j,r? ? ?r,r? .
3 Models
In this section we will present three structured pre-
diction models of increasing complexity and expres-
siveness, as well as their corresponding MAP infer-
ence algorithms. Each model m can be represented
by a mapping from sentence x to a set of legal struc-
tures Ym (x), and a linear scoring function
sm (y;x,w) = ?w, f (y,x)? . (1)
Here f is a feature function on structures y and input
x, and w is a weight vector for these features.
We can use the scoring function sm and the set of
legal structures Ym (x) to predict the event hm (x)
for a given sentence x according to
hm (x) def= arg max
y?Ym(x)
sm (y;x,w) . (2)
For brevity we will from now on omit observations x
and weights w when they are clear from the context.
3.1 Model 1
Model 1 performs a simple version of joint trigger
and argument extraction. It independently scores
trigger labels and argument roles:
s1 (e,a) def=
?
ei,t=1
sT (i, t) +
?
ai,j,r=1
sR (i, j, r) . (3)
Here sT (i, t) = ?wT, fT (i, t)? is a per-trigger scor-
ing function that measures how well the event la-
bel t fits to token i. Likewise, sR (i, j, r) =
?wR, fR (i, j, r)? measures the compatibility of role
r as label for the edge i? j.
The jointness of Model 1 stems from enforcing
consistency between the trigger label of i and its out-
going edges. By consistency we mean that: (a) there
is at least one Theme whenever there is an event at i;
(b) only regulation events are allowed to have Cause
arguments; (c) all arguments of a None trigger must
have the None role. We will denote the set assign-
ments that fulfill these constraints by O and hence
have Y1 def= O.
Enforcing (e,a) ? O guarantees that we never
predict triggers i for which no sensible, high-
scoring, argument j can be found. It also ensures
that when we see an ?obvious? argument edge i r? j
with high score sR (i, j, r) there is pressure to extract
a trigger at i, even if the fact that i is a trigger may
not be as obvious.
3.1.1 Inference
As it turns out, the maximizer of equation 2 can be
found very efficiently in O (mn) time where m =
|Trig (x)| and n = |Cand (x)|. The corresponding
procedure, bestOut(?), is shown in algorithm 1. It
takes as input a vector of trigger and edge penalties
c that are added to the local scores of the sT and
sR functions. For Model 2 and 3 we will use these
4
penalties to enforce agreement with predictions of
other inference subroutines. When using Model 1
by itself we set them to 0. We point out that the
scoring function s1 is multiplied with 12 throughoutthe algorithm. For doing inference in Model 1 and
2 this has no effect, but when we use bestOut(?) for
Model 3 inference, it is required.
The bestOut (c) routine exploits the fact that the
constraints of Model 1 only act on the label for
trigger i and its outgoing edges. In particular, en-
forcing consistency between ei,t and outgoing edges
ai,j,r has no effect on consistency between el,t and
ai?,j?,r? for any other trigger i? 6= i. Moreover,
for a given trigger the constraints only differenti-
ate between three cases: (a) regulation event, (b)
non-regulation event and (c) no event. This means
that we can extract events on a per-trigger basis,
and find the best per-trigger structure by compar-
ing cases (a), (b) and (c). Note that bestOut (c)
uses the shorthand emptyOut (i) to denote the par-
tial assignment ei ? None and ?j : ai,j ? None.
The function sc1 (i,y) def=
?
t ei,t
(
ci,t + 12sT (i, t)
)
+?
j,r ai,j,r
(
ci,j,r + 12sR (i, j, r)
) is a per-trigger
frame score with penalties c.
3.2 Model 2
Model 1 may still predict structures that cannot be
mapped to events. For example, in figure 1b) we
may label token 5 as Regulation, add the edge
5 Cause? 2 but fail to label token 2 as an event. While
consistent with (e,a) ? O, this violates the con-
straint that every active edge must either end at a
protein, or at an active event trigger. This is a re-
quirement on the label of a trigger and the assign-
ment of roles for its incoming edges.
Model 2 enforces the above constraint in addition
to (e,a) ? O, while inheriting the scoring function
fromModel 1. Hence, using I to denote the set of as-
signments with consistent trigger labels and incom-
ing edges, we get Y2 def= Y1 ? I and s2 (y) def= s1 (y).
3.2.1 Inference
Inference in Model 2 amounts to optimizing
s2 (e,a) over O ? I. This is more involved, as we
now have to ensure that when predicting an outgoing
edge from trigger i to trigger l there is a high-scoring
event at l. We follow Rush et al (2010) and solve
this problem in the framework of dual decomposi-
Algorithm 1 Sub-procedures for inference in Model
1, 2 and 3.
best label and outgoing edges for all triggers under penalties c
bestOut (c) :
?i y0 ? emptyOut (i)
y1 ? out (i, c, Treg,R
)
y2 ? out (i, c, T?reg,R \ {Cause}
)
yi ? argmaxy?{y0,y1,y2} sc1(i,y)
return (yi)i
best label and incoming edges for all triggers under penalties c
bestIn (c) :
?l y0 ? emptyIn (l)
y1 ? in (l, c, T ,R \ {None})
yl ? argmaxy?{y0,y1} sc2 (l,y)
return (yl)l
pick best binding pairs p, q and trigger i for each using penalties c
bestBind (c) :
?p, q bp,q ? [sB (p, q) + maxi ci,p,q > 0]
Ip,q ?
{
i|ci,p,q = maxi? ci?,p,q
}
if bp,q = 1 or maxi? ci?,p,q > 0
?i : ti,p,q ? [i ? Ip,q] |Ip,q|?1
else
?i : ti,p,q ? 0
return (b, t)
best label in T and outgoing edge roles in R for i, using penalties c
out (i, c, T,R) :
ei ? argmaxt?T 12sT (i, t) + ci,t
ai,bestTheme(i,c) ? Theme
?j ai,j ? argmaxr?R 12sR (i, j, r) + ci,j,rreturn (ei,ai)
best label in T , incoming edge roles in R
and outgoing protein roles, using costs c
in (l, c, T,R) :
el ? argmaxt?T 12sT (l, t) + cl,t
?i ai,l ? argmaxr?R 12sR (i, l, r) + ci,l,r
?p al,p ? argmaxr?R 12sR (l, p, r) + cl,p,rreturn (ei,ai)
best Theme argument for i
bestTheme (i, c) :
s (j) def= maxj,r 12sR (i, j, r) + ci,j,r
?(j) def= 12sR (i, j, Theme) + ci,j,Theme ? s (j)return argmaxj ?(j)
5
tion. To this end we write our optimization problem
as
maximize
e,a,e?,a?
1
2s2 (e,a) +
1
2s2 (e?, a?)
subject to (e,a) ? O ? (e?, a?) ? I?
e = e? ? a = a?
(M2)
and note that this problem could be solved separately
for e,a and e?, a? if the coupling constraints e = e?
and a = a? were removed.
M2 is an Integer Linear Program, as variables are
binary and both objective and constraints can be rep-
resented through linear constraints.1 Dual decompo-
sition solves a Linear Programming (LP) relaxation
of M2 (that allows fractional values for all binary
variables) through subgradient descent on a particu-
lar dual of M2. This dual can be derived by intro-
ducing Lagrange multipliers for the coupling con-
straints. Its attractiveness stems from the fact that
calculating the subgradient amounts to solving the
decoupled problems in isolation. If, by design, these
decoupled problems can be solved efficiently, we
can often quickly find the optimal solution to an LP
relaxation of our original problem.
Dual decomposition applied to Model 2 is shown
in algorithm 2. It maintains the dual variables ?
that will appear as local penalties in the subprob-
lems to be solved. The algorithm will try to tune
these variables such that at convergence the coupling
constraints will be fulfilled. This is done by first op-
timizing s2 (e,a) over O and s2 (e?, a?) over I. Now,
whenever there is disagreement between two vari-
ables to be coupled, the corresponding dual param-
eter is shifted, increasing the chance that next time
both models will agree. For example, if in the first
iteration we predict e6,Bind = 1 but e?6,Bind = 0, we
set ?6,Bind = ?? where ? is some stepsize (chosen
according to Koo et al (2010)). This will decrease
the coefficient for e6,Bind, and increase the coeffi-
cient for e?6,Bind. Hence, we have a higher chance of
agreement for this variable in the next iteration.
The algorithm repeats the process described
above until all variables agree, or some predefined
numberR of iterations is reached. In the former case
we in fact have the exact solution to the original ILP.
1The ILP representation could be taken from the MLNs of
Riedel et al (2009) and the mapping to ILPs of Riedel (2008).
Algorithm 2 Subgradient descent for Model 2, and
projected subgradient descent for Model 3.
require:
R: max. iteration, ?t: stepsizes
t? 0 [model 2,3] ?? 0 [model 2,3] ?? 0 [model 3]
repeat
model
2 (e,a)? bestOut (?)
2,3 (e?, a?)? bestIn (??)
3 (e,a)? bestOut (cout (?,?))
3 (b, t)? bestBind (cbind (?))
2,3 ?i,t ? ?i,t ? ?t (ei,t ? e?i,t)
2,3 ?i,j,r ? ?i,j,r ? ?t (ai,j,r ? a?i,j,r)
3 ?trigi,p,q ?
[
?trigi,p,q ? ?t (ei,Bind ? ti,p,q)
]
+
3 ?arg1i,j,k ?
[
?arg1i,p,q ? ?t (ai,p,Theme ? ti,p,q)
]
+
3 ?arg2i,p,q ?
[
?arg2i,p,q ? ?t (ai,q,Theme ? ti,p,q)
]
+
2,3 t ? t + 1
until no ?, ? changed or t > R
return (e,a)[model 2] or (e,a,b) [model 3]
In the later case we have no such guarantee, but find
that in practice the solutions are still of high qual-
ity. Notice that we could still assess the quality of
this approximation by measuring the duality gap be-
tween primal score and the final dual score.
Algorithm 2 for Model 2 requires us to opti-
mize s2 (e,a) over O and s2 (e?, a?) over I. The
former, with added penalties, can be done with
bestOut(c). As the constraint set for I again
decomposes on a per-token basis, solving the
latter problem requires a very similar procedure,
and again O (mn) time. Algorithm 1 shows this
procedure under bestIn(c). It chooses, for each
trigger candidate, the best label and incoming
set of arguments together with the best outgoing
edges to proteins. Adding edges to proteins is
not strictly required, but simplifies our exposition.
Algorithm bestIn(c) requires a per-trigger incoming
score: sc2 (l,yl) def=
?
t el,t
(
cl,t + 12sT (l, t)
)
+?
i,r ai,l,r
(
ci,l,r + 12sR (i, l, r)
)
+?
p,r al,p,r
(
cl,p,r + 12sR (l, p, r)
)
. Finally, note
that emptyIn (i) not only assigns None as trigger la-
bel of i and to all incoming edges, but also greedily
picks outgoing protein edges (as done within in(?)).
6
3.3 Model 3
Model 2 does not predict the bp,q variables that rep-
resent protein pairs p, q in bindings. Model 3 fixes
this by (a) adding binding variables bp,q into the ob-
jective, and (b) enforcing that the binding assign-
ment b is consistent with the trigger and argument
assignments e and a. We will also enforce that the
same pair of entities p, q cannot be arguments in
more than one event together.
The scoring function for Model 3 is simply
s3 (e,a,b) def= s2 (e,a,b) +
?
bp,q=1
sB (p, q) . (4)
Here sB (p, q) = ?wB, fB (p, q)? is a per-protein-pair
score based on a feature representation of the lexical
and syntactic relation between both protein heads.
Our strategy will be based on enforcing consis-
tency partly through linear constraints which we du-
alize, and partly within our search algorithm. To
this end we first introduce a set of auxiliary binary
variables ti,p,q . When a ti,p,q is active, we enforce
that there is a binding trigger at i with proteins p
and q as Theme arguments. A set of linear con-
straints can be used for this: ei,Bind ? ti,p,q ? 0,
ai,p,Theme ? ti,p,q ? 0 and ai,q,Theme ? ti,p,q ? 0 for
all suitable i, p and q. We denote the set of assign-
ments (e,a, t) that fulfill these constraints by T.
Consistency between e, a and b can now be en-
forced by making sure that t is consistent with e and
a, and that b is consistent with this t. The latter
means that an active bp,q requires a trigger i to point
to p and q. Or in other words, ti,p,q = 1 for exactly
one trigger i.
With the set of consistent assignments (b, t) re-
ferred to as B, and a slight abuse of notation, this
gives us Y3 def= Y2?T?B. Note that it is (e,a, t) ? T
that will be enforced by dualizing constraints, and
(b, t) ? B that will be enforced within search.
3.3.1 Inference
We note that inference in Model 3 can be per-
formed by solving the following problem:
maximize
e,a,e?,a?,b,t
1
2s1 (e,a) +
1
2s2 (e?, a?) +
?
bp,q=1
sB (p, q)
subject to (e,a) ? O ? (e?, a?) ? I ? (b, t) ? B?
e = e? ? a = a? ? (e,a, t) ? T.
(M3)
Again, without the final row, M3 would be separa-
ble. We exploit this by performing dual decompo-
sition with a dual objective that has multipliers ?
for the coupling constraints and multipliers? for the
constraints which enforce (e,a, t) ? T. The result-
ing subgradient descent method is also shown in al-
gorithm 2. Notably, since the constraints for T are
inequalities, we require a projected version of the
descent algorithm which enforces ? ? 0. This man-
ifests itself when ? is updated using the [?]+ projec-
tion.
We have already described how to find the best
e,a and e?, a? assignments. What changes for Model
3 is the derivation of the penalties for e and a
that now come from both ? and ?. We set
couti,t (?,?)
def= ?i,t + ?t,Bind
?
p,q ?
trig
i,p,q. For j /?
Prot (x) we set couti,j,r (?,?) def= ?i,j,r; otherwise we
use couti,j,r (?,?) def= ?i,j,r +
?
p ?
arg1
i,j,p +
?
q ?
arg2
i,q,j .
For finding a (b, t) ? B that maximizes?
bp,q=1 sB (p, q) we use bestBind (c), as shown inalgorithm 1. It groups together two proteins p, q if
their score plus the penalty of the best possible trig-
ger i exceeds 0. In this case, or if there is at least one
trigger with positive penalty ci,p,q > 0 , we activate
the set of triggers I (p, q) with maximal score.
Note that when several triggers i maximize the
score, we assign them all the same fractional value
|I (p, q)|?1. This enforces the constraint that at most
one binding event can point to both p and q and also
means that we are solving an LP relaxation. We
could enforce integer solutions and pick arbitrary
triggers at a tie, but this would lower the chances
of matching against predictions of other routines.
The penalties for bestBind (c) are derived from
the dual ? by setting cbindi,p,q (?) = ??trigi,p,q ? ?arg1i,p,q ?
?arg2i,,p,q.
3.4 Training
We choose prediction-based passive-aggressive (PA)
online learning (Crammer and Singer, 2003) with
averaging to estimate the weights w for each of our
models. PA is an error-driven learner that shifts
weights towards features of the gold solution, and
away from features of the current guess, whenever
the current model makes a mistake.
PA learning takes into account a user-defined
loss function for which we use a weighted sum
7
of false positives and false negatives: l (y,y?) def=
FP (y,y?) + ?FN (y,y?). We set ? = 3.8 by op-
timizing on the BioNLP 2009 development set.
4 Related Work
Riedel et al (2009) use Integer Linear Programming
and cutting planes (Riedel, 2008) for inference in
a model similar to Model 2. By using dual de-
composition instead, we can exploit tractable sub-
structure and achieve quadratic (Model 2) and cu-
bic (Model 3) runtime guarantees. An advantage of
ILP inference are guaranteed certificates of optimal-
ity. However, in practice we also gain certificates
of optimality for a large fraction of the instances
we process. Poon and Vanderwende (2010) use lo-
cal search and hence provide no such certificates.
Their problem formulation also makes n-gram de-
pendency path features harder to incorporate. Mc-
Closky et al (2011b) cast event extraction as depen-
dency parsing task. Their model assumes that event
structures are trees, an assumption that is frequently
violated in practice. Finally, all previous joint ap-
proaches use heuristics to decide whether binding
arguments are part of the same event, while we cap-
ture these decisions in the joint model.
We follow a long line of research in NLP that ad-
dresses search problems using (Integer) Linear Pro-
grams (Germann et al, 2001; Roth and Yih, 2004;
Riedel and Clarke, 2006). However, instead of us-
ing off-the-shelf solvers, we work in the framework
of dual decomposition. Here we extend the approach
of Rush et al (2010) in that in addition to equality
constraints we dualize more complex coupling con-
straints between models. This requires us to work
with a projected version of subgradient descent.
While tailored towards (biomedical) event extrac-
tion, we believe that our models can also be ef-
fective in a more general Semantic Role Label-
ing (SRL) context. Using variants of Model 1,
we can enforce many of the SRL constraints?such
as ?unique agent? constraints (Punyakanok et al,
2004)?without having to call out to ILP optimiz-
ers. Meza-Ruiz and Riedel (2009) showed that in-
ducing pressure on arguments to be attached to at
least one predicate is helpful; this is a soft incoming
edge constraint. Finally, Model 3 can be used to effi-
ciently capture compatibilities between semantic ar-
guments; such compatibilities have also been shown
to be helpful in SRL (Toutanova et al, 2005).
5 Experiments
We evaluate our models on several tracks of the 2009
and 2011 BioNLP shared tasks, using the official
?Approximate Span Matching/Approximate Recur-
sive Matching? F1 metric for each. We also investi-
gate the runtime behavior of our algorithms.
5.1 Preprocessing
Each document is first processed by the Stanford
CoreNLP2 tokenizer and sentence splitter. Parse
trees come from the Charniak-Johnson parser (Char-
niak and Johnson, 2005) with a self-trained biomed-
ical parsing model (McClosky and Charniak, 2008),
and are converted to dependency structures again us-
ing Stanford CoreNLP. Based on trigger words col-
lected from the training set, a set of candidate trigger
tokens Trig (x) is generated for each sentence x.
5.2 Features
The feature function fT (i, t) extracts a per-trigger
feature vector for trigger i and type t ? T .
It creates one active feature for each element in{
t, t ? TReg
}
? feats (i). Here feats (i) denotes a
collection of representations for the token i: word-
form, lemma, POS tag, syntactic heads, syntactic
children, and membership in two dictionaries taken
from Riedel et al (2009).
For fR (i, j, r) we create active features for each
element of {r} ? feats (i, j). Here feats (i, j) is
a collection of representations of the token pair
(i, j) taken from Miwa et al (2010c) and contains:
labelled and unlabeled n-gram dependency paths;
edge and vertex walk features, argument and trigger
modifiers and heads, words in between.
For fB (p, q) we re-use the token pair representa-
tions from fR. In particular, we create one active
feature for each element in feats (p, q).
5.3 Shared Task 2009
We first evaluate our models on the Bionlp 2009 task
1. The training, development and test sets for this
2http://nlp.stanford.edu/software/
corenlp.shtml
8
SVT BIND REG TOT
McClosky 75.4 48.4 40.4 53.5
Poon 77.5 47.9 44.1 55.5
Bjoerne 77.9 42.2 45.5 55.7
Miwa 78.6 46.9 47.7 57.8
M1 77.2 43.0 45.8 56.2
M2 77.9 42.4 47.6 57.2
M3 78.4 48.0 49.1 58.7
Table 1: F1 scores for the development set of Task 1 of
the BioNLP 2009 shared task.
task consist of 797, 150 and 250 documents, respec-
tively.
Table 1 shows our results for the development set.
We compare our three models (M1, M2 andM3) and
previous state-of-the-art systems: McClosky (Mc-
Closky et al, 2011a), Poon (Poon and Vander-
wende, 2010), Bjoerne (Bj?rne et al, 2009) and
Miwa (Miwa et al, 2010b; Miwa et al, 2010a). Pre-
sented is F1 score for all events (TOT), regulation
events (REG), binding events (BIND) and simple
events (SVT).
Model 1 is outperforming the previous best joint
models of Poon and Vanderwende (2010), as well as
the best entry of the 2009 task (Bj?rne et al, 2009).
This is achieved without careful tuning of thresh-
olds that control flow of information between trigger
and argument extraction. Notably, training Model 1
takes approximately 20 minutes using a single core
implementation. Contrast this with 20 minutes on 32
cores reported by Poon and Vanderwende (2010).
Model 2 focuses on regulation structures and re-
sults demonstrate this: F1 for regulations goes up by
nearly 2 points. While the impact of joint modeling
relative to weaker local baselines has been shown
shown by Poon and Vanderwende (2010) and Riedel
et al (2009), our findings here provide evidence that
it remains effective even when the baseline system
is very competitive.
With Model 3 our focus is extended to binding
events, improving F1 for such events by at least 5 F1.
This also has a positive effect on regulation events,
as regulations of binding events can now be more
accurately extracted. In total we see a 1.1 F1 in-
crease over the best results reported so far (Miwa et
al., 2010b). Crucially, this is achieved using only a
single parse tree per sentence, as opposed to three
SVT BIND REG TOT
McClosky 68.3 46.9 33.3 48.6
Poon 69.5 42.5 37.5 50.0
Bjoerne 70.2 44.4 40.1 52.0
Miwa 72.1 50.6 45.3 56.3
M1 71.0 42.1 41.9 53.4
M2 70.5 41.3 43.6 53.7
M3 71.1 52.9 45.2 55.8
M3+enju 72.6 52.6 46.9 57.4
Table 2: F1 scores for the test set of Task 1 of the BioNLP
2009 shared task.
used by Miwa et al (2010a).
Table 2 shows results for the test set. Here with
Model 1 we again already outperform all but the re-
sults of Miwa et al (2010a). Model 2 improves F1
for regulations, while Model 3 again increases F1
for both regulations and binding events. This yields
the best binding event results reported so far. No-
tably, not only are we able to resolve binding am-
biguity better. Binding attachments themselves also
improve, as we increase attachment F1 from 61.4 to
62.7 when going from Model 2 to Model 3.
Miwa et al (2010b) use two parsers to generate
their input features. For fairer comparison we aug-
ment Model 3 with syntactic features based on the
enju parser (Miyao et al, 2009). With these features
(M3+enju) we achieve the best results on this dataset
reported so far, and outperform Miwa et al (2010b)
by 1.1 F1 in total, 1.6 F1 on regulation events and
2.0 F1 on binding events.
We also apply Model 3, with slight modifications,
to the BioNLP 2009 task 2 which requires cellu-
lar locations to be extracted as well. With 53.0 F1
we fall 2 points short of the results of Miwa et al
(2010b) but still substantially outperform any other
reported results on the dataset. More parse trees may
again substantially improve results, as well as task-
specific constraint and feature sets.
5.4 Shared Task 2011
We entered the Shared Task 2011 with Model 3,
primarily focusing on Genia track (task 1), and the
Infectious Diseases track. The Genia track differs
from the 2009 task by including both abstracts and
full text articles. In total 908 training, 259 develop-
ment and 347 test documents are provided.
9
Genia Task 1 Infectious Diseases
System TOT System TOT
M3+Stanford 56.0 M3+Stanford 55.6
M3 55.2 M3 53.4
UTurku 53.3 Stanford 50.6
MSR-NLP 51.5 UTurku 44.2
ConcordU 50.3 PNNL 42.6
Table 3: F1 scores for the test sets of two tracks in the
BioNLP 2011 Shared Task.
The top five entries are shown in table 3. Model
3 is the best-performing system that does not use
model combination, only outperformed by a version
of Model 3 that includes Stanford predictions (Mc-
Closky et al, 2011b) as input features (Riedel et al,
2011). Not shown in the table are results for full pa-
pers only. Here M3 ranks first with 53.1 F1, while
M3+Stanford comes in second with 52.7 F1.
The Infectious Diseases (ID) track of the 2011
task has 152 train, 46 development and 118 test
documents. Relative to Genia it provides less data
and introduces more types of entities as well as
the biological process event type. Incorporating
these changes into our models is straightforward,
and hence we omit details for brevity.
Table 3 shows the top five entries for the Infec-
tious Diseases track. Again Model 3 is the best-
performing system that does not use model combi-
nation, outperformed only by Model 3 with Stanford
predictions as features. We should point out that
the feature sets and learning parameters were kept
constant when moving from Genia to ID data. The
strong results we observe without any tuning to the
domain indicate the robustness of joint modeling.
5.5 Runtime Behavior
Table 4 shows the asymptotic complexity of our
three models with respect to m = |Trig (x)|, n =
|Cand (x)| and p = |Prot (x)|. We also show the
number of iterations needed on average, the average
time in milliseconds per sentence,3 and the fraction
of sentences we get certificates of optimality for.
As expected, Model 1 is most efficient, both
asymptotically and on average. Given that its ac-
curacy is already good, it can serve as a basis for
3Measured without preprocessing and feature extraction.
Complexity Iter. Time Exact
M1 O (nm) 1.0 60ms 100%
M2 O (Rnm) 10.4 183ms 96%
M3 O (Rnm + Rp2m) 11.7 297ms 94%
Table 4: Complexity and Runtime Behavior.
large-scale extraction tasks. Models 2 and 3 re-
quire several iterations and more time, while pro-
viding slightly less certificates. However, given the
improvement in F1 they deliver, and the fact prepro-
cessing steps such as parsing would still dominate
the average time, this seems like a reasonable price
to pay.
6 Conclusion
We presented three joint models for biomedical
event extraction. Model 1 reaches near-state-of-the-
art results, outperforms all previous joint models
and has quadratic runtime guarantees. By explicitly
capturing regulation events (Model 2), and binding
events (Model 3) we achieve the best results reported
so far on several event extraction tasks. The runtime
penalty we pay is kept minimal by using dual de-
composition. We also show how dual decomposition
can be used for constraints that go beyond coupling
equalities.
We use joint models, a decomposition technique
and supervised online learning. This recipe can be
successful in many settings, but requires expensive
manual annotation. In the future we want to inte-
grate weak supervision techniques to train extractors
with existing biomedical databases, such as KEGG,
and only minimal amounts of annotated text.
Acknowledgements
This work was supported in part by the Center
for Intelligent Information Retrieval. The Univer-
sity of Massachusetts gratefully acknowledges the
support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
10
References
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the Natural Language
Processing in Biomedicine NAACL 2009 Workshop
(BioNLP ?09), pages 10?18, Morristown, NJ, USA.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ?05),
pages 173?180.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL ?01), pages 228?235.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of bionlp?09 shared task on event extraction. In
Proceedings of the Natural Language Processing in
Biomedicine NAACL 2009 Workshop (BioNLP ?09).
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In Proceedings of the 11st
IEEE International Conference on Computer Vision
(ICCV ?07).
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with nonprojective head automata. In
Proceedings of the Conference on Empirical methods
in natural language processing (EMNLP ?10).
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL ?08).
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11), Port-
land, Oregon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in bionlp 2011. In BioNLP 2011 Shared Task.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ?06), pages 81?88.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In Joint Human Language Technol-
ogy Conference/Annual Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL ?09).
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, BioNLP ?10, pages 37?45, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010b. Evaluating dependency rep-
resentation for event extraction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, COLING ?10, pages 779?787, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Makoto Miwa, Rune Saetre, Jin-Dong D. Kim, and
Jun?ichi Tsujii. 2010c. Event extraction with com-
plex event classification using rich features. Journal of
bioinformatics and computational biology, 8(1):131?
146, February.
Yusuke Miyao, Kenji Sagae, Rune S?tre, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2009. Evaluating contribu-
tions of natural language parsers to protein-protein in-
teraction extraction. Bioinformatics/computer Appli-
cations in The Biosciences, 25:394?400.
Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Lit-
erature. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
813?821, Los Angeles, California, June. Association
for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Proceedings of the 20th in-
ternational conference on Computational Linguistics
(COLING ?04), pages 1346?1352, Morristown, NJ,
USA. Association for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?06), pages 129?137.
Sebastian Riedel and Andrew McCallum. 2011. Robust
biomedical event extraction with dual decomposition
and minimal domain adaptation. In Proceedings of the
11
Natural Language Processing in Biomedicine NAACL
2011 Workshop (BioNLP ?11), June.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ?09), pages 41?49.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Christopher D. Manning, and Andrew McCallum.
2011. Model combination for event extraction in
BioNLP 2011. In Proceedings of the Natural Lan-
guage Processing in Biomedicine NAACL 2011 Work-
shop (BioNLP ?11), June.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ?08), pages 468?475.
D. Roth andW. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of the 8th Conference on Computational
Natural Language Learning (CoNLL? 04), pages 1?8.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?10).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics (ACL
?05), pages 589?596, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
12
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456?1466,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Relation Discovery using Generative Models
Limin Yao? Aria Haghighi+ Sebastian Riedel? Andrew McCallum?
? Department of Computer Science, University of Massachusetts at Amherst
+ CSAIL, Massachusetts Institute of Technology
{lmyao,riedel,mccallum}@cs.umass.edu
{aria42}@csail.mit.edu
Abstract
We explore unsupervised approaches to rela-
tion extraction between two named entities;
for instance, the semantic bornIn relation be-
tween a person and location entity. Con-
cretely, we propose a series of generative
probabilistic models, broadly similar to topic
models, each which generates a corpus of ob-
served triples of entity mention pairs and the
surface syntactic dependency path between
them. The output of each model is a cluster-
ing of observed relation tuples and their as-
sociated textual expressions to underlying se-
mantic relation types. Our proposed models
exploit entity type constraints within a relation
as well as features on the dependency path be-
tween entity mentions. We examine effective-
ness of our approach via multiple evaluations
and demonstrate 12% error reduction in preci-
sion over a state-of-the-art weakly supervised
baseline.
1 Introduction
Many NLP applications would benefit from large
knowledge bases of relational information about
entities. For instance, knowing that the entity
Steve Balmer bears the leaderOf relation to the
entity Microsoft, would facilitate question answer-
ing (Ravichandran and Hovy, 2002), data mining,
and a host of other end-user applications. Due to
these many potential applications, relation extrac-
tion has gained much attention in information ex-
traction (Kambhatla, 2004; Culotta and Sorensen,
2004; Mintz et al, 2009; Riedel et al, 2010; Yao et
al., 2010). We propose a series of generative prob-
abilistic models, broadly similar to standard topic
models, which generate a corpus of observed triples
of entity mention pairs and the surface syntactic de-
pendency path between them. Our proposed mod-
els exploit entity type constraints within a relation
as well as features on the dependency path between
entity mentions. The output of our approach is a
clustering over observed relation paths (e.g. ?X was
born in Y? and ?X is from Y?) such that expressions
in the same cluster bear the same semantic relation
type between entities.
Past work has shown that standard supervised
techniques can yield high-performance relation de-
tection when abundant labeled data exists for a
fixed inventory of individual relation types (e.g.
leaderOf ) (Kambhatla, 2004; Culotta and Sorensen,
2004; Roth and tau Yih, 2002). However, less ex-
plored are open-domain approaches where the set
of possible relation types are not fixed and little to
no labeled is given for each relation type (Banko et
al., 2007; Banko and Etzioni, 2008). A more re-
lated line of research has explored inducing rela-
tion types via clustering. For example, DIRT (Lin
and Pantel, 2001) aims to discover different repre-
sentations of the same semantic relation using dis-
tributional similarity of dependency paths. Poon
and Domingos (2008) present an Unsupervised se-
mantic parsing (USP) approach to partition depen-
dency trees into meaningful fragments (or ?parts?
to use their terminology). The combinatorial nature
of this dependency partition model makes it difficult
for USP to scale to large data sets despite several
necessary approximations during learning and infer-
1456
ence. Our work is similar to DIRT and USP in that
we induce relation types from observed dependency
paths, but our approach is a straightforward and
principled generative model which can be efficiently
learned. As we show empirically, our approach out-
performs these related works when trained with the
same amount of data and further gains are observed
when trained with more data.
We evaluate our approach using ?intrinsic? clus-
tering evaluation and ?extrinsic? evaluation settings.1
The former evaluation is performed using subset of
induced clusters against Freebase relations, a large
manually-built entity and relational database. We
also show some clusters which are not included as
Freebase relations, as well as some entity clusters
found by our approach. The latter evaluation uses
the clustering induced by our models as features for
relation extraction in distant supervision framework.
Empirical results show that we can find coherent
clusters. In relation extraction, we can achieve 12%
error reduction in precision over a state-of-the-art
weakly supervised baseline and we show that using
features from our proposed models can find more
facts for a relation without significant accuracy loss.
2 Problem and Experimental Setup
The task of relation extraction is mapping surface
textual relations to underlying semantic relations.
For instance, the textual expression ?X was born in
Y? indicates a semantic relation bornIn between en-
tities ?X? and ?Y?. This relation can be expressed
textually in several ways: for instance, ?X, a native
of Y? or ?X grew up in Y?. There are several com-
ponents to a coherent relation type, including a tight
small number of textual expressions as well as con-
straints on the entities involved in the relation. For
instance, in the bornIn relation ?X? must be a person
entity and ?Y? a location (typically a city or nation).
In this work, we present an unsupervised probabilis-
tic generative model for inducing clusters of relation
types and recognizing their textual expressions. The
set of relation types is not pre-specified but induced
from observed unlabeled data. See Table 4 for ex-
amples of learned semantic relations.
Our observed data consists of a corpus of docu-
ments and each document is represented by a bag
1See Section 4 for a fuller discussion of evaluation.
of relation tuples. Each tuple represents an ob-
served syntactic relationship between two Named
Entities (NE) and consists of three components: the
dependency path between two NE mentions, the
source argument NE, and the destination argument
NE. A dependency path is a concatenation of depen-
dency relations (edges) and words (nodes) along a
path in a dependency tree. For instance, the sentence
?John Lennnon was born in Liverpool? would yield
the relation tuple (Lennon, [? ?nsubjpass, born, ?
?in], Liverpool). This relation tuple reflects a se-
mantic bornIn relation between the John Lennon and
Liverpool entities. The dependency path in this ex-
ample corresponds to the ?X was born in Y? textual
expression given earlier. Note that for the above ex-
ample, the bornIn relation can only occur between a
person and a location. The relation tuple is the pri-
mary observed random variable in our model and we
construct our models (see Section 3) so that clusters
consist of textual expressions representing the same
underlying relation type.
3 Models
We propose three generative models for modeling
tuples of entity mention pairs and the syntactic de-
pendency path between them (see Section 2). The
first two models, Rel-LDA and Rel-LDA1 are sim-
ple extensions of the standard LDA model (Blei et
al., 2003). At the document level, our model is iden-
tical to standard LDA; a multinomial distribution
is drawn over a fixed number of relation types R.
Changes lie in the observations. In standard LDA,
the atomic observation is a word drawn from a la-
tent topic distribution determined by a latent topic
indicator variable for that word position. In our ap-
proach, a document consists of an exchangeable set
of relation tuples. Each relation tuple is drawn from
a relation type ?topic? distribution selected by a la-
tent relation type indicator variable. Relation tuples
are generated using a collection of independent fea-
tures drawn from the underlying relation type distri-
bution. These changes to standard LDA are intended
to have the effect that instead of representing seman-
tically related words, the ?topic? latent variable rep-
resents a relation type.
Our third model exploits entity type constraints
within a relation and induces clusters of relations
1457
and entities jointly. For each tuple, a set of rela-
tion level features and two latent entity type indica-
tors are drawn independently from the relation type
distribution; a collection of entity mention features
for each argument is drawn independently from the
entity type distribution selected by the entity type
indicator.
Path X, made by Y
Source Gamma Knife
Dest Elekta
Trigger make
Lex , made by the Swedish
medical technology firm
POS , VBN IN DT JJ JJ NN NN
NER pair MISC-ORG
Sync pair partmod-pobj
Table 1: The features of tuple ?(Gamma Knife, made
by, Elekta)? in sentence ?Gamma Knife, made by the
Swedish medical technology firm Elekta, focuses low-
dosage gamma radiation ...?
3.1 Rel-LDA Model
This model is an extension to the standard LDA
model. At the document level, a multinomial dis-
tribution over relations ?doc is drawn from a prior
Dir(?). To generate a relation tuple, we first draw a
relation ?topic? r from Multi(?). Then we generate
each feature f of a tuple independently from a multi-
nomial distribution Multi(?rf ) selected by r. In this
model, each tuple has three features, i.e. its three
components, shown in the first three rows in Table 1.
Figure 1 shows the graphical representation of Rel-
LDA. Table 2 lists all the notation used in describing
our models.
The learning process of the models is an EM pro-
cess. The procedure is similar to that used by the
standard topic model. In the variational E-step (in-
ference), we sample the relation type indicator for
each tuple using p(r|f):
P (r|f(p, s, d)) ? p(r)?f p(f |r)
? (?r + nr|d)
?
f
?f+nf |rP
f ? (?f ?+nf ?|r)
|R| Number of relations
|D| Number of documents
r A relation
doc A document
p, s, d Dep path, source and dest args
f A feature/feature type
T Entity type of one argument
? Dirichlet prior for ?doc
?x Dirichlet prior for ?rx
? Dirichlet prior for ?t
?doc p(r|doc)
?rx p(x|r)
?t p(fs|T ), p(fd|T )
Table 2: The notation used in our models
      
  
             |R|  
      
  
  
......
                                        N 
r
f
?
?
rf
?
?
f
f
      
  
  
                           
                           
                                             |D|  
Figure 1: Rel-LDA model. Shaded circles are observa-
tions, and unshaded ones are hidden variables. A docu-
ment consists of N tuples. Each tuple has a set of fea-
tures. Each feature of a tuple is generated independently
from a hidden relation variable r.
p(r) and p(f |r) are estimated in the M-step:
?doc =
?+ nr|doc?
r?(?+ nr?|doc)
?rf =
?f + nf |r?
f ?(?f ? + nf ?|r)
where nf |r indicates the number of times a feature f
is assigned with r.
3.2 Rel-LDA1
Looking at results of Rel-LDA, we find the clus-
ters sometimes are in need of refinement, and we
can address this by adding more features. For in-
stance, adding trigger features can encourage spar-
sity over dependency paths. We define trigger words
as all the words on the dependency path except stop
words. For example, from path ?X, based in Y?,
?base? is extracted as a trigger word. The intuition
1458
for using trigger words is that paths sharing the same
set of trigger words should go to one cluster. Adding
named entity tag pair can refine the cluster too. For
example, a cluster found by Rel-LDA contains ?X
was born in Y? and ?X lives in Y?; but it also con-
tains ?X, a company in Y?. In this scenario, adding
features ?PER-LOC? and ?ORG-LOC? can push the
model to split the clusters into two and put the third
case into a new cluster.
Hence we propose Rel-LDA1. It is similar to
Rel-LDA, except that each tuple is represented with
more features. Besides p, s, and d, we introduce
trigger words, lexical pattern, POS tag pattern, the
named entity pair and the syntactic category pair fea-
tures for each tuple. Lexical pattern is the word se-
quence between the two arguments of a tuple and
POS tag pattern is the POS tag sequence of the lexi-
cal pattern. See Table 1 as an example.
Following typical EM learning(Charniak and El-
sner, 2009), we start with a much simpler genera-
tive model, expose the model to fewer features first,
and iteratively add more features. First, we train a
Rel-LDA model, i.e. the model only generates the
dependency path, source and destination arguments.
After each interval of 10 iterations, we introduce one
additional feature. We add the features in the order
of trigger, lexical pattern, POS, NER pair, and syn-
tactic pair.
3.3 Type-LDA model
We know that relations can only hold between
certain entity types, known as selectional prefer-
ences (Ritter et al, 2010; Seaghdha, 2010; Kozareva
and Hovy, 2010). Hence we propose Type-LDA
model. This model can capture the selectional pref-
erences of relations to their arguments. In the mean
time, it clusters tuples into relational clusters, and
arguments into different entity clusters. The entity
clusters could be interesting in many ways, for ex-
ample, defining fine-grained entity types and finding
new concepts.
We split the features of a tuple into relation level
features and entity level features. Relation level fea-
tures include the dependency path, trigger, lex and
POS features; entity level features include the entity
mention itself and its named entity tag.
The generative storyline is as follows. At the doc-
ument level, a multinomial distribution over rela-
      
  
  
                                        N   
      
  
  
        
                      
                                                   |D|  
      
  
             |R|  
r
f
f
s
?
?
rf
?
t
f
d
     
  
              |R| 
?
rt2
?
?
t2
?
?
f
T
1
T
2
     
  
              |T| 
     
  
              |R| 
?
rt1
?
t1
Figure 2: Type-LDA model. Each document consists of
N tuples. Each tuple has a set of features, relation level
features f and entity level features of source argument fs
and destination argument fd. Relation level features and
two hidden entity types T1 and T2 are generated from
hidden relation variable r independently. Source entity
features are generated from T1 and destination features
are generated from T2.
tions ?doc is drawn from a Dirichlet prior. A doc-
ument consists of N relation tuples. Each tuple is
represented by relation level features (f ) and entity
level features of source argument (fs) and destina-
tion argument (fd). For each tuple, a relation r is
drawn from Multi(?doc). The relation level features
and two hidden entity types T1 and T2 are indepen-
dently generated from r. Features fs are generated
from T1 and fd from T2. Figure 2 shows the graphi-
cal representation of this model.
At inference time, we sample r, T1 and T2 for
each tuple. For efficient inference, we first initialize
the model without T1 and T2, i.e. all the features are
generated directly from r. Here the model degener-
ates to Rel-LDA1. After some iterations, we intro-
duce T1 and T2. We sample the relation variable (r)
and two mention types variables (T1,T2) iteratively
for each tuple. We can sample them together, but
this is not very efficient. In addition, we found that
it does not improve performance.
4 Experiments
Our experiments are carried out on New York Times
articles from year 2000 to 2007 (Sandhaus, 2008).
We filter out some noisy documents, for example,
1459
obituary content, lists and so on. Obituary arti-
cles often contain syntax that diverges from stan-
dard newswire text. This leads to parse errors with
WSJ-trained parsers and in turn, makes extraction
harder. We also filter out documents that contain
lists or tables of items (such as books, movies) be-
cause this semi-structured information is not the fo-
cus of our current work. After filtering we are left
with approximately 428K documents. They are pre-
processed in several steps. First we employ Stanford
tools to tokenize, sentence-split and Part-Of-Speech
tag (Toutanova et al, 2003) a document. Next we
recognize named entities (Finkel et al, 2005) by
labelling tokens with PERSON, ORGANIZATION,
LOCATION, MISC and NONE tags. Consecutive
tokens which share the same category are assembled
into entity mentions. They serve as source and des-
tination arguments of the tuples we seek to model.
Finally we parse each sentence of a document using
MaltParser (Nivre et al, 2004) and extract depen-
dency paths for each pair of named entity mentions
in one sentence.
Following DIRT (Lin and Pantel, 2001), we fil-
ter out tuples that do not satisfy the following con-
straints. First, the path needs to be shorter than
10 edges, since longer paths occur less frequently.
Second, the dependency relations in the path should
connect two content words, i.e. nouns, verbs, ad-
jectives and adverbs. For example, in phrase ?solve
a problem?, ?obj(solve, problem)? is kept, while
?det(problem, a)? is discarded. Finally, the de-
pendency labels on the path must not be: ?conj?,
?ccomp?, ?parataxis?, ?xcomp?, ?pcomp?, ?advcl?,
?punct?, and ?infmod?. This selection is based on the
observation that most of the times the corresponding
dependency relations do not explicitly state a rela-
tion between two candidate arguments.
After all entity mentions are generated and paths
are extracted, we have nearly 2.5M tuples. After
clustering (inference), each of these tuple will be-
long to one cluster/relation and is associated with its
clusterID.
We experimented with the number of clusters and
find that in a range of 50-200 the performance does
not vary significantly with different numbers. In our
experiments, we cluster the tuples into 100 relation
clusters for all three models. For Type-LDA model,
we use 50 entity clusters.
We evaluate our models in two ways. The first
aims at measuring the clustering quality by mapping
clusters to Freebase relations. The second seeks to
assess the utility of our predicted clusters as features
for relation extraction.
4.1 Relations discovered by different models
Looking closely at the clusters we predict, we find
that some of them can be mapped to Freebase rela-
tions. We discover clusters that roughly correspond
to the parentCom (parent company relation), filmDi-
rector, authorOf, comBase (base of a company rela-
tion) and dieIn relations in Freebase. We treat Free-
base annotations as ground truth and measure recall.
We count each tuple in a cluster as true positive if
Freebase states the corresponding relation between
its argument pair. We find that precision numbers
against Freebase are low, below 10%. However,
these numbers are not reliable mainly because many
correct instances found by our models are missing
in Freebase. One reason why our predictions are
missing in Freebase is coreference. For example,
we predict parentCom relation between ?Linksys?
and ?Cisco?, while Freebase only considers ?Cisco
Systems, Inc.? as the parent company of ?Linksys?.
It does not corefer ?Cisco? to ?Cisco Systems, Inc.?.
Incorporating coreference in our model may fix this
problem and is a focus of future work. Instead of
measuring precision against Freebase, we ask hu-
mans to label 50 instances for each cluster and report
precision according to this annotated data. Table 3
shows the scores.
We can see that in most cases Rel-LDA1 and
Type-LDA substantially outperform the Rel-LDA
model. This is due to the fact that both models can
exploit more features to make clustering decisions.
For example, in Rel-LDA1 model, the NER pair fea-
ture restricts the entity types the two arguments can
take.
In the following, we take parentCom relation as
an example to analyze the behaviors of different
models. Rel-LDA includes spurious instances such
as ?A is the chief executive of B?, while Rel-LDA1
has fewer such instances due to the NER pair fea-
ture. Similarly, by explicitly modeling entity type
constraints, Type-LDA makes fewer such errors. All
our models make mistakes when sentences have co-
ordination structures on which the parser has failed.
1460
Rel. Sys. Rec. Prec.
parentCom
Rel-LDA 51.4 76.0
Rel-LDA1 49.5 78.0
Type-LDA 55.3 72.0
filmDirector
Rel-LDA 42.5 32.0
Rel-LDA1 70.5 40.0
Type-LDA 74.2 26.0
comBase
Rel-LDA 31.5 12.0
Rel-LDA1 54.2 22.0
Type-LDA 57.1 30.0
authorOf
Rel-LDA 25.2 84.0
Rel-LDA1 46.9 86.0
Type-LDA 20.2 68.0
dieIn
Rel-LDA 26.5 34.0
Rel-LDA1 55.9 40.0
Type-LDA 50.2 28.0
Table 3: Clustering quality evaluation (%), Rec. is mea-
sured against Freebase, Prec. is measured according to
human annotators
For example, when a sentence has the following pat-
tern ?The winners are A, a part of B; C, a part of
D; E, a part of F?, our models may predict parent-
Com(A,F), because the parser connects A with F via
the pattern ?a part of?.
Some clusters found by our models cannot be
mapped to Freebase relations. Consider the Free-
base relation worksFor as one example. This re-
lation subsumes all types of employment relation-
ships, irrespective of the role the employee plays for
the employer. By contrast, our models discover clus-
ters such as leaderOf, editorOf that correspond to
more specific roles an employee can have. We show
some example relations in Table 4. In the table, the
2nd row shows a cluster of employees of news media
companies; the 3rd row shows leaders of companies;
the last one shows birth and death places of persons.
We can see that the last cluster is noisy since we
do not handle antonyms in our models. The argu-
ments of the clusters have noise too. For example,
?New York? occurs as a destination argument in the
2nd cluster. This is because ?New York? has high
frequency in the corpus and it brings noise to the
clustering results. In Table 5 some entity clusters
found by Type-LDA are shown. We find different
types of companies, such as financial companies and
news companies. We also find subclasses of person,
for example, reviewer and politician, because these
different entity classes participate in different rela-
tions. The last cluster shown in the table is a mix-
ture of news companies and government agencies.
This may be because this entity cluster is affected
by many relations.
4.2 Distant Supervision based Relation
Extraction
Our generative models detect clusters of dependency
paths and their arguments. Such clusters are inter-
esting in their own right, but we claim that they can
also be used to help a supervised relation extractor.
We validate this hypothesis in the context of relation
extraction with distant supervision using predicted
clusters as features.
Following previous work (Mintz et al, 2009), we
use Freebase as our distant supervision source, and
align related entity pairs to the New York Times arti-
cles discussed earlier. Our training and test instances
are pairs of entities for which both arguments appear
in at least one sentence together. Features of each
instance are extracted from all sentences in which
both entities appear together. The gold label for each
instance comes from Freebase. If a pair of entities
is not related according to Freebase, we consider it
a negative example. Note that this tends to create
some amount of noise: some pairs may be related,
but their relationships are not yet covered in Free-
base.
After filtering out relations with fewer than 10 in-
stances we have 65 relations and an additional ?O?
label for unrelated pairs of entities. We call related
instances positive examples and unrelated instances
negative examples.
We train supervised classifiers using maximum
entropy. The baseline classifier employs features
that Mintz et al (2009) used. To extract features
from the generative models we proceed as follows.
For each pair of entities, we collect all tuples asso-
ciated with it. For each of these tuples we extract its
clusterID, and use this ID as a binary feature.
The baseline system without generative model
features is called Distant. The classifiers with ad-
ditional features from generative models are named
after the generative models. Thus we have Rel-LDA,
Rel-LDA1 and Type-LDA classifiers. We compare
1461
Source New York, Euro RSCG Worldwide, BBDO Worldwide, American, DDB Worldwide
Path X, a part of Y; X, a unit of Y; X unit of Y; X, a division of Y; X is a part of Y
Dest Omnicom Group, Interpublic Group of Companies, WPP Group, Publicis Groupe
Source Supreme Court, Anna Wintour, William Kristol, Bill Keller, Charles McGrath
Path X, an editor of Y; X, a publisher of Y; X, an editor at Y; X, an editor in chief of Y; X is an editor of Y;
Dest The Times, The New York Times, Vogue, Vanity Fair, New York
Source Kenneth L. Lay, L. Dennis Kozlowski, Bernard J. Ebbers, Thomas R. Suozzi, Bill Gates
Path X, the executive of Y; X, Y?s executive; X, Y executive; X, the chairman of Y; X, Y?s chairman
Dest Enron, Microsoft, WorldCom, Citigroup, Nassau County
Source Paul J. Browne, John McArdle, Tom Cocola, Claire Buchan, Steve Schmidt
Path X, a spokesman for Y; X, a spokeswoman for Y; X, Y spokesman; X, Y spokeswoman; X, a commissioner of Y
Dest White House, Justice Department, Pentagon, United States, State Department
Source United Nations, Microsoft, Intel, Internet, M. D. Anderson
Path X, based in Y; X, which is based in Y; X, a company in Y; X, a company based in Y; X, a consultant in Y
Dest New York, Washington, Manhattan, Chicago, London
Source Army, Shiite, Navy, John, David
Path X was born in Y; X die at home in Y; X die in Y; X, son of Y; X die at Y
Dest Manhattan, World War II, Brooklyn, Los Angeles, New York
Table 4: The path, source and destination arguments of some relations found by Rel-LDA1.
Company Microsoft, Enron, NBC, CBS, Disney
FinanceCom Merrill Lynch, Morgan Stanley, Goldman Sachs, Lehman Brothers, Credit Suisse First Boston
News Notebook, New Yorker, Vogue, Vanity Fair, Newsweek
SportsTeam Yankees, Mets, Giants, Knicks, Jets
University University of California, Harvard, Columbia University, New York University, University of Penn.
Art Reviewer Stephen Holden, Ken Johnson, Roberta Smith, Anthony Tommasini, Grace Glueck
Games World Series, Olympic, World Cup, Super Bowl, Olympics
Politician Eliot Spitzer, Ari Fleischer, Kofi Annan, Scott McClellan, Karl Rove
Gov. Agency Congress, European Union, NATO, Federal Reserve, United States Court of Appeals
News/Agency The New York Times, The Times, Supreme Court, Security Council, Book Review
Table 5: The entity clusters found by Type-LDA
these against Distant and the DIRT database. For
the latter we parse our data using Minipar (Lin,
1998) and extract dependency paths between pairs
of named entity mentions. For each path, the top 3
similar paths are extracted from DIRT database. The
Minipar path and the similar paths are used as addi-
tional features.
For held-out evaluation, we construct the training
data from half of the positive examples and half of
the negative examples. The remaining examples are
used as test data. Note that the number of negative
instances is more than 10 times larger than the num-
ber of positive instances. At test time, we rank the
predictions by the conditional probabilities obtained
from the Maximum Entropy classifier. We report
precision of top ranked 50 instances for each relation
in table 6. From the table we can see that all systems
using additional features outperform the Distant sys-
tem. In average, our best model achieves 4.1%
improvement over the distant supervision baseline,
12% error reduction. The precision of bornIn is low
because in most cases we predict bornIn instances
as liveIn.
We expect systems using generative model fea-
tures to have higher recall than the baseline. This
is difficult to measure, but precision in the high re-
call area is a signal. We look at top ranked 1000
instances of each system and show the precision in
the last row of the table. We can see that our best
model Type-LDA outperforms the distant supervi-
sion baseline by 4.5%.
Why do generative model features help to im-
1462
Relation Dist Rel Rel1 Type DIRT
worksFor 80.0 92.0 86.0 90.0 84.0
authorOf 98.0 98.0 98.0 98.0 98.0
containedBy 92.0 96.0 96.0 92.0 96.0
bornIn 16.0 18.0 22.0 24.0 10.0
dieIn 28.0 30.0 28.0 24.0 24.0
liveIn 50.0 52.0 54.0 54.0 56.0
nationality 92.0 94.0 90.0 90.0 94.0
parentCom 94.0 96.0 96.0 96.0 90.0
founder 65.2 76.3 61.2 64.0 68.3
parent 52.0 54.0 50.0 52.0 52.0
filmDirector 54.0 60.0 60.0 64.0 62.0
Avg 65.6 69.7 67.4 68.0 66.8
Prec@1K 82.8 85.8 85.3 87.3 82.8
Table 6: Precision (%) of some frequent relations
prove relation extraction? One reason is that gen-
erative models can transfer information from known
patterns to unseen patterns. For example, given
?Sidney Mintz, the great food anthropologist at
Johns Hopkins University?, we want to predict the
relation between ?Sidney Mintz? and ?Johns Hopkins
University?. The distant supervision system incor-
rectly predicts the pair as ?O? since it has not seen
the path ?X, the anthropologist at Y? in the training
data. By contrast, Rel-LDA can predict this pair cor-
rectly as worksFor because the dependency path of
this pair is in a cluster which contains the path ?X, a
professor at Y?.
In addition to held-out evaluation we also carry
out manual evaluation. To this end, we use all the
positive examples and randomly select five times
the number of positive examples as negative ex-
amples to train a classifier. The remaining nega-
tive examples are candidate instances. We rank the
predicted instances according to their classification
scores. For each relation, we ask human annotators
to judge its top ranked 50 instances.
Table 7 lists the manual evaluation results for
some frequent relations. We also list how many in-
stances are found for each relation. For almost all
the relations, systems using generative model fea-
tures find more instances. In terms of precision, our
models perform comparatively to the baseline, even
better for some relations.
We also notice that clustering quality is not con-
sistent with distant supervision performance. Rel-
LDA1 can find better clusters than Rel-LDA but it
has lower precision in held-out evaluation. Type-
LDA underperforms Rel-LDA in average precision
but it gets higher precision in a higher recall area, i.e.
precision at 1K. One possible reason for the incon-
sistency is that the baseline distant supervision sys-
tem already employs features that are used in Rel-
LDA1. Another reason may be that the clusters do
not overlap with Freebase relations very well, see
section 4.1.
4.3 Comparing against USP
We also try to compare against USP (Poon and
Domingos, 2008). Due to memory requirements of
USP, we are only able to run it on a smaller data
set consisting of 1,000 NYT documents; this is three
times the amount of data Poon and Domingos (2008)
used to train USP.2 For distant supervision based re-
lation extraction, we only match about 500 Freebase
instances to this small data set.
USP provides a parse tree for each sentence and
for each mention pair we can extract a path from
the tree. Since USP provides clusters of words and
phrases, we use the USP clusterID associated with
the words on the path as binary features in the clas-
sifier.
All models are less accurate when trained on this
smaller dataset; we can do as well as USP does,
even a little better. USP achieves 8.6% in F1, Rel-
LDA 8.7%, Rel-LDA1 10.3%, Type-LDA 8.9% and
Distant 10.3%. Of course, given larger datasets,
the performance of Rel-LDA, Rel-LDA1, and Type-
LDA improves considerably. In summary, compar-
ing against USP, our approach scales much more
easily to large data.
5 Related Work
Many approaches have been explored in relation ex-
traction, including bootstrapping, supervised classi-
fication, distant supervision, and unsupervised ap-
proaches.
Bootstrapping employs a few labeled examples
for each relation, iteratively extracts patterns from
the labeled seeds, and uses the patterns to extract
2Using the publicly released USP code, training a model
with 1,000 documents resulted in about 45 gigabytes of heap
space in the JVM.
1463
Relation Top 50 (%) #InstancesDist Rel Type Dist Rel Type
worksFor 100.0 100.0 100.0 314 349 349
authorOf 94.0 94.0 96.0 185 208 229
containedBy 98.0 98.0 98.0 670 714 804
bornIn 82.6 88.2 88.0 46 36 56
dieIn 100.0 100.0 100.0 167 176 231
liveIn 98.0 98.0 94.0 77 86 109
nationality 78.0 82.0 76.0 84 92 114
parentCom 79.2 77.4 85.7 24 31 28
founder 80.0 80.0 50.0 5 5 14
parent 97.0 92.3 94.7 33 39 38
filmDirector 92.6 96.9 97.1 27 32 34
Table 7: Manual evaluation, Precision and recall of some frequent relations
more seeds (Brin, 1998). This approach may suffer
from low recall since the patterns can be too specific.
Supervised learning can discover more general
patterns (Kambhatla, 2004; Culotta and Sorensen,
2004). However, this approach requires labeled data,
and most work only carry out experiments on small
data set.
Distant supervision for relation extraction re-
quires no labeled data. The approach takes some
existing knowledge base as supervision source,
matches its relational instances against the text cor-
pus to build the training data, and extracts new in-
stances using the trained classifiers (Mintz et al,
2009; Bunescu and Mooney, 2007; Riedel et al,
2010; Yao et al, 2010).
All these approaches can not discover new rela-
tions and classify instances which do not belong to
any of the predefined relations. Other past work has
explored inducing relations using unsupervised ap-
proaches.
For example, DIRT (Lin and Pantel, 2001) aims
to discover different representations of the same se-
mantic relation, i.e. similar dependency paths. They
employ the distributional similarity based approach
while we use generative models. Both DIRT and our
approach take advantage of the arguments of depen-
dency paths to find semantic relations. Moreover,
our approach can cluster the arguments into differ-
ent types.
Unsupervised semantic parsing (USP) (Poon and
Domingos, 2008) discovers relations by merging
predicates which have similar meanings; it proceeds
to recursively cluster dependency tree fragments (or
?parts?) to best explain the observed sentence. It is
not focused on capturing any particular kind of re-
lation between sentence constituents, but to capture
repeated patterns. Our approach differs in that we
are focused on capturing a narrow range of binary
relations between named entities; some of our mod-
els (see Section 3) utilize entity type information to
constraint relation type induction. Also, our models
are built to be scalable and trained on a very large
corpus. In addition, we use a distant supervision
framework for evaluation.
Relation duality (Bollegala et al, 2010) employs
co-clustering to find clusters of entity pairs and pat-
terns. They identify each cluster of entity pairs as a
relation by selecting representative patterns for that
relation. This approach is related to our models,
however, it does not identify any entity clusters.
Generative probabilistic models are widely em-
ployed in relation extraction. For example, they are
used for in-domain relation discovery while incorpo-
rating constraints via posterior regularization (Chen
et al, 2011). We are focusing on open domain re-
lation discovery. Generative models are also ap-
plied to selectional preference discovery (Ritter et
al., 2010; Seaghdha, 2010). In this scenario, the
authors assume relation labels are given while we
automatically discover relations. Generative models
are also used in unsupervised coreference (Haghighi
and Klein, 2010).
1464
Clustering is also employed in relation extraction.
Hasegawa et al (2004) cluster pairs of named en-
tities according to the similarity of context words
intervening between them. Their approach is not
probabilistic. Researchers also use topic models to
perform dimension reduction on features when they
cluster relations (Hachey, 2009). However, they do
not explicitly model entity types.
Open information extraction aims to discover re-
lations independent of specific domains and rela-
tions (Banko et al, 2007; Banko and Etzioni, 2008).
A self-learner is employed to extract relation in-
stances but the systems do not cluster the instances
into relations. Yates and Etzioni (2009) present RE-
SOLVER for discovering relational synonyms as a
post processing step. Our approach integrates entity
and relation discovery in a probabilistic model.
6 Conclusion
We have presented an unsupervised probabilistic
generative approach to relation extraction between
two named entities. Our proposed models exploit
entity type constraints within a relation as well
as features on the dependency path between entity
mentions to cluster equivalent textual expressions.
We demonstrate the effectiveness of this approach
by comparing induced relation clusters against a
large knowledge base. We also show that using clus-
ters of our models as features in distant supervised
framework yields 12% error reduction in precision
over a weakly supervised baseline and outperforms
other state-of-the art relation extraction techniques.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and the Uni-
versity of Massachusetts gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181, ITR#1, and NSF
MALLET. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the US govern-
ment. Any opinions, findings and conclusions or
recommendations expressed in this material are the
authors? and do not necessarily reflect those of the
sponsor.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI2007.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2010. Relational duality: Unsupervised ex-
traction of semantic relations between entities on the
web. In Proceedings of WWW.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In Proc. of WebDB Work-
shop at 6th International Conference on Extending
Database Technology.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Eugene Charniak and Micha Elsner. 2009. Em works for
pronoun anaphora resolution. In Proceedings of ACL.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of ACL.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
Benjamin Hachey. 2009. Towards Generic Relation Ex-
traction. Ph.D. thesis, University of Edinburgh.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for extracting relations. In Proceedings of ACL.
1465
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of ACL 10.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of
inference rules from text. In Proceedings of KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation of Parsing Systems.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Hoifung Poon and Pedro Domingos. 2008. Unsuper-
vised semantic parsing. In Proceedings of the Confer-
ence on Empirical methods in natural language pro-
cessing (EMNLP).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of ACL10.
Dan Roth and Wen tau Yih. 2002. Probabilistic reason-
ing for entity and relation recognition. In Proceedings
of Coling.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Diarmuid O Seaghdha. 2010. Latent variable models of
selectional preference. In Proceedings of ACL 10.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL, pages 252?259.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1013?1023, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
1466
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 732?743, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Parse, Price and Cut?Delayed Column and Row Generation for Graph
Based Parsers
Sebastian Riedel David Smith Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{riedel,dasmith,mccallum}@cs.umass.edu
Abstract
Graph-based dependency parsers suffer from
the sheer number of higher order edges they
need to (a) score and (b) consider during opti-
mization. Here we show that when working
with LP relaxations, large fractions of these
edges can be pruned before they are fully
scored?without any loss of optimality guar-
antees and, hence, accuracy. This is achieved
by iteratively parsing with a subset of higher-
order edges, adding higher-order edges that
may improve the score of the current solu-
tion, and adding higher-order edges that are
implied by the current best first order edges.
This amounts to delayed column and row gen-
eration in the LP relaxation and is guaranteed
to provide the optimal LP solution. For second
order grandparent models, our method consid-
ers, or scores, no more than 6?13% of the sec-
ond order edges of the full model. This yields
up to an eightfold parsing speedup, while pro-
viding the same empirical accuracy and cer-
tificates of optimality as working with the full
LP relaxation. We also provide a tighter LP
formulation for grandparent models that leads
to a smaller integrality gap and higher speed.
1 Introduction
Many problems in NLP, and structured prediction in
general, can be cast as finding high-scoring struc-
tures based on a large set of candidate parts. For
example, in second order graph-based dependency
parsing (K?bler et al2009) we have to choose a
quadratic number of first order and a cubic number
of second order edges such that the graph is both
high-scoring and a tree. In coreference, we have
to select high-scoring clusters of mentions from an
exponential number of candidate clusters, such that
each mention is in exactly one cluster (Culotta et
al., 2007). In segmentation of citation strings, we
need to consider a quadratic number of possible seg-
ments such that every token is part of exactly one
segment (Poon and Domingos, 2007).
What makes such problems challenging is the
large number of possible parts to consider. This
number not only affects the cost of search or opti-
mization but also slows down the process of scor-
ing parts before they enter the optimization prob-
lem. For example, the cubic grandparent edges in
second-order dependency parsing slow down dy-
namic programs (McDonald and Pereira, 2006), be-
lief propagation (Smith and Eisner, 2008) and LP
solvers (Martins et al2009), since there are more
value functions to evaluate, more messages to pass,
or more variables to consider. But to even calculate
the score for each part we need a cubic number of
operations that usually involve expensive feature ex-
traction. This step often becomes a major bottleneck
in parsing, and structured prediction in general.
Candidate parts can often be heuristically pruned.
In the case of dependency parsing, previous work
has used coarse-to-fine strategies where simpler first
order models are used to prune unlikely first or-
der edges, and hence all corresponding higher or-
der edges (Koo and Collins, 2010; Martins et al
2009; Riedel and Clarke, 2006). While such meth-
ods can be effective, they are more convoluted, often
require training of addition models as well as tuning
of thresholding hyper-parameters, and usually pro-
vide no guarantees of optimality.
We present an approach that can solve problems
with large sets of candidate parts without consider-
ing all of these parts in either optimization or scor-
732
ing. And in contrast to most pruning heuristics, our
algorithm can give certificates of optimality before
having optimized over, or even scored, all parts. It
does so without the need of auxiliary models or tun-
ing of threshold parameters. This is achieved by a
delayed column and row generation algorithm that
iteratively solves an LP relaxation over a small sub-
set of current candidate parts, and then finds new
candidates that score highly and can be inserted into
the current optimal solution without removing high
scoring existing structure. The latter step subtracts
from the cost of a part the price of resources the part
requires, and is often referred as pricing. Sometimes
parts may score highly after pricing, but are neces-
sary in order to make the current solution feasible.
We add such parts in a step that roughly amounts to
violated cuts to the LP.
We illustrate our approach in terms of a second-
order grandparent model for dependency parsing.
We solve these models by iteratively parsing, pric-
ing, and cutting. To this end we use a variant of the
LP relaxation formulated by Martins et al2009).
Our variant of this LP is designed to be amenable to
column generation. It also turns out to be a tighter
outer bound that leads to fewer fractional solutions
and faster runtimes. To find high scoring grandpar-
ent edges without explicitly enumerating all of them,
we prune out a large fraction using factorized upper
bounds on grandparent scores.
Our parse, price and cut algorithm is evaluated
using a non-projective grandparent model on three
languages. Compared to a brute force approach of
solving the full LP, we only score about 10% of the
grandparent edges, consider only 8% in optimiza-
tion, and so observe an increase in parsing speed of
up to 750%. This is possible without loss of opti-
mality, and hence accuracy. We also find that our
extended LP formulation leads to a 15% reduction
of fractional solutions, up to 12 times higher speed,
and generally higher accuracy when compared to the
grandparent formulation of Martins et al2009).
2 Graph-Based Dependency Parsing
Dependency trees are representations of the syntac-
tic structure of a sentence (Nivre et al2007). They
determine, for each token of a sentence, the syntac-
tic head the token is modifying. As a lightweight al-
ternative to phrase-based constituency trees, depen-
dency representations have by now seen widespread
use in the community in various domains such as
question answering, machine translation, and infor-
mation extraction.
To simplify further exposition, we now formalize
the task, and mostly follow the notation of Martins et
al. (2009). Consider a sentence x = ?t0, t1, . . . , tn?
where t1, . . . , tn correspond to the n tokens of the
sentence, and t0 is an artificial root token. Let
V , {0, . . . , n} be a set of vertices corresponding
to the tokens in x, and C ? V ?V a set of candidate
directed edges. Then a directed graph y ? C is a
legal dependency parse if and only if it is a tree over
V rooted at vertex 0. Given a sentence x, we use Y
to denote the set of its legal parses. Note that all of
the above definitions depend on x, but for simplicity
we omit this dependency in our notation.
2.1 Arc-Factored Models
Graph-based models define parametrized scoring
functions that are trained to discriminate between
correct and incorrect parse trees. So called arc-
factored or first order models are the most basic
variant of such functions: they assess the quality of a
tree by scoring each edge in isolation (McDonald et
al., 2005b; McDonald et al2005a). Formally, arc-
factored models are scoring functions of the form
s (y;x,w) =
?
?h,m??y
s?h,m? (x,w) (1)
where w is a weight vector and s?h,m? (x,w) scores
the edge ?h,m? with respect to sentence x and
weightsw. From here on we will omit both x andw
from our notation if they are clear from the context.
Given such a scoring function, parsing amounts to
solving:
maximize
y
?
?h,m??y
s?h,m?
subject to y ? Y.
(2)
2.2 Higher Order Models
Arc-factored models cannot capture higher order de-
pendencies between two or more edges. Higher
order models remedy this by introducing scores
for larger configurations of edges appearing in the
733
tree (McDonald and Pereira, 2006). For example,
in grandparent models, the score of a tree also in-
cludes a score sgp?g,p,c? for each grandparent-parent-
child triple ?g, p, c?:
s (y) =
?
?h,m??y
s?h,m? +
?
?g,p??y,?p,c??y
sgp?g,p,c? (3)
There are other variants of higher order models
that include, in addition to grandparent triples, pairs
of siblings (adjacent or not) or third order edges.
However, to illustrate our approach we will focus
on grandparent models and note that most of what
we present can be generalized to other higher order
models.
2.3 Feature Templates
For our later exposition the factored and
parametrized nature of the scoring functions
will be crucial. In the following we therefore
illustrate this property in more detail.
The scoring functions for arcs or higher order
edges usually decompose into a sum of feature tem-
plate scores. For example, the grandparent edge
score sgp?g,p,c? is defined as
sgp?g,p,c? ,
?
t?T gp
sgp,t?g,p,c? (4)
where T gp is the set of grandparent templates, and
each template t ? T gp defines a scoring func-
tion sgp,t?g,p,c? to assess a specific property of the
grandparent-parent-child edge ?g, p, c?.
The template scores again decompose. Consider-
ing grandparent scores, we get
st?g,p,c? , w
>
t f
t (htg, h
t
p, h
t
c, d
t
g,p,c
)
(5)
where hti is an attribute of token ti, say h
101
i =
Part-of-Speech (ti). The term dtg,p,c corresponds to
a representation of the relation between tokens cor-
responding to g, p and g. For example, for template
101 it could return their relative positions to each
other:
d101g,p,c , ?I [g > p] , I [g > c] , I [p > c]? . (6)
The feature function f t maps the representations
of g, p and c into a vector space. For the purposes of
our work this mapping is not important, and hence
we omit details.
2.4 Learning
The scoring functions we consider are parametrized
by a family of per-template weight vectors w =
?wt?t?T . During learning we need to estimate w
such that our scoring functions learns to differenti-
ate between correct and incorrect parse trees. This
can be achieved in many ways: large margin train-
ing, maximizing conditional likelihood, or variants
in between. In this work we follow Smith and Eis-
ner (2008) and train the models with stochastic gra-
dient descent on the conditional log-likelihood of the
training data, using belief propagation in order to
calculate approximate gradients.
3 LP and ILP Formulations
Riedel and Clarke (2006) showed that dependency
parsing can be framed as Integer Linear Pro-
gram (ILP), and efficiently solved using an off-the-
shelf optimizer if a cutting plane approach is used.1
Compared to tailor made dynamic programs, such
generic solvers give the practitioner more modeling
flexibility (Martins et al2009), albeit at the cost
of efficiency. Likewise, compared to approximate
solvers, ILP and Linear Program (LP) formulations
can give strong guarantees of optimality. The study
of Linear LP relaxations of dependency parsing has
also lead to effective alternative methods for parsing,
such as dual decomposition (Koo et al2010; Rush
et al2010). As we see later, the capability of LP
solvers to calculate dual solutions is also crucial for
efficient and exact pruning. Note, however, that dy-
namic programs provide dual solutions as well (see
section 4.5 for more details).
3.1 Arc-Factored Models
To represent a parse y ? Y we first introduce an
vector of variables z , ?za?a where za is 1 if a ? y
and 0 otherwise. With this representation parsing
amounts to finding a vector z that corresponds to a
legal parse tree and that maximizes
?
a zasa. One
way to achieve this is to search through the convex
hull of all legal incidence vectors, knowing that any
linear objectives would take on its maximum on one
of the hull?s vertices. We will use Z to denote this
convex hull of incidence vectors of legal parse trees,
1Such as the highly efficient and free-for-academic-use
Gurobi solver.
734
and callZ the arborescence polytope (Martins et al
2009). The Minkowski-Weyl theorem tells us thatZ
can be represented as an intersection of halfspaces,
or constraints, Z = {z|Az ? b}. Hence optimal
dependency parsing, in theory, can be addressed us-
ing LPs.
However, it is difficult to describe Z with a com-
pact number of constraints and variables that lend
themselves to efficient optimization. In general we
therefore work with relaxations, or outer bounds, on
Z . Such outer bounds are designed to cut off all
illegal integer solutions of the problem, but still al-
low for fractional solutions. In case the optimum is
achieved at an integer vertex of the outer bound, it
is clear that we have found the optimal solution to
the original problem. In case we find a fractional
point, we need to map it onto Z (e.g., by projection
or rounding). Alternatively, we can use the outer
bound together with 0/1 constraints on z, and then
employ an ILP solver (say, branch-and-bound) to
find the true optimum. Given the NP-hardness of
ILP, this will generally be slow.
In the following we will present the outer bound
Z? ? Z proposed by Martins et al2009).
Compared to the representation Riedel and Clarke
(2006), this bound has the benefit a small polyno-
mial number of constraints. Note, however, that of-
ten exponentially many constraints can be efficiently
handled if polynomial separation algorithms exists,
and that such representations can lead to tighter
outer bounds.
The constraints we employ are:
No Head For Root In a dependency tree the root
node never has a head. While this could be captured
through linear constraints, it is easier to simply re-
strict the candidate set C to never contain edges of
the form ??, 0?.
Exactly One Head for Non-Roots Any non-root
token has to have exactly one head token. We can
enforce this property through the set of constraints:
m > 0 :
?
h
z?h,m? = 1. (OneHead)
No Cycles A parse tree cannot have cycles. This is
equivalent, together with the head constraints above,
to enforcing that the tree be fully connected. Mar-
tins et al2009) capture this connectivity constraint
using a single commodity flow formulation. This
requires the introduction of flow variables ? ,
??a?a?C . By enforcing that token 0 has n outgoing
flow, ?
m>0
??0,m? = n, (Source)
that any other token consumes one unit of flow,
t > 0 :
?
h
??h,t? ?
?
m>0
??t,m? = 1 (Consume)
and that flow is zero on disabled arcs
??h,m? ? nz?h,m?, (NoFlow)
connectivity can be ensured.
Assuming we have such a representation, parsing
with an LP relaxation amounts to solving
maximize
z?0
?
a?A
zasa
subject to A
[
z
?
]
? b.
(7)
3.2 Higher Order Models
The 1st-Order LP can be easily extended to capture
second (or higher) order models. For for the case
of grandparent models, this amounts to introduc-
ing another class of variables, zgpg,p,c, that indicate if
the parse contains both the edge ?g, p? and the edge
?p, c?. With the help of the indicators zgp we can rep-
resent the second order objective as a linear function.
We now need an outer bound on the convex hull of
vectors ?z, zgp? where z is a legal parse tree and zgp
is a consistent set of grandparent indicators. We will
refer to this convex hull as the grandparent polytope
Zgp.
We can re-use the constraints A of section 3.1 to
ensure that z is in Z . To make sure zgp is consistent
with z, Martins et al2009) linearize the equiva-
lence zgpg,p,c ? zg,p ? zp,c we know to hold for legal
incidence vectors, yielding
g, p, c : z?g,p? + z?p,c? ? z
gp
?g,p,c? ? 1 (ArcGP)
and
g, p, c : z?g,p? ? z
gp
?g,p,c?, z?p,c? ? z
gp
?g,p,c? (GPArc)
There are additional constraints we know to hold in
Zgp. First, we know that for any active edge ?p, c? ?
735
y with p > 0 there is exactly one grandparent edge
?g, p, c?. Likewise, for an inactive edge ?p, c? /? y
there must be no grandparent edge ?g, p, c?. This
can be captured through the constraint:
p > 0, c :
?
g
zgp?g,p,c? = z?p,c?. (OneGP)
We also know that if an edge ?g, p? in inactive,
there must not be any grandparent edge ?g, p, c? that
goes through ?g, p?:
g, p :
?
c
zgp?g,p,c? ? nz?g,p?. (NoGP)
It can be easily shown that for integer solu-
tions the constraints ArcGP and GPArc of Martins
et al2009) are sufficient conditions for consis-
tency between z and zgp. It can equally be shown
that the same holds for the constraints OneGP and
NoGP. However, when working with LP relax-
ations, the two polytopes have different fractional
vertices. Hence, by combining both constraint sets,
we can get a tighter outer bound on the grandparent
polytope Zgp. In section 6 we show empirically that
this combined polytope in fact leads to fewer frac-
tional solutions. Note that when using the union of
all four types of constraints, the NoGP constraint is
implied by the constraint GPArc (left) by summing
over c on both sides, and can hence be omitted.
4 Parse, Price and Cut
We now introduce our parsing algorithm. To this
end, we first give a general description of column
and row generation for LPs; then, we illustrate how
these techniques can be applied to dependency pars-
ing.
4.1 Column and Row Generation
LPs often have too many variables and constraints
to be efficiently solved. In such cases delayed
column and row generation can substantially re-
duce runtime by lazily adding variables only when
needed (Gilmore and Gomory, 1961; L?bbecke and
Desrosiers, 2004).
To illustrate column and row generation let us
consider the following general primal LP and its cor-
responding dual problem:
Primal
maximize
z?0
s?z
subject to Az ? b
Dual
minimize
??0
??b
subject to A?? ? s.
Say you are given a primal feasible z? and a dual fea-
sible ?? for which complementary slackness holds:
for all variables i we have z?i > 0? si =
?
j ?
?
jai,j
and for all constraints j we have ??j > 0 ? bj =?
i z
?
iai,j . In this case it is easy to show that z
? is
an optimal primal solution, ?? and optimal dual so-
lution, and that both objectives meet at these val-
ues (Bertsekas, 1999).
The idea behind delayed column and row gener-
ation is to only consider a small subset of variables
(or columns) I and subset of constraints (or rows) J .
Optimizing over this restricted problem, either with
an off-the-shelf solver or a more specialized method,
yields the pair
(
z?I ,?
?
J
)
of partial primal and dual
solutions. This pair is feasible and complementary
with respect to variables I and constraints J . We
can extend it to a solution (z?,y?) over all variables
and constraints by heuristically setting the remain-
ing primal and dual variables. If it so happens that
(z?,y?) is feasible and complementary for all vari-
ables and constraints, we have found the optimal so-
lution. If not, we add the constraints and variables
for which feasibility and slackness are violated, and
resolve the new partial problem.
In practice, the uninstantiated primal and dual
variables are often set to 0. In this case complemen-
tary slackness holds trivially, and we only need to
find violated primal and dual constraints. For primal
constraints,
?
i ziai,j ? bi, searching for violating
constraints j is the well-known separation step in
cutting plane algorithms. For the dual constraints,
?
j ?jai,j ? si, the same problem is referred to
as pricing. Pricing is often framed as searching for
all, or some, variables i with positive reduced cost
ri , si?
?
j ?jai,j . Note that while these problems
are, naturally, dual to each other, they can have very
different flavors. When we assess dual constraints
we need to calculate a cost si for variable i, and
usually this cost would be different for different i.
For primal constraints the corresponding right-hand-
sides are usually much more homogenous.
736
Algorithm 1 Parse, Price and Cut.
Require: Initial candidate edges and hyperedges P .
Ensure: The optimal z.
1: repeat
2: z,? ? parse(P )
3: N ? price(?)
4: M ? cut(z)
5: P ? P ?N ?M
6: until N = ? ?M = ?
7: return z
The reduced cost ri = si ?
?
j ?jai,j has sev-
eral interesting interpretations. First, intuitively it
measures the score we could gain by setting zi = 1,
and subtracts an estimate of what we would loose
because zi = 1 may compete with other variables
for shared resources (constraints). Second, it cor-
responds to the coefficient of zi in the Lagrangian
L (?, z) , s?z + ? [b?Az]. For any ?, Uzi=k =
maxz?0,zi=k L (?, z) is an upper bound on the best
possible primal objective with zi = k. This means
that ri = Uzi=1 ? Uzi=0 is the difference between
an upper bound that considers zi = 1, and one that
considers zi = 0. The tighter the bound Uzi=0 is,
the closer ri is to an upper bound on the maximal
increase we can get for setting zi to 1. At conver-
gence of column generation, complementary slack-
ness guarantees that Uzi=0 is tight for all z
?
i = 0, and
hence ri is a true an upper bound.
4.2 Application to Dependency Parsing
The grandparent formulation in section 3.2 has a cu-
bic number of variables z?g,p,c? as well as a cubic
number of constraints. For longer sentences this
number can slow us down in two ways. First, the
optimizer works with a large search space, and will
naturally become slower. Second, for every grand-
parent edge we need to calculate the score s?g,p,c?,
and this calculation can often be a major bottleneck,
in particular when using complex feature functions.
To overcome this bottleneck, our parse, price and cut
algorithm, as shown in algorithm 1, uses column and
row generation. In particular, it lazily instantiates
the grandparent edge variables zgp?g,p,c?, and the cor-
responding cubic number of constraints. All unin-
stantiated variables are implicitly set to 0.
The algorithm requires some initial set of vari-
ables to start with. In our case this set P contains all
first-order edges ?h,m? in the candidate set C, and
for each of these one grandparent edge ?0, h,m?.
The primary purpose of these grandparent edges is
to ensure feasibility of the OneGP constraints.
In step 2, the algorithm parses with the current
set of candidates P by solving the corresponding LP
relaxation. The LP contains all columns and con-
straints that involve the edges and grandparent edges
of P . The solver returns both the best primal solu-
tion z (for both edges and grandparents), and a com-
plementary dual solution ?.
In step 3 the dual variables? are used to find unin-
stantiated grandparent edges ?g, p, c? with positive
reduced cost. The price routine returns such edges
in N . In step 4 the primal solution is inspected for
violations of constraint ArcGP. The cut routine per-
forms this operation, and returns M , the set of edges
?g, p, c? that violate ArcGP.
In step 5 the algorithm converges if no more con-
straint violations, or promising new columns, can
be found. If there have been violations (M 6= ?)
or promising columns (N 6= ?), steps 2 to 4 are
repeated, with the newly found parts added to the
problem. Note that LP solvers can be efficiently
warm-started after columns and rows have been
added, and hence the cost of calls to the solver in
step 2 is substantially reduced after the first itera-
tion.
4.3 Pricing
In the pricing step we need to efficiently find a
set of grandparent edge variables zgp?g,p,c? with posi-
tive reduced cost, or the empty set if no such vari-
ables exist. Let ?OneGP?p,c? be the dual variables for
the OneGP constraints and ?NoGP?g,p? the duals for con-
straints NoGP. Then for the reduced cost of zgp?g,p,c?
we know that:
r?g,p,c? = s?g,p,c? ? ?
OneGP
?p,c? ? ?
NoGP
?g,p? . (8)
Notice that the duals for the remaining two con-
straints ArcGP and GPArc do not appear in this
equation. This is valid because we can safely set
their duals to zero without violating dual feasibility
or complementary slackness of the solution returned
by the solver.
737
4.3.1 Upper Bounds for Efficient Pricing
A naive pricing implementation would exhaus-
tively iterate over all ?g, p, c? and evaluate r?g,p,c?
for each. In this case we can still substantially re-
duce the number of grandparent variables that en-
ter the LP, provided many of these variables have
non-positive reduced cost. However, we still need to
calculate the score s?g,p,c? for each ?g, p, c?, an ex-
pensive operation we hope to avoid. In the follow-
ing we present an upper bound on the reduced cost,
r?gp?g,p,c? ? r
gp
?g,p,c?, which decomposes in a way that
allows for more efficient search. Using this bound,
we find all new grandparent edges N? for which this
upper bound is positive:
N? ?
{
?g, p, c? |r?gp?g,p,c? > 0
}
. (9)
Next we prune away all but the grandparent edges
for which the exact reduced cost is positive:
N ? N? \ {e : rgpe > 0} . (10)
Our bound r?gp?g,p,c? on the reduced cost of ?g, p, c?
is based on an upper bound s?gp?g,p,?? ? maxc s
gp
?g,p,c?
on the grandparent score involving ?g, p? as grand-
parent and parent, and the bound s?gp??,p,c? ?
maxg s
gp
?g,p,c? on the grandparent score involving
?p, c? as parent and child. Concretely, we have
r?gp?g,p,c? , min
(
s?gp?g,p,??, s?
gp
??,p,c?
)
? ?OneGP?p,c? ? ?
NoGP
?g,p? .
(11)
To find edges ?g, p, c? for which this bound is
positive, we can filter out all edges ?p, c? such that
sgp??,p,c?? ?
OneGP
?p,c? is non-positive. This is possible be-
cause NoGP is a? constraint and therefore ?NoGP?g,p? ?
0.2 Hence r?gp?g,p,c? is at most s?
gp
??,p,c? ? ?
OneGP
?p,c? . This
filtering step cuts off a substantial number of edges,
and is the main reason why can avoid scoring all
edges.
Next we filter, for each remaining ?p, c?, all pos-
sible grandparents g according to the definition of
r?gp?g,p,c?. This again allows us to avoid calling the
2Notice that in section 4.1 we discussed the LP dual in
case were all constraints are inequalities. When equality con-
straints are used, the corresponding dual variables have no sign
constraints. Hence we could not make the same argument for
?OneGP?p,c? .
grandparent scoring function on ?g, p, c?, and yields
the candidate set N? . Only if r?gp?g,p,c? is positive do we
have to evaluate the exact reduced cost and score.
4.3.2 Upper Bounds on Scores
What remains to be done is the calculation of up-
per bounds s?gp?g,p,?? and s?
gp
??,p,c?. Our bounds factor
into per-template bounds according to the definitions
in section 2.3. In particular, we have
s?gp??,p,c? ,
?
t?T gp
s?gp,t??,p,c? (12)
where s?t??,p,c? is a per-template upper bound defined
as
s?gp,t??,p,c? , max
v?range(ht)
e?range
`
dt
?
w>t f
t (v, htp, h
t
c, e
)
. (13)
That is, we maximize over all possible attribute val-
ues v any token g could have, and any possible rela-
tion e a token g can have to p and c.
Notice that these bounds can be calculated offline,
and hence amortize after deployment of the parser.
4.3.3 Tightening Duals
To price variables, we use the duals returned by
the solver. This is a valid default strategy, but may
lead to ? with overcautious reduced costs. Note,
however, that we can arbitrary alter ? to minimize
reduced costs of uninstantiated variables, as long as
we ensure that feasibility and complementary slack-
ness are maintained for the instantiated problem.
We use this flexibility for increasing ?OneGP?p,c? , and
hence lowering reduced costs zgp?g,p,c? for all tokens c.
Assume that z?p,c? = 0 and let r?p,c? = ?
OneGP
?p,c? + K
be the current reduced cost for z?p,c? in the instanti-
ated problem. Here K is a value depending on s?p,c?
and the remaining constraints z?p,c? is involved in.
We know that r?p,c? ? 0 due to dual feasibility
and hence r?p,c? may be 0, but note that r?p,c? < 0 in
many cases. In such cases we can increase ?OneGP?p,c?
to ?K and get r?p,c? = 0. With respect to z?p,c? this
maintains dual feasibility (because r?p,c? ? 0) and
complementary slackness (because z?p,c? = 0). Fur-
thermore, with respect to the zgp?g,p,c? for all tokens c
this also maintains feasibility (because the increased
?OneGP?p,c? appears with negative sign in 8) and com-
plementary slackness (because zgp?g,p,c? = 0 due to
z?p,c? = 0).
738
4.4 Separation
What happens if both z?g,p? and z?p,c? are active
while zgp?g,p,c? is still implicitly set to 0? In this case
we violate constraint ArcGP. We could remedy this
by adding the cut z?g,p? + z?p,c? ? 1, resolve the
LP, and then use the dual variable corresponding to
this constraint to get an updated reduced cost r?g,p,c?.
However, in practice we found this does not happen
as often, and when it does, it is cheaper for us to add
the corresponding column r?g,p,c? right away instead
of waiting to the next iteration to price it.
To find all pairs of variables for z?g,p? + z?p,c? ? 1
is violated, we first filter out all edges ?h,m? for
which z?h,m? = 0 as these automatically satisfy
any ArcGP constraint they appear in. Now for each
z?g,p? > 0 all z?p,c? > 0 are found, and if their sum
is larger than 1, the corresponding grandparent edge
?g, p, c? is returned in the result set.
4.5 Column Generation in Dynamic Programs
Column and Row Generation can substantially re-
duce the runtime of an off-the-shelf LP solver, as
we will find in section 6. Perhaps somewhat sur-
prisingly, it can also be applied in the context of dy-
namic programs. It is well known that for each dy-
namic program there is an equivalent polynomial LP
formulation (Martin et al1990). Roughly speak-
ing, in this formulation primal variables correspond
to state transitions, and dual variables to value func-
tions (e.g., the forward scores in the Viterbi algo-
rithm).
In pilot studies we have already used DCG to
speed up (exact) Viterbi on linear chains (Belanger
et al2012). We believe it could be equally applied
to dynamic programs for higher order dependency
parsing.
5 Related Work
Our work is most similar in spirit to the relaxation
method presented by Riedel and Smith (2010) that
incrementally adds second order edges to a graphi-
cal model based on a gain measure?the analog of
our reduced cost. However, they always score every
higher order edge, and also provide no certificates of
optimality.
Several works in parsing, and in MAP inference
in general, perform some variant of row genera-
tion (Riedel and Clarke, 2006; Tromble and Eis-
ner, 2006; Sontag and Jaakkola, 2007; Sontag et al
2008). However, none of the corresponding methods
lazily add columns, too. The cutting plane method
of Riedel (2008) can omit columns, but only if their
coefficient is negative. By using the notion of re-
duced costs we can also omit columns with positive
coefficient. Niepert (2010) applies column gener-
ation, but his method is limited to the case of k-
Bounded MAP Inference.
Several ILP and LP formulations of dependency
parsing have been proposed. Our formulation is in-
spired by Martins et al2009), and hence uses fewer
constraints than Riedel and Clarke (2006). For the
case of grandparent edges, our formulation also im-
proves upon the outer bound of Martins et al2009)
in terms of speed, tightness, and utility for column
generation. Other recent LP relaxations are based
on dual decomposition (Rush et al2010; Koo et
al., 2010; Martins et al2011). These relaxations
allow the practitioner to utilize tailor-made dynamic
programs for tractable substructure, but still every
edge needs to be scored. Given that column gener-
ation can also be applied in dynamic programs (see
section 4.5), our algorithm could in fact accelerate
dual decomposition parsing as well.
Pruning methods are a major part of many struc-
tured prediction algorithms in general, and of pars-
ing algorithms in particular (Charniak and Johnson,
2005; Martins et al2009; Koo and Collins, 2010;
Rush and Petrov, 2012). Generally these meth-
ods follow a coarse-to-fine scheme in which sim-
pler models filter out large fractions of edges. Such
methods are effective, but require tuning of thresh-
old parameters, training of additional models, and
generally lead to more complex pipelines that are
harder to analyze and have fewer theoretical guar-
antees.
A* search (Ahuja et al1993) has been used
to search for optimal parse trees, for example by
Klein and Manning (2003) or, for dependency pars-
ing, by Dienes et al2003). There is a direct rela-
tion between both A* and Column Generation based
on an LP formulation of the shortest path problem.
Roughly speaking, in this formulation any feasible
dual assignments correspond to a consistent (and
thus admissible) heuristic, and the corresponding re-
duced costs can be used as edge weights. Run-
739
ning Dijkstra?s algorithm with these weights then
amounts to A*. Column generation for the shortest
path problem can then be understood as a method to
lazily construct a consistent heuristic. In every step
this method finds edges for which consistency is vi-
olated, and updates the heuristic such that all these
edges are consistent.
6 Experiments
We claim that LP relaxations for higher order pars-
ing can be solved without considering, and scoring,
all candidate higher order edges. In practice, how
many grandparent edges do we need to score, and
how many do we need to add to the optimization
problem? And what kind of reduction in runtime
does this reduction in edges lead to?
We have also pointed out that our outer bound on
the grandparent polytope of legal edge and grand-
parent vectors is tighter than the one presented by
Martins et al2009). What effect does this bound
have on the number of fractional solutions and the
overall accuracy?
To answer these questions we will focus on a set
of non-projective grandparent models, but point out
that our method and formulation can be easily ex-
tended to projective parsing as well as other types
of higher order edges. We use the Danish test data
of Buchholz and Marsi (2006) and the Italian and
Hungarian test datasets of Nivre et al2007).
6.1 Impact of Price and Cut
Table 1 compares brute force optimization (BF) with
the full model, in spirit of Martins et al2009),
to running parse, price and cut (PPC) on the same
model. This model contains all constraints presented
in 3.2. The table shows the average number of
parsed sentences per second, the average objective,
number of grandparent edges scored and added, all
relative to the brute force approach. We also present
the average unlabeled accuracy, and the percentage
of sentences with integer solutions. This number
shows us how often we not only found the optimal
solution to the LP relaxation, but also the optimal
solution to the full ILP.
We first note that both systems achieve the same
objective, and therefore, also the same accuracy.
This is expected, given that column and row gen-
eration are known to yield optimal solutions. Next
we see that the number of grandparent edges scored
and added to the problem is reduced to 5?13% of the
full model. This leads to up to 760% improvement
in speed. This improvement comes for free, without
any sacrifice in optimality or guarantees. We also
notice that in all cases at least 97% of the sentences
have no fractional solutions, and are therefore opti-
mal even with respect to the ILP. Table 1 also shows
that our bounds on reduced costs are relatively tight.
For example, in the case of Italian we score only
one percent more grandparent edges than we actu-
ally need to add.
Our fastest PCC parser processes about one sen-
tence per second. This speed falls below the reported
numbers of Martins et al2009) of about 0.6 sec-
onds per sentence. Crucially, however, in contrast to
their work, our speed is achieved without any first-
order pruning. In addition, we expect further im-
provements in runtime by optimizing the implemen-
tation of our pricing algorithm.
6.2 Tighter Grandparent Polytope
To investigate how the additional grandparent con-
straints in section 3.2 help, we compare three mod-
els, this time without PPC. The first model follows
Martins et al2009) and uses constraints ArcGP and
GPArc only. The second model uses only constraints
OneGP and NoGP. The final model incorporates all
four constraints.
Table 2 shows speed relative to the baseline model
with constraints ArcGP and GPArc, as well as the
percentage of integer solutions and the average un-
labeled accuracy?all for the Italian and Hungarian
datasets. We notice that the full model has less frac-
tional solutions than the partial models, and either
substantially (Italian) or slightly (Hungarian) faster
runtimes than ArcGP+GPArc. Interestingly, both
sets of constraints in isolation perform worse, in par-
ticular the OneGP and NoGP model.
7 Conclusion
We have presented a novel method for parsing in
second order grandparent models, and a general
blueprint for more efficient and optimal structured
prediction. Our method lazily instantiates candidate
parts based on their reduced cost, and on constraint
740
Italian Hungarian Danish
BF PPC BF PPC BF PPC
Sent./sec. relative to BF 100% 760% 100% 380% 100% 390%
GPs Scored relative to BF 100% 6% 100% 12% 100% 13%
GPs Added relative to BF 100% 5% 100% 7% 100% 7%
Objective rel. to BF 100% 100% 100% 100% 100% 100%
% of Integer Solutions 98% 98% 97% 97% 97% 97%
Unlabeled Acc. 88% 88% 81% 81% 88% 88%
Table 1: Parse, Price and Cut (PPC) vs Brute Force (BF). Speed is the number of sentences per second,
relative to the speed of BF. Objective, GPs scored and added are also relative to BF.
GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 1000% 1200%
% Integer 77% 9% 98%
Unlabeled Acc. 87% 85% 88%
(a) Italian
GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 162% 105%
% Integer 71% 3% 97%
Unlabeled Acc. 80% 77% 81%
(b) Hungarian
Table 2: Different outer bounds on the grandpar-
ent polytope, for nonprojective parsing of Italian and
Danish.
violations. This allows us to discard a large fraction
of parts during both scoring and optimization, lead-
ing to nearly 800% speed-ups without loss of accu-
racy and certificates. We also present a tighter bound
on the grandparent polytope that is useful in its own
right.
Delayed column and row generation is very useful
when solving large LPs with off-the-shelf solvers.
Given the multitude of work in NLP that uses LPs
and ILPs in this way (Roth and Yih, 2004; Clarke
and Lapata, 2007), we hope that our approach will
prove itself useful for other applications. We stress
that this approach can also be used when working
with dynamic programs, as pointed out in section
4.5, and therefore also in the context of dual de-
composition. This suggests even wider applicabil-
ity, and usefulness in various structured prediction
problems.
The underlying paradigm could also be useful for
more approximate methods. In this paradigm, al-
gorithms maintain an estimate of the cost of certain
resources (duals), and use these estimates to guide
search and the propose new structures. For exam-
ple, a local-search based dependency parser could
estimate how contested certain tokens, or edges, are,
and then use these estimates to choose better next
proposals. The notion of reduced cost can give guid-
ance on what such estimates should look like.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval and the Univer-
sity of Massachusetts and in part by UPenn NSF
medium IIS-0803847. We gratefully acknowledge
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
References
Ravindra K. Ahuja, Thomas L. Magnanti, and James B.
Orlin. 1993. Network Flows: Theory, Algorithms, and
Applications. Prentice Hall, 1 edition, February.
David Belanger, Alexandre Passos, Sebastian Riedel, and
Andrew McCallum. 2012. A column generation ap-
proach to connecting regularization and map infer-
ence. In Inferning: Interactions between Inference
and Learning, ICML 2012 Workshop.
741
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific, 2nd edition, September.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natu-
ral Language Learning (CoNLL? 06), CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ?05),
pages 173?180.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
?07), pages 1?11.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ?07), pages 81?88.
Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical a-star dependency parsing. In Pro-
ceedings of the workshop on Prospects and Advances
of the Syntax/Semantics Interface, Nancy, 2003, pp.85-
89.
P.C. Gilmore and R.E. Gomory. 1961. A linear program-
ming approach to the cutting-stock problem. Opera-
tions research, pages 849?859.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact viterbi parse selection. In Proceedings
of the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL ?03), pages 119?126.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL ?11).
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with nonprojective head automata. In
Proceedings of the Conference on Empirical methods
in natural language processing (EMNLP ?10).
Sandra K?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.
Marco L?bbecke and Jacques Desrosiers. 2004. Selected
topics in column generation. Operations Research,
53:1007?1023.
R. Kipp Martin, Ronald L. Rardin, and Brian A. Camp-
bell. 1990. Polyhedral characterization of discrete
dynamic programming. Oper. Res., 38(1):127?138,
February.
Andr? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP (ACL
?09), pages 342?350, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Andr? F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,
and M?rio A. T. Figueiredo. 2011. Dual decomposi-
tion with many overlapping components. In Proceed-
ings of the Conference on Empirical methods in natu-
ral language processing (EMNLP ?11), EMNLP ?11,
pages 238?249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ?06), pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?05), pages
91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In HLT-EMNLP, 2005.
Mathias Niepert. 2010. A delayed column generation
strategy for exact k-bounded map inference in markov
logic networks. In Proceedings of the 26th Annual
Conference on Uncertainty in AI (UAI ?10), pages
384?391, Corvallis, Oregon. AUAI Press.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Conference on Em-
pirical Methods in Natural Language Processing and
Natural Language Learning, pages 915?932.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
22nd AAAI Conference on Artificial Intelligence (AAAI
?07), pages 913?918.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?06), pages 129?137.
Sebastian Riedel and David A. Smith. 2010. Relaxed
marginal inference and its application to dependency
742
parsing. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ?10), pages 760?768, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ?08), pages 468?475.
D. Roth andW. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of the 8th Conference on Computational
Natural Language Learning (CoNLL? 04), pages 1?8.
Alexander Rush and Slav Petrov. 2012. Vine pruning for
efficient multi-pass dependency parsing. In Joint Hu-
man Language Technology Conference/Annual Meet-
ing of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL ?12).
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?10).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 145?156, Hon-
olulu, October.
D. Sontag and T. Jaakkola. 2007. New outer bounds on
the marginal polytope. In Advances in Neural Infor-
mation Processing Systems (NIPS ?07), pages 1393?
1400.
David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proceedings of the 24th An-
nual Conference on Uncertainty in AI (UAI ?08).
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Joint Human Lan-
guage Technology Conference/Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ?06), pages 423?
430.
743
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 810?820, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Improving NLP through Marginalization of Hidden Syntactic Structure
Jason Naradowsky, Sebastian Riedel, and David A. Smith
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, U.S.A.
{narad, riedel, dasmith}@cs.umass.edu
Abstract
Many NLP tasks make predictions that are in-
herently coupled to syntactic relations, but for
many languages the resources required to pro-
vide such syntactic annotations are unavail-
able. For others it is unclear exactly how
much of the syntactic annotations can be ef-
fectively leveraged with current models, and
what structures in the syntactic trees are most
relevant to the current task.
We propose a novel method which avoids
the need for any syntactically annotated data
when predicting a related NLP task. Our
method couples latent syntactic representa-
tions, constrained to form valid dependency
graphs or constituency parses, with the predic-
tion task via specialized factors in a Markov
random field. At both training and test time we
marginalize over this hidden structure, learn-
ing the optimal latent representations for the
problem. Results show that this approach pro-
vides significant gains over a syntactically un-
informed baseline, outperforming models that
observe syntax on an English relation extrac-
tion task, and performing comparably to them
in semantic role labeling.
1 Introduction
Many NLP tasks are inherently tied to syntax, and
state-of-the-art solutions to these tasks often rely on
syntactic annotations as either a source for useful
features (Zhang et al2006, path features in relation
extraction) or as a scaffolding upon which a more
narrow, specialized classification can occur (as of-
ten done in semantic role labeling). This decou-
pling of the end task from its intermediate repre-
sentation is sometimes known as the two-stage ap-
proach (Chang et al2010) and comes with several
drawbacks. Most notably this decomposition pro-
hibits the learning method from utilizing the labels
from the end task when predicting the intermediate
representation, a structure which must have some
correlation to the end task to provide any benefit.
Relying on intermediate representations that are
specifically syntactic in nature introduces its own
unique set of problems. Large amounts of syntac-
tically annotated data is difficult to obtain, costly
to produce, and often tied to a particular domain
that may vary greatly from that of the desired end
task. Additionally, current systems often utilize only
a small amount of the annotation for any particular
task. For instance, performing named entity recogni-
tion (NER) jointly with constituent parsing has been
shown to improve performance on both tasks, but
the only aspect of the syntax which is leveraged by
the NER component is the location of noun phrases
(Finkel and Manning, 2009). By instead discover-
ing a latent representation jointly with the end task
we address all of these concerns, alleviating the need
for any syntactic annotations, while simultaneously
attempting to learn a latent syntax relevant to both
the particular domain and structure of the end task.
We phrase the joint model as factor graph and
marginalize over the hidden structure of the inter-
mediate representation at both training and test time,
to optimize performance on the end task. Infer-
ence is done via loopy belief propagation, making
this framework trivially extensible to most graph
structures. Computation over latent syntactic rep-
810
resentations is made tractable with the use of special
combinatorial factors which implement unlabeled
variants of common dynamic-programming parsing
algorithms, constraining the hidden representation
to realize valid dependency graphs or constituency
trees.
We apply this strategy to two common NLP tasks,
coupling a model for the end task prediction with
latent and general syntactic representations via spe-
cialized logical factors which learn associations be-
tween latent and observed structure. In comparisons
with identical models which observe ?gold? syntac-
tic annotations, derived from off-the-shelf parsers or
provided with the corpora, we find that our hidden
marginalization method is comparable in both tasks
and almost every language tested, sometimes signifi-
cantly outperforming models which observe the true
syntax.
The following sections serves as a preliminary,
introducing an inventory of factors and variables
for constructing factor graph representations of
syntactically-coupled NLP tasks. Section 3 explores
the benefits of this method on relation extraction
(RE), where we compare the use dependency and
constituency structure as latent representations. We
then turn to a more established semantic role label-
ing (SRL) task (?4) where we evaluate across a wide
range of languages.
2 Latent Pseudo-Syntactic Structure
The models presented in this paper are phrased in
terms of variables in an undirected graphical model,
Markov random field. More specifically, we imple-
ment the model as a factor graph, a bipartite graph
composed of factors and variables in which we can
efficiently compute the marginal beliefs of any vari-
able set with the sum-product algorithm for cyclic
graphs, loopy belief propagation,. We now intro-
duce the basic variable and factor components used
throughout the paper.
2.1 Latent Dependency Structure
Dependency grammar is a lexically-oriented syn-
tactic formalism in which syntactic relationships
are expressed as dependencies between individual
words. Each non-root word specifies another as
its head, provided that the resulting structure forms
a valid directed graph, ie. there are no cycles in
the graph. Due to the flexibility of this representa-
tion it is often used to describe free-word-order lan-
guages, and increasingly preferred in NLP for more
language-in-use scenarios. A dependency graph can
be modeled with the following nodes, as first pro-
posed by Smith and Eisner (2008):
? Let {Link(i, j) : 0 ? i ? j ? n, n 6= j}
be O(n2) boolean variables corresponding to
the possible links in a dependency parse. Li,j
= true implies that there is a dependency from
parent i to child j.
? Let {LINK(i, j) : 0 ? i ? j ? n, n 6= j}
be O(n2) unary factors, each paired with a cor-
responding Link(i, j) variable and expressing
the independent belief that Link(i, j) = true.
2.2 Latent Constituency Structure
Alternatively we can describe the more structured
constituency formalism by setting up a representa-
tion over span variables:
? Let {Span(i, j) : 0 ? i < j ? n} be O(n2)
boolean variables such that Span(i, j) = true
iff there is a bracket spanning i to j 1.
? Let {SPAN(i, j) : 0 ? i < j ? n} be O(n2)
unary factors, each attached to the correspond-
ing Span(i, j) variable. These factors score the
independent suitability of each span to appear
in an unlabeled constituency tree.
All boolean variables presented in this paper will
be paired to unary factors in this manner, which
we will omit in future descriptions. This encom-
passes the necessary representational structure for
both syntactic formalisms, but nothing introduced
up to this point guarantees that either of these rep-
resentations will form a valid tree or DAG.
2.3 Combinatorial Factors
Naively constraining these latent representations
through the introduction of many interconnected
ternary factors is possible, but would likely be com-
putationally intractable. However, as observed in
1In practice, we do not need to include variables for spans
of width 1 or n, since they will always be true.
811
Smith and Eisner (2008), we can encapsulating
common dynamic programming algorithms within
special-purpose factors to efficiently globally con-
strain variable configurations . Since the outgoing
messages from such factors to a variable can be com-
puted from the factor?s posterior beliefs about that
variable, there is no difficulty in exchanging beliefs
between these special-purpose factors and the rest
of the graph, and inference can proceed using the
standard sum-product or max-product belief prop-
agation. Here we present two combinatorial factors
that provide efficient ways of constraining the model
to fit common syntactic frameworks.
? Let CKYTREE be a global combinatorial fac-
tor, as used in previous work in efficient pars-
ing (Naradowsky and Smith, 2012), attached to
all the Span(i, j) variables. This factor con-
tributes a factor of 1 to the model?s score iff the
span variables collectively form a legal, binary
bracketing and a factor of 0 otherwise. It en-
forces, therefore, a hard constraint on the vari-
ables, computing beliefs via an unlabeled vari-
ant of the inside-outside algorithm.
? Let DEP-TREE be a global combinatorial fac-
tor, as presented in Smith and Eisner (2008),
which attaches to all Link(i, j) variables and
similarly contributes a factor of 1 iff the config-
uration of Link variables forms a valid projec-
tive dependency graph. A graph is projective if
its edges do not cross.
2.4 Marginal MAP Inference
It is straightforward to train these latent variable
models to maximize the marginal probability of their
outputs, conditioning on their inputs, and marginal-
izing out the latent syntactic variables. To compute
feature expectations, we can use marginal inference
techniques such as sampling and sum-product belief
propagation to compute marginal probabilities.
A knottier problem arises when we want to find
the best assignment to the variables of interest
while marginalizing out ?nuisance? latent variables.
This is the problem of marginal MAP inference?
sometimes known as consensus decoding?which
has been shown to be NP-hard and without a poly-
nomial time approximation scheme (Sima?an, 1996;
Casacuberta and Higuera, 2000). In the NLP com-
munity, these inference problems often arise when
dealing with spurious ambiguity where multiple
derivations can lead to the same derived structure. In
tree substitution grammars, for instance, there may
be many ways of combining elementary trees to pro-
duce the same output tree; in machine translation,
many different elementary phrases or elementary
tree pairs might produce the same output string. For
syntactic parsing, Goodman (1996) proposed a vari-
ational method for summing out spurious ambiguity
that was equivalent to minimum Bayes risk decoding
(Goel and Byrne, 2000; Kumar and Byrne, 2004)
with a constituent-recall loss function. For MT,
May and Knight (2006) proposed methods for de-
terminizing tree automata to reduce ambiguity, and
Li et al2009) proposed a variational method based
on n-gram loss functions. More recently, Liu and Ih-
ler (2011) analyzed message-passing algorithms for
marginal MAP.
In this paper, we adopt a simple minimum Bayes
risk decoding scheme. First, we perform sum-
product belief propagation on the full factor graph.
Then, we maximize the expected accuracy of the
variables of interest, subject to any hard constraints
on them (such as mutual exclusion among labels). In
some cases with complex combinatorial constraints,
this simple MBR scheme has proved more effec-
tive than exact decoding over all variables (Auli and
Lopez, 2011).
3 Relation Extraction
Performing a syntax-based NLP task in most real-
world scenarios requires that the incoming data first
be parsed using a pre-trained parsing model. For
some tasks, like relation extraction, many data sets
lack syntactic annotation and these circumstances
persist even into the training phase. In this sec-
tion we explore such scenarios and contrast the use
of parser-provided syntactic annotation to marginal-
izing over latent representations of constituency or
dependency syntax. We show the hidden syntactic
models are not just competitive with these ?oracle?
models, but in some configurations can actually out-
perform them.
Relation extraction is the task of identifying se-
mantic relations between sets of entities in text (as
812
illustrated in Fig. 1b), and a good proving ground
for latent syntactic methods for two reasons. First,
because entities share a semantic relationship, un-
der most linguistic analyses these entities will also
share some syntactic relation. Indeed, syntactic fea-
tures have long been an extremely useful source
of information for relation extraction systems (Cu-
lotta and Sorensen, 2004; Mintz et al2009). Sec-
ondly, relation extraction has been a common task
for pioneering efforts in processing data mined from
the internet, and otherwise noisy or out-of-domain
data. In particular, large noisily-annotated data sets
have been generated by leveraging freely available
knowledge bases such as Freebase (Bollacker et al
2008; Mintz et al2009). Such data sets have been
utilized successfully for relation extraction from the
web (Bunescu and Mooney, 2007).
3.1 Model
We present a simple model for representing rela-
tional structure, with the only variables present be-
ing a set of boolean-valued variables representing an
undirected dependency between two entities, and an
additional set of boolean label variables representing
the type label of the relation.
? Let {Rel(i, j : 0 ? i < j ? n} be O(n2)
boolean variables such that Rel(i, j) = true iff
there is a relation spanning i to j.
? Let {Rel-Label(i, j, ?) : ? ? L, and 0 ? i <
j ? n} be O(|L|n2) boolean variables such
that Rel-Label(i, j, ?) = true iff there is a re-
lation spanning i to j with relation type ?.
? Let {ATMOST1(i, j) : 0 ? i < j ? n} be
O(n2) factors, each coordinating the set L of
possible nonterminal variables to the Rel vari-
able at each i, j tuple, allowing a Rel-Label
variable to be true iff all other label variables
are false and Rel(i, j) = true.
Here the Rel(i, j) and Rel-Label(i, j) variables
simply express the representation of the problem,
while the ATMOST1 factors are logical constraints
ensuring that only one label will apply to a particu-
lar relation.
3.2 Coordination Factors
An important contribution of this work is the intro-
duction of a flexible, general framework for connect-
ing the latent and observable partitions of the model.
We accomplish this through the use of two addi-
tional factors, each expressing the same basic logic,
which learn when to coordinate and when to ignore
correlations between the latent syntax and the end
task. While here we specify binary and ternary ver-
sions of these factors, they also generalize to higher
dimensions.
? Let {D-CONNECT(i, j, k) : 0 ? i < j ?
n; 0 ? k ? n} be O(n3) factors coordinating
any number of dependency syntax Link(i, j)
variables with representational variables on the
end task, multiplying in 1 to the model score
unless all variables are on, in which case it mul-
tiplies a connective potential ? derived from
its features. Thus it functions logically as a
soft NAND factor. In this ternary formulation k
represents a hidden dependency head or pivot
which is shared between two syntactic depen-
dencies anchored at the indices of the entities
in the relation (as illustrated in Fig. 1).
? Let {C-CONNECT(i, j) : 0 ? i < j ?
n} be O(n2) factors coordinating syntactic
Span(i, j) and relation arc Rel(i, j), identi-
cally to D-CONNECT but with a 1-to-1 map-
ping. Intuitively the joint model might learn
? > 1, i.e., constituency spans and task predic-
tion relations are more likely to be coterminous.
The difficulty in working with latent dependency
syntax is that we posit that the RE variables do not
share a 1-to-1 mapping with variables in the hid-
den representation. We expect instead, according
to linguistic intuition, that a relation between enti-
ties at position i and j in the sentence should have
corresponding syntactic dependencies but that they
are likely to realize this by sharing the same head
word (as depicted in Fig.1), a word whose identity
should help label the relation. Therefore we intro-
duce a special coordination factor, D-CONNECT as
a ternary factor to capture the relationship between
pairs of latent syntactic variables and a single rela-
tion variable, pivoting on the same unknown head
word.
813
Figure 1: Latent Dependency coupling for the RE task.
The D-CONNECT factor expresses ternary connection re-
lations because the shared head word of the proposed re-
lation is unknown. As is convention, variables are repre-
sented by circles, factors by rectangles.
We introduce six model scenarios.
? Baseline, simply the arc-factored model con-
sisting only of Rel and corresponding Label
variables for each entity. Features on the re-
lation factors, which are common to all model
configurations, are combinations of lexical in-
formation (i.e., the words that form the entity,
the pos-tags of the entities, etc.) as well as the
distance between the relation. This is a light-
weight model and generally does not attempt
to exhaustively leverage all possible proven
sources of useful features (Zhou et al2005)
towards a higher absolute score, but rather to
serve as a point of comparison to the models
which rely on syntactic information.
? Baseline-Ent, a variant of Baseline with addi-
tional features which include combinations of
mention type, entity type, and entity sub-type.
? Oracle D-Parse, in which we also instantiate a
full set of latent dependency syntax variables,
and connect them to the baseline model us-
ing D-CONNECT factors. Syntax variables are
clamped to their true values.
? Oracle C-Parse, the constituency syntax ana-
logue of Oracle D-Parse.
? Hidden D-Parse, which is an extension of Or-
acle D-Parse in which we connect all syntax
variables to a DEP-TREE factor, syntax vari-
ables are unobserved, and are learned jointly
with the end task. The features for latent syntax
are a subset of those used in dependency pars-
ing (McDonald et al2005).
? Hidden C-Parse, the constituency syntax ana-
logue of Hidden D-Parse. The feature set is
similar but bigrams are taken over the words
defining the constituent span, rather than the
words defining the head/modifier relation.
Coordination factor features for the syntactically-
informed models are particularly important. This
became evident in initial experiments where the
baseline was often able to outperform the hidden
syntactic model. However, inclusion of entity and
mention label features into the connection factors
provides the model with greater reasoning over
when to coordinate or ignore the relation predictions
with the underlying syntax. These are a proper sub-
set of the Baseline-Ent features.
3.3 Data
We evaluate these models using the 2005 Auto-
matic Content Extraction (ACE) data set (Walker,
2006), using the English (dual-annotated) and Chi-
nese (solely annotator #1 data set) sections. Each
corpus is annotated with entity mentions?tagged as
PER, ORG, LOC, or MISC?and, where applica-
ble, what type of relation exists between them (e.g.,
coarse: PHYS; fine: Located). But like most cor-
pora available for the task, the burden of acquiring
corresponding syntactic annotation is left to the re-
searcher. In this situation it is common to turn to
existing pre-trained parsing models.
We generate our data by first splitting the raw
text paragraphs into sentences. Chinese sentences
814
ACE Results
English Chinese
Unlabeled Labeled Unlabeled Labeled
Model P R F1 P R F1 P R F1 P R F1
Baseline 85.4 57.0 68.4 83.0 55.3 66.4 42.9 26.8 33.0 42.6 21.3 28.4
Baseline-Ent 87.2 65.4 74.8 85.8 64.4 73.6 55.2 31.1 39.8 51.2 29.4 37.4
Oracle D-Parse 89.3 67.4 76.8 89.3 66.2 75.4 60.0 32.6 42.2 58.1 31.3 40.7
Hidden D-Parse 87.8 69.8 77.7 85.3 67.8 75.6 48.0 32.0 38.4 47.2 30.0 36.7
Oracle C-Parse 89.1 68.7 77.6 87.5 67.5 76.2 66.8 37.8 48.3 63.8 37.0 46.8
Hidden C-Parse 90.5 69.9 78.9 88.8 68.6 77.4 56.3 32.3 41.0 53.4 31.6 39.7
Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the
syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on
the smaller Chinese data set.
are also tokenized according to Penn Chinese Tree-
bank standards (Xue et al2005). The sentences are
then tagged and parsed using the Stanford CoreNLP
tools, using the standard pre-trained models for tag-
ging (Toutanvoa and Manning, 2000), and the fac-
tored parsing model of Klein and Manning (2002).
The distributed grammar is trained on a variety of
sources, including the standard Wallstreet Journal
corpus, but also biomedical, translation, and ques-
tions. We then apply entity and relation annota-
tions noisily to the data, collapsing multi-word en-
tities into one term. We filter out sentences with
fewer than two entities (and are thus incapable of
containing relations) and sentences with more than
40 words (to keep the parses more reliable). This
yields 6966 sentences for English data, but unfortu-
nately only 747 sentences for the Chinese. Nine of
every ten sentences comprise the training set, with
every tenth sentence reserved for test.
3.4 Results
We train all models using 20 iterations of stochastic
gradient descent, each with a maximum of 10 BP it-
erations (though in practice we find convergence to
often occur much earlier). The results are presented
in Table 1, showing precision, recall, and F-measure
for both labeled and unlabeled prediction. For En-
glish, not only is the hidden marginalization method
a suitable replacement for the syntactic trees pro-
vided by pre-trained, state-of-the-art models, but in
both configurations we find that inducing an optimal
hidden structure is preferable to the parser-produced
annotations. On Chinese, where the data set is atyp-
ically small, we still observe improved performance
over the baseline in the constituency-based model
though it is not able to match the observed syntax
model.
Despite the intuition that both entities occupy
roles as modifiers of the same verb, we find that
the Hidden D-Parse model often fails to recover the
correct latent structure, and that even when success-
ful dependency parses are observed, the head word
is often not uniquely indicative of the relation type
(as known is not strongly correlated with the relation
type EMPLOYS in the phrase: Shigeru Miyamoto,
best known for his work at the video game company
Nintendo). Hence when it comes to relation extrac-
tion, at least on our relatively small data sets, we find
the simplest approach to latent syntactic structure is
the best.
We now turn to the task of semantic role label-
ing to evaluate this method on a more established
hand-annotated data set, and a more varied set of
languages.
4 Semantic Role Labeling
The task of semantic role labeling (SRL) aims to
detect and label the semantic relationships between
particular words, most commonly verbs (referred to
in the domain as predicates), and their arguments
(Meza-Ruiz and Riedel, 2009).
In a manner similar to RE, there is a strong corre-
lation between the presence of an SRL relation and
there existing an underlying syntactic dependency,
though this is not always expressed as directly as a
1-to-1 correspondence. This has historically moti-
vated a reliance on syntactic annotation, and some
of the most successful methods have simply applied
815
Pred
5
At Most 1
sense.
01
sense.
02
sense. 
|S|
.
d.) Sense Prediction
Arg
5, 1
Arg
5, 2
role
A0
role
A1
.  .  .
c.) Argument Prediction
b.) Syntactic Layer
Link
5, 1
D-Connect
5, 1
a.) Syntactic Combinatorial Constraint
DEP-Tree
Link
5, 2
D-Connect
5, 2
Link
5, 3
D-Connect
5, 3
At Most 1
Arg
5, 3
Link
5, n
role
A2
.   .   .
D-Connect
5, n
role
A-TM
.
.
Figure 2: A tiered graphic representing the three different SRL model configurations. The baseline system is described
in the bottom (c & d), the separate panels highlighting the independent predictions of this model: sense labels are
assigned in an entirely separate process from argument prediction. Pruning in the model takes place primarily in
this tier, since we observe true predicates we only instantiate over these indices. The middle tier (b.) illustrates the
syntactic representation layer, and the connective factors between syntax and SRL. In the observed syntax model
the Link variables are clamped to their correct values, with no need for a factor to coordinate them to form a valid
tree. Finally, the hidden model comprises all layers, including a combinatorial syntactic constraint (a.) over syntactic
variables. In this scenario all labels in (b.) are hidden at both training and test time.
feature-rich classifiers to the parsed trees. Related
work has recognized the large annotation burden the
task demands, but aimed to keep the syntactic anno-
tations and induce semantic roles (Lang and Lapata,
2010). In this section we will take the opposite ap-
proach, disregarding the syntactic annotations which
we argue are more costly to acquire, as they require
more formal linguistic training to produce.
4.1 Model
We present a simple, flexible model for SRL in
which sense predictions are made independently of
the rest of the model, and argument predictions are
made independently of each other. The model struc-
ture is composed as depicted in Fig. 2.
? Let {Arg(i, j) : 0 ? i < j ? n} be O(n2)
boolean variables such that Arg(i, j) = true
iff predicate i takes token j as an argument.
? Let {Role(i, j, ?) : ? ? L, and 0 ? i <
j ? n} be O(|L|n2) boolean variables such
that Role(i, j, ?) = true iff Arg(i, j) is true
and takes the role label ?.
? Let {Sense(i, ?) : ? ? S, and 0 ? i ?
n} be O(|S|n) boolean variables such that
Sense(i, ?) = true iff predicate i has sense
?.
4.1.1 Features
At the coarsest level both the SRL and RE models
are specifying binary predictions between a pair of
indices in the sentence, and a set of labels for each
dependency that happens to be true. Similarly we
use almost identical features in SRL as we did in
816
Figure 3: Examining the learned hidden representation for SRL. In this example the syntactic dependency arcs derived
from gold standard syntactic annotations (left) are entirely disjoint from the correct predicate/arguments pairs (shown
in the heatmaps by the squares outlined in black), and the observed syntax model fails to recover any of the correct
predictions. In contrast, the hidden model structure (right) learns a representation that closely parallels the desired end
task predictions, helping it recover three of the four correct SRL predictions (shaded arcs: red corresponds to a correct
prediction, with true labels GA, KARA, etc.), and providing some evidence towards the fourth. The dependency tree
corresponding to the hidden structure is derived by edge-factored decoding: dependency variables whose beliefs> 0.5
are classified as true (though some arcs not relevant to the SRL predictions are omitted for clarity).
RE, with the sole exception that we incorporate the
observable lemma and morphological features into
bigrams on predicate/argument pairs. For sense pre-
diction we rely only on unigram features taken in a
close (w = 2) window of the target predicate.
For the coordinating factors we use subsets of
combinations of word, part-of-speech, and capital-
ization features taken between head and argument,
and concatenate these with the distance and direc-
tion between the predicate and argument. We do not
find the performance of the system to be as sensi-
tive to which features are present in the coordinating
factors as we did in the RE task.
4.2 Data
We evaluate our SRL model using the data set devel-
oped for the CoNLL 2009 shared task competition
(Hajic? et al2009), which features seven languages
and provides an ideal opportunity to measure the
ability of the hidden structure to generalize across
languages of disparate origin and varied character-
istics. It also provides the opportunity to observe
a variety of different annotation styles and biases,
some of which our model was able to uncover as ill-
suited to common models for the task. The data it-
self provides word, lemma, part-of-speech, and mor-
phological feature information, along with gold de-
pendency parses. Words which denote predicates are
identified, and their (train time) arguments are pro-
vided. They are also annotated with a sense label
for each predicate, which is scored as an additional
SRL dependency. Thus the task involves predicting
for each predicate a set of argument dependencies
and the sense label associated with that predicate.
817
Unlabeled Labeled CoNLL 2009 F1
Data Model P R F1 P R F1 MAX. MEAN MED.
Catalan
Baseline 92.20 62.43 74.48 73.80 58.76 65.43
Oracle Syn. 98.48 96.17 97.31 70.42 68.78 69.59 80.3 71.0 74.1
Hidden Syn. 95.21 92.84 94.01 68.86 67.15 67.99
Chinese
Baseline 72.48 64.82 68.44 65.97 59.00 62.29
Oracle Syn. 98.57 78.98 87.69 87.64 70.22 77.97 78.6 72.2 70.4
Hidden Syn. 90.79 79.09 84.53 81.97 71.40 76.32
Czech
Baseline 97.73 56.50 71.61 84.80 48.80 61.84
Oracle Syn. 98.62 81.25 89.09 92.94 68.25 74.84 85.4 72.4 71.7
Hidden Syn. 92.39 89.35 90.85 74.41 71.96 73.16
English
Baseline 92.46 71.56 80.68 84.56 65.45 73.78
Oracle Syn. 96.75 82.25 88.91 85.48 72.67 78.55 85.6 75.6 72.1
Hidden Syn. 95.06 79.06 86.32 83.82 69.72 76.12
German
Baseline 93.49 44.24 60.06 75.00 35.49 48.18
Oracle Syn. 95.18 79.11 86.41 73.24 60.87 66.49 79.7 68.1 67.8
Hidden Syn. 91.92 86.26 89.00 69.47 65.19 67.26
Japanese
Baseline 91.64 43.36 58.87 80.41 38.05 51.66
Oracle Syn. 93.84 48.15 63.64 90.06 46.21 61.08 78.2 62.7 72.0
Hidden Syn. 90.88 73.47 81.25 73.42 59.36 65.65
Spanish
Baseline 82.90 39.47 53.48 67.64 32.21 43.64
Oracle Syn. 98.96 94.19 96.52 70.68 67.27 68.93 80.5 70.4 73.4
Hidden Syn. 96.15 90.53 93.25 68.81 64.79 66.74
Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained
using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where
poor sense prediction hindered absolute performance.
4.3 Results
We evaluate across a set of model configurations
analogous to before. All experiments used 30 itera-
tions of SGD with a Gaussian prior, and a max 10 it-
erations of BP to compute the marginals for each ex-
ample. In comparison to the CoNLL competition en-
tries (Table 2, rightmost columns) our syntactically-
informed models generally fall in the middle of the
rankings. This is not surprising given the indepen-
dent predictions of the model and the very general,
language universal assumptions we have made in the
model structure and feature sets. However, in terms
of gauging the usefulness of the hidden syntactic
marginalization method the results are extremely
compelling, with only marginal differences between
the performance of the observed-syntax model, es-
pecially relative to the baseline.
And despite the simplicity of the model, we still
manage to perform at state-of-the-art levels in a
few instances, sometimes outperforming most of the
competition entries without observing any syntax.
The performance on Chinese is an example of this,
with our system outperforming all but the best sys-
tem, and the hidden syntactic model only slightly
behind.
Abstracting away from the performance compar-
isons against other systems, the unlabeled results are
the more revealing evidence for the use of hidden
syntactic structure. Here the average hidden model
score (88.89) almost outperforms the observed syn-
tax model (90.22, and vs. 66.80 baseline), mostly
due to the large margins on the unlabeled Japanese
scores. The strong independence between sense
prediction and argument prediction hinders perfor-
mance on the labeled task, but on all languages we
find an extremely significant improvement exploit-
ing hidden syntactic structure in comparison to the
baseline system?the hidden model recovers more
than 92% of the gap between the baseline and the
observed syntax model. It is also interesting to note
that in the shared task competition the two languages
which systems lost the most performance between
their parsing F1 and their SRL F1 were Japanese
and German. As illustrated in Fig. 3, the corre-
818
spondence between syntax and SRL are extremely,
and systematically, poor. In this example our hid-
den structure model was able to assign strong beliefs
to the latent syntactic variables which correspond to
the correct predicate/argument pairs, allowing it to
correctly identify three of the four SRL arguments
when the joint model failed to recover one.
5 Related Work
This work is perhaps mostly closely related to
the Learning over Constrained Latent Representa-
tions (LCLR) framework of Chang et al2010).
Their abstract problem formulation is identical: both
paradigms seek to couple the end task to an interme-
diate representation which is not accessible to the
learning algorithm. However much of the intent,
scale, and methodology is different. LCLR aims
to provide a flexible latent structure for increasing
the representational power of the model in a use-
ful way, and is demonstrated on tasks and domains
where data availability is not a key concern. In con-
trast, while our hidden structure models may outper-
form their observed syntax counterparts, our focus
is as much on alleviating the burden of procuring
large amounts of syntactic annotation as it is about
increasing the expressiveness of the model. To that
end we constrain a more sophisticated latent repre-
sentation and couple it to highly structured output
predictions, opposed to binary classification prob-
lems. In methodology, we perform the more com-
putationally intensive marginalization operation in-
stead of maximizing.
Marginalization of hidden structure is also funda-
mental to other work, and featured most prominently
in generative Bayesian latent variable models (Teh
et al2006). Our approach is trained discrimina-
tively, affording the use of very rich feature sets and
the prediction of partial structures without needing
to specify a full derivation. Similar approaches have
been used in more linear latent variable CRF-based
models (McCallum et al2005), but these must only
marginalize only over hidden states of a much more
compact representation. Naively extending this to
tree-based constraints would often be computation-
ally inefficient, and we avoid intractability through
the encapsulation of much of the dynamic program-
ming machinery into specialized factors. Moreover,
using loopy belief propagation means that the in-
ference method is not closely coupled to the task
structure, and need not change when applying this
method to other types of graphs.
6 Conclusion
We have presented a novel method of coupling
syntactically-oriented NLP tasks to combinatorially-
constrained hidden syntactic representations, and
have shown that marginalizing over this latent rep-
resentation not only provides significant improve-
ments over syntactically-uninformed baselines, but
occasionally improves performance when compared
to systems which observe syntax. On the task of
relation extraction we find that a constituency rep-
resentation provides the most improvement over the
baseline, while in the SRL domain our model is ex-
tremely competitive with the best reported results on
Chinese, and outperforms the model using the pro-
vided parses on German and Japanese.
We believe this method delivers very promising
results in our presented tasks, opening the door to
new lines of research examining what types of con-
straints and what configurations of hidden struc-
ture are most beneficial for particular tasks and lan-
guages. Moreover, we present one type of coordinat-
ing factor, as both D-CONNECT and C-CONNECT
logically express a soft NAND function, but more
sophisticated coupling schemes are another natural
direction to pursue. Finally, we use sum-product
variant of belief propagation inference, but more
specialized inference schemes may show additional
benefits.
Acknowledgements
We would like to thank Andrea Gesmundo for help in
procuring sections of the CoNLL 2009 shared task data.
This work was supported in part by the Center for Intel-
ligent Information Retrieval and in part by Army prime
contract number W911NF-07-1-0216 and University of
Pennsylvania subaward number 103-548106. The Uni-
versity of Massachusetts also gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract no.
FA8750-09-C-0181. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
view of the DARPA, AFRL, or the US government.
819
References
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated CCG supertagging and parsing. In ACL,
pages 470?480.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In SIGMOD, pages 1247?1250, New
York, NY, USA. ACM.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In ACL.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In ICGI, pages 15?24.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained latent
representations. In NAACL.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL, Barcelona,
Spain.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In
NAACL, pages 326?334.
Vaibbhava Goel and William J. Byrne. 2000. Minimum
Bayes risk automatic speech recognition. Computer
Speech and Language, 14(2):115?135.
Joshua T. Goodman. 1996. Parsing algorithms and met-
rics. In ACL, pages 177?183.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL: Shared Task,
pages 1?18.
Dan Klein and Chris Manning. 2002. Fast exact infer-
ence with a factored model for natural language pro-
cessing. In NIPS.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL, pages 169?176.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In HLT-NAACL, pages 939?
947.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593?601.
Qiang Liu and Alexander Ihler. 2011. Variational algo-
rithms for marginal MAP. In UAI, pages 453?462.
Jonathan May and Kevin Knight. 2006. A better n-best
list: Practical determinization of weighted finite tree
automata. In HLT-NAACL, pages 351?358.
Andrew McCallum, Kedar Bellare, and Fernando C. N.
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In UAI, pages 388?395.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT-EMNLP,
pages 523?530.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
Markov logic. In NAACL.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In ACL, pages 1003?1011.
Jason Naradowsky and David A. Smith. 2012. Combina-
torial constraints for constituency parsing in graphical
novels. Technical report, University of Massachusetts
Amherst.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In COLING, pages 1175?1180.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145?
156.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Kristina Toutanvoa and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP, pages 63?
70.
Christopher Walker. 2006. Ace 2005 multilingual train-
ing corpus. number ldc2006t06. In Linguistic Data
Consortium.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, pages 207?238.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In NAACL, pages 288?295.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In ACL, pages 427?434.
820
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 729?732,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Constraint-Driven Rank-Based Learning for Information Extraction
Sameer Singh Limin Yao Sebastian Riedel Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst MA 01003
{sameer,lmyao,riedel,mccallum}@cs.umass.edu
Abstract
Most learning algorithms for undirected
graphical models require complete inference
over at least one instance before parameter up-
dates can be made. SampleRank is a rank-
based learning framework that alleviates this
problem by updating the parameters during in-
ference. Most semi-supervised learning algo-
rithms also perform full inference on at least
one instance before each parameter update.
We extend SampleRank to semi-supervised
learning in order to circumvent this compu-
tational bottleneck. Different approaches to
incorporate unlabeled data and prior knowl-
edge into this framework are explored. When
evaluated on a standard information extraction
dataset, our method significantly outperforms
the supervised method, and matches results of
a competing state-of-the-art semi-supervised
learning approach.
1 Introduction
Most supervised learning algorithms for undirected
graphical models require full inference over the
dataset (e.g., gradient descent), small subsets of the
dataset (e.g., stochastic gradient descent), or at least
a single instance (e.g., perceptron, Collins (2002))
before parameter updates are made. Often this is the
main computational bottleneck during training.
SampleRank (Wick et al, 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within inference. Ev-
ery pair of samples generated during inference is
ranked according to the model and the ground truth,
and the parameters are updated when the rankings
disagree. SampleRank has enabled efficient learn-
ing for massive information extraction tasks (Culotta
et al, 2007; Singh et al, 2009).
The problem of requiring a complete inference it-
eration before parameters are updated also exists in
the semi-supervised learning scenario. Here the sit-
uation is often considerably worse since inference
has to be applied to potentially very large unlabeled
datasets. Most semi-supervised learning algorithms
rely on marginals (GE, Mann and McCallum, 2008)
or MAP assignments (CODL, Chang et al, 2007).
Calculating these is computationally inexpensive for
many simple tasks (such as classification and re-
gression). However, marginal and MAP inference
tends to be expensive for complex structured pre-
diction models (such as the joint information extrac-
tion models of Singh et al (2009)), making semi-
supervised learning intractable.
In this work we employ a fast rank-based learning
algorithm for semi-supervised learning to circum-
vent the inference bottleneck. The ranking function
is extended to capture both the preference expressed
by the labeled data, and the preference of the domain
expert when the labels are not available. This allows
us to perform SampleRank as is, without sacrificing
its scalability, which is crucial for future large scale
applications of semi-supervised learning.
We applied our method to a standard information
extraction dataset used for semi-supervised learning.
Empirically we demonstrate improvements over the
supervised model, and closely match the results of a
competing state-of-the-art semi-supervised learner.
2 Background
Conditional random fields (Lafferty et al, 2001) are
undirected graphical models represented as factor
729
graphs. A factor graph G = {?i} defines a prob-
ability distribution over assignments y to a set of
output variables, conditioned on an observation x.
A factor ?i computes the inner product between
the vector of sufficient statistics f(xi,yi) and pa-
rameters ?. Let Z(x) be the data-dependent par-
tition function used for normalization. The proba-
bility distribution defined by the graph is:
p(y|x,?) =
1
Z(x)
?
?i?G
e??f(xi,yi)
2.1 Rank-Based Learning
SampleRank (Wick et al, 2009) is a rank-based
learning framework for that performs parameter up-
dates within MCMC inference. Every pair of con-
secutive samples in the MCMC chain is ranked ac-
cording to the model and the ground truth, and the
parameters are updated when the rankings disagree.
This allows the learner to acquire more supervision
per sample, and has led to efficient training of mod-
els for which inference is very expensive (Singh
et al, 2009).
SampleRank considers two ranking functions: (1)
the unnormalized conditional probability (model
ranking), and (2) a truth function F(y) (objective
ranking) which is defined as ?L(y,yL), the neg-
ative loss between the possible assignment y and
the true assignment yL. The truth function can take
different forms, such as tokenwise accuracy or F1-
measure with respect to some labeled data.
In order to learn the parameters for which model
rankings are consistent with objective rankings,
SampleRank performs the following update for each
consecutive pair of samples ya and yb of the MCMC
chain. Let ? be the learning rate, and ? =
f(xi,yai )? f(xi,y
b
i ), then ? is updated as follows:
?
+
?
?
??
??
?? if p(y
a|x)
p(yb|x) < 1 ? F(y
a) > F(yb)
??? if p(y
a|x)
p(yb|x) > 1 ? F(y
a) < F(yb)
0 otherwise.
This update is usually fast: in order to calculate
the required model ratio, only factors that touch
changed variables have to be taken into account.
SampleRank has been incorporated into the FAC-
TORIE toolkit for probabilistic programming with
imperatively-defined factor graphs (McCallum et al,
2009).
3 Semi-Supervised Rank-Based Learning
To apply SampleRank to the semi-supervised set-
ting, we need to specify the truth function F over
both labeled and unlabeled data. For labeled data
YL, we can use the true labels. These are not avail-
able for unlabeled data YU , and we present alterna-
tive ways of defining a truth function FU : YU ? <
for this case.
3.1 Self-Training
Self-training, which uses predictions as truth, fits di-
rectly into our SampleRank framework. After per-
forming SampleRank on training data (using FL),
MAP inference is performed on the unlabeled data.
The prediction y?U is used as the ground truth for
the unlabeled data. Thus the self-training objective
function Fs over the unlabeled data can be defined
as Fs(y) = ?L(y, y?U ).
3.2 Encoding Constraints
Constraint-driven semi-supervised learning uses
constraints to incorporate external domain knowl-
edge when labels are missing (Chang et al, 2007;
Mann and McCallum, 2008; Bellare et al, 2009).
Constraints prefer certain label configurations over
others. For example, one constraint may be that oc-
currences of the word ?California? are preferred to
have the label ?location?.
We can encode constraints directly into the objec-
tive function FU . Let a constraint i be specified as
?pi, ci?, where ci(y) denotes whether assignment y
satisfies the constraint i (+1), violates it (?1), or the
constraint does not apply (0), and pi is the constraint
strength. Then the objective function is:
Fc(y) =
?
i
pici(y)
3.3 Incorporating Model Predictions
When the objective function Fc is used, every pre-
diction on unlabeled data is ranked only according to
the constraints, and thus the model is trained to sat-
isfy all the constraints. This is a problem when the
constraints prefer a wrong solution while the model
favors the correct solution, resulting in SampleR-
ank updating the model away from the true solution.
To avoid this, the ranking function needs to balance
preferences of the constraints and the current model.
730
One option is to incorporate the self-training ob-
jective function Fs. A new objective function that
combines self-training with constraints can be de-
fined as:
Fsc(y) = Fs(y) + ?sFc(y)
= ?L(y, y?U ) + ?s
?
i
pici(y)
This objective function has at least two limita-
tions. First, self-training involves a complete infer-
ence step to obtain y?U . Second, the model might
have low confidence in its prediction (this is the case
when the underlying marginals are almost uniform),
but the self-training objective des not take this into
account. Hence, we also propose an objective func-
tion that incorporates the model score directly, i.e.
Fmc(y) = log p(y|x,?) + logZ(x) + ?mFc(y)
=
?
?i
? ? f(xi,yi) + ?m
?
i
pici(y)
This objective does not require inference, and also
takes into account model confidence.
In both objective functions Fsc and Fmc, ? con-
trols the relative contribution of the constraint pref-
erences to the objective function. With higher ?,
SampleRank will make updates that never try to vi-
olate constraints, while with low ?, SampleRank
trusts the model more. ? corresponds to constraint
satisfaction weights ? used in (Chang et al, 2007).
4 Related Work
Chang et al propose constraint-driven learn-
ing (CODL, Chang et al, 2007) which can be in-
terpreted as a variation of self-training: Instances
are selected for supervision based not only on the
model?s prediction, but also on their consistency
with a set of user-defined constraints. By directly in-
corporating the model score and the constraints (as
inFmc in Section 3.3) we follow the same approach,
but avoid the expensive ?Top-K? inference step.
Generalized expectation criterion (GE, Mann and
McCallum, 2008) and Alternating Projections (AP,
Bellare et al, 2009) encode preferences by speci-
fying constraints on feature expectations, which re-
quire expensive inference. Although AP can use on-
line training, it still involves full inference over each
instance. Furthermore, these methods only support
constraints that factorize according to the model.
Li (2009) incorporates prior knowledge into con-
ditional random fields as variables. They require full
inference during learning, restricting the application
to simple models. Furthermore, higher-order con-
straints are specified using large cliques in the graph,
which slow down inference. Our approach directly
incorporates these constraints into the ranking func-
tion, with no impact on inference time.
5 Experiments
We carried out experiments on the Cora citation
dataset. The task is to segment each citation into
different fields, such as ?author? and ?title?. We use
300 instances as training data, 100 instances as de-
velopment data, and 100 instances as test data. Some
instances from the training data are selected as la-
beled instances, and the remaining data (including
development) as unlabeled. We use the same token-
label constraints as Chang et al (2007).
We use the objective functions defined in Sec-
tion 3, specifically self-training (Self:Fs), direct
constraints (Cons:Fc), the combination of the two
(Self+Cons:Fsc), and combination of the model
score and the constraints (Model+Cons:Fmc). We
set pi = 1.0, ? = 1.0, ?s = 10, and ?m = 0.0001.
Average token accuracy for 5 runs is reported and
compared with CODL1 in Table 1. We also report
supervised results from (Chang et al, 2007) and
SampleRank. All of our methods show vast im-
provement over the supervised method for smaller
training sizes, but this difference decreases as the
training size increases. When the complete training
data is used, additional unlabeled data hurts our per-
formance. This is not observed in CODL since they
use more unlabeled data, which may also explain
their slightly higher accuracy. Note that Self+Cons
performs better than Self or Cons individually.
Model+Cons also performs competitively, and
may potentially outperform other methods if a bet-
ter ?m is chosen. Note, however, that ?m is much
harder to tune than ?s since ?m weighs the contri-
bution of the unnormalized model score, the range
1We report inference without constraints results from
CODL. Their results that incorporated constraints were higher,
but we do not implement this alternative due to the difficulty in
balancing the model score and constraint weights.
731
Method 5 10 15 20 25 300
Sup. (CODL) 55.1 64.6 68.7 70.1 72.7 86.1
SampleRank 66.5 74.6 75.6 77.6 79.5 90.7
CODL 71 76.7 79.4 79.4 82 88.2
Self 67.6 75.1 75.8 78.6 80.4 88
Cons 67.2 75.3 77.5 78.6 79.4 88.3
Self+Cons 71.3 77 77.5 79.5 81.1 87.4
Model+Cons 69.8 75.4 75.7 79.3 79.3 90.6
Table 1: Tokenwise Accuracy: for different methods as we vary the size of the labeled data
of which depends on many different factors such as
properties of the data, the learning rate, number of
samples, proposal function, etc. For self+cons (?s),
the ranges of the predictions and constraint penalties
are fixed and known, making the task simpler.
Self training takes 90 minutes to run on average,
while Self+Cons and Model+Cons need 100 min-
utes. Since the Cons method skips the inference
step over unlabeled data, it takes only 30 minutes
to run. As the size of the model and unlabeled data
set grows, this saving will become more significant.
Running time of CODL was not reported.
6 Conclusion
This work extends the rank-based learning frame-
work to semi-supervised learning. By integrating
the two paradigms, we retain the computational effi-
ciency provided by parameter updates within infer-
ence, while utilizing unlabeled data and prior knowl-
edge. We demonstrate accuracy improvements on a
real-word information extraction dataset.
We believe that the method will be of greater ben-
efit to learning in complex factor graphs such as
joint models over multiple extraction tasks. In future
work we will investigate our approach in such set-
tings. Additionally, various sensitivity, convergence,
and robustness properties of the method need to be
analyzed.
Acknowledgments
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by SRI Inter-
national subcontract #27-001338 and ARFL prime
contract #FA8750-09-C-0181, and in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation under
NSF grant #IIS-0326249. Any opinions, findings
and conclusions or recommendations expressed in
this material are the authors? and do not necessarily
reflect those of the sponsor.
References
Kedar Bellare, Gregory Druck, and Andrew McCallum.
Alternating projections for learning with expectation
constraints. In UAI, 2009.
Mingwei Chang, Lev Ratinov, and Dan Roth. Guiding
semi-supervision with constraint-driven learning. In
ACL, 2007.
Michael Collins. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithm. In ACL, 2002.
Aron Culotta, Michael Wick, and Andrew McCallum.
First-order probabilistic models for coreference reso-
lution. In NAACL/HLT, 2007.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: probabilistic models for
segmenting and labeling sequence data. In ICML,
2001.
Xiao Li. On the use of virtual evidence in conditional
random fields. In EMNLP, 2009.
Gideon S. Mann and Andrew McCallum. Generalized ex-
pectation criteria for semi-supervised learning of con-
ditional random fields. In ACL, 2008.
Andrew McCallum, Karl Schultz, and Sameer Singh.
FACTORIE: probabilistic programming via impera-
tively defined factor graphs. In NIPS, 2009.
Sameer Singh, Karl Schultz, and Andrew McCallum.
Bi-directional joint inference for entity resolution
and segmentation using imperatively-defined factor
graphs. In ECML/PKDD, 2009.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. SampleRank: Learning pref-
erences from atomic gradients. In NIPS Workshop on
Advances in Ranking, 2009.
732
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 760?768,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Relaxed Marginal Inference and its Application to Dependency Parsing
Sebastian Riedel David A. Smith
Department of Computer Science
University of Massachusetts, Amherst
{riedel,dasmith}@cs.umass.edu
Abstract
Recently, relaxation approaches have been
successfully used for MAP inference on NLP
problems. In this work we show how to extend
the relaxation approach to marginal inference
used in conditional likelihood training, pos-
terior decoding, confidence estimation, and
other tasks. We evaluate our approach for the
case of second-order dependency parsing and
observe a tenfold increase in parsing speed,
with no loss in accuracy, by performing in-
ference over a small subset of the full factor
graph. We also contribute a bound on the error
of the marginal probabilities by a sub-graph
with respect to the full graph. Finally, while
only evaluated with BP in this paper, our ap-
proach is general enough to be applied with
any marginal inference method in the inner
loop.
1 Introduction
In statistical natural language processing (NLP) we
are often concerned with finding the marginal proba-
bilities of events in our models or the expectations of
features. When training to optimize conditional like-
lihood, feature expectations are needed to calculate
the gradient. Marginalization also allows a statis-
tical NLP component to give confidence values for
its predictions or to marginalize out latent variables.
Finally, given the marginal probabilities of variables,
we can pick the values that maximize these marginal
probabilities (perhaps subject to hard constraints) in
order to predict a good variable assignment.1
1With a loss function that decomposes on the variables, this
amounts to Minimum Bayes Risk (MBR) decoding, which is
Traditionally, marginal inference in NLP has been
performed via dynamic programming (DP); how-
ever, because this requires the model to factor in
a way that lends itself to DP algorithms, we have
to restrict the class of probabilistic models we con-
sider. For example, since we cannot derive a dy-
namic program for marginal inference in second or-
der non-projective dependency parsing (McDonald
and Satta, 2007), we have non-projective languages
such as Dutch using second order projective mod-
els if we want to apply DP. Some previous work
has circumvented this problem for MAP inference
by starting with a second-order projective solution
and then greedily flipping edges to find a better non-
projective solution (McDonald and Pereira, 2006).
In order to explore richer model structures, the
NLP community has recently started to investigate
the use of other, well-known machine learning tech-
niques for marginal inference. One such technique is
Markov chain Monte Carlo, and in particular Gibbs
sampling (Finkel et al, 2005), another is (loopy)
sum-product belief propagation (Smith and Eisner,
2008). In both cases we usually work in the frame-
work of graphical models?in our case, with factor
graphs that describe our distributions through vari-
ables, factors, and factor potentials. In theory, meth-
ods such as belief propagation can take any graph
and perform marginal inference. This means that we
gain a great amount of flexibility to represent more
global and joint distributions for NLP tasks.
The graphical models of interest, however, are
often too large and densely connected for efficient
inference in them. For example, in second order
often very effective.
760
dependency parsing models, we have O(n2) vari-
ables and O(n3) factors, each of which may have
to be inspected several times. While belief prop-
agation is still tractable here (assuming we follow
the approach of Smith and Eisner (2008) to enforce
tree constraints), it is still much slower than sim-
pler greedy parsing methods, and the advantage sec-
ond order models give in accuracy is often not sig-
nificant enough to offset the lack of speed in prac-
tice. Moreover, if we extend such parsing models to,
say, penalizing all pairs of crossing edges or scoring
syntax-based alignments, we will need to inspect at
least O
(
n4
)
factors, increasing our efficiency con-
cerns.
When looking at the related task of finding the
most likely assignment in large graphical models
(i.e., MAP inference), we notice that several recent
approaches have significantly sped up computation
through relaxation methods (Tromble and Eisner,
2006; Riedel and Clarke, 2006). Here we start with
a small subset of the full graph, and run inference
for this simpler problem. Then we search for factors
that are ?violated? in the solution, and add them to
the graph. This is repeated until no more new factors
can be added. Empirically this approach has shown
impressive success. It often dramatically reduces the
effective network size, with no loss in accuracy.
How can we extend or generalize MAP relax-
ation algorithms to the case of marginal inference?
Roughly speaking, we answer it by introducing a
notion of factor gain that is defined as the KL di-
vergence between the current distribution with and
without the given factor. This quantity is then used
in an algorithm that starts with a sub-model, runs
marginal inference in it and then determines the
gains of the not-yet-added factors. In turn, all fac-
tors for which the gain exceeds some threshold are
added to the current model. This process is repeated
until no more new factors can be found or a maxi-
mum number of iterations is reached.
We evaluate this form of relaxed marginal infer-
ence for the case of second-order dependency pars-
ing. We follow Smith and Eisner?s tree-aware be-
lief propagation procedure for inference in the inner
loop of our algorithm. This leads to a tenfold in-
crease in parsing speed with no loss in accuracy.
We also contribute a bound on the error on
marginal probabilities the sub-graph defines with re-
spect to the full graph. This bound can be used both
for terminating (although not done here) and under-
standing the dynamics of inference. Finally, while
only evaluated with BP so far, it is general enough
to be applied with any marginal inference method in
the inner loop.
In the following, we first give a sketch of the
graphical model we apply. Then we briefly discuss
marginal inference. In turn we describe our relax-
ation algorithm for marginal inference and some of
its theoretic guarantees. Then we present empirical
support for the effectiveness of our approach, and
conclude.
2 Graphical Models of Dependency Trees
We give a brief overview of the graphical model we
apply in our experiments. We chose the grandpar-
ents and siblings model, together with language spe-
cific multiroot and projectivity options as taken from
Smith and Eisner (2008). All our models are defined
over a set of binary variables Lij that indicate a de-
pendency between token i and j of the input sen-
tence W .
2.1 Markov Random Fields
Following Smith and Eisner (2008), we define a
probability distribution over all dependency trees as
a collection of edges y for a fixed input sentence
W . This distribution is represented by an undirected
graphical model, or Markov random field (MRF):
pF (y)
def
=
1
Z
?
i?F
?i (y) (1)
specified by an index set F and a corresponding
family (?i)F of factors ?i : Y 7? <
+. Here Z
is the partition function ZF =
?
y
?
i ?i (y).
We will restrict our attention to binary factors that
can be represented as ?i (y) = e?i?i(y) with binary
functions ?i (y) ? {0, 1} and weights ?i ? <.2 This
2These ?i are also called sufficient statistics or feature func-
tions, not to be confused with the features whose weighted sum
forms the weight ?i. The restriction to binary functions is with-
out loss of generality since we can combine constraints on par-
ticular variable assignments into potential tables with several
dimensions.
761
leads to
pF (y)
def
=
1
Z
exp
(
?
i?F
?i?i (y)
)
as an alternative representation for pF . Note that
when ?i (y) = 1 we will say that ?i fires for y.
Note that a factor function ?i(y) can depend on
any part of the observed input sentenceW ; however,
for brevity we will suppress this extra argument to
?i.
2.2 Hard and Soft Constraints on Trees
A particular model specifies its preference for set of
dependency edges over another by a set of hard and
soft constraints. We use hard constraints to rule out
a priori illegal structures, such as trees where a word
has two parents, and soft constraints to raise or lower
the score of trees that contain particular good or bad
substructures.
A hard factor (or constraint) ?i evaluates an as-
signment y with respect to some specified condi-
tion and fires only if this condition is violated; in
this case it evaluates to 0. It is therefore ruling out
all configurations in which the condition does not
hold. Note that a hard constraint ?i corresponds to
?i = ?? in our loglinear representation.
For dependency parsing, we consider two partic-
ular hard constraints, each of which touches all edge
variables in y: the constraint Tree requires that all
edges form a directed spanning tree rooted at the
root node 0; the constraint PTree enforces the more
stringent condition that all edges form a projective
directed tree. As in (Smith and Eisner, 2008), we
used algorithms from edge-factored parsing to com-
pute BP messages for these factors. In our experi-
ments, we enforced one or the other constraint de-
pending on the projectivity of given treebank data.
A soft factor ?i acts as a soft constraint that
prefers some assignments to others. This is equiv-
alent to saying that its weight ?i is finite. Note that
the weight of a soft factor is usually itself composed
as a sum of (sub-)weights wj for feature functions
that have the same input-output behavior as ?i (y)
when conditioned on the current sentence. It is these
wj which are adjusted at training time.
We use three kinds of soft factors from Smith and
Eisner (2008). In the full model, there are: O(n2)
LINKi,j factors that judge dependency edges in iso-
lation; O(n3) GRANDi,j,k factors that judge pairs
of dependency edges in a grandparent-parent-child
chain; and O(n3) SIBi,j,k factors that judge pairs of
dependency edges that share the same parent.
3 Marginal Inference
Formally, given our set of factors F and an observed
sentence W , marginal inference amounts to calcu-
lating the probability ?Fi that our binary features ?i
are active. That is, for each factor ?i
?Fi
def
=
?
?i(y)=1
pF (y) = EF [?i] (2)
For compactness, we follow the convention of Wain-
wright and Jordan (2008) and represent the belief for
a variable using the marginal probability of its cor-
responding unary factor. Hence, if we want to calcu-
late pF (Lij) we use ?FLINKij in place. Moreover we
will use ?F?i
def
= 1??Fi when we need the probability
of the event ?i (y) = 0.
The two most prominent approaches to marginal
inference in general graphical models are Markov
Chain Monte Carlo (MCMC) and variational meth-
ods. In a nutshell, MCMC iteratively generates a
Markov chain that yields pF as its stationary distri-
bution. Any expectation ?Fi can then be calculated
simply by counting the corresponding statistics in
the generated chain.
Generally speaking, variational methods frame
marginal inference as an optimization problem. Ei-
ther in the sense of minimizing the KL divergence
of a much simpler distribution to the actual distribu-
tion pF , as in mean field methods. Or in the sense of
maximizing a variational representation of the log-
partition function over the setM of valid mean vec-
tors (Wainwright and Jordan, 2008). Note that the
variational representation of the log partition func-
tion involves an entropy term that is intractable to
calculate in general and therefore usually approxi-
mated. Likewise, the set of constraints that guaran-
tee vectors ? to be valid mean vectors is intractably
large and is often simplified.
Because we use belief propagation (BP) as base-
line to compare to, and as a subroutine in our pro-
posed algorithm, a brief characterization of it is in
order. BP can be seen as a variational method that
762
uses the Bethe Free Energy as approximation to the
entropy, and the setML of locally consistent mean
vectors as an outer bound onM. A mean vector is
locally consistent if its beliefs on factors are consis-
tent with the beliefs of the factor neighbors.
BP solves the variational problem by iteratively
updating the beliefs of factors and variables based
on the current beliefs of their neighbors. When ap-
plied to acyclic graphical models BP yields the exact
marginals at convergence. For general graphs, BP is
not guaranteed to converge, and the beliefs it calcu-
lates are generally not the true marginals; however,
in practice BP often does converge and lead to accu-
rate marginals.
4 Relaxed Incremental Marginal Inference
Generally the runtime and accuracy of a marginal in-
ference method depends on size, density, tree-width
and interaction strength (i.e. the magnitude of its
weights) of the Graphical Model. For example, in
Belief Propagation the number of messages we have
to send in each iteration scales with the number of
factors (and their degrees). This means that when
we add a large number of extra factors to our model,
such as the O(n3) grandparent and sibling factors
for dependency parsing, we have to pay a price in
terms of speed, sometimes even accuracy.
However, on close inspection often many of the
additional factors we use to model some higher or-
der interactions are somewhat unnecessary or redun-
dant. To illustrate this, let us look at a second or-
der parsing model with grandparent factors. Surely
determiners are not heads of other determiners, and
this should be easy to encourage using LINK fea-
tures only. Hence, a grandparent factor that dis-
courages a determiner-determiner-determiner chain
seems unnecessary.
This raises two questions: (a) can we get away
without most of these factors, and (b) can we effi-
ciently tell which factors should be discarded. We
will see in section 5 that question (a) can be an-
swered affirmatively: with a only fraction of all sec-
ond order factors we can calculate marginals that are
very close to the BP marginals, and when used in
MBR decoding, lead to the same trees.
Question (b) can be approached by looking at how
a similar problem has been tackled in combinato-
rial optimization and MAP inference. Riedel and
Clarke (2006) tackled the MAP problem for depen-
dency parsing by an incremental approach that starts
with a relaxation of the problem, solves it, and adds
additional constraints only if they are violated. If
constraints were added, the process is repeated, oth-
erwise we terminate.
4.1 Evaluating Candidate Factors
To develop such an incremental relaxation approach
to marginal inference, we generalize the notion of a
violated constraint. What does it mean for a factor to
be violated with respect to the solution of a marginal
inference problem?
One answer is to interpret the violation of a con-
straint as ?adding this constraint will impact our cur-
rent belief?. To assess the impact of adding factor
?i to a sub-graph F ? ? F we can then use the fol-
lowing intuition: if the distribution F ? ? {i} is very
similar to the distribution corresponding to F ?, it is
probably safe to say that the marginals we get from
both are close, too. If we use the KL divergence be-
tween the (distributions of) F ? ? {i} and F ? for our
interpretation of the above mentioned closeness, we
can define a potential gain for adding ?i as follows:
gF ? (?i)
def
= DKL
(
pF ? ||pF ??{i}
)
.
Together with a threshold  on this gain we can
now adapt the relaxation approach to marginal in-
ference by simply replacing the question, ?Is ?i vi-
olated?? with the question, ?Is gF ? (i) > ?? We
can see the latter question as a generalization of the
former if we interpret MAP inference as the zero-
temperature limit of marginal inference (Wainwright
and Jordan, 2008).
The form of the gain function is chosen to be eas-
ily evaluated using the beliefs we have already avail-
able for the current sub-graph F ?. It is easy to show
(see Appendix) that the following holds:
Proposition 1. The gain of a factor ?i with respect
to the sub-graph F ? ? F is
gF ? (?i) = log
(
?F
?
?i + ?
F ?
i e
?i
)
? ?F
?
i ?i (3)
That is, the gain of a factor ?i depends on two
properties of ?i. First, the expectation ?F
?
i that
?i fires under the current model F ?, and second,
763
its loglinear weight ?i. To get an intuition for this
gain, consider the limit lim?F?i ?1
gF ? (?i) of a fac-
tor with positive weight that is expected to be active
under F ?. In this case the gain becomes zero, mean-
ing that the more likely ?i fires under the current
model, the less useful will it be to add according to
our gain. For lim?F?i ?0
gF ? (?i) the gain also disap-
pears. Here the confidence of the current model in ?i
being inactive is so high that any single factor which
indicates the opposite cannot make a difference.
Fortunately, the marginal probability ?F
?
i is usu-
ally available after inference, or can be approxi-
mated. This allows us to maintain the same basic
algorithm as in the MAP case: in each ?inspection
step? we can use the results of the last run of infer-
ence in order to evaluate whether a factor has to be
added or not.
4.2 Algorithm
Algorithm 1 shows our proposed algorithm, Relaxed
Marginal Inference. We are given an initial factor
graph (for example, the first order dependency pars-
ing model), a threshold  on the minimal gain a fac-
tor needs to have in order to be added, and a solver S
for marginal inference in the partial graphs we gen-
erate along the way.
We start by finding the marginals ? for the initial
graph. These marginals are then used in step 4 to
find the factors that would, when added in isolation,
change the distribution substantially (i.e., by more
than  in terms of KL divergence). We will refer
to this step as separation, in line with cutting plane
terminology. The factors are added to the current
graph, and we start from the top unless there were
no new factors added. In this case we return the last
marginals ?.
Clearly, this algorithm is guaranteed to converge:
either we add at least one factor per iteration until
we reach the full graph F , or we converge before.
However, it is difficult to make any general state-
ments about the number of iterations it takes until
convergence. Nevertheless, in our experiments we
find that algorithm 1 converges to a much smaller
graph after a small number of iterations, and hence
we are always faster than inference on the full graph.
Finally, note that calculating the gain for all fac-
tors in F \ F ? in step 4 (separation) takes time pro-
Algorithm 1 Relaxed Marginal Inference.
1: require:
F ?:init. graph, : threshold, S:solver, R: max. it
2: repeat
Find current marginals using solver S
3: ?? marginals(F
?
, S)
Find factors with high gain not yet added
4: ?F ? {i ? F \ F
?
|gF ? (?i) > }
Add factors to current graph
5: F ? ? F ? ??F
Check: no more new factors were added or R reached
6: until ?F = ? or iteration >R
return the marginals for the last graph F ?
7: return ?
portional to |F \ F ?|.
4.3 Accuracy
We have seen how to evaluate the potential gain
when adding a single factor. However, this does
not tell us how good the current sub-model is with
respect to the complete graph. After all, while all
remaining factors individually might not contribute
much, in concert they may. We therefore present a
(calculable) bound on the KL divergence of the par-
tial graph from the full graph that can give us confi-
dence in the solutions we return at convergence.
Note that for this bound we still only need fea-
ture expectations from the current model. More-
over, we assume all weights ?i are positive?without
loss of generality since we can always replace ?i
with its negation 1 ? ?i and then change the sign
of ?i (Richardson and Domingos, 2006).
Proposition 2. Assume non-negative weights, let
F ? ? F be a subset of factors, G
def
= F \ F ? and
?
def
= ??G?1 ? ??G, ?G? ? 0. Then
1. for the KL divergence between F ? and the full
network F we have:
DKL
(
pF ? ||pF
)
? ?.
2. for the error we make when estimating ?i?s true
expectation ?Fi by ?
F ?
i we have:
? (e? ? 1)?F
?
?i ? ?
F
i ? ?
F ?
i ? (e
? ? 1)?F
?
i .
764
This says that (1) we get closer to the full distri-
bution and that (2) our marginals closer to the true
marginals, if the remaining factors G either have
a low total weight ??G?, or the current belief ?G
already assigns high probability to the features ?G
being active (and hence ???G, ?G? is small). The
latter condition is the probabilistic analog to con-
straints already being satisfied. Finally, since ? can
be easily calculated, we plan to investigate its utility
as a convergence criterion in future work.
4.4 Related Work
Our approach is inspired by earlier work on re-
laxation algorithms for performing MAP inference
by incrementally tightening relaxations of a graph-
ical model (Anguelov et al, 2004; Riedel, 2008),
weighted Finite State Machine (Tromble and Eisner,
2006), Integer Linear Program (Riedel and Clarke,
2006) or Marginal Polytope (Sontag et al, 2008).
However, none of these methods apply to marginal
inference.
Sontag and Jaakkola (2007) compute marginal
probabilities by using a cutting plane approach that
starts with the local polytope and then optimizes
some approximation of the log partition function.
Cycle consistency constraints are added if they are
violated by the current marginals, and the process is
repeated until no more violations appear. While this
approach does tackle marginalization, it is focused
on improving its accuracy. In particular, the opti-
mization problems they solve in each iteration are in
fact larger than the problem we want to relax.
Our approach is also related to edge deletion
in Bayesian networks (Choi and Darwiche, 2006).
Here edges are removed from a Bayesian network in
order to find a close approximation to the full net-
work useful for other inference-related tasks (such
as combined marginal and MAP inference). The
core difference to our approach is the fact that they
ask which edges to remove from the full graph, in-
stead of which to add to a partial graph. This re-
quires inference in the full model?the very opera-
tion we want to avoid.
5 Experiments
In our experiments we seek to answer the following
questions. First, how fast is our relaxation approach
compared to full marginal inference at comparable
dependency accuracy? This requires us to find the
best tree in terms of marginal probabilities on the
link variables (Smith and Eisner, 2008). Second,
how good is the final relaxed graph as an approxima-
tion of the full graph? Finally, how does incremental
relaxation scale with sentence length?
5.1 Data and Models
We trained and tested on a subset of languages
from the CoNLL Dependency Parsing Shared
Tasks (Nivre et al, 2007): Dutch, Danish, Italian,
and English. We apply non-projective second order
models for Dutch, Danish and Italian, and a projec-
tive second order model for English. To be able to
compare inference on the same model, we trained
using BP on the full set of LINK, GRAND, and SIB
factors.
Note that our models would rank highly among
the shared task submissions, but could surely be fur-
ther improved. For example, we do not use any lan-
guage specific features. Since our focus in this paper
is speeding up marginal inference, we will search for
better models in future work.
5.2 Runtime and Dependency Accuracy
In our first set of experiments we explore the speed
and accuracy of relaxed BP in comparison to full BP.
To this end we first tested BP configurations with at
most 5, at most 10, and at most 50 iterations to find
the best setup in terms of speed and accuracy. Smith
and Eisner (2008) use 5 iterations but we found that
by using 10 iterations accuracy could be slightly im-
proved. Running at most 50 iterations led to the
same accuracy but was significantly slower. Hence
we only report BP results with 10 iterations here.
For relaxed BP we tested along three dimensions:
the threshold  on the gain of factors, the maximum
number of BP iterations in the inner loop of relaxed
BP, and the maximum number of relaxation itera-
tions. A configuration with maximum relaxation it-
erations R, threshold , and maximum BP iterations
B will be identified by RelR,,B . In all settings we
use the LINK factors and the hard factors as initial
graph F ?.
Table 1 shows the results for several configura-
tions and our four languages in terms of unlabeled
dependency accuracy (percentage of correctly iden-
765
Dutch Danish English Italian
Configuration Acc. Time Acc. Time Acc. Time Acc. Time
BP 84.9 0.665 88.1 1.44 88.3 2.43 87.4 1.68
Rel?,0.0001,5 85.0 0.120 88.1 0.234 88.2 0.575 87.4 0.261
Rel?,0.0001,50 84.9 0.121 88.2 0.236 88.3 0.728 87.4 0.266
Rel1,0.0001,50 84.9 0.060 88.2 0.110 88.4 0.352 87.4 0.132
Table 1: Dependency accuracy (%) and average parsing time (sec.) using second order models.
tified heads) in comparison to the gold data, and av-
erage parsing time in seconds. Here parsing time
includes both time spent for marginal inference and
the MBR decoding step after the marginals are avail-
able.
We notice that by relaxing BP with no limit on the
number of iterations we gain a 4-6 fold increase in
parsing speed across all languages when using the
threshold  = 0.0001, while accuracy remains as
high as for full BP. This can be achieved with fewer
BP iterations (at most 5) in each round of relaxation
than full BP needs per sentence (at most 10). Intu-
itively this makes sense: since our factor graphs are
smaller in each iteration there will be fewer cycles
to slow down convergence. This only has a small
impact on overall parsing time for languages other
than English, since for most sentences even full BP
converges after less than 10 iterations.
We also observe that running just one iteration of
our relaxation algorithm (Rel1,0.0001,50) is enough to
achieve accurate solutions. This leads to a twofold
speed-up in comparison to running relaxation until
convergence (primarily because of fewer calls to the
separation routine), and a 7-13 fold speed-up (ten-
fold on average) when compared to full BP.
5.3 Quality of Relaxed Subgraphs
How large is the fraction of the full graph needed
for accurate marginal probabilities? And do we re-
ally need our relaxation algorithm with repeated in-
ference or could we instead just prune the graph in
advance? Here we try to answer these questions, and
will focus on the Danish dataset. Note that our re-
sults for the other languages follow the same pattern.
In table 2, we present the average ratio of the sizes
of the partial and the full graph in terms of the sec-
ond order factors. We also show the total runtime
needed to find the subgraph and run inference in it.
Configuration Size Time Err. Acc.
BP 100% 1.44 ? 88.1
Rel?,0.1,50 ? 0% 0.12 0.20 87.5
Rel?,0.0001,50 0.8% 0.24 0.012 88.2
Rel1,0.0001,50 0.8% 0.11 0.015 88.2
Pruned0.1 42% 0.56 0.022 88.0
Pruned0.5 22% 0.40 0.098 87.7
Table 2: Ratio of partial and full graph size (Size),
runtime in seconds (Time), avg. error on marginals
(Err.) and tree accuracy (Acc.) for Danish.
As a measure of accuracy for marginal probabilities
we find the average error in marginal probability for
the variables of a sentence. Note that this measure
does not necessarily correspond to the true error of
our marginals because BP itself is approximate and
may not return the correct marginals.
The first row shows the full BP system, working
on 100% of the factor graph. The next three rows
look at relaxed marginal inference. We notice that
with a low threshold  = 0.1 we pick almost no ad-
ditional factors (0.003%), and this does affect accu-
racy. However, by lowering the threshold to 0.0001
and adding about 0.8% of the second order factors,
we already match the dependency accuracy of full
BP. On average we are also very close to the BP
marginals.
Can we find such small graphs without running
extra iterations of inference? One approach could
be to simply cut off factors ?i with absolute weights
|?i| that fall under a certain threshold t. In the final
rows of the table we test such an approach with t =
0.1, 0.5.
We notice that pruning can reduce the second or-
der factors to 42% while yielding (almost) the same
accuracy, and close marginals. However, it is 5 times
slower than our fastest approach. When reducing
766
0 20 40 60
0
20
40
60
Sentence Length
Tim
e
BP
Pruned
Relaxed
Relaxed 1 It.
Figure 1: Total runtimes by sentence length.
size further to about 20%, accuracy drops below the
values we achieved with our relaxation approach at
0.8% of the second order factors. Hence simple
pruning removes factors that do have a low weight,
but are still important to keep.
5.4 Runtime with Varying Sentence Length
We have seen how relaxed BP is faster than full
BP on average. But how does its speed scale with
sentence length? To answer this question figure 1
shows a plot of runtime by sentence length for full
BP, pruned BP with threshold 0.1, Rel?,0.0001,50 and
Rel1,0.0001,50.
The graph indicates that the advantage of relaxed
BP over both full BP and Pruned BP becomes even
more significant for longer sentences, in particular
when running only one iteration. This shows that by
using our technique, second order parsing becomes
more practical, in particular for very long sentences.
6 Conclusion
We have presented a novel incremental relaxation al-
gorithm that can be applied to marginal inference.
Instead of adding violated constraints in each iter-
ation, it adds factors that significantly change the
distribution of the graph. This notion is formalized
by the introduction of a gain function that calculates
the KL divergence between the current network with
and without the candidate factor. We show how this
gain can be calculated and provide bounds on the er-
ror made by the marginals of the relaxed graph in
place of the full one.
Our algorithm led to a tenfold reduction in run-
time at comparable accuracy when applied to multi-
lingual dependency parsing with Belief Propagation.
It is five times faster than pruning factors by their
absolute weight, and results in smaller graphs with
better marginals.
In future work we plan to apply relaxed marginal
inference to larger joint inference problems within
NLP, and test its effectiveness with other marginal
inference algorithms as solvers in the inner loop.
Acknowledgments
This work was supported in part by the Center for
Intelligent Information Retrieval and in part by SRI
International subcontract #27-001338 and ARFL
prime contract #FA8750-09-C-0181. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are the authors? and do not
necessarily reflect those of the sponsor.
Appendix: Proof Sketches
For Proposition 1 we use the primal form of the KL diver-
gence (Wainwright and Jordan, 2008)
D
`
p?F ||pF
?
= log
`
ZFZ
?1
F?
?
? ??F ? , ?F ? ?F??
and represent the ratio ZFZ?1F? of partition functions as
ZF
ZF?
=
X
y
e??F? ,?F? (y)?
ZF?
e??G,?G(y)? = EF?
h
e??G,?G?
i
where G
def
= F \ F ?. With G = {i} we get the desired gain.
For Proposition 2, part 1, we first pick a simple upper bound
on ZFZ?1F? by replacing the expectation with e
??G?1 . Insert-
ing this into the primal form KL divergence leads to the given
bound. For part 2 we represent pF using pF?
pF (y) = ZF?Z
?1
F e
??G,?G(y)?pF? (y)
and reuse our above representation of ZFZ?1F? . This gives
pF (y) = EF?
h
e??G,?G(y)?
i?1
pF? (y) e
??G,?G(y)?
which can be upper bounded by lower bounding the expectation
and upper bounding the log-linear term. For the latter we use
e??G?1 , for the first Jensen?s inequality gives
EF?
h
e??G,?G(y)?
i?1
? eEF? [??G,?G(y)?] = e
D
?G,?
F?
G
E
where the equality follows from linearity of expectations. This
yields pF (y) ? pF? (y) e
? and therefore upper bounds on ?Fi
and ?F?i. Basic algebra then gives the desired error interval for
?Fi in terms of ?
F?
i .
767
References
D. Anguelov, D. Koller, P. Srinivasan, S. Thrun, H.-C.
Pang, and J. Davis. 2004. The correlated correspon-
dence algorithm for unsupervised registration of non-
rigid surfaces. In Advances in Neural Information
Processing Systems (NIPS ?04), pages 33?40.
Arthur Choi and Adnan Darwiche. 2006. A varia-
tional approach for approximating bayesian networks
by edge deletion. In Proceedings of the Proceedings
of the Twenty-Second Conference Annual Conference
on Uncertainty in Artificial Intelligence (UAI-06), Ar-
lington, Virginia. AUAI Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL?
05), pages 363?370, June.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ?06), pages 81?88.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In IWPT ?07: Proceedings of the 10th Inter-
national Conference on Parsing Technologies, pages
121?132, Morristown, NJ, USA. Association for Com-
putational Linguistics.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Conference on Em-
pirical Methods in Natural Language Processing and
Natural Language Learning, pages 915?932.
Matt Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62:107?136.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?06), pages 129?137.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ?08), pages 468?475.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 145?156, Hon-
olulu, October.
D. Sontag and T. Jaakkola. 2007. New outer bounds on
the marginal polytope. In Advances in Neural Infor-
mation Processing Systems (NIPS ?07), pages 1393?
1400.
David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proceedings of the 24th An-
nual Conference on Uncertainty in AI (UAI ?08).
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Joint Human Lan-
guage Technology Conference/Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ?06), pages 423?
430.
Martin Wainwright and Michael Jordan. 2008. Graphi-
cal Models, Exponential Families, and Variational In-
ference. Now Publishers.
768
Proceedings of NAACL-HLT 2013, pages 74?84,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Relation Extraction with Matrix Factorization and Universal Schemas
Sebastian Riedel
Department of Computer Science
University College London
s.riedel@ucl.ac.uk
Limin Yao, Andrew McCallum, Benjamin M. Marlin
Department of Computer Science
University of Massachusetts at Amherst
{lmyao,mccallum,marlin}@cs.umass.edu
Abstract
Traditional relation extraction predicts rela-
tions within some fixed and finite target
schema. Machine learning approaches to this
task require either manual annotation or, in
the case of distant supervision, existing struc-
tured sources of the same schema. The need
for existing datasets can be avoided by us-
ing a universal schema: the union of all in-
volved schemas (surface form predicates as in
OpenIE, and relations in the schemas of pre-
existing databases). This schema has an al-
most unlimited set of relations (due to surface
forms), and supports integration with existing
structured data (through the relation types of
existing databases). To populate a database of
such schema we present matrix factorization
models that learn latent feature vectors for en-
tity tuples and relations. We show that such
latent models achieve substantially higher ac-
curacy than a traditional classification ap-
proach. More importantly, by operating simul-
taneously on relations observed in text and in
pre-existing structured DBs such as Freebase,
we are able to reason about unstructured and
structured data in mutually-supporting ways.
By doing so our approach outperforms state-
of-the-art distant supervision.
1 Introduction
Most previous work in relation extraction uses a pre-
defined, finite and fixed schema of relation types
(such as born-in or employed-by). Usually some tex-
tual data is labeled according to this schema, and
this labeling is then used in supervised training of
an automated relation extractor, e.g. Culotta and
Sorensen (2004). However, labeling textual rela-
tions is time-consuming and difficult, leading to sig-
nificant recent interest in distantly-supervised learn-
ing. Here one aligns existing database records with
the sentences in which these records have been ?ren-
dered???effectively labeling the text?and from this
labeling we can train a machine learning system as
before (Craven and Kumlien, 1999; Mintz et al,
2009; Bunescu and Mooney, 2007; Riedel et al,
2010). However, this method relies on the availabil-
ity of a large database that has the desired schema.
The need for pre-existing datasets can be avoided
by using language itself as the source of the schema.
This is the approach taken by OpenIE (Etzioni et al,
2008). Here surface patterns between mentions of
concepts serve as relations. This approach requires
no supervision and has tremendous flexibility, but
lacks the ability to generalize. For example, Ope-
nIE may find FERGUSON?historian-at?HARVARD
but does not know FERGUSON?is-a-professor-at?
HARVARD. OpenIE has traditionally relied on a
large diversity of textual expressions to provide good
coverage. But this diversity is not always available,
and, in any case, the lack of generalization greatly
inhibits the ability to support reasoning.
One way to gain generalization is to cluster tex-
tual surface forms that have similar meaning (Lin
and Pantel, 2001; Pantel et al, 2007; Yates and
Etzioni, 2009; Yao et al, 2011). While the clus-
ters discovered by all these methods usually contain
semantically related items, closer inspection invari-
ably shows that they do not provide reliable impli-
cature. For example, a typical representative clus-
ter may include historian-at, professor-at, scientist-
at, worked-at. Although these relation types are in-
deed semantically related, note that scientist-at does
not necessarily imply professor-at, and worked-at
74
certainly does not imply scientist-at. In fact, we
contend that any relational schema would inherently
be brittle and ill-defined??having ambiguities, prob-
lematic boundary cases, and incompleteness.1 For
example, Freebase, in spite of its extensive effort to-
wards high coverage, has no critized nor scientist-at
relation.
In response to this problem, we present a new ap-
proach: implicature with universal schemas. Here
we embrace the diversity and ambiguity of original
inputs; we avoid forcing textual meaning into pre-
defined boxes. This is accomplished by defining
our schema to be the union of all source schemas:
original input forms, e.g. variants of surface pat-
terns similarly to OpenIE, as well as relations in
the schemas of many available pre-existing struc-
tured databases. But then, unlike OpenIE, our fo-
cus lies on learning asymmetric implicature among
relations. This allows us to probabilistically ?fill
in? inferred unobserved entity-entity relations in
this union. For example, after observing FERGU-
SON?historian-at?HARVARD our system infers that
FERGUSON?professor-at?HARVARD, but not vice
versa.
At the heart of our approach is the hypothesis that
we should concentrate on predicting source data??a
relatively well defined task that can be evaluated and
optimized??as opposed to modeling semantic equiv-
alence, which we believe will always be illusive.
Note that by operating simultaneously on rela-
tions observed in text and in pre-existing structured
databases such as Freebase, we are able to reason
about unstructured and structured data in mutually-
supporting ways. For example, we can predict sur-
face pattern relations that effectively serve as addi-
tional features when predicting Freebase relations,
hence improving generalization. Also notice that
users of our system will not have to study and un-
derstand the complexities of a particular schema in
order to issue queries; they can ask in whatever form
naturally occurs to them, and our system will likely
already have that relation in our universal schema.
Our technical approach is based on extensions
to probabilistic models of matrix factorization and
1At NAACL 2012 Lucy Vanderwende asked ?Where do the
relation types come from?? There was no satisfying answer. At
the same meeting, and in line with Brachman (1983), Ed Hovy
stated ?We don?t even know what is-a means.?
collaborative filtering (Collins et al, 2001; Koren,
2008; Rendle et al, 2009). We represent the prob-
abilistic knowledge base as a matrix with entity-
entity pairs in the rows and relations in the columns
(see figure 1). The rows come from running cross-
document entity resolution across pre-existing struc-
tured databases and textual corpora. The columns
come from the union of surface forms and DB rela-
tions. We present a series of models that learn lower
dimensional manifolds for tuples, relations and enti-
ties, and a set of weights that capture direct correla-
tions between relations. Weights and lower dimen-
sional representations act, through dot products, as
the natural parameters of a single log-linear model
to derive per-cell probabilities.
In experiments we show that our models can ac-
curately predict surface patterns relationships which
do not appear explicitly in text, and that learning la-
tent representations of entities, tuples and relations
substantially improves results over a traditional clas-
sifier approach. Moreover, we can improve accu-
racy by simultaneously operating on relations ob-
served in the New York Times corpus and in Free-
base. In particular, our model outperforms the cur-
rent state-of-the-art distant supervision method (Sur-
deanu et al, 2012) by 10% points Mean Average
Precision through joint implicature among surface
patterns and Freebase relations.
2 Model
Before we present our approach in more detail, we
briefly introduce some notation. We use R to de-
note the set of relations we seek to predict (such as
works-written in Freebase, or the X?historian-at?Y
pattern), and T to denote the set of input tuples. For
simplicity we assume each relation to be binary, al-
though our approach can be easily generalized to the
n-ary case. Given a relation r ? R and a tuple t ? T
the pair ?r, t? is a fact, or relation instance. The in-
put to our model is a set of observed facts O, and
the observed facts for a given tuple is denoted by
Ot := {?r, t? ? O}.
Our goal is a model that can estimate, for a
given relation r (such as X?historian-at?Y) and a
given tuple t (such as <FERGUSON,HARVARD>),
the probability p (yr,t = 1) where yr,t is a binary
random variable that is true iff t is in relation r. We
75
Tr
a
i
n
0.95
T
e
s
t
Surface Patterns KB Relations
X-professor-at-Y
1
1
0.05
X-historian-at-Y employee(X,Y) member(X,Y)
1 1
1
1
0.97
Rel. Extraction
1 0.93
0.97
Cluster Align
Reasoning with Universal Schema
F
e
r
g
u
s
o
n
,
H
a
r
v
a
r
d
O
m
a
n
,
O
x
f
o
r
d
F
i
r
t
h
,
O
x
f
o
r
d
G
?
d
e
l
,
P
r
i
n
c
e
t
o
n
0.95
Figure 1: Filling up a database of universal schema.
Dark circles are observed facts, shaded circles are in-
ferred facts. Relation Extraction (RE) maps surface pat-
tern relations (and other features) to structured relations.
Surface form clustering models correlations between pat-
terns, and can be fed into RE (Yao et al, 2011). Database
alignment and integration models correlations between
structured relations (not done in this work). Reasoning
with the universal schema incorporates these tasks in a
joint fashion.
introduce a series of exponential family models that
estimate this probability using a natural parameter
?r,t and the logistic function:
p (yr,t = 1|?r,t) := ? (?r,t) =
1
1 + exp (??r,t)
.
We will first describe our models through differ-
ent definitions of the natural parameter ?r,t. In each
case ?r,t will be a function of r, t and a set of weights
and/or latent feature vectors. In section 2.5 we will
then show how these weights and vectors can be es-
timated based on the observed facts O.
Notice that we can interpret p (yr,t = 1) as the
probability that a customer t likes product r. This
analogy allows us to draw from a large body of work
in collaborative filtering, such as work in probabilis-
tic matrix factorization and implicit feedback.
2.1 Latent Feature Model
One way to define ?r,t is through a latent feature
model F. Here we measure compatibility between
relation r and tuple t as dot product of two latent
feature representations of size KF: ar for relation r,
and vt for tuple t. This gives:
?Fr,t :=
KF?
k
ar,kvt,k.
This corresponds to generalized PCA (Collins et al,
2001), a model were the matrix ? = (?r,t) of natural
parameters is defined as the low rank factorization
AV.
Notice that we intentionally omit any per-relation
bias-terms. In section 4 we evaluate ranked answers
to queries on a per-relation basis, and a per-relation
bias term will have no effect on ranking facts of the
same relation. Also consider that such latent feature
models can capture asymmetry by assigning more
peaked vectors to specific relations, and more uni-
form vectors to general relations.
2.2 Neighborhood Model
We can interpolate the confidence for a given tuple
and relation based on the trueness of other similar
relations for the same tuple. In collaborative filter-
ing this is referred to as a neighborhood-based ap-
proach (Koren, 2008). In terms of our natural pa-
rameter, we implement a neighborhood model N via
a set of weights wr,r? , where each corresponds to a
directed association strength between relations r and
r?. For a given tuple t and relation r we then sum
up the weights corresponding to all relations r? that
have been observed for tuple t:
?Nr,t :=
?
(r?,t)?O\{(r,t)}
wr,r? .
Notice that the neighborhood model amounts to
a collection of local log-linear classifiers, one for
each relation r with feature functions fr,r? (t) =
I [r? 6= r ? (r?, t) ? O] and weights wr. This means
that in contrast to model F, this model cannot har-
ness any synergies between textual and pre-existing
DB relations.
76
2.3 Entity Model
Relations have selectional preferences: they allow
only certain types in their argument slots. While
knowledge bases such as Freebase or DBPedia have
extensive ontologies of types of entities, these are of-
ten not sufficiently fine to allow relations to discrim-
inate (Yao et al, 2012b). Hence, instead of using a
predetermined set of entity types, in our entity model
E we learn a latent entity representation from data.
More concretely, for each entity e we introduce a la-
tent feature vector te of dimension KE. In addition,
for each relation r and argument slot i we introduce
a feature vector di of the same dimension. For ex-
ample, binary relations have feature representations
d1 for argument 1, and d2 for argument 2. Mea-
suring compatibility of an entity tuple and relation
amounts to measuring, and summing up, compati-
bility between each argument slot representation and
the corresponding entity representation. This leads
to:
?Er,t :=
arity(r)?
i=1
KE?
k
di,ktti,k.
Note that due to entity resolution, tuples may
share entities, and hence parameters are shared
across rows.
2.4 Combined Model
In practice all the above models can capture impor-
tant aspects of the data. Hence we also use various
combinations, such as:
?NFEr,t := ?
N
r,t + ?
F
r,t + ?
E
r,t.
2.5 Parameter Estimation
Our models are parametrized through weights and
latent component vectors. We could estimate these
parameters by maximizing the loglikelihood of the
observed data akin to Collins et al (2001). How-
ever, as we do not have access to negative facts, the
model would simply learn to predict all facts to be
true. In our initial attempt to overcome this issue
we sampled a set of unobserved facts as designated
negative facts, as is done in related distant supervi-
sion approaches. However, we found that (a) our
results were sensitive to the choice of negative data
and (b) runtime was increased substantially because
of a large number of required negative facts.
In collaborative filtering positive-only data is also
known as implicit feedback. This type of feedback
arises, for example, when users buy but not rate
items. One successful approach to learning with im-
plicit feedback is based on the observation that the
actual task is not necessarily one of prediction (here:
to predict a number between 0 and 1) but one of
(generally simpler) ranking: to give true ?user-item?
cells higher scores than false ones. Bayesian Person-
alized Ranking (BPR) uses a variant of this ranking:
giving observed true facts higher scores than unob-
served (true or false) facts (Rendle et al, 2009). This
relaxed constraint is to be contrasted with the log-
likelihood setting that essentially requires (randomly
sampled) negative facts to score below a globally de-
fined threshold.
2.5.1 Objective
We first create a dataset of ranked pairs: for each
relation r and each observed fact f+ := ?r, t+? ? O
we choose all tuples t? such that f? := ?r, t?? /?
O?that is, tuples we have not observed to be in
relation r. For each pair of facts f+ and f? we
want p (f+) > p (f?) and hence ?f+ > ?f? . In
BPR this is achieved by maximizing a sum terms of
the form Objf+,f? := log
(
?
(
?f+ ? ?f?
))
, one for
each ranked pair:
Obj :=
?
?r,t+??O
?
?r,t??/?O
Obj?r,t+?,?r,t??. (1)
Notice that this objective differs slightly from the
one used by Rendle et al (2009). Consider tuples
as users and items as relations. We rank different
users with respect to the same item, while BPR ranks
items with respect to the same user. Also notice that
the BPR objective is an approximation to the per-
relation AUC (area under the ROC curve), and hence
directly correlated to what we want to achieve: well-
ranked tuples per relation.
Note that all parameters are regularized with
quadratic penalty which we omit here for brevity.
2.5.2 Optimization
To maximize the objective2 in equation 1 we fol-
low Rendle et al (2009) and employ Stochastic Gra-
dient Descent (SGD). In particular, in each epoch
2This objective is non-convex for all models excluding the
N model.
77
we sample |O| facts with replacement from O. For
each sampled fact ?r, t+? we then sample a tuple
t? ? T such that ?r, t?? /? O is not an observed
fact. This gives us |O| fact pairs ?f+, f??, and for
each pair we do an SGD update using the corre-
sponding gradients of Objf+,f? . For the F model
the gradients correspond to those presented by Ren-
dle et al (2009). The remaining gradients are easy
to derive; we omit details for brevity.
3 Related Work
This work extends a previous workshop paper (Yao
et al, 2012a) by introducing the neighborhood and
entity model, by working with the BPR objective,
and by more extensive experiments.
Relational Clustering There is a large body of
work aiming to discover latent relations by clus-
tering surface patterns (Hasegawa et al, 2004;
Shinyama and Sekine, 2006; Kok and Domingos,
2008; Yao et al, 2011; Takamatsu et al, 2011), or
by inducing synonymy relationships between pat-
terns independently of the entities (Yates and Et-
zioni, 2009; Pantel et al, 2007; Lin and Pantel,
2001). Our approach has a fundamentally different
objective: we are not (primarily) interested in clus-
ters of patterns or their semantic representation, but
in predicting patterns where they are not observed.
Moreover, these related methods rely on a symmetric
notion of synonymy in which clustered patterns are
assumed to have the same meaning. Our approach
rejects this assumption in favor of a model which
learns that certain patterns, or combinations thereof,
entail others in one direction, but not necessarily the
other. This is similar in spirit to work on learning
entailment rules (Szpektor et al, 2004; Zanzotto et
al., 2006; Szpektor and Dagan, 2008). However, for
us even entailment rules are just a by-product of our
goal to improve prediction, and it is this goal we di-
rectly optimize for and evaluate.
Matrix Factorization Our approach is also re-
lated to work on factorizing YAGO to predict new
links (Nickel et al, 2012). The primary differences
are that we include surface patterns in our schema,
use a ranking objective, and learn latent vectors for
entities and tuples. Likewise, matrix factorization in
various flavors has received significant attention in
the lexical semantics community, from LSA to re-
cent work on non-negative sparse embeddings (Mur-
phy et al, 2012). In our problem columns corre-
spond to relations, and rows correspond to entity tu-
ples. By contrast, there columns are words, and rows
are contextual features such as ?words in a local win-
dow.? Consequently, our objective is to complete
the matrix, whereas their objective is to learn better
latent embeddings of words (which by themselves
again cannot capture any sense of asymmetry).
OpenIE Open IE (Etzioni et al, 2008) extracts
facts mentioned in text, but does not predict poten-
tial facts not mentioned in text. Finding answers
requires explicit mentions, and hence suffers from
lower recall for not-so-frequently mentioned facts.
Methods that learn rules between textual patterns in
OpenIE aim at a similar goal as our proposed ap-
proach (Schoenmackers et al, 2008; Schoenmack-
ers et al, 2010). However, their approach is sub-
stantially more complex, requires a categorization
of entities into fine grained entity types, and needs
inference in high tree-width Markov Networks. By
contrast, our approach is based on a single unified
model, requires no entity types, and for us inferring
a fact amounts to not more than a few dot products.
In addition, in our Universal Schema approach Ope-
nIE surface patterns are just one kind of relations,
and our aim is populate relations of all kinds. In the
future we may even include relations between enti-
ties and continuous attributes (say, gene expression
measurements).
Distant Supervision In Distant Supervision (DS)
a set of facts from pre-existing structured sources
is aligned with surface patterns mentioned in
text (Bunescu and Mooney, 2007; Mintz et al, 2009;
Riedel et al, 2010; Hoffmann et al, 2011; Surdeanu
et al, 2012), and this alignment is then used to train
a relation extractor. A core difference to our ap-
proach is the number of target relations: In DS it
is the relatively small schema size of the knowledge
base, while we also include surface patterns. This
allows us to answer more expressive queries. More-
over, by learning from surface-pattern correlations,
our latent models induce feature representations for
patterns that do not appear in the DS training set. As
we will see in section 4, this allows us to outperform
state-of-the-art DS models.
78
Never-Ending Learning and Bootstrapping Our
latent feature models are capable of never-ending
learning (Carlson et al, 2010). That is, we can con-
tinue to train these models with incoming data, even
if no structured annotation is available. In bootstrap-
ping approaches the current model is used to predict
new relations, and these hypothesized relations are
used as new supervision targets (i.e. self-training).
By contrast, our model only strengthens the correla-
tions between incoming co-occurring observations.
This has the advantage that wrong predictions are
less likely be reinforced, hence reducing the risk of
semantic drift.
4 Experiments
How accurately can we fill a database of Universal
Schema, and does reasoning jointly across a uni-
versal schema help to improve over more isolated
approaches? In the following we seek to answer
this question empirically. To this end we train our
models on observed facts in a newswire corpus and
Freebase, and then manually evaluate ranked predic-
tions: first for structured relations and then for sur-
face form relations.
4.1 Data
Following previous work (Riedel et al, 2010),
our documents are taken from the NYTimes cor-
pus (Sandhaus, 2008). Articles after 2000 are used
as training corpus, articles from 1990 to 1999 as
test corpus. We also split Freebase facts 50/50 into
train and test facts, and their corresponding tuples
into train and test tuples. Then we align training tu-
ples with the training corpus, and test tuples with the
test corpus. This alignment relies on a preprocessing
step that links NER mentions in text with entities in
Freebase. In our case we use a simple string-match
heuristic to find this linking. Now we align an entity
tuple ?t1, t2? with a pair of mentions ?m1,m2? in
the same sentence if m1 is linked to t1 and m2 to t2.
Based on this alignment we filter out all relations for
which we find fewer than 10 tuples with mentions in
text.
The above alignment and filtering process reduces
the total number of tuples related according to Free-
base to 16k: approximately 8k tuples with facts
mentioned in the training set, and approximately 8k
such tuples for the test set. In addition we have a
set of approximately 200k training tuples for which
both arguments appear in the same sentence and
both can be linked to Freebase entities, but for which
no Freebase fact is recorded. This can either be be-
cause they are not related, or simply because Free-
base does not contain the relationship yet. We also
have about 200k such tuples in the test set. To sim-
plify evaluation, we create a subsampled test set by
randomly choosing 10k of the original test set tuples.
The above alignment allows us to determine, for
each tuple t, the observed facts Ot as follows. To
find the surface pattern facts OPATt for the tuple t =
?t1, t2? we extract, for each mention m = ?m1,m2?
of t, the lexicalized dependency path p between m1
and m2. Then we add ?p, t? to OPATt . For example,
we get ?<-subj<-head->obj->? for ?M1 heads M2.?
Filtering out patterns with fewer than 10 mentions
in text yields approximately 4k patterns. For train-
ing tuples we add as Freebase facts OFBt all facts
?r, t? that appear in Freebase, and for which r has
not been filtered out beforehand. For the test setOFBt
remains empty. The total set of observed facts Ot is
OFBt ?O
PAT
t , and their union over all tuples forms the
set of observed facts O.
4.2 Evaluation
For evaluation we use collections of relations: sur-
face patterns in one experiment and Freebase re-
lations in the other. In either case we compare
the competing systems with respect to their ranked
results for each relation in the collection. Given
this ranking task, our evaluation is inspired by the
TREC competitions and work in information re-
trieval (Manning et al, 2008). That is, we treat
each relation as query and receive the top 1000 (run
depth) entity pairs from each system. Then we pool
the top 100 (pool depth) answers from each system
and manually judge their relevance or ?truth.? This
gives a set of relevant results that we can use to cal-
culate recall and precision measures. In particular,
we can use these annotations to measure an average
precision across the precision-recall curve, and an
aggregate mean average precision (MAP) across all
relations. This metric has shown to be very robust
and stable (Manning et al, 2008). In addition we
also present a weighted version of MAP (weighted
MAP) in which the average precision for each re-
79
Relation # MI09 YA11 SU12 N F NF NFE
person/company 103 0.67 0.64 0.70 0.73 0.75 0.76 0.79
location/containedby 74 0.48 0.51 0.54 0.43 0.68 0.67 0.69
author/works_written 29 0.50 0.51 0.52 0.45 0.61 0.63 0.69
person/nationality 28 0.14 0.40 0.13 0.13 0.19 0.18 0.21
parent/child 19 0.14 0.25 0.62 0.46 0.76 0.78 0.76
person/place_of_death 19 0.79 0.79 0.86 0.89 0.83 0.85 0.86
person/place_of_birth 18 0.78 0.75 0.82 0.50 0.83 0.81 0.89
neighborhood/neighborhood_of 12 0.00 0.00 0.08 0.43 0.65 0.66 0.72
person/parents 7 0.24 0.27 0.58 0.56 0.53 0.58 0.39
company/founders 4 0.25 0.25 0.53 0.24 0.77 0.80 0.68
film/directed_by 4 0.06 0.15 0.25 0.09 0.26 0.26 0.30
sports_team/league 4 0.00 0.43 0.18 0.21 0.59 0.70 0.63
team/arena_stadium 3 0.00 0.06 0.06 0.03 0.08 0.09 0.08
team_owner/teams_owned 2 0.00 0.50 0.70 0.55 0.38 0.61 0.75
roadcast/area_served 2 1.00 0.50 1.00 0.58 0.58 0.83 1.00
structure/architect 2 0.00 0.00 1.00 0.27 1.00 1.00 1.00
composer/compositions 2 0.00 0.00 0.00 0.50 0.67 0.83 0.12
person/religion 1 0.00 1.00 1.00 0.50 1.00 1.00 1.00
film/produced_by 1 1.00 1.00 1.00 1.00 0.50 0.50 0.33
MAP 0.32 0.42 0.56 0.45 0.61 0.66 0.63
Weighted MAP 0.48 0.52 0.57 0.52 0.66 0.67 0.69
Table 1: Average and (weighted) Mean Average Precisions for Freebase relations based on pooled results. The #
column shows the number of true facts in the pool. NFE is statistically different to all but NF and F according to the
sign test. Bold faced are winners per relation, italics indicate ties.
lation is weighted by the relation?s number of true
facts.
Notice that we deviate from previous work in dis-
tant supervision that (a) combines the results from
several relations in a single precision recall curve,
and (b) uses held-out evaluation to measure how
well the predictions match existing Freebase facts.
This has several benefits. First, when aggregating
across relations results are often dominated by a few
very frequent relations, such as containedby, provid-
ing little information about how the models perform
across the board. Second, evaluating with Freebase
held-out data is biased. For example, we find that
frequently mentioned entity pairs are more likely to
have relations in Freebase. Systems that rank such
tuples higher receives higher precision than those
that do not have such bias, regardless of how cor-
rect their predictions are. Third, we can aggregate
per-relation comparisons to establish statistical sig-
nificance, for example via the sign test.
Also note that while we run our models on the
complete training and test set, evaluation is re-
stricted to the subsampled test set.
4.3 Predicting Freebase Relations
Table 1 shows our results for Freebase relations,
omitting those for which none of the systems can
find any relevant facts. Our first baseline is MI09,
a distantly supervised classifier based on the work
of Mintz et al (2009). This classifier only learns
from observed pattern-relation pairs in the training
set (of which we only have about 8k). By contrast,
our latent feature models can learn pattern-pattern
correlations both on the unlabeled training and test
set (comparable to bootstrapping). We hence also
compare against YA11, a version of MI09 that uses
preprocessed cluster features according to Yao et al
(2011). The third baseline is SU12, the state-of-the-
art Multi-Instance Multi-Label system by Surdeanu
et al (2012).
The remaining systems are our neighborhood
80
 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 0.9 1
 0  0.2  0.4  0.6  0.8  1Precision Recall
Averaged 11-point Precision/Recall MI09YA11SU12NFNFNFE
Figure 2: Averaged 11-point precision recall curve for
Freebase relations in table 1.
model (N), the factorized model (F), their combi-
nation (NF) and the combined model with a latent
entity representation (NFE). For all our models we
use the same number of components when applica-
ble (KF = KE = 100), 1000 epochs, and 0.01 as
regularizer for component weights and 0.1 for neigh-
borhood weights.
Table 1 shows that adding pattern cluster features
(and hence incorporating more data) helps YA11
to improve over MI09. Likewise, we see that the
factorized model F improves over N, again learn-
ing from unlabeled data. This improvement is big-
ger than the corresponding change between MI09
and YA11, possibly indicating that our latent rep-
resentations are optimized directly towards improv-
ing prediction performance. The combination of N,
F and E outperforms all other models in terms of
weighted MAP, indicating the power of selectional
preferences learned from data. Note that NFE is
significantly different (p  0.05 in sign test) to all
but the NF and F models. In terms of MAP the NF
model outperforms NFE, indicating that it does not
do as well for frequent relations, but better for infre-
quent ones.
Figure 2 shows an averaged 11-point precision re-
call graph (Manning et al, 2008) for Freebase re-
lations. We notice that our latent models outper-
form all remaining models across all recall levels,
and that combining neighborhood and latent models
is helpful. This finding is consistent with our MAP
results. Figure 3 shows the recall-precision curve for
the works_written relation with respect to our three
baselines and the NFE model. Observe how preci-
 0 0.2 0.4 0.6 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Precision Recall
Recall/Precision MI09YA11SU12NFE
Figure 3: Precision and recall for works_written(X,Y).
Relation # N F NF NFE
visit 80 0.19 0.68 0.49 0.42
attend 69 0.23 0.10 0.07 0.10
base 61 0.46 0.87 0.81 0.68
head 38 0.47 0.67 0.70 0.68
scientist 36 0.25 0.84 0.79 0.73
support 18 0.16 0.29 0.32 0.38
adviser 11 0.19 0.15 0.19 0.28
criticize 9 0.09 0.60 0.67 0.64
praise 4 0.01 0.03 0.05 0.10
vote 3 0.18 0.18 0.34 0.34
MAP 0.22 0.44 0.44 0.43
Weighted MAP 0.28 0.56 0.50 0.46
Table 2: Average and (weighted) Mean Average Preci-
sions for surface patterns.2
sion drops for both MI09 and SU12 at about 50%
recall. At this point the remaining unretrieved facts
have patterns that have not been seen together with
works_written in the training set. By using cluster
features, YA11 can overcome this problem partly,
but not as dramatically as NFE?a pattern we ob-
serve for many relations.
All our models are fast to train. The slowest
model trains in just 45 minutes. By contrast, training
the topic model in YA11 alone takes 4 hours. Train-
ing SU12 takes two hours (on less data). Also notice
that our models not only learn to predict Freebase
relations, but also approximately 4k surface pattern
relations.
4.4 Predicting Surface Patterns
Table 2 presents a comparison of our models with re-
spect to 10 surface pattern relations. These relations
81
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 0.9 1
 0  0.2  0.4  0.6  0.8  1Precision Recall
Averaged 11-point Precision/Recall NFNFNFE
Figure 4: Averaged 11-point precision recall curve for
surface pattern relations in table 2.
were chosen according to what we believe are inter-
esting questions not currently captured in Freebase.
We again see that learning a latent representation (F,
NF and NFE) from additional data helps quite sub-
stantially over the N model. For in the weighted
MAP metric we note that incorporating entity rep-
resentations (in the NFE model) in fact hurts total
performance.3 One reason may be the fact that Free-
base relations are typed?they require very specific
types of entities as arguments. By contrast, for a
surface pattern like ?X visits Y? X could be a person
or organization, and Y could be a location, organi-
zation or person. However, in terms of MAP score
this time there is no obvious winner among the la-
tent models. This is also confirmed by the averaged
11-point precision recall curve in figure 4.
Notice that we can accurately predict the X?
scientist-at?Y surface pattern relation in table 2,
as well as the more general person/company (em-
ployedBy) relation in table 1. This indicates that
our models can capture asymmetry?a symmetric
model would either over-predict X?scientist-at?Y
or under-predict person/company.
5 Conclusion
We present relation extraction into universal
schemas. Such schemas contain surface patterns
as relations, as well as relations from structured
sources. By predicting missing tuples for surface
pattern relations we can populate a database with-
out any labelled data, and answer questions not sup-
3Due to the small set of relations only N is significantly dif-
ferent to F, NF and NFE (p 0.05 in sign test).
ported by the structured schema alone. By predict-
ing missing tuples in the structured schema we can
expand a knowledge base of fixed schema, and only
require a set of existing facts from this schema. Cru-
cially, by predicting and modeling both surface pat-
terns and structured relations simultaneously we can
improve performance. We show this experimentally
by contrasting a series of the popular weakly super-
vised models to our collaborative filtering models
that learn latent feature representations across sur-
face patterns and structured relations. Moreover, our
models are computationally efficient, requiring less
time than comparable methods, while learning more
relations.
Reasoning with universal schemas is not merely a
tool for information extraction. It can also serve as
a framework for various data integration tasks. For
example, we could integrate facts from one schema
(say, Freebase) into another (say, the TAC KBP
schema) by adding both sets of relations to the set
of surface patterns. Reasoning with this schema
will mean populating each database with facts from
the other, and would leverage information in surface
patterns to improve integration. In future work we
also plan to integrate universal entity types and at-
tributes into the model.
The source code of our system, its output, and
all data annotations are available at http://www.
riedelcastro.org/uschema.
Acknowledgments
We thank the reviewers for very helpful comments.
This work was supported in part by the Center for In-
telligent Information Retrieval and the University of
Massachusetts, in part by UPenn NSF medium IIS-
0803847, in part by DARPA under agreement num-
ber FA8750-13-2-0020 and FA8750-09-C-0181, and
in part by an award from Google. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the view of DARPA,
AFRL, or the US government.
References
Ronald J. Brachman. 1983. What is-a is and isn t:
An analysis of taxonomic links in semantic networks.
IEEE Computer, 16(10):30?36.
82
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proceedings of the 25th AAAI Con-
ference on Artificial Intelligence (AAAI ?10).
Michael Collins, Sanjoy Dasgupta, and Robert E.
Schapire. 2001. A generalization of principal com-
ponent analysis to the exponential family. In Proceed-
ings of NIPS.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge-bases by extracting information from
text sources. In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for Molecular
Biology, pages 77?86, Germany.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, Barcelona, Spain.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from Large
Corpora. Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL
?04), pages 415?422.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of ACL.
Stanley Kok and Pedro Domingos. 2008. Extracting Se-
mantic Networks from Text Via Relational Clustering.
In ECML.
Yehuda Koren. 2008. Factorization meets the neighbor-
hood: a multifaceted collaborative filtering model. In
Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?08, pages 426?434, New York, NY, USA.
ACM.
Dekang Lin and Patrick Pantel. 2001. DIRT - discovery
of inference rules from text. In Knowledge Discovery
and Data Mining, pages 323?328.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ?09),
pages 1003?1011. Association for Computational Lin-
guistics.
Brian Murphy, Partha Pratim Talukdar, and Tom
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In COLING, pages 1933?1950.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
learning for linked data. In Proceedings of the 21st
international conference on World Wide Web, WWW
?12, pages 271?280, New York, NY, USA. ACM.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning Inferential Selectional Preferences. In Pro-
ceedings of NAACL HLT.
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,
and Lars Schmidt-Thieme. 2009. Bpr: Bayesian per-
sonalized ranking from implicit feedback. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, UAI ?09, pages 452?461, Ar-
lington, Virginia, United States. AUAI Press.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ?08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1088?1098,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, HLT-NAACL ?06, pages 304?
311, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D. Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
83
of the Conference on Empirical methods in natural
language processing (EMNLP ?12), pages 455?465.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 849?856,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2011. Probabilistic matrix factorization leveraging
contexts for unsupervised relation discovery. In Pro-
ceedings of PAKDD.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?11), July.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012a. Probabilistic databases of universal schema.
In Proceedings of the AKBC-WEKEX Workshop at
NAACL 2012, June.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012b. Unsupervised relation discovery with sense
disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?12), July.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In Proceedings of the 44th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?06).
84
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 712?720,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Relation Discovery with Sense Disambiguation
Limin Yao Sebastian Riedel Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{lmyao,riedel,mccallum}@cs.umass.edu
Abstract
To discover relation types from text, most
methods cluster shallow or syntactic patterns
of relation mentions, but consider only one
possible sense per pattern. In practice this
assumption is often violated. In this paper
we overcome this issue by inducing clusters
of pattern senses from feature representations
of patterns. In particular, we employ a topic
model to partition entity pairs associated with
patterns into sense clusters using local and
global features. We merge these sense clus-
ters into semantic relations using hierarchical
agglomerative clustering. We compare against
several baselines: a generative latent-variable
model, a clustering method that does not dis-
ambiguate between path senses, and our own
approach but with only local features. Exper-
imental results show our proposed approach
discovers dramatically more accurate clusters
than models without sense disambiguation,
and that incorporating global features, such as
the document theme, is crucial.
1 Introduction
Relation extraction (RE) is the task of determin-
ing semantic relations between entities mentioned in
text. RE is an essential part of information extraction
and is useful for question answering (Ravichandran
and Hovy, 2002), textual entailment (Szpektor et al,
2004) and many other applications.
A common approach to RE is to assume that rela-
tions to be extracted are part of a predefined ontol-
ogy. For example, the relations are given in knowl-
edge bases such as Freebase (Bollacker et al, 2008)
or DBpedia (Bizer et al, 2009). However, in many
applications, ontologies do not yet exist or have low
coverage. Even when they do exist, their mainte-
nance and extension are considered to be a substan-
tial bottleneck. This has led to considerable inter-
est in unsupervised relation discovery (Hasegawa et
al., 2004; Banko and Etzioni, 2008; Lin and Pantel,
2001; Bollegala et al, 2010; Yao et al, 2011). Here,
the relation extractor simultaneously discovers facts
expressed in natural language, and the ontology into
which they are assigned.
Many relation discovery methods rely exclusively
on the notion of either shallow or syntactic patterns
that appear between two named entities (Bollegala et
al., 2010; Lin and Pantel, 2001). Such patterns could
be sequences of lemmas and Part-of-Speech tags, or
lexicalized dependency paths. Generally speaking,
relation discovery attempts to cluster such patterns
into sets of equivalent or similar meaning. Whether
we use sequences or dependency paths, we will en-
counter the problem of polysemy. For example, a
pattern such as ?A beat B? can mean that person A
wins over B in competing for a political position,
as pair ?(Hillary Rodham Clinton, Jonathan Tasini)?
in ?Sen Hillary Rodham Clinton beats rival Jonathan
Tasini for Senate.? It can also indicate that an athlete
A beat B in a sports match, as pair ?(Dmitry Tur-
sunov, Andy Roddick)? in ?Dmitry Tursunov beat
the best American player Andy Roddick.? More-
over, it can mean ?physically beat? as pair ?(Mr.
Harris, Mr. Simon)? in ?On Sept. 7, 1999, Mr. Har-
ris fatally beat Mr. Simon.? This is known as poly-
semy. If we work with patterns alone, our extractor
will not be able to differentiate between these cases.
Most previous approaches do not explicitly ad-
dress this problem. Lin and Pantel (2001) assumes
only one sense per path. In (Pantel et al, 2007),
they augment each relation with its selectional pref-
712
erences, i.e. fine-grained entity types of two ar-
guments, to handle polysemy. However, such fine
grained entity types come at a high cost. It is difficult
to discover a high-quality set of fine-grained entity
types due to unknown criteria for developing such
a set. In particular, the optimal granularity of en-
tity types depends on the particular pattern we con-
sider. For example, a pattern like ?A beat B? could
refer to A winning a sports competition against B, or
a political election. To differentiate between these
senses we need types such as ?Politician? or ?Ath-
lete?. However, for ?A, the parent of B? we only
need to distinguish between persons and organiza-
tions (for the case of the sub-organization relation).
In addition, there are senses that just cannot be de-
termined by entity types alone: Take the meaning
of ?A beat B? where A and B are both persons; this
could mean A physically beats B, or it could mean
that A defeated B in a competition.
In this paper we address the problem of polysemy,
while we circumvent the problem of finding fine-
grained entity types. Instead of mapping entities to
fine-grained types, we directly induce pattern senses
by clustering feature representations of pattern con-
texts, i.e. the entity pairs associated with a pattern.
This allows us to employ not only local features such
as words, but also global features such as the docu-
ment and sentence themes.
To cluster the entity pairs of a single relation pat-
tern into senses, we develop a simple extension to
Latent Dirichlet Allocation (Blei et al, 2003). Once
we have our pattern senses, we merge them into
clusters of different patterns with a similar sense.
We employ hierarchical agglomerative clustering
with a similarity metric that considers features such
as the entity arguments, and the document and sen-
tence themes.
We perform experiments on New York Times ar-
ticles and consider lexicalized dependency paths as
patterns in our data. In the following we shall use
the term path and pattern exchangeably. We com-
pare our approach with several baseline systems, in-
cluding a generative model approach, a clustering
method that does not disambiguate between senses,
and our approach with different features. We per-
form both automatic and manual evaluations. For
automatic evaluation, we use relation instances in
Freebase as ground truth, and employ two clustering
metrics, pairwise F-score and B3 (as used in cofer-
ence). Experimental results show that our approach
improves over the baselines, and that using global
features achieves better performance than using en-
tity type based features. For manual evaluation, we
employ a set intrusion method (Chang et al, 2009).
The results also show that our approach discovers re-
lation clusters that human evaluators find coherent.
2 Our Approach
We induce pattern senses by clustering the entity
pairs associated with a pattern, and discover seman-
tic relations by clustering these sense clusters. We
represent each pattern as a list of entity pairs and
employ a topic model to partition them into different
sense clusters using local and global features. We
take each sense cluster of a pattern as an atomic clus-
ter, and use hierarchical agglomerative clustering to
organize them into semantic relations. Therefore, a
semantic relation comprises a set of sense clusters of
patterns. Note that one pattern can fall into different
semantic relations when it has multiple senses.
2.1 Sense Disambiguation
In this section, we discuss the details of how we dis-
cover senses of a pattern. For each pattern, we form
a clustering task by collecting all entity pairs the pat-
tern connects. Our goal is to partition these entity
pairs into sense clusters. We represent each pair by
the following features.
Entity names: We use the surface string of the en-
tity pair as features. For example, for pattern ?A play
B?, pairs which contain B argument ?Mozart? could
be in one sense, whereas pairs which have ?Mets?
could be in another sense.
Words: The words between and around the two
entity arguments can disambiguate the sense of a
path. For example, ?A?s parent company B? is dif-
ferent from ?A?s largest company B? although they
share the same path ?A?s company B?. The former
describes the sub-organization relationship between
two companies, while the latter describes B as the
largest company in a location A. The two words to
the left of the source argument, and to the right of the
destination argument also help sense discovery. For
example, in ?Mazurkas played by Anna Kijanowska,
pianist?, ?pianist? tells us pattern ?A played by B?
713
takes the ?music? sense.
Document theme: Sometimes, the same pattern
can express different relations in different docu-
ments, depending on the document?s theme. For
instance, in a document about politics, ?A defeated
B? is perhaps about a politician that won an elec-
tion against another politician. While in a document
about sports, it could be a team that won against an-
other team in a game, or an athlete that defeated an-
other athlete. In our experiments, we use the meta-
descriptors of a document as side information and
train a standard LDA model to find the theme of a
document. See Section 3.1 for details.
Sentence theme: A document may cover several
themes. Moreover, sometimes the theme of a doc-
ument is too general to disambiguate senses. We
therefore also extract the theme of a sentence as a
feature. Details are in 3.1.
We call entity name and word features local, and
the two theme features global.
We employ a topic model to discover senses for
each path. Each path pi forms a document, and it
contains a list of entity pairs co-occurring with the
path in the tuples. Each entity pair is represented
by a list of features fk as we described. For each
path, we draw a multinomial distribution ? over top-
ics/senses. For each feature of an entity pair, we
draw a topic/sense from ?pi . Formally, the gener-
ative process is as follows:
?pi ? Dirichlet(?)
?z ? Dirichlet(?)
ze ? Multinomial(?pi)
fk ? Multinomial(?ze)
Assume we have m paths and l entity pairs for each
path. We denote each entity pair of a path as e(pi) =
(f1, . . . , fn). Hence we have:
P (e1(pi), e2(pi), . . . , el(pi)|z1, z2, . . . , zl)
=
l?
j=1
n?
k=1
p(fk|zj)p(zj)
We assume the features are conditionally indepen-
dent given the topic assignments. Each feature is
generated from a multinomial distribution ?. We
use Dirichlet priors on ? and ?. Figure 1 shows the
graphical representation of this model.
S
p
?
e(p)
f
?
?
z
?
n
Figure 1: Sense-LDA model.
This model is a minor variation on standard LDA
and the difference is that instead of drawing an ob-
servation from a hidden topic variable, we draw
multiple observations from a hidden topic variable.
Gibbs sampling is used for inference. After infer-
ence, each entity pair of a path is assigned to one
topic. One topic is one sense. Entity pairs which
share the same topic assignments form one sense
cluster.
2.2 Hierarchical Agglomerative Clustering
After discovering sense clusters of paths, we employ
hierarchical agglomerative clustering (HAC) to dis-
cover semantic relations from these sense clusters.
We apply the complete linkage strategy and take co-
sine similarity as the distance function. The cutting
threshold is set to 0.1.
We represent each sense cluster as one vector by
summing up features from each entity pair in the
cluster. The weight of a feature indicates how many
entity pairs in the cluster have the feature. Some
features may get larger weights and dominate the co-
sine similarity. We down-weigh these features. For
example, we use binary features for word ?defeat?
in sense clusters of pattern ?A defeat B?. The two
theme features are extracted from generative mod-
els, and each is a topic number.
Our approach produces sense clusters for each
path and semantic relation clusters of the whole data.
Table 1 and 2 show some example output.
3 Experiments
We carry out experiments on New York Times ar-
ticles from years 2000 to 2007 (Sandhaus, 2008).
Following (Yao et al, 2011), we filter out noisy doc-
uments and use natural language packages to anno-
tate the documents, including NER tagging (Finkel
et al, 2005) and dependency parsing (Nivre et al,
2004). We extract dependency paths for each pair of
named entities in one sentence. We use their lemmas
714
Path 20:sports 30:entertainment 25:music/art
A play B
Americans, Ireland Jean-Pierre Bacri, Jacques Daniel Barenboim, recital of Mozart
Yankees, Angels Rita Benton, Gay Head Dance Mr. Rose, Ballade
Ecuador, England Jeanie, Scrabble Gil Shaham, Violin Romance
Redskins, Detroit Meryl Streep, Leilah Ms. Golabek, Steinways
Red Bulls, F.C. Barcelona Kevin Kline, Douglas Fairbanks Bruce Springsteen, Saints
doc theme sports music books television music theater
sen theme game yankees theater production book film show music reviews opera
lexical words beat victory num-num won played plays directed artistic director conducted production
entity names - r:theater r:theater r:hall r:york l:opera
Table 1: Example sense clusters produced by sense disambiguation. For each sense, we randomly sample 5 entity
pairs. We also show top features for each sense. Each row shows one feature type, where ?num? stands for digital
numbers, and prefix ?l:? for source argument, prefix ?r:? for destination argument. Some features overlap with each
other. We manually label each sense for easy understanding. We can see the last two senses are close to each other.
For two theme features, we replace the theme number with the top words. For example, the document theme of the
first sense is Topic30, and Topic30 has top words ?sports?.
relation paths
entertainment A, who play B:30; A play B:30; star A as B:30
sports
lead A to victory over B:20; A play to B:20; A play B:20; A?s loss to B:20; A beat B:20; A trail B:20;
A face B:26; A hold B:26; A play B:26; A acquire (X) from B:26; A send (X) to B:26;
politics
A nominate B:39; A name B:39; A select B:39; A name B:42; A select B:42;
A ask B:42; A choose B:42; A nominate B:42; A turn to B:42;
law A charge B:39; A file against B:39; A accuse B:39; A sue B:39
Table 2: Example semantic relation clusters produced by our approach. For each cluster, we list the top paths in it,
and each is followed by ?:number?, indicating its sense obtained from sense disambiguation. They are ranked by the
number of entity pairs they take. The column on the left shows sense of each relation. They are added manually by
looking at the sense numbers associated with each path.
for words on the dependency paths. Each entity pair
and the dependency path which connects them form
a tuple.
We filter out paths which occur fewer than 200
times and use some heuristic rules to filter out paths
which are unlikely to represent a relation, for exam-
ple, paths in with both arguments take the syntac-
tic role ?dobj? (direct objective) in the dependency
path. In such cases both arguments are often part
of a coordination structure, and it is unlikely that
they are related. In summary, we collect about one
million tuples, 1300 patterns and half million named
entities. In terms of named entities, the data is very
sparse. On average one named entity occurs four
times.
3.1 Feature Extraction
For the entity name features, we split each entity
string of a tuple into tokens. Each token is a fea-
ture. The source argument tokens are augmented
with prefix ?l:?, and the destination argument tokens
with prefix ?r:?. We use tokens to encourage overlap
between different entities.
For the word features, we extract all the words be-
tween the two arguments, removing stopwords and
the words with capital letters. Words with capital
letters are usually named entities, and they do not
tend to indicate relations. We also extract neigh-
boring words of source and destination arguments.
The two words to the left of the source argument are
added with prefix ?lc:?. Similarly the two words to
the right of the destination arguments are added with
prefix ?rc:?.
Each document in the NYT corpus is associated
with many descriptors, indicating the topic of the
document. For example, some documents are la-
beled as ?Sports?, ?Dallas Cowboys?, ?New York
Giants?, ?Pro Football? and so on. Some are labeled
715
as ?Politics and Government?, and ?Elections?. We
shall extract a theme feature for each document from
these descriptors. To this end we interpret the de-
scriptors as words in documents, and train a standard
LDA model based on these documents. We pick the
most frequent topic as the theme of a document.
We also train a standard LDA model to obtain
the theme of a sentence. We use a bag-of-words
representation for a document and ignore sentences
from which we do not extract any tuples. The LDA
model assigns each word to a topic. We count the
occurrences of all topics in one sentence and pick
the most frequent one as its theme. This feature
captures the intuition that different words can indi-
cate the same sense, for example, ?film??, ?show?,
?series? and ?television? are about ?entertainment?,
while ?coach?, ?game?, ?jets?, ?giants? and ?sea-
son? are about ?sports?.
3.2 Sense clusters and relation clusters
For the sense disambiguation model, we set the
number of topics (senses) to 50. We experimented
with other numbers, but this setting yielded the best
results based on our automatic evaluation measures.
Note that a path has a multinomial distribution over
50 senses but only a few senses have non-zero prob-
abilities.
We look at some sense clusters of paths. For
path ?A play B?, we examine the top three senses,
as shown in Table 1. The last two senses ?enter-
tainment? and ?music? are close. Randomly sam-
pling some entity pairs from each of them, we find
that the two sense clusters are precise. Only 1% of
pairs from the sense cluster ?entertainment? should
be assigned to the ?music? sense. For the path ?play
A in B? we discover two senses which take the
most probabilities: ?sports? and ?art?. Both clus-
ters are precise. However, the ?sports? sense may
still be split into more fine-grained sense clusters. In
?sports?, 67% pairs mean ?play another team in a
location? while 33% mean ?play another team in a
game?.
We also closely investigate some relation clusters,
shown in Table 2. Both the first and second relation
contain path ?A play B? but with different senses.
For the second relation, most paths state ?play? re-
lations between two teams, while a few of them
express relations of teams acquiring players from
other teams. For example, the entity pair ?(Atlanta
Hawks, Dallas Mavericks)? mentioned in sentence
?The Atlanta Hawks acquired point guard Anthony
Johnson from the Dallas Mavericks.? This is due to
that they share many entity pairs of team-team.
3.3 Baselines
We compare our approach against several baseline
systems, including a generative model approach and
variations of our own approach.
Rel-LDA: Generative models have been suc-
cessfully applied to unsupervised relation extrac-
tion (Rink and Harabagiu, 2011; Yao et al, 2011).
We compare against one such model: An extension
to standard LDA that falls into the framework pre-
sented by Yao et al (2011). Each document con-
sists of a list of tuples. Each tuple is represented by
features of the entity pair, as listed in 2.1, and the
path. For each document, we draw a multinomial
distribution over relations. For each tuple, we draw
a relation topic and independently generate all the
features. The intuition is that each document dis-
cusses one domain, and has a particular distribution
over relations.
In our experiments, we test different numbers of
relation topics. As the number goes up, precision in-
creases whereas recall drops. We report results with
300 and 1000 relation topics.
One sense per path (HAC): This system uses
only hierarchical clustering to discover relations,
skipping sense disambiguation. This is similar to
DIRT (Lin and Pantel, 2001). In DIRT, each path
is represented by its entity arguments. DIRT cal-
culates distributional similarities between different
paths to find paths which bear the same semantic re-
lation. It does not employ global topic model fea-
tures extracted from documents and sentences.
Local: This system uses our approach (both sense
clustering with topic models and hierarchical clus-
tering), but without global features.
Local+Type This system adds entity type features to
the previous system. This allows us to compare per-
formance of using global features against entity type
features. To determine entity types, we link named
entities to Wikipedia pages using the Wikifier (Rati-
nov et al, 2011) package and extract categories from
the Wikipedia page. Generally Wikipedia provides
many types for one entity. For example, ?Mozart? is
716
a person, musician, pianist, composer, and catholic.
As we argued in Section 1, it is difficult to determine
the right granularity of the entity types to use. In our
experiments, we use all of them as features. In hier-
archical clustering, for each sense cluster of a path,
we pick the most frequent entity type as a feature.
This approach can be seen as a proxy to ISP (Pantel
et al, 2007), since selectional preferences are one
way of distinguishing multiple senses of a path.
Our Approach+Type This system adds Wikipedia
entity type features to our approach. The Wikipedia
feature is the same as used in the previous system.
4 Evaluations
4.1 Automatic Evaluation against Freebase
We evaluate relation clusters discovered by all ap-
proaches against Freebase. Freebase comprises a
large collection of entities and relations which come
from varieties of data sources, including Wikipedia
infoboxes. Many users also contribute to Freebase
by annotating relation instances. We use coreference
evaluation metrics: pairwise F-score and B3 (Bagga
and Baldwin, 1998). Pairwise metrics measure how
often two tuples which are clustered in one seman-
tic relation are labeled with the same Freebase label.
We evaluate approximately 10,000 tuples which oc-
cur in both our data and Freebase. Since our sys-
tem predicts fine-grained clusters comparing against
Freebase relations, the measure of recall is underes-
timated. The precision measure is more reliable and
we employ F-0.5 measure, which places more em-
phasis on precision.
Matthews correlation coefficient (MCC) (Baldi et
al., 2000) is another measure used in machine learn-
ing, which takes into account true and false positives
and negatives and is generally regarded as a bal-
anced measure which can be used when the classes
are of very different sizes. In our case, the true nega-
tive number is 100 times larger than the true positive
number. Therefor we also employ MCC, calculated
as
MCC = TP?TN?FP?FN?
(TP+FP )(TP+FN)(TN+FP )(TN+FN)
The MCC score is between -1 and 1. The larger the
better. In perfect predictions, FP and FN are 0, and
the MCC score is 1. A random prediction results in
score 0.
Table 3 shows the results of all systems. Our ap-
proach achieves the best performance in most mea-
sures. Without using sense disambiguation, the per-
formance of hierarchical clustering decreases signif-
icantly, losing 17% in precision in the pairwise mea-
sure, and 15% in terms ofB3. The generative model
approach with 300 topics achieves similar precision
to the hierarchical clustering approach. With more
topics, the precision increases, however, the recall
of the generative model is much lower than those
of other approaches. We also show the results of
our approach without global document and sentence
theme features (Local). In this case, both precision
and recall decrease. We compare global features
(Our approach) against Wikipedia entity type fea-
tures (Local+Type). We see that using global fea-
tures achieves better performance than using entity
type based features. When we add entity type fea-
tures to our approach, the performance does not in-
crease. The entity type features do not help much
is due to that we cannot determine which particular
type to choose for an entity pair. Take pair ?(Hillary
Rodham Clinton, Jonathan Tasini)? as an example,
choosing politician for both arguments instead of
person will help.
We should note that these measures provide com-
parison between different systems although they
are not accurate. One reason is the following:
some relation instances should have multiple la-
bels but they have only one label in Freebase.
For example, instances of a relation that a per-
son ?was born in? a country could be labeled
as ?/people/person/place of birth? and as ?/peo-
ple/person/nationality?. This decreases the pairwise
precision. Further discussion is in Section 4.3.
4.2 Path Intrusion
We also evaluate coherence of relation clusters pro-
duced by different approaches by creating path in-
trusion tasks (Chang et al, 2009). In each task, some
paths from one cluster and an intruding path from
another are shown, and the annotator?s job is to iden-
tify one single path which is out of place. For each
path, we also show the annotators one example sen-
tence. Three graduate students in natural language
processing annotate intruding paths. For disagree-
ments, we use majority voting. Table 4 shows one
example intrusion task.
717
System
Pairwise B3
Prec. Rec. F-0.5 MCC Prec. Rec. F-0.5
Rel-LDA/300 0.593 0.077 0.254 0.191 0.558 0.183 0.396
Rel-LDA/1000 0.638 0.061 0.220 0.177 0.626 0.160 0.396
HAC 0.567 0.152 0.367 0.261 0.523 0.248 0.428
Local 0.625 0.136 0.364 0.264 0.626 0.225 0.462
Local+Type 0.718 0.115 0.350 0.265 0.704 0.201 0.469
Our Approach 0.736 0.156 0.422 0.314 0.677 0.233 0.490
Our Approach+Type 0.682 0.110 0.334 0.250 0.687 0.199 0.460
Table 3: Pairwise and B3 evaluation for various systems. Since our systems predict more fine-grained clusters than
Freebase, the recall measure is underestimated.
Path Example sentence
A beat B Dmitry Tursunov beat the best American player, Andy Roddick
A, who lose to B Sluman, Loren Roberts (who lost a 1994 Open playoff to Ernie Els at Oakmont ...
A, who beat B ... offender seems to be the Russian Mariya Sharapova, who beat Jelena Dokic
A, a broker at B Robert Bewkes, a broker at UBS for 12 years
A meet B Howell will meet Geoff Ogilvy, Harrington will face Davis Love III
Table 4: A path intrusion task. We show 5 paths and ask the annotator to identify one path which does not belong to
the cluster. And we show one example sentence for each path. The entities (As and Bs) in the sentences are bold. And
the italic row here indicates the intruder.
System Correct
Rel-LDA/300 0.737
Rel-LDA/1000 0.821
HAC 0.852
Local+Type 0.773
Our approach 0.887
Table 5: Results of intruding tasks of all systems.
From Table 5, we see that our approach achieves
the best performance. We concentrate on some in-
trusion tasks and compare the clusters produced by
different systems.
The clusters produced by HAC (without sense dis-
ambiguation) is coherent if all the paths in one rela-
tion take a particular sense. For example, one task
contains paths ?A, director at B?, ?A, specialist at
B?, ?A, researcher at B?, ?A, B professor? and ?A?s
program B?. It is easy to identify ?A?s program B?
as an intruder when the annotators realize that the
other four paths state the relation that people work
in an educational institution. The generative model
approach produces more coherent clusters when the
number of relation topics increases.
The system which employs local and entity type
features (Local+Type) produces clusters with low
coherence because the system puts high weight on
types. For example, (United States, A talk with B,
Syria) and (Canada, A defeat B, United States) are
clustered into one relation since they share the argu-
ment types ?country?-?country?. Our approach us-
ing the global theme features can correct such errors.
4.3 Error Analysis
We also closely analyze the pairwise errors that we
encounter when comparing against Freebase labels.
Some errors arise because one instance can have
multiple labels, as we explained in Section 4.1. One
example is the following: Our approach predicts that
(News Corporation, buy, MySpace) and (Dow Jones
& Company, the parent of, The Wall Street Journal)
are in one relation. In Freebase, one is labeled as
?/organization/parent/child?, the other is labeled as
?/book/newspaper owner/newspapers owned?. The
latter is a sub-relation of the former. We can over-
come this issue by introducing hierarchies in relation
labels.
Some errors are caused by selecting the incorrect
sense for an entity pair of a path. For instance, we
put (Kenny Smith, who grew up in, Queens) and
(Phil Jackson, return to, Los Angeles Lakers) into
718
the ?/people/person/place of birth? relation cluster
since we do not detect the ?sports? sense for the en-
tity pair ?(Phil Jackson, Los Angeles Lakers)?.
5 Related Work
There has been considerable interest in unsupervised
relation discovery, including clustering approach,
generative models and many other approaches.
Our work is closely related to DIRT (Lin and Pan-
tel, 2001). Both DIRT and our approach represent
dependency paths using their arguments. Both use
distributional similarity to find patterns representing
similar semantic relations. Based on DIRT, Pantel
et al (2007) addresses the issue of multiple senses
per path by automatically learning admissible argu-
ment types where two paths are similar. They cluster
arguments to fine-grained entity types and rank the
associations of a relation with these entity types to
discover selectional preferences. Selectional prefer-
ences discovery (Ritter et al, 2010; Seaghdha, 2010)
can help path sense disambiguation, however, we
show that using global features performs better than
entity type features.
Our approach is also related to feature parti-
tioning in cross-cutting model of lexical seman-
tics (Reisinger and Mooney, 2011). And our sense
disambiguation model is inspired by this work.
There they partition features of words into views and
cluster words inside each view. In our case, each
sense of a path can be seen as one view. However,
we allow different views to be merged since some
views overlap with each other.
Hasegawa et al (2004) cluster pairs of named en-
tities according to the similarity of context words in-
tervening between them. Hachey (2009) uses topic
models to perform dimensionality reduction on fea-
tures when clustering entity pairs into relations. Bol-
legala et al (2010) employ co-clustering to find clus-
ters of entity pairs and patterns jointly. All the ap-
proaches above neither deal with polysemy nor in-
corporate global features, such as sentence and doc-
ument themes.
Open information extraction aims to discover re-
lations independent of specific domains (Banko et
al., 2007; Banko and Etzioni, 2008). They employ
a self-learner to extract relation instances, but no
attempt is made to cluster instances into relations.
Yates and Etzioni (2009) present RESOLVER for
discovering relational synonyms as a post process-
ing step. Our approach falls into the same category.
Moreover, we explore path senses and global fea-
tures for relation discovery.
Many generative probabilistic models have been
applied to relation extraction. For example, vari-
eties of topic models are employed for both open
domain (Yao et al, 2011) and in-domain relation
discovery (Chen et al, 2011; Rink and Harabagiu,
2011). Our approach employs generative models
for path sense disambiguation, which achieves better
performance than directly applying generative mod-
els to unsupervised relation discovery.
6 Conclusion
We explore senses of paths to discover semantic re-
lations. We employ a topic model to partition en-
tity pairs of a path into different sense clusters and
use hierarchical agglomerative clustering to merge
senses into semantic relations. Experimental results
show our approach discovers precise relation clus-
ters and outperforms a generative model approach
and a clustering method which does not address
sense disambiguation. We also show that using
global features improves the performance of unsu-
pervised relation discovery over using entity type
based features.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and the Uni-
versity of Massachusetts gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
719
Pierre Baldi, S?ren Brunak, Yves Chauvin, Claus A. F.
Andersen, and Henrik Nielsen. 2000. Assessing the
accuracy of prediction algorithms for classification: an
overview. Bioinformatics, 16:412?424.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI2007.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Se-
bastian Hellmann. 2009. DBpedia - a crystallization
point for the web of data. Journal of Web Semantics:
Science, Services and Agents on the World Wide Web,
pages 154?165.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research, 3:993?1022, January.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD ?08: Proceedings of the 2008
ACM SIGMOD international conference on Manage-
ment of data, pages 1247?1250, New York, NY, USA.
ACM.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2010. Relational duality: Unsupervised ex-
traction of semantic relations between entities on the
web. In Proceedings of WWW.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of NIPS.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of ACL.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
Benjamin Hachey. 2009. Towards Generic Relation Ex-
traction. Ph.D. thesis, University of Edinburgh.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proceedings of KDD.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning Inferential Selectional Preferences. In Pro-
ceedings of NAACL HLT.
Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to Wikipedia. In Proceedings of ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Joseph Reisinger and Raymond J. Mooney. 2011. Cross-
cutting models of lexical semantics. In Proceedings of
EMNLP.
Bryan Rink and Sanda Harabagiu. 2011. A generative
model for unsupervised discovery of relations and ar-
gument classes from clinical texts. In Proceedings of
EMNLP.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation method for Selectional Pref-
erences. In Proceedings of ACL10.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Diarmuid O Seaghdha. 2010. Latent variable models of
selectional preference. In Proceedings of ACL 10.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of EMNLP.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
720
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 193?197
Manchester, August 2008
Collective Semantic Role Labelling with Markov Logic
Sebastian Riedel Ivan Meza-Ruiz
Institute for Communicating and Collaborative Systems
School of Informatics
University of Edinburgh, Scotland
{S.R.Riedel,I.V.Meza-Ruiz}@sms.ed.ac.uk
Abstract
This paper presents our system for the
Open Track of the CoNLL 2008 Shared
Task (Surdeanu et al, 2008) in Joint De-
pendency Parsing
1
and Semantic Role La-
belling. We use Markov Logic to define
a joint SRL model and achieve a semantic
F-score of 74.59%, the second best in the
Open Track.
1 Introduction
Many SRL systems use a two-stage pipeline that
first extracts possible argument candidates (argu-
ment identification) and then assigns argument
labels to these candidates (argument classifica-
tion) (Xue and Palmer, 2004). If we also con-
sider the necessary previous step of identifying
the predicates and their senses (predicate identi-
fication) this yields a three-stage pipeline: predi-
cate identification, argument identification and ar-
gument classification.
Our system, on the other hand, follows a joint
approach in the spirit of Toutanova et al (2005)
and performs the above steps collectively . We de-
cided to use Markov Logic (ML, Richardson and
Domingos, 2005), a First Order Probabilistic Lan-
guage, to develop a global probabilistic model of
SRL. By using ML we are able to incorporate the
dependencies between the decisions of different
stages in the pipeline and the well-known global
correlations between the arguments of a predi-
cate (Punyakanok et al, 2005). And since learning
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
Note that in this work we do not consider the parsing
task; instead we use the provided dependencies of the open
track datatsets.
and inference methods were already implemented
in theML software we use, only minimal engineer-
ing efforts had to be done.
In contrast to the work of Toutanova et al (2005)
our system applies online learning to train its pa-
rameters and exact inference to predict a collective
role labelling. Moreover, we jointly label the argu-
ments of all predicates in a sentence. This allows
us, for example, to require that certain tokens have
to be an argument of some predicates in the sen-
tence.
In this paper we also investigate the impact of
different levels of interaction between the layers of
the joint SRL model. We find that a probabilis-
tic model which resembles a traditional bottom-up
pipeline (though jointly trained and globally nor-
malised) performs better than the complete joint
model on the WSJ test set and worse on the Brown
test set. The worst performance is observed when
no interaction between SRL stages is allowed.
In terms of semantic F-score (74.59%) our sub-
mitted results are the second best in the Open
Track of the Shared Task. Our error analysis in-
dicates that a) the training regime can be improved
and b) nominalizations are difficult to handle for
the model as it is.
In the next sections we will first briefly intro-
duce Markov Logic. Then we present the Markov
Logic model we used in our final submission. We
present and analyse our results in section 4 before
we conclude in Section 5.
2 Markov Logic
Markov Logic (ML, Richardson and Domingos,
2005) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First
Order Logic to allow formulae that can be vi-
olated with some penalty. From an alternative
193
point of view, it is an expressive template language
that uses First Order Logic formulae to instantiate
Markov Networks of repetitive structure.
Let us describe Markov Logic by considering
the predicate identification task. In Markov Logic
we can model this task by first introducing a set
of logical predicates
2
such as isPredicate(Token)
or word(Token,Word). Then we specify a set of
weighted first order formulae that define a distribu-
tion over sets of ground atoms of these predicates
(or so-called possible worlds).
Ideally, the distribution we define with these
weighted formulae assigns high probability to pos-
sible worlds where SRL predicates are correctly
identified and a low probability to worlds where
this is not the case. For example, a suitable set of
weighted formulae would assign a high probability
to the world
3
{word (1,Haag) , word(2, plays),
word(3, Elianti), isPredicate(2)}
and a low one to
{word (1,Haag) , word(2, plays),
word(3, Elianti), isPredicate(3)}
In Markov Logic a set M = {(?,w
?
)}
?
of
weighted first order formulae is called a Markov
Logic Network (MLN). It assigns the probability
p (y) =
1
Z
exp
?
?
?
(?,w)?M
w
?
c?C
n
?
f
?
c
(y)
?
?
(1)
to the possible world y. Here f
?
c
is a feature func-
tion that returns 1 if in the possible world y the
ground formula we get by replacing the free vari-
ables in ? by the constants in c is true and 0 oth-
erwise. C
n
?
is the set of all tuples of constants we
can replace the free variables in ? with. Z is a nor-
malisation constant. Note that this distribution cor-
responds to a Markov Network where nodes rep-
resent ground atoms and factors represent ground
formulae.
For example, if M contains the formula ?
word
(
x,
?
take
?
)
? isPredicate (x)
then its corresponding log-linear model has,
among others, a feature f
?
t1
for which x in ? has
2
In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
3
?Haag plays Elianti? is a segment of a sentence in train-
ing corpus.
been replaced by the constant t
1
and that returns 1
if
word
(
1,
?
take
?
)
? isPredicate (1)
is true in y and 0 otherwise.
We will refer predicates such as word as ob-
served because they are known in advance. In con-
trast, isPredicate is hidden because we need to in-
fer it at test time.
2.1 Learning
An MLN we use to model the collective SRL task
is presented in section 3. We learn the weights as-
sociated this MLN using 1-best MIRA (Crammer
and Singer, 2003) Online Learning method.
2.2 Inference
Assuming that we have an MLN, a set of weights
and a given sentence then we need to predict
the choice of predicates, frame types, arguments
and role labels with maximal a posteriori prob-
ability. To this end we apply a method that
is both exact and efficient: Cutting Plane Infer-
ence (CPI, Riedel, 2008) with Integer Linear Pro-
gramming (ILP) as base solver. We use it for infer-
ence at test time as well as during the MIRA online
learning process.
3 Model
We define five hidden predicates for the three
stages of the task. For predicate identification, we
use the predicates isPredicate and sense. isPred-
icate(p) indicates that the word in the position p
is an SRL predicate while sense(p,e) signals that
predicate in position p has the sense e.
For argument identification, we use the predi-
cates isArgument and hasRole. The atom isArgu-
ment(a) signals that the word in the position a is
a SRL argument of some (unspecified) SRL predi-
cate while hasRole(p,a) indicates that the token at
position a is an argument of the predicate in posi-
tion p.
Finally, for the argument classification stage we
define the predicate role. Here role(p,a,r) corre-
sponds to the decision that the argument in the po-
sition a has the role r with respect to the predicate
in the position p.
3.1 Local formulae
We define a set of local formulae. A formula is lo-
cal if its groundings relate any number of observed
ground atoms to exactly one hidden ground atom.
For example, a grounding of the local formula
lemma(p,+l
1
)?lemma(a,+l
2
) ? hasRole(p, a)
194
Figure 1: Factor graph for the local formula in sec-
tion 3.1.
can be seen in the Markov Network of Figure 1. It
connects a hidden hasRole ground atom to two ob-
served lemma ground atoms. Note that the ?+? pre-
fix for variables indicates that there is a different
weight for each possible pair of lemmas (l
1
, l
2
).
For the hasRole and role predicates we defined
local formulae that aimed to reproduce the stan-
dard features used in previous work (Xue and
Palmer, 2004). This also required us to develop
dependency-based versions of the constituent-
based features such as the syntactic path between
predicate and argument, as proposed by Xue and
Palmer (2004).
The remaining hidden predicates, isPredicate,
isArgument and sense, have local formulae that
relate their ground atoms to properties of a con-
textual window around the token the atom corre-
sponds to. For this we used the information pro-
vided in the closed track training corpus of the
shared task (i.e. both versions of lemma and POS
tags plus a coarse version of the POS tags).
Instead of describing the local feature set in
more detail we refer the reader to our MLN model
files.
4
They can be used both as a reference and
as input to our Markov Logic Engine
5
, and thus al-
low the reader to easily reproduce our results. We
believe that this is another advantage of explicitly
separating model and algorithms by using first or-
der probabilistic logic languages.
3.2 Global formulae
Global formulae relate several hidden ground
atoms. We use them for two purposes: to en-
sure consistency between the decisions of all SRL
stages and to capture some of our intuition about
the task. We will refer to formulae that serve the
first purpose as structural constraints.
For example, a structural constraint is given by
4
http://thebeast.googlecode.com/svn/
mlns/conll08
5
http://thebeast.googlecode.com
the (deterministic) formula
role(p, a, r) ? hasRole(p, a)
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a). Note that this formula by itself mod-
els the traditional ?bottom-up? argument identifi-
cation and classification pipeline: it is possible to
not assign a role r to an predicate-argument pair
(p, a) proposed by the identification stage; how-
ever, it is impossible to assign a role r to token
pairs (p, a) that have not been proposed as poten-
tial arguments.
One example of another class of structural con-
straints is
hasRole(p, a) ? ?r.role(p, a, r)
which, by itself, models an inverted or ?top-down?
pipeline. In this architecture the argument classi-
fication stage can assign roles to tokens that have
not been proposed by the argument identification
stage. However, it must assign a label to any token
pair the previous stage proposes. Figure 2 illus-
trates the structural formulae we use in form of a
Markov Network.
The formulae we use to ensure consistency be-
tween the remaining hidden predicates are omitted
for brevity as they are very similar to the bottom-
up and top-down formulae we presented above.
For the SRL predicates that perform a labelling
task (role and sense) we also need a structural con-
straint which ensures that not more than one label
is assigned. For instance,
(role(p, a, r
1
) ? r
1
6= r
2
? ?role(p, a, r
2
))
forbids two different semantic roles for a pair of
words.
The global formulae that capture our intuition
about the task itself can be further divided into two
classes. The first one uses deterministic or hard
constraints such as
role (p, a
1
, r) ? ?mod (r) ? a
1
6= a
2
?
?role (p, a
2
, r)
which forbids cases where distinct arguments of
a predicate have the same role unless the role de-
scribes a modifier.
The second class of global formulae is soft or
nondeterministic. For instance, the formula
lemma(p,+l) ? ppos(a,+p)
?hasRole(p, a) ? sense(p,+f)
195
Figure 2: Markov Network that illustrates the
structural constraints we use.
is a soft global formula. It captures the observation
that the sense of a verb or noun depends on the type
of its arguments. Here the type of an argument
token is represented by its POS tag.
4 Results
We only submitted results for the Open Track of
the Shared Task. Moreover, we focused on SRL
and did not infer dependencies; instead we used
the MALT dependencies parses provided in the
Open Track dataset. Our submission was ranked
second out of five with a semantic F1-score of
74.59%.
6
After submission we also set up additional ex-
periments to evaluate different types and degrees
of connectivity between the decisions made by our
model. To this end we created four new models:
a model that omits top-down structural constraints
and thus resembles a (globally trained) bottom-
up pipeline (Up); a model that does not contain
bottom-up structural constraints and thus resem-
bles a top-down architecture (Down); a model
in which stages are not connected at all (Iso-
lated); and finally, a model in which additional
global formulae are omitted and the only remain-
ing global formulae are structural (Structural). The
results we submitted were generated using the full
model (Full).
Table 1 summarises the results for each of these
models. We report the F-scores for the WSJ and
Brown test corpora provided for the task. In addi-
tion we show training and test times for each sys-
tem.
There are four findings we take from this. First,
and somewhat surprisingly, the jointly trained
bottom-up model (Up) performs substantially bet-
6
While we did use information of the open dataset we do
believe that it is possible to train a stacked parsing-SRL sys-
tem that would perform similarily. If so, our system would
have the 5th best semantic scores among the 20 participants
of the closed track.
Model WSJ Brown Train Test
Time Time
Full 75.72% 65.38% 25h 24m
Up 76.96% 63.86% 11h 14m
Down 73.48% 59.34% 22h 23m
Isolated 60.49% 48.12% 11h 14m
Structural 74.93% 64.23% 22h 33m
Table 1: F-scores for different models.
ter than the full model on the WSJ test corpus. We
will try to give an explanation for this result in
the next section. Second, the bottom-up model is
twice as fast compared to both the full and the top-
down model. This is due to the removal of formu-
lae with existential quantifiers that would result in
large clique sizes of the ground Markov Network.
Third, the isolated model performs extremely poor,
particularly for argument classification. Here fea-
tures defined for the role predicate can not make
any use of the information in previous stages. Fi-
nally, the additional global formulae do improve
performance, although not substantially.
4.1 Analysis
A substantial amount of errors in our submitted re-
sults (Full) can be attributed to the seemingly ran-
dom assignment of the very low frequency label
?R-AA? (appears once in the training set) to token
pairs that should either have a different role or no
role at all. Without these false positives, precision
would increase by about 1%. Interestingly, this
type of error completely disappears for the bottom-
up model (Up) and thus seem to be crucial in order
understand why this model can outperform the full
model.
We believe that this type of error is an artifact of
the training regime. For the full model the weights
of the role predicate only have ensure that the right
(true positive) role is the relative winner among
all roles. In the bottom-up model they also have
to make sure that their cumulative weight is non-
negative ? otherwise simply not assigning a role
r for (p, a) would increase the score even if has-
Role(p,a) is predicted with high confidence. Thus
more weight is shifted towards the correct roles.
This helps the right label to win more likely over
the ?R-AA? label, whose weights have rarely been
touched and are closer to zero.
Likewise, in the bottom-up model the total
weight of the hasRole features of a wrong (false
positive) candidate token pair must be nonpositive.
Otherwise picking the wrong candidate would in-
crease overall score and no role features can re-
196
ject this decision because the corresponding struc-
tural constraints are missing. Thus more weight
is shifted away from false positive candidates, re-
sulting in a higher precision of the hasRole pred-
icate. This also means that less wrong candidates
are proposed, for which the ?R-AA? role is more
likely to be picked because its weights have hardly
been touched.
However, it seems that by increasing precision
in this way, we decrease recall for out-of-domain
data. This leads to a lower F1 score for the bottom-
up model on the Brown test set.
Another prominent type of errors appear for
nominal predicates. Our system only recovers only
about 80% of predicates with ?NN?, ?NNS? and
?NNP? tags (and classifies about 90% of these with
the right predicate sense). Argument identification
and classification performs equally bad. For exam-
ple, for the ?A0? argument of ?VB? predicates we
get an F-score of 82.00%. For the ?A0? of ?NN?
predicates F-score is 65.92%. The features of our
system are essentially taken from the work done on
PropBank predicates and we did only little work
to adapt these to the case of nominal predicates.
Putting more effort into designing features specific
to the case of nominal predicates might improve
this situation.
5 Conclusion
We presented a Markov Logic Network that jointly
performs predicate identification, argument identi-
fication and argument classification for SRL. This
network achieves the second best semantic F-
scores in the Open Track of the CoNLL shared
task.
Experimentally we show that results can be fur-
ther improved by using an MLN that resembles a
conventional SRL bottom-up pipeline (but is still
jointly trained and globally normalised) instead
of a fully connected model. We hypothesise that
when training this model more weight is shifted
away from wrong argument candidates and more
weight is shifted towards correct role labels. This
results in higher precision for argument identifica-
tion and better accuracy for argument classifica-
tion.
Possible future work includes better treatment
of nominal predicates, for which we perform quite
poorly. We would also like to investigate the im-
pact of linguistically motivated global formulae
more thoroughly. So far our model benefits from
them, albeit not substantially.
References
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 2003.
V. Punyakanok, D. Roth, and W. Yih. Generalized
inference with multiple semantic role labeling
systems. In Proceedings of the Annual Con-
ference on Computational Natural Language
Learning, 2005.
Matthew Richardson and Pedro Domingos.
Markov logic networks. Technical report,
University of Washington, 2005.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
Proceedings of the Annual Conference on Un-
certainty in AI, 2008.
Mihai Surdeanu, Richard Johansson, Adam Mey-
ers, Llu??s M`arquez, and Joakim Nivre. The
CoNLL-2008 shared task on joint parsing of
syntactic and semantic dependencies. In Pro-
ceedings of the 12th Conference on Compu-
tational Natural Language Learning (CoNLL-
2008), 2008.
Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. Joint learning improves se-
mantic role labeling. In Proceedings of the 43rd
Annual Meeting on Association for Computa-
tional Linguistics, 2005.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In Proceedings
of the Annual Conference on Empirical Methods
in Natural Language Processing, 2004.
197
Proceedings of BioNLP Shared Task 2011 Workshop, pages 46?50,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Robust Biomedical Event Extraction with Dual Decomposition and Minimal
Domain Adaptation
Sebastian Riedel Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{riedel,mccallum}@cs.umass.edu
Abstract
We present a joint model for biomedical event
extraction and apply it to four tracks of the
BioNLP 2011 Shared Task. Our model de-
composes into three sub-models that concern
(a) event triggers and outgoing arguments, (b)
event triggers and incoming arguments and
(c) protein-protein bindings. For efficient de-
coding we employ dual decomposition. Our
results are very competitive: With minimal
adaptation of our model we come in second
for two of the tasks?right behind a version
of the system presented here that includes pre-
dictions of the Stanford event extractor as fea-
tures. We also show that for the Infectious
Diseases task using data from the Genia track
is a very effective way to improve accuracy.
1 Introduction
This paper presents the UMass entry to the BioNLP
2011 shared task (Kim et al, 2011a). We introduce
a simple joint model for the extraction of biomedical
events, and show competitive results for four tracks
of the competition. Our model subsumes three
tractable sub-models, one for extracting event trig-
gers and outgoing edges, one for event triggers and
incoming edges and one for protein-protein bind-
ings. Fast and accurate joint inference is provided by
combining optimizing methods for these three sub-
models via dual decomposition (Komodakis et al,
2007; Rush et al, 2010). Notably, our model con-
stitutes the first joint approach that explicitly pre-
dicts which protein should share the same binding
event. So far this has either been done through post-
processing heuristics (Bj?rne et al, 2009; Riedel et
al., 2009; Poon and Vanderwende, 2010), or through
a local classifier at the end of a pipeline (Miwa et al,
2010).
Our model is very competitive. For Genia (GE)
Task 1 (Kim et al, 2011b) we achieve the second-
best results. In addition, the best-performing FAUST
system (Riedel et al, 2011) is a variant of the model
presented here. Its advantage stems from the fact
that it uses predictions of the Stanford system (Mc-
Closky et al, 2011a; McClosky et al, 2011b), and
hence performs model combination. The same holds
for the Infectious Diseases (ID) track (Pyysalo et al,
2011), where we come in as second right behind
the FAUST system. For the Epigenetics and Post-
translational Modifications (EPI) track (Ohta et al,
2011) we achieve the 4th rank, partly because we did
not aim to extract speculations, negations or cellular
locations. Finally, for Genia Task 2 we rank 3rd?
with the 1st rank achieved by the FAUST system.
In the following we will briefly describe our
model and inference algorithm, as far as this is pos-
sible in limited space. Then we show our results on
the three tasks and conclude. Note we will assume
familiarity with the task, and refer the reader to the
shared task overview paper for more details.
2 Biomedical Event Extraction
Our goal is to extract biomedical events as shown
in figure 1a). To formulate the search for such
structures as an optimization problem, we represent
structures through a set of binary variables. Our rep-
resentation is inspired by previous work (Riedel et
al., 2009; Bj?rne et al, 2009) and based on a projec-
tion of events to a labelled graph over tokens in the
46
... phosphorylation of TRAF2 inhibits binding to the CD40 ...
Phosphorylation
Regulation
Binding
Theme
Cause
Theme
Theme
Theme
Regulation BindingPhosphorylation
Theme
Cause
Theme
Theme
Theme
Same Binding
 2 3
4 5 6 7 8 9
b
4,9
e
2,Phos.
a
6,9,Theme
(a)
(b)
Figure 1: (a) sentence with target event structure; (b) pro-
jection to labelled graph.
sentence, as seen figure 1b).
We will first present some basic notation to sim-
plify our exposition. For each sentence x we have
a set candidate trigger words Trig (x), and a set of
candidate proteins Prot (x). We will generally use
the indices i and l to denote members of Trig (x), the
indices p, q for members of Prot (x) and the index j
for members of Cand (x) def= Trig (x) ? Prot (x).
We label each candidate trigger i with an event
Type t ? T (with None ? T ), and use the binary
variable ei,t to indicate this labeling. We use binary
variables ai,l,r to indicate that between i and l there
is an edge labelled r ? R (with None ? R).
The representation so far has been used in previ-
ous work (Riedel et al, 2009; Bj?rne et al, 2009).
Its shortcoming is that it does not capture whether
two proteins are arguments of the same binding
event, or arguments of two binding events with the
same trigger. To overcome this problem, we intro-
duce binary ?same Binding? variables bp,q that are
active whenever there is a binding event that has
both p and q as arguments. Our inference algorithm
will also need, for each trigger i and protein pair p, q,
a binary variable ti,p,q that indicates that at i there is
a binding event with arguments p and q. All ti,p,q are
summarized in t.
Constructing events from solutions (e,a,b) can
be done almost exactly as described by Bj?rne et al
(2009). However, while Bj?rne et al (2009) group
arguments according to ad-hoc rules based on de-
pendency paths from trigger to argument, we simply
query the variables bp,q.
3 Model
We use the following objective to score the struc-
tures we like to extract:
s (e,a,b) def=
?
ei,t=1
sT (i, t) +
?
ai,j,r=1
sR (i, j, r)+
?
bp,q=1
sB (p, q)
with local scoring functions sT (i, t)
def=
?wT, fT (i, t)?, sR (i, j, r)
def= ?wR, fR (i, j, r)?
and sB (p, q)
def= ?wB, fB (p, q)?.
Our model scores all parts of the structure in isola-
tion. It is a joint model due to the three types of con-
straints we enforce. The first type acts on trigger la-
bels and their outgoing edges. It includes constraints
such as ?an active label at trigger i requires at least
one active outgoing Theme argument?. The second
type enforces consistency between trigger labels and
their incoming edges. That is, if an incoming edge
has a label that is not None, the trigger must not be
labelled None either. The third type of constraints
ensures that when two proteins p and q are part of
the same binding (as indicated by bp,q = 1), there
needs to be a binding event at some trigger i that
has p and q as arguments. We will denote the set of
structures (e,a,b) that satisfy all above constraints
as Y .
To learn w we choose the passive-aggressive
online learning algorithm (Crammer and Singer,
2003). As loss function we apply a weighted sum of
false positives and false negative labels and edges.
The weighting scheme penalizes false negatives 3.8
times more than false positives.
3.1 Features
For feature vector fT (i, t) we use a collection of
representations for the token i: word-form, lemma,
POS tag, syntactic heads, syntactic children; mem-
bership in two dictionaries used by Riedel et al
(2009).For fR (a; i, j, r) we use representations of
the token pair (i, j) inspired by Miwa et al (2010) .
They contain: labelled and unlabeled n-gram depen-
dency paths; edge and vertex walk features (Miwa et
al., 2010), argument and trigger modifiers and heads,
words in between (for close distance i and j). For
fB (b; p, q) we use a small subset of the token pair
representations in fR.
47
Algorithm 1 Dual Decomposition.
require:
R: max. iteration, ?t: stepsizes
t? 0 ?? 0?? 0
repeat
(e?, a?)? bestIncoming (??)
(e,a)? bestOutgoing (cout (?,?))
(b, t)? bestBinding
(
cbind (?)
)
?i,t ? ?i,t ? ?t (ei,t ? e?i,t)
?i,j,r ? ?i,j,r ? ?t (ai,j,r ? a?i,j,r)
?trigi,j,k ?
[
?trigi,j,k ? ?t (ei,Bind ? ti,j,k)
]
+
?arg1i,j,k ?
[
?arg1i,j,k ? ?t (ai,j,Theme ? ti,j,k)
]
+
?arg2i,j,k ?
[
?arg2i,j,k ? ?t (ai,k,Theme ? ti,j,k)
]
+
t ? t + 1
until no ?, ? changed or t > R
return(e,a,b)
3.2 Inference
Inference in our model amounts to solving
arg max
(e,a,b)?Y
s (e,a,b) . (1)
Our approach to finding the maximizer is dual de-
composition (Komodakis et al, 2007; Rush et al,
2010), a technique that allows us to exploit effi-
cient search algorithms for tractable substructures
of our problem. We divide the problem into three
sub-problems: (1) finding the highest-scoring trig-
ger labels and edges (e,a) such that constraints on
triggers and their outgoing edges are fulfilled; (2)
finding the highest-scoring trigger labels and edges
(e?, a?) such that constraints on triggers and their in-
coming edges are fulfilled; (3) finding the highest-
scoring pairs of proteins b to appear in the same
binding, and make binding event trigger decisions
t for these. Due to space constraints we only state
that the first two problems can be solved exactly in
O
(
n2 + nm
)
time while the last needs O
(
m2n
)
.
Here n is the number of trigger candidates and m
the number of proteins.
The subroutines to solve these three sub-problems
are combined in algorithm 1?an instantiation of
subgradient descent on the dual of an LP relaxation
of problem 1. In the first three steps in the main
loop of this algorithm, the individual sub-problems
are solved. Note that to each subroutine a parame-
ter is passed. For example, when finding the struc-
ture (e?, a?) that maximizes the objective under the
incoming edge constraints, we pass the parameter
??. This parameter represents a set of penalties to
be added to the objective used for the subproblem.
In this case we have penalties ??i,e to be added to
the scores of trigger-label pairs (i, e), and penalties
??i,j,r to be added for labelled edges i
r
? j.
One way to understand dual decomposition is as
iterative tuning of the penalties such that eventu-
ally all individual solutions are consistent with each
other. In our case this would mean, among other
things, that the solutions (e,a) and (e?, a?) are iden-
tical. This tuning happens in the second part of the
main loop which updates the dual variables? and?.
We see, for example, how the penalties ?i,e are de-
creased by ei,e? e?i,e scaled by a step-size ?t. Effec-
tively this change to ?i,e will decrease the score of
e?i,e within bestIn (??) by ?t if e?i,e was true while
ei,e was false in the current solutions.1 If e?i,e was
false but ei,e was true, the score is increased by ?t.
If both agree, no change is needed.
Consistency between solutions also means that
the binding decisions in b and t are consistent
with the rest of the solution. This is achieved in
algorithm 1 through tuning of the dual variables
? but we omit details for brevity. For complete-
ness we state how the penalties used for solving
the other subproblems are set based on the dual
variables ? and ?. We set couti,t (?,?)
def= ?i,t +
?t,Bind
?
p,q ?
trig
i,p,q; for the case that j ? Prot (x) we
get couti,j,r (?,?)
def= ?i,j,r +
?
p ?
arg1
i,j,p +
?
q ?
arg2
i,q,j ,
otherwise couti,j,r (?,?)
def= ?i,j,r . For bestBind (c)
we set cbindi,p,q (?) = ??
trig
i,p,q ? ?
arg1
i,,p,q ? ?
arg2
i,,p,q.
3.3 Preprocessing
After basic tokenization and sentence segmentation,
we generate a set of protein head tokens Prot (x)
for each sentence x based on protein span defi-
nitions from the shared task. To ensure tokens
contain not more than one protein we split them
at protein boundaries. Parsing is performed using
the Charniak-Johnson parser (Charniak and John-
son, 2005) with the self-trained biomedical parsing
1We refer to Koo et al (2010) for details on how to set ?t.
48
SVT BIND REG TOT
Task 1 73.5 48.8 43.8 55.2
Task 1 (abst.) 71.5 50.8 45.5 56.1
Task 1 (full) 79.2 44.4 40.1 53.1
Task 2 71.4 38.6 39.1 51.0
Table 1: Results for the GE track, task 1 and 2;
abst.=abstract; full=full text.
model of McClosky and Charniak (2008). Finally,
based on the set of trigger words in the training data,
we generate a set of candidate triggers Trig (x).
4 Results
We apply the same model to the GE, ID and EPI
tracks, with minor modifications in order to deal
with the different event type sets T and role sets R
of each track. Training and testing together took be-
tween 30 (EPI) to 120 (GE) minutes using a single-
core implementation.
4.1 Genia
Our results for GE task 1 and 2 can be seen in table
1. We also show results for abstracts only (abst.),
and for full text only (full). Note that binding events
(BIND) and general regulation events (REG) seem
to be harder to extract in full text. Somewhat surpris-
ingly, for simple events (SVT) the opposite holds.
We also like to point out that for full text extrac-
tion we rank first?the second best FAUST system
achieves an F1 score of 52.67.
4.2 Infectious Diseases
The Infectious Diseases track differs from the Genia
track in two important ways. First, it introduces the
event type Process that is allowed to have no ar-
guments at all. Second, it comes with significantly
less training data (152 vs 908 documents). We can
accommodate the first difference by making simple
changes in our inference algorithms. For example,
for Process events we do not force the algorithm to
pick a Theme argument.
To compensate for the lack of training data we
simply add data from the GE track. This is reason-
able because annotations overlap quite significantly.
In table 2 we show the impact of mixing different
amounts of ID data (I) and GE data (G) into the
training set. We point out that adding the ID training
I/G BIND REG PRO TOT
DEV 1/0 18.6 27.1 34.3 41.5
DEV 0/1 18.2 26.8 0.00 35.5
DEV 1/1 20.0 33.1 49.3 47.2
DEV 2/1 20.0 34.5 52.0 48.5
TEST 2/1 34.6 46.4 62.3 53.4
Table 2: ID results for different amounts of ID (I) and (G)
training data.
set twice, and the GENIA set once, leads to the best
performance (I/G=2/1). Remarkably, the F1 score
for Process increases by including data, although
this data does not include any such events. This may
stem from a shared model of None arguments that is
improved with more data.
4.3 Epigenetics and Post-translational
Modifications
For this track a different set of events is to be pre-
dicted. However, it is straightforward to adapt our
model and algorithms to this setting. For brevity we
only report our total results here and omit a table
with details. The first metric (ALL) includes nega-
tion, speculation and cellular location targets. We
omitted these in our model and hence our result of
33.52 F1 is relatively weak. For the metric that ne-
glects these aspects (CORE), we achieve 64.15 F1
and come in 4th. Note that in this metric the FAUST
system, based on the model presented here, comes
in as very close second.
5 Conclusion
We have presented a robust joint model for event
extraction from biomedical text that performs well
across all tasks. Remarkably, no feature set or pa-
rameter tuning was necessary to achieve this. We
also show substantial improvements for the ID task
by adding GENIA data into the training set.
Acknowledgements
This work was supported in part by the Center for Intelli-
gent Information Retrieval. The University of Massachusetts
gratefully acknowledges the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Program
under Air Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are those
of the authors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government.
49
References
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the Natural Language
Processing in Biomedicine NAACL 2009 Workshop
(BioNLP ?09), pages 10?18, Morristown, NJ, USA.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ?05),
pages 173?180.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In In ICCV.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
46rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL ?08).
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Con-
ference (ACL-HLT?11), Main Conference (to appear),
Portland, Oregon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in BioNLP 2011. In BioNLP 2011 Shared
Task.
Makoto Miwa, Rune Saetre, Jin-Dong D. Kim, and
Jun?ichi Tsujii. 2010. Event extraction with com-
plex event classification using rich features. Journal of
bioinformatics and computational biology, 8(1):131?
146, February.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Lit-
erature. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
813?821, Los Angeles, California, June. Association
for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ?09), pages 41?49.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Christopher D. Manning, and Andrew McCallum.
2011. Model combination for event extraction in
BioNLP 2011. In BioNLP 2011 Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In In Proc. EMNLP.
50
Proceedings of BioNLP Shared Task 2011 Workshop, pages 51?55,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Model Combination for Event Extraction in BioNLP 2011
Sebastian Riedela, David McCloskyb, Mihai Surdeanub,
Andrew McCalluma, and Christopher D. Manningb
a Department of Computer Science, University of Massachusetts at Amherst
b Department of Computer Science, Stanford University
{riedel,mccallum}@cs.umass.edu
{mcclosky,mihais,manning}@stanford.edu
Abstract
We describe the FAUST entry to the BioNLP
2011 shared task on biomolecular event ex-
traction. The FAUST system explores sev-
eral stacking models for combination using
as base models the UMass dual decomposi-
tion (Riedel and McCallum, 2011) and Stan-
ford event parsing (McClosky et al, 2011b)
approaches. We show that using stacking is
a straightforward way to improving perfor-
mance for event extraction and find that it is
most effective when using a small set of stack-
ing features and the base models use slightly
different representations of the input data. The
FAUST system obtained 1st place in three out
of four tasks: 1st place in Genia Task 1 (56.0%
f-score) and Task 2 (53.9%), 2nd place in the
Epigenetics and Post-translational Modifica-
tions track (35.0%), and 1st place in the In-
fectious Diseases track (55.6%).
1 Introduction
To date, most approaches to the BioNLP event ex-
traction task (Kim et al, 2011a) use a single model
to produce their output. However, model combina-
tion techniques such as voting, stacking, and rerank-
ing have been shown to consistently produce higher
performing systems by taking advantage of multi-
ple views of the same data. The Netflix Prize (Ben-
nett et al, 2007) is a prime example of this. System
combination essentially allows systems to regular-
ize each other, smoothing over the artifacts of each
(c.f. Nivre and McDonald (2008), Surdeanu and
Manning (2010)). To our knowledge, the only previ-
ous example of model combination for the BioNLP
shared task was performed by Kim et al (2009). Us-
ing a weighted voting scheme to combine the out-
puts from the top six systems, they obtained a 4%
absolute f-score improvement over the best individ-
ual system.
This paper shows that using a straightforward
model combination strategy on two competitive
systems produces a new system with substantially
higher accuracy. This is achieved with the frame-
work of stacking: a stacking model uses the output
of a stacked model as additional features.
While we initially considered voting and rerank-
ing model combination strategies, it seemed that
given the performance gap between the UMass and
Stanford systems that the best option was to in-
clude the predictions from the Stanford system into
the UMass system (e.g., as in Nivre and McDon-
ald (2008)). This has the advantage that one model
(Umass) determines how to integrate the outputs of
the other model (Stanford) into its own structure,
whereas in reranking, for example, the combined
model is required to output a complete structure pro-
duced by only one of the input models.
2 Approach
In the following we briefly present both the stacking
and the stacked model and some possible ways of
integrating the stacked information.
2.1 Stacking Model
As our stacking model, we employ the UMass ex-
tractor (Riedel and McCallum, 2011). It is based on
a discriminatively trained model that jointly predicts
trigger labels, event arguments and protein pairs in
51
binding. We will briefly describe this model but first
introduce three types of binary variables that will
represent events in a given sentence. Variables ei,t
are active if and only if the token at position i has
the label t. Variables ai,j,r are active if and only if
there is an event with trigger i that has an argument
with role r grounded at token j. In the case of an
entity mention this means that the mention?s head is
j. In the case of an event j is the position of its trig-
ger. Finally, variables bp,q indicate whether or not
two entity mentions at p and q appear as arguments
in the same binding event.
Two parts form our model: a scoring function, and
a set of constraints. The scoring function over the
trigger variables e, argument variables a and binding
pair variables b is
s (e,a,b) def=
?
ei,t=1
sT (i, t) +
?
ai,j,r=1
sR (i, j, r)+
?
bp,q=1
sB (p, q)
with local scoring functions sT (i, t)
def=
?wT, fT (i, t)?, sR (i, j, r)
def= ?wR, fR (i, j, r)?
and sB (p, q)
def= ?wB, fB (p, q)?.
Our model scores all parts of the structure in iso-
lation. It is a joint model due to the nature of the
constraints we enforce: First, we require that each
active event trigger must have at least one Theme ar-
gument; second, only regulation events (or Catalysis
events for the EPI track) are allowed to have Cause
arguments; third, any trigger that is itself an argu-
ment of another event has to be labelled active, too;
finally, if we decide that two entities p and q are part
of the same binding (as indicated by bp,q = 1), there
needs to be a binding event at some trigger i that
has p and q as arguments. We will denote the set of
structures (e,a,b) that satisfy these constraints as
Y .
Stacking with this model is simple: we only
need to augment the local feature functions fT (i, t),
fR (i, j, r) and fB (p, q) to include predictions from
the systems to be stacked. For example, for every
system S to be stacked and every pair of event types
(t?, tS) we add the features
fS,t? ,tS (i, t) =
{
1 hS (i) = tS ? t? = t
0 otherwise
to fT (i, t). Here hS (i) is the event label given to to-
ken i according to S. These features allow different
weights to be given to each possible combination of
type t? that we want to assign, and type tS that S
predicts.
Inference in this model amounts to maximizing
s (e,a,b) over Y . Our approach to solving this
problem is dual decomposition (Komodakis et al,
2007; Rush et al, 2010). We divide the problem into
three subproblems: (1) finding the best trigger label
and set of outgoing edges for each candidate trigger;
(2) finding the best trigger label and set of incoming
edges for each candidate trigger; (3) finding the best
pairs of entities to appear in the same binding. Due
to space limitations we refer the reader to Riedel and
McCallum (2011) for further details.
2.2 Stacked Model
For the stacked model, we use a system based on an
event parsing framework (McClosky et al, 2011a)
referred to as the Stanford model in this paper. This
model converts event structures to dependency trees
which are parsed using MSTParser (McDonald et
al., 2005).1 Once parsed, the resulting dependency
tree is converted back to event structures. Using the
Stanford model as the stacked model is helpful since
it captures tree structure which is not the focus in
the UMass model. Of course, this is also a limita-
tion since actual BioNLP event graphs are DAGs,
but the model does well considering these restric-
tions. Additionally, this constraint encourages the
Stanford model to provide different (and thus more
useful for stacking) results.
Of particular interest to this paper are the four
possible decoders in MSTParser. These four de-
coders come from combinations of feature order
(first or second) and whether the resulting depen-
dency tree is required to be projective.2 Each de-
coder presents a slightly different view of the data
and thus has different model combination proper-
ties. Projectivity constraints are not captured in the
UMass model so these decoders incorporate novel
information.
To produce stacking output from the Stanford sys-
tem, we need its predictions on the training, devel-
1http://sourceforge.net/projects/mstparser/
2For brevity, the second-order non-projective decoder is ab-
breviated as 2N, first-order projective as 1P, etc.
52
UMass FAUST+All
R P F1 R P F1
GE T1 48.5 64.1 55.2 49.4 64.8 56.0
GE T2 43.9 60.9 51.0 46.7 63.8 53.9
EPI (F) 28.1 41.6 33.5 28.9 44.5 35.0
EPI (C) 57.0 73.3 64.2 59.9 80.3 68.6
ID (F) 46.9 62.0 53.4 48.0 66.0 55.6
ID (C) 49.5 62.1 55.1 50.6 66.1 57.3
Table 1: Results on test sets of all tasks we submitted to.
T1 and T2 stand for task 1 and 2, respectively. C stands
for CORE metric, F for FULL metric.
opment and test sets. For predictions on test and de-
velopment sets we used models learned from the the
complete training set. Predictions over training data
were produced using crossvalidation. This helps to
avoid a scenario where the stacking model learns to
rely on high accuracy at training time that cannot be
matched at test time.
Note that, unlike Stanford?s individual submission
in this shared task, the stacked models in this paper
do not include the Stanford reranker. This is because
it would have required making a reranker model for
each crossvalidation fold.
We made 19 crossvalidation training folds for Ge-
nia (GE) (Kim et al, 2011b), 12 for Epigenetics
(EPI), and 17 for Infectious Diseases (ID) (Kim et
al., 2011b; Ohta et al, 2011; Pyysalo et al, 2011,
respectively). Note that while ID is the smallest and
would seem like it would have the fewest folds, we
combined the training data of ID with the training
and development data from GE. To produce predic-
tions over the test data, we combined the training
folds with 6 development folds for GE, 4 for EPI,
and 1 for ID.
3 Experiments
Table 1 gives an overview of our results on the test
sets for all four tasks we submitted to. Note that
for the EPI and ID tasks we show the CORE metric
next to the official FULL metric. The former is suit-
able for our purposes because it does not measure
performance for negations, speculations and cellular
locations?all of these we did not attempt to predict.
We compare the UMass standalone system to the
FAUST+All system which stacks the Stanford 1N,
1P, 2N and 2P predictions. For all four tasks we
System SVT BIND REG TOTAL
UMass 74.7 47.7 42.8 54.8
Stanford 1N 71.4 38.6 32.8 47.8
Stanford 1P 70.8 35.9 31.1 46.5
Stanford 2N 69.1 35.0 27.8 44.3
Stanford 2P 72.0 36.2 32.2 47.4
FAUST+All 76.9 43.5 44.0 55.9
FAUST+1N 76.4 45.1 43.8 55.6
FAUST+1P 75.8 43.1 44.6 55.7
FAUST+2N 74.9 42.8 43.8 54.9
FAUST+2P 75.7 46.0 44.1 55.7
FAUST+All 76.4 41.2 43.1 54.9
(triggers)
FAUST+All 76.1 41.7 43.6 55.1
(arguments)
Table 2: BioNLP f-scores on the development section of
the Genia track (task 1) for several event categories.
observe substantial improvements due to stacking.
The increase is particular striking for the EPI track,
where stacking improves f-score by more than 4.0
points on the CORE metric.
To analyze the impact of stacking further, Ta-
ble 2 shows a breakdown of our results on the Ge-
nia development set. Presented are f-scores for sim-
ple events (SVT), binding events (BIND), regulation
events (REG) and the set of all event types (TOTAL).
We compare the UMass standalone system, various
Stanford-standalone models and stacked versions of
these (FAUST+X).
Remarkably, while there is a 7 point gap between
the best individual Stanford system and the stand-
alone UMass systems, integrating the Stanford pre-
diction still leads to an f-score improvement of 1.
This can be seen when comparing the UMass, Stan-
ford 1N and FAUST+All results, where the latter
stacks 1N, 1P, 2N and 2P. We also note that stack-
ing the projective 1P and 2P systems helps almost
as much as stacking all Stanford systems. Notably,
both 1P and 2P do not do as well in isolation when
compared to the 1N system. When stacked, how-
ever, they do slightly better. This suggests that pro-
jectivity is a missing aspect in the UMass standalone
system.
The FAUST+All (triggers) and FAUST+All (ar-
guments) lines represent experiments to determine
whether it is useful to incorporate only portions of
53
the stacking information from the Stanford system.
Given the small gains over the original UMass sys-
tem, it is clear that stacking information is only use-
ful when attached to triggers and arguments. Our
theory is that most of our gains come from when the
UMass and Stanford systems disagree on triggers
and the Stanford system provides not only its trig-
gers but also their attached arguments to the UMass
system. This is supported by a pilot experiment
where we trained the Stanford model to use the
UMass triggers and saw no benefit from stacking
(even when both triggers and arguments were used).
Table 3 shows our results on the development set
of the ID task, this time in terms of recall, precision
and f-score. Here the gap between Stanford-only
results, and the UMass results, is much smaller. This
seems to lead to more substantial improvements for
stacking: FAUST+All obtains a f-score 2.2 points
larger than the standalone UMass system. Also note
that, similarly to the previous table, the projective
systems do worse on their own, but are more useful
when stacked.
Another possible approach to stacking conjoins
all the original features of the stacking model with
the predicted features of the stacked model. The
hope is that this allows the learner to give differ-
ent weights to the stacked predictions in different
contexts. However, incorporating Stanford predic-
tions by conjoining them with all features of the
UMass standalone system (FAUST+2P-Conj in Ta-
ble 3) does not help here.
We note that for our results on the ID task we
augment the training data with events from the GE
training set. Merging both training sets is reasonable
since there is a significant overlap between both in
terms of events as well as lexical and syntactic pat-
terns to express these. When building our training
set we add each training document from GE once,
and each ID training document twice?this lead to
substantially better results than including ID data
only once.
4 Discussion
Generally stacking has led to substantial improve-
ments across the board. There are, however, some
exceptions. One is binding events for the GE task.
Here the UMass model still outperforms the best
System Rec Prec F1
UMass 46.2 51.1 48.5
Stanford 1N 43.1 49.1 45.9
Stanford 1P 40.8 46.7 43.5
Stanford 2N 41.6 53.9 46.9
Stanford 2P 42.8 48.1 45.3
FAUST+All 47.6 54.3 50.7
FAUST+1N 45.8 51.6 48.5
FAUST+1P 47.6 52.8 50.0
FAUST+2N 45.4 52.4 48.6
FAUST+2P 49.1 52.6 50.7
FAUST+2P-Conj 48.0 53.2 50.4
Table 3: Results on the development set for the ID track.
stacked system (see Table 2). Likewise, for full pa-
pers in the Genia test set, the UMass model still does
slightly better with 53.1 f-score compared to 52.7
f-score. This suggests that a more informed com-
bination of our systems (e.g., metaclassifiers) could
lead to better performance.
5 Conclusion
We have presented the FAUST entry to the BioNLP
2011 shared task on biomolecular event extraction.
It is based on stacking, a simple approach for model
combination. By using the predictions of the Stan-
ford entry as features of the UMass model, we sub-
stantially improved upon both systems in isolation.
This helped us to rank 1st in three of the four tasks
we submitted results to. Remarkably, in some cases
we observed improvements despite a 7.0 f-score
margin between the models we combined.
In the future we would like to investigate alter-
native means for model combination such as rerank-
ing, union, intersection, and other voting techniques.
We also plan to use dual decomposition to encourage
models to agree. In particular, we will seek to incor-
porate an MST component into the dual decomposi-
tion algorithm used by the UMass system.
Acknowledgments
We thank the BioNLP shared task organizers for setting this
up and their quick responses to questions. This work was sup-
ported in part by the Center for Intelligent Information Re-
trieval. We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181.
54
References
James Bennett, Stan Lanning, and Netflix. 2007. The
netflix prize. In KDD Cup and Workshop in conjunc-
tion with KDD.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the Workshop on BioNLP: Shared Task,
pages 1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In ICCV.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Confer-
ence (ACL-HLT?11), Main Conference, Portland, Ore-
gon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in BioNLP 2011. In BioNLP 2011 Shared
Task.
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP. The Association for Computational
Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decomposi-
tion and minimal domain adaptation. In BioNLP 2011
Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proc. EMNLP.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble models for dependency parsing: Cheap and
good? In Proceedings of the North American Chapter
of the Association for Computational Linguistics Con-
ference (NAACL-2010), Los Angeles, CA, June.
55
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 45?49,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Low-Dimensional Embeddings of Logic
Tim Rockt
?
aschel
?
Matko Bosnjak
?
Sameer Singh
?
Sebastian Riedel
?
?
Department of Computer Science, University College London, UK
?
Computer Science & Engineering, University of Washington, Seattle
{t.rocktaschel,m.bosnjak,s.riedel}@cs.ucl.ac.uk, sameer@cs.washington.edu
Abstract
Many machine reading approaches, from
shallow information extraction to deep
semantic parsing, map natural language
to symbolic representations of meaning.
Representations such as first-order logic
capture the richness of natural language
and support complex reasoning, but often
fail in practice due to their reliance on log-
ical background knowledge and the diffi-
culty of scaling up inference. In contrast,
low-dimensional embeddings (i.e. distri-
butional representations) are efficient and
enable generalization, but it is unclear how
reasoning with embeddings could support
the full power of symbolic representations
such as first-order logic. In this proof-of-
concept paper we address this by learning
embeddings that simulate the behavior of
first-order logic.
1 Introduction
Much of the work in machine reading follows an
approach that is, at its heart, symbolic: language
is transformed, possibly in a probabilistic way,
into a symbolic world model such as a relational
database or a knowledge base of first-order for-
mulae. For example, a statistical relation extractor
reads texts and populates relational tables (Mintz
et al., 2009). Likewise, a semantic parser can
turn sentences into complex first-order logic state-
ments (Zettlemoyer and Collins, 2005).
Several properties make symbolic representa-
tions of knowledge attractive as a target of ma-
chine reading. They support a range of well under-
stood symbolic reasoning processes, capture se-
mantic concepts such as determiners, negations
and tense, can be interpreted, edited and curated
by humans to inject prior knowledge. However, on
practical applications fully symbolic approaches
have often shown low recall (e.g. Bos and Markert,
2005) as they are affected by the limited coverage
of ontologies such as WordNet. Moreover, due to
their deterministic nature they often cannot cope
with noise and uncertainty inherent to real world
data, and inference with such representations is
difficult to scale up.
Embedding-based approaches address some of
the concerns above. Here relational worlds are de-
scribed using low-dimensional embeddings of en-
tities and relations based on relational evidence in
knowledge bases (Bordes et al., 2011) or surface-
form relationships mentioned in text (Riedel et al.,
2013). To overcome the generalization bottleneck,
these approaches learn to embed similar entities
and relations as vectors close in distance. Subse-
quently, unseen facts can be inferred by simple and
efficient linear algebra operations (e.g. dot prod-
ucts).
The core argument against embeddings is their
supposed inability to capture deeper semantics,
and more complex patterns of reasoning such
as those enabled by first-order logic (Lewis and
Steedman, 2013). Here we argue that this does
not need to be true. We present an approach that
enables us to learn low-dimensional embeddings
such that the model behaves as if it follows a com-
plex first-order reasoning process?but still oper-
ates in terms of simple vector and matrix repre-
sentations. In this view, machine reading becomes
the process of taking (inherently symbolic) knowl-
edge in language and injecting this knowledge into
a sub-symbolic distributional world model. For
example, one could envision a semantic parser that
turns a sentence into a first-order logic statement,
45
worksFor(A), profAt(A)
profAt(B) ...
profAt(x) => worksFor(x)
worksFor(B)
Curate
IE
SP
worksFor
profAt
A
Logical Inference Algebra
B
C
worksFor(B)
D
Logic Embedded LogicEvidence
Figure 1: Information extraction (IE) and semantic
parsing (SP) extract factual and more general log-
ical statements from text, respectively. Humans
can manually curate this knowledge. Instead of
reasoning with this knowledge directly (A) we in-
ject it into low dimensional representations of en-
tities and relations (B). Linear algebra operations
manipulate embeddings to derive truth vectors (C),
which can be discretized or thresholded to retrieve
truth values (D).
just to then inject this statement into the embed-
dings of relations and entities mentioned in the
sentence.
2 Background
Figure 1 shows our problem setup. We as-
sume a domain of a set of entities, such as
SMITH and CAMBRIDGE, and relations among
these (e.g. profAt(?, ?)). We start from a
knowledge base of observed logical statements,
e.g., profAt(SMITH, CAMBRIDGE) or ?x, y :
profAt(x, y) =? worksFor(x, y). These state-
ments can be extracted from text through informa-
tion extraction (for factual statements), be the out-
put from a semantic parsing (for first-order state-
ments) or come from human curators or external
knowledge bases.
The task at hand is to predict the truth
value of unseen statements, for example
worksFor(SMITH, CAMBRIDGE). Assuming we
have the corresponding formulae, logical infer-
ence can be used to arrive at this statement (arrow
A in Figure 1). However, in practice the relevant
background knowledge is usually missing. By
contrast, a range of work (e.g. Bordes et al., 2011;
Riedel et al., 2013) has successfully predicted
unseen factual statements by learning entity and
relation embeddings that recover the observed
facts and generalize to unseen facts through
dimensionality reduction (B). Inference in these
approaches amounts to a series of algebraic
operations on the learned embeddings that returns
a numeric representation of the degree of truth
(C), which can be thresholded to arrive back at a
true or false statement (D) if needed.
Our goal in this view is to generalize (B) to al-
low richer logical statements to be recovered by
low-dimensional embeddings. To this end we first
describe how richer logical statements can be em-
bedded at full dimension where the number of di-
mensions equals to the number of entities in the
domain.
2.1 Tensor Calculus
Grefenstette (2013) presents an isomorphism be-
tween statements in predicate logic and expres-
sions in tensor calculus. Let [?] denote this map-
ping from a logical expression F to an expression
in tensor algebra. Here, logical statements evaluat-
ing to true or false are mapped to [true]
:
=
> =
[
1 0
]
T
and [false]
:
= ? =
[
0 1
]
T
re-
spectively.
Entities are represented by logical constants and
mapped to one-hot vectors where each component
represents a unique entity. For example, let k = 3
be the number of entities in a domain, then SMITH
may be mapped to [SMITH] =
[
1 0 0
]
T
. Unary
predicates are represented as 2?k matrices, whose
columns are composed of > and ? vectors. For
example, for a isProfessor predicate we may get
[isProfessor ] =
[
1 0 1
0 1 0
]
.
In this paper we treat binary relations as unary
predicates over constants ?X, Y? that correspond to
pairs of entities X and Y in the domain.
1
The application of a unary predicate to a con-
stant is realized through matrix-vector multiplica-
tion. For example, for profAt and the entity pair
?X, Y? we get
[profAt(?X, Y?)] = [profAt ] [?X, Y?] .
In Grefenstette?s calculus, binary boolean oper-
ators are mapped to mode 3 tensors. For example,
for the implication operator holds:
[ =? ]
:
=
[
1 0 1 1
0 1 0 0
]
.
Let A and B be two logical statements that,
when evaluated in tensor algebra, yield a vector
1
This simplifies our exposition and approach, and it can
be shown that both representations are logically equivalent.
46
in {>,?}. The application of a binary operator
to statements A and B is realized via two con-
secutive tensor-vector products in their respective
modes (see Kolda and Bader (2009) for details),
e.g.,
[A =? B]
:
= [ =? ]?
1
[A]?
2
[B] .
3 Method
Grefenstette?s mapping to tensors exactly recov-
ers the behavior of predicate logic. However, it
also inherits the lack of generalization that comes
with a purely symbolic representation. To over-
come this problem we propose an alternate map-
ping. We retain the representation of truth val-
ues and boolean operators as the 2 ? 1 and the
2 ? 2 ? 2 sized tensors respectively. However,
instead of mapping entities and predicates to one-
hot representations, we estimate low-dimensional
embeddings that recover the behavior of their one-
hot counterparts when plugged into a set of tensor-
logic statements.
In the following we first present a general learn-
ing objective that encourages low-dimensional
embeddings to behave like one-hot representa-
tions. Then we show how this objective can be
optimized for facts and implications.
3.1 Objective
Let R be the set of all relation embeddings and
P be the set of all entity pair embeddings. Given
a knowledge base (KB) of logical formulae K
which we assume to hold, the objective is
min
[p]?P, [R]?R
?
F?K
?[F ]?>?
2
. (1)
That is, we prefer embeddings for which the given
formulae evaluate to the vector representation for
truth. The same can be done for negative data by
working with ?, but we omit details for brevity.
To optimize this function we require the gradi-
ents of ?[F ]?>?
2
terms. Below we discuss these
for two types of formulae: ground atoms and first-
order formulae.
3.2 Ground Atoms
The KB may contain ground atoms (i.e. facts) of
the form F = R(p) for a pair of entities p and a
relation R. These atoms correspond to observed
cells in an entity-pair-relation matrix, and inject-
ing these facts into the embedding roughly corre-
sponds to matrix factorization for link prediction
or relation extraction (Riedel et al., 2013).
Let ??
F
:
= ([F ]?>) / ?[F ]?>?
2
, then it is
easy to show that the gradients with respect to re-
lation embedding [R] and entity pair embedding
[p] are
?/? [p] = [R] ??
F
and ?/? [R] = ??
F
? [p] .
3.3 First-order Formulae
Crucially, and in contrast to matrix factorization,
we can inject more expressive logical formulae
than just ground atoms. For example, the KB
K may contain a universally quantified first-order
rule such as ?x : R
1
(x) =? R
2
(x). Assum-
ing a finite domain, this statement can be unrolled
into a conjunction of propositional statements of
the form F = R
1
(p) =? R
2
(p), one for each
pair p. We can directly inject these propositional
statements into the embeddings, and their gradi-
ents are straightfoward to derive. For example,
?/? [R
1
] = (([ =? ]?
2
[R
2
(p)]) ??
F
)? [p] .
3.4 Learning and Inference
We learn embeddings for entity pairs and relations
by minimizing objective 1 using stochastic gradi-
ent descent (SGD). To infer the (two-dimensional)
truth value (C in Figure 1) of a formula F in em-
bedded logic we evaluate [F ]. An easier to intpret
one-dimensional representation can be derived by
(
?[F ] ,
[
1 ?1
]
T
?+ 1
)
/2,
followed by truncation to the interval [0, 1]. Other
ways of projecting [F ] to R, such as using cosine
similarity to >, are possible as well.
4 Experiments
We perform experiments on synthetic data defined
over 7 entity pairs and 6 relations. We fix the em-
bedding size k to 4 and train the model for 100
epochs using SGD with `
2
-regularization on the
values of the embeddings. The learning rate and
the regularization parameter are set to 0.05.
The left part of Table 1 shows the observed
(bold) and inferred truth values for a set of fac-
tual staments of the form R(p), mapped to R as
discussed above. Due to the generalization ob-
tained by low-dimensional embeddings, the model
infers that, for example, SMITH is an employee
at CAMBRIDGE and DAVIES lives in LONDON.
However, we would like the model to also capture
that every professor works for his or her university
47
With Factual Constraints With Factual and First-Order Constraints
profAt worksFor employeeAt registeredIn livesIn bornIn profAt worksFor employeeAt registeredIn livesIn bornIn
?JONES, UWASH? 1.00 1.00 1.00 0.00 0.18 0.01 0.98 0.98 0.95 0.03 0.00 0.04
?TAYLOR, UCL? 1.00 1.00 0.98 0.00 0.20 0.00 0.98 0.96 0.95 0.05 0.00 0.06
?SMITH, CAMBRIDGE? 0.98
>
0.00
>
0.64 0.75 0.07 0.72 0.92
>
0.97
>
0.89 0.04 0.04 0.05
?WILLIAMS, OXFORD?
?
0.02 1.00 0.08 0.00 0.93 0.02
?
0.05 0.91 0.02 0.05 0.87 0.06
?BROWN, CAMBRIDGE?
?
0.00 0.97 0.02
?
0.01 0.95 0.06
?
0.01 0.90 0.00
?
0.07 0.92 0.07
?DAVIES, LONDON? 0.00 0.00 0.00 0.99
>
0.50 1.00 0.01 0.00 0.00 0.98
>
0.98 0.97
?EVANS, PARIS? 0.00 0.00 0.00 1.00
>
0.48 1.00 0.00 0.00 0.00 0.97
>
1.00 0.96
Table 1: Reconstructed matrix without (left) and with (right) the first-order constraints profAt =?
worksFor and registeredIn =? livesIn . Predictions for training cells of factual constraints [R(p)] =
> are shown in bold, and true and false test cells are denoted by
>
and
?
respectively.
and that, when somebody is registered in a city, he
or she also lives in that city.
When including such first-order constraints
(right part of Table 1), the model?s predictions
improve concerning different aspects. First, the
model gets the implication right, demonstrating
that the low-dimensional embeddings encode first-
order knowledge. Second, this implication transi-
tively improves the predictions on other columns
(e.g. SMITH is an employee at CAMBRIDGE).
Third, the implication works indeed in an asym-
metric way, e.g., the model does not predict that
WILLIAMS is a professor at OXFORD just because
she is working there.
5 Related Work
The idea of bringing together distributional se-
mantics and formal logic is not new. Lewis and
Steedman (2013) improve the generalization per-
formance of a semantic parser via the use of dis-
tributional representations. However, their target
representation language is still symbolic, and it is
unclear how this approach can cope with noise and
uncertainty in data.
Another line of work (Clark and Pulman, 2007;
Mitchell and Lapata, 2008; Coecke et al., 2010;
Socher et al., 2012; Hermann and Blunsom, 2013)
uses symbolic representations to guide the com-
position of distributional representations. Read-
ing a sentence or logical formula there amounts
to compositionally mapping it to a k-dimensional
vector that then can be used for downstream tasks.
We propose a very different approach: Reading a
sentence amounts to updating the involved entity
pair and relation embeddings such that the sen-
tence evaluates to true. Afterwards we cannot use
the embeddings to calculate sentence similarities,
but to answer relational questions about the world.
Similar to our work, Bowman (2014) provides
further evidence that distributed representations
can indeed capture logical reasoning. Although
Bowman demonstrates this on natural logic ex-
pressions without capturing factual statements,
one can think of ways to include the latter in
his framework as well. However, the ap-
proach presented here can conceptually inject
complex nested logical statements into embed-
dings, whereas it is not obvious how this can be
achieved in the neural-network based multi-class
classification framework proposed by Bowman.
6 Conclusion
We have argued that low dimensional embeddings
of entities and relations may be tuned to simu-
late the behavior of logic and hence combine the
advantages of distributional representations with
those of their symbolic counterparts. As a first
step into this direction we have presented an ob-
jective that encourages embeddings to be consis-
tent with a given logical knowledge base that in-
cludes facts and first-order rules. On a small syn-
thetic dataset we optimize this objective with SGD
to learn low-dimensional embeddings that indeed
follow the behavior of the knowledge base.
Clearly we have only scratched the surface
here. Besides only using toy data and logical for-
mulae of very limited expressiveness, there are
fundamental questions we have yet to address.
For example, even if the embeddings could en-
able perfect logical reasoning, how do we pro-
vide provenance or proofs of answers? More-
over, in practice a machine reader (e.g. a semantic
parser) incrementally gathers logical statements
from text? how could we incrementally inject this
knowledge into embeddings without retraining the
whole model? Finally, what are the theoretical
limits of embedding logic in vector spaces?
48
Acknowledgments
We would like to thank Giorgos Spithourakis,
Thore Graepel, Karl Moritz Hermann and Ed-
ward Grefenstette for helpful discussions, and An-
dreas Vlachos for comments on the manuscript.
This work was supported by Microsoft Research
through its PhD Scholarship Programme. This
work was supported in part by the TerraSwarm Re-
search Center, one of six centers supported by the
STARnet phase of the Focus Center Research Pro-
gram (FCRP) a Semiconductor Research Corpora-
tion program sponsored by MARCO and DARPA.
References
Antoine Bordes, Jason Weston, Ronan Collobert,
and Yoshua Bengio. 2011. Learning structured
embeddings of knowledge bases. In AAAI.
Johan Bos and Katja Markert. 2005. Recognis-
ing textual entailment with logical inference. In
Proc. of HLT/EMNLP, pages 628?635.
Samuel R Bowman. 2014. Can recursive neural
tensor networks learn logical reasoning? In
ICLR?14.
Stephen Clark and Stephen Pulman. 2007. Com-
bining symbolic and distributional models of
meaning. In AAAI Spring Symposium: Quan-
tum Interaction, pages 52?55.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a
compositional distributional model of meaning.
CoRR, abs/1003.4394.
Edward Grefenstette. 2013. Towards a formal dis-
tributional semantics: Simulating logical calculi
with tensors. In Proc. of *SEM, pages 1?10.
Karl Moritz Hermann and Phil Blunsom. 2013.
The role of syntax in vector space models of
compositional semantics. In Proc. of ACL,
pages 894?904.
Tamara G Kolda and Brett W Bader. 2009. Tensor
decompositions and applications. SIAM review,
51(3):455?500.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. In TACL,
volume 1, pages 179?192.
Mike Mintz, Steven Bills, Rion Snow, and Daniel
Jurafsky. 2009. Distant supervision for rela-
tion extraction without labeled data. In Proc.
of ACL-IJCNLP, pages 1003?1011.
Jeff Mitchell and Mirella Lapata. 2008. Vector-
based models of semantic composition. In Proc.
of ACL, pages 236?244.
Sebastian Riedel, Limin Yao, Andrew McCallum,
and Benjamin M Marlin. 2013. Relation ex-
traction with matrix factorization and universal
schemas. In Proc. of NAACL-HLT, pages 74?
84.
Richard Socher, Brody Huval, Christopher D
Manning, and Andrew Y Ng. 2012. Seman-
tic compositionality through recursive matrix-
vector spaces. In Proc. of EMNLP, pages 1201?
1211.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form:
Structured classification with probabilistic cat-
egorial grammars. In Proc. of UAI, pages 658?
666.
49
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18?22,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Fact Checking: Task definition and dataset construction
Andreas Vlachos
Dept. of Computer Science
University College London
London, United Kingdom
a.vlachos@cs.ucl.ac.uk
Sebastian Riedel
Dept. of Computer Science
University College London
London, United Kingdom
s.riedel@ucl.ac.uk
Abstract
In this paper we introduce the task of fact
checking, i.e. the assessment of the truth-
fulness of a claim. The task is commonly
performed manually by journalists verify-
ing the claims made by public figures. Fur-
thermore, ordinary citizens need to assess
the truthfulness of the increasing volume
of statements they consume. Thus, de-
veloping fact checking systems is likely
to be of use to various members of soci-
ety. We first define the task and detail the
construction of a publicly available dataset
using statements fact-checked by journal-
ists available online. Then, we discuss
baseline approaches for the task and the
challenges that need to be addressed. Fi-
nally, we discuss how fact checking relates
to mainstream natural language processing
tasks and can stimulate further research.
1 Motivation
Fact checking is the task of assessing the truth-
fulness of claims made by public figures such
as politicians, pundits, etc. It is commonly per-
formed by journalists employed by news organisa-
tions in the process of news article creation. More
recently, institutes and websites dedicated to this
cause have emerged such as Full Fact
1
and Politi-
Fact
2
respectively. Figure 1 shows two examples
of fact checked statements, together with the ver-
dicts offered by the journalists.
Fact-checking is a time-consuming process. In
assessing the first claim in Figure 1 a journalist
would need to consult a variety of sources to find
1
http://fullfact.org
2
http://politifact.com
the average ?full-time earnings? for criminal bar-
risters. Fact checking websites commonly provide
the detailed analysis (not shown in the figure) per-
formed to support the verdict.
Automating the process of fact checking has re-
cently been discussed in the context of computa-
tional journalism (Cohen et al., 2011; Flew et al.,
2012). Inspired by the recent progress in natural
language processing, databases and information
retrieval, the vision is to provide journalists with
tools that would allow them to perform this task
automatically, or even render the articles ?live? by
updating them with most current data. This au-
tomation is further enabled by the increasing on-
line availability of datasets, survey results, and re-
ports in machine readable formats by various insti-
tutions, e.g. EUROSTAT releases detailed statis-
tics for all European economies.
3
Furthermore, ordinary citizens need to fact
check the information provided to them. This need
is intensified with the proliferation of social media
such as Twitter, since the dissemination of news
and information commonly circumvents the tra-
ditional news channels (Petrovic, 2013). In addi-
tion, the rise of citizen journalism (Goode, 2009)
suggests that often citizens become the sources
of information. Since the information provided
by them is not edited or curated, automated fact
checking would assist in avoiding the spreading
false information.
In this paper we define the task of fact-checking.
We then detail the construction of a dataset using
fact-checked statements available online. Finally,
we describe the challenges it poses and its relation
to current research in natural language processing.
3
http://epp.eurostat.ec.europa.eu/
portal/page/portal/eurostat/home
18
2 Task definition
We define fact-checking to be the assignment of a
truth value to a claim made in a particular con-
text. Thus it is natural to consider it as a bi-
nary classification task. However, it is often the
case that the statements are not completely true
or false. For example, the verdict for the third
claim in Figure 1 is MOSTLYTRUE because some
of the sources dispute it, while in the fourth exam-
ple the statistics can be manipulated to support or
disprove the claim as desired. Therefore it is bet-
ter to consider fact-checking as an ordinal classifi-
cation task (Frank and Hall, 2001), thus allowing
systems to capture the nuances of the task.
The verdict by itself, even if graded, needs to be
supported by an analysis (e.g., what is the systems
interpretation of the statement). However, given
the difficulty of carving out exactly what the cor-
rect analysis for a statement might be, we restrict
the task to be a prediction problem so that we can
evaluate performance automatically.
Context can be crucial in fact-checking. For ex-
ample, knowing that the fourth claim of Figure 1
is made by a UK politician is necessary in order
to assess it using data about this country. Fur-
thermore, time is also important since the vari-
ous comparisons usually refer to time-frames an-
chored at the time a claim is made.
The task is rather challenging. While some
claims such as the one about Crimea can be fact-
checked by extracting relations from WikiPedia,
the verdict often hinges on interpreting relatively
fine points, e.g. the last claim refers to a partic-
ular definition of income. Journalists also check
multiple sources in producing their verdicts, as in
the case of the third claim. Interestingly, they also
consider multiple interpretations of the data; e.g.
in the last claim is assessed as HALFTRUE since
different but reasonable interpretations of the same
data lead to different conclusions.
We consider all of the aspects mentioned (time,
speaker, multiple sources and interpretations) as
part of the task of fact checking. However, we
want to restrict the task to statements that can be
fact-checked objectively, which is not always true
for the statements assessed by journalists. There-
fore, we do not consider statements such as ?New
Labour promised social improvement but deliv-
ered a collapse in social mobility? to be part to
the task since there are no universal definitions of
?social improvement? and ?social mobility?.
4
4
http://blogs.channel4.com/factcheck/
factcheck-social-mobility-collapsed/
Claim (by Minister Shailesh Vara)
?The average criminal bar barrister working full-
time is earning some ?84,000.?
Verdict: FALSE (by Channel 4 Fact Check)
The figures the Ministry of Justice have stressed
this week seem decidedly dodgy. Even if you do
want to use the figures, once you take away the
many overheads self-employed advocates have to
pay you are left with a middling sum of money.
Claim (by U.S. Rep. Mike Rogers)
?Crimea was part of Russia until 1954, when it
was given to the Soviet Republic of the Ukraine.?
Verdict: TRUE (by Politifact)
Rogers said Crimea belonged to Russia until
1954, when Khrushchev gave the land to
Ukraine, then a Soviet republic.
Claim (by President Barack Obama)
?For the first time in over a decade, business
leaders around the world have declared that
China is no longer the world?s No. 1 place to
invest; America is.?
Verdict: MOSTLYTRUE (by Politifact)
The president is accurate by citing one particular
study, and that study did ask business leaders
what they thought about investing in the United
States. A broader look at other rankings doesn?t
make the United States seem like such a power-
house, even if it does still best China in some lists.
Claim (by Chancellor George Osborne)
?Real household disposable income is rising.?
Verdict: HALFTRUE (by Channel 4 Fact Check)
RHDI did grow in latest period we know about
(the second quarter of 2013), making Mr Osborne
arguably right to say that it is rising as we speak.
But over the last two quarters we know about,
income was down 0.1 per cent. If you want to
compare the latest four quarters of data with the
previous four, there was a fall in household
income, making the chancellor wrong. But if
you compare the latest full year of results, 2012,
with 2011, income is up and he?s right again.
Figure 1: Fact-checked statements.
19
3 Dataset construction
In order to construct a dataset to develop and eval-
uate approaches to fact checking, we first surveyed
popular fact checking websites. We decided to
consider statements from two of them, the fact
checking blog of Channel 4
5
and the Truth-O-
Meter from PolitiFact.
6
Both websites have large
archives of fact-checked statements (more than
1,000 statements each), they cover a wide range of
prevalent issues of U.K. and U.S. public life, and
they provide detailed verdicts with fine-grained la-
bels such as MOSTLYFALSE and HALFTRUE.
We examined recent fact-checks from each
website at the time of writing. For each state-
ment, apart from the statement itself, we recorded
the date it was made, the speaker, the label of
the verdict and the URL. As the two websites
use different labelling schemes, we aligned the la-
bels of the verdicts to a five-point scale: TRUE,
MOSTLYTRUE, HALFTRUE, MOSTLYFALSE and
FALSE. The speakers included, apart from pub-
lic figures, associations such as the American Bev-
erage Association, activists, even viral FaceBook
posts submitted by the public.
We then decided which of the statements should
be considered for the task proposed. As discussed
in the previous section we want to avoid state-
ments that cannot be assessed objectively. Follow-
ing this, we deemed unsuitable statements:
? assessing causal relations, e.g. whether a
statistic should be attributed to a particular law
? concerning the future, e.g. speculations involv-
ing oil prices
? not concerning facts, e.g. whether a politician
is supporting certain policies
For the statements that were considered suit-
able, we also collected the sources used by the
journalists in the analysis provided for the verdict.
Common sources include tables with statistics and
reports from governments, think tanks and other
organisations, available online. Automatic identi-
fication of the sources needed to fact check a state-
ment is an important stage in the process, which is
potentially useful in its own right in the context
of assisting journalists in a semi-automated fact-
checking approach Cohen et al. (2011). Some-
16444
5
http://blogs.channel4.com/factcheck/
6
http://www.politifact.com/
truth-o-meter/statements/
times the verdicts relied on data that were not
available online such personal communications;
statements whose verdict relied on such data were
also deemed unsuitable for the task.
As mentioned earlier, the verdicts on the web-
sites are accompanied by lengthy analyses. While
such analyses could be useful annotation for in-
termediate stages of the task ? e.g. we could use
it as supervision to learn how to combine the in-
formation extracted from the various sources into
a verdict ? we noticed that the language used in
them is indicative of the verdict.
7
Thus we decided
not to include them in the dataset, as it would en-
able tackling part of the task as sentiment analy-
sis. Out of the 221 fact-checked statements exam-
ined, we judged 106 as suitable. The dataset col-
lected including our suitability judgements is pub-
licly available
8
and we are working on extending
it so that it can support the development and the
automatic evaluation of fact checking approaches.
4 Baseline approaches
As discussed in Section 2, we consider fact check-
ing as an ordinal classification task. Thus, in the-
ory it would be possible to tackle it as a supervised
classification task using algorithms that learn from
statements annotated with the verdict labels. How-
ever this is unlikely to be successful, since state-
ments such as the ones verified by journalists do
not contain the world knowledge and the temporal
and spatial context needed for this purpose.
A different approach would be to match state-
ments to ones already fact-checked by journalists
and return the label in a K-nearest neighbour fash-
ion.
9
Thus the task is reduced to assessing the se-
mantic similarity between statements, which was
explored in a recent shared task (Agirre et al.,
2013). An obvious shortcoming of this approach
is that it cannot be applied to new claims that have
not been fact-checked, thus it can only be used to
detect repetitions and paraphrases of false claims.
A possible mechanism to extend the coverage of
such an approach to novel statements is to assume
that some large text collection is the source of all
true statements. For example, Wikipedia is likely
7
E.g. part of the analysis of the first claim in Figure 1
reads: ?the full-time figure has the handy effect of stripping
out the very lowest earners and bumping up the average?.
8
https://sites.google.com/site/
andreasvlachos/resources
9
The Truth-Teller by Washington Post (http://
truthteller.washingtonpost.com/) follows this
approach.
20
to contain a statement that would match the sec-
ond claim in Figure 1. However, it would still be
unable to tackle the other claims mentioned, since
they require calculations based on the data.
5 Discussion
The main drawback of the baseline approaches
mentioned (aside from their potential coverage) is
the lack of interpretability of their verdicts, also re-
ferred to as algorithmic accountability (Diakopou-
los, 2014). While it is possible for a natural lan-
guage processing expert to inspect aspects of the
prediction such as feature weights, this tends to
become harder as the approaches become more so-
phisticated. Ultimately, the user of a fact checking
system would trust a verdict only if it is accom-
panied by an analysis similar to the one provided
by the journalists. This desideratum is present in
other tasks such as the recently proposed science
test question answering (Clark et al., 2013).
Cohen et al. (2011) propose that fact checking
is about asking the right questions. These ques-
tions might be database queries, requests for in-
formation to be extracted from textual resources,
etc. For example, in checking the last claim in Fig-
ure 1 a critical reader would like to know what are
the possible interpretations of ?real household dis-
posable income? and what the calculations might
be for other reasonable time spans.
The manual fact checking process suggests an
approach that is more likely to give an inter-
pretable analysis and would decompose the task
into the following stages:
1. extract statements to be fact-checked
2. construct appropriate questions
3. obtain the answers from relevant sources
4. reach a verdict using these answers
The stages of this architecture can be mapped
to tasks well-explored in the natural language pro-
cessing community. Statement extraction could
be tackled as a sentence classification problem,
following approaches similar to those proposed
for speculation detection (Farkas et al., 2010) and
veridicality assessment (de Marneffe et al., 2012).
Furthermore, obtaining answers to questions from
databases is a task typically addressed in the con-
text of semantic parsing research, while obtaining
such answers from textual sources is usually con-
sidered in the context of information extraction.
Finally, the compilation of the answers into a ver-
dict could be considered as a form of logic-based
textual entailment (Bos and Markert, 2005).
However, the fact-checking stages described in-
clude a novel task, namely question construc-
tion for a given statement. This task is likely
to rely on semantic parsing of the statement fol-
lowed by restructuring of the logical form gener-
ated. Since question construction is a rather un-
common task, it is likely to require human supervi-
sion, which could possibly be obtained via crowd-
sourcing. Furthermore, the open-domain nature of
fact checking places greater demands on the estab-
lished tasks of information extraction and seman-
tic parsing. Thus, fact-checking is likely to stim-
ulate research in these tasks on methods that do
not require domain-specific supervision (Riedel et
al., 2013) and are able to adapt to new information
requests (Kwiatkowski et al., 2013).
Fact-checking is related to the tasks of textual
entailment (Dagan et al., 2006) and machine com-
prehension (Richardson et al., 2013), with the dif-
ference that the text which should be used to pre-
dict the entailment of the hypothesis or the correct
answer respectively is not provided in the input.
Instead, systems need to locate the sources needed
to predict the verdict label as part of the task. Fur-
thermore, by defining the task in the context of
real-world journalism we are able to obtain labeled
statements at no annotation cost, apart from the as-
sessment of their suitability for the task.
6 Conclusions
In this paper we introduced the task of fact check-
ing and detailed the construction of a dataset us-
ing statements fact-checked by journalists avail-
able online. In addition, we discussed baseline ap-
proaches that could be applied to perform the task
and the challenges that need to be addressed.
Apart from being a challenging testbed to stim-
ulate progress in natural language processing, re-
search in fact checking is likely to inhibit the in-
tentional or unintentional dissemination of false
information. Even an approach that would return
the sources related to a statement could be very
helpful to journalists as well as other critical read-
ers in a semi-automated fact checking approach.
Acknowledgments
The authors would like to thank the members of
the Machine Reading lab for useful discussions
21
and their help in compiling the dataset.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics and the Shared Task: Semantic
Textual Similarity, pages 32?43, Atlanta, GA.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of the 2005 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2005),
pages 628?635.
Peter Clark, Philip Harrison, and Niranjan Balasubra-
manian. 2013. A study of the knowledge base re-
quirements for passing an elementary science test.
In Proceedings of the 2013 Workshop on Automated
Knowledge Base Construction, pages 37?42.
Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu.
2011. Computational journalism: A call to arms to
database researchers. In Proceedings of the Confer-
ence on Innovative Data Systems Research, volume
2011, pages 148?151.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Proceedings of the First International
Conference on Machine Learning Challenges: Eval-
uating Predictive Uncertainty Visual Object Classi-
fication, and Recognizing Textual Entailment, pages
177?190.
Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it happen?
the pragmatic complexity of veridicality assessment.
Computational Linguistics, 38(2):301?333, June.
Nick Diakopoulos. 2014. Algorithmic accountabil-
ity reporting: On the investigation of black boxes.
Technical report, Tow Center for Digital Journalism.
Richard Farkas, Veronika Vincze, Gyorgy Mora, Janos
Csirik, and Gyorgy Szarvas. 2010. The CoNLL
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the CoNLL 2010 Shared Task.
Terry Flew, Anna Daniel, and Christina L. Spurgeon.
2012. The promise of computational journalism.
In Proceedings of the Australian and New Zealand
Communication Association Conference, pages 1?
19.
Eibe Frank and Mark Hall. 2001. A simple approach
to ordinal classification. In Proceedings of the 12th
European Conference on Machine Learning, pages
145?156.
Luke Goode. 2009. Social news, citizen journalism
and democracy. New Media & Society, 11(8):1287?
1305.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
WA.
Sasa Petrovic. 2013. Real-time event detection in mas-
sive streams. Ph.D. thesis, School of Informatics,
University of Edinburgh.
Matthew Richardson, Christopher J.C. Burges, and
Erin Renshaw. 2013. MCTest: A challenge dataset
for the open-domain machine comprehension of
text. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 193?203, Seattle, WA.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin,
and Andrew McCallum. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, GA.
22
