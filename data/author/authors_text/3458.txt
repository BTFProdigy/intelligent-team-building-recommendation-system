Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 435?442, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Cluster-specific Named Entity Transliteration 
 
 
Fei Huang 
 
School of Computer Science 
Carnegie Mellon University, Pittsburgh, PA 15213 
fhuang@cs.cmu.edu 
 
Abstract 
Existing named entity (NE) transliteration 
approaches often exploit a general model to 
transliterate NEs, regardless of their origins. 
As a result, both a Chinese name and a 
French name (assuming it is already trans-
lated into Chinese) will be translated into 
English using the same model, which often 
leads to unsatisfactory performance. In this 
paper we propose a cluster-specific NE 
transliteration framework. We group name 
origins into a smaller number of clusters, 
then train transliteration and language mod-
els for each cluster under a statistical ma-
chine translation framework. Given a source 
NE, we first select appropriate models by 
classifying it into the most likely cluster, 
then we transliterate this NE with the corre-
sponding models. We also propose a phrase-
based name transliteration model, which ef-
fectively combines context information for 
transliteration. Our experiments showed 
substantial improvement on the translitera-
tion accuracy over a state-of-the-art baseline 
system, significantly reducing the 
transliteration character error rate from 
50.29% to 12.84%. 
1 Introduction 
Named Entity (NE) translation and transliteration 
are very important to many multilingual natural 
language processing tasks, such as machine trans-
lation, crosslingual information retrieval and ques-
tion answering. Although some frequently 
occurring NEs can be reliably translated using in-
formation from existing bilingual dictionaries and 
parallel or monolingual corpora (Al-Onaizan and 
Knight, 2002; Huang and Vogel, 2002; Lee and 
Chang, 2003), less frequently occurring NEs, espe-
cially new names, still rely on machine translitera-
tion to generate their translations. 
NE machine transliteration generates a phoneti-
cally similar equivalent in the target language for a 
source NE, and transliteration patterns highly de-
pend on the name?s origin, e.g., the country or the 
language family this name is from. For example, 
when transliterating names 1  from Chinese into 
English, as shown in the following example, the 
same Chinese character ??? is transliterated into 
different English letters according to the origin of 
each person. 
??? --- Jin Renqing (China) 
??? --- Kim Dae-jung (Korea) 
??  ?? ? --- Martin Luther King (USA) 
??? --- Kanemaru Shin (Japan) 
?? ?? ??? --- Jose Joaquin Brunner (Chile) 
Several approaches have been proposed for 
name transliteration. (Knight and Graehl, 1997) 
proposed a generative transliteration model to 
transliterate foreign names in Japanese back to 
English using finite state transducers. (Stalls and 
Knight, 1998) expanded that model to Arabic-
English transliteration. (Meng et al 2001) devel-
oped an English-Chinese NE transliteration tech-
nique using pronunciation lexicon and phonetic 
mapping rules. (Virga and Khudanpur, 2003) ap-
plied statistical machine translation models to 
?translate? English names into Chinese characters 
for Mandarin spoken document retrieval. All these 
approaches exploit a general model for NE trans-
literation, where source names from different ori-
gins or language families are transliterated into the 
target language with the same rules or probability 
distributions, which fails to capture their different 
                                                 
1 Assuming foreign names are already transliterated into Chi-
nese. 
435
transliteration patterns. Alternatively, (Qu and Gre-
fenstette, 2004) applied language identification of 
name origins to select language-specific translit-
erations when back-transliterating Japanese names 
from English to Japanese. However, they only 
classified names into three origins: Chinese, Japa-
nese and English, and they used the Unihan data-
base to obtain the mapping between kenji 
characters and romanji representations.  
Ideally, to explicitly model these transliteration 
differences we should construct a transliteration 
model and a language model for each origin. How-
ever, some origins lack enough name translation 
pairs for reliable model training. In this paper we 
propose a cluster-specific NE transliteration 
framework. Considering that several origins from 
the same language family may share similar trans-
literation patterns, we group these origins into one 
cluster, and build cluster-specific transliteration 
and language models.  
Starting from a list of bilingual NE translation 
pairs with labeled origins, we group closely related 
origins into clusters according to their language 
and transliteration model perplexities. We train 
cluster-specific language and transliteration models 
with merged name translation pairs. Given a source 
name, we first select appropriate models by classi-
fying it into the most likely cluster, then we trans-
literate the source name with the corresponding 
models under the statistical machine translation 
framework. This cluster-specific transliteration 
framework greatly improves the transliteration per-
formance over a general transliteration model. Fur-
ther more, we propose a phrase-based 
transliteration model, which effectively combines 
context information for name transliteration and 
achieves significant improvements over the tradi-
tional character-based transliteration model.  
The rest of the paper is organized as following: 
in section 2 we introduce the NE clustering and 
classification schemes, and we discuss the phrase-
based NE transliteration in section 3. Experiment 
settings and results are given in section 4, which is 
followed by our conclusion. 
2 Name Clustering and Classification 
Provided with a list of bilingual name translation 
pairs whose origins are already labeled, we want to 
find the origin clusters where closely related ori-
gins (countries sharing similar languages or cul-
tural heritages) are grouped together.  
We define the similarity measure between two 
clusters as their LM and TM perplexities. Let 
)},{( iii EFS = denote a set of name translation 
pairs from origin i , from which model i? is trained: 
),,( )(it)()( ieici PPP=? . Here and are N-gram 
character language models (LM) for source and 
target languages, and is a character translation 
model trained based on IBM translation model 1 
(Brown et.al. 1993). The distance between origin i  
and origin 
)(icP )(ieP
)(itP
j  can be symmetrically defined as: 
)|(log
||
1
)|(log
||
1
),( ij
j
ji
i
SP
S
SP
S
jid ?? ??= , 
where, assuming name pairs are generated inde-
pendently, 
)]|()(
)|()(log[)|(
)()(
||
1
)()(
t
i
t
ijt
t
ije
S
t
t
i
t
ijt
t
ijcji
EFPEP
FEPFPSP
i?
=
+??  
We calculate the pair-wise distances among 
these origins, and cluster them with group-average 
agglomerative clustering. The distance between 
clusters and is defined as the average dis-
tance between all origin pairs in each cluster. This 
clustering algorithm initially sets each origin as a 
single cluster, then recursively merges the closest 
cluster pair into one cluster until an optimal num-
ber of clusters is formed.  
iC jC
Among all possible cluster configurations, we 
select the optimal cluster number based on the 
model perplexity. Given a held-out data set L, a list 
of name translation pairs from different origins, the 
probability of generating L from a cluster configu-
ration ?? is the product of generating each name 
pair from its most likely origin cluster: 
?
?
=
??
=
??
=
=?
||
1
)()(
||
1
)()()(max
)()|,(max)|(
L
t
j
t
je
t
jcj
L
t
jj
tt
j
PEPFP
PEFPLP
?
??
?
??
 
We calculate the language model perplexity: 
||/1
)|(log
||
1
)|(2),( L
LP
L LPLpp ?
?? ?==? ??
?
, 
and select the model configuration with the small-
est perplexity. We clustered 56K Chinese-English 
name translation pairs from 112 origins, and evalu-
ate the perplexities of different models (number of  
436
 Figure 1. Perplexity value of LMs with different 
number of clusters  
Afghanistan, Algeria, Egypt, Iran, Iraq, 
Jordan, Kuwait, Pakistan, Palestine, 
clusters) with regard to a held-out 3K name pairs. 
As shown in Figure 1, the perplexity curve reaches 
its minimum when . This indicates that the 
optimal cluster number is 45. 
45=n
Table 1 lists some typical origin clusters. One 
may notice that countries speaking languages from 
the same family are often grouped together. These 
countries are either geographically adjacent or his-
torically affiliated. For example, in the English 
cluster, the Netherlands (Dutch) seems an abnor-
mality. In the clustering process it was first 
grouped with the South Africa, which was colo-
nized by the Dutch and the English in the seven-
teenth century. This cluster was further grouped 
into the English-speaking cluster. Finally, some 
origins cannot be merged with any other clusters 
because they have very unique names and transla-
tion patterns, such as China and Japan, thus they 
are kept as single origin clusters.  
For name transliteration task, given a source 
name F we want to classify it into the most likely 
cluster, so that the appropriate cluster-specific 
model can be selected for transliteration. Not 
knowing F?s translation E, we cannot apply the 
translation model and the target language model 
for name origin classification. Instead we train a 
Bayesian classifier based on N-gram source char-
acter language models, and assign the name to the 
cluster with the highest LM probability. Assuming 
a source name is composed of a sequence of source 
characters: . We want to find the 
cluster such that  
},...,,{ 21 lfffF =
*j
                        (1) 
)()(maxarg
)|()(maxarg
)|(maxarg
)(
*
FPP
FPP
FPj
jcjj
jjj
jj
?
??
?
=
=
=
where )( jP ? is the prior probability of cluster j, 
estimated based on its distribution in all the train-
ing data, and is the probability of generat-
ing this source name based on cluster
)()( FP jc
j ?s character 
language model. 
3 Phrase-Based Name Transliteration  
Statistical NE transliteration is similar to the statis-
tical machine translation in that an NE translation 
pair can be considered as a parallel sentence pair, 
where ?words? are characters in source and target 
languages. Due to the nature of name translitera-
tion, decoding is mostly monotone.  
 
Arabic Saudi Arabia, Sudan, Syria, Tunisia, 
Yemen, ? 
Spanish- 
Portuguese 
Angola, Argentina, Bolivia, Brazil, 
Chile, Colombia, Cuba, Ecuador, Mex-
ico, Peru, Portugal, Spain, Venezuela, 
? 
English Australia, Canada, Netherlands, New Zealand, South Africa, UK, USA, ? 
Russian Belarus, Kazakhstan, Russia, Ukraine 
East Euro-
pean 
Bosnia and Herzegovina, Croatia, 
Yugoslavia 
French  
(African) 
Benin, Burkina Faso, Cameroon, Cen-
tral African Republic, Congo, Gabon, 
Ivory Coast 
German Austria, Germany, Switzerland 
French Belgium, France, Haiti 
Korean North Korea, South Korea 
Danish- 
Swedish Denmark, Norway, Sweden 
Single Clus-
ters 
China 
Japan 
Indonesia 
Israel 
?? 
Table 1 Typical name clusters (n=45) 
437
NE transliteration process can be formalized as: 
)()|(maxarg)|(maxarg* EPEFPFEPE EE ==  
where *E is the most likely transliteration for the 
source NE F, P(F|E) is the transliteration model 
and P(E) is the character-based target language 
model. We train a transliteration model and a lan-
guage model for each cluster, using the name 
translation pairs from that cluster. 
3.1 Transliteration Model 
A transliteration model provides a conditional 
probability distribution of target candidates for a 
given source transliteration unit: a single character 
or a character sequence, i.e., ?phrase?. Given 
enough name translation pairs as training data, we 
can select appropriate source transliteration units, 
identify their target candidates from a character 
alignment path within each name pair, and estimate 
their transliteration probabilities based on their co-
occurrence frequency.  
A naive choice of source transliteration unit is a 
single character. However, single characters lack 
contextual information, and their combinations 
may generate too many unlikely candidates. Moti-
vated by the success of phrase-based machine 
translation approaches (Wu 1997, Och 1999, 
Marcu and Wong 2002 and Vogel et. al., 2003), we 
select transliteration units which are long enough 
to capture contextual information while flexible 
enough to compose new names with other units. 
We discover such source transliteration phrases 
based on a character collocation likelihood ratio 
test (Manning and Schutze 1999). This test accepts 
or rejects a null hypothesis that the occurrence of 
one character is independent of the other, , by 
calculating the likelihood ratio between the inde-
pendent ( ) and dependent ( ) hypotheses: 
1f 2f
0H 1H
),,(log),,(log
),,(log),,(log
)(
)(
loglog
211221112
1122112
1
0
pcNccLpccL
pcNccLpccL
HL
HL
????
??+=
=?
 
L is the likelihood of getting the observed character 
counts under each hypothesis. Assuming the char-
acter occurrence frequency follows a binomial dis-
tribution,        knk xx
k
n
xnkL ?????
?
???
?= )1(),,( , 
1221 ,, ccc  are the frequencies of , and , 
and 
1f 2f 21 ^ ff
N is the total number of characters. and 
are defined as: 
1, pp
2p
N
c
p 2= ,     
2
12
1 c
c
p = ,     
1
122
2 cN
cc
p ?
?= . 
We calculate the likelihood ratio for any adja-
cent source character pairs, and select those pairs 
whose ratios are higher than a predefined threshold.  
Adjacent character bigrams with one character 
overlap can be recursively concatenated to form 
longer source transliteration phrases. All these 
phrases and single characters are combined to con-
struct a cluster-specific phrase segmentation vo-
cabulary list, T. For each name pair in that cluster, 
we  
1. Segment the Chinese character sequence 
into a source transliteration phrase se-
quence based on maximum string match-
ing using T; 
2. Convert Chinese characters into their ro-
manization form, pinyin, then align the 
pinyin with English letters via phonetic 
string matching, as described in (Huang et. 
al., 2003); 
3. Identify the initial phrase alignment path 
based on the character alignment path; 
4. Apply a beam search around the initial 
phrase alignment path, searching for the 
optimal alignment which minimizes the 
overall phrase alignment cost, defined as: 
?
?
=
Aa
aiA
i
i
efDA ),(minarg* . 
Here is the i th source phrase in F, is its tar-
get candidate under alignment A. Their alignment 
cost D is defined as the linear interpolation of the 
phonetic transliteration cost log  and semantic 
translation cost log : 
if iae
trlP
transP
)|(log)1()|(log),( fePfePefD transtrl ?? ?+= , 
where is the trlP product of the letter transliteration 
probabilities over aligned pinyin-English letter 
pairs, transP is the phrase translation probability  
calculated from word translation probabilities, 
where a ?word? refers to a Chinese character or a 
English letter. More details about these costs are 
described in (Huang et. al., 2003). ?  is a cluster-
438
specific interpolation weight, reflecting the relative 
contributions of the transliteration cost and the 
translation cost. For example, most Latin language 
names are often phonetically translated into Chi-
nese, thus the transliteration cost is usually the 
dominant feature. However, Japanese names are 
often semantically translated when they contain 
characters borrowed from Chinese, therefore the 
translation cost is more important for the Japanese 
model ( ? =0 in this case). We empirically select 
the interpolation weight for each cluster, based on 
their transliteration performance on held-out name 
pairs, and the combined model with optimal inter-
polation weights achieves the best overall perform-
ance. 
We estimate the phrase transliteration probabil-
ity according to their normalized alignment fre-
quencies. We also include frequent sub-name 
translations (first, middle and last names) in the 
transliteration dictionary. Table 2 shows some 
typical transliteration units (characters or phrases) 
from three clusters. They are mostly names or sub-
names capturing cluster-specific transliteration 
patterns. It also illustrates that in different clusters 
the same character has different transliteration 
candidates with different probabilities, which justi-
fies the cluster-specific transliteration modeling. 
 
        ????   mohamed 
???? abdul 
???? ahmed Arabic 
?: yo (0.27)  y(0.19)  you(0.14)? 
?? john 
?? william 
?? peter English 
?: u(0.25)  you(0.38)  joo(0.16)? 
????? vladimir 
???? ivanov 
-??? -yevich Russian 
?? yu(0.49)  y(0.08)  iu(0.07)? 
Table 2. Transliteration units examples from three 
name clusters. 
3.2 Language model and decoding 
For each cluster we train a target character lan-
guage model from target NEs. We use the N-gram 
models with standard smoothing techniques. 
During monotone decoding, a source NE is 
segmented into a sequence of transliteration units, 
and each source unit is associated with a set of tar-
get candidate translations with corresponding prob-
abilities. A transliteration lattice is constructed to 
generate all transliteration hypotheses, among 
which the one with the minimum transliteration 
and language model costs is selected as the final 
hypothesis.   
4 Experiment Results 
We selected 62K Chinese-English person name 
translation pairs for experiments. These origin-
labeled NE translation pairs are from the name 
entity translation lists provided by the LDC 2  
(including the who?swho (china) and who?swho 
(international) lists), and devided into three parts: 
system training (90%), development (5%) and 
testing (5%). In the development and test data, 
names from each cluster followed the same 
distribution as in the training data. 
4.1 NE Classification Evaluation 
We evaluated the source name classification ac-
curacy, because classification errors will lead to 
incorrect model selection, and result in bad 
transliteration performance in the next step. We 
trained 45 cluster-specific N-gram source character 
language models, and classified each source name 
into the most likely cluster according to formula 1. 
We evaluated the classification accuracy on a held-
out test set with 3K NE pairs. We also experi-
mented with different N values. Table 3 shows the 
classification accuracy, where the 3-gram model 
achieves the highest classification accuracy. A de-
tailed analysis indicates that some classification 
errors are due to the inherent uncertainty of some 
names, e. g, ???? (Gary Locke)?, a Chinese 
American, was classified as a Chinese name based 
on its source characters while his origin was la-
beled as USA. 
 
Table 3. Source name origin classification accura-
cies
                                                 
2 http://www.ldc.upenn.edu 
N=2 N=3 N=4 N=5 N=6 N=7 
83.62 84.88 84.00 84.04 83.94 83.94
439
4.2 NE Transliteration Evaluation 
We first evaluated transliteration results for each 
cluster, then evaluated the overall results on the 
whole test set, where a name was transliterated 
using the cluster-specific model in which it was 
classified. The evaluation metrics are:  
? Top1 accuracy (Top1), the percentage 
that the top1 hypothesis is correct, i.e., 
the same as the reference translation; 
? Top 5 accuracy (Top5), the percentage 
that the reference translation appears in 
the generated top 5 hypotheses; 
? Character error rate (CER), the percent-
age of incorrect characters (inserted, de-
leted and substituted English letters) 
when the top 1 hypothesis is aligned to 
the reference translation. 
Our baseline system was a character-based 
general  transliteration model, where 56K NE pairs 
from all clusters were merged to train a general 
transliteration model and a language model 
(CharGen). We compare it with a character-based 
cluster-specific model (CharCls) and a phrase-
based cluster-specific model (PhraCls). The CERs 
of several typical clusters are shown in Table 4. 
Because more than half of the training name 
pairs are from Latin language clusters, the general 
transliteration and language models adopted the 
Latin name transliteration patterns. As a result, it 
obtained reasonable performance (20-30% CERs) 
on  Latin language names, such as Spanish, 
English and French names, but strikingly high 
(over 70%) CERs on oriental language names such 
as Chinese and Japanese names, even though the 
Chinese cluster has the most training data.  
When applying the character-based cluster-
specific models, transliteration CERs consistently 
decreased for all clusters (ranging from 6.13% 
relative reduction for the English cluster to 97% 
for the Chinese cluster). As expected, the oriental 
language names obtained the most significant error 
reduction because the cluster-specific models were 
able to represent their unique transliteration 
patterns. When we applied the phrased-based 
transliteration models, CERs were further reduced 
by 23% ~ 51% for most clusters, because the 
context information were encapsulated in the 
transliteration phrases. An exception was the 
Chinese cluster, where names were often translated 
according to the pinyin of single characters, thus 
phrase-based transliteration slightly decreased the 
performance.  
The transliteration performance of different 
clusters varied a lot. The Chinese cluster achieved 
96.09% top 1 accuracy and 1.69% CER with the 
character-based model, and other clusters had 
CERs ranging from 7% to 30%. This was partly 
because of the lack of training data (e.g, for the 
Japanese cluster), and partly because of unique 
transliteration patterns of different languages. We  
try to measure this difference using the average 
number of translations per source phrase 
(AvgTrans), as shown in Table 4. This feature 
reflected the transliteration pattern regularity, and 
seemed linearly correlated with the CERs. For 
example, compared with the English cluster, 
Russian names have more regular translation 
patterns, and its CER is only 1/3 of the English 
cluster, even with only half size of training data.  
In Table 5 we compared translation examples 
from the baseline system (CharGen), the phrase-
based cluster-specific system (PhraCls) and a 
online machine translation system, the BabelFish3. 
The CharGen system transliterated every name in 
the Latin romanization way, regardless of each 
name?s original language. The BabelFish system 
inappropriately translated source characters based 
on their semantic meanings, and the results were 
difficult to understand. The PhraCls model 
captured cluster-specific contextual information, 
and achieved the best results. 
We evaluated three models? performances on all 
the test data, and showed the result in Table 6. The 
CharGen model performed rather poorly 
transliterating oriental names, and the overall CER 
was around 50%. This result was comparable to 
other state-of-the-art statistical name transliteration 
systems (Virga and Khudanpur, 2003). The 
CharCls model significantly improved the top1 
and top 5 transliteration accuracies from 3.78% to 
51.08%, and from 5.84% to 56.50%, respectively.  
Consistently, the CER was also reduced from 
50.29% to 14.00%. Phrase-based transliteration 
further increased the top 1 accuracy by 9.3%, top 5 
accuracy by 10.7%, and reduced the CER by 8%, 
relatively. All these improvements were 
statistically significant. 
                                                 
3 http://babelfish.altavista.com/ 
440
  
 
 
 
Table 4. Cluster-specific transliteration comparison 
 
 
 
 
Table 5. Transliteration examples from some typical clusters 
 
 
 
 
Cluster Training data size 
CharGen 
(CER) 
CharCls 
(CER) 
PhraCls 
(CER) AvgTrans 
Arabic 8336 22.88 18.93 14.47 4.58 
Chinese 27093 76.45 1.69 1.71 3.43 
English 8778 31.12 29.21 17.27 5.02 
French 2328 27.66 18.81 9.07 3.51 
Japanese 2161 86.94 38.65 29.60 7.57 
Russian 4407 29.17 9.62 6.55 3.64 
Spanish 8267 18.87 15.99 10.33 3.61 
Cluster Source  Reference CharGen PhraCls BabelFish 
Arabic 
?? ???
???? 
Nagui Sabri 
Ahmed 
Naji Saburi 
Ahamed 
Naji Sabri  
Ahmed 
In natrium ?
? cloth    
Aihamaide 
Chinese ??? Fan Zhilun Van Tylen Fan zhilun Fan Zhilun 
English 
???       
????? 
Robert    
Steadward 
Robert   
Stdwad 
Robert       
Sterdeward 
Robert Stead 
Warder 
French 
?-??      
??? 
Jean-luc    
Cretier 
Jean-luk  
Crete 
Jean-luc    
Cretier 
Let - Lu Keke 
lei Jie 
Japanese ???? Kobayashi Ryoji Felinonge 
Kobayashi 
Takaji 
Xiaolin pros-
perous gov-
erns 
Russian 
????? 
????? 
Vladimir  
Samsonov 
Frakimir  
Samsonof 
Vladimir  
Samsonov 
??? mil 
sum rope 
Knoff 
Spanish 
???        
??? 
Rodolfo     
Cardoso 
Rudouf      
Cardoso 
Rodolfo 
Cadozo 
Rudolph card 
multi- ropes 
441
Model Top1 (%) Top5 (%) CER (%) 
CharGen 3.78?0.69 5.84?0.88 50.29?1.21 
CharCls 51.08?0.84 56.50?0.87 14.00?0.34 
PhraCls 56.00?0.84 62.66?0.91 12.84?0.41 
Table 6 Transliteration result comparison 
5 Conclusion 
We have proposed a cluster-specific NE translit-
eration framework. This framework effectively 
modeled the transliteration differences of source 
names from different origins, and has demon-
strated substantial improvement over the baseline 
general model. Additionally, phrase-based translit-
eration further improved the transliteration per-
formance by a significant margin. 
 
References  
Y. Al-Onaizan and K. Knight. 2002. Translating 
named entities using monolingual and bilingual 
resources. In Proceedings of the ACL-2002, 
pp400-408, Philadelphia, PA, July, 2002. 
F. Huang and S. Vogel. 2002. Improved Named 
Entity Translation and Bilingual Named Entity 
Extraction, Proceedings of the ICMI-2002. 
Pittsburgh, PA, October 2002 
F. Huang, S. Vogel and A. Waibel. 2003. Auto-
matic Extraction of Named Entity Translingual 
Equivalence Based on Multi-feature Cost Mini-
mization.  Proceedings of the ACL-2003, Work-
shop on Multilingual and Mixed Language 
Named Entity Recognition. Sapporo, Japan. 
K. Knight and J. Graehl. 1997. Machine Translit-
eration. Proceedings of the ACL-1997. pp.128-
135, Somerset, New Jersey. 
C. J. Lee and J. S. Chang. 2003. Acquisition of 
English-Chinese Transliterated Word Pairs from 
Parallel-Aligned Texts using a Statistical Ma-
chine Transliteration Model. HLT-NAACL 2003 
Workshop: Building and Using Parallel Texts: 
Data Driven Machine Translation and Beyond. 
pp96-103, Edmonton, Alberta, Canada. 
C. D. Manning and H. Sch?tze. 1999. Foundations 
of Statistical Natural Language Processing. MIT 
Press. Boston MA. 
H. Meng, W. K. Lo, B. Chen and K. Tang. 2001. 
Generating Phonetic Cognates to Handle Named 
Entities in English-Chinese Cross-Language 
Spoken Document Retrieval. Proceedings of the 
ASRU-2001, Trento, Italy, December.2001 
D. Marcu and W. Wong. A Phrase-Based, Joint 
Probability Model for Statistical Machine Trans-
lation. Proceedings of EMNLP-2002, Philadel-
phia, PA, 2002 
F. J. Och, C. Tillmann, and H. Ney. Improved 
Alignment Models for Statistical Machine 
Translation. pp. 20-28; Proc. of the Joint Conf. 
of Empirical Methods in Natural Language 
Processing and Very Large Corpora; University 
of Maryland, College Park, MD, June 1999. 
Y. Qu, and G. Grefenstette. Finding Ideographic 
Representations of Japanese Names Written in 
Latin Script via Language Identification and 
Corpus Validation. ACL 2004: 183-190 
P. Virga and S. Khudanpur. 2003. Transliteration 
of Proper Names in Cross-Lingual Information 
Retrieval. Proceedings of the ACL-2003 Work-
shop on Multi-lingual Named Entity Recognition 
Japan. July 2003. 
S. Vogel, Y. Zhang, F. Huang, A. Tribble, A. 
Venogupal, B. Zhao and A. Waibel. The CMU 
Statistical Translation System, Proceedings of 
MT Summit IX New Orleans, LA, USA, Sep-
tember 2003 
D. Wu. Stochastic Inversion Transduction Gram-
mars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics 23(3):377-404, Sep-
tember 1997. 
442
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 483?490, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Mining Key Phrase Translations from Web Corpora 
 
Fei Huang       Ying Zhang       Stephan Vogel 
 
School of Computer Science 
Carnegie Mellon University, Pittsburgh, PA 15213 
{fhuang, joy, vogel}@cs.cmu.edu 
 
 
Abstract 
Key phrases are usually among the most 
information-bearing linguistic structures. 
Translating them correctly will improve 
many natural language processing appli-
cations. We propose a new framework to 
mine key phrase translations from web 
corpora. We submit a source phrase to a 
search engine as a query, then expand 
queries by adding the translations of 
topic-relevant hint words from the re-
turned snippets. We retrieve mixed-
language web pages based on the ex-
panded queries.  Finally, we extract the 
key phrase translation from the second-
round returned web page snippets with 
phonetic, semantic and frequency-
distance features. We achieve 46% phrase 
translation accuracy when using top 10 re-
turned snippets, and 80% accuracy with 
165 snippets. Both results are signifi-
cantly better than several existing meth-
ods. 
1 Introduction 
Key phrases such as named entities (person, loca-
tion and organization names), book and movie ti-
tles, science, medical or military terms and others 
1, are usually among the most information-bearing 
linguistic structures. Translating them correctly 
will improve the performance of cross-lingual in-
formation retrieval, question answering and ma-
chine translation systems. However, these key 
phrases are often domain-specific, and people con-
                                                                                                                    
1 Some name and terminology is a single word, which could 
be regarded as a one-word phrase. 
stantly create new key phrases which are not cov-
ered by existing bilingual dictionaries or parallel 
corpora, therefore standard data-driven or knowl-
edge-based machine translation systems cannot 
translate them correctly. 
 As an increasing amount of web information be-
comes available, exploiting such a huge informa-
tion resource is becoming more attractive. (Resnik 
1999) searched the web for parallel corpora while 
(Lu et al 2002) extracted translation pairs from 
anchor texts pointing to the same webpage. How-
ever, parallel webpages or anchor texts are quite 
limited, and these approaches greatly suffer from 
the lack of data.  
However, there are many web pages containing 
useful bilingual information where key phrases and 
their translations both occur. See the example in 
Figure 1. This example demonstrates web page 
snippets2 containing both a Chinese key phrase ??
??? and its translation, ?Faust?. 
We thus can transform the translation problem 
into a data mining problem by retrieving these 
mixed-language web pages and extracting their 
translations. We propose a new framework to mine 
key phrase translations from web corpora. Given a 
source key phrase (here a Chinese phrase), we first 
retrieve web page snippets containing this phrase 
using the Google search engine. We then expand 
queries by adding the translations of topic-relevant 
hint words from the returned snippets. We submit 
the source key phrase and expanded queries again 
to Google to retrieve mixed-language web page 
snippets.  Finally, we extract the key phrase trans-
lation from the second-round returned snippets 
with phonetic, semantic and frequency-distance 
features.  
2A snippet is a sentence or paragraph containing the key 
phrase, returned with the web page URLs. 
483
  
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Returned mixed-language web page snip-
pets using source query 
 
We achieve 46% phrase translation accuracy 
when using 10 returned snippets, and 80% accu-
racy with 165 snippets. Both results are signifi-
cantly better than several existing methods. 
   The reminder of this paper is organized as fol-
lows: cross-lingual query expansion is discussed in 
section 2; key phrase translation extraction is ad-
dressed in section 3. In section 4 we present ex-
perimental results, which is followed by relevant 
works and conclusions. 
2 Retrieving Web Page Snippets through 
Cross-lingual Query Expansion 
For a Chinese key phrase f, we want to find its 
translation e from the web, more specifically, from 
the mixed-language web pages or web page snip-
pets containing both f and e. As we do not know e, 
we are unable to directly retrieve such mixed-
language web page using (f,e) as the query.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Returned mixed-language web page snip-
pets using cross-lingual query expansion 
However, we observed that when the author of a 
web page lists both f and e in a page, it is very 
likely that f' and e' are listed in the same page, 
where f? is a Chinese hint word topically relevant 
to f, and e? is f?s translation. Therefore if we know 
a Chinese hint word f?, and we know its reliable 
translation, e?, we can send (f, e?) as a query to re-
trieve mixed language web pages containing (f, e).    
For example, to find web pages which contain 
translations of ?????(Faust), we expand the 
query to ????+goethe? since ???? (Goethe) 
is the author of ?????(Faust). Figure 2 illus-
trates retrieved web page snippets with expanded 
queries. We find that newly returned snippets con-
tain more correct translations with higher ranks. 
   To propose a ?good? English hint e' for f, first we 
need to find a Chinese hint word f' that is relevant 
to f. Because f is often an OOV word, it is unlikely 
that such information can be obtained from exist-
ing Chinese monolingual corpora. Instead, we 
484
query Google for web pages containing f. From the 
returned snippets we select Chinese words f' based 
on the following criteria: 
 
1. f' should be relevant to f based on the co-
occurrence frequency. On average, 300 
Chinese words are returned for each query 
f. We only consider those words that occur 
at least twice to be relevant. 
2. f' can be reliably translated given the cur-
rent bilingual resources (e.g. the LDC 
Chinese-English lexicon 3  with 81,945 
translation entries). 
3. The meaning of f' should not be too am-
biguous. Words with many translations 
are not used. 
4. f' should be translated into noun or noun 
phrases. Given the fact that most OOV 
words are noun or noun phrases, we ig-
nore those source words which are trans-
lated into other part-of-speech words. The 
British National Corpus4 is used to gener-
ate the English noun lists. 
 
For each f, the top Chinese words f' with the 
highest frequency are selected. Their correspond-
ing translations are then used as the cross-lingual 
hint words for f. For example, for OOV word f = 
??? (Faust), the top candidate f's are ???
(Goethe)?, ? ?? (introduction)?, ???
(literature)? and ???(tragedy)?. We expand 
the original query ????? to ???? + 
goethe?, ???? + introduction?, ???? + lit-
erature?, ???? + tragic?, and then query Google 
again for web page snippets containing the correct 
translation ?Faust?. 
3 Extracting Key Phrase Translation 
When the Chinese key phrase and its English hint 
words are sent to Google as the query, returned 
web page snippets contain the source query and 
possibly its translation. We preprocess the snippets 
to remove irrelevant information. The preprocess-
ing steps are: 
1. Filter out HTML tags; 
                                                          
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogI
d=LDC2002L27 
4 http://www.natcorp.ox.ac.uk/ 
2. Convert HTML special characters (e.g., 
?&lt?) to corresponding ASCII code (?>?); 
3. Segment Chinese words based on a maxi-
mum string matching algorithm, which is 
used to calculate the translation probability 
between a Chinese key phrase and an Eng-
lish translation candidate. 
4. Replace punctuation marks with phrase sepa-
rator ?|?; 
5. Replace non-query Chinese words with 
placeholder mark ?+?, as they indicate the 
distance between an English phrase and the 
Chinese key phrase. 
For example, the snippet  
? <b>???? </b>? (the bridges of 
madison county)[review]. ????anjing | 
?????2004-01-25 ??? 02:13 | ?
????? 
is converted into 
| <b> ?  ?  ?  ? </b> | 
the_bridges_of_Madison_county | review | 
++ + | anjing | ++ ++  | 2004-01-25 +++ 02 
13 | + + ++ ++, 
where ?<b>? and ?</b>? mark the start and end 
positions of the Chinese key phrase. The candidate 
English phrases, ?the bridges of madison county?, 
?review? and ?anjing?, will be aligned to the 
source key phrase according to a combined feature 
set using a transliteration model which captures the 
pronunciation similarity, a translation model which 
captures the semantic similarity and a frequency-
distance model reflecting their relevancy. These 
models are described below. 
3.1 Transliteration Model 
The transliteration model captures the phonetic 
similarity between a Chinese phrase and an Eng-
lish translation candidate via string alignment. 
Many key phrases are person and location names, 
which are phonetically translated and whose writ-
ten forms resemble their pronunciations. Therefore 
it is possible to discover these translation pairs 
through their surface strings. Surface string trans-
literation does not need a pronunciation lexicon to 
map words into phoneme sequences; thus it is es-
pecially appealing for OOV word translation. For 
non-Latin languages like Chinese, a romanization 
485
script called ?pinyin? maps each Chinese character 
into Latin letter strings. This normalization makes 
the string alignment possible. 
     We adopt the transliteration model proposed in 
(Huang, et al 2003). This model calculates the 
probabilistic Levinstein distance between a roman-
ized source string and a target string. Unlike the 
traditional Levinstein distance calculation, the 
character alignment cost is not binary (0/1); rather 
it is the logarithm of character alignment probabil-
ity, which ensures that characters with similar pro-
nunciations (e.g. `p` and `b`) have higher 
alignment probabilities and lower cost. These 
probabilities are automatically learned from bilin-
gual name lists using EM. 
Assume the Chinese phrase f has J Chinese 
characters, , and the English candidate 
phrase e has L English words, . The 
transliteration cost between a Chinese query and 
an English translation candidate  is calculated as: 
Jfff ,..., 21
Leee ,...,, 21
f
e
 
 
where is the pinyin of Chinese character ,  
is the i th letter in , and and are their 
aligned English letters, respectively.  
is the letter transliteration probability. The translit-
eration costs between a Chinese phrase and an 
English phrase is approximated by the sum of their 
letter transliteration cost along the optimal align-
ment path, which is identified based on dynamic 
programming.   
jy jf
ijy , jy jae ),( ijae
)|( ,),( jiji yep
3.2 Translation Model 
The translation model measures the semantic 
equivalence between a Chinese phrase and an Eng-
lish candidate. One widely used model is the IBM 
model (Brown et al 1993). The phrase translation 
probability is computed using the IBM model-1 as: 
  
 
 
where is the lexical translation probabili-
ties, which can be calculated according to the IBM 
models. This alignment model is asymmetric, as 
one source word can only be aligned to one target 
word, while one target word can be aligned to mul-
tiple source words. We estimate both  
and , and define the NE translation 
cost as: 
)|( lj efp
)|( efPtrans
)|( fePtrans
).|(log)|(log),( efPfePfeC transtranstrans +=
3.3 Frequency-Distance Model 
The more often a bilingual phrase pair co-occurs, 
or the closer a bilingual phrase pair is within a 
snippet, the more likely they are translations of 
each other. The frequency-distance model meas-
ures this correlation.  
   Suppose S is the set of returned snippets for 
query , and a single returned snippet isf Ssi ? . 
The source phrase occurs in si as  ( since f 
may occur several times in a snippet). The fre-
quency-distance weight of an English candidate 
is  
jif , 1?j
e
??=
i jis f ji efd
ew
,
),(
1
)(
,
 
 
.)|(log)|(log),( ,),(??? =?
j i
jia
j
jatrl yepyepfe where is the distance between phrase   
and e, i.e., how many words are there between the 
two phrases (the separator `|` is not counted).  
),( efd jif ,
3.4 Feature Combination 
Define the confidence measure for the translitera-
tion model as: 
 
 
where e and e? are English candidate phrases, and 
m is the weight of the distance model. We empiri-
cally choose m=2 in our experiments. This 
measure indicates how good the English phrase e is 
compared with other candidates based on translit-
eration model. Similarly the translation model con-
fidence measure is defined as: 
 
 
 
 
   The overall feature cost is the linear combination 
of transliteration cost and translation cost, which 
are weighted by their confidence scores respec-
tively: 
 
 
C
jij
,
)'()],'(exp[
)()],(exp[
)|(
'
?=
e
m
trl
m
trl
trl ewfeC
ewfeC
fe?
.
)'()],'(exp[
)()],(exp[
)|(
'
?=
e
m
trans
m
trans
trans ewfeC
ewfeC
fe???
= =
=
J
j
L
l
ljJtrans efpL
efP
1 1
)|(
1
)|(
486
 ???? the Bridges of Madison-
County                                                                                   
where the linear combination weight ?  is chosen 
empirically. While trl? and trans?  represent the rela-
tive rank of the current candidate among all com-
pared candidates, C and  indicate its 
absolute likelihood, which is useful to reject the 
top 1 incorrect candidate if the true translation does 
not occur in any returned snippets.  
trl transC
                                                          
4 Experiments 
We evaluated our approach by translating a set of 
key phrases from different domains. We selected 
310 Chinese key phrases from 12 domains as the 
test set, which were almost equally distributed 
within these domains. We also manually translated 
them as the reference translations. Table 1 shows 
some typical phrases and their translations, where 
one may find that correct key phrase translations 
need both phonetic transliterations and semantic 
translations. We evaluated inclusion rate, defined 
as the percentage of correct key phrase translations 
which can be retrieved in the returned snippets; 
alignment accuracy, defined as the percentage of 
key phrase translations which can be correctly 
aligned given that these translations are included in 
the snippets; and overall translation accuracy, de-
fined as the percentage of key phrases which can 
be translated correctly. We compared our approach 
with the LiveTrans5 (Cheng et.al. 2004) system, an 
unknown word translator using web corpora, and 
we observed better translation performance using 
our approach. 
4.1 Query Translation Inclusion Rate 
In the first round query search, for each Chinese 
key phrase f, on average 13 unique snippets were 
returned to identify relevant Chinese hint words f?, 
and the top 5 f's were selected to generate hint 
words e?s. In the second round f and e?s were sent 
to Google again to retrieve mixed language snip-
pets, which were used to extract e, the correct 
translation of f. 
Figure 3 shows the inclusion rate vs. the number 
of snippets used for three mixed-language web 
page searching strategies: 
                                                          
5 http://livetrans.iis.sinica.edu.tw/lt.html 
 Table 1. Test set key phrases 
? Search any web pages containing f (Zhang 
and Vines 2004); 
? Only search English web pages6 contain-
ing f (Cheng et al 2004); 
? Search any web pages containing f and 
hint words e?, as proposed in this paper.  
 
   The first search strategy resulted in a relatively 
low inclusion rate; the second achieved a much 
higher inclusion rate. However, because such Eng-
lish pages were limited, and on average only 45 
unique snippets could be found for each f, which 
resulted in a maximum inclusion rate of 85.8%. In 
the case of the cross-lingual query expansion, the 
search space was much larger but more focused 
and we achieved a high inclusion rate of 89.7% 
using 32 mixed-language snippets and 95.2% using 
165 snippets, both from the second round retrieval.  
6 These web pages are labeled by Google as ?English? web 
pages, though they may contain non-English characters. 
Movie Title 
????            Forrest Gump 
Book Title 
???   Dream of the Red Mansion 
???    La Dame aux camellias 
Organization 
Name 
????   University of Notre Dame  
??????????? David and 
Lucile Packard Foundation  
Person 
Name 
???          Ludwig Van Beethoven 
????? Audrey Hepburn 
Location 
Name 
????? Kamchatka 
??????? Taklamakan desert 
Company /
Brand 
???? Lufthansa German 
Airlines 
???? Estee Lauder 
Sci&Tech 
Terms 
???? genetic algorithm 
???? speech recognition  
Specie Term 
??               Aegypius monachus 
???              Manispentadactyla 
Military 
Term 
???              Aegis  
???              Phalcon 
Medical 
Term 
?????? SARS 
???? Arteriosclerosis 
Music Term 
????     Bird-call in the Mountain 
???        Bassoon 
Sports Term 
?????? Houston Rockets 
?????? Tour de France 
)]()|()( ff?? exp[1
)],(exp[)|(),(
eCe
feCfefeC
transtrans
trltrl= ?? +
,?
487
Table 2. Alignment accuracies using different features 
 
These search strategies are further discussed in the 
section 5. 
4.2 Translation Alignment Accuracy 
We evaluated our key phrase extraction model by 
testing queries whose correct translations were in-
cluded in the returned snippets. We used different 
feature combinations on differently sized snippets 
to compare their alignment accuracies. Table 2 
shows the result. Here ?Trl? means using the trans-
literation model, ?Trans? means using the transla-
tion model, and ?Fq-dis? means using Frequency-
Distance model. The frequency-distance model 
seemed to be the strongest single model in both 
cases (with and without hint words), while incor-
porating phonetic and semantic features provided 
additional strength to the overall performance. 
Combining all three features together yielded the 
best accuracy. Note that when more candidate 
translations were available through query expan-
sion, the alignment accuracy improved by 30% 
relative due to the frequency-distance model. 
However, using transliteration and/or translation 
models alone decreased performance because of 
more incorrect translation candidates from returned 
snippets. After incorporating the frequency-
distance model, correct translations have the 
maximum frequency-distance weights and are 
more likely to be selected as the top hypothesis. 
Therefore the combined model obtained the high-
est translation accuracy. 
4.3 Overall Translation Quality  
The overall translation qualities are listed in Table 
3, where we showed the translation accuracies of  
 
No Hints 
(Inc = 44.19%) 
With Hints 
(Inc = 95.16%) 
Table 3. Overall translation accuracy 
the top 5 hypotheses using different number of 
snippets. A hypothesized translation was consid-
ered to be correct when it matched one of the ref-
erence translations.  Using more snippets always 
increased the overall translation accuracy, and with 
all the 165 snippets (on average per query), our 
approach achieved 80% top-1 translation accuracy, 
and 90% top-5 accuracy. 
We compared the translations from a research 
statistical machine translation system (CMU-SMT, 
Vogel et al 2003) and a web-based MT engine 
(BabelFish). Due to the lack of topic-relevant con-
texts and many OOV words occurring in the source 
key phrases, their results were not satisfactory. We 
also compare our system with LiveTrans, which 
only searched within English web pages, thus with 
limited search space and more noises (incorrect 
English candidates). Therefore it was more diffi-
cult to select the correct translation. Table 4 lists 
some example key phrase translations mined from 
web corpora, as well as translations from the Ba-
belFish.  
5 Relevant Work 
Both (Cheng et al 2004) and (Zhang and Vines 
2004) exploited web corpora for translating OOV 
terms and queries. Compared with their work, our 
proposed method differs in both webpage search 
                                                          
7 http://babelfish.altavista.com/ 
Features (avg. snippets = 
10) 
(avg. snip-
pets=130) 
Trl 51.45 17.97 
Trans 51.45 40.68 
Fq-dis 53.62 73.22 
Trl+Trans 63.04 51.36 
Trl+Trans+ 
Fq-dis 65.94 86.73 
Accuracy of the Top-N Hyp. (%) Snippets 
Used Top1 Top2 Top3 Top4 Top5 
10 46.1 55.2 59.0 61.3 62.3 
20 57.4 64.2 69.7 72.6 72.9 
50 63.2 74.5 77.7 79.7 80.6 
100 75.2 84.5 85.8 87.4 87.4 
165 80.0 86.5 89.0 90.0 90.0 
Babel-
Fish7 MT 31.3 N/A N/A N/A N/A 
CMU-
SMT 21.9 N/A N/A N/A N/A 
LiveTrans
(Fast) 20.6 30.0 36.8 41.9 45.2 
LiveTrans
(Smart) 30.0 41.9 48.7 51.0 52.9 
488
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
Figure 3. Inclusion rate vs. the number of snippets used 
 
Examples Category 
Chinese Key Phrase Web-Mining Translation BabelFish? Result 
Movie  
Title ???? 
the Bridges of Madison 
County 
*Love has gone and only good 
memory has left in the dream 
Book  
Title ????? Sense and Sensibility *Reason and emotion 
Organization 
Name 
Woodrow Wilson National 
Fellowship Foundation 
*Wood the Wilson nation gets to-
gether the foundation 
??????????
?? 
Person  ???? Seiji Ozawa *Young Ze drafts you Name 
Location 
Name ????? Tsaidam Basin Qaidam Basin 
Company / ?? Clinique *Attractive blue Brand 
Sci&Tech 
Terms ????? Bayesian network *Shell Ye Si network 
Specie  ?? walrus walrus Term 
Military 
Term ????? stratofortress stratofortress 
Medical 
Term ??? glaucoma glaucoma 
Music  ??? bassoon bassoon Term 
Sports  ?????? Km Tour de France *Link law bicycle match Term 
*: Incorrect translations 
 
Table 4. Key phrase translation from web mining and a MT engine 
 
489
space and translation extraction features. Figure 4 
illustrates three different search strategies. Suppose 
we want to translate the Chinese query ?????. 
(Cheng et al 2004) only searched 188 English web 
pages which contained the source query, and 53% 
of them (100 pages) had the correct translations.  
(Zhang and Vines 2004) searched the whole 
55,100 web pages, 10% of them (5490 pages) had 
the correct translation. Our approach used query 
expansion to search any web pages containing ??
??? and English hint words, which was a larger 
search space than (Cheng et al 2004) and more 
focused compared with (Zhang and Vines 2004), 
as illustrated with the shaded region in Figure 4. 
For translation extraction features, we took advan-
tage of machine transliteration and machine trans-
lation models, and combined them with frequency 
and distance information.  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Web search space strategy comparison 
6 Discussion and Future Work 
In this paper we demonstrated the feasibility of 
the proposed approach by searching for the English 
translation for a given Chinese key phrase, where 
we use punctuations and Chinese words as the 
boundary of candidate English translations. In the 
future we plan to try more flexible translation can-
didate selection methods, and apply them to other 
language pairs. We also would like to test our ap-
proach on more standard test sets, and compare the 
performance with other systems.  
Our approach works on short snippets for query 
expansion and translation extraction, and the com-
putation time is short. Therefore the search en-
gine?s response time is the major factor of 
computational efficiency.  
 
 
7 Conclusion 
We proposed a novel approach to mine key phrase 
translations from web corpora. We used cross-
lingual query expansion to retrieve more relevant 
web pages snippets, and extracted target transla-
tions combining transliteration, translation and fre-
quency-distance models. We achieved significantly 
better results compared to the existing methods.  
8 References  
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra and 
R.L. Mercer. The Mathematics of Machine Translation: 
Parameter Estimation. In Computational Linguistics, vol 
19, number 2. pp.263-311, June, 1993. 
 
P.?J. Cheng, J.-W. Teng, R.-C. Chen, J.-H. Wang, W.-H. 
Lu, and L.-F. Chien. Translating unknown queries with 
web corpora for cross-language information retrieval. In 
the Proceedings of 27th ACM SIGIR, pp146-153. ACM 
Press, 2004. 
 
F. Huang, S.Vogel and A. Waibel. Automatic extraction 
of named entity translingual equivalence based on 
multi-feature cost minimization. In the Proceedings of 
the 41st ACL. Workshop on Multilingual and Mixed-
language Named Entity Recognition, pp124-129, Sap-
poro, Japan, July 2003. 
 
W.-H. Lu, L.-F. Chien, H.-J. Lee. Translation of web 
queries using anchor text mining. ACM Trans. Asian 
Language Information Processing  (TALIP) 1(2): 159-
172 (2002) 
 
P. Resnik and N. A. Smith, The Web as a Parallel Cor-
pus, Computational Linguistics 29(3), pp. 349-380, Sep-
tember 2003 
 
S. Vogel, Y. Zhang, F. Huang, A. Tribble, A. Venogu-
pal, B. Zhao and A. Waibel.  The CMU statistical ma-
chine translation system. In Proceedings of the MT 
Summit IX Conference New Orlean, LA, September, 
2003. 
 
Y. Zhang and P. Vines. Detection and Translation of 
OOV Terms Prior to Query Time, In the Proceedings of 
27th ACM SIGIR. pp524-525, Sheffield, England, 2004. 
 
Y. Zhang and P. Vines 2004, Using the Web for Auto-
mated Translation Extraction in Cross-Language Infor-
mation Retrieval, In Proceedings of 27th ACM SIGIR, 
pp.162-169, Sheffield, United Kingdom, 2004. 
 
Y. Zhang, F. Huang and S. Vogel, Mining Translations 
of OOV Terms from the Web through Cross-lingual 
Query Expansion, in the Proceedings of the 28th ACM 
SIGIR, Salvador, Brazil, August 2005. 
490
,PSURYLQJ1DPHG(QWLW\7UDQVODWLRQ&RPELQLQJ3KRQHWLF
DQG6HPDQWLF6LPLODULWLHV
)HL+XDQJ6WHSKDQ9RJHODQG$OH[:DLEHO/DQJXDJH7HFKQRORJLHV,QVWLWXWH6FKRRORI&RPSXWHU6FLHQFHV&DUQHJLH0HOORQ8QLYHUVLW\^IKXDQJYRJHODKZ`#FVFPXHGX
$EVWUDFW
7KLVSDSHUGHVFULEHV DQ DSSURDFK WR WUDQVODWHUDUHO\RFFXUULQJQDPHGHQWLWLHV1(E\FRPELQLQJSKRQHWLFDQGVHPDQWLFVLPLODULWLHV7KHSKRQHWLFVLPLODULW\LVHVWLPDWHGIURPDVXUIDFHVWULQJ WUDQVOLWHUDWLRQPRGHO DQG WKH VHPDQWLFVLPLODULW\ LV FDOFXODWHG IURP D FRQWH[W YHFWRUVHPDQWLFPRGHO*LYHQDVRXUFH&KLQHVH1(DQG LWV FRQWH[W WKLV DSSURDFK ILUVW JHQHUDWHVTXHULHV LQ WKH WDUJHW (QJOLVK ODQJXDJH DFFRUGLQJ WR WKHFRQWH[W WUDQVODWLRQK\SRWKHVHVWKHQ VHDUFKHV IRU UHOHYDQW GRFXPHQWV IURP DWDUJHW ODQJXDJH FRUSXV 7DUJHW 1(V LQ UHWULHYHG GRFXPHQWV DUH FRPSDUHG ZLWK WKHVRXUFH1(EDVHGRQWKHLUSKRQHWLFDQGFRQWH[WXDO VHPDQWLF VLPLODULWLHV DQG WKH EHVWPDWFKHGRQHLVVHOHFWHGDVWKHFRUUHFWWUDQVODWLRQ ([SHULPHQWV VKRZ WKDW WKLV DSSURDFKDFKLHYHV $XWRPDWLF([WUDFWLRQRI1DPHG(QWLW\7UDQVOLQJXDO(TXLYDOHQFH%DVHGRQ0XOWL)HDWXUH&RVW0LQLPL]DWLRQ)HL+XDQJ6WHSKDQ9RJHODQG$OH[:DLEHO/DQJXDJH7HFKQRORJLHV,QVWLWXWH&DUQHJLH0HOORQ8QLYHUVLW\3LWWVEXUJK3$^IKXDQJYRJHODKZ`#FVFPXHGX $EVWUDFW7UDQVOLQJXDO HTXLYDOHQFH UHIHUV WR WKH UHODWLRQVKLSEHWZHHQH[SUHVVLRQVRIWKHVDPHPHDQLQJIURPGLIIHUHQWODQJXDJHV ,GHQWLI\LQJ WUDQVOLQJXDO HTXLYDOHQFH RIQDPHG HQWLWLHV 1( FDQ VLJQLILFDQWO\ FRQWULEXWH WRPXOWLOLQJXDO QDWXUDO ODQJXDJH SURFHVVLQJ VXFK DVFURVVOLQJXDO LQIRUPDWLRQ UHWULHYDO FURVVOLQJXDOLQIRUPDWLRQ H[WUDFWLRQ DQG VWDWLVWLFDO PDFKLQHWUDQVODWLRQ ,Q WKLV SDSHU ZH SUHVHQW DQ LQWHJUDWHGDSSURDFKWRH[WUDFW1(WUDQVOLQJXDOHTXLYDOHQFHIURPDSDUDOOHO&KLQHVH(QJOLVKFRUSXV6WDUWLQJ IURP D ELOLQJXDO FRUSXV ZKHUH 1(V DUHDXWRPDWLFDOO\ WDJJHG IRU HDFK ODQJXDJH 1( SDLUV DUHDOLJQHG LQ RUGHU WR PLQLPL]H WKH RYHUDOO PXOWLIHDWXUHDOLJQPHQW FRVW  $Q 1( WUDQVOLWHUDWLRQ PRGHO LVSUHVHQWHG DQG LWHUDWLYHO\ WUDLQHG XVLQJ QDPHG HQWLW\SDLUV H[WUDFWHG IURP D ELOLQJXDO GLFWLRQDU\ 7KHWUDQVOLWHUDWLRQ FRVW FRPELQHG ZLWK WKH QDPHG HQWLW\WDJJLQJFRVWDQGZRUGEDVHGWUDQVODWLRQFRVWFRQVWLWXWHWKH PXOWLIHDWXUH DOLJQPHQW FRVW 7KHVH IHDWXUHV DUHGHULYHG IURP VHYHUDO LQIRUPDWLRQ VRXUFHV XVLQJXQVXSHUYLVHGDQGSDUWO\VXSHUYLVHGPHWKRGV $JUHHG\VHDUFK DOJRULWKP LV DSSOLHG WR PLQLPL]H WKH DOLJQPHQWFRVW ([SHULPHQWV VKRZ WKDW WKH SURSRVHG DSSURDFKH[WUDFWV1(WUDQVOLQJXDOHTXLYDOHQFHZLWK)VFRUHDQGLPSURYHVWKHWUDQVODWLRQVFRUHIURPProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 277?286, Prague, June 2007. c?2007 Association for Computational Linguistics
Hierarchical System Combination for Machine Translation
Fei Huang
IBM T.J. Watson Research Center
Yorktown Heights, NY 10562
huangfe@us.ibm.com
Kishore Papineni ?
Yahoo! Research
New York, NY 10011
kpapi@yahoo-inc.com
Abstract
Given multiple translations of the same
source sentence, how to combine them to
produce a translation that is better than any
single system output? We propose a hier-
archical system combination framework for
machine translation. This framework inte-
grates multiple MT systems? output at the
word-, phrase- and sentence- levels. By
boosting common word and phrase trans-
lation pairs, pruning unused phrases, and
exploring decoding paths adopted by other
MT systems, this framework achieves bet-
ter translation quality with much less re-
decoding time. The full sentence translation
hypotheses from multiple systems are addi-
tionally selected based on N-gram language
models trained on word/word-POS mixed
stream, which further improves the transla-
tion quality. We consistently observed sig-
nificant improvements on several test sets in
multiple languages covering different gen-
res.
1 Introduction
Many machine translation (MT) frameworks have
been developed, including rule-based transfer MT,
corpus-based MT (statistical MT and example-based
MT), syntax-based MT and the hybrid, statistical
MT augmented with syntactic structures. Different
MT paradigms have their strengths and weaknesses.
?This work was done when the author was at IBM Research.
Systems adopting the same framework usually pro-
duce different translations for the same input, due
to their differences in training data, preprocessing,
alignment and decoding strategies. It is beneficial
to design a framework that combines the decoding
strategies of multiple systems as well as their out-
puts and produces translations better than any single
system output. More recently, within the GALE1
project, multiple MT systems have been developed
in each consortium, thus system combination be-
comes more important.
Traditionally, system combination has been con-
ducted in two ways: glass-box combination and
black-box combination. In the glass-box combi-
nation, each MT system provides detailed decod-
ing information, such as word and phrase transla-
tion pairs and decoding lattices. For example, in the
multi-engine machine translation system (Nirenburg
and Frederking, 1994), target language phrases from
each system and their corresponding source phrases
are recorded in a chart structure, together with their
confidence scores. A chart-walk algorithm is used
to select the best translation from the chart. To com-
bine words and phrases from multiple systems, it is
preferable that all the systems adopt similar prepro-
cessing strategies.
In the black-box combination, individual MT sys-
tems only output their top-N translation hypothe-
ses without decoding details. This is particularly
appealing when combining the translation outputs
from COTS MT systems. The final translation may
be selected by voted language models and appropri-
ate confidence rescaling schemes ((Tidhar and Kuss-
1http://www.darpa.mil/ipto/programs/gale/index.htm
277
ner, 2000) and (Nomoto, 2004)). (Mellebeek et al,
2006) decomposes source sentences into meaning-
ful constituents, translates them with component MT
systems, then selects the best segment translation
and combine them based on majority voting, lan-
guage models and confidence scores.
(Jayaraman and Lavie, 2005) proposed another
black-box system combination strategy. Given sin-
gle top-one translation outputs from multiple MT
systems, their approach reconstructs a phrase lat-
tice by aligning words from different MT hypothe-
ses. The alignment is based on the surface form
of individual words, their stems (after morphology
analysis) and part-of-speech (POS) tags. Aligned
words are connected via edges. The algorithm finds
the best alignment that minimizes the number of
crossing edges. Finally the system generates a new
translation by searching the lattice based on align-
ment information, each system?s confidence scores
and a language model score. (Matusov et al, 2006)
and (Rosti et al, 2007) constructed a confusion net-
work from multiple MT hypotheses, and a consen-
sus translation is selected by redecoding the lattice
with arc costs and confidence scores.
In this paper, we introduce our hierarchical sys-
tem combination strategy. This approach allows
combination on word, phrase and sentence levels.
Similar to glass-box combination, each MT sys-
tem provides detailed information about the trans-
lation process, such as which source word(s) gener-
ates which target word(s) in what order. Such in-
formation can be combined with existing word and
phrase translation tables, and the augmented phrase
table will be significantly pruned according to reli-
able MT hypotheses. We select an MT system to re-
translate the test sentences with the refined models,
and encourage search along decoding paths adopted
by other MT systems. Thanks to the refined trans-
lation models, this approach produces better transla-
tions with a much shorter re-decoding time. As in
the black-box combination, we select full sentence
translation hypotheses from multiple system outputs
based on n-gram language models. This hierarchical
system combination strategy avoids problems like
translation output alignment and confidence score
normalization. It seamlessly integrates detailed de-
coding information and translation hypotheses from
multiple MT engines, and produces better transla-
tions in an efficient manner. Empirical studies in a
later section show that this algorithm improves MT
quality by 2.4 BLEU point over the best baseline de-
coder, with a 1.4 TER reduction. We also observed
consistent improvements on several evaluation test
sets in multiple languages covering different genres
by combining several state-of-the-art MT systems.
The rest of the paper is organized as follows: In
section 2, we briefly introduce several baseline MT
systems whose outputs are used in the system com-
bination. In section 3, we present the proposed hi-
erarchical system combination framework. We will
describe word and phrase combination and pruning,
decoding path imitation and sentence translation se-
lection. We show our experimental results in section
4 and conclusions in section 5.
2 Baseline MT System Overview
In our experiments, we take the translation out-
puts from multiple MT systems. These include
phrase-based statistical MT systems (Al-Onaizan
and Papineni, 2006) (Block) and (Hewavitharana et
al., 2005) (CMU SMT) , a direct translation model
(DTM) system (Ittycheriah and Roukos, 2007) and a
hierarchical phrased-based MT system (Hiero) (Chi-
ang, 2005). Different translation frameworks are
adopted by different decoders: the DTM decoder
combines different features (source words, mor-
phemes and POS tags, target words and POS tags)
in a maximum entropy framework. These features
are integrated with a phrase translation table for
flexible distortion model and word selection. The
CMU SMT decoder extracts testset-specific bilin-
gual phrases on the fly with PESA algorithm. The
Hiero system extracts context-free grammar rules
for long range constituent reordering.
We select the IBM block decoder to re-translate
the test set for glass-box system combination. This
system is a multi-stack, multi-beam search decoder.
Given a source sentence, the decoder tries to find
the translation hypothesis with the minimum trans-
lation cost. The overall cost is the log-linear combi-
nation of different feature functions, such as trans-
lation model cost, language model cost, distortion
cost and sentence length cost. The translation cost
278
between a phrase translation pair (f, e) is defined as
TM(e, f) =
?
i
?i?(i) (1)
where feature cost functions ?(i) includes:
? log p(f |e), a target-to-source word translation
cost, calculated based on unnormalized IBM model1
cost (Brown et al, 1994);
p(f |e) =
?
j
?
i
t(fj|ei) (2)
where t(fj|ei) is the word translation probabilities,
estimated based on word alignment frequencies over
all the training data. i and j are word positions in
target and source phrases.
? log p(e|f), a source-to-target word translation
cost, calculated similar to ? log p(f |e);
S(e, f), a phrase translation cost estimated ac-
cording to their relative alignment frequency in the
bilingual training data,
S(e, f) = ? log P (e|f) = ? log C(f, e)C(f) . (3)
??s in Equation 1 are the weights of different fea-
ture functions, learned to maximize development set
BLEU scores using a method similar to (Och, 2003).
The SMT system is trained with testset-specific
training data. This is not cheating. Given a test set,
from a large bilingual corpora we select parallel sen-
tence pairs covering n-grams from source sentences.
Phrase translation pairs are extracted from the sub-
sampled alignments. This not only reduces the size
of the phrase table, but also improves topic relevancy
of the extracted phrase pairs. As a results, it im-
proves both the efficiency and the performance of
machine translation.
3 Hierarchical System Combination
Framework
The overall system combination framework is
shown in Figure 1. The source text is translated
by multiple baseline MT systems. Each system pro-
duces both top-one translation hypothesis as well as
phrase pairs and decoding path during translation.
The information is shared through a common XML
file format, as shown in Figure 2. It demonstrates
how a source sentence is segmented into a sequence
of phrases, the order and translation of each source
phrase as well as the translation scores, and a vector
of feature scores for the whole test sentence. Such
XML files are generated by all the systems when
they translate the source test set.
We collect phrase translation pairs from each de-
coder?s output. Within each phrase pair, we iden-
tify word alignment and estimate word translation
probabilities. We combine the testset-specific word
translation model with a general model. We aug-
ment the baseline phrase table with phrase trans-
lation pairs extracted from system outputs, then
prune the table with translation hypotheses. We re-
translate the source text using the block decoder with
updated word and phrase translation models. Ad-
ditionally, to take advantage of flexible reordering
strategies of other decoders, we develop a word or-
der cost function to reinforce search along decod-
ing paths adopted by other decoders. With the re-
fined translation models and focused search space,
the block decoder efficiently produces a better trans-
lation output. Finally, the sentence hypothesis se-
lection module selects the best translation from each
systems? top-one outputs based on language model
scores. Note that the hypothesis selection module
does not require detailed decoding information, thus
can take in any MT systems? outputs.
3.1 Word Translation Combination
The baseline word translation model is too general
for the given test set. Our goal is to construct a
testset-specific word translation model, combine it
with the general model to boost consensus word
translations. Bilingual phrase translation pairs are
read from each system-generated XML file. Word
alignments are identified within a phrase pair based
on IBM Model-1 probabilities. As the phrase pairs
are typically short, word alignments are quite accu-
rate. We collect word alignment counts from the
whole test set translation, and estimate both source-
to-target and target-to-source word translation prob-
abilities. We combine such testset-specific transla-
tion model with the general model.
t??(e|f) = ?t?(e|f) + (1 ? ?)t(e|f); (4)
where t?(e|f) is the testset-specific source-to-target
word translation probability, and t(e|f) is the prob-
279
<tr engine="XXX"> 
<s id="0"> <w>  </w><w> 	
 </w><w>  </w><w> 	 </w><w>  
</w><w>  </w><w>  </w><w> Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 391?399,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
When Harry Met Harri,  and : 
Cross-lingual Name Spelling Normalization 
 
Fei Huang , Ahmad Emami and Imed Zitouni 
IBM T. J. Watson Research Center 
1101 Kitchawan Road 
Yorktown Heights, NY 10598 
{huangfe, emami, izitouni}@us.ibm.com 
 
Abstract 
Foreign name translations typically include 
multiple spelling variants. These variants 
cause data sparseness problems, increase 
Out-of-Vocabulary (OOV) rate, and present 
challenges for machine translation, 
information extraction and other NLP tasks. 
This paper aims to identify name spelling 
variants in the target language using the 
source name as an anchor. Based on word-
to-word translation and transliteration 
probabilities, as well as the string edit 
distance metric, target name translations with 
similar spellings are clustered. With this 
approach tens of thousands of high precision 
name translation spelling variants are 
extracted from sentence-aligned bilingual 
corpora. When these name spelling variants 
are applied to Machine Translation and 
Information Extraction tasks, improvements 
over strong baseline systems are observed in 
both cases. 
1 Introduction 
Foreign names typically have multiple spelling 
variants after translation, as seen in the 
following examples:   
He confirmed that "al-Kharroub 
province is at the top of our 
priorities."  
?for the Socialist Progressive 
Party in upper Shuf and the Al-
Kharrub region,? 
?during his tour of a number of 
villages in the region of Al-
Kharub,? 
?Beirut and its suburbs and 
Iqlim al-Khurub,? 
  
 
 
 
Such name spelling variants also frequently 
appear in other languages, such as (bushi) / 
(bushu) / (buxi) (for Bush) in Chinese, 
and 	 (sbrngfyld) /
	  (sbryngfyld) / 
	 (sbrynjfyld) (for Springfield) in Arabic.  
These spelling variants present challenges for 
many NLP tasks, increasing vocabulary size and 
OOV rate, exacerbating the data sparseness 
problem and reducing the readability of MT 
output when different spelling variants are 
generated for the same name in one document. 
We address this problem by replacing each 
spelling variant with its corresponding canonical 
form. Such text normalization could potentially 
benefit many NLP tasks including information 
retrieval, information extraction, question 
answering, speech recognition and machine 
translation. 
Research on name spelling variants has been 
studied mostly in Information Retrieval research, 
especially in query expansion and cross-lingual 
IR.  Baghat and Hovy (2007) proposed two 
approaches for spelling variants generation, 
based on the letters-to-phonemes mapping and 
Soundex algorithm (Knuth 1973). Raghaven and 
Allan (2005) proposed several techniques to 
group names in ASR output and evaluated their 
effectiveness in spoken document retrieval 
(SDR). Both approaches use a named entity 
extraction system to automatically identify 
names. For multi-lingual name spelling variants, 
Linden (2005) proposed to use a general edit 
distance metric with a weighted FST to find 
technical term translations (which were referred 
to as ?cross-lingual spelling variants?). These 
391
variants are typically translated words with 
similar stems in another language. Toivonen and 
colleagues (2005) proposed a two-step fuzzy 
translation technique to solve similar problems. 
Al-Onaizan and Knight (2002), Huang (2003) 
and Ji and Grishman (2007) investigated the 
general name entity translation problem, 
especially in the context of machine translation. 
This paper aims to identify mono-lingual 
name spelling variants using cross-lingual 
information. Instead of using a named entity 
tagger to identify name spelling variants, we 
treat names in one language as the anchor of 
spelling variants in another language. From 
sentence-aligned bilingual corpora we collect 
word co-occurrence statistics and calculate word 
translation1 probabilities. For each source word, 
we group its target translations into clusters 
according to string edit distances, then calculate 
the transliteration cost between the source word 
and each target translation cluster. Word pairs 
with small transliteration costs are considered as 
name translations, and the target cluster contains 
multiple spelling variants corresponding to the 
source name.  
We apply this approach to extract name 
transliteration spelling variants from bilingual 
corpora. We obtained tens of thousands of high 
precision name translation pairs. We further 
apply these spelling variants to Machine 
Translation (MT) and Information Extraction (IE) 
tasks, and observed statistically significant 
improvement on the IE task, and close to oracle 
improvement on the MT task.  
The rest of the paper is organized as follows. 
In section 2 we describe the technique to 
identify name spelling variants from bilingual 
data. In section 3 and 4 we address their 
application to MT and IE respectively. We 
present our experiment results and detailed 
analysis in section 5. Section 6 concludes this 
paper with future work. 
2 Finding Name Translation Variants 
                                                          
1
 In this paper, the translation cost measures the semantic 
difference between source and target names, which are 
estimated from their co-occurrence statistics. The 
transliteration cost measures their phonetic distance and are 
estimated based on a character transliteration model. 
Starting from sentence-aligned parallel data, we 
run HMM alignment (Vogel et. al. 1996 & Ge 
2004) to obtain a word translation model. For 
each source word this model generates target 
candidate translations as well as their translation 
probabilities. A typical entry is shown in Table 1.  
It can be observed that the Arabic name?s 
translations include several English words with 
similar spellings, all of which are correct 
translations. However, because the lexical 
translation probabilities are distributed among 
these variants, none of them has the highest 
probability. As a result, the incorrect translation, 
iqlim, is assigned the highest probability and 
often selected in MT output. To fix this problem, 
it is desirable to identify and group these target 
spelling variants, convert them into a canonical 
form and merge their translation probabilities.  
 | Alxrwb
iqlim 
[0.22] 
al-kharrub 
[0.16] 
al-kharub 
[0.11] 
overflew 
[0.09] 
junbulat 
[0.05] 
al-khurub 
[0.05] 
hours 
[0.04] 
al-kharroub 
[0.03] 
 
Table 1. English translations of a Romanized Arabic 
name Alxrwb with translation probabilities. 
   For each source word in the word translation 
model, we cluster its target translations based on 
string edit distances using group average 
agglomerative clustering algorithm (Manning 
and Sch?tze, 2000). Initially each target word is 
a single word cluster. We calculate the average 
editing distance between any two clusters, and 
merge them if the distance is smaller than a 
certain threshold. This process repeats until the 
minimum distance between any two clusters is 
above a threshold. In the above example, al-
kharrub, al-kharub, al-khurub and al-kharroub 
are grouped into a single cluster, and each of the 
ungrouped words remains in its single word 
cluster. Note that the source word may not be a 
name while its translations may still have similar 
spellings. An example is the Arabic word   
which is aligned to English words brief, briefing, 
briefed and briefings. To detect whether a source 
word is a name, we calculate the transliteration 
cost between the source word and its target 
translation cluster, which is defined as the 
average transliteration cost between the source 
word and each target word in the cluster. As 
392
many names are translated based on their 
pronunciations, the source and target names 
have similar phonetic features and lower 
transliteration costs. Word pairs whose 
transliteration cost is lower than an empirically 
selected threshold are considered as name 
translations. 
2.1 Name Transliteration Cost 
The transliteration cost measures the phonetic 
similarity between a source word and a target 
word. It is calculated based on the character 
transliteration model, which can be trained from 
bilingual name translation pairs. We segment the 
source and target names into characters, then run 
monotone2 HMM alignment on the source and 
target character pairs. After the training, 
character transliteration probabilities can be 
estimated from the relevant frequencies of 
character alignments. 
Suppose the source word f contains m 
characters, f1, f2, ?, fm,  and the target word e 
contains n characters, e1, e2, ?, en. For j=1, 2,?, 
n, letter  ej is aligned to character jaf according 
to the HMM aligner. Under the assumption that 
character alignments are independent, the word 
transliteration probability is calculated as 
 ?
=
=
n
j
aj jfepfeP
1
)|()|(          (2.1) 
where )|(
jaj fep is the character transliteration 
probability.  Note that in the above configuration 
one target character can be aligned to only one 
source character, and one source character can 
be aligned to multiple target characters.  
An example of the trained A-E character 
transliteration model is shown in Figure 1. The 
Arabic character  is aligned with high 
probabilities to English letters with similar 
pronunciation. Because Arabic words typically 
omit vowels, English vowels are also aligned to 
Arabic characters. Given this model, the 
characters within a Romanized Arabic name and 
its English translation are aligned as shown in 
Figure 1.   
                                                          
2
 As name are typically phonetically translated, the 
character alignment are often monotone. There is no cross-
link in character alignments. 
2.2 Transliteration Unit Selection 
The transliteration units are typically characters. 
The Arabic alphabet includes 32 characters, and 
the English alphbet includes 56 letters 3 . 
However, Chinese has about 4000 frequent 
characters. The imbalance of Chinese and 
English vocabulary sizes results in suboptimal 
transliteration model estimation. Each Chinese 
character also has a pinyin, the Romanized 
representation of its pronunciation. Segmenting 
the Chinese pinyin into sequence of Roman 
letters, we now have comparable vocabulary 
sizes for both Chinese and English. We build a 
pinyin transliteration model using Chinese-
English name translation pairs, and compare its 
performance with a character transliteration 
model in Experiment section 5.1. 

h 
[0.44] 
K 
[0.29] 
k 
[0.21] 
a 
[0.03] 
u 
[0.015] 
i 
[0.004] 
 
Figure 1. Example of the learned A-E character 
transliteration model with probabilities, and its 
application in the alignment between an Romanized 
Arabic name and an English translation. 
3 Application to Machine Translation 
We applied the extracted name translation 
spelling variants to the machine translation task. 
Given the name spelling variants, we updated 
both the translation and the language model, 
adding variants? probabilities to the canonical 
form. 
   Our baseline MT decoder is a phrase-based 
decoder as described in (Al-Onaizan and 
Papineni 2006). Given a source sentence, the 
decoder tries to find the translation hypothesis 
with minimum translation cost, which is defined 
as the log-linear combination of different feature 
functions, such as translation model cost, 
language model cost, distortion cost and 
                                                          
3Uppercase and lowercase letters plus some special 
symbols such as ?_?, ?-?. 
393
sentence length cost. The translation cost 
includes word translation probability and phrase 
translation probability. 
3.1 Updating The Translation Model 
Given target name spelling variants { mttt ,...,, 21  
} for a source name s, here mttt ,...,, 21 are sorted 
based on their lexical translation probabilities, 
).|(...)|()|( 21 stpstpstp m???  
We select 1t  as the canonical spelling, and 
merge other spellings? translation probabilities 
with this one: 
?
=
=
m
j
m stpstp
1
1 ).|()|(  
Other spelling variants get zero probability. 
Table 2 shows the updated word translation 
probabilities for ?|Alxwrb?. Compared 
with Figure 1, the translation probabilities from 
several spelling variants are merged with the 
canonical form, al-kharrub, which now has the 
highest probability in the new model. 

Table 2. English translations of an Arabic name |Alxrwb with the updated word translation 
model. 
 
   The phrase translation table includes source 
phrases, their target phrase translations and the 
frequencies of the bilingual phrase pair 
alignment. The phrase translation probabilities 
are calculated based on their alignment 
frequencies, which are collected from word 
aligned parallel data. To update the phrase 
translation table, for each phrase pair including a 
source name and its spelling variant in the target 
phrase, we replace the target name with its 
canonical spelling. After the mapping, two target 
phrases differing only in target names may end 
up with the identical target phrase, and their 
alignment frequencies are added. Phrase 
translation probabilities are re-estimated with the 
updated frequencies. 
3.2 Updating The Language Model 
The machine translation decoder uses a language 
model as a measure of a well-formedness of the 
output sentence. Since the updated translation 
model can produce only the canonical form of a 
group of spelling variants, the language model 
should be updated in that all m-grams 
( Nm ??1 ) that are spelling variants of each 
other are merged (and their counts added), 
resulting in the canonical form of the m-gram. 
Two m-grams are considered spelling variants of 
each other if they contain words it1 , 
it2 ( ii tt 21 ? ) 
at the same position i in the m-gram, and that it1  
and it2 belong to the same spelling variant group. 
   An easy way to achieve this update is to 
replace every spelling variant in the original 
language model training data with its 
corresponding canonical form, and then build 
the language model again. However, since we do 
not want to replace words that are not names we 
need to have a mechanism for detecting names.  
For simplicity, in our experiments we assumed a 
word is a name if it is capitalized, and we 
replaced spelling variants with their canonical 
forms only for words that start with a capital 
letter.  
4 Applying to Information Extraction 
Information extraction is a crucial step toward 
understanding a text, as it identifies the 
important conceptual objects in a discourse. We 
address here one important and basic task of 
information extraction: mention detection4: we 
call instances of textual references to objects 
mentions, which can be either named (e.g. John 
Smith), nominal (the president) or pronominal 
(e.g. he, she). For instance, in the sentence  
? President John Smith said he has no 
comments.   
there are two mentions: John Smith and he. 
Similar to many classical NLP tasks, we 
formulate the mention detection problem as a 
classification problem, by assigning to each 
token in the text a label, indicating whether it 
starts a specific mention, is inside a specific 
mention, or is outside any mentions. Good 
                                                          
4We adopt here the ACE (NIST 2007) nomenclature. 
 | Alxwrb
al-kharrub 
 [0.35] 
 iqlim 
 [0.22] 
al-kharub 
[0.0] 
overflew 
[0.09] 
junbulat 
[0.05] 
al-khurub 
[0.0] 
hours 
[0.04] 
al-kharroub 
[0.0] 
 
394
performance in many natural language 
processing tasks has been shown to depend 
heavily on integrating many sources of 
information (Florian et al 2007). We select an 
exponential classifier, the Maximum Entropy 
(MaxEnt henceforth) classifier that can integrate 
arbitrary types of information and make a 
classification decision by aggregating all 
information available for a given classification 
(Berger et al 1996). In this paper, the MaxEnt 
model is trained using the sequential conditional 
generalized iterative scaling (SCGIS) technique 
(Goodman, 2002), and it uses a Gaussian prior 
for regularization (Chen and Rosenfeld, 2000). 
   In ACE, there are seven possible mention 
types: person, organization, location, facility, 
geopolitical entity (GPE), weapon, and vehicle. 
Experiments are run on Arabic and English. Our 
baseline system achieved very competitive result 
among systems participating in the ACE 2007 
evaluation. It uses a large range of features, 
including lexical, syntactic, and the output of 
other information extraction models. These 
features were described in (Zitouni and Florian, 
2008 & Florian et al 2007), and are not 
discussed here. In this paper we focus on 
examining the effectiveness of name spelling 
variants in improving mention detection 
systems. We add a new feature that for each 
token xi  to process we fire its canonical form 
(class label) C(xi) ,  representative of name 
spelling variants of xi . This name spelling 
variant feature is also used in conjunction with 
the lexical (e.g., words and morphs in a 3-word 
window, prefixes and suffixes of length up to 4, 
stems in a 4-word window for Arabic) and 
syntactic (POS tags, text chunks) features. 
5 Experiments 
5.1 Evaluating the precision of name 
spelling variants 
We extracted Arabic-English and English-
Arabic name translation variants from sentence-
aligned parallel corpora released by LDC. The 
accuracy of the extracted name translation 
spelling variants are judged by proficient Arabic 
and Chinese speakers. 
   The Arabic-English parallel corpora include 
5.6M sentence pairs, 845K unique Arabic words 
and 403K unique English words. We trained a 
word translation model by running HMM 
alignment on the parallel data, grouped target 
translation with similar spellings and computed 
the average transliteration cost between the 
Arabic word and each English word in the 
translation clusters according to Formula 2.1. 
We sorted the name translation groups according 
to their transliteration costs, and selected 300 
samples at different ranking position for 
evaluation (20 samples at each ranking position). 
The quality of the name translation variants are 
judged as follows: for each candidate name 
translation group }|,...,,{ 21 sttt m , if the source 
word s is a name and all the target spelling 
variants are correct translations, it gets a credit 
of 1. If s is not a name, the credit is 0. If s is a 
name but only part of the target spelling variants 
are correct, it gets partial credit n/m, where n is 
the number of correct target translations. We 
evaluate only the precision of the extracted 
spelling variants 5 . As seen in Figure 2, the 
precision of the top 22K A-E name translations 
is 96.9%. Among them 98.5% of the Arabic 
words are names. The precision gets lower and 
lower when more non-name Arabic words are 
included. On average, each Arabic name has 
2.47 English spelling variants, although there are 
some names with more than 10 spelling variants. 
   Switching the source and target languages, we 
obtained English-Arabic name spelling variants, 
i.e., one English name with multiple Arabic 
spellings. As seen in Figure 3, top 20K E-A 
name pairs are obtained with a precision above 
87.9%, and each English name has 3.3 Arabic 
spellings on average. Table 3 shows some A-E 
and E-A name spelling variants, where Arabic 
words are represented in their Romanized form. 
  We conduct a similar experiment on the 
Chinese-English language pair, extracting 
Chinese-English and English-Chinese name 
spelling variants from 8.7M Chinese-English 
sentence pairs. After word segmentation, the 
Chinese vocabulary size is 1.5M words, and 
English vocabulary size is 1.4M words. With the  
                                                          
5
 Evaluating recall requires one to manually look through 
the space of all possible transliterations (hundreds of 
thousands of entries), which is impractical. 
395
Chinese pinyin transliteration model, we extract 
64K C-E name spelling variants with 93.6% 
precision. Figure 4 also shows the precision 
curve of the Chinese character transliteration 
model. On average the pinyin transliteration 
model has about 6% higher precision than the 
character transliteration model. The pinyin 
transliteration model is particularly better on the 
tail of the curve, extracting more C-E 
transliteration variants. Figure 5 shows the 
precision curve for E-C name spelling variants, 
where 20K name pairs are extracted using letter-
to-character transliteration model, and obtaining 
a precision of 74.3%. 
 Table 4 shows some C-E and E-C name 
spelling variants. We observed errors due to 
word segmentation. For example, the last two 
Chinese words corresponding to ?drenica? have 
additional Chinese characters, meaning ?drenica 
region? and ?drenica river?. Similarly for tenet, 
the last two Chinese words also have 
segmentation errors due to missing or spurious 
characters. Note that in the C-E spelling variants, 
the source word ? ? has 14 spelling 
variants. Judge solely from the spelling, it is 
hard to tell whether they are the same person 
name with different spellings. 
5.2   Experiments on Machine Translation 
We apply the Arabic-English name spelling 
variants on the machine translation task. Our 
baseline system is trained with 5.6M Arabic- 
English sentence pairs, the same training data 
used to extract A-E spelling variants. The 
language model is a modified Kneser-Ney 5-
gram model trained on roughly 3.5 billion words. 
After pruning (using count cutoffs), it contains a 
total of 935 million N-grams. We updated the 
translation models and the language model with 
the name spelling variant class. 
   Table 5 shows a Romanized Arabic sentence, 
the translation output from the baseline system 
and the output from the updated models. In the 
baseline system output, the Arabic name 
?Alxrwb? was incorrectly translated into 
?regional?. This error was fixed in the updated 
model, where both translation and language 
models assign higher probabilities to the correct 
translation ?al-kharroub? after spelling variant 
normalization.  
 
 
	
	
		

























	

















	
	













	











	









	

 
Figure 2. Arabic-English name spelling variants 
precision curve (Precision of evaluation sample at 
different ranking positions. The larger square indicates 
the cutoff point). 
	
	
		

























	

















	
	













	











	









	
ranking
pinyin char
 
Figure 4. Chinese-English name spelling variants 
precision curve. 
	
	
		

























	

















	
	













	











	




 
Figure 3. English-Arabic name spelling variants 
precision curve. 
	
	
		

























	

















	
	













	











	




 
Figure 5. English-Chinese name spelling variants 
precision curve. 
396
Source Alm&tmr AlAwl lAqlym Alxrwb AlErby AlmqAwm 
Reference the first conference of the Arab resistance in Iqlim Kharoub 
Baseline the first conference of the Arab regional resistance 
Updated model first conference of the Al-Kharrub the Arab resistance 
 
Table 5. English translation output with the baseline MT system and the system with updated models 
 
    
 BLEU 
r1n4 TER 
Baseline 0.2714 51.66 
Baseline+ULM+UTM 0.2718 51.46 
Ref. Normalization 0.2724 51.40 
Table 6. MT scores with updated TM and LM 
  We also evaluated the updated MT models on a 
MT test set. The test set includes 70 documents 
selected from GALE 2007 Development set. It 
contains 42 newswire documents and 28 weblog 
and newsgroup documents. There are 669 
sentences with 16.3K Arabic words in the test 
data. MT results are evaluated against one 
reference human translation using BLEU 
(Papineni et. al. 2001) and TER (Snover et. al. 
2006) scores. The results using the baseline 
decoder and the updated models are shown in 
Table 6. Applying the updated language model 
(ULM) and the translation model (UTM) lead to 
a small reduction in TER. After we apply similar 
name spelling normalization on the reference 
translation, we observed some additional 
improvements. Overall, the BLEU score is 
increased by 0.1 BLEU point and TER is 
reduced by 0.26. 
   Although the significance of correct name 
translation can not be fully represented by 
 
Table 3. Arabic-English and English-Arabic name spelling variant examples. Italic words represent different 
persons with similar spelling names. 
 
Lang. Pair Source Name Target Spelling Variants 
Alxmyny khomeini al-khomeini al-khomeni khomeni khomeyni khamenei khameneh'i 
krwby     karroubi karrubi krobi karubi karoubi kroubi 
Arabic-
English 
gbryAl     gabriel gabrielle gabrial ghobrial ghybrial 
cirebon   syrybwn syrbwn syrbn kyrybwn bsyrybwn bsyrwbwn 
mbinda     mbyndA mbndA mbydA AmbyndA AmbAndA mbynydA  
English-
Arabic 
nguyen     njwyn ngwyn ngwyyn ngyyn Angwyn nygwyyn nygwyn wnjwyn njwyyn 
nyjyn bnjwyn wngyyn ngwyAn njyn nykwyn  
 
Table 4. Chinese-English and English-Chinese name spelling variant examples with pinyin for Chinese characters. 
Italic words represent errors due to word segmentation. 
Lang. Pair Source Name  Target Spelling Variants 
	

(yan/duo/wei/ci/ji) 
endovitsky jendovitski yendovitski endovitski 
  
(si/te/fan/ni) 
stefani steffani stephani stefanni stefania 
Chinese-
English 
 
(wei/er/man) 
woermann wellman welman woellmann wohrmann wormann velman 
wollmann wehrmann verman woehrmann wellmann welmann wermann 
tenet (te/ni/te) (te/nei/te) (tai/nei/te) (te/nai/te) 
(te/nai/te) (te/nei/te/yu) (te/nei) 
drenica (de/lei/ni/cha) (de/lei/ni/ka) (te/lei/ni/cha) 
(te/lei/ni/cha) (de/lei/ni/cha/qu) 	
(de/lei/ni/cha/he) 
English-
Chinese 
ahmedabad Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 932?940,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Confidence Measure for Word Alignment
Fei Huang
IBM T.J.Watson Research Center
Yorktown Heights, NY 10598, USA
huangfe@us.ibm.com
Abstract
In this paper we present a confidence mea-
sure for word alignment based on the
posterior probability of alignment links.
We introduce sentence alignment confi-
dence measure and alignment link con-
fidence measure. Based on these mea-
sures, we improve the alignment qual-
ity by selecting high confidence sentence
alignments and alignment links from mul-
tiple word alignments of the same sen-
tence pair. Additionally, we remove
low confidence alignment links from the
word alignment of a bilingual training
corpus, which increases the alignment
F-score, improves Chinese-English and
Arabic-English translation quality and sig-
nificantly reduces the phrase translation
table size.
1 Introduction
Data-driven approaches have been quite active in
recent machine translation (MT) research. Many
MT systems, such as statistical phrase-based and
syntax-based systems, learn phrase translation
pairs or translation rules from large amount of
bilingual data with word alignment. The qual-
ity of the parallel data and the word alignment
have significant impacts on the learned transla-
tion models and ultimately the quality of transla-
tion output. Due to the high cost of commissioned
translation, many parallel sentences are automat-
ically extracted from comparable corpora, which
inevitably introduce many ?noises?, i.e., inaccu-
rate or non-literal translations. Given the huge
amount of bilingual training data, word alignments
are automatically generated using various algo-
rithms ((Brown et al, 1994), (Vogel et al, 1996)
Figure 1: An example of inaccurate translation
and word alignment.
and (Ittycheriah and Roukos, 2005)), which also
introduce many word alignment errors.
The example in Figure 1 shows the word align-
ment of the given Chinese and English sentence
pair, where the English words following each Chi-
nese word is its literal translation. We find untrans-
lated Chinese and English words (marked with
underlines). These spurious words cause signifi-
cant word alignment errors (as shown with dash
lines), which in turn directly affect the quality of
phrase translation tables or translation rules that
are learned based on word alignment.
In this paper we introduce a confidence mea-
sure for word alignment, which is robust to extra
or missing words in the bilingual sentence pairs,
as well as word alignment errors. We propose
a sentence alignment confidence measure based
on the alignment?s posterior probability, and ex-
tend it to the alignment link confidence measure.
We illustrate the correlation between the align-
ment confidence measure and the alignment qual-
ity on the sentence level, and present several ap-
proaches to improve alignment accuracy based on
the proposed confidence measure: sentence align-
ment selection, alignment link combination and
alignment link filtering. Finally we demonstrate
932
the improved alignments also lead to better MT
quality.
The paper is organized as follows: In section
2 we introduce the sentence and alignment link
confidence measures. In section 3 we demon-
strate two approaches to improve alignment accu-
racy through alignment combination. In section 4
we show how to improve a MaxEnt word align-
ment quality by removing low confidence align-
ment links, which also leads to improved transla-
tion quality as shown in section 5.
2 Sentence Alignment Confidence
Measure
2.1 Definition
Given a bilingual sentence pair (S,T ) where
S={s1,. . . , sI} is the source sentence and T={t1,
. . . ,tJ} is the target sentence. Let A = {aij} be
the alignment between S and T . The alignment
confidence measure C(A|S, T ) is defined as the
geometric mean of the alignment posterior proba-
bilities calculated in both directions:
C(A|S, T ) =
?
Ps2t(A|S, T )Pt2s(A|T, S), (1)
where
Ps2t(A|S, T ) =
P (A, T |S)
?
A? P (A
?, T |S)
. (2)
When computing the source-to-target algnment
posterior probability, the numerator is the sentence
translation probability calculated according to the
given alignment A:
P (A, T |S) =
J?
j=1
p(tj |si, aij ? A). (3)
It is the product of lexical translation probabili-
ties for the aligned word pairs. For unaligned tar-
get word tj , consider si = NULL. The source-to-
target lexical translation model p(t|s) and target-
to-source model p(s|t) can be obtained through
IBM Model-1 or HMM training. The denomina-
tor is the sentence translation probability summing
over all possible alignments, which can be calcu-
lated similar to IBM Model 1 in (Brown et al,
1994):
?
A?
P (A?, T |S) =
J?
j=1
I?
i=1
p(tj |si). (4)
Aligner F-score Cor. Coeff.
HMM 54.72 -0.710
BM 62.53 -0.699
MaxEnt 69.26 -0.699
Table 1: Correlation coefficients of multiple align-
ments.
Note that here only the word-based lexicon
model is used to compute the confidence measure.
More complex models such as alignment models,
fertility models and distortion models as described
in (Brown et al, 1994) could estimate the proba-
bility of a given alignment more accurately. How-
ever the summation over all possible alignments is
very complicated, even intractable, with the richer
models. For the efficient computation of the de-
nominator, we use the lexical translation model.
Similarly,
Pt2s(A|T, S) =
P (A,S|T )
?
A? P (A
?, S|T )
, (5)
and
P (A,S|T ) =
I?
i=1
p(si|tj , aij ? A). (6)
?
A?
P (A?, S|T ) =
I?
i=1
J?
j=1
p(si|tj). (7)
We randomly selected 512 Chinese-English (C-
E) sentence pairs and generated word alignment
using the MaxEnt aligner (Ittycheriah and Roukos,
2005). We evaluate per sentence alignment F-
scores by comparing the system output with a
reference alignment. For each sentence pair, we
also calculate the sentence alignment confidence
score ? logC(A|S, T ). We compute the corre-
lation coefficients between the alignment confi-
dence measure and the alignment F-scores. The
results in Figure 2 shows strong correlation be-
tween the confidence measure and the alignment
F-score, with the correlation coefficients equals to
-0.69. Such strong correlation is also observed on
an HMM alignment (Ge, 2004) and a Block Model
(BM) alignment (Zhao et al, 2005) with varying
alignment accuracies, as seen in Table1.
2.2 Sentence Alignment Selection Based on
Confidence Measure
The strong correlation between the sentence align-
ment confidence measure and the alignment F-
933
Figure 2: Correlation between sentence alignment
confidence measure and F-score.
measure suggests the possibility of selecting the
alignment with the highest confidence score to ob-
tain better alignments. For each sentence pair in
the C-E test set, we calculate the confidence scores
of the HMM alignment, the Block Model align-
ment and the MaxEnt alignment, then select the
alignment with the highest confidence score. As a
result, 82% of selected alignments have higher F-
scores, and the F-measure of the combined align-
ments is increased over the best aligner (the Max-
Ent aligner) by 0.8. This relatively small improve-
ment is mainly due to the selection of the whole
sentence alignment: for many sentences the best
alignment still contains alignment errors, some of
which could be fixed by other aligners. Therefore,
it is desirable to combine alignment links from dif-
ferent alignments.
3 Alignment Link Confidence Measure
3.1 Definition
Similar to the sentence alignment confidence mea-
sure, the confidence of an alignment link aij in the
sentence pair (S, T ) is defined as
c(aij |S, T ) =
?
qs2t(aij |S, T )qt2s(aij |T, S)
(8)
where the source-to-target link posterior probabil-
ity
qs2t(aij |S, T ) =
p(tj |si)
?J
j?=1 p(tj? |si)
, (9)
which is defined as the word translation probabil-
ity of the aligned word pair divided by the sum
of the translation probabilities over all the target
words in the sentence. The higher p(tj |si) is,
the higher confidence the link has. Similarly, the
target-to-source link posterior probability is de-
fined as:
qt2s(aij |T, S) =
p(si|tj)
?I
i?=1 p(si? |tj)
. (10)
Intuitively, the above link confidence definition
compares the lexical translation probability of the
aligned word pair with the translation probabilities
of all the target words given the source word. If a
word t occurs N times in the target sentence, for
any i ? {1, ..., I},
J?
j?=1
p(tj? |si) ? Np(t|si),
thus for any tj = t,
qs2t(aij) ?
1
N
.
This indicates that the confidence score of any
link connecting tj to any source word is at most
1/N . On the one hand this is expected because
multiple occurrences of the same word does in-
crease the confusion for word alignment and re-
duce the link confidence. On the other hand, ad-
ditional information (such as the distance of the
word pair, the alignment of neighbor words) could
indicate higher likelihood for the alignment link.
We will introduce a context-dependent link confi-
dence measure in section 4.
3.2 Alignment Link Selection
From multiple alignments of the same sentence
pair, we select high confidence links from different
alignments based on their link confidence scores
and alignment agreement ratio.
Typically, links appearing in multiple align-
ments are more likely correct alignments. The
alignment agreement ratio measures the popular-
ity of a link. Suppose the sentence pair (S, T ) have
alignments A1,. . . , AD, the agreement ratio of a
link aij is defined as
r(aij |S, T ) =
?
dC(Ad|S, T : aij ? Ad)?
d? C(Ad? |S, T )
, (11)
where C(A) is the confidence score of the align-
ment A as defined in formula 1. This formula
computes the sum of the alignment confidence
scores for the alignments containing aij , which is
934
Figure 3: Example of alignment link selection by combining MaxEnt, HMM and BM alignments.
normalized by the sum of all alignments? confi-
dence scores.
We collect all the links from all the alignments.
For each link we calculate the link confidence
score c(aij) and the alignment agreement ratio
r(aij). We link the word pair (si, tj) if either
c(aij) > h1 or r(aij) > r1, where h1 and r1 are
empirically chosen thresholds.
We combine the HMM alignment, the BM
alignment and the MaxEnt alignment (ME) us-
ing the above link selection algorithm. Figure
3 shows such an example, where alignment er-
rors in the MaxEnt alignment are shown with dot-
ted lines. As some of the links are correctly
aligned in the HMM and BM alignments (shown
with solid lines), the combined alignment corrects
some alignment errors while still contains com-
mon incorrect alignment links.
Table 2 shows the precision, recall and F-score
of individual alignments and the combined align-
ment. F-content and F-function are the F-scores
for content words and function words, respec-
tively. The link selection algorithm improves
the recall over the best aligner (the ME align-
ment) by 7 points (from 65.4 to 72.5) while de-
creasing the precision by 4.4 points (from 73.6
to 69.2). Overall it improves the F-score by 1.5
points (from 69.3 to 70.8), 1.8 point improvement
for content words and 1.0 point for function words.
It also significantly outperforms the traditionally
used heuristics, ?intersection-union-refine? (Och
and Ney, 2003) by 6 points.
4 Improved MaxEnt Aligner with
Confidence-based Link Filtering
In addition to the alignment combination, we also
improve the performance of the MaxEnt aligner
through confidence-based alignment link filtering.
Here we select the MaxEnt aligner because it has
935
Precision Recall F-score F-content F-function
HMM 62.65 48.57 54.72 62.10 34.39
BM 72.76 54.82 62.53 68.64 43.93
ME 72.66 66.17 69.26 72.52 61.41
Link-Select 69.19 72.49 70.81 74.31 60.26
Intersection-Union-Refine 63.34 66.07 64.68 70.15 49.72
Table 2: Link Selection and Combination Results
the highest F-measure among the three aligners,
although the algorithm described below can be ap-
plied to any aligner.
It is often observed that words within a con-
stituent (such as NP, PP) are typically translated
together, and their alignments are close. As a re-
sult the confidence measure of an alignment link
aij can be boosted given the alignment of its con-
text words. From the initial sentence alignment
we first identify an anchor link amn, the high con-
fidence alignment link closest to aij . The an-
chor link is considered as the most reliable con-
nection between the source and target context.
The context is then defined as a window center-
ing at amn with window width proportional to
the distance between aij and amn. When com-
puting the context-dependent link confidence, we
only consider words within the context window.
The context-dependent alignment link confidence
is calculated in the following steps:
1. Calculate the context-independent link con-
fidence measure c(aij) according to formula
(8).
2. Sort all links based on their link confidence
measures in decreasing order.
3. Select links whose confidence scores are
higher than an empirically chosen threshold
H as anchor links 1.
4. Walking along the remaining sorted links.
For each link {aij : c(aij) < H},
(a) Find the closest anchor link amn2,
(b) Define the context window width w =
|m? i|+ |n? j|.
1H is selected to maximize the F-score on an alignment
devset.
2When two equally close alignment links have the same
confidence score), we randomly select one of the tied links as
the anchor link.
(c) Compute the link posterior probabilities
within the context window:
qs2t(aij |amn) =
p(tj |si)
?j+w
j?=j?w p(tj? |si)
,
qt2s(aij |amn) =
p(si|tj)
?i+w
i?=i?w p(si? |tj)
.
(d) Compute the context-dependent link
confidence score c(aij |amn) =
?
qs2t(aij |amn)qt2s(aij |amn).
If c(aij |amn) > H , add aij into the set
of anchor links.
5. Only keep anchor links and remove all the re-
maining links with low confidence scores.
The above link filtering algorithm is designed to
remove incorrect links. Furthermore, it is possible
to create new links by relinking unaligned source
and target word pairs within the context window if
their context-dependent link posterior probability
is high.
Figure 4 shows context-independent link con-
fidence scores for the given sentence alignment.
The subscript following each word indicates the
word?s position. Incorrect alignment links are
shown with dashed lines, which have low confi-
dence scores (a5,7, a7,3, a8,2, a11,9) and will be
removed through filtering. When the anchor link
a4,11 is selected, the context-dependent link confi-
dence of a6,12 is increased from 0.12 to 0.51. Also
note that a new link a7,12 (shown as a dotted line)
is created because within the context window, the
link confidence score is as high as 0.96. This ex-
ample shows that the context-dependent link filter-
ing not only removes incorrect links, but also cre-
ate new links based on updated confidence scores.
We applied the confidence-based link filter-
ing on Chinese-English and Arabic-English word
alignment. The C-E alignment test set is the same
936
Figure 4: Alignment link filtering based on context-independent link confidence.
Precision Recall F-score
Baseline 72.66 66.17 69.26
+ALF 78.14 64.36 70.59
Table 3: Confidence-based Alignment Link Filter-
ing on C-E Alignment
Precision Recall F-score
Baseline 84.43 83.64 84.04
+ALF 88.29 83.14 85.64
Table 4: Confidence-based Alignment Link Filter-
ing on A-E Alignment
512 sentence pairs, and the A-E alignment test
set is the 200 Arabic-English sentence pairs from
NIST MT03 test set.
Tables 3 and 4 show the improvement of
C-E and A-E alignment F-measures with the
confidence-based alignment link filtering (ALF).
For C-E alignment, removing low confidence
alignment links increased alignment precision by
5.5 point, while decreased recall by 1.8 point, and
the overall alignment F-measure is increased by
1.3 point. When looking into the alignment links
which are removed during the alignment link fil-
tering process, we found that 80% of the removed
links (1320 out of 1661 links) are incorrect align-
ments, For A-E alignment, it increased the pre-
cision by 3 points while reducing recall by 0.5
points, and the alignment F-measure is increased
by about 1.5 points absolute, a 10% relative align-
ment error rate reduction. Similarly, 90% of the
removed links are incorrect alignments.
5 Translation
We evaluate the improved alignment on sev-
eral Chinese-English and Arabic-English machine
translation tasks. The documents to be trans-
lated are from difference genres: newswire (NW)
and web-blog (WB). The MT system is a phrase-
based SMT system as described in (Al-Onaizan
and Papineni, 2006). The training data are bilin-
gual sentence pairs with word alignment, from
which we obtained phrase translation pairs. We
extract phrase translation tables from the baseline
MaxEnt word alignment as well as the alignment
with confidence-based link filtering, then trans-
late the test set with each phrase translation ta-
ble. We measure the translation quality with au-
tomatic metrics including BLEU (Papineni et al,
2001) and TER (Snover et al, 2006). The higher
the BLEU score is, or the lower the TER score
is, the better the translation quality is. We com-
bine the two metrics into (TER-BLEU)/2 and try
to minimize it. In addition to the whole test set?s
scores, we also measure the scores of the ?tail?
documents, whose (TER-BLEU)/2 scores are at
the bottom 10 percentile (for A-E translation) and
20 percentile (for C-E translation) and are consid-
ered the most difficult documents to translate.
In the Chinese-English MT experiment, we se-
lected 40 NW documents, 41 WB documents as
the test set, which includes 623 sentences with
16667 words. The training data includes 333 thou-
sand C-E sentence pairs subsampled from 10 mil-
lion sentence pairs according to the test data. Ta-
bles 5 and 6 show the newswire and web-blog
translation scores as well as the number of phrase
translation pairs obtained from each alignment.
Because the alignment link filtering removes many
incorrect alignment links, the number of phrase
translation pairs is reduced by 15%. For newswire,
the translation quality is improved by 0.44 on the
whole test set and 1.1 on the tail documents, as
measured by (TER-BLEU)/2. For web-blog, we
observed 0.2 improvement on the whole test set
and 0.5 on the tail documents. The tail documents
typically have lower phrase coverage, thus incor-
rect phrase translation pairs derived from incorrect
937
# phrase pairs Average Tail
TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2
Baseline 934206 60.74 28.05 16.35 69.02 17.83 25.60
ALF 797685 60.33 28.52 15.91 68.31 19.27 24.52
Table 5: Improved Chinese-English Newswire Translation with Alignment Link Filtering
# phrase pairs Average Tail
TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2
Baseline 934206 62.87 25.08 18.89 66.55 18.80 23.88
ALF 797685 62.30 24.89 18.70 65.97 19.25 23.36
Table 6: Improved Chinese-English Web-Blog Translation with Alignment Link Filtering
alignment links are more likely to be selected. The
removal of incorrect alignment links and cleaner
phrase translation pairs brought more gains on the
tail documents.
In the Arabic-English MT, we selected 80 NW
documents and 55 WB documents. The NW train-
ing data includes 319 thousand A-E sentence pairs
subsampled from 7.2 million sentence pairs with
word alignments. The WB training data includes
240 thousand subsampled sentence pairs. Tables 7
and 8 show the corresponding translation results.
Similarly, the phrase table size is significantly re-
duced by 35%, while the gains on the tail docu-
ments range from 0.6 to 1.4. On the whole test
set the difference is smaller, 0.07 for the newswire
translation and 0.58 for the web-blog translation.
6 Related Work
In the machine translation area, most research on
confidence measure focus on the confidence of
MT output: how accurate a translated sentence is.
(Gandrabur and Foster, 2003) used neural-net to
improve the confidence estimate for text predic-
tions in a machine-assisted translation tool. (Ueff-
ing et al, 2003) presented several word-level con-
fidence measures for machine translation based on
word posterior probabilities. (Blatz et al, 2004)
conducted extensive study incorporating various
sentence-level and word-level features thru multi-
layer perceptron and naive Bayes algorithms for
sentence and word confidence estimation. (Quirk,
2004) trained a sentence level confidence mea-
sure using a human annotated corpus. (Bach et
al., 2008) used the sentence-pair confidence scores
estimated with source and target language mod-
els to weight phrase translation pairs. However,
there has been little research focusing on confi-
dence measure for word alignment. This work
is the first attempt to address the alignment con-
fidence problem.
Regarding word alignment combination, in ad-
dition to the commonly used ?intersection-union-
refine? approach (Och and Ney, 2003), (Ayan
and Dorr, 2006b) and (Ayan et al, 2005) com-
bined alignment links from multiple word align-
ment based on a set of linguistic and alignment
features within the MaxEnt framework or a neural
net model. While in this paper, the alignment links
are combined based on their confidence scores and
alignment agreement ratios.
(Fraser and Marcu, 2007) discussed the impact
of word alignment?s precision and recall on MT
quality. Here removing low confidence links re-
sults in higher precision and slightly lower recall
for the alignment. In our phrase extraction, we
allow extracting phrase translation pairs with un-
aligned functional words at the boundary. This is
similar to the ?loose phrases? described in (Ayan
and Dorr, 2006a), which increased the number of
correct phrase translations and improved the trans-
lation quality. On the other hand, removing incor-
rect content word links produced cleaner phrase
translation tables. When translating documents
with lower phrase coverage (typically the ?tail?
documents), high quality phrase translations are
particularly important because a bad phrase trans-
lation can be picked up more easily due to limited
phrase translation pairs available.
7 Conclusion
In this paper we presented two alignment confi-
dence measures for word alignment. The first is
the sentence alignment confidence measure, based
on which the best whole sentence alignment is se-
938
# phrase pairs Average Tail
TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2
Baseline 939911 43.53 50.51 -3.49 53.14 40.60 6.27
ALF 618179 43.11 50.24 -3.56 51.75 42.05 4.85
Table 7: Improved Arabic-English Newswire Translation with Alignment Link Filtering
# phrase pairs Average Tail
TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2
Baseline 598721 49.91 39.90 5.00 57.30 30.98 13.16
ALF 383561 48.94 40.00 4.42 55.99 31.92 12.04
Table 8: Improved Arabic-English Web-Blog Translation with Alignment Link Filtering
lected among multiple alignments and it obtained
0.8 F-measure improvement over the single best
Chinese-English aligner. The second is the align-
ment link confidence measure, which selects the
most reliable links from multiple alignments and
obtained 1.5 F-measure improvement. When we
removed low confidence links from the MaxEnt
aligner, we reduced the Chinese-English align-
ment error by 5% and the Arabic-English align-
ment error by 10%. The cleaned alignment sig-
nificantly reduced the size of phrase translation ta-
bles by 15-35%. It furthermore led to better trans-
lation scores for Chinese and Arabic documents
with different genres. In particular, it improved the
translation scores of the tail documents by 0.5-1.4
points measured by the combined metric of (TER-
BLEU)/2.
For future work we would like to explore richer
models to estimate alignment posterior probabil-
ity. In most cases, exact calculation by summing
over all possible alignments is impossible, and ap-
proximation using N-best alignments is needed.
Acknowledgments
We are grateful to Abraham Ittycheriah, Yaser Al-
Onaizan, Niyu Ge and Salim Roukos and anony-
mous reviewers for their constructive comments.
This work was supported in part by the DARPA
GALE project, contract No. HR0011-08-C-0110.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 529?536, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Necip Fazil Ayan and Bonnie J. Dorr. 2006a. Going
beyond aer: An extensive analysis of word align-
ments and their impact on mt. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 9?16,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Necip Fazil Ayan and Bonnie J. Dorr. 2006b. A max-
imum entropy approach to combining word align-
ments. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Con-
ference, pages 96?103, New York City, USA, June.
Association for Computational Linguistics.
Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.
2005. Neuralign: Combining word alignments us-
ing neural networks. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 65?72, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Nguyen Bach, Qin Gao, and Stephan Vogel. 2008. Im-
proving word alignment with language model based
confidence scores. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
151?154, Columbus, Ohio, June. Association for
Computational Linguistics.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In COLING ?04:
Proceedings of the 20th international conference on
Computational Linguistics, page 315, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The Mathe-
matic of Statistical Machine Translation: Parameter
Estimation. Computational Linguistics, 19(2):263?
311.
939
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Comput. Linguist., 33(3):293?303.
Simona Gandrabur and George Foster. 2003. Confi-
dence estimation for translation prediction. In Pro-
ceedings of the seventh conference on Natural lan-
guage learning at HLT-NAACL 2003, pages 95?102,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Niyu Ge. 2004. Max-posterior hmm alignment
for machine translation. In Presentation given at
DARPA/TIDES NIST MT Evaluation workshop.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 89?96, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a Method for Automatic
Evaluation of Machine Translation. In ACL ?02:
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 311?
318, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Chris Quirk. 2004. Training a sentence-level machine
translation confidence measure. In In Proc. LREC
2004, pages 825?828, Lisbon, Portual. Springer-
Verlag.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003. Confidence measures for statistical machine
translation. In In Proc. MT Summit IX, pages 394?
401. Springer-Verlag.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics, pages 836?841, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Bing Zhao, Niyu Ge, and Kishore Papineni. 2005.
Inner-outer bracket models for word alignment us-
ing hidden blocks. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 177?184, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
940
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 495?503,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Distributional Representations for Handling Sparsity in Supervised
Sequence-Labeling
Fei Huang
Temple University
1805 N. Broad St.
Wachman Hall 324
tub58431@temple.edu
Alexander Yates
Temple University
1805 N. Broad St.
Wachman Hall 324
yates@temple.edu
Abstract
Supervised sequence-labeling systems in
natural language processing often suffer
from data sparsity because they use word
types as features in their prediction tasks.
Consequently, they have difficulty estimat-
ing parameters for types which appear in
the test set, but seldom (or never) ap-
pear in the training set. We demonstrate
that distributional representations of word
types, trained on unannotated text, can
be used to improve performance on rare
words. We incorporate aspects of these
representations into the feature space of
our sequence-labeling systems. In an ex-
periment on a standard chunking dataset,
our best technique improves a chunker
from 0.76 F1 to 0.86 F1 on chunks begin-
ning with rare words. On the same dataset,
it improves our part-of-speech tagger from
74% to 80% accuracy on rare words. Fur-
thermore, our system improves signifi-
cantly over a baseline system when ap-
plied to text from a different domain, and
it reduces the sample complexity of se-
quence labeling.
1 Introduction
Data sparsity and high dimensionality are the twin
curses of statistical natural language processing
(NLP). In many traditional supervised NLP sys-
tems, the feature space includes dimensions for
each word type in the data, or perhaps even combi-
nations of word types. Since vocabularies can be
extremely large, this leads to an explosion in the
number of parameters. To make matters worse,
language is Zipf-distributed, so that a large frac-
tion of any training data set will be hapax legom-
ena, very many word types will appear only a few
times, and many word types will be left out of
the training set alogether. As a consequence, for
many word types supervised NLP systems have
very few, or even zero, labeled examples from
which to estimate parameters.
The negative effects of data sparsity have been
well-documented in the NLP literature. The per-
formance of state-of-the-art, supervised NLP sys-
tems like part-of-speech (POS) taggers degrades
significantly on words that do not appear in the
training data, or out-of-vocabulary (OOV) words
(Lafferty et al, 2001). Performance also degrades
when the domain of the test set differs from the do-
main of the training set, in part because the test set
includes more OOV words and words that appear
only a few times in the training set (henceforth,
rare words) (Blitzer et al, 2006; Daume? III and
Marcu, 2006; Chelba and Acero, 2004).
We investigate the use of distributional repre-
sentations, which model the probability distribu-
tion of a word?s context, as techniques for find-
ing smoothed representations of word sequences.
That is, we use the distributional representations
to share information across unannotated examples
of the same word type. We then compute features
of the distributional representations, and provide
them as input to our supervised sequence label-
ers. Our technique is particularly well-suited to
handling data sparsity because it is possible to im-
prove performance on rare words by supplement-
ing the training data with additional unannotated
text containing more examples of the rare words.
We provide empirical evidence that shows how
distributional representations improve sequence-
labeling in the face of data sparsity.
Specifically, we investigate empirically the
effects of our smoothing techniques on two
sequence-labeling tasks, POS tagging and chunk-
ing, to answer the following:
1. What is the effect of smoothing on sequence-
labeling accuracy for rare word types? Our best
smoothing technique improves a POS tagger by
11% on OOV words, and a chunker by an impres-
sive 21% on OOV words.
495
2. Can smoothing improve adaptability to new do-
mains? After training our chunker on newswire
text, we apply it to biomedical texts. Remark-
ably, we find that the smoothed chunker achieves
a higher F1 on the new domain than the baseline
chunker achieves on a test set from the original
newswire domain.
3. How does our smoothing technique affect sam-
ple complexity? We show that smoothing drasti-
cally reduces sample complexity: our smoothed
chunker requires under 100 labeled samples to
reach 85% accuracy, whereas the unsmoothed
chunker requires 3500 samples to reach the same
level of performance.
The remainder of this paper is organized as fol-
lows. Section 2 discusses the smoothing problem
for word sequences, and introduces three smooth-
ing techniques. Section 3 presents our empirical
study of the effects of smoothing on two sequence-
labeling tasks. Section 4 describes related work,
and Section 5 concludes and suggests items for fu-
ture work.
2 Smoothing Natural Language
Sequences
To smooth a dataset is to find an approximation of
it that retains the important patterns of the origi-
nal data while hiding the noise or other compli-
cating factors. Formally, we define the smoothing
task as follows: let D = {(x, z)|x is a word se-
quence, z is a label sequence} be a labeled dataset
of word sequences, and let M be a machine learn-
ing algorithm that will learn a function f to pre-
dict the correct labels. The smoothing task is to
find a function g such that when M is applied to
D? = {(g(x), z)|(x, z) ? D}, it produces a func-
tion f ? that is more accurate than f .
For supervised sequence-labeling problems in
NLP, the most important ?complicating factor?
that we seek to avoid through smoothing is the
data sparsity associated with word-based represen-
tations. Thus, the task is to find g such that for
every word x, g(x) is much less sparse, but still
retains the essential features of x that are useful
for predicting its label.
As an example, consider the string ?Researchers
test reformulated gasolines on newer engines.? In
a common dataset for NP chunking, the word ?re-
formulated? never appears in the training data, but
appears four times in the test set as part of the
NP ?reformulated gasolines.? Thus, a learning al-
gorithm supplied with word-level features would
have a difficult time determining that ?reformu-
lated? is the start of a NP. Character-level features
are of little help as well, since the ?-ed? suffix is
more commonly associated with verb phrases. Fi-
nally, context may be of some help, but ?test? is
ambiguous between a noun and verb, and ?gaso-
lines? is only seen once in the training data, so
there is no guarantee that context is sufficient to
make a correct judgment.
On the other hand, some of the other contexts
in which ?reformulated? appears in the test set,
such as ?testing of reformulated gasolines,? pro-
vide strong evidence that it can start a NP, since
?of? is a highly reliable indicator that a NP is to
follow. This example provides the intuition for our
approach to smoothing: we seek to share informa-
tion about the contexts of a word across multiple
instances of the word, in order to provide more in-
formation about words that are rarely or never seen
in training. In particular, we seek to represent each
word by a distribution over its contexts, and then
provide the learning algorithm with features com-
puted from this distribution. Importantly, we seek
distributional representations that will provide fea-
tures that are common in both training and test
data, to avoid data sparsity. In the next three sec-
tions, we develop three techniques for smoothing
text using distributional representations.
2.1 Multinomial Representation
In its simplest form, the context of a word may be
represented as a multinomial distribution over the
terms that appear on either side of the word. If V is
the vocabulary, or the set of word types, and X is a
sequence of random variables over V , the left and
right context of Xi = v may each be represented
as a probability distribution over V: P (Xi?1|Xi =
v) and P (Xi+1|X = v) respectively.
We learn these distributions from unlabeled
texts in two different ways. The first method com-
putes word count vectors for the left and right con-
texts of each word type in the vocabulary of the
training and test texts. We also use a large col-
lection of additional text to determine the vectors.
We then normalize each vector to form a proba-
bility distribution. The second technique first ap-
plies TF-IDF weighting to each vector, where the
context words of each word type constitute a doc-
ument, before applying normalization. This gives
greater weight to words with more idiosyncratic
distributions and may improve the informativeness
of a distributional representation. We refer to these
techniques as TF and TF-IDF.
496
To supply a sequence-labeling algorithm with
information from these distributional representa-
tions, we compute real-valued features of the con-
text distributions. In particular, for every word
xi in a sequence, we provide the sequence labeler
with a set of features of the left and right contexts
indexed by v ? V: F leftv (xi) = P (Xi?1 = v|xi)
and F rightv (xi) = P (Xi+1 = v|xi). For exam-
ple, the left context for ?reformulated? in our ex-
ample above would contain a nonzero probability
for the word ?of.? Using the features F(xi), a se-
quence labeler can learn patterns such as, if xi has
a high probability of following ?of,? it is a good
candidate for the start of a noun phrase. These
features provide smoothing by aggregating infor-
mation across multiple unannotated examples of
the same word.
2.2 LSA Model
One drawback of the multinomial representation
is that it does not handle sparsity well enough,
because the multinomial distributions themselves
are so high-dimensional. For example, the two
phrases ?red lamp? and ?magenta tablecloth?
share no words in common. If ?magenta? is never
observed in training, the fact that ?tablecloth? ap-
pears in its right context is of no help in connecting
it with the phrase ?red lamp.? But if we can group
similar context words together, putting ?lamp? and
?tablecloth? into a category for household items,
say, then these two adjectives will share that cat-
egory in their context distributions. Any pat-
terns learned for the more common ?red lamp?
will then also apply to the less common ?magenta
tablecloth.? Our second distributional represen-
tation aggregates information from multiple con-
text words by grouping together the distributions
P (xi?1 = v|xi = w) and P (xi?1 = v?|xi = w)
if v and v? appear together with many of the same
words w. Aggregating counts in this way smooths
our representations even further, by supplying bet-
ter estimates when the data is too sparse to esti-
mate P (xi?1|xi) accurately.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is a widely-used technique for comput-
ing dimensionality-reduced representations from a
bag-of-words model. We apply LSA to the set of
right context vectors and the set of left context vec-
tors separately, to find compact versions of each
vector, where each dimension represents a com-
bination of several context word types. We nor-
malize each vector, and then calculate features as
above. After experimenting with different choices
for the number of dimensions to reduce our vec-
tors to, we choose a value of 10 dimensions as the
one that maximizes the performance of our super-
vised sequence labelers on held-out data.
2.3 Latent Variable Language Model
Representation
To take smoothing one step further, we present
a technique that aggregates context distributions
both for similar context words xi?1 = v and v?,
and for similar words xi = w and w?. Latent
variable language models (LVLMs) can be used to
produce just such a distributional representation.
We use Hidden Markov Models (HMMs) as the
main example in the discussion and as the LVLMs
in our experiments, but the smoothing technique
can be generalized to other forms of LVLMs, such
as factorial HMMs and latent variable maximum
entropy models (Ghahramani and Jordan, 1997;
Smith and Eisner, 2005).
An HMM is a generative probabilistic model
that generates each word xi in the corpus con-
ditioned on a latent variable Yi. Each Yi in the
model takes on integral values from 1 to S, and
each one is generated by the latent variable for the
preceding word, Yi?1. The distribution for a cor-
pus x = (x1, . . . , xN ) given a set of state vectors
y = (y1, . . . , yN ) is given by:
P (x|y) =
?
i
P (xi|yi)P (yi|yi?1)
Using Expectation-Maximization (Dempster et
al., 1977), it is possible to estimate the distribu-
tions for P (xi|yi) and P (yi|yi?1) from unlabeled
data. We use a trained HMM to determine the op-
timal sequence of latent states y?i using the well-
known Viterbi algorithm (Rabiner, 1989). The
output of this process is an integer (ranging from 1
to S) for every word xi in the corpus; we include a
new boolean feature for each possible value of yi
in our sequence labelers.
To compare our models, note that in the multi-
nomial representation we directly model the prob-
ability that a word v appears before a word w:
P (xi?1 = v|xi = w)). In our LSA model, we find
latent categories of context words z, and model the
probability that a category appears before the cur-
rent word w: P (xi?1 = z|xi = w). The HMM
finds (probabilistic) categories Y for both the cur-
rent word xi and the context word xi?1, and mod-
els the probability that one category follows the
497
other: P (Yi|Yi?1). Thus the HMM is our most
extreme smoothing model, as it aggregates infor-
mation over the greatest number of examples: for
a given consecutive pair of words xi?1, xi in the
test set, it aggregates over all pairs of consecutive
words x?i?1, x?i where x?i?1 is similar to xi?1 and
x?i is similar to xi.
3 Experiments
We tested the following hypotheses in our experi-
ments:
1. Smoothing can improve the performance of
a supervised sequence labeling system on words
that are rare or nonexistent in the training data.
2. A supervised sequence labeler achieves greater
accuracy on new domains with smoothing.
3. A supervised sequence labeler has a better sam-
ple complexity with smoothing.
3.1 Experimental Setup
We investigate the use of smoothing in two test
systems, conditional random field (CRF) models
for POS tagging and chunking. To incorporate
smoothing into our models, we follow the follow-
ing general procedure: first, we collect a set of
unannotated text from the same domain as the test
data set. Second, we train a smoothing model on
the text of the training data, the test data, and the
additional collection. We then automatically an-
notate both the training and test data with features
calculated from the distributional representation.
Finally, we train the CRF model on the annotated
training set and apply it to the test set.
We use an open source CRF software package
designed by Sunita Sajarwal and William W. Co-
hen to implement our CRF models.1 We use a set
of boolean features listed in Table 1.
Our baseline CRF system for POS tagging fol-
lows the model described by Lafferty et al(2001).
We include transition features between pairs of
consecutive tag variables, features between tag
variables and words, and a set of orthographic fea-
tures that Lafferty et al found helpful for perfor-
mance on OOV words. Our smoothed models add
features computed from the distributional repre-
sentations, as discussed above.
Our chunker follows the system described by
Sha and Pereira (2003). In addition to the tran-
sition, word-level, and orthographic features, we
include features relating automatically-generated
POS tags and the chunk labels. Unlike Sha and
1Available from http://sourceforge.net/projects/crf/
CRF Feature Set
Transition zi=z
zi=z and zi?1=z?
Word xi=w and zi=z
POS ti=t and zi=z
Orthography for every s ? {-ing, -ogy, -
ed, -s, -ly, -ion, -tion, -ity},
suffix(xi)= s and zi=z
xi is capitalized and zi = z
xi has a digit and zi = z
TF, TF-IDF, and
LSA features
for every context type v,
F leftv (xi) and F rightv (xi)
HMM features yi=y and zi = z
Table 1: Features used in our CRF systems. zi vari-
ables represent labels to be predicted, ti represent tags (for
the chunker), and xi represent word tokens. All features are
boolean except for the TF, TF-IDF, and LSA features.
Pereira, we exclude features relating consecutive
pairs of words and a chunk label, or features re-
lating consecutive tag labels and a chunk label,
in order to expedite our experiments. We found
that including such features does improve chunk-
ing F1 by approximately 2%, but it also signifi-
cantly slows down CRF training.
3.2 Rare Word Accuracy
For these experiments, we use the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993). Following the CoNLL shared task from
2000, we use sections 15-18 of the Penn Treebank
for our labeled training data for the supervised
sequence labeler in all experiments (Tjong et al,
2000). For the tagging experiments, we train and
test using the gold standard POS tags contained in
the Penn Treebank. For the chunking experiments,
we train and test with POS tags that are automati-
cally generated by a standard tagger (Brill, 1994).
We tested the accuracy of our models for chunking
and POS tagging on section 20 of the Penn Tree-
bank, which corresponds to the test set from the
CoNLL 2000 task.
Our distributional representations are trained on
sections 2-22 of the Penn Treebank. Because we
include the text from the train and test sets in our
training data for the distributional representations,
we do not need to worry about smoothing them
? when they are decoded on the test set, they
498
Freq: 0 1 2 0-2 all
#Samples 438 508 588 1534 46661
Baseline .62 .77 .81 .74 .93
TF .76 .72 .77 .75 .92
TF-IDF .82 .75 .76 .78 .94
LSA .78 .80 .77 .78 .94
HMM .73 .81 .86 .80 .94
Table 2: POS tagging accuracy: our HMM-smoothed
tagger outperforms the baseline tagger by 6% on rare
words. Differences between the baseline and the HMM are
statistically significant at p < 0.01 for the OOV, 0-2, and all
cases using the two-tailed Chi-squared test with 1 degree of
freedom.
will not encounter any previously unseen words.
However, to speed up training during our exper-
iments and, in some cases, to avoid running out
of memory, we replaced words appearing twice or
fewer times in the data with the special symbol
*UNKNOWN*. In addition, all numbers were re-
placed with another special symbol. For the LSA
model, we had to use a more drastic cutoff to fit
the singular value decomposition computation into
memory: we replaced words appearing 10 times or
fewer with the *UNKNOWN* symbol. We initial-
ize our HMMs randomly. We run EM ten times
and take the model with the best cross-entropy on
a held-out set. After experimenting with differ-
ent variations of HMM models, we settled on a
model with 80 latent states as a good compromise
between accuracy and efficiency.
For our POS tagging experiments, we measured
the accuracy of the tagger on ?rare? words, or
words that appear at most twice in the training
data. For our chunking experiments, we focus on
chunks that begin with rare words, as we found
that those were the most difficult for the chunker
to identify correctly. So we define ?rare? chunks
as those that begin with words appearing at most
twice in training data. To ensure that our smooth-
ing models have enough training data for our test
set, we further narrow our focus to those words
that appear rarely in the labeled training data, but
appear at least ten times in sections 2-22. Tables 2
and 3 show the accuracy of our smoothed models
and the baseline model on tagging and chunking,
respectively. The line for ?all? in both tables indi-
cates results on the complete test set.
Both our baseline tagger and chunker achieve
respectable results on their respective tasks for
all words, and the results were good enough for
Freq: 0 1 2 0-2 all
#Samples 133 199 231 563 21900
Baseline .69 .75 .81 .76 .90
TF .70 .82 .79 .77 .89
TF-IDF .77 .77 .80 .78 .90
LSA .84 .82 .83 .84 .90
HMM .90 .85 .85 .86 .93
Table 3: Chunking F1: our HMM-smoothed chunker
outperforms the baseline CRF chunker by 0.21 on chunks
that begin with OOV words, and 0.10 on chunks that be-
gin with rare words.
us to be satisfied that performance on rare words
closely follows how a state-of-the-art supervised
sequence-labeler behaves. The chunker?s accuracy
is roughly in the middle of the range of results for
the original CoNLL 2000 shared task (Tjong et
al., 2000) . While several systems have achieved
slightly higher accuracy on supervised POS tag-
ging, they are usually trained on larger training
sets.
As expected, the drop-off in the baseline sys-
tem?s performance from all words to rare words
is impressive for both tasks. Comparing perfor-
mance on all terms and OOV terms, the baseline
tagger?s accuracy drops by 0.31, and the baseline
chunker?s F1 drops by 0.21. Comparing perfor-
mance on all terms and rare terms, the drop is less
severe but still dramatic: 0.19 for tagging and 0.15
for chunking.
Our hypothesis that smoothing would improve
performance on rare terms is validated by these ex-
periments. In fact, the more aggregation a smooth-
ing model performs, the better it appears to be at
smoothing. The HMM-smoothed system outper-
forms all other systems in all categories except
tagging on OOV words, where TF-IDF performs
best. And in most cases, the clear trend is for
HMM smoothing to outperform LSA, which in
turn outperforms TF and TF-IDF. HMM tagging
performance on OOV terms improves by 11%, and
chunking performance by 21%. Tagging perfor-
mance on all of the rare terms improves by 6%,
and chunking by 10%. In chunking, there is a
clear trend toward larger increases in performance
as words become rarer in the labeled data set, from
a 0.02 improvement on words of frequency 2, to an
improvement of 0.21 on OOV words.
Because the test data for this experiment is
drawn from the same domain (newswire) as the
499
training data, the rare terms make up a relatively
small portion of the overall dataset (approximately
4% of both the tagged words and the chunks).
Still, the increased performance by the HMM-
smoothed model on the rare-word subset con-
tributes in part to an increase in performance on
the overall dataset of 1% for tagging and 3% for
chunking. In our next experiment, we consider
a common scenario where rare terms make up a
much larger fraction of the test data.
3.3 Domain Adaptation
For our experiment on domain adaptation, we fo-
cus on NP chunking and POS tagging, and we
use the labeled training data from the CoNLL
2000 shared task as before. For NP chunking, we
use 198 sentences from the biochemistry domain
in the Open American National Corpus (OANC)
(Reppen et al, 2005) as or our test set. We man-
ually tagged the test set with POS tags and NP
chunk boundaries. The test set contains 5330
words and a total of 1258 NP chunks. We used
sections 15-18 of the Penn Treebank as our labeled
training set, including the gold standard POS tags.
We use our best-performing smoothing model, the
HMM, and train it on sections 13 through 19 of
the Penn Treebank, plus the written portion of
the OANC that contains journal articles from bio-
chemistry (40,727 sentences). We focus on chunks
that begin with words appearing 0-2 times in the
labeled training data, and appearing at least ten
times in the HMM?s training data. Table 4 con-
tains our results. For our POS tagging experi-
ments, we use 561 MEDLINE sentences (9576
words) from the Penn BioIE project (PennBioIE,
2005), a test set previously used by Blitzer et
al.(2006). We use the same experimental setup as
Blitzer et al: 40,000 manually tagged sentences
from the Penn Treebank for our labeled training
data, and all of the unlabeled text from the Penn
Treebank plus their MEDLINE corpus of 71,306
sentences to train our HMM. We report on tagging
accuracy for all words and OOV words in Table
5. This table also includes results for two previous
systems as reported by Blitzer et al (2006): the
semi-supervised Alternating Structural Optimiza-
tion (ASO) technique and the Structural Corre-
spondence Learning (SCL) technique for domain
adaptation.
Note that this test set for NP chunking con-
tains a much higher proportion of rare and OOV
words: 23% of chunks begin with an OOV word,
and 29% begin with a rare word, as compared with
Baseline HMM
Freq. # R P F1 R P F1
0 284 .74 .70 .72 .80 .89 .84
1 39 .85 .87 .86 .92 .88 .90
2 39 .79 .86 .83 .92 .90 .91
0-2 362 .75 .73 .74 .82 .89 .85
all 1258 .86 .87 .86 .91 .90 .91
Table 4: On biochemistry journal data from the OANC,
our HMM-smoothed NP chunker outperforms the base-
line CRF chunker by 0.12 (F1) on chunks that begin with
OOV words, and by 0.05 (F1) on all chunks. Results in
bold are statistically significantly different from the baseline
results at p < 0.05 using the two-tailed Fisher?s exact test.
We did not perform significance tests for F1.
All Unknown
Model words words
Baseline 88.3 67.3
ASO 88.4 70.9
SCL 88.9 72.0
HMM 90.5 75.2
Table 5: On biomedical data from the Penn BioIE
project, our HMM-smoothed tagger outperforms the
SCL tagger by 3% (accuracy) on OOV words, and by
1.6% (accuracy) on all words. Differences between the
smoothed tagger and the SCL tagger are significant at p <
.001 for all words and for OOV words, using the Chi-squared
test with 1 degree of freedom.
1% and 4%, respectively, for NP chunks in the test
set from the original domain. The test set for tag-
ging also contains a much higher proportion: 23%
OOV words, as compared with 1% in the original
domain. Because of the increase in the number of
rare words, the baseline chunker?s overall perfor-
mance drops by 4% compared with performance
on WSJ data, and the baseline tagger?s overall per-
formance drops by 5% in the new domain.
The performance improvements for both the
smoothed NP chunker and tagger are again im-
pressive: there is a 12% improvement on OOV
words, and a 10% overall improvement on rare
words for chunking; the tagger shows an 8% im-
provement on OOV words compared to out base-
line and a 3% improvement on OOV words com-
pared to the SCL model. The resulting perfor-
mance of the smoothed NP chunker is almost iden-
tical to its performance on the WSJ data. Through
smoothing, the chunker not only improves by 5%
500
in F1 over the baseline system on all words, it in
fact outperforms our baseline NP chunker on the
WSJ data. 60% of this improvement comes from
improved accuracy on rare words.
The performance of our HMM-smoothed chun-
ker caused us to wonder how well the chunker
could work without some of its other features. We
removed all tag features and all features for word
types that appear fewer than 20 times in training.
This chunker achieves 0.91 F1 on OANC data, and
0.93 F1 on WSJ data, outperforming the baseline
system in both cases. It has only 20% as many fea-
tures as the baseline chunker, greatly improving
its training time. Thus our smoothing features are
more valuable to the chunker than features from
POS tags and features for all but the most common
words. Our results point to the exciting possibil-
ity that with smoothing, we may be able to train a
sequence-labeling system on a small labeled sam-
ple, and have it apply generally to other domains.
Exactly what size training set we need is a ques-
tion that we address next.
3.4 Sample Complexity
Our complete system consists of two learned com-
ponents, a supervised CRF system and an unsu-
pervised smoothing model. We measure the sam-
ple complexity of each component separately. To
measure the sample complexity of the supervised
CRF, we use the same experimental setup as in
the chunking experiment on WSJ text, but we vary
the amount of labeled data available to the CRF.
We take ten random samples of a fixed size from
the labeled training set, train a chunking model on
each subset, and graph the F1 on the labeled test
set, averaged over the ten runs, in Figure 1. To
measure the sample complexity of our HMM with
respect to unlabeled text, we use the full labeled
training set and vary the amount of unlabeled text
available to the HMM. At minimum, we use the
text available in the labeled training and test sets,
and then add random subsets of the Penn Tree-
bank, sections 2-22. For each subset size, we take
ten random samples of the unlabeled text, train an
HMM and then a chunking model, and graph the
F1 on the labeled test set averaged over the ten
runs in Figure 2.
The results from our labeled sample complex-
ity experiment indicate that sample complexity is
drastically reduced by HMM smoothing. On rare
chunks, the smoothed system reaches 0.78 F1 us-
ing only 87 labeled training sentences, a level that
the baseline system never reaches, even with 6933
baseline (all)
HMM (all)
HMM (rare)
0.6
0.7
0.8
0.9
1
F1
 (C
hu
nk
in
g)
Labeled Sample Complexity
baseline (rare)
0.2
0.3
0.4
0.5
1 10 100 1000 10000
F1
 (C
hu
nk
in
g)
Number of Labeled Sentences (log scale)
Figure 1: The smoothed NP chunker requires less than
10% of the samples needed by the baseline chunker to
achieve .83 F1, and the same for .88 F1.
Baseline (all)
HMM (all) HMM (rare)
0.80
0.85
0.90
0.95
F1
 (C
hu
nk
in
g)
Unlabeled Sample Complexity
Baseline (rare)
0.70
0.75
0 10000 20000 30000 40000
F1
 (C
hu
nk
in
g)
Number of Unannotated Sentences
Figure 2: By leveraging plentiful unannotated text, the
smoothed chunker soon outperforms the baseline.
labeled sentences. On the overall data set, the
smoothed system reaches 0.83 F1 with 50 labeled
sentences, which the baseline does not reach un-
til it has 867 labeled sentences. With 434 labeled
sentences, the smoothed system reaches 0.88 F1,
which the baseline system does not reach until it
has 5200 labeled samples.
Our unlabeled sample complexity results show
that even with access to a small amount of unla-
beled text, 6000 sentences more than what appears
in the training and test sets, smoothing using the
HMM yields 0.78 F1 on rare chunks. However, the
smoothed system requires 25,000 more sentences
before it outperforms the baseline system on all
chunks. No peak in performance is reached, so
further improvements are possible with more unla-
beled data. Thus smoothing is optimizing perfor-
mance for the case where unlabeled data is plenti-
ful and labeled data is scarce, as we would hope.
4 Related Work
To our knowledge, only one previous system ?
the REALM system for sparse information extrac-
501
tion ? has used HMMs as a feature represen-
tation for other applications. REALM uses an
HMM trained on a large corpus to help determine
whether the arguments of a candidate relation are
of the appropriate type (Downey et al, 2007). We
extend and generalize this smoothing technique
and apply it to common NLP applications involv-
ing supervised sequence-labeling, and we provide
an in-depth empirical analysis of its performance.
Several researchers have previously studied
methods for using unlabeled data for tagging and
chunking, either alone or as a supplement to la-
beled data. Ando and Zhang develop a semi-
supervised chunker that outperforms purely su-
pervised approaches on the CoNLL 2000 dataset
(Ando and Zhang, 2005). Recent projects in semi-
supervised (Toutanova and Johnson, 2007) and un-
supervised (Biemann et al, 2007; Smith and Eis-
ner, 2005) tagging also show significant progress.
Unlike these systems, our efforts are aimed at us-
ing unlabeled data to find distributional represen-
tations that work well on rare terms, making the
supervised systems more applicable to other do-
mains and decreasing their sample complexity.
HMMs have been used many times for POS
tagging and chunking, in supervised, semi-
supervised, and in unsupervised settings (Banko
and Moore, 2004; Goldwater and Griffiths, 2007;
Johnson, 2007; Zhou, 2004). We take a novel per-
spective on the use of HMMs by using them to
compute features of each token in the data that
represent the distribution over that token?s con-
texts. Our technique lets the HMM find param-
eters that maximize cross-entropy, and then uses
labeled data to learn the best mapping from the
HMM categories to the POS categories.
Smoothing in NLP usually refers to the prob-
lem of smoothing n-gram models. Sophisticated
smoothing techniques like modified Kneser-Ney
and Katz smoothing (Chen and Goodman, 1996)
smooth together the predictions of unigram, bi-
gram, trigram, and potentially higher n-gram se-
quences to obtain accurate probability estimates in
the face of data sparsity. Our task differs in that we
are primarily concerned with the case where even
the unigram model (single word) is rarely or never
observed in the labeled training data.
Sparsity for low-order contexts has recently
spurred interest in using latent variables to repre-
sent distributions over contexts in language mod-
els. While n-gram models have traditionally dom-
inated in language modeling, two recent efforts de-
velop latent-variable probabilistic models that ri-
val and even surpass n-gram models in accuracy
(Blitzer et al, 2005; Mnih and Hinton, 2007).
Several authors investigate neural network mod-
els that learn not just one latent state, but rather a
vector of latent variables, to represent each word
in a language model (Bengio et al, 2003; Emami
et al, 2003; Morin and Bengio, 2005).
One of the benefits of our smoothing technique
is that it allows for domain adaptation, a topic
that has received a great deal of attention from
the NLP community recently. Unlike our tech-
nique, in most cases researchers have focused on
the scenario where labeled training data is avail-
able in both the source and the target domain
(e.g., (Daume? III, 2007; Chelba and Acero, 2004;
Daume? III and Marcu, 2006)). Our technique uses
unlabeled training data from the target domain,
and is thus applicable more generally, including
in web processing, where the domain and vocab-
ulary is highly variable, and it is extremely diffi-
cult to obtain labeled data that is representative of
the test distribution. When labeled target-domain
data is available, instance weighting and similar
techniques can be used in combination with our
smoothing technique to improve our results fur-
ther, although this has not yet been demonstrated
empirically. HMM-smoothing improves on the
most closely related work, the Structural Corre-
spondence Learning technique for domain adap-
tation (Blitzer et al, 2006), in experiments.
5 Conclusion and Future Work
Our study of smoothing techniques demonstrates
that by aggregating information across many
unannotated examples, it is possible to find ac-
curate distributional representations that can pro-
vide highly informative features to supervised se-
quence labelers. These features help improve se-
quence labeling performance on rare word types,
on domains that differ from the training set, and
on smaller training sets.
Further experiments are of course necessary
to investigate distributional representations as
smoothing techniques. One particularly promis-
ing area for further study is the combination of
smoothing and instance weighting techniques for
domain adaptation. Whether the current tech-
niques are applicable to structured prediction
tasks, like parsing and relation extraction, also de-
serves future attention.
502
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In ACL.
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
C. Biemann, C. Giuliano, and A. Gliozzo. 2007. Un-
supervised pos tagging supporting supervised meth-
ods. Proceeding of RANLP-07.
J. Blitzer, A. Globerson, and F. Pereira. 2005. Dis-
tributed latent variable models of lexical cooccur-
rences. In Proceedings of the Tenth International
Workshop on Artificial Intelligence and Statistics.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
E. Brill. 1994. Some Advances in Rule-Based Part of
Speech Tagging. In AAAI, pages 722?727, Seattle,
Washington.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In EMNLP.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310?318, Morristown, NJ, USA. Association
for Computational Linguistics.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society of Information Science, 41(6):391?407.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
Doug Downey, Stefan Schoenmackers, and Oren Et-
zioni. 2007. Sparse information extraction: Unsu-
pervised language models to the rescue. In ACL.
A. Emami, P. Xu, and F. Jelinek. 2003. Using a
connectionist model in a syntactical based language
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 372?
375.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2-3):245?273.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers. In EMNLP.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
In Proceedings of the International Conference on
Machine Learning.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine Learning, pages 641?648, New York,
NY, USA. ACM.
F. Morin and Y. Bengio. 2005. Hierarchical probabilis-
tic neural network language model. In Proceedings
of the International Workshop on Artificial Intelli-
gence and Statistics, pages 246?252.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu/.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
285.
Randi Reppen, Nancy Ide, and Keith Suderman. 2005.
American national corpus (ANC) second release.
Linguistic Data Consortium.
F. Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
Human Language Technology - NAACL.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 354?362, Ann Arbor, Michigan, June.
Erik F. Tjong, Kim Sang, and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the 4th Conference on
Computational Natural Language Learning, pages
127?132.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian LDA-based model for semi-supervised
part-of-speech tagging. In NIPS.
GuoDong Zhou. 2004. Discriminative hidden Markov
modeling with long state dependence using a kNN
ensemble. In COLING.
503
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 492?500,
Beijing, August 2010
Feature-Rich Discriminative Phrase Rescoring for SMT 
Fei Huang   and    Bing Xiang 
 
IBM T. J. Watson Research Center 
{huangfe, bxiang}@us.ibm.com  
Abstract 
This paper proposes a new approach to 
phrase rescoring for statistical machine 
translation (SMT).  A set of novel fea-
tures capturing the translingual equiva-
lence between a source and a target 
phrase pair are introduced. These features 
are combined with linear regression 
model and neural network to predict the 
quality score of the phrase translation 
pair. These phrase scores are used to dis-
criminatively rescore the baseline MT 
system?s phrase library: boost good 
phrase translations while prune bad ones. 
This approach not only significantly im-
proves machine translation quality, but 
also reduces the model size by a consid-
erable margin. 
1 Introduction 
Statistical Machine Translation (SMT) systems, 
including phrase-based (Och and Ney 2002; 
Koehn et. al. 2003), syntax-based (Yamada and 
Knight 2001; Galley et. al. 2004) or hybrid sys-
tems (Chiang 2005; Zollmann and Venugopal 
2006), are typically built with bilingual phrase 
pairs, which are extracted from parallel sentences 
with word alignment. Due to the noises in the 
bilingual sentence pairs and errors from auto-
matic word alignment, the extracted phrase pairs 
may contain errors, such as  
? dropping content words  
(the $num countries ,||?:<null>),  
? length mismatch  
                 (along the lines of the || ?:of)  
? content irrelevance  
          (the next $num years, || 
??:level ??:aspect ?:<null>) 
   These incorrect phrase pairs compete with cor-
rect phrase pairs during the decoding process, 
and are often selected when their counts are high 
(if they contain systematic alignment errors) or 
certain model costs are low (for example, when 
some source content words are translated into 
target function words in an incorrect phrase pair, 
the language model cost of the incorrect pair may 
be small, making it more likely that the pair will 
be selected for the final translation). As a result, 
the translation quality is degraded when these 
incorrect phrase pairs are selected. 
Various approaches have been proposed over 
the past decade for the purpose of improving the 
phrase pair quality for SMT. For example, a term 
weight based model was presented in (Zhao, et 
al., 2004) to rescore phrase translation pairs. It 
models the translation probability with similari-
ties between the query (source phrase) and 
document (target phrase). Significant improve-
ment was obtained in the translation performance. 
In (Johnson, et al, 2007; Yang and Zheng, 2009), 
a statistical significance test was used to heavily 
prune the phrase table and thus achieved higher 
precision and better MT performance. 
In (Deng, et al, 2008), a generic phrase train-
ing algorithm was proposed with the focus on 
phrase extraction.  Multiple feature functions are 
utilized based on information metrics or word 
alignment. The feature parameters are optimized 
to directly maximize the end-to-end system per-
formance. Significant improvement was reported 
for a small MT task. But when the phrase table is 
large, such as in a large-scale SMT system, the 
computational cost of tuning with this approach 
will be high due to many iterations of phrase ex-
traction and re-decoding. 
In this paper we attempt to improve the quality 
of the phrase table using discriminative phrase 
rescoring method. We develop extensive set of 
features capturing the equivalence of bilingual 
492
phrase pairs. We combine these features using 
linear and nonlinear models in order to predict 
the quality of phrase pairs. Finally we boost the 
score of good phrases while pruning bad phrases. 
This approach not only significantly improves 
the translation quality, but also reduces the 
phrase table size by 16%. 
The paper is organized as follows: in section 2 
we discuss two regression models for phrase pair 
quality prediction: linear regression and neural 
network. In section 3 we introduce the rich set of 
features. We describe how to obtain the training 
data for supervised learning of the two models in 
section 4. Section 5 presents some approaches to 
discriminative phrase rescoring using these 
scores, followed by experiments on model re-
gression and machine translation in section 6. 
2 Problem Formulation 
Our goal is to predict the translation quality of a 
given bilingual phrase pair based on a set of 
features capturing their similarities. These 
features are combined with linear regression 
model and neural network. The training data for 
both models are derived from phrase pairs 
extracted from small amount of parallel 
sentences with hand alignment and machine 
alignment. Details are given in section 4. 
2.1 Linear regression model 
In the linear regression model, the predicted 
phrase pair quality score is defined as 
 
?=
i
ii feffeSco ),(),( ?  (1) 
where ),( fef i is the feature for the phrase pair 
(e,f), as to be defined in section 3. These feature 
values can be binary (0/1), integers or real val-
ues. ? s are the feature weights to be learned 
from training data. The phrase pair quality score 
in the training data is defined as the sum of the 
target phrase?s BLEU score (Papineni et. al. 
2002) and the source phrase?s BLEU score, 
where the reference translation is obtained from 
phrase pairs extracted from human alignment. 
Details about the training data are given in sec-
tion 4. The linear regression model is trained us-
ing a statistical package R1. After training, the 
                                               
1
 http://www.r-project.org/ 
learned feature weights are applied on a held-out 
set of phrase pairs with known quality scores to 
evaluate the model?s regression accuracy. 
2.2 Neural Network model 
A feed-forward back-propagation network (Bry-
son and Ho, 1969) is created with one hidden 
layer and 20 nodes. During training, the phrase 
pair features are fed into the network with their 
quality scores as expected outputs. After certain 
iterations of training, the neural net?s weights are 
stable and its mean square error on the training 
set has been significantly reduced.  Then the 
learned network weights are fixed, and are ap-
plied to the test phrase pairs for regression accu-
racy evaluation. We use MatLab??s neural net 
toolkit for training and test.   
      We will compare both models? prediction 
accuracy in section 6. We would like to know 
whether the non-linear regression model outper-
forms linear regression model in terms of score 
prediction error, and if fewer regression errors 
correspond to better translation quality. 
3 Feature Description 
In this section we will describe the features we 
use to model the equivalence of a bilingual 
phrase pair (e,f). These features are defined on 
the phrase pair, its compositional units (words 
and characters), attributes (POS tags, numbers), 
co-occurrence frequency, length ratio, coverage 
ratio and alignment pattern.  
? Phrase : )|( efPp , )|( fePp   
)(
),()|( fC
feCfePp =   (2) 
where ),( feC is the co-occurrence frequency of 
the phrase pair (e,f), and C(f) is the occurrence 
frequency of the source phrase f. )|( efPp is 
defined similarly. 
 
? Word : )|( efPw , )|( fePw    
?=
i
jijw fetfeP )|(max)|(   (3) 
where )|( ji fet  is the lexical translation prob-
ability. This is similar to the word-level phrase 
493
translation probability, as typically calculated in 
SMT systems (Brown et. al. 1993). Here we use 
max instead of sum. )|( efPw is calculated simi-
larly. 
? Character: )|( efPc , )|( fePc  
   When the source or target words are composed 
of smaller units, such as characters for Chinese 
words, or prefix/stem/suffix for Arabic words, 
we can calculate their translation probability on 
the sub-unit level. This is helpful for languages 
where the meaning of a word is closely related to 
its compositional units, such as Chinese and 
Arabic. 
?=
i
ninc cetfeP )|(max)|(  (4) 
where nc is the n-th character in the source 
phrase  f  (n=1,?,N). 
? POS tag: )|( efPt , )|( fePt  
   In addition to the probabilities estimated at the 
character, word and phrase levels based on the 
surface forms, we also compute the POS-based 
phrase translation probabilities.  For each source 
and target word in a phrase pair, we automati-
cally label their POS tags. Then POS-based 
probabilities are computed in a way similar to the 
calculation of the word-level phrase translation 
probability (formula 3). It is believed that such 
syntactic information can help to distinguish 
good phrase pairs from bad ones (for example, 
when a verb is aligned to a noun, its POS transla-
tion probability should be low). 
? Length ratio 
   This feature computes the ratio of the number 
of content words in the source and target phrases. 
It is designed to penalize phrases where content 
words in the source phrase are dropped in the 
target phrase (or vice versa). The ratio is defined 
to be 10 if the target phrase has zero content 
word while the source phrase has non-zero con-
tent words.  If neither phrase contains a content 
word, the ratio is defined to be 1.  
? Log frequency 
   This feature takes the logarithm of the co-
occurrence frequency of the phrase pair. High 
frequency phrase pairs are more likely to be cor-
rect translations if they are not due to systematic 
alignment errors. 
? Coverage ratio 
   We propose this novel feature based on the 
observation that if a phrase pair is a correct trans-
lation, it often includes correct sub-phrase pair 
translations (decomposition). Similarly a correct 
phrase pair will also appear in correct longer 
phrase pair translations (composition) unless it is 
a very long phrase pair itself. Formally we define 
the coverage ratio of a phrase pair (e,f) as: 
 
),(),(),( feCovfeCovfeCov cd += . (5) 
 
Here ),( feCovd is the decomposition coverage: 
? ?
?
?
?
?
?
=
ff
Pf
Pfe
i
d
i
Li
Lii
ee
feCov
)(*,
)(
1
),(
),( ,  (6) 
where if  is a sub-phrase of  f, and ( ie , if ) is a 
phrase pair in  the MT system?s bilingual phrase 
library LP . ),( 21 ee?  is defined to be 1 
if 21 ee ? , otherwise it is 0.  For each source 
sub-phrase if , this formula calculates the ratio 
that its target translation ie  is also a sub-phrase 
of the target phrase e, then the ratio is summed 
over all the source sub-phrases.  
Similarly the composition coverage is defined 
as  
? ?
?
?
?
?
?
=
j
L
j
L
jj
ff
Pf
Pfe
j
c
ee
feCov
)(*,
),(
1
),(
),(   (7) 
where jf is any source phrase containing f  and 
je  is one of jf ?s translations in LP . We call 
jf a super-phrase of f. For each source super-
phrase jf , this formula calculates the ratio that 
its target translation je  is also a super-phrase of 
the target phrase e, then the ratio is summed over 
all the source super-phrases.  
Short phrase pairs (such as a phrase pair with 
one source word translating into one target word) 
have less sub-phrases but more super-phrases 
(for long phrase pairs, it is the other way around).  
494
Combining the two coverage factors produces 
balanced coverage ratio, not penalizing too short 
or too long phrases.  
? Number match 
   During preprocessing of the training data, 
numbers are mapped into a special token ($num) 
for better generalization. Typically one number 
corresponds to one special token. During transla-
tion numbers should not be arbitrarily dropped or 
inserted. Therefore we can check whether the 
source and target phrases have the right number 
of $num to be matched. If they are the same the 
number match feature has value 1, otherwise it  
is 0. 
? Alignment pattern 
   This feature calculates the number of unaligned 
content words in a given phrase pair, where word 
alignment is obtained simply based on the maxi-
mum lexical translation probability of the source 
(target) word given all the target (source) words 
in the phrase pair.  
 
Among the above 13 features, the number 
match feature is a binary feature, the alignment 
pattern feature is an integer-value feature, and 
the rest are real-value features. Also note that 
most features are positively correlated with the 
phrase translation quality (the greater the feature 
value, the more likely it is a correct phrase trans-
lation) except the alignment pattern feature, 
where more unaligned content words corre-
sponds to bad phrase translations. 
4 Training Data  
The training data for both the linear regression 
and neural network models are bilingual phrase 
pairs with the above 13 feature values as well as 
their expected phrase quality scores. The feature 
values can be computed according to the 
description in section 3. The expected translation 
quality score for the phrase pair (e,f) is defined as 
)|,()|,(),( ** effBleufeeBleufeB +=
 (8) 
where *e is the human translation of the source 
phrase f, and *f is the human translation of the 
target phrase e. These human translations are 
obtained from hand alignment of some parallel 
sentences. 
1. Given hand alignment of some bilingual 
sentence pairs, extract gold phrase 
translation pairs. 
2. Apply automatic word alignment on the 
same bilingual sentences, and extract 
phrase pairs. Note that due to the word 
alignment errors, the extracted phrase 
pairs are noisy.  
3. For each phrase pair (e, f) in the noisy 
phrase table, find whether the source 
phrase f also appears in the gold phrase 
table as (e*, f). If so, use the correspond-
ing target phrase(s) e* as reference trans-
lation(s) to evaluate the BLEU score of 
the target phrase e in the noisy phrase ta-
ble. 
4. Similarly, for each e in (e, f), identify (e, 
f*) in the gold phrase table and compute 
the BLEU score of f using f* as the ref-
erence. 
5. The sum of the above two BLEU scores 
is the phrase pair?s translation quality 
score.   
5 Phrase Rescoring 
Given the bilingual phrase pairs? quality score, 
there are several ways to use them for statistical 
machine translation.  
5.1 Quality score as a decoder feature 
A straightforward way is to use the quality scores 
as an additional feature in the SMT system, com-
bined with other features (phrase scores, word 
scores, distortion scores, LM scores etc.) for MT 
hypotheses scoring. The feature weight can be 
empirically learned using manual tuning or 
automatic tuning such as MERT (Och 2003). In 
this situation, all the phrase pairs and their qual-
ity scores are stored in the MT system, which is 
different from the following approach where in-
correct phrase translations are pruned. 
5.2 Discriminative phrase rescoring 
Another approach is to select good and bad 
phrase pairs based on their predicted quality 
scores, then discriminatively rescore the phrase 
pairs in the baseline phrase library.  We sort the 
phrase pairs based on their quality scores in a 
decreasing order. The bottom N phrase pairs are 
495
considered as incorrect translations and pruned 
from the phrase library. The top M phrase pairs 
MP  are considered as good phrases with correct 
translations. As identifying correct sub-phrase 
translation requires accurate word alignment 
within phrase pairs, which is not easy to obtain 
due to the lack of rich context information within 
the phrase pair, we only boost the good phrase 
pairs? super-phrases in the phrase library. Given 
a phrase pair (e,f) with phrase co-occurrence 
count C(e,f), the weighted co-occurrence count is 
defined as: 
?
?
=
),(),(
),(),('
fefe
i
ii
bfeCfeC   (9) 
where ( ii fe , ) is a good sub-phrase pair of (e,f) 
belonging to MP , with quality score ib . Note 
that if (e,f) contains multiple good sub-phrase 
pairs, its co-occurrence count will be boosted 
multiple times. Here the boost factor is defined 
as the product of quality scores of good sub-
phrase pairs. Instead of product, one can also use 
sum, which did not perform as well in our ex-
periments. The weighted co-occurrence count is 
used to calculate the new phrase translation 
scores:  
?= )(*,'
),(')|(' fC
feCfeP  (10) 
?= ,*)('
),(')|('
eC
feC
efP  (11) 
which replace the original phrase translation 
scores in the SMT system. In addition to phrase 
co-occurrence count rescoring, the quality scores 
can also be used to rescore word translation lexi-
cons by updating word co-occurrence counts ac-
cordingly.  
6 Experiments 
We conducted several experiments to evaluate 
the proposed phrase rescoring approach. First we 
evaluate the two regression models? quality score 
prediction accuracy. Secondly, we apply the pre-
dicted phrase scores on machine translation tasks. 
We will measure the improvement on translation 
quality as well as the reduction of model size. 
Our experiments are on English-Chinese transla-
tion.  
  
6.1 Regression model evaluation 
We select 10K English-Chinese sentence pairs 
with both hand alignment and automatic HMM 
alignment, and extract 106K phrase pairs with 
true phrase translation quality scores as com-
puted according to formula 8. We choose 53K 
phrase pairs for regression model training and 
another 53K phrase pairs for model evaluation. 
There are 14 parameters to be learned (13 feature 
weights plus an intercept parameter) for the lin-
ear regression model, and 280 weights ( 2013?   
MSE of Phrase Pair Quality Scores
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
phr
t2s
cha
rt2
s cov alig
n
num log
fq
wo
rdt
2s
pos
t2s
pos
s2t
len
gth
wo
rds
2t
pha
rs2
t
cha
rs2
t
 
Figure 1. Linear regression model phrase pair pre-
diction MSE curve. Errors are significantly reduced 
when more features are introduced (phrs2t /phrt2s: 
phrase source-to-target/target-to-source features; 
words2t/wordt2s: word-level; chars2t/chart2s: 
character-level; poss2t/post2s: POS-level; cov: cov-
erage ratio; align: alignment pattern; logfq: log fre-
quency; num: number match; length: length ratio). 
 
 
Figure 2. Neural network model phrase pair predic-
tion MSE curve. Errors are significantly reduced 
with more training iterations.  
 
496
for the input weight matrix plus 120 ?  for the 
output weight vector) for the neural network 
model. In both cases, the training data size is 
much more than the parameters size, so there is 
no data sparseness problem.  
   After the model parameters are learned from 
the training data, we apply the regression model 
to the evaluation data set, then compute the 
phrase quality score prediction mean squared 
error (MSE, also known as the average residual 
sum of squares): 
[ ]2),(),(1 ? ?=
k
kktkkp feBfeBKMSE (12) 
where pB is the predicted quality score of the 
phrase pair ( kk fe , ), while tB is the true score 
calculated based on human translations. 
   Figure 1 shows the reduction of the regression 
error in the linear regression model trained with 
different features. One may find that the MSE is 
significantly reduced (from 0.78 to 0.70) when 
additional features are added into the regression 
model.  
Similarly, the neural network?s MSE curve is 
shown in Figure 2. It can be seen that the MSE is 
significantly reduced with more iterations of 
training (from the initial error of 1.33 to 0.42 
after 40 iterations). 
Table 2 shows some phrase pairs with 
high/low quality scores predicted by the linear 
regression model and the neural network. One 
can see that both models assign high scores to 
good phrase translations and low scores to noisy 
phrase pairs. Although the values of these scores 
are beyond the range of [0, 2] as defined in for-
mula 8, this is not a problem for our MT tasks, 
since they are only used as phrase boosting 
weights or pruning threshold. 
6.2 Machine translation evaluation 
We test the above phrase rescoring approach on 
English-Chinese machine translation. The SMT 
system is a phrase-based decoder similar to the 
description in (Tillman 2006), where various 
features are combined within the log-linear 
framework. These features include source-to-
target phrase translation score based on relative 
frequency, source-to-target and target-to-source 
word-to-word translation scores, language model 
score, distortion model scores and word count. 
The training data for these features are 10M Chi- 
 Linear Regression Neural Network  
Good  
phrase 
pairs 
 and|?|5.52327 
 amount|?? ??|4.03006 
 us|, ? -|3.91992 
 her husband|? ??|3.85536 
 the program|?? , ?|3.81078 
 the job|? ? ? ??|3.77406 
 shrine|; ????|3.74336 
 of course ,|, ?? , ? ?|3.7174 
 is only|? ? ? ?|3.69426 
 visit|?? ?|3.67256 
 facilities and|?? , ? ?|3.65402 
  rights|?? |6.96817 
  has become|? ?? |4.16468 
  why|??? |3.82629 
  by armed|? ?? |3.62988 
  o|O |3.47795 
  of drama|? ?? |3.36601 
  government and|?? ? |3.27347 
  introduction|?? |3.19113 
  heart disease|?? ?? |3.11829 
  heads|??? |3.05467 
  american consumers|?? ??? |2.99706 
Bad  
phrase 
pairs 
 as well|? ?|1.03234 
 closed|?? ??|1.01271 
 she was|???|0.99011 
 way|?? ??|0.955918 
 of a|? ? ?|0.914717 
 knowledge|??|0.875116 
 made|?? "|0.837358 
 the|?? ??|0.801142 
 end|??|0.769938 
 held|? ?? ?|0.742588 
  letter|?? ?? |0.39203 
  , though|?? ? |0.37020 
  levels of|? ? ?? |0.34892 
  - board|?? |0.32826 
  number of|? ?? |0.30499 
  indonesia|????? |0.27827 
  xinhua at|$num |0.24433 
  provinces|?? |0.20281 
  new .|?? ? ? ? , |0.15430 
  can|? ?? |0.09502 
Table 2. Examples of good and bad phrase pairs based on the linear regression model and neural network?s 
predicted quality scores. 
 
497
 BLEU NIST Phrase 
Table 
Size 
Baseline 38.67 9.3738 3.65M 
LR-mtfeat 39.31 9.5356 3.65M 
LR-boost (top30k) 39.36 9.5465 3.65M 
LR-prune (tail600k) 39.06 9.4890 3.05M 
LR-disc 
(top30K/tail600K) 
39.75 9.6388 3.05M 
NN-disc 
(top30K/tail600K) 
39.76 9.6547 3.05M 
LR-disc tuning 39.87 9.6594 3.05M 
Significance-prune 38.96 9.3953 3.01M 
Count-Prune 38.65 9.3549 3.05M 
 
Table 3. Translation quality improvements 
with rescored phrase tables. Best result (1.2 
BLEU gain) is obtained with discriminative res-
coring by boosting top 30K phrase pairs and 
pruning bottom 600K phrase pairs, with some 
weight tuning. 
 
nese-English sentence pairs, mostly newswire 
and UN corpora released by LDC. The parallel 
sentences have word alignment automatically 
generated with HMM and MaxEnt word aligner.  
Bilingual phrase translations are extracted from 
these word-aligned parallel corpora. Due to the 
noise in the bilingual sentence pairs and 
automatic word alignment errors, the phrase 
translation library contains many incorrect phrase 
translations, which lead to inaccurate translations, 
as seen in Figure 3.  
Our evaluation data is NIST MT08 English-
Chinese evaluation testset, which includes 1859 
sentences from 129 news documents. The auto-
matic metrics are BLEU and NIST scores, as 
used in the NIST 2008 English-Chinese MT 
evaluation. Note that as there is no whitespace as 
Chinese word boundary, the Chinese translations 
are segmented into characters before scoring in 
order to reduce the variance and errors caused by 
automatic word segmentation, which is also done 
in the NIST MT evaluation.  
Table 3 shows the automatic MT scores using 
the baseline phrase table and rescored phrase 
tables. When the phrase quality scores from the 
linear regression model are used as a separate 
feature in the SMT system (LR-mtfeat as de-
scribed in section 5.1), the improvement is 0.7 
BLEU points (0.16 in terms of NIST scores). By 
boosting the good phrase pairs (top 30K2 phrase 
pairs, LR-boost) from linear regression model, 
the MT quality is improved by 0.7 BLEU points 
over the baseline system. Pruning the bad phrase 
pairs (tail 600K phrase pairs) without using the 
quality scores as features (LR-prune) also im-
proves the MT by 0.4 BLEU points. Combining 
LR-boost and LR_prune, a discriminatively res-
cored phrase table (LR-disc) improved the BLEU 
score by 1.1 BLEU points, and reduce the phrase 
table size by 16% (from 3.6M to 3.0M phrase 
pairs). Manually tuning the boosting weights of 
good phrase pairs leads to additional improve-
ment. Discriminative rescoring using the neural 
net work scores (NN-disc) produced similar im-
provement. 
We also experiment with phrase table pruning 
using Fisher significant test, as proposed in 
(Johnson et. al. 2007). We tuned the pruning 
threshold for the best result. It shows that the 
significance pruning improves over the baseline 
by 0.3 BLEU pts with 17.5% reduction in phrase 
table, but is not as good as our proposed phrase 
rescoring method. In addition, we also show the 
MT result using a count pruning phrase table 
(Count-Prune) where 600K phrase translation 
pairs are pruned based on their co-occurrence 
counts. The MT performance of such phrase ta-
ble pruning is slightly worse than the baseline 
MT system, and significantly worse than the re-
sult using the proposed rescored phrase table. 
When comparing the linear regression and 
neural network models, we find rescoring with 
both models lead to similar MT improvements, 
even though the neural network model has much 
fewer regression errors (0.44 vs. 0.7 in terms of 
MSE). This is due to the rich parameter space of 
the neural network. 
Overall, the discriminative phrase rescoring 
improves the SMT quality by 1.2 BLEU points 
and reduces the phrase table size by 16%. With 
statistical significance test (Zhang and Vogel  
2004), all the improvements are statistically sig-
nificant with p-value < 0.0001.  
Figure 3 presents some English sentences, 
with phrase translation pairs selected in the final 
translations (the top one is from the baseline MT 
system and the bottom one is from the LR-disc 
system).  
                                               
2
 These thresholds are empirically chosen. 
498
 We find that incorrect phrase translations in the 
baseline system (as highlighted with blue bold 
font) are corrected and better translation results 
are obtained. 
7 Conclusion 
We introduced a discriminative phrase rescoring 
approach, which combined rich features with 
linear regression and neural network to predict 
phrase pair translation qualities. Based on these 
quality scores, we boost good phrase translations 
while pruning bad phrase translations. This led to 
statistically significant improvement (1.2 BLEU 
points) in MT and reduced phrase table size by 
16%. 
For the future work, we would like to explore 
other models for quality score prediction, such as 
SVM. We will want to try other approaches to 
utilize the phrase pair quality scores, in addition 
to rescoring the co-occurrence frequency. Finally, 
we will test this approach in other domain appli-
cations and language pairs. 
References 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, Robert L. Mercer. 1993.  The Mathe-
matics of Statistical Machine Translation: Parame-
ter Estimation, Computational Linguistics, v.19 n.2, 
June 1993. 
Arthur Earl Bryson, Yu-Chi Ho. 1969. Applied Opti-
mal Control: Optimization, Estimation, and Con-
trol. Blaisdell Publishing Company. p481. 
David Chiang. 2005. A Hierarchical Phrase-based 
Model for Statistical Machine Translation. 2005. 
In Proc. of ACL, pp. 263?270. 
Yonggang Deng, Jia Xu, and Yuqing Gao. 2008. 
Phrase Table Training for Precision and Recall: 
What Makes a Good Phrase and a Good Phrase 
Pair? In Proc. of ACL/HLT, pp. 81-88. 
Michel Galley, Mark Hopkins, Kevin Knight, Daniel 
Marcu. 2004. What's in a Translation Rule? In 
Proc. of NAACL 2004, pp. 273-280. 
Howard Johnson, Joel Martin, George Foster, and 
Roland Kuhn. 2007. Improving Translation Quality 
by Discarding Most of the Phrase Table. In Proc. 
of EMNLP-CoNLL, pp. 967-975. 
Src 
Baseline 
 
 
PhrResco 
 
Indonesian bird flu victim contracted virus indirectly: 
<indonesian bird flu|?? ???> <virus|??> <victim contracted|???> <indi-
rectly :|?? :> 
<indonesian bird flu|?? ???> <victim|???> <contracted|??> <virus|??
> <indirectly :|?? :> 
Src 
 
Baseline 
 
 
 
PhrResco 
The director of Palestinian human rights group Al-Dhamir, Khalil Abu Shammaleh, said 
he was also opposed to the move. 
<the director of|?? ?> <palestinian|????> <human rights group|?? ??> 
<al -|" ?? " ??> <,|,> <abu|Abu> <khalil|Khalil> <, said he was|? 
? , ?> <also opposed to|? ??> <the move .|? ? ?? ?> 
<the director of|?? ?> <palestinian|????> <human rights group|?? ??> 
<al -|al -> <, khalil|, khalil> <abu|??> <, said he was|? , ?> <also opposed to|? 
??> <the move .|? ? ?? ?> 
Src 
Baseline 
 
 
 
PhrResco 
A young female tourist and two of her Kashmiri friends were among the victims. 
<a young female|? ? ? ?? ??> <tourist and|?? ?> <$num of her|? ? 
$num ?> <kashmiri|????> <friends were|??> <among the|?? ?> <victims 
.|??? ?> 
<a young|? ? ?? ?> <female|??> <tourist and|?? ?> <$num of her|? ? 
$num ?> <kashmiri|????> <friends were|??> <among the|?? ?> <victims 
.|??? ?> 
Figure 3.  Examples of English sentences and their translation, with phrase pairs from baseline sys-
tem and phrase rescored system. Highlighted text are initial phrase translation errors which are cor-
rected in the PhrResco translations. 
 
499
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. 
Statistical Phrase-based Translation, In Proc. of 
NAACL, pp. 48-54. 
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models, Computational Linguistics, v.29 n.1, 
pp.19-51, March 2003   
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation, In Proc. of ACL, 
2003, pp. 160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation, In Proc.  of 
ACL, pp. 311-318. 
Christoph Tillmann. 2006. Efficient Dynamic Pro-
gramming Search Algorithms for Phrase-based 
SMT. In Proc. of the Workshop CHPSLP at 
HLT'06. 
Kenji Yamada and Kevin Knight. 2001. A Syntax-
based Statistical Translation Model, In Proc.  of 
ACL, pp.523-530. 
Mei Yang and Jing Zheng. 2009. Toward Smaller, 
Faster, and Better Hierarchical Phrase-based 
SMT. In Proc. of ACL-IJCNLP, pp. 237-240. 
Ying Zhang and Stephan Vogel. 2004. Measuring 
Confidence Intervals for the Machine Translation 
Evaluation Metrics, In Proc. TMI, pp. 4-6. 
Bing Zhao, Stephan Vogel, and Alex Waibel. 2004. 
Phrase Pair Rescoring with Term Weighting for 
Statistical Machine Translation. In Proc. of 
EMNLP, pp. 206-213. 
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax Augmented Machine Translation via Chart 
Parsing. In Proc. of NAACL 2006- Workshop on 
statistical machine translation. 
 
 
 
 
500
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1313?1323, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Biased Representation Learning for Domain Adaptation
Fei Huang , Alexander Yates
Temple University
Computer and Information Sciences
324 Wachman Hall
Philadelphia, PA 19122
{fhuang,yates}@temple.edu
Abstract
Representation learning is a promising tech-
nique for discovering features that allow su-
pervised classifiers to generalize from a source
domain dataset to arbitrary new domains. We
present a novel, formal statement of the rep-
resentation learning task. We argue that be-
cause the task is computationally intractable
in general, it is important for a representa-
tion learner to be able to incorporate expert
knowledge during its search for helpful fea-
tures. Leveraging the Posterior Regularization
framework, we develop an architecture for in-
corporating biases into representation learn-
ing. We investigate three types of biases, and
experiments on two domain adaptation tasks
show that our biased learners identify signif-
icantly better sets of features than unbiased
learners, resulting in a relative reduction in er-
ror of more than 16% for both tasks, with re-
spect to existing state-of-the-art representation
learning techniques.
1 Introduction
Supervised natural language processing (NLP) sys-
tems have been widely used and have achieved im-
pressive performance on many NLP tasks. Howev-
er, they exhibit a significant drop-off in performance
when tested on domains that differ from their train-
ing domains. (Gildea, 2001; Sekine, 1997; Pradhan
et al 2007) One major cause for poor performance
on out of-domain texts is the traditional representa-
tion used by supervised NLP systems (Ben-David et
al., 2007). Most systems depend on lexical features,
which can differ greatly between domains, so that
important words in the test data may never be seen
in the training data. The connection between word-
s and labels may also change across domains. For
instance, ?signaling? appears only as a present par-
ticiple (VBG) in WSJ text (as in, ?signaling that...?),
but predominantly as a noun (as in ?signaling path-
way?) in biomedical text.
Recently, several authors have found that learning
new features based on distributional similarity can
significantly improve domain adaptation (Blitzer et
al., 2006; Huang and Yates, 2009; Turian et al
2010; Dhillon et al 2011). This framework is at-
tractive for several reasons: experimentally, learned
features can yield significant improvements over s-
tandard supervised models on out-of-domain test-
s. Moreover, since the representation-learning tech-
niques are unsupervised, they can easily be applied
to arbitrary new domains. There is no need to supply
additional labeled examples for each new domain.
Traditional representations still hold one signif-
icant advantage over representation-learning, how-
ever: because features are hand-crafted, these rep-
resentations can readily incorporate the linguistic
or domain expert knowledge that leads to state-of-
the-art in-domain performance. In contrast, the on-
ly guide for existing representation-learning tech-
niques is a corpus of unlabeled text.
To address this shortcoming, we introduce
representation-learning techniques that incorporate
a domain expert?s preferences over the learned fea-
tures. For example, out of the set of all possi-
ble distributional-similarity features, we might pre-
fer those that help predict the labels in a labeled
training data set. To capture this preference, we
might bias a representation-learning algorithm to-
wards features with low joint entropy with the labels
in the training data. This particular biased form of
1313
representation learning is a type of semi-supervised
learning that allows our system to learn task-specific
representations from a source domain?s training da-
ta, rather than the single representation for all tasks
produced by current, unsupervised representation-
learning techniques.
We present a novel formal statement of represen-
tation learning, and demonstrate that it is computa-
tionally intractable in general. It is therefore criti-
cal for representation learning to be flexible enough
to incorporate the intuitions and knowledge of hu-
man experts, to guide the search for representations
efficiently and effectively. Leveraging the Posteri-
or Regularization framework (Ganchev et al 2010),
we present an architecture for learning representa-
tions for sequence-labeling tasks that allows for bi-
ases. In addition to a bias towards task-specific rep-
resentations, we investigate a bias towards repre-
sentations that have similar features across domain-
s, to improve domain-independence; and a bias to-
wards multi-dimensional representations, where d-
ifferent dimensions are independent of one another.
In this paper, we focus on incorporating the bias-
es with HMM-type representations (Hidden Markov
Model). However, this technique can also be ap-
plied to other graphical model-based representations
with little modification. Our experiments show that
on two different domain-adaptation tasks, our biased
representations improve significantly over unbiased
ones. In a part-of-speech tagging experiment, our
best model provides a 25% relative reduction in er-
ror over a state-of-the-art Chinese POS tagger, and
a 19% relative reduction in error over an unbiased
representation from previous work.
The next section describes background and previ-
ous work. Section 3 introduces our framework for
learning biased representations. Section 4 describes
how we estimate parameters for the biased objective
functions efficiently. Section 5 details our experi-
ments and results, and section 6 concludes and out-
lines directions for future work.
2 Background and Previous Work
2.1 Terminology and Notation
A representation is a set of features that describe da-
ta points. Formally, given an instance set X , it is a
functionR : X ? Y for some suitable space Y (of-
ten Rd), which is then used as the input space for a
classifier. For instance, a traditional representation
for POS tagging over vocabulary V would include
(in part) |V | dimensions, and would map a word to a
binary vector with a 1 in only one of the dimensions.
By a structured representation, we mean a function
R that incorporates some form of joint inference. In
this paper, we use Viterbi decoding of variants of
Hidden Markov Models (HMMs) for our structured
representations, although our techniques are appli-
cable to arbitrary (Dynamic) Bayes Nets. A domain
is a probability distribution D over the instance set
X ; R(D) denotes the induced distribution over Y .
In domain adaptation tasks, a learner is given sam-
ples from a source domain DS , and is evaluated on
samples from a target domain DT .
2.2 Theoretical Background
Ben-David et al(2010) give a theoretical analysis
of domain adaptation which shows that the choice
of representation is crucial. A good choice is one
that minimizes error on the training data, but equally
important is that the representation must make data
from the two domains look similar. Ben-David et al
show that for every hypothesis h, we can provably
bound the error of h on the target domain by its error
on the source domain plus a measure of the distance
between DS and DT :
Ex?DTL(x,R, f, h) ? Ex?DSL(x,R, f, h)
+ d1(R(DS), R(DT ))
where L is a loss function, f is the target function,
and the variation divergence d1 is given by
d1(D,D
?) = 2 sup
B?B
|PrD[B]? PrD? [B]| (1)
where B is the set of measurable sets under D,D?.
2.3 Problem Formulation
Ben-David et als theory provides learning bound-
s for domain adaptation under a fixed R. We now
reformulate this theory to define the task of repre-
sentation learning for domain adaptation as the fol-
lowing optimization problem: given a set of unla-
beled instances US drawn from the source domain
and unlabeled instances UT from the target domain,
as well as a set of labeled instances LS drawn from
1314
the source domain, identify a function R? from the
space of possible representationsR:
R? = argmin
R?R
{min
h?H
(Ex?DSL(x,R, f, h))
+ d1(R(DS), R(DT ))}
(2)
Unlike most learning problems, where the repre-
sentation R is fixed, this problem formulation in-
volves a search over the space of representation-
s and hypotheses. The equation also highlights an
important underlying tension: the best representa-
tion for the source domain would naturally include
domain-specific features, and allow a hypothesis to
learn domain-specific patterns. We are aiming, how-
ever, for the best general classifier, that happens to
be trained on training data from one or a few do-
mains. Domain-specific features would contribute
to distance between domains, and to classifier errors
on data taken from unseen domains. By optimizing
for this combined objective function, we allow the
optimization method to trade off between features
that are best for classifying source-domain data and
features that allow generalization to new domains.
Naturally, the objective function in Equation 2 is
completely intractable. Just finding the optimal hy-
pothesis for a fixed representation of the training da-
ta is intractable for many hypothesis classes. And
the d1 metric is intractable to compute from samples
of a distribution, although Ben-David et alpropose
some tractable bounds (2007; 2010). We view Equa-
tion 2 as a high-level goal rather than a computable
objective. We leverage prior knowledge to bias the
representation learner towards attractive regions of
the representations space R, and we develop effi-
cient, greedy optimization techniques for learning
effective representations.
2.4 Previous Work
There is a long tradition of research on representa-
tions for NLP, mostly falling into one of three cat-
egories: 1) vector space models and dimensionality
reduction techniques (Salton and McGill, 1983; Tur-
ney and Pantel, 2010; Sahlgren, 2005; Deerwester et
al., 1990; Honkela, 1997) 2) using structured repre-
sentations to identify clusters based on distributional
similarity, and using those clusters as features (Lin
and Wu, 2009; Candito and Crabbe?, 2009; Huang
and Yates, 2009; Ahuja and Downey, 2010; Turi-
an et al 2010; Huang et al 2011); 3) and struc-
tured representations that induce multi-dimensional
real-valued features (Dhillon et al 2011; Emami et
al., 2003; Morin and Bengio, 2005). Our work fall-
s into the second category, but builds on the pre-
vious work by demonstrating how to improve the
distributional-similarity clusters with prior knowl-
edge. To our knowledge, we are the first to apply
semi-supervised representation learning techniques
for structured NLP tasks.
Most previous work on domain adaptation has fo-
cused on the case where some labeled data is avail-
able in both the source and target domains (Daume?
III, 2007; Jiang and Zhai, 2007; Daume? III et al
2010). Learning bounds are known (Blitzer et al
2007; Mansour et al 2009). A few authors have
considered domain adaptation with no labeled data
from the target domain (Blitzer et al 2006; Huang
et al 2011) by using features based on distributional
similarity. We demonstrate empirically that incorpo-
rating biases into this type of representation-learning
process can significantly improve results.
3 Biased Representation Learning
As before, let US and UT be unlabeled data, and LS
be labeled data from the source domain only. Pre-
vious work on representation learning with Hidden
Markov Models (HMMs) (Huang and Yates, 2009)
has estimated parameters ? for the HMM from un-
labeled data alone, and then determined the Viterbi-
optimal latent states for training and test data to pro-
duce new features for a supervised classifier. The
objective function for HMM learning in this case is
marginal log-likelihood, optimized using the Baum-
Welch algorithm:
L(?) =
?
x?US?UT
log
?
y
p(x,Y = y|?) (3)
where x is a sentence, Y is the sequence of latent
random variables for the sentence, and y is an in-
stance of the latent sequence. The joint distribution
in an HMM factors into observation and transition
distributions, typically mixtures of multinomials:
p(x,y|?) = P (y1)P (x1|y1)
?
i?2
P (yi|yi?1)P (xi|yi)
1315
Innocent
bystanders are often the
JJ NNS RB VBP DT
y1
y2 y3 y4 y5
...
victims
y6
NNS
Innocent
bystanders are often the victims
...
E?
entropy
(Y,z)
P(Y)
p
1
p
2
p
3
p
m
p
n
KL(p
m
|| p
n
)
Monday, March 26, 12
Figure 1: Illustration of how the entropy bias is incor-
porated into HMM learning. The dotted oval shows the
space of desired distributions in the hidden space, which
have small or zero entropy with the real labels. The learn-
ing algorithm aims to maximize the log-likelihood of the
unlabeled data, and to minimize the KL divergence be-
tween the real distribution, pm, and the closest desired
distribution, pn.
Intuitively, this form of representation learning i-
dentifies clusters of distributionally-similar words:
those words with the same Viterbi-optimal latent s-
tate. The Viterbi-optimal latent states are then used
as features for the supervised classifier. Our previ-
ous work (2009) has shown that the features from
the learned HMM significantly improve the accura-
cy of POS taggers and chunkers on benchmark do-
main adaptation datasets.
We use the HMM model from our previous work
(2009) as our baseline. Our techniques follow the
same general setup, as it provides an efficient and
empirically-proven starting point for exploring (one
part of) the space of possible representations. Note,
however, that the HMM on its own does not provide
even an approximate solution to the objective func-
tion in our problem formulation (Eqn. 2), since it
makes no attempt to find the representation that min-
imizes loss on labeled data. To address this and other
concerns, we modify the objective function for HM-
M training. Specifically, we encode biases for rep-
resentation learning by defining a set of properties ?
that we believe a good representation function would
minimize. One possible bias is that the HMM states
should be predictive of the labels in labeled training
data. We can encode this as a property that computes
the entropy between the HMM states and the label-
s. For example, in Figure 1, we want to learn the
best HMM distribution for the sentence ?Innocen-
t bystanders are often the victims? for POS tagging
task. The hidden sequence y1, y2, y3, y4, y5, y6 can
have any distribution p1, p2, p3, ..., pm, ..., pn from
the latent space Y . Since we are doing POS tagging,
we want the distribution to learn the information en-
coded in the original POS labels ?JJ NNS RB VBP
DT NNS?. Therefore, by calculating the entropy be-
tween the hidden sequence and real labels, we can
identify a subset of desired distributions that have
low entropy, shown in the dotted oval. By minimiz-
ing the KL divergence between the learned distribu-
tion and the set of desired distributions, we can find
the best distribution which is the closest to our de-
sire.
The following subsections describe the specific
properties we investigate; here we show how to in-
corporate them into the objective function. Let z
be the sequence of labels in LS , and let ?(x,y, z)
be a property of the completed data that we wish
the learned representation to minimize, based on our
prior beliefs. Let Q be the subspace of the possible
distributions over Y that have a small expected val-
ue for ?: Q = {q(Y)|EY?q[?(x,Y, z)] ? ?}, for
some constant ?. We then add penalty terms to the
objective function (3) for the divergence between the
HMM distribution p and the ?good? distributions q,
as well as for ?:
L(?)?min
q,?
[KL(q(Y)||p(Y|x, ?)) + ?|?|] (4)
s.t. EY?q[?(x,Y, z)] ? ? (5)
where KL is the Kullback-Leibler divergence, and
? is a free parameter indicating how important the
bias is compared with the marginal log likelihood.
To incorporate multiple biases, we define a vec-
tor of properties ?, and we constrain each property
?i ? ?i. Everything else remains the same, except
that in the penalty term ?|?|, the absolute value is
replaced with a suitable norm: ? ???. To allow our-
selves to place weights on the relative importance
of the different biases, we use a norm of the form
?x?A =
?
(xtAx), where A is a diagonal matrix
whose diagonal entries Aii are free parameters that
provide weights on the different properties. For our
1316
experiments, we set the free parameters ? and Aii
using a grid search over development data, as de-
scribed in Section 5.1
3.1 A Bias for Task-specific Representations
Current representation learning techniques are unsu-
pervised, so they will generate the exact same repre-
sentation for different tasks. Yet it is exceedingly
rare that two state-of-the-art NLP systems for differ-
ent tasks share the same feature set, even if they do
tend to share some core set of lexical features.
Traditional non-learned (i.e., manually-
engineered) representations essentially always
include task-specific features. In response, we
propose to bias our representation learning such
that the learned representations are optimized for a
specific task. In particular, we propose a property
that measures how difficult it is to predict the labels
in training data, given the learned latent states.
Our entropy property uses conditional entropy of
the labels given the latent state as the measure of
unpredictability:
?entropy(y, z) = ?
?
i
P? (yi, zi) log P? (zi|yi) (6)
where P? is the empirical probability and i indicates
the ith position in the data. We can plug this feature
into Equation 5 to obtain a new version of Equation
4 as an objective function for task-specific represen-
tations. We refer to this model as HMM+E. Un-
like previous formulations for supervised and semi-
supervised dimensionality reduction (Zhang et al
2007; Yang et al 2006), our framework works effi-
ciently for structured representations.
3.2 A Bias for Domain-Independent Features
Following the theory in Section 2.2, we devise a bi-
ased objective to provide an explicit mechanism for
minimizing the distance between the source and tar-
get domain. As before, we construct a property of
the completed data:
?distance(y) = d1(P?S , P?T )
where P?S(Y ) is the empirical distribution over la-
tent state values estimated from source-domain la-
tent states, and similarly for P?T (Y ). Essentially,
1Note that ?, unlike A and ?, is not a free parameter. It is
explicitly minimized in the modified objective function.
minimizing this property will bias the the represen-
tation towards features that appear approximately as
often in the source domain as the target domain. We
refer to the model trained with a bias of minimiz-
ing ?distance as HMM+D, and the model with both
?distance and ?entropy biases as HMM+D+E.
3.3 A Bias for Multi-Dimensional
Representations
Words are multidimensional objects. In English,
words can be nouns or verbs, singular or plural,
count or mass, just to name a few dimensions along
which they may vary. Factorial HMMs (FHMM-
s) (Ghahramani and Jordan, 1997) can learn multi-
dimensional models, but inference and learning are
complex and computationally expensive even in su-
pervised settings. Our previous work (2010) creat-
ed a multi-dimensional representation called an ?I-
HMM? by training several HMM layers indepen-
dently; we showed that by finding several latent cat-
egories for each word, this representation can pro-
vide useful and domain-independent features for su-
pervised learners. In this work, we also learn a sim-
ilar multi-dimensional model (I-HMM+D+E), but
within each layer we add in the two biases described
above. While more efficient than FHMMs, the draw-
back of these I-HMM-based models is that there
is no mechanism to encourage the different HMM
models to learn different things. As a result, the lay-
ers may produce similar or equivalent features de-
scribing the dominant aspect of distributional sim-
ilarity in the data, but miss features that are less
strong, but still important, in the data.
To encourage learning a truly multi-dimensional
representation, we add a bias towards I-HMM mod-
els in which each layer is different from all previ-
ous layers. We define an entropy-based predictabili-
ty property that measures how predictable each pre-
vious layer is, given the current one. Formally, let
yli denote the hidden state at the ith position in lay-
er l of the model. For a given layer l, this proper-
ty measures the conditional entropy of ym given yl,
summed over layers m < l, and subtracts this from
the maximum possible entropy:
?predictl (y) = MAX+
?
i;m<l
P? (yli, y
m
i ) log P? (y
m
i |y
l
i)
The entropy between layer l and the previous layer-
1317
s m measures how unpredictable the previous lay-
ers are, given layer l. By biasing the model such
that MAX minus the entropy approaches zero, we
encourage layer l towards completely different fea-
tures from previous layers. We call the model with
this bias P-HMM+D+E.
4 Efficient Parameter Estimation
Several machine learning paradigms have been de-
veloped recently for incorporating biases and con-
straints into parameter estimation (Liang et al
2009; Chang et al 2007; Mann and McCallum,
2007). We leverage the Posterior Regularization
(PR) framework for our problem because of its flex-
ibility in handling different kinds of biases; we pro-
vide a brief overview of the technique here, but see
(Ganchev et al 2010) for full details.
4.1 Overview of PR
PR introduces a modified EM algorithm to handle
constrained objectives, like Equation 4. The modi-
fied E-step estimates a distribution q(Y) that is close
to the current estimate of p(Y|x, ?), but also close
to the ideal set of distributions that (in expectation)
have ? = 0 for each property ?. The M step re-
mains the same, except that it re-estimates parame-
ters with respect to expected latent states computed
with q rather than p.
E step:
qt+1 = argmin
q
min
?
KL(q(Y)||p(Y|x, ?t)) + ? ???
s.t. Eq[?(x,Y, z)] ? ?
M step:
?t+1 = argmax
?
Eqt+1 [log p(x,Y|?
t))]
To make the optimization task in the E-step more
tractable, PR transforms it to a dual problem:
max
??0,??????
? log
?
Y
p(Y|x, ?) exp{????(x,Y, z)}
where ???? is the dual norm of ???. The gradient of
this dual objective is ?Eq[?(x,Y, z)]. A projected
subgradient descent algorithm is used to perform the
optimization.
4.2 Modifying ? for Tractability
In unstructured settings, this optimization problem
is relatively straightforward. However, for struc-
tured representations, we need to ensure that the
dynamic programming algorithms needed for infer-
ence remain tractable for the biased objectives. For
efficient PR over structured models, the properties ?
need to be decomposed as a sum over the cliques in
the structured model. Unfortunately, the properties
we mention above do not decompose so nicely, so
we must resort to approximations.
In order to efficiently compute the expected val-
ue of the entropy property with respect to Y ? q,
we need to be able to compute each componen-
t EYi?q[?
entropy(Yi, zi)] separately. Yet P? depends
on the setting of other latent states Yj in the corpus.
To avoid this problem, we pre-compute the expected
empirical distributions over the completed data. For
each specific value y and z:
P?q(y, z) =
1
|LS |
?
x
|x|?
i=1
1[zi = z]q(Yi = y)
P?q(y) =
1
|LS |
?
x
|x|?
i=1
q(Yi = y)
These expected empirical distributions P?q can be
computed efficiently using standard inference algo-
rithms, such as the forward algorithm for HMMs.
Note that P?q depends on q, but unlike the original
P? from Equation 6, they do not depend on the data
completions y. Thus we can compute P?q once for
each qt, and then substitute it for P? for all values
of Y in the computation of EY?q?entropy(Y, z),
making this computation tractable. For the entropy-
based predictability properties, the calculation is
similar, but instead of using the label z, we use the
decoded states yli from previous layers.
For the distance property, Ben-David et als anal-
ysis depends on a particular notion of distance (E-
qn. 1) that is computationally intractable. They also
propose more tractable lower bounds, but these are
again incompatible with the PR framework. Since
no computationally feasible exact algorithm exists
for this distance feature, we resort to a crude but ef-
ficient approximation of this measure: for each pos-
1318
sible value y of the latent states, we define:
?disty (y) =
?
i|xi?US
1[yi = y]q(Yi = y)
|US |
?
?
i|xi?UT
1[yi = y]q(Yi = y)
|UT |
Each of these individual properties is tractable for
structured models. Combining these properties us-
ing the ???A norm results in a Euclidean distance
(weighted byA) between the frequencies of features
in each domain, rather than d1 distance.
5 Experiments
We tested the structured representations with biases
on two NLP tasks: Chinese POS tagging and En-
glish NER. In both cases, we use a domain adapta-
tion setting where no labeled data is available for the
target domain ? a particularly difficult setting, but
one that provides a strong test for an NLP system?s
ability to generalize . In our work (Huang and Yates,
2009), we used a plain HMM for domain adaptation
tasks in which there is labeled source data and un-
labeled source and target data, but no labeled target
data for training. Therefore, here, we use the HMM
technique as a baseline, and build on it by including
biases.
5.1 Chinese POS tagging
We use the UCLA Corpus of Written Chinese,
which is a part of The Lancaster Corpus of Man-
darin Chinese (LCMC). The UCLA Corpus consists
of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres. We use gold-standard
word segmentation labels during training and test-
ing. The LCMC tagset consists of 50 Chinese POS
tags. Each genre averages 5284 word tokens, for a
total of 68,695 tokens among all genres. We use the
?news? genre as our source domain and randomly se-
lect 20% of every other genre as labeled test data. To
train our representation models, we use the ?news?
text, plus the remaining 80% of the texts from the
other genres. We use 90% of the labeled news text
for training, and 10% for development. We replace
hapax legomena in the unlabeled data with the spe-
cial symbol *UNKNOWN*, and also do the same
for word types in the labeled test sets that never ap-
pear in our unlabeled training texts.
0.888
0.893
0.898
0.903
0.908
0.913
0.918
0.923
0.928
0.1 1 10 100 1000
Ac
cu
rac
y 
? (log scale) 
News Domain (development data) 
alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100
Figure 2: Grid search for parameters on news text
Following our previous HMM setup in (Huang
and Yates, 2009) for consistency, we use an HMM
with 80 latent states. For our multi-layer models,
we use 7 layers of HMMs. We tuned the free pa-
rameters ? and A on development data. We varied
? from 0.1 to 1000. To tune A, we start by setting
the diagonal entry for ?entropy to 1, without loss of
generality. We then tie all the entries in A for ?disty
to a single parameter ?, and tie all of the entries for
?predicty to a parameter ?. We vary ? and ? over the
set {0.01,0.1,1,10,100}. Figure 2 shows our results
for ? and ? on news development data. A setting
of ? = 0.01 and ? = 100 performs best, with all
? = 100 doing reasonably well. Results for each
of these models on the general fiction test text con-
firm the general trends seen on development data ?
a comforting sign, since this indicates we can opti-
mize the free parameters on in-domain development
data, rather than requiring labeled data from the tar-
get domain. Our models tended to perform better
with increasing ? on development data, though with
diminishing returns. We pick the largest setting test-
ed, ? = 100, for our final models.
We use a linear-chain Conditional Random Field
(CRF) for our supervised classifier. To incorporate
the learned representations, we use the Viterbi Algo-
rithm to find the optimal latent state sequence from
each HMM-based model and then use the optimal
states as features in the CRF. Table 1 presents the
full list of features in the CRF. To handle Chinese,
we add in two features introduced in previous work
(Wang et al 2009): radical features and repeated
characters. A radical is a portion of a Chinese char-
acter that consists of a small number of pen or brush
strokes in a regular pattern.
1319
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
0.92
0.1 1 10 100 1000
Ac
cu
rac
y 
?ORJVFDOH 
General Fiction Domain (test data)  
alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100
Figure 3: Validating parameter settings on fiction text
CRF Feature Set
Transition
?z1[zj = z]
?z,z?1[zj = z and zj?1 = z?]
Word
?w,z1[xj = w and zj = z]
Radical
?z,r1[?c?xjradical(c) = r and zj = z]
Repeated Words
?A,B,z1[xj = AABB and zj = z]
?A,z1[(xj = AAor xj = AA/) and zj = z]
?A,B,z1[xj = ABAB and zj = z]
Features from Representation Learning
?y,l,z1[ylj = y and zj = z]
Table 1: Features used in our Chinese POS tagging
CRF systems. c represents a character within a word.
Table 2 shows our results. We compare against
the Baseline CRF without any additional representa-
tions and the unbiased HMM, a state-of-the-art do-
main adaptation technique from previous work, over
all 13 domains (source and target). We also com-
pare against a state-of-the-art Chinese POS tagger
for in-domain text, the CRF-based Stanford tagger
(Tseng et al 2005), retrained for this corpus. H-
MM+D+E outperforms the Stanford tagger on 10
out of 12 target domains and the unbiased HMM on
all domains, while the P-HMM+D+E outperform-
s the Stanford tagger (2.6% average improvement)
and HMM (1.7%) on all 12 target domains. The I-
HMM+D+E is slightly better than the HMM+D+E
(.3%), but incorporating the multi-dimensional bias
(P-HMM+D+E) adds an additional 0.6% improve-
ment.
Our interpretation for the success of I-
HMM+D+E and P-HMM+D+E is that the increase
in the state space of the models yields improved
performance. Because P-HMM+D+E biases against
redundant states found in I-HMM+D+E, it effective-
ly increases the state space beyond I-HMM+D+E.
Ahuja and Downey (2010) and our own work with
HMMs as representations (2010) have previously
shown that increasing the state space of the HMM
can significantly improve the representation, but
memory constraints eventually prevent further
progress this way. The I-HMM+D+E and P-
HMM+D+E models can provide similar benefits,
but because they split parameters across multiple
HMMs, they can accommodate much greater state
spaces in the same amount of memory.
We also tested the entropy and distance biases
separately. Figure 4 shows the result of the distance-
biased HMM+D on the general-fiction test text, as
we vary ? over the set {0.1,1,10,100,1000} (we ob-
served similar results for other domains). For all val-
ues of ?, the biased representation outperforms the
unbiased HMM. There is also a strong negative cor-
relation between the expected value of ??distance?
and the resulting accuracy, as expected from Ben-
David et als theoretical analysis. The HMM+E
model outperforms the HMM on the (source) news
domain by 0.3%, but actually performs worse for
most target domains. We suspect that the entropy
feature, which is learned only from labeled source-
domain data, makes the representation biased to-
wards features that are important in the source do-
main only. However, after we add in the distance
bias and a parameter to balance the weights from
both biases, the representation is able to capture the
label information as well as the target domain fea-
tures. Thus, the representation won?t solely depend
on source data. HMM+D+E, which combines both
biases, outperforms HMM+D, suggesting that task-
specific features for domain adaptation can be help-
ful, but only if there is some control for the domain-
independence of the features.
5.2 English Named Entity Recognition
To evaluate on a second task, we turn to Named En-
tity Recognition. We use the training data from the
1320
news (source) lore reli humour gen-fic essay mystery romance sci-fi skill science adv-fic report avg
words 9774 5428 3248 3326 4913 5214 5774 5489 3070 5464 5262 5071 6662 5284
CRF w/o HMM 93.8 85.0 80.0 85.4 85.0 83.8 84.7 86.0 82.8 78.2 82.2 77.1 85.3 84.5
HMM+E 97.1 88.2 83.1 87.5 87.4 89.2 89.5 87.1 86.7 82.1 87.2 79.4 91.7 88.3
Stanford 98.8 88.4 83.5 89.0 87.5 88.4 87.4 87.5 88.6 82.7 86.0 82.1 91.7 88.7
HMM 96.9 89.7 85.2 89.6 89.4 89.0 90.1 89.0 87.0 84.9 87.8 80.0 91.4 89.2
HMM+D 97.4 89.9 85.4 89.4 89.6 89.9 90.1 88.6 87.9 85.3 87.9 80.0 92.0 89.5
HMM+D+E 97.7 90.1 86.1 89.8 90.9 89.7 90.3 89.8 88.4 85.6 87.9 81.2 92.0 89.9
I-HMM+D+E 97.8 90.5 87.0 89.1 91.1 90.2 90.0 90.5 89.8 86.0 87.1 82.2 92.1 90.2
P-HMM+D+E 98.2 91.5 87.7 89.0 91.8 91.0 89.9 91.4 90.4 87.0 87.7 83.4 92.4 90.8
Table 2: POS tagging accuracy: The P-HMM+D+E tagger outperforms the unbiased HMM tagger and the
Stanford tagger on all target domains. The ?avg? column includes source-domain development data results. Differ-
ences between the P-HMM+D+E and the Stanford tagger are statistically significant at p < 0.01 on average and on 11
out of 12 target domain. We used the two-tailed Chi-square test with Yates? correction.
0.894
0.895
0.896
0.897
0.898
0.899
0.9
0.901
0.902
0.903
0.904
4.55E -05 4.60E-05 4.65E -05 4.70E-05 4.75E -05 4.80E-05
Ac
cu
rac
y 
?E q (?distance)? 
HMM+D on General Fiction Test  
?=1000  
?=100 
?=1  ?=0.1  
?=10  
Unconstrained HMM  
Figure 4: Greater distance between domains correlates
with worse target-domain tagging accuracy.
CoNLL 2003 shared task for our labeled training set,
consisting of 204k tokens from the newswire do-
main. We tested the system on the MUC7 formal
run test data, consisting of 59k tokens of stories on
the telecommunications and aerospace industries.
To train our representations, we use the CoNL-
L training data and the MUC7 training data without
labels. We again use a CRF, with features introduced
by Zhang and Johnson (2003) for our baseline. We
use the same setting of free parameters from our
POS tagging experiments.
Results are shown in Table 3. Our best biased
representation P-HMM+D+E outperformed the un-
biased HMM representation by 3.6%, and beats the
I-HMM+D+E by 1.6%. The domain-distance and
multi-dimensional biases help most, while the task-
specific bias helps somewhat, but only when the
domain-distance bias is included. The best sys-
System F1
CRF without HMM 66.15
HMM+E 74.25
HMM 75.06
HMM+D 75.75
HMM+D+E 76.03
I-HMM+D+E 77.04
P-HMM+D+E 78.62
Table 3: English Named Entity recognition results
tem tested on this dataset achieved a slightly bet-
ter F1 score (78.84) (Turian et al 2010), but used
a much larger training corpus (they use RCV1 cor-
pus which contains approximately 63 million token-
s). Other studies (Turian et al 2010; Huang et
al., 2011) have performed a detailed comparison be-
tween these types of systems, so we concentrate on
comparisons between biased and unbiased represen-
tations here.
5.3 Does the task-specific bias actually help?
In this section, we test whether the task-specific
bias (entropy bias) actually learns something task-
specific. We learn the entropy-biased representa-
tions for two tasks on the same set of sentences,
labeled differently for the two tasks: English POS
tagging and Named Entity Recognition. Then we
switch the representations to see whether they will
help or hurt the performance on the other task. We
randomly picked 500 sentences from WSJ section
1321
Representation/Task POS Accuracy NER F1
HMM 88.5 66.3
HMM+E(POS labels) 89.7 64.5
HMM+E(NER labels) 86.5 68.0
Table 4: Results of POS tagging and Named Entity
recognition tasks with different representations. With the
entropy-biased representation, the system has better per-
formance on the task which the bias is trained for, but
worse performance on the other task.
0-18 as our labeled training data and 500 sentences
from WSJ section 20-23 as testing data. Because
WSJ data does not have gold standard NER tags,
we manually labeled these sentences with NER tags.
For simplicity, we only use three types of NER tags:
person, organization and location. The result is
shown in Table 4. When the entropy bias uses la-
bels from the same task as the classifier, the perfor-
mance is improved: about 1.2% in accuracy on POS
tagging and 1.7% in F1 score on NER. Switching
the representations for the tasks actually hurts the
performance compared with the unbiased represen-
tation. The results suggest that the entropy bias does
indeed yield a task-specific representation.
6 Conclusion and Future Work
We introduce three types of biases into represen-
tation learning for sequence labeling using the PR
framework. Our experiments on POS tagging and
NER indicate domain-independent biases and multi-
dimensional biases significantly improve the repre-
sentations, while the task-specific bias improves per-
formance on out-of-domain data if it is combined
with the domain-independent bias. Our results indi-
cate the power of representation learning in building
domain-agnostic classifiers, but also the complexi-
ty of the task and the limitations of current tech-
niques, as even the best models still fall significantly
short of in-domain performance. Important consid-
erations for future work include identifying further
effective and tractable biases, and extending beyond
sequence-labeling to other types of NLP tasks.
Acknowledgments
This research was supported in part by NSF grant
IIS-1065397.
References
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Proceedings of the Annual Meeting of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics (NAACL-HLT).
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems 20, Cambridge, MA. MIT
Press.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151?175.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
M. Candito and B. Crabbe?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT, pages 138?141.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Pro-
ceedings of the ACL.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-view learning of word embeddings via
cca. In Neural Information Processing Systems (NIP-
S).
A. Emami, P. Xu, and F. Jelinek. 2003. Using a con-
nectionist model in a syntactical based language mod-
el. In Proceedings of the International Conference on
Spoken Language Processing, pages 372?375.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:10?49.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29(2-
3):245?273.
1322
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Conference on Empirical Methods in
Natural Language Processing.
T. Honkela. 1997. Self-organizing maps of words for
natural language processing applications. In In Pro-
ceedings of the International ICSC Symposium on Soft
Computing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Fei Huang, Alexander Yates, Arun Ahuja, and Doug
Downey. 2011. Language models as representation-
s for weakly supervised nlp tasks. In Conference on
Natural Language Learning (CoNLL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In ACL.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
from measurements in exponential families. In Inter-
national Conference on Machine Learning (ICML).
D. Lin and X Wu. 2009. Phrase clustering for discrimi-
native learning. In ACL-IJCNLP, pages 1030?1038.
G. S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In In Proc. ICML.
Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Do-
main adaptation with multiple sources. In Advances in
Neural Information Processing Systems.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In Proceedings of the
International Workshop on Artificial Intelligence and
Statistics, pages 246?252.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2007. Towards robust semantic role labeling. In Pro-
ceedings of NAACL-HLT, pages 556?563.
M. Sahlgren. 2005. An introduction to random indexing.
In In Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Ter-
minology and Knowledge Engineering (TKE).
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proc. Applied Natural Language Processing
(ANLP), pages 96?102.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 384?394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Lijie Wang, Wanxiang Che, and Ting Liu. 2009. An
svmtool-based chinese pos tagger. In Journal of Chi-
nese Information Processing.
X. Yang, H. Fu, H. Zha, and J. Barlow. 2006. Semi-
supervised nonlinear dimensionality reduction. In
Proceedings of the 23rd International Conference on
Machine Learning.
T. Zhang and D. Johnson. 2003. A robust risk mini-
mization based named entity recognition system. In
CoNLL.
D. Zhang, Z.H. Zhou, and S. Chen. 2007. Semi-
supervised dimensionality reduction. In Proceedings
of the 7th SIAM International Conference on Data
Mining.
1323
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?9,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Word Alignment Using Linguistic Code Switching Data
Fei Huang
?
and Alexander Yates
Temple University
Computer and Information Sciences
324 Wachman Hall
Philadelphia, PA 19122
{fei.huang,yates}@temple.edu
Abstract
Linguist Code Switching (LCS) is a
situation where two or more languages
show up in the context of a single
conversation. For example, in English-
Chinese code switching, there might
be a sentence like ???15?? k
?meeting (We will have a meeting in 15
minutes)?. Traditional machine translation
(MT) systems treat LCS data as noise,
or just as regular sentences. However, if
LCS data is processed intelligently, it can
provide a useful signal for training word
alignment and MT models. Moreover,
LCS data is from non-news sources which
can enhance the diversity of training data
for MT. In this paper, we first extract
constraints from this code switching data
and then incorporate them into a word
alignment model training procedure. We
also show that by using the code switching
data, we can jointly train a word alignment
model and a language model using co-
training. Our techniques for incorporating
LCS data improve by 2.64 in BLEU score
over a baseline MT system trained using
only standard sentence-aligned corpora.
1 Introduction
Many language users are competent in multiple
languages, and they often use elements of multiple
languages in conversations with other speakers
with competence in the same set of languages.
For example, native Mandarin speakers who
also speak English might use English words in
a Chinese sentence, like ?\???K
solution??(Do you know the solution to
this problem ?)?. This phenomenon of mixing
?
*The author is working at Raytheon BBN Technologies
now
languages within a single utterance is known as
Linguistic Code Switching (LCS). Examples of
these utterances are common in communities of
speakers with a shared competency in multiple
languages, such as Web forums for Chinese
emigr?es to the United States. For example, more
than 50% of the sentences we collected from a
Web forum (MITBBS.com) contains both Chinese
and English.
Traditional word alignment models take a
sentence-level aligned corpus as input and gener-
ate word-level alignments for each pair of parallel
sentences. Automatically-gathered LCS data
typically contains no sentence-level alignments,
but it still has some advantages for training
word alignment models and machine translation
(MT) systems which are worth exploring. First,
because it contains multiple languages in the same
sentence and still has a valid meaning, it will tell
the relationship between the words from different
languages to some extent. Second, most LCS
data is formed during people?s daily conversation,
and thus it contains a diversity of topics that
people care about, such as home furnishings,
cars, entertainment, etc, that may not show up in
standard parallel corpora. Moreover, LCS data is
easily accessible from Web communities, such as
MITBBS.com, Sina Weibo, Twitter, etc.
However, like most unedited natural language
text on the Web, LCS data contains symbols like
emotions, grammar and spelling mistakes, slang
and strongly idiomatic usage, and a variety of
other phenomena that are difficult to handle. LCS
data with different language pairs may also need
special handling. For instance, Sinha and Thakur
(2005) focus on words in mixed English and
Hindi texts where a single word contains elements
from both languages; they propose techniques
for translating such words into both pure English
and pure Hindi. Our study focuses on Chinese-
English LCS, where this is rarely a problem,
1
but for other language pairs, Sinha and Thakur?s
techniques may be required as preprocessing
steps. Primarily, though, LCS data requires
special-purpose algorithms to use it for word
alignment, since it contains no explicit alignment
labels.
In this paper, we investigate two approaches to
using LCS data for machine translation. The first
approach focuses exclusively on word alignment,
and uses patterns extracted from LCS data to guide
the EM training procedure for word alignment
over a standard sentence-aligned parallel corpus.
We focus on two types of patterns in the LCS
data: first, English words are almost never correct
translations for any Chinese word in the same
LCS utterance. Second, for sentences that are
mostly Chinese but with some English words, if
we propose substitutes for the English words using
a Chinese language model, those substitutes are
often good translations of the English words. We
incorporate these patterns into EM training via
the posterior regularization framework (Ganchev
et al., 2010).
Our second approach treats the alignment and
language model as two different and comple-
mentary views of the data. We apply the co-
training paradigm for semi-supervised learning
to incorporate the LCS data into the training
procedures for the alignment model and the
language model. From the translation table of
the alignment model, the training procedure finds
candidate translations of the English words in
the LCS data, and uses those to supplement the
language model training data. From the language
model, the training procedure identifies Chinese
words that complete the Chinese sentence with
high probability, and it uses the English word
paired with these completion words as additional
training points for translation probabilities. These
models are trained repeatedly until they converge
to similar predictions on the LCS data. In
combination with a larger phrase-based MT
system (Koehn et al., 2003), these two training
procedures yield an MT system that achieves a
BLEU score of 31.79 on an English-to-Chinese
translation task, an improvement of 2.64 in BLEU
score over a baseline MT system trained on only
our parallel corpora.
The rest of this paper is organized as follows.
The next section presents related work. Section 3
gives an overview of word alignment. Sections 4
and 5 detail our two algorithms. Section 6 presents
our experiments and discusses results, and Section
7 concludes and discusses future work.
2 Related Work
There has been a lot of research on LCS from
the theoretical and socio-linguistic communities
(Nilep, 2006; De Fina, 2007). Computational
research on LCS has studied how to identify
the boundaries of an individual language within
LCS data, or how to predict when an utterance
will switch to another language (Chan et al.,
2004; Solorio and Liu, 2008). Manandise and
Gdaniec (2011) analyzed the effect on machine
translation quality of LCS of Spanish-English and
showed that LCS degrades the performance of
the syntactic parser. Sinha and Thakur (2005)
translate mixed Hindi and English (Hinglish)
to pure Hindi and pure English by using two
morphological analyzers from both Hindi and
English. The difficulty in their problem is
that Hindi and English are often mixed into a
single word which uses only the English alphabet;
approaches based only on the character set cannot
tell these words apart from English words. Our
current study is for a language pair (English-
Chinese) where the words are easy to tell apart,
but for MT using code-switching data for other
language pairs (such as Hindi-English), we can
leverage some of the techniques from their work
to separate the tokens into source and target.
Like our proposed methods, other researchers
have used co-training before for MT (Callison-
Burch and Osborne, 2003). They use target
strings in multiple languages as different views on
translation. However, in our work, we treat the
alignment model and language model as different
views of LCS data.
In addition to co-training, various other semi-
supervised approaches for MT and word align-
ment have been proposed, but these have relied on
sentence alignments among multiple languages,
rather than LCS data. Kay (2000) proposes using
multiple target documents as a way of informing
subsequent machine translations. Kumar et al.
(2007) described a technique for word alignment
in a multi-parallel sentence-aligned corpus and
showed that this technique can be used to obtain
higher quality bilingual word alignments. Other
work like (Eisele, 2006) took the issue one step
further that they used bilingual translation systems
2
which share one or more common pivot languages
to build systems which non-parallel corpus is used.
Unlike the data in these techniques, LCS data
requires no manual alignment effort and is freely
available in large quantities.
Another line of research has attempted to
improve word alignment models by incorporating
manually-labeled word alignments in addition to
sentence alignments. Callison-Burch et al. (2004)
tried to give a higher weight on manually labeled
data compared to the automatic alignments. Fraser
and Marcu (2006) used a log-linear model with
features from IBM models. They alternated the
traditional Expectation Maximization algorithm
which is applied on a large parallel corpus with
a discriminative step aimed at increasing word-
alignment quality on a small, manually word-
aligned corpus. Ambati et al.(2010) tried to man-
ually correct the alignments which are informative
during the unsupervised training and applied them
to an active learning model. However, labeled
word alignment data is expensive to produce. Our
approach is complementary, in that we use mixed
data that has no word alignments, but still able to
learn constraints on word alignments.
Our techniques make use of posterior regular-
ization (PR) framework (Ganchev et al., 2010),
which has previously been used for MT (Graca
et al., 2008), but with very different constraints
on EM training and different goals. (Graca et
al., 2008) use PR to enforce the constraint that
one word should not translate to many words, and
that if a word s translates to a word t in one MT
system, then a model for translation in the reverse
direction should translate t to s. Both of these
constraints apply to sentence-aligned training data
directly, and complement the constraints that we
extract from LCS data.
3 Statistical Word Alignment
Statistical word alignment (Brown et al., 1994) is
the task identifying which words are translations
of each other in a bilingual sentence corpus. It
is primarily used for machine translation. The
input to an alignment system is a sentence-level
aligned bilingual corpus, which consists of pairs
of sentences in two languages. One language
is denoted as the target language, and the other
language as the source language.
We now introduce the baseline model for word
alignment and how we can incorporate the LCS
data to improve the model. IBM Model 1
(Brown et al., 1994) and the HMM alignment
model (Vogel et al., 1996) are cascaded to
form the baseline model for alignment. These
two models have a similar formulation L =
P (t, a|s) = P (a)
?
j
P (t
j
|s
a
j
) with a different
distortion probability P (a). s and t denote the
source and target sentences. a is the alignment,
and a
j
is the index of the source language word
that generates the target language word at position
j. The HMM model assumes the alignments have
a first-order Markov dependency, so that P (a) =
?
j
P (a
j
|a
j
? a
j?1
). IBM Model 1 ignores the
word position and uses a uniform distribution, so
P (a) =
?
j
P (a
j
) where P (a
j
) =
1
|t|
, where |t|
is the length of t.
Expectation Maximization (Dempster et al.,
1977) is typically used to train the alignment
model. It tries to maximize the marginal
likelihood of the sentence-level aligned pairs.
For the HMM alignment model, the forward-
backward algorithm can be used the optimize the
posterior probability of the hidden alignment a.
4 Learning Constraints for Word
Alignments from LCS Data
We observed that most LCS sentences are
predominantly in one language, which we call
the majority language, with just a small number
of words from another language, which we
call the minority language. The grammar of
each sentence appears to mirror the structure
of the majority language. Speakers appear to
be substituting primarily content words from the
minority language, especially nouns and verbs,
without changing the structure of the majority
language. In this section, we explain two types
of constraints we extract from the LCS data
that can be helpful for guiding the training of a
word alignment model, and we describe how we
incorporate those constraints into a full training
procedure.
4.1 Preventing bad alignments
After inspecting sentences in our LCS data, we
found that the words from the target language
occurring in the sentence are highly likely not to
be the translation of the remaining source word.
Figure 1 shows an example LCS sentence where
the speaker has replaced the Chinese word ????
with the corresponding English word ?request?.
3
?? ?? ?? ?? 
People request to change the Constitution 
   ?? request ?? ?? 
Chinese Translation: 
English Translation: 
LCS sentence:  
Figure 1: The upper sentence is the original LCS sentence. The bottom ones are its translation in pure Chinese and English.
Underlined words are the original words in the LCS sentence.
In most LCS utterances, the minority language
replaces or substitutes for words in the majority
language, and thus it does not serve as a translation
of any majority-language words in the sentence.
If we can enforce that a word alignment model
avoids pairing words that appear in the same
LCS sentence, we can significantly narrow down
the possible choices of the translation candidates
during word alignment training.
Formally, let t
LCS
be the set of target (Chinese)
words and s
LCS
be the source (English) words in
the same sentence of the LCS data. According to
our observation, each s
LCS
j
in s
LCS
should not
be aligned with any word t
LCS
i
in t
LCS
. We call
every target-source word pair (t
LCS
i
, s
LCS
j
) from
LCS data a blocked alignment. For a set of word
alignments WA = {(s
w
, t
w
)} produced by a word
alignment model, define
?
BA
=
?
(s
w
,t
w
)?WA
1[(s
w
, t
w
) ? BA] (1)
where BA is the set of blocked alignments
extracted from the LCS data. We want to minimize
?
BA
. Figure 2 shows a graphical illustration of this
constraint.
?? (People) ?? (change) ?? (constitution) 
request 
Figure 2: Illustration of the blocked alignment constraint.
4.2 Encouraging alignments with substitutes
proposed by a language model
Another perspective of using the LCS data is
that if we can find some target word set t
similar
from the target language which shares similar
contexts as the source word s
LCS
j
in the LCS
data, then we can encourage s
LCS
j
to be aligned
with the each word t
similar
m
in t
similar
. Figure
3 shows example phrases (??????U? ,
? ?????U?, ??????U? etc) that
appear in a Chinese language model and which
share the same left context and right context as
the word ?request.? Our second objective is to
encourage minority language words like ?request?
to align with possible substitutes from the majority
language?s language model. If we see any of
???, ??, ??? in the parallel corpus, we
should encourage the word ?request? to be aligned
with them. We call this target-source word pair
(t
similar
m
, s
LCS
j
) an encouraged alignment.
Formally, we define
?
EA
= |C| ?
?
(s
w
,t
w
)?WA
1[(s
w
, t
w
) ? EA] (2)
where |C| is the size of the parallel corpus and EA
is the encouraged alignment set. We define this
expression in such a way that if the optimization
procedure minimizes it, it will increase the number
of encouraged alignments.
?? (People)  ?? (change)  ?? (constitution)  request  
Trigrams  ??  ?? (r efuse)   ?? ??  ?? (r equest) ?? ??  ?? (suggest) ?? 
Figure 3: Illustration of the encouraged alignment
constraint. The dotted rectangle shows the candidate
translations of the English word from the tri-gram output
from the language model
Algorithm 1 shows the algorithm of calculating
t
similar
. (t
LCS
l
, s
LCS
j
, t
LCS
r
) is a (target, source,
target)word tuple contained in the LCS data. l
and r denote the left and right target words to the
source word. We use the language model output
from the target language. For each pair of contexts
t
l
and t
r
for the source word, we find the exact
match of this pair in the ngram. Then we extract
the middle word as the candidates for t
similar
.
Here, we only use 3 grams in our experiments, but
it is possible to extend this to 5grams, which might
lead to further improvements. The EA constraint
4
Algorithm 1: finding t
similar
1: Input: s
LCS
,t
LCS
, language model LM
2: Set t
similar
={}
3: Extract the 3 grams (t
l
, t
m
, t
r
) ? gram
3
from
LM
4: set S = {}
5: For j from 1 to size(gram
3
)
if (t
j
l
, t
j
r
) ? S
add t
j
m
into C
t
j
l
,t
j
r
else
put (t
j
l
, t
j
r
) into S
set C
t
j
l
,t
j
r
= {}
6: Extract tuple (t
LCS
l
, s
LCS
j
, t
LCS
r
)
if (t
LCS
l
, t
LCS
r
) ? S
add C
t
LCS
l
,t
LCS
r
into t
similar
7: Output: t
similar
is similar to a bilingual dictionary. However, in the
bilingual dictionary, each source word might have
several target translations (senses), so it might be
ambiguous. The candidate translations used in
EA are from language model (3 grams in this
paper, but it can be extended to 5 grams), which
will always match the contexts. Additionally,
the bilingual dictionary contains the standard
English/Chinese word pairs. But the LCS data
is generated from people.s daily conversation; it
reflects usage in a variety of domains, including
colloquial and figurative usages that may not
appear in a dictionary.
4.3 Constrained parameter estimation
We incorporate ?
BA
and ?
EA
into the EM
training procedure for the alignment model using
posterior regularization (PR) (Ganchev et al.,
2010). Formally, let x be the sentence pairs s and
t. During the E step, instead of using the posterior
p(a|x) to calculate the expected counts, the PR
framework tries to find a distribution q(a) which
is close to p(a|x), but which also minimizes the
properties ?(a,x):
min
q,?
[KL(q(a)||p(a|x, ?)) + ?||?||] (3)
s.t. E
a?q
[?(a,x)] ? ? (4)
where KL is the Kullback-Leibler divergence, ?
is a free parameter indicating how important the
constraints are compared with the marginal log
likelihood and ? is a small violation allowed in
?? ?? ?? ?? (0.025)  
?? ?? ?? ?? (0.05)  
?? ?? ?? ?? (0.009)  ?? 
Chinese Monolingual data  
?? ??  ?? (0.06)  
?? ??  ?? (0.002)  
?? ??  ?? (0.01)  
?? ??  ?? (0.04)  
?? 
?? 
?? 
Translation Table  
?? 
request ?? 0.025  
R equest ?? 0.05  request ?? 0.009  ?? Translation Table  
?? 
request ?? 0.06  
R equest ?? 0.0002  request ?? 0.01  
request ?? 0.04  Update Translation Table  
Update mixed data  
LM  
A M  
?? request ?? ?? (People request to change the constitution) 
Figure 4: The framework of co-training in word alignment.
AM represents alignment model and LM represents language
model. Green italic words are the encouraged translation and
red italic words are the discouraged translation.
the optimization. To impose multiple constraints,
we define a norm ||?||
A
=
?
(?
t
A?), where A
is a diagonal matrix whose diagonal entries A
ii
are free parameters that provide weights on the
different constraints. Since we only have two
constraints here from LCS data, A =
(
1 0
0 ?
)
where ? controls the relative importance of the
two constraints.
To make the optimization task in the E-step
more tractable, PR transforms it to a dual problem:
max
??0,???
?
??
? log
?
a
p(a|x, ?) exp{?? ??(a,x)}
where ???
?
is the dual norm of ???
A
. The gradient
of this dual objective is?E
q
[?(a,x)]. A projected
subgradient descent algorithm is used to perform
the optimization.
5 Co-training using the LCS data
The above approaches alter the translation and
distortion probabilities in the alignment model.
However, they leave the language model un-
changed. We next investigate a technique that
uses LCS data to re-estimate parameters for the
language model as well as the alignment model
simultaneously. Co-training (Blum and Mitchell,
1998) is a semi-supervised learning technique
that requires two different views of the data. It
assumes that each example can be described using
two different feature sets which are conditionally
independent. Also, each feature set of the data
should be sufficient to make accurate prediction.
5
The schema fits perfectly into our problem. We
can treat the alignment model and the language
model as two different views of the LCS data.
We use the same example ???request ?U
{? to show how co-training works, shown in
Figure 4. From the translation table generated
by the alignment model, we can get a set of
candidate translations of ?request?, such as ???
??,????,etc. We can find the candidate with the
highest probability as the translation. Similarly,
from the language model, we can extract all the
ngrams containing ? ??? and ??U? as the left
and right words and pick the words in the middle
such as ? ??, ??, ??? etc as the candidate
translations. We can then use the candidate
with the highest probability as the translation
for ?request?. Thus both models can predict
translations for the English (minority language) in
this example. Each model?s predictions can be
used as supplemental training data for the other
model.
Algorithm 2 shows the co-training algorithm for
word alignment. At each iteration, a language
model and an alignment model are trained. The
language model is trained on a Chinese-only
corpus plus a corpus of probabilistic LCS sen-
tences where the source words are replaced with
target candidates from the alignment model. The
alignment model is retrained using a translation
table which is updated according to the output
word pairs from the language model output and the
LCS data. In order to take the sentence probability
into consideration, we modify the language model
training procedure: when it counts the number of
times each ngram appears, instead of adding 1,
it adds the probability from the translation model
for ngrams in the LCS data that contain predicted
translations.
6 Experiments and Results
6.1 Experimental Setup
We evaluated our LCS-driven training algorithms
on an English-to-Chinese translation task. We
use Moses (Koehn et al., 2003), a phrase-
based translation system that learns from bilingual
sentence-aligned corpora as the MT system. We
supplement the baseline word alignment model in
Moses with our LCS data, constrained training
procedure, and co-training algorithm as well as
IBM 3 model. Because IBM 3 model is a
fertility based model which might also alleviate
Algorithm 2: Co-training for word alignment and
language modeling
1: Input: parallel data X
p
, LCS data X
LCS
,
language model training data X
l
2: Initialize translation table tb for IBM1 model
3: For iteration from 1 to MAX
tb? Train-IBM(X
p
)
tb
?
? Train-HMM(X
p
|tb)
4: For each sentence x
i
in X
LCS
:
For each source word s
j
in x
i
:
1) find the translation t
j
of s
j
with
with probability p
j
from tb
?
2) replace s
j
with t
j
and update
sentence?s probability p
s
= p
s
?p
j
X
new
l
? X
l
? x
i
5: LM? Train-LM(X
new
l
)
6: Extract the tri-gram gram
3
from LM
7: For each sentence x
i
in X
LCS
:
run Algorithm 1: finding t
similar
8: update tb
?
using (t
m
, s
j
) where
t
m
? t
similar
and s
j
? x
i
9: End For
10: Output: word alignment for X
p
and LM
some of the problems caused by LCS data. To
clarify, we use IBM1 model and HMM models in
succession for the baseline. We trained the IBM1
model first and used the resulting parameters
as the initial parameter values to train HMM
model. Parameters for the final MT system
are tuned with Minimum Error Rate Training
(MERT) (Och, 2003). The tuning set for MERT
is the NIST MT06 data set, which includes 1664
sentences. We test the system on NIST MT02
(878 sentences). To evaluate the word alignment
results, we manually aligned 250 sentences from
NIST MT02 data set. For simplicity, we only
have two types of labels for evaluating word
alignments: either two words are aligned together
or not. (Previous evaluation metrics also consider
a third label for ?possible? alignments.) Out of
the word-aligned data, we use 100 sentences as a
development set and the rest as our testing set.
Our MT training corpus contains 2,636,692
sentence pairs from two parallel corpora: Hong
Kong News (LDC2004T08) and Chinese English
News Magazine Parallel Text (LDC2005T10). We
use the Stanford Chinese segmenter to segment
the Chinese data. We use a ngram model
package called SRILM (Stolcke, 2002) to train
6
the language model. Because our modified
ngram counts contain factions, we used Witten-
Bell smoothing(Witten and Bell, 1991) which
supports fractional counts. The 3-gram language
model is trained on the Xinhua section of the
Chinese Gigaword corpus (LDC2003T09) as well
as the Chinese side of the parallel corpora. We
also removed the sentences in MT02 from the
Gigaword corpus if there is any to avoid the biases.
We gather the LCS data from ?MITBBS.com,?
a popular forum for Chinese people living in
the United States. This forum is separated by
discussion topic, and includes topics such as
?Travel?, ?News?, and ?Living style?. We extract
data from 29 different topics. To clean up the
LCS data, we get rid of HTML mark-up, and we
remove patterns that are commonly repeated in
forums, like ?Re:? (for ?reply? posts) and ?[=
1]? (for ?repost?). We change all English letters
written in Chinese font into English font. We stem
the English words in both the parallel training data
and the LCS data. After the cleaning step, we have
245,470 sentences in the LCS data. 120,922 of
them actually contain both Chinese and English in
the same sentence. 101,302 of them contain only
Chinese, and we add these into the language model
training data. We discard the sentences that only
contain English.
6.2 Word Alignment Results
In order to incorporate the two constraints during
the Posterior Regularization, we need to tune the
parameters ? which controls the weights between
the constraints and the marginal likelihood and
? which controls the relative importance between
two constraints on development data. We varied
? from 0.1 to 1000 and varied ? over the
set {0.01, 0.1, 1, 10, 100}. After testing the
25 different combinations of ? and ? on the
development data, we find that the setting with
? = 100 and ? = 0.1 achieves the best
performance. During PR training, we trained the
model 20 iterations for the dual optimization and
5 iterations for the modified EM.
Table 1 shows the word alignment results. We
can see that incorporating the LCS data into
our alignment model improves the performance.
Our best co-training+PR
+
system outperforms
the baseline by 8 points. Figure 5 shows an
example of how BA is extracted from LCS data
can help the word alignment performance. The
System F1
Baseline 0.68
IBM 3 0.70
PR+BA 0.71
PR+EA 0.70
PR
+
0.73
co-training 0.74
co-training+PR
+
0.76
Table 1: Word alignment results (PR
+
means PR+BA+EA).
upper figure shows that alignment by the baseline
system. We can see that the word ?badminton?
is aligned incorrectly with word ?>??(Taufik)?
. However, in the LCS data, we see that ? >?
?(Taufik)? and ?badminton? appear in the same
sentence ?>??badmintonx?
(Taufik
plays badminton so well)? and by adding the
blocked constraint into the alignment model, it
correctly learns that ? >??(Taufik)? should be
aligned with something else, and it finds ?Taufik?
at end. Table 2 shows some of the translations
of ?badminton? before and after incorporating the
LCS data. We can see that it contains some wrong
translations like ??	??(pingpong room)?,?>
??(Taufik)?etc using baseline model. After
using the LCS data as constraints and the co-
training framework, these wrong alignments are
eliminated and the translation ?? ?(another
way of expressing badminton)? get a higher
probability. We found that IBM 3 model can
also correct this specific case. However, our
co-training+PR
+
system still outperforms it by 6
points.
Figure 6 shows an example of how EA is
extracted from LCS data can help the word
alignment. The solid lines show the alignment
by the baseline model and we can see that
the word ?compiled? is not aligned with any
Chinese word. After using the LCS data and the
language model, we find that ?8B(compile)?
shows up in the same context ??(book) ?
5(up)?as ?compile? along with ?C?(staple)?
and ??(staple)?, therefore ?(compile,8B)? will
be an encouraged alignment. After adding the EA
constraint, the model learns that ?compile? should
be aligned with ?8B?.
6.3 Phrase-based machine translation
In this section, we investigated whether improved
alignments can improve MT performance. We
7
?? ??? ?? ?? ??? ? ?? ??? 
Indonesia badminton experts think Taufik?s ranking favorable  
?? ??? ?? ?? ??? ? ?? ??? 
Indonesia badminton experts think Taufik?s ranking favorable  
Baseline:  
PR+BA:  
Figure 5: After incorporating the BA constraint from the LCS data, the word ?Taufik(>??)? is aligned correctly.
Baseline PR+co-training
Translation Probability Translation Probability
?f?(badminton) 0.500 ?f?(badminton) 0.500
W	?(pingpong)?(room) 0.500 ??(two of the three characters in badminton) 0.430
?(play)?f(feather) 0.250 ?(play)?f(feather) 0.326
?f?(shuttlecock)?(head) 0.125 ?f?(shuttlecock)?(head) 0.105
... ... ... ...
>??(Taufik) 0.005 ??(racket) 0.002
Table 2: Translation tables of ?badminton? before and after incorporation of LCS data.
? ?? ? ? ?? ?? ? ??
Winning entries after the review will be compiled
?? ? ? compile ???
(How to compile the book ?)
Trigrams
?(book) ??(compile)  ??(up)
?(book)  ??(staple)    ??(up)
?(book)    ?(staple)      ??
(up)
...
Wednesday, October 16, 13
Figure 6: After incorporating the EA constraint from the
LCS data, the word ?compiled(8B)? is aligned correctly.
use different word alignment models? outputs as
the first step for Moses and keep the rest of
Moses system the same. We incorporate Moses?s
eight standard features as well as the lexicalized
reordering model. We also use the grow-diag-final
and alignment symmetrization heuristic.
Table 3 shows the machine translation results.
We can see that 3 techniques we proposed for word
alignment all improve the machine translation
result over the baseline system as well as the
IBM 3 model. However, although co-training
has a bigger improvement on the word alignment
compared with PR
+
, it actually has a lower
BLEU score. This phenomenon shows that the
improvement in the word alignment does not
necessarily lead to the improvement on machine
translation. After combining the co-training
and the PR
+
together, co-training+PR
+
improved
slightly over PR
+
for MT.
System BLEU score
Baseline 29.15
IBM 3 30.24
PR
+
31.59*
co-training 31.04*
co-training+PR
+
31.79*
Table 3: Machine translation results. All entries marked
with an asterisk are better than the baseline with 95%
statistical significance computed using paired bootstrap
resampling (Koehn, 2004).
7 Conclusion and Future Work
In this paper, we explored two different ways to
use LCS data in a MT system: 1) PR framework
to incorporate with Blocked Alignment and
Encouraged Alignment constraints. 2) A semi-
supervised co-training procedure. Both techniques
improve the performance of word alignment and
MT over the baseline. Our techniques are
currently limited to sentences where the LCS data
contains very short (usually one word) phrases
from a minority language. An important line of
investigation for generalizing these approaches is
to consider techniques that cover longer phrases in
the minority language; this can help add more of
the LCS data into training.
Acknowledgements
This work was supported in part by NSF awards
1065397 and 1218692.
8
References
S.and Carbonell J. Ambati, V.and Vogel. 2010.
Active semi-supervised learning for improving word
alignment. In In Proceedings of the Active Learning
for NLP Workshop, NAACL.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Annual Conference on Computational Learning
Theaory.
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Chris Callison-Burch and Miles Osborne. 2003. Co-
training for statistical machine translation. In In
Proceedings of the 6th Annual CLUK Research
Colloquium.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In In
Proceedings of ACL.
J. Y. C. Chan, P. C. Ching, and H. M. LEE, T.and Meng.
2004. Detection of language boundary in code-
switching utterances by bi-phone probabilities. In
In Proceedings of the International Symposium on
Chinese Spoken Language Processing.
A De Fina. 2007. Code-switching and the construction
of ethnic identity in a community of practice. In
Language in Society, volume 36.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. In Royal Statistical Society, Ser,
volume 39.
Andreas Eisele. 2006. Parallel corpora and
phrase-based statistical machine translation for new
language pairs via multiple intermediaries. In
International Conference on Language Resources
and Evaluation.
Alex Fraser and Daniel Marcu. 2006. Semi-supervised
training for statistical word alignment. In In
Proceedings of ACL.
Kuzman Ganchev, J. Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for
structured latent variable models. In Journal of
Machine Learning Research, volume 11.
J. Graca, K. Ganchev, and B. Taskar. 2008.
Expectation maximization and posterior constraints.
In NIPS.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL-HLT.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In In Proceedings of
EMNLP.
Shankar Kumar, Franz Josef Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In EMNLP.
Esme Manandise and Claudia Gdaniec. 2011. Mor-
phology to the rescue redux: Resolving borrowings
and code-mixing in machine translation. In SFCM.
C. Nilep. 2006. Code switching in sociocultural
linguistics. In Colorado Research in Linguistics,
volume 19.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In ACL.
R.M.K. Sinha and A. Thakur. 2005. Machine
translation of bi-lingual hindi-english (hinglish)
text. In In Proceedings of the 10th Conference on
Machine Translation.
T. Solorio and Y. Liu. 2008. Learning to predict
code-switching points. In In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
A. Stolcke. 2002. An extensible language modeling
toolkit. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 901?904.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-
based word alignment in statistical translation. In
In Proc.COLING.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabil- ities of
novel events in adaptive text compression. In IEEE
Transactions on Information Theory, volume 4,
pages 1085?1094.
9
Learning Representations for Weakly
Supervised Natural Language
Processing Tasks
Fei Huang?
Temple University
Arun Ahuja??
Northwestern University
Doug Downey?
Northwestern University
Yi Yang?
Northwestern University
Yuhong Guo?
Temple University
Alexander Yates?
Temple University
Finding the right representations for words is critical for building accurate NLP systems when
domain-specific labeled data for the task is scarce. This article investigates novel techniques for
extracting features from n-gram models, Hidden Markov Models, and other statistical language
models, including a novel Partial Lattice Markov Random Field model. Experiments on part-
of-speech tagging and information extraction, among other tasks, indicate that features taken
from statistical language models, in combination with more traditional features, outperform
traditional representations alone, and that graphical model representations outperform n-gram
models, especially on sparse and polysemous words.
? 1805 N. Broad St., Wachman Hall 324, Philadelphia, PA 19122, USA.
E-mail: {fei.huang,yuhong,yates}@temple.edu.
?? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ahuja@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ddowney@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: yya518@eecs.northwestern.edu.
Submission received: 13 June 2012; revised submission received: 25 November 2012, accepted for publication:
15 January 2013.
doi:10.1162/COLI a 00167
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
1. Introduction
NLP systems often rely on hand-crafted, carefully engineered sets of features to achieve
strong performance. Thus, a part-of-speech (POS) tagger would traditionally use a
feature like, ?the previous token is the? to help classify a given token as a noun
or adjective. For supervised NLP tasks with sufficient domain-specific training data,
these traditional features yield state-of-the-art results. However, NLP systems are in-
creasingly being applied to the Web, scientific domains, personal communications like
e-mails and tweets, among many other kinds of linguistic communication. These texts
have very different characteristics from traditional training corpora in NLP. Evidence
from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing
(Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan,
Ward, and Martin 2007), among other NLP tasks (Daume? III and Marcu 2006; Chelba
and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer,
Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades
significantly when tested on domains different from those used for training. Collecting
labeled training data for each new target domain is typically prohibitively expensive.
In this article, we investigate representations that can be applied to weakly supervised
learning, that is, learning when domain-specific labeled training data are scarce.
A growing body of theoretical and empirical evidence suggests that traditional,
manually crafted features for a variety of NLP tasks limit systems? performance in this
weakly supervised learning for two reasons. First, feature sparsity prevents systems
from generalizing accurately, because many words and features are not observed in
training. Also because word frequencies are Zipf-distributed, this often means that there
is little relevant training data for a substantial fraction of parameters (Bikel 2004b), espe-
cially in new domains (Huang and Yates 2009). For example, word-type features form
the backbone of most POS-tagging systems, but types like ?gene? and ?pathway? show
up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier
trained on newswire data and tested on biomedical data will have seen few training
examples related to sentences with features ?gene? and ?pathway? (Blitzer, McDonald,
and Pereira 2006; Ben-David et al. 2010).
Further, because words are polysemous, word-type features prevent systems from
generalizing to situations in which words have different meanings. For instance, the
word type ?signaling? appears primarily as a present participle (VBG) in Wall Street
Journal (WSJ) text, as in, ?Interest rates rose, signaling that . . . ? (Marcus, Marcinkiewicz,
and Santorini 1993). In biomedical text, however, ?signaling? appears primarily in the
phrase ?signaling pathway,? where it is considered a noun (NN) (PennBioIE 2005); this
phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates 2010).
Our response to the sparsity and polysemy challenges with traditional NLP repre-
sentations is to seek new representations that allow systems to generalize to previously
unseen examples. That is, we seek representations that permit classifiers to have close
to the same accuracy on examples from other domains as they do on the domain of the
training data. Our approach depends on the well-known distributional hypothesis,
which states that a word?s meaning is identified with the contexts in which it appears
(Harris 1954; Hindle 1990). Our goal is to develop probabilistic statistical language
models that describe the contexts of individual words accurately. We then construct
representations, or mappings from word tokens and types to real-valued vectors,
from statistical language models. Because statistical language models are designed to
model words? contexts, the features they produce can be used to combat problems
with polysemy. And by careful design of the statistical language models, we can limit
86
Huang et al. Computational Linguistics
the number of features that they produce, controlling how sparse those features are in
training data.
Our specific contributions are as follows:
1. We show how to generate representations from a variety of language
models, including n-gram models, Brown clusters, and Hidden Markov
Models (HMMs). We also introduce a Partial-Lattice Markov Random
Field (PL-MRF), which is a tractable variation of a Factorial Hidden
Markov Model (Ghahramani and Jordan 1997) for language modeling,
and we show how to produce representations from it.
2. We quantify the performance of these representations in experiments
on POS tagging in a domain adaptation setting, and weakly supervised
information extraction (IE). We show that the graphical models outperform
n-gram representations, even when the n-gram models leverage larger
corpora for training. The PL-MRF representation achieves a state-of-the-art
93.8% accuracy on a biomedical POS tagging task, which represents a
5.5 percentage point absolute improvement over more traditional POS
tagging representations, a 4.8 percentage point improvement over a tagger
using an n-gram representation, and a 0.7 percentage point improvement
over a tagger with an n-gram representation using several orders of
magnitude more training data. The HMM representation improves
over the n-gram model by 7 percentage points on our IE task.
3. We analyze how sparsity, polysemy, and differences between domains
affects the performance of a classifier using different representations.
Results indicate that statistical language model representations, and
especially graphical model representations, provide the best features
for sparse and polysemous words.
The next section describes background material and related work on representation
learning for NLP. Section 3 presents novel representations based on statistical language
models. Sections 4 and 5 discuss evaluations of the representations, first on sequence-
labeling tasks in a domain adaptation setting, and second on a weakly supervised set-
expansion task. Section 6 concludes and outlines directions for future work.
2. Background and Previous Work on Representation Learning
2.1 Terminology and Notation
In a traditional machine learning task, the goal is to make predictions on test data using
a hypothesis that is optimized on labeled training data. In order to do so, practitioners
predefine a set of features and try to estimate classifier parameters from the observed
features in the training data. We call these feature sets representations of the data.
Formally, let X be an instance space for a learning problem. Let Z be the space of
possible labels for an instance, and let f : X ? Z be the target function to be learned.
A representation is a function R: X ? Y , for some suitable feature space Y (such as Rd).
We refer to dimensions of Y as features, and for an instance x ? X we refer to values
for particular dimensions of R(x) as features of x. Given a set of training examples, a
learning machine?s task is to select a hypothesis h from the hypothesis space H, a subset
of ZR(X ). Errors by the hypothesis are measured using a loss function L(x, R, f, h) that
87
Computational Linguistics Volume 40, Number 1
measures the cost of the mismatch between the target function f (x) and the hypothesis
h(R(x)).
As an example, the instance set for POS tagging in English is the set of all English
sentences, and Z is the space of POS sequences containing labels like NN (for noun) and
VBG (for present participle). The target function f is the mapping between sentences
and their correct POS labels. A traditional representation in NLP converts sentences
into sequences of vectors, one for each word position. Each vector contains values for
features like, ?+1 if the word at this position ends with -tion, and 0 otherwise.? A
typical loss function would count the number of words that are tagged differently by
f (x) and h(R(x)).
2.2 Representation-Learning Problem Formulation
Machine learning theory assumes that there is a distribution D over X from which
data is sampled. Given a training set S = {(x1, z1), . . . , (xN, zN )} ? (D(X ),Z )N, a fixed
representation R, a hypothesis space H, and a loss function L, a machine learning
algorithm seeks to identify the hypothesis in H that will minimize the expected loss
over samples from distribution D:
h? = argmin
h?H
Ex?D(X )L(x, R, f, h) (1)
The representation-learning paradigm breaks the traditional notion of a fixed rep-
resentation R. Instead, we allow a space of possible representations R. The full learning
problem can then be formulated as the task of identifying the best R ? R and h ? H
simultaneously:
R?, h? = argmin
R?R,h?H
Ex?D(X )L(x, R, f, h) (2)
The representation-learning problem formulation in Equation (2) can in fact be
reduced to the general learning formulation in Equation (1) by setting the fixed rep-
resentation R to be the identity function, and setting the hypothesis space to be R?H
from the representation-learning task. We introduce the new formulation primarily as
a way of changing the perspective on the learning task: most NLP systems consider
a fixed, manually crafted transformation of the original data to some new space, and
investigate hypothesis classes over that space. In the new formulation, systems learn
the transformation to the feature space, and then apply traditional classification or
regression algorithms.
2.3 Theory on Domain Adaptation
We refer to the distribution D over the instance space X as a domain. For example,
the newswire domain is a distribution over sentences that gives high probability to
sentences about governments and current events; the biomedical literature domain
gives high probability to sentences about proteins and regulatory pathways. In domain
adaptation, a system observes a set of training examples (R(x), f (x)), where instances
x ? X are drawn from a source domain DS, to learn a hypothesis for classifying ex-
amples drawn from a separate target domain DT. We assume that large quantities of
unlabeled data are available for the source domain and target domain, and call these
88
Huang et al. Computational Linguistics
samples US and UT, respectively. For any domain D, let R(D) represent the induced
distribution over the feature space Y given by PrR(D)[y] = PrD[{x such that R(x) = y}].
Previous work by Ben-David et al. (2007, 2010) proves theoretical bounds on an
open-domain learning machine?s performance. Their analysis shows that the choice of
representation is crucial to domain adaptation. A good choice of representation must
allow a learning machine to achieve low error rates on the source domain. Just as
important, however, is that the representation must simultaneously make the source
and target domains look as similar to one another as possible. That is, if the labeling
function f is the same on the source and target domains, then for every h ? H, we can
provably bound the error of h on the target domain by its error on the source domain
plus a measure of the distance between DS and DT:
Ex?DTL(x, R, f, h) ? Ex?DSL(x, R, f, h) + d1(R(DS), R(DT )) (3)
where the variation divergence d1 is given by
d1(D,D?) = 2 sup
B?B
|PrD[B] ? PrD? [B]| (4)
where B is the set of measurable sets under D and D? (Ben-David et al. 2007, 2010).
Crucially, the distance between domains depends on the features in the representa-
tion. The more that features appear with different frequencies in different domains, the
worse this bound becomes. In fact, one lower bound for the d1 distance is the accuracy
of the best classifier for predicting whether an unlabeled instance y = R(x) belongs to
domain S or T (Ben-David et al. 2010). Thus, if R provides one set of common features for
examples from S, and another set of common features for examples from T, the domain
of an instance becomes easy to predict, meaning the distance between the domains
grows, and the bound on our classifier?s performance grows worse.
In light of Ben-David et al.?s theoretical findings, traditional representations in
NLP are inadequate for domain adaptation because they contribute to the d1 distance
between domains. Although many previous studies have shown that lexical features
allow learning systems to achieve impressively low error rates during training, they also
make texts from different domains look very dissimilar. For instance, a feature based on
the word ?bank? or ?CEO? may be common in a domain of newswire text, but scarce
or nonexistent in, say, biomedical literature. Ben David et al.?s theory predicts greater
variance in the error rate of the target domain classifier as the distance grows.
At the same time, traditional representations contribute to data sparsity, a lack of
sufficient training data for the relevant parameters of the system. In traditional super-
vised NLP systems, there are parameters for each word type in the data, or perhaps
even combinations of word types. Because vocabularies can be extremely large, this
leads to an explosion in the number of parameters. As a consequence, for many of their
parameters, supervised NLP systems have zero or only a handful of relevant labeled
examples (Bikel 2004a, 2004b). No matter how sophisticated the learning technique, it
is difficult to estimate parameters without relevant data. Because vocabularies differ
across domains, domain adaptation greatly exacerbates this issue of data sparsity.
2.4 Problem Formulation for the Domain Adaptation Setting
Formally, we define the task of representation learning for domain adaptation as the
following optimization problem: Given a set of unlabeled instances US drawn from the
89
Computational Linguistics Volume 40, Number 1
source domain and unlabeled instances UT from the target domain, as well as a set of
labeled instances LS drawn from the source domain, identify a function R? from the
space of possible representations R that minimizes
R?, h? = argmin
R?R,h?H
(
Ex?DSL(x, R, f, h)
)
+ ?d1(R(DS), R(DT )) (5)
where ? is a free parameter.
Note that there is an underlying tension between the two terms of the objec-
tive function: The best representation for the source domain would naturally include
domain-specific features, and allow a hypothesis to learn domain-specific patterns.
We are aiming, however, for the best general classifier, which happens to be trained
on training data from one domain (or a few domains). The domain-specific features
contribute to distance between domains, and to classifier errors on data taken from
domains not seen in training. By optimizing for this combined objective function, we
allow the optimization method to trade off between features that are best for classifying
source-domain data and features that allow generalization to new domains.
Unlike the representation-learning problem-formulation in Equation (2), Equa-
tion (5) does not reduce to the standard machine-learning problem (Equation (1)). In
a sense, the d1 term acts as a regularizer on R, which also affects H. Representation
learning for domain adaptation is a fundamentally novel learning task.
2.5 Tractable Representation Learning: Statistical Language Models
as Representations
For most hypothesis classes and any interesting space of representations, Equations (2)
and (5) are completely intractable to optimize exactly. Even given a fixed representation,
it is intractable to compute the best hypothesis for many hypothesis classes. And the d1
metric is intractable to compute from samples of a distribution, although Ben-David
et al. (2007, 2010) propose some tractable bounds. We view these problem formulations
as high-level goals rather than as computable objectives.
As a tractable objective, in this work we describe an investigation into the use of
statistical language models as a way to represent the meanings of words. This approach
depends on the well-known distributional hypothesis, which states that a word?s
meaning is identified with the contexts in which it appears (Harris 1954; Hindle 1990).
From this hypothesis, we can formulate the following testable prediction, which we call
the statistical language model representation hypothesis, or LMRH:
To the extent that a model accurately describes a word?s possible contexts, parameters
of that model are highly informative descriptors of the word?s meaning, and are
therefore useful as features in NLP tasks like POS tagging, chunking, NER, and
information extraction.
The LMRH says, essentially, that for NLP tasks, we can decouple the task of optimiz-
ing a representation from the task of optimizing a hypothesis. To learn a representation,
we can train a statistical language model on unlabeled text, and then use parameters
or latent states from the statistical language model to create a representation function.
Optimizing a hypothesis then follows the standard learning framework, using the
representation from the statistical language model.
90
Huang et al. Computational Linguistics
The LMRH is similar to the manifold and cluster assumptions behind other semi-
supervised approaches to machine learning, such as Alternating Structure Optimization
(ASO) (Ando and Zhang 2005) and Structural Correspondence Learning (SCL) (Blitzer,
McDonald, and Pereira 2006). All three of these techniques use predictors built on
unlabeled data as a way to harness the manifold and cluster assumptions. However,
the LMRH is distinct from at least ASO and SCL in important ways. Both ASO and SCL
create multiple ?synthetic? or ?pivot? prediction tasks using unlabeled data, and find
transformations of the input feature space that perform well on these tasks. The LMRH,
on the other hand, is more specific ? it asserts that for language problems, if we opti-
mize word representations on a single task (the language modeling task), this will lead
to strong performance on weakly supervised tasks. In reported experiments on NLP
tasks, both ASO and SCL use certain synthetic predictors that are essentially language
modeling tasks, such as the task of predicting whether the next token is of word type w.
To the extent that these techniques? performance relies on language-modeling tasks as
their ?synthetic predictors,? they can be viewed as evidence in support of the LMRH.
One significant consequence of the LMRH is that it allows us to leverage well-
developed techniques and models from statistical language modeling. Section 3
presents a series of statistical language models that we investigate for learning repre-
sentations for NLP.
2.6 Previous Work
There is a long tradition of NLP research on representations, mostly falling into one of
four categories: 1) vector space models of meaning based on document-level lexical co-
occurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010);
2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990;
Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and
Jordan 2003; Va?yrynen and Honkela 2004, 2005; Va?yrynen, Honkela, and Lindqvist
2007); 3) using clusters that are induced from distributional similarity (Brown et al.
1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse
features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu
2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao
et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008;
Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih
and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert
and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering
for representations, but where previous work has used bigram and trigram statistics to
form clusters, we build sophisticated models that attempt to capture the context of a
word, and hence its similarity to other words, more precisely. Our experiments show
that the new graphical models provide representations that outperform those from
previous work on several tasks.
Neural network statistical language models have recently achieved state-of-the-art
perplexity results (Mnih and Hinton 2009), and representations based on them have im-
proved in-domain chunking, NER, and SRL (Weston, Ratle, and Collobert 2008; Turian,
Bergstra, and Bengio 2009; Turian, Ratinov, and Bengio 2010). As far as we are aware,
Turian, Ratinov, and Bengio (2010) is the only other work to test a learned representation
on a domain adaptation task, and they show improvement on out-of-domain NER
with their neural net representations. Though promising, the neural network models
are computationally expensive to train, and these statistical language models work
only on fixed-length histories (n-grams) rather than full observation sequences. Turian,
91
Computational Linguistics Volume 40, Number 1
Ratinov, and Bengio?s (2010) tests also show that Brown clusters perform as well or
better than neural net models on all of their chunking and NER tests. We concentrate on
probabilistic graphical models with discrete latent states instead. We show that HMM-
based and other representations significantly outperform the more commonly used
Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings
of sequence-labeling tasks.
Most previous work on domain adaptation has focused on the case where some
labeled data are available in both the source and target domains (Chan and Ng 2006;
Daume? III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daume? III 2007; Jiang
and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze,
Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are
known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this
problem setting have focused on appropriately weighting examples from the source and
target domains so that the learning algorithm can balance the greater relevance of the
target-domain data with the larger source-domain data set. In some cases, researchers
combine this approach with semi-supervised learning to include unlabeled examples
from the target domain as well (Daume? III, Kumar, and Saha 2010). These techniques
do not handle open-domain corpora like the Web, where they require expert input to
acquire labels for each new single-domain corpus, and it is difficult to come up with
a representative set of labeled training data for each domain. Our technique requires
only unlabeled data from each new domain, which is significantly easier and cheaper to
acquire. Where target-domain labeled data is available, however, these techniques can
in principle be combined with ours to improve performance, although this has not yet
been demonstrated empirically.
A few researchers have considered the more general case of domain adaptation
without labeled data in the target domain. Perhaps the best known is Blitzer, McDonald,
and Pereira?s (2006) Structural Correspondence Learning (SCL). SCL uses ?pivot? words
common to both source and target domains, and trains linear classifiers to predict these
pivot words from their context. After an SVD reduction of the weight vectors for these
linear classifiers, SCL projects the original features through these weight vectors to
obtain new features that are added to the original feature space. Like SCL, our language
modeling techniques attempt to predict words from their context, and then use the
output of these predictions as new features. Unlike SCL, we attempt to predict all words
from their context, and we rely on traditional probabilistic methods for language mod-
eling. Our best learned representations, which involve significantly different techniques
from SCL, especially latent-variable probabilistic models, significantly outperform SCL
in POS tagging experiments.
Other approaches to domain adaptation without labeled data from the target do-
main include Satpal and Sarawagi (2007), who show that by changing the optimization
function during conditional random field (CRF) training, they can learn classifiers that
port well to new domains. Their technique selects feature subsets that minimize the
distance between training text and unlabeled test text, but unlike our techniques, theirs
cannot learn representations with features that do not appear in the original feature set.
In contrast, we learn hidden features through statistical language models. McClosky,
Charniak, and Johnson (2010) use classifiers from multiple source domains and features
that describe how much a target document diverges from each source domain to deter-
mine an optimal weighting of the source-domain classifiers for parsing the target text.
However, it is unclear if this ?source-combination? technique works well on domains
that are not mixtures of the various source domains. Dai et al. (2007) use KL-divergence
between domains to directly modify the parameters of their naive Bayes model for a
92
Huang et al. Computational Linguistics
text classification task trained purely on the source domain. These last two techniques
are not representation learning, and are complementary to our techniques.
Our representation-learning approach to domain adaptation is an instance of
semi-supervised learning. Of the vast number of semi-supervised approaches to
sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki?s
(2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text
to achieve the current best performance on in-domain chunking, and semi-supervised
approaches to improving in-domain SRL with large quantities of unlabeled text
(Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Fu?rstenau and
Lapata 2009). Ando and Zhang?s (2005) semi-supervised sequence labeling technique
has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and
Pereira 2006); our representation-learning approaches outperform it. Unlike most semi-
supervised techniques, we concentrate on a particularly simple task decomposition: un-
supervised learning for new representations, followed by standard supervised learning.
In addition to our task decomposition being simple, our learned representations are also
task-independent, so we can learn the representation once, and then apply it to any task.
One of the best-performing representations that we consider for domain adaptation
is based on the HMM (Rabiner 1989). HMMs have of course also been used for super-
vised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and
Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised
POS tagging have focused on incorporating prior knowledge into the POS induction
model (Grac?a et al. 2009; Toutanova and Johnson 2007), or on new training techniques
like contrastive estimation (Smith and Eisner 2005) for alternative sequence models.
Despite the fact that completely connected, standard HMMs perform poorly at the POS
induction task (Johnson 2007), we show that they still provide very useful features
for a supervised POS tagger. Experiments in information extraction have previously
also shown that HMMs provide informative features for this quite different, semantic
processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010).
This article extends our previous work on learning representations for do-
main adaptation (Huang and Yates 2009, 2010) by investigating new language
representations?the naive Bayes representation and PL-MRF representation (Huang
et al. 2011)?by analyzing results in terms of polysemy, sparsity, and domain diver-
gence; by testing on new data sets including a Chinese POS tagging task; and by pro-
viding an empirical comparison with Brown clusters as representations.
3. Learning Representations of Distributional Similarity
In this section, we will introduce several representation learning models.
3.1 Traditional POS-Tagging Representations
As an example of our terminology, we begin by describing a representation used in
traditional POS taggers (this representation will later form a baseline for our POS
tagging experiments). The instance set X is the set of English sentences, and Z is the set
of POS tag sequences. A traditional representation TRAD-R maps a sentence x ? X to a
sequence of boolean-valued vectors, one vector per word xi in the sentence. Dimensions
for each latent vector include indicators for the word type of xi and various orthographic
features. Table 1 presents the full list of features in TRAD-R. Because our IE task classifies
word types rather than tokens, this baseline is not appropriate for that task. Herein, we
93
Computational Linguistics Volume 40, Number 1
Table 1
Summary of features provided by our representations. ?a1[g(a)] represents a set of boolean
features, one for each value of a, where the feature is true iff g(a) is true. xi represents a token at
position i in sentence x, w represents a word type, Suffixes = {-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity},
k (and k) represents a value for a latent state (set of latent states) in a latent-variable model, y?
represents the maximum a posteriori sequence of states y for x, yi is the latent variable for xi, and
yi,j is the latent variable for xi at layer j. prefix(y,p) is the p-length prefix of the Brown cluster y.
Representation Features
TRAD-R ?w1[xi = w]
?s?Suffixes1[xi ends with s]
1[xi contains a digit]
n-GRAM-R ?w? ,w??P(w?ww??)/P(w)
LSA-R ?w,j{v?left(w)}j
?w,j{v?right(w)}j
NB-R ?k1[y?i = k]
HMM-TOKEN-R ?k1[y?i = k]
HMM-TYPE-R ?kP(y = k|x = w)
I-HMM-TOKEN-R ?j,k1[y?i,j = k]
I-HMM-TYPE-R ?j,kP(y.,j = k|x = w)
BROWN-TOKEN-R ?j?{?2,?1,0,1,2}
?p?{4,6,10,20} prefix(yi+j, p)
BROWN-TYPE-R ?p prefix(y, p)
LATTICE-TOKEN-R ?j,k1[y?i,j = k]
LATTICE-TYPE-R ?kP(y = k|x = w)
describe how we can learn representations R by using a variety of statistical language
models, for use in both our IE and POS tagging tasks. All representations for POS
tagging inherit the features from TRAD-R; all representations for IE do not.
3.2 n-gram Representations
n-gram representations, which we call n-GRAM-R, model a word type w in terms of the
n-gram contexts in which w appears in a corpus. Specifically, for word w we generate
the vector P(w?ww??)/P(w), the conditional probability of observing the word sequence
w? to the left and w?? to the right of w. Each dimension in this vector represents a com-
bination of the left and right words. The experimental section describes the particular
corpora and statistical language modeling methods used for estimating probabilities.
Note that these features depend only on the word type w, and so for every token xi = w,
n-GRAM-R provides the same set of features regardless of local context.
One drawback of n-GRAM-R is that it does not handle sparsity well?the features
are as sparsely observed as the lexical features in TRAD-R, except that n-GRAM-R fea-
tures can be obtained from larger corpora. As an alternative, we apply latent semantic
analysis (LSA) (Deerwester et al. 1990) to compute a reduced-rank representation. For
word w, let vright(w) represent the right context vector of w, which in each dimension
contains the value of P(ww??)/P(w) for some word w??, as observed in the n-gram
model. Similarly, let vleft(w) be the left context vector of w. We apply LSA to the set
94
Huang et al. Computational Linguistics
   
 


	 	 	 	
 	


 
Figure 1
A graphical representation of the naive Bayes statistical language model. The B and E are special
dummy words for the beginning and end of the sentence.
of right context vectors and the set of left context vectors separately,1 to find reduced-
rank versions v?right(w) and v
?
left(w), where each dimension represents a combination
of several context word types. We then use each component of v?right(w) and v
?
left(w)
as features. After experimenting with different choices for the number of dimensions to
reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes
the performance of our supervised sequence labelers on held-out data. We call this
model LSA-R.
3.3 A Context-Dependent Representation Using Naive Bayes
The n-GRAM-R and LSA-R representations always produce the same features F for a
given word type w, regardless of the local context of a particular token xi = w. Our
remaining representations are all context-dependent, in the sense that the features
provided for token xi depend on the local context around xi. We begin with a statis-
tical language model based on the Naive Bayes model with categorical latent states
S = {1, . . . , K}. First, we form trigrams from our sentences. For each trigram, we form a
separate Bayes net in which each token from the trigram is conditionally independent
given the latent state. For tokens xi?1, xi, and xi+1, the probability of this trigram given
latent state Yi = y is given by:
P(xi?1, xi, xi+1|yi) = Pleft(xi?1|yi)Pmid(xi|yi)Pright(xi+1|yi) (6)
where Pleft, Pmid, and Pright are multinomial distributions conditioned on the latent state.
The probability of a whole sentence is then given by the product of the probabilities
of its trigrams. Figure 1 shows a graphical representation of this model. We train our
models using standard expectation-maximization (Dempster, Laird, and Rubin 1977)
with random initialization of the parameters.
Because our factorization of the sentence does not take into account the fact that the
trigrams overlap, the resulting statistical language model is mass-deficient. Worse still,
it is throwing away information from the dependencies among trigrams which might
help make better clustering decisions. Nevertheless, this model closely mirrors many
of the clustering algorithms used in previous approaches to representation learning for
sequence labeling (Ushioda 1996; Miller, Guinness, and Zamanian 2004; Koo, Carreras,
1 Compare with Dhillon, Foster, and Ungar (2011), who use canonical correlation analysis to find a
simultaneous reduction of the left and right context vectors, a significantly more complex undertaking.
95
Computational Linguistics Volume 40, Number 1
and Collins 2008; Lin and Wu 2009; Ratinov and Roth 2009), and therefore serves as an
important benchmark.
Given a naive Bayes statistical language model, we construct an NB-R representa-
tion that produces |S| boolean features Fs(xi) for each token xi and each possible latent
state s ? S:
Fs(xi) =
{
true if s = arg maxs??SP(xi?1, xi, xi+1|yi = s?),
false otherwise.
For a reasonable choice of S (i.e., |S|  |V|), each feature should be observed often
in a sufficiently large training data set. Therefore, compared with n-GRAM-R, NB-R
produces far fewer features. On the other hand, its features for xi depend not just on
the contexts in which xi has appeared in the statistical language model?s training data,
but also on xi?1 and xi+1 in the current sentence. Furthermore, because the range of
the features is much more restrictive than real-valued features, it is less prone to data
sparsity or variations across domains than real-valued features.
3.4 Context-Dependent, Structured Representations: The Hidden Markov Model
In previous work, we have implemented several representations based on hidden
Markov models (Rabiner 1989), which we used for both sequential labeling (like POS
tagging [Huang et al. 2011] and NP chunking [Huang and Yates 2009]) and IE (Downey,
Schoenmackers, and Etzioni 2007). Figure 2 shows a graphical model of an HMM. An
HMM is a generative probabilistic model that generates each word xi in the corpus
conditioned on a latent variable yi. Each yi in the model takes on integral values from 1
to K, and each one is generated by the latent variable for the preceding word, yi?1. The
joint distribution for a corpus x = (x1, . . . , xN ) and a set of state vectors y = (y1, . . . , yN )
is given by: P(x, y) =
?
i P(xi|yi)P(yi|yi?1). Using expectation-maximization (EM)
(Dempster, Laird, and Rubin 1977), it is possible to estimate the distributions for
P(xi|yi) and P(yi|yi?1) from unlabeled data.
We construct two different representations from HMMs, one for sequence-labeling
tasks and one for IE. For sequence labeling, we use the Viterbi algorithm to produce the
optimal setting y? of the latent states for a given sentence x, or y? = argmax
y
P(x, y). We
use the value of y?i as a new feature for xi that represents a cluster of distributionally
similar words. For IE, we require features for word types w, rather than tokens xi.
Applying Bayes? rule to the HMM parameters, we compute a distribution P(Y|x = w),
where Y is a single latent node, x is a single token, and w is its word type. We then use
each of the K values for P(Y = k|x = w), where k ranges from 1 to K, as features. This set
   
 

	 	 	 	
 	

Figure 2
The Hidden Markov Model.
96
Huang et al. Computational Linguistics
of features represents a ?soft clustering? of w into K different clusters. We refer to these
representations as HMM-TOKEN-R and HMM-TYPE-R, respectively.
We also compare against a multi-layer variation of the HMM from our previous
work (Huang and Yates 2010). This model trains an ensemble of M independent HMM
models on the same corpus, initializing each one randomly. We can then use the Viterbi-
optimal decoded latent state of each independent HMM model as a separate feature for
a token, or the posterior distribution for P(Y|x = w) from each HMM as a separate set
of features for each word type. We refer to this statistical language model as an I-HMM,
and the representations as I-HMM-TOKEN-R and I-HMM-TYPE-R, respectively.
Finally, we compare against Brown clusters (Brown et al. 1992) as learned features.
Although not traditionally described as such, Brown clustering involves constructing
an HMM model in which each word type is restricted to having exactly one latent state
that may generate it. Brown et al. describe a greedy agglomerative clustering algorithm
for training this model on unlabeled text. Following Turian, Ratinov, and Bengio (2010),
we use Percy Liang?s implementation of this algorithm for our comparison, and we test
runs with 100, 320, 1,000 and 3,200 clusters. We use features from these clusters identical
to Turian et al.?s.2 Turian et al. have shown that Brown clusters match or exceed the
performance of neural network-based statistical language models in domain adaptation
experiments for named-entity recognition, as well as in-domain experiments for NER
and chunking.
Because HMM-based representations offer a small number of discrete states as
features, they have a much greater potential to combat sparsity than do n-gram mod-
els. Furthermore, for token-based representations, these models can potentially handle
polysemy better than n-gram statistical language models by providing different features
in different contexts.
3.5 A Novel Lattice Statistical Language Model Representation
Our final statistical language model is a novel latent-variable statistical language model,
called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3. The
model contains a lattice of M ? N latent states, where N is the number of words in a
sentence and M is the number of layers in the model. The dotted and solid lines in the
figure together form a complete lattice of edges between these nodes; the PL-MRF uses
only the solid edges. Formally, let c = 	N2 
, where N is the length of the sentence; let i
denote a position in the sentence, and let j denote a layer in the lattice. If i < c and j is
odd, or if j is even and i > c, we delete edges between yi,j and yi,j+1 from the complete
lattice. The same set of nodes remains, but the partial lattice contains fewer edges and
paths between the nodes. A central ?trunk? at i = c connects all layers of the lattice, and
branches from this trunk connect either to the branches in the layer above or the layer
below (but not both).
The result is a model that retains most of the edges of the complete lattice, but
unlike the complete lattice, it supports tractable inference. As M, N ? ?, five out of
every six edges from the complete lattice appear in the PL-MRF. However, the PL-MRF
makes the branches conditionally independent from one another, except through the
trunk. For instance, the left branch between layers 1 and 2 ((y1,1, y1,2) and (y2,1, y2,2)) in
Figure 3 are disconnected; similarly, the right branch between layers 2 and 3 ((y4,2, y4,3)
and (y5,2, y5,3)) are disconnected, except through the trunk and the observed nodes. As
2 Percy Liang?s implementation is available at http://metaoptimize.com/projects/wordreprs/.
97
Computational Linguistics Volume 40, Number 1
y4,1
y3,1
y4,2
y3,2
y4,3
y3,3
y4,4
y3,4
y4,5
y3,5
x1
y2,1
y1,1
x2
y2,2
y1,2
x3
y2,3
y1,3
x4
y2,4
y1,4
x5
y2,5
y1,5
Figure 3
The PL-MRF model for a five-word sentence and a four-layer lattice. Dashed gray edges are part
of a complete lattice, but not part of the PL-MRF.
a result, excluding the observed nodes, this model has a low tree-width of 2 (excluding
observed nodes), and a variety of efficient dynamic programming and message-passing
algorithms for training and inference can be readily applied (Bodlaender 1988). Our
inference algorithm passes information from the branches inwards to the trunk, and
then upward along the trunk, in time O(K4MN). In contrast, a fully connected lattice
model has tree-width = min(M, N), making inference and learning intractable (Sutton,
McCallum, and Rohanimanesh 2007), partly because of the difficulty in enumerating
and summing over the exponentially-many configurations y for a given x.
We can justify the choice of this model from a linguistic perspective as a way to
capture the multi-dimensional nature of words. Linguists have long argued that words
have many different features in a high dimensional space: They can be separately
described by part of speech, gender, number, case, person, tense, voice, aspect, mass
vs. count, and a host of semantic categories (agency, animate vs. inanimate, physical vs.
abstract, etc.), to name a few (Sag, Wasow, and Bender 2003). In the PL-MRF, each layer
of nodes is intended to represent some latent dimension of words.
We represent the probability distribution for PL-MRFs as log-linear models that
decompose over cliques in the MRF graph. Let Cliq(x, y) represent the set of all maximal
cliques in the graph of the MRF model for x and y. Expressing the lattice model in log-
linear form, we can write the marginal probability P(x) of a given sentence x as:
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x?,y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
where score(c, x, y) = exp(?c ? fc(xc, yc)). Our model includes parameters for transitions
between two adjacent latent variables on layer j: ?transi,s,i+1,s?,j for yi,j = s and yi+1,j = s
?. It
also includes observation parameters for latent variables and tokens, as well as for pairs
of adjacent latent variables in different layers and their tokens: ?obsi,j,s,w and ?
obs
i,j,s,j+1,s?,w for
yi,j = s, yi,j+1 = s?, and xi = w.
98
Huang et al. Computational Linguistics
As with our HMM models, we create two representations from PL-MRFs, one for
tokens and one for types. For tokens, we decode the model to compute y?, the matrix of
optimal latent state values for sentence x. For each layer j and and each possible latent
state value k, we add a boolean feature for token xi that is true iff y?i,j = k. For word
types, we compute distributions over the latent state space. Let y be a column vector of
latent variables for word type w. For a PL-MRF model with M layers of binary variables,
there are 2M possible values for y. Our type representation computes a probability
distribution over these 2M possible values, and uses each probability as a feature for
w.3 We refer to these two representations as LATTICE-TOKEN-R and LATTICE-TYPE-R,
respectively.
We train the PL-MRF using contrastive estimation (Smith and Eisner 2005), which
iteratively optimizes the following objective function on a corpus X:
?
x?X
log
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x??N (x),y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
(7)
where N (x), the neighborhood of x, indicates a set of perturbed variations of the original
sentence x. Contrastive estimation seeks to move probability mass away from the per-
turbed neighborhood sentences and onto the original sentence. We use a neighborhood
function that includes all sentences which can be obtained from the original sentence by
swapping the order of a consecutive pair of words. Training uses gradient descent over
this non-convex objective function with a standard software package (Liu and Nocedal
1989) and converges to a local maximum or saddle point.
For tractability, we modify the training procedure to train the PL-MRF one layer
at a time. Let ?i represent the set of parameters relating to features of layer i, and let
??i represent all other parameters. We fix ??0 = 0, and optimize ?0 using contrastive
estimation. After convergence, we fix ??1, and optimize ?1, and so on. For training each
layer, we use a convergence threshold of 10?6 on the objective function in Equation (7),
and each layer typically converges in under 100 iterations.
4. Domain Adaptation with Learned Representations
We evaluate the representations described earlier on POS tagging and NP chunking
tasks in a domain adaptation setting.
4.1 A Rich Problem Setting for Representation Learning
Existing supervised NLP systems are domain-dependent: There is a substantial drop in
their performance when tested on data from a new domain. Domain adaptation is the
task of overcoming this domain dependence. The aim is to build an accurate system for
3 This representation is only feasible for small numbers of layers, and in our experiments that require type
representations, we used M = 10. For larger values of M, other representations are also possible. We also
experimented with a representation which included only M possible values: For each layer l, we included
P(yl = 0|w) as a feature. We used the less-compact representation in our experiments because results
were better.
99
Computational Linguistics Volume 40, Number 1
a target domain by training on labeled examples from a separate source domain. This
problem is sometimes also called transfer learning (Raina et al. 2007).
Two of the challenges for NLP representations, sparsity and polysemy, are exacer-
bated by domain adaptation. New domains come with new words and phrases that
appear rarely (or even not at all) in the training domain, thus increasing problems
with data sparsity. And even for words that do appear commonly in both domains, the
contexts around the words will change from the training domain to the target domain.
As a result, domain adaptation adds to the challenge of handling polysemous words,
whose meaning depends on context.
In short, domain adaptation is a challenging setting for testing NLP representations.
We now present several experiments testing our representations against state-of-the-
art POS taggers in a variety of domain adaptation settings, showing that the learned
representations surpass the previous state-of-the-art, without requiring any labeled data
from the target domain.
4.2 Experimental Set-up
For domain adaptation, we test our representations on two sequence labeling tasks:
POS tagging and chunking. To incorporate learned representation into our models, we
follow this general procedure, although the details vary by experiment and are given in
the following sections. First, we collect a set of unannotated text from both the training
domain and test domain. Second, we learn representations on the unannotated text.
We then automatically annotate both the training and test data with features from the
learned representation. Finally, we train a supervised linear-chain CRF model on the
annotated training set and apply it to the test set.
A linear-chain CRF is a Markov random field (Darroch, Lauritzen, and Speed 1980)
in which the latent variables form a path with edges only between consecutive nodes in
the path, and all latent variables are globally conditioned on the observations. Let X be a
random variable over data sequences, and Z be a random variable over corresponding
label sequences. The conditional distribution over the label sequence Z given X has the
form
p?(Z = z|X = x) ? exp
?
?
?
i
?
j
?j fj(zi?1, zi, x, i)
?
? (8)
where fj(zi?1, zi, x, i) is a real-valued feature function of the entire observation sequence
and the labels at positions i and i ? 1 in the label sequence, and ?j is a parameter to be
estimated from training data.
We use an open source CRF software package designed by Sunita Sarawagi to train
and apply our CRF models.4 As is standard, we use two kinds of feature functions:
transition and observation. Transition feature functions indicate, for each pair of labels
l and l?, whether zi = l and zi?1 = l?. Boolean observation feature functions indicate, for
each label l and each feature f provided by a representation, whether zi = l and xi has
feature f . For each label l and each real-valued feature f in representation R, real-valued
observation feature functions have value f (x) if zi = l, and are zero otherwise.
4 Available from http://sourceforge.net/projects/crf/.
100
Huang et al. Computational Linguistics
4.3 Domain Adaptation for POS Tagging
Our first experiment tests the performance of all the representations we introduced
earlier on an English POS tagging task, trained on newswire text, to tag biomedical re-
search literature. We follow Blitzer et al.?s experimental set-up. The labeled data consists
of the WSJ portion of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993)
as source domain data, and 561 labeled sentences (9,576 tokens) from the biomedical
research literature database MEDLINE as target domain data (PennBioIE 2005). Fully
23% of the tokens in the labeled test text are never seen in the WSJ training data. The
unlabeled data consists of the WSJ text plus 71,306 additional sentences of MEDLINE
text (Blitzer, McDonald, and Pereira 2006). As a preprocessing step, we replace hapax
legomena (defined as words that appear once in our unlabeled training data) with
the special symbol *UNKNOWN*, and do the same for words in the labeled test sets that
never appeared in any of our unlabeled training text.
For representations, we tested TRAD-R, n-GRAM-R, LSA-R, NB-R, HMM-TOKEN-
R, I-HMM-TOKEN-R (between 2 and 8 layers), and LATTICE-TOKEN-R (8, 12, 16,
and 20 layers). Each latent node in the I-HMMs had 80 possible values, creating
808 ? 1015 possible configurations of the eight-layer I-HMM for a single word. Each
node in our PL-MRF is binary, creating a much smaller number (220 ? 106) of possible
configurations for each word in a 20-layer representation. To give the n-gram model
the largest training data set available, we trained it on the Web 1Tgram corpus (Brants
and Franz 2006). We included the top 500 most common n-grams for each word type,
and then used mutual information on the training data to select the top 10,000 most
relevant n-gram features for all word types, in order to keep the number of features
manageable. We incorporated n-gram features as binary values indicating whether xi
appeared with the n-gram or not. For comparison, we also report on the performance of
Brown clusters (100, 320, 1,000, and 3,200 possible clusters), following Turian, Ratinov,
and Bengio (2010). Finally, we compare against Blitzer, McDonald, and Pereira (2006)
SCL technique, described in Section 2.6, and the standard semi-supervised learning
algorithm ASO (Ando and Zhang 2005), whose results on this task were previously
reported by Blitzer, McDonald, and Pereira (2006).
Table 2 shows the results for the best variation of each kind of model?20 layers for
the PL-MRF, 7 layers for the I-HMM, and 3,200 clusters for the Brown clustering. All
statistical language model representations outperform the TRAD-R baseline.
In nearly all cases, learned representations significantly outperformed TRAD-R. The
best representation, the 20-layer LATTICE-TOKEN-R, reduces error by 47% (35% on
OOV) relative to the baseline TRAD-R, and by 44% (24% on out-of-vocabulary words
(OOV)) relative to the benchmark SCL system. For comparison, this model achieved a
96.8% in-domain accuracy on Sections 22?24 of the Penn Treebank, about 0.5 percentage
point shy of a state-of-the-art in-domain system with more sophisticated supervised
learning (Shen, Satta, and Joshi 2007). The BROWN-TOKEN-R representation, which
Turian, Ratinov, and Bengio (2010) demonstrated performed as well or better than
a variety of neural network statistical language models as representations, achieved
accuracies between the SCL system and the HMM-TOKEN-R. The WEB1T-n-GRAM-R,
I-HMM-TOKEN-R, and LATTICE-TOKEN-R all performed quite close to one another,
but the I-HMM-TOKEN-R and LATTICE-TOKEN-R were trained on many orders of
magnitude less text. The LSA-R and NB-R outperformed the TRAD-R baseline but
not the SCL system. The n-GRAM-R, which was trained on the same text as the
other representations except the WEB1T-n-GRAM-R, performed far worse than the
WEB1T-n-GRAM-R.
101
Computational Linguistics Volume 40, Number 1
Table 2
Learned representations, and especially latent-variable statistical language model
representations, significantly outperform a traditional CRF system on domain adaptation for
POS tagging. Percent error is shown for all words and out-of-vocabulary (OOV) words. The
SCL+500bio system was given 500 labeled training sentences from the biomedical domain.
1.8% of tokens in the biomedical test set had POS tags like ?HYPHENATED?, which are not
part of the tagset for the training data, and were labeled incorrectly by all systems without
access to labeled data from the biomedical domain. As a result, an error rate of 1.8 + 3.9 = 5.7
serves as a reasonable lower bound for a system that has never seen labeled examples from
the biomedical domain.
Model All words OOV words
TRAD-R 11.7 32.7
n-GRAM-R 11.7 32.2
LSA-R 11.6 31.1
NB-R 11.6 30.7
ASO 11.6 29.1
SCL 11.1 28
BROWN-TOKEN-R 10.0 25.2
HMM-TOKEN-R 9.5 24.8
WEB1T-n-GRAM-R 6.9 24.4
I-HMM-TOKEN-R 6.7 24
LATTICE-TOKEN-R 6.2 21.3
SCL+500bio 3.9 ?
The amount of unlabeled training data has a significant impact on the performance
of these representations. This is apparent in the difference between WEB1T-n-GRAM-
R and n-GRAM-R, but it is also true for our other representations. Figure 4 shows the
accuracy of a representative subset of our taggers on words not seen in labeled training
data, as we vary the amount of unlabeled training data available to the language
Figure 4
Learning curve for representations: target domain accuracy of our taggers on OOV words
(not seen in labeled training data), as a function of the number of unlabeled examples given
to the language models.
102
Huang et al. Computational Linguistics
models. Performance grows steadily for all representations we measured, and none
of the learning curves appears to have peaked. Furthermore, the margin between the
more complex graphical models and the simpler n-gram models grows with increasing
amounts of training data.
4.3.1 Sparsity and Polysemy. We expected that statistical language model represen-
tations would perform well in part because they provide meaningful features for
sparse and polysemous words. For sparse tokens, these trends are already evident
in the results in Table 2: Models that provide a constrained number of features, like
HMM-based models, tend to outperform models that provide huge numbers of fea-
tures (each of which, on average, is only sparsely observed in training data), like
TRAD-R.
As for polysemy, HMM models significantly outperform naive Bayes models and
the n-GRAM-R. The n-GRAM-R?s features do not depend on a token type?s context at all,
and the NB-R?s features depend only on the tokens immediately to the right and left of
the current word. In contrast, the HMM takes into account all tokens in the surrounding
sentence (although the strength of the dependence on more distant words decreases
rapidly). Thus the performance of the HMM compared with n-GRAM-R and NB-R,
as well as the performance of the LATTICE-TOKEN-R compared with the WEB1T-n-
GRAM-R, suggests that representations that are sensitive to the context of a word
produce better features.
To test these effects more rigorously, we selected 109 polysemous word types from
our test data, along with 296 non-polysemous word types. The set of polysemous word
types was selected by filtering for words in our labeled data that had at least two
POS tags that began with distinct letters (e.g., VBZ and NNS). An initial set of non-
polysemous word types was selected by filtering for types that appeared with just
one POS tag. We then manually inspected these initial selections to remove obvious
cases of word types that were in fact polysemous within a single part-of-speech, such
as ?bank.? We further define sparse word types as those that appear five times or
fewer in all of our unlabeled data, and we define non-sparse word types as those that
appear at least 50 times in our unlabeled data. Table 3 shows our POS tagging results
on the tokens of our labeled biomedical data with word types matching these four
categories.
As expected, all of our statistical language models outperform the baseline by
a larger margin on polysemous words than on non-polysemous words. The margin
between graphical model representations and the WEB1T-n-GRAM-R model also in-
creases on polysemous words, except for the NB-R. The WEB1T-n-GRAM-R uses none
of the local context to decide which features to provide, and the NB-R uses only the
immediate left and right context, so both models ignore most of the context. In contrast,
the remaining graphical models use Viterbi decoding to take into account all tokens
in the surrounding sentence, which helps to explain their relative improvement over
WEB1T-n-GRAM-R on polysemous words.
The same behavior is evident for sparse words, as compared with non-sparse
words: All of the statistical language model representations outperform the baseline
by a larger margin on sparse words than not-sparse words, and all of the graphical
models perform better relative to the WEB1T-n-GRAM-R on sparse words than not-
sparse words. By reducing the feature space from millions of possible n-gram fea-
tures to L categorical features, these models ensure that each of their features will
be observed often in a reasonably sized training data set. Thus representations based
103
Computational Linguistics Volume 40, Number 1
Table 3
Graphical models consistently outperform n-gram models by a larger margin on sparse words
than not-sparse words, and by a larger margin on polysemous words than not-polysemous
words. One exception is the NB-R, which performs worse relative to WEB1T-n-GRAM-R on
polysemous words than non-polysemous words. For each graphical model representation,
we show the difference in performance between that representation and WEB1T-n-GRAM-R
in parentheses. For each representation, differences in accuracy on polysemous and
non-polysemous subsets were statistically significant at p < 0.01 using a two-tailed
Fisher?s exact test. Likewise for performance on sparse vs. non-sparse categories.
polysemous not polysemous sparse not sparse
tokens 159 4,321 463 12,194
TRAD-R 59.5 78.5 52.5 89.6
WEB1T-n-GRAM-R 68.2 85.3 61.8 94.0
NB-R 64.5 88.7 57.8 89.4
(-WEB1T-n-GRAM-R) (?3.7) (+3.4) (?4.0) (?4.6)
HMM-TOKEN-R 67.9 83.4 60.2 91.6
(-WEB1T-n-GRAM-R) (?0.3) (?1.9) (?1.6) (?2.4)
I-HMM-TOKEN-R 75.6 85.2 62.9 94.5
(-WEB1T-n-GRAM-R) (+7.4) (?0.1) (+1.1) (+0.5)
LATTICE-TOKEN-R 70.5 86.9 65.2 94.6
(-WEB1T-n-GRAM-R) (+2.3) (+1.6) (+3.4) (+0.6)
on graphical models help address two key issues in building representations for POS
tagging.
4.3.2 Domain Divergence. Besides sparsity and polysemy, Ben-David et al.?s (2007, 2010)
theoretical analysis of domain adaptation shows that the distance between two domains
under a representation R of the data is crucial for a good representation. We test their
predictions using learned representations.
Ben-David et al.?s (2007, 2010) analysis depends on a particular notion of distance,
the d1 divergence, that is computationally intractable to calculate. For our analysis, we
resort instead to two different computationally efficient approximations of this measure.
The first uses a more standard notion of distance: the Jensen-Shannon Divergence (dJS),
a distance metric for probability distributions:
dJS(p||q) = 12
?
i
[
pilog
( pi
mi
)
+ qilog
( qi
mi
)]
where mi =
pi+qi
2 .
Intuitively, we aim to measure the distance between two domains by measuring
whether features appear more commonly in one domain than in the other. For instance,
the biomedical domain is far from the newswire domain under the TRAD-R repre-
sentation because word-based features like protein, gene, and pathway appear far more
commonly in the biomedical domain than the newswire domain. Likewise, bank and
president appear far more commonly in newswire text. Since the d1 distance is related
to the optimal classifier for distinguishing two domains, it makes sense to measure the
distance by comparing the frequencies of these features: a classifier can easily use the
occurrence of words like bank and protein to accurately predict whether a given sentence
belongs to the newswire or biomedical domain.
104
Huang et al. Computational Linguistics
More formally, let S and T be two domains, and let f be a feature5 in representation
R?that is, a dimension of the image space of R. Let V be the set of possible values
that f can take on. Let US be an unlabeled sample drawn from S, and likewise for
UT. We first compute the relative frequencies of the different values of f in R(US) and
R(UT ), and then compute dJS between these empirical distributions. Let pf represent the
empirical distribution over V estimated from observations of feature f in R(US), and let
qf represent the same distribution estimated from R(UT ).
Definition 1
JS domain divergence for a feature or df (US, UT ) is the domain divergence between
domains S and T under feature f from representation R, and is given by
df (US, UT ) = dJS(pf ||qf )
For a multidimensional representation, we compute the full domain divergence as a
weighted sum over the domain divergences for its features. Because individual features
may vary in their relevance to a sequence-labeling task, we use weights to indicate
their importance to the overall distance between the domains. We set the weight wf
for feature f proportional to the L1 norm of CRF parameters related to f in the trained
POS tagger. That is, let ? be the CRF parameters for our trained POS tagger, and let
?f = {?l,v|l be the state for zi and v be the value for f}. We set wf =
||?f ||1
||?||1 .
Definition 2
JS Domain Divergence or dR(US, UT ), is the distance between domains S and T under
representation R, and is given by
dR(US, UT ) =
?
f
wf df (US, UT )
Blitzer (2008) uses a different notion of domain divergence to approximate the d1
divergence, which we also experimented with. He trains a CRF classifier on examples
labeled with a tag indicating which domain the example was drawn from. We refer to
this type of classifier as a domain classifier. Note that these should not be confused
with our CRFs used for POS tagging, which take as input examples which are labeled
with POS sequences. For the domain classifier, we tag every token from the WSJ domain
as 0, and every token from the biomedical domain as 1. Blitzer then uses the accuracy
of his domain classifier on a held-out test set as his measure of domain divergence. A
high accuracy for the domain classifier indicates that the representation makes the two
domains easy to separate, and thus high accuracy signifies a high domain divergence. To
measure domain divergence using a domain classifier, we trained our representations
on all of the unlabeled data for this task, as before. We then used 500 randomly sampled
sentences from the WSJ domain, and 500 randomly sampled biomedical sentences, and
labeled these with 0 for the WSJ data and 1 for the biomedical data. We measured
the error rate of our domain-classifier CRF as the average error rate across folds when
performing three-fold cross-validation on these 1,000 sentences.
5 For simplicity, the definition we provide here works only for discrete features, although it is possible to
extend this definition to continuous-valued features.
105
Computational Linguistics Volume 40, Number 1
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.32 0.37 0.42 0.47
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Divergence under the Representation 
LATTICE-R
I-HMM-R
Trad-R
Ngram-R
1 HMM 
7 HMMs 
8 layer LATTICE 
20 layer LATTICE 
Figure 5
Target-domain POS tagging accuracy for a model developed using a representation R correlates
strongly with lower JS domain divergence between WSJ and biomedical text under each
representation R. The correlation coefficients r2 for the linear regressions drawn in the
figure are both greater than 0.97.
Figure 5 plots the accuracies and JS domain divergences for our POS taggers.
Figure 6 shows the difference between target-domain error and source-domain error
as a function of JS domain divergence. Figures 7 and 8 show the same information,
except that the x axis plots the accuracy of a domain classifier as the way of mea-
suring domain divergence. These results give empirical support to Ben-David et al.?s
(2007, 2010) theoretical analysis: Smaller domain divergence?whether measured by
JS domain divergence or by the accuracy of a domain classifier?correlates strongly
with better target-domain accuracy. Furthermore, smaller domain divergence correlates
strongly with a smaller difference in the accuracy of the taggers on the source and
target domains.
Figure 6
Smaller JS domain divergence correlates with a smaller difference between target-domain error
and source-domain error.
106
Huang et al. Computational Linguistics
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Classification Accuracy 
Trad Rep
I-HMM
Ngram
PL-MRF
1 HMM 
7 HMMs 
20 layer PL-MRF 
8 layer PL-MRF 
Figure 7
Target-domain tagging accuracy decreases with the accuracy of a CRF domain classifier.
Intuitively, this means that training data from a source domain is less helpful for tagging in
a target domain when source-domain data is easy to distinguish from target-domain data.
Figure 8
Better domain classification correlates with a larger difference between target-domain error and
source-domain error.
Although both the JS domain divergence and the domain classifier provide only
approximations of the d1 metric for domain divergence, they agree very strongly:
In both cases, the LATTICE-TOKEN-R representations had the lowest domain diver-
gence, followed by the I-HMM-TOKEN-R representations, followed by TRAD-R, with
n-GRAM-R somewhere between LATTICE-TOKEN-R and I-HMM-TOKEN-R. The main
difference between the two metrics appears to be that the JS domain divergence gives
a greater domain divergence to the eight-layer LATTICE-TOKEN-R model and the
n-GRAM-R, placing them past the four- through eight-layer I-HMM-TOKEN-R represen-
tations. The domain classifier places these models closer to the other LATTICE-TOKEN-R
representations, just past the seven-layer I-HMM-TOKEN-R representation.
107
Computational Linguistics Volume 40, Number 1
The domain divergences of all models, using both techniques for measuring diver-
gence, remain significantly far from zero, even under the best representation. As a result,
there is ample room to experiment with even less-divergent representations of the two
domains, to see if they might yield ever-increasing target-domain accuracies. Note that
this is not simply a matter of adding more layers to the layered models. The I-HMM-
TOKEN-R model performed best with seven layers, and the eight-layer representation
had about the same accuracy and domain divergence as the five-layer model. This
may be explained by the fact that the I-HMM layers are trained independently, and so
additional layers may be duplicating other ones, and causing the supervised classifier
to overfit. But it also shows that our current methodology has no built-in technique
for constraining the domain divergence in our representations?the decrease in domain
divergence from our more sophisticated representations is a coincidental byproduct of
our training methodology, but there is no guarantee that our current mechanisms will
continue to decrease domain divergence simply by increasing the number of layers. An
important consideration for future research is to devise explicit learning mechanisms
that guide representations towards smaller domain divergences.
4.4 Domain Adaptation for Noun-Phrase Chunking and Chinese POS Tagging
We test the generality of our representations by using them for other tasks, domains, and
languages. Here, we report on further sequence-labeling tasks in a domain adaptation
setting: noun phrase chunking for adaptation from news text to biochemistry journals,
and POS tagging in Mandarin for a variety of domains. In the next section, we describe
the use of our representations in a weakly supervised information extraction task.
For chunking, the training set consists of the CoNLL 2000 shared task data for
source-domain labeled data (Sections 15?18 of the WSJ portion of the Penn Treebank,
labeled with chunk tags) (Tjong, Sang, and Buchholz 2000). For test data, we used
biochemistry journal data from the Open American National Corpus6 (OANC). One
of the authors manually labeled 198 randomly selected sentences (5,361 tokens) from
the OANC biochemistry text with noun-phrase chunk information.7 We focus on noun
phrase chunks because they are relatively easy to annotate manually, but contain a large
variety of open-class words that vary from domain to domain. The labeled training set
consists of 8,936 sentences and 211,726 tokens. Twenty-three percent of chunks in the
test set begin with an OOV word (especially adjective-noun constructions like ?aqueous
formation? and ?angular recess?), and 29% begin with a word seen at most twice in
training data; we refer to these as OOV chunks and rare chunks. For our unlabeled
data, we use 15,000 sentences (358,000 tokens; Sections 13?19) of the Penn Treebank
and 45,000 sentences (1,083,000 tokens) from the OANC?s biochemistry section. We
tested TRAD-R (augmented with features for automatically generated POS tags), LSA-R,
n-GRAM-R, NB-R, HMM-TOKEN-R, I-HMM-TOKEN-R (7 layers, which performed best
for POS tagging) and LATTICE-TOKEN-R (20 layers) representations.
Figure 9 shows our NP chunking results for this domain adaptation task. The
performance improvements for the HMM-based chunkers are impressive: LATTICE-
TOKEN-R reduces error by 57% with respect to TRAD-R, and comes close to state-of-the-
art results for chunking on newswire text. The results suggest that this representation
allows the CRF to generalize almost as well to out-of-domain text as in-domain text.
6 Available from http://www.anc.org/OANC/.
7 The labeled data for this experiment are available from the first author?s Web site.
108
Huang et al. Computational Linguistics
F1
on
B
io
ch
em
is
tr
y
Te
xt
0.72 0.74 
0.75 0.76 
0.84 
0.87 0.89 0.86 0.87 0.87 0.88 
0.91 
0.94 0.94 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Trad-R Ngram-R LSA-R NB-R HMM-R I-HMM-R LATTICE-R
OOV ALL
Freq: 0 1 2 all
Chunks: 284 39 39 1,258
R P R P R P R P
TRAD-R .74 .70 .85 .87 .79 .86 .86 .87
n-GRAM-R .74 .74 .85 .85 .79 .86 .87 .87
LSA-R .76 .74 .82 .83 .78 .85 .87 .88
NB-R .73 .78 .86 .73 .86 .75 .88 .88
HMM-TOKEN-R .80 .89 .92 .88 .92 .90 .91 .90
I-HMM-TOKEN-R .90 .86 .92 .95 .87 .97 .95 .92
LATTICE-TOKEN-R .92 .85 .94 .95 .87 .97 .95 .93
Figure 9
On biomedical journal data from the OANC, our best NP chunker outperforms the baseline
CRF chunker by 0.17 F1 on chunks that begin with OOV words, and by 0.08 on all chunks. The
table shows performance breakdowns (recall and precision) for chunks whose first word has
frequency 0, 1, and 2 in training data, and the number of chunks in test data that fall into each
of these categories.
Improvements are greatest on OOV and rare chunks, where LATTICE-TOKEN-R made
absolute improvements over TRAD-R by 0.17 and 0.09 F1, respectively. Improvements
for the single-layer HMM-TOKEN-R were smaller but still significant: 36% relative re-
duction in error overall, and 32% for OOV chunks.
The improved performance from our HMM-based chunker caused us to wonder
how well the chunker could work without some of its other features. We removed all
tag features and orthographic features and all features for word types that appear fewer
than 20 times in training. This chunker still achieves 0.91 F1 on OANC data, and 0.93
F1 on WSJ data (Section 20), outperforming the TRAD-R system in both cases. It has
only 20% as many features as the baseline chunker, greatly improving its training time.
Thus these features are more valuable to the chunker than features from automatically
produced tags and features for all but the most common words.
For Chinese POS tagging, we use text from the UCLA Corpus of Written Chinese
(Tao and Xiao 2007), which is part of the Lancaster Corpus of Mandarin Chinese
(LCMC). The UCLA Corpus consists of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres (see Table 4). We use gold-standard word segmentation labels
for training and testing. The LCMC tagset consists of 50 Chinese POS tags. On average,
each genre contains 5,284 word tokens, for a total of 68,695 tokens among all genres. We
use the ?news? genre as our source domain, which we use for training and development
109
Computational Linguistics Volume 40, Number 1
Table 4
POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representations
outperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains. For target
domains, * indicates the performance is statistically significantly better than the Stanford and
TRAD-R baselines at p < 0.05, using a two-tailed ?2 test; ** indicates significance at p < 0.01.
On the news domain, the Stanford tagger is significantly different from all other systems
using a two-tailed ?2 test with p < 0.01.
Domain Stanford TRAD NGR LSA NB HMM I-H LAT
lore 88.4 84.0 84.2 85.3 85.3 89.7 89.9 90.1*
religion 83.5 79.1 79.4 79.8 80.0 85.2 85.6 85.9*
humour 89.0 84.2 84.5 86.2 86.8 89.6 89.6 89.9*
general-fic 87.5 84.5 85.0 85.3 85.7 89.4 89.7 89.9*
essay 88.4 83.2 83.7 84.0 84.3 89.0 89.1 90.1*
mystery 87.4 82.4 83.4 84.3 85.3 90.1 91.1 91.3**
romance 87.5 84.2 84.5 85.3 86.1 89.0 89.5 89.8**
science-fic 88.6 82.1 82.5 83.0 83.0 87.0 88.3 88.6
skills 82.7 77.3 77.7 78.2 78.4 84.9 85.0 85.1**
science 86.0 82.0 82.3 82.4 82.4 87.8 87.8 87.9*
adventure-fic 82.1 74.3 75.2 76.1 77.8 81.7 82.0 82.2
report 91.7 84.2 85.1 85.3 86.1 91.9 91.9 91.9
news 98.8** 96.9 92.3 93.4 94.3 94.2 97.0 97.1
all but news 87.0 81.2 82.0 82.8 83.6 88.1 88.4 88.8**
all domains 88.7 83.2 83.6 84.4 85.5 89.5 89.7 90.0**
data. For test data, we randomly select 20% of every other genre. For our unlabeled
data, we use all of the ?news? text, plus the remaining 80% of the texts from the other
genres. As before, we replace hapax legomena in the unlabeled data with the special
symbol *UNKNOWN*, and do the same for word types in the labeled test sets that never
appear in our unlabeled training texts. We compare against a state-of-the-art Chinese
POS tagger for in-domain text, the CRF-based Stanford tagger (Tseng, Jurafsky, and
Manning 2005). We obtained the code for this tagger,8 and retrained it on our training
data set.
The Chinese POS tagging results are shown in Table 4. The LATTICE-TOKEN-R
outperforms the state-of-the-art Stanford tagger on all target domains. Overall, on all
out-of-domain tests, LATTICE-TOKEN-R provides a relative reduction in error of 13.8%
compared with the Stanford tagger. The best performance is on the ?mystery? domain,
where the LATTICE-TOKEN-R model reaches 91.3% accuracy, a 3.9 percentage points
improvement over the Stanford tagger. Its performance on the in-domain ?news? test set
is significantly worse (1.7 percentage points) than the Stanford tagger, suggesting that
the Stanford tagger relies on domain-dependent features that are helpful for tagging
news, but not for tagging in general. The LATTICE-TOKEN-R?s accuracy is still signifi-
cantly worse on out-of-domain text than in-domain text, but the gap between the two
(8.3 percentage points) is better than the gap for the Stanford tagger (11.8 percentage
points). We believe that the lower out-of-domain performance of our Chinese POS
tagger, compared with our English POS tagger and our chunker, was at least in part
due to having far less unlabeled text available for this task.
8 Available at http://nlp.stanford.edu/software/tagger.shtml.
110
Huang et al. Computational Linguistics
5. Information Extraction Experiments
In this section, we evaluate our learned representations on their ability to capture
semantic, rather than syntactic, information. Specifically, we investigate a set-expansion
task in which we?re given a corpus and a few ?seed? noun phrases from a semantic
category (e.g., Superheroes), and our goal is to identify other examples of the category
in the corpus. This is a different type of weakly supervised task from the earlier domain
adaptation tasks because we are given only a handful of positive examples from a cate-
gory, rather than a large sample of positively and negatively labeled training examples
from a separate domain.
Existing set-expansion techniques utilize the distributional hypothesis: Candidate
noun phrases for a given semantic class are ranked based on how similar their contex-
tual distributions are to those of the seeds. Here, we measure how performance on the
set-expansion task varies when we employ different representations for the contextual
distributions.
5.1 Methods
The set-expansion task we address is formalized as follows. Given a corpus, a set of
seeds from some semantic category C, and a separate set of candidate phrases P, output
a ranking of the phrases in P in decreasing order of likelihood of membership in the
semantic category C.
For any given representation R, the set-expansion algorithm we investigate is
straightforward: We rank candidate phrases in increasing order of the distance between
their feature vectors and those of the seeds. The particular distance metrics utilized are
detailed subsequently.
Because set expansion is performed at the level of word types rather than to-
kens, it requires type-based representations. We compare HMM-TYPE-R, n-GRAM-R,
LATTICE-TYPE-R, and BROWN-TYPE-R in this experiment. We used a 25-state HMM,
and the LATTICE-TYPE-R as described in the previous section. Following previous set-
expansion experiments with n-grams (Ahuja and Downey 2010), we use a trigram
model with Kneser-Ney smoothing for n-GRAM-R.
The distances between the candidate phrases and the seeds for HMM-TYPE-R,
n-GRAM-R, and LATTICE-TYPE-R representations are calculated by first creating a
prototypical ?seed feature vector? equal to the mean of the feature vectors for each
of the seeds in the given representation. Then, we rank candidate phrases in order
of increasing distance between their feature vector and the seed feature vector. As a
distance measure between vectors (in this case, probability distributions), we compute
the average of five standard distance measures, including KL and JS divergence, and
cosine, Euclidean, and L1 distance. In experiments, we found that improving upon
this simple averaging was not easy?in fact, tuning a weighted average of the distance
measures for each representation did not improve results significantly on held-out data.
For Brown clusters, we use prefixes of all possible lengths as features. We define
the similarity between two Brown representation feature vectors to be the number of
features they share in common (this is equivalent to the length of the longest common
prefix between the two original Brown cluster labels). The candidate phrases are then
ranked in decreasing order of the sum of their similarity scores to each of the seeds. We
experimented with normalizing the similarity scores by the longer of the two vector
lengths, and found this to decrease results slightly. We use unnormalized (integer)
similarity scores for Brown clusters in our experiments.
111
Computational Linguistics Volume 40, Number 1
5.2 Data Sets
We utilized a set of approximately 100,000 sentences of Web text, joining multi-word
named entities in the corpus into single tokens using the Lex algorithm (Downey,
Broadhead, and Etzioni 2007). This process enables each named entity (the focus of the
set-expansion experiments) to be treated as a single token, with a single representation
vector for comparison. We developed all word type representations using this corpus.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia
?listOf? pages from Pantel et al. (2009) and augmented these with our own manually
defined categories, such that each list contained at least ten distinct examples occurring
in our corpus. In all, we had 432 examples across 16 distinct categories such as Coun-
tries, Greek Islands, and Police TV Dramas.
5.3 Results
For each semantic category, we tested five different random selections of five seed
examples, treating the unselected members of the category as positive examples, and
all other candidate phrases as negative examples. We evaluate using the area under the
precision-recall curve (AUC) metric.
The results are shown in Table 5. All representations improve performance over
a random baseline, equal to the average AUC over five random orderings for each
category, and the graphical models outperform the n-gram representation. I-HMM-
TYPE-R and Brown clustering in the particular case of 1,000 clusters perform best, with
HMM-TYPE-R performing nearly as well. Brown clusters give somewhat lower results
as the number of clusters varies.
As with POS tagging, we expect that language model representations improve
performance on the IE task by providing informative features for sparse word types.
However, because the IE task classifies word types rather than tokens, we expect the rep-
resentations to provide less benefit for polysemous word types. To test these hypotheses,
we measured how IE performance changed in sparse or polysemous settings. We identi-
fied polysemous categories as those for which fewer than 90% of the category members
had the category as a clear dominant sense (estimated manually); other categories were
considered non-polysemous. Categories whose members had a median number of
occurrences in the corpus of less than 30 were deemed sparse, and others non-sparse.
Table 5
I-HMM-TYPE-R outperforms the other methods, improving performance over a random
baseline by twice as much as either n-GRAM-R or LATTICE-TYPE-R.
model AUC
I-HMM-TYPE-R 0.18
HMM-TYPE-R 0.17
BROWN-TYPE-R-3200 0.16
BROWN-TYPE-R-1000 0.18
BROWN-TYPE-R-320 0.15
BROWN-TYPE-R-100 0.13
LATTICE-TYPE-R 0.11
n-GRAM-R baseline 0.10
Random baseline 0.10
112
Huang et al. Computational Linguistics
Table 6
Graphical models as representations for IE consistently perform better relative to n-gram models
on sparse words, but not necessarily polysemous words.
polysemous not-polysemous sparse not-sparse
types 222 210 266 166
categs. 12 4 13 3
n-GRAM-R 0.07 0.17 0.06 0.25
LATTICE-TYPE-R 0.09 0.15 0.1 0.19
-n-GRAM-R +0.02 ?0.02 +0.04 ?0.06
HMM-TYPE-R 0.14 0.26 0.15 0.32
-n-GRAM-R +0.07 +0.09 +0.09 +0.07
IE performance on these subsets of the data are shown in Table 6. Both graphical
model representations outperform the n-gram representation more on sparse words, as
expected. For polysemy, the picture is mixed: The LATTICE-TYPE-R outperforms
n-GRAM-R on polysemous categories, whereas HMM-TYPE-R?s performance advan-
tage over n-GRAM-R decreases.
One surprise on the IE task is that the LATTICE-TYPE-R performs significantly less
well than the HMM-TYPE-R, whereas the reverse is true on POS tagging. We suspect
that the difference is due to the issue of classifying types vs. tokens. Because of their
more complex structure, PL-MRFs tend to depend more on transition parameters than
do HMMs. Furthermore, our decision to train the PL-MRFs using contrastive estimation
with a neighborhood that swaps consecutive pairs of words also tends to emphasize
transition parameters. As a result, we believe the posterior distribution over latent states
given a word type is more informative in our HMM model than the PL-MRF model.
We measured the entropy of these distributions for the two models, and found that
H(PPL-MRF(y|x = w)) = 9.95 bits, compared with H(PHMM(y|x = w)) = 2.74 bits, which
supports the hypothesis that the drop in the PL-MRF?s performance on IE is due to its
dependence on transition parameters. Further experiments are warranted to investigate
this issue.
5.4 Testing the Language Model Representation Hypothesis in IE
The language model representation hypothesis (Section 2) suggests that all else being
equal, more accurate language models will provide features that lead to better perfor-
mance on NLP tasks. Here, we test this hypothesis on the set expansion IE task.
Figures 10 and 11 show how the performance of the HMM-TYPE-R varies with the
language modeling accuracy of the underlying HMM. Language modeling accuracy
is measured in terms of perplexity on held-out text. Here, we use set expansion data
sets from previous work (Ahuja and Downey 2010). The first two are composed of
extractions from the TextRunner information extraction system (Banko et al. 2007) and
are denoted as Unary (361 examples) and Binary (265 examples). The second, Wikipedia
(2,264 examples), is a sample of Wikipedia concept names. We evaluate the performance
of several different trained HMMs with numbers of latent states K ranging from 5 to
1,600 (to help illustrate how IE and LM performance varies even when model capacity
is fixed, we include three distinct models with K = 100 states trained separately over
the full corpus). We used a distributed implementation of HMM training and corpus
113
Computational Linguistics Volume 40, Number 1
K = 5
K = 10
K = 25 K = 50
K = 100
K = 100
K = 100
K = 200
K = 400
Figure 10
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy of
the HMM varies, on TextRunner data sets. IE accuracy (in terms of area under the precision-recall
curve) tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
5 10
25 50
100
100
100
200 400
5
5
25
2550
50
100
200
100
200
800
1600 400
Figure 11
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy
of the HMM varies on the Wikipedia data set. Number labels indicate the number of latent
states K, and performance is shown for three training corpus sizes (the full corpus consists of
approximately 60 million tokens). IE accuracy (in terms of area under the precision-recall curve)
tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
partitioning techniques (Yang, Yates, and Downey 2013) to enable training of our larger
capacity HMM models on large data sets.
The results provide support for the language model representation hypothesis,
showing that IE performance does tend to improve as language model perplexity
decreases. On the smaller Unary and Binary sets (Figure 10), although IE accuracy
114
Huang et al. Computational Linguistics
does decrease for the lowest-perplexity models, overall language model perplexity
exhibits a negative correlation with IE area under the precision-recall curve (the Pearson
correlation coefficient is ?0.18 for Unary, and ?0.28 for Binary). For Wikipedia (Fig-
ure 11), the trend is more consistent, with IE performance increasing monotonically
as perplexity decreases for models trained on the full training corpus (the Pearson
correlation coefficient is ?0.90).
Figure 11 also illustrates how LM and IE performance changes as the amount
of training text varies. In general, increasing the training corpus size increases IE
performance and decreases perplexity. Over all data points in the figure, IE perfor-
mance correlates most strongly with model perplexity (?0.68 Pearson correlation, ?0.88
Spearman correlation), followed by corpus size (0.66, 0.71) and model capacity (?0.05,
0.38). The small negative Pearson correlation between model capacity and IE perfor-
mance is primarily due to the model with 1,600 states trained on 4% of the corpus.
This model has a large parameter space and sparse training data, and thus suffers from
overfitting in terms of both model perplexity and IE performance. If we ignore this
overfit model, the Pearson correlation between model capacity and IE performance for
the other models in the Figure is 0.24.
Our results show that IE based on distributional similarity tends to improve as the
quality of the latent variable model used to measure distributional similarity improves.
A similar trend was exhibited in our previous work (Ahuja and Downey 2010); here, we
extend the previous results to models with more latent states and a larger, more reliable
test set (Wikipedia). The results suggest that scaling up the training of latent variable
models to utilize larger training corpora and more latent states may be a promising
direction for improving IE capabilities.
6. Conclusion and Future Work
Our study of representation learning demonstrates that by using statistical language
models to aggregate information across many unannotated examples, it is possible to
find accurate distributional representations that can provide highly informative features
to weakly supervised sequence labelers and named-entity classifiers. For both domain
adaptation and weakly supervised set expansion, our results indicate that graphical
models outperform n-gram models as representations, in part for their greater ability to
handle sparsity and polysemy. Our IE task provides important evidence to support the
Language Model Representation Hypothesis, showing that the AUC of the IE system
correlates more with language model perplexity than the size of the training data or
the capacity of the language model. Finally, our sequence labeling experiments provide
empirical evidence in support of theoretical work on domain adaptation, showing that
target-domain tagging accuracy is highly correlated with two different measures of
domain divergence.
Representation learning remains a promising area for finding further improve-
ments in various NLP tasks. The representations we have described are trained in
an unsupervised fashion, so a natural extension is to investigate supervised or semi-
supervised representation-learning techniques. As mentioned previously, our current
techniques have no built-in methods for enforcing that they provide similar features in
different domains; devising a mechanism that enforces this could allow for less domain-
divergent and potentially more accurate representations. We have considered sequence
labeling, but another promising direction is to apply these techniques to more complex
structured prediction tasks, like parsing or relation extraction. Our current approach
to sequence labeling requires retraining of a CRF for every new domain; incremental
115
Computational Linguistics Volume 40, Number 1
retraining techniques for new domains would speed up the process. Finally, models
that combine our representation learning approach with instance weighting and other
forms of supervised domain adaptation may take better advantage of labeled data in
target domains, when it is available.
Acknowledgments
This material is based on work supported
by the National Science Foundation under
grant no. IIS-1065397.
References
Ahuja, Arun and Doug Downey. 2010.
Improved extraction assessment through
better language models. In Proceedings of
the Annual Meeting of the North American
Chapter of the Association of Computational
Linguistics (NAACL-HLT), pages 225?228,
Los Angeles, CA.
Ando, Rie Kubota and Tong Zhang. 2005.
A high-performance semi-supervised
learning method for text chunking.
In Proceedings of the ACL, pages 1?9,
Ann Arbor, MI.
Banko, Michele, Michael J. Cafarella,
Stephen Soderland, Matt Broadhead, and
Oren Etzioni. 2007. Open information
extraction from the web. In Proceedings of
the IJCAI, pages 2670?2676, Hyderabad.
Banko, Michele and Robert C. Moore.
2004. Part of speech tagging in context.
In Proceedings of the COLING, pages
556?561, Geneva.
Ben-David, Shai, John Blitzer, Koby
Crammer, Alex Kulesza, Fernando Pereira,
and Jenn Wortman. 2010. A theory of
learning from different domains. Machine
Learning, 79:151?175.
Ben-David, Shai, John Blitzer, Koby
Crammer, and Fernando Pereira. 2007.
Analysis of representations for domain
adaptation. In Advances in Neural
Information Processing Systems 20,
pages 127?144, Vancouver.
Bengio, Yoshua. 2008. Neural net language
models. Scholarpedia, 3(1):3,881.
Bengio, Yoshua, Re?jean Ducharme, Pascal
Vincent, and Christian Janvin. 2003.
A neural probabilistic language model.
Journal of Machine Learning Research,
3:1,137?1,155.
Bengio, Yoshua, Jerome Louradour,
Ronan Collobert, and Jason Weston.
2009. Curriculum learning. In Proceedings
of the International Conference on Machine
Learning (ICML), pages 41?48,
Montreal.
Bikel, Daniel M. 2004a. A distributional
analysis of a lexicalized statistical
parsing model. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 182?189,
Barcelona.
Bikel, Daniel M. 2004b. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993?1,022.
Blitzer, John. 2008. Domain Adaptation of
Natural Language Processing Systems.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
Blitzer, John, Koby Crammer, Alex Kulesza,
Fernando Pereira, and Jenn Wortman.
2007. Learning bounds for domain
adaptation. In Advances in Neural
Information Processing Systems,
pages 129?136, Vancouver.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Association for Computational Linguistics
(ACL), pages 40?47, Prague.
Blitzer, John, Ryan McDonald, and
Fernando Pereira. 2006. Domain
adaptation with structural correspondence
learning. In Proceedings of the EMNLP,
pages 120?128, Sydney.
Bodlaender, Hans L. 1988. Dynamic
programming on graphs with bounded
treewidth. In Proceedings of the 15th
International Colloquium on Automata,
Languages and Programming,
pages 105?118, Tampere.
Brants, Thorsten and Alex Franz. 2006.
Web 1t 5-gram version 1. www.ldc.
upenn.edu/catalog/.
Brown, Peter F., Vincent J. Della Pietra,
Peter V. deSouza, Jenifer C. Lai, and
Robert L. Mercer. 1992. Class-based
n-gram models of natural language.
Computational Linguistics, 18:467?479.
Candito, Marie and Benoit Crabbe. 2009.
Improving generative statistical parsing
with semi-supervised word clustering.
In Proceedings of the IWPT, pages 138?141,
Paris.
116
Huang et al. Computational Linguistics
Chan, Yee Seng and Hwee Tou Ng. 2006.
Estimating class priors in domain
adaptation for word sense disambiguation.
In Proceedings of the Association for
Computational Linguistics (ACL),
pages 89?96, Sydney.
Chelba, Ciprian and Alex Acero. 2004.
Adaptation of maximum entropy
classifier: Little data can help a lot.
In Proceedings of the EMNLP,
pages 285?292, Barcelona.
Collobert, Robert and Jason Weston. 2008. A
unified architecture for natural language
processing: Deep neural networks with
multitask learning. In Proceedings of the
International Conference on Machine Learning
(ICML), pages 160?167, Helsinki.
Dai, Wenyuan, Gui-Rong Xue, Qiang Yang,
and Yong Yu. 2007. Transferring naive
Bayes classifiers for text classification.
In Proceedings of the National Conference
on Artificial Intelligence (AAAI),
pages 540?545, Vancouver.
Darroch, J. N., S. L. Lauritzen, and
T. P. Speed. 1980. Markov fields and
log-linear interaction models for
contingency tables. The Annals of
Statistics, 8(3):522?539.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
ACL, pages 256?263, Prague.
Daume? III, Hal, Abhishek Kumar, and
Avishek Saha. 2010. Frustratingly easy
semi-supervised domain adaptation.
In Proceedings of the ACL Workshop
on Domain Adaptation (DANLP),
pages 53?59, Uppsala.
Daume? III, Hal and Daniel Marcu. 2006.
Domain adaptation for statistical
classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society of Information Science,
41(6):391?407.
Dempster, Arthur, Nan Laird, and Donald
Rubin. 1977. Likelihood from incomplete
data via the EM algorithm. Journal of
the Royal Statistical Society, Series B,
39(1):1?38.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the latent words language
model. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 21?29,
Singapore.
Dhillon, Paramveer S., Dean Foster, and
Lyle Ungar. 2011. Multi-View Learning of
Word Embeddings via CCA. In Proceedings
of the Advances in Neural Information
Processing Systems (NIPS), volume 24,
pages 886?874, Granada.
Downey, Doug, Matthew Broadhead, and
Oren Etzioni. 2007. Locating complex
named entities in web text. In Proceedings
of the 20th International Joint Conference
on Artificial Intelligence (IJCAI 2007),
pages 2,733?2,739, Hyderabad.
Downey, Doug, Stefan Schoenmackers, and
Oren Etzioni. 2007. Sparse information
extraction: Unsupervised language models
to the rescue. In Proceedings of the ACL,
pages 696?703, Prague.
Dredze, Mark and Koby Crammer. 2008.
Online methods for multi-domain learning
and adaptation. In Proceedings of EMNLP,
pages 689?697, Honolulu, HI.
Dredze, Mark, Alex Kulesza, and Koby
Crammer. 2010. Multi-domain learning
by confidence weighted parameter
combination. Machine Learning,
79:123?149.
Finkel, Jenny Rose and Christopher D.
Manning. 2009. Hierarchical Bayesian
domain adaptation. In Proceedings of
HLT-NAACL, pages 602?610, Boulder, CO.
Fu?rstenau, Hagen and Mirella Lapata. 2009.
Semi-supervised semantic role labeling.
In Proceedings of the 12th Conference of the
European Chapter of the ACL, pages 220?228,
Athens.
Ghahramani, Zoubin and Michael I. Jordan.
1997. Factorial hidden Markov models.
Machine Learning, 29(2-3):245?273.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Conference on
Empirical Methods in Natural Language
Processing, pages 167?202, Pittsburgh, PA.
Goldwater, Sharon and Thomas L. Griffiths.
2007. A fully Bayesian approach to
unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages 744?751,
Prague.
Grac?a, Joa?o V., Kuzman Ganchev, Ben Taskar,
and Fernando Pereira. 2009. Posterior vs.
parameter sparsity in latent variable
models. In Proceedings of the Neural
Information Processing Systems Conference
(NIPS), pages 664?672, Vancouver.
Harris, Z. 1954. Distributional structure.
Word, 10(23):146?162.
Hindle, Donald. 1990. Noun classification
from predicage-argument structures.
In Proceedings of the ACL, pages 268?275,
Pittsburgh, PA.
117
Computational Linguistics Volume 40, Number 1
Honkela, Timo. 1997. Self-organizing
maps of words for natural language
processing applications. In Proceedings
of the International ICSC Symposium on
Soft Computing, pages 401?407, Millet,
Alberta.
Huang, Fei and Alexander Yates. 2009.
Distributional representations for
handling sparsity in supervised sequence
labeling. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 495?503,
Singapore.
Huang, Fei and Alexander Yates. 2010.
Exploring representation-learning
approaches to domain adaptation. In
Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language
Processing (DANLP), pages 23?30, Uppsala.
Huang, Fei, Alexander Yates, Arun Ahuja,
and Doug Downey. 2011. Language
models as representations for weakly
supervised NLP tasks. In Proceedings
of the Conference on Natural Language
Learning (CoNLL), pages 125?134,
Portland, OR.
Jiang, Jing and ChengXiang Zhai. 2007a.
Instance weighting for domain
adaptation in NLP. In Proceedings
of ACL, pages 264?271, Prague.
Jiang, Jing and ChengXiang Zhai. 2007b. A
two-stage approach to domain adaptation
for statistical classifiers. In Proceedings of
the Conference on Information and Knowledge
Management (CIKM), pages 401?410, Lisbon.
Johnson, Mark. 2007. Why doesn?t EM find
good HMM POS-taggers. In Proceedings of
the EMNLP, pages 296?305, Prague.
Kaski, S. 1998. Dimensionality reduction
by random mapping: Fast similarity
computation for clustering. In
Proceedings of the IJCNN, pages 413?418,
Washington, DC.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings of
the Annual Meeting of the Association of
Computational Linguistics (ACL),
pages 595?603, Columbus, OH.
Lin, Dekang and Xiaoyun Wu. 2009. Phrase
clustering for discriminative learning.
In Proceedings of the ACL-IJCNLP,
pages 1,030?1,038, Singapore.
Liu, Dong C. and Jorge Nocedal. 1989. On
the limited memory method for large scale
optimization. Mathematical Programming B,
45(3):503?528.
Mansour, Y., M. Mohri, and
A. Rostamizadeh. 2009. Domain
adaptation with multiple sources.
In Proceedings of the Advances in Neural
Information Processing Systems,
pages 1,041?1,048, Vancouver.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Martin, Sven, Jorg Liermann, and Hermann
Ney. 1998. Algorithms for bigram and
trigram word clustering. Speech
Communication, 24:19?37.
McClosky, David. 2010. Any Domain Parsing:
Automatic Domain Adaptation for Parsing.
Ph.D. thesis, Brown University,
Providence, RI.
McClosky, David, Eugene Charniak, and
Mark Johnson. 2010. Automatic domain
adaptation for parsing. In North American
Chapter of the Association for Computational
Linguistics - Human Language Technologies
2010 Conference (NAACL-HLT 2010),
pages 28?36, Los Angeles, CA.
Miller, Scott, Jethran Guinness, and
Alex Zamanian. 2004. Name tagging with
word clusters and discriminative training.
In Proceedings of the Annual Meeting of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL),
pages 337?342, Boston, MA.
Mnih, Andriy and Geoffrey Hinton. 2007.
Three new graphical models for statistical
language modelling. In Proceedings of
the 24th International Conference on
Machine Learning, pages 641?648,
Corvallis, OR.
Mnih, Andriy and Geoffrey Hinton. 2009.
A scalable hierarchical distributed
language model. In Proceedings of the
Neural Information Processing Systems
(NIPS), pages 1,081?1,088, Vancouver.
Mnih, Andriy, Zhang Yuecheng, and
Geoffrey Hinton. 2009. Improving a
statistical language model through
non-linear prediction. Neurocomputing,
72(7-9):1414?1418.
Morin, Frederic and Yoshua Bengio. 2005.
Hierarchical probabilistic neural network
language model. In Proceedings of the
International Workshop on Artificial
Intelligence and Statistics, pages 246?252,
Barbados.
Pantel, Patrick, Eric Crestan, Arkady
Borkovsky, Ana-Maria Popescu,
and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set
expansion. In Proceedings of the EMNLP,
pages 938?947, Singapore.
118
Huang et al. Computational Linguistics
PennBioIE. 2005. Mining the bibliome
project. http://bioie.ldc.upenn.edu/.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 183?190, Columbus, OH.
Pradhan, Sameer, Wayne Ward, and James H.
Martin. 2007. Towards robust semantic role
labeling. In Proceedings of NAACL-HLT,
pages 556?563, Rochester, NY.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition.
Proceedings of the IEEE, 77(2):257?285.
Raina, Rajat, Alexis Battle, Honglak Lee,
Benjamin Packer, and Andrew Y. Ng.
2007. Self-taught learning: Transfer
learning from unlabeled data.
In Proceedings of the 24th International
Conference on Machine Learning,
pages 759?766, Corvallis, OR.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Conference on Natural Language Learning
(CoNLL), pages 147?155, Boulder, CO.
Ritter, H. and T. Kohonen. 1989.
Self-organizing semantic maps.
Biological Cybernetics, 61(4):241?254.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Synactic Theory: A Formal
Introduction. CSLI Publications, Stanford,
CA, second edition.
Sahlgren, Magnus. 2001. Vector-based
semantic analysis: Representing word
meanings based on random labels.
In Proceedings of the Semantic Knowledge
Acquisition and Categorization Workshop,
pages 1?12, Helsinki.
Sahlgren, Magnus. 2005. An introduction
to random indexing. In Methods and
Applications of Semantic Indexing Workshop
at the 7th International Conference on
Terminology and Knowledge Engineering
(TKE), 87:1?9.
Sahlgren, Magnus. 2006. The word-space
model: Using distributional analysis to
represent syntagmatic and paradigmatic
relations between words in high-dimensional
vector spaces. Ph.D. thesis, Stockholm
University.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill.
Satpal, Sandeep and Sunita Sarawagi.
2007. Domain adaptation of conditional
probability models via feature subsetting.
In Proceedings of ECML/PKDD,
pages 224?235, Warsaw.
Sekine, Satoshi. 1997. The domain
dependence of parsing. In Proceedings of
Applied Natural Language Processing
(ANLP), pages 96?102, Washington, DC.
Shen, Libin, Giorgio Satta, and Aravind K.
Joshi. 2007. Guided learning for
bidirectional sequence classification.
In Proceedings of the ACL, pages 760?767,
Prague.
Smith, Noah A. and Jason Eisner. 2005.
Contrastive estimation: Training
log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 354?362,
Ann Arbor, MI.
Sutton, Charles, Andrew McCallum, and
Khashayar Rohanimanesh. 2007. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. Journal of
Machine Learning Research, 8:693?723.
Suzuki, Jun and Hideki Isozaki. 2008.
Semi-supervised sequential labeling and
segmentation using giga-word scale
unlabeled data. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL-HLT),
pages 665?673, Columbus, OH.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras,
and Michael Collins. 2009. An empirical
study of semi-supervised structured
conditional models for dependency
parsing. In Proceedings of the EMNLP,
pages 551?560, Singapore.
Tao, Hongyin and Richard Xiao. 2007.
The UCLA Chinese corpus. UCREL.
www.lancaster.ac.uk/fass/projects/
corpus/UCLA/.
Tjong, Erik F., Kim Sang, and Sabine
Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking.
In Proceedings of the 4th Conference
on Computational Natural Language
Learning, pages 127?132, Lisbon.
Toutanova, Kristina and Mark Johnson.
2007. A Bayesian LDA-based model for
semi-supervised part-of-speech
tagging. In Proceedings of the NIPS,
pages 1,521?1,528, Vancouver.
Tseng, Huihsin, Daniel Jurafsky, and
Christopher Manning. 2005.
Morphological features help POS
tagging of unknown words across
language varieties. In Proceedings
of the Fourth SIGHAN Workshop,
pages 32?39, Jeju Island.
119
Computational Linguistics Volume 40, Number 1
Turian, Joseph, James Bergstra, and
Yoshua Bengio. 2009. Quadratic
features and deep architectures for
chunking. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics - Human
Language Technologies (NAACL HLT),
pages 245?248, Boulder, CO.
Turian, Joseph, Lev Ratinov, and Yoshua
Bengio. 2010. Word representations:
A simple and general method for
semi-supervised learning. In Proceedings
of the Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 384?394, Uppsala.
Turney, Peter D. and Patrick Pantel. 2010.
From frequency to meaning: Vector
space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
Ushioda, Akira. 1996. Hierarchical clustering
of words. In Proceedings of the International
Conference on Computational Linguistics
(COLING), pages 1,159?1,162, Copenhagen.
Va?yrynen, Jaakko and Timo Honkela. 2004.
Word category maps based on emergent
features created by ICA. In Proceedings of
the STePs 2004 Cognition + Cybernetics
Symposium, pages 173?185, Tikkurila.
Va?yrynen, Jaakko and Timo Honkela. 2005.
Comparison of independent component
analysis and singular value decomposition
in word context analysis. In Proceedings
of the International and Interdisciplinary
Conference on Adaptive Knowledge
Representation and Reasoning (AKRR),
pages 135?140, Espoo.
Va?yrynen, Jaakko, Timo Honkela, and
Lasse Lindqvist. 2007. Towards explicit
semantic features using independent
component analysis. In Proceedings of the
Workshop Semantic Content Acquisition
and Representation (SCAR), pages 20?27,
Stockholm.
Weston, Jason, Frederic Ratle, and
Ronan Collobert. 2008. Deep learning
via semi-supervised embedding.
In Proceedings of the 25th International
Conference on Machine Learning,
pages 1,168?1,175, Helsinki.
Yang, Yi, Alexander Yates, and Doug
Downey. 2013. Overcoming the memory
bottleneck in distributed training
of latent variable models of text.
In Proceedings of the NAACL-HLT,
pages 579?584, Atlanta, GA.
Zhao, Hai, Wenliang Chen, Chunyu Kit,
and Guodong Zhou. 2009. Multilingual
dependency learning: A huge feature
engineering method to semantic
dependency parsing. In Proceedings of the
CoNLL 2009 Shared Task, pages 55?60,
Boulder, CO.
120
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 968?978,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Open-Domain Semantic Role Labeling by Modeling Word Spans
Fei Huang
Temple University
1805 N. Broad St.
Wachman Hall 318
fei.huang@temple.edu
Alexander Yates
Temple University
1805 N. Broad St.
Wachman Hall 303A
yates@temple.edu
Abstract
Most supervised language processing sys-
tems show a significant drop-off in per-
formance when they are tested on text
that comes from a domain significantly
different from the domain of the training
data. Semantic role labeling techniques
are typically trained on newswire text, and
in tests their performance on fiction is
as much as 19% worse than their perfor-
mance on newswire text. We investigate
techniques for building open-domain se-
mantic role labeling systems that approach
the ideal of a train-once, use-anywhere
system. We leverage recently-developed
techniques for learning representations of
text using latent-variable language mod-
els, and extend these techniques to ones
that provide the kinds of features that are
useful for semantic role labeling. In exper-
iments, our novel system reduces error by
16% relative to the previous state of the art
on out-of-domain text.
1 Introduction
In recent semantic role labeling (SRL) competi-
tions such as the shared tasks of CoNLL 2005 and
CoNLL 2008, supervised SRL systems have been
trained on newswire text, and then tested on both
an in-domain test set (Wall Street Journal text)
and an out-of-domain test set (fiction). All sys-
tems tested on these datasets to date have exhib-
ited a significant drop-off in performance on the
out-of-domain tests, often performing 15% worse
or more on the fiction test sets. Yet the baseline
from CoNLL 2005 suggests that the fiction texts
are actually easier than the newswire texts. Such
observations expose a weakness of current super-
vised natural language processing (NLP) technol-
ogy for SRL: systems learn to identify semantic
roles for the subset of language contained in the
training data, but are not yet good at generalizing
to language that has not been seen before.
We aim to build an open-domain supervised
SRL system; that is, one whose performance on
out-of-domain tests approaches the same level of
performance as that of state-of-the-art systems on
in-domain tests. Importantly, an open-domain sys-
tem must not use any new labeled data beyond
what is included in the original training text when
running on a new domain. This allows the sys-
tem to be ported to any new domain without any
manual effort. In particular, it ought to apply to
arbitrary Web documents, which are drawn from a
huge variety of domains.
Recent theoretical and empirical evidence sug-
gests that the fault for poor performance on out-of-
domain tests lies with the representations, or sets
of features, traditionally used in supervised NLP.
Building on recent efforts in domain adaptation,
we develop unsupervised techniques for learning
new representations of text. Using latent-variable
language models, we learn representations of texts
that provide novel kinds of features to our su-
pervised learning algorithms. Similar represen-
tations have proven useful in domain-adaptation
for part-of-speech tagging and phrase chunking
(Huang and Yates, 2009). We demonstrate how
to learn representations that are effective for SRL.
Experiments on out-of-domain test sets show that
our learned representations can dramatically im-
prove out-of-domain performance, and narrow the
gap between in-domain and out-of-domain perfor-
mance by half.
The next section provides background informa-
tion on learning representations for NLP tasks us-
ing latent-variable language models. Section 3
presents our experimental setup for testing open-
domain SRL. Sections 4, 5, 6 describe our SRL
system: first, how we identify predicates in open-
domain text, then how our baseline technique
968
identifies and classifies arguments, and finally how
we learn representations for improving argument
identification and classification on out-of-domain
text. Section 7 presents previous work, and Sec-
tion 8 concludes and outlines directions for future
work.
2 Open-Domain Representations Using
Latent-Variable Language Models
Let X be an instance set for a learning problem;
for SRL, this is the set of all (sentence,predicate)
pairs. Let Y be the space of possible labels for an
instance, and let f : X ? Y be the target func-
tion to be learned. A representation is a func-
tion R: X ? Z , for some suitable feature space
Z (such as Rd). A domain is defined as a dis-
tribution D over the instance set X . An open-
domain system observes a set of training examples
(R(x), f(x)), where instances x ? X are drawn
from a source domain, to learn a hypothesis for
classifying examples drawn from a separate target
domain.
Previous work by Ben-David et al (2007; 2009)
uses Vapnik-Chervonenkis (VC) theory to prove
theoretical bounds on an open-domain learning
machine?s performance. Their analysis shows that
the choice of representation is crucial to open-
domain learning. As is customary in VC the-
ory, a good choice of representation must allow
a learning machine to achieve low error rates dur-
ing training. Just as important, however, is that
the representation must simultaneously make the
source and target domains look as similar to one
another as possible.
For open-domain SRL, then, the traditional rep-
resentations are problematic. Typical represen-
tations in SRL and NLP use features of the lo-
cal context to produce a representation. For in-
stance, one dimension of a traditional represen-
tation R might be +1 if the instance contains the
word ?bank? as the head of a noun-phrase chunk
that occurs before the predicate in the sentence,
and 0 otherwise. Although many previous studies
have shown that these features allow learning sys-
tems to achieve impressively low error rates dur-
ing training, they also make texts from different
domains look very dissimilar. For instance, a fea-
ture based on the word ?bank? or ?CEO? may be
common in a domain of newswire text, but scarce
or nonexistent in, say, biomedical literature.
In our recent work (Huang and Yates, 2009) we
show how to build systems that learn new rep-
resentations for open-domain NLP using latent-
variable language models like Hidden Markov
Models (HMMs). An HMM is a generative prob-
abilistic model that generates each word xi in the
corpus conditioned on a latent variable Yi. Each
Yi in the model takes on integral values from 1 to
K, and each one is generated by the latent variable
for the preceding word, Yi?1. The distribution for
a corpus x = (x1, . . . , xN ) and a set of state vec-
tors s = (s1, . . . , sN ) is given by:
P (x, s) =
?
i
P (xi|si)P (si|si?1)
Using Expectation-Maximization (Dempster et
al., 1977), it is possible to estimate the distribu-
tions for P (xi|si) and P (si|si?1) from unlabeled
data. The Viterbi algorithm (Rabiner, 1989) can
then be used to produce the optimal sequence of
latent states si for a given instance x. The output
of this process is an integer (ranging from 1 to K)
for every word xi in the corpus. We use the inte-
ger value of si as a new feature for every xi in the
sentence.
In POS-tagging and chunking experiments,
these learned representations have proven to meet
both of Ben-David et al?s criteria for open-domain
representations: first, they are useful in making
predictions on the training text because the HMM
latent states categorize tokens according to dis-
tributional similarity. And second, it would be
difficult to tell two domains apart based on their
HMM labels, since the same HMM state can gen-
erate similar words from a variety of domains.
In what follows, we adapt these representation-
learning concepts to open-domain SRL.
3 Experimental Setup
We test our open-domain semantic role labeling
system using data from the CoNLL 2005 shared
task (Carreras and Ma`rquez, 2005). We use the
standard training set, consisting of sections 02-21
of the Wall Street Journal (WSJ) portion of the
Penn Treebank, labeled with PropBank (Palmer
et al, 2005) annotations for predicates and argu-
ments. We perform our tests on the Brown corpus
(Kucera and Francis, 1967) test data from CoNLL
2005, consisting of 3 sections (ck01-ck03) of
propbanked Brown corpus data. This test set con-
sists of 426 sentences containing 7,159 tokens,
804 propositions, and 2,177 arguments. While the
969
training data contains newswire text, the test sen-
tences are drawn from the domain of ?general fic-
tion,? and contain an entirely different style (or
styles) of English. The data also includes a sec-
ond test set of in-domain text (section 23 of the
Treebank), which we refer to as the WSJ test set
and use as a reference point.
Every sentence in the dataset is automatically
annotated with a number of NLP pipeline systems,
including part-of-speech (POS) tags, phrase chunk
labels (Carreras and Ma`rquez, 2003), named-
entity tags, and full parse information by multiple
parsers. These pipeline systems are important for
generating features for SRL, and one key reason
for the poor performance of SRL systems on the
Brown corpus is that the pipeline systems them-
selves perform worse. The Charniak parser, for
instance, drops from an F1 of 88.25 on the WSJ
test to a F1 of 80.84 on the Brown corpus. For
the chunker and POS tagger, the drop-offs are less
severe: 94.89 to 91.73, and 97.36 to 94.73.
Toutanova et al (2008) currently have the best-
performing SRL system on the Brown corpus test
set with an F1 score of 68.81 (80.8 for the WSJ
test). They use a discriminative reranking ap-
proach to jointly predict the best set of argu-
ment boundaries and the best set of argument la-
bels for a predicate. Like the best systems from
the CoNLL 2005 shared task (Punyakanok et al,
2008; Pradhan et al, 2005), they also use features
from multiple parses to remain robust in the face
of parser error. Owing to the established difficulty
of the Brown test set and the different domains of
the Brown test and WSJ training data, this dataset
makes for an excellent testbed for open-domain
semantic role labeling.
4 Predicate Identification
In order to perform true open-domain SRL, we
must first consider a task which is not formally
part of the CoNLL shared task: the task of iden-
tifying predicates in a given sentence. While this
task is almost trivial in the WSJ test set, where
all but two out of over 5000 predicates can be ob-
served in the training data, it is significantly more
difficult in an open-domain setting. In the Brown
test set, 6.1% of the predicates do not appear in the
training data, and 11.8% of the predicates appear
at most twice in the training data (c.f. 1.5% of the
WSJ test predicates that appear at most twice in
training). In addition, many words which appear
Baseline HMM
Freq P R F1 P R F1
0 89.1 80.4 84.5 93.5 84.3 88.7
0-2 87.4 84.7 86.0 91.6 88.8 90.2
all 87.8 92.5 90.1 90.8 96.3 93.5
Table 1: Using HMM features in predicate iden-
tification reduces error in out-of-domain tests by
34.3% overall, and by 27.1% for OOV predicates.
?Freq? refers to frequency in the training data.
There were 831 predicates in total; 51 never ap-
peared in training and 98 appeared at most twice.
as predicates in training may not be predicates in
the test set. In an open-domain setting, therefore,
we cannot rely solely on a catalog of predicates
from the training data.
To address the task of open-domain predicate
identification, we construct a Conditional Random
Field (CRF) (Lafferty et al, 2001) model with tar-
get labels of B-Pred, I-Pred, and O-Pred (for the
beginning, interior, and outside of a predicate).
We use an open source CRF software package to
implement our CRF models.1 We use words, POS
tags, chunk labels, and the predicate label at the
preceding and following nodes as features for our
Baseline system. To learn an open-domain repre-
sentation, we then trained an 80 state HMM on the
unlabeled texts of the training and Brown test data,
and used the Viterbi optimum states of each word
as categorical features.
The results of our Baseline and HMM systems
appear in Table 1. For predicates that never or
rarely appear in training, the HMM features in-
crease F1 by 4.2, and they increase the overall F1
of the system by 3.5 to 93.5, which approaches
the F1 of 94.7 that the Baseline system achieves
on the in-domain WSJ test set. Based on these re-
sults, we were satisfied that our system could find
predicates in open-domain text. In all subsequent
experiments, we fall back on the standard evalua-
tion in which it is assumed that the boundaries of
the predicate are given. This allows us to compare
with previous work.
5 Semantic Role Labeling with
HMM-based Representations
Following standard practice, we divide the SRL
task into two parts: argument identification and
1Available from http://sourceforge.net/projects/crf/
970
argument classification. We treat both sub-tasks
as sequence-labeling problems. During argument
identification, the system must label each token
with labels that indicate either the beginning or in-
terior of an argument (B-Arg or I-Arg), or a label
that indicates the token is not part of an argument
(O-Arg). During argument classification, the sys-
tem labels each token that is part of an argument
with a class label, such as Arg0 or ArgM. Follow-
ing argument classification, multi-word arguments
may have different classification labels for each to-
ken. We post-process the labels by changing them
to match the label of the first token. We use CRFs
as our models for both tasks (Cohn and Blunsom,
2005).
Most previous approaches to SRL have relied
heavily on parsers, and especially constituency
parsers. Indeed, when SRL systems use gold stan-
dard parses, they tend to perform extremely well
(Toutanova et al, 2008). However, as several pre-
vious studies have noted (Gildea, 2001; Pradhan
et al, 2007), using parsers can cause problems for
open-domain SRL. The parsers themselves may
not port well to new domains, or the features they
generate for SRL may not be stable across do-
mains, and therefore may cause sparse data prob-
lems on new domains. Our first step is therefore
to build an SRL system that relies on partial pars-
ing, as was done in CoNLL 2004 (Carreras and
Ma`rquez, 2004). We then gradually add in less-
sparse alternatives for the syntactic features that
previous systems derive from parse trees.
During argument identification we use the fea-
tures below to predict the label Ai for token wi:
? words: wi, wi?1, and wi+1
? parts of speech (POS): POS tags ti, ti?1,
and ti+1
? chunk labels: (e.g., B-NP, I-VP, or O)
chunk tags ci, ci?1, and ci+1
? combinations: citi, tiwi, citiwi
? NE: the named entity type ni of wi
? position: whether the word occurs before
or after the predicate
? distance: the number of intervening
tokens between wi and the target predicate
? POS before, after predicate: the POS tag
of the tokens immediately preceding and
following the predicate
? Chunk before, after predicate: the chunk
type of the tokens immediately preceding
and following the predicate
? Transition: for prediction node Ai, we use
Ai?1and Ai+1 as features
For argument classification, we add the features
below to those listed above:
? arg ID: the labels Ai produced by arg.
identification (B-Arg, I-Arg, or O)
? combination: predicate + first argument
word, predicate+ last argument word,
predicate + first argument POS, predicate
+ last argument POS
? head distance: the number of tokens
between the first token of the argument
phrase and the target predicate
? neighbors: the words immediately before
and after the argument.
We refer to the CRF model with these features as
our Baseline SRL system; in what follows we ex-
tend the Baseline model with more sophisticated
features.
5.1 Incorporating HMM-based
Representations
As a first step towards an open-domain representa-
tion, we use an HMM with 80 latent state values,
trained on the unlabeled text of the training and
test sets, to produce Viterbi-optimal state values
si for every token in the corpus. We then add the
following features to our CRFs for both argument
identification and classification:
? HMM states: HMM state values si, si?1,
and si+1
? HMM states before, after predicate: the
state value of the tokens immediately
preceding and following the predicate
We call the resulting model our Baseline+HMM
system.
5.2 Path Features
Despite all of the features above, the SRL sys-
tem has very little information to help it determine
the syntactic relationship between a target predi-
cate and a potential argument. For instance, these
baseline features provide only crude distance in-
formation to distinguish between multiple argu-
ments that follow a predicate, and they make it
difficult to correctly identify clause arguments or
arguments that appear far from the predicate. Our
system needs features that can help distinguish
between different syntactic relationships, without
being overly sensitive to the domain.
As a step in this direction, we introduce path
features: features for the sequence of tokens be-
971
System P R F1
Baseline 63.9 59.7 61.7
Baseline+HMM 68.5 62.7 65.5
Baseline+HMM+Paths 70.0 65.6 67.7
Toutanova et al (2008) NR NR 68.8
Table 2: Na??ve path features improve our base-
line, but not enough to match the state-of-the-art.
Toutanova et al do not report (NR) separate val-
ues for precision and recall on this dataset. Dif-
ferences in both precision and recall between the
baseline and the other systems are statistically sig-
nificant at p < 0.01 using the two-tailed Fisher?s
exact test.
tween a predicate and a potential argument. In
standard SRL systems, these path features usually
consist of a sequence of constituent parse nodes
representing the shortest path through the parse
tree between a word and the predicate (Gildea and
Jurafsky, 2002). We substitute paths that do not
depend on parse trees. We use four types of paths:
word paths, POS paths, chunk paths, and HMM
state paths. Given an input sentence labeled with
POS tags, and chunks, we construct path features
for a token wi by concatenating words (or tags or
chunk labels) between wi and the predicate. For
example, in the sentence ?The HIV infection rate
is expected to peak in 2010,? the word path be-
tween ?rate? and predicate ?peak? would be ?is
expected to?, and the POS path would be ?VBZ
VBD TO.?
Since word, POS, and chunk paths are all sub-
ject to data sparsity for arguments that are far from
the predicate, we build less-sparse path features by
using paths of HMM states. If we use a reason-
able number of HMM states, each category label
is much more common in the training data than
the average word, and paths containing the HMM
states should be much less sparse than word paths,
and even chunk paths. In our experiments, we use
80-state HMMs.
We call the result of adding path features to
our feature set the Baseline+HMM+Paths sys-
tem((BL). Table 2 shows the performance of our
three baseline systems. In this open-domain SRL
experiment, path features improve over the Base-
line?s F1 by 6 points, and by 2.2 points over
Baseline+HMM, although the improvement is not
enough to match the state-of-the-art system by
Toutanova et al
Y1 Y2 Y6
The is expected to peak in 2010
Y3 Y4 Y5 Y7 Y8
HIV infection rate
Figure 1: The Span-HMM over the sentence. It
shows the span of length 3.
6 Representations for Word Spans
Despite partial success in improving our baseline
SRL system with path features, these features still
suffer from data sparsity ? many paths in the
test set are never or very rarely observed during
training, so the CRF model has little or no data
points from which to estimate accurate parameters
for these features. In response, we introduce la-
tent variable models of word spans, or sequences
of words. As with the HMM models above, the
latent states for word spans can be thought of as
probabilistic categories for the spans. And like the
HMM models, we can turn the word span models
into representations by using the state value for a
span as a feature in our supervised SRL system.
Unlike path features, the features from our models
of word spans consist of a single latent state value
rather than a concatenation of state values, and as
a consequence they tend to be much less sparse in
the training data.
6.1 Span-HMM Representations
We build our latent-variable models of word spans
using variations of Hidden Markov Models, which
we call Span-HMMs. Figure 1 shows a graphi-
cal model of a Span-HMM. Each Span-HMM be-
haves just like a regular HMM, except that it in-
cludes one node, called a span node, that can gen-
erate an entire span rather than a single word. For
instance, in the Span-HMM of Figure 1, node y5 is
a span node that generates a span of length 3: ?is
expected to.?
Span-HMMs can be used to provide a single
categorical value for any span of a sentence us-
ing the usual Viterbi algorithm for HMMs. That
is, at test time, we generate a Span-HMM feature
for word wj by constructing a Span-HMM that has
a span node for the sequence of words between wj
and the predicate. We determine the Viterbi opti-
mal state of this span node, and use that state as the
value of the new feature. In our example in Figure
1, the value of span node y5 is used as a feature for
972
the token ?rate?, since y5 generates the sequence
of words between ?rate? and the predicate ?peak.?
Notice that by using Span-HMMs to provide
these features, we have condensed all paths in our
data into a small number of categorical values.
Whereas there are a huge number of variations to
the spans themselves, we can constrain the number
of categories for the Span-HMM states to a rea-
sonable number such that each category is likely to
appear often in the training data. The value of each
Span-HMM state then represents a cluster of spans
with similar delimiting words; some clusters will
correlate with spans between predicates and argu-
ments, and others with spans that do not connect
predicates and arguments. As a result, Span-HMM
features are not sparse, and they correlate with the
target function, making them useful in learning an
SRL model.
6.2 Parameter Estimation
We use a variant of the Baum-Welch algorithm to
train our Span-HMMs on unlabeled text. In order
for this to work, we need to provide Baum-Welch
with a modified view of the data so that span nodes
can generate multiple consecutive words in a sen-
tence. First, we take every sentence S in our train-
ing data and generate the set Spans(S) of all valid
spans in the sentence. For efficiency?s sake, we use
only spans of length less than 15; approximately
95% of the arguments in our dataset were within
15 words of the predicate, so even with this re-
striction we are able to supply features for nearly
all valid arguments. The second step of our train-
ing procedure is to create a separate data point for
each span of S. For each span t ? Spans(S), we
construct a Span-HMM with a regular node gen-
erating each element of S, except that a span node
generates all of t. Thus, our training data contains
many different copies of each sentence S, with a
different Span-HMM generating each copy.
Intuitively, running Baum-Welch over this data
means that a span node with state k will be likely
to generate two spans t1 and t2 if t1 and t2 tend to
appear in similar contexts. That is, they should
appear between words that are also likely to be
generated by the same latent state. Thus, certain
values of k will tend to appear for spans between
predicates and arguments, and others will tend
to appear between predicates and non-arguments.
This makes the value k informative for both argu-
ment identification and argument classification.
6.3 Memory Considerations
Memory usage is a major issue for our Span-
HMM models. We represent emission distribu-
tions as multinomials over discrete observations.
Since there are millions of different spans in our
data, a straightforward implementation would re-
quire millions of parameters for each latent state
of the Span-HMM.
We use two related techniques to get around this
problem. In both cases, we use a second HMM
model, which we call the base HMM to distin-
guish from our Span-HMM, to back-off from the
explicit word sequence. We use the largest num-
ber of states for HMMs that can be fit into mem-
ory. Let S be a sentence, and let s? be the sequence
of optimal latent state values for S produced by
our base HMM. Our first approach trains the Span-
HMM on Spans(s?), rather than Spans(S). If
we use a small enough number of latent states in
the base HMM (in experiments, we use 10 latent
states), we drastically reduce the number of differ-
ent spans in the data set, and therefore the num-
ber of parameters required for our model. We call
this representation Span-HMM-Base10. As with
our other HMM-based models, we use the largest
number of latent states that will allow the result-
ing model to fit in our machine?s memory ? our
previous experiments on representations for part-
of-speech tagging suggest that more latent states
are usually better.
While our first technique solves the memory is-
sue, it also loses some of the power of our orig-
inal Span-HMM model by using a very coarse-
grained base HMM clustering of the text into 10
categories. Our second approach trains a separate
Span-HMM model for spans of different lengths.
Since we need only one model in memory at a
time, this allows each one to consume more mem-
ory. We therefore use base HMM models with
more latent states (up to 20) to annotate our sen-
tences, and then train on the resulting Spans(s?)
as before. With this technique, we produce fea-
tures that are combinations of the state value for
span nodes and the length of the span, in order
to indicate which of our Span-HMM models the
state value came from. We call this representation
Span-HMM-BaseByLength.
6.4 Combining Multiple Span-HMMs
So far, our Span-HMM models produce one new
feature for every token during argument identifi-
973
System P R F1
Baseline+HMM+Paths 70.0 65.6 67.7
Toutanova et al NR NR 68.8
Span-HMM-Base10 74.5 69.3 71.8
Span-HMM-BaseByLength 76.3 70.2 73.1
Multi-Span-HMM 77.0 70.9 73.8
Table 3: Span-HMM features significantly im-
prove over state-of-the-art results in out-of-
domain SRL. Differences in both precision and re-
call between the baseline and the Span-HMM sys-
tems are statistically significant at p < 0.01 using
the two-tailed Fisher?s exact test.
cation and classification. While these new fea-
tures may be very helpful, ideally we would like
our learned representations to produce multiple
useful features for the CRF model, so that the
CRF can combine the signals from each feature
to learn a sophisticated model. Towards this goal,
we train N independent versions of our Span-
HMM-BaseByLength models, each with a ran-
dom initialization for the Baum-Welch algorithm.
Since Baum-Welch is a hill-climbing algorithm,
it should find local, but not necessarily global,
optima for the parameters of each Span-HMM-
BaseByLength model. When we decode each of
the models on training and test texts, we will ob-
tain N different sequences of latent states, one
for each locally-optimized model. Thus we obtain
N different, independent sources of features. We
call the CRF model with these N Span-HMM fea-
tures the Multi-Span-HMM model(MSH); in ex-
periments we use N = 5.
6.5 Results and Discussion
Results for the Span-HMM models on the CoNLL
2005 Brown corpus are shown in Table 3. All three
versions of the Span-HMM outperform Toutanova
et al?s system on the Brown corpus, with the
Multi-Span-HMM gaining 5 points in F1. The
Multi-Span-HMM model improves over the Base-
line+HMM+Paths model by 7 points in precision,
and 5.3 points in recall. Among the Span-HMM
models, the use of more states in the Span-HMM-
BaseByLength model evidently outweighed the
cost of splitting the model into separate versions
for different length spans. Using multiple in-
dependent copies of the Span-HMMs provides a
small (0.7) gain in precision and recall. Dif-
ferences among the different Span-HMM models
System WSJ Brown Diff
Multi-Span-HMM 79.2 73.8 5.4
Toutanova et al (2008) 80.8 68.8 12.0
Pradhan et al (2005) 78.6 68.4 10.2
Punyakanok et al (2008) 79.4 67.8 11.6
Table 4: Multi-Span-HMM has a much smaller
drop-off in F1 than comparable systems on out-
of-domain test data vs in-domain test data.
were not statistically significant, except that the
difference in precision between the Multi-Span-
HMM and the Span-HMM-Base10 is significant
at p < .1.
Table 4 shows the performance drop-off for top
SRL systems when applied to WSJ test data and
Brown corpus test data. The Multi-Span-HMM
model performs near the state-of-the-art on the
WSJ test set, and its F1 on out-of-domain data
drops only about half as much as comparable sys-
tems. Note that several of the techniques used
by other systems, such as using features from k-
best parses or jointly modeling the dependencies
among arguments, are complementary to our tech-
niques, and may boost the performance of our sys-
tem further.
Table 5 breaks our results down by argument
type. Most of our improvement over the Baseline
system comes from the core arguments A0 and
A1, but also from a few adjunct types like AM-
TMP and AM-LOC. Figure 2 shows that when the
argument is close to the predicate, both systems
perform well, but as the distance from the predi-
cate grows, our Multi-Span-HMM system is bet-
ter able to identify and classify arguments than the
Baseline+HMM+Paths system.
Table 6 provides results for argument identifi-
cation and classification separately. As Pradhan et
al.previously showed (Pradhan et al, 2007), SRL
systems tend to have an easier time with porting
argument identification to new domains, but are
less strong at argument classification on new do-
mains. Our baseline system decreases in F-score
from 81.5 to 78.9 for argument identification, but
suffers a much larger 8% drop in argument classi-
fication. The Multi-Span-HMM model improves
over the Baseline in both tasks and on both test
sets, but the largest improvement (6%) is in argu-
ment classification on the Brown test set.
To help explain the success of the Span-HMM
techniques, we measured the sparsity of our path
974
Overall A0 A1 A2 A3 A4 ADV DIR DIS LOC MNR MOD NEG PNC TMP R-A0 R-A1
Num 2177 566 676 147 12 15 143 53 22 85 110 91 50 17 112 25 21
BL 67.7 76.2 70.6 64.8 59.0 71.2 52.7 54.8 71.9 67.5 58.3 90.9 90.0 50.0 76.5 76.5 71.3
MSH 73.8 82.5 73.6 63.9 60.3 73.3 50.8 52.9 70.0 70.3 52.7 94.2 92.9 51.6 81.6 84.4 75.7
Table 5: SRL results (F1) on the Brown test corpus broken down by role type. BL is the Base-
line+HMM+Paths model, MSH is the Multi-Span-HMM model. Column 8 to 16 are all adjuncts (AM-).
We omit roles with ten or fewer examples.
50
55
60
65
70
75
80
85
90
F1
 sc
or
e
Words between predicate and argument 
MSH
BL
Figure 2: The Multi-Span-HMM (MSH) model
is better able to identify and classify arguments
that are far from the predicate than the Base-
line+HMM+Paths (BL) model.
Test Id.F1 Accuracy
BL WSJ 81.5 93.7
Brown 78.9 85.8
MSH WSJ 83.9 94.4
Brown 80.3 91.9
Table 6: Baseline (BL) and Multi-Span-HMM
(MSH) performance on argument identification
(Id.F1) and argument classification.
and Span-HMM features. Figure 3 shows the per-
centage of feature values in the Brown corpus that
appear more than twice, exactly twice, or exactly
once in the training data. While word path fea-
tures can be highly valuable when there is train-
ing data available for them, only about 11% of the
word paths in the Brown test set alo appeared at
all in the training data. POS and chunk paths fared
a bit better (22% and 33% respectively), but even
then nearly 70% of all feature values had no avail-
able training data. HMM and Span-HMM-Base10
paths achieved far better success in this respect.
Importantly, the improvement is mostly due to fea-
tures that are seen often in training, rather than fea-
tures that were seen just once or twice. Thus Span-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Fr
ac
tio
n 
of
 Fe
at
ur
e 
Va
lu
es
 in
 
Br
ow
n 
Co
rp
us
Occurs 1x 
in WSJ
Occurs 2x 
in WSJ
Occurs 3x 
or more in 
WSJ
Fr
ac
tio
n 
of
 Fe
at
ur
e 
Va
lu
es
 in
 
Br
ow
n 
Co
rp
us
Figure 3: HMM path and Span-HMM features are
far more likely to appear often in training data than
the word, POS, and chunk path features. Over
70% of Span-HMM-Base10 features in the Brown
corpus appear at least three times during training;
in contrast, fewer than 33% of chunk path features
in the Brown corpus appear at all during training.
HMMs derive their power as representations for
open-domain SRL from the fact that they provide
features that are mostly the same across domains;
80% of the features of our Span-HMM-Base10 in
the Brown corpus were observed at least once in
the training data.
Table 7 shows examples of spans that were
clustered into the same Span-HMM state, along
with word to either side. All four examples
are cases where the Span-HMM-Base10 model
correctly tagged the following argument, but the
Baseline+HMM+Paths model did not. We can see
that the paths of these four examples are com-
pletely different, but the words surrounding them
are very similar. The emission from a span node
are very sparse, so the Span-HMM has unsurpris-
ingly learned to cluster spans according to the
HMM states that precede and follow the span
node. This is by design, as this kind of distri-
butional clustering is helpful for identifying and
classifying arguments. One potentially interesting
975
Predicate Span B-Arg
picked the things up from
passed through the barbed wire at
come down from Sundays to
sat over his second rock in
Table 7: Example spans labeled with the same
Span-HMM state. The examples are taken from
sentences where the Span-HMM-Base10 model
correctly identified the argument on the right, but
the Baseline+HMM+Paths model did not.
question for future work is whether a less sparse
model of the spans themselves, such as a Na??ve
Bayes model for the span node, would yield a bet-
ter clustering for producing features for semantic
role labeling.
7 Previous Work
Deschact and Moens (2009) use a latent-variable
language model to provide features for an SRL
system, and they show on CoNLL 2008 data that
they can significantly improve performance when
little labeled training data is available. They do
not report on out-of-domain tests. They use HMM
language models trained on unlabeled text, much
like we use in our baseline systems, but they do not
consider models of word spans, which we found to
be most beneficial. Downey et al (2007b) also in-
corporate HMM-based representations into a sys-
tem for the related task of Web information extrac-
tion, and are able to show that the system improves
performance on rare terms.
Fu?rstenau and Lapata (2009b; 2009a) use semi-
supervised techniques to automatically annotate
data for previously unseen predicates with seman-
tic role information. This task differs from ours
in that it focuses on previously unseen predicates,
which may or may not be part of text from a new
domain. Their techniques also result in relatively
lower performance (F1 between 15 and 25), al-
though their tests are on a more difficult and very
different corpus. Weston et al (2008) use deep
learning techniques based on semi-supervised em-
beddings to improve an SRL system, though their
tests are on in-domain data. Unsupervised SRL
systems (Swier and Stevenson, 2004; Grenager
and Manning, 2006; Abend et al, 2009) can natu-
rally be ported to new domains with little trouble,
but their accuracy thus far falls short of state-of-
the-art supervised and semi-supervised systems.
The disparity in performance between in-
domain and out-of-domain tests is by no means
restricted to SRL. Past research in a variety of
NLP tasks has shown that parsers (Gildea, 2001),
chunkers (Huang and Yates, 2009), part-of-speech
taggers (Blitzer et al, 2006), named-entity tag-
gers (Downey et al, 2007a), and word sense dis-
ambiguation systems (Escudero et al, 2000) all
suffer from a similar drop-off in performance on
out-of-domain tests. Numerous domain adapta-
tion techniques have been developed to address
this problem, including self-training (McClosky et
al., 2006) and instance weighting (Bacchiani et al,
2006) for parser adaptation and structural corre-
spondence learning for POS tagging (Blitzer et al,
2006). Of these techniques, structural correspon-
dence learning is closest to our technique in that it
is a form of representation learning, but it does not
learn features for word spans. None of these tech-
niques have been successfully applied to SRL.
8 Conclusion and Future Work
We have presented novel representation-learning
techniques for building an open-domain SRL sys-
tem. By incorporating learned features from
HMMs and Span-HMMs trained on unlabeled
text, our SRL system is able to correctly iden-
tify predicates in out-of-domain text with an F1
of 93.5, and it can identify and classify argu-
ments to predicates with an F1 of 73.8, out-
performing comparable state-of-the-art systems.
Our successes so far on out-of-domain tests bring
hope that supervised NLP systems may eventually
achieve the ideal where they no longer need new
manually-labeled training data for every new do-
main. There are several potential avenues for fur-
ther progress towards this goal, including the de-
velopment of more portable SRL pipeline systems,
and especially parsers. Developing techniques that
can incrementally adapt to new domains without
the computational expense of retraining the CRF
model every time would help make open-domain
SRL more practical.
Acknowledgments
We wish to thank the anonymous reviewers for
their helpful comments and suggestions.
976
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the ACL.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In Advances in Neural In-
formation Processing Systems 20, Cambridge, MA.
MIT Press.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jenn Wortman.
2009. A theory of learning from different domains.
Machine Learning, (to appear).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Xavier Carreras and Llu??s Ma`rquez. 2003. Phrase
recognition by filtering and ranking with percep-
trons. In Proceedings of RANLP-2003.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proceedings of the Conference on Nat-
ural Language Learning (CoNLL).
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Conference on Nat-
ural Language Learning (CoNLL).
Trevor Cohn and Phil Blunsom. 2005. Semantic role
labelling with tree conditional random fields. In
Proceedings of CoNLL.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
D. Downey, M. Broadhead, and O. Etzioni. 2007a. Lo-
cating complex named entities in web text. In Procs.
of the 20th International Joint Conference on Artifi-
cial Intelligence (IJCAI 2007).
Doug Downey, Stefan Schoenmackers, and Oren Et-
zioni. 2007b. Sparse information extraction: Unsu-
pervised language models to the rescue. In ACL.
G. Escudero, L. Ma?rquez, and G. Rigau. 2000. An
empirical study of the domain dependence of su-
pervised word sense disambiguation systems. In
EMNLP/VLC.
Hagen Fu?rstenau and Mirella Lapata. 2009a. Graph
alignment for semi-supervised semantic role label-
ing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 11?20.
Hagen Fu?rstenau and Mirella Lapata. 2009b. Semi-
supervised semantic role labeling. In Proceedings
of the 12th Conference of the European Chapter of
the ACL, pages 220?228.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Conference on Empirical Methods in
Natural Language Processing.
Trond Grenager and Christopher D Manning. 2006.
Unsupervised discovery of a statistical verb lexi-
con. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence labeling. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
H. Kucera and W.N. Francis. 1967. Computational
Analysis of Present-Day American English. Brown
University Press.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
In Proceedings of the International Conference on
Machine Learning.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL, pages 337?344.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A corpus annotated with se-
mantic roles. Computational Linguistics Journal,
31(1).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role chunking combining complementary
syntactic views. In Proc. of the Annual Confer-
ence on Computational Natural Language Learning
(CoNLL).
Sameer Pradhan, Wayne Ward, and James H. Martin.
2007. Towards robust semantic role labeling. In
Proceedings of NAACL-HLT, pages 556?563.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
977
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
285.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for se-
mantic role labeling. Computational Linguistics,
34(2):161?191.
Jason Weston, Frederic Ratle, and Ronan Collobert.
2008. Deep learning via semi-supervised embed-
ding. In Proceedings of the 25th International Con-
ference on Machine Learning.
978
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211?219,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Goodness: A Method for Measuring Machine Translation Confidence
Nguyen Bach?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nbach@cs.cmu.edu
Fei Huang and Yaser Al-Onaizan
IBM T.J. Watson Research Center
1101 Kitchawan Rd
Yorktown Heights, NY 10567, USA
{huangfe, onaizan}@us.ibm.com
Abstract
State-of-the-art statistical machine translation
(MT) systems have made significant progress
towards producing user-acceptable translation
output. However, there is still no efficient
way for MT systems to inform users which
words are likely translated correctly and how
confident it is about the whole sentence. We
propose a novel framework to predict word-
level and sentence-level MT errors with a large
number of novel features. Experimental re-
sults show that the MT error prediction accu-
racy is increased from 69.1 to 72.2 in F-score.
The Pearson correlation between the proposed
confidence measure and the human-targeted
translation edit rate (HTER) is 0.6. Improve-
ments between 0.4 and 0.9 TER reduction are
obtained with the n-best list reranking task us-
ing the proposed confidence measure. Also,
we present a visualization prototype of MT er-
rors at the word and sentence levels with the
objective to improve post-editor productivity.
1 Introduction
State-of-the-art Machine Translation (MT) systems are
making progress to generate more usable translation
outputs. In particular, statistical machine translation
systems (Koehn et al, 2007; Bach et al, 2007; Shen
et al, 2008) have advanced to a state that the transla-
tion quality for certain language pairs (e.g. Spanish-
English, French-English, Iraqi-English) in certain do-
mains (e.g. broadcasting news, force-protection, travel)
is acceptable to users.
However, a remaining open question is how to pre-
dict confidence scores for machine translated words
and sentences. An MT system typically returns the
best translation candidate from its search space, but
still has no reliable way to inform users which word
is likely to be correctly translated and how confident it
is about the whole sentence. Such information is vital
? Work done during an internship at IBM T.J. Watson
Research Center
to realize the utility of machine translation in many ar-
eas. For example, a post-editor would like to quickly
identify which sentences might be incorrectly trans-
lated and in need of correction. Other areas, such as
cross-lingual question-answering, information extrac-
tion and retrieval, can also benefit from the confidence
scores of MT output. Finally, even MT systems can
leverage such information to do n-best list reranking,
discriminative phrase table and rule filtering, and con-
straint decoding (Hildebrand and Vogel, 2008).
Numerous attempts have been made to tackle the
confidence estimation problem. The work of Blatz et
al. (2004) is perhaps the best known study of sentence
and word level features and their impact on transla-
tion error prediction. Along this line of research, im-
provements can be obtained by incorporating more fea-
tures as shown in (Quirk, 2004; Sanchis et al, 2007;
Raybaud et al, 2009; Specia et al, 2009). Sori-
cut and Echihabi (2010) developed regression models
which are used to predict the expected BLEU score
of a given translation hypothesis. Improvement also
can be obtained by using target part-of-speech and null
dependency link in a MaxEnt classifier (Xiong et al,
2010). Ueffing and Ney (2007) introduced word pos-
terior probabilities (WPP) features and applied them in
the n-best list reranking. From the usability point of
view, back-translation is a tool to help users to assess
the accuracy level of MT output (Bach et al, 2007).
Literally, it translates backward the MT output into the
source language to see whether the output of backward
translation matches the original source sentence.
However, previous studies had a few shortcomings.
First, source-side features were not extensively inves-
tigated. Blatz et al(2004) only investigated source n-
gram frequency statistics and source language model
features, while other work mainly focused on target
side features. Second, previous work attempted to in-
corporate more features but faced scalability issues,
i.e., to train many features we need many training ex-
amples and to train discriminatively we need to search
through all possible translations of each training exam-
ple. Another issue of previous work was that they are
all trained with BLEU/TER score computing against211
the translation references which is different from pre-
dicting the human-targeted translation edit rate (HTER)
which is crucial in post-editing applications (Snover et
al., 2006; Papineni et al, 2002). Finally, the back-
translation approach faces a serious issue when forward
and backward translation models are symmetric. In this
case, back-translation will not be very informative to
indicate forward translation quality.
In this paper, we predict error types of each word
in the MT output with a confidence score, extend it to
the sentence level, then apply it to n-best list reranking
task to improve MT quality, and finally design a vi-
sualization prototype. We try to answer the following
questions:
? Can we use a rich feature set such as source-
side information, alignment context, and depen-
dency structures to improve error prediction per-
formance?
? Can we predict more translation error types i.e
substitution, insertion, deletion and shift?
? How good do our prediction methods correlate
with human correction?
? Do confidence measures help the MT system to
select a better translation?
? How confidence score can be presented to im-
prove end-user perception?
In Section 2, we describe the models and training
method for the classifier. We describe novel features
including source-side, alignment context, and depen-
dency structures in Section 3. Experimental results and
analysis are reported in Section 4. Section 5 and 6
present applications of confidence scores.
2 Confidence Measure Model
2.1 Problem setting
Confidence estimation can be viewed as a sequen-
tial labelling task in which the word sequence is
MT output and word labels can be Bad/Good or
Insertion/Substitution/Shift/Good. We first esti-
mate each individual word confidence and extend it to
the whole sentence. Arabic text is fed into an Arabic-
English SMT system and the English translation out-
puts are corrected by humans in two phases. In phase
one, a bilingual speaker corrects the MT system trans-
lation output. In phase two, another bilingual speaker
does quality checking for the correction done in phase
one. If bad corrections were spotted, they correct them
again. In this paper we use the final correction data
from phase two as the reference thus HTER can be
used as an evaluation metric. We have 75 thousand sen-
tences with 2.4 million words in total from the human
correction process described above.
We obtain training labels for each word by perform-
ing TER alignment between MT output and the phase-
two human correction. From TER alignments we ob-
served that out of total errors are 48% substitution, 28%
deletion, 13% shift, and 11% insertion errors. Based
on the alignment, each word produced by the MT sys-
tem has a label: good, insertion, substitution and shift.
Since a deletion error occurs when it only appears in the
reference translation, not in the MT output, our model
will not predict deletion errors in the MT output.
2.2 Word-level model
In our problem, a training instance is a word from MT
output, and its label when the MT sentence is aligned
with the human correction. Given a training instance x,
y is the true label of x; f stands for its feature vector
f(x, y); and w is feature weight vector. We define a
feature-rich classifier score(x, y) as follow
score(x, y) = w.f(x, y) (1)
To obtain the label, we choose the class with the high-
est score as the predicted label for that data instance.
To learn optimized weights, we use the Margin Infused
Relaxed Algorithm or MIRA (Crammer and Singer,
2003; McDonald et al, 2005) which is an online learner
closely related to both the support vector machine and
perceptron learning framework. MIRA has been shown
to provide state-of-the-art performance for sequential
labelling task (Rozenfeld et al, 2006), and is also able
to provide an efficient mechanism to train and opti-
mize MT systems with lots of features (Watanabe et
al., 2007; Chiang et al, 2009). In general, weights are
updated at each step time t according to the following
rule:
wt+1 = argminwt+1 ||wt+1 ? wt||
s.t. score(x, y) ? score(x, y?) + L(y, y?)
(2)
where L(y, y?) is a measure of the loss of using y? in-
stead of the true label y. In this problem L(y, y?) is 0-1
loss function. More specifically, for each instance xi in
the training data at a time t we find the label with the
highest score:
y? = argmax
y
score(xi, y) (3)
the weight vector is updated as follow
wt+1 = wt + ?(f(xi, y)? f(xi, y
?)) (4)
? can be interpreted as a step size; when ? is a large
number we want to update our weights aggressively,
otherwise weights are updated conservatively.
? = max(0, ?)
? = min
{
C, L(y,y
?)?(score(xi,y)?score(xi,y
?))
||f(xi,y)?f(xi,y?)||22
}
(5)
where C is a positive constant used to cap the maxi-
mum possible value of ? . In practice, a cut-off thresh-
old n is the parameter which decides the number of
features kept (whose occurrence is at least n) during212
training. Note that MIRA is sensitive to constant C,
the cut-off feature threshold n, and the number of iter-
ations. The final weight is typically normalized by the
number of training iterations and the number of train-
ing instances. These parameters are tuned on a devel-
opment set.
2.3 Sentence-level model
Given the feature sets and optimized weights, we use
the Viterbi algorithm to find the best label sequence.
To estimate the confidence of a sentence S we rely on
the information from the forward-backward inference.
One approach is to directly use the conditional prob-
abilities of the whole sequence. However, this quan-
tity is the confidence measure for the label sequence
predicted by the classifier and it does not represent the
goodness of the whole MT output. Another more ap-
propriated method is to use the marginal probability of
Good label which can be defined as follow:
p(yi = Good|S) =
?(yi|S)?(yi|S)
?
j ?(yj |S)?(yj |S)
(6)
p(yi = Good|S) is the marginal probability of label
Good at position i given the MT output sentence S.
?(yi|S) and ?(yi|S) are forward and backward values.
Our confidence estimation for a sentence S of k words
is defined as follow
goodness(S) =
?k
i=1 p(yi = Good|S)
k
(7)
goodness(S) is ranging between 0 and 1, where 0 is
equivalent to an absolutely wrong translation and 1
is a perfect translation. Essentially, goodness(S) is
the arithmetic mean which represents the goodness of
translation per word in the whole sentence.
3 Confidence Measure Features
Features are generated from feature types: abstract
templates from which specific features are instantiated.
Features sets are often parameterized in various ways.
In this section, we describe three new feature sets intro-
duced on top of our baseline classifier which has WPP
and target POS features (Ueffing and Ney, 2007; Xiong
et al, 2010).
3.1 Source-side features
From MT decoder log, we can track which source
phrases generate target phrases. Furthermore, one can
infer the alignment between source and target words
within the phrase pair using simple aligners such as
IBM Model-1 alignment.
Source phrase features: These features are designed
to capture the likelihood that source phrase and target
word co-occur with a given error label. The intuition
behind them is that if a large percentage of the source
phrase and target have often been seen together with the
Sour
ce P
OS a
nd P
hras
es
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
Targ
et PO
S: PR
P  V
BZ  
 
IN   
DT  
 
NN  
 
 
RB  
 
VBZ
 
 
 
TO  
 
DT  
 
NN  
IN   
DT  
 
JJ   J
JNN
S
VBP
 
 
 
IN   
 
 
DT  
 
 
 
DTN
N    
 
RB  
 
 
VBP
 
 
 
IN   
NN  
 
 
NN
DTJJ
 
 
 
 
 
 
DTJ
JD
TNN
S    D
TJJ
MT o
utpu
t
Sour
ce P
OS
Sour
ce
He  
 
adds
 
 
 
that 
 
this 
proc
ess
also
 
 
refer
s  to
 
 
the  
inab
ility  
of  th
e  m
ultin
ation
al  n
ava
l  for
ces
wydy
fan
 
 
 
hdhh
alam
lyt
ayda
tshyr
alya
dm
qdrt
almt
addt
aljnsy
tal
qwat
albh
ryt
(a) Source phrase
Sour
ce P
OS a
nd P
hras
es
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
Targ
et PO
S: PR
P  V
BZ  
 
IN   
DT  
 
NN  
 
 
RB  
 
VBZ
 
 
 
TO  
 
DT  
 
NN  
IN   
DT  
 
JJ   J
JNN
S
1 if  
sou
rce-
POS
-
sequ
enc
e =
 
?DT 
DTN
N?
f 125(ta
rget-
wor
d = ?
proc
ess
?) = 
0 oth
erw
ise
MT o
utpu
t
Sour
ce P
OS
Sour
ce
wydy
fan
 
 
 
hdhh
alam
lyt
ayda
tshyr
alya
dm
qdrt
almt
addt
aljnsy
tal
qwat
albh
ryt
He  
 
adds
 
 
 
that 
 
this 
proc
ess
also
 
 
refer
s  to
 
 
the  
inab
ility  
of  th
e  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
IN   
 
 
DT  
 
 
 
DTN
N
RB  
 
 
VBP
 
 
 
IN   
NN  
 
 
NN
DTJJ
 
 
 
 
 
 
DTJ
JD
TNN
S    D
TJJ
(b) Source POS
Sour
ce P
OS a
nd P
hras
es
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
Targ
et PO
S: PR
P  V
BZ  
 
IN   
DT  
 
NN  
 
 
RB  
 
VBZ
 
 
 
TO  
 
DT  
 
NN  
IN   
DT  
 
JJ   J
JNN
S
MT o
utpu
t
Sour
ce P
OS
Sour
ce
He  
adds
 
 
that 
this 
 
 
proc
ess
also
 
 
 
 
refer
s  to
 
 
the  
inab
ility  
of  th
e  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
IN   
 
 
DT  
 
 
 
DTN
N
RB  
 
 
 
 
VBP
IN   
NN  
 
 
NN
DTJJ
 
 
 
 
 
 
DTJJ
DTN
NS  
 
 
DTJJ
wydy
fan
 
 
 
hdhh
alam
lyt
ayda
tshy
ra
lyad
m
qdrt
almt
addt
aljnsy
tal
qwat
albh
ryt
(c) Source POS and phrase in right context
Figure 1: Source-side features.
same label, then the produced target word should have
this label in the future. Figure 1a illustrates this feature
template where the first line is source POS tags, the
second line is the Buckwalter romanized source Arabic
sequence, and the third line is MT output. The source
phrase feature is defined as follow
f102(process) =
{
1 if source-phrase=?hdhh alamlyt?
0 otherwise
Source POS: Source phrase features might be suscep-
tible to sparseness issues. We can generalize source
phrases based on their POS tags to reduce the number
of parameters. For example, the example in Figure 1a
is generalized as in Figure 1b and we have the follow-
ing feature:
f103(process) =
{
1 if source-POS=? DT DTNN ?
0 otherwise
Source POS and phrase context features: This fea-
ture set alows us to look at the surrounding context
of the source phrase. For example, in Figure 1c we
have ?hdhh alamlyt? generates ?process?. We also
have other information such as on the right hand side
the next two phrases are ?ayda? and ?tshyr? or the se-
quence of source target POS on the right hand side is
?RB VBP?. An example of this type of feature is
f104(process) =
{
1 if source-POS-context=? RB VBP ?
0 otherwise
3.2 Alignment context features
The IBM Model-1 feature performed relatively well in
comparison with the WPP feature as shown by Blatz et
al. (2004). In our work, we incorporate not only the213
Align
me
nt C
onte
xt
WP
P: 1
.
0 0.
67 1
.
0 1.
01.
00.
67 ?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
RB 
 
 
 
 
 
VBZ
 
 
 
TO
DT 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
 
JJ   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
VBP
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
DTN
N    
 
 
 
 
 
RB 
 
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S    
 
DTJ
J
wyd
yfa
n hd
hh
alam
lyt
ayd
a
tshy
ra
lya
dm
qdrt
alm
tadd
tal
jnsyt
alqw
at
albh
ryt
He  
add
s  th
at  t
his  
 
proc
ess
 
 
 
also
 
 
refe
rs
to  t
he  
inab
ility 
 
 
 
of   
the 
 
mu
ltina
tion
al  n
ava
l  fo
rce
s
MT 
outp
ut
Sou
rce
 
POS
Sou
rce
Targ
et P
OS
(a) Left sourceAlign
me
nt C
onte
xt
WP
P: 1
.
0 0.
67 1
.
0 1.
01.
00.
67 ?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
RB 
 
 
 
 
 
VBZ
 
 
 
 
 
TO
DT 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
 
JJ   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
VBP
 
 
 
 
 
IN   
 
 
 
 
 
DT 
 
 
 
 
 
DTN
N    
 
 
RB 
 
 
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S    
 
DTJ
J
wyd
yfa
n   
hdh
ha
lam
lyta
yda
tshy
ra
lya
dm
qdrt
alm
tadd
tal
jnsyt
alqw
at
albh
ryt
He  
add
s  th
at  t
his  
 
 
proc
ess
 
also
 
 
 
refe
rs
to  t
he  
inab
ility 
 
 
of   
the 
 
mu
ltina
tion
al  n
ava
l  fo
rce
s
MT 
outp
ut
Sou
rce
 
POS
Sou
rce
Targ
et P
OS
(b) Rig t source
Align
me
nt C
onte
xt
WP
P: 1
.
0 0.
67 1
.
0 1.
01.
00.
67 ?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
DT 
 
 
 
 
 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
RB 
 
 
 
 
 
VBZ
 
 
 
 
 
 
TO  
 
DT 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
 
JJ   
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
VBP
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
DTN
N    
 
 
 
 
 
RB 
 
 
 
 
 
VBP
 
 
 
 
 
 
 
IN   
 
 
NN 
 
 
 
 
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S    
 
DTJ
J
wyd
yfa
n hd
hh
alam
lyt
ayda
tshy
r
aly
adm
qdrt
alm
tadd
tal
jnsyt
alqw
at
albh
ryt
He  
add
s  th
at th
is    
proc
ess
also
 
 
refe
rs  
 
to  t
he  
inab
ility 
 
 
of   
the 
 
mu
ltina
tion
al  n
ava
l  fo
rce
s
MT 
outp
ut
Sou
rce
 
POS
Sou
rce
Targ
et P
OS
(c) Left targetAlign
me
nt C
onte
xt
WP
P: 1
.
0 0.
67 1
.
0 1.
01.
00.
67 ?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
NN 
 
 
 
 
 
 
 
 
RB 
 
 
 
 
 
VBZ
 
 
 
 
 
TO
DT 
 
 
 
NN 
 
 
 
 
 
 
 
 
 
 
 
IN   
 
 
DT 
 
 
 
 
 
 
 
 
 
 
JJ   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
wyd
yfa
n  h
dhh
alam
lyt
ayda
tshy
r
aly
adm
qdrt
alm
tadd
tal
jnsyt
alqw
at
albh
ryt
MT 
outp
ut
Sou
rce
 
POS
Sou
rce
Targ
et P
OS
He  
add
s  th
at  t
his  
 
proc
ess
 
 
 
also
 
 
refe
rs
tot
he  
inab
ility 
 
 
of   
the 
 
mu
ltina
tion
al  n
ava
l  for
ces
VBP
 
 
 
 
 
IN   
 
 
 
 
DT 
 
 
 
 
DTN
N    
 
 
 
 
 
RB 
 
 
 
 
VBP
IN   
 
 
 
NN 
 
 
 
 
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S    
 
DTJ
J
(d) Source POS & right tar-
get
Figur 2: Alignment context features.
IBM Model-1 feature but also the surr unding align-
ment context. The key intuition is that collocation is a
reliable indicator for judging if a target word is gener-
ated by a particular source word (Huang, 2009). More-
over, the IBM Model-1 feature was already used in sev-
eral steps of a translation system such as word align-
ment, phrase extraction and scoring. Also the impact of
this feature alone might fade away when the MT sys-
tem is scaled up.
We obtain word-to-word alignments by applying
IBM Model-1 to bilingual phrase pairs that generated
the MT output. The IBM Model-1 assumes one
target word can only be aligned to one source word.
Therefore, given a target word we can always identify
which source word it is aligned to.
Source alignment context feature: We anchor the
target word and derive context features surround-
ing its source word. For example, in Figure 2a
and 2b we have an alignment between ?tshyr? and
?refers? The source contexts ?tshyr? with a window
of one word are ?ayda? to the left and ?aly? to the right.
Target algnment context feature: Similar to source
alignment context features, we anchor the source word
and derive context features surrounding the aligned
target word. Figure 2c shows a left target context
feature of word ?refers?. Our features are derived from
a window of four words.
Combining alignment context with POS tags: In-
stead of using lexical context we have features to look
at source and target POS alignment context. For in-
stance, the feature in Figure 2d is
f141(refers) =
{
1 if source-POS = ?VBP?
and target-context = ?to?
0 otherwise
Sour
ce &
 
Targ
et D
epen
denc
y 
Struc
ture
s
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
NN  
 
 
 
 
 
 
 
RB  
 
 
 
 
VBZ
 
 
 
 
 
TO
DT  
 
 
NN  
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
 
JJ    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
wydy
fan
 
 
 
hdhh
alam
lyta
yda
tshyr
alya
dm
qdrt
almt
addt
aljnsy
talq
wat
albh
ryt
He  
adds
 
 
that 
 
this 
 
 
proc
ess
 
 
 
also
 
 
refer
s   to
 
 
the  
inab
ility  
 
of   t
he  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
 
 
IN   
 
 
 
 
 
DT  
 
 
DTN
N    
 
 
 
 
 
RB  
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S     
DTJ
J
null
(a) Source-Target dependency
Sour
ce &
 
Targ
et D
epen
denc
y 
Struc
ture
s
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
NN  
 
 
 
 
 
 
 
RB  
 
 
 
 
VBZ
 
 
 
 
 
TO
DT  
 
 
NN  
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
 
JJ    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
wydy
fan
 
 
 
hdhh
alam
lyta
yda
tshy
ra
lyad
m
qdrt
almt
addt
aljnsy
talq
wat
albh
ryt
He  
adds
 
 
that 
 
this 
 
 
proc
ess
 
 
 
also
 
 
refe
rs
to  th
e  in
abilit
y   o
f   th
e  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
 
 
IN   
 
 
 
 
 
DT  
 
 
DTN
N    
 
 
 
 
 
RB  
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S     
DTJ
J
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
(b) Child-Father agreement
Sour
ce &
 
Targ
et D
epen
denc
y 
Struc
ture
s
PRP
 
 
VBZ
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
NN  
 
 
 
 
 
 
 
RB  
 
 
 
 
VBZ
 
 
 
 
 
TO
DT  
 
 
NN  
 
 
 
 
 
 
 
IN   
 
 
DT  
 
 
 
 
 
 
 
 
 
JJ    
 
 
 
 
 
 
 
 
 
 
 
 
 
 
JJ
NNS
wydy
fan
 
 
 
hdhh
alam
lyta
yda
tshy
ra
lyad
m
qdrt
almt
addt
aljnsy
talq
wat
albh
ryt
He  
adds
 
 
that 
 
this 
 
 
proc
ess
 
 
 
also
 
 
refe
rs
to  th
e  in
abilit
y   o
f   th
e  m
ultin
ation
al  n
ava
l  for
ces
VBP
 
 
 
 
 
IN   
 
 
 
 
 
DT  
 
 
DTN
N    
 
 
 
 
 
RB  
 
 
 
 
VBP
 
 
 
 
 
 
IN   
 
 
NN
NN
DTJ
J     
 
 
 
 
DTJ
JD
TNN
S     
DTJ
J
Child
ren
 
Agre
em
ent: 
2
WPP
: 1.0
 
0.67
 
1.0 1
.
01.0
0.67
 
?
(c) Children agreement
Figure 3: Dependency structures features.
3.3 Source and target dependency structure
features
The contextual and source information in the previous
sections only take into account surface structures of
source and target sentences. Meanwhile, dependency
structures have been extensively used in various
translation systems (Shen et al, 2008; Ma et al,
2008; Bach et al, 2009). The adoption of dependency
structures might enable the classifier to utilize deep
structures to predict translation errors. Source and tar-
get structures are unlikely to be isomorphic as shown
in Figure 3a. However, we expect some high-level
linguistic structures are likely to transfer across certain
language pairs. For example, prepositional phrases
(PP) in Arabic and English are similar in a sense
that PPs generally appear at the end of the sentence
(after all the verbal arguments) and to a lesser extent
at its beginning (Habash and Hu, 2009). We use the
Stanford parser to obtain dependency trees and POS
tags (Marneffe et al, 2006).
Child-Father agreement: The motivation is to take
advantage of the long distance dependency relations
between source and target words. Given an alignment
between a source word si and a target word tj . A child-214
father agreement exists when sk is aligned to tl, where
sk and tl are father of si and tj in source and target
dependency trees, respectively. Figure 3b illustrates
that ?tshyr? and ?refers? have a child-father agreement.
To verify our intuition, we analysed 243K words of
manual aligned Arabic-English bitext. We observed
29.2% words having child-father agreements. In term
of structure types, we found 27.2% of copula verb
and 30.2% prepositional structures, including object
of a preposition, prepositional modifier, and preposi-
tional complement, are having child-father agreements.
Children agreement: In the child-father agreement
feature we look up in the dependency tree, however,
we also can look down to the dependency tree with a
similar motivation. Essentially, given an alignment be-
tween a source word si and a target word tj , how many
children of si and tj are aligned together? For exam-
ple, ?tshyr? and ?refers? have 2 aligned children which
are ?ayda-also? and ?aly-to? as shown in Figure 3c.
4 Experiments
4.1 Arabic-English translation system
The SMT engine is a phrase-based system similar to
the description in (Tillmann, 2006), where various
features are combined within a log-linear framework.
These features include source-to-target phrase transla-
tion score, source-to-target and target-to-source word-
to-word translation scores, language model score, dis-
tortion model scores and word count. The training
data for these features are 7M Arabic-English sentence
pairs, mostly newswire and UN corpora released by
LDC. The parallel sentences have word alignment au-
tomatically generated with HMM and MaxEnt word
aligner (Ge, 2004; Ittycheriah and Roukos, 2005).
Bilingual phrase translations are extracted from these
word-aligned parallel corpora. The language model is
a 5-gram model trained on roughly 3.5 billion English
words.
Our training data contains 72k sentences Arabic-
English machine translation with human corrections
which include of 2.2M words in newswire and weblog
domains. We have a development set of 2,707 sen-
tences, 80K words (dev); an unseen test set of 2,707
sentences, 79K words (test). Feature selection and pa-
rameter tuning has been done on the development set in
which we experimented values of C, n and iterations in
range of [0.5:10], [1:5], and [50:200] respectively. The
final MIRA classifier was trained by using pocket crf
toolkit1 with 100 iterations, hyper-parameter C was 5
and cut-off feature threshold n was 1.
We use precision (P ), recall (R) and F-score (F ) to
evaluate the classifier performance and they are com-
1http://pocket-crf-1.sourceforge.net/
puted as follow:
P = the number of correctly tagged labelsthe number of tagged labels
R = the number of correctly tagged labelsthe number of reference labels
F = 2*P*RP+R
(8)
4.2 Contribution of feature sets
We designed our experiments to show the impact
of each feature separately as well as their cumu-
lative impact. We trained two types of classifiers
to predict the error type of each word in MT out-
put, namely Good/Bad with a binary classifier and
Good/Insertion/Substitution/Shift with a 4-class classi-
fier. Each classifier is trained with different feature sets
as follow:
? WPP: we reimplemented WPP calculation based
on n-best lists as described in (Ueffing and Ney,
2007).
? WPP + target POS: only WPP and target POS fea-
tures are used. This is a similar feature set used by
Xiong et al (2010).
? Our features: the classifier has source side, align-
ment context, and dependency structure features;
WPP and target POS features are excluded.
? WPP + our features: adding our features on top of
WPP.
? WPP + target POS + our features: using all fea-
tures.
binary 4-class
dev test dev test
WPP 69.3 68.7 64.4 63.7
+ source side 72.1 71.6 66.2 65.7
+ alignment context 71.4 70.9 65.7 65.3
+ dependency structures 69.9 69.5 64.9 64.3
WPP+ target POS 69.6 69.1 64.4 63.9
+ source side 72.3 71.8 66.3 65.8
+ alignment context 71.9 71.2 66 65.6
+ dependency structures 70.4 70 65.1 64.4
Table 1: Contribution of different feature sets measure
in F-score.
To evaluate the effectiveness of each feature set, we
apply them on two different baseline systems: using
WPP and WPP+target POS, respectively. We augment
each baseline with our feature sets separately. Ta-
ble 1 shows the contribution in F-score of our proposed
feature sets. Improvements are consistently obtained
when combining the proposed features with baseline
features. Experimental results also indicate that source-
side information, alignment context and dependency215
Pred
ictin
g Go
od/B
ad w
ords
 
59.4
59.3
69.3
68.7
69.6
69.1
72.1
71.5
72.4
72
72.6
72.2
586062646668707274
dev
test
Test
 
sets
F-score
WPP
+targ
etPO
S+Ou
rfeat
ures
WPP
+Our
featu
res
Our f
eatur
es
WPP
+targ
etPO
S
WPP
All-G
ood
(a) Binary
Pred
icting
 
Good
/Inse
rtion
/Subs
titutio
n/Shi
ft wo
rds
59.4
59.3
64.4
63.7
64.4
63.9
66.2
65.6
66.6
65.9
66.8
66.1
5859606162636465666768
dev
test
Test 
sets
F-score
WPP
+targ
etPO
S+Ou
rfeat
ures
WPP
+Our
featu
res
Our f
eatur
es
WPP
+targ
etPO
S
WPP
All-Go
od
(b) 4-class
Figure 4: Performance of binary and 4-class classifiers trained with different feature sets on the development and
unseen test sets.
structures have unique and effective levers to improve
the classifier performance. Among the three proposed
feature sets, we observe the source side information
contributes the most gain, which is followed by the
alignment context and dependency structure features.
4.3 Performance of classifiers
We trained several classifiers with our proposed feature
sets as well as baseline features. We compare their per-
formances, including a naive baseline All-Good classi-
fier, in which all words in the MT output are labelled
as good translations. Figure 4 shows the performance
of different classifiers trained with different feature sets
on development and unseen test sets. On the unseen test
set our proposed features outperform WPP and target
POS features by 2.8 and 2.4 absolute F-score respec-
tively. Improvements of our features are consistent in
development and unseen sets as well as in binary and
4-class classifiers. We reach the best performance by
combining our proposed features with WPP and target
POS features. Experiments indicate that the gaps in F-
score between our best system with the naive All-Good
system is 12.9 and 6.8 in binary and 4-class cases, re-
spectively. Table 2 presents precision, recall, and F-
score of individual class of the best binary and 4-class
classifiers. It shows that Good label is better predicted
than other labels, meanwhile, Substitution is gener-
ally easier to predict than Insertion and Shift.
4.4 Correlation between Goodness and HTER
We estimate sentence level confidence score based
on Equation 7. Figure 5 illustrates the correla-
tion between our proposed goodness sentence level
confidence score and the human-targeted translation
edit rate (HTER). The Pearson correlation between
goodness and HTER is 0.6, while the correlation of
WPP and HTER is 0.52. This experiment shows that
goodness has a large correlation with HTER. The
black bar is the linear regression line. Blue and red
Label P R F
Binary
Good 74.7 80.6 77.5
Bad 68 60.1 63.8
4-class
Good 70.8 87 78.1
Insertion 37.5 16.9 23.3
Substitution 57.8 44.9 50.5
Shift 35.2 14.1 20.1
Table 2: Detailed performance in precision, recall
and F-score of binary and 4-class classifiers with
WPP+target POS+Our features on the unseen test set.
bars are thresholds used to visualize good and bad sen-
tences respectively. We also experimented goodness
computation in Equation 7 using geometric mean and
harmonic mean; their Pearson correlation values are 0.5
and 0.35 respectively.
5 Improving MT quality with N-best list
reranking
Experiments reporting in Section 4 indicate that the
proposed confidence measure has a high correlation
with HTER. However, it is not very clear if the core MT
system can benefit from confidence measure by provid-
ing better translations. To investigate this question we
present experimental results for the n-best list rerank-
ing task.
The MT system generates top n hypotheses and for
each hypothesis we compute sentence-level confidence
scores. The best candidate is the hypothesis with high-
est confidence score. Table 3 shows the performance of
reranking systems using goodness scores from our best
classifier in various n-best sizes. We obtained 0.7 TER
reduction and 0.4 BLEU point improvement on the de-
velopment set with a 5-best list. On the unseen test, we
obtained 0.6 TER reduction and 0.2 BLEU point im-
provement. Although, the improvement of BLEU score216
0.91
Goo
d?
Bad Line
ar?f
it
0.70.8 040.50.6
Goodness
0.20.30.4 00.10.2
0
20
40
60
80
100
HTE
R
Figure 5: Correlation between Goodness and HTER.
Dev Test
TER BLEU TER BLEU
Baseline 49.9 31.0 50.2 30.6
2-best 49.5 31.4 49.9 30.8
5-best 49.2 31.4 49.6 30.8
10-best 49.2 31.2 49.5 30.8
20-best 49.1 31.0 49.3 30.7
30-best 49.0 31.0 49.3 30.6
40-best 49.0 31.0 49.4 30.5
50-best 49.1 30.9 49.4 30.5
100-best 49.0 30.9 49.3 30.5
Table 3: Reranking performance with goodness score.
is not obvious, TER reductions are consistent in both
development and unseen sets. Figure 6 shows the im-
provement of reranking with goodness score. Besides,
the figure illustrates the upper and lower bound perfor-
mances with TER metric in which the lower bound is
our baseline system and the upper bound is the best hy-
pothesis in a given n-best list. Oracle scores of each n-
best list are computed by choosing the translation can-
didate with lowest TER score.
6 Visualizing translation errors
Besides the application of confidence score in the n-
best list reranking task, we propose a method to visual-
ize translation error using confidence scores. Our pur-
pose is to visualize word and sentence-level confidence
scores with the following objectives 1) easy for spotting
translations errors; 2) simple and intuitive; and 3) help-
ful for post-editing productivity. We define three cate-
gories of translation quality (good/bad/decent) on both
word and sentence level. On word level, the marginal
probability of good label is used to visualize translation
errors as follow:
Li =
?
?
?
good if p(yi = Good|S) ? 0.8
bad if p(yi = Good|S) ? 0.45
decent otherwise
42
43
44
45
46
47
48
49
50
51
1 2 5 10 20 30 40 50 100
TE
R
N-best size
Oracle
Our models
Baseline
Figure 6: A comparison between reranking and oracle
scores with different n-best size in TER metric on the
development set.
On sentence level, the goodness score is used as follow:
LS =
?
?
?
good if goodness(S) ? 0.7
bad if goodness(S) ? 0.5
decent otherwise
Choices Intention
Font size
big bad
small good
medium decent
Colors
red bad
black good
orange decent
Table 4: Choices of layout
Different font sizes and colors are used to catch the
attention of post-editors whenever translation errors are
likely to appear as shown in Table 4. Colors are ap-
plied on word level, while font size is applied on both
word and sentence level. The idea of using font size
and colour to visualize translation confidence is simi-
lar to the idea of using tag/word cloud to describe the
content of websites2. The reason we are using big font
size and red color is to attract post-editors? attention
and help them find translation errors quickly. Figure 7
shows an example of visualizing confidence scores by
font size and colours. It shows that ?not to deprive
yourself ?, displayed in big font and red color, is likely
to be bad translations. Meanwhile, other words, such
as ?you?, ?different?, ?from?, and ?assimilation?, dis-
played in small font and black color, are likely to be
good translation. Medium font and orange color words
are decent translations.
2http://en.wikipedia.org/wiki/Tag cloud217
you
 
tota
lly d
iffe
ren
t fro
m 
za
ida
mr
, 
an
d n
ot t
o d
epr
ive
 
you
rse
lf in
 
a b
as
em
en
t of
 
imi
tati
on
 
an
d a
ss
imi
lati
on
 
.
  ?? 
	

??? ?
 
 
 
 
 ?
? 
?Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 387?392,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generalized Reordering Rules for Improved SMT 
 
          Fei Huang Cezar Pendus 
IBM T. J. Watson Research Center IBM T. J. Watson Research Center 
huangfe@us.ibm.com cpendus@us.ibm.com 
 
 
 
Abstract 
We present a simple yet effective 
approach to syntactic reordering for 
Statistical Machine Translation (SMT). 
Instead of solely relying on the top-1 
best-matching rule for source sentence 
preordering, we generalize fully 
lexicalized rules into partially lexicalized 
and unlexicalized rules to broaden the 
rule coverage. Furthermore, , we consider 
multiple permutations of all the matching 
rules, and select the final reordering path 
based on the weighed sum of reordering 
probabilities of these rules.  Our 
experiments in English-Chinese and 
English-Japanese translations 
demonstrate the effectiveness of the 
proposed approach: we observe 
consistent and significant improvement 
in translation quality across multiple test 
sets in both language pairs judged by 
both humans and automatic metric.  
1 Introduction 
Languages are structured data. The proper 
handling of linguistic structures (such as word 
order) has been one of the most important yet 
most challenging tasks in statistical machine 
translation (SMT). It is important because it has 
significant impact on human judgment of 
Machine Translation (MT) quality: an MT output 
without structure is just like a bag of words. It is 
also very challenging due to the lack of effective 
methods to model the structural difference 
between source and target languages.  
   A lot of research has been conducted in this 
area. Approaches include distance-based penalty 
function (Koehn et. al. 2003) and lexicalized 
distortion models such as (Tillman 2004), (Al-
Onaizan and Papineni 2006). Because these 
models are relatively easy to compute, they are 
widely used in phrase-based SMT systems. 
Hierarchical phrase-based system (Hiero, 
Chiang, 2005) utilizes long range reordering 
information without syntax. Other models use 
more syntactic information (string-to-tree, tree-
to-string, tree-to-tree, string-to-dependency etc.) 
to capture the structural difference between 
language pairs, including (Yamada and Knight, 
2001), (Zollmann and Venugopal, 2006), (Liu et. 
al. 2006), and (Shen et. al. 2008). These models 
demonstrate better handling of sentence 
structures, while the computation is more 
expensive compared with the distortion-based 
models.  
    In the middle of the spectrum, (Xia and 
McCord 2004), (Collins et.  al 2005),  (Wang  et. 
al.  2007), and (Visweswariah et. al. 2010)  
combined the benefits of the above two   
strategies: their approaches  reorder an input  
sentence  based  on  a set  of  reordering  rules  
defined  over  the  source sentence?s  syntax  
parse  tree.  As  a  result,  the  re-ordered  source  
sentence  resembles  the  word  order of  its  
target  translation.  The  reordering  rules  are 
either hand-crafted or automatically learned from 
the training  data  (source  parse  trees  and  
bitext  word alignments). These rules can be 
unlexicalized (only including the constituent    
labels) or fully lexicalized (including both the 
constituent labels and their head words). The  
unlexicalized  reordering  rules are more  general  
and  can be applied  broadly, but sometimes  they  
are  not  discriminative  enough.  In the following 
English-Chinese reordering rules, 
0.44  NP PP ? 0 1 
0.56  NP PP ? 1 0 
the NP and PP nodes are reordered with close to 
random probabilities. When the constituents are 
attached with their headwords, the reordering 
probability is much higher than that of the 
unlexicalized rules.  
0.20 NP:testimony PP:by --> 0 1  
0.80  NP:testimony PP:by --> 1 0  
   Unfortunately, the application of lexicalized 
reordering rules is constrained by data 
sparseness: it is unlikely to train the NP:<noun> 
387
PP:<prep> reordering rules for every noun-
preposition combination. Even  for the  learnt  
lexicalized  rules,  their  counts  are  also 
relatively  small,  thus  the  reordering  
probabilities may  not  be  estimated  reliably,  
which  could lead   to incorrect reordering 
decisions. 
   To alleviate this problem, we generalize fully 
lexicalized rules into partially lexicalized rules, 
which are further generalized into unlexicalized 
rules. Such generalization allows partial match 
when the fully lexicalized rules can not be found, 
thus achieving broader rule coverage.  
   Given a node of a source parse tree, we find all 
the matching rules and consider all their possible 
reorder permutations. Each permutation has a 
reordering score, which is the weighted sum of 
reordering probabilities of all the matching rules. 
We reorder the child nodes based on the 
permutation with the highest reordering score. 
Finally we translate the reordered sentence in a 
phrase-based SMT system. Our experiments in 
English to Chinese (EnZh) and English to 
Japanese (EnJa) translation demonstrate the 
effectiveness of the proposed approach: we 
observe consistent improvements across multiple 
test sets in multiple language pairs and 
significant gain in human judgment of the MT 
quality. 
   This paper is organized as follows: in section 2 
we briefly introduce the syntax-based reordering 
technique. In section 3, we describe our 
approach. In section 4, we show the experiment 
results, which is followed by conclusion in 
section 5.  
2 Baseline Syntax-based Reordering 
In the general syntax-based reordering, 
reordering is achieved by permuting the children 
of any interior node in the source parse tree.  
Although there are cases where reordering is 
needed across multiple constituents, this still is a  
simple and effective technique.  
   Formally, the reordering rule is a triple {p, lhs, 
rhs}, where p is the reordering probability, lhs is 
the left hand side of the rule, i.e., the constituent 
label sequence of a parse tree node, and rhs is the 
reordering permutation derived either from hand-
crafted rules as in (Collins et.  al 2005) and 
(Wang  et. al.  2007), or from training data as in 
(Visweswariah et.  al.  2010). 
   The training data includes bilingual sentence 
pairs with word alignments, as well as the source 
sentences' parse trees. The children?s relative 
order of each node is decided according to their 
average alignment position in the target sentence. 
Such relative order is a permutation of the 
integer sequence [0, 1, ? N-1], where N is the 
number of children of the given parse node. The 
counts of each permutation of each parse label 
sequence will be collected from the training data 
and converted to probabilities as shown in the 
examples in Section 1. Finally, only the 
permutation with the highest probability is 
selected to reorder the matching parse node. The 
SMT system is re-trained on reordered training 
data to translate reordered input sentences. 
   Following the above approach, only the 
reordering rule [0.56 NP PP  1 0] is kept in the 
above example. In other words, all the NP PP 
phrases will be reordered, even though the 
reordering is only slightly preferred in all the 
training data.  
3 Generalized Syntactic Reordering  
As shown in the previous examples, reordering 
depends not only on the constituents? parse 
labels, but also on the headwords of the 
constituents. Such fully lexicalized rules suffer 
from data sparseness: there is either no matching 
lexicalized rule for a given parse node or the 
matching rule?s reordering probability is 
unreliable.  We address the above issues with 
rule generalization, then consider all the 
permutations from multi-level rule matching. 
3.1 Rule Generalization 
Lexicalized rules are applied only when both the 
constituent labels and headwords match. When 
only the labels match, these reordering rules are 
not used. To increase the rule coverage, we 
generalize the fully lexicalized rules into 
partially lexicalized and unlexicalized rules.  
   We notice that many lexicalized rules share 
similar reordering permutations, thus it is 
possible to merge them to form a partially 
lexicalized rule, where lexicalization only 
appears at selected constituent?s headword. 
Although it is possible to have multiple 
lexicalizations in a partially lexicalized rule 
(which will exponentially increase the total 
number of rules), we observe that most of the 
time reordering is triggered by a single 
constituent. Therefore we keep one lexicalization 
in the partially lexicalized rules. For example, the 
following lexicalized rule: 
  
VB:appeal PP-MNR:by PP-DIR:to --> 1 2 0 
 
 
388
will be converted into the following 3 partially 
lexicalized rules: 
 
VB:appeal PP-MNR PP-DIR --> 1 2 0 
VB PP-MNR:by PP-DIR    --> 1 2 0 
VB PP-MNR PP-DIR:to    --> 1 2 0 
 
 
The count of each rule will be the sum of the 
fully lexicalized rules which can derive the given 
partially lexicalized rule. In the above 
preordering rules, ?MNR? and ?DIR? are 
functional labels, indicating the semantic labels 
(?manner?, ?direction?) of the parse node. 
We could go even further, converting the 
partially lexicalized rules into unlexicalized 
rules. This is similar to the baseline syntax 
reordering model, although we will keep all their 
possible permutations and counts for rule 
matching, as shown below. 
5   VB PP-MNR PP-DIR --> 2 0 1 
22  VB PP-MNR PP-DIR --> 2 1 0 
21  VB PP-MNR PP-DIR --> 0 1 2 
41  VB PP-MNR PP-DIR --> 1 2 0 
35  VB PP-MNR PP-DIR --> 1 0 2 
   Note that to reduce the noise from paring and 
word alignment errors, we only keep the 
reordering rules that appear at least 5 times. Then 
we convert the counts into probabilities: 
 
?= )(*,
),()|(
ii
ii
ii lhsC
lhsrhsClhsrhsp  
where },,{ upfi ? represents the fully, partially 
and un-lexicalized rules, and ),( ii lhsrhsC  is the 
count of rule (lhsi  rhs) in type i rules.  
   When we convert the most specific fully 
lexicalized rules to the more general partially 
lexicalized rules and then to the most general 
unlexicalized rules, we increase the rule coverage 
while keep their discriminative power at different 
levels as much as possible. 
3.2 Multiple Permutation Multi-level Rule 
Matching 
When applying the three types of reordering 
rules to reorder a parse tree node, we find all the 
matching rules and consider all possible 
permutations. As multiple levels of rules can lead 
to the same permutation with different 
probabilities, we take the weighted sum of 
probabilities from all matching rules (with the 
same rhs). Therefore, the permutation decision is 
not based on any particular rule, but the 
combination of all the rules matching different 
levels of context. As opposed to the general 
syntax-based reordering approaches, this strategy 
achieves a desired balance between broad rule 
coverage and specific rule match: when a fully 
lexicalized rule matches, it has strong influence 
on the permutation decision given the richer 
context. If such specific rule is unavailable or has  
low probability, more general (partial and 
unlexicalized) rules will have higher weights. For 
each permutation we compute the weighted 
reordering probability, then select the 
permutation that has the highest score.  
   Formally, given a parse tree node T, let lhsf be 
the label:head_word sequence of the fully 
lexicalized rules matching T. Similarly, lhsp and 
lhsu are the sequences of the matching partially 
lexicalized and unlexicalized rules, respectively, 
and let rhs be their possible permutations. The 
top-score permutation is computed as: 
?
?
=
},,{
* )|(maxarg
upfi
iiirhs lhsrhspwrhs  
where wi?s are the weights of different kind of 
rules and pi is reordering probability of each rule. 
The weights are chosen empirically based on the 
performance on a held-out tuning set. In our 
experiments, wf=1.0, wp=0.5, and wu=0.2, where 
higher weights are assigned to more specific 
rules. 
   For each parse tree node, we identify the top 
permutation choice and reorder its children 
accordingly.      The source parse tree is traversed 
breadth-first.  
4 Experiments 
We applied the generalized syntax-based 
reordering on both English-Chinese (EnZh) and 
English-Japanese (EnJa) translations. Our 
English parser is IBM?s maximum entropy 
constituent parser (Ratnaparkhi 1999) trained on 
Penn Treebank. Experiments in (Visweswariah 
et. al. 2010) indicated that minimal difference 
was observed using Berkeley?s parser or IBM?s 
parser for reordering. 
   Our EnZh training data consists of 20 million 
sentence pairs (~250M words), half of which are 
from LDC released bilingual corpora and the 
other half are from technical domains (e.g., 
software manual). We first trained automatic 
word alignments (HMM alignments in both 
directions and a MaxEnt alignment (Ittycheriah 
and Roukos, 2005)), then parsed the English 
sentences with the IBM parser. We extracted 
different reordering rules from the word 
alignments and the English parse trees. After 
389
frequency-based pruning, we obtained 12M 
lexicalized rules, 13M partially lexicalized rules 
and 600K unlexicalized rules. Using these rules, 
we applied preordering on the English sentences 
and then built an SMT system with the reordered 
training data. Our decoder is a phrase-based 
decoder (Tillman 2006), where various features 
are combined within the log-linear framework. 
These features include source-to-target phrase 
translation score based on relative frequency, 
source-to-target and target-to-source word-to-
word translation scores, a 5-gram language 
model score, distortion model scores and word 
count. 
 
 Tech1 Tech2 MT08 
# of sentences 582 600 1859 
PBMT  33.08 31.35 36.81 
UnLex 33.37 31.38 36.39 
FullLex  34.12 31.62 37.14 
PartLex 34.13 32.58 37.60 
MPML 34.34 32.64 38.02 
Table 1: MT experiment comparison using different 
syntax-based reordering techniques on English-
Chinese test sets.  
 
   We selected one tuning set from software 
manual domain (Tech1), and used PRO tuning 
(Hopkins and May 2011) to select decoder 
feature weights. Our test sets include one from 
the online technical support domain (Tech2) and 
one from the news domain: the NIST MT08 
English-Chinese evaluation test data. The 
translation quality is measured by BLEU score 
(Papineni et. al., 2001). Table 1 shows the BLEU 
score of the baseline phrase-based system 
(PBMT) that  
uses lexicalized reordering at decoding time 
rather than preordering. Next, Table 1 shows the 
translation results with several preordered 
systems that use unlexicalized (UnLex), fully 
lexicalized (FullLex) and partially lexicalized 
(PartLex) rules, respectively. The lexicalized 
reordering model is still applicable for 
preordered systems so that some preordering 
errors can be recovered at run time. 
   First we observed that the UnLex preordering 
model on average does not improve over the 
typical phrase-based MT baseline due to its 
limited discriminative power. When the 
preordering decision is conditioned on the head 
word, the FullLex model shows some gains 
(~0.3 pt) thanks to the richer matching context, 
while the PartLex model improves further over 
the FullLex model because of its broader 
coverage. Combining all three with multi-
permutation, multi-level rule matching (MPML) 
brings the most gains, with consistent (~1.3 Bleu 
points) improvement over the baseline system on 
all the test sets. Note that the Bleu scores on the 
news domain (MT08) are higher than those on 
the tech domain. This is because the Tech1 and 
Tech2 have one reference translation while 
MT08 has 4 reference translations.  
   In addition to the automatic MT evaluation, we 
also used human judgment of quality of the MT 
translation on a set of randomly selected 125 
sentences from the baseline and improved 
reordering systems. The human judgment score 
is 2.82 for the UnLex system output, and 3.04 
for the improved MPML reordering output. The 
0.2 point improvement on the 0-5 scale is 
considered significant.  
 
 Tech1 Tech2 News 
# of sentences 1000 600 600 
PBMT 56.45 35.45 21.70 
UnLex 59.22 38.36 23.08 
FullLex 57.55 36.56 22.23 
PartLex 59.80 38.47 23.13 
MPML 59.94 38.62 23.31 
Table 2: MT experiment comparison using 
generalized syntax-based reordering techniques on 
English-Japanese test sets.  
 
   We also apply the same generalized reordering 
technique on English-Japanese (EnJa) 
translation. As there is very limited publicly 
available English-Japanese parallel data, most 
our training data (20M sentence pairs) is from 
the in-house software manual domain. We use 
the same English parser and phrase-based 
decoder as in EnZh experiment. Table 2 shows 
the translation results on technical and news 
domain test sets. All the test sets have single 
reference translation.     
   First, we observe that the improvement from 
preordering is larger than that in EnZh MT (1.6-3 
pts vs. 1 pt). This is because the word order 
difference between English and Japanese is 
larger than that between English and Chinese 
(Japanese is a SOV language while both English 
and Chinese are SVO languages). Without 
preordering, correct word orders are difficult to 
obtain given the typical skip-window beam 
search in the PBMT. Also, as in EnZh, the 
PartLex model outperforms the UnLex model, 
both of which being significantly better than the 
FullLex model due to the limited rule coverage 
in the later model: only 50% preordering rules 
390
are applied in the FullLex model. Tech1 test set 
is a very close match to the training data thus its 
BLEU score is much higher.  
5 Conclusion and Future Work 
To summarize, we made the following 
improvements: 
1. We generalized fully lexicalized 
reordering rules to partially lexicalized 
and unlexicalized rules for broader rule 
coverage and reduced data sparseness. 
2. We allowed multiple permutation, multi-
level rule matching to select the best 
reordering path. 
  Experiment results show consistent and 
significant improvements on multiple English-
Chinese and English-Japanese test sets judged by 
both automatic and human judgments. 
   In future work we would like to explore new 
methods to prune the phrase table without 
degrading MT performance and to make rule 
extraction and reordering more robust to parsing 
errors. 
Acknowledgement 
The authors appreciate helpful comments from 
anonymous reviewers as well as fruitful 
discussions with Karthik Visweswariah and 
Salim Roukos.  
References  
Yaser Al-Onaizan , Kishore Papineni, Distortion 
models for statistical machine translation, 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th annual 
meeting of the Association for Computational 
Linguistics, p.529-536, July 17-18, 2006, Sydney, 
Australia   
David Chiang, A hierarchical phrase-based model for 
statistical machine translation, Proceedings of the 
43rd Annual Meeting on Association for 
Computational Linguistics, p.263-270, June 25-30, 
2005, Ann Arbor, Michigan   
Michael Collins , Philipp Koehn , Ivona Kucerov, 
Clause restructuring for statistical machine 
translation, Proceedings of the 43rd Annual 
Meeting on Association for Computational 
Linguistics, p.531-540, June 25-30, 2005, Ann 
Arbor, Michigan 
Mark Hopkins, Jonathan May, Tuning as ranking, In 
Proceedings of the Conference on Empirical 
Methods in Natural Language Processing 2011, pp. 
1352-1362.  Association for Computational 
Linguistics.   
Abraham Ittycheriah , Salim Roukos, A maximum 
entropy word aligner for Arabic-English machine 
translation, Proceedings of the conference on 
Human Language Technology and Empirical 
Methods in Natural Language Processing, p.89-96, 
October 06-08, 2005, Vancouver, British 
Columbia, Canada   
Philipp Koehn , Franz Josef Och , Daniel Marcu, 
Statistical phrase-based translation, Proceedings of 
the 2003 Conference of the North American 
Chapter of the Association for Computational 
Linguistics on Human Language Technology, p.48-
54, May 27-June 01, 2003, Edmonton, Canada   
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. In Proceedings of COLING/ACL 
2006, pages 609-616, Sydney, Australia, July. 
Libin Shen, Jinxi Xu and Ralph Weischedel 2008. A 
New String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. in Proceedings of the 46th Annual Meeting 
of the Association for Computational Linguistics 
(ACL). Columbus, OH, USA, June 15 - 20, 2008. 
Christoph Tillmann, A unigram orientation model for 
statistical machine translation, Proceedings of 
HLT-NAACL 2004: Short Papers, p.101-104, May 
02-07, 2004, Boston, Massachusetts 
Christoph Tillmann. 2006. Efficient Dynamic 
Programming Search Algorithms for Phrase-based 
SMT. In Proc. of the Workshop CHPSLP at 
HLT'06. 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese syntactic reordering for statistical 
machine translation. In Proceedings of EMNLP-
CoNLL. 
Karthik Visweswariah , Jiri Navratil , Jeffrey 
Sorensen , Vijil Chenthamarakshan , Nanda 
Kambhatla, Syntax based reordering with 
automatically derived rules for improved statistical 
machine translation, Proceedings of the 23rd 
International Conference on Computational 
Linguistics, p.1119-1127, August 23-27, 2010, 
Beijing, China  
Adwait Ratnaparkhi. 1999. Learning to parse natural 
language with maximum entropy models. Machine 
Learning, 34(1-3).  
Fei Xia , Michael McCord, Improving a statistical MT 
system with automatically learned rewrite patterns, 
Proceedings of the 20th international conference on 
Computational Linguistics, p.508-es, August 23-
27, 2004, Geneva, Switzerland   
391
Kenji Yamada , Kevin Knight, A syntax-based 
statistical translation model, Proceedings of the 
39th Annual Meeting on Association for 
Computational Linguistics, p.523-530, July 06-11, 
2001, Toulouse, France   
Andreas Zollmann , Ashish Venugopal, Syntax 
augmented machine translation via chart parsing, 
Proceedings of the Workshop on Statistical 
Machine Translation, June 08-09, 2006, New York 
City, New YorkAlfred. V. Aho and Jeffrey D. 
Ullman. 1972. The Theory of Parsing, Translation 
and Compiling, volume 1. Prentice-Hall, 
Englewood Cliffs, NJ.  
392
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861?870,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Adaptive HTER Estimation for Document-Specific MT Post-Editing
Fei Huang
?
Facebook Inc.
Menlo Park, CA
feihuang@fb.com
Jian-Ming Xu Abraham Ittycheriah
IBM T.J. Watson Research Center
Yorktown Heights, NY
{jianxu, abei, roukos}@us.ibm.com
Salim Roukos
Abstract
We present an adaptive translation qual-
ity estimation (QE) method to predict
the human-targeted translation error rate
(HTER) for a document-specific machine
translation model. We first introduce fea-
tures derived internal to the translation de-
coding process as well as externally from
the source sentence analysis. We show
the effectiveness of such features in both
classification and regression of MT qual-
ity. By dynamically training the QE model
for the document-specific MT model, we
are able to achieve consistency and pre-
diction quality across multiple documents,
demonstrated by the higher correlation co-
efficient and F-scores in finding Good sen-
tences. Additionally, the proposed method
is applied to IBM English-to-Japanese MT
post editing field study and we observe
strong correlation with human preference,
with a 10% increase in human translators?
productivity.
1 Introduction
Machine translation (MT) systems suffer from an
inconsistent and unstable translation quality. De-
pending on the difficulty of the input sentences
(sentence length, OOV words, complex sentence
structures and the coverage of the MT system?s
training data), some translation outputs can be per-
fect, while others are ungrammatical, missing im-
portant words or even totally garbled. As a result,
users do not know whether they can trust the trans-
lation output unless they spend time to analyze
?
This work was done when the author was with IBM Re-
search.
the MT output. This shortcoming is one of the
main obstacles for the adoption of MT systems,
especially in machine assisted human translation:
MT post-editing, where human translators have
an option to edit MT proposals or translate from
scratch. It has been observed that human trans-
lators often discard MT proposals even if some
are very accurate. If MT proposals are used prop-
erly, post-editing can increase translators produc-
tivity and lead to significant cost savings. There-
fore, it is beneficial to provide MT confidence es-
timation, to help the translators to decide whether
to accept MT proposals, making minor modifica-
tions on MT proposals when the quality is high
or translating from scratching when the quality is
low. This will save the time of reading and parsing
low quality MT and improve user experience.
In this paper we propose an adaptive qual-
ity estimation that predicts sentence-level human-
targeted translation error rate (HTER) (Snover et
al., 2006) for a document-specific MT post-editing
system. HTER is an ideal quality measurement
for MT post editing since the reference is ob-
tained from human correction of the MT output.
Document-specific MT model is an MT model that
is specifically built for the given input document.
It is demonstrated in (Roukos et al, 2012) that
document-specific MT models significantly im-
prove the translation quality. However, this raises
two issues for quality estimation. First, existing
approaches to MT quality estimation rely on lex-
ical and syntactical features defined over parallel
sentence pairs, which includes source sentences,
MT outputs and references, and translation models
(Blatz et al, 2004; Ueffing and Ney, 2007; Spe-
cia et al, 2009a; Xiong et al, 2010; Soricut and
Echihabi, 2010a; Bach et al, 2011). Therefore,
when the MT quality estimation model is trained,
861
it can not be adapted to provide accurate estimates
on the outputs of document-specific MT models.
Second, the MT quality estimation might be in-
consistent across different document-specific MT
models, thus the confidence score is unreliable and
not very helpful to users.
In contrast to traditional static MT quality es-
timation methods, our approach not only trains
the MT quality estimator dynamically for each
document-specific MT model to obtain higher pre-
diction accuracy, but also achieves consistency
over different document-specific MT models. The
experiments show that our MT quality estima-
tion is highly correlated with human judgment
and helps translators to increase the MT proposal
adoption rate in post-editing.
We will review related work on MT quality es-
timation in section 2. In section 3 we will intro-
duce the document-specific MT system built for
post-editing. We describe the static quality estima-
tion method in section 4, and propose the adaptive
quality estimation method in section 5. In section
6 we demonstrate the improvement of MT quality
estimation with our method, followed by discus-
sion and conclusion in section 7.
2 Related Work
There has been a long history of study in con-
fidence estimation of machine translation. The
work of (Blatz et al, 2004) is among the best
known study of sentence and word level features
for translation error prediction. Along this line of
research, improvements can be obtained by incor-
porating more features as shown in (Quirk, 2004;
Sanchis et al, 2007; Raybaud et al, 2009; Specia
et al, 2009b). Soricut and Echihabi (2010b) pro-
posed various regression models to predict the ex-
pected BLEU score of a given sentence translation
hypothesis. Ueffing and Hey (2007) introduced
word posterior probabilities (WPP) features and
applied them in the n-best list reranking. Target
part-of-speech and null dependency link are ex-
ploited in a MaxEnt classifier to improve the MT
quality estimation (Xiong et al, 2010).
Quality estimation focusing on MT post-editing
has been an active research topic, especially after
the WMT 2012 (Callison-Burch et al, 2012) and
WMT2013 (Bojar et al, 2013) workshops with
the ?Quality Estimation? shared task. Bic?ici et
al. (2013) proposes a number of features mea-
suring the similarity of the source sentence to the
source side of the MT training corpus, which,
combined with features from translation output,
achieved significantly superior performance in the
MT QE evaluation. Felice and Specia (2012) in-
vestigates the impact of a large set of linguisti-
cally inspired features on quality estimation accu-
racy, which are not able to outperform the shal-
lower features based on word statistics. Gonz?alez-
Rubio et al (2013) proposed a principled method
for performing regression for quality estimation
using dimensionality reduction techniques based
on partial least squares regression. Given the fea-
ture redundancy in MT QE, their approach is able
to improve prediction accuracy while significantly
reducing the size of the feature sets.
3 Document-specific MT System
In our MT post-editing setup, we are given docu-
ments in the domain of software manuals, techni-
cal outlook or customer support materials. Each
translation request comes as a document with sev-
eral thousand sentences, focusing on a specific
topic, such as the user manual of some software.
The input documents are automatically seg-
mented into sentences, which are also called seg-
ments. Thus in the rest of the paper we will use
sentences and segments interchangeably. Our par-
allel corpora includes tens of millions of sentence
pairs covering a wide range of topics. Building
a general MT system using all the parallel data
not only produces a huge translation model (unless
with very aggressive pruning), the performance on
the given input document is suboptimal due to the
unwanted dominance of out-of-domain data. Past
research suggests using weighted sentences or cor-
pora for domain adaptation (Lu et al, 2007; Mat-
soukas et al, 2009; Foster et al, 2010). Here
we adopt the same strategy, building a document-
specific translation model for each input docu-
ment.
The document-specific system is built based on
sub-sampling: from the parallel corpora we se-
lect sentence pairs that are the most similar to
the sentences from the input document, then build
the MT system with the sub-sampled sentence
pairs. The similarity is defined as the number of
n-grams that appear in both source sentences, di-
vided by the input sentence?s length, with higher
weights assigned to longer n-grams. From the
extracted sentence pairs, we utilize the standard
pipeline in SMT system building: word align-
862
Figure 1: Adaptive QE for document-specific MT system.
ment (HMM (Vogel et al, 1996) and MaxEnt (It-
tycheriah and Roukos, 2005) alignment models,
phrase pair extraction, MT model training (Itty-
cheriah and Roukos, 2007) and LM model train-
ing. The top region within the dashed line in Fig-
ure 1 shows the overall system built pipeline.
3.1 MT Decoder
The MT decoder (Ittycheriah and Roukos, 2007)
employed in our study extracts various features
(source words, morphemes and POS tags, target
words and POS tags, etc.) with their weights
trained in a maximum entropy framework. These
features are combined with other features used in
a typical phrase-based translation system. Alto-
gether the decoder incorporates 17 features with
weights estimated by PRO (Hopkins and May,
2011) in the decoding process, and achieves
state-of-the-art translation performance in vari-
ous Arabic-English translation evaluations (NIST
MT2008, GALE and BOLT projects).
4 Static MT Quality Estimation
MT quality estimation is typically formulated as
a prediction problem: estimating the confidence
score or translation error rate of the translated sen-
tences or documents based on a set of features. In
this work, we adopt HTER in (Snover et al, 2006)
as our prediction output. HTER measures the per-
centage of insertions, deletions, substitutions and
shifts needed to correct the MT outputs. In the
rest of the paper, we use TER and HTER inter-
changably.
In this section we will first introduce the set of
features, and then discuss MT QE problem from
classification and regression point of views.
4.1 Features for MT QE
The features for quality estimation should reflect
the complexity of the source sentence and the de-
coding process. Therefore we conduct syntactic
analysis on the source sentences, extract features
from the decoding process and select the follow-
ing 26 features:
? 17 decoding features, including phrase
translation probabilities (source-to-target and
target-to-source), word translation probabil-
ities (also in both directions), maxent prob-
abilities
1
, word count, phrase count, distor-
1
The maxent probability is the translation probability
863
tion probabilities, as well as a set of language
model scores.
? Sentence length, i.e., the number of words in
the source sentence.
? Source sentence syntactic features, including
the number of noun phrases, verb phrases,
adjective phrases, adverb phrases, as in-
spired by (Green et al, 2013).
? The length of verb phrases, because verbs are
typically the roots in dependency structure
and they have more varieties during transla-
tion.
? The maximum length of source phrases in
the final translation, since longer matching
source phrase indicates better coverage of the
input sentence with possibly better transla-
tions.
? The number of phrase pairs with high fuzzy
match (FM) score. The high FM phrases are
selected from sentence pairs which are clos-
est in terms of n-gram overlap to the input
sentence. These sentences are often found in
previous translations of the software manual,
and thus are very helpful for translating the
current sentence.
? The average translation probability of the
phrase translation pairs in the final transla-
tion, which provides the overall translation
quality on the phrase level.
The first 17 features come from the decod-
ing process, which are called ?decoding features?.
The remaining 9 features not related to the de-
coder are called ?external features?. To evaluate
the effectiveness of the proposed features, we train
various classifiers with different feature configura-
tions to predict whether a translation output is use-
ful (with lower TER) as described in the following
section.
4.2 MT QE as Classification
Predicting TER with various input features can
be treated as a regression problem. However for
the post-editing task, we argue that it could also
be cast as a classification problem: MT system
derived from a Maximum Entropy translation model (Itty-
cheriah and Roukos, 2005).
Configuration Training set Test set
Baseline (All negative) 80% 77%
17 decoding features only 89% 79%
9 external features only 85% 81%
total 26 features 92% 83%
Table 1: QE classification accuracy with different
feature configurations
users (including the translators) are often inter-
ested to know whether a given translation is rea-
sonably good or not. If useful, they can quickly
look through the translation and make minor mod-
ifications. On the other hand, they will just skip
reading and parsing the bad translation, and prefer
to translate by themselves from scratch. Therefore
we also develop algorithms that classify the trans-
lation at different levels, depending on whether the
TER is less than a given threshold. In our experi-
ments, we set TER=0.1 as the threshold.
We randomly select one input document with
2067 sentences for the experiment. We build
a document-specific MT system to translate this
document, then ask human translator to correct
the translation output. We compute TER for each
sentence using the human correction as the refer-
ence. The TER of the whole document is 0.31,
which means about 30% errors should be cor-
rected. In the classification task, our goal is to pre-
dict whether a sentence is a Good translation (with
TER ? 0.1), and label them for human correction.
We adopt a decision tree-based classifier, experi-
menting with different feature configurations. We
select the top 1867 sentences for training and the
bottom 200 sentences for test. In the test set, there
are 46 sentences with TER ? 0.1. Table 1 shows
the classification accuracy.
First we can see that as the overall TER is
around 0.3, predicting all the sentences being neg-
ative already has a strong baseline: 77%. How-
ever this is not helpful for the human translators,
because that means they have to translate every
sentence from scratch, and consequently there is
no productivity gain from MT post-editing. If we
only use the 17 decoding features, it improves the
classification accuracy by 9% on the training set,
but only 2% on the test set. This is probably due to
the overfitting when training the decision tree clas-
sifier. While using the 7 external features, the gain
on training set is less but the gain on the test set
864
is greater (4% improvement), because the trans-
lation output is generated based on the log-linear
combination of these decoding features, which are
biased towards the final translations. The exter-
nal features capture the syntactic structure of the
source sentence, as well as the coverage of the
training data with regard to the input sentence,
which are good indicators of the translation qual-
ity. Combining both the decoding features and the
external features, we observed the best accuracy
on both the training and test set. We will use the
combined 26 features in the following work.
4.3 MT QE as Regression
For the QE regression task, we predict the TER for
each sentence translation using the above 26 fea-
tures. We experiment with several classifiers: lin-
ear regression model, decision tree based regres-
sion model and SVM model. With the same train-
ing and test data set up, we predict the TER for
each sentence in the test set, and compute the cor-
relation coefficient (r) and root mean square error
(RMSE). Our experiments show that the decision
tree-based regression model obtains the highest
correlation coefficients (0.53) and lowest RMSE
(0.23) in both the training and test sets. We will
use this model for the adaptive MT QE in the fol-
lowing work.
5 Adaptive MT Quality Estimation
The above QE regression model is trained on a
portion of the sentences from the input document,
and evaluated on the remaining sentences from the
same document. One would like to know whether
the trained model can achieve consistent TER pre-
diction accuracy on other documents. When we
use the cross-document models for prediction, the
correlation is significantly worse (the details are
discussed in section 6.1). Therefore it is neces-
sary to build a QE regression model that?s robust
to different document-specific translation models.
To deal with this problem, we propose this adap-
tive MT QE method described below.
Our proposed method is as follows: we select a
fixed set of sentence pairs (S
q
, R
q
) to train the QE
model. The source side of the QE training data
S
q
is combined with the input document S
d
for
MT system training data subsampling. Once the
document-specific MT system is trained, we use it
to translate both the input document and the source
QE training data, obtaining the translation T
d
and
Figure 2: Correlation coefficient r between pre-
dicted TER (x-axis) and true TER (y-axis) for QE
models trained from the same document (top fig-
ure) or different document (bottom figure).
T
q
. We compute the TER of T
q
using R
q
as the
reference, and train a QE regression model with
the 26 features proposed in section 4.1. Then we
use this document-specific QE model to predict the
TER of the document translation T
d
. As the QE
model is adaptively re-trained for each document-
specific MT system, its prediction is more accurate
and consistent. Figure 1 shows the flow of our MT
system with the adaptive QE training integrated as
part of the built.
6 Experiments
In this section, we first discuss experiments that
compare adaptive QE method and static QE
method on a few documents, and then present
results we obtained after deploying the adaptive
QE method in an English-to-Japanese MT Post-
Editing project. As mentioned before, the main
motivation for us to develop MT QE classification
scheme is that translators often discard good MT
proposals and translate the segments from scratch.
We would like to provide translators with some
guidance on reasonably good MT proposals?the
sentences with low TERs?to help them increase
the leverage on MT proposals to achieve improved
productivity.
865
6.1 Evaluation on Test Set
Our experiment and evaluation is conducted over
three documents, each with about 2000 segments.
We first build document-specific MT model for
each document, then ask human translators to cor-
rect the MT outputs and obtain the reference trans-
lation. In a typical MT QE scenario, the QE model
is pre-trained and applied to various MT outputs,
even though the QE training data and MT out-
puts are generated from different translation mod-
els. To evaluate whether such model mismatch
matters, we compare the cross-model QE with the
same-model QE, where the QE training data and
the MT outputs are generated from the same MT
model.
We select one document LZA with 2067 sen-
tences. We use the first 1867 sentences to train the
static QE model and the remaining 200 sentences
are used as test set for TER prediction. We com-
pute the correlation coefficient (r) between each
predicted TER and true TER, as shown in Figure
2. We find that the TER predictions are reason-
ably correct when the training and test sentences
are from the same MT model (the top figure), with
correlation coefficients around 0.5. For the cross-
model QE, we train a static QE model with 1867
sentences from another document RTW, and use it
to predict the TER of the same 200 sentences from
document LZA (the bottom figure). We observe
significant degradation of correlation coefficient,
dropping from 0.5 to 0.1. This degradation and
unstable nature is the prime motivation to develop
a more robust MT quality estimation model.
We select 1700 sentences from multiple pre-
viously translated documents as the QE training
data, which are independent of the test documents.
We train the static QE model with this training set,
including the source sentences, references and MT
outputs (from multiple translation models). To
train the adaptive QE model for each test docu-
ment, we build a translation model whose subsam-
pling data includes source sentences from both the
test document and the QE training data. We trans-
late the QE source sentences with this newly built
MT model, and the translation output is used to
train the QE model specific to each test document.
We compare these two QE models on three doc-
uments, LZA, RTW and WC7, measuring r and
RMSE for each QE model. The result is shown
in Table 2. We find that the adaptive QE model
demonstrates higher r and lower RMSE than the
static QE model for all the test documents.
Besides the general correlation with human
judgment, we particularly focus on those reason-
ably good translations, i.e., the sentences with low
TERs which can help improve the translator?s pro-
ductivity most. Here we report the precision, re-
call and F-score of finding such ?Good? sentences
(with TER ? 0.1) on the three documents in Ta-
ble 3. Again, the adaptive QE model produces
higher recall, mostly higher precision, and signif-
icantly improved F-score. The overall F-score of
the adaptive QE model is 0.28
2
. Compared with
the static QE model?s 0.17 F-score, this is rela-
tively 64% improvement.
In the adaptive QE model, the source side QE
training data is included in the subsampling pro-
cess to build the document-specific MT model. It
would be interesting to know whether this process
will negatively affect the MT quality. We evaluate
the TER of MT outputs with and without the adap-
tive QE training on the same three documents. As
seen in Table 4, we do not notice translation qual-
ity degradation. Instead, we observe slightly im-
provement on two document, with TERs reduction
by 0.1-0.4 pt. As our MT model training data in-
clude proprietary data, the MT performance is sig-
nificantly better than publicly available MT soft-
ware.
6.2 Impact on Human Translators
We apply the proposed adaptive QE model to
large scale English-to-Japanese MT Post-Editing
project on 36 documents with 562K words. Each
English sentence can be categorized into 3 classes:
? Exact Match (EM): the source sentence is
completely covered in the bilingual training
corpora thus the corresponding target sen-
tence is returned as the translation;
? Fuzzy Match (FM): the source sentence is
similar to some sentence in the training data
(similarity measured by string editing dis-
tance), the corresponding fuzzy match target
sentence (FM proposal) as well as the MT
translation output (MT proposal) are returned
for human translators to select and correct;
? No Proposal (NP): there is no close match
source sentences in the training data (the FM
2
The adaptive QE model obtains much higher F-score
(80%) on the rest of the sentences (with TER > 0.1).
866
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
r ? RMSE ? r ? RMSE ? r ? RMSE ?
Static QE 0.10 0.38 0.40 0.32 0.13 0.36
Adaptive QE 0.58 0.23 0.61 0.22 0.47 0.20
Table 2: QE regression with static and adaptive models
Document LZA RTW WC7
Num. of Sents 2067 2003 2405
P/R/F-score P/R/F-score P/R/F-score
Static QE 0.73/0.08/0.14 0.69/ 0.11/ 0.19 0.74/ 0.10/ 0.18
Adaptive QE 0.69/0.14/0.24 0.84/ 0.16/ 0.26 0.80/ 0.23/ 0.35
Table 3: Performance on predicting Good sentences with static and adaptive models
similarity score of 70% is used as the thresh-
old), therefore only the MT output is re-
turned.
EM sentences are excluded from the study be-
cause in general they do not require editing. We
focus on the FM and NP sentences
3
. In Table 5
we present the precision, recall and F-score of the
?Good? sentences in the FM and NP categories,
similar to those shown in Table 3. We consistently
observe higher performance on the FM sentences,
in terms of precision, recall and F-score. This is
expected because these sentences are well covered
in the training data. The overall F-score is in line
with the test set results shown in Table 3.
We are also interested to know whether the pro-
posed adaptive QE method is helpful to human
translators in the MT post-editing task. Based on
the TERs predicted by the adaptive QE model, we
assign each MT proposal with a confidence label:
High (0 ? TER ? 0.2), Medium (0.2 < TER ?
0.3), or Low (TER > 0.3). We present the MT pro-
posals with confidence labels to human translators,
then measure the percentage of sentences whose
MT proposals are used. From Table 6 and 7,
we can see that sentences with High and Medium
confidence labels are more frequently used by the
translators than those with Low labels, for both the
FM and NP categories. The MT usage for the FM
category is less than that for the NP category be-
cause translators can choose FM proposals instead
of the MT proposals for correction.
We also measure the translator?s productivity
gain for MT proposals with different confidence
3
The word count distribution of EM, FM and NP is 21%,
38% and 41%, respectively.
Document LZA RTW WC7
TER-Baseline 30.81 30.74 29.96
TER-with Adaptive QE 30.69 30.78 29.56
Table 4: MT Quality with and without Adaptive
QE measured by TER
labels. The productivity of a translator is defined
as the number of source words translated per unit
time. The post editing tool, IBM TranslationMan-
ager, records the time that a translator spends on
a segment and computes the number of characters
that a translator types on the segment so that we
can compute how many words the translator has
finished in a given time.
We choose the overall productivity of NP0 as
the base unit 1, where there is no proposal presents
and the translator has to translate the segments
from scratch. Measured with this unit, for exam-
ple, the overall productivity of FM0 being 1.14
implies a relative gain of 14% over that of NP0,
which demonstrates the effectiveness of FM pro-
posals.
Table 6 and 7 also show the productivity gain
on sentences with High, Medium and Low labels
from FM and NP categories. Again, the produc-
tivity gain is consistent with the confidence labels
from the adaptive QE model?s prediction. The
overall productivity gain with confidence-labeled
MT proposals is about 10% (comparing FM1 vs.
FM0 and NP1 vs. NP0). These results clearly
demonstrate the effectiveness of the adaptive QE
model in aiding the translators to make use of MT
proposals and improve productivity.
867
Category Class FM usage MT usage Productivity
High 33% 34% 1.35
FM1 Medium 47% 18% 1.21
Low 60% 8% 1.20
Overall 45% 21% 1.26
High 53% - 1.12
FM0 Medium 64% - 1.14
Low 67% - 1.16
Overall 59% - 1.14
Table 6: MT proposal usage and productivity gain in FM category.
In FM1, both Fuzzy Match and MT proposals present. In control class FM0, only Fuzzy Match proposals
present, and therefore, MT usage is not available for FM0. Strong correlation is observed between
predicted ?High? , ?Medium? and ?Low? sentences with MT usage and post editing productivity.
Category Class MT usage Productivity
High 50% 1.25
NP1 Medium 42% 1.08
Low 27% 1.00
Overall 38% 1.09
High - 1.08
NP0 Medium - 1.00
Low - 0.96
Overall - 1.00
Table 7: MT proposal usage and productivity gain in NP category.
In NP1, MT is the only proposal available, while in control NP0, there presents no proposal at all and
the translator has to translate from scratch. Strong correlation is observed between predicted ?High? ,
?Medium? and ?Low? sentences with MT usage and post editing productivity
868
Type Precision Recall F-score
FM 0.71 0.23 0.35
NP 0.67 0.18 0.29
Overall 0.69 0.21 0.32
Table 5: Performance on predicting Good sen-
tences (TER ? 0.1) by adaptive QE model
7 Discussion and Conclusion
In this paper we proposed a method to adaptively
train a quality estimation model for document-
specific MT post editing. With the 26 pro-
posed features derived from decoding process and
source sentence syntactic analysis, the proposed
QE model achieved better TER prediction, higher
correlation with human correction of MT output
and higher F-score in finding good translations.
The proposed adaptive QE model is deployed to
a large scale English-to-Japanese MT post edit-
ing project, showing strong correlation with hu-
man preference and leading to about 10% gain in
human translator productivity.
The training data for QE model can be selected
independent of the input document. With such
fixed QE training data, it is possible to measure the
consistency of the trained QE models, and to al-
low the sanity check of the document-specific MT
models. However, adding such data in the sub-
sampling process extracts more bilingual data for
building the MT models, which slightly increase
the model building time but increased the transla-
tion quality. Another option is to select the sen-
tence pairs from the MT system subsampled train-
ing data, which is more similar to the input docu-
ment thus the trained QE model could be a better
match to the input document. However, the QE
model training data is no longer constant. The
model consistency is no longer guaranteed, and
the QE training data must be removed from the
MT system training data to avoid data contamina-
tion.
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A method for measuring machine
translation confidence. In ACL, pages 211?219.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the 20th international conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut,
and Lucia Specia. 2013. Findings of the 2013
Workshop on Statistical Machine Translation. In
Eighth Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1?44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada.
Mariano Felice and Lucia Specia. 2012. Linguistic
features for quality estimation. In Seventh Workshop
on Statistical Machine Translation, pages 96?103,
Montr?eal, Canada.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 451?459, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jes?us Gonz?alez-Rubio, Jose Ram?on Navarro-Cerd?an,
and Francisco Casacuberta. 2013. Dimensionality
reduction methods for machine translation quality
estimation. Machine Translation, 27(3-4):281?301.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ?13, pages 439?448, New York, NY,
USA. ACM.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In In Proceedings of HLT-
EMNLP, pages 89?96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In In HLT-NAACL 2007: Main
Conference, pages 57?64.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by
869
training data selection and optimization. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 343?350, Prague, Czech Republic,
June. Association for Computational Linguistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 708?717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christopher B. Quirk. 2004. Training a sentence-level
machine translation confidence measure. In In Pro-
ceedings of LREC.
Sylvain Raybaud, Caroline Lavecchia, David Langlois,
and Kamel Sma??li. 2009. New confidence mea-
sures for statistical machine translation. CoRR,
abs/0902.1033.
Salim Roukos, Abraham Ittycheriah, and Jian-Ming
Xu. 2012. Document-specific statistical machine
translation for improving human translation produc-
tivity. In Proceedings of the 13th international con-
ference on Computational Linguistics and Intelli-
gent Text Processing - Volume Part II, CICLing?12,
pages 25?39, Berlin, Heidelberg. Springer-Verlag.
Alberto Sanchis, Alfons Juan, Enrique Vidal, and De-
partament De Sistemes Informtics. 2007. Estima-
tion of confidence measures for machine translation.
In In Procedings of Machine Translation Summit XI.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Radu Soricut and Abdessamad Echihabi. 2010a.
Trustrank: inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?10, pages 612?621, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Soricut and Abdessamad Echihabi. 2010b.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621. Association for Computa-
tional Linguistics.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-taylor. 2009a. Improving
the confidence of machine translation quality esti-
mates. In In Proceedings of MT Summit XII.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009b. Improv-
ing the confidence of machine translation quality es-
timates.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33(1):9?40.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2, COLING
?96, pages 836?841, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 604?611, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
870
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 23?30,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Exploring Representation-Learning Approaches to Domain Adaptation
Fei Huang and Alexander Yates
Temple University
Computer and Information Sciences
324 Wachman Hall
Philadelphia, PA 19122
{fei.huang,yates}@temple.edu
Abstract
Most supervised language processing sys-
tems show a significant drop-off in per-
formance when they are tested on text
that comes from a domain significantly
different from the domain of the training
data. Sequence labeling systems like part-
of-speech taggers are typically trained on
newswire text, and in tests their error
rate on, for example, biomedical data can
triple, or worse. We investigate techniques
for building open-domain sequence label-
ing systems that approach the ideal of a
system whose accuracy is high and con-
stant across domains. In particular, we in-
vestigate unsupervised techniques for rep-
resentation learning that provide new fea-
tures which are stable across domains, in
that they are predictive in both the train-
ing and out-of-domain test data. In exper-
iments, our novel techniques reduce error
by as much as 29% relative to the previous
state of the art on out-of-domain text.
1 Introduction
Supervised natural language processing (NLP)
systems exhibit a significant drop-off in perfor-
mance when tested on domains that differ from
their training domains. Past research in a vari-
ety of NLP tasks, like parsing (Gildea, 2001) and
chunking (Huang and Yates, 2009), has shown that
systems suffer from a drop-off in performance on
out-of-domain tests. Two separate experiments
with part-of-speech (POS) taggers trained on Wall
Street Journal (WSJ) text show that they can reach
accuracies of 97-98% on WSJ test sets, but achieve
accuracies of at most 90% on biomedical text
(R.Codena et al, 2005; Blitzer et al, 2006).
The major cause for poor performance on out-
of-domain texts is the traditional representation
used by supervised NLP systems. Most systems
depend to varying degrees on lexical features,
which tie predictions to the words observed in
each example. While such features have been used
in a variety of tasks for better in-domain perfor-
mance, they are pitfalls for out-of-domain tests for
two reasons: first, the vocabulary can differ greatly
between domains, so that important words in the
test data may never be seen in the training data.
And second, the connection between words and
labels may also change across domains. For in-
stance, ?signaling? appears only as a present par-
ticiple (VBG) in WSJ text (as in, ?signaling that
...?), but predominantly as a noun (as in ?signaling
pathway?) in biomedical text.
Representation learning is a promising new ap-
proach to discovering useful features that are sta-
ble across domains. Blitzer et al (2006) and our
previous work (2009) demonstrate novel, unsu-
pervised representation learning techniques that
produce new features for domain adaptation of a
POS tagger. This framework is attractive for sev-
eral reasons: experimentally, learned features can
yield significant improvements over standard su-
pervised models on out-of-domain tests. Since
the representation learning techniques are unsu-
pervised, they can be applied to arbitrary new do-
mains to yield the best set of features for learning
on WSJ text and predicting on the new domain.
There is no need to supply additional labeled ex-
amples for each new domain. This reduces the ef-
fort for domain adaptation, and makes it possible
to apply systems to open-domain text collections
like the Web, where it is prohibitively expensive
to collect a labeled sample that is truly representa-
tive of all domains.
Here we explore two novel directions in the
representation-learning framework for domain
adaptation. Specifically, we investigate empiri-
cally the effects of representation learning tech-
niques on POS tagging to answer the following:
23
1. Can we produce multi-dimensional represen-
tations for domain adaptation? Our previous ef-
forts have provided only a single new feature in
the learned representations. We now show how
we can perform a multi-dimensional clustering
of words such that each dimension of the clus-
tering forms a new feature in our representation;
such multi-dimensional representations dramati-
cally reduce the out-of-domain error rate of our
POS tagger from 9.5% to 6.7%.
2. Can maximum-entropy models be used to pro-
duce representations for domain adaptation? Re-
cent work on contrastive estimation (Smith and
Eisner, 2005) has shown that maximum-entropy-
based latent variable models can yield more accu-
rate clusterings for POS tagging than more tradi-
tional generative models trained with Expectation-
Maximization. Our preliminary results show that
such models can be used effectively as represen-
tations for domain adaptation as well, matching
state-of-the-art results while using far less data.
The next section provides background informa-
tion on learning representations for NLP tasks us-
ing latent-variable language models. Section 3 de-
scribes our experimental setup. In Sections 4 and
5, we empirically investigate our two questions
with a series of representation-learning methods.
Section 6 analyzes our best learned representation
to help explain its effectiveness. Section 7 presents
previous work, and Section 8 concludes and out-
lines directions for future work.
2 Open-Domain Sequence Labeling by
Learning Representations
Let X be an instance set for a learning problem;
for POS tagging, for instance, this could be the set
of all English sentences. Let Y be the space of
possible labels for an instance, and let f : X ? Z
be the target function to be learned. A represen-
tation is a function R: X ? Y , for some suitable
feature space Y (such as Rd). A domain is defined
as a distribution D over the instance set X . An
open-domain system observes a set of training ex-
amples (R(x), f(x)), where instances x ? X are
drawn from a source domain, to learn a hypothe-
sis for classifying examples drawn from a separate
target domain.
Previous work by Ben-David et al (2007) uses
Vapnik-Chervonenkis (VC) theory to show that
the choice of representation is crucial to open-
domain learning. As is customary in VC the-
ory, a good choice of representation must allow
a learning machine to achieve low error rates dur-
ing training. Just as important, however, is that
the representation must simultaneously make the
source and target domains look as similar to one
another as possible.
For open-domain sequence-labeling, then, the
traditional representations are problematic. Typ-
ical representations in NLP use functions of the
local context to produce features. Although many
previous studies have shown that such lexical
features allow learning systems to achieve im-
pressively low error rates during training, they
also make texts from different domains look very
dissimilar. For instance, a sentence containing
?bank? is almost certainly from the WSJ rather
than biomedical text; a sentence containing ?path-
way? is almost certainly from a biomedical text
rather than from the WSJ.
Our recent work (2009) shows how to build
systems that learn new representations for open-
domain NLP using latent-variable language mod-
els like Hidden Markov Models (HMMs). In POS-
tagging and chunking experiments, these learned
representations have proven to meet both of Ben-
David et al?s criteria for representations. They
help discriminate among classes of words, since
HMMs learn distributional similarity classes of
words that often correlate with the labels that need
to be predicted. Moreover, it would be difficult to
tell apart two domains based on the set of HMM
states that generated the texts, since a given HMM
state may generate words from any number of do-
mains.
In the rest of this paper, we investigate ways to
improve the predictive power of the learned rep-
resentations, without losing the essential property
that the features remain stable across domains. We
stay within the framework of using graphical mod-
els to learn representations, and demonstrate sig-
nificant improvements on our original technique.
3 Experimental Setup
We use the same experimental setup as Blitzer
et al (2006): the Penn Treebank (Marcus et al,
1993) Wall Street Journal portion for our labeled
training data; 561 MEDLINE sentences (9576
words) from the Penn BioIE project (PennBioIE,
2005) for our labeled test set; and all of the un-
labeled text from the Penn Treebank WSJ portion
plus Blitzer et al?s MEDLINE corpus of 71,306
24
unlabeled sentences to train our latent variable
models. The two texts come from two very dif-
ferent domains, making this data a tough test for
domain adaptation. 23% of the word types in the
test text are Out-Of-Vocabulary (OOV), meaning
that they are never observed in the training data.
We use a number of unsupervised representa-
tion learning techniques to discover features from
our unlabeled data, and a supervised classifier to
train on the training set annotated with learned fea-
tures. We use an open source Conditional Random
Field (CRF) (Lafferty et al, 2001) software pack-
age1 designed by Sunita Sajarwal and William W.
Cohen to implement our supervised models. We
refer to the baseline system with feature set fol-
lowing our previous work (2009) as PLAIN-CRF.
Our learned features will supplement this set.
For comparison, we also report on the perfor-
mance of Blitzer et al?s Structural Correspon-
dence Learning (SCL) (2006), our HMM-based
model (2009)(HY09), and two other baselines:
? TEST-CRF: Our baseline model, trained and
tested on the test data. This is our upper
bound.
? SELF-CRF: Following the self-training
paradigm (e.g., (McClosky et al, 2006b;
McClosky et al, 2006a)), we train our
baseline first on the training set, then apply it
to the test set, then retrain it on the training
set plus the automatically labeled test set.
We perform only one iteration of retraining,
although in general multiple iterations are
possible, usually with diminishing marginal
returns.
4 Multi-dimensional Representations
From a linguistic perspective, words are multi-
dimensional objects. For instance, the word ?we?
in ?We like doing domain adaptation research? is a
pronoun, a subject, first person, and plural, among
other things. Each of these properties is a sepa-
rate feature of this word, which can be changed
without changing the other features. For exam-
ple, if ?we? is changed to ?they? in the previ-
ous example, it is exactly the same as ?we? in
all aspects, except that it is third person; if ?we?
is changed to ?us?, then it changes from subject
case to object case. In morphologically rich lan-
guages, many syntactic distinctions are marked in
1Available from http://sourceforge.net/projects/crf/
the surface forms of words; in more analytic or
isolating languages like English, the distinctions
are still there, but must often be inferred from con-
text rather than word form. Beyond syntactic di-
mensions, numerous semantic properties can also
distinguish words, such as nouns that refer to cog-
nitive agents versus nouns that refer to materials
and tools.
We seek to learn multidimensional representa-
tions of words. Our HMM-based model is able to
categorize words in one dimension, by assigning
a single HMM latent state to each word. Since
the HMM is trained on unlabeled data, this di-
mension may partially reflect POS categories, but
more likely represents a mixture of many different
word dimensions. By adding in multiple hidden
layers to our sequence model, we aim to learn a
multi-dimensional representation that may help us
to capture word features from multiple perspec-
tives. The supervised CRF system can then sort
out which dimensions are relevant to the sequence-
labeling task at hand.
A Factorial HMM (FHMM) can be used to
model multiple hidden dimensions of a word.
However, the memory requirements of an FHMM
increase exponentially with the number of lay-
ers in the graphical model, making it hard to use
(see Table 1). Although other parameterizations
may require much less memory, like using a log-
linear output distribution conditioned on the fac-
tors, exact inference is still computationally in-
tractable; exploring FHMMs with approximate in-
ference and learning is an interesting area for fu-
ture work. Here, we choose to create several
single-layer HMMs separately. Figure 1 shows
our Independent-HMM model (I-HMM). I-HMM
has several copies of the observation sequence and
each copy is associated with its own hidden label
sequence. To encourage each layer of the I-HMM
model to find a different local maximum in pa-
rameter space during training (and thus a different
model of the observation sequence), we initialize
the parameters randomly.
Suppose there are L independent layers in an I-
HMM model for corpus x = (x1, . . . , xN ), and
each layer is (yl1,yl2,...ylN ), where l = 1...L and
each y can have K states. The distribution of the
corpus and one hidden layer l is
P (x,yl) =
?
i
P (xi|yli)P (yli|yli?1)
For each layer l, for each position i, each HMM
25
XN
?
?
?
XN
X2 XN
X1 X2
X1
X1
X2
Y11
Y12
Y1L
Y21
Y22
Y2L YNL
YN2
YN1
Figure 1: Graphical models of an Independent Hidden
Markov Model. The dash line rectangle indicates that they
are copies of the observation sequence
Model Number of Memorylayers words states
HMM 1 W K O(WK + K2)
FHMM L W K O(WKL + LK2)
I-HMM L W K O(WKL + LK2)
Table 1: The memory requirement for HMM, FHMM, and
I-HMM models.
state y and each POS tag z, we add a new boolean
feature to our CRF system that indicates whether
Y li = y and Zi=z.
We experiment with two versions of I-HMM:
first, we fix the number of states in each layer at
80 states, and increase the number of HMM lay-
ers from 1 to 8 (I-HMM(80)). Second, to provide
greater encouragement for each layer to represent
separate information, we vary the number of states
in each layer (I-HMM(vary)). The detailed config-
uration for this model is shown in Table 2.
The results for our two models are shown in Fig-
ure 2. We can see that the accuracy of I-HMM(80)
model keeps increasing from 90.5% to 93.3% until
7 layers of HMM features (we call this 7-layer rep-
resentation I-HMM*). This is a dramatic 29% de-
crease in the best reported error rate for this dataset
when no labeled data from the biomedical domain
is used. Unlike with an FHMM, there is no guar-
antee that the different layers of an I-HMM will
model different aspects of the observation signal,
but our results indicate that for at least several lay-
ers, the induced models are complementary. After
7 layers, results begin to decrease, most likely be-
cause the added layer is no longer complementary
to the existing latent-variable models and is caus-
ing the supervised CRF to overfit the training data.
For the I-HMM(vary) model with up to 5 lay-
Number Number of States
of Layers in each Layer
1 10
2 10 20
3 10 20 40
4 10 20 40 60
5 10 20 40 60 80
Table 2: The configuration of HMM layers and HMM states
for the I-HMM(vary) model
86
87
88
89
90
91
92
93
94
1 2 3 4 5 6 7 8
Ac
cu
ra
cy
Number of HMM layers
Accuracy on different number of 
HMM layers
I-HMM(80)
I-HMM(vary)
HY09(90.5%)
Figure 2: Our best multi-dimensional smoothed-HMM tag-
ger with 7 layers reaches 93.3% accuracy, a drop of nearly 3%
in the error rate from the previous state of the art (HY09).
ers, the accuracy is not as good as I-HMM(80), al-
though the 5-layer model still outperforms HY09.
Individually, HMM models with fewer than 80
states perform worse than the 80-state model (a
model with 40 states achieved 89.4% accuracy,
and a model with 20 states achieved 88.9%). We
had hoped that by using layers with different
numbers of states, we could force the layers to
learn complementary models, but the results indi-
cate that any benefit from complementarity is out-
weighed by the lower performance of the individ-
ual layers.
5 Learning Representations with
Contrastive Estimation
In recent years, many NLP practitioners have be-
gun using discriminative models, and especially
maximum-entropy-based models like CRFs, be-
cause they allow the modeler to incorporate ar-
bitrary, interacting features of the observation se-
quence while still providing tractable inference.
To see if the same benefit can carry over to our rep-
resentation learning, we aim to build maximum-
entropy-based linear-chain models that, unlike
26
most discriminative models, train on unannotated
data. We follow Smith and Eisner (2005) in
training our models using a technique called con-
trastive estimation, which we explain below. We
call the resulting model the Smith and Eisner
Model (SEM).
The key to SEM is that the contrastive estima-
tion training procedure forces the model to explain
why the given training data are better than per-
turbed versions of the data, called neighbor points.
For example, the sentence ?We like doing domain
adaptation research? is a valid sentence, but if we
switched ?like? and ?doing?, the new sentence
?We doing like domain adaptation research? is not
valid. SEM learns a model of the original sen-
tence by contrasting it with the invalid neighbor
sentences.
Let ~x =< x1, x2, ..., xN > be the observed ex-
ample sentences, and let Y be the space of possible
hidden structures for xi. Let N (xi) be a ?neigh-
borhood? for xi, or a set of negative examples ob-
tained by perturbing xi, plus xi itself. Given a vec-
tor of feature functions ~f(x, y), SEM tries to find
a set of weights ~? that maximize a log-likelihood
function:
LN (~?) = log
?
i
?
y?Y u(xi, y|~?)
?
(x,y)?N (xi)?Y u(x, y|~?)
where u(x, y|~?) = exp(~? ? ~f(x, y)) is the ?un-
normalized probability? of an (example, hidden
structure) pair (x,y). Following Smith and Eisner,
we use the best performing neighborhood, called
TRANS1, to conduct our experiments. TRANS1
is the set of sentences resulting from transposing
any pair of adjacent words for any given training
example.
The base feature space for SEM includes two
kinds of boolean features analogous to HMM
emission and transition probabilities. For an ob-
servation sequence x1, . . . , xT and a label se-
quence y1, . . . , yT , a boolean emission feature in-
dicates whether xt = x and yt = y for all possible
t, x, and y. A boolean transition feature indicates
whether yt?1 = y and yt = y? for all possible t, y,
and y?.
Because contrastive estimation is a computa-
tionally expensive training procedure, we take two
steps to reduce the computational cost: we reduce
the unlabeled data set, and we prune the feature
set of SEM. For our training data, we use only the
sentences with length less than or equal to 10. We
also get rid of punctuation and the corresponding
tags, change all words to lowercase and change all
numbers into a single symbol.
To reduce the feature space, we create a tag-
ging dictionary from Penn Treebank sections 02-
21: for every word in these sections, the dictionary
records the set of POS tags that were ever asso-
ciated with that word. We then prune the emis-
sion features for words that appear in this dic-
tionary to include only the features that associate
words with their corresponding POS tags in the
dictionary. For the words that don?t appear in the
Penn Treebank, they are associated with all pos-
sible POS tags. This procedure reduces the total
number of features in our SEM model from over
500,000 to just over 60,000.
After we train the model, we use a Viterbi-like
algorithm to decode it on the testing set. Unlike
the HMM model, the decoded states of SEM are
already meaningful POS tags, so we can use these
decoded states as POS tags (PLAIN-SEM), or use
them as features for a CRF model (SEM-CRF).
We show the result of both models, as well as
several comparison models, in Table 3. From the
result, we can see that the unsupervised PLAIN-
SEM outperforms the supervised PLAIN-CRF on
both all words and OOV words. This impres-
sive performance results from its ability to adapt
to the new domain through the unlabeled train-
ing examples and the contrastive estimation train-
ing procedure. In addition, the SEM-CRF model
significantly outperforms the SCL model (88.9%)
and the HMM-based CRF with 40 hidden states
(89.4%) while using only 36 hidden states, al-
though it does not quite reach the performance
of HY09. These results, which use a subset of
the available unlabeled training text, suggest that
maximum-entropy-style representation learning is
a promising area for further investigation.
6 Analysis
As we mention in Section 2, the choice of repre-
sentation is crucial to open-domain learning. In
Sections 4 and 5, we demonstrate empirically that
learned representations based on latent-variable
graphical models can significantly improve the ac-
curacy of a POS tagger on a new domain, com-
pared with using the traditional word-level repre-
sentations. We now examine our best representa-
tion, I-HMM*, in light of the theoretical predic-
tions made by VC theory.
27
All OOV
Model words words
PLAIN-CRF 88.3 67.3
SELF-CRF 88.5 70.4
PLAIN-SEM 88.5 69.8
SCL 88.9 72.0
SEM-CRF 90.0 71.9
HY09 90.5 75.2
I-HMM* 93.3 76.3
TEST-CRF 98.9 NA
Table 3: SEM-CRF reduces error compared with
SCL by 1.1% on all words; I-HMM* closes 33%
of the gap between the state-of-the-art HY09 and
the upper-bound, TEST-CRF.
In particular, Ben-David et al?s analysis shows
that the distance between two domains under a
representation R of the data is crucial to domain
adaptation. However, their analysis depends on
a particular notion of distance, the H-divergence,
that is computationally intractable to calculate.
For our analysis, we resort instead to a crude
but telling approximation of this measure, using a
more standard notion of distance: Jensen-Shannon
Divergence (DJS).
To calculate the distance between domains un-
der a representation R, we represent a domain D
as a multinomial probability distribution over the
set of features in R. We take maximum-likelihood
estimates of this distribution using our samples
from the WSJ and MEDLINE domains. We then
measure the Jensen-Shannon Divergence between
the two distributions, which for discrete distribu-
tions is calculated as
DJS(p||q) =
1
2
?
i
[
pilog
(
pi
mi
)
+ qilog
(
qi
mi
)]
where m = p+q2 .
Figure 3 shows the divergence between these
two domains under purely lexical features, and un-
der only HMM-based features. OOV words make
up a substantial portion of the divergence between
the two domains under the lexical representation,
but even if we ignore them the HMM features are
substantially less variable across the two domains,
which helps to explain their ability to provide su-
pervised classifiers with stable features for domain
adaptation. Because there are so few HMM states
compared with the number of word types, there is
no such thing as an OOV HMM state, and the word
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Words I-HMM* States
Di
st
an
ce
 b
et
w
ee
n 
Do
m
ai
ns
OOV
Non-OOV
Figure 3: The Jensen-Shannon Divergence be-
tween the newswire domain and the biomedical
domain, according to a word-based representation
of the domains and a HMM-based representation.
The portion of the distance that is due to words
which appear in the biomedical domain but not the
newswire domain is shown in gray.
states that appear in training data appear roughly
as often in test data. This means that any asso-
ciations that the CRF might learn between HMM
states and predicted outcomes is likely to remain
useful on the test data, but associations between
words and outcomes are less likely to be useful.
7 Previous Work
Previous work on artificial neural networks
(ANNs) (Fahlman and Lebiere, 1990) has shown
that it is possible to learn effectively by adding
more hidden units to the neural network that cor-
relate with the residual error of the existing hidden
units (Cascade-Correlation learning). Like our I-
HMM technique, this work aims to build a multi-
dimensional model, and it is capable of learning
the number of appropriate dimensions. Unlike
the ANN scenario, our multi-dimensional learn-
ing techniques must handle unlabeled data, and
they rely on the sequential structure of language
to learn effectively, whereas Cascade-Correlation
learning assumes samples are independent and
identically distributed. Our techniques do not (yet)
automatically determine the best number of layers
in the model.
Unlike our techniques for domain adaptation, in
most cases researchers have focused on the sce-
nario where labeled training data is available in
both the source and the target domain (e.g., (Bac-
chiani et al, 2006; Daume? III, 2007; Chelba and
28
Acero, 2004; Daume? III and Marcu, 2006; Blitzer
et al, 2007)). Our techniques use only raw text
from the target domain. This reduces the cost
of domain adaptation and makes the techniques
more widely applicable to new domains like web
processing, where the domain and vocabulary is
highly variable, and it is extremely difficult to
obtain labeled data that is representative of the
test distribution. When labeled target-domain data
is available, instance weighting and similar tech-
niques can potentially be used in combination with
our techniques to improve our results further.
Several researchers have previously studied
methods for using unlabeled data for sequence la-
beling, either alone or as a supplement to labeled
data. Ando and Zhang develop a semi-supervised
chunker that outperforms purely supervised ap-
proaches on the CoNLL 2000 dataset (Ando and
Zhang, 2005). Recent projects in semi-supervised
(Toutanova and Johnson, 2007) and unsupervised
(Biemann et al, 2007; Smith and Eisner, 2005)
tagging also show significant progress. HMMs
have been used many times for POS tagging in
supervised, semi-supervised, and in unsupervised
settings (Banko and Moore, 2004; Goldwater and
Griffiths, 2007; Johnson, 2007). The REALM sys-
tem for sparse information extraction has also used
unsupervised HMMs to help determine whether
the arguments of a candidate relation are of the
appropriate type (Downey et al, 2007). Schu?tze
(1994) has presented an algorithm that categorizes
word tokens in context instead of word types for
tagging words. We take a novel perspective on the
use of unsupervised latent-variable models by us-
ing them to compute features of each token that
represent the distribution over that token?s con-
texts. These features prove to be highly useful
for supervised sequence labelers in out-of-domain
tests.
In the deep learning (Bengio, 2009) paradigm,
researchers have investigated multi-layer latent-
variable models for language modeling, among
other tasks. While n-gram models have tradition-
ally dominated in language modeling, two recent
efforts develop latent-variable probabilistic mod-
els that rival and even surpass n-gram models in
accuracy (Blitzer et al, 2005; Mnih and Hinton,
2007). Several authors investigate neural network
models that learn a vector of latent variables to
represent each word (Bengio et al, 2003; Emami
et al, 2003; Morin and Bengio, 2005). And facto-
rial Hidden Markov Models (Ghahramani and Jor-
dan, 1997) are a multi-layer variant of the HMM
that has been used in speech recognition, among
other things. We use simpler mixtures of single-
layer models for the sake of memory-efficiency,
and we use our models as representations in a su-
pervised task, rather than as language models.
8 Conclusion and Future Work
Our representation learning approach to domain
adaptation yields state-of-the-art results in POS
tagging experiments. Our best models use multi-
dimensional clustering to find several latent cate-
gories for each word; the latent categories serve
as useful and domain-independent features for
our supervised learner. Our exploration has
yielded significant progress already, but it has only
scratched the surface of possible models for this
task. The current representation learning tech-
niques we use are unsupervised, meaning that they
provide the same set of categories, regardless of
what task they are to be used for. Semi-supervised
learning approaches could be developed to guide
the representation learning process towards fea-
tures that are best-suited for a particular task, but
are still useful across domains. Our current ap-
proach also requires retraining of a CRF for every
new domain; incremental retraining techniques for
new domains would speed up the process and
make domain adaptation much more accessible.
Finally, there are cases where small amounts of la-
beled data are available for new domains; models
that combine our representation learning approach
with instance weighting and other forms of super-
vised domain adaptation may take better advan-
tage of these cases.
Acknowledgments
We wish to thank the anonymous reviewers for
their helpful comments and suggestions.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In ACL.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING.
29
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In Advances in Neural In-
formation Processing Systems 20, Cambridge, MA.
MIT Press.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Y. Bengio. 2009. Learning deep architectures for AI.
Foundations and Trends in Machine Learning, 2.
C. Biemann, C. Giuliano, and A. Gliozzo. 2007. Un-
supervised pos tagging supporting supervised meth-
ods. Proceeding of RANLP-07.
J. Blitzer, A. Globerson, and F. Pereira. 2005. Dis-
tributed latent variable models of lexical cooccur-
rences. In Proceedings of the Tenth International
Workshop on Artificial Intelligence and Statistics.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In EMNLP.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Doug Downey, Stefan Schoenmackers, and Oren Et-
zioni. 2007. Sparse information extraction: Unsu-
pervised language models to the rescue. In ACL.
A. Emami, P. Xu, and F. Jelinek. 2003. Using a
connectionist model in a syntactical based language
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 372?
375.
Scott E. Fahlman and Christian Lebiere. 1990. The
cascade-correlation learning architecture. Advances
in Neural Information Processing Systems 2.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2-3):245?273.
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Conference on Empirical Methods in
Natural Language Processing.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence labeling. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers. In EMNLP.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
In Proceedings of the International Conference on
Machine Learning.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing. In
Proc. of HLT-NAACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL, pages 337?344.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine Learning, pages 641?648, New York,
NY, USA. ACM.
F. Morin and Y. Bengio. 2005. Hierarchical probabilis-
tic neural network language model. In Proceedings
of the International Workshop on Artificial Intelli-
gence and Statistics, pages 246?252.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu/.
Anni R.Codena, Serguei V.Pakhomovb, Rie K.Andoa,
Patrick H.Duffyb, and Christopher G.Chute. 2005.
Domain-specific language models and lexicons
for tagging. Journal of Biomedical Informatics,
38(6):422?430.
Hinrich Schu?tze. 1994. Distributional part-of-speech
tagging. In Proceedings of the 7th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL).
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 354?362, Ann Arbor, Michigan, June.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian LDA-based model for semi-supervised
part-of-speech tagging. In NIPS.
30
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 125?134,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Language Models as Representations for Weakly-Supervised NLP Tasks
Fei Huang and Alexander Yates
Temple University
Broad St. and Montgomery Ave.
Philadelphia, PA 19122
fei.huang@temple.edu
yates@temple.edu
Arun Ahuja and Doug Downey
Northwestern University
2133 Sheridan Road
Evanston, IL 60208
a-ahuja@northwestern.edu
ddowney@eecs.northwestern.edu
Abstract
Finding the right representation for words is
critical for building accurate NLP systems
when domain-specific labeled data for the
task is scarce. This paper investigates lan-
guage model representations, in which lan-
guage models trained on unlabeled corpora
are used to generate real-valued feature vec-
tors for words. We investigate ngram mod-
els and probabilistic graphical models, includ-
ing a novel lattice-structured Markov Random
Field. Experiments indicate that language
model representations outperform traditional
representations, and that graphical model rep-
resentations outperform ngram models, espe-
cially on sparse and polysemous words.
1 Introduction
NLP systems often rely on hand-crafted, carefully
engineered sets of features to achieve strong perfor-
mance. Thus, a part-of-speech (POS) tagger would
traditionally use a feature like, ?the previous token
is the? to help classify a given token as a noun or
adjective. For supervised NLP tasks with sufficient
domain-specific training data, these traditional fea-
tures yield state-of-the-art results. However, NLP
systems are increasingly being applied to texts like
the Web, scientific domains, and personal commu-
nications like emails, all of which have very differ-
ent characteristics from traditional training corpora.
Collecting labeled training data for each new target
domain is typically prohibitively expensive. We in-
vestigate representations that can be applied when
domain-specific labeled training data is scarce.
An increasing body of theoretical and empirical
evidence suggests that traditional, manually-crafted
features limit systems? performance in this setting
for two reasons. First, feature sparsity prevents sys-
tems from generalizing accurately to words and fea-
tures not seen during training. Because word fre-
quencies are Zipf distributed, this often means that
there is little relevant training data for a substantial
fraction of parameters (Bikel, 2004), especially in
new domains (Huang and Yates, 2009). For exam-
ple, word-type features form the backbone of most
POS-tagging systems, but types like ?gene? and
?pathway? show up frequently in biomedical liter-
ature, and rarely in newswire text. Thus, a classifier
trained on newswire data and tested on biomedical
data will have seen few training examples related to
sentences with features ?gene? and ?pathway? (Ben-
David et al, 2009; Blitzer et al, 2006).
Further, because words are polysemous, word-
type features prevent systems from generalizing to
situations in which words have different meanings.
For instance, the word type ?signaling? appears pri-
marily as a present participle (VBG) in Wall Street
Journal (WSJ) text, as in, ?Interest rates rose, sig-
naling that . . . ? (Marcus et al, 1993). In biomedical
text, however, ?signaling? appears primarily in the
phrase ?signaling pathway,? where it is considered
a noun (NN) (PennBioIE, 2005); this phrase never
appears in the WSJ portion of the Penn Treebank
(Huang and Yates, 2010a).
Our response to these problems with traditional
NLP representations is to seek new representations
that allow systems to generalize more accurately to
previously unseen examples. Our approach depends
on the well-known distributional hypothesis, which
states that a word?s meaning is identified with the
contexts in which it appears (Harris, 1954; Hin-
dle, 1990). Our goal is to develop probabilistic lan-
125
guage models that describe the contexts of individ-
ual words accurately. We then construct represen-
tations, or mappings from word tokens and types
to real-valued vectors, from these language models.
Since the language models are designed to model
words? contexts, the features they produce can be
used to combat problems with polysemy. And by
careful design of the language models, we can limit
the number of features that they produce, controlling
how sparse those features are in training data.
In this paper, we analyze the performance
of language-model-based representations on tasks
where domain-specific training data is scarce. Our
contributions are as follows:
1. We introduce a novel factorial graphical model
representation, a Partial-Lattice Markov Random
Field (PL-MRF), which is a tractable variation of
a Factorial Hidden Markov Model (HMM) for lan-
guage modeling.
2. In experiments on POS tagging in a domain adap-
tation setting and on weakly-supervised informa-
tion extraction (IE), we quantify the performance of
representations derived from language models. We
show that graphical models outperform ngram rep-
resentations. The PL-MRF representation achieves a
state-of-the-art 93.8% accuracy on the POS tagging
task, while the HMM representation improves over
the ngram model by 10% on the IE task.
3. We analyze how the performance of the different
representations varies due to the fundamental chal-
lenges of sparsity and polysemy.
The next section discusses previous work. Sec-
tions 3 and 4 present the existing representations we
investigate and the new PL-MRF, respectively. Sec-
tions 5 and 6 describe our two tasks and the results
of using our representations on each of them. Sec-
tion 7 concludes.
2 Previous Work
There is a long tradition of NLP research on rep-
resentations, mostly falling into one of four cate-
gories: 1) vector space models of meaning based
on document-level lexical cooccurrence statistics
(Salton and McGill, 1983; Turney and Pantel, 2010;
Sahlgren, 2006); 2) dimensionality reduction tech-
niques for vector space models (Deerwester et al,
1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005;
Blei et al, 2003; Va?yrynen et al, 2007); 3) using
clusters that are induced from distributional similar-
ity (Brown et al, 1992; Pereira et al, 1993; Mar-
tin et al, 1998) as non-sparse features (Lin and Wu,
2009; Candito and Crabbe, 2009; Koo et al, 2008;
Zhao et al, 2009); 4) and recently, language models
(Bengio, 2008; Mnih and Hinton, 2009) as represen-
tations (Weston et al, 2008; Collobert and Weston,
2008; Bengio et al, 2009), some of which have al-
ready yielded state of the art performance on domain
adaptation tasks (Huang and Yates, 2009; Huang and
Yates, 2010a; Huang and Yates, 2010b; Turian et al,
2010) and IE (Ahuja and Downey, 2010; Downey et
al., 2007b). In contrast to this previous work, we de-
velop a novel Partial Lattice MRF language model
that incorporates a factorial representation of latent
states, and demonstrate that it outperforms the pre-
vious state-of-the-art in POS tagging in a domain
adaptation setting. We also analyze the novel PL-
MRF representation on an IE task, and several repre-
sentations along the key dimensions of sparsity and
polysemy.
Most previous work on domain adaptation has fo-
cused on the case where some labeled data is avail-
able in both the source and target domains (Daume?
III, 2007; Jiang and Zhai, 2007; Daume? III and
Marcu, 2006; Finkel and Manning, 2009; Dredze
et al, 2010; Dredze and Crammer, 2008). Learn-
ing bounds are known (Blitzer et al, 2007; Man-
sour et al, 2009). Daume? III et al (2010) use semi-
supervised learning to incorporate labeled and unla-
beled data from the target domain. In contrast, we
investigate a domain adaptation setting where no la-
beled data is available for the target domain.
3 Representations
A representation is a set of features that describe
instances for a classifier. Formally, let X be an
instance set, and let Z be the set of labels for a
classification task. A representation is a function
R : X ? Y for some suitable feature space Y (such
as Rd). We refer to dimensions of Y as features, and
for an instance x ? X we refer to values for partic-
ular dimensions of R(x) as features of x.
3.1 Traditional POS-Tagging Representations
As a baseline for POS tagging experiments and an
example of our terminology, we describe a repre-
sentation used in traditional supervised POS taggers.
The instance set X is the set of English sentences,
and Z is the set of POS tag sequences. A traditional
representation TRAD-R maps a sentence x ? X to a
sequence of boolean-valued vectors, one vector per
126
Representation Feature
TRAD-R ?w1[xi = w]
?s?Suffixes1[xi ends with s]
1[xi contains a digit]
NGRAM-R ?w?,w??P (w?ww??)/P (w)
HMM-TOKEN-R ?k1[yi? = k]
HMM-TYPE-R ?kP (y = k|x = w)
I-HMM-TOKEN-R ?j,k1[yi,j? = k]
BROWN-TOKEN-R ?j?{?2,?1,0,1,2}
?p?{4,6,10,20} prefix(yi+j , p)
BROWN-TYPE-R ?p prefix(y, p)
LATTICE-TOKEN-R ?j,k1[yi,j? = k]
LATTICE-TYPE-R ?kP (y = k|x = w)
Table 1: Summary of features provided by our repre-
sentations. ?a1[g(a)] represents a set of boolean fea-
tures, one for each value of a, where the feature is
true iff g(a) is true. xi represents a token at position
i in sentence x, w represents a word type, Suffixes =
{-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity}, k (and k) represents
a value for a latent state (set of latent states) in a latent-
variable model, y? represents the optimal setting of latent
states y for x, yi is the latent variable for xi, and yi,j is
the latent variable for xi at layer j. prefix(y,p) is the p-
length prefix of the Brown cluster y.
word xi in the sentence. Dimensions for each latent
vector include indicators for the word type of xi and
various orthographic features. Table 1 presents the
full list of features in TRAD-R. Since our IE task
classifies word types rather than tokens, this base-
line is not appropriate for that task. Below, we de-
scribe how we can learn representations R by using
a variety of language models, for use in both our IE
and POS tagging tasks. All representations for POS
tagging inherit the features from TRAD-R; all repre-
sentations for IE do not.
3.2 Ngram Representations
N-gram representations model a word type w in
terms of the n-gram contexts in which w appears
in a corpus. Specifically, for word w we generate
the vector P (w?ww??)/P (w), the conditional prob-
ability of observing the word sequence w? to the left
and w?? to the right of w. The experimental section
describes the particular corpora and language mod-
eling methods used for estimating probabilities.
3.3 HMM-based Representations
In previous work, we have implemented several
representations based on HMMs (Rabiner, 1989),
which we used for both POS tagging (Huang and
Yates, 2009) and IE (Downey et al, 2007b). An
HMM is a generative probabilistic model that gen-
erates each word xi in the corpus conditioned on a
latent variable yi. Each yi in the model takes on in-
tegral values from 1 to K, and each one is generated
by the latent variable for the preceding word, yi?1.
The joint distribution for a corpus x = (x1, . . . , xN )
and a set of state vectors y = (y1, . . . , yN ) is
given by: P (x,y) = ?i P (xi|yi)P (yi|yi?1). Us-
ing Expectation-Maximization (EM) (Dempster et
al., 1977), it is possible to estimate the distributions
for P (xi|yi) and P (yi|yi?1) from unlabeled data.
We construct two different representations from
HMMs, one for POS tagging and one for IE. For
POS tagging, we use the Viterbi algorithm to pro-
duce the optimal setting y? of the latent states for a
given sentence x, or y? = arg maxy P (x,y). We
use the value of yi? as a new feature for xi that repre-
sents a cluster of distributionally-similar words. For
IE, we require features for word types w, rather than
tokens xi. We use the K-dimensional vector that
represents the distribution P (y|x = w) as the fea-
ture vector for word type w. This set of features
represents a ?soft clustering? of w into K different
clusters. We refer to these representations as HMM-
TOKEN-R and HMM-TYPE-R, respectively.
Because HMM-based representations offer a
small number of discrete states as features, they have
a much greater potential to combat feature sparsity
than do ngram models. Furthermore, for token-
based representations, these models can potentially
handle polysemy better than ngram language models
by providing different features in different contexts.
We also compare against a variation of the HMM
from our previous work (Huang and Yates, 2010a),
henceforth HY10. This model independently trains
M separate HMM models on the same corpus, ini-
tializing each one randomly. We can then use the
Viterbi-optimal decoded latent state of each inde-
pendent HMM model as a separate feature for a to-
ken. We refer to this language model as an I-HMM,
and the representation as I-HMM-TOKEN-R.
Finally, we compare against Brown clusters
(Brown et al, 1992) as learned features. Although
not traditionally described as such, Brown cluster-
ing involves constructing an HMM model in which
127
each type is restricted to having exactly one latent
state that may generate it. Brown et al describe a
greedy agglomerative clustering algorithm for train-
ing this model on unlabeled text. Following Turian
et al (2010), we use Percy Liang?s implementation
of this algorithm for our comparison, and we test
runs with 100, 320, and 1000 clusters. We use fea-
tures from these clusters identical to Turian et al?s.1
Turian et al have shown that Brown clusters match
or exceed the performance of neural network-based
language models in domain adaptation experiments
for named-entity recognition, as well as in-domain
experiments for NER and chunking.
4 A Novel Lattice Language Model
Representation
Our final language model is a novel latent-variable
language model with rich latent structure, shown in
Figure 1. The model contains a lattice of M ?N la-
tent states, where N is the number of words in a sen-
tence and M is the number of layers in the model.
We can justify the choice of this model from a lin-
guistic perspective as a way to capture the multi-
dimensional nature of words. Linguists have long
argued that words have many different features in a
high dimensional space: they can be separately de-
scribed by part of speech, gender, number, case, per-
son, tense, voice, aspect, mass vs. count, and a host
of semantic categories (agency, animate vs. inani-
mate, physical vs. abstract, etc.), to name a few (Sag
et al, 2003). Our model seeks to capture a multi-
dimensional representation of words by creating a
separate layer of latent variables for each dimension.
The values of the M layers of latent variables for a
single word can be used as M distinct features in
our representation. The I-HMM attempts to model
the same intuition, but unlike a lattice model the I-
HMM layers are entirely independent, and as a re-
sult there is no mechanism to enforce that the layers
model different dimensions. Duh (2005) previously
used a 2-layer lattice for tagging and chunking, but
in a supervised setting rather than for representation
learning.
Let Cliq(x,y) represent the set of all maximal
cliques in the graph of the MRF model for x and y.
1Percy Liang?s implementation is available at
http://metaoptimize.com/projects/wordreprs/. Turian et al
also tested a run with 3200 clusters in their experiments, which
we have been training for months, but which has not finished in
time for publication.
y4,1
y3,1
y
y4,2
y3,2
y
y4,3
y3,3
y
y4,4
y3,4
y
y4,5
y3,5
y
x1
2,1
y1,1
x2
2,2
y1,2
x3
2,3
y1,3
x4
2,4
y1,4
x5
2,5
y1,5
Figure 1: The Partial Lattice MRF (PL-MRF) Model for a
5-word sentence and a 4-layer lattice. Dashed gray edges
are part of a full lattice, but not the PL-MRF.
Expressing the lattice model in log-linear form, we
can write the marginal probability P (x) of a given
sentence x as:
?
y
?
c?Cliq(x,y) score(c,x,y)
?
x?,y?
?
c?Cliq(x?,y?) score(c,x?,y?)
where score(c,x,y) = exp(?c ? fc(xc,yc)). Our
model includes parameters for transitions between
two adjacent latent variables on layer j: ?transi,s,i+1,s?,j
for yi,j = s and yi+1,j = s?. It also includes obser-
vation parameters for latent variables and tokens, as
well as for pairs of adjacent latent variables in differ-
ent layers and their tokens: ?obsi,j,s,w and ?obsi,j,s,j+1,s?,w
for yi,j = s, yi,j+1 = s?, and xi = w.
Computationally, the lattice MRF is preferable to
a na??ve Factorial HMM (Ghahramani and Jordan,
1997) representation, which would require O(2M )
parameters for an M -layer model. However, ex-
act training and inference in supervised settings are
still intractable for this model (Sutton et al, 2007),
and thus it has not yet been explored as a language
model, which requires even more difficult, unsuper-
vised training. Training is intractable in part because
of the difficulty in enumerating and summing over
the exponentially-many configurations y for a given
x. We address this difficulty in two ways: by modi-
fying the model, and by modifying the training pro-
cedure.
4.1 Partial Lattice MRF
Instead of the full lattice model, we construct a
Partial Lattice MRF (PL-MRF) model by deleting
128
certain edges between latent layers of the model
(dashed gray edges in Figure 1). Let c = bN2 c,
where N is the length of the sentence. If i < c
and j is odd, or if j is even and i > c, we delete
edges between yi,j and yi,j+1. The same lattice of
nodes remains, but fewer edges and paths. A cen-
tral ?trunk? at i = c connects all layers of the lat-
tice, and branches from this trunk connect either to
the branches in the layer above or the layer below
(but not both). The result is a model that retains
most2 of the edges of the full model. Additionally,
the pruned model makes the branches conditionally
independent from one another, except through the
trunk. For instance, the right branch at layers 1
and 2 in Figure 1 (y1,4, y1,5, y2,4, and y2,5) are dis-
connected from the right branch at layers 3 and 4
(y3,4, y3,5, y4,4, and y4,5), except through the trunk
and the observed nodes. As a result, excluding the
observed nodes, this model has a low tree-width of
2 (excluding observed nodes), and a variety of ef-
ficient dynamic programming and message-passing
algorithms for training and inference can be readily
applied (Bodlaender, 1988).3 Our inference algo-
rithm passes information from the branches inwards
to the trunk, and then upward along the trunk, in
time O(K4MN).
As with our HMM models, we create two repre-
sentations from PL-MRFs, one for tokens and one
for types. For tokens, we decode the model to com-
pute y?, the matrix of optimal latent state values for
sentence x. For each layer j and and each possi-
ble latent state value k, we add a boolean feature
for token xi that is true iff y?i,j = k. For types,
we compute distributions over the latent state space.
Let y be the column vector of latent variables for
word x. For each possible configuration of values k
of the latent variables y, we add a real-valued fea-
tures for x given by P (y = k|x = w). We refer
to these two representations as LATTICE-TOKEN-R
and LATTICE-TYPE-R, respectively.
4.2 Parameter Estimation
We train the PL-MRF using contrastive estimation,
which iteratively optimizes the following objective
function on a corpus X:
?
x?X
log
?
y
?
c?Cliq(x,y) score(c,x,y)
?
x??N (x),y?
?
c?Cliq(x?,y?) score(c,x?,y?)
2As M, N ??, 5 out of every 6 edges are kept.
3c.f. a tree-width of min(M ,N ) for the unpruned model
where N (x), the neighborhood of x, indicates a
set of perturbed variations of the original sentence
x. Contrastive estimation seeks to move probability
mass away from the perturbed neighborhood sen-
tences and onto the original sentence. We use a
neighborhood function that includes all sentences
which can be obtained from the original sentence by
swapping the order of a consecutive pair of words.
Training uses gradient descent over this non-convex
objective function with a standard software package
(Liu and Nocedal, 1989) and converges to a local
maximum (Smith and Eisner, 2005).
For tractability, we modify the training procedure
to train the PL-MRF one layer at a time. Let ?i rep-
resent the set of parameters relating to features of
layer i, and let ??i represent all other parameters.
We fix ??0 = 0, and optimize ?0 using contrastive
estimation. After convergence, we fix ??1, and opti-
mize ?1, and so on. We use a convergence threshold
of 10?6, and each layer typically converges in under
100 iterations.
5 Domain Adaptation for a POS Tagger
We evaluate the representations described above on
a POS tagging task in a domain adaptation setting.
5.1 Experimental Setup
We use the same experimental setup as in HY10:
the Penn Treebank (Marcus et al, 1993) Wall Street
Journal portion for our labeled training data; 561
MEDLINE sentences (9576 types, 14554 tokens,
23% OOV tokens) from the Penn BioIE project
(PennBioIE, 2005) for our labeled test set; and all of
the unlabeled text from the Penn Treebank WSJ por-
tion plus a MEDLINE corpus of 71,306 unlabeled
sentences to train our language models. The two
texts come from two very different domains, mak-
ing this data a tough test for domain adaptation.
We use an open source Conditional Random Field
(CRF) (Lafferty et al, 2001) software package4 de-
signed by Sunita Sarawagi and William W. Cohen
to implement our supervised models. Let X be a
training corpus, Z the corresponding labels, and R
a representation function. For each token xi in X,
we include a parameter in our CRF model for all
features R(xi) and all possible labels in Z. Further-
more, we include transition parameters for pairs of
consecutive labels zi, zi+1.
4Available from http://sourceforge.net/projects/crf/
129
For representations, we tested TRAD-R,
NGRAM-R, HMM-TOKEN-R, I-HMM-TOKEN-R
(between 2 and 8 layers), and LATTICE-TOKEN-R
(8, 12, 16, and 20 layers). Following HY10, each
latent node in the I-HMMs have 80 possible values,
creating 808 ? 1015 possible configurations of the
8-layer I-HMM for a single word. Each node in
the PL-MRF is binary, creating a much smaller
number (220 ? 106) of possible configurations for
each word in a 20-layer representation. NGRAM-R
was trained using an unsmoothed trigram model on
the Web 1Tgram corpus. To keep the feature set
manageable, we included the top 500 most common
ngrams for each word type, and then used mutual
information on the training data to select the top
10,000 most relevant ngram features for all word
types. We incorporated ngram features as binary
values indicating whether xi appeared with the
ngram or not. We also report on the performance
of Brown clusters and Blitzer et al?s Structural
Correspondence Learning (SCL) (2006) technique,
which uses manually-selected ?pivot? words (like
?of?, ?the?) to learn domain-independent features.
Finally, we compare against the self-training CRF
technique from HY10.
5.2 Results and Discussion
For each representation, we measured the accuracy
of the POS tagger on the biomedical test text. Ta-
ble 2 shows the results for the best variation of each
kind of model ? 20 layers for the PL-MRF, 7 lay-
ers for the I-HMM, and 1000 clusters for the Brown
clustering. All language model representations sig-
nificantly outperform the SCL model and the TRAD-
R baseline. The novel PL-MRF model outperforms
the previous state of the art, the I-HMM model, and
much of the performance increase comes from a
11.3% relative reduction in error on words that ap-
pear in biomedical texts but not in newswire texts.
Both graphical model representations significantly
outperform the ngram model, which is trained on far
more text. For comparison, our best model, the PL-
MRF, achieved a 96.8% in-domain accuracy on sec-
tions 22-24 of the Penn Treebank, about 0.5% shy
of a state-of-the-art in-domain system (Shen et al,
2007) with more sophisticated supervised learning.
We expected that language model representations
perform well in part because they provide meaning-
ful features for sparse and polysemous words. To
test this, we selected 109 polysemous word types
model % error OOV % error
TRAD-R 11.7 32.7
TRAD-R+self-training 11.5 29.6
SCL 11.1 -
BROWN-TOKEN-R 10.8 25.4
HMM-TOKEN-R 9.5 24.8
NGRAM-R 6.9 24.4
I-HMM-TOKEN-R 6.7 24
LATTICE-TOKEN-R 6.2 21.3
SCL+500bio 3.9 -
Table 2: PL-MRF representations reduce error by 7.5%
relative to the previous state-of-the-art I-HMM, and ap-
proach within 2.3% absolute error a SCL+500bio model
with access to 500 labeled sentences from the target do-
main. 1.8% of the tags in the test set are new tags that
do not occur in the WSJ training data, so an error rate of
3.9+1.8 = 5.7% error is a reasonable bound for the best
possible performance of a model that has seen no exam-
ples from the target domain.
from our test data, along with 296 non-polysemous
word types, chosen based on POS tags and manual
inspection. We further define sparse word types as
those that appear 5 times or fewer in all of our unla-
beled data, and non-sparse word types as those that
appear at least 50 times in our unlabeled data. Table
3 shows results on these subsets of the data.
As expected, all of our language models outper-
form the baseline by a larger margin on polysemous
words than on non-polysemous words. The mar-
gin between graphical model representations and the
ngram model also increases on polysemous words,
presumably because the Viterbi decoding of these
models takes into account the tokens in the sur-
rounding sentence. The same behavior is evident for
sparsity: all of the language model representations
outperform the baseline by a larger margin on sparse
words than not-sparse words, and all of the graphical
models perform better relative to the ngram model
on sparse words as well. Thus representations based
on graphical models address two key issues in build-
ing representations for POS tagging.
6 Information Extraction Experiments
In this section, we evaluate our learned representa-
tions on a different task that investigates the abil-
ity of each representation to capture semantic, rather
than syntactic, information. Specifically, we inves-
130
POS Tagging Information Extraction
polys. not polys. sparse not sparse polys. not polys. sparse not sparse
tokens/types 159 4321 463 12194 222 210 266 166
categories - - - - 12 4 13 3
TRAD-R 59.5 78.5 52.5 89.6 - - - -
Ngram 68.2 85.3 61.8 94.0 0.07 0.17 0.06 0.25
HMM 67.9 83.4 60.2 91.6 0.14 0.26 0.15 0.32
(-Ngram) (-0.3) (-1.9) (-1.6) (-2.4) (+0.07) (+0.09) (+0.09) (+0.07)
I-HMM 75.6 85.2 62.9 94.5 - - - -
(-Ngram) (+7.4) (-0.1) (+1.1) (+0.5) - - - -
PL-MRF 70.5 86.9 65.2 94.6 0.09 0.15 0.1 0.19
(-Ngram) (+2.3) (+1.6) (+3.4) (+0.6) (+0.02) (-0.02) (+0.04) (-0.06)
Table 3: Graphical models consistently outperform ngram models by a larger margin on sparse words than not-sparse
words. On polysemous words, the difference between graphical model performance and ngram performance grows
for POS tagging, where the context surrounding polysemous words is available to the language model, but not for
information extraction. For tagging, we show number of tokens and accuracies. For IE, we show number of types,
categories, and AUCs.
tigate a set-expansion task in which we?re given a
corpus and a few ?seed? noun phrases from a se-
mantic category (e.g. Superheroes), and our goal is
to identify other examples of the category in the cor-
pus. This is a weakly-supervised task because we are
given only a handful of examples of the category,
rather than a large sample of positively and nega-
tively labeled training examples.
Existing set-expansion techniques utilize the dis-
tributional hypothesis: candidate noun phrases for a
given semantic class are ranked based on how sim-
ilar their contextual distributions are to those of the
seeds. Here, we measure how performance on the
set-expansion task varies when we employ different
representations for the contextual distributions.
6.1 Methods
The set-expansion task we address is formalized as
follows: given a corpus, a set of seeds from some
semantic category C, and a separate set of candidate
phrases P , output a ranking of the phrases in P in
decreasing order of likelihood of membership in C.
For any given representation R, the set-expansion
algorithm we investigate is straightforward: we cre-
ate a prototypical ?seed representation vector? equal
to the mean of the representation vectors for each
of the seeds. Then, we rank candidate phrases in
increasing order of the distance between the candi-
date phrase representation and the seed representa-
tion vector. As a measure of distance between rep-
resentations, we compute the average of five stan-
dard distance measures, including KL and Jensen-
Shannon divergence, and cosine, Euclidean, and L1
distance. In experiments, we found that improving
upon this simple averaging was not easy?in fact,
tuning a weighted average of the distance measures
for each representation did not improve results sig-
nificantly on held-out data.
Because set expansion is performed at the level
of word types rather than tokens, it requires type-
based representations. We compare HMM-TYPE-
R, NGRAM-R, LATTICE-TYPE-R, and BROWN-
TYPE-R in this experiment. We used a 25-state
HMM, and the same PL-MRF as in the previous
section. Following previous set-expansion experi-
ments with n-grams (Ahuja and Downey, 2010), we
employ a trigram model with Kneser-Ney smooth-
ing for NGRAM-R. For Brown clusters, instead of
distance metrics like KL divergence (which assume
distributions), we rank extractions by the number
of matches between a word?s BROWN-TYPE-R fea-
tures and seed features.
6.2 Data Sets
We utilized a set of approximately 100,000 sen-
tences of Web text, joining multi-word named enti-
ties in the corpus into single tokens using the Lex
algorithm (Downey et al, 2007a). This process
enables each named entity (the focus of the set-
expansion experiments) to be treated as a single to-
ken, with a single representation vector for compar-
ison. We developed all word type representations
131
model AUC
HMM-TYPE-R 0.18
BROWN-TYPE-R 0.16
LATTICE-TYPE-R 0.11
NGRAM-R 0.10
Random baseline 0.10
Table 4: HMM-TYPE-R outperforms the other methods,
improving performance by 12.5% over Brown clusters,
and by 80% over the traditional NGRAM-R.
using this corpus.
To obtain examples of multiple semantic cat-
egories, we utilized selected Wikipedia ?listOf?
pages from (Pantel et al, 2009) and augmented these
with our own manually defined categories, such that
each list contained at least ten distinct examples oc-
curring in our corpus. In all, we had 432 exam-
ples across 16 distinct categories such as Countries,
Greek Islands, and Police TV Dramas.
6.3 Results
For each semantic category, we tested five differ-
ent random selections of five seed examples, treating
the unselected members of the category as positive
examples, and all other candidate phrases as nega-
tive examples. We evaluate using the area under the
precision-recall curve (AUC) metric.
The results are shown in Table 4. All represen-
tations improve performance over a random base-
line, equal to the average AUC over five random or-
derings for each category, and the graphical models
outperform the ngram representation. HMM-TYPE-
R performs the best overall, and Brown clustering
with 1000 clusters is comparable (320 and 100 clus-
ter perform slightly worse).
As with POS tagging, we expect that language
model representations improve performance on the
IE task by providing informative features for sparse
word types. However, because the IE task classifies
word types rather than tokens, we expect the rep-
resentations to provide less benefit for polysemous
word types. To test these hypotheses, we measured
how IE performance changed in sparse or polyse-
mous settings. We identified polysemous categories
as those for which fewer than 90% of the category
members had the category as a clear dominant sense
(estimated manually); other categories were consid-
ered non-polysemous. Categories whose members
had a median number of occurrences in the cor-
pus less than 30 were deemed sparse, and others
non-sparse. IE performance on these subsets of the
data are shown in Table 3. Both graphical model
representations outperform the ngram representation
more on sparse words, as expected. For polysemy,
the picture is mixed: the PL-MRF outperform n-
grams on polysemous categories, whereas HMM?s
performance advantage over n-grams decreases.
One surprise on the IE task is that the LATTICE-
TYPE-R performs significantly less well than the
HMM-TYPE-R, whereas the reverse is true on POS
tagging. We suspect that the difference is due to the
issue of classifying types vs. tokens. Because of
their more complex structure, PL-MRFs tend to de-
pend more on transition parameters than do HMMs.
Furthermore, our decision to train the PL-MRFs
using contrastive estimation with a neighborhood
that swaps consecutive pairs of words also tends to
emphasize transition parameters. As a result, we
believe the posterior distribution over latent states
given a word type is more informative in our HMM
model than the PL-MRF model. We measured the
entropy of these distributions for the two models,
and found that H(PPL-MRF(y|x = w)) = 9.95 bits,
compared with H(PHMM(y|x = w)) = 2.74 bits,
which supports the hypothesis that the drop in the
PL-MRF?s performance on IE is due to its depen-
dence on transition parameters. Further experiments
are warranted to investigate this issue.
7 Conclusion and Future Work
Our investigation into language models as represen-
tations shows that graphical models can be used to
combat polysemy and, especially, sparsity in rep-
resentations for weakly-supervised classifiers. Our
novel factorial graphical model yields a state-of-the-
art POS tagger for domain adaptation, and HMMs
improve significantly over all other representations
in an information extraction task. Important direc-
tions for future research include models for han-
dling polysemy in IE, and richer language models
that incorporate more linguistic intuitions about how
words interact with their contexts.
Acknowledgments
This research was supported in part by NSF grant
IIS-1065397 and a Microsoft New Faculty Fellow-
ship.
132
References
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Proceedings of the Annual Meeting of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics (NAACL-HLT).
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jenn Wortman. 2009.
A theory of learning from different domains. Machine
Learning, (to appear).
Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In International Confer-
ence on Machine Learning (ICML).
Yoshua Bengio. 2008. Neural net language models.
Scholarpedia, 3(1):3881.
Daniel M. Bikel. 2004. Intricacies of Collins Parsing
Model. Computational Linguistics, 30(4):479?511.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
Hans L. Bodlaender. 1988. Dynamic programming on
graphs with bounded treewidth. In Proc. 15th Interna-
tional Colloquium on Automata, Languages and Pro-
gramming, pages 105?118.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,
and J. C. Lai. 1992. Class-based n-gram models of
natural language. Computational Linguistics, pages
467?479.
M. Candito and B. Crabbe. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT, pages 138?141.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning (ICML).
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series
B, 39(1):1?38.
D. Downey, M. Broadhead, and O. Etzioni. 2007a. Lo-
cating complex named entities in web text. In Procs.
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI 2007).
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007b. Sparse information extraction: Unsupervised
language models to the rescue. In ACL.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of EMNLP, pages 689?697.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2010.
Multi-domain learning by confidence weighted param-
eter combination. Machine Learning, 79.
Kevin Duh. 2005. Jointly labeling multiple sequences: A
Factorial HMM approach. In 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL 2005),
Student Research Workshop.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of HLT-NAACL, pages 602?610.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29(2-
3):245?273.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
D. Hindle. 1990. Noun classification from predicage-
argument structures. In ACL.
T. Honkela. 1997. Self-organizing maps of words for
natural language processing applications. In Proceed-
ings of the International ICSC Symposium on Soft
Computing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010a. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Fei Huang and Alexander Yates. 2010b. Open-domain
semantic role labeling by modeling word spans. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
133
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In ACL.
S. Kaski. 1998. Dimensionality reduction by random
mapping: Fast similarity computation for clustering.
In IJCNN, pages 413?418.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 595?603.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning.
D. Lin and X Wu. 2009. Phrase clustering for discrimi-
native learning. In ACL-IJCNLP, pages 1030?1038.
D.C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Do-
main adaptation with multiple sources. In Advances in
Neural Information Processing Systems.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
S. Martin, J. Liermann, and H. Ney. 1998. Algorithms
for bigram and trigram word clustering. Speech Com-
munication, 24:19?37.
A. Mnih and G. E. Hinton. 2009. A scalable hierarchi-
cal distributed language model. In Neural Information
Processing Systems (NIPS), pages 1081?1088.
P. Pantel, E. Crestan, A. Borkovsky, A. M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proc. of EMNLP.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu/.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 183?190.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?285.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Synactic Theory: A Formal Introduction. CSLI,
Stanford, CA, second edition.
M. Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Index-
ing Workshop at the 7th International Conference on
Terminology and Knowledge Engineering (TKE).
M. Sahlgren. 2006. The word-space model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics (ACL 2007), pages
760?767.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
354?362, Ann Arbor, Michigan, June.
Charles Sutton, Andrew McCallum, and Khashayar Ro-
hanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn. Res.,
8:693?723.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 384?394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
J. J. Va?yrynen, T. Honkela, and L. Lindqvist. 2007.
Towards explicit semantic features using independent
component analysis. In Proceedings of the Work-
shop Semantic Content Acquisition and Representa-
tion (SCAR).
Jason Weston, Frederic Ratle, and Ronan Collobert.
2008. Deep learning via semi-supervised embedding.
In Proceedings of the 25th International Conference
on Machine Learning.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In CoNLL 2009 Shared Task.
134
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 122?126,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Scoring Spoken Responses Based on Content Accuracy
Fei Huang
CS Dept. Temple Univ.
Philadelphia, PA, 19122
tub58431@temple.edu
Lei Chen
Educational Testing Service (ETS)
Princeton, NJ, 08541
lchen@ets.org
Jana Sukkarieh
ETS
JSukkarieh@ets.org
Abstract
Accuracy of content have not been fully uti-
lized in the previous studies on automated
speaking assessment. Compared to writing
tests, responses in speaking tests are noisy
(due to recognition errors), full of incomplete
sentences, and short. To handle these chal-
lenges for doing content-scoring in speaking
tests, we propose two new methods based
on information extraction (IE) and machine
learning. Compared to using an ordinary
content-scoring method based on vector anal-
ysis, which is widely used for scoring written
essays, our proposed methods provided con-
tent features with higher correlations to human
holistic scores.
1 Introduction
In recent years, there is an increasing interest of
using speech processing and natural language pro-
cessing (NLP) technologies to automatically score
speaking tests (Eskenazi, 2009). A set of features
related to speech delivery, such as fluency, pronun-
ciation, and intonation, has been utilized in these
studies. However, accuracy of an answer?s content
to the question being asked, important factors to be
considered during the scoring process, have not been
fully utilized. In this paper, we will report our ini-
tial efforts exploring content scoring in an automated
speaking assessment task. To start, we will briefly
describe the speaking test questions in our research.
In the test we used for evaluation, there were
two types of questions. The first type, survey,
requires a test-taker to provide answers specific
to one or several key points in a survey ques-
tion without any background reading/listening re-
lated to the topic of the survey. Typical questions
could be ?how frequently do you go shopping?? or
?what kind of products did you purchase recently??
In contrast, the second type, opinion, requires a test-
taker to speak as long as 60 seconds to present his
or her opinions about some topic. An example of
such questions could be, ?Do you agree with the
statement that online shopping will be dominant in
future or not?? Compared to the essays in writing
tests, these spoken responses could just be incom-
plete sentences. For example, for the survey ques-
tions, test-takers could just say several words. For
the questions described above, some test-takers may
just use phrases like ?once a week? or ?books?. In
addition, given short responding durations, the num-
ber of words in test-takers? responses is limited. Fur-
thermore, since scoring speech responses requires
speech recognition, more noisy inputs are expected.
To tackle these challenges, we propose two novel
content scoring methods in this paper.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the related previous re-
search efforts; Section 3 proposes the two content-
scoring methods we designed for two types of ques-
tions described above; Section 4 reports the experi-
mental results of applying the proposed methods; fi-
nally, Section 5 concludes our reported research and
describes our plans for future research.
2 Related Work
For writing tests, previous content scoring investiga-
tions can be divided into the following three groups.
The first group relies on obtaining and matching pat-
terns associated with the correct answers (Leacock
and Chodorow, 2003; Sukkarieh and Blackmore,
2009).
The second group of methods, also mostly used
122
for content-scoring, is to rely on a variety of text
similarity measurements to compare a response with
either pre-defined correct answers or a group of re-
sponses rated with a high score (Mohler and Mihal-
cea, 2009). Compared to the first group, such meth-
ods can bypass a labor intensive pattern-building
step. A widely used approach to measuring text
similarity between two text strings is to convert
each text string into a word vector and then use
the angle between these two vectors as a similar-
ity metric. For example, Content Vector Analy-
sis (CVA) has been successfully utilized to detect
off-topic essays (Higgins et al, 2006) and to pro-
vide content-related features for essay scoring (At-
tali and Burstein, 2004). For this group of meth-
ods, measuring the semantics similarity between two
terms is a key question. A number of metrics have
been proposed, including metrics (Courley and Mi-
halcea, 2005) derived from WordNet, a semantics
knowledge database (Fellbaum, 1998), and metrics
related to terms? co-occurrence in corpora or on the
Web (Turney, 2001).
The third group of methods treats content scor-
ing as a Text Categorization (TC) task, which treats
the responses being scored on different score levels
as different categories. Therefore, a large amount
of previous TC research, such as the many machine
learning approaches proposed for the TC task, can
be utilized. For example, Furnkranz et al (1998)
compared the performance of applying two machine
learning methods on a web-page categorization task
and found that the Repeated Incremental Pruning to
Produce Error Reduction algorithm (RIPPER) (Co-
hen, 1995) shows an advantage concerning the fea-
ture sparsity issue.
3 Methodology
As described in Section 1, for the two types of ques-
tions considered, the number of words appearing
in a response is quite limited given the short re-
sponse time. Therefore, compared to written es-
says, when applying the content-scoring methods
based on vector analysis, e.g., CVA, feature sparsity
becomes a major factor negatively influencing the
performance of these methods. Furthermore, there
are more challenges when applying vector analysis
on survey questions because test-takers could just
use words/phrases rather than completed sentences.
Also, some survey questions could have a very large
range of correct answers. For example, if a question
is about the name of a book, millions of book ti-
tles could be potential answers. Therefore, a simple
phrase-matching solution cannot work.
3.1 Semi-Automatic Information Extraction
For survey responses, the answers should be related
to the key points mentioned in the questions. For
example, for the question, ?What kind of TV pro-
grams do you like to watch??, possible correct an-
swers should be related to TV programs. Moreover,
it should be the instances of specific TV programs,
like news, comedy, talk shows, etc. Note that the ac-
ceptable answers may be infinite, so it is not realis-
tic to enumerate all possible answers. Therefore, we
proposed a method to extract the potential answer
candidates and then measure their semantic similar-
ities to the answer keys that could be determined
manually. In particular, the answer keys were deter-
mined by the first author based on her analysis of the
test prompts. For example, for the question ?What
kind of books do you like to read??, two answer keys,
?book? and ?reading? were selected. After a fur-
ther analysis of the questions, we found that most of
the survey questions are about ?when? ?where? and
?what?, and the answers in the responses were usu-
ally nouns or noun phrases. Therefore, we decided
to extract the noun phrases from each response and
use them as potential candidates.
We use two semantic similarity metrics (SSMs)
to evaluate how each candidate relates to an answer
key, including PMI-IR (Turney, 2001) and a word-
to-word similarity metric from WordNet (Courley
and Mihalcea, 2005). The PMI-IR is a measure
based on web query analysis using Pointwise Mutual
Information (PMI) and Information Retrieval (IR).
For an answer candidate (c) and an answer key (k),
their PMI-IR is computed as:
SSMPMI-IR(c, k) =
hits(cNEARk)
hits(c)
where the hits(x) function obtains the count of term
x returned by a web search engine and NEAR is a
query operator for proximity search, searching the
pages on which both k and c appear within a spec-
ified distance. Among many WordNet (WN) based
SSMs summarized in Courley and Mihalcea (2005),
123
we found that the Wu-Palmer metric proposed by
Wu and Palmer (1994) worked the best in our pilot
study. This metric is a score denoting how similar
two word senses are, based on the depth of the two
word senses in the taxonomy and their Least Com-
mon Subsumer 1 (LCS):
SSMWN(c, k) =
2 ? depth(LCS)
depth(c) + depth(k)
For each answer key, we calculated two sets of
SSMs (SSMPMI-IR and SSMWN , respectively)
from all candidates. Then, we selected the largest
SSMPMI-IR and SSMWN as the final SSMs for this
particular answer key. For each test question, using
the corresponding responses in the training set, we
built a linear regression model between these SSMs
for all answer keys and the human judged scores.
The learned regression model was applied to the re-
sponses to this particular testing question in the test-
ing set to convert a set of SSMs to predictions of
human scores. The predicted scores were then used
as a content feature. Since answer keys were deter-
mined manually, we refer to this method as semi-
automatic information extraction (Semi-IE).
3.2 Machine Learning Using Smoothed Inputs
For the opinion responses, inspired by Furnkranz
et al (1998), we decided to try sophisticated ma-
chine learning methods instead of the simple vector-
distance computation used in CVA. Due to short
response-time in the speaking test being considered,
the ordinary vector analysis may face a problem that
the obtained vectors are too short to be reliably used.
In addition, using other non-CVA machine learning
methods can enable us to try other types of linguis-
tic features. To address the feature sparsity issue, a
smoothing method, which converts word-based text
features into features based on other entities with
a much smaller vocabulary size, is used. We use
a Hidden Markov Model (HMM) based smooth-
ing method (Huang and Yates, 2009), which in-
duces classes, corresponding to hidden states in the
HMM model, from the observed word strings. This
smoothing method can use contextual information
of the word sequences due to the nature of HMM.
Then, we convert word-entity vectors to the vec-
tors based on the induced classes. TF-IDF (term
1Most specific ancestor node
frequency and inverse document frequency) weight-
ing is applied on the new class vectors. Finally,
the processed class vectors are used as input fea-
tures (smoothed) to a machine learning method. In
this research, after comparing several widely used
machine learning approaches, such as Naive Bayes,
CART, etc., we decided to use RIPPER proposed by
Cohen (1995), a rule induction method, similar to
Furnkranz et al (1998).
4 Experiments
Our experimental data was from a test for interna-
tional workplace English. Six testing papers were
used in our study and each individual test contains
three survey questions (1, 2, and 3) and two opin-
ion questions (4 and 5). Table 1 lists examples
for these question types. From the real test, we
collected spoken responses from a total of 1, 838
test-takers. 1, 470 test-takers were used for training
and 368 were used for testing. Following scoring
rubrics developed for this test by considering speak-
ers? various language skill aspects, such as fluency,
pronunciation, vocabulary, as well as content accu-
racy, the survey and opinion responses were scored
by a group of experienced human raters by using a
3-point scale and a 5-point scale respectively. For
the survey responses, the human judged scores were
centered on 2; for the opinion responses, the human
judged scores were centered on 3 and 4.
Qs. Example
1 How frequently do you go shopping?
2 What kinds of products do you buy often?
3 How should retailers improve their services?
4 Make a purchase decision based on the chart
provided and justify your decision.
5 Do you agree with the statement that online
shopping will be dominant in the future or
not? Please justify your point.
Table 1: Examples of the five kinds of questions investi-
gated in the study
All of these non-native speech responses were
manually transcribed. A state-of-the-art HMM Au-
tomatic Speech Recognition (ASR) system which
was trained from a large set of non-native speech
data was used. For each type of test question, acous-
tic and language model adaptations were applied
to further lower the recognition error rate. Finally,
124
a word error rate around 30% to 40% could be
achieved on the held-out speech data. In our exper-
iments, we used speech transcriptions in the model
training stage and used ASR outputs in the testing
stage. Note that we decided to use speech transcrip-
tions, instead of noisy ASR outputs that match to
the testing condition, to make sure that the learned
content-scoring model are based on correct word en-
tities related to content accuracy.
For the survey responses, we manually selected
the key points from the testing questions. Then,
using a Part-Of-Speech (POS) tagger and a sen-
tence chunker implemented by using the OpenNLP 2
toolkit, we found all possible nouns and noun-
phrases that could serve as answer candidates and
applied the Semi-IE method described in Sec-
tion 3.1. For opinion questions, based on Huang and
Yates (2009), we used 80 hidden states and applied
the method described in Section 3.2 for content scor-
ing. We used JRip, a Java implementation of the
RIPPER (Cohen, 1995) algorithm in the Weka (Hall
et al, 2009) machine learning toolkit, in our experi-
ments.
When measuring performance of content-related
features, following many automated assessment
studies (Attali and Burstein, 2004; Leacock and
Chodorow, 2003; Sukkarieh and Blackmore, 2009),
we used the Pearson correlation r between the con-
tent features and human scores as an evaluation met-
ric. We compared the proposed methods with a base-
line method, CVA. It works as follows: it first groups
all the training responses by scores, then it calculates
a TF vector from all the responses under a score
level. Also, an IDF matrix is generated from all
the training responses. After that, for each testing
response, CVA first converts it into a TF-IDF vec-
tor and then calculates the cosine similarity between
this vector with each score-level vector respectively
and uses the largest cosine similarity as the content
feature for that response. The experimental results,
including content-features? correlations r to human
scores from each proposed method and the correla-
tion increases measured on CVA results, are shown
in Table 2. First, we find that CVA, which is de-
signed for scoring lengthy written essays, does not
work well for the survey questions, especially on
2http://opennlp.sourceforge.net
Question rCV A rSemi?IE r ?
1 0.12 0.30 150%
2 0.15 0.27 80%
3 0.21 0.26 23.8%
Question rCV A rRipperHMM r ?
4 0.47 0.54 14.89%
5 0.33 0.39 18.18%
Table 2: Comparisons of the proposed content-scoring
methods with CVA on survey and opinion responses
first two questions, which are mostly phrases (not
completed sentences). By contrast, our proposed
Semi-IE method can provide more informative con-
tent measurements, indicated by substantially in-
creased r. Second, CVA works better on opinion
questions than on survey questions. This is because
that opinion questions can be treated as short spo-
ken essays and therefore are closer to the data on
which the CVA method was originally designed to
work. However, even on such a well-performing
CVA baseline, the HMM smoothing method allows
the Ripper algorithm to outperform the CVA method
in content-features? correlations to human scores.
For example, on question 4, on which either a table
or a chart has been provided to test-takers, the CVA
achieves a r of 0.47. The proposed method can still
improve the r by about 15%.
5 Conclusions and Future Works
In this paper, we proposed two content-scoring
methods for the two types of test questions in an
automated speaking assessment task. For particu-
lar properties of these two question types, we uti-
lized information extraction (IE) and machine learn-
ing technologies to better score them on content
accuracy. In our experiments, we compared these
two methods, Semi-IE and machine learning us-
ing smoothed inputs, with an ordinary word-based
vector analysis method, CVA. The content features
computed using the proposed methods show higher
correlations to human scores than what was obtained
by using the CVA method.
For the Semi-IE method, one direction of investi-
gation will be how to find the expected answer keys
automatically from testing questions. In addition,
we will investigate better ways to integrate many se-
125
mantic similarly measurements (SSMs) into a single
content feature. For the machine learning approach,
inspired by Furnkranz et al (1998), we will inves-
tigate how to use some linguistic features related to
response structures rather than just TF-IDF weights.
References
Y. Attali and J. Burstein. 2004. Automated essay scoring
with e-rater v.2.0. In Presented at the Annual Meet-
ing of the International Association for Educational
Assessment.
W. Cohen. 1995. Text categorization and relational
learning. In In Proceedings of the 12th International
Conference on Machine Learning.
C. Courley and R. Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, pages 13?18.
M. Eskenazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51(10):832?844.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
J. Furnkranz, T. Mitchell, and E. Riloff. 1998. A case
study in using linguistic phrases for text categorization
on the WWW. In Proceedings from the AAAI/ICML
Workshop on Learning for Text Categorization, page
512.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H Witten. 2009. The WEKA data min-
ing software: An update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
D. Higgins, J. Burstein, and Y. Attali. 2006. Identifying
off-topic student essays without topic-specific training
data. Natural Language Engineering, 12.
F. Huang and A. Yates. 2009. Distributional represen-
tations for handling sparsity in supervised sequence-
labeling. In Proceedings of ACL.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated scoring of short-answer questions. Computers
and the Humanities, 37(4):385?405.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 567?575.
J. Z. Sukkarieh and J. Blackmore. 2009. c-rater: Auto-
matic content scoring for short constructed responses.
In Paper presented at the Florida Artificial Intelli-
gence Research Society (FLAIRS) Conference, Sani-
bel, FL.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Procs. of the
Twelfth European Conference on Machine Learning
(ECML), pages 491?502, Freiburg, Germany.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexi-
cal selection. In Proceeding ACL ?94 Proceedings of
the 32nd annual meeting on Association for Computa-
tional Linguistics.
126
