Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614?1623,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Clustering Aspect-related Phrases by Leveraging Sentiment Distribution
Consistency
Li Zhao, Minlie Huang, Haiqiang Chen*, Junjun Cheng*, Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
National Laboratory for Information Science and Technology
Dept. of Computer Science and Technology, Tsinghua University, Beijing, PR China
*China Information Technology Security Evaluation Center
zhaoli19881113@126.com aihuang@tsinghua.edu.cn
Abstract
Clustering aspect-related phrases in terms
of product?s property is a precursor pro-
cess to aspect-level sentiment analysis
which is a central task in sentiment analy-
sis. Most of existing methods for address-
ing this problem are context-based models
which assume that domain synonymous
phrases share similar co-occurrence con-
texts. In this paper, we explore a novel
idea, sentiment distribution consistency,
which states that different phrases (e.g.
?price?, ?money?, ?worth?, and ?cost?) of
the same aspect tend to have consistent
sentiment distribution. Through formal-
izing sentiment distribution consistency as
soft constraint, we propose a novel unsu-
pervised model in the framework of Poste-
rior Regularization (PR) to cluster aspect-
related phrases. Experiments demonstrate
that our approach outperforms baselines
remarkably.
1 Introduction
Aspect-level sentiment analysis has become a cen-
tral task in sentiment analysis because it can ag-
gregate various opinions according to a product?s
properties, and provide much detailed, complete,
and in-depth summaries of a large number of re-
views. Aspect finding and clustering, a precursor
process of aspect-level sentiment analysis, has at-
tracted more and more attentions (Mukherjee and
Liu, 2012; Chen et al., 2013; Zhai et al., 2011a;
Zhai et al., 2010).
Aspect finding and clustering has never been a
trivial task. People often use different words or
phrases to refer to the same product property (also
called product aspect or feature in the literature).
Some terms are lexically dissimilar while seman-
tically close, which makes the task more challeng-
ing. For example, ?price?, ?money? , ?worth? and
?cost? all refer to the aspect ?price? in reviews.
In order to present aspect-specific summaries of
opinions, we first of all, have to cluster different
aspect-related phrases. It is expensive and time-
consuming to manually group hundreds of aspect-
related phrases. In this paper, we assume that the
aspect phrases have been extracted in advance and
we keep focused on clustering domain synony-
mous aspect-related phrases.
Existing studies addressing this problem are
mainly based on the assumption that different
phrases of the same aspect should have similar co-
occurrence contexts. In addition to the traditional
assumption, we develop a new angle to address the
problem, which is based on sentiment distribution
consistency assumption that different phrases of
the same aspect should have consistent sentiment
distribution, which will be detailed soon later.
Figure 1: A semi-structured Review.
This new angle is inspired by this simple obser-
vation (as illustrated in Fig. 1): two phrases within
the same cluster are not likely to be simultaneously
placed in Pros and Cons of the same review. A
straightforward way to use this information is to
formulate cannot-link knowledge in clustering al-
gorithms (Chen et al., 2013; Zhai et al., 2011b).
However, we have a particularly different manner
to leverage the knowledge.
Due to the availability of large-scale semi-
structured customer reviews (as exemplified in
Fig. 1) that are supported by many web sites,
we can easily get the estimation of sentiment dis-
tribution for each aspect phrase by simply count-
ing how many times a phrase appears in Pros and
1614
Cons respectively. As illustrated in Fig. 2, we
can see that the estimated sentiment distribution
of a phrase is close to that of its aspect. The
above observation suggests the sentiment distri-
bution consistency assumption: different phrases
of the same aspect tend to have the same senti-
ment distribution, or to have statistically close
distributions. This assumption is also verified by
our data: for most (above 91.3%) phrase with rela-
tively reliable estimation (whose occurrence?50),
the KL-divergence between the sentiment distri-
bution of a phrase and that of its corresponding
aspect is less than 0.05.
Figure 2: The sentiment distribution of aspect
?battery? and its related-phrases on nokia 5130
with a large amount of reviews.
It is worth noting that, the sentiment distribution
of a phrase can be estimated accurately only when
we obtain a sufficient number of reviews. When
the number of reviews is limited, however, the es-
timated sentiment distribution for each phrase is
unreliable (as shown in Fig. 3). A key issue,
arisen here, is how to formulate this assumption in
a statistically robust manner. The proposed model
should be robust when only a limited number of
reviews are available.
Figure 3: The sentiment distribution of aspect
?battery? and its related-phrases on nokia 3110c
with a small mumber of reviews.
To deal with this issue, we model sentiment dis-
tribution consistency as soft constraint, integrated
into a probabilistic model that maximizes the data
likelihood. We design the constraint to work in
the following way: when we have sufficient ob-
servations, the constraint becomes tighter, which
plays a more important role in the learning pro-
cess; when we have limited observations, the con-
straint becomes very loose so that it will have less
effect on the model.
In this paper, we propose a novel unsupervised
model, Sentiment Distribution Consistency Reg-
ularized Multinomial Naive Bayes (SDC-MNB).
The context part is modeled by Multinomial Naive
Bayes in which aspect is treated as latent variable,
and Sentiment distribution consistency is encoded
as soft constraint within the framework of Poste-
rior Regularization (PR) (Graca et al., 2008). The
main contributions of this paper are summarized
as follows:
? We study the problem of clustering phrases
by integrating both context information
and sentiment distribution of aspect-related
phrases.
? We explore a novel concept, sentiment distri-
bution consistency(SDC), and model it as soft
constraint to guide the clustering process.
? Experiments show that our model outper-
forms the state-of-art approaches for aspect
clustering.
The rest of this paper is organized as follows.
We introduce the SDC-MNB model in Section 2.
We present experiment results in Section 3. In
Section 4, we survey related work. We summarize
the work in Section 5.
2 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
In this section, we firstly introduce our assumption
sentiment distribution consistency formally and
show how to model the above assumption as soft
constraint , which we term SDC-constraint. Sec-
ondly, we show how to combine SDC-constraint
with the probabilistic context model. Finally, we
present the details for context and sentiment ex-
traction.
2.1 Sentiment Distribution Consistency
We define aspect as a set of phrases that refer to
the same property of a product and each phrase is
termed aspect-related phrase (or aspect phrase in
short). For example, the aspect ?battery? contains
aspect phrases such as ?battery?, ?battery life?,
?power?, and so on.
1615
F the aspect phrase set
f
j
the j
th
aspect phrase
y
j
the aspect for aspect phrase f
j
A the aspect set
a
i
the i
th
aspect
D the set of context documents
d
j
the context document of f
j
V the word vocabulary
w
t
the t
th
word in vocabulary V
w
d
j
,k
the k
th
word in d
j
N
tj
the number of times word w
t
occurs in d
j
P the product set
p
k
the k
th
product
u
ik
the sentiment distribution parameter
of aspect a
i
on p
k
s?
jk
the estimated sentiment distribution parameter
of phrase f
j
on p
k
n
jk
the occurrence times of aspect phrase f
j
on p
k
??
jk
the sample standard deviation
? the model parameters
p
?
(a
i
|d
j
) the posterior distribution of a
i
given d
j
q(y
j
= a
i
)
the projected posterior distribution
of a
i
given d
j
Table 1: Notations
Let us consider the sentiment distribution on a
certain aspect a
i
. In a large review dataset, as-
pect a
i
could receive many comments from differ-
ent reviewers. For each comment, we assume that
people either praise or complain about the aspect.
So each comment on the aspect can be seen as a
Bernoulli trial, where the aspect receives positive
comments with probability p
a
i
1
. We introduce a
random variable X
a
i
to denote the sentiment on
aspect a
i
, where X
a
i
= 1 means that aspect a
i
receives positive comments, X
a
i
= 0 means that
aspect a
i
receives negative comments. Obviously,
the sentiment on aspect a
i
follows the Bernoulli
distribution,
Pr(X
a
i
) = p
X
a
i
a
i
? (1 ? p
a
i
)
1?X
a
i
, X
a
i
? {0, 1}. (1)
Or in short,
X
a
i
? Bernoulli(p
a
i
)
Let us see the case for aspect phrase f
j
, where
f
j
? aspect a
i
. Similarly, each comment on an as-
pect phrase f
j
can also be seen as a Bernoulli trial.
We introduce a random variable X
f
j
to denote the
sentiment on aspect phrase f
j
, where X
f
j
= 1
means that aspect f
j
receives positive comments,
X
f
j
= 0 means that aspect f
j
receives negative
comments. As just discussed, we assume that each
aspect phrase follows the same distribution with
1
positive comment means that an aspect term is observed
in Pros of a review.
the corresponding aspect. This leads to the fol-
lowing formal description:
? Sentiment Distribution Consistency : The
sentiment distribution of aspect phrase is the
same as that of the corresponding aspect.
Formally, for all aspect phrase f
j
? aspect
a
i
, X
f
j
? Bernoulli(p
a
i
).
2.2 Sentiment Distribution Consistency
Constraint
Assuming the sentiment distribution of aspect a
i
is
given in advance, we need to judge whether an as-
pect phrase f
j
belongs to the aspect a
i
with limited
observations for f
j
. Let?s consider the example in
Fig. 4. For aspect phrase 3, we have no definite
answer due to the limited number of observations.
For aspect phrase 1, it seems that the sentiment
distribution is consistent with that of the left as-
pect. However, we can not say that the phrase be-
longs to the aspect because the distribution may
be the same for two different aspects. For aspect
phrase 2, we are confident that its sentiment dis-
tribution is different from that of the left aspect,
given sufficient observations.
Figure 4: Sentiment distribution of an aspect, and
observations on aspect phrases.
To be concise, we judge an aspect phrase
doesn?t belong to certain aspect only when we are
confident that they follow different sentiment dis-
tributions.
Inspired by the intuition, we conduct interval
parameter estimation for parameter p
f
j
(sentiment
distribution for phrase f
j
) with limited observa-
tions, and thus get a confidence interval for p
f
j
.
If p
a
i
(sentiment distribution for aspect a
i
) is not
in the confidence interval of p
f
j
, we then are con-
fident that they follow different distributions. In
other words, if aspect phrase f
j
? aspect a
i
, we
are confident that p
a
i
is in the confidence interval
of p
f
j
.
More formally, we use u
ik
to denote the senti-
ment distribution parameter of aspect a
i
on prod-
uct p
k
, and assume that u
ik
is given in advance.
1616
We want to know whether the sentiment distribu-
tion on aspect phrase f
j
is the same as that of as-
pect a
i
on product p
k
given a limited number of
observations (samples). It?s straightforward to cal-
culate the confidence interval for parameter s
jk
in
the Bernoulli distribution function. Let the sam-
ple mean of n
jk
samples be s?
jk
, and the sample
standard deviation be ??
jk
. Since the sample size
is small here, we use the Student-t distribution to
calculate the confidence interval. According to our
assumption, we are confident that u
ik
is in the con-
fidence interval if f
j
? a
i
.
s?
jk
?C
??
jk
?
n
jk
? u
ik
? s?
jk
+C
??
jk
?
n
jk
, ?f
j
? a
i
,?k. (2)
where we look for t-table to find C corresponding
to a certain confidence level(such as 95%) with the
freedom of n
jk
? 1. For simplicity, we represent
the above confidence interval by [s?
jk
? d
jk
, s?
jk
+
d
jk
], where d
jk
= C
??
jk
?
n
jk
.
We introduce an indicator variable z
ij
to repre-
sent whether the aspect phrase f
j
belongs to aspect
a
i
, as follows:
z
ji
=
{
1 ; if f
j
? a
i
0 ; otherwise
(3)
This leads to our SDC-constraint function.
? = z
ji
|u
ik
? s?
jk
| ? d
jk
,?i, j, k (4)
SDC-constraint are flexible for modeling Senti-
ment Distribution Consistency. The more obser-
vations we have, the smaller d
jk
is. For frequent
aspect phrase, the constraint can be very informa-
tive because it can filter unrelated aspects for as-
pect phrase f
j
. The less observations we have,
the larger d
jk
is. For rare aspect phrases, the con-
straint can be very loose, and will not have much
effect on the clustering process for aspect phrase
f
j
. In this way, the model can work very robustly.
SDC-constraints are data-driven constraints.
Usually we have many reviews about hundreds of
products in our dataset. For each aspect phrase,
there are |A| ? |P | constraints (the number of as-
pects times the number of product). With thou-
sands of constraints about which aspect it is not
likely to belong to, the model learns to which as-
pect a phrase f
j
should be assigned. Although
most constraints may be loose because of the lim-
ited observations, SDC-constraint can still play an
important role in the learning process.
2.3 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
(SDC-MNB)
In this section, we present our probabilistic model
which employs both context information and sen-
timent distribution.
First of all, we extract a context document d
for each aspect phrase, which will be described in
Section 2.5. In other word, a phrase is represented
by its context document. Assuming that the doc-
uments in D are independent and identically dis-
tributed, the probability of generating D is then
given by:
p
?
(D) =
|D|
?
j=1
p
?
(d
j
) =
|D|
?
j=1
?
y
j
?A
p
?
(d
j
, y
j
) (5)
where y
j
is a latent variable indicating the aspect
label for aspect phrase f
j
, and ? is the model pa-
rameter.
In our problem, we are actually more inter-
ested in the posterior distribution over aspect,
i.e., p
?
(y
j
|d
j
). Once the learned parameter ? is
obtained, we can get our clustering result from
p
?
(y
j
|d
j
), by assigning aspect a
i
with the largest
posterior to phrase f
j
. We can also enforce SDC-
constraint in expectation(on posterior p
?
). We use
q(Y ) to denote the valid posterior distribution that
satisfy our SDC-constraint, and Q to denote the
valid posterior distribution space, as follows:
Q = {q(Y ) : E
q
[z
ji
|u
ik
? s?
jk
|] ? d
jk
, ?i, j, k}. (6)
Since posterior plays such an important role in
joining the context model and SDC-constraint, we
formulate our problem in the framework of Poste-
rior Regularization (PR). PR is an efficient frame-
work to inject constraints on the posteriors of la-
tent variables. Instead of restricting p
?
directly,
which might not be feasible, PR penalizes the dis-
tance of p
?
to the constraint set Q. The posterior-
regularized objective is termed as follows:
max
?
{log p
?
(D) ? min
q?Q
KL(q(Y )||p
?
(Y |D))} (7)
By trading off the data likelihood of the ob-
served context documents (as defined in the first
term), and the KL divergence of the posteriors
to the valid posterior subspace defined by SDC-
constraint (as defined in the second term), the ob-
jective encourages models with both desired pos-
terior distribution and data likelihood. In essence,
the model attempts to maximize data likelihood of
context subject (softly) to SDC-constraint.
1617
2.3.1 Multinomial Naive Bayes
In spirit to (Zhai et al., 2011a), we use Multino-
mial Naive Bayes (MNB) to model the context
document. Let w
d
j
,k
denotes the k
th
word in doc-
ument d
j
, where each word is from the vocabulary
V = {w
1
, w
2
, ..., w
|V |
}. For each aspect phrase
f
j
, the probability of its latent aspect being a
i
and
generating context document d
i
is
p
?
(d
j
, y
j
= a
i
) = p(a
i
)
|d
j
|
?
k=1
p(w
d
j
,k
|a
i
) (8)
where p(a
i
) and p(w
d
j
,k
|a
i
) are parameters of this
model. Each word w
d
j
,k
is conditionally indepen-
dent of all other words given the aspect a
i
.
Although MNB has been used in existing work
for aspect clustering, all of the studies used it in
a semi-supervised manner, with labeled data or
pseudo-labeled data. In contrast, MNB proposed
here is used in an unsupervised manner for aspect-
related phrases clustering.
2.3.2 SDC-constraint
As mentioned above, the constraint posterior setQ
is defined by
Q = {q(Y ) : q(y
j
= a
i
)|u
ik
? s?
jk
| ? d
jk
,?i, j, k}. (9)
We can see that Q denotes a set of linear con-
straints on the projected posterior distribution q.
Note that we do not directly observe u
ik
, the sen-
timent distribution of aspect a
i
on product p
k
. For
aspect phrase f
j
that belongs to aspect a
i
, we es-
timate u
ik
by counting all sentiment samples. We
use the posterior p
?
(a
i
|d
j
) to approximately rep-
resent how likely phrase f
j
belongs to aspect a
i
.
u
ik
=
1
?
|D|
j=1
n
jk
p
?
(a
i
|d
j
)
|D|
?
j=1
n
jk
p
?
(a
i
|d
j
)s?
jk
(10)
where p
?
(a
i
|d
j
) is short for p
?
(y
j
= a
i
|d
j
), the
probability that aspect phrase f
j
belongs to a
i
given the context document d
j
. We estimate u
ik
in
this way because observations for aspect are rela-
tively sufficient for a reliable estimation since ob-
servations for an aspect are aggregated from those
for all phrases belonging to that aspect.
2.4 The Optimization Algorithm
The optimization algorithm for the objective (see
Eq. 7) is an EM-like two-stage iterative algorithm.
In E-step, we first calculate the posterior distri-
bution p
?
(a
i
|d
j
), then project it onto the valid pos-
terior distribution space Q. Given the parameters
?, the posterior distribution can be calculated by
Eq. 11.
p
?
(a
i
|d
j
) =
p(a
i
)
?
|d
j
|
k=1
p(w
d
j
,k
|a
i
)
?
|A|
r=1
p(a
r
)
?
|d
j
|
k=1
p(w
d
j
,k
|a
r
)
(11)
We use the above posterior distribution to update
the sentiment parameter for each aspect by Eq. 10.
The projected posterior distribution q is calculated
by
q = argmin
q?Q
KL(q(Y )||p
?
(Y |D)) (12)
For each instance, there are |A| ? |P | constraints.
However, we can prune a large number of useless
constraints derived from limited observations. All
constraints with d
jk
> 1 can be pruned, due to
the fact that the parameter u
ik
, s?
jk
is within [0,1],
and the difference can not be larger than 1. This
optimization problem in Eq. 12 is easily solved via
the dual form by the projected gradient algorithm
(Boyd and Vandenberghe, 2004):
max
??0
(
?
|A|
?
i=1
|P |
?
k=1
?
ik
d
jk
?
log
|A|
?
i=1
p
?
(a
i
|d
j
)exp{?
|P |
?
k=1
?
ik
|u
ik
? s?
jk
|} ? ????
)
(13)
where ? controls the slack size for constraint. After
solving the above optimization problem and ob-
taining the optimal ?, we can calculate the pro-
jected posterior distribution q by
q(y
j
= a
i
) =
1
Z
p
?
(a
i
|d
j
)exp{?
|P |
?
k=1
?
ik
|u
ik
?s?
jk
|} (14)
where Z is the normalization factor. Note that sen-
timent distribution consistency is actually modeled
as instance-level constraint here, which makes it
very efficient to solve.
In M-step, the projected posteriors q(Y ) are
then used to compute sufficient statistics and up-
date the models parameters ?. Given the projected
posteriors q(Y ), the parameters can be updated by
Eq. 15,16.
p(a
i
) =
1 +
?
|D|
j=1
q(y
j
= a
i
)
|A| + |D|
(15)
p(w
t
|a
i
) =
1 +
?
|D|
j=1
N
ti
q(y
j
= a
i
)
|V | +
?
|V |
m=1
?
|D|
j=1
N
mj
q(y
j
= a
i
)
(16)
where N
tj
is the number of times that the word w
t
occurs in document d
j
.
The parameters are initialized randomly, and we
repeat E-step and M-step until convergence.
1618
2.5 Data Extraction
2.5.1 Context Extraction
In order to extract the context document d for each
aspect phrase, we follow the approach in Zhai et
al. (2011a). For each aspect phrase, we generate
its context document by aggregating the surround-
ing texts of the phrase in all reviews. The preced-
ing and following t words of a phrase are taken as
the context where we set t = 3 in this paper. Stop-
words and other aspect phrases are removed. For
example, the following review contains two aspect
phrases, ?screen? and ?picture?,
The LCD screen gives clear picture.
For ?screen?, the surrounding texts are {the,
LCD, gives, clear, picture}. We remove stop-
words ?the?, and the aspect term ?picture?, and
the resultant context of ?screen? in this review is
context(screen) ={LCD, screen, gives, clear}.
Similarly, the context of ?picture? in this review is
context(picture) ={gives, clear}.
By aggregating the contexts of all the reviews
that contain aspect phrase f
j
, we obtain the cor-
responding context document d
j
.
2.5.2 Sentiment Extraction
Since we use semi-structured reviews, we ob-
tain the estimated sentiment distribution by sim-
ply counting how many times each aspect phrase
appears in Pros and Cons reviews for each prod-
uct respectively. So for each aspect phrase f
j
, let
n
+
jk
denotes the times that f
j
appears in Pros of
all reviews for product p
k
, and let n
?
jk
denotes the
times that f
j
appears in Cons of all reviews for
product p
k
. So the total number of occurrence of a
phrase is n
jk
= n
+
jk
+ n
?
jk
. We have samples like
(1,1,1,0,0) where 1 means a phrase occurs in Pros
of a review, and 0 in Cons. Given a sequence of
such observations, the sample mean is easily com-
puted as s?
jk
=
n
+
jk
n
+
jk
+n
?
jk
. And the sample standard
deviation is ??
jk
=
?
(1?s?
jk
)
2
?n
+
jk
+(s?
jk
)
2
?n
?
jk
n
jk
?1
.
3 Experiments
3.1 Data Preparation
The details of our review corpus are given
in Table 2. This corpus contains semi-
structured customer reviews from four do-
mains: Camera, Cellphone, Laptop, and MP3.
These reviews were crawled from the following
web sites: www.amazon.cn, www.360buy.com,
www.newegg.com.cn, and www.zol.com. The as-
pect label of each aspect phrases is annotated by
human curators.
Camera Cellphone Laptop MP3
#Products 449 694 702 329
#Reviews 101,235 579,402 102,439 129,471
#Aspect Phrases 236 230 238 166
#Aspect 12 10 14 8
Table 2: Statistics of the review corpus. # denotes
the size.
3.2 Evaluation Measures
We adapt three measures Purity, Entropy, and
Rand Index for performance evaluation. These
measures have been commonly used to evaluate
clustering algorithms.
Given a data set DS, suppose its gold-standard
partition is G = {g
1
, ..., g
j
, ..., g
k
}, where k
is the number of clusters. A clustering algo-
rithm partitions DS into k disjoint subsets, say
DS
1
, DS
2
, ..., DS
k
.
Entropy: For each resulting cluster, we can mea-
sure its entropy using Eq. 17, where P
i
(g
j
) is the
proportion of data points of class g
j
in DS
i
. The
entropy of the entire clustering result is calculated
by Eq. 18.
entropy(DS
i
) = ?
k
?
j=1
P
i
(g
j
)log
2
P
i
(g
j
) (17)
entropy(DS) =
k
?
i=1
|DS
i
|
|DS|
entropy(DS
i
) (18)
Purity: Purity measures the extent that a cluster
contains only data from one gold-standard parti-
tion. The cluster purity is computed with Eq. 19.
The total purity of the whole clustering result (all
clusters) is computed with Eq. 20.
purity(DS
i
) = max
j
P
i
(g
j
) (19)
purity(DS) =
k
?
i=1
|DS
i
|
|DS|
purity(DS
i
) (20)
RI: The Rand Index(RI) penalizes both false posi-
tive and false negative decisions during clustering.
Let TP (True Positive) denotes the number of pairs
of elements that are in the same set in DS and in
the same set in G. TN (True Negative) denotes
number of pairs of elements that are in different
sets in DS and in different sets in G. FP (False
1619
Camera Cellphone Laptop MP3
P RI E P RI E P RI E P RI E
Kmeans 43.48% 83.52% 2.098 48.91% 84.80% 1.792 43.46% 87.11% 2.211 40.00% 70.98% 2.047
L-EM 54.89% 87.07% 1.690 51.96% 86.64% 1.456 48.94% 84.53% 2.039 44.24% 75.37% 1.990
LDA 36.84% 83.28% 2.426 48.65% 85.33% 1.833 35.02% 83.53% 2.660 36.12% 76.08% 2.296
Constraint-LDA 43.30% 86.01% 2.216 47.89% 86.04% 1.974 32.35% 84.86% 2.676 50.70% 81.42% 1.924
SDC-MNB 56.42% 88.16% 1.725 67.95% 90.62% 1.266 55.52% 90.72% 1.780 58.06% 83.57% 1.578
Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random
index.)
Positive) denotes number of pairs of elements in
S that are in the same set in DS and in different
sets in G. FN (False Negative) denotes number of
pairs of elements that are in different sets in DS
and in the same set in G. The Rand Index(RI) is
computed with Eq. 21.
RI(DS) =
TP + TN
TP + TN + FP + FN
(21)
3.3 Evaluation Results
3.3.1 Comparison to unsupervised baselines
We compared our approach with several existing
unsupervised methods. Some of the methods aug-
mented unsupervised models by incorporating lex-
ical similarity and other domain knowledge. All
of them are context-based models.
2
We list these
models as follows.
? Kmeans: Kmeans is the most popular cluster-
ing algorithm. Here we use the context distri-
butional similarity (cosine similarity) as the
similarity measure.
? L-EM: This is a state-of-the-art unsupervised
method for clustering aspect phrases (Zhai et
al., 2011a). L-EM employed lexical knowl-
edge to provide a better initialization for EM.
? LDA: LDA is a popular topic model(Blei et
al., 2003). Given a set of documents, it out-
puts groups of terms of different topics. In
our case, each aspect phrase is processed as a
term.
3
Each sentence in a review is consid-
ered as a document. Each aspect is consid-
ered as a topic. In LDA, a term may belong
to more than one topic/group, but we take the
topic/group with the maximum probability.
2
In our method, we collect context document for each
aspect phrase. This process is conducted for L-EM and K-
means. But for LDA and Constraint-LDA, we take each sen-
tence of reviews as a document. This setting for the LDA
baselines is adapted from previous work.
3
Each aspect phrase is pre-processed as a single word
(e.g., ?battery life? is treated as battery-life). Other words
are normally used in LDA.
? Constraint-LDA: Constraint-LDA (Zhai et
al., 2011b) is a state-of-the-art LDA-based
method that incorporates must-link and
cannot-link constraints for this task. We set
the damping factor ? = 0.3 and relaxation
factor ? = 0.9, as suggested in the original
reference.
For all methods that depend on the random ini-
tiation, we use the average results of 10 runs as the
final result. For all LDA-based models, we choose
? = 50/T , ? = 0.1, and run 1000 iterations.
Experiment results are shown in Table 3. We
can see that our approach almost outperforms all
unsupervised baseline methods by a large margin
on all domains. In addition, we have the following
observations:
? LDA and Kmeans perform poorly due to the
fact that the two methods do not use any prior
knowledge. It is also shown that only using
the context distributional information is not
sufficient for clustering aspect phrases.
? Constraint-LDA and L-EM that utilize prior
knowledge perform better. We can see that
Constraint-LDA outperforms LDA in terms
of RI (Rand Index) on all domains. L-EM
achieves the best results against the baselines.
This demonstrates the effectiveness to incor-
porate prior knowledge.
? SDC-MNB produces the optimal results
among all models for clustering. Methods
that use must-links and cannot-links may suf-
fer from noisy links. For L-EM, we find
that it is sensitive to noisy must-links. As
L-EM assumes that must-link is transitive,
several noisy must-links may totally misla-
bel the softly annotated data. For Constraint-
LDA, it is more robust than L-EM, because
it doesn?t assume the transitivity of must-
link. However, it only promotes the RI (Rand
Index) consistently by leveraging pair-wise
prior knowledge, but sometimes it hurts the
1620
performance with respect to purity or en-
tropy. Our method is consistently better on
almost all domains, which shows the advan-
tages of the proposed model.
? SDC-MNB is remarkably better than base-
lines, particularly for the cellphone domain.
We argue that this is because we have the
largest number of reviews for each product
in the cellphone domain. The larger dataset
gives us more observations on each phrase,
so that we obtain more reliable estimation of
model parameters.
3.3.2 Comparison to supervised baselines
We further compare our methods with two super-
vised models. For each supervised model, we
provide a proportion of manually labeled data for
training, which is randomly selected from gold-
standard annotations. However, we didn?t use any
labeled data for our approach.
? MNB: The labeled seeds are used to train a
MNB classifier to classify all unlabeled as-
pect phrases into different classes.
? L-Kmeans: In L-Kmeans, the clusters of the
labeled seeds are fixed at the initiation and
remain unchanged during iteration.
Purity RI Entropy
MNB-5% 53.21% 85.77% 1.854
MNB-10% 59.55% 86.70% 1.656
MNB-15% 66.06% 88.39% 1.449
L-Kmeans-10% 53.54% 86.15% 1.745
L-Kmeans-15% 57.00% 86.89% 1.643
L-Kmeans-20% 60.97% 87.63% 1.528
SDC-MNB 59.49% 88.26% 1.580
Table 4: Comparison to supervised baselines.
MNB-5% means MNB with 5% labeled data.
We experiment with several settings: taking
5%, 10% and 15% of the manually labeled aspect
phrases for training, and the remainder as unla-
beled data. Experiment results is shown in Table
4 (the results are averaged over 4 domains). We
can see that our unsupervised approach is roughly
as good as the supervised MNB with 10% labeled
data. Our unsupervised approach is also slightly
better than L-Kmeans with 15% labeled data. This
result further demonstrates the effectiveness of our
model.
3.3.3 Influence of parameters
We vary the confidence level from 90% to 99.9%
to see how it impacts on the performance of SDC-
MNB. The results are presented in Fig. 5 (the re-
sults are averaged over 4 domains). We can see
that the performance of clustering is fairly stable
when changing the confidence level, which im-
plies the robustness of our model.
Figure 5: Influence of the confidence level on
SDC-MNB.
3.3.4 Analysis of SDC-constraint
As mentioned in Section 2.2, SDC-constraint is
dependent on the number of observations. More
observations we get, more informative the con-
straint is, which means the constraint is tighter and
d
jk
(see Eq.4) is smaller. For all k, we count how
many d
jk
is less than 0.2 (and 1) on average for
each aspect phrase f
j
. d
jk
is calculated with a
confidence level of 99%. The statistics of con-
straints is given in Table 5. We can see that the
cellphone domain has the most informative and
largest constraint set, that may explain why SDC-
MNB achieves the largest purity gain(over L-EM)
in cellphone domain.
#(d
jk
< 0.2) #(0.2 < d
jk
< 1) purity gain
Camera 3.02 8.78 1.53%
Cellphone 17.29 30.5 15.99%
Laptop 4.6 13.22 6.58%
MP3MP4 6.1 10.7 13.82%
Table 5: Constraint statistics on different domains.
4 Related Work
Our work is related to two important research
topics: aspect-level sentiment analysis, and
constraint-driven learning. For aspect-level senti-
ment analysis, aspect extraction and clustering are
key tasks. For constraint-driven learning, a variety
of frameworks and models for sentiment analysis
have been studied extensively.
There have been many studies on clustering
aspect-related phrases. Most existing studies are
1621
based on context information. Some works also
encoded lexical similarity and synonyms as prior
knowledge. Carenini et al. (2005) proposed a
method that was based on several similarity met-
rics involving string similarity, synonyms, and lex-
ical distances defined with WordNet. Guo et al.
(2009) proposed a multi-level latent semantic as-
sociation model to capture expression-level and
context-level topic structure. Zhai et al. (2010)
proposed an EM-based semi-supervised learning
method to group aspect expressions into user-
specified aspects. They employed lexical knowl-
edge to provide a better initialization for EM. In
Zhai et al. (2011a), an EM-based unsupervised
version was proposed. The so-called L-EM model
first generated softly labeled data by grouping fea-
ture expressions that share words in common, and
then merged the groups by lexical similarity. Zhai
et al. (2011b) proposed a LDA-based method
that incorporates must-link and cannot-link con-
straints.
Another line of work aimed to extract and clus-
ter aspect words simultaneously using topic mod-
eling. Titov and McDonald (2008) proposed the
multi-grain topic models to discover global and
local aspects. Branavan et al. (2008) proposed
a method which first clustered the key-phrases
in Pros and Cons into some aspect categories
based on distributional similarity, then built a topic
model modeling the topics or aspects. Zhao et al.
(2010) proposed the MaxEnt-LDA (a Maximum
Entropy and LDA combination) hybrid model to
jointly discover both aspect words and aspect-
specific opinion words, which can leverage syn-
tactic features to separate aspects and sentiment
words. Mukherjee and Liu (2012) proposed a
semi-supervised topic model which used user-
provided seeds to discover aspects. Chen et al.
(2013) proposed a knowledge-based topic model
to incorporate must-link and cannot-link informa-
tion. Their model can adjust topic numbers auto-
matically by leveraging cannot-link.
Our work is also related to general constraint-
driven(or knowledge-driven) learning models.
Several general frameworks have been proposed to
fully utilize various prior knowledge in learning.
Constraint-driven learning (Chang et al., 2008)
(CODL) is an EM-like algorithm that incorpo-
rates per-instance constraints into semi-supervised
learning. Posterior regularization (Graca et al.,
2007) (PR) is a modified EM algorithm in which
the E-step is replaced by the projection of the
model posterior distribution onto the set of dis-
tributions that satisfy auxiliary expectation con-
straints. Generalized expectation criteria (Druck
et al., 2008) (GE) is a framework for incorporating
preferences about model expectations into param-
eter estimation objective functions. Liang et al.
(2009) developed a Bayesian decision-theoretic
framework to learn an exponential family model
using general measurements on the unlabeled data.
In this paper, we model our problem in the frame-
work of posterior regularization.
Many works promoted the performance of sen-
timent analysis by incorporating prior knowledge
as weak supervision. Li and Zhang (2009) in-
jected lexical prior knowledge to non-negative ma-
trix tri-factorization. Shen and Li (2011) further
extended the matrix factorization framework to
model dual supervision from both document and
word labels. Vikas Sindhwani (2008) proposed a
general framework for incorporating lexical infor-
mation as well as unlabeled data within standard
regularized least squares for sentiment prediction
tasks. Fang (2013)proposed a structural learning
model with a handful set of aspect signature terms
that are encoded as weak supervision to extract la-
tent sentiment explanations.
5 Conclusions
Aspect finding and clustering is an important task
for aspect-level sentiment analysis. In order to
cluster aspect-related phrases, this paper has ex-
plored a novel concept, sentiment distribution con-
sistency. We formalize the concept as soft con-
straint, integrate the constraint with a context-
based probabilistic model, and solve the problem
in the posterior regularization framework. The
proposed model is also designed to be robust with
both sufficient and insufficient observations. Ex-
periments show that our approach outperforms
state-of-the-art baselines consistently.
Acknowledgments
This work was partly supported by the following
grants from: the National Basic Research Program
(973 Program) under grant No.2012CB316301
and 2013CB329403, the National Science Foun-
dation of China project under grant No.61332007
and No. 61272227, and the Beijing Higher Educa-
tion Young Elite Teacher Project.
1622
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993?1022, March.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press, New
York, NY, USA.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Association for Computational
Linguistics (ACL).
Giuseppe Carenini, Raymond T. Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In Proceedings of the 3rd International Conference
on Knowledge Capture, K-CAP ?05, pages 11?18,
New York, NY, USA. ACM.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with
constraints. In Proceedings of the 23rd National
Conference on Artificial Intelligence - Volume 3,
AAAI?08, pages 1513?1518. AAAI Press.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Mal Castellanos, and Riddhiman Ghosh. 2013.
Exploiting domain knowledge in aspect extraction.
In EMNLP, pages 1655?1667. ACL.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In Proceedings of
the 31st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?08, pages 595?602, New York,
NY, USA. ACM.
Lei Fang, Minlie Huang, and Xiaoyan Zhu. 2013. Ex-
ploring weakly supervised latent sentiment expla-
nations for aspect-level review analysis. In Qi He,
Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev
Rastogi, editors, CIKM, pages 1057?1066. ACM.
Joao V. Graca, Lf Inesc-id, Kuzman Ganchev, Ben
Taskar, Joo V. Graa, L F Inesc-id, Kuzman Ganchev,
and Ben Taskar. 2007. Expectation maximization
and posterior constraints. In In Advances in NIPS,
pages 569?576.
Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang,
and Zhong Su. 2009. Product feature categorization
with multilevel latent semantic association. In Pro-
ceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management, CIKM ?09, pages
1087?1096, New York, NY, USA. ACM.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to senti-
ment classification with lexical prior knowledge. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
244?252, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, ICML ?09,
pages 641?648, New York, NY, USA. ACM.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chao Shen and Tao Li. 2011. A non-negative matrix
factorization based approach for active dual super-
vision from document and word labels. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?11, pages 949?
958, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Vikas Sindhwani and Prem Melville. 2008.
Document-word co-regularization for semi-
supervised sentiment analysis. In ICDM, pages
1025?1030. IEEE Computer Society.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International Conference on
World Wide Web, WWW ?08, pages 111?120, New
York, NY, USA. ACM.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, COLING ?10, pages 1272?1280,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011a. Clustering product features for opinion min-
ing. In Proceedings of the Fourth ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ?11, pages 347?354, New York, NY, USA.
ACM.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011b. Constrained lda for grouping product fea-
tures in opinion mining. In Proceedings of the 15th
Pacific-Asia Conference on Advances in Knowl-
edge Discovery and Data Mining - Volume Part
I, PAKDD?11, pages 448?459, Berlin, Heidelberg.
Springer-Verlag.
Wayne X. Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 56?
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
1623
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 531?541,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
New Word Detection for Sentiment Analysis
Minlie Huang, Borui Ye*, Yichen Wang, Haiqiang Chen**, Junjun Cheng**, Xiaoyan Zhu
State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science
and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China
*Dept. of Communication Engineering, Beijing University of Posts and Telecommunications
**China Information Technology Security Evaluation Center
aihuang@tsinghua.edu.cn
Abstract
Automatic extraction of new words is
an indispensable precursor to many NLP
tasks such as Chinese word segmentation,
named entity extraction, and sentimen-
t analysis. This paper aims at extract-
ing new sentiment words from large-scale
user-generated content. We propose a ful-
ly unsupervised, purely data-driven frame-
work for this purpose. We design statisti-
cal measures respectively to quantify the
utility of a lexical pattern and to measure
the possibility of a word being a newword.
The method is almost free of linguistic re-
sources (except POS tags), and requires
no elaborated linguistic rules. We also
demonstrate how new sentiment word will
benefit sentiment analysis. Experiment re-
sults demonstrate the effectiveness of the
proposed method.
1 Introduction
New words on the Internet have been emerg-
ing all the time, particularly in user-generated con-
tent. Users like to update and share their infor-
mation on social websites with their own language
styles, among which new political/social/cultural
words are constantly used.
However, such new words have made many
natural language processing tasks more challeng-
ing. Automatic extraction of new words is indis-
pensable to many tasks such as Chinese word seg-
mentation, machine translation, named entity ex-
traction, question answering, and sentiment analy-
sis. New word detection is one of the most critical
issues in Chinese word segmentation. Recent stud-
ies (Sproat and Emerson, 2003) (Chen, 2003) have
shown that more than 60% of word segmentation
errors result from new words. Statistics show that
more than 1000 new Chinese words appear every
year (Thesaurus Research Center, 2003). These
words are mostly domain-specific technical terms
and time-sensitive political/social /cultural terms.
Most of them are not yet correctly recognized by
the segmentation algorithm, and remain as out of
vocabulary (OOV) words.
New word detection is also important for sen-
timent analysis such as opinionated phrase ex-
traction and polarity classification. A sentiment
phrase with complete meaning should have a cor-
rect boundary, however, characters in a new word
may be broken up. For example, in a sentence
" ??/ n ??/ adv ?/ v ?/ n?artists' perfor-
mance is very impressive?" the two Chinese char-
acters??/v?/n(cool; powerful)?should always
be extracted together. In polarity classification,
new words can be informative features for clas-
sification models. In the previous example, "?
?(cool; powerful)" is a strong feature for clas-
sification models while each single character is
not. Adding new words as feature in classification
models will improve the performance of polarity
classification, as demonstrated later in this paper.
This paper aims to detect new word for senti-
ment analysis. We are particulary interested in ex-
tracting new sentiment word that can express opin-
ions or sentiment, which is of high value toward-
s sentiment analysis. New sentiment word, as ex-
emplified in Table 1, is a sub-class of multi-word
expressions which is a sequence of neighboring
words "whose exact and unambiguous meaning
or connotation cannot be derived from the mean-
ing or connotation of its components" (Choueka,
1988). Such new words cannot be directly iden-
tified using grammatical rules, which poses a ma-
jor challenge to automatic analysis. Moreover, ex-
isting lexical resources never have adequate and
timely coverage since new words appear constant-
ly. People thus resort to statistical methods such as
Pointwise Mutual Information (Church and Han-
ks, 1990), Symmetrical Conditional Probability
531
(da Silva and Lopes, 1999), Mutual Expectation
(Dias et al, 2000), Enhanced Mutual Information
(Zhang et al, 2009), and Multi-word Expression
Distance (Bu et al, 2010).
New word English Translation Polarity
?? lovely positive
?? tragic/tragedy negative
?? very cool; powerful positive
?? reverse one's expectation negative
Table 1: Examples of new sentiment word.
Our central idea for new sentiment word de-
tection is as follows: Starting from very few seed
words (for example, just one seed word), we can
extract lexical patterns that have strong statistical
association with the seed words; the extracted lex-
ical patterns can be further used in finding more
new words, and the most probable new words can
be added into the seed word set for the next iter-
ation; and the process can be run iteratively un-
til a stop condition is met. The key issues are to
measure the utility of a pattern and to quantify the
possibility of a word being a new word. The main
contributions of this paper are summarized as fol-
lows:
? We propose a novel framework for new word
detection from large-scale user-generated da-
ta. This framework is fully unsupervised
and purely data-driven, and requires very
lightweight linguistic resources (i.e., only
POS tags).
? We design statistical measures to quantify the
utility of a pattern and to quantify the possi-
bility of a word being a newword, respective-
ly. No elaborated linguistic rules are needed
to filter undesirable results. This feature may
enable our approach to be portable to other
languages.
? We investigate the problem of polarity predic-
tion of new sentiment word and demonstrate
that inclusion of new sentiment word benefits
sentiment classification tasks.
The rest of the paper is structured as follows:
we will introduce related work in the next section.
Wewill describe the proposedmethod in Section 3,
including definitions, the overview of the algorith-
m, and the statistical measures for addressing the
two key issues. We then present the experiments
in Section 4. Finally, the work is summarized in
Section 5.
2 Related Work
New word detection has been usually inter-
weaved with word segmentation, particularly in
Chinese NLP. In these works, new word detection
is considered as an integral part of segmentation,
where new words are identified as the most proba-
ble segments inferred by the probabilistic models;
and the detected new word can be further used to
improve word segmentation. Typical models in-
clude conditional random fields proposed by (Peng
et al, 2004), and a joint model trained with adap-
tive online gradient descent based on feature fre-
quency information (Sun et al, 2012).
Another line is to treat new word detection as
a separate task, usually preceded by part-of-speech
tagging. The first genre of such studies is to lever-
age complex linguistic rules or knowledge. For
example, Justeson and Katz (1995) extracted tech-
nical terminologies from documents using a regu-
lar expression. Argamon et al (1998) segmented
the POS sequence of a multi-word into small POS
tiles, counted tile frequency in the new word and
non-new-word on the training set respectively, and
detected new words using these counts. Chen and
Ma (2002) employed morphological and statisti-
cal rules to extract Chinese new word. The sec-
ond genre of the studies is to treat new word de-
tection as a classification problem. Zhou (2005)
proposed a discriminative Markov Model to de-
tect new words by chunking one or more separat-
ed words. In (Li et al, 2005), new word detec-
tion was viewed as a binary classification problem.
However, these supervisedmodels requires not on-
ly heavy engineering of linguistic features, but also
expensive annotation of training data.
User behavior data has recently been explored
for finding new words. Zheng et al (2009) ex-
plored user typing behaviors in Sogou Chinese
Pinyin input method to detect new words. Zhang
et al (2010) proposed to use dynamic time warp-
ing to detect new words from query logs. Howev-
er, both of the work are limited due to the public
unavailability of expensive commercial resources.
Statistical methods for new word detection
have been extensively studied, and in some sense
exhibit advantages over linguistics-based method-
s. In this setting, new word detection is mostly
532
known as multi-word expression extraction. To
measure multi-word association, the first model
is Pointwise Mutual Information (PMI) (Church
and Hanks, 1990). Since then, a variety of sta-
tistical methods have been proposed to measure
bi-gram association, such as Log-likelihood (Dun-
ning, 1993) and Symmetrical Conditional Proba-
bility (SCP) (da Silva and Lopes, 1999). Among
all the 84 bi-gram association measures, PMI has
been reported to be the best one in Czech data
(Pecina, 2005). In order to measure arbitrary n-
grams, most common strategies are to separate n-
gram into two parts X and Y so that existing bi-
gram methods can be used (da Silva and Lopes,
1999; Dias et al, 2000; Schone and Jurafsky,
2001). Zhang et al (2009) proposed Enhanced
Mutual Information (EMI) which measures the co-
hesion of n-gram by the frequency of itself and the
frequency of each single word. Based on the in-
formation distance theory, Bu et al (2010) pro-
posed multi-word expression distance (MED) and
the normalized version, and reported superior per-
formance to EMI, SCP, and other measures.
3 Methodology
3.1 Definitions
Definition 3.1 (Adverbial word). Words that are
used mainly to modify a verb or an adjective, such
as "?(too)", "??(very)", "??(very)", and "?
?(specially)".
Definition 3.2 (Auxiliary word). Words that are
auxiliaries, model particles, or punctuation marks.
In Chinese, such words are like "?,?,?,?,?",
and punctuation marks include "??????" and
so on.
Definition 3.3 (Lexical Pattern). A lexical pat-
tern is a triplet < AD, ?, AU >, where AD is an
adverbial word, the wildcard ? means an arbitrary
number of words 1, and AU denotes an auxiliary
word.
Table 2 gives some examples of lexical pat-
terns. In order to obtain lexical patterns, we can
define regular expressions with POS tags 2 and ap-
ply the regular expressions on POS tagged texts.
Since the tags of adverbial and auxiliary words are
1We set the number to 3 words in this work considering
computation costs.
2Such expressions are very simple and easy to write be-
cause we only need to consider POS tags of adverbial and
auxiliary word.
relatively static and can be easily identified, such
a method can safely obtain lexical patterns.
Pattern Frequency
<"?",*,"?"> 562,057
<"?",*,"?"> 387,649
<"?",*,"?"> 380,470
<"?",*,"?"> 369,702
Table 2: Examples of lexical pattern. The frequen-
cy is counted on 237,108,977 Weibo posts.
3.2 The Algorithm Overview
The algorithm works as follows: starting
from very few seed words (for example, a word
in Table 1), the algorithm can find lexical pattern-
s that have strong statistical association with the
seed words in which the likelihood ratio test (L-
RT) is used to quantify the degree of association.
Subsequently, the extracted lexical patterns can be
further used in finding more new words. We de-
sign several measures to quantify the possibility of
a candidate word being a new word, and the top-
ranked words will be added into the seed word set
for the next iteration. The process can be run iter-
atively until a stop condition is met. Note that we
do not augment the pattern set (P) at each iteration,
instead, we keep a fixed small number of patterns
during iteration because this strategy produces op-
timal results.
From linguistic perspectives, new sentiment
words are commonly modified by adverbial words
and thus can be extracted by lexical patterns. This
is the reason why the algorithm will work. Our al-
gorithm is in spirit to double propagation (Qiu et
al., 2011), however, the differences are apparen-
t in that: firstly, we use very lightweight linguis-
tic information (except POS tags); secondly, our
major contributions are to propose statistical mea-
sures to address the following key issues: first, to
measure the utility of lexical patterns; second, to
measure the possibility of a candidate word being
a new word.
3.3 Measuring the Utility of a Pattern
The first key issue is to quantify the utility of
a pattern at each iteration. This can be measured
by the association of a pattern to the current word
set used in the algorithm. The likelihood ratio test-
s (Dunning, 1993) is used for this purpose. This
association model has also been used to model as-
sociation between opinion target words by (Hai et
533
Algorithm 1: New word detection algorithm
Input:
D: a large set of POS tagged posts
W
s
: a set of seed words
k
p
: the number of patterns chosen at each
iteration
k
c
: the number of patterns in the candidate
pattern set
k
w
: the number of words added at each
iteration
K: the number of words returned
Output: A list of ranked new wordsW
1 Obtain all lexical patterns using regular
expressions on D;
2 Count the frequency of each lexical pattern
and extract words matched by each pattern ;
3 Obtain top k
c
frequent patterns as candidate
pattern set P
c
and top 5,000 frequent words as
candidate word setW
c
;
4 P = ?;W=W
s
; t = 0 ;
5 for |W| < K do
6 UseW to score each pattern in P
c
with
U(p) ;
7 P = {top k
p
patterns} ;
8 Use P to extract new words and if the
words are inW
c
, score them with F (w) ;
9 W = W
?
{top k
w
words} ;
10 W
c
=W
c
-W ;
11 Sort words inW with F (w) ;
12 Output the ranked list of words inW ;
al., 2012).
The LRT is well known for not relying crit-
ically on the assumption of normality, instead, it
uses the asymptotic assumption of the generalized
likelihood ratio. In practice, the use of likelihood
ratios tends to result in significant improvements
in text-analysis performance.
In our problem, LRT computes a contingency
table of a pattern p and a word w, derived from
the corpus statistics, as given in Table 3, where
k
1
(w, p) is the number of documents thatwmatch-
es pattern p, k
2
(w, p?) is the number of documents
that w occurs while p does not, k
3
(w?, p) is the
number of documents that p occurs while w does
not, and k
4
(w?, p?) is the number of documents con-
taining neither p nor w.
Statistics p p?
w k
1
(w, p) k
2
(w, p?)
w? k
3
(w?, p) k
4
(w?, p?)
Table 3: Contingency table for likelihood ratio test
(LRT).
Based on the statistics shown in Table 3, the
likelihood ratio tests (LRT) model captures the sta-
tistical association between a pattern p and a word
w by employing the following formula:
LRT (p, w) = log
L(?
1
, k
1
, n
1
) ? L(?
2
, k
2
, n
2
)
L(?, k
1
, n
1
) ? L(?, k
2
, n
2
)
(1)
where:
L(?, k, n) = ?
k
? (1 ? ?)
n?k; n
1
= k
1
+ k
3
;
n
2
= k
2
+ k
4
; ?
1
= k
1
/n
1
; ?
2
= k
2
/n
2
; ? =
(k
1
+ k
2
)/(n
1
+ n
2
).
Thus, the utility of a pattern can be measured
as follows:
U(p) =
?
w
i
?W
LRT (p, w
i
) (2)
where W is the current word set used in the algo-
rithm (see Algorithm 1).
3.4 Measuring the Possibility of Being New
Words
Another key issue in the proposed algorithm
is to quantify the possibility of a candidate word
being a new word. We consider several factors for
this purpose.
3.4.1 Likelihood Ratio Test
Very similar to the pattern utility measure, L-
RT can also be used to measure the association of
a candidate word to a given pattern set, as follows:
LRT (w) =
?
p
i
?P
LRT (w, p
i
) (3)
where P is the current pattern set used in the algo-
rithm (see Algorithm 1), and p
i
is a lexical pattern.
This measure only quantifies the association
of a candidate word to the given pattern set. It
tells nothing about the possibility of a word be-
ing a new word, however, a new sentiment word,
should have close association with the lexical pat-
terns. This has linguistic interpretations because
new sentiment words are commonly modified by
adverbial words and thus should have close associ-
ation with lexical patterns. This measure is proved
to be an influential factor by our experiments in
Section 4.3.
534
3.4.2 Left Pattern Entropy
If a candidate word is a new word, it will be
more commonly used with diversified lexical pat-
terns since the non-compositionality of new word
means that the word can be used in many differ-
ent linguistic scenarios. This can be measured by
information entropy, as follows:
LPE(w) = ?
?
l
i
?L(P
c
,w)
c(l
i
, w)
N(w)
? log
c(l
i
, w)
N(w)
(4)
where L(P
c
, w) is the set of left word of all pat-
terns by which word w can be matched in P
c
,
c(l
i
, w) is the count that word w can be matched
by patterns whose left word is l
i
, and N(w) is the
count that word w can be matched by the patterns
in P
c
. Note that we use P
c
, instead of P , because
the latter set is very small while computing entropy
needs a large number of patterns. Tuning the size
of P
c
will be further discussed in Section 4.4.
3.4.3 New Word Probability
Some words occur very frequently and can be
widely matched by lexical patterns, but they are
not new words. For example, "??(love to eat)"
and "??(love to talk)" can be matched by many
lexical patterns, however, they are not new words
due to the lack of non-compositionality. In such
words, each single character has high probability
to be a word. Thus, we design the following mea-
sure to favor this observation.
NWP (w) =
n
?
i=1
p(w
i
)
1? p(w
i
)
(5)
where w = w
1
w
2
. . . w
n
, each w
i
is a single char-
acter, and p(w
i
) is the probability of the character
w
i
being a word, as computed as follows:
p(w
i
) =
all(w
i
)? s(w
i
)
all(w
i
)
where all(w
i
) is the total frequency of w
i
, and
s(w
i
) is the frequency of w
i
being a single char-
acter word. Obviously, in order to obtain the value
of s(w
i
), some particular Chinese word segmen-
tation tool is required. In this work, we resort to
ICTCLAS (Zhang et al, 2003), a widely used tool
in the literature.
3.4.4 Non-compositionality Measures
New words are usually multi-word expres-
sions, where a variety of statistical measures have
been proposed to detect multi-word expressions.
Thus, such measures can be naturally incorporated
into our algorithm.
The first measure is enhanced mutual infor-
mation (EMI) (Zhang et al, 2009):
EMI(w) = log
2
F/N
?
n
i=1
F
i
?F
N
(6)
where F is the number of posts in which a multi-
word expression w = w
1
w
2
. . . w
n
occurs, F
i
is
the number of posts where w
i
occurs, andN is the
total number of posts. The key idea of EMI is to
measure word pair?s dependency as the ratio of its
probability of being a multi-word to its probability
of not being amulti-word. The larger the value, the
more possible the expression will be a multi-word
expression.
The second measure we take into account is
normalized multi-word expression distance (Bu et
al., 2010), which has been proposed to measure the
non-compositionality of multi-word expressions.
NMED(w) =
log|?(w)| ? log|?(w)|
logN ? log|?(w)|
(7)
where ?(w) is the set of documents in which all
single words in w = w
1
w
2
. . . w
n
co-occur, ?(w)
is the set of documents in which word w occurs
as a whole, and N is the total number of docu-
ments. Different from EMI, this measure is a strict
distance metric, meaning that a smaller value in-
dicates a larger possibility of being a multi-word
expression. As can be seen from the formula, the
key idea of this metric is to compute the ratio of the
co-occurrence of all words in a multi-word expres-
sions to the occurrence of the whole expression.
3.4.5 Configurations to Combine Various
Factors
Taking into account the aforementioned fac-
tors, we have different settings to score a new
word, as follows:
F
LRT
(w) = LRT (w) (8)
F
LPE
(w) = LRT (w) ? LPE(w) (9)
F
NWP
(w) = LRT (w) ? LPE(w) ?NWP (w) (10)
F
EMI
(w) = LRT (w) ? LPE(w) ? EMI(w) (11)
F
NMED
(w) =
LRT (w) ? LPE(w)
NMED(w)
(12)
535
4 Experiment
In this section, we will conduct the following
experiments: first, we will compare our method
to several baselines, and perform parameter tun-
ing with extensive experiments; second, we will
classify polarity of new sentiment words using t-
wo methods; third, we will demonstrate how new
sentiment words will benefit sentiment classifica-
tion.
4.1 Data Preparation
We crawled 237,108,977 Weibo posts from
http://www.weibo.com, the largest social website
in China. These posts range from January of 2011
to December of 2012. The posts were then part-of-
speech tagged using a Chinese word segmentation
tool named ICTCLAS (Zhang et al, 2003).
Then, we asked two annotators to label the top
5,000 frequent words that were extracted by lexi-
cal patterns as described in Algorithm 1. The an-
notators were requested to judge whether a candi-
date word is a new word, and also to judge the po-
larity of a new word (positive, negative, and neu-
tral). If there is a disagreement on either of the
two tasks, discussions are required to make the fi-
nal decision. The annotation led to 323 new word-
s, among which there are 116 positive words, 112
negative words, and 95 neutral words3.
4.2 Evaluation Metric
As our algorithm outputs a ranked list of
words, we adapt average precision to evaluate
the performance of new sentiment word detection.
The metric is computed as follows:
AP (K) =
?
K
k=1
P (k) ? rel(k)
?
K
k=1
rel(k)
where P (k) is the precision at cut-off k, rel(k) is
1 if the word at position k is a new word and 0 oth-
erwise, andK is the number of words in the ranked
list. A perfect list (all topK items are correct) has
an AP value of 1.0.
4.3 Evaluation of Different Measures and
Comparison to Baselines
First, we assess the influence of likelihood ra-
tio test, which measures the association of a word
to the pattern set. As can be seen from Table 4,
the associationmodel (LRT) remarkably boosts the
3All the resources are available upon request.
performance of new word detection, indicating L-
RT is a key factor for new sentiment word extrac-
tion. From linguistic perspectives, new sentiment
words are commonly modified by adverbial words
and thus should have close association with lexical
patterns.
Second, we compare different settings of our
method to two baselines. The first one is en-
hanced mutual information (EMI) where we set
F (w) = EMI(w) (Zhang et al, 2009) and the
second baseline is normalized multi-word expres-
sion distance (NMED) (Bu et al, 2010) where we
set F (w) = NMED(w). The results are shown
in Figure 1. As can be seen, all the proposed
measures outperform the two baselines (EMI and
NMED) remarkably and consistently. The set-
ting of F
NMED
produces the best performance.
AddingNMED orEMI leads to remarkable im-
provements because of their capability of measur-
ing non-compositionality of new words. Only us-
ingLRT can obtain a fairly good results whenK is
small, however, the performance drops sharply be-
cause it's unable to measure non-compositionality.
Comparison between LRT + LPE (or LRT +
LPE + NWP ) and LRT shows that inclusion
of left pattern entropy also boosts the performance
apparently. However, the new word probabili-
ty (NWP ) has only marginal contribution to im-
provement.
In the above experiments, we set k
p
= 5 (the
number of patterns chosen at each iteration) and
k
w
= 10 (the number of words added at each iter-
ation), which is the optimal setting and will be dis-
cussed in the next subsection. And only one seed
word "??(reverse one's expectation)" is used.
Figure 1: Comparative results of different measure
settings. X-axis is the number of words returned
(K), and Y-axis is average precision (AP (K)).
536
top K words ? 100 200 300 400 500
LPE 0.366 0.324 0.286 0.270 0.259
LRT+LPE 0.743 0.652 0.613 0.582 0.548
LPE+NWP 0.467 0.400 0.350 0.330 0.320
LRT+LPE+NWP 0.755 0.680 0.612 0.571 0.543
LPE+EMI 0.608 0.551 0.519 0.486 0.467
LRT+LPE+EMI 0.859 0.759 0.717 0.662 0.632
LPE+NMED 0.749 0.690 0.641 0.612 0.576
LRT+LPE+NMED 0.907 0.808 0.741 0.723 0.699
Table 4: Results with vs. without likelihood ratio test (LRT).
4.4 Parameter Tuning
Firstly, we will show how to obtain the op-
timal settings of k
p
and k
w
. The measure setting
we take here is F
NMED
(w), as shown in Formula
(12). Again, we choose only one seed word "?
?(reverse one's expectation)", and the number of
words returned is set to K = 300. Results in Ta-
ble 5 show that the performance drops consistent-
ly across different k
w
settings when the number of
patterns increases. Note that at the early stage of
Algorithm 1, larger k
p
(perhaps with noisy pattern-
s) may lead to lower quality of new words; while
larger k
w
(perhaps with noisy seed words) may
lead to lower quality of lexical patterns. Therefore,
we choose the optimal setting to small numbers, as
k
p
= 5, k
w
= 10.
Secondly, we justify whether the proposed al-
gorithm is sensitive to the number of seed words.
We set k
p
= 5 and k
w
= 10, and take F
NMED
as the weighting measure of new word. We exper-
imented with only one seed word, two, three, and
four seed words, respectively. The results in Ta-
ble 6 show very stable performance when different
numbers of seed words are chosen. It's interesting
that the performance is totally the same with dif-
ferent numbers of seed words. By looking into the
pattern set and the selected words at each iteration,
we found that the pattern set (P) converges soon
to the same set after a few iterations; and at the be-
ginning several iterations, the selected words are
almost the same although the order of adding the
words is different. Since the algorithm will finally
sort the words at step (11) and P is the same, the
ranking of the words becomes all the same.
Lastly, we need to decide the optimal number
of patterns in P
c
(that is, k
c
in Algorithm 1) be-
cause the set has been used in computing left pat-
tern entropy, see Formula (4). Too small size of
P
c
may lead to insufficient estimation of left pat-
tern entropy. Results in Table 7 shows that larg-
er P
c
decrease the performance, particularly when
the number of words returned (K) becomes larger.
Therefore, we set |P
c
| = 100.
4.5 Polarity Prediction of New Sentiment
Words
In this section, we attempt to classifying the
polarity of the annotated 323 new words. Two
methods are adapted with different settings for this
purpose. The first one is majority vote (MV), and
the second one is pointwise mutual information,
similar to (Turney and Littman, 2003). The ma-
jority vote method is formulated as below:
MV (w) =
?
w
p
?PW
#(w,w
p
)
|PW |
?
?
w
n
?NW
#(w,w
n
)
|NW |
where PW and NW are a positive and negative
set of emoticons (or seed words) respectively, and
#(w,w
p
) is the co-occurrence count of the input
wordw and the itemw
p
. The polarity is judged ac-
cording to this rule: ifMV (w) > th
1
, the word w
is positive; ifMV (w) < ?th
1
the word negative;
otherwise neutral. The threshold th
1
is manually
tuned.
And PMI is computed as follows:
PMI(w) =
?
w
p
?PW
PMI(w,w
p
)
|PW |
?
?
w
n
?NW
PMI(w,w
n
)
|NW |
where PMI(x, y) = log
2
(
Pr(x,y)
Pr(x)?Pr(y)
), and
Pr(?) denotes probability. The polarity is judged
according to the rule: if PMI(w) > th
2
, w is
positive; if PMI(w) < ?th
2
negative; otherwise
neutral. The threshold th
2
is manually tuned.
As for the resources PW and NW , we
have three settings. The first setting (denoted by
537
HH
H
H
H
H
k
w
k
p 2 3 4 5 10 20 50
5 0.753 0.738 0.746 0.741 0.741 0.734 0.715
10 0.753 0.738 0.746 0.741 0.741 0.728 0.712
15 0.753 0.738 0.746 0.741 0.754 0.734 0.718
20 0.763 0.738 0.744 0.749 0.749 0.735 0.717
Table 5: Parameter tuning results for k
p
and k
w
. The measure setting is F
NMED
(w), the seed word set
is {"??(reverse one's expectation)"}, and the number of words returned isK = 300.
# seeds ? 1 2 3 4
K=100 0.907 0.907 0.907 0.907
K=200 0.808 0.808 0.808 0.808
K=300 0.741 0.741 0.741 0.741
K=400 0.709 0.709 0.709 0.709
K=500 0.685 0.685 0.685 0.685
Table 6: Performance with different numbers of
seed words. The measure setting is F
NMED
(w),
and k
p
= 5, k
w
= 10. The seed words are chosen
from Table 1.
Large_Emo) is a set of most frequent 36 emoticons
in which there are 21 positive and 15 negative e-
moticons respectively. The second one (denoted
by Small_Emo) is a set of 10 emoticons, which
are chosen from the 36 emoticons, as shown in
Table 8. The third one (denoted by Opin_Words)
is two sets of seed opinion words, where PW={
??(happy),??(generous),??(beautiful), ?
?(kind),??(smart)} and NW ={??(sad),?
?(mean),??(ugly),??(wicked),?(stupid)}.
The performance of polarity prediction is
shown in Table 9. In two-class polarity classifi-
cation, we remove neutral words and only make
prediction with positive/negative classes. The first
observation is that the performance of using emoti-
cons is much better than that of using seed opin-
ion words. We conjecture that this may be be-
cause new sentiment words are more frequently
co-occurring with emoticons than with these opin-
ion words. The second observation is that three-
class polarity classification is much more diffi-
cult than two-class polarity classification because
many extracted new words are nouns such as "?
?(gay)","??(girl)", and "??(friend)". Such
nouns are more difficult to classify sentiment ori-
entation.
4.6 Application of New Sentiment Words to
Sentiment Classification
In this section, we justifywhether inclusion of
new sentiment word would benefit sentiment clas-
sification. For this purpose, we randomly sampled
and annotated 4,500 Weibo posts that contain at
least one opinion word in the union of the Hownet
4 opinion lexicons and our annotated new word-
s. We apply two models for polarity classification.
The first model is a lexicon-based model (denot-
ed by Lexicon) that counts the number of positive
and negative opinion words in a post respective-
ly, and classifies a post to be positive if there are
more positive words than negative ones, and to be
negative otherwise. The second model is a SVM
model in which opinion words are used as feature,
and 5-fold cross validation is conducted.
We experiment with different settings of
Hownet lexicon resources:
? Hownet opinion words (denoted by Hownet):
After removing some obviously inappropri-
ate words, the left lexicons have 627 posi-
tive opinion words and 1,038 negative opin-
ion words, respectively.
? Compact Hownet opinion words (denoted by
cptHownet): we count the frequency of the
above opinion words on the training data and
remove words whose document frequency is
less than 2. This results in 138 positive words
and 125 negative words.
Then, we add into the above resources the la-
beled new polar words(denoted byNW , including
116 positive and 112 negative words) and the top
100 words produced by the algorithm (denoted by
T100), respectively. Note that the lexicon-based
model requires the sentiment orientation of each
dictionary entry 5, we thus manually label the po-
4http://www.keenage.com/html/c_index.html.
5This is not necessary for the SVM model. All words in
the top 100 words can be used as feature.
538
|P
c
| ? 50 100 200 300 400 500
K=100 0.907 0.905 0.916 0.916 0.888 0.887
K=200 0.808 0.810 0.778 0.776 0.766 0.764
K=300 0.741 0.731 0.722 0.726 0.712 0.713
K=400 0.709 0.708 0.677 0.675 0.656 0.655
K=500 0.685 0.683 0.653 0.646 0.626 0.627
Table 7: Tuning the number of patterns in P
c
. The measure setting is F
NMED
(w), k
p
= 5, k
w
= 10,
and the seed word set is {"??(reverse one's expectation)"}.
Emoticon Polarity Emoticon Polarity
positive negative
positive negative
positive negative
positive negative
positive negative
Table 8: The ten emoticons used for polarity pre-
diction.
Methods? Majority vote PMI
Two-class polarity classification
Large_Emo 0.861 0.865
Small_Emo 0.846 0.851
Opin_Words 0.697 0.654
Three-class polarity classification
Large_Emo 0.598 0.632
Small_Emo 0.551 0.635
Opin_Words 0.449 0.486
Table 9: The accuracy of two/three-class polarity
classification.
larity of all top 100 words (we did NOT remove
incorrect new word). This results in 52 positive
and 34 negative words.
Results in Table 10 show that inclusion of
new words in both models improves the perfor-
mance remarkably. In the setting of the original
lexicon (Hownet), both models obtain 2-3% gains
from the inclusion of newwords. Similar improve-
ment is observed in the setting of the compact lex-
icon. Note, that T100 is automatically obtained
from Algorithm 1 so that it may contain words that
are not new sentiment words, but the resource also
improves performance remarkably.
5 Conclusion
In order to extract new sentiment words from
large-scale user-generated content, this paper pro-
poses a fully unsupervised, purely data-driven, and
# Pos/Neg Lexicon SVM
Hownet 627/1,038 0.737 0.756
Hownet+NW 743/1,150 0.770 0.779
Hownet+T100 679/1,172 0.761 0.774
cptHownet 138/125 0.738 0.758
cptHownet+NW 254/237 0.774 0.782
cptHownet+T100 190/159 0.764 0.775
Table 10: The accuracy of polarity classfication of
Weibo post with/without new sentiment words. N-
W includes 116/112 positive/negative words, and
T100 contains 52/34 positive/negative words.
almost knowledge-free (except POS tags) frame-
work. We design statistical measures to quantify
the utility of a lexical pattern and to measure the
possibility of a word being a new word, respec-
tively. The method is almost free of linguistic re-
sources (except POS tags), and does not rely on
elaborated linguistic rules. We conduct extensive
experiments to reveal the influence of different sta-
tistical measures in new word finding. Compara-
tive experiments show that our proposed method
outperforms baselines remarkably. Experiments
also demonstrate that inclusion of new sentiment
words benefits sentiment classification definitely.
From linguistic perspectives, our framework
is capable to extract adjective new words because
the lexical patterns usually modify adjective word-
s. As future work, we are considering how to ex-
tract other types of new sentiment words, such as
nounal new words that can express sentiment.
Acknowledgments
This work was partly supported by the fol-
lowing grants from: the National Basic Re-
search Program (973 Program) under grant No.
2012CB316301 and 2013CB329403, the National
Science Foundation of China project under grant
No. 61332007 and No. 60803075, and the Beijing
Higher Education Young Elite Teacher Project.
539
References
Shlomo Argamon, Ido Dagan, and Yuval Krymolows-
ki. 1998. A memory-based approach to
learning shallow natural language patterns. In
Proceedings of the 17th International Conference
on Computational Linguistics - Volume 1, COL-
ING '98, pages 67--73, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expres-
sions. In Proceedings of the 23rd International
Conference on Computational Linguistics, COL-
ING '10, pages 116--124, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Un-
known word extraction for chinese documents. In
Proceedings of the 19th International Conference
on Computational Linguistics - Volume 1, COL-
ING '02, pages 1--7, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Aitao Chen. 2003. Chinese word segmentation us-
ingminimal linguistic knowledge. In Proceedings
of the Second SIGHAN Workshop on Chinese
Language Processing - Volume 17, SIGHAN '03,
pages 148--151, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yaacov Choueka. 1988. Looking for nee-
dles in a haystack or locating interesting col-
location expressions in large textual databas-
es. In Proceeding of the RIAO'88 Conference
on User-Oriented Content-Based Text and Image
Handling, pages 21--24.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lex-
icography. Comput. Linguist., 16(1): 22--29,
March.
J Ferreira da Silva and G Pereira Lopes. 1999. A local
maxima method and a fair dispersion normaliza-
tion for extracting multi-word units from corpora.
In Sixth Meeting on Mathematics of Language,
pages 369--381.
Ga?l Dias, Sylvie Guillor?, and Jos? Gabriel Pereira
Lopes. 2000. Mining textual associations in text
corpora. 6th ACM SIGKDD Work. Text Mining.
TedDunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Comput. Linguist.,
19(1):61--74, March.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012.
One seed to find them all: Mining opinion fea-
tures via association. In Proceedings of the 21st
ACM International Conference on Information
and Knowledge Management, CIKM '12, pages
255--264, New York, NY, USA. ACM.
John S Justeson and SlavaMKatz. 1995. Technical ter-
minology: some linguistic properties and an algo-
rithm for identification in text. Natural language
engineering, 1(1):9--27.
Hongqiao Li, Chang-Ning Huang, Jianfeng Gao, and
Xiaozhong Fan. 2005. The use of svm for
chinese new word identification. In Natural
Language Processing--IJCNLP 2004, pages 723-
-732. Springer.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings
of the ACL Student ResearchWorkshop, ACLstu-
dent '05, pages 13--18, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fuchun Peng, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese segmentation and new
word detection using conditional random field-
s. In Proceedings of the 20th International
Conference on Computational Linguistics, COL-
ING '04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Computational
linguistics, 37(1):9--27.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dic-
tionary headwords a solved problem. In Proc.
of the 6th Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001),
pages 100--108.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff.
In Proceedings of the Second SIGHANWorkshop
on Chinese Language Processing - Volume 17,
SIGHAN '03, pages 133--143, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Xu Sun, Houfeng Wang, and Wenjie Li. 2012.
Fast online training with frequency-adaptive
learning rates for chinese word segmentation
and new word detection. In Proceedings of
the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers -
Volume 1, ACL '12, pages 253--262, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Beijing Thesaurus Research Center. 2003. Xinhua Xin
Ciyu Cidian. Commercial Press, Beijing.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of seman-
tic orientation from association. ACM Trans. Inf.
Syst., 21(4):315--346, October.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing -
540
Volume 17, SIGHAN '03, pages 184--187,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Wen Zhang, Taketoshi Yoshida, Xijin Tang, and Tu-
Bao Ho. 2009. Improving effectiveness of
mutual information for substantival multiword
expression extraction. Expert Systems with
Applications, 36(8):10919--10930.
Yan Zhang, Maosong Sun, and Yang Zhang. 2010.
Chinese new word detection from query logs. In
Advanced Data Mining and Applications, pages
233--243. Springer.
Yabin Zheng, Zhiyuan Liu, Maosong Sun, Liyun Ru,
and Yang Zhang. 2009. Incorporating user be-
haviors in new word detection. In Proceedings of
the 21st International Jont Conference onArtifical
Intelligence, IJCAI'09, pages 2101--2106, San
Francisco, CA, USA.Morgan Kaufmann Publish-
ers Inc.
GuoDong Zhou. 2005. A chunking strategy towards
unknownword detection in chinese word segmen-
tation. In Natural Language Processing--IJCNLP
2005, pages 530--541. Springer.
541
