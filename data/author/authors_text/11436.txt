Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 157?160,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Opinion and Generic Question Answering Systems: a Performance 
Analysis 
 
 
Alexandra Balahur 1,2 
1DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
2IPSC, EC Joint Research Centre 
Via E. Fermi, 21027, Ispra 
abalahur@dlsi.ua.es 
Ester Boldrini 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
eboldrini@dlsi.ua.es 
 
Andr?s Montoyo 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
montoyo@dlsi.ua.es 
Patricio Mart?nez-Barco 
DLSI, University of Alicante  
Ap. De Correos 99, 03080, Alicante 
patricio@dlsi.ua.es 
 
Abstract 
The importance of the new textual genres such 
as blogs or forum entries is growing in parallel 
with the evolution of the Social Web. This pa-
per presents two corpora of blog posts in Eng-
lish and in Spanish, annotated according to the 
EmotiBlog annotation scheme. Furthermore, 
we created 20 factual and opinionated ques-
tions for each language and also the Gold 
Standard for their answers in the corpus. The 
purpose of our work is to study the challenges 
involved in a mixed fact and opinion question 
answering setting by comparing the perform-
ance of two Question Answering (QA) sys-
tems as far as mixed opinion and factual set-
ting is concerned. The first one is open do-
main, while the second one is opinion-
oriented. We evaluate separately the two sys-
tems in both languages and propose possible 
solutions to improve QA systems that have to 
process mixed questions. 
Introduction and motivation 
In the last few years, the number of blogs has 
grown exponentially. Thus, the Web contains 
more and more subjective texts. A research from 
the Pew Institute shows that 75.000 blogs are 
created daily (Pang and Lee, 2008). They ap-
proach a great variety of topics (computer sci-
ence, sociology, political science or economics) 
and are written by different types of people, thus 
are a relevant resource for large community be-
havior analysis. Due to the high volume of data 
contained in blogs, new Natural Language Proc-
essing (NLP) resources, tools and methods are 
needed in order to manage their language under-
standing. Our fist contribution consists in carry-
ing out a multilingual research, for English and 
Spanish. Secondly, many sources are present in 
blogs, as people introduce quotes from newspa-
per articles or other information to support their 
arguments and make references to previous posts 
in the discussion thread. Thus, when performing 
a task such as Question Answering (QA), many 
new aspects have to be taken into consideration. 
Previous studies in the field (Stoyanov, Cardie 
and Wiebe, 2005) showed that certain types of 
queries, which are factual in nature, require the 
use of Opinion Mining (OM) resources and tech-
niques to retrieve the correct answers. A further 
contribution this paper brings is the analysis and 
definition of the criteria for the discrimination 
among types of factual versus opinionated ques-
tions. Previous researchers mainly concentrated 
on newspaper collections. We formulated and 
annotated of a set of questions and answers over 
a multilingual blog collection. A further contri-
bution is the evaluation and comparison of two 
different approaches to QA a fact-oriented one 
and another designed for opinion QA scenarios.  
Related work 
Research in building factoid QA systems has a 
long history. However, it is only recently that 
studies have started to focus also on the creation 
and development of QA systems for opinions. 
Recent years have seen the growth of interest in 
this field, both by the research performed and the 
publishing of various studies on the requirements 
157
and peculiarities of opinion QA systems (Stoy-
anov, Cardie and Wiebe, 2005), (Pustejovsky 
and Wiebe, 2006), as well as the organization of 
international conferences that promote the crea-
tion of effective QA systems both for general and 
subjective texts, as, for example, the Text Analy-
sis Conference (TAC)1. Last year?s TAC 2008 
Opinion QA track proposed a mixed setting of 
factoid (?rigid list?) and opinion questions 
(?squishy list?), to which the traditional systems 
had to be adapted. The Alyssa system (Shen et 
al., 2007), classified the polarity of the question 
and of the extracted answer snippet, using a Sup-
port Vector Machines classifier trained on the 
MPQA corpus (Wiebe, Wilson and Cardie, 
2005), English NTCIR2 data and rules based on 
the subjectivity lexicon (Wilson, Wiebe and 
Hoffman, 2005). The PolyU (Wenjie et al, 
2008) system determines the sentiment orienta-
tion with two estimated language models for the 
positive versus negative categories. The 
QUANTA (Li, 2008) system detects the opinion 
holder, the object and the polarity of the opinion 
using a semantic labeler based on PropBank3 and 
some manually defined patterns.  
Evaluation 
In order to carry out our evaluation, we em-
ployed a corpus of blog posts presented in 
(Boldrini et al, 2009). It is a collection of blog 
entries in English, Spanish and Italian. However, 
for this research we used the first two languages. 
We annotated it using EmotiBlog (Balahur et al, 
2009) and we also created a list of 20 questions 
for each language. Finally, we produced the Gold 
Standard, by labeling the corpus with the correct 
answers corresponding to the questions. 
1.1 Questions 
No TYPE QUESTION 
 
1 
 
F 
 
F 
What international organization do people criticize for 
its policy on carbon emissions? 
?Cu?l fue uno de los primeros pa?ses que se preocup? 
por el problema medioambiental? 
 
 
2 
 
 
O 
 
 
F 
What motivates people?s negative opinions on the 
Kyoto Protocol? 
?Cu?l es el pa?s con mayor responsabilidad de la 
contaminaci?n mundial seg?n la opini?n p?blica? 
 
 
3 
 
 
F 
 
 
F 
What country do people praise for not signing the 
Kyoto Protocol? 
?Qui?n piensa que la reducci?n de la contaminaci?n se 
deber?a apoyar en los consejos de los cient?ficos? 
 
 
4 
 
 
F 
 
 
F 
What is the nation that brings most criticism to the 
Kyoto Protocol? 
?Qu? administraci?n act?a totalmente en contra de la 
lucha contra el cambio clim?tico? 
                                                 
1 http://www.nist.gov/tac/ 
2 http://research.nii.ac.jp/ntcir/ 
3 http://verbs.colorado.edu/~mpalmer/projects/ace.html 
 
 
5 
 
 
O 
 
 
F 
What are the reasons for the success of the Kyoto 
Protocol? 
?Qu? personaje importante est? a favor de la 
colaboraci?n del estado en la lucha contra el 
calentamiento global? 
 
 
6 
 
 
O 
 
 
F 
What arguments do people bring for their criticism of 
media as far as the Kyoto Protocol is concerned? 
?A qu? pol?ticos americanos culpa la gente por la 
grave situaci?n en la que se encuentra el planeta? 
 
7 
 
O 
 
F 
Why do people criticize Richard Branson? 
?A qui?n reprocha la gente el fracaso del Protocolo de 
Kyoto? 
 
8 
 
F 
 
F 
What president is criticized worldwide for his reaction 
to the Kyoto Protocol? 
?Qui?n acusa a China por provocar el mayor da?o al 
medio ambiente? 
 
9 
 
F 
 
O 
What American politician is thought to have developed 
bad environmental policies? 
?C?mo ven los expertos el futuro? 
 
10 
 
F 
 
O 
What American politician has a positive opinion on the 
Kyoto protocol? 
C?mo se considera el atentado del 11 de septiembre? 
 
11 
 
O 
 
O 
What negative opinions do people have on Hilary 
Benn? 
?Cu?l es la opini?n sobre EEUU? 
 
12 
 
O 
 
O 
Why do Americans praise Al Gore?s attitude towards 
the Kyoto protocol and other environmental issues? 
?De d?nde viene la riqueza de EEUU? 
 
13 
 
F 
 
O 
What country disregards the importance of the Kyoto 
Protocol? 
?Por qu? la guerra es negativa? 
 
14 
 
F 
 
O 
What country is thought to have rejected the Kyoto 
Protocol due to corruption? 
?Por qu? Bush se retir? del Protocolo de Kyoto? 
 
15 
 
F/
O 
 
O 
What alternative environmental friendly resources do 
people suggest to use instead of gas en the future? 
?Cu?l fue la posici?n de EEUU sobre el Protocolo de 
Kyoto? 
 
16 
 
F/
O 
 
O 
 Is Arnold Schwarzenegger pro or against the reduction 
of CO2 emissions? 
?Qu? piensa Bush sobre el cambio clim?tico? 
 
17 
 
F 
 
O 
What American politician supports the reduction of 
CO2 emissions? 
?Qu? impresi?n da Bush? 
 
18 
 
F/
O 
 
O 
What improvements are proposed to the Kyoto Proto-
col? 
?Qu? piensa China del calentamiento global? 
 
19 
 
F/
O 
 
O 
What is Bush accused of as far as political measures 
are concerned? 
?Cu?l es la opini?n de Rusia sobre el Protocolo de 
Kyoto? 
 
20 
 
F/
O 
 
O 
What initiative of an international body is thought to be 
a good continuation for the Kyoto Protocol? 
?Qu? cree que es necesario hacer Yvo Boer? 
 
Table 1: List of question in English and Spanish 
 
As it can be seen in the table above, we created 
factoid (F) and opinion (O) queries for English 
and for Spanish; however, there are some that 
could be defined between factoid and opinion 
(F/O) and the system can retrieve multiple an-
swers after having selected, for example, the po-
larity of the sentences in the corpus. 
1.2 Performance of the two systems 
We evaluated and compared the generic QA sys-
tem of the University of Alicante (Moreda et al, 
2008) and the opinion QA system presented in 
(Balahur et al, 2008), in which Named Entity 
Recognition with LingPipe4 and FreeLing5 was 
                                                 
4 http://alias-i.com/lingpipe/ 
5 http://garraf.epsevg.upc.es/freeling/ 
158
added, in order to boost the scores of answers 
containing NEs of the question Expected Answer 
Type (EAT). Table 2 presents the results ob-
tained for English and Table 3 for Spanish. We 
indicate the id of the question (Q), the question 
type (T) and the number of answer of the Gold 
Standard (A). We present the number of the re-
trieved questions by the traditional system 
(TQA) and by the opinion one (OQA). We take 
into account the first 1, 5, 10 and 50 answers. 
 
Number of found answers Q T A 
@1 @5 @10 @ 50 
   TQA OQA TQA OQA TQA OQA TQA OQA 
1 F 5 0 0 0 2 0 3 4 4 
2 O 5 0 0 0 1 0 1 0 3 
3 F 2 1 1 2 1 2 1 2 1 
4 F 10 1 1 2 1 6 2 10 4  
5 O 11 0 0 0 0 0 0 0 0 
6 O 2 0 0 0 0 0 1 0 2 
7 O 5 0 0 0 0 0 1 0 3 
8 F 5 1 0 3 1 3 1 5 1 
9 F 5 0 1 0 2 0 2 1 3 
10 F 2 1 0 1 0 1 1 2 1 
11 O 2 0 1 0 1 0 1 0 1 
12 O 3 0 0 0 1 0 1 0 1 
13 F 1 0 0 0 0 0 0 0 1 
14 F 7 1 0 1 1 1 2 1 2 
15 F/O 1 0 0 0 0 0 1 0 1 
16 F/O 6 0 1 0 4 0 4 0 4 
17 F 10 0 1 0 1 4 1 0 2 
18 F/O 1 0 0 0 0 0 0 0 0 
19 F/O 27 0 1 0 5 0 6 0 18 
20 F/O 4 0 0 0 0 0 0 0 0 
 
Table 2: Results for English 
 
Number of found answers Q T A 
@1 @5 @10 @ 50 
    TQA  OQA  TQA  OQA  TQA  OQA  TQA  OQA 
1 F 9 1 0 0 1 1 1 1 3 
2 F 13 0 1 2 3 0 6 11 7 
3 F 2 0 1 0 2 0 2 2 2 
4 F 1 0 0 0 0 0 0 1 0 
5 F 3 0 0 0 0 0 0 1 0 
6 F 2 0 0 0 1 0 1 2 1 
7 F 4 0 0 0 0 1 0 4 0 
8 F 1 0 0 0 0 0 0 1 0 
9 O 5 0 1 0 2 0 2 0 4 
10 O 2 0 0 0 0 0 0 0 0 
11 O 5 0 0 0 1 0 2 0 3 
12 O 2 0 0 0 1 0 1 0 1 
13 O 8 0 1 0 2 0 2 0 4 
14 O 25 0 1 0 2 0 4 0 8 
15 O 36 0 1 0 2 0 6 0 15 
16 O 23 0 0 0 0 0 0 0 0 
17 O 50 0 1 0 5 0 6 0 10 
18 O 10 0 1 0 1 0 2 0 2 
19 O 4 0 1 0 1 0 1 0 1 
20 O 4 0 1 0 1 0 1 0 1 
 
Table 3: Results for Spanish 
1.3 Results and discussion 
There are many problems involved when trying 
to perform mixed fact and opinion QA. The first 
can be the ambiguity of the questions e.g. ?De 
d?nde viene la riqueza de EEUU?. The answer 
can be explicitly stated in one of the blog sen-
tences, or a system might have to infer them 
from assumptions made by the bloggers and their 
comments. Moreover, most of the opinion ques-
tions have longer answers, not just a phrase snip-
pet, but up to 2 or 3 sentences. As we can ob-
serve in Table 2, the questions for which the 
TQA system performed better were the pure fac-
tual ones (1, 3, 4, 8, 10 and 14), although in some 
cases (question number 14) the OQA system re-
trieved more correct answers.  At the same time, 
opinion queries, although revolving around NEs, 
were not answered by the traditional QA system, 
but were satisfactorily answered by the opinion 
QA system (2, 5, 6, 7, 11, 12). Questions 18 and 
20 were not correctly answered by any of the two 
systems. We believe the reason is that question 
18 was ambiguous as far as polarity of the opin-
ions expressed in the answer snippets (?im-
provement? does not translate to either ?positive? 
or ?negative?) and question 20 referred to the 
title of a project proposal that was not annotated 
by any of the tools used. Thus, as part of the fu-
ture work in our OQA system, we must add a 
component for the identification of quotes and 
titles, as well as explore a wider range of polar-
ity/opinion scales. Furthermore, questions 15, 16, 
18, 19 and 20 contain both factual as well as 
opinion aspects and the OQA system performed 
better than the TQA, although in some cases, 
answers were lost due to the artificial boosting of 
the queries containing NEs of the EAT (Ex-
pected Answer Type). Therefore, it is obvious 
that an extra method for answer ranking should 
be used, as Answer Validation techniques using 
Textual Entailment. In Table 3, the OQA missed 
some of the answers due to erroneous sentence 
splitting, either separating text into two sentences 
where it was not the case or concatenating two 
consecutive sentences; thus missing out on one 
of two consecutively annotated answers. Exam-
ples are questions number 16 and 17, where 
many blog entries enumerated the different ar-
guments in consecutive sentences. Another 
source of problems was the fact that we gave a 
high weight to the presence of the NE of the 
sought type within the retrieved snippet and in 
some cases the name was misspelled in the blog 
entries, whereas in other NER performed by 
159
FreeLing either attributed the wrong category to 
an entity, failed to annotate it or wrongfully an-
notated words as being NEs.  Not of less impor-
tance is the question duality aspect in question 
17. Bush is commented in more than 600 sen-
tences; therefore, when polarity is not specified, 
it is difficult to correctly rank the answers. Fi-
nally, also the problems of temporal expressions 
and the coreference need to be taken into ac-
count.  
Conclusions and future work 
In this article, we created a collection of both 
factual and opinion queries in Spanish and Eng-
lish. We labeled the Gold Standard of the an-
swers in the corpora and subsequently we em-
ployed two QA systems, one open domain, one 
for opinion questions. Our main objective was to 
compare the performances of these two systems 
and analyze their errors, proposing solutions to 
creating an effective QA system for both factoid 
an opinionated queries. We saw that, even using 
specialized resources, the task of QA is still chal-
lenging. Opinion QA can benefit from a snippet 
retrieval at a paragraph level, since in many 
cases the answers were not simple parts of sen-
tences, but consisted in two or more consecutive 
sentences. On the other hand, we have seen cases 
in which each of three different consecutive sen-
tences was a separate answer to a question. Our 
future work contemplates the study of the impact 
anaphora resolution and temporality on opinion 
QA, as well as the possibility to use Answer 
Validation techniques for answer re-ranking. 
 
Acknowledgments 
 
The authors would like to thank Paloma Moreda, 
Hector Llorens, Estela Saquete and Manuel 
Palomar for evaluating the questions on their QA 
system. This research has been partially funded 
by the Spanish Government under the project 
TEXT-MESS (TIN 2006-15265-C06-01), by the 
European project QALL-ME (FP6 IST 033860) 
and by the University of Alicante, through its 
doctoral scholarship. 
References 
Alexandra Balahur, Ester Boldrini, Andr?s Montoyo, 
and Patricio Mart?nez-Barco, 2009. Cross-topic 
Opinion Mining for Real-time Human-Computer 
Interaction. In Proceedings of the 6th Workshop in 
Natural Language Processing and Cognitive Sci-
ence, ICEIS 2009 Conference, Milan, Italy. 
Alexandra Balahur, Elena Lloret, Oscar Ferrandez, 
Andr?s Montoyo, Manuel Palomar, Rafael Mu?oz. 
2008. The DLSIUAES Team?s Participation in the 
TAC 2008 Tracks. In Proceedings of the Text 
Analysis Conference (TAC 2008). 
Ester Boldrini, Alexandra Balahur, Patricio Mart?nez-
Barco, and Andr?s Montoyo. 2009. EmotiBlog: An 
Annotation Scheme for Emotion Detection and 
Analysis in Non-Traditional Textual Genres. To 
appear in Proceedings of the 5th Conference on 
data Mining. Las Vegas, Nevada, USA. 
W. Li, Y. Ouyang, Y. Hu, F. Wei. PolyU at TAC 
2008. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical 
methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2008. 
Fangtao Li, Zhicheng Zheng, Tang Yang, Fan Bu, 
Rong Ge, Xiaoyan Zhu, Xian Zhang, and Minlie 
Huang. THU QUANTA at TAC 2008 QA and RTE 
track. In Proceedings of Human Language Tech-
nologies Conference/Conference on Empirical 
methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2008. 
Bo Pang, and Lilian. Lee, Opinion mining and senti-
ment analysis. Foundations and Trends R. In In-
formation Retrieval Vol. 2, Nos. 1?2 (2008) 1?135, 
2008. 
James Pustejovsky and Janyce. Wiebe. Introduction 
to Special Issue on Advances in Question Answer-
ing. In Language Resources and Evaluation (2005) 
39: 119?122. Springer, 2006. 
Dan Shen, Jochen L. Leidner, Andreas Merkel, Diet-
rich Klakow. The Alyssa system at TREC QA 2007: 
Do we need Blog06? In Proceedings of The Six-
teenth Text Retrieval Conference (TREC 2007), 
Gaithersburg, MD, USA, 2007 
Vaselin, Stoyanov, Claire Cardie, Janyce Wiebe. 
Multi-Perspective Question Answering Using the 
OpQA Corpus. In Proceedings of HLT/EMNLP. 
2005. 
Paloma Moreda, Hector Llorens, Estela Saquete, 
Manuel Palomar. 2008. Automatic Generalization 
of a QA Answer Extraction Module Based on Se-
mantic Roles. In: AAI - IBERAMIA, Lisbon, Portu-
gal, pages 233-242, Springer. 
Janyce. Wiebe, Theresa Wilson, and Claire Cardie 
Annotating expressions of opinions and emotions 
in language. Language Resources and Evaluation, 
volume 39, issue 2-3, pp. 165-210, 2005. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
Recognising Contextual Polarity in Phrase-level 
sentiment Analysis. In Proceedings of Human lan-
guage Technologies Conference/Conference on 
Empirical methods in Natural Language Processing 
(HLT/EMNLP), Vancouver, BC, Canada, 2005. 
160
Coling 2010: Poster Volume, pages 27?35,
Beijing, August 2010
Going Beyond Traditional QA Systems: Challenges and Keys 
in Opinion Question Answering 
Alexandra Balahur 
Dept. of Software and Computing Systems  
University of Alicante 
abalahur@dlsi.ua.es 
Ester Boldrini  
Dept. of Software and Computing Systems  
University of Alicante 
eboldrini@dlsi.ua.es 
 
Andr?s Montoyo 
Dept. of Software and Computing Systems  
University of Alicante 
montoyo@dlsi.ua.es 
Patricio Mart?nez-Barco 
Dept. of Software and Computing Systems  
University of Alicante 
patricio@dlsi.ua.es 
Abstract  
The treatment of factual data has been 
widely studied in different areas of Nat-
ural Language Processing (NLP). How-
ever, processing subjective information 
still poses important challenges. This 
paper presents research aimed at assess-
ing techniques that have been suggested 
as appropriate in the context of subjec-
tive - Opinion Question Answering 
(OQA). We evaluate the performance of 
an OQA with these new components 
and propose methods to optimally tackle 
the issues encountered. We assess the 
impact of including additional resources 
and processes with the purpose of im-
proving the system performance on two 
distinct blog datasets. The improve-
ments obtained for the different combi-
nation of tools are statistically signifi-
cant. We thus conclude that the pro-
posed approach is adequate for the OQA 
task, offering a good strategy to deal 
with opinionated questions. 
1 Introduction 
The State of the Blogosphere 2009 survey pub-
lished by Technorati 1 concludes that in the past 
years the blogosphere has gained a high influ-
ence on a high variety of topics, ranging from 
cooking and gardening, to economics, politics 
and scientific achievements. The development 
                                                 
1 http://technorati.com/ 
of the Social Web and the new communication 
frameworks also influenced the way informa-
tion is transmitted through communities. Blogs 
are part of the so-called new textual genres. 
They have distinctive features when compared 
to the traditional ones, such as newspaper ar-
ticles. Blog language contains formal and in-
formal expressions, and other elements, as re-
peated punctuation or emoticons (used to stress 
upon different text elements). With the growth 
in the content of the blogosphere, the quantity 
of subjective data of the Web is increasing ex-
ponentially (Cui et al, 2006). As it is being up-
dated in real-time, this data becomes a source of 
timely information on many topics, exploitable 
by different applications. In order to properly 
manage the content of this subjective informa-
tion, its processing must be automated. The 
NLP task, which deals with the classification of 
opinionated content is called Sentiment Analy-
sis (SA). Research in this field aims at discover-
ing appropriate mechanisms to properly re-
trieve, extract and classify opinions expressed in 
text. While techniques to retrieve objective in-
formation have been widely studied, imple-
mented and evaluated, opinion-related tasks still 
represent an important challenge. As a conse-
quence, the aim of our research is to study, im-
plement and evaluate appropriate methods for 
the task of Question Answering (QA) in the 
opinion treatment framework.  
2 Motivation and Contribution 
Research in opinion-related tasks gained impor-
tance in the past years. However, there are still 
many aspects that require analysis and im-
27
provement, especially for approaches that com-
bine SA with other NLP tasks such as QA or 
automatic summarization. The TAC 2008 Opi-
nion Pilot task and the subsequent research per-
formed on the competition data have demon-
strated that answering opinionated questions 
and summarizing subjective information are 
significantly different from the equivalent tasks 
in the same context, but dealing with factual 
data.  This finding was confirmed by the recent 
work by (Kabadjov et al, 2009). The first moti-
vation of our work is the need to detect and ex-
plore the challenges raised by opinion QA 
(OQA), as compared to factual QA. To this aim, 
we analyze the improvements that can be 
brought at the different steps of the OQA 
process: question treatment (identification of 
expected polarity ? EPT, expected source ? ES 
and expected target ?ET-), opinion retrieval (at 
the level of one and three-sentences long snip-
pets, using topic-related words or using paraph-
rases), opinion analysis (using topic detection 
and anaphora resolution). This preliminary re-
search is motivated by the conclusions drawn by 
previous studies (Balahur et al, 2009). Our pur-
pose is to verify if the inclusion of new ele-
ments and methods - source and target detection 
(using semantic role labeling (SRL)), topic de-
tection (using Latent Semantic Analysis), pa-
raphrasing and joint topic-sentiment analysis 
(classification of the opinion expressed only in 
sentences related to the topic), followed by ana-
phora resolution (using a system whose perfor-
mance is not optimal), affects the results of the 
system and how. Our contribution to this respect 
is the identification of the challenges related to 
OQA compared to traditional QA. A further 
contribution consists in adding the appropriate 
methods, tools and resources to resolve the 
identified challenges. With the purpose of test-
ing the effect of each tool, resource and tech-
nique, we carry out a separate and a global 
evaluation. An additional motivation of our 
work is the fact that although previous ap-
proaches showed that opinion questions have 
longer answers than factual ones, the research 
done in OQA so far has only considered a sen-
tence-level approach. Another contribution this 
paper brings is the retrieval at 1 and 3-sentence 
level and the retrieval based on similarity to 
query paraphrases enriched with topic-related 
words). We believe retrieving longer text could 
cause additional problems such as redundancy, 
coreference and temporal expressions or the 
need to apply contextual information. Paraph-
rasing, on the other hand, had account for lan-
guage variability in a more robust manner; 
however, the paraphrase collections that are 
available at the moment are known to be noisy. 
The following sections are structured as fol-
lows: Section 3 presents the related work in the 
field and the competitions organized for systems 
tackling the OQA task. In Section 4 we describe 
the corpora used for the experiments we carried 
out and the set of questions asked over each of 
them. Section 5 presents the experimental set-
tings and the different system configurations we 
assessed. Section 6 shows the results of the 
evaluations, discusses the improvements and 
drops in performance using different configura-
tions. We finally conclude on our approaches in 
Section 7, proposing the lines for future work. 
3 Related Work 
QA can be defined as the task in which given a 
set of questions and a collection of documents, 
an automatic NLP system is employed to re-
trieve the answer to the queries in Natural Lan-
guage (NL). Research focused on building fac-
toid QA systems has a long tradition; however, 
it is only recently that researchers have started 
to focus on the development of OQA systems. 
(Stoyanov et al, 2005) and (Pustejovsky and 
Wiebe, 2006) studied the peculiarities of opi-
nion questions. (Cardie et al, 2003) employed 
opinion summarization to support a Multi-
Perspective QA system, aiming at identifying 
the opinion-oriented answers for a given set of 
questions. (Yu and Hatzivassiloglou, 2003) se-
parated opinions from facts and summarized 
them as answer to opinion questions. (Kim and 
Hovy, 2005) identified opinion holders, which 
are a key component in retrieving the correct 
answers to opinion questions. Due to the rea-
lized importance of blog data, recent years have 
also marked the beginning of NLP research fo-
cused on the development of opinion QA sys-
tems and the organization of international con-
ferences encouraging the creation of effective 
QA systems both for fact and subjective texts. 
The TAC 20082 QA track proposed a collection 
                                                 
2 http://www.nist.gov/tac/ 
28
of factoid and opinion queries called ?rigid list? 
(factoid) and ?squishy list? (opinion) respective-
ly, to which the traditional QA systems had to 
be adapted. Some participating systems treated 
opinionated questions as ?other? and thus they 
did not employ opinion specific methods. How-
ever, systems that performed better in the 
?squishy list? questions than in the ?rigid list? 
implemented additional components to classify 
the polarity of the question and of the extracted 
answer snippet. The Alyssa system (Shen et al 
2007) uses a Support Vector Machines (SVM) 
classifier trained on the MPQA corpus (Wiebe 
et al, 2005), English NTCIR3 data and rules 
based on the subjectivity lexicon (Wilson et al, 
2005). (Varma et al, 2008) performed query 
analysis to detect the polarity of the question 
using defined rules. Furthermore, they filter 
opinion from fact retrieved snippets using a 
classifier based on Na?ve Bayes with unigram 
features, assigning for each sentence a score that 
is a linear combination between the opinion and 
the polarity scores. The PolyU (Venjie et al, 
2008) system determines the sentiment orienta-
tion of the sentence using the Kullback-Leibler 
divergence measure with the two estimated lan-
guage models for the positive versus negative 
categories. The QUANTA (Li et al, 2008) sys-
tem performs opinion question sentiment analy-
sis by detecting the opinion holder, the object 
and the polarity of the opinion. It uses a seman-
tic labeler based on PropBank 4  and manually 
defined patterns. Regarding the sentiment clas-
sification, they extract and classify the opinion 
words. Finally, for the answer retrieval, they 
score the retrieved snippets depending on the 
presence of topic and opinion words and only 
choose as answer the top ranking results. Other 
related work concerns opinion holder and target 
detection. NTCIR 7 and 8 organized MOAT 
(the Multilingual Opinion Analysis Task), in 
which most participants employed machine 
learning approaches using syntactic patterns 
learned on the MPQA corpus (Wiebe et al, 
2005). Starting from the abovementioned re-
search, our aim is to take a step forward to 
present approaches and employ opinion specific 
methods focused on improving the performance 
of our OQA. We perform the retrieval at 1 sen-
                                                 
3 http://research.nii.ac.jp/ntcir/ 
4http://verbs.colorado.edu/~mpalmer/projects/ace.html 
tence and 3 sentence-level and also determine 
the expected source (ES) and the expected tar-
get (ET) of the questions, which are fundamen-
tal to properly retrieve the correct answer. These 
two elements are selected employing semantic 
roles (SR). The expected answer type (EAT) is 
determined using Machine Learning (ML) using 
Support Vector Machine (SVM), by taking into 
account the interrogation formula, the subjectiv-
ity of the verb and the presence of polarity 
words in the target SR. In the case of expected 
opinionated answers, we also compute the ex-
pected polarity type (EPT) ? by applying opi-
nion mining (OM) on the affirmative version of 
the question (e.g. for the question ?Why do 
people prefer Starbucks to Dunkin Donuts??, 
the affirmative version is ?People prefer Star-
bucks to Dunkin Donuts because X?). These 
experiments are presented in more detail in  
Section 5.  
4 Corpora 
In order to carry out the present research for 
detecting and solving the complexities of opi-
nion QA, we employed two corpora of blog 
posts: EmotiBlog (Boldrini et al, 2009a) and 
the TAC 2008 Opinion Pilot test collection (part 
of the Blog06 corpus). 
The TAC 2008 Opinion Pilot test collection is 
composed by documents with the answers to the 
opinion questions given on 25 targets. EmotiB-
log is a collection of blog posts in English ex-
tracted form the Web. As a consequence, it 
represents a genuine example of this textual ge-
nre. It consists in a monothematic corpus about 
the Kyoto Protocol, annotated with the im-
proved version of EmotiBlog (Boldrini et al, 
2009b). It is well know that Opinion Mining 
(OM) is a very complex task due to the high 
variability of the language employed. Thus, our 
objective is to build an annotation model that is 
able to capture the whole range of phenomena 
specific to subjectivity expression. Additional 
criteria employed when choosing the elements 
to be annotated were effectiveness and noise 
minimization. Thus, from the first version of the 
model, the elements which did not prove to be 
statistically relevant have been eliminated. The 
elements that compose the improved version of 
the annotation model are presented in Table 1.   
 
29
Elements Description 
Obj. speech Confidence, comment, source, target. 
Subj. speech Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Adjec-
tives/Adverbs 
Confidence, comment, level, emotion, 
phenomenon, modifier/not, polarity, 
source and target. 
Verbs/ Names Confidence, comment, level, emotion, 
phenomenon, polarity, mode, source 
and target. 
Anaphora Confidence, comment, type, source and 
target. 
Capital letter/ 
Punctuation 
Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Phenomenon Confidence, comment, type, colloca-
tion, saying, slang, title, and rhetoric. 
Reader/Author 
Interpr. (obj.) 
Confidence, comment, level, emotion, 
phenomenon, polarity, source and 
target. 
Emotions Confidence, comment, accept, anger, 
anticipation, anxiety, appreciation, bad, 
bewilderment, comfort, compassion? 
Table 1: EmotiBlog improved structure 
 
The first distinction consists in separating objec-
tive and subjective speech. Subsequently, a fin-
er-grained annotation is employed for each of 
the two types of data. Objective sentences are 
annotated with source and target (when neces-
sary, also the level of confidence of the annota-
tor and a comment). Subjective elements can be 
annotated at a sentence level, but they also have 
to be labeled at a word and/or phrase level. 
EmotiBlog also contains annotations of anapho-
ra at a cross-document level (to interpret the 
storyline of the posts) and the sentence type 
(simple sentence or title, but also saying or col-
location). Finally, the Reader and the Writer 
interpretation have to be marked in objective 
sentences. These elements are employed to 
mark and interpret correctly an apparent objec-
tive discourse, whose aim is to implicitly ex-
press an opinion (e.g. ?The camera broke in two 
days?). The first is useful to extract what is the 
interpretation of the reader (for example if the 
writer says The result of their governing was an 
increase of 3.4% in the unemployment rate in-
stead of The result of their governing was a dis-
aster for the unemployment rate) and the second 
to understand the background of the reader (i.e.. 
These criminals are not able to govern instead 
of saying the x party is not able to govern). 
From this sentence, for example, the reader can 
deduce the political ideas of the writer. The 
questions whose answers are annotated with 
EmotiBlog are the subset of opinion questions in 
English presented in (Balahur et al, 2009). The 
complete list of questions is shown in Table 2.  
 
N Question 
2 What motivates people?s negative opinions on the 
Kyoto Protocol? 
5 What are the reasons for the success of the Kyoto 
Protocol? 
6 What arguments do people bring for their criticism 
of media as far as the Kyoto Protocol is concerned? 
7 Why do people criticize Richard Branson? 
11 What negative opinions do people have on Hilary 
Benn? 
12 Why do Americans praise Al Gore?s attitude towards 
the Kyoto protocol? 
15 What alternative environmental friendly resources 
do people suggest to use instead of gas en the future? 
16 Is Arnold Schwarzenegger pro or against the reduc-
tion of CO2 emissions? 
18 What improvements are proposed to the Kyoto Pro-
tocol? 
19 What is Bush accused of as far as political measures 
are concerned? 
20 What initiative of an international body is thought to 
be a good continuation for the Kyoto Protocol? 
Table 2: Questions over the EmotiBlog  
corpus 
 
The main difference between the two corpora 
employed is that Emotiblog is monothematic, 
containing only posts about the Kyoto Protocol, 
while the TAC 2008 corpus contains documents 
on a multitude of subjects. Therefore, different 
techniques must be adjusted in order to treat 
each of them.  
5 Experiments 
5.1 Question Analysis 
In order to be able to extract the correct answer 
to opinion questions, different elements must be 
considered. As stated in (Balahur et al, 2009) 
we need to determine both the expected answer 
type (EAT) of the question ? as in the case of 
factoid ones - as well as new elements ? such as 
expected polarity type (EPT). However, opi-
nions are directional ? i.e., they suppose the ex-
istence of a source and a target to which they 
are addressed. Thus, we introduce two new 
elements in the question analysis ? expected 
source (ES) and expected target (ET). These 
two elements are selected by applying SR and 
choosing the source as the agent in the sentence 
and the direct object (patient) as the target of the 
opinion. Of course, the source and target of the 
30
opinions expressed can also be found in other 
roles, but at this stage we only consider these 
cases. The expected answer type (EAT) (e.g. 
opinion or other) is determined using Machine 
Learning (ML) using Support Vector Machine 
(SVM), by taking into account the interrogation 
formula, the subjectivity of the verb and the 
presence of polarity words in the target SR. In 
the case of expected opinionated answers, we 
also compute the expected polarity type (EPT) ? 
by applying OM on the affirmative version of 
the question. An example of such a transforma-
tion is: given the question ?What are the rea-
sons for the success of the Kyoto Protocol??, 
the affirmative version of the question is ?The 
reasons for the success of the Kyoto Protocol 
are X?.  
5.2 Candidate Snippet Retrieval 
In the answer retrieval stage, we employ four 
strategies:  
1. Using the JIRS (JAVA Information Re-
trieval System) IR engine (G?mez et al, 
2007) to find relevant snippets. JIRS re-
trieves passages (of the desired length), 
based on searching the question struc-
tures (n-grams) instead of the keywords, 
and comparing them.  
2. Using the ?Yahoo? search engine to re-
trieve the first 20 documents that are 
most related to the query. Subsequently, 
we apply LSA on the retrieved docu-
ments and extract the words that are 
most related to the topic. Finally, we 
expand the query using words that are 
very similar to the topic and retrieve 
snippets that contain at least one of 
them and the ET. 
3. Generating equivalent expressions for 
the query, using the DIRT paraphrase 
collection (Lin and Pantel, 2001) and 
retrieving candidate snippets of length 1 
and 3 (length refers to the number of 
sentences retrieved) that are similar to 
each of the new generated queries and 
contain the ET. Similarity is computed 
using the cosine measure. Examples of 
alternative queries for ?People like 
George Clooney? are ?People adore 
George Clooney?, ?People enjoy 
George Clooney?, ?People prefer 
George Clooney?. 
4. Enriching the equivalent expressions for 
the query in 3. with the topic-related 
words discovered in 2. using LSA. 
5.3 Polarity and topic-polarity classifica-
tion of snippets 
In order to determine the correct answers from 
the collection of retrieved snippets, we must 
filter for the next processing stage only the can-
didates that have the same polarity as the ques-
tion EPT. For polarity detection, we use a com-
bined system employing SVM ML on unigram 
and bigram features trained on the NTCIR 
MOAT 7 data and an unsupervised lexicon-
based system. In order to compute the features 
for each of the unigrams and bigrams, we com-
pute the tf-idf scores. 
The unsupervised system uses the Opinion 
Finder lexicon to filter out subjective sentences 
? that contain more than two subjective words 
or a subjective word and a valence shifter (ob-
tained from the General Inquirer resource). Sub-
sequently, it accounts for the presence of opi-
nionated words from four different lexicons ? 
MicroWordNet (Cerini et al, 2007), WordNet 
Affect (Strapparava and Valitutti, 2004) Emo-
tion Triggers (Balahur and Montoyo, 2008) and 
General Inquirer (Stone et al, 1966). For the 
joint topic-polarity analysis, we first employ 
LSA to determine the words that are strongly 
associated to the topic, as described in Section 
5.2 (second list item). Consequently, we com-
pute the polarity of the sentences that contain at 
least one topic word and the question target. 
5.4 Filtering using SR 
Finally, answers are filtered using the Semrol 
system for SR labeling described in (Moreda, 
2008). Subsequently, we filter all snippets with 
the required target and source as agent or pa-
tient. Semrol receives as input plain text with 
information about grammar, syntax, word 
senses, Named Entities and constituents of each 
verb. The system output is the given text, in 
which the semantic roles information of each 
constituent is marked. Ambiguity is resolved 
31
depending on the machine algorithm employed, 
which in this case is TIMBL5. 
6 Evaluation and Discussion 
We evaluate our approaches on both the Emo-
tiBlog question collection, as well as on the 
TAC 2008 Opinion Pilot test set. We compare 
them against the performance of the system eva-
luated in (Balahur et al, 2009) and the best 
(Copeck et al, 2008) and worst (Varma et al, 
2008) scoring systems (as far as F-measure is 
concerned) in the TAC 2008 task.  For both the 
TAC 2008 and EmotiBlog sets of questions, we 
employ the SR system in SA and determine the 
ES, ET and EPT. Subsequently, for each of the 
two corpora, we retrieve 1-phrase and 3-phrase 
snippets. The retrieval of the of the EmotiBlog 
candidate snippets is done using query expan-
sion with LSA and filtering according to the ET. 
Further on, we apply sentiment analysis (SA) 
using the approach described in Section 5.3 and 
select only the snippets whose polarity is the 
same as the determined question EPT. The re-
sults are presented in Table 3.  
 
Q 
N
o. 
N
o.  
A 
Baseline 
(Balahur et al, 
2009) 
1 phrase + 
ET+SA 
3 phrases 
+ET+SA 
  @ 
1 
@ 
5 
@ 
1
0 
@ 
5
0 
@ 
1 
@ 
5 
@ 
1
0 
@ 
5
0 
@ 
1 
@ 
5 
@ 
1
0 
@
2
0 
2 5 0 2 3 4 1 2 3 4 1 2 3 4 
5 1
1 
0 0 0 0 0 2 2 2 1 2 3 4 
6 2 0 0 1 2 1 1 2 2 0 1 2 2 
7 5 0 0 1 3 1 1 1 3 0 2 2 4 
1
1 
2 1 1 1 1 0 0 0 0 0 0 0 1 
1
2 
3 0 1 1 1 0 1 2 3 0 0 1 2 
1
5 
1 0 0 1 1 0 0 1 1 1 1 1 1 
1
6 
6 1 4 4 4 0 1 1 2 1 2 2 6 
1
8 
1 0 0 0 0 0 0 0 0 0 0 0 0 
1
9 
2
7 
1 5 6 1
8 
0 1 1 2 0 1 1 1 
2
0 
4 0 0 0 0 0 0 1 1 0 0 1 2 
Table 3: Results for questions over  
EmotiBlog 
 
                                                 
5
http://ilk.uvt.nl/downloads/pub/papers/Timbl_6.2_Manual
.pdf and http://ilk.uvt.nl/timbl/ 
The retrieval of the TAC 2008 1-phrase and 3-
phrase candidate snippets was done using JIRS 
and, in a second approach, using the cosine si-
milarity measure between alternative queries 
generated using paraphrases and candidate 
snippets. Subsequently, we performed different 
evaluations, in order to assess the impact of us-
ing different resources and tools. Since the TAC 
2008 had a limit of the output of 7000 charac-
ters, in order to compute a comparable F-
measure, at the end of each processing chain, 
we only considered the snippets for the 1-phrase 
retrieval and for the 3-phases one until this limit 
was reached. 
1. In the first evaluation, we only apply the 
sentiment analysis tool and select the snip-
pets that have the same polarity as the ques-
tion EPT and the ET is found in the snippet.  
(i.e. What motivates peoples negative opi-
nions on the Kyoto Protocol? The Kyoto 
Protocol becomes deterrence to economic 
development and international cooperation/ 
Secondly, in terms of administrative aspect, 
the Kyoto Protocol is difficult to implement.  
- same EPT and ET) 
We also detected cases of same polarity but 
no ET, e.g. These attempts mean annual ex-
penditures of $700 million in tax credits in 
order to endorse technologies, $3 billion in 
developing research and $200 million in 
settling technology into developing coun-
tries ? EPT negative but not same ET. 
2. In the second evaluation, we add the result 
of the LSA process to filter out the snippets 
from 1., containing the words related to the 
topic starting from the retrieval performed 
by Yahoo, which extracts the first 20 docu-
ments about the topic. 
3. In the third evaluation, we filter the results 
in 2 by applying the Semrol system and set-
ting the condition that the ET and ES are the 
agent or the patient of the snippet. 
4. In the fourth evaluation setting, we replaced 
the set of snippets retrieved using JIRS with 
the ones obtained by generating alternative 
queries using paraphrases (as explained in 
the third method in section 5.2.). We subse-
quently filtered these results based on their 
polarity  (so that it corresponds to the EPT) 
and on the condition that the source and tar-
get of the opinion (identified through SRL 
using Semrol) correspond to the ES and ET.  
32
5. In the fourth evaluation setting, we replaced 
the set of snippets retrieved using JIRS with 
the ones obtained by generating alternative 
queries using paraphrases, enriched with the 
topic words determined using LSA. We 
subsequently filtered these results based on 
their polarity (so that it corresponds to the 
EPT) and on the condition that the source 
and target of the opinion (identified through 
SRL using Semrol) correspond to the ES 
and ET.  
 
System F-measure 
Best TAC 0.534 
Worst TAC 0.101 
JIRS + SA+ET (1 phrase)  0.377 
JIRS + SA+ET (3 phrases)  0.431 
JIRS + SA+ET+LSA (1 phrase)  0.489 
JIRS + SA+ET+LSA (3 phrases)  0.505 
JIRS + SA+ET+LSA+SR (1 
phrase)  
0. 533 
JIRS + SA+ET+LSA+SR (3 
phrases) 
0.571 
PAR+SA+ET+SR(1 phrase) 0.345 
PAR+SA+ET+SR(2 phrase) 0.386 
PAR_LSA+SA+ET+SR (1 phra-
se) 
0.453 
PAR_LSA+SA+ET+SR (3 phra-
ses) 
0.434 
Table 4: Results for the TAC 2008 test set 
 
From the results obtained (Table 3 and Table 4), 
we can draw the following conclusions. Firstly, 
the hypothesis that OQA requires the retrieval 
of longer snippets was confirmed by the im-
proved results, both in the case of EmotiBlog, as 
well as the TAC 2008 corpus. Secondly, opi-
nion questions require the use of joint topic-
sentiment analysis. As we can see from the re-
sults, the use of topic-related words when com-
puting of the affect influences the results in a 
positive manner and joint topic-sentiment anal-
ysis is especially useful for the cases of ques-
tions asked on a monothematic corpus. Thirdly, 
another conclusion that we can draw is that tar-
get and source detection are highly relevant 
steps at the time of answer filtering, not only 
helping the more accurate retrieval of answers, 
but also at placing at the top of the retrieval the 
relevant results (as more relevant information is 
contained within these 7000 characters). The 
use of paraphrases at the retrieval stage was 
shown to produce a significant drop in results, 
which we explain by the noise introduced and 
the fact that more non-relevant answer candi-
dates were introduced among the results. None-
theless, as we can see from the overall relatively 
low improvement in the results, much remains 
to be done in order to appropriately tackle 
OQA. As seen in the results, there are still ques-
tions for which no answer is found (e.g. 18). 
This is due to the fact that the treatment of such 
questions requires the use of inference tech-
niques that are presently unavailable (i.e. define 
terms such as ?improvement?, possibly as ?X 
better than Y?, in which case opinion extraction 
from comparative sentences should be intro-
duced in the model).  
The results obtained when using all the compo-
nents for the 3-sentence long snippets signifi-
cantly improve the results obtained by the best 
system participating in the TAC 2008 Opinion 
Pilot competition (determined using a paired t-
test for statistical significance, with confidence 
level 5%). Finally, from the analysis of the er-
rors, we could see that even though some tools 
are in theory useful and should produce higher 
improvements ? such as SR ? their performance 
in reality does not produce drastically higher 
results. The idea to use paraphrases for query 
expansion also proved to decrease the system 
performance. From preliminary results obtained 
using JavaRap6  for coreference resolution, we 
also noticed that the performance of the OQA 
lowered, although theoretically it should have 
improved. 
7 Conclusions ad Future Work 
In this paper, we presented and evaluated differ-
ent methods and techniques with the objective 
of improving the task of QA in the context of 
opinion data. From the evaluations performed 
using different NLP resources and tools, we 
concluded that joint topic-sentiment analysis, as 
well as the target and source identification, are 
crucial for the correct performance of this task. 
We have also demonstrated that by retrieving 
longer answers, the results have improved. We 
tested, within a simple setting, the impact of 
using paraphrases in the context of opinion 
questions and saw that their use lowered the 
system results. Although such paraphrase col-
                                                 
6http://wing.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.ht
m 
33
lections include a lot of noise and have been 
shown to decrease system performance even in 
the case of factual questions, we believe that 
other types of paraphrasing methods should be 
investigated in the context of OQA. We thus 
showed that opinion QA requires the develop-
ment of appropriate strategies at the different 
stages of the task (recognition of subjective 
questions, detection of subjective content of the 
questions, source and target identification, re-
trieval and classification of the candidate an-
swer data). Due to the high level of complexity 
of subjective language, our future work will be 
focused on testing higher-performing tools for 
coreference resolution, other (opinion) paraph-
rases collections and paraphrasing methods and 
the employment of external knowledge sources 
that refine the semantics of queries. We also 
plan to include other SA methods and extend 
the semantic roles considered for ET and ES, 
with the purpose of checking if they improve or 
not the performance of the QA system. 
 
Acknowledgements 
This paper has been partially supported by Mi-
nisterio de Ciencia e Innovaci?n - Spanish Gov-
ernment (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat Va-
lenciana (grant no. PROMETEO/2009/119 and 
ACOMP/2010/286). 
References 
Balahur, A. and Montoyo, A. 2008. Applying a 
Culture Dependent Emotion Triggers Data-
base for Text Valence and Emotion 
Classification. In Proceedings of the AISB 
2008 Symposium on Affective Language in 
Human and Machine, Aberdeen, Scotland. 
Balahur, A., Lloret, E., Ferr?ndez, O., Montoyo, 
A., Palomar, M., and Mu?oz, R. 2008. The 
DLSIUAES Team?s Participation in the TAC 
2008 Tracks. In Proceedings of the Text 
Analysis Conference 2008 Workshop. 
Balahur, A., Boldrini, E., Montoyo A. and 
Mart?nez-Barco P. 2009. Opinion and Generic 
Question Answering Systems: a Performance 
Analysis. In Proceedings of ACL. Singapur.  
Boldrini, E., Balahur, A., Mart?nez-Barco, P. 
and  Montoyo. A. 2009a. EmotiBlog: an An-
notation Scheme for Emotion Detection and 
Analysis in Non-traditional Textual Genre. In 
Proceedings of DMIN 2009, Las Vegas. Ne-
vada. 
Boldrini, E., Balahur, A., Mart?nez-Barco, P. 
and Montoyo. A. 2009b. EmotiBlog: a fine-
grained model for emotion detection in non-
traditional textual genre. In Proceedings of 
WOMSA 2009. Seville. 
Cardie, C., Wiebe, J., Wilson, T. and Litman, D. 
2003. Combining Low-Level and Summary 
Representations of Opinions for Multi-
Perspective Question Answering. AAAI 
Spring Symposium on New Directions in 
Question Answering. 
Cerini, S., Compagnoni, V., Demontis, A., 
Formentelli, M. and Gandini, C. 2007. Mi-
cro-WNOp: A gold standard for the evalua-
tion of automatically compiledlexical re-
sources for opinion mining. In: A.Sanso 
(ed.): Language resources and linguistic 
theory: Typology, Second Language Acqui-
sition, English Linguistics. Milano. IT. 
Copeck, T.,  Kazantseva, A., Kennedy, A., 
Kunadze, A., Inkpen, D. and Szpakowicz, 
S. 2008. Update Summary Update. In Pro-
ceedings of the Text Analysis Conference 
(TAC) 2008. 
Cui, H., Mittal, V. and Datar, M. 2006. Com-
parative Experiments on Sentiment Classifi-
cation for Online Product Review. Proceed-
ings, The Twenty-First National Conference 
on Artificial Intelligence and the Eighteenth 
Innovative Applications of Artificial Intelli-
gence Conference. Boston, Massachusetts, 
USA. 
G?mez, J.M., Rosso, P. and Sanchis, E. 2007. 
JIRS Language-Independent Passage Re-
trieval System: A Comparative Study. 5th 
International Conference on Natural 
Language Proceeding (ICON 2007). 
Kabadjov, M., Balahur, A. And Boldrini, E. 
2009. Sentiment Intensity: Is It a Good 
Summary Indicator?. Proceedings of the 4th 
Language Technology Conference LTC, pp. 
380-384. Poznan, Poland, 6-8.11.2009. 
Kim, S. M. and Hovy, E. 2005. Identifying 
Opinion Holders for Question Answering in 
Opinion Texts. Proceedings of the 
Workshop on Question Answering in 
Restricted Domain at the Conference of the 
American Association of Artificial 
Intelligence (AAAI-05).  Pittsburgh, PA. 
34
Li, F., Zheng, Z.,Yang T., Bu, F., Ge, R., Zhu, 
X., Zhang, X., and Huang, M. 2008. THU 
QUANTA at TAC 2008. QA and RTE track. 
In Proceedings of the Text Analysis 
Conference (TAC). 
Lin, D. and Pantel, P. 2001. Discovery of 
Inference Rules for Question Answering. 
Natural Language Engineering 7(4):343-
360. 
Moreda. P. 2008. Los Roles Sem?nticos en la 
Tecnolog?a del Lengauje Humano: Anota-
ci?n y Aplicaci?n. Doctoral Thesis. Univer-
sity of Alicante. 
Pustejovsky, J. and Wiebe, J. 2006. Introduction 
to Special Issue on Advances in Question 
Answering. Language Resources and Eval-
uation (2005), (39). 
Shen, D., Wiegand, M., Merkel, A., Kazalski, 
S., Hunsicker, S., Leidner, J. L. and 
Klakow, D. 2007. The Alyssa System at 
TREC QA 2007: Do We Need Blog06? In 
Proceedings of the Sixteenth Text Retrieval 
Conference (TREC 2007), Gaithersburg, 
MD, USA. 
Strapparava, C. and Valitutti, A. 2004. Word-
Net-Affect: an affective extension of Word-
Net. In Proceedings of 4th International Con-
ference on Language Resources and Evalua-
tion (LREC 2004), pages 1083 ? 1086, Lis-
bon. 
Stoyanov, V., Cardie, C., and Wiebe, J. 2005. 
Multiperspective question answering using 
the opqa corpus. In Proceedings of the 
Human Language Technology Conference 
and the Conference on Empirical Methods 
in Natural Language Processing 
(HLT/EMNLP 2005). 
Varma, V., Pingali, P., Katragadda, S., Krishna, 
R., Ganesh, S., Sarvabhotla, K. Garapati, 
H., Gopisetty, H., Reddy, K. and 
Bharadwaj, R. 2008. IIIT Hyderabad at 
TAC 2008. In Proceedings of Text Analysis 
Conference (TAC).  
Wenjie, L., Ouyang, Y., Hu, Y. and Wei, F. 
2008. PolyU at TAC 2008. In Proceedings 
of the Text Analysis Conference (TAC). 
Wiebe, J., Wilson, T., and Cardie, C. 2005. 
Annotating expressions of opinions and 
emotions in language. Language Resources 
and Evaluation, volume 39, issue 2-3, pp. 
165-210. 
Wilson, T., J. Wiebe, and Hoffmann, P. 2005. 
Recognizing Contextual Polarity in Phrase-
level sentiment Analysis. In Proceedings of 
the Human Language Technologies 
Conference/Conference on Empirical 
Methods in Natural Language Processing 
(HLT/ EMNLP). 
Yu, H. and Hatzivassiloglou, V. 2003. Towards 
Answering Opinion Questions: Separating 
Facts from Opinions. In Proceedings of 
EMNLP-03. 
Wiebe, J., Wilson, T., and Cardie, C. (2005). 
Annotating expressions of opinions and 
emotions in language. In Language 
Resources and Evaluation. Vol. 39. 
35
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1?10,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
EmotiBlog: a finer-grained and more precise learning of subjectivity expression models 
Ester Boldrini University of Alicante, Department of Software and Computing Systems eboldrini@dlsi.ua.es 
Alexandra Balahur University of Alicante, Department of Software and Computing Systems abalahur@dlsi.ua.es Patricio Mart?nez-Barco University of Alicante, Department of Software and Computing Systems patricio@dlsi.ua.es 
Andr?s Montoyo University of Alicante, Department of Software and Computing Systems montoyo@dlsi.ua.es  Abstract  
The exponential growth of the subjective in-formation in the framework of the Web 2.0 has led to the need to create Natural Language Processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. This paper presents EmotiBlog ? a fine-grained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the sys-tems using it for training, through the experi-ments we carried out on opinion mining and emotion detection. We employ corpora of dif-ferent textual genres ?a set of annotated re-ported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the SemEval 2007 (Task 14) and ISEAR, a corpus of real-life self-expressed emotion. We also show how the model built from the EmotiBlog annotations can be enhanced with external resources. The results demonstrate that EmotiBlog, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detec-tion. 1 Credits This paper has been supported by Ministe-rio de Ciencia e Innovaci?n- Spanish Gov-ernment (grant no. TIN2009-13391-C04-01), and Conselleria d'Educaci?n-Generalitat Valenciana (grant no. PRO-METEO/2009/119 and A-COMP/2010/288).  
2 Introduction The exponential growth of the subjective infor-mation with Web 2.0 created the need to develop new Natural Language Processing (NLP) tools to automatically process and manage the content available on the Internet. Apart from the tradi-tional textual genres, at present we have new ones such as blogs, forums and reviews. The main difference between them is that the latter are predominantly subjective, containing per-sonal judgments. At the moment, NLP tools and methods for analyzing objective information have a better performance than the new ones the research community is creating for managing the subjective content. The survey called ?The State of the Blogosphere 2009?, published by Tech-norati 1 , demonstrates that users are blogging more than ever. Furthermore, in contrast to the general idea about bloggers, each day it is more and more the number of professionals who de-cide to use this means of communication, contra-dicting the common belief about the predomi-nance of an informal editing (Balahur et al, 2009). Due to the growing interest in this text type, the subjective data of the Web is increasing on a daily basis, becoming a reflection of peo-ple?s opinion about a wide range of topics. (Cui, Mittal and Datar, 2006). Blogs represent an im-portant source of real-time, unbiased informa-tion, useful for the development of many applica-tions for concrete purposes. Given the proved importance of automatically processing this data, a new task has appeared in NLP task, dealing with the treatment of subjective data: Sentiment Analysis (SA). The main objective of this paper is to present EmotiBlog (Boldrini et al, 2009), a fine-grained annotation scheme for labeling sub-jectivity in the new textual genres. Subjectivity                                                 1 http://technorati.com/ 
1
can be reflected in text through expressions of emotions beliefs, views (a way of considering something) 2  and opinions, generally denomi-nated ?private states? (Uspensky, 1973), not open to verification (Wiebe, 1994). We per-formed a series of experiments focused on dem-onstrating that EmotiBlog represents a step for-ward to previous research in this field; its use allows a finer-grained and more precise learning of subjectivity expression models. Starting form (Wiebe, Wilson and Cardie, 2005) we created an annotation schema able to capture a wide range and key elements, which give subjectivity, mov-ing a step forward the mere polarity recognition. In particular, the experiments concern expres-sions of emotion, as a finer-grained analysis of affect in text and a subsequent task to opinion mining (OM) and classification. To that aim, we employ corpora of different textual genres? a set of annotated reported speech extracted from news articles (denominated JRC quotes) (Bala-hur et al, 2010) and the set of news titles anno-tated with polarity and emotion from the SemE-val 2007 Task No. 14 (Strapparava and Mihal-cea, 2007), as well as a corpus of real-life self-expressed emotion entitled ISEAR (Scherer and Walbott, 1999). We subsequently show, through the quality of the results obtained, that Emoti-Blog, through its structure and annotation para-digm, offers high quality training for systems dealing both with opinion mining, as well as emotion detection.  3 Motivation and Contribution The main motivation of this research is the dem-onstrated necessity to work towards the harmoni-zation and interoperability of the increasingly large number of tools and frameworks that sup-port the creation, instantiation, manipulation, querying, and exploitation of annotated resource. This necessity is stressed by the new tools and resources, which have been recently created for processing the subjectivity in the new-textual genres born with the Web 2.0. Such predomi-nantly subjective data is increasing at an expo-nential rate (about 75000 new blogs are reported to be created every day) and contains opinions on the most diverse set of topics. Given its world-wide availability, the subjective data on the Web has become a primary source of information (Balahur et al, 2009). As a consequence, new mechanisms have to be implemented so that this                                                 2 http://dictionary.cambridge.org/ 
data is effectively analyzed and processed. The main challenge of the opinionated content is that, unlike the objective one, which presents facts, the subjective information is most of the times difficult and complex to extract and classify us-ing in grammatically static and fixed rules. Ex-pression of subjectivity is more spontaneous and even if the majority is quite formal, new means of expressivity can be encountered, such as the use of colloquialisms, sayings, collocations or anomalies in the use of punctuation; this is moti-vated by the fact that subjectivity expression is part of our daily life. For example, at the time of taking a decision, people search for information and opinions expressed on the Web on their mat-ter of interest and base their final decision on the information found. At the same time, when using a product, people often write reviews on it, so that others can have a better idea of the perform-ance of that product before purchasing it. There-fore, on the one hand, the growing volume of opinion information available on the Web allows for better and more informed decisions of the users. On the other hand, the amount of data to be analyzed requires the development of special-ized NLP systems that automatically extract, classify and summarize the data available on the Web on different topics. (Esuli and Sebastiani, 2006) define OM as a recent discipline at the crossroads of Information Retrieval and Compu-tational Linguistics, which is concerned not with the topic a document is about, but with the opin-ion it expresses. Research in this field has proven the task to be very difficult, due to the high se-mantic variability of affective language. Differ-ent authors have addressed the problem of ex-tracting and classifying opinion from different perspectives and at different levels, depending on a series of factors which can be level of interest (overall/specific), querying formula (?Nokia E65?/?Why do people buy Nokia E65??), type of text (review on forum/blog/dialogue/press arti-cle), and manner of expression of opinion - di-rectly (using opinion statements, e.g. ?I think this product is wonderful!?/?This is a bright initia-tive?), indirectly (using affect vocabulary, e.g. ?I love the pictures this camera takes!?/?Personally, I am shocked one can pro-pose such a law!?) or implicitly (using adjectives and evaluative expressions, e.g. ?It?s light as a feather and fits right into my pocket!?). While determining the overall opinion on a movie is sufficient for taking the decision to watch it or not, when buying a product, people are interested in the individual opinions on the different prod-
2
uct characteristics. When discussing a person, one can judge and give opinion on the person?s actions. Moreover, the approaches taken can vary depending on the manner in which a user asks for the data (general formula such as ?opinions on X? or a specific question ?Why do people like X?? and the text source that needs to be queried). Retrieving opinion information in newspaper articles or blogs posts is more complex, because it involves the detection of different discussion topics, the subjective phrases present and subse-quently their classification according to polarity. Especially in the blog area, determining points of view expressed in dialogues together with the mixture of quotes and pastes from newspapers on a topic can, additionally, involve determining the persons present and whether or not the opinion expressed is on the required topic or on a point previously made by another speaker. This diffi-cult NLP problem requires the use of specialized data for system training and tuning, gathered, annotated and tested within the different text spheres. At the present moment, these specialized resources are scarce and when they exist, they are rather simplistically annotated or highly domain-dependent. Moreover, most of these resources created are for the English. The contribution we describe in this paper intends to propose solutions to the above-mentioned problems, and consists of the following points: first of all, we overcome the problem of corpora scarcity in other languages except English and also improve the English ones; we present the manner in which we compiled a multilingual corpus of blog posts on different topics of interest in three languages-Spanish, Italian and English. The second issue we tried to solve was the coarse-grained annotation schemas employed in other annotation schema. Thus, we describe the new annotation model, EmotiBlog built up in order to capture the different subjectivity/objectivity, emotion/opinion/attitude aspects we are interested in at a finer-grained level. We justify the need for a more detailed annotation model, the sources and the reasons taken into consideration when constructing the corpus and its annotation. Thirdly, we address an aspect strongly related to blogs annotation: due the presence of ?copy and pastes? from news articles or other blogs, the frequent quotes, we include the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level. We discuss on the problems encountered at different stages and comment upon some of the conclusions we have reached while performing this research. 
this research. Finally, we conclude on our ap-proach and propose the lines for future work. 4 Related Work In recent years, different researchers have ad-dressed the needs and possible methodologies from the linguistic, theoretical and practical points of view. Thus, the first step involved re-sided in building lexical resources of affect, such as WordNet Affect (Strapparava and Valitutti, 2004), SentiWordNet (Esuli and Sebastiani, 2006), Micro-WNOP (Cerini et. Al, 2007) or ?emotion triggers? (Balahur and Montoyo, 2009). All these lexicons contain single words, whose polarity and emotions are not necessarily the ones annotated within the resource in a larger context. We also employed the ISEAR corpus, consisting of phrases where people describe a situation when they felt a certain emotion. Our work, therefore, concentrates on annotating larger text spans, in order to consider the undeni-able influence of the context. The starting point of research in emotion is represented by (Balahur and Montoyo, 2008), who centered the idea of subjectivity around that of private states, and set the benchmark for subjectivity analysis as the recognition of opinion-oriented language in order to distinguish it from objective language and giv-ing a method to annotate a corpus depending on these two aspects ? MPQA (Wiebe, Wilson and Cardie, 2005). Furthermore, authors show that this initial discrimination is crucial for the senti-ment task, as part of Opinion Information Re-trieval  (last three editions of the TREC Blog tracks 3  competitions, the TAC 2008 competi-tion4), Information Extraction (Riloff and Wiebe, 2003) and Question Answering (Stoyanov et al, 2004) systems. Once this discrimination is done, or in the case of texts containing only or mostly subjective language (such as e-reviews), opinion mining becomes a polarity classification task. Our work takes into consideration this initial dis-crimination, but we also add a deeper level of emotion annotation. Since expressions of emo-tion are also highly related to opinions, related work also includes customer review classifica-tion at a document level, sentiment classification using unsupervised methods (Turney, 2002), Machine Learning techniques (Pang and Lee, 2002), scoring of features (Dave, Lawrence and Pennock, 2003), using PMI, syntactic relations                                                 3 http://trec.nist.gov/data/blog.html 4 http://www.nist.gov/tac/ 
3
and other attributes with SVM (Mullena and Col-lier, 2004), sentiment classification considering rating scales (Pang and Lee, 2002), supervised and unsupervised methods (Chaovalit and Zhou, 2005) and semisupervised learning (Goldberg and Zhou, 2006). Research in classification at a document level included sentiment classification of reviews (Ng, Dasgupta and Arifin, 2006), sen-timent classification on customer feedback data (Gamon, Aue, Corston-Oliver, Ringger, 2005), comparative experiments (Cui, Mittal and Datar, 2006). Other research has been conducted in ana-lysing sentiment at a sentence level using boot-strapping techniques (Riloff, Wiebe, 2003), con-sidering gradable adjectives (Hatzivassiloglou, Wiebe, 2000), semisupervised learning with the initial training some strong patterns and then ap-plying NB or self-training (Wiebe and Riloff, 2005) finding strength of opinions (Wolson, Wiebe, Hwa, 2004) sum up orientations of opin-ion words in a sentence (or within some word window) (Kim and Hovy, 2004), (Wilson and Wiebe, 2004), determining the semantic orienta-tion of words and phrases (Turney and Littman, 2003), identifying opinion holders (Stoyanov and Cardie, 2006), comparative sentence and relation extraction and feature-based opinion mining and summarization (Turney, 2002). Finally, fine-grained, feature-based opinion summarization is defined in (Hu and Liu, 2004) and researched in (Turney, 2002) or (Pang and Lee, 2002). All these approaches concentrate on finding and classifying the polarity of opinion words, which are mostly adjectives, without taking into ac-count modifiers or the context in general. Our work, on the other hand, represents the first step towards achieving a contextual comprehension of the linguistic roots of emotion expression. 5 Corpora It is well known that nowadays blogs are the second way of communication most used after the e-mail. They are extremely useful and a poll for discussing about any topic with the world. For this reason, the first corpus object of our study is a collection of blog posts extracted from the Web. The texts we selected have distinctive features, extremely different from traditional tex-tual ones. In fact people writing a post can use an informal language colloquialism, emoticons, etc. to express their feelings and it is not rare to find a mix of sources in the same post; people usually mention some facts or discourses and then they give their opinion about them. As we can deduce, 
the source detection represents one of the most complex tasks. As we mentioned above, we car-ried out a multilingual research, collecting texts in three languages: Spanish, Italian, and English about three subjects of interest. The first one contains blog posts commenting upon the signing of the Kyoto Protocol against global warming, the second collection consists of blog entries about the Mugabe government in Zimbabwe, and finally we selected a series of blog posts discuss-ing the issues related to the 2008 USA presiden-tial elections. For each of the abovementioned topics, we have gathered 100 texts, summing up a total of 30.000 words approximately for each language. However in this research we start with English but consider as future work labeling the other languages we have. The second corpus we employed for this research is a collection of 1592 quotes extracted from the news in April 2008. As a consequence they are about many different top-ics and in English (Balahur and Steinberg, 2009). Both of these corpora have been annotated with EmotiBlog that is presented in the next section. 6 EmotiBlog Annotation Model Our annotation schema can be defined as a fine-grained model for labelling subjectivity of the new-textual genres born with the Web 2.0. As mentioned above, it represents a step forward to previous research and it is focused on detecting the linguistic elements, which give subjectivity to the text. The EmotiBlog annotation is divided into different levels (Figure 1).  
 Figure 1: General structure of EmotiBlog.  As we can observe in Figure 1, the first distinc-tion to be made is between objective and subjec-tive speech. If we are labelling an objective sen-tence, we insert the source element, while if we are annotating a subjective discourse, a list of elements with the corresponding attributes have to be added. We select among the list of subjec-tive elements and specify the element?s attrib-
4
utes. Table 1 presents the annotation model in detail.  Elem. Description Obj. speech Confidence, comment, source, target. Subj. speech Confidence, comment, level, emotion, phenomenon, polarity, source and target. Adjectives Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Adverbs Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Verbs Confidence, comment, level, emotion, phenomenon, polarity, mode, source and target. Anaphora Confidence, comment, type, source and target. Capital letter Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Punctuation Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, source and target. Names Confidence, comment, level, emotion, phenomenon, modifier/not, polarity, and source. Phenomenon Confidence, comment, type: collocation, saying, slang, title, and rhetoric. Reader Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target. Author Inter-pretation Confidence, comment, level, emotion, phenomenon, polarity, source and target. Emotions Confidence, comment, accept, anger, anticipation, anxiety, appreciation, bad, bewilderment, comfort, ? Table 1: EmotiBlog structure  Each element of the discourse has its own attrib-utes with a series of features, which have to be annotated. Due to space reasons it is impossible to detail each one of them, however we would like to underline the most innovative and rel-evant. For each element we are labelling the an-notator has to insert his level of confidence. In this way we will assign each label a weight that will be computed for future evaluations. More-over, the annotator has to insert the polarity, which can be positive or negative, the level (high, medium, low) and also the sentiment this element is expressing. Table 2 presents a com-plete list of the emotions we selected to be part of EmotiBlog. We grouped all sentiments into subgroups in order to help the evaluation pro-cess. In fact emotions of the same subgroup will have less impact when calculating the inter-annotation agreement. In order to make this sub-division proper and effective division, we were inspired by (Scherer, 2005) who created an alter-native dimensional structure of the semantic space for emotions. The graph below represents the mapping of the term Russell (1983) uses for his claim of an emotion circumflex in two-
dimensional valence by activity/arousal space (upper-case terms). As we can appreciate, the circle is divided by 4 axes. Moreover, Scherer distinguishes between positive and negative sen-timents and after that between active and passive. Furthermore emotions are grouped between ob-structive and conductive, and finally between high power and low power control. We started form this classification, grouping sentiments into positive and negative, but we divided them as high/low power control, obstructive/conductive and active/passive. Further on, we distributed the sentiments within our list into the Scherer slots creating other smaller categories included in the abovementioned general ones. The result of this division is shown in Table 2: 
Table 2: Alternative dimensional structures of the semantic space for emotions  Following with the description of the model, we said that the first distinction to be made is be-tween objective and subjective speech. Analys-ing the texts we collected, we realised that even if the writer uses an objective speech, sometimes it is just apparently objective and for this reason we added two elements: reader and author inter-pretation. The first one is the impres-sion/feeling/reaction the reader has reading the intervention and what s/he can deduce from the piece of text and the author interpretation is what we can understand from the author (politic orien-tation, preferences). All this information can be deduced form some linguistic elements that ap-parently are not so objective as they may appear. Another innovative element we inserted in the model is the coreference but just at a cross-post level. It is necessary because blogs are composed by posts linked between them and thus cross-
Group Emotions Criticism Sarcasm, irony, incorrect, criticism, objection, opposition, scepticism. Happiness Joy, joke. Support Accept, correct, good, hope, support, trust, rapture, respect, patience, appreciation, excuse. Importance Important, interesting, will, justice, longing, anticipation, revenge. Gratitude Thank. Guilt Guilt, vexation. Fear Fear, fright, troubledness, anxiety. Surprise Surprise, bewilderment, disappoint-ment, consternation. Anger Rage, hatred, enmity, wrath, force, anger, revendication. Envy Envy, rivalry, jealousy. Indifference Unimportant, yield, sluggishness. Pity Compassion, shame, grief. Pain Sadness, lament, remorse, mourning, depression, despondency. Shyness Timidity. Bad Bad, malice, disgust, greed. 
5
document coreference can help the reader to fol-low the conversations. We also label the unusual usage of capital letters and repeated punctuation. In fact, it is very common in blogs to find words written in capital letter or with no conventional usage of punctuation; these features usually mean shouts or a particular mood of the writer. Using EmotiBlog, we annotate the single ele-ments, but we also mark sayings or collocations, representative of each language. A saying is a well-known and wise statement, which often has a meaning, different from the simple meanings of the words it contains5; while a collocation is a word or phrase, which is frequently used with another word or phrase, in a way that sounds cor-rect to native speakers, but might not be expected from the individual words? meanings6. Finally we insert for each element the source and topic. An example of annotation can be:  <phenomenon target="Kyoto Protocol" category="phrase" degree="medium" source="w" polarity="positive" emotion="good">The Onion has a <adjective target="Kyoto Protocol" phenomenon="phrase" de-gree="medium" polarity="positive" emotion="good" source="w" ismodifier="yes">great</adjective> story today titled ?Bush Told to Sign Birthday Treaty for Someone Named Kyoto." </phenomenon> 7 Experiments and Evaluation In order to evaluate the appropriateness of the EmotiBlog annotation scheme and to prove that the fine-grained level it aims at has a positive impact on the performance of the systems em-ploying it as training, we performed several ex-periments. Given that a) EmotiBlog contains an-notations for individual words, as well as for multi-word expressions and at a sentence level, and b) they are labeled with polarity, but also emotion, our experiments show how the anno-tated elements can be used as training for the opinion mining and polarity classification task, as well as for emotion detection. Moreover, tak-ing into consideration the fact that EmotiBlog labels the intensity level of the annotated ele-ments, we performed a brief experiment on de-termining the sentiment intensity, measured on a three-level scale: low, medium and high. In order to perform these three different evaluations, we chose three different corpora. The first one is a collection of quotes (reported speech) from newspaper articles presented in (Balahur et al, 2010), enriched with the manual fine-grained 
                                                5  Definition according to the Cambridge Advanced Learner?s Dictionary 6   Definition according to the Cambridge Advanced Learner?s Dictionary 
annotation of EmotiBlog7; the second one is the collection of newspaper titles in the test set of the SemEval 2007 task number 14 ? Affective Text. Finally, the third one is a corpus of self-reported emotional response ? ISEAR (Scherer and Wal-bott, 1999). The intensity classification task is evaluated only on the second corpus, given that it is the only one in which scores between -100 and 0 and 0 and 100, respectively, are given for the polarity of the titles.  6.1 Creation of training models For the OM and polarity classification task, we first extracted the Named Entities contained in the annotations using Lingpipe and united through a ?_? all the tokens pertaining to the NE. All the annotations of punctuation signs that had a specific meaning together were also united un-der a single punctuation sign. Subsequently, we processed the annotated data, using Minipar. We compute, for each word in a sentence, a series of features (some of these features are used in (Choi et al, 2005): ? the part of speech (POS)  ? capitalization (if all letters are in capitals, if only the first letter is in capitals, and if it is a NE or not) ? opinionatedness/intensity/emotion - if the word is annotated as opinion word, its polar-ity, i.e. 1 and -1 if the word is positive or negative, respectively and 0 if it is not an opinion word, its intensity (1.2 or 3) and 0 if it is not a subjective word, its emotion (if it has, none otherwise) ? syntactic relatedness with other opinion word ? if it is directly dependent of an opin-ion word or modifier (0 or 1), plus the polar-ity/intensity and emotion of this word (0 for all the components otherwise) ?  role in 2-word, 3-word and 4-word annota-tions: opinionatedness, intensity and emo-tion of the other words contained in the an-notation, direct dependency relations with them if they exist and 0 otherwise.  We compute the length of the longest sentence in EmotiBlog. The feature vector for each of the sentences contains the feature vectors of each of its words and 0s for the corresponding feature vectors of the words, which the current sentence has less than the longest annotated sentence. Fi-nally, we add for each sentence as feature binary features for subjectivity and polarity, the value corresponding to the intensity of opinion and the                                                 7 Freely available on request to the authors. 
6
general emotion. These feature vectors are fed into the Weka8 SVM SMO ML algorithm and a model is created (EmotiBlog I). A second model (EmotiBlog II) is created by adding to the collec-tion of single opinion and emotion words anno-tated in EmotiBlog, the Opinion Finder lexicon and the opinion words found in MicroWordNet, the General Inquirer resource and WordNet Af-fect.   6.2 Evaluation of models on test sets  In order to evaluate the performance of the mod-els extracted from the features of the annotations in EmotiBlog, we performed different tests. The first one regarded the evaluation of the polarity and intensity classification task using the Emoit-blog I and II constructed models on two test sets ? the JRC quotes collection and the SemEval 2007 Task Number 14 test set. Since the quotes often contain more than a sentence, we consider the polarity and intensity of the entire quote as the most frequent result in each class, corre-sponding to its constituent sentences. Also, given the fact that the SemEval Affective Text head-lines were given intensity values between -100 and 100, we mapped the values contained in the Gold Standard of the task into three categories: [-100, -67] is high (value 3 in intensity) and nega-tive (value -1 in polarity), [-66, 34] medium negative and [33, 1] is low negative. The values between [1 and 100] are mapped in the same manner to the positive category. 0 was consid-ered objective, so containing the value 0 for in-tensity. The results are presented in Table 3 (the values I and II correspond to the models Emoti-Blog I and EmotiBlog II):   Test  Corpus Evaluation type Precision Recall Polarity 32.13 54.09 JRC quotes I Intensity 36.00 53.2 Polarity 36.4 51.00 JRC quotes II Intensity 38.7 57.81 Polarity 38.57 51.3 SemEval I Intensity 37.39 50.9 Polarity 35.8 58.68 SemEval II Intensity 32.3 50.4 Table 3. Results for polarity and intensity classifi-cation using the models built from the EmotiBlog annotations The results shown in Table 2 show a signifi-cantly high improvement over the results ob-tained in the SemEval task in 2007. This is ex-plainable, on the one hand, by the fact that sys-                                                8 http://www.cs.waikato.ac.nz/ml/weka/ 
tems performing the opinion task did not have at their disposal the lexical resources for opinion employed in the EmotiBlog II model, but also because of the fact that they did not use machine learning on a corpus comparable to EmotiBlog (as seen from the results obtained when using solely the EmotiBlog I corpus). Compared to the NTCIR 8 Multilingual Analysis Task this year, we obtained significant improvements in preci-sion, with a recall that is comparable to most of the participating systems. In the second experi-ment, we tested the performance of emotion clas-sification using the two models built using Emo-tiBlog on the three corpora ? JRC quotes, SemE-val 2007 Task No.14 test set and the ISEAR cor-pus. The JRC quotes are labeled using Emoti-Blog; however, the other two are labeled with a small set of emotions ? 6 in the case of the Se-mEval data (joy, surprise, anger, fear, sadness, disgust) and 7 in ISEAR (joy, sadness, anger, fear, guilt, shame, disgust). Moreover, the Se-mEval data contains more than one emotion per title in the Gold Standard, therefore we consider as correct any of the classifications containing one of them. In order to unify the results and ob-tain comparable evaluations, we assessed the performance of the system using the alternative dimensional structures defined in Table 1. The ones not overlapping with the category of any of the 8 different emotions in SemEval and ISEAR are considered as ?Other? and are not included either in the training, nor test set. The results of the evaluation are presented in Table 4. Again, the values I and II correspond to the models EmotiBlog I and II. The ?Emotions? category contains the following emotions: joy, sadness, anger, fear, guilt, shame, disgust, surprise.  Test  corpus Evaluation  type Precision Recall JRC quotes I Emotions   24.7 15.08 
JRC quotes II Emotions  33.65 18.98 
SemEval I Emotions 29.03 18.89 SemEval II Emotions 32.98 18.45 ISEAR I Emotions 22.31 15.01 ISEAR II Emotions 25.62 17.83 Table 4. Results for emotion classification using the models built from the EmotiBlog annotations. The best results for emotion detection were ob-tained for the ?anger? category, where the preci-sion was around 35 percent, for a recall of 19 percent. The worst results obtained were for the ISEAR category of ?shame?, where precision was around 12 percent, with a recall of 15 per-
7
cent. We believe this is due to the fact that the latter emotion is a combination of more complex affective states and it can be easily misclassified to other categories of emotion. Moreover, from the analysis performed on the errors, we realized that many of the affective phenomena presented were more explicit in the case of texts expressing strong emotions such as ?joy? and ?anger?, and were mostly related to common-sense interpreta-tion of the facts presented in the weaker ones. As it can be seen in Table 3, results for the texts per-taining to the news category obtain better results, most of all news titles. This is due to the fact that such texts, although they contain a few words, have a more direct and stronger emotional charge than direct speech (which may be biased by the need to be diplomatic, find the best suited words etc.). Finally, the error analysis showed that emo-tion that is directly reported by the persons expe-riencing is more ?hidden?, in the use of words carrying special signification or related to gen-eral human experience. This fact makes emotion detection in such texts a harder task. Neverthe-less, the results in all corpora are comparable, showing that the approach is robust enough to handle different text types. All in all, the results obtained using the fine and coarse-grained anno-tations in EmotiBlog increased the performance of emotion detection as compared to the systems in the SemEval competition.   6.3 Discussion on the overall results  From the results obtained, we can see that this approach combining the features extracted from the EmotiBlog fine and coarse-grained annota-tions helps to balance between the results ob-tained for precision and recall. The impact of using additional resources that contain opinion words is that of increasing the recall of the sys-tem, at the cost of a slight drop in precision, which proves that the approach is robust enough so that additional knowledge sources can be added. Although the corpus is small, the results obtained show that the phenomena it captures is relevant in the OM task, not only for the blog sphere, but also for other types of text (newspa-per articles, self-reported affect). 8 Conclusions and future work Due to the exponential increase of the subjective information result of the high-level usage of the Internet and the Web 2.0, NLP able to process this data are required. In this paper we presented 
the procedure by which we compiled a multilin-gual corpus of blog posts on different topics of interest in three languages: Spanish, Italian and English. Further on, we explained the need to create a finer-grained annotation schema that can be used to improve the performance of subjectiv-ity mining systems. Thus, we presented the new annotation model, EmotiBlog and justified the benefits of this detailed annotation schema, pre-senting the sources and the reasons taken into consideration when building up the corpus and its labeling. Furthermore, we addressed the pres-ence of ?copy and pastes? from news articles or other blogs, the frequent quotes. For solving this possible ambiguity we included the annotation of both the directly indicated source, as well as the anaphoric references at cross-document level. We performed several experiments on three dif-ferent corpora, aimed at finding and classifying both the opinion, as well as the expressions of emotion they contained; we showed that the fine and coarse-grained levels of annotation that EmotiBlog contains offers important information on the structure of affective texts, leading to an improvement of the performance of systems trained on it. Although the EmotiBlog corpus is small, the results obtained are promising and show that the phenomena it captures are relevant in the OM task, not only for the blog sphere, but also for other textual-genres. It is well known that OM is an extremely challenging task and a young discipline, thus there is room for im-provement above all to solve linguistic phenom-ena such as the correference resolution at a cross document level, temporal expression recognition. In addition to this, more experiments would need to be done in order to verify the complete ro-bustness of EmotiBlog. Last but not least, our idea is to include the existing tools for a more effective semi-supervised annotation. After the training of the ML system we obtain automati-cally some markables which have to be validated or not by the annotator and the ideal option would be to connect these terms the system de-tects automatically with tools, such as the map-ping with an opinion lexicon based on WordNet (SentiWordNet, WordNet Affect, MicroWord-Net), in order to automatically annotate all the synonyms and antonyms with the same or the opposite polarity respectively and assigning them some other elements contemplated into the Emo-tiBlog annotation schema. This would mean an important step forward for saving time during the annotation process and it will also assure a high quality annotation due to the human supervision. 
8
References Balahur A., Steinberger R., Kabadjov M., Zavarella V., van der Goot E., Halkia M., Pouliquen B., and Belyaeva J. 2010. Sentiment Analysis in the News.  In Proceedings of LREC 2010. Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009. A Comparative Study of Open Domain and Opinion Question Answering Systems for Fac-tual and Opinionated Queries. In Proceedings of the Recent Advances in Natural Language Proc-essing. Balahur A., Montoyo A. 2008. Applying a Culture Dependent Emotion Triggers Database for Text Valence and Emotion Classification. In Proceed-ings of the AISB 2008 Symposium on Affective Language in Human and Machine, Aberdeen, Scot-land. Balahur A., Steinberger R., Rethinking Sentiment Analysis in the News: from Theory to Practice and back. In Proceeding of WOMSA 2009. Seville. Balahur A., Boldrini E., Montoyo A., Mart?nez-Barco P. 2009. Summarizing Threads in Blogs Using Opinion Polarity. In Proceedings of ETTS work-shop. RANLP. 2009. Boldrini E., Balahur A., Mart?nez-Barco P., Montoyo A. 2009. EmotiBlog: a fine-grained model for emotion detection in non-traditional textual gen-res. In Proceedings of WOMSA. Seville, Spain. Boldrini E., Fern?ndez J., G?mez J.M., Mart?nez-Barco P. 2009. Machine Learning Techniques for Automatic Opinion Detection in Non-Traditional Textual Genres. In Proceedings of WOMSA 2009. Seville, Spain. Chaovalit P, Zhou L. 2005. Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches. In Proceedings of HICSS-05. Carletta J. 1996. Assessing agreement on classification task: the kappa statistic. Computa-tional Linguistics, 22(2): 249?254. Cui H., Mittal V., Datar M. 2006. Comparative Ex-periments on Sentiment Classification for Online Product Reviews. In Proceedings of the 21st Na-tional Conference on Artificial Intelligence AAAI. Cerini S., Compagnoni V., Demontis A., Formentelli M., and Gandini G. 2007. Language resources and linguistic theory: Typology, second language ac-quisition. English linguistics (Forthcoming), chap-ter Micro-WNOp: A gold standard for the evalua-tion of automatically compiled lexical resources for opinion mining. Franco Angeli Editore, Milano, IT. Choi Y., Cardie C., Rilloff E., Padwardhan S. 2005. Identifying Sources of Opinions with Conditional Random  Fields and Extraction Patterns.  In Pro-ceedings of the HLT/EMNLP.  Dave K., Lawrence S., Pennock, D. ?Mining the Pea-nut Gallery: Opinion Extraction and Semantic Classification of Product Reviews?. In Proceedings of WWW-03. 2003. 
Esuli A., Sebastiani F. 2006. SentiWordNet: A Pub-licly Available Resource for Opinion Mining. In Proceedings of the 6th International Conference on Language Resources and Evaluation, LREC 2006, Genoa, Italy.  Gamon M., Aue S., Corston-Oliver S., Ringger E. 2005. Mining Customer Opinions from Free Text. Lecture Notes in Computer Science. Goldberg A.B., Zhu J. 2006. Seeing stars when there aren?t many stars: Graph-based semi-supervised learning for sentiment categorization. In HLT-NAACL 2006 Workshop on Textgraphs: Graph-based Algorithms for Natural Language Process-ing. Hu M., Liu B. 2004. Mining Opinion Features in Cus-tomer Reviews. In Proceedings of Nineteenth Na-tional Conference on Artificial Intelligence AAAI. Hatzivassiloglou V., Wiebe J. 2000. Effects of adjec-tive orientation and gradability on sentence subjec-tivity. In Proceedings of COLING.  Kim S.M., Hovy E. 2004. Determining the Sentiment of Opinions. In Proceedings of COLING. Mullen T., Collier N. 2006. Sentiment Analysis Using Support Vector Machines with Diverse Information Sources. In Proceedings of EMNLP. 2004. Lin, W.H., Wilson, T., Wiebe, J., Hauptman, A. ?Which Side are You On? Identifying Perspectives at the Document and Sentence Levels?. In Proceedings of the Tenth Conference on Natural Language Learn-ing CoNLL.2006.  Ng V., Dasgupta S. and Arifin S. M. 2006. Examining the Role of Linguistics Knowledge Sources in the Automatic Identification and Classification of Re-views. In the proceedings of the ACL, Sydney. Pang B., Lee L., Vaithyanathan S. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Lan-guage Processing. Riloff E., Wiebe J. 2003. Learning Extraction Pat-terns for Subjective Expressions. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.  Strapparava C. Valitutti A. 2004. WordNet-Affect: an affective extension of WordNet. In Proceedings ofthe 4th International  Conference on Language Resources and Evaluation, LREC. Russell J.A. 1983. Pancultural aspects of the human conceptual organization of emotions. Journal of Personality and Social Psychology 45: 1281?8. Scherer K. R. 2005. What are emotions? And how can they be measured? Social Science Information, 44(4), 693?727. Stoyanov V. and Cardie C. 2006. Toward Opinion Summarization: Linking the Sources. COLING-ACL. Workshop on Sentiment and Subjectivity in Text. Stoyanov V., Cardie C., Litman D., and Wiebe J. 2004. Evaluating an Opinion Annotation Scheme Using a New Multi-Perspective Question and An-
9
swer Corpus. AAAI Spring Symposium on Explor-ing Attitude and Affect in Text.  Strapparava and Mihalcea, 2007 - SemEval 2007 Task 14: Affective Text. In  Proceedings of the ACL.   Turney P. 2002. Thumbs Up or Thumbs Down? Se-mantic Orientation Applied to Unsupervised Clas-sification of Reviews. ACL 2002: 417-424. Turney P., Littman M. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems 21. Uspensky B. 1973. A Poetics of Composition. Univer-sity of California Press, Berkeley, California. Wiebe J. M. 1994. Tracking point of view in narra-tive. Computational Linguistics, vol. 20, pp. 233?287. Wiebe J., Wilson T. and Cardie C. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation. Wilson T., Wiebe J., Hwa R. 2004. Just how mad are you? Finding strong and weak opinion clauses. In: Proceedings of AAAI. Wiebe J., Wilson T. and Cardie C. 2005. ?Annotation Expressions of Opinions and Emotions in Lan-guage. Language Resources and Evaluation.  Wiebe J., Riloff E. 2005. Creating Subjective and Objective Sentence Classifiers from Unannotated Texts. In Proceedings of the 6th International Con-ference on Computational Linguistics and Intelli-gent Text Processing (CICLing).      
10
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 153?160,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
EMOCause: An Easy-adaptable Approach to Emotion Cause Contexts 
 Irene Russo  Tommaso Caselli Francesco Rubino ILC ?A.Zampolli? ? CNR  Via G. Moruzzi, 1- 56124 Pisa {irene.russo}{tommaso.caselli}{francesco.rubino}@ilc.cnr.it Ester Boldrini  Patricio Mart?nez-Barco DSLI ? University of Alicante Ap. de Correos, 99 ? 03080 Alicante {eboldrini}{patricio}@dlsi.ua.es    Abstract 
In this paper we present a method to automatically identify linguistic contexts which contain possible causes of emotions or emotional states from Italian newspaper articles (La Repubblica Corpus). Our methodology is based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge on emotional states and emotion eliciting situations. Our approach has been evaluated with respect to manually annotated data. The results obtained so far are satisfying and support the validity of the methodology proposed. 
1 Introduction As it has been demonstrated in Balahur et al (2010), mining the web to discriminate between objective and subjective content and extract the relevant opinions about a specific target is today a crucial as well as a challenging task due to the growing amount of available information.  Opinions are just a part of the subjective content, which is expressed in texts. Emotions and emotional states are a further set of subjective data. Natural Language is commonly used to express emotions, attribute them and, most importantly, to indicate their cause(s).  Due to the importance of linking the emotion to its cause, a recent subtask of Sentiment Analysis 
(SA) consists in the detection of the emotion cause event (ECE, Lee et al, 2010; Chen et al, 2010) and focuses on the identification of the phrase (if present, as in 1 in bold) mentioning the event that is related to the emotional state (in italics):   (1) Non poteva mancare un accenno alla strage di Bologna, che costringe l' animo a infinita vergogna. [There was a mention of Bologna massacre,   that forces us to feel ashamed.]  This kind of information is extremely interesting, since it can provide pragmatic knowledge about content words and their emotional/subjective polarity and consequently it can be employed for building up useful applications with practical purposes.  The paper focuses on the development of a method for the identification of Italian sentences which contain an emotion cause phrase. Our approach is based on the interplay between linguistic patterns which allow the retrieval of emotion ? emotion cause phrase couples and on the exploitation of an associated incremental repository of commonsense knowledge about events which elicit emotions or emotional states. The methodology is only partially language dependent and this approach can be easily extended to other languages such as Spanish. The repository is one of the main results of this work. It allows the discovery of pragmatic knowledge associated with various content 
153
words and can assign them a polarity value which can be further exploited in more complex SA and Opinion Mining tasks. The present paper is structured as follows. Section 2 shortly describes related work and state of the art on this task. Section 3 focuses on the description of the methodology. Section 4 describes the annotation scheme and the corpus used for the creation of the test set. Section 5 reports on the experiments and their results. Conclusions and future works are described in Section 6. 1 Related Works Emotional states are often triggered by the perception of external events (pre-events) (Wierzbicka, 1999). In addition to this, emotional states can also be the cause of events (post-events; Chun-Ren, 2010). This suggests to consider emotional states as a pivot and structure the relations between emotional states and related events as a tri-tuple of two pairs:  (2) <<pre-events, emotional state> <emotional state, post-event>>  This study focuses on the relationship between the first pair of the tri-tuple, namely pre-events (or ECE), and emotional states.  Previous works on this task have been carried out for Chinese (Lee et al, 2009, Chen et al, 2009, Lee et al, 2010). ECE can be explicitly expressed as arguments, events, propositions, nominalizations and nominals. Lee et al(2010) restrict the definition of ECE as the immediate cause of the emotional state which does not necessarily correspond to the actual emotional state trigger or what leads to the emotional state.  Their work considers all possible linguistic realization of EKs (nouns, verbs, adjectives, prepositional phrases) and ECEs.  On the basis of an annotated corpus, correlations between emotional states and ECEs have been studied in terms of different linguistic cues (e.g. position of the cause events, presence of epistemic markers...) thus identifying seven groups of cues. After that, they have been implemented in a rule-based system, which is able to identify: i.) the EK; ii.) the ECE and its position (same sentence as the EK, previous sentence with 
respect to the EK, following sentence with respect to the EK) and (iii.) the experiencer of the emotional state(s).The system evaluation has been performed on the annotated corpus in two phases: firstly, identifying those sentences containing a co-occurrence of EK and ECE; secondly, for those contexts where an EK and ECE co-occurs, identifying the correct ECE. Standard Precision, Recall and F-measure have been used. The baseline is computed by assuming that the first verb on the left of the EK is the ECE. The system outperforms the baseline f-score by 0.19. Although the results are not very high, the system accuracy for the detection of ECEs is reported to be three times more accurate than the baseline. 2 Emotional states between linguistic patterns and commonsense knowledge The work of Lee et al (2010) represents the starting point for the development of our method. We depart from their approach in the following points: i.) use of data mining techniques (clustering plus a classifier) to automatically induce the rules for sentential contexts in which an event cause phrase is expressed; and ii.) exploitation of a commonsense repository of EK - eliciting ECE noun couples for the identification of the correct ECE noun. The remaining of this section will describe in details the creation of the repository and the methodology we have adopted. 2.1 A source for commonsense knowledge of EKs and ECEs in Italian Recently crowdsourcing techniques that exploit the functionalities of the Web 2.0 have been used in AI and NLP for reducing the efforts, costs and time for the creation of Language Resources. We have exploited the data from an on-line initiative launched in December 2010 by the Italian newspaper ?Il Corriere della Sera? which asked its readers to describe the year 2010 with 10 words. 2,378 people participated in the data collection for a total of 22,469 words. We exploited these data to identify preliminary couples of emotional states and cause events, and thus create a repository of affective commonsense knowledge, by extracting all 
154
bigrams realized by nouns for a total of 18,240 couples noun1-noun2. After this operation, an adapted Italian version of WN-Affect (Strapparava ? Valitutti, 2004) obtained by means of mapping procedures through MultiWordNet (MWN) has been applied to each item of the bigrams. By means of a simple query, we have extracted all bigrams where at least one item has an associated sense corresponding to the ?emotion? category in WN-Affect. We have applied WN-Affect again to these results and extracted only those bigrams where the unclassified item corresponded to the WN-Affect label of ?emotion eliciting situation?. Finally, two lists of keywords have been obtained: one denoting EKs (133 lemmas) and the other denoting possible ECEs associated with a specific EK. The possible ECEs have been extended by exploiting MWN synsets and lexical relations of similar-to, pertains-to, attribute and is-value-of. We have filtered the set of ECE keywords by selecting only those nouns whose top nodes uniquely belongs to the following ontological classes, namely: event, state, phenomenon, and act. After this operation we have 161 nominal lemmas of possible ECEs.  2.2 Exploiting the repository for pattern induction The preliminary version of the repository of EK - ECE couples has been exploited in order to identify relevant syntagmatic patterns for the detection of nominal ECEs. The pattern induction phase has been performed on a parsed version of a large corpus of Italian, the La Repubblica Corpus (Baroni et al, 2004).  We have implemented a pattern extractor that takes as input the couples of the seed words from the commonsense repository and extracted all combinations of EKs and its/their associated ECEs occurring in the same sentence, with a distance ranging from 1 to 8 possible intervening parts-of-speech. We have thus obtained 1,339 possible patterns. This set has been cleaned both on the basis of pattern frequencies and with manual exploration. In total 47 patterns were selected and were settled among the features for the clustering and classifier ensemble which will be exploited for the identification of the 
sentential contexts which may contain an emotion cause phrase (see Section 5 for details). 3 Developing a gold standard and related annotation scheme With the purpose of evaluating the validity and reliability of our approach, a reference annotated corpus (gold standard) has been created.  The data collection has been performed in a semi-automatic way. In particular, we have extracted from an Italian lexicon, SIMPLE/CLIPS (Ruimy et al, 2003), all nouns marked with semantic type ?Sentiment? to avoid biases for the evaluation and measure the coverage of the commonsense repository. The keywords have been used to query the La Repubblica Corpus and thus creating the corpus collection. We have restricted the length of the documents to be annotated to a maximum of three sentences, namely the sentence containing the emotion keyword, the one preceding it and the sentence immediately following. As a justification for this choice, we have assumed that causes are a local focus discourse phenomenon and should not be found at a long distance with respect to their effects (i.e. the emotion keyword). Finally, the corpus is composed by 6,000 text snippets for a total of 738,558 tokens.  The corresponding annotation scheme, It-EmoCause, is based on recommendations and previous experience in event annotation (ISO-TimeML), emotion event annotation (Lee et al, 2009, Chen et al, 2010), emotion and affective computing annotation (EARL1, the HUMAINE Emotion Annotation and Representation Language, EmotiBlog, Boldrini et al 2010). The scheme applies at two levels: phrase level and token level and it allows nested tags. Figure 1 reports the BNF description of the scheme.  Text consuming markables are  <emotionWord>, <causePhrase> and <causeEmotion> tags, which are responsible, respectively, for marking the emotion keyword, the phrase expressing the cause emotion event and the token expressing the cause emotion. The values of the attribute emotionClass is derived from Ekman                                                            1 http://emotion-research.net/earl 
155
(1972)'s classification and extended with the value UNDERSPECIFIED. This value is used as a cover term for all other types of emotion reducing disagreement and allowing further classifications on the basis of more detailed and different lists of emotions that each user can specify. Finally, the non-text consuming <EmLink> link puts in relation the cause emotion event or phrase with the emotion keyword.  entry ::= <emotionWord> <causePhrase>+ <ELink>*  <emotionWord> ::= ewid lemma emotionClass appraisalDimension, emotionHolder polarity comment ewid ::= ew<digit> lemma ::= CDATA emotionClass ::= HAPPINESS | ANGER | FEAR | SURPRISE| SADNESS| DISGUST |               UNDERSPECIFIED appraisalDimension ::= CDATA emotionHolder ::= CDATA polarity ::= POSITIVE | NEGATIVE comment ::= CDATA  <causePhrase> ::= epid <causeEmotion>+ epid ::= ep<digit> <causeEmotion> ::= eid lemma eid ::= e<digit> lemma ::= CDATA  <EmLink> ::= elid linkType emotionInstanceID causeEventInstanceID causePhraseID comment elid ::= el<digit> linkType ::= POSITIVE | NEGATIVE  relatedToEmotion ::= IDREF {relatedToEmotion ::= ewid} causeEventID ::= IDREF {causeEventID ::= eid} causePhraseID  ::= IDREF {causePhraseID ::= epid} comment ::= CDATA Figure 1 ? BNF description of the EmoContext Scheme  The annotation has been performed by two expert linguists and validated by a judge. The tool used for the annotation is the Brandeis Annotation Tool (BAT)2. The corpus is currently under annotation and we concentrated mainly on the development of a test set. Not all markables and attributes have been annotated in this phase.  
                                                           2 http://www.batcaves.org/bat/tool/ 
The inter-annotator agreement (IAA)3 on the detection of the cause event and the cause phrase are not satisfactory. To have reliable data, we have adopted a correction strategy by asking the annotators to assign a common value to disagreements. This has increased the IAA on cause emotion to K=0.45, and P&R= 0.46. A revision procedure of the annotation guidelines is necessary and annotation specifications must be developed so that the disagreement can be further reduced. Table 1 reports the figures about the annotated data so far.  It-EmoContext Corpus # of tokens 32,525 # of emotion keyword 356 # of cause emotion 84 # of causePhrase emotion  104 # emotion ? cause emotion couples 95 # of emotion ? cause phrase couples 121 Agreement on emotion keyword detection K = 0.91 P&R = 0.91 Agreement on cause emotion detection K = 0.34 P&R = 0.33 Agreement on causePhrase detection K = 0.21 P&R = 0.26 Table 1 - It-EmoContext Corpus Figures 4 Emotion cause detection: experiments and results In order to find out a set of rules for the detection of emotion cause phrase contexts, we experimented a combination of Machine Learning techniques, namely clustering and rule induction classifier algorithms. In particular, we want to exploit the output of a clustering algorithm as input to a rule learner classifier both available in the Weka platform (Witten and Frank, 2005). The clustering algorithm is the Expectation-Maximization algorithm (EM; Hofmann and Puzicha, 1998). The EM is an unsupervised algorithm, commonly used for model-based                                                            3 Cohen's Kappa, Precision and Recall have been used for computing the IAA. 
156
clustering and also applied in SA tasks (Takamura et al 2006). In this work, we equipped the EM clustering model with syntagmatic, lexical and contextual features. The clustering algorithm has been trained on 2,000 corpus instances of the potential EK - ECE couples of the repository from the La Repubblica corpus along with a three sentence context (i.e the sentence immediately preceding and that immediately following the sentence containing the EK). Four groups of features have been identified: the first set of features corresponds to a re-adaptation of the rules implemented in Lee et al (2010); the second set of features implements the 47 syntagmatic patterns that specifically codify the relation between the EK and the ECE (see Section 3.2); the last two set of features are composed, respectively, by a list of intra-sentential bigrams, trigrams and fourgrams for a total of 364 different part-of-speech sequences with the EK as the first element and by a list of 6 relevant collocational patterns which express cause-effect relationship between the  ECE and the EK, manually identified on the basis of the authors' intuitions. In Table 2 some examples of each group of features are reported4.   Group of feature Instance Re-adaptation of Lee et al, 2010's rules Presence of an ECE after the EK in the same sentence Syntagmatic patterns manually identified S E S | S E RI S | S V RI A S ... Bigrams, trigrams and fourgrams POS sequences S EA | S EA AP | S EA AP S  Relevant collocational patterns S A per RD/RI S ... Table 2 ? Features for the EM cluster.  We expected two data clusters, one which includes cause emotion sentential contexts where the EK and the emotion cause co-occurs in the same sentence and another where either 
                                                           4 The tags S, EA, RI and similar reported for the last three groups of features are abbreviations for the POS used by the parser. The complete list can be found at http://medialab.di.unipi.it/wiki/Tanl_POS_Tagset 
the emotion cause it is not present or it occurs in a different sentence (i.e. the one before the EK or in the one following it). In order to evaluate the goodness of the cluster configuration created by the Weka version of the EM algorithm, we have run different clustering experiments. The results of each clustering analysis have been passed to the Weka PART rule-induction classifier. The best results were those which confirmed our working hypothesis, i.e. two clusters. The first cluster contains 869 items while the second 1,131 items.  The PART classifier provided a total of 49 detection rules for the detection of EK ? ECE contexts. The classifier identifies the occurrence of a cause phrase in the same sentence but is not able to identify the noun which corresponds to the ECE. The evaluation of the classifier has been performed on the 121 couples of EK ? cause phrase of the test set. As we are aiming at spotting nominal causes of EKs, we have computed the baseline by considering as the correct phrase containing the ECE the first noun phrase occurring at the right of the emotion keyword and in the same sentence since this kind of ECEs tends to occur mostly at this position. In this way the baseline has an accuracy of  0.14 (only 33 NPs were correct over a total of 227 NPs at the right of the EKs). By applying the rules of the PART classifier, we have obtained an overall accuracy of 0.71, outperforming the baseline. As for the identification of the EK - cause phrase couples occurring in the same sentence, we computed standard Precision, Recall and F-measure. The results are reported in Table 3. The system tends to have a high precision (0.70) and a low recall (0.58).    Total Correct P R F EK ? cause phrase couple 121 85 0.70 0.58 0.63 Table 3 ? Evaluation of the classifier in detecting EF ? cause phrase couples.  After this, we tried to identify the correct nominal ECE in the cause phrase. Provided the reduced dimensions of the annotated corpus, no training set was available to train a further 
157
classifier. Thus, to perform this task we decided to exploit the commonsense repository. However, the first version of the repository is too small to obtain any relevant results. We enlarged it by applying two set of features (the syntagmatic patterns manually identified and the collocational patterns used for the clustering analysis). 4.1 Incrementing the repository and discovering EK ? ECE couples Our hypothesis is that the identification of the ECE(s) in context could be performed by looking for a plausible set of nouns which are associated with a specific EK and assumed to be its cause. This type of information is exactly the one contained in the repository described in Section 3.1. In order to work with a larger data set of ECE entries per emotion keyword, we have applied the syntagmatic patterns manually identified and the collocational patterns on two corpora: i.) La Repubblica and ii.) ItWaC5 (Baroni et al, 2009). For each EK - ECE couple identified we have kept track of the co-occurrence frequencies and computed the Mutual Information (MI). Frequency and MI are extremely relevant because they provide a reliability threshold for each couple of EK and ECE. In Table 4 we report some co-occurrences of the EK ?ansia? [anxiety] and ECEs.  ECE Frequency (La Repubblica Corpus) Mutual  Information crisi [crisis] 119 5,514 angoscia [anguish] 80 8.762 guerra [war] 185 6.609 pianificazione [planning] 1 4.117 ricostruzione [reconstruction] 19 5.630 Table 4- ECEs co-occurrences with EK ?ansia?[anxiety].  Each ECE has been associated to a probability measure of eventivity derived from MWN top                                                            5 http://wacky.sslmit.unibo.it 
ontological classes, obtained from the ratio between 1 and the sum of all top ontological classes associated to the ECE lemma. The top nodes ?event?, ?state?, ?phenomenon?, and ?act? have been considered as a unique top class by applying the TimeML definition of event6. This measure is useful in case more than one ECEs is occurring in the context in analysis as a disambiguation strategy. In fact, if more than one ECEs is present, that with the higher frequency, MI and eventivity score should be preferred.  Furthermore, to make the repository more effective and also to associate an emotional polarity to the ECEs (i.e. whether they have positive, negative or neutral values) we have further extended the set of information by exploiting WN-Affect 1.1. In particular we have associated each EK to its emotional category (e.g. despondency, resentment, joy) and its emotional superclass (e.g. positive-emotion, negative-emotion, ambiguous-emotion).  This extended version of the repository has been applied to identify the correct ECE noun for the 95 couples of EK ? ECE in the test set. We have splitted the whole set of EK ? ECE couples into two subgroups: i.) EK ? ECE couples occurring in the same sentence (82/95); and ii.) EK ? ECE couples occurring in different sentences (13/95). By applying the repository to the first group, we were able to correctly identify 50% (41/82) of the ECE nouns for each specific EK when occurring in the same sentence. Moreover, we applied the repository also to the EK ? ECE couples of the second group: a rough 30.76% (4/13) of the ECE occurring in sentences other that the one containing the EK can be correctly retrieved without increasing the number of false positives. This is possible thanks to the probability score computed by means of MWN top ontological classes, even if the number of annotated examples is too small to justify strong conclusions. 
                                                           6 To clarify, the ECE ?guerra? [war] has four senses in MWN. Three of them belong to the top ontological class of ?event? and one to ?state?. This possible ECE has 1 top ontological node, and its eventivity mesure is 1. 
158
5 Conclusions and future works  In this paper we describe a methodology based on the interplay between relevant linguistic patterns and an incremental repository of common sense knowledge of EK ? ECE couples, which can be integrated into more complex systems for SA and Opinion Mining. The experimental results show that clustering techniques (EM clustering model) and a rule learner classifier (the PART classifier) can be efficiently combined to select and induce relevant linguistic patterns for the discovery of EK ? ECE couples in the same sentence. The information thus collected has been organized into the repository of commonsense knowledge about emotions and their possible causes. The repository has been extended by using corpora of varying dimensions (la Repubblica and ItWaC) and effectively used to identify ECEs of specific emotion keywords.  One interesting aspect of this approach is represented by the reduced manual effort both for the identification of linguistic patterns for the extraction of reliable information and for the maintenance and extension of specific language resources which can be applied also to domains other than SA. In addition to this, the method can be extended and applied to identify ECE realized by other POS, such as verbs and adjectives. As future works, we aim to extend the repository by extracting data from the Web and connecting it to SentiWordNet and WN-Affect. In particular, the connection to the existing language resources could be used to spot possible misclassifications and polarity values. Acknowledgments The authors want to thank the RSC Media Group. This work has been partially founded by the projects TEXTMESS 2.0 (TIN2009-13391-C04-01), Prometeo (PROMETEO/2009/199), the Generalitat valenciana (ACOMP/2011/001) and the EU FP7 project METANET (grant agreement n? 249119) References  Baccianella S., A. Esuli and F. Sebastiani. (2010) . SentiWordNet 3.0: An Enhanced Lexical Resource 
for Sentiment Analysis and Opinion Mining. In:  Proceedings of the 7th conference on International Language Resources and Evaluation (LREC 2010), Malta, May 2010 Balahur A., R. Steinberger, M.A. Kabadjov, V. Zavarella, E. van der Goot, M. Halkia, B. Pouliquen, J. Belyaeva. (2010). Sentiment Analysis in the News. In: Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010), Malta, May 2010.  Baroni, M., Bernardini, S., Comastri, F., Piccioni, L., Volpi, A., Aston, G.,Mazzoleni, M. (2004). Introducing the ?la Repubblica? corpus: A large, annotated, TEI(XML)-compliant corpus of newspaper italian. In: Proceedings of the 4th International conference on Language Resources and Evaluation (LREC-04), Lisbon, May 2004.  Boldrini E, A. Balahur, P. Mart?nez-Barco and A. Montoyo. (2010). EmotiBlog: a finer-grained and more precise learning of subjectivity expression models. In: Proceedings of the Fourth Linguistic Annotation Workshop (LAW IV '10). Association for Computational Linguistics. Chen Y., S.Y.M. Lee, S. Li, and C. Huang. (2010) Emotion Cause Detection with Linguistic Constructions. In: Proceeding of the 23rd International Conference on Computational Linguistics (COLING 2010). Ekman, P. (1972). Universals And Cultural Differences In Facial Expressions Of Emotions.In: J. Cole (ed.), Nebraska Symposium on Motivation, 1971. Lincoln, Neb.: University of Nebraska Press, 1972. pp. 207- 283.3. Huang, C. (2010). Emotions as Events (and Cause as Pre-Events). Communication at the Chinese Temporal/discourse annotation workshop, Los Angeles, June 2010,. Lee S.Y.M., Y. Chen, C. Huang. (2010). A Text-driven Rule-based System for Emotion Cause Detection. In: Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text. Pianta, E., Bentivogli, L., Girardi, C. (2002). Multiwordnet: Developing and aligned multilingual database. In: Proceedings of the First International Conference on Global WordNet, Mysore, India, January 2002. Pustejovsky, J., Castao, J., Saur`?, R., Ingria, R., Gaizauskas, R., Setzer, A., Katz, G. (2003). TimeML: Robust specification of event and temporal expressions in text. In: Proceedings of 
159
the 5th International Workshop on Computational Semantics (IWCS-5). Ruimy, N., Monachini, M., Gola, E., Calzolari, N., Fiorentino, M.D., Ulivieri, M., Rossi, S. (2003). A computational semantic lexicon of italian: SIMPLE. In: Linguistica Computazionale XVIII-XIX, Pisa, pp. 821?64 Schroeder M., H. Pirker and M. Lamolle. (2006). First Suggestion for an Emotion Annotation and Representation Language. In: Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), Genoa, May 2006.  
Strapparava C. and A. Valitutti. (2004) WordNet-Affect: an affective extension ofWordNet". In: Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon, May 2004. Takamura H., I. Takashi, M. Okumura. (2006). Latent Variables Models for Semantic Orientation of Phrases. In: Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2006).  Wierzbicka, A. (1999) Emotion Across Languages and Cultures Diversity and Universals. Cambidge.CUP. 
160
