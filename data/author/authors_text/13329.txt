Proceedings of the ACL 2010 System Demonstrations, pages 1?6,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Grammar Prototyping and Testing with the
LinGO Grammar Matrix Customization System
Emily M. Bender, Scott Drellishak, Antske Fokkens, Michael Wayne Goodman,
Daniel P. Mills, Laurie Poulson, and Safiyyah Saleem
University of Washington, Seattle, Washington, USA
{ebender,sfd,goodmami,dpmills,lpoulson,ssaleem}@uw.edu,
afokkens@coli.uni-saarland.de
Abstract
This demonstration presents the LinGO
Grammar Matrix grammar customization
system: a repository of distilled linguis-
tic knowledge and a web-based service
which elicits a typological description of
a language from the user and yields a cus-
tomized grammar fragment ready for sus-
tained development into a broad-coverage
grammar. We describe the implementation
of this repository with an emphasis on how
the information is made available to users,
including in-browser testing capabilities.
1 Introduction
This demonstration presents the LinGO Gram-
mar Matrix grammar customization system1 and
its functionality for rapidly prototyping grammars.
The LinGO Grammar Matrix project (Bender et
al., 2002) is situated within the DELPH-IN2 col-
laboration and is both a repository of reusable
linguistic knowledge and a method of delivering
this knowledge to a user in the form of an ex-
tensible precision implemented grammar. The
stored knowledge includes both a cross-linguistic
core grammar and a series of ?libraries? contain-
ing analyses of cross-linguistically variable phe-
nomena. The core grammar handles basic phrase
types, semantic compositionality, and general in-
frastructure such as the feature geometry, while
the current set of libraries includes analyses of
word order, person/number/gender, tense/aspect,
case, coordination, pro-drop, sentential negation,
yes/no questions, and direct-inverse marking, as
well as facilities for defining classes (types) of lex-
ical entries and lexical rules which apply to those
types. The grammars produced are compatible
with both the grammar development tools and the
1
http://www.delph-in.net/matrix/customize/
2
http://www.delph-in.net
grammar-based applications produced by DELPH-
IN. The grammar framework used is Head-driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994) and the grammars map bidirectionally
between surface strings and semantic representa-
tions in the format of Minimal Recursion Seman-
tics (Copestake et al, 2005).
The Grammar Matrix project has three goals?
one engineering and two scientific. The engineer-
ing goal is to reduce the cost of creating gram-
mars by distilling the solutions developed in exist-
ing DELPH-IN grammars and making them easily
available for new projects. The first scientific goal
is to support grammar engineering for linguistic
hypothesis testing, allowing users to quickly cus-
tomize a basic grammar and use it as a medium in
which to develop and test analyses of more inter-
esting phenomena.3 The second scientific goal is
to use computational methods to combine the re-
sults of typological research and formal syntactic
analysis into a single resource that achieves both
typological breadth (handling the known range of
realizations of the phenomena analyzed) and ana-
lytical depth (producing analyses which work to-
gether to map surface strings to semantic represen-
tations) (Drellishak, 2009).
2 System Overview
Grammar customization with the LinGO Gram-
mar Matrix consists of three primary activities:
filling out the questionnaire, preliminary testing of
the grammar fragment, and grammar creation.
2.1 Questionnaire
Most of the linguistic phenomena supported by the
questionnaire vary across languages along multi-
ple dimensions. It is not enough, for example,
3Research of this type based on the Grammar Matrix
includes (Crysmann, 2009) (tone change in Hausa) and
(Fokkens et al, 2009) (Turkish suspended affixation).
1
simply to know that the target language has coor-
dination. It is also necessary to know, among other
things, what types of phrases can be coordinated,
how those phrases are marked, and what patterns
of marking appear in the language. Supporting a
linguistic phenomenon, therefore, requires elicit-
ing the answers to such questions from the user.
The customization system elicits these answers us-
ing a detailed, web-based, typological question-
naire, then interprets the answers without human
intervention and produces a grammar in the format
expected by the LKB (Copestake, 2002), namely
TDL (type description language).
The questionnaire is designed for linguists who
want to create computational grammars of natu-
ral languages, and therefore it freely uses techni-
cal linguistic terminology, but avoids, when possi-
ble, mentioning the internals of the grammar that
will be produced, although a user who intends to
extend the grammar will need to become familiar
with HPSG and TDL before doing so.
The questionnaire is presented to the user as a
series of connected web pages. The first page the
user sees (the ?main page?) contains some intro-
ductory text and hyperlinks to direct the user to
other sections of the questionnaire (?subpages?).
Each subpage contains a set of related questions
that (with some exceptions) covers the range of
a single Matrix library. The actual questions in
the questionnaire are represented by HTML form
fields, including: text fields, check boxes, ra-
dio buttons, drop-downs, and multi-select drop-
downs. The values of these form fields are stored
in a ?choices file?, which is the object passed on
to the grammar customization stage.
2.1.1 Unbounded Content
Early versions of the customization system (Ben-
der and Flickinger, 2005; Drellishak and Bender,
2005) only allowed a finite (and small) number
of entries for things like lexical types. For in-
stance, users were required to provide exactly one
transitive verb type and one intransitive verb type.
The current system has an iterator mechanism in
the questionnaire that allows for repeated sections,
and thus unlimited entries. These repeated sec-
tions can also be nested, which allows for much
more richly structured information.
The utility of the iterator mechanism is most
apparent when filling out the Lexicon subpage.
Users can create an arbitrary number of lexical
rule ?slots?, each with an arbitrary number of
morphemes which each in turn bear any num-
ber of feature constraints. For example, the
user could create a tense-agreement morpholog-
ical slot, which contains multiple portmanteau
morphemes each expressing some combination of
tense, subject person and subject number values
(e.g., French -ez expresses 2nd person plural sub-
ject agreement together with present tense).
The ability provided by the iterators to create
unbounded content facilitates the creation of sub-
stantial grammars through the customization sys-
tem. Furthermore, the system allows users to ex-
pand on some iterators while leaving others un-
specified, thus modeling complex rule interactions
even when it cannot cover features provided by
these rules. A user can correctly model the mor-
photactic framework of the language using ?skele-
tal? lexical rules?those that specify morphemes?
forms and their co-occurrence restrictions, but per-
haps not their morphosyntactic features. The user
can then, post-customization, augment these rules
with the missing information.
2.1.2 Dynamic Content
In earlier versions of the customization system, the
questionnaire was static. Not only was the num-
ber of form fields static, but the questions were
the same, regardless of user input. The current
questionnaire is more dynamic. When the user
loads the customization system?s main page or
subpages, appropriate HTML is created on the fly
on the basis of the information already collected
from the user as well as language-independent in-
formation provided by the system.
The questionnaire has two kinds of dynamic
content: expandable lists for unbounded entry
fields, and the population of drop-down selec-
tors. The lists in an iterated section can be ex-
panded or shortened with ?Add? and ?Delete? but-
tons near the items in question. Drop-down selec-
tors can be automatically populated in several dif-
ferent ways.4 These dynamic drop-downs greatly
lessen the amount of information the user must
remember while filling out the questionnaire and
can prevent the user from trying to enter an invalid
value. Both of these operations occur without re-
freshing the page, saving time for the user.
4These include: the names of currently-defined features,
the currently-defined values of a feature, or the values of vari-
ables that match a particular regular expression.
2
2.2 Validation
It makes no sense to attempt to create a consis-
tent grammar from an empty questionnaire, an in-
complete questionnaire, or a questionnaire con-
taining contradictory answers, so the customiza-
tion system first sends a user?s answers through
?form validation?. This component places a set
of arbitrarily complex constraints on the answers
provided. The system insists, for example, that
the user not state the language contains no deter-
miners but then provide one in the Lexicon sub-
page. When a question fails form validation, it
is marked with a red asterisk in the questionnaire,
and if the user hovers the mouse cursor over the as-
terisk, a pop-up message appears describing how
form validation failed. The validation component
can also produce warnings (marked with red ques-
tion marks) in cases where the system can gen-
erate a grammar from the user?s answers, but we
have reason to believe the grammar won?t behave
as expected. This occurs, for example, when there
are no verbal lexical entries provided, yielding a
grammar that cannot parse any sentences.
2.3 Creating a Grammar
After the questionnaire has passed validation, the
system enables two more buttons on the main
page: ?Test by Generation? and ?Create Gram-
mar?. ?Test by Generation? allows the user to test
the performance of the current state of the gram-
mar without leaving the browser, and is described
in ?3. ?Create Grammar? causes the customiza-
tion system to output an LKB-compatible grammar
that includes all the types in the core Matrix, along
with the types from each library, tailored appropri-
ately, according to the specific answers provided
for the language described in the questionnaire.
2.4 Summary
This section has briefly presented the structure
of the customization system. While we antici-
pate some future improvements (e.g., visualiza-
tion tools to assist with designing type hierarchies
and morphotactic dependencies), we believe that
this system is sufficiently general to support the
addition of analyses of many different linguistic
phenomena. The system has been used to create
starter grammars for more than 40 languages in the
context of a graduate grammar engineering course.
To give sense of the size of the grammars
produced by the customization system, Table 1
compares the English Resource Grammar (ERG)
(Flickinger, 2000), a broad-coverage precision
grammar in the same framework under develop-
ment since 1994, to 11 grammars produced with
the customization system by graduate students in
a grammar engineering class at the University of
Washington. The students developed these gram-
mars over three weeks using reference materials
and the customization system. We compare the
grammars in terms of the number types they de-
fine, as well as the number of lexical rule and
phrase structure rule instances.5 We separate
types defined in the Matrix core grammar from
language-specific types defined by the customiza-
tion system. Not all of the Matrix-provided types
are used in the definition of the language-specific
rules, but they are nonetheless an important part of
the grammar, serving as the foundation for further
hand-development. The Matrix core grammar in-
cludes a larger number of types whose function is
to provide disjunctions of parts of speech. These
are given in Table 1, as ?head types?. The final col-
umn in the table gives the number of ?choices? or
specifications that the users gave to the customiza-
tion system in order to derive these grammars.
3 Test-by-generation
The purpose of the test-by-generation feature is to
provide a quick method for testing the grammar
compiled from a choices file. It accomplishes this
by generating sentences the grammar deems gram-
matical. This is useful to the user in two main
ways: it quickly shows whether any ungrammat-
ical sentences are being licensed by the grammar
and, by providing an exhaustive list of licensed
sentences for an input template, allows users to see
if an expected sentence is not being produced.
It is worth emphasizing that this feature of the
customization system relies on the bidirectional-
ity of the grammars; that is, the fact that the same
grammar can be used for both parsing and genera-
tion. Our experience has shown that grammar de-
velopers quickly find generation provides a more
stringent test than parsing, especially for the abil-
ity of a grammar to model ungrammaticality.
3.1 Underspecified MRS
Testing by generation takes advantage of the gen-
eration algorithm include in the LKB (Carroll et al,
5Serious lexicon development is taken as a separate task
and thus lexicon size is not included in the table.
3
Language Family Lg-specific types Matrix types Head types Lex rules Phrasal rules Choices
ERG Germanic 3654 N/A N/A 71 226 N/A
Breton Celtic 220 413 510 57 49 1692
Cherokee Iroquoian 182 413 510 95 27 985
French Romance 137 413 510 29 22 740
Jamamad?? Arauan 188 413 510 87 11 1151
Lushootseed Salish 95 413 510 20 8 391
Nishnaabemwin Algonquian 289 413 510 124 50 1754
Pashto Iranian 234 413 510 86 19 1839
Pali Indo-Aryan 237 413 510 92 55 1310
Russian Slavic 190 413 510 56 35 993
Shona Bantu 136 413 510 51 9 591
Vietnamese Austro-Asiatic 105 413 510 2 26 362
Average 182.9 413 510 63.5 28.3 1073.5
Table 1: Grammar sizes in comparison to ERG
1999). This algorithm takes input in the form of
Minimal Recursion Semantics (MRS) (Copestake
et al, 2005): a bag of elementary predications,
each bearing features encoding a predicate string,
a label, and one or more argument positions that
can be filled with variables or with labels of other
elementary predications.6 Each variable can fur-
ther bear features encoding ?variable properties?
such as tense, aspect, mood, sentential force, per-
son, number or gender.
In order to test our starter grammars by gen-
eration, therefore, we must provide input MRSs.
The shared core grammar ensures that all of
the grammars produce and interpret valid MRSs,
but there are still language-specific properties in
these semantic representations. Most notably, the
predicate strings are user-defined (and language-
specific), as are the variable properties. In addi-
tion, some coarser-grained typological properties
(such as the presence or absence of determiners)
lead to differences in the semantic representations.
Therefore, we cannot simply store a set of MRSs
from one grammar to use as input to the generator.
Instead, we take a set of stored template MRSs
and generalize them by removing all variable
properties (allowing the generator to explore all
possible values), leaving only the predicate strings
and links between the elementary predications.
We then replace the stored predicate strings with
ones selected from among those provided by the
user. Figure 1a shows an MRS produced by a
grammar fragment for English. Figure 1b shows
the MRS with the variable properties removed
and the predicate strings replaced with generic
place-holders. One such template is needed for
every sentence type (e.g., intransitive, transitive,
6This latter type of argument encodes scopal dependen-
cies. We abstract away here from the MRS approach to scope
underspecification which is nonetheless critical for its com-
putational tractability.
a. ? h1,e2, {h7: cat n rel(x4:SG:THIRD),
h3:exist q rel(x4, h5, h6),
h1: sleep v rel(e2:PRES, x4)},
{h5 qeq h7} ?
b. ? h1,e2, {h7:#NOUN1#(x4),
h3:#DET1#(x4, h5, h6),
h1:#VERB#(e2, x4)},
{h5 qeq h7} ?
Figure 1: Original and underspecified MRS
negated-intransitive, etc.). In order to ensure that
the generated strings are maximally informative to
the user testing a grammar, we take advantage of
the lexical type system. Because words in lexical
types as defined by the customization system dif-
fer only in orthography and predicate string, and
not in syntactic behavior, we need only consider
one word of each type. This allows us to focus the
range of variation produced by the generator on
(a) the differences between lexical types and (b)
the variable properties.
3.2 Test by generation process
The first step of the test-by-generation process is
to compile the choices file into a grammar. Next,
a copy of the LKB is initialized on the web server
that is hosting the Matrix system, and the newly-
created grammar is loaded into this LKB session.
We then construct the underspecified MRSs in
order to generate from them. To do this, the pro-
cess needs to find the proper predicates to use for
verbs, nouns, determiners, and any other parts of
speech that a given MRS template may require. For
nouns and determiners, the choices file is searched
for the predicate for one noun of each lexical noun
type, all of the determiner predicates, and whether
or not each noun type needs a determiner or not.
For verbs, the process is more complicated, re-
quiring valence information as well as predicate
strings in order to select the correct MRS template.
In order to get this information, the process tra-
verses the type hierarchy above the verbal lexical
4
types until it finds a type that gives valence infor-
mation about the verb. Once the process has all
of this information, it matches verbs to MRS tem-
plates and fills in appropriate predicates.
The test-by-generation process then sends these
constructed MRSs to the LKB process and displays
the generation results, along with a brief explana-
tion of the input semantics that gave rise to them,
in HTML for the user.7
4 Related Work
As stated above, the engineering goal of the Gram-
mar Matrix is to facilitate the rapid development
of large-scale precision grammars. The starter
grammars output by the customization system are
compatible in format and semantic representations
with existing DELPH-IN tools, including software
for grammar development and for applications in-
cluding machine translation (Oepen et al, 2007)
and robust textual entailment (Bergmair, 2008).
More broadly, the Grammar Matrix is situated
in the field of multilingual grammar engineer-
ing, or the practice of developing linguistically-
motivated grammars for multiple languages within
a consistent framework. Other projects in this
field include ParGram (Butt et al, 2002; King
et al, 2005) (LFG), the CoreGram project8 (e.g.,
(Mu?ller, 2009)) (HPSG), and the MetaGrammar
project (de la Clergerie, 2005) (TAG).
To our knowledge, however, there is only one
other system that elicits typological information
about a language and outputs an appropriately cus-
tomized implemented grammar. The system, de-
scribed in (Black, 2004) and (Black and Black,
2009), is called PAWS (Parser And Writer for
Syntax) and is available for download online.9
PAWS is being developed by SIL in the context
of both descriptive (prose) grammar writing and
?computer-assisted related language adaptation?,
the practice of writing a text in a target language
by starting with a translation of that text in a
related source language and mapping the words
from target to source. Accordingly, the output of
PAWS consists of both a prose descriptive grammar
7This set-up scales well to multiple users, as the user?s in-
teraction with the LKB is done once per customized grammar,
providing output for the user to peruse as his or her leisure.
The LKB process does not persist, but can be started again
by reinvoking test-by-generation, such as when the user has
updated the grammar definition.
8
http://hpsg.fu-berlin.de/Projects/core.html
9
http://www.sil.org/computing/catalog/show_
software.asp?id=85
and an implemented grammar. The latter is in the
format required by PC-PATR (McConnel, 1995),
and is used primarily to disambiguate morpholog-
ical analyses of lexical items in the input string.
Other systems that attempt to elicit linguistic in-
formation from a user include the Expedition (Mc-
Shane and Nirenburg, 2003) and Avenue projects
(Monson et al, 2008), which are specifically tar-
geted at developing machine translation for low-
density languages. These projects differ from the
Grammar Matrix customization system in elic-
iting information from native speakers (such as
paradigms or translations of specifically tailored
corpora), rather than linguists. Further, unlike the
Grammar Matrix customization system, they do
not produce resources meant to sustain further de-
velopment by a linguist.
5 Demonstration Plan
Our demonstration illustrates how the customiza-
tion system can be used to create starter gram-
mars and test them by invoking test-by-generation.
We first walk through the questionnaire to illus-
trate the functionality of libraries and the way that
the user interacts with the system to enter infor-
mation. Then, using a sample grammar for En-
glish, we demonstrate how test-by-generation can
expose both overgeneration (ungrammatical gen-
erated strings) and undergeneration (gaps in gen-
erated paradigms). Finally, we return to the ques-
tionnaire to address the bugs in the sample gram-
mar and retest to show the result.
6 Conclusion
This paper has presented an overview of the
LinGO Grammar Matrix Customization System,
highlighting the ways in which it provides ac-
cess to its repository of linguistic knowledge. The
current customization system covers a sufficiently
wide range of phenomena that the grammars it
produces are non-trivial. In addition, it is not al-
ways apparent to a user what the implications will
be of selecting various options in the question-
naire, nor how analyses of different phenomena
will interact. The test-by-generation methodology
allows users to interactively explore the conse-
quences of different linguistic analyses within the
platform. We anticipate that it will, as a result, en-
courage users to develop more complex grammars
within the customization system (before moving
on to hand-editing) and thereby gain more benefit.
5
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
0644097. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Emily M. Bender and Dan Flickinger. 2005. Rapid
prototyping of scalable grammars: Towards modu-
larity in extensions to a language-independent core.
In Proc. of IJCNLP-05 (Posters/Demos).
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proc. of the Workshop on Grammar Engineering
and Evaluation at COLING 2002, pages 8?14.
Richard Bergmair. 2008. Monte Carlo semantics:
McPIET at RTE4. In Text Analysis Conference (TAC
2008) Workshop-RTE-4 Track. National Institute of
Standards and Technology, pages 17?19.
Cheryl A. Black and H. Andrew Black. 2009. PAWS:
Parser and writer for syntax: Drafting syntactic
grammars in the third wave. In SIL Forum for Lan-
guage Fieldwork, volume 2.
Cheryl A. Black. 2004. Parser and writer for syn-
tax. Paper presented at the International Confer-
ence on Translation with Computer-Assisted Tech-
nology: Changes in Research, Teaching, Evaluation,
and Practice, University of Rome ?La Sapienza?,
April 2004.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proc. of the Workshop
on Grammar Engineering and Evaluation at COL-
ING 2002, pages 1?7.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznan?ski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proc. of the 7th
European workshop on natural language generation
(EWNLG99), pages 86?95.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language & Compu-
tation, 3(4):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI, Stanford.
Berthold Crysmann. 2009. Autosegmental representa-
tions in an HPSG for Hausa. In Proc. of the Work-
shop on Grammar Engineering Across Frameworks
2009.
E?ric Villemonte de la Clergerie. 2005. From meta-
grammars to factorized TAG/TIG parsers. In Proc.
of IWPT?05, pages 190?191.
Scott Drellishak and Emily M. Bender. 2005. A co-
ordination module for a crosslinguistic grammar re-
source. In Stefan Mu?ller, editor, Proc. of HPSG
2005, pages 108?128, Stanford. CSLI.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6:15 ? 28.
Antske Fokkens, Laurie Poulson, and Emily M. Ben-
der. 2009. Inflectional morphology in Turkish VP-
coordination. In Stefan Mu?ller, editor, Proc. of
HPSG 2009, pages 110?130, Stanford. CSLI.
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The feature space in parallel
grammar writing. Research on Language & Com-
putation, 3(2):139?163.
Stephen McConnel. 1995. PC-PATR Refer-
ence Manual. Summer Institute for Linguistics.
http://www.sil.org/pcpatr/manual/pcpatr.html.
Marjorie McShane and Sergei Nirenburg. 2003. Pa-
rameterizing and eliciting text elements across lan-
guages for use in natural language processing sys-
tems. Machine Translation, 18:129?165.
Christian Monson, Ariadna Font Llitjs, Vamshi Am-
bati, Lori Levin, Alon Lavie, Alison Alvarez,
Roberto Aranovich, Jaime Carbonell, Robert Fred-
erking, Erik Peterson, and Katharina Probst. 2008.
Linguistic structure and bilingual informants help
induce machine translation of lesser-resourced lan-
guages. In LREC?08.
Stefan Mu?ller. 2009. Towards an HPSG analysis of
Maltese. In Bernard Comrie, Ray Fabri, Beth Hume,
Manwel Mifsud, Thomas Stolz, and Martine Van-
hove, editors, Introducing Maltese linguistics. Pa-
pers from the 1st International Conference on Mal-
tese Linguistics, pages 83?112. Benjamins, Amster-
dam.
Stephan Oepen, Erik Velldal, Jan Tore Lnning, Paul
Meurer, Victoria Rosn, and Dan Flickinger. 2007.
Towards hybrid quality-oriented machine transla-
tion. On linguistics and probabilities in MT. In
11th International Conference on Theoretical and
Methodological Issues in Machine Translation.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press, Chicago, IL.
6
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1066?1076,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Metagrammar Engineering:
Towards systematic exploration of implemented grammars
Antske Fokkens
Department of Computational Linguistics, Saarland University &
German Research Center for Artificial Intelligence (DFKI) Project Office Berlin
Alt-Moabit 91c, 10559 Berlin, Germany
afokkens@coli.uni-saarland.de
Abstract
When designing grammars of natural lan-
guage, typically, more than one formal anal-
ysis can account for a given phenomenon.
Moreover, because analyses interact, the
choices made by the engineer influence the
possibilities available in further grammar de-
velopment. The order in which phenomena
are treated may therefore have a major impact
on the resulting grammar. This paper proposes
to tackle this problem by using metagrammar
development as a methodology for grammar
engineering. I argue that metagrammar engi-
neering as an approach facilitates the system-
atic exploration of grammars through compar-
ison of competing analyses. The idea is illus-
trated through a comparative study of auxil-
iary structures in HPSG-based grammars for
German and Dutch. Auxiliaries form a cen-
tral phenomenon of German and Dutch and
are likely to influence many components of
the grammar. This study shows that a spe-
cial auxiliary+verb construction significantly
improves efficiency compared to the standard
argument-composition analysis for both pars-
ing and generation.
1 Introduction
One of the challenges in designing grammars of nat-
ural language is that, typically, more than one for-
mal analysis can account for a given phenomenon.
The criteria for choosing between competing analy-
ses are fairly clear (observational adequacy, analyti-
cal clarity, efficiency), but given that analyses of dif-
ferent phenomena interact, actually evaluating anal-
yses on those criteria in a systematic manner is far
from straightforward. The standard methodology in-
volves either picking one analysis, and seeing how
it goes, then backing out if it does not work out,
or laboriously adapting a grammar to two versions
supporting different analyses (Bender, 2010). The
former approach is not in any way systematic, in-
creasing the risk that the grammar is far from opti-
mal in terms of efficiency. The latter approach po-
tentially causes the grammar engineer an amount of
work that will not scale for considering many differ-
ent phenomena.
This paper proposes a more systematic and
tractable alternative to grammar development: meta-
grammar engineering. I use ?metagrammar? as a
generic term to refer to a system that can generate
implemented grammars. The key idea is that the
grammar engineer adds alternative plausable anal-
yses for linguistic phenomena to a metagrammar.
This metagrammar can generate all possible com-
binations of these analyses automatically, creating
different versions of a grammar that cover the same
phenomena. The engineer can test directly how
competing analyses for different phenomena inter-
act, and determine which combinations are possible
(after minor adaptations) and which analyses are in-
compatible.
The idea of metagrammar engineering is illus-
trated here through a case study of word order and
auxiliaries in Germanic languages, which forms the
second goal of this paper. Auxiliaries form a central
phenomenon of German and Dutch and are likely to
influence many components of the grammar. The re-
sults show that the analysis of auxiliary+verb struc-
tures presented in Bender (2010) significantly im-1066
proves efficiency of the grammar compared to the
standard argument-composition analysis within the
range of phenomena studied. Because future re-
search is needed to determine whether the auxil-
iary+verb alternative can interact properly with ad-
ditional phenomena and still lead to more efficient
results than argument-composition, it is particularly
useful to have a grammar generator that can auto-
matically create grammars with either of the two
analyses.
The remainder of this paper starts with the case
study. Section 2 provides a description of the con-
text of the study. The relevant linguistic properties
and alternative analyses are described in Sections
3 and 4. After evaluating and discussing the case
study?s results, I return to the general approach of
metagrammar engineering. Section 6 presents re-
lated work on metagrammars. It is followed by a
conclusion and discussion on using metagrammars
as a methodology for grammar engineering.
2 A metagrammar for Germanic
Languages
2.1 The LinGO Grammar Matrix
The LinGO Grammar Matrix (Bender et al, 2002;
Bender et al, 2010) provides the main context for
the experiments described in this paper. To begin
with, its further development plays a significant role
for the motivation of the present study. More impor-
tantly, the Germanic metagrammar is implemented
as a special branch of the LinGO Grammar Matrix
and uses a significant amount of its code.
The Grammar Matrix customization system al-
lows users to derive a starter grammar for a particu-
lar language from a common multi-lingual resource
by specifying linguistic properties through a web-
based questionnaire. The grammars are intended for
parsing and generation with the LKB (Copestake,
2002) using Minimal Recursion Semantics (Copes-
take et al, 2005, MRS) as parsing output and gener-
ation input. After the starter grammar has been cre-
ated, its development continues independently: en-
gineers can thus make modifications to their gram-
mar without affecting the multi-lingual resource.
Internally, the customization system works as fol-
lows: The web-based questionnaire registers lin-
guistic properties in a file called ?choices? (hence-
forth choices file). The customization system takes
this choices file as input to create grammar frag-
ments, using so-called ?libraries? that contain imple-
mentations of cross-linguistically variable phenom-
ena. Depending on the definitions provided in the
choices file, different analyses are retrieved from the
customization system?s libraries. The language spe-
cific implementations inherit from a core grammar
which handles basic phrase types, semantic compo-
sitionality and general infrastructure, such as feature
geometry (Bender et al, 2002).
The present study is part of a larger effort to im-
prove the customization library for auxiliary struc-
tures in free word order and verb second languages.
It examines whether Bender?s observations concern-
ing an improved analysis for auxiliaries in Wambaya
(Bender, 2010) also hold for Germanic languages. A
more elaborate study of German and Dutch (includ-
ing both Flemish and (Northern) Dutch, which have
slightly different word order constraints) is informa-
tive, because these languages are well-described and
known to have distinctly challenging word order be-
havior.
2.2 Germanic branch
In order to create grammars for Germanic lan-
guages, a specialized branch of the Grammar Ma-
trix customization system was developed. This Ger-
manic grammars generator uses the Grammar Ma-
trix?s facilities to generate types in type description
language (tdl). At present, the generator uses the
Grammar Matrix analyses for agreement and case
marking as well as basics from its morphotactics,
coordination and lexicon implementations.
In the first stage, the word order library and aux-
iliary implementation were extended to cover two
alternative analyses for Germanic word order (see
Section 4). The coordination library was adapted to
ensure correct interactions with the new word order
analyses and agreement. The morphotactics library
was extended to cover Dutch and Flemish interac-
tions between word order and morphology. Finally,
the lexicon and verbal case pattern implementations
were extended to cover ditransitive verbs.
Both versions of word order analyses can be
tweaked to include or exclude a rarely occurring
variant of partial VP fronting (see Section 4.3) re-
sulting in four distinct grammars for each of the1067
Vorfeld LB Mittelfeld RB Nachfeld
Der Mann hat den Jungen gesehen nach der Party
The man.nom has the boy.acc seen after the party
Der Mann hat den Jungen nach der Party gesehen
Den Jungen hat der Mann gesehen nach der Party
Nach der Party hat der Mann den Jungen gesehen
Den Jungen gesehen hat der Mann nach der Party
Gesehen hat der Mann den Jungen nach der Party
The man saw the boy after the party
Table 1: Basic structure of German word order (not exhaustive)
languages under investigation. These 12 grammars
cover Dutch, Flemish and German main clauses with
up to three core arguments.1
3 Germanic word order
3.1 German word order
Topological fields (Erdmann, 1886; Drach, 1937)
form the easiest way to describe German word or-
der. The sentence structure for declarative main
clauses, consists of five topological fields: Vorfeld
(?pre-field?), Left Bracket (LB), Mittelfeld (?middle
field?), Right Bracket (RB) and the Nachfeld (?after
field?). A subset of permissible alternations in Ger-
man are provided in Table 1. The last two sentences
present an example of partial VP fronting.
The fields are defined with regard to verbal forms,
which are placed in the Left and Right Brackets.
Each topological field has word order restrictions
of its own. The Vorfeld must contain exactly one
constituent in an affirmative main clause. The Left
Bracket contains the finite verb and no other ele-
ments. Other verbal forms (if not fronted to the Vor-
feld) must be placed in the Right Bracket. Most non-
verbal elements are placed in the Mittelfeld. When
main verbs are placed in the Vorfeld, their object(s)
may stay in the Mittelfeld. This kind of partial VP
fronting is illustrated by the last example in Table 1.
The Nachfeld typically contains subordinate clauses
and sometimes adverbial phrases.
In German, the respective order between the verbs
in the Right Bracket is head-final, i.e. auxiliaries fol-
low their complements. The only exception is the
1The grammar generation system also creates Danish gram-
mars. Danish results are not presented, because the language
does not pose the challenges explained in Section 4.
auxiliary flip: under certain conditions in subordi-
nate clauses, the finite verb precedes all other verbal
forms.
3.2 Dutch word order
Dutch word order reveals the same topological fields
as German. There are two main differences between
the languages where word order is concerned. First,
whereas the order of arguments in the German Mit-
telfeld allows some flexibility depending on infor-
mation structure, Dutch argument order is fixed, ex-
cept for the possibility of placing any argument in
the Vorfeld. A related aspect is that Dutch is less
flexible as to what partial VPs can be placed in the
Vorfeld.
The second difference is the word order in the
Right Bracket. The order of auxiliaries and their
complements is less rigid in Dutch and typically
auxiliary-complement, the inverse of German order.
Most Dutch auxiliaries can occur in both orders, but
this may be restricted according to their verb form.
Four groups of auxiliary verbs can be distinguished
that have different syntactic restrictions.
1. Verbs selecting for participles which may ap-
pear on either side of their complement (e.g.
hebben (?have?), zijn (?be?)).
2. Verbs selecting for participles which prefer to
follow their complement and must do so if they
are in participle form themselves (e.g. blijven
(?remain?), krijgen (?get?)).
3. Modals selecting for infinitives which prefer to
precede their complement and must do so if
they appear in infinitive form themselves.1068
VF LB MF RB
De man zou haar kunnen hebben gezien
the man would her.acc can have seen
De man zou haar gezien kunnen hebben
%De man zou haar kunnen gezien hebben
The man should have been able to see her
Table 2: Variations of Dutch auxiliary order
4. Verbs selecting for ?to infinitives? which must
precede their complement.
While there is some variation among speakers,
the generalizations above are robust. The permitted
variations assuming a verb of the 3rd and 1st cate-
gory in the right bracket are presented in Table 2.2
The variant %De man zou haar kunnen gezien
hebben is typical of speakers from Belgium (Hae-
seryn, 1997); speakers from the Netherlands tend to
regard such structures as ungrammatical. Our sys-
tem can both generate a Flemish grammar accepting
all of the above and a (Northern) Dutch grammar,
rejecting the third variant.
4 Alternative auxiliary approaches
This section presents the alternative analyses for
auxiliary-verb structures in Germanic languages
compared in this study. For reasons of space, I limit
my description to an explanation of the differences
and relevance of the compared analyses.3
4.1 Argument-composition
The standard analysis for German and Dutch
auxiliaries in HPSG is a so-called ?argument-
composition? analysis (Hinrichs and Nakazawa,
1994), which I will explain through the following
Dutch example:4
(1) Ik
I
zou
would
het
the
boek
book
willen
want
lezen.
read.
?I would like to read the book.?
In the sentence above, the auxiliary willen ?want?
separates the verb lezen ?read? from its object het
2Note that the same orders as in the Right Brackets may also
occur in the Vorfeld (with or without the object).
3Details of the implementations can be found by using the
metagrammar, which can be found on my homepage.
4Hinrichs and Nakazawa (1994) present an analysis for the
German auxiliary flip. The relevant observations are the same.
2
6
6
6
6
4
VAL
2
6
6
6
6
4
SUBJ 1
COMPS
*
2
6
4
HEAD verb
VAL
"
SUBJ 1
COMPS 2
#
3
7
5
, 2
+
3
7
7
7
7
5
3
7
7
7
7
5
Figure 1: Standard Auxiliary Subcategorization
boek ?the book?. A parser respecting surface order
can thus not combine lezen and het boek before com-
bining willen and lezen.
The argument-composition analysis was intro-
duced to make sure that het boek can be picked up
as the object of the embedded verb lezen. The sub-
categorization of an auxiliary under this analysis is
presented in Figure 1. The subject of the auxiliary
is identical to the subject of the auxiliary?s com-
plement. Its complement list consists of the con-
catenation of the verbal complement and any com-
plement this verbal complement may select for. In
the sentence above, willen will add the subject and
the object of lezen to its own subcatorization lists.5
This standard solution for auxiliary-verb structures
is (with minor differences) also what is provided by
the Matrix customization system.
Argument-composition can capture the grammat-
ical behavior of auxiliaries in German and Dutch.
However, grammaticality and coverage is not all
that matters for grammars of natural language. Ef-
ficiency remains an important factor, and argument-
composition has some undesirable properties on this
level. The problem lies in the fact that lexical en-
tries of auxiliaries have underspecified elements on
their subcategorization lists. With the current chart
parsing and chart generation algorithms (Carroll and
Oepen, 2005), an auxiliary in a language with flex-
ible word order will speculatively add edges to the
chart for potential analyses with the adjacent con-
stituent as subject or complement. Because the
length of the lists are underspecified as well, it can
continue wrongly combining with all elements in the
string. In the worse case scenario, the number of
edges created by an auxiliary grows exponentially in
the number of words and constituents in the string.
The efficiency problem is even worse for generation:
while the parser is restricted by the surface order of
5In the semantic representation, both arguments will be di-
rectly related to the main verb exclusively.1069
`i
?
2
4VAL
"
SUBJ ??
COMPS
D
?
HEAD verb
?
E
#
3
5
`
ii
?
2
6
6
6
6
6
6
4
VAL
"
SUBJ 1
COMPS 2
#
HEAD-DTR|VAL| COMPS 3
NON-HEAD-DTR 3
"
VAL
"
SUBJ 1
COMPS 2
##
3
7
7
7
7
7
7
5
Figure 2: Auxiliary lexical type (i) and Auxiliary+verb
construction (ii) under alternative analysis
the string, the generator will attempt to combine all
lexical items suggested by the input semantics, as
well as lexical items with empty semantics, in ran-
dom order.
4.2 Aux+verb construction
Bender (Bender, 2010)6 presents an alternative ap-
proach to auxiliary-verb structures for the Australian
language Wambaya. The analysis introduces auxil-
iaries that only subcategorize for one verbal com-
plement, not raising any of the complement?s ar-
guments or its subject. Auxiliaries combine with
their complement using a special auxiliary+verb
rule. Figure 2 presents this alternative solution. In
principle, the new analysis uses the same technique
as argument composition. The difference is that the
auxiliary now starts out with only one element in its
subcategorization lists and can only combine with
potential verbal complements that are appropriately
constrained. The structure that combines the auxil-
iary with its complement places the remaining ele-
ments on the complement?s SUBJ and COMPS lists
on the respective lists of the newly formed phrase,
as can be seen in Figure 2 (ii). The constraints on
raised arguments are known when the construction
applies. The efficiency problem sketched above is
thus avoided.
4.3 A small wrinkle: partial VP fronting
In its basic form, the auxiliary+verb structure cannot
handle partial VP fronting where the main verb is
placed in first position leaving one or more verbal
6Bender credits the key idea behind this analysis to Dan
Flickinger (Bender, 2010).
forms in the verbal cluster, as illustrated in (2) for
Dutch:
(2) Gezien
Seen
zou
should
de
the
man
man
haar
her
kunnen
can
hebben.
have
?The man should have been able to see her.?
The problem is that hebben ?have? cannot com-
bine with gezien ?seen?, because they are sepa-
rated by the head of the clause. Because the verb
hebben cannot combine with its complement, it can-
not raise its complement?s arguments either: the
auxiliary+verb analysis only permits raising when
auxiliary and complement combine.
This shortcoming is no reason to immediately dis-
miss the proposal. Structures such as (2) are ex-
tremely rare. The difference in coverage of a parser
that can and a parser that cannot handle such struc-
tures is likely to be tiny, if present at all, nor is it
vital for a sentence generator to be able to produce
them. However, a correct grammar should be able to
analyze and produce all grammatical structures.
I implemented an additional version of the aux-
iliary+verb construction using two rather complex
rules that capture examples such as (2). Because
the structure in (2) also presented difficulties for
the argument-composition analysis in Dutch, I tested
both of the analyses with and without the inclusion
of these structures. In the ideal case, the full cov-
erage version will remain efficient enough as the
grammar grows. But if this turns out not to be the
case, the decision can be made to exclude the ad-
ditional rule from the grammar or to use it as a ro-
bustness rule that is only called when regular rules
fail. Given the metagrammar engineering approach,
it will be straightforward to decide at a later point to
exclude the special rule, if corpus studies reveal this
is favourable.
5 Grammars and evaluation
5.1 Experimental set-up
As described above, the Germanic metagrammar is
a branch of the customization system. As such, it
takes a choices file as input to create a grammar. The
basic choices files for Dutch and German were cre-
ated through the LinGO Grammar Matrix web inter-1070
Complete Set Reduced Set
Positive Total Positive Total Av.
s s s s w/s
Du 177 14654 138 14591 6.61
Fl 195 14654 156 14606 6.61
Ge 116 6926 84 6914 6.65
Table 3: Number of test examples (s) used in evaluation
and average words per sentence (w/s)
face.7 The choices files defined artificial grammars
with a dummy vocabulary. The system can produce
real fragments of the languages, but strings repre-
senting syntactic properties through dummy vocab-
ulary were used to give better control over ambiguity
facilitating the evaluation of coverage and overgen-
eration of the grammars. The grammars have a lexi-
con of 9-10 unambiguous dummy words.
The created choices files were extended offline to
define those properties that the Germanic metagram-
mar captures, but are not incorporated in the Matrix
customization system. This included word order of
the auxiliary and complement, fixed or free argu-
ment order, influence of inflection on word order,
a more elaborate case hierarchy, ditransitive verbs,
and the choice of auxiliary/verb analysis. Four
choices files with different combinations of analy-
ses were created for each language, resulting in 12
choices files in total.
A basic test suite was developed that covers in-
transitive, transitive and ditransitive main clauses
with up to three auxiliaries. The German set was
based on a description provided by Kathol (2000),
Dutch and Flemish were based on Haeseryn (1997).
For each verb and auxiliary combination, all permis-
sible word orders were defined based on descriptive
resources. In order to make sure the grammars do
not reveal unexpected forms of overgeneration, all
possible ungrammatical orders were automatically
generated. Table 3 provides the sizes of the test
suites. Each language has both a complete set for
the 6 grammars that provide full coverage, and a re-
duced set for the 6 grammars that can not handle
split verbal clusters (see Section 4.3 for the motiva-
tion to test grammars that do not have full coverage).
7http://www.delph-in.net/matrix/
customize/
Each grammar was created using the metagram-
mar, ensuring that all components except the com-
peting analyses were held constant among compared
grammars. The [incr tsdb()] competence and per-
formance profiling environment (Oepen, 2001) was
used in combination with the LKB to evaluate pars-
ing performance of the individual grammars on the
test suites. For each grammar, the number of re-
quired parsing tasks, memory (space) and CPU time
per sentence, as well as the number of passive edges
created during an average parse were compared.
Performance on language generation was evaluated
using the LKB.
5.2 Parsing results
Table 4 presents the results from the parsing ex-
periment. Note that all directly compared gram-
mars have the same empirical coverage (100% cov-
erage and 0% overgeneration on the phenomena in-
cluded in the test suites). The comparison there-
fore addresses the effect on efficiency of the al-
ternative analyses. Three tests per grammar were
carried out: one on positive data, one on nega-
tive data and one on the complete dataset. Re-
sults were similar for all three sets, with slightly
larger differences in efficiency for negative exam-
ples. For reasons of space, only the results on pos-
itive examples are presented, which are more rele-
vant for most applications involving parsing. The
results show that the auxiliary+verb (aux+v) leads to
a more efficient grammar according to all measures
used. There is an average reduction of 73.2% in per-
formed tasks, 56.3% in produced passive edges and
32.9% in memory when parsing grammatical exam-
ples using the auxiliary+verb structure compared to
argument-composition. CPU-time per sentence also
improved significantly, but, due to the short average
sentence length (5-10 words) the value is too small
for exact comparison with [incr tsdb()].
5.3 Sentence generation evaluation
The complete coverage versions of Dutch and Ger-
man were used to create the exhaustive set of sen-
tences with an intransitive, transitive and ditransitive
verb combined with none, one or two auxiliaries but
rapidly loses ground when one or more auxiliaries8
8All auxiliaries in the grammars contribute an ep.1071
Average Performed Tasks
Compl. Cov. Gram. No Split Cl. Gram.
arg-comp aux+v arg-comp aux+v
Du 524 149 480 134
Fl 529 150 483 137
Ge 684 148 486 136
Average Created Edges
Compl. Cov. Gram. No Split Cl. Gram.
arg-comp aux+v arg-comp aux+v
Du 58 25 52 25
Fl 58 26 52 25
Ge 67 23 52 24
Average Memory Use (kb)
Compl. Cov. Gram. No Split Cl. Gram.
arg-comp aux+v arg-comp aux+v
Du 9691 6692 8944 6455
Fl 9716 6717 8989 6504
Ge 10289 5675 8315 5468
Average CPU Time (s)
Compl. Cov. Gram. No Split Cl. Gram.
arg-comp aux+v arg-comp aux+v
Du 0.04 0.02 0.03 0.01
Fl 0.04 0.02 0.03 0.01
Ge 0.06 0.01 0.04 0.01
Table 4: Parsing results positive examples
from a total of 18 MRSs. The input MRSs were ob-
tained by parsing a sentence with canonical word or-
der. Both versions provide the same set of sentences
as output, confirming their identical empirical cover-
age. Table 5 presents the number of edges required
by the generator to produce the full set of generated
sentences from a given MRS. The cells with no num-
ber represent conditions under which the LKB gen-
erator reaches the maximum limit of edges, set at
40,000, without completing its exhaustive search.
The grammar using argument-composition is
slightly more efficient when there are no aux-
iliaries, are added, in particular when sentence
length increases: For ditransitive verbs (dv), the
Dutch argument-composition grammar maxes out
the 40,000 edge limit with two auxiliaries, whereas
the auxiliary+verb grammar creates 910 edges, a
manageable number. Due to the more liberal order
of arguments, results are even worse for German:
the argument-composition grammar reaches its limit
with the first auxiliary for ditransitive verbs. These
results indicate that the auxiliary+verb analysis is
Required edges
Du No Aux 1 Aux 2 Aux
arg-c aux+v arg-c aux+v arg-c aux+v
iv 54 57 221 99 792 248
tv 124 141 1311 211 7455 500
dv 212 230 14968 378 ? 910
Ge No Aux 1 Aux 2 Aux
arg-c aux+v arg-c aux+v arg-c aux+v
iv 54 57 295 84 1082 165
tv 130 142 4001 212 18473 422
dv 306 351 ? 608 ? 1379
Table 5: Performance on Sentence Generation
strongly preferable where natural language genera-
tion is concerned.
5.4 In summary
The results of the experiment presented above show
that avoiding underspecified subcategorization lists,
as found in the standard argument-composition anal-
ysis, significantly increases the efficiency of the
grammar for both parsing and generation. On av-
erage, they show a reduction of 73.2% in performed
tasks, 56.3% in produced passive edges and 32.9%
in memory for parsing. In generation experiments,
results are even more impressive: the reduction of
edges for German sentences with one auxiliary and
a ditransitve verb is at least 98.5%. These results
show that the auxiliary+verb alternative should be
considered seriously as an alternative to the HPSG
standard analysis of argument-composition, though
further investigation in a larger context is needed be-
fore final conclusions can be drawn.
Future work will focus on increasing the cover-
age of the grammars, as well as the number of al-
ternative options explored. In particular, both ap-
proaches for auxiliaries should be compared us-
ing alternative analyses for verb-second word order
found in other HPSG-based grammars, such as the
GG (Mu?ller and Kasper, 2000; Crysmann, 2005),
Grammix (Mu?ller, 2009; Mu?ller, 2008) and Cheetah
(Cramer and Zhang, 2009) for German, and Alpino
(Bouma et al, 2001) for Dutch. These grammars
may use approaches that somewhat reduce the prob-
lem of argument-composition, leading to less sig-
nificant differences between the auxiliary+verb and
argument-composition analyses. On the other hand,
planned extensions that cover modification and sub-1072
ordinate clauses will increase local ambiguities. The
advantage of the auxiliary+verb analysis is likely to
become more important as a result.
In addition to providing a clearer picture of aux-
iliary structures, these extensions will also lead to
a better insight into efforts involved in using gram-
mar generation to explore alternative versions of a
grammar over time. In particular, it should pro-
vide an indication of the feasibility of maintaining
a higher number of competing analyses as the gram-
mar grows. After providing background on related
metagrammar projects and their goals, I will elabo-
rate on the importance of systematic exploration of
grammars in the discussion.
6 Related work
Metagrammars (or grammar generators) have been
established in the field for over a decade. This sec-
tion provides an overview of the goals and set-up of
some of the most notable projects.
The MetaGrammar project (Candito, 1998; de la
Clergerie, 2005; Kinyon et al, 2006) started as
an effort to encode syntactic knowledge in an ab-
stract class hierarchy. The hierarchy can contain
cross-linguistically invariable properties and syntac-
tic properties that hold across frameworks (Kinyon
et al, 2006). The factorized descriptions of Meta-
Grammar support Tree-Adjoining Grammars (Joshi
et al, 1975, TAG) as well as Lexical Functional
Grammars (Bresnan, 2001, LFG). The eXtensible
MetaGrammar (Crabbe?, 2005, XMG) defines its
MetaGrammar as classes that are part of a multiple
inheritance hierarchy. Kinyon et al (Kinyon et al,
2006) use XMG to perform a cross-linguistic com-
parison of verb-second structures. Their study fo-
cuses on code-sharing between the languages, but
does not address the problem of competing analyses
investigated in this paper.
The GF Resource Grammar Library (Ranta, 2009)
is a multi-lingual linguistic resource that contains a
set of syntactic analyses implemented in GF (Gram-
matical Framework). The purpose of the library is
to allow engineers working on NLP applications to
write simple grammar rules that can call more com-
plex syntactic implementations from the grammar li-
brary. The grammar library is written by researchers
with linguistic expertise. It makes extensive use of
code sharing: general categories and constructions
that are used by all languages are implemented in
a core syntax grammar. Each language9 has its own
lexicon and morphology, as well as a set of language
specific syntactic structures. Code sharing also takes
place between the subset of languages explored, in
particular by means of common modules for Ro-
mance languages and for Scandanavian languages.
PAWS creates PC-PATR (McConnel, 1995) gram-
mars based on field linguists? input. The main
purpose of PAWS lies in descriptive grammar writ-
ing and ?computer-assisted related language adap-
tation?, where the grammar is used to map words
from a text in a source language to a target language.
PAWS differs from the other projects discussed here,
because grammar engineering or syntactic research
are not the main focus of the project.
The LinGO Grammar Matrix, described in Sec-
tion 2.1, is most closely related to the work pre-
sented in this paper. Like the other projects reviewed
here, the Grammar Matrix does not offer alterna-
tive analyses for the same phenomenon. Moreover,
starter grammars created by the Grammar Matrix are
developed manually and individually after their cre-
ation. The approach taken in this paper differs from
the original goal of the Grammar Matrix in that it
continues the development of new grammars within
the system, introducing a novel application for meta-
grammars. By using a metagrammar to store alter-
native analyses, grammars can be explored system-
atically over time. As such, the paper introduces a
novel methodology for grammar engineering. The
discussion and conclusion will elaborate on the ad-
vantages of the approach.
7 Discussion and conclusion
7.1 The challenge of choosing the right analysis
As mentioned in the introduction, most phenomena
in natural languages can be accounted for by more
than one formal analysis. An engineer may imple-
ment alternative solutions and test the impact on the
grammar concerning interaction with other phenom-
ena (Bierwisch, 1963; Mu?ller, 1999; Bender, 2008;
Bender et al, 2011) and efficiency to decide between
analyses.
9Ranta (Ranta, 2009) reports that GF is developed for four-
teen languages, and more are under development.1073
However, it is not feasible to carry out compara-
tive tests by manually creating different versions of a
grammar every time a decision about an implemen-
tation is made. Moreover, even if such a study were
carried out at each stage, only the interaction with
the current state of the grammar would be tested.
This has two undesirable consequences. First, op-
tions may be rejected that would have worked per-
fectly well if different decisions had been made in
the past. Second, because each decision is only
based on the current state of the grammar, the result-
ing grammar is partially (or even largely) a product
of the order in which phenomena are treated.10
For grammar engineers with practical applica-
tions in mind, this is undesirable because the re-
sulting grammar may end up far from optimal. For
grammar writers that use engineering to find valid
linguistic analyses, the problem is even more seri-
ous: if there is a truth in a declarative grammar,
surely, this should not depend on the order in which
phenomena are treated.
7.2 Metagrammar engineering
This paper proposes to systematically explore anal-
yses throughout the development of a grammar by
writing a metagrammar (or grammar generator),
rather than directly implementing the grammar. A
metagrammar can contain several different analyses
for the same phenomenon. After adding a new phe-
nomenon to the metagrammar, the engineer can au-
tomatically generate versions of the grammar con-
taining different combinations of previous analyses.
As a result, the engineer can not only systematically
explore how alternative analyses interact with the
current grammar, but also continue to explore inter-
actions with phenomena added in the future. Espe-
cially for alternative approaches to basic properties
of the language, such as the auxiliary-verb structures
examined in this study, parallel analyses may pre-
vent the cumbersome scenario of changing a deeply
embedded property of a large grammar.
An additional advantage is that the engineer can
use the methodology to make different versions of
the grammar depending on its intended application.
10It is, of course, possible to go back and change old anal-
yses based on new evidence. In practice, the large effort in-
volved will only be undertaken if the advantages are apparent
beforehand.
For instance, it is possible to develop a highly re-
stricted version for grammar checking that provides
detailed feedback on detected errors (Bender et al,
2004), next to a version with fewer constraints to
parse open text.
As far as finding optimal solutions is concerned,
it must be noted that this approach does not guar-
antee a perfect result, partially because there is no
guarantee the grammar engineer will think of the
perfect solution for each phenomenon, but mainly
because it is not maintainable to implement all pos-
sible alternatives for each phenomenon and make
them interact correctly with all other variations in
the grammar. The grammar engineer still needs to
decide which alternatives are the most promising
and therefore the most important to implement and
maintain. The resulting grammar therefore partially
remains a result of the order in which phenomena
are implemented. Nevertheless, the grammar engi-
neer can keep and try out solutions in parallel for
a longer time, increasing the possibility of explor-
ing more alternative versions of the grammar. These
additional investigations allow for better informed
decisions to stop exploring certain analyses. In ad-
dition, by breaking up analyses into possible alter-
natives, chances are that the resulting metagrammar
will be more modular than a directly written gram-
mar would have been, which facilitates exploring al-
ternatives further.
In sum, even though metagrammar engineering
does not completely solve the challenge of complete
explorations of a grammar?s possibilities, it does fa-
cilitate this process so that finding optimal solutions
becomes more likely, leading to better supported
choices among alternatives and a more scientific ap-
proach to grammar development.
Acknowledgments.
The work described in this paper has been sup-
ported by the project TAKE (Technologies for Ad-
vanced Knowledge Extraction), funded under con-
tract 01IW08003 by the German Federal Ministry
of Education and Research. Emily M. Bender, Lau-
rie Poulson, Christoph Zwirello, Bart Cramer, Kim
Gerdes and three anonymous reviewers provided
valuable feedback that resulted in significant im-
provement of the paper. Naturally, all remaining er-
rors are my own.1074
References
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sutcliffe,
editors, Proceedings of the Workshop on Grammar
Engineering and Evaluation at the 19th International
Conference on Computational Linguistics, pages 8?
14, Taipei, Taiwan.
Emily M. Bender, Dan Flickinger, Stephan Oepen, An-
nemarie Walsh, and Tim Baldwin. 2004. Arboretum:
Using a precision grammar for grammar checking in
call. In Proceedings of the InSTIL/ICAL Symposium:
NLP and Speech Technologies in Advance Language
Learning Systems, Venice, Italy.
Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language & Compu-
tation, 8(1):23?72.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2011. Grammar engineering and linguistic hypoth-
esis testing. In Emily M. Bender and Jennifer E.
Arnold, editors, Language from a Cognitive Perspec-
tive: Grammar, Usage and Processing, pages 5?29.
Stanford: CSLI Publications, Palo Alto, USA.
Emily M. Bender. 2008. Grammar engineering for
linguistic hypothesis testing. In Nicholas Gaylord,
Alexis Palmer, and Elias Ponvert, editors, Proceedings
of the Texas Linguistics Society X Conference: Compu-
tational Linguistics for Less-Studied Languages, pages
16?36, Stanford. CSLI Publications.
Emily M. Bender. 2010. Reweaving a grammar for
Wambaya: A case study in grammar engineering for
linguistic hypothesis testing. Linguistic Issues in Lan-
guage Technology, 3(3):1?34.
Manfred Bierwisch. 1963. Grammatik des deutschen
Verbs, volume II of Studia Grammatica. Akademie
Verlag.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide coverage computational analysis
of Dutch. In Computational Linguistics in the Nether-
lands CLIN 2000.
Joan Bresnan. 2001. Lexical Functional Syntax. Black-
well Publishers, Oxford.
Marie-Helene Candito. 1998. Building parallel LTAG
for French and Italian. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, Volume 1, pages 211?
217, Montreal, Quebec, Canada. Association for Com-
putational Linguistics.
John Carroll and Stephan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In IJCNLP, Jeju Island. Springer-Verlag LNCS.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. an introduc-
tion. Journal of Research on Language and Computa-
tion, 3(2?3):281 ? 332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA.
Beno??t Crabbe?. 2005. Repre?sentation modulaire
et parame?trable de grammaires e?lectroniques lexi-
calise?es. Ph.D. thesis, Universite? de Paris 7.
Bart Cramer and Yi Zhang. 2009. Constructon of a
German HPSG grammar from a detailed treebank. In
Proceedings of the ACL 2009 Grammar Engineering
across Frameworks workshop, pages 37?45, Singa-
pore, Singapore.
Berthold Crysmann. 2005. Relative clause extraposition
in German: An efficient and portable implementation.
Research on Language and Computation, 3(1):61?82.
?Eric Villemonte de la Clergerie. 2005. From metagram-
mars to factorized TAG/TIG parsers. In Proceedings
of IWPT?05, pages 190?191.
Erich Drach. 1937. Grundgedanken der Deutschen Sat-
zlehre. Diesterweg, Frankfurt am Main, Germany.
Oskar Erdmann. 1886. Grundzu?ge der deutschen Syntax
nach ihrer geschichtlichen Entwicklung dargestellt.
Erste Abteilung. Verlag der Cotta?schen Buchhand-
lung, Stuttgart, Germany.
Walter Haeseryn. 1997. De gebruikswaarde van de
ans voor tekstschrijvers, taaltrainers en taaladviseurs.
Tekst[blad], 3.
Erhard Hinrichs and Tsuneko Nakazawa. 1994. Lin-
earizing auxs in German verbal complexes. In John
Nerbonne, Klaus Netter, and Carl Pollard, editors,
German in HPSG. CSLI, Stanford, USA.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree adjunct grammars. Journal of Computer
and System Sciences, 10(1):136?163.
Andreas Kathol. 2000. Linear Syntax. Oxford Press.
Alexandra Kinyon, Owen Rambow, Tatjana Scheffler,
SinWon Yoon, and Aravind K. Joshi. 2006. The meta-
grammar goes multilingual: A cross-linguistic look at
the V2-phenomenon. In Proceedings of the Eighth In-
ternational Workshop on Tree Adjoining Grammar and
Related Formalisms, pages 17?24, Sydney, Australia.
Association for Computational Linguistics.
Stephen McConnel. 1995. PC-PATR reference manual.
Stefan Mu?ller and Walter Kasper. 2000. HPSG analy-
sis for German. In Wolfgang Wahlster, editor, Verb-
mobil: Foundations of Speech-to-Speech translation,
pages 238 ? 253, Berlin, Germany. Springer.1075
Stefan Mu?ller. 1999. Deutsche Syntax deklarativ. Head-
Driven Phrase Structure Grammar fu?r das Deutsche.
Max Niemeyer Verlag, Tu?bingen.
Stefan Mu?ller. 2008. Depictive secondary predicates in
german and english. In Christoph Schroeder, Gerd
Hentschel, and Winfried Boeder, editors, Secondary
Predicates in Eastern European Languages and Be-
yond, number 16 in Studia Slavica Oldenburgensia,
pages 255?273, Oldenburg, Germany. BIS-Verlag.
Stefan Mu?ller. 2009. On predication. In Stefan Mu?ller,
editor, Proceedings of the 16th International Con-
ference on Head-Driven Phrase Structure Grammar,
Stanford, USA. CSLI Publications.
Stephan Oepen. 2001. [incr tsdb()] ? competence
and performance laboratory. Technical report, DFKI,
Saarbru?cken, Germany.
Aarne Ranta. 2009. The GF resource grammar library.
Linguistic Issues in Language Technology, 2(2).
1076
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1691?1701,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Offspring from Reproduction Problems:
What Replication Failure Teaches Us
Antske Fokkens and Marieke van Erp
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
{a.s.fokkens,m.g.j.van.erp}@vu.nl
Marten Postma
Utrecht University
Utrecht, The Netherlands
martenp@gmail.com
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
Piek Vossen
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
piek.vossen@vu.nl
Nuno Freire
The European Library
The Hague, The Netherlands
nfreire@gmail.com
Abstract
Repeating experiments is an important in-
strument in the scientific toolbox to vali-
date previous work and build upon exist-
ing work. We present two concrete use
cases involving key techniques in the NLP
domain for which we show that reproduc-
ing results is still difficult. We show that
the deviation that can be found in repro-
duction efforts leads to questions about
how our results should be interpreted.
Moreover, investigating these deviations
provides new insights and a deeper under-
standing of the examined techniques. We
identify five aspects that can influence the
outcomes of experiments that are typically
not addressed in research papers. Our use
cases show that these aspects may change
the answer to research questions leading
us to conclude that more care should be
taken in interpreting our results and more
research involving systematic testing of
methods is required in our field.
1 Introduction
Research is a collaborative effort to increase
knowledge. While it includes validating previous
approaches, our experience is that most research
output in our field focuses on presenting new ap-
proaches, and to a somewhat lesser extent building
upon existing work.
In this paper, we argue that the value of research
that attempts to replicate previous approaches goes
beyond simply validating what is already known.
It is also an essential aspect for building upon
existing approaches. Especially when validation
fails or variations in results are found, systematic
testing helps to obtain a clearer picture of both the
approach itself and of the meaning of state-of-the-
art results leading to a better insight into the qual-
ity of new approaches in relation to previous work.
We support our claims by presenting two use
cases that aim to reproduce results of previous
work in two key NLP technologies: measuring
WordNet similarity and Named Entity Recogni-
tion (NER). Besides highlighting the difficulty of
repeating other researchers? work, new insights
about the approaches emerged that were not pre-
sented in the original papers. This last point shows
that reproducing results is not merely part of good
practice in science, but also an essential part in
gaining a better understanding of the methods we
use. Likewise, the problems we face in reproduc-
ing previous results are not merely frustrating in-
conveniences, but also pointers to research ques-
tions that deserve deeper investigation.
We investigated five aspects that cause exper-
imental variation that are not typically described
in publications: preprocessing (e.g. tokenisa-
tion), experimental setup (e.g. splitting data for
cross-validation), versioning (e.g. which version
of WordNet), system output (e.g. the exact fea-
tures used for individual tokens in NER), and sys-
tem variation (e.g. treatment of ties).
As such, reproduction provides a platform for
systematically testing individual aspects of an ap-
proach that contribute to a given result. What is
the influence of the size of the dataset, for exam-
ple? How does using a different dataset affect the
results? What is a reasonable divergence between
different runs of the same experiment? Finding
answers to these questions enables us to better in-
terpret our state-of-the-art results.
1691
Moreover, the experiments in this paper show
that even while strictly trying to replicate a pre-
vious experiment, results may vary up to a point
where they lead to different answers to the main
question addressed by the experiment. The Word-
Net similarity experiment use case compares the
performance of different similarity measures. We
will show that the answer as to which measure
works best changes depending on factors such as
the gold standard used, the strategy towards part-
of-speech or the ranking coefficient, all aspects
that are typically not addressed in the literature.
The main contributions of this paper are the
following:
1) An in-depth analysis of two reproduction use
cases in NLP
2) New insights into the state-of-the-art results
for WordNet similarities and NER, found because
of problems in reproducing prior research
3) A categorisation of aspects influencing
reproduction of experiments and suggestions on
testing their influence systematically
The code, data and experimental setup
for the WordNet experiments are avail-
able at http://github.com/antske/
WordNetSimilarity, and for the NER exper-
iments at http://github.com/Mvanerp/
NER. The experiments presented in this paper
have been repeated by colleagues not involved in
the development of the software using the code
included in these repositories. The remainder of
this paper is structured as follows. In Section 2,
previous work is discussed. Sections 3 and 4
describe our real-world use cases. In Section 5,
we present our observations, followed by a more
general discussion in Section 6. In Section 7, we
present our conclusions.
2 Background
This section provides a brief overview of recent
work addressing reproduction and benchmark re-
sults in computer science related studies and dis-
cusses how our research fits in the overall picture.
Most researchers agree that validating results
entails that a method should lead to the same over-
all conclusions rather than producing the exact
same numbers (Drummond, 2009; Dalle, 2012;
Buchert and Nussbaum, 2012, etc.). In other
words, we should strive to reproduce the same an-
swer to a research question by different means,
perhaps by re-implementing an algorithm or eval-
uating it on a new (in domain) data set. Replica-
tion has a somewhat more limited aim, and simply
involves running the exact same system under the
same conditions in order to get the exact same re-
sults as output.
According to Drummond (2009) replication is
not interesting, since it does not lead to new in-
sights. On this point we disagree with Drum-
mond (2009) as replication allows us to: 1) vali-
date prior research, 2) improve on prior research
without having to rebuild software from scratch,
and 3) compare results of reimplementations and
obtain the necessary insights to perform reproduc-
tion experiments. The outcome of our use cases
confirms the statement that deeper insights into an
approach can be obtained when all resources are
available, an observation also made by Ince et al
(2012).
Even if exact replication is not a goal many
strive for, Ince et al (2012) argue that insightful
reproduction can be an (almost) impossible un-
dertaking without the source code being available.
Moreover, it is not always clear where replication
stops and reproduction begins. Dalle (2012) dis-
tinguishes levels of reproducing results related to
how close they are to the original work and how
each contributes to research. In general, an in-
creasing awareness of the importance of reproduc-
tion research and open code and data can be ob-
served based on publications in high-profile jour-
nals (e.g. Nature (Ince et al, 2012)) and initiatives
such as myExperiment.1
Howison and Herbsleb (2013) point out that,
even though this is important, often not enough
(academic) credit is gained from making resources
available. What is worse, the same holds for re-
search that investigates existing methods rather
than introducing new ones, as illustrated by the
question that is found on many review forms ?how
novel is the presented approach??. On the other
hand, initiatives for journals addressing exactly
this issue (Neylon et al, 2012) and tracks focus-
ing on results verification at conferences such as
VLDB2 show that this opinion is not universal.
A handful of use cases on reproducing or repli-
cating results have been published. Louridas and
Gousios (2012) present a use case revealing that
source code alone is not enough for reproducing
1http://www.myexperiment.org
2http://www.vldb.org/2013/
1692
results, a point that is also made by Mende (2010)
who provides an overview of all information re-
quired to replicate results.
The experiments in this paper provide use cases
that confirm the points brought out in the litera-
ture mentioned above. This includes both obser-
vations that a detailed level of information is re-
quired for truly insightful reproduction research as
well as the claim that such research leads to better
understanding of our techniques. Furthermore, the
work in this paper relates to Bikel (2004)?s work.
He provides all information needed in addition to
Collins (1999) to replicate Collins? benchmark re-
sults. Our work is similar in that we also aim to fill
in the blanks needed to replicate results. It must
be noted, however, that the use cases in this paper
have a significantly smaller scale than Bikel?s.
Our research distinguishes itself from previous
work, because it links the challenges of reproduc-
tion to what they mean for reported results be-
yond validation. Ruml (2010) mentions variations
in outcome as a reason not to emphasise compar-
isons to benchmarks. Vanschoren et al (2012)
propose to use experimental databases to system-
atically test variations for machine learning, but
neither links the two issues together. Raeder et al
(2010) come closest to our work in a critical study
on the evaluation of machine learning. They show
that choices in the methodology, such as data sets,
evaluation metrics and type of cross-validation can
influence the conclusions of an experiment, as we
also find in our second use case. However, they
focus on the problem of evaluation and recom-
mendations on how to achieve consistent repro-
ducible results. Our contribution is to investigate
how much results vary. We cannot control how
fellow researchers carry out their evaluation, but
if we have an idea of the variations that typically
occur within a system, we can better compare ap-
proaches for which not all details are known.
3 WordNet Similarity Measures
Patwardhan and Pedersen (2006) and Pedersen
(2010) present studies where the output of a va-
riety of WordNet similarity and relatedness mea-
sures are compared. They rank Miller and Charles
(1991)?s set (henceforth ?mc-set?) of 30 word
pairs according to their semantic relatedness with
several WordNet similarity measures.
Each measure ranks the mc-set of word pairs
and these outputs are compared to Miller and
Charles (1991)?s gold standard based on human
rankings using the Spearman?s Correlation Coeffi-
cient (Spearman, 1904, ?). Pedersen (2010) also
ranks the original set of 65 word pairs ranked
by humans in an experiment by Rubenstein and
Goodenough (1965) (rg-set) which is a superset of
Miller and Charles?s set.
3.1 Replication Attempts
This research emerged from a project run-
ning a similar experiment for Dutch on Cor-
netto (Vossen et al, 2013). First, an attempt
was made to reproduce the results reported in
Patwardhan and Pedersen (2006) and Peder-
sen (2010) on the English WordNet using their
WordNet::Similarity web-interface.3 Results dif-
fered from those reported in the aforementioned
works, even when using the same versions as
the original, WordNet::Similarity-1.02 and Word-
Net 2.1 (Patwardhan and Pedersen, 2006) and
WordNet::Similarity-2.05 and WordNet 3.0 (Ped-
ersen, 2010), respectively.4
The fact that results of similarity measures on
WordNet can differ even while the same software
and same versions are used indicates that proper-
ties which are not addressed in the literature may
influence the output of similarity measures. We
therefore conducted a range of experiments that,
in addition to searching for the right settings to
replicate results of previous research, address the
following questions:
1) Which properties have an impact on the per-
formance of WordNet similarity measures?
2) How much does the performance of individ-
ual measures vary?
3) How do commonly used measures compare
when the variation of their performance are taken
into account?
3.2 Methodology and first observations
The questions above were addressed in two stages.
In the first stage, Fokkens, who was not involved
in the first replication attempt implemented a
script to calculate similarity measures using Word-
Net::Similarity. This included similarity mea-
sures introduced by Wu and Palmer (1994) (wup),
3Obtained from http://talisker.d.umn.edu/
cgi-bin/similarity/similarity.cgi, Word-
Net::Similarity version 2.05. This web interface has now
moved to http://maraca.d.umn.edu
4WordNet::Similarity were obtained http://
search.cpan.org/dist/WordNet-Similarity/.
1693
Leacock and Chodorow (1998) (lch), Resnik
(1995) (res), Jiang and Conrath (1997) (jcn),
Lin (1998) (lin), Banerjee and Pedersen (2003)
(lesk), Hirst and St-Onge (1998) (hso) and
Patwardhan and Pedersen (2006) (vector and
vpairs) respectively.
Consequently, settings and properties were
changed systematically and shared with Pedersen
who attempted to produce the new results with his
own implementations. First, we made sure that
the script implemented by Fokkens could produce
the same WordNet similarity scores for each in-
dividual word pair as those used to calculate the
ranking on the mc-set by Pedersen (2010). Finally,
the gold standard and exact implementation of the
Spearman ranking coefficient were compared.
Differences in results turned out to be related
to variations in the experimental setup. First,
we made different assumptions on the restriction
of part-of-speech tags (henceforth ?PoS-tag?) con-
sidered in the comparison. Miller and Charles
(1991) do not discuss how they deal with words
with more than one PoS-tag in their study. Ped-
ersen therefore included all senses with any PoS-
tag in his study. The first replication attempt had
restricted PoS-tags to nouns based on the idea
that most items are nouns and subjects would be
primed to primarily think of the noun senses. Both
assumptions are reasonable. Pos-tags were not re-
stricted in the second replication attempt, but be-
cause of a bug in the code only the first identified
PoS-tag (?noun? in all cases) was considered. We
therefore mistakenly assumed that PoS-tag restric-
tions did not matter until we compared individual
scores between Pedersen and the replication at-
tempts.
Second, there are two gold standards for the
Miller and Charles (1991) set: one has the scores
assigned during the original experiment run by
Rubenstein and Goodenough (1965), the other
has the scores assigned during Miller and Charles
(1991)?s own experiment. The ranking correlation
between the two sets is high, but they are not iden-
tical. Again, there is no reason why one gold stan-
dard would be a better choice than the other, but in
order to replicate results, it must be known which
of the two was used. Third, results changed be-
cause of differences in the treatment of ties while
calculating Spearman ?. The influence of the ex-
act gold standard and calculation of Spearman ?
could only be found because Pedersen could pro-
measure Spearman ? Kendall ? ranking
min max min max variation
path based similarity
path 0.70 0.78 0.55 0.62 1-8
wup 0.70 0.79 0.53 0.61 1-6
lch 0.70 0.78 0.55 0.62 1-7
path based information content
res 0.65 0.75 0.26 0.57 4-11
lin 0.49 0.73 0.36 0.53 6-10
jcn 0.46 0.73 0.32 0.55 5, 7-11
path based relatedness
hso 0.73 0.80 0.36 0.41 1-3,5-10
dictionary and corpus based relatedness
vpairs 0.40 0.70 0.26 0.50 7-11
vector 0.48 0.92 0.33 0.76 1,2,4,6-11
lesk 0.66 0.83 -0.02 0.61 1-8,11,12
Table 1: Variation WordNet measures? results
vide the output of the similarity measures he used
to calculate the coefficient. It is unlikely we would
have been able to replicate his results at all with-
out the output of this intermediate step. Finally,
results for lch, lesk and wup changed accord-
ing to measure specific configuration settings such
as including a PoS-tag specific root node or turn-
ing on normalisation.
In the second stage of this research, we ran ex-
periments that systematically manipulate the influ-
ential factors described above. In this experiment,
we included both the mc-set and the complete rg-
set. The implementation of Spearman ? used in
Pedersen (2010) assigned the lowest number in
ranking to ties rather than the mean, resulting in
an unjustified drop in results for scores that lead
to many ties. We therefore experimented with a
different correlation measure, Kendall tau coeffi-
cient (Kendall, 1938, ? ) rather than two versions
of Spearman ?.
3.3 Variation per measure
All measures varied in their performance.
The complete outcome of our experiments
(both the similarity measures assigned to
each pair as well as the output of the ranking
coefficients) are included in the data set pro-
vided at http://github.com/antske/
WordNetSimilarity. Table 1 presents an
overview of the main point we wish to make
through this experiment: the minimal and maxi-
mal results according to both ranking coefficients.
Results for similarity measures varied from 0.06-
0.42 points for Spearman ? and from 0.05-0.60
points for Kendall ? . The last column indi-
cates the variation of performance of a measure
1694
compared to the other measures, where 1 is the
best performing measure and 12 is the worst.5
For instance, path has been best performing
measure, second best, eighth best and all positions
in between, vector has ranked first, second and
fourth, but also occupied all positions from six to
eleven.
In principle, it is to be expected that num-
bers are not exactly the same while evaluating
against a different data set (the mc-set versus the
rg-set), taking a different set of synsets to evalu-
ate on (changing PoS-tag restrictions) or changing
configuration settings that influence the similarity
score. However, a variation of up to 0.44 points
in Spearman ? and 0.60 in Kendall ? 6 leads to
the question of how indicative these results really
are. A more serious problem is the fact that the
comparative performance of individual measure
changes. Which measure performs best depends
on the evaluation set, ranking coefficient, PoS-tag
restrictions and configuration settings. This means
that the answer to the question of which similarity
measure is best to mimic human similarity scores
depends on aspects that are often not even men-
tioned, let alne systematically compared.
3.4 Variation per category
For each influential category of experimental vari-
ation, we compared the variation in Spearman ?
and Kendall ? , while similarity measure and other
influential categories were kept stable. The cat-
egories we varied include WordNet and Word-
Net::Similarity version, the gold standard used to
evaluate, restrictions on PoS-tags, and measure
specific configurations. Table 2 presents the maxi-
mum variation found across measures for each cat-
egory. The last column indicates how often the
ranking of a specific measure changed as the cat-
egory changed, e.g. did the measure ranking third
using specific configurations, PoS-tag restrictions
and a specific gold standard using WordNet 2.1
still rank third when WordNet 3.0 was used in-
stead? The number in parentheses next to the ?dif-
ferent ranks? in the table presents the total num-
ber of scores investigated. Note that this num-
ber changes for each category, because we com-
5Some measures ranked differently as their individual
configuration settings changed. In these cases, the measure
was included in the overall ranking multiple times, which is
why there are more ranking positions than measures.
6Section 3.4 explains why the variation in Kendall is this
extreme and ? is more appropriate for this task.
Variation Maximum difference Different
Spearman ? Kendall ? rank (tot)
WN version 0.44 0.42 223 (252)
gold standard 0.24 0.21 359 (504)
PoS-tag 0.09 0.08 208 (504)
configuration 0.08 0.60 37 (90)
Table 2: Variations per category
pared two WordNet versions (WN version), three
gold standard and PoS-tag restriction variations
and configuration only for the subset of scores
where configuration matters.
There are no definite statements to make as to
which version (Patwardhan and Pedersen (2006)
vs Pedersen (2010)), PoS-tag restriction or con-
figuration gives the best results. Likewise, while
most measures do better on the smaller data set,
some achieve their highest results on the full set.
This is partially due to the fact that ranking coef-
ficients are sensitive to outliers. In several cases
where PoS-tag restrictions led to different results,
only one pair received a different score. For in-
stance, path assigns a relatively high score to
the pair chord-smile when verbs are included, be-
cause the hierarchy of verbs in WordNet is rela-
tively flat. This effect is not observed in wup and
lch which correct for the depth of the hierarchy.
On the other hand, res, lin and jcn score bet-
ter on the same set when verbs are considered, be-
cause they cannot detect any relatedness for the
pair crane-implement when restricted to nouns.
On top of the variations presented above, we no-
tice a discrepancy between the two coefficients.
Kendall ? generally leads to lower coefficiency
scores than Spearman ?. Moreover, they each
give different relative indications: where lesk
achieves its highest Spearman ?, it has an ex-
tremely low Kendall ? of 0.01. Spearman ? uses
the difference in rank as its basis to calculate a cor-
relation, where Kendall ? uses the number of items
with the correct rank. The low Kendall ? for lesk
is the result of three pairs receiving a score that is
too high. Other pairs that get a relatively accurate
score are pushed one place down in rank. Because
only items that receive the exact same rank help to
increase ? , such a shift can result in a drastic drop
in the coefficient. In our opinion, Spearman ? is
therefore preferable over Kendall ? . We included
? , because many authors do not mention the rank-
ing coefficient they use (cf. Budanitsky and Hirst
(2006), Resnik (1995)) and both ? and ? are com-
1695
monly used coefficients.
Except for WordNet, which Budanitsky and
Hirst (2006) hold accountable for minor variations
in a footnote, the influential categories we investi-
gated in this paper, to our knowledge, have not yet
been addressed in the literature. Cramer (2008)
points out that results from WordNet-Human sim-
ilarity correlations lead to scattered results report-
ing variations similar to ours, but she compares
studies using different measures, data and exper-
imental setup. This study shows that even if
the main properties are kept stable, results vary
enough to change the identity of the measure that
yields the best performance. Table 1 reveals a
wide variation in ranking relative to alternative ap-
proaches. Results in Table 2 show that it is com-
mon for the ranking of a score to change due to
variations that are not at the core of the method.
This study shows that it is far from clear how
different WordNet similarity measures relate to
each other. In fact, we do not know how we can
obtain the best results. This is particularly chal-
lenging, because the ?best results? may depend on
the intended use of the similarity scores (Meng
et al, 2013). This is also the reason why we
presented the maximum variation observed, rather
than the average or typical variation (mostly be-
low 0.10 points). The experiments presented in
this paper resulted in a vast amount of data. An
elaborate analysis of this data is needed to get a
better understanding of how measures work and
why results vary to such an extent. We leave this
investigation to future work. If there is one take-
home message from this experiment, it is that one
should experiment with parameters such as restric-
tions on PoS-tags or configurations and determine
which score to use depending on what it is used
for, rather than picking something that did best in
a study using different data for a different task and
may have used a different version of WordNet.
4 Reproducing a NER method
Freire et al (2012) describe an approach to clas-
sifying named entities in the cultural heritage do-
main. The approach is based on the assumption
that domain knowledge, encoded in complex fea-
tures, can aid a machine learning algorithm in
NER tasks when only little training data is avail-
able. These features include information about
person and organisation names, locations, as well
as PoS-tags. Additionally, some general features
are used such as a window of three preceding and
two following tokens, token length and capitalisa-
tion information. Experiments are run in a 10-fold
cross-validation setup using an open source ma-
chine learning toolkit (McCallum, 2002).
4.1 Reproducing NER Experiments
This experiment can be seen as a real-world case
of the sad tale of the Zigglebottom tagger (Peder-
sen, 2008). The (fictional) Zigglebottom tagger is
a tagger with spectacular results that looks like it
will solve some major problems in your system.
However, the code is not available and a new im-
plementation does not yield the same results. The
original authors cannot provide the necessary de-
tails to reproduce their results, because most of the
work has been done by a PhD student who has fin-
ished and moved on to something else. In the end,
the newly implemented Zigglebottom tagger is not
used, because it does not lead to the promised bet-
ter results and all effort went to waste.
Van Erp was interested in the NER approach
presented in Freire et al (2012). Unfortunately,
the code could not be made available, so she de-
cided to reimplement the approach. Despite feed-
back from Freire about particular details of the
system, results remained 20 points below those
reported in Freire et al (2012) in overall F-score
(Van Erp and Van der Meij, 2013).
The reimplementation process involved choices
about seemingly small details such as rounding
to how many decimals, how to tokenise or how
much data cleanup to perform (normalisation of
non-alphanumeric characters for example). Try-
ing different parameter combinations for feature
generation and the algorithm never yielded the ex-
act same results as Freire et al (2012). The results
of the best run in our first reproduction attempt,
together with the original results from Freire et al
(2012) are presented in Table 3. Van Erp and Van
der Meij (2013) provide an overview of the imple-
mentation efforts.
4.2 Following up from reproduction
Since the experiments in Van Erp and Van der Meij
(2013) introduce several new research questions
regarding the influence of data cleaning and the
limitations of the dataset, we performed some ad-
ditional experiments.
First, we varied the tokenisation, removing non-
alphanumeric characters from the data set. This
yielded a significantly smaller data set (10,442
1696
(Freire et al, 2012) results Van Erp and Van der Meij?s replication results
Precision Recall F?=1 Precision Recall F?=1
LOC (388) 92% 55% 69 77.80% 39.18% 52.05
ORG (157) 90% 57% 70 65.75% 30.57% 41.74
PER (614) 91% 56% 69 73.33% 37.62% 49.73
Overall (1,159) 91% 55% 69 73.33% 37.19% 49.45
Table 3: Precision, recall and F?=1 scores for the original experiments from Freire et al 2012 and our
replication of their approach as presented in Van Erp and Van der Meij (2013)
tokens vs 12,510), and a 15 point drop in over-
all F-score. Then, we investigated whether vari-
ation in the cross-validation splits made any dif-
ference as we noticed that some NEs were only
present in particular fields in the data, which can
have a significant impact on a small dataset. We
inspected the difference between different cross-
validation folds by computing the standard devi-
ations of the scores and found deviations of up
to 25 points in F-score between the 10 splits. In
the general setup, database records were randomly
distributed over the folds and cut off to balance the
fold sizes. In a different approach to dividing the
data by distributing individual sentences from the
records over the folds, performance increases by
8.57 points in overall F-score to 58.02. This is not
what was done in the original Freire et al (2012)
paper, but shows that the results obtained with this
dataset are quite fragile.
As we worried about the complexity of the fea-
ture set relative to the size of the data set, we de-
viated somewhat from Freire et al (2012)?s exper-
iments in that we switched some features on and
off. Removal of complex features pertaining to the
window around the focus token improved our re-
sults by 3.84 points in overall F-score to 53.39.
The complex features based on VIAF,7 GeoN-
ames8 and WordNet do contribute to the classifica-
tion in the Mallet setup as removing them and only
using the focus token, window and generic fea-
tures causes a slight drop in overall F-score from
49.45 to 47.25.
When training the Stanford NER system (Finkel
et al, 2005) on just the tokens from the
Freire data set and the parameters from en-
glish.all.3class.distsim.prop (included in the Stan-
ford NER release, see also Van Erp and Van der
Meij (2013)), our F-scores come very close to
those reported by Freire et al (2012), but mostly
with a higher recall and lower precision. It is puz-
zling that the Stanford system obtains such high
7http://www.viaf.org
8http://www.geonames.org
results with only very simple features, whereas
for Mallet the complex features show improve-
ment over simpler features. This leads to ques-
tions about the differences between the CRF im-
plementations and the influence of their parame-
ters, which we hope to investigate in future work.
4.3 Reproduction difficulties explained
Several reasons may be the cause of why we fail to
reproduce results. As mentioned, not all resources
and data were available for this experiment, thus
causing us to navigate in the dark as we could not
reverse-engineer intermediate steps, but only com-
pare to the final precision, recall and F-scores.
The experiments follow a general machine
learning setup consisting roughly of four steps:
preprocess data, generate features, train model and
test model. The novelty and replication problems
lie in the first three steps. How the data was pre-
processed is a major factor here. The data set con-
sisted of XML files marked up with inline named
entity tags. In order to generate machine learn-
ing features, this data has to be tokenised, possi-
bly cleaned up and the named entity markup had
to be converted to a token-based scheme. Each of
these steps can be carried out in several ways, and
choices made here can have great influence on the
rest of the pipeline.
Similar choices have to be made for prepro-
cessing external resources. From the descriptions
in the original paper, it is unclear how records
in VIAF and GeoNames were preprocessed, or
even which versions of these resources were used.
Preprocessing and calculating occurrence statis-
tics over VIAF takes 30 hours for each run. It
is thus not feasible to identify the main potential
variations without the original data to verify this
prepatory step.
Numbers had to be rounded when generating
the features, leading to the question of how many
decimals are required to be discriminative with-
out creating an overly sparse dataset. Freire recalls
that encoding features as multi-value discrete fea-
1697
tures versus several boolean features can have sig-
nificant impact. These settings are not mentioned
in the paper, making reproduction very difficult.
As the project in which the original research
was performed has ended, and there is no cen-
tral repository where such information can be re-
trieved, we are left to wonder how to reuse this
approach in order to further domain-specific NER.
5 Observations
In this section, we generalise the observations
from our use cases to the main categories that can
influence reproduction.
Despite our efforts to describe our systems as
clearly as possible, details that can make a tremen-
dous difference are often omitted in papers. It will
be no surprise to researchers in the field that pre-
processing of data can make or break an experi-
ment.
The choice of which steps we perform, and how
each of these steps is carried out exactly are part
of our experimental setup. A major difference in
the results for the NER experiments was caused by
variations in the way in which we split the data for
cross-validation.
As we fine-tune our techniques, software gets
updated, data sets are extended or annotation bugs
are fixed. In the WordNet experiment, we found
that there were two different gold standard data
sets. There are also different versions of Word-
Net, and the WordNet::Similarity packages. Sim-
ilarly for the NER experiment, GeoNames, VIAF
and Mallet are updated regularly. It is therefore
critical to pay attention to versioning.
Our experiments often consist of several differ-
ent steps whose outputs may be difficult to retrace.
In order to check the output of a reproduction ex-
periment at every step of the way, system out-
put of experiments, including intermediate steps,
is vital. The WordNet replication was only pos-
sible, because Pedersen could provide the similar-
ity scores of each word pair. This enabled us to
compare the intermediate output and identify the
source of differences in output.
Lastly, there may be inherent system variations
in the techniques used. Machine learning algo-
rithms may for instance use coin flips in case of
a tie. This was not observed in our experiments,
but such variations may be determined by running
an experiment several times and taking the average
over the different runs (cf. Raeder et al (2010)).
All together, these observations show that shar-
ing data and software play a key role in gaining in-
sight into how our methods work. Vanschoren et
al. (2012) propose a setup that allows researchers
to provide their full experimental setup, which
should include exact steps followed in preprocess-
ing the data, documentation of the experimen-
tal setup, exact versions of the software and re-
sources used and experimental output. Having
access to such a setup allows other researchers
to validate research, but also tweak the approach
to investigate system variation, systematically test
the approach in order to learn its limitations and
strengths and ultimately improve on it.
6 Discussion
Many of the aspects addressed in the previous sec-
tion such as preprocessing are typically only men-
tioned in passing, or not at all. There is often not
enough space to capture all details, and they are
generally not the core of the research described.
Still, our use cases have shown that they can have a
tremendous impact on reproduction, and can even
lead to different conclusions. This leads to serious
questions on how we can interpret our results and
how we can compare the performance of different
methods. Is an improvement of a few per cent re-
ally due to the novelty of the approach if larger
variations are found when the data is split differ-
ently? Is a method that does not quite achieve the
highest reported state-of-the-art result truly less
good? What does a state-of-the-art result mean if
it is only tested on one data set?
If one really wants to know whether a result
is better or worse than the state-of-the-art, the
range of variation within the state-of-the-art must
be known. Systematic experiments such as the
ones we carried out for WordNet similarity and
NER, can help determine this range. For results
that fall within the range, it holds that they can
only be judged by evaluations going beyond com-
paring performance numbers, i.e. an evaluation of
how the approach achieves a given result and how
that relates to alternative approaches.
Naturally, our use cases do not represent the en-
tire gamut of research methodologies and prob-
lems in the NLP community. However, they do
represent two core technologies and our observa-
tions align with previous literature on replication
and reproduction.
Despite the systematic variation we employed
1698
in our experiments, they do not answer all ques-
tions that the problems in reproduction evoked.
For the WordNet experiments, deeper analysis is
required to gain full understanding of how indi-
vidual influential aspects interact with each mea-
surement. For the NER experiments, we are yet to
identify the cause of our failure to reproduce.
The considerable time investment required for
such experiments forms a challenge. Due to pres-
sure to publish or other time limitations, they can-
not be carried out for each evaluation. There-
fore, it is important to share our experiments, so
that other researchers (or students) can take this
up. This could be stimulated by instituting repro-
duction tracks in conferences, thus rewarding sys-
tematic investigation of research approaches. It
can also be aided by adopting initiatives that en-
able authors to easily include data, code and/or
workflows with their publications such as the
PLOS/figshare collaboration.9 We already do a
similar thing for our research problems by organ-
ising challenges or shared tasks, why not extend
this to systematic testing of our approaches?
7 Conclusion
We have presented two reproduction use cases for
the NLP domain. We show that repeating other
researchers? experiments can lead to new research
questions and provide new insights into and better
understanding of the investigated techniques.
Our WordNet experiments show that the perfor-
mance of similarity measures can be influenced by
the PoS-tags considered, measure specific varia-
tions, the rank coefficient and the gold standard
used for comparison. We not only find that such
variations lead to different numbers, but also dif-
ferent rankings of the individual measures, i.e.
these aspects lead to a different answer to the
question as to which measure performs best. We
did not succeed in reproducing the NER results
of Freire et al (2012), showing the complexity
of what seems a straightforward reproduction case
based on a system description and training data
only. Our analyses show that it is still an open
question whether additional complex features im-
prove domain specific NER and that this may par-
tially depend on the CRF implementation.
Some observations go beyond our use cases. In
particular, the fact that results vary significantly
9http://blogs.plos.org/plos/2013/01/
easier-access-to-plos-data/
because of details that are not made explicit in
our publications. Systematic testing can provide
an indication of this variation. We have classi-
fied relevant aspects in five categories occurring
across subdisciplines of NLP: preprocessing, ex-
perimental setup, versioning, system output,
and system variation.
We believe that knowing the influence of differ-
ent aspects in our experimental workflow can help
increase our understanding of the robustness of
the approach at hand and will help understand the
meaning of the state-of-the-art better. Some tech-
niques are reused so often (the papers introducing
WordNet similarity measures have around 1,000-
2,000 citations each as of February 2013, for ex-
ample) that knowing their strengths and weak-
nesses is essential for optimising their use.
As mentioned many times before, sharing is key
to facilitating reuse, even if the code is imper-
fect and contains hacks and possibly bugs. In the
end, the same holds for software as for documen-
tation: it is like sex: if it is good, it is very good
and if it is bad, it is better than nothing!10 But
most of all: when reproduction fails, regardless of
whether original code or a reimplementation was
used, valuable insights can emerge from investi-
gating the cause of this failure. So don?t let your
failing reimplementations of the Zigglebottom tag-
ger collect dusk on a shelf while others reimple-
ment their own failing Zigglebottoms. As a com-
munity, we need to know where our approaches
fail, as much ?if not more? as where they succeed.
Acknowledgments
We would like to thank the anonymous review-
ers for their eye to detail and useful comments
to make this a better paper. We furthermore
thank Ruben Izquierdo, Lourens van der Meij,
Christoph Zwirello, Rebecca Dridan and the Se-
mantic Web Group at VU University for their
help and useful feedback. The research leading to
this paper was supported by the European Union?s
7th Framework Programme via the NewsReader
Project (ICT-316404), the Agora project, by NWO
CATCH programme, grant 640.004.801, and the
BiographyNed project, a joint project with Huy-
gens/ING Institute of the Dutch Academy of Sci-
ences funded by the Netherlands eScience Center
(http://esciencecenter.nl/).
10The documentation variant of this quote is attributed to
Dick Brandon.
1699
References
Stanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pages 805?
810, Acapulco, August.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Tomasz Buchert and Lucas Nussbaum. 2012. Lever-
aging business workflows in distributed systems re-
search for the orchestration of reproducible and scal-
able experiments. In Anne Etien, editor, 9e`me
e?dition de la confe?rence MAnifestation des JE-
unes Chercheurs en Sciences et Technologies de
l?Information et de la Communication - MajecSTIC
2012 (2012).
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Phd dissertation,
University of Pennsylvania.
Irene Cramer. 2008. How well do semantic related-
ness measures perform? a meta-study. In Semantics
in Text Processing. STEP 2008 Conference Proceed-
ings, volume 1, pages 59?70.
Olivier Dalle. 2012. On reproducibility and trace-
ability of simulations. In WSC-Winter Simulation
Conference-2012.
Chris Drummond. 2009. Replicability is not repro-
ducibility: nor is it good science. In Proceedings of
the Twenty-Sixth International Conference on Ma-
chine Learning: Workshop on Evaluation Methods
for Machine Learning IV.
Jenny Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363?370, Ann Arbor, USA.
Nuno Freire, Jose? Borbinha, and Pa?vel Calado. 2012.
An approach for named entity recognition in poorly
structured data. In Proceedings of ESWC 2012.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detection
and correction of malapropisms. In C. Fellbaum, ed-
itor, WordNet: An electronic lexical database, pages
305?332. MIT Press.
James Howison and James D. Herbsleb. 2013. Shar-
ing the spoils: incentives and collaboration in sci-
entific software development. In Proceedings of the
2013 conference on Computer Supported Coopera-
tive Work, pages 459?470.
Darrel C. Ince, Leslie Hatton, and John Graham-
Cumming. 2012. The case for open computer pro-
grams. Nature, 482(7386):485?488.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of the International Confer-
ence on Research in Computational Linguistics (RO-
CLING X), pages 19?33, Taiwan.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, 30(1-2):81?93.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In C. Fellbaum, edi-
tor, WordNet: An electronic lexical database, pages
265?283. MIT Press.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296?304, Madison, USA.
Panos Louridas and Georgios Gousios. 2012. A note
on rigour and replicability. SIGSOFT Softw. Eng.
Notes, 37(5):1?4.
Andrew K. McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
Thilo Mende. 2010. Replication of defect prediction
studies: problems, pitfalls and recommendations. In
Proceedings of the 6th International Conference on
Predictive Models in Software Engineering. ACM.
Lingling Meng, Runqing Huang, and Junzhong Gu.
2013. A review of semantic similarity measures in
wordnet. International Journal of Hybrid Informa-
tion Technology, 6(1):1?12.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Cameron Neylon, Jan Aerts, C Titus Brown, Si-
mon J Coles, Les Hatton, Daniel Lemire, K Jar-
rod Millman, Peter Murray-Rust, Fernando Perez,
Neil Saunders, Nigam Shah, Arfon Smith, Gae?l
Varoquaux, and Egon Willighagen. 2012. Chang-
ing computational research. the challenges ahead.
Source Code for Biology and Medicine, 7(2).
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing wordnet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
the EACL 2006 Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 1?8, Trento, Italy.
Ted Pedersen. 2008. Empiricism is not a matter of
faith. Computational Linguistics, 34(3):465?470.
1700
Ted Pedersen. 2010. Information content measures
of semantic similarity perform better without sense-
tagged text. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2010), pages 329?332, Los Angeles, USA.
Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla.
2010. Consequences of variability in classifier per-
formance estimates. In Proceedings of ICDM?2010.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI), pages 448?453,
Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Wheeler Ruml. 2010. The logic of benchmarking: A
case against state-of-the-art performance. In Pro-
ceedings of the Third Annual Symposium on Combi-
natorial Search (SOCS-10).
Charles Spearman. 1904. Proof and measurement of
association between two things. American Journal
of Psychology, 15:72?101.
Marieke Van Erp and Lourens Van der Meij. 2013.
Reusable research? a case study in named entity
recognition. CLTL 2013-01, Computational Lexi-
cology & Terminology Lab, VU University Amster-
dam.
Joaquin Vanschoren, Hendrik Blockeel, Bernhard
Pfahringer, and Geoffrey Holmes. 2012. Experi-
ment databases. Machine Learning, 87(2):127?158.
Piek Vossen, Isa Maks, Roxane Segers, Hennie van der
Vliet, Marie-Francine Moens, Katja Hofmann, Erik
Tjong Kim Sang, and Maarten de Rijke. 2013. Cor-
netto: a Combinatorial Lexical Semantic Database
for Dutch. In Peter Spyns and Jan Odijk, editors, Es-
sential Speech and Language Technology for Dutch
Results by the STEVIN-programme, number XVII in
Theory and Applications of Natural Language Pro-
cessing, chapter 10. Springer.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, Las Cruces,
USA.
1701
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 11?20,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
GAF: A Grounded Annotation Framework for Events
Antske Fokkens, Marieke van Erp, Piek Vossen
The Network Institute
VU University Amsterdam
antske.fokkens@vu.nl
marieke.van.erp@vu.nl
piek.vossen@vu.nl
Sara Tonelli
FBK
Trento, Italy
satonelli@fbk.eu
Willem Robert van Hage
SynerScope B.V.
Eindhoven, The Netherlands
willem.van.hage
@synerscope.com
Luciano Serafini, Rachele Sprugnoli
FBK
Trento, Italy
serafini@fbk.eu
sprugnoli@fbk.eu
Jesper Hoeksema
The Network Institute
VU University Amsterdam
j.e.hoeksema@vu.nl
Abstract
This paper introduces GAF, a grounded an-
notation framework to represent events in a
formal context that can represent information
from both textual and extra-textual sources.
GAF makes a clear distinction between men-
tions of events in text and their formal rep-
resentation as instances in a semantic layer.
Instances are represented by RDF compliant
URIs that are shared across different research
disciplines. This allows us to complete textual
information with external sources and facili-
tates reasoning. The semantic layer can inte-
grate any linguistic information and is com-
patible with previous event representations in
NLP. Through a use case on earthquakes in
Southeast Asia, we demonstrate GAF flexibil-
ity and ability to reason over events with the
aid of extra-linguistic resources.
1 Introduction
Events are not only described in textual documents,
they are also represented in many other non-textual
sources. These sources include videos, pictures,
sensors or evidence from data registration such as
mobile phone data, financial transactions and hos-
pital registrations. Nevertheless, many approaches
to textual event annotation consider events as text-
internal-affairs, possibly across multiple documents
but seldom across different modalities. It follows
from the above that event representation is not ex-
clusively a concern for the NLP community. It also
plays a major role in several other branches of in-
formation science such as knowledge representation
and the Semantic Web, which have created their own
models for representing events.
We propose a grounded annotation framework
(GAF) that allows us to interconnect different ways
of describing and registering events, including non-
linguistic sources. GAF representations can be used
to reason over the cumulated and linked sources of
knowledge and information to interpret the often in-
complete and fragmented information that is pro-
vided by each source. We make a clear distinction
between mentions of events in text or any other form
of registration and their formal representation as in-
stances in a semantic layer.
Mentions in text are annotated using the Terence
Annotation Format (Moens et al, 2011, TAF) on top
of which the semantic layer is realized using Seman-
tic Web technologies and standards. In this semantic
layer, instances are denoted with Uniform Resource
Identifiers (URIs). Attributes and relations are ex-
pressed according to the Simple Event Model (Van
Hage et al, 2011, SEM) and other established on-
tologies. Statements are grouped in named graphs
based on provenance and (temporal) validity, en-
abling the representation of conflicting information.
External knowledge can be related to instances from
a wide variety of sources such as those found in the
Linked Open Data Cloud (Bizer et al, 2009a).
Instances in the semantic layer can optionally be
linked to one or more mentions in text or to other
sources. Because linking instances is optional, our
11
representation offers a straightforward way to in-
clude information that can be inferred from text,
such as implied participants or whether an event is
part of a series that is not explicitly mentioned. Due
to the fact that each URI is unique, it is clear that
mentions connected to the same URI have a coref-
erential relation. Other relations between instances
(participants, subevents, temporal relations, etc.) are
represented explicitly in the semantic layer.
The remainder of this paper is structured as fol-
lows. In Section 2, we present related work and ex-
plain the motivation behind our approach. Section 3
describes the in-text annotation approach. Our se-
mantic annotation layer is presented in Section 4.
Sections 5-7 present GAF through a use case on
earthquakes in Indonesia. This is followed by our
conclusions and future work in section 8.
2 Motivation and Background
Annotation of events and of relations between them
has a long tradition in NLP. The MUC confer-
ences (Grishman and Sundheim, 1996) in the 90s
did not explicitly annotate events and coreference
relations, but the templates used for evaluating the
information extraction tasks indirectly can be seen
as annotation of events represented in newswires.
Such events are not ordered in time or further related
to each other. In response, Setzer and Gaizauskas
(2000) describe an annotation framework to create
coherent temporal orderings of events represented
in documents using closure rules. They suggest that
reasoning with text independent models, such as a
calendar, helps annotating textual representations.
More recently, generic corpora, such as Prop-
bank (Palmer et al, 2005) and the Framenet cor-
pus (Baker et al, 2003) have been built according to
linguistic principles. The annotations aim at prop-
erly representing verb structures within a sentence
context, focusing on verb arguments, semantic roles
and other elements. In ACE 2004 (Linguistic Data
Consortium, 2004b), event detection and linking is
included as a pilot task for the first time, inspired by
annotation schemes developed for named entities.
They distinguish between event mentions and the
trigger event, which is the mention that most clearly
expresses its occurrence (Linguistic Data Consor-
tium, 2004a). Typically, agreement on the trigger
event is low across annotators (around 55% (Moens
et al, 2011)). Timebank (Pustejovsky et al, 2006b)
is a more recent corpus for representing events and
time-expressions that includes temporal relations in
addition to plain coreference relations.
All these approaches have in common that they
consider the textual representation as a closed world
within which events need to be represented. This
means that mentions are linked to a trigger event
or to each other but not to an independent semantic
representation. More recently, researchers started to
annotate events across multiple documents, such as
the EventCorefBank (Bejan and Harabagiu, 2010).
Cross-document coreference is more challenging for
establishing the trigger event, but it is in essence not
different from annotating textual event coreference
within a single document. Descriptions of events
across documents may complement each other pro-
viding a more complete picture, but still textual de-
scriptions tend to be incomplete and sparse with re-
spect to time, place and participants. At the same
time, the comparison of events becomes more com-
plex. We thus expect even lower agreement in as-
signing trigger events across documents. Nothman
et al (2012) define the trigger as the first new ar-
ticle that mentions an event, which is easier than
to find the clearest description and still report inter-
annotator agreement of .48 and .73, respectively.
Recent approaches to automatically resolve event
coreference (cf. Chambers and Jurafsky (2011a),
Bejan and Harabagiu (2010)) use some background
data to establish coreference and other relations be-
tween events in text. Background information, in-
cluding resources, and models learned from textual
data do not represent mentions of events directly but
are useful to fill gaps of knowledge in the textual
descriptions. They do not alter the model for anno-
tation as such.
We aim to take these recent efforts one step fur-
ther and propose a grounded annotation framework
(GAF). Our main goal is to integrate information
from text analysis in a formal context shared with
researchers across domains. Furthermore, GAF is
flexible enough to contain contradictory informa-
tion. This is both important to represent sources
that (partially) contradict each other and to com-
bine alternative annotations or output of different
NLP tools. Because conflicting information may be
12
present, provenance of information is provided in
our framework, so that we may decide which source
to trust more or use it as a feature to decide which in-
terpretation to follow. Different models of event rep-
resentation exist that can contribute valuable infor-
mation. Therefore our model is compliant with prior
approaches regardless of whether they are manual or
automatic. Finally, GAF makes a clear distinction
between instances and instance mentions avoiding
the problem of determining a trigger event. Addi-
tionally, it facilitates the integration of information
from extra-textual sources and information that can
be inferred from texts, but is not explicitly men-
tioned. Sections 5 to 7 will explain how we can
achieve this with GAF.
3 The TERENCE annotation format
The TERENCE Annotation Format (TAF) is de-
fined within the TERENCE Project1 with the goal
to include event mentions, temporal expressions and
participant mentions in a single annotation proto-
col (Moens et al, 2011). TAF is based on ISO-
TimeML (Pustejovsky et al, 2010), but introduces
several adaptations in order to fit the domain of chil-
dren?s stories for which it was originally developed.
The format has been used to annotate around 30 chil-
dren stories in Italian and 10 in English.
We selected TAF as the basis for our in-text anno-
tation for three reasons. First, it incorporates the (in
our opinion crucial) distinction between instances
and instance mentions. Second, it adapts some con-
solidated paradigms for linguistic annotation such as
TimeML for events and temporal expressions and
ACE for participants and participant mentions (Lin-
guistic Data Consortium, 2005). It is thus compat-
ible with other annotation schemes. Third, it inte-
grates the annotation of event mentions, participants
and temporal expressions into a unified framework.
We will elaborate briefly on these properties below.
As mentioned, TAF makes a clear distinction be-
tween instances and instance mentions. Originally,
this distinction only applied to nominal and named
entities, similar to ACE (Linguistic Data Consor-
tium, 2005), because children?s stories can gener-
ally be treated as a closed world, usually present-
1ICT FP7 Programme, ICT-2010-25410, http://www.
terenceproject.eu/
ing a simple sequence of events that do not corefer.
Event coreference and linking to other sources was
thus not relevant for this domain. In GAF, we ex-
tend the distinction between instances and instance
mentions to events to model event coreference, link
them to other sources and create a consistent model
for all instances.
Children?s stories usually include a small set of
characters, event sequences (mostly in chronologi-
cal order), and a few generic temporal expressions.
In the TERENCE project, modeling characters in
the stories is necessary. This requires an extension
of TimeML to deal with event participants. Puste-
jovsky et al (2006a) address the need to include ar-
guments in TimeML annotations, but that proposal
did not include specific examples and details on how
to perform annotation (e.g., on the participants? at-
tributes). Such guidelines were created for TAF.
The TAF annotation of event mentions largely
follows TimeML in annotating tense, aspect, class,
mood, modality and polarity and temporal expres-
sions. However, there are several differences be-
tween TAF and TimeML. First, temporal expres-
sions are not normalized into the ISO-8601 form,
because most children?s stories are not fixed to a spe-
cific date. In GAF, the normalization of expressions
takes place in the semantic layer as these go beyond
the scope of the text. As a result, temporal vague-
ness of linguistic expressions in text do not need to
be normalized in the textual representation to actual
time points and remain underspecified.2
In TAF, events and participant mentions are linked
through a has participant relation, which is defined
as a directional, one-to-one relation from the event
to the participant mentions. Only mentions corre-
sponding to mandatory arguments of the events in
the story are annotated. Annotators look up each
verb in a reference dictionary providing information
on the predicate-argument structure of each verb.
This makes annotation easier and generally not con-
troversial. However, this kind of information can be
provided only by annotators having a good knowl-
edge of linguistics.
All annotations are performed with the Celct An-
2Note that we can still use existing tools for normalization
at the linguistic level: early normalizations can be integrated
in the semantic layer alongside normalizations carried out at a
later point.
13
sem:sub
EventOf
sem:Event sem:Actor sem:Place sem:Time
sem:hasTime
sem:hasActor
sem:hasPlace
sem:PlaceType
sem:placeType
sem:EventType
sem:eventType
sem:ActorType
sem:actorType
sem:TimeType
sem:Type
sem:timeType
sem:Core
sem:subTypeOf
C
o
r
e
 
C
l
a
s
s
e
s
(
F
o
r
e
i
g
n
)
T
y
p
e
 
S
y
s
t
e
m
Literal sem:hasTimeStamp
Literal sem:hasTimeStamp
Figure 1: The SEM ontology
notation Tool (Bartalesi Lenzi et al, 2012), an online
tool supporting TimeML that can easily be extended
to include participant information. The annotated
file can be exported to various XML formats and im-
ported into the semantic layer. The next section de-
scribes SEM, the event model used in our semantic
layer, and how it complements the TAF annotations.
4 The Simple Event Model
The Simple Event Model (SEM) is an RDF
schema (Carroll and Klyne, 2004; Guha and Brick-
ley, 2004) to express who did what, where, and
when. There are many RDF schemas and OWL on-
tologies (Motik et al, 2009) that describe events,
e.g., Shaw et al (2009), Crofts et al (2008) and
Scherp et al (2009). SEM is among the most
flexible and easiest to adapt to different domains.
SEM describes events and related instances such as
the place, time and participants (called Actors in
SEM) by representing the interactions between the
instances with RDF triples. SEM models are se-
mantic networks that include events, places, times,
participants and all related concepts, such as their
types.
An overview of all the classes in the SEM ontol-
ogy and the relations connecting them is shown in
Figure 1. Nodes can be identified by URIs, which
universally identify them across all RDF models. If
for example one uses the URI used by DBpedia3
(Bizer et al, 2009b) for the 2004 catastrophe in In-
3http://dbpedia.org
donesia, then one really means the same event as ev-
erybody else who uses that URI. SEM does not put
any constraints on the RDF vocabulary, so vocabu-
laries can easily be reused. Places and place types
can for example be imported from GeoNames4 and
event types from the RDF version of WordNet.
SEM supports two types of abstraction: gener-
alization with hierarchical relations from other on-
tologies, such as the subclass relation from RDFS,
and aggregation of events into superevents with the
sem:subEventOf relation, as exemplified in Fig-
ure 2. Other types of abstractions can be represented
using additional schemas or ontologies in combina-
tion with SEM. For instance, temporal aggregation
can be done with constructs from the OWL Time
ontology (Hobbs and Pan, 2004).
Relations between events and other instances,
which could be other events, places, actors, times,
or external concepts, can be modeled using the
sem:eventProperty relation. This relation can
be refined to represent specific relations, such as
specific participation, causality or simultaneity rela-
tions. The provenance of information in the SEM
graph is captured through assigning contexts to
statements using the PROV Data Model (Moreau et
al., 2012). In this manner, all statements derived
from a specific newspaper article are stored in a
named graph that represents that origin. Conflicting
statements can be stored in different named graphs,
and can thus coexist. This gives us the possibility
4http://www.geonames.org/ontology/
14
sem:Event
sem:Place
sem:EventType
sem:Time
dbpedia:2004_Indian_Ocean_
earthquake_and_ tsunami
rdf:type
"December 2004 
Earthquake and 
Tsunami"@en
rdfs:label
rdf:type
rdf:type
"3.316"^^xsd:decimal
"2004-12-26"^^xsd:date
"95.854"^^xsd:decimal
wgs84:long
wgs84:lat
owltime:inXSD
DateTime
sem:hasPlace sem:hasTime
naacl:INSTANCE_186
rdf:type
sem:subEventOf
wn30:synset-
earthquake-noun-1
sem:eventType
rdf:type
naacl:INSTANCE_188
rdf:type
sem:subEventOf
naacl:INSTANCE_198
sem:hasTime
naacl:TIMEX3_81 "2004"str:anchorOfnwr:denotedBy
naacl:INSTANCE_MENTION_118
nwr:denotedBy "temblor"@en
str:anchorOf
nwr:denotedBy
"tsunami"@en
naacl:INSTANCE_MENTION_120
str:anchorOf
naacl:INSTANCE_189
sem:subEventOf
naacl:INSTANCE_MENTION_121
nwr:denotedBy
"swept"@en
str:anchorOf
sem:hasPlace
naacl:INSTANCE_67
naacl:INSTANCE_MENTION_19nwr:denotedBy
"Indian Ocean"@en
str:anchorOf
taf:LOCATION
taf:NSUBJ
geonames:1545739
skos:exactMatch
gaf:G1
gaf:G2
gaf:G3
gaf:G4
gaf:G5
dbpedia:Bloomberg
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
sem:
derived
From
gaf:causes
Figure 2: Partial SEM representation of December 26th 2004 Earthquake
of delaying or ignoring the resolution of the conflict,
which enables use cases that require the analysis of
the conflict itself.
5 The GAF Annotation Framework
This section explains the basic idea behind GAF by
using texts on earthquakes in Indonesia. GAF pro-
vides a general model for event representation (in-
cluding textual and extra-textual mentions) as well
as exact representation of linguistic annotation or
output of NLP tools. Simply put, GAF is the combi-
nation of textual analyses and formal semantic rep-
resentations in RDF.
5.1 A SEM for earthquakes
We selected newspaper texts on the January 2009
West Papua earthquakes from Bejan and Harabagiu
(2010) to illustrate GAF. This choice was made be-
cause the topic ?earthquake? illustrates the advan-
tage of sharing URIs across domains. Gao and
Hunter (2011) propose a Linked Data model to cap-
ture major geological events such as earthquakes,
volcano activity and tsunamis. They combine infor-
mation from different seismological databases with
the intention to provide more complete information
to experts which may help to predict the occurrence
of such events. The information can also be used
in text interpretation. We can verify whether in-
terpretations by NLP tools correspond to the data
and relations defined by geologists or, through gen-
eralization, which interpretation is the most sensi-
ble given what we know about the events. General
information on events obtained from automatic text
processing, such as event templates (Chambers and
Jurafsky, 2011b) or typical event durations (Gusev
et al, 2010) can be integrated in SEM in a similar
manner. Provenance indications can be used to in-
dicate whether information is based on a model cre-
ated by an expert or an automatically derived model
obtained by a particular approach.
Figure 2 provides a fragment of a SEM represen-
tation for the earthquake and tsunami of December
26 2004.5 The model is partially inspired by Gao
and Hunter (2011)?s proposal. It combines infor-
mation extracted from texts with information from
DBpedia. The linking between the two can be es-
tablished either manually or automatically through
5The annotation and a larger representation including the
sentence it represents can be found on the GAF website http:
//wordpress.let.vu.nl/gaf.
15
an entity linking system.6 The combined event of
the earthquake and tsunami is represented by a DB-
pedia URI. The node labeled naacl:INSTANCE 186
represents the earthquake itself. The unambiguous
representation of the 2004 earthquake leads us to ad-
ditional information about it, for instance that the
earthquake is an event (sem:Event) and that the
sem:EventType is an earthquake, in this case
represented by a synset from WordNet, but also the
exact date it occurred and the exact location (cf
sem:hasTime, sem:hasPlace).
5.2 Integrating TAF representations into SEM
TAF annotations are converted to SEM relations.
For example, the TAF as participant relations
are translated to sem:hasActor relations, and
temporal relations are translated to sem:hasTime.
We use the relation nwr:denotedBy to link in-
stances to their mentions in the text which are repre-
sented by their unique identifiers in Figure 2.
Named graphs are used to model the source of
information as discussed in Section 4. The re-
lation sem:accordingTo indicates provenance
of information in the graph.7 For instance, the
mentions from the text in named graph gaf:G1
come from the source dbpedia:Bloomberg.
Relations between instances (e.g. between IN-
STANCE 189 and INSTANCE 188) are derived
from a specific grammatical relation in the text
(here, that tsunami is subject of swept) indicated
by the nwr:derivedFrom relation from gaf:G5
to gaf:G4. The grammatical relations included
in graph gaf:G5 come from a TAF annotation
(tag:annotation 2013 03 24).
6 GAF Earthquake Examples
This section takes a closer look at a few selected sen-
tences from the text that illustrate different aspects
of GAF. Figure 2 showed how a URI can provide a
formal context including important background in-
6Entity linking is the task of associating a mention to an
instance in a knowledge base. Several approaches and tools for
entity linking w.r.t. DBpedia and other data sets in the Linked
Open Data cloud are available and achieve good performances,
such as DBpedia Spotlight (Mendes et al, 2011); see (Rizzo
and Troncy, 2011) for a comparison of tools.
7The use of named graphs in this way to denote context is
compatible with the method used by Bozzato et al (2012).
formation on the event. Several texts in the corpus
refer to the tsunami of December 26, 2004, a 9.1
temblor in 2004 caused a tsunami and The catastro-
phe four years ago, among others. Compared to time
expressions such as 2004 and four years ago, time
indications extracted from external sources like DB-
pedia are not only more precise, but also permit us to
correctly establish the fact that these expressions re-
fer to the same event and thus indicate the same time.
The articles were published in January 2009: a direct
normalization of time indications would have placed
the catastrophe in 2005. The flexibility to combine
these seemingly conflicting time indications and de-
lay normalization can be used to correctly interpret
that four years ago early January 2009 refers to an
event taking place at the end of December 2004.
A fragment relating to one of the earthquakes of
January 2009: The quake struck off the coast [...] 75
kilometers (50 miles) west of [....] Manokwari pro-
vides a similar example. The expressions 75 kilo-
meters and 50 miles are clearly meant to express
the same distance, but not identical. The location
is most likely neither exactly 75 km nor 50 miles.
SEM can represent an underspecified location that
is included in the correct region. The exact location
of the earthquake can be found in external resources.
We can include both distances as expressions of the
location and decide whether they denote the general
location or include the normalized locations as alter-
natives to those from external resources.
Different sources may report different details.
Details may only be known later, or sources may
report from a different perspective. As provenance
information can be incorporated into the semantic
layer, we can represent different perspectives, and
choose which one to use when reasoning over the
information. For example, the following phrases
indicate the magnitude of the earthquakes that
struck Manokwari on January 4, 2009:
the 7.7 magnitude quake (source: Xinhuanet)
two quakes, measuring 7.6 and 7.4 (source: Bloomberg)
One 7.3-magnitude tremor (source: Jakartapost)
The first two magnitude indicators (7.7, 7.6)
are likely to pertain to the same earthquake, just as
the second two (7.4, 7.3) are. Trust indicators can
be found through the provenance trace of each men-
16
tion. Trust indicators can include the date on which
it was published, properties of the creation process,
the author, or publisher (Ceolin et al, 2010).
Furthermore, because the URIs are shared across
domains, we can link the information from the text
to information from seismological databases, which
may contain the exact measurement for the quake.
Similarly, external information obtained through
shared links can help us establish coreference. Con-
sider the sentences in Figure 3. There are several
ways to establish that the same event is meant in all
three sentences by using shared URIs and reasoning.
All sentences give us approximate time indications,
location of the affected area and casualties. Rea-
soning over these sentences combined with external
knowledge allows us to infer facts such as that un-
dersea [...] off [...] Aceh will be in the Indian Ocean,
or that the affected countries listed in the first sen-
tence are countries around the Indian Ocean, which
constitutes the Indian Ocean Community. The num-
ber of casualties in combination of the approximate
time indication or approximate location suffices to
identify the earthquake and tsunami in Indonesia on
December 26, 2004. The DBpedia representation
contains additional information such as the magni-
tude, exact location of the quake and a list of affected
countries, which can be used for additional verifica-
tion. This example illustrates how a formal context
using URIs that are shared across disciplines of in-
formation science can help to determine exact refer-
ents from limited or imprecise information.
7 Creating GAF
GAF entails integrating linguistic information
(e.g. TAF annotations) into RDF models (e.g. SEM).
The information in the model includes provenance
that points back to specific annotations. There are
two approaches to annotate text according to GAF.
The first approach is bottom-up. Mentions are
marked in the text as well as relations between them
(participants, time, causal relations, basically any-
thing except coreference). Consequently, these an-
notations are converted to SEM representations as
explained above. Coreference is established by link-
ing mentions to the same instance in SEM. The sec-
ond approach is top-down. Here, annotators mark
relations between instances (events, their partici-
pants, time relations, etc.) directly into SEM and
then link these to mentions in the text.
As mention in Section 2, inter-annotator agree-
ment on event annotation is generally low showing
that it is challenging. The task is somewhat simpli-
fied in GAF, since it removes the problem of identi-
fying an event trigger in the text. The GAF equiva-
lent of the event trigger in other linguistic annotation
approaches is an instance in SEM. However, other
challenges such as which mentions to select are in
principle not addressed by GAF, though differences
in inter-annotator agreement may be found depend-
ing on whether the bottom-up approach or the top-
down approach is selected. The formal context of
SEM may help frame annotations, especially for do-
mains such as earthquakes, where expert knowledge
was used to create basic event models. This may
help annotators while defining the correct relations
between events. On the other hand, the top-down
approach may lead to additional challenges, because
annotators are forced to link events to unambiguous
instances leading to hesitations as to when new in-
stances should be introduced.
Currently, we only use the bottom-up approach.
The main reason is the lack of an appropriate anno-
tation tool to directly annotate information in SEM.
We plan to perform comparative studies between the
two annotation approaches in future work.
8 Conclusion and Future Work
We presented GAF, an event annotation framework
in which textual mentions of events are grounded in
a semantic model that facilitates linking these events
to mentions in external (possibly non-textual) re-
sources and thereby reasoning. We illustrated how
GAF combines TAF and SEM through a use case
on earthquakes. We explained that we aim for a
representation that can combine textual and extra-
linguistic information, provides a clear distinction
between instances and instance mentions, is flexi-
ble enough to include conflicting information and
clearly marks the provenance of information.
GAF ticks all these boxes. All instances are rep-
resented by URIs in a semantic layer following stan-
dard RDF representations that are shared across re-
search disciplines. They are thus represented com-
pletely independent of the source and clearly distin-
17
There have been hundreds of earthquakes in Indonesia since a 9.1 temblor in 2004 caused a
tsunami that swept across the Indian Ocean, devastating coastal communities and leaving more
than 220,000 people dead in Indonesia, Sri Lanka, India, Thailand and other countries.
(Bloomberg, 2009-01-07 01:55 EST)
The catastrophe four years ago devastated Indian Ocean community and killed more than 230,000
people, over 170,000 of them in Aceh at northern tip of Sumatra Island of Indonesia.
(Xinhuanet, 2009-01-05 13:25:46 GMT)
In December 2004, a massive undersea quake off the western Indonesian province of Aceh
triggered a giant tsunami that left at least 230,000 people dead and missing in a dozen
countries facing the Indian Ocean. (Aljazeera, 2009-01-05 08:49 GMT)
Figure 3: Sample sentences mentioning the December 2004 Indonesian earthquake from sample texts
guished from mentions in text or mentions in other
sources. The Terence Annotation Format (TAF) pro-
vides a unified framework to annotate events, par-
ticipants and temporal expressions (and the corre-
sponding relations) by leaning on past, consolidated
annotation experiences such TimeML and ACE. We
will harmonize TAF, the Kyoto Annotation Format
(Bosma et al, 2009, KAF) and the NLP Interchange
Format (Hellmann et al, 2012, NIF) with respect
to the textual representation in the near future. The
NAF format includes the lessons learned from these
predecessors: layered standoff representations using
URI as identifiers and where possible standardized
data categories. The formal semantic model (SEM)
provides the flexibility to include conflicting infor-
mation as well as indications of the provenance of
this information. This allows us to use inferencing
and reasoning over the cumulated and aggregated
information, possibly exploiting the provenance of
the type of information source. This flexibility also
makes our representation compatible with all ap-
proaches dealing with event representation and de-
tections mentioned in Section 2. It can include au-
tomatically learned templates as well as specific re-
lations between events and time expressed in text.
Moreover, it may simultaneously contain output of
different NLP tools.
The proposed semantic layer may be simple, its
flexibility in importing external knowledge may in-
crease complexity in usage as it can model events in
every thinkable domain. To resolve this issue, it is
important to scope the domain by importing the ap-
propriate vocabularies, but no more. When keeping
this in mind, reasoning with SEM is shown to be rich
but still versatile (Van Hage et al, 2012).
While GAF provides us with the desired granu-
larity and flexibility for the event annotation tasks
we envision, a thorough evaluation still needs to be
carried out. This includes an evaluation of the anno-
tations created with GAF compared to other anno-
tation formats, as well as testing it within a greater
application. A comparative study of top-down and
bottom-up annotation will also be carried out. As al-
ready mentioned in Section 7, there is no appropriate
modeling tool for SEM yet. We are currently using
the CAT tool to create TAF annotations and convert
those to SEM, but will develop a tool to annotate the
semantic layer directly for this comparative study.
The most interesting effect of the GAF annota-
tions is that it provides us with relatively simple ac-
cess to a vast wealth of extra-linguistic information,
which we can utilize in a variety of NLP tasks; some
of the reasoning options that are made available by
the pairing up with Semantic Web technology may
for example aid us in identifying coreference rela-
tions between events. Investigating the implications
of this combination of NLP and Semantic Web tech-
nologies lies at the heart of our future work.
Acknowledgements
We thank Francesco Corcoglioniti for his helpful
comments and suggestions. The research lead-
ing to this paper was supported by the European
Union?s 7th Framework Programme via the News-
Reader Project (ICT-316404) and by the Biogra-
phyNed project, funded by the Netherlands eScience
Center (http://esciencecenter.nl/). Partners in Biog-
raphyNed are Huygens/ING Institute of the Dutch
Academy of Sciences and VU University Amster-
dam.
18
References
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
2003. The structure of the FrameNet database. Inter-
national Journal of Lexicography, 16(3):281?296.
Valentina Bartalesi Lenzi, Giovanni Moretti, and Rachele
Sprugnoli. 2012. CAT: the CELCT Annotation Tool.
In Proceedings of LREC 2012.
Cosmin Bejan and Sandra Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguistic
features. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1412?1422.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009a. Linked data - the story so far. International
Journal on Semantic Web and Information Systems,
5(3):1?22.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009b. DBpedia - A crystallization
point for the Web of Data. Web Semantics: Science,
Services and Agents on the World Wide Web, 7(3):154
? 165.
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic se-
mantic annotation format. In Proceedings of the 5th
International Conference on Generative Approaches
to the Lexicon GL 2009, Pisa, Italy.
Loris Bozzato, Francesco Corcoglioniti, Martin Homola,
Mathew Joseph, and Luciano Serafini. 2012. Manag-
ing contextualized knowledge with the ckr (poster). In
Proceedings of the 9th Extended Semantic Web Con-
ference (ESWC 2012), May 27-31.
Jeremy J. Carroll and Graham Klyne. 2004. Re-
source description framework (RDF): Concepts and
abstract syntax. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-concepts-
20040210/.
Davide Ceolin, Paul Groth, and Willem Robert Van Hage.
2010. Calculating the trust of event descriptions using
provenance. Proceedings Of The SWPM.
Nathanael Chambers and Dan Jurafsky. 2011a.
Template-based information extraction without the
templates. In Proceedings of ACL-2011.
Nathanael Chambers and Dan Jurafsky. 2011b.
Template-based information extraction without the
templates. In Proceedings of ACL-2011, Portland, OR.
Nick Crofts, Martin Doerr, Tony Gill, Stephen Stead,
and Matthew Stiff. 2008. Definition of the CIDOC
Conceptual Reference Model. Technical report,
ICOM/CIDOC CRM Special Interest Group. version
4.2.5.
Lianli Gao and Jane Hunter. 2011. Publishing, link-
ing and annotating events via interactive timelines: an
earth sciences case study. In DeRiVE 2011 (Detec-
tion, Representation, and Exploitation of Events in the
Semantic Web) Workshop in conjunction with ISWC
2011, Bonn, Germany.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics (COLING?96), pages 466?471.
Ramanathan V. Guha and Dan Brickley. 2004.
RDF vocabulary description language 1.0: RDF
schema. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-schema-
20040210/.
Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Jurafsky.
2010. Using query patterns to learn the duration of
events. In Proceedings of ISWC 2010.
Sebastian Hellmann, Jens Lehmann, and So?ren Auer.
2012. NIF: An ontology-based and linked-data-aware
NLP Interchange Format. Working Draft.
Jerry R Hobbs and Feng Pan. 2004. An ontology of time
for the semantic web. ACM Transactions on Asian
Language Information Processing (TALIP), 3(1):66?
85.
Linguistic Data Consortium. 2004a. Annotation
Guidelines for Event Detection and Characterization
(EDC). http://projects.ldc.upenn.edu/
ace/docs/EnglishEDCV2.0.pdf.
Linguistic Data Consortium. 2004b. The ACE 2004
Evaluation Plan. Technical report, LDC.
Linguistic Data Consortium. 2005. ACE (Automatic
Content Extraction) English annotation guidelines for
entities. Version 6.6, July.
Pablo N. Mendes, Max Jakob, Andre?s Garc??a-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In Proceedings of the
7th International Conference on Semantic Systems, I-
Semantics ?11, pages 1?8.
Marie-Francine Moens, Oleksandr Kolomiyets,
Emanuele Pianta, Sara Tonelli, and Steven Bethard.
2011. D3.1: State-of-the-art and design of novel
annotation languages and technologies: Updated
version. Technical report, TERENCE project ? ICT
FP7 Programme ? ICT-2010-25410.
Luc Moreau, Paolo Missier, Khalid Belhajjame, Reza
B?Far, James Cheney, Sam Coppens, Stephen Cress-
well, Yolanda Gil, Paul Groth, Graham Klyne, Timo-
thy Lebo, Jim McCusker, Simon Miles, James Myers,
Satya Sahoo, and Curt Tilmes. 2012. PROV-DM: The
PROV Data Model. Technical report.
Boris Motik, Bijan Parsia, and Peter F. Patel-
Schneider. 2009. OWL 2 Web Ontology
19
Language structural specification and functional-
style syntax. W3C recommendation, W3C,
October. http://www.w3.org/TR/2009/
REC-owl2-syntax-20091027/.
Joel Nothman, Matthew Honnibal, Ben Hachey, and
James R. Curran. 2012. Event linking: Ground-
ing event reference in a news archive. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers),
pages 228?232, Jeju Island, Korea, July. Association
for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106, 2013/03/12.
James Pustejovsky, Jessica Littman, and Roser Saur?`.
2006a. Argument Structure in TimeML. In Dagstuhl
Seminar Proceedings. Internationales Begegnungs-
und Forschungszentrum.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006b. Timebank 1.2 documentation.
Technical report, Brandeis University, April.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An international
standard for semantic annotation. In Proceedings o
the Fifth International Workshop on Interoperable Se-
mantic Annotation.
Giuseppe Rizzo and Raphae?l Troncy. 2011. NERD:
A framework for evaluating named entity recognition
tools in the Web of data. In Workshop on Web Scale
Knowledge Extraction, colocated with ISWC 2011.
Ansgar Scherp, Thomas Franz, Carsten Saathoff, and
Steffen Staab. 2009. F?a model of events based on
the foundational ontology dolce+ dns ultralight. In
Proceedings of the fifth international conference on
Knowledge capture, pages 137?144. ACM.
Andrea Setzer and Robert J. Gaizauskas. 2000. Annotat-
ing events and temporal information in newswire texts.
In LREC. European Language Resources Association.
Ryan Shaw, Raphae?l Troncy, and Lynda Hardman. 2009.
LODE: Linking Open Descriptions of Events. In 4th
Annual Asian Semantic Web Conference (ASWC?09),
Shanghai, China.
Willem Robert Van Hage, Ve?ronique Malaise?, Roxane
Segers, Laura Hollink, and Guus Schreiber. 2011. De-
sign and use of the simple event model (SEM). Jour-
nal of Web Semantics.
Willem Robert Van Hage, Marieke Van Erp, and
Ve?ronique Malaise?. 2012. Linked open piracy: A
story about e-science, linked data, and statistics. Jour-
nal on Data Semantics, 1(3):187?201.
20
