A Semantic Kernel for Predicate Argument Classification
Alessandro Moschitti and Cosmin Adrian Bejan
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
alessandro.moschitti@utdallas.edu
ady@hlt.utdallas.edu
Abstract
Automatically deriving semantic structures
from text is a challenging task for machine
learning. The flat feature representations, usu-
ally used in learning models, can only partially
describe structured data. This makes difficult
the processing of the semantic information that
is embedded into parse-trees.
In this paper a new kernel for automatic clas-
sification of predicate arguments has been de-
signed and experimented. It is based on sub-
parse-trees annotated with predicate argument
information from PropBank corpus. This ker-
nel, exploiting the convolution properties of
the parse-tree kernel, enables us to learn which
syntactic structures can be associated with the
arguments defined in PropBank. Support Vec-
tor Machines (SVMs) using such a kernel clas-
sify arguments with a better accuracy than
SVMs based on linear kernel.
1 Introduction
Several linguistic theories, e.g. (Jackendoff, 1990), claim
that semantic information in natural language texts is
connected to syntactic structures. Hence, to deal with nat-
ural language semantics, the learning algorithm should be
able to represent and process structured data. The classi-
cal solution adopted for such tasks is to convert syntax
structures in a flat feature representation, which is suit-
able for a given learning model. The main drawback is
structures may not be properly represented by flat fea-
tures as: (1) these latter may not be able to capture the
required properties or (2) the feature designer may not
know what structure properties enable the processing of
semantic information.
In particular, these problems arise for semantic infor-
mation represented via predicate argument structures de-
fined on syntactic parse trees. For example, Figure 1
shows the parse tree of the sentence: "Paul gives
a lecture in Rome" along with the annotation of
predicate arguments.
A predicate may be a verb or a noun or an adjective
whereas generally Arg 0 stands for agent, Arg 1 for di-
rect object or theme or patient and ArgM may indicate
locations, as in our example. A standard for predicate ar-
gument annotation is provided in the PropBank project
(Kingsbury and Palmer, 2002). It has produced one
million word corpus annotated with predicate-argument
structures on top of the Penn Treebank 2 Wall Street Jour-
nal texts. In this way, for a large number of the Penn
TreeBank parse-trees, there are available predicate anno-
tations in a style similar to that shown in Figure 1.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Predicate 
Arg. 0 
Arg. M 
S
N
NP 
D N 
VP 
V Paul 
in 
gives 
a lecture 
PP 
IN N 
Rome 
Arg. 1 
Figure 1: Predicate arguments in a parse-tree representation.
In PropBank only verbs are considered to be predicates
whereas arguments are labeled sequentially from Arg 0
to Arg 91. In addition to these core arguments, adjunctive
arguments are marked up. They include functional tags,
e.g. ArgM-DIR indicates a directional, ArgM-LOC in-
dicates a locative and ArgM-TMP stands for a temporal.
An example of PropBank markup is:
1Other arguments are: Arg 2 for indirect object or benefac-
tive or instrument or attribute or end state, Arg 3 for start point
or benefactive or attribute, Arg4 for end point and so on.
[Arg10 Analysts ] have been [predicate1 expecting ] [Arg11
a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 the
U.S. car maker ] [Arg21 an eventual 30% state in the British
Company ].
Automatically recognizing the boundaries and classi-
fying the type of arguments allows Natural Language
Processing systems (e.g. Information Extraction, Ques-
tion Answering or Summarization) to answer questions
such as ?Who?, ?When?, ?What?, ?Where?, ?Why?, and
so on.
Given the importance of this task for Natural Lan-
guage Processing applications, several machine learning
approaches for argument identification and classification
have been developed (Gildea and Jurasky, 2002; Sur-
deanu et al, 2003; Hacioglu et al, 2003; Chen and Ram-
bow, 2003; Gildea and Hockenmaier, 2003). Their com-
mon characteristic is the adoption of feature spaces that
model predicate-argument structures in a flat representa-
tion. The major problem of this choice is that there is
no linguistic theory that supports the selection of syntac-
tic features to recognize semantic structures. As a con-
sequence, researchers are still trying to extend the basic
features with other ones, e.g. (Surdeanu et al, 2003), to
improve the flat feature space.
Convolution kernels are a viable alternative to flat fea-
ture representation that aims to capture the structural in-
formation in term of sub-structures. The kernel functions
can be used to measure similarities between two objects
without explicitly evaluating the object features. That
is, we do not need to understand which syntactic feature
may be suited for representing semantic data. We need
only to define the similarity function between two seman-
tic structures. An example of convolution kernel on the
parse-tree space is given in (Collins and Duffy, 2002).
The aim was to design a novel syntactic parser by look-
ing at the similarity between the testing parse-trees and
the correct parse-trees available for training.
In this paper, we define a kernel in a semantic struc-
ture space to learn the classification function of predicate
arguments. The main idea is to select portions of syn-
tactic/semantic trees that include the target <predicate,
argument> pair and to define a kernel function between
these objects. If our similarity function is well defined the
learning model will converge and provide an effective ar-
gument classification.
Experiments on PropBank data show not only that
Support Vector Machines (SVMs) trained with the pro-
posed semantic kernel converge but also that they have a
higher accuracy than SVMs trained with a linear kernel
on the standard features proposed in (Gildea and Jurasky,
2002). This provides a piece of evidence that convolution
kernel can be used to learn semantic linguistic structures.
Moreover, interesting research lines on the use of ker-
nel for NLP are enabled, e.g. question classification in
Question/Answering or automatic template designing in
Information Extraction.
The remaining of this paper is organized as follows:
Section 2 defines the Predicate Argument Extraction
problem and the standard solution to solve it. In Section
3 we present our approach based on the parse-tree kernel
whereas in Section 4 we show our comparative results
between SVMs using standard features and the proposed
kernel. Finally, Section 5 summarizes the conclusions.
2 Automatic Predicate-Argument
extraction
Given a sentence in natural language, all the predicates
associated with its verbs have to be identified along with
their arguments. This problem can be divided in two sub-
tasks: (a) detection of the target argument boundaries,
i.e. all its compounding words, and (b) classification of
the argument type, e.g. Arg0 or ArgM.
A direct approach to learn both detection and classifi-
cation of predicate arguments is summarized by the fol-
lowing steps:
1. Given a sentence from the training-set, generate a
full syntactic parse-tree;
2. let P and A be the set of predicates and the set of
parse-tree nodes (i.e. the potential arguments), re-
spectively;
3. for each pair <p, a> ? P ?A:
? extract the feature representation set, Fp,a;
? if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
For example, in Figure 1, for each combination of the
predicate give with the nodes N, S, VP, V, NP, PP, D or
IN the instances F?give?,a are generated. In case the node
a exactly covers Paul, a lecture or in Rome, it will be a
positive instance otherwise it will be a negative one, e.g.
F?give?,?IN?.
The above T+ and T? sets can be re-organized as posi-
tive T+argi and negative T?argi examples for each argument
i. In this way, an individual ONE-vs-ALL classifier for
each argument i can be trained. We adopted this solution
as it is simple and effective (Pradhan et al, 2003). In the
classification phase, given a sentence of the test-set, all
its Fp,a are generated and classified by each individual
classifier. As a final decision, we select the argument as-
sociated with the maximum value among the scores pro-
vided by the SVMs2, i.e. argmaxi?S Ci, where S is
the target set of arguments.
2This is a basic method to pass from binary categorization
2.1 Standard feature space
The discovering of relevant features is, as usual, a com-
plex task, nevertheless there is a common consensus on
the basic features that should be adopted. These stan-
dard features, firstly proposed in (Gildea and Jurasky,
2002), refer to a flat information derived from parse trees,
i.e. Phrase Type, Predicate Word, Head Word, Governing
Category, Position and Voice. Table 1 presents the stan-
dard features and exemplifies how they are extracted from
a given parse tree.
- Phrase Type: This feature indicates the syntactic type
of the phrase labeled as a predicate argument, e.g. NP
for Arg1 in Figure 1.
- Parse Tree Path: This feature contains the path in
the parse tree between the predicate and the argument
phrase, expressed as a sequence of nonterminal labels
linked by direction (up or down) symbols, e.g. V ? VP
? NP for Arg1 in Figure 1.
- Position: Indicates if the constituent, i.e. the potential
argument, appears before or after the predicate in the
sentence, e.g. after for Arg1 and before for Arg0 (see
Figure 1).
- Voice: This feature distinguishes between active or
passive voice for the predicate phrase, e.g. active for
every argument (see Figure 1).
- Head Word: This feature contains the head word of the
evaluated phrase. Case and morphological information
are preserved, e.g. lecture for Arg1 (see Figure 1).
- Governing Category: This feature applies to noun
phrases only, and it indicates if the NP is dominated by
a sentence phrase (typical for subject arguments with
active voice predicates), or by a verb phrase (typical for
object arguments), e.g. the NP associated with Arg1 is
dominated by a verbal phrase VP (see Figure 1).
- Predicate Word: In our implementation this feature
consists of two components: (1) the word itself with
the case and morphological information preserved, e.g.
gives for all arguments; and (2) the lemma which rep-
resents the verb normalized to lower case and infinitive
form, e.g. give for all arguments (see Figure 1).
Table 1: Standard features extracted from parse-trees.
For example, the Parse Tree Path feature represents the
path in the parse-tree between a predicate node and one of
its argument nodes. It is expressed as a sequence of non-
terminal labels linked by direction symbols (up or down),
e.g. in Figure 1, V?VP?NP is the path between the pred-
icate to give and the argument 1, a lecture. If two pairs
<p1, a1> and <p2, a2> have a Path that differs even for
one character (e.g. a node in the parse-tree) the match
will not be carried out, preventing the learning algorithm
to generalize well on unseen data. In order to address also
into a multi-class categorization problem; several optimization
have been proposed, e.g. (Goh et al, 2001).
this problem, next section describes a novel kernel space
for predicate argument classification.
3 A semantic kernel for argument
classification
We consider the predicate argument structures annotated
in PropBank as our semantic space. Many semantic struc-
tures may constitute the objects of our space. Some possi-
bilities are: (a) the selection of the whole sentence parse-
tree, in which the target predicate is contained or (b) the
selection of the sub-tree that encloses the whole predi-
cate annotation (i.e. all its arguments). However, both
choices would cause an exponential explosion on the po-
tential sub-parse-trees that have to be classified during
the testing phase. In fact, during this phase we do not
know which are the arguments associated with a predi-
cate. Thus, we need to build all the possible structures,
which contain groups of potential arguments for the tar-
get predicate. More in detail, assuming that S is the set of
PropBank argument types, and m is the maximum num-
ber of entries that the target predicate can have, we have
to evaluate
(|S|
m
)
argument combinations for each target
predicate.
In order to define an efficient semantic space we se-
lect as objects only the minimal sub-structures that in-
clude one predicate with only one of its arguments. For
example, Figure 2 illustrates the parse-tree of the sen-
tence "Paul delivers a lecture in formal
style". The circled substructures in (a), (b) and (c) are
our semantic objects associated with the three arguments
of the verb to deliver, i.e. <deliver, Arg0>, <deliver,
Arg1> and <deliver, ArgM>. In this formulation, only
one of the above structures is associated with each pred-
icate/argument pair, i.e. Fp,a contain only one of the cir-
cled sub-trees.
We note that our approach has the following properties:
? The overall semantic feature space F contain sub-
structures composed of syntactic information em-
bodied by parse-tree dependencies and semantic in-
formation under the form of predicate/argument an-
notation.
? This solution is efficient as we have to classify at
maximum |A| nodes for each predicate, i.e. the set
of the parse-tree nodes of a testing sentence.
? A constituent cannot be part of two different argu-
ments of the target predicate, i.e. there is no over-
lapping between the words of two arguments. Thus,
two semantic structures Fp1,a1 and Fp2,a23, asso-
3Fp,a was defined as the set of features of our objects
<p, a>. Since in our kernel we have only one element in Fp,a
with an abuse of notation we use it to indicate the objects them-
selves.
 S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
Fdeliver, Arg0 
 formal 
 N 
      style 
Arg. 0 
a) S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
 formal 
 N 
      style 
Fdeliver, Arg1 
b) S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
 formal 
 N 
      style 
Arg. 1 
Fdeliver, ArgM 
 
c) 
Arg. M 
Figure 2: Semantic feature space for predicate argument classification.
ciated with two different arguments, cannot be in-
cluded one in the other. This property is important
because, a convolution kernel would not be effective
to distinguish between an object and its sub-parts.
Once defined our semantic space we need to design a
kernel function to measure a similarity between two ob-
jects. These latter may still be seen as described by com-
plex features but such a similarity is carried out avoiding
the explicit feature computation. For this purpose we de-
fine a mapping ? : F ? F ? such as:
~x = (x1, ..., x|F |) ? ?(~x) = (?1(~x), .., ?|F ?|(~x)),
where F ? allows us to design an efficient semantic kernel
K(~x, ~z) =<?(~x) ? ?(~z)>.
3.1 The Semantic Kernel (SK)
Given the semantic objects defined in the previous sec-
tion, we design a convolution kernel in a way similar
to the parse-tree kernel proposed in (Collins and Duffy,
2002). Our feature set F ? is the set of all possible sub-
structures (enumerated from 1 to |F ?|) of the semantic
objects extracted from PropBank. For example, Figure
3 illustrates all valid fragments of the semantic structure
Fdeliver,Arg1 (see also Figure 2). It is worth noting that
the allowed sub-trees contain the entire (not partial) pro-
duction rules. For instance, the sub-tree [NP [D a]] is
excluded from the set of the Figure 3 since only a part
of the production NP ? D N is used in its generation.
However, this constraint does not apply to the production
VP? V NP PP along with the fragment [VP [V NP]] as
the subtree [VP [PP [...]]] is not considered part of the
semantic structure.
Even if the cardinality of F ? will be very large the eval-
uation of the kernel function is polynomial in the number
of parse-tree nodes.
More precisely, a semantic structure ~x is mapped in
?(~x) = (h1(~x), h2(~x), ...), where the feature function
hi(~x) simply counts the number of times that the i-
th sub-structure of the training data appears in ~x. Let
 
 
 
 
 
 
 
 
 
 
NP 
D N 
a 
  talk 
NP 
D N 
NP 
D N 
a 
D N 
a   talk 
NP 
D N NP 
D N 
VP 
V 
delivers 
a 
   talk 
V 
delivers 
NP 
D N 
VP 
V 
a 
   talk 
NP 
D N 
VP 
V 
NP 
D N 
VP 
V 
a 
NP 
D
VP 
V 
   talk 
N 
a 
NP 
D N 
VP 
V 
delivers 
   talk 
NP 
D N 
VP 
V 
delivers NP D N 
VP 
V 
delivers 
NP 
VP 
V NP 
VP 
V 
delivers 
  talk 
Figure 3: All 17 valid fragments of the semantic structure as-
sociated with Arg 1 (see Figure 2).
Ii(n) be the indicator function: 1 if the sub-structure
i is rooted at node n and 0 otherwise. It follows that
h(~x) = ?n?N Ii(n), where N is the set of the ~x?s nodes.
Therefore, the kernel4 function is:
K(~x, ~z) = ~h(~x) ? ~h(~z) =
=
?
i
( ?
nx?Nx
Ii(nx)
)( ?
nz?Nz
Ii(nz)
) =
=
?
nx?Nx
?
nz?Nz
?
i
Ii(nx)Ii(nz) (1)
where Nx and Nz are the nodes in x and z, respec-
tively. In (Collins and Duffy, 2002), it has been shown
that Eq. 1 can be computed in O(|Nx| ? |Nz|) by eval-
uating ?(nx, nz) =
?
i Ii(nx)Ii(nz) with the following
recursive equations:
? if the production at nx and nz are different then
?(nx, nz) = 0;
4Additionally, we carried out the normalization in the kernel
space, thus the final kernel is K?(~x, ~z) = K(~x,~z)?
K(~x,~x)?K(~z,~z)
.
? if the production at nx and nz are the same, and nx
and nz are pre-terminals then
?(nx, nz) = 1; (2)
? if the production at nx and nz are the same, and nx
and nz are not pre-terminals then
?(nx, nz) =
nc(nx)?
j=1
(1 + ?(ch(nx, j), ch(nz, j))),
(3)
where nc(nx) is the number of children of nx and
ch(n, i) is the i-th child of the node n. Note that as
the productions are the same ch(nx, i) = ch(nz, i).
This kind of kernel has the drawback of assigning more
weight to larger structures while the argument type does
not depend at all on the size of its structure. In fact two
sentences such as:
(1) [Arg0 Paul ][predicate delivers ] [Arg1 a lecture] and(2) [Arg0 Paul ][predicate delivers ][Arg1 a plan on the de-
tection of theorist groups active in the North Iraq]
have the same argument type with a very different size.
To overcome this problem we can scale the relative im-
portance of the tree fragments with their size. For this
purpose a parameter ? is introduced in equations 2 and 3
obtaining:
?(nx, nz) = ? (4)
?(nx, nz) = ?
nc(nx)?
j=1
(1+?(ch(nx, j), ch(nz, j))) (5)
It is worth noting that even if the above equations
define a kernel function similar to the one proposed in
(Collins and Duffy, 2002), the substructures on which SK
operates are different from the parse-tree kernel. For ex-
ample, Figure 3 shows that structures such as [VP [V]
[NP]], [VP [V delivers ] [NP]] and [VP [V] [NP [DT
N]]] are valid features, but these fragments (and many
others) are not generated by a complete production, i.e.
VP? V NP PP. As a consequence they are not included
in the parse-tree kernel representation of the sentence.
3.2 Comparison with Standard Features
We have synthesized the comparison between stan-
dard features and the SK representation in the follow-
ing points. First, SK estimates a similarity between
two semantic structures by counting the number of
sub-structures that are in common. As an example,
the similarity between the two structures in Figure 2,
F?delivers?,Arg0 and F?delivers?,Arg1, is equal to 1 since
they have in common only the [V delivers] substruc-
ture. Such low value depends on the fact that different
argument types tend to appear in different structures.
On the contrary, if two structures differ only for a few
nodes (especially terminal or near terminal nodes) the
similarity remains quite high. For example, if we change
the tense of the verb to deliver (Figure 2) in delivered,
the [VP [V delivers] NP] subtree will be transformed
in [VP [VBD delivered] NP], where the NP is un-
changed. Thus, the similarity with the previous structure
will be quite high as: (1) the NP with all sub-parts will
be matched and (2) the small difference will not highly
affect the kernel norm and consequently the final score.
This conservative property does not apply to the Parse
Tree Path feature which is very sensible to small changes
in the tree-structure, e.g. two predicates, expressed in dif-
ferent tenses, generate two different Path features.
Second, some information contained in the standard
features is embedded in SK: Phrase Type, Predicate Word
and Head Word explicitly appear as structure fragments.
For example, in Figure 3 are shown fragments like [NP
[DT] [N]] or [NP [DT a] [N talk]] which explicitly en-
code the Phrase Type feature NP for Arg 1 in Figure 2.b.
The Predicate Word is represented by the fragment [V
delivers] and the Head Word is present as [N talk].
Finally, Governing Category, Position and Voice can-
not be expressed by SK. This suggests that a combination
of the flat features (especially the named entity class (Sur-
deanu et al, 2003)) with SK could furthermore improve
the predicate argument representation.
4 The Experiments
For the experiments, we used PropBank
(www.cis.upenn.edu/?ace) along with Penn-
TreeBank5 2 (www.cis.upenn.edu/?treebank)
(Marcus et al, 1993). This corpus contains about 53,700
sentences and a fixed split between training and testing
which has been used in other researches (Gildea and
Jurasky, 2002; Surdeanu et al, 2003; Hacioglu et al,
2003; Chen and Rambow, 2003; Gildea and Hocken-
maier, 2003; Gildea and Palmer, 2002; Pradhan et al,
2003). In this split, Sections from 02 to 21 are used for
training, section 23 for testing and sections 1 and 22 as
developing set. We considered all PropBank arguments
from Arg0 to Arg9, ArgA and ArgM even if only Arg0
from Arg4 and ArgM contain enough training/testing
data to affect the global performance. In Table 2 some
characteristics of the corpus used in our experiments are
reported.
The classifier evaluations were carried out using
the SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/ with the de-
fault linear kernel for the standard feature evaluations.
5We point out that we removed from the Penn TreeBank the
special tags of noun phrases like Subj and TMP as parsers usu-
ally are not able to provide this information.
Table 2: Characteristics of the corpus used in the experiments.
Number of Args Number of unique
train. test-set Std. features
Arg0 34,955 2,030 12,520
Arg1 44,369 2,714 14,442
Arg2 10,491 579 6,422
Arg3 2,028 106 1,591
Arg4 1,611 67 918
ArgM 30,464 1,930 7,647
Total 123,918 7,426 21,710
For processing our semantic structures, we implemented
our own kernel and we used it inside SVM-light.
The classification performances were evaluated using
the f1 measure for single arguments as each of them has
a different Precision and Recall and by using the accu-
racy for the final multi-class classifier as the global Pre-
cision = Recall = accuracy. The latter measure allows
us to compare the results with previous literature works,
e.g. (Gildea and Palmer, 2002; Surdeanu et al, 2003; Ha-
cioglu et al, 2003; Chen and Rambow, 2003; Gildea and
Hockenmaier, 2003).
To evaluate the effectiveness of our new kernel we di-
vided the experiments in 3 steps:
? The evaluation of SVMs trained with standard fea-
tures in a linear kernel, for comparison purposes.
? The estimation of the ? parameter (equations 4 and
5) for SK from the validation-set .
? The performance measurement of SVMs, using SK
along with ? computed in the previous step.
Additionally, both Linear and SK kernels were evalu-
ated using different percentages of training data to com-
pare the gradients of their learning curves.
4.1 SVM performance on Linear and Semantic
Kernel
The evaluation of SVMs using a linear kernel on the stan-
dard features did not raise particular problems. We used
the default regularization parameter (i.e., C = 1 for nor-
malized kernels) and we tried a few cost-factor values
(i.e., j ? {0.1, 1, 2, 3, 4, 5}) to adjust the rate between
precision and recall. Given the huge amount of training
data, we used only 30% of training-set in these valida-
tion experiments. Once the parameters were derived, we
learned 6 different classifiers (one for each role) and mea-
sured their performances on the test-set.
For SVM, using the Semantic Kernel, we derived that
a good ? parameter for the validation-set is 0.4. In Figure
4 we report the curves, f1 function of ?, for the 3 largest
(in term of training examples) arguments on the test-set.
0.82
0.85
0.88
0.91
0.94
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9?
f1
Arg0Arg1ArgM
Figure 4: SVM f1 for Arg0, Arg1 and ArgM with respect to
different ? values.
We note that the maximal value from the validation-set is
also the maximal value from the test-set for every argu-
ment. This suggests that: (a) it is easy to detect an optimal
parameter and (b) there is a common (to all arguments) ?-
value which defines how much the size of two structures
impacts on their similarity. Moreover, some experiments
using ? greater than 1 have shown a remarkable decrease
in performance, i.e. a correct ? seems to be essential to
obtain a good generalization6 of the training-set.
Table 3: f1 of SVMs using linear and semantic kernel com-
pared to literature models for argument classification.
Args SVM SVM Prob. C4.5
STD SK STD STD EXT
Arg0 87.79 88.35 - - -
Arg1 82.43 86.25 - - -
Arg2 54.10 68.52 - - -
Arg3 31.65 49.46 - - -
Arg4 62.81 66.66 - - -
ArgM 91.97 94.07 - - -
multi-class
accuracy 84.07 86.78 82.8 78.76 83.74
Table 3 reports the performances of SVM trained with
the standard features (STD column) and with the Seman-
tic Kernel (SK column). In columns Prob. and C4.5 are
reported the results for argument classification achieved
in (Gildea and Palmer, 2002) and (Surdeanu et al, 2003).
This latter used C4.5 model on standard feature set (STD
sub-column) and on an extended feature set (EXT sub-
column). We note that: (a) SVM performs better than
the probabilistic approach and C4.5 learning model inde-
pendently of the adopted features and (b) the Semantic
Kernel considerably improves the standard feature set.
In order to investigate if SK generalizes better than the
6For example, ? = 1 would generate low kernel values be-
tween small and large structures. This is in contrast with the
observation in Section 3.1, i.e. argument type is independent of
its constituent size.
linear kernel, we measured the performances by select-
ing different percentages of training data. Figure 5 shows
the curves for the three roles Arg0, Arg1 and ArgM, re-
spectively for linear and semantic kernel whereas Figure
6 shows the multi-class classifier f1 plots.
0.7
0.73
0.76
0.79
0.82
0.85
0.88
0.91
0.94
0 15 30 45 60 75 90% Training Data
f1
Arg0-SK Arg1-SK ArgM-SK
Arg0-STD Arg1-STD ArgM-STD
Figure 5: Arg0, Arg1 and ArgM evaluations over SK and the
linear kernel of standard features with respect to different per-
centages of training data.
0.7
0.73
0.76
0.79
0.82
0.85
0.88
0 10 20 30 40 50 60 70 80 90 100
% Trai ing Data
Acc
ura
cy
SK
STD
Figure 6: Accuracy of the multi-class classifier using standard
features and SK with respect to different percentages of training
data.
We note that not only SK produces higher accuracy
but also the gradient of the learning curves is higher: for
example, Figure 6 shows that with only 20% of training
data, SVM using SK approaches the accuracy of SVM
trained with all data on standard features.
Additionally, we carried out some preliminary exper-
iments for argument identification (boundary detection),
but the learning algorithm was not able to converge. In
fact, for this task the non-inclusion property (discussed
in Section 3) does not hold. A constituent ai, which has
incorrect boundaries, can include or be included in the
correct argument ac. Thus, the similarity K(ai, ac) be-
tween ai and ac is quite high preventing the algorithm to
learn the structures of correct arguments.
4.2 Discussion and Related Work
The material of the previous sections requires a discus-
sion of the following points: firstly, in Section 3.2 we
have noted that some standard features are explicitly
coded in SK but Governing Category, Position and Voice
features are not expressible as a single fragment of a se-
mantic structure. For example, to derive the Position of
an argument relatively to the target predicate is required a
visit of the tree. No parse-tree information, i.e. node tags
or edges, explicitly indicates this feature. A similar ratio-
nale applies to Governing Category and Voice, even if for
the latter some tree fragments may code the to be feature.
Since these three features have been proved important for
role classification we argue that either (a) SK implicitly
produces this kind of information or (b) SK is able to pro-
vide a different but equally effective information which
allows it to perform better than the standard features. In
this latter case, it would be interesting to study which
features can be backported from SK to the linear kernel
to obtain a fast and improved system (Cumby and Roth,
2003). As an example, the fragment [VP [V NP]] defines
a sort of sub-categorization frame that may be used to
cluster together syntactically similar verbs.
Secondly, it is worth noting that we compared SK
against a linear kernel of standard features. A recent
study, (Pradhan et al, 2003), has suggested that a poly-
nomial kernel with degree = 2 performs better than the
linear one. Using such a kernel, the authors obtained
88% in classification but we should take into account
that they also used a larger set of flat features, i.e. sub-
categorization information (e.g. VP? V NP PP for the
tree in Figure 1), Named Entity Classes and a Partial Path
feature.
Thirdly, this is one of the first massive use of convo-
lution kernels for Natural Language Processing tasks, we
trained SK and tested it on 123,918 and 7,426 arguments,
respectively. For training each large argument (in term
of instances) were required more than 1.5 billion of ker-
nel iterations. This was a little time consuming (about
a couple of days for each argument on a Intel Pentium
4, 1,70 GHz, 512 Mbytes Ram) as the SK computation
complexity is quadratic in the number of semantic struc-
ture nodes7. This prevented us to carry out cross/fold val-
idation. An important aspect is that a recent paper (Vish-
wanathan and Smola, 2002) assesses that the tree-kernel
complexity can be reduced to linear one; this would make
our approach largely applicable.
Finally, there is a considerable work in Natural Lan-
guage Processing oriented kernel (Collins and Duffy,
2002; Lodhi et al, 2000; Ga?rtner, 2003; Cumby and
Roth, 2003; Zelenko et al, 2003) about string, parse-
7More precisely, it is O(|Fp,a|2) where Fp,a is the largest
semantic structure of the training data.
tree, graph, and relational kernels but, to our knowledge,
none of them was used to derive semantic information
on the form of predicate argument structures. In particu-
lar, (Cristianini et al, 2001; Kandola et al, 2003) address
the problem of semantic similarity between two terms by
using, respectively, document sets as term context and
the latent semantic indexing. Both techniques attempt
to cluster together terms that express the same meaning.
This is quite different in means and purpose of our ap-
proach that derives more specific semantic information
expressed as argument/predicate relations.
5 Conclusions
In this paper, we have experimented an original kernel
based on semantic structures from PropBank corpus. The
results have shown that:
? the Semantic Kernel (SK) can be adopted to classify
predicate arguments defined in PropBank;
? SVMs using SK performs better than SVMs trained
with the linear kernel of standard features; and
? the higher gradient in the accuracy/training percent-
age plots shows that SK generalizes better than the
linear kernel.
Finally, SK suggests that some features, contained in
the fragments of semantic structures, should be back-
ported in a flat feature space. Conversely, the good per-
formance of the linear kernel suggests that standard fea-
tures, e.g. Head Word, Predicate Word should be empha-
sized in the definition of a convolution kernel for argu-
ment classification. Moreover, other selections of predi-
cate/argument substructures (able to capture different lin-
guistic relations) as well as kernel combinations (e.g. flat
features with SK) could furthermore improve semantic
shallow parsing.
6 Acknowledgements
This research has been sponsored by the ARDA
AQUAINT program. In addition, we would like to thank
prof. Sanda Harabagiu to support us with interesting ad-
vices. Many thanks to the anonymous reviewers for their
professional and committed suggestions.
References
John Chen and Owen Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic argu-
ments. In Proceedings EMNLP03.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of ACL02.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2001.
Latent semantic kernels. In Proceedings of ICML01, pages
66?73, Williams College, US. Morgan Kaufmann Publish-
ers, San Francisco, US.
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. In Proceedings of ICML03.
Thomas Ga?rtner. 2003. A survey of kernels for structured data.
SIGKDD Explor. Newsl., 5(1):49?58.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar. In Pro-
ceedings of EMNLP03.
Daniel Gildea and Daniel Jurasky. 2002. Automatic labeling of
semantic roles. Computational Linguistic, 28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The necessity of pars-
ing for predicate argument recognition. In Proceedings of
ACL02, Philadelphia, PA.
King-Shy Goh, Edward Chang, and Kwang-Ting Cheng. 2001.
SVM binary classifier ensembles for image classification.
Proceedings of CIKM01, pages 395?402.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Martin, and
Dan Jurafsky. 2003. Shallow semantic parsing using Sup-
port Vector Machines. Technical report.
R. Jackendoff. 1990. Semantic Structures, Current Studies
in Linguistics series. Cambridge, Massachusetts: The MIT
Press.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Schlkopf, C. Burges, and MIT-Press.
A. Smola (ed.), editors, Advances in Kernel Methods - Sup-
port Vector Learning.
J. Kandola, N. Cristianini, and J. Shawe-Taylor. 2003. Learn-
ing semantic similarity. In Advances in Neural Information
Processing Systems, volume 15.
Paul Kingsbury and Martha Palmer. 2002. From TreeBank to
PropBank. In Proceedings of LREC02, Las Palmas, Spain.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cris-
tianini, and Christopher Watkins. 2000. Text classification
using string kernels. In NIPS, pages 563?569.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn Tree-
Bank. Computational Linguistics, 19:313?330.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Mar-
tin, and Daniel Jurafsky. 2003. Semantic role parsing:
Adding semantic structure to unstructured text. In Proceed-
ings of ICDM03.
Mihai Surdeanu, Sanda M. Harabagiu, John Williams, and John
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In Proceedings of ACL03, Sapporo,
Japan.
S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on
strings and trees. In Proceedings of Neural Information Pro-
cessing Systems.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journal of Machine Learning Re-
search.
A Study on Convolution Kernels for Shallow Semantic Parsing
Alessandro Moschitti
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
alessandro.moschitti@utdallas.edu
Abstract
In this paper we have designed and experi-
mented novel convolution kernels for automatic
classification of predicate arguments. Their
main property is the ability to process struc-
tured representations. Support Vector Ma-
chines (SVMs), using a combination of such ker-
nels and the flat feature kernel, classify Prop-
Bank predicate arguments with accuracy higher
than the current argument classification state-
of-the-art.
Additionally, experiments on FrameNet data
have shown that SVMs are appealing for the
classification of semantic roles even if the pro-
posed kernels do not produce any improvement.
1 Introduction
Several linguistic theories, e.g. (Jackendoff,
1990) claim that semantic information in nat-
ural language texts is connected to syntactic
structures. Hence, to deal with natural lan-
guage semantics, the learning algorithm should
be able to represent and process structured
data. The classical solution adopted for such
tasks is to convert syntax structures into flat
feature representations which are suitable for a
given learning model. The main drawback is
that structures may not be properly represented
by flat features.
In particular, these problems affect the pro-
cessing of predicate argument structures an-
notated in PropBank (Kingsbury and Palmer,
2002) or FrameNet (Fillmore, 1982). Figure
1 shows an example of a predicate annotation
in PropBank for the sentence: "Paul gives a
lecture in Rome". A predicate may be a verb
or a noun or an adjective and most of the time
Arg 0 is the logical subject, Arg 1 is the logical
object and ArgM may indicate locations, as in
our example.
FrameNet alo describes predicate/argument
structures but for this purpose it uses richer
semantic structures called frames. These lat-
ter are schematic representations of situations
involving various participants, properties and
roles in which a word may be typically used.
Frame elements or semantic roles are arguments
of predicates called target words. In FrameNet,
the argument names are local to a particular
frame.
 
Predicate 
Arg. 0 
Arg. M 
S 
N 
NP 
D N 
VP 
V Paul 
in 
gives 
a lecture 
PP 
IN N 
Rome 
Arg. 1 
Figure 1: A predicate argument structure in a
parse-tree representation.
Several machine learning approaches for argu-
ment identification and classification have been
developed (Gildea and Jurasfky, 2002; Gildea
and Palmer, 2002; Surdeanu et al, 2003; Ha-
cioglu et al, 2003). Their common characteris-
tic is the adoption of feature spaces that model
predicate-argument structures in a flat repre-
sentation. On the contrary, convolution kernels
aim to capture structural information in term
of sub-structures, providing a viable alternative
to flat features.
In this paper, we select portions of syntactic
trees, which include predicate/argument salient
sub-structures, to define convolution kernels for
the task of predicate argument classification. In
particular, our kernels aim to (a) represent the
relation between predicate and one of its argu-
ments and (b) to capture the overall argument
structure of the target predicate. Additionally,
we define novel kernels as combinations of the
above two with the polynomial kernel of stan-
dard flat features.
Experiments on Support Vector Machines us-
ing the above kernels show an improvement
of the state-of-the-art for PropBank argument
classification. On the contrary, FrameNet se-
mantic parsing seems to not take advantage of
the structural information provided by our ker-
nels.
The remainder of this paper is organized as
follows: Section 2 defines the Predicate Argu-
ment Extraction problem and the standard so-
lution to solve it. In Section 3 we present our
kernels whereas in Section 4 we show compar-
ative results among SVMs using standard fea-
tures and the proposed kernels. Finally, Section
5 summarizes the conclusions.
2 Predicate Argument Extraction: a
standard approach
Given a sentence in natural language and the
target predicates, all arguments have to be rec-
ognized. This problem can be divided into two
subtasks: (a) the detection of the argument
boundaries, i.e. all its compounding words and
(b) the classification of the argument type, e.g.
Arg0 or ArgM in PropBank or Agent and Goal
in FrameNet.
The standard approach to learn both detec-
tion and classification of predicate arguments
is summarized by the following steps:
1. Given a sentence from the training-set gene-
rate a full syntactic parse-tree;
2. let P and A be the set of predicates and
the set of parse-tree nodes (i.e. the potential
arguments), respectively;
3. for each pair <p, a> ? P ?A:
? extract the feature representation set, Fp,a;
? if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T ?
(negative examples).
For example, in Figure 1, for each combina-
tion of the predicate give with the nodes N, S,
VP, V, NP, PP, D or IN the instances F?give?,a are
generated. In case the node a exactly covers
Paul, a lecture or in Rome, it will be a positive
instance otherwise it will be a negative one, e.g.
F?give?,?IN?.
To learn the argument classifiers the T + set
can be re-organized as positive T +argi and neg-
ative T?argi examples for each argument i. In
this way, an individual ONE-vs-ALL classifier
for each argument i can be trained. We adopted
this solution as it is simple and effective (Ha-
cioglu et al, 2003). In the classification phase,
given a sentence of the test-set, all its Fp,a
are generated and classified by each individ-
ual classifier. As a final decision, we select the
argument associated with the maximum value
among the scores provided by the SVMs, i.e.
argmaxi?S Ci, where S is the target set of ar-
guments.
- Phrase Type: This feature indicates the syntactic type
of the phrase labeled as a predicate argument, e.g. NP
for Arg1.
- Parse Tree Path: This feature contains the path in
the parse tree between the predicate and the argument
phrase, expressed as a sequence of nonterminal labels
linked by direction (up or down) symbols, e.g. V ? VP
? NP for Arg1.
- Position: Indicates if the constituent, i.e. the potential
argument, appears before or after the predicate in the
sentence, e.g. after for Arg1 and before for Arg0.
- Voice: This feature distinguishes between active or pas-
sive voice for the predicate phrase, e.g. active for every
argument.
- Head Word : This feature contains the headword of the
evaluated phrase. Case and morphological information
are preserved, e.g. lecture for Arg1.
- Governing Category indicates if an NP is dominated by
a sentence phrase or by a verb phrase, e.g. the NP asso-
ciated with Arg1 is dominated by a VP.
- Predicate Word : This feature consists of two compo-
nents: (1) the word itself, e.g. gives for all arguments;
and (2) the lemma which represents the verb normalized
to lower case and infinitive form, e.g. give for all argu-
ments.
Table 1: Standard features extracted from the
parse-tree in Figure 1.
2.1 Standard feature space
The discovery of relevant features is, as usual, a
complex task, nevertheless, there is a common
consensus on the basic features that should be
adopted. These standard features, firstly pro-
posed in (Gildea and Jurasfky, 2002), refer to
a flat information derived from parse trees, i.e.
Phrase Type, Predicate Word, Head Word, Gov-
erning Category, Position and Voice. Table 1
presents the standard features and exemplifies
how they are extracted from the parse tree in
Figure 1.
For example, the Parse Tree Path feature rep-
resents the path in the parse-tree between a
predicate node and one of its argument nodes.
It is expressed as a sequence of nonterminal la-
bels linked by direction symbols (up or down),
e.g. in Figure 1, V?VP?NP is the path between
the predicate to give and the argument 1, a lec-
ture. Two pairs <p1, a1> and <p2, a2> have
two different Path features even if the paths dif-
fer only for a node in the parse-tree. This pre-
 S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
Fdeliver, Arg0 
 formal 
 N 
      style 
Arg. 0 
a) S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
 formal 
 N 
      style 
Fdeliver, Arg1 
b) S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
 formal 
 N 
      style 
Arg. 1 
Fdeliver, ArgM 
 
c) 
Arg. M 
Figure 2: Structured features for Arg0, Arg1 and ArgM.
vents the learning algorithm to generalize well
on unseen data. In order to address this prob-
lem, the next section describes a novel kernel
space for predicate argument classification.
2.2 Support Vector Machine approach
Given a vector space in <n and a set of posi-
tive and negative points, SVMs classify vectors
according to a separating hyperplane, H(~x) =
~w ? ~x + b = 0, where ~w ? <n and b ? < are
learned by applying the Structural Risk Mini-
mization principle (Vapnik, 1995).
To apply the SVM algorithm to Predicate
Argument Classification, we need a function
? : F ? <n to map our features space F =
{f1, .., f|F|} and our predicate/argument pair
representation, Fp,a = Fz , into <n, such that:
Fz ? ?(Fz) = (?1(Fz), .., ?n(Fz))
From the kernel theory we have that:
H(~x) =
(
?
i=1..l
?i~xi
)
?~x+ b =
?
i=1..l
?i~xi ?~x+ b =
=
?
i=1..l
?i?(Fi) ? ?(Fz) + b.
where, Fi ?i ? {1, .., l} are the training in-
stances and the product K(Fi, Fz) =<?(Fi) ?
?(Fz)> is the kernel function associated with
the mapping ?. The simplest mapping that we
can apply is ?(Fz) = ~z = (z1, ..., zn) where
zi = 1 if fi ? Fz otherwise zi = 0, i.e.
the characteristic vector of the set Fz with re-
spect to F . If we choose as a kernel function
the scalar product we obtain the linear kernel
KL(Fx, Fz) = ~x ? ~z.
Another function which is the current state-
of-the-art of predicate argument classification is
the polynomial kernel: Kp(Fx, Fz) = (c+~x ?~z)d,
where c is a constant and d is the degree of the
polynom.
3 Convolution Kernels for Semantic
Parsing
We propose two different convolution kernels
associated with two different predicate argu-
ment sub-structures: the first includes the tar-
get predicate with one of its arguments. We will
show that it contains almost all the standard
feature information. The second relates to the
sub-categorization frame of verbs. In this case,
the kernel function aims to cluster together ver-
bal predicates which have the same syntactic
realizations. This provides the classification al-
gorithm with important clues about the possible
set of arguments suited for the target syntactic
structure.
3.1 Predicate/Argument Feature
(PAF)
We consider the predicate argument structures
annotated in PropBank or FrameNet as our se-
mantic space. The smallest sub-structure which
includes one predicate with only one of its ar-
guments defines our structural feature. For
example, Figure 2 illustrates the parse-tree of
the sentence "Paul delivers a talk in formal
style". The circled substructures in (a), (b)
and (c) are our semantic objects associated
with the three arguments of the verb to de-
liver, i.e. <deliver, Arg0>, <deliver, Arg1>
and <deliver, ArgM>. Note that each predi-
cate/argument pair is associated with only one
structure, i.e. Fp,a contain only one of the cir-
cled sub-trees. Other important properties are
the followings:
(1) The overall semantic feature space F con-
tains sub-structures composed of syntactic in-
formation embodied by parse-tree dependencies
and semantic information under the form of
predicate/argument annotation.
(2) This solution is efficient as we have to clas-
sify as many nodes as the number of predicate
arguments.
(3) A constituent cannot be part of two differ-
ent arguments of the target predicate, i.e. there
is no overlapping between the words of two ar-
guments. Thus, two semantic structures Fp1,a1
and Fp2,a21, associated with two different ar-
1Fp,a was defined as the set of features of the object
<p, a>. Since in our representations we have only one
SNP VP
VP VPCC
VBD NP
flushed DT NN
the pan
and VBD NP
buckled PRP$ NN
his belt
PRP
He
Arg0
(flush and buckle)
Arg1 
(flush) Arg1 (buckle)
Predicate 1 Predicate 2
Fflush
Fbuckle
Figure 3: Sub-Categorization Features for two
predicate argument structures.
guments, cannot be included one in the other.
This property is important because a convolu-
tion kernel would not be effective to distinguish
between an object and its sub-parts.
3.2 Sub-Categorization Feature (SCF)
The above object space aims to capture all
the information between a predicate and one of
its arguments. Its main drawback is that im-
portant structural information related to inter-
argument dependencies is neglected. In or-
der to solve this problem we define the Sub-
Categorization Feature (SCF). This is the sub-
parse tree which includes the sub-categorization
frame of the target verbal predicate. For
example, Figure 3 shows the parse tree of
the sentence "He flushed the pan and buckled
his belt". The solid line describes the SCF
of the predicate flush, i.e. Fflush whereas the
dashed line tailors the SCF of the predicate
buckle, i.e. Fbuckle. Note that SCFs are features
for predicates, (i.e. they describe predicates)
whereas PAF characterizes predicate/argument
pairs.
Once semantic representations are defined,
we need to design a kernel function to esti-
mate the similarity between our objects. As
suggested in Section 2 we can map them into
vectors in <n and evaluate implicitly the scalar
product among them.
3.3 Predicate/Argument structure
Kernel (PAK)
Given the semantic objects defined in the previ-
ous section, we design a convolution kernel in a
way similar to the parse-tree kernel proposed
in (Collins and Duffy, 2002). We divide our
mapping ? in two steps: (1) from the semantic
structure space F (i.e. PAF or SCF objects)
to the set of all their possible sub-structures
element in Fp,a with an abuse of notation we use it to
indicate the objects themselves.
 
 
 
 
 
 
 
 
 
 
NP 
D N 
a 
  talk 
NP 
D N 
NP 
D N 
a 
D N 
a   talk 
NP 
D N 
NP 
D N 
VP 
V 
delivers 
a 
   talk 
V 
delivers 
NP 
D N 
VP 
V 
a 
   talk 
NP 
D N 
VP 
V 
NP 
D N 
VP 
V 
a 
NP 
D
VP 
V 
   talk 
N 
a 
NP 
D N 
VP 
V 
delivers 
   talk 
NP 
D N 
VP 
V 
delivers 
NP 
D N 
VP 
V 
delivers 
NP 
VP 
V 
NP 
VP 
V 
delivers 
  talk 
Figure 4: All 17 valid fragments of the semantic
structure associated with Arg 1 of Figure 2.
F ? = {f ?1, .., f
?
|F ?|} and (2) from F ? to <|F
?|.
An example of features in F ? is given
in Figure 4 where the whole set of frag-
ments, F ?deliver,Arg1, of the argument structure
Fdeliver,Arg1, is shown (see also Figure 2).
It is worth noting that the allowed sub-trees
contain the entire (not partial) production rules.
For instance, the sub-tree [NP [D a]] is excluded
from the set of the Figure 4 since only a part of
the production NP ? D N is used in its gener-
ation. However, this constraint does not apply
to the production VP ? V NP PP along with the
fragment [VP [V NP]] as the subtree [VP [PP [...]]]
is not considered part of the semantic structure.
Thus, in step 1, an argument structure Fp,a is
mapped in a fragment set F ?p,a. In step 2, this
latter is mapped into ~x = (x1, .., x|F ?|) ? <|F
?|,
where xi is equal to the number of times that
f ?i occurs in F ?p,a2.
In order to evaluate K(?(Fx), ?(Fz)) without
evaluating the feature vector ~x and ~z we de-
fine the indicator function Ii(n) = 1 if the sub-
structure i is rooted at node n and 0 otherwise.
It follows that ?i(Fx) =
?
n?Nx Ii(n), where Nx
is the set of the Fx?s nodes. Therefore, the ker-
nel can be written as:
K(?(Fx), ?(Fz)) =
|F ?|
?
i=1
(
?
nx?Nx
Ii(nx))(
?
nz?Nz
Ii(nz))
=
?
nx?Nx
?
nz?Nz
?
i
Ii(nx)Ii(nz)
where Nx and Nz are the nodes in Fx and Fz, re-
spectively. In (Collins and Duffy, 2002), it has
been shown that ?i Ii(nx)Ii(nz) = ?(nx, nz)
can be computed in O(|Nx| ? |Nz|) by the fol-
lowing recursive relation:
(1) if the productions at nx and nz are different
then ?(nx, nz) = 0;
2A fragment can appear several times in a parse-tree,
thus each fragment occurrence is considered as a different
element in F ?p,a.
(2) if the productions at nx and nz are the
same, and nx and nz are pre-terminals then
?(nx, nz) = 1;
(3) if the productions at nx and nz are the same,
and nx and nz are not pre-terminals then
?(nx, nz) =
nc(nx)
?
j=1
(1 + ?(ch(nx, j), ch(nz , j))),
where nc(nx) is the number of the children of nx
and ch(n, i) is the i-th child of the node n. Note
that as the productions are the same ch(nx, i) =
ch(nz, i).
This kind of kernel has the drawback of
assigning more weight to larger structures
while the argument type does not strictly
depend on the size of the argument (Moschitti
and Bejan, 2004). To overcome this prob-
lem we can scale the relative importance of
the tree fragments using a parameter ? for
the cases (2) and (3), i.e. ?(nx, nz) = ? and
?(nx, nz) = ?
?nc(nx)
j=1 (1 + ?(ch(nx, j), ch(nz , j)))
respectively.
It is worth noting that even if the above equa-
tions define a kernel function similar to the one
proposed in (Collins and Duffy, 2002), the sub-
structures on which it operates are different
from the parse-tree kernel. For example, Figure
4 shows that structures such as [VP [V] [NP]], [VP
[V delivers ] [NP]] and [VP [V] [NP [DT] [N]]] are
valid features, but these fragments (and many
others) are not generated by a complete produc-
tion, i.e. VP ? V NP PP. As a consequence they
would not be included in the parse-tree kernel
of the sentence.
3.4 Comparison with Standard
Features
In this section we compare standard features
with the kernel based representation in order
to derive useful indications for their use:
First, PAK estimates a similarity between
two argument structures (i.e., PAF or SCF)
by counting the number of sub-structures that
are in common. As an example, the sim-
ilarity between the two structures in Figure
2, F?delivers?,Arg0 and F?delivers?,Arg1, is equal
to 1 since they have in common only the [V
delivers] substructure. Such low value de-
pends on the fact that different arguments tend
to appear in different structures.
On the contrary, if two structures differ only
for a few nodes (especially terminals or near
terminal nodes) the similarity remains quite
high. For example, if we change the tense of
the verb to deliver (Figure 2) in delivered, the
[VP [V delivers] [NP]] subtree will be trans-
formed in [VP [VBD delivered] [NP]], where the
NP is unchanged. Thus, the similarity with
the previous structure will be quite high as:
(1) the NP with all sub-parts will be matched
and (2) the small difference will not highly af-
fect the kernel norm and consequently the fi-
nal score. The above property also holds for
the SCF structures. For example, in Figure
3, KPAK (?(Fflush), ?(Fbuckle)) is quite high as
the two verbs have the same syntactic realiza-
tion of their arguments. In general, flat features
do not possess this conservative property. For
example, the Parse Tree Path is very sensible
to small changes of parse-trees, e.g. two predi-
cates, expressed in different tenses, generate two
different Path features.
Second, some information contained in the
standard features is embedded in PAF: Phrase
Type, Predicate Word and Head Word explicitly
appear as structure fragments. For example, in
Figure 4 are shown fragments like [NP [DT] [N]] or
[NP [DT a] [N talk]] which explicitly encode the
Phrase Type feature NP for the Arg 1 in Fig-
ure 2.b. The Predicate Word is represented by
the fragment [V delivers] and the Head Word
is encoded in [N talk]. The same is not true for
SCF since it does not contain information about
a specific argument. SCF, in fact, aims to char-
acterize the predicate with respect to the overall
argument structures rather than a specific pair
<p, a>.
Third, Governing Category, Position and
Voice features are not explicitly contained in
both PAF and SCF. Nevertheless, SCF may
allow the learning algorithm to detect the ac-
tive/passive form of verbs.
Finally, from the above observations follows
that the PAF representation may be used with
PAK to classify arguments. On the contrary,
SCF lacks important information, thus, alone it
may be used only to classify verbs in syntactic
categories. This suggests that SCF should be
used in conjunction with standard features to
boost their classification performance.
4 The Experiments
The aim of our experiments are twofold: On
the one hand, we study if the PAF represen-
tation produces an accuracy higher than stan-
dard features. On the other hand, we study if
SCF can be used to classify verbs according to
their syntactic realization. Both the above aims
can be carried out by combining PAF and SCF
with the standard features. For this purpose
we adopted two ways to combine kernels3: (1)
K = K1 ? K2 and (2) K = ?K1 + K2. The re-
sulting set of kernels used in the experiments is
the following:
? Kpd is the polynomial kernel with degree d
over the standard features.
? KPAF is obtained by using PAK function over
the PAF structures.
? KPAF+P = ? KPAF|KPAF | +
Kpd
|Kpd |
, i.e. the sum be-
tween the normalized4 PAF-based kernel and
the normalized polynomial kernel.
? KPAF ?P =
KPAF ?Kpd
|KPAF |?|Kpd |
, i.e. the normalized
product between the PAF-based kernel and the
polynomial kernel.
? KSCF+P = ? KSCF|KSCF | +
Kpd
|Kpd |
, i.e. the summa-
tion between the normalized SCF-based kernel
and the normalized polynomial kernel.
? KSCF ?P =
KSCF ?Kpd
|KSCF |?|Kpd |
, i.e. the normal-
ized product between SCF-based kernel and the
polynomial kernel.
4.1 Corpora set-up
The above kernels were experimented over two
corpora: PropBank (www.cis.upenn.edu/?ace)
along with Penn TreeBank5 2 (Marcus et al,
1993) and FrameNet.
PropBank contains about 53,700 sentences
and a fixed split between training and test-
ing which has been used in other researches
e.g., (Gildea and Palmer, 2002; Surdeanu et al,
2003; Hacioglu et al, 2003). In this split, Sec-
tions from 02 to 21 are used for training, section
23 for testing and sections 1 and 22 as devel-
oping set. We considered all PropBank argu-
ments6 from Arg0 to Arg9, ArgA and ArgM for
a total of 122,774 and 7,359 arguments in train-
ing and testing respectively. It is worth noting
that in the experiments we used the gold stan-
dard parsing from Penn TreeBank, thus our ker-
nel structures are derived with high precision.
For the FrameNet corpus (www.icsi.berkeley
3It can be proven that the resulting kernels still sat-
isfy Mercer?s conditions (Cristianini and Shawe-Taylor,
2000).
4To normalize a kernel K(~x, ~z) we can divide it by
?
K(~x, ~x) ? K(~z, ~z).
5We point out that we removed from Penn TreeBank
the function tags like SBJ and TMP as parsers usually
are not able to provide this information.
6We noted that only Arg0 to Arg4 and ArgM con-
tain enough training/testing data to affect the overall
performance.
.edu/?framenet) we extracted all 24,558 sen-
tences from the 40 frames of Senseval 3 task
(www.senseval.org) for the Automatic Labeling
of Semantic Roles. We considered 18 of the
most frequent roles and we mapped together
those having the same name. Only verbs are se-
lected to be predicates in our evaluations. More-
over, as it does not exist a fixed split between
training and testing, we selected randomly 30%
of sentences for testing and 70% for training.
Additionally, 30% of training was used as a
validation-set. The sentences were processed us-
ing Collins? parser (Collins, 1997) to generate
parse-trees automatically.
4.2 Classification set-up
The classifier evaluations were carried out using
the SVM-light software (Joachims, 1999) avail-
able at svmlight.joachims.org with the default
polynomial kernel for standard feature evalu-
ations. To process PAF and SCF, we imple-
mented our own kernels and we used them in-
side SVM-light.
The classification performances were evalu-
ated using the f1 measure7 for single arguments
and the accuracy for the final multi-class clas-
sifier. This latter choice allows us to compare
the results with previous literature works, e.g.
(Gildea and Jurasfky, 2002; Surdeanu et al,
2003; Hacioglu et al, 2003).
For the evaluation of SVMs, we used the de-
fault regularization parameter (e.g., C = 1 for
normalized kernels) and we tried a few cost-
factor values (i.e., j ? {0.1, 1, 2, 3, 4, 5}) to ad-
just the rate between Precision and Recall. We
chose parameters by evaluating SVM using Kp3
kernel over the validation-set. Both ? (see Sec-
tion 3.3) and ? parameters were evaluated in a
similar way by maximizing the performance of
SVM using KPAF and ? KSCF|KSCF | +
Kpd
|Kpd |
respec-
tively. These parameters were adopted also for
all the other kernels.
4.3 Kernel evaluations
To study the impact of our structural kernels we
firstly derived the maximal accuracy reachable
with standard features along with polynomial
kernels. The multi-class accuracies, for Prop-
Bank and FrameNet using Kpd with d = 1, .., 5,
are shown in Figure 5. We note that (a) the
highest performance is reached for d = 3, (b)
for PropBank our maximal accuracy (90.5%)
7f1 assigns equal importance to Precision P and Re-
call R, i.e. f1 = 2P ?RP+R .
is substantially equal to the SVM performance
(88%) obtained in (Hacioglu et al, 2003) with
degree 2 and (c) the accuracy on FrameNet
(85.2%) is higher than the best result obtained
in literature, i.e. 82.0% in (Gildea and Palmer,
2002). This different outcome is due to a differ-
ent task (we classify different roles) and a differ-
ent classification algorithm. Moreover, we did
not use the Frame information which is very im-
portant8.
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
1 2 3 4 5d
A
cc
u
ra
cy FrameNet
PropBank
Figure 5: Multi-classifier accuracy according to dif-
ferent degrees of the polynomial kernel.
It is worth noting that the difference between
linear and polynomial kernel is about 3-4 per-
cent points for both PropBank and FrameNet.
This remarkable difference can be easily ex-
plained by considering the meaning of standard
features. For example, let us restrict the classi-
fication function CArg0 to the two features Voice
and Position. Without loss of generality we can
assume: (a) Voice=1 if active and 0 if passive,
and (b) Position=1 when the argument is af-
ter the predicate and 0 otherwise. To simplify
the example, we also assume that if an argu-
ment precedes the target predicate it is a sub-
ject, otherwise it is an object 9. It follows that
a constituent is Arg0, i.e. CArg0 = 1, if only
one feature at a time is 1, otherwise it is not
an Arg0, i.e. CArg0 = 0. In other words, CArg0
= Position XOR Voice, which is the classical ex-
ample of a non-linear separable function that
becomes separable in a superlinear space (Cris-
tianini and Shawe-Taylor, 2000).
After it was established that the best ker-
nel for standard features is Kp3 , we carried out
all the other experiments using it in the kernel
combinations. Table 2 and 3 show the single
class (f1 measure) as well as multi-class classi-
fier (accuracy) performance for PropBank and
FrameNet respectively. Each column of the two
tables refers to a different kernel defined in the
8Preliminary experiments indicate that SVMs can
reach 90% by using the frame feature.
9Indeed, this is true in most part of the cases.
previous section. The overall meaning is dis-
cussed in the following points:
First, PAF alone has good performance, since
in PropBank evaluation it outperforms the lin-
ear kernel (Kp1), 88.7% vs. 86.7% whereas in
FrameNet, it shows a similar performance 79.5%
vs. 82.1% (compare tables with Figure 5). This
suggests that PAF generates the same informa-
tion as the standard features in a linear space.
However, when a degree greater than 1 is used
for standard features, PAF is outperformed10.
Args P3 PAF PAF+P PAF?P SCF+P SCF?P
Arg0 90.8 88.3 90.6 90.5 94.6 94.7
Arg1 91.1 87.4 89.9 91.2 92.9 94.1
Arg2 80.0 68.5 77.5 74.7 77.4 82.0
Arg3 57.9 56.5 55.6 49.7 56.2 56.4
Arg4 70.5 68.7 71.2 62.7 69.6 71.1
ArgM 95.4 94.1 96.2 96.2 96.1 96.3
Acc. 90.5 88.7 90.2 90.4 92.4 93.2
Table 2: Evaluation of Kernels on PropBank.
Roles P3 PAF PAF+P PAF?P SCF+P SCF?P
agent 92.0 88.5 91.7 91.3 93.1 93.9
cause 59.7 16.1 41.6 27.7 42.6 57.3
degree 74.9 68.6 71.4 57.8 68.5 60.9
depict. 52.6 29.7 51.0 28.6 46.8 37.6
durat. 45.8 52.1 40.9 29.0 31.8 41.8
goal 85.9 78.6 85.3 82.8 84.0 85.3
instr. 67.9 46.8 62.8 55.8 59.6 64.1
mann. 81.0 81.9 81.2 78.6 77.8 77.8
Acc. 85.2 79.5 84.6 81.6 83.8 84.2
18 roles
Table 3: Evaluation of Kernels on FrameNet se-
mantic roles.
Second, SCF improves the polynomial kernel
(d = 3), i.e. the current state-of-the-art, of
about 3 percent points on PropBank (column
SCF?P). This suggests that (a) PAK can mea-
sure the similarity between two SCF structures
and (b) the sub-categorization information pro-
vides effective clues about the expected argu-
ment type. The interesting consequence is that
SCF together with PAK seems suitable to au-
tomatically cluster different verbs that have the
same syntactic realization. We note also that to
fully exploit the SCF information it is necessary
to use a kernel product (K1 ? K2) combination
rather than the sum (K1 + K2), e.g. column
SCF+P.
Finally, the FrameNet results are completely
different. No kernel combinations with both
PAF and SCF produce an improvement. On
10Unfortunately the use of a polynomial kernel on top
the tree fragments to generate the XOR functions seems
not successful.
the contrary, the performance decreases, sug-
gesting that the classifier is confused by this
syntactic information. The main reason for the
different outcomes is that PropBank arguments
are different from semantic roles as they are
an intermediate level between syntax and se-
mantic, i.e. they are nearer to grammatical
functions. In fact, in PropBank arguments are
annotated consistently with syntactic alterna-
tions (see the Annotation guidelines for Prop-
Bank at www.cis.upenn.edu/?ace). On the con-
trary FrameNet roles represent the final seman-
tic product and they are assigned according to
semantic considerations rather than syntactic
aspects. For example, Cause and Agent seman-
tic roles have identical syntactic realizations.
This prevents SCF to distinguish between them.
Another minor reason may be the use of auto-
matic parse-trees to extract PAF and SCF, even
if preliminary experiments on automatic seman-
tic shallow parsing of PropBank have shown no
important differences versus semantic parsing
which adopts Gold Standard parse-trees.
5 Conclusions
In this paper, we have experimented with
SVMs using the two novel convolution kernels
PAF and SCF which are designed for the se-
mantic structures derived from PropBank and
FrameNet corpora. Moreover, we have com-
bined them with the polynomial kernel of stan-
dard features. The results have shown that:
First, SVMs using the above kernels are ap-
pealing for semantically parsing both corpora.
Second, PAF and SCF can be used to improve
automatic classification of PropBank arguments
as they provide clues about the predicate argu-
ment structure of the target verb. For example,
SCF improves (a) the classification state-of-the-
art (i.e. the polynomial kernel) of about 3 per-
cent points and (b) the best literature result of
about 5 percent points.
Third, additional work is needed to design
kernels suitable to learn the deep semantic con-
tained in FrameNet as it seems not sensible to
both PAF and SCF information.
Finally, an analysis of SVMs using poly-
nomial kernels over standard features has ex-
plained why they largely outperform linear clas-
sifiers based-on standard features.
In the future we plan to design other struc-
tures and combine them with SCF, PAF and
standard features. In this vision the learning
will be carried out on a set of structural features
instead of a set of flat features. Other studies
may relate to the use of SCF to generate verb
clusters.
Acknowledgments
This research has been sponsored by the ARDA
AQUAINT program. In addition, I would like to
thank Professor Sanda Harabagiu for her advice,
Adrian Cosmin Bejan for implementing the feature
extractor and Paul Mora?rescu for processing the
FrameNet data. Many thanks to the anonymous re-
viewers for their invaluable suggestions.
References
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In
proceeding of ACL-02.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In proceedings of
the ACL-97, pages 16?23, Somerset, New Jersey.
Nello Cristianini and John Shawe-Taylor. 2000. An
introduction to Support Vector Machines. Cam-
bridge University Press.
Charles J. Fillmore. 1982. Frame semantics. In Lin-
guistics in the Morning Calm, pages 111?137.
Daniel Gildea and Daniel Jurasfky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistic.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition.
In proceedings of ACL-02, Philadelphia, PA.
R. Jackendoff. 1990. Semantic Structures, Current
Studies in Linguistics series. Cambridge, Mas-
sachusetts: The MIT Press.
T. Joachims. 1999. Making large-scale SVM learn-
ing practical. In Advances in Kernel Methods -
Support Vector Learning.
Paul Kingsbury and Martha Palmer. 2002. From
treebank to propbank. In proceedings of LREC-
02, Las Palmas, Spain.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of english: The penn treebank.
Computational Linguistics.
Alessandro Moschitti and Cosmin Adrian Bejan.
2004. A semantic kernel for predicate argu-
ment classification. In proceedings of CoNLL-04,
Boston, USA.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2003.
Shallow Semantic Parsing Using Support Vector
Machines. TR-CSLR-2003-03, University of Col-
orado.
Mihai Surdeanu, Sanda M. Harabagiu, John
Williams, and John Aarseth. 2003. Using
predicate-argument structures for information ex-
traction. In proceedings of ACL-03, Sapporo,
Japan.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag New York, Inc.
Semantic Parsing Based on FrameNet
Cosmin Adrian Bejan, Alessandro Moschitti, Paul Mora?rescu,
Gabriel Nicolae and Sanda Harabagiu
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
Abstract
This paper describes our method based on Support
Vector Machines for automatically assigning seman-
tic roles to constituents of English sentences. This
method employs four different feature sets, one of
which being first reported herein. The combination
of features as well as the extended training data we
considered have produced in the Senseval-3 experi-
ments an F1-score of 92.5% for the unrestricted case
and of 76.3% for the restricted case.
1 Introduction
The evaluation of the Senseval-3 task for Automatic
Labeling of Semantic Roles is based on the annota-
tions made available by the FrameNet Project (Baker
et al, 1998). The idea of automatically identifying
and labeling frame-specific roles, as defined by the
semantic frames, was first introduced by (Gildea and
Jurasfky, 2002). Each semantic frame is character-
ized by a set of target words which can be nouns,
verbs or adjectives. This helps abstracting the the-
matic roles and adding semantics to the given frame,
highlighting the characteristic semantic features.
Frames are characterized by (1) target words or
lexical predicates whose meaning includes aspects of
the frame; (2) frame elements (FEs) which represent
the semantic roles of the frame and (3) examples of
annotations performed on the British National Cor-
pus (BNC) for instances of each target word. Thus
FrameNet frames are schematic representations of
situations lexicalized by the target words (predicates)
in which various participants and conceptual roles
are related (the frame elements), exemplified by sen-
tences from the BNC in which the target words and
the frame elements are annotated.
In Senseval-3 two different cases of automatic la-
beling of the semantic roles were considered. The
Unrestricted Case requires systems to assign FE la-
bels to the test sentences for which (a) the bound-
aries of each frame element were given and the tar-
get words identified. The Restricted Case requires
systems to (i) recognize the boundaries of the FEs
for each evaluated frame as well as to (ii) assign a
label to it. Both cases can be cast as two different
classifications: (1) a classification of the role when
its boundaries are known and (2) a classification of
the sentence words as either belonging to a role or
not1.
A similar approach was used for automati-
cally identifying predicate-argument structures in
English sentences. The PropBank annotations
(www.cis.upenn.edu/?ace) enable training for two
distinct learning techniques: (1) decision trees (Sur-
deanu et al, 2003) and (2) Support Vector Machines
(SVMs) (Pradhan et al, 2004). The SVMs produced
the best results, therefore we decided to use the same
learning framework for the Senseval-3 task for Auto-
matic Labeling of Semantic Roles. Additionally, we
have performed the following enhancements:
? we created a multi-class classifier for each frame,
thus achieving improved accuracy and efficiency;
? we combined some new features with features from
(Gildea and Jurasfky, 2002; Surdeanu et al, 2003;
Pradhan et al, 2004);
? we resolved the data sparsity problem generated
by limited training data for each frame, when using
the examples associated with any other frame from
FrameNet that had at least one FE shared with each
frame that was evaluated;
? we crafted heuristics that improved mappings from
the syntactic constituents to the semantic roles.
We believe that the combination of these four exten-
sions are responsible for our results in Senseval-3.
The remainder of this paper is organized as fol-
lows. Section 2 describes our methods of classify-
ing semantic roles whereas Section 3 describes our
method of identifying role boundaries. Section 4 de-
tails our heuristics and Section 5 details the exper-
imental results. Section 6 summarizes the conclu-
sions.
1The second classification represents the detection of role
boundaries. The semantic parsing defined as two different clas-
sification tasks was introduced in (Gildea and Jurasfky, 2002).
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2 Semantic role classification
The result of the role classifier on a sentence, as il-
lustrated in Figure 1, is the identification of semantic
roles of the FEs when the boundaries of each FE are
known. To be able to assign the labels of each FE, we
used three sets of features. Feature Set 1, illustrated
in Figure 2 was used in the work reported in (Gildea
and Jurasfky, 2002).
VP
S
VP
NP
People were fastening
TARGETAgent
NP
a rope
PP
to the ring
NP
GoalItem
Figure 1: Sentence with annotated frame elements.
? POSITION (pos) ? Indicates if the constituent appears  
before or after the the predicate in the sentence.
? VOICE (voice) ? This feature distinguishes between
active or passive voice for the predicate phrase.
are preserved.
of the evaluated phrase. Case and morphological information
? HEAD WORD (hw) ? This feature contains the head word
? PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the 
labels linked by direction symbols (up or down), e.g.
? PHRASE TYPE (pt): This feature indicates the syntactic
noun phrases only, and it indicates if the NP is dominated
by a sentence phrase (typical for subject arguments with
active?voice predicates), or by a verb phrase (typical 
for object arguments).
? GOVERNING CATEGORY (gov) ? This feature applies to
type of the phrase labeled as a frame element, e.g.
target word, expressed as a sequence of nonterminal
? TARGET WORD ? In our implementation this feature
(2) LEMMA which represents the target normalized to lower 
the case and morphological information preserved; and 
consists of two components: (1) WORD: the word itself with
case and infinitive form for the verbs or singular for nouns. 
NP for Agent in Figure 1.
NP    S    VP    VP for Agent in Figure 1.
Figure 2: Feature Set 1 (FS1)
Feature Set 2 was introduced in (Surdeanu et al,
2003) and it is illustrated in Figure 3. The CON-
TENT WORD (cw) feature illustrated in Figure 3
applies to PPs, SBARs and VPs, as it was reported
in (Surdeanu et al, 2003). For example, if the PP is
?in the past month?, instead of using ?in?, the head
of the PP, as a feature, ?month?, the head of the NP
is selected since it is more informative. Similarly, if
the SBAR is ?that occurred yesterday?, instead of us-
ing the head ?that? we select ?occurred?, the head of
the VP. When the VP ?to be declared? is considered,
?declared? is selected over ?to?.
Feature set 3 is a novel set of features introduced
in this paper and illustrated in Figure 4. Some
of the new features characterize the frame, e.g.
the frame name (FRAME-NAME); the frame FEs,
(NUMBER-FEs); or the target word associated with
the frame (TAGET-TYPE). Additional characteriza-
tion of the FEs are provided by by the GRAMMATI-
CAL FUNCTION feature and by the list of grammat-
ical functions of all FEs recognized in each sentence(
LIST Grammatical Function feature).
BOOLEAN NAMED ENTITY FLAGS ? A feature set comprising: 
? neOrganization: set to 1 if an organization is recognized in the phrase
? neLocation: set to 1 a location is recognized in the phrase
? nePerson: set to 1 if a person name is recognized in the phrase
? neMoney: set to 1 if a currency expression is recognized in the phrase
? nePercent: set to 1 if a percentage expression is recognized in the phrase
? neTime: set to 1 if a time of day expression is recognized in the phrase
? neDate: set to 1 if a date temporal expression is recognized in the phrase 
word from the constituent, different from the head word.
? CONTENT WORD (cw) ? Lexicalized feature that selects an informative 
PART OF SPEECH OF HEAD WORD (hPos) ? The part of speech tag of
the head word.
PART OF SPEECH OF CONTENT WORD (cPos) ?The part of speech 
tag of the content word.
NAMED ENTITY CLASS OF CONTENT WORD (cNE) ? The class of 
the named entity that includes the content word
Figure 3: Feature Set 2 (FS2)
In FrameNet, sentences are annotated with the
name of the sub-corpus. There are 12,456 possi-
ble names of sub-corpus. For the 40 frames eval-
uated in Senseval-3, there were 1442 names asso-
ciated with the example sentences in the training
data and 2723 names in the test data. Three of
the most frequent sub-corpus names are: ?V-trans-
other? (frequency=613), ?N-all? (frequency=562)
and ?V-trans-simple?(frequency=560). The name
of the sub-corpus indicates the relations between
the target word and some of its FEs. For ex-
ample, the ?V-trans-other? name indicated that the
target word is a transitive verb, and thus its FEs
are likely to have other roles than object or indi-
rect object. A sentence annotated with this sub-
corpus name is: ?Night?s coming, you can see
the black shadow on [Self?mover the stones] that
[TARGET rush] [Pathpast] and [Pathbetween your
feet.?]. For this sentence both FEs with the role of
Path are neither objects or indirect objects of the
transitive verb.
Feature SUPPORT VERBS considers the usage of
support expressions in FrameNet. We have found
that whenever adjectives are target words, their se-
mantic interpretation depends on their co-occurrence
with verbs like ?take?, ?become? or ?is?. Support
verbs are defined as those verbs that combine with a
state-noun, event-noun or state-adjective to create a
verbal predicate, allowing arguments of the verb to
serve as FEs of the frame evoked by the noun or the
adjective.
The CORENESS feature takes advantage of a
more recent implementation concept of core FEs
(vs. non-core FEs) in FrameNet. More specifi-
cally, the FrameNet developers classify frame ele-
ments in terms of how central they are to a particular
frame, distinguishing three levels: core, peripheral
and extra-thematic.
The features were used to produce two types of
examples: positive and negative examples. For each
FE of a frame, aside from the positive examples ren-
dered by the annotations, we considered as negative
examples all the annotations of the other FEs for the
same frame. The positive and the negative examples
were used for training the multi-class classifiers.
SUPPORT_VERBS that are recognized for adjective or noun target words
target word. The values of this feature are either (1) The POS of the head
of the VP containing the target word or (2) NULL if the target word does
not belong to a VP
or ADJECTIVE
LIST_CONSTITUENT (FEs): This feature represents a list of the syntactic
Grammatical Function: This feature indicates whether the FE is:
? an External Argument (Ext)
? an Object (Obj)
? a Complement (Comp)
? a Modifier (Mod)
? Head noun modified by attributive adjective (Head)
? Genitive determiner (Gen)
? Appositive (Appos)
LIST_Grammatical_Function: This feature represents a list of the 
grammatical functions of the FEs recognized in the sentence.
in each sentence.
FRAME_NAME: This feature indicates the name of the semantic frame 
for which FEs are labeled
COVERAGE: This feature indicates whether there is a syntactic structure
in the parse tree that perfectly covers the FE
a conceptually necessary participant of a frame. For example, in the 
are: (1) core; (2) peripheral and (3) extrathemathic. FEs that mark notions
such as Time, Place, Manner and Degree are peripheral. Extrathematic
FEs situate an event against a backdrop of another event, by evoking 
a larger frame for which the target event fills a role.
SUB_CORPUS: In FrameNet, sentences are annotated with the name
of the subcorpus they belong to. For example, for a verb target word,
to a FE included in a relative clause headed by a wh?word.
(2) a hyponym of sense 1 of PERSON in WordNet
(1) a personal pronoun or
HUMAN: This feature indicates whether the syntactic phrase is either
TARGET?TYPE: the lexical class of the target word, e.g. VERB, NOUN
consituents covering each FE of the frame recognized in a sentence. 
For the example illustrated in Figure 1, the list is: [NP, NP, PP]
NUMBER_FEs: This feature indicates how many FEs were recognized 
have the role of predicate for the FEs. For example, if the target word is
"clever" in the sentence "Smith is very clever, but he?s no Einstein", the 
the FE "Smith" is an argument of the support verb "is"? rather than of the
CORENESS: This feature indicates whether the FE instantiates
REVENGE frame, Punishment is a core element. The values 
V?swh represents a subcorpus in which the target word is a predicate
Figure 4: Feature Set 3 (FS3)
Our multi-class classification allows each FE to be
initially labeled with more than one role when sev-
eral classifiers decide so. For example, for the AT-
TACHING frame, an FE may be labeled both as Goal
and as Item if the classifiers for the Goal and Item
select it as a possible role. To choose the final label,
we select the classification which was assigned the
largest score by the SVMs.
PARSE TREE PATH WITH UNIQUE DELIMITER ?  This feature removes
the direction in the path, e.g. VBN?VP?ADVP
PARTIAL PATH ? This feature uses only the path from the constituent to
the lowest common ancestor of the predicate and the constituent
FIRST WORD ? First word covered by constituent
FIRST POS ? POS of first word covered by constituent
LEFT CONSTITUENT ? Left sibling constituent label
RIGHT HEAD ? Right sibling head word
RIGHT POS HEAD ? Right sibling POS of head word
LAST POS ? POS of last word covered by the constituent
LEFT HEAD ? Left sibling head word
LEFT POS HEAD ? Left sibling POS of head word
RIGHT CONSTITUENT ? Right sibling constituent label
PP PREP ? If constituent is labeled PP get first word in PP
DISTANCE ? Distance in the tree from constituent to the target word
LAST WORD ? Last word covered by the constituent
Figure 5: Feature Set 4 (FS4)
3 Boundary Detection
The boundary detection of each FE was required
in the Restricted Case of the Senseval-3 evalua-
tion. To classify a word as belonging to an FE
or not, we used all the entire Feature Set 1 and
2. From the Feature Set 3 we have used only
four features: the Support- Verbs feature; the
Target-Type feature, the Frame-Name feature
and the Sub Corpus feature. For this task we have
also used Feature Set 4, which were first introduced
in (Pradhan et al, 2004). The Feature Set 4 is il-
lustrated in Figure 5. After the boundary detec-
tion was performed, the semantic roles of each FE
were assigned using the role classifier trained for the
Restricted Case
4 Heuristics
Frequently, syntactic constituents do not cover ex-
actly FEs. For the Unrestricted Case we imple-
mented a very simple heuristic: when there is no
parse-tree node that exactly covers the target role r
but a subset of adjacent nodes perfectly match r,
we merge them in a new NPmerge node. For the
Restricted Case, a heuristic for adjectival and nomi-
nal target words w adjoins consecutive nouns that are
in the same noun phrase as w.
5 Experimental Results
In the Senseval-3 task for Automatic Labeling of
Semantic Roles 24,558 sentences from FrameNet
were assigned for training while 8,002 for testing.
We used 30% of the training set (7367 sentences)
as a validation-set for selecting SVM parameters
that optimize accuracy. The number of FEs for
which labels had to be assigned were: 51,010 for
the training set; 15,924 for the validation set and
16,279 for the test set. We used an additional set
of 66,687 sentences (hereafter extended data) as ex-
tended data produced when using the examples as-
sociated with any other frame from FrameNet that
had at least one FE shared with any of the 40
frames evaluated in Senseval-3. These sentences
were parsed with the Collins? parser (Collins, 1997).
The classifier experiments were carried out using the
SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/with a poly-
nomial kernel2 (degree=3).
5.1 Unrestricted Task Experiments
For this task we devised four different experiments
that used four different combination of features: (1)
FS1 indicates using only Feature Set 1; (2) +H in-
dicates that we added the heuristics; (3) +FS2+FS3
indicates that we add the feature Set 2 and 3; and
(4) +E indicates that the extended data has also been
used. For each of the four experiments we trained 40
multi-class classifiers, (one for each frame) for a total
of 385 binary role classifiers. The following Table il-
lustrates the overall performance over the validation-
set. To evaluate the results we measure the F1-score
by combining the precision P with the recall R in the
formula F1 = 2?P?RP+R .
FS1 +H +H+FS2+FS3 +H+FS2+FS3+E
84.4 84.9 91.7 93.1
5.2 Restricted Task Experiments
In order to find the best feature combination for this
task we carried out some preliminary experiments
over five frames. In Table 1, the row labeled B lists
the F1-score of boundary detection over 4 different
feature sets: FS1, +H, +FS4 and +E, the extended
data. The row labeled R lists the same results for the
whole Restricted Case.
Table 1: Restrictive experiments on validation-set.
+FS1 +H +H+FS2+FS3 +H+FS4+E
B 80.29 80.48 84.76 84.88
R 74.9 75.4 78 78.9
Table 1 illustrates the overall performance (bound-
ary detection and role classification) of automatic se-
mantic role labeling. The results listed in Tables 1
and 2 were obtained by comparing the FE bound-
aries identified by our parser with those annotated in
FrameNet. We believe that these results are more
2In all experiments and for any classifier, we used the default
SVM-light regularization parameter (e.g., C = 1 for normalized
kernels) and a cost-factor j = 100 to adjust the rate between
Precision and Recall.
indicative of the performance of our systems than
those obtained when using the scorer provided by
Senseval-3. When using this scorer, our results have
a precision of 89.9%, recall of 77.2% and an F1-
score of 83.07% for the Restricted Case.
Table 2: Results on the test-set.
Precision Recall F1
Unrestricted Case 94.5 90.6 92.5
Boundary Detection 87.3 75.1 80.7
Restricted Case 82.4 71.1 76.3
To generate the final Senseval-3 submissions we
selected the most accurate models (for unrestricted
and restricted tasks) of the validation experiments.
Then we re-trained such models with all training data
(i.e. our training plus validation data) and the set-
ting (parameters, heuristics and extended data) de-
rived over the validation-set. Finally, we run all clas-
sifiers on the test-set of the task. Table 2 illustrates
the final results for both sub-tasks.
6 Conclusions
In this paper we describe a method for automatically
labeling semantic roles based on support vector ma-
chines (SVMs). The training benefits from an ex-
tended data set on which multi-class classifiers were
derived. The polynomial kernel of the SVMs en-
able the combination of four feature sets that pro-
duced very good results both for the Restricted Case
and the Unrestricted Case. The paper also describes
some heuristics for mapping syntactic constituents
onto FEs.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings
of the COLING-ACL, Montreal, Canada.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
ACL-97, pages 16?23.,
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
T. Joachims. 1999. Making Large-Scale SVM Learning
Practical. In B. Schlkopf, C. Burges, and MIT-Press.
A. Smola (ed.), editors, Advances in Kernel Methods -
Support Vector Learning.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2004.
Support vector learning for semantic argument classifi-
cation. Journal of Machine Learning Research.
Mihai Surdeanu, Sanda M. Harabagiu, John Williams,
and John Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceedings
of (ACL-03).
Intentions, Implicatures and Processing of Complex Questions
Sanda M. Harabagiu and Steven J. Maiorano and Alessandro Moschitti and Cosmin A. Bejan
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
Abstract
In this paper we introduce two methods for
deriving the intentional structure of complex
questions. Techniques that enable the deriva-
tion of implied information are also presented.
We show that both the intentional structure
and the implicatures enabled by it are essen-
tial components of Q/A systems capable of suc-
cessfully processing complex questions. The
results of our evaluation support the claim that
there are multiple interactions between the pro-
cess of answer finding and the coercion of in-
tentions and implicatures.
1 Introduction
The Problem of Question Intentions.
When using a Question Answering system to find
information, the user cannot separate the intentions
and beliefs from the formulation of the question. A
direct consequence of this phenomenon is that the user
incorporates his or her intentions and beliefs into the
interrogation. For example, when asking the question:
Q1: What kind of assistance has North Korea received
from the USSR/Russia for its missile program?
the user associate with the question a number of in-
tentions, that maybe expressed a set of intended ques-
tions. Each intended question, in turn generates implied
information, that maybe expresses as implied questions.
For question Q1, a list of intended questions and implied
questions is detailed in Table1.
Most of the intended questions are similar with the
questions evaluated in TREC1. For example questions
1The Text REtrieval Conferences (TREC) are evaluation
workshops in which Information Retrieval tasks are annually
tested. Since 1999 the performance of question answering sys-
tems are measured in the TREC QA track.
Qi1, Qi2 and Qi3 are so-called definition questions, since
they ask about defining properties of an object. However
unlike the TREC definition questions, these questions ex-
press unstated intentions of the questioner and need to be
processed in the context of the original complex question
Q1. Questions Qi4 and Qi5 are factoid questions, request-
ing information about facts or events. Qi6 asks about the
source of information that enables the answers of ques-
tion Q1.
Questions Qi1, Qi2, Qi3, Qi4 and Qi5 result from the in-
tentional structure generated when processing question
Q1 or questions similar to it. When intended questions
are generated, their sequential processing (a) represents a
decomposition of the complex question and (b) generates
a scenario for finding information; thus questions like Q1
are also known as scenario questions.
Intentions and Implicatures.
As Table 1 suggests, the implied information takes the
form of alternatives that guide the answers to intended
questions. For example, question Qm11 lists alternatives
for the answer to Qi1 whereas Qm21 lists components of
the answer of Qi1. Implicatures may also involve tem-
poral inference, e.g. the implied questions pertaining
to Qi3 and Qi4. Additionally, the reliability of infor-
mation is commonly an implicature in the case of sce-
nario questions, since the causal and temporal inference
is based on the quality and correctness of the available
data sources. Neither intentions or implicatures are rec-
ognizable at syntactic or semantic level, but they both
play an important role in the question interpretation. In-
terpretations disregard the implied information or the user
intentions determine the extraction of incorrect answers,
thus influence the performance of Q/A systems.
Our solution.
In this paper we present two different mechanisms of
deriving the question implicatures. Both methods start
from the syntactic and semantic content of the interro-
gation. The first method considers only the semantic
Intended Questions Implied Questions
Qi1 :What is the USSR/Russia? Qm11 :Is this the Soviet/Russian government?
Qm12 :Does it include private firms, state-owned firms, educational
institutions, and individuals?
Qi2 :What is North Korea? Qm21 :Is this the North Korean government only?
Qm22 :Does it include private firms, state-owned firms, educational
institutions, and individuals?
Qi3 :What is assistance? Qm31 :Is it the transfer of complete missile systems, licensing agreements,
components, materials, or plans?
Qm32 :Is it the training of personnel?
Qm33 :What kind of training?
Qm34 :Does transfer include data, and, if so, what kind of data?
Qm35 :Does transfer include financial assistance, and, if so, what kind of
financial assistance?
Qi4 :What are the missiles in the North Qm41 :Are any based upon Soviet/Russian designs?
Korean inventory? Qm42 :If so, which ones?
Qm43 :What was the development timeline of the missiles?
Qm44 :Did any timeline differ significantly from others?
Qm45 :Did North Korea receive assistance from other sources besides
USSR/Russia to develop these missiles?
Qi5 :When did North Korea receive assistance Qm51 :Was any intended assistance halted, stopped or intercepted?
from the USSR/Russia?
Qi6 :What are the sources of information? Qm61 :Are the sources reliable?
Qm62 :Is some information contradictory?
Table 1: Question decomposition associated with question Q1
meaning of the words used in the question whereas the
second method considers the predicate-argument struc-
ture of the question and candidate answers as a form of
shallow semantics that enables the inference of the inten-
tional structure. Question implicatures are derived from
lexico-semantic paths retrieved from the WordNet lexico-
semantic database. These paths bring forward new con-
cepts, that may be associated with the question implica-
tures when testing the paths against the conversational
maxims introduced by Grice in (Grice, 1975a). For ex-
ample, if the user asks ?Will Prime Minister Mori survive
the crisis??, the first method detects the user?s belief that
the position of the Prime Minister is in jeopardy, since
the concept DANGER is coerced although none of the
question words directly imply it.
The second method generates the intentional structure
of the question, enabling a more structured representa-
tion of the pragmatics of question interpretation. The in-
tentional structure is based on a study that we have con-
ducted for capturing the motivations of a group of users
when asking series of questions in several scenarios. We
show how the intentional structures that we have gathered
guide the coercion of knowledge that helps to support the
acceptance of rejection of computational implicatures.
The derivation of intentional structures is made possi-
ble by predicate-argument structures that are recognized
both at the question level and at the candidate answer
level. In this paper we show how richer semantic ob-
jects can be derived around predicate-argument structures
and how inferential mechanisms can be associated with
such semantic objects for obtaining correct answers. The
rest of the paper is organized as follows. In Section 2
we describe several forms of complex questions that re-
quire the derivation of computational implicatures. Sec-
tion 3 details the models of Question Answering that we
considered and Section 4 shows our methods of deriving
predicate-argument structures and their usage in identi-
fying answers for questions. Section 5 details the inten-
tional structures whereas Section 6 summarizes the con-
clusions.
2 Question Complexity
Since 1999, the TREC QA evaluations focused on fac-
toid questions, such as ?In what year did Joe Di Maggio
compile his 56-game hitting streak?? or ?Name a film in
which Jude Law acted.?. The answers to most of these
questions belong to semantic categories associated with
each question class. For example, questions asking about
a date or a year can be answered because Named Entity
Recognizers identify a temporal expression in a candidate
text span. Similarly, names of people or organizations
are provided as answers to questions such as ?Who is the
first Russian astronaut?? or ?What is the largest software
company in the world??. Most Named Entity Recogniz-
ers detect names of PEOPLE, ORGANIZATIONS, LOCA-
TIONS, DATES, PRICES and NUMBERS. For factoid Q/A,
the list of name categories needs to be extended, as re-
ported in (Harabagiu et al, 2003) for recognizing many
What kind od assistance has North Korea received fromComplex Question:
What kind of assistance has X received from Y for Z?Question PATTERN:
X=North Korea FOCUS=Z=misile program
Intended Questions:
Definition Questions:
What is Y? "What is USSR/Russia?"
What is assistance? 
Elaboration of FOCUS:
Reliability: "What are the sources of information?"
the USSR/Russia for its missile program?
assistance from the USSR/Russia?"
(1) RESULTATIVE
(2) TEMPORAL
North Korean inventory?"
"When did North Korea receive
Y=USSR/Russia
What is X? "What is North Korea?"
"What are the missiles in the
Figure 1: Decomposition of scenario question into intended questions
more types of names, e.g. names of movies, names of
diseases, names of battles. Moreover, the semantic cat-
egories of the extended set of names need to be incor-
porated into an answer type taxonomy that enables the
recognition of (a) the expected answer type and (b) the
question class. The taxonomy of expected answer types
is useful because the answer is not always a name; it can
be a lexicalized concept or a concept that is expressed by
a paraphrase.
The TREC evaluations have also considered two more
classes of questions: (1) list questions and (2) definition
questions. The list questions have answers that are typ-
ically assembled from different documents. Such ques-
tions are harder to answer than factoid questions be-
cause the systems must detect duplications. Example of
list questions are ?Name singers performing the role of
Donna Elvira in performances of Mozart?s ?Don Gio-
vani?.? or ?What companies manufacture golf clubs??.
Definition questions require a different form of process-
ing that factoid questions because no taxonomy of answer
types needs to be used. The expected answer type is a def-
inition, which cannot be represented by a single concept.
Q/A systems assume that definitions are given by follow-
ing a set of linguistic patterns that need to be matched for
extracting the answer. Example of definition questions
are ?What is a golden parachute?? or ?What is ETA in
Spain??.
In (Echihabi and Marcu, 2003) a noisy channel model
for Q/A was introduced. This model is based on the
idea that if a given sentence SA contains an answer sub-
string A to a question Q, then SA can be re-written into
Q through a sequence of stochastic operators. Not only a
justification of the answer is produced, but the conditional
probability P(Q?SA) re-ranks all candidate answers.
A different viewpoint of Q/A was reported in (Itty-
cheriah et al, 2000). Finding the answers A to a ques-
tion Q was considered a classification problem that maxi-
mizes the conditional probability P(A?Q). This model is
not tractable currently, because (a) the search space is too
large for a text collection like the TREC or the AQUAINT
corpora; and (b) the training data is insufficient. There-
fore, Q/A is modeled by the distribution P(C?A,Q)
where C measures the ?correctness? of A to question
Q. By using a hidden variable E that represents the ex-
pected answer type, P(C?A,Q) = ?E p(C,E?Q,A) =
?E p(C?E,Q,A) * p(E?Q,A). Both distributions are
modeled by using the maximum entropy.
All three forms of questions are also useful when pro-
cessing complex questions, determined by a scenario re-
sulting from a problem-solving situation. As illustrated
in Figure 1, a scenario question may be associated with a
pattern. One of the pattern variables represents the focus
of the question. The notion of the question focus was first
introduced by (Lehnert, 1978). The focus represents the
most important concept of the question; a concept deter-
mining the domain of question. In the case of question
Q1, the focus is missile program. The identification of
the focus is based on the predicate-structure of the ques-
tion pattern and on the order of the arguments. Figure 3
shows both the question pattern associated with Q1 and
its predicate-argument structure. The argument with the
role of purpose is ranked highest, and thus it determines
the question focus.
With the exception of the focus, all arguments from
the predicate-argument structure may be used for gener-
ating definition questions. The focus is elaborated upon.
Several forms of elaborations are possible. One is a tem-
poral one, as illustrated in Figure 1. Other are resultative,
causative or manner-based. For example, the knowledge
that assistance in a missile program results in an inven-
When did North Korea receive from USSR/Russiaassistance
WRB VBD NNP NNP NNPVB NN
NPB
IN
PP
NPB
VP
NPB
SQ
WHADVP
SBARQ
OBJECTBENEFICIARY SOURCEDATE
when=DATE
Step 2(a): Binary Semantic Dependencies
receive North Korea USSR/Russia assistance
TypeExpected Answer
Step 2(b): Predicate?Argument Structures
Predicate: receive
Arguments: assistance=OBJECT
North Korea=BENEFICIARY
USSR/Russia=SOURCE
When=DATE=Expected Answer Type
Question: When did North Korea receive assistance from USSR/Russia?
Step 1: Syntactic Parse
Figure 2: Deriving the Expected Answer Type
Predicate?argument structure:
Predicate:
Arguments:
receive
Purpose: Z
Beneficiary: X Source: Y
Object: assistance
Question Pattern: What kind of assistance has X received
from Y for Z?
Figure 3: Predicate-argument structure
tory of missiles allows for resultative elaboration. Further
knowledge needs to be coerced for generating the implied
questions as possible follow-ups to intended questions.
The relationship between intended questions and im-
plied questions is marked by the presence of multiple
references, e.g. the pronouns it and this or any and
ones. The generation of implied questions is made pos-
sible by knowledge that is coerced from the intended
questions. For example, when asking Qi1 :?What is
the USSR/Russia?? the coercion process abstracts away
from the concept that needs to be defined, i.e. a coun-
try. The implied question requests confirmation of
the metonymy resolution involving USSR/Russia.This
named entity may represent a country but most likely it
refers to its government or, as Qm12 suggests, organiza-
tions or individuals acting on behalf of the country. Both
Qm11 and Qm12 , implied questions derived from the in-
tended question Qi1, refer to the metonymy by using the
pronouns this and it respectively. Different forms of coer-
cion are used for Qi3 because in this case the knowledge
is associated with the predicate. The implied questions
associated with the focus, i.e. the intended question Qi4,
coerce the design and development predicates which are
associated with the missiles as well as the timelines of
possible additional assistance.
3 Models of Question Answering
The processing of questions is typically performed as a
sequence of three processes: (1) Question Processing; (2)
Document Processing and (3) Answer Extraction. In the
case of factoid questions , question processing involves
the classification of questions with the purpose of pre-
dicting what semantic class the answer should belong
to. Thus we may have questions asking about PEOPLE,
ORGANIZATIONS, TIME or LOCATIONS. Since open-
domain Q/A systems process questions regardless of the
domain of interest, question processing must be based on
an extended ontology of answer types. The identification
of the expected answer type is based either on binary se-
mantic dependencies extracted from the syntactic parse of
the question (Harabagiu et al, 2001) or on the predicate-
argument structure of the question. In both cases, the re-
lation to the question stem (i.e. what, who, when) enables
the classification. Figure 2 illustrates a factoid question
generated as an intended question and the derivation of
its expected answer type.
However, many times the expected answer type needs
to be identified from an ontology that has high lexico-
semantic coverage. Many Q/A systems use the WordNet
database for this purpose. In contrast, definition ques-
tions do not require the identification of the expected an-
Answer Pattern:
Answer:
Question Pattern:
Question?Point, a Definition
has killed nearly 800 people since
taking up arms in 1968
for Basque Homeland and Freedom ?
ETA, a Basque language acronym
What is Question?Point in Country?
What is in SpainETA
NNP
NPB
IN
PP
NNP
NPB
NP
VBZ
SQ
WP
WHNP
SBARQ
Definition Question: What is ETA in Spain?
Question Parse:
Figure 4: Patterns for Processing Definition Questions
swer type, since they always request a definition. How-
ever, definition questions are matched against a set of pat-
terns, which enables the extraction of the definition from
the candidate answers. Figure 4 illustrates a definition
question, the pattern it matched as well as the extracted
answer.
Both factoid and definition questions can be answered
only if candidate passages are available. The retrieval of
these passages is made possible by keywords that are se-
lected from the question words. The Documents Process-
ing module implements a search engine that returns pas-
sages that are likely to contain the expected answer type
in the case of factoid questions or the definition pattern
in the case of definition questions. The answer extraction
module optimizes the extraction of the correct answer by
unifying the question information with the answer infor-
mation. The unification may be based on pattern match-
ing; on machine learning algorithms based on the ques-
tion and answer features or on abductive reasoning that
justifies the answer correctness.
Current state-of-the-art QA systems search for the can-
didate answer by assuming that the answers are single
concepts, that can be recognized from a hierarchy or by
a Named Entity Recognizer. This is a serious limitation,
but it works well for the factoid, list or definition ques-
tions evaluated in TREC.
The three modules of current Q/A systems reflect the
three functions that need to be considered by any Q/A
model: (1) understanding what the question asks; (2)
identify candidate text passage that might contain the an-
swer; and (3) the extraction of the correct answer. Cur-
rently, the expected answer type represents what ques-
tion asks about: a semantic concept, e.g. the name of a
person, location or organization, kinds of diseases, types
of animals or plants. Generally these semantic concepts
are lexicalized in a single word or in 2-word collocations.
Clearly, this represents a limitation, since often the ques-
tions ask for more than a single concept. As we have
seen in Table1, there is additional intended and implied
information that is requested. Therefore new models of
Question/Answering need to incorporate these additional
forms of knowledge.
When definition questions are processed in current
Q/A systems, they are matched against a pattern, which is
different from the question patterns associated with com-
plex questions similar to those illustrated in Figure 1. In
the case of a definition question like ?What is ETA in
Spain??, the pattern identifies the question-point (QP) as
ETA- the concept that needs to be defined and Spain as
its context. The definition question pattern also contains
several surface-form patterns that are matched in the can-
didate paragraphs. One such pattern is recognized in an
apposition, by [QP, a AP] where AP represents the an-
swer phrase. In the following passage:
?ETA, a Basque language acronym for Basque Homeland
and Freedom - has killed nearly 800 people since taking
up arms in 1968.?
the exact answer representing the definition is identified
in AP: Basque language acronym for Basque Homeland
and Freedom. The fact that Basque country is a region in
Spain allows a justification of the question context.
In this paper, by considering the intentional informa-
tion and the implied information that can be derived when
processing questions, we introduce a novel model of Q/A,
which has access to rich semantic structures and enables
the retrieval of more accurate answers as well as inference
processes that explain the validity and contextual cover-
age of answers.
Figure 5 shows the structure of the novel model of Q/A
we propose. Both Question Processing and Document
Processing have the recognition of predicate-argument
structures as a crux of their models. As reported in (Sur-
deanu et al, 2003), the recognition of predicate-argument
structures depends on features made available by full syn-
tactic parses and by Named Entity Recognizers. As we
shall show in this paper, the predicate-argument struc-
tures enable the recognition of question pattern, the ques-
tion focus and the intentional structure associated with
Question
Syntactic Parse Named EntityRecognition
Identification of
Predicate?Argument Structure
Structure
Recognition of Answer
based on extended
Indexing & Retrieval
lexico?semantic knowledge
Named EntityRecognitionSyntactic Parse
Intentional Structure
Identification ofPredicate?ArgumentStructures Question Pattern
Recognition of
Identification ofQuestion Focus
Recognition of Answer Structure
Keyword Extraction
Validation of Implied Information
Answer Structure
Recognition of
Recognition and extention
of intentional structure
Reference Resolution
Question Processing Answer ProcessingDocument Processing
Answer
Figure 5: Novel Question/Answering Architecture.
a question. When the intentions are known, the answer
structure can be identified and the keywords extracted.
For better retrieval of candidate answers, documents are
indexed and retrieved based on the predicate-argument
structures as well as on complex semantic structure asso-
ciated with different question patterns. Similarly, the in-
tentional structures are used for indexing/retrieving can-
didate passages. The Answer Processing function in-
volves the recognition of the answer structure and inten-
tional structure. Often this requires reference resolution.
The implied information coerced from both the question
and the candidate answer is also validated before decid-
ing on the answer correctness.
4 Predicate-Argument Structures
To identify predicate-argument structures in questions
and passages, we have: (1) used the Proposition Bank or
PropBank as training data; and (2) a mode for predicting
argument roles similar to the one employed by (Gildea
and Jurafsky, 2002).
PropBank is a one million word corpus annotated with
predicate-argument structures on top of the Penn Tree-
bank 2 Wall Street Journal texts. For any given predicate,
the expected arguments are labeled sequentially from Arg
0 to Arg 4. Generally, Arg 0 stands for agent, Arg 1 for
direct object or theme or patient, Arg 2 for indirect object
or benefactive or instrument or attribute or end state, Arg
3 for start point or benefactive or attribute and Arg4 for
end point. In addition to these core arguments, adjunc-
tative arguments are marked up. They include functional
tags from Treebank, e.g. ArgM-DIR indicates a direc-
tional, ArgM-LOC indicates a locative, and ArgM-TMP
stands for a temporal.
An example of PropBank markup is:
[Arg10 Analysts ] have been [predicate1 expecting ] [Arg11
a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 the
U.S. car maker ] [Arg21 an eventual 30% state in the British
Company ].
The model of identifying the arguments of each pred-
icate consists of two tasks: (1) the recognition of the
boundaries of each argument in the syntactic parse tree;
(2) the identification of the argument role. Each task can
be cast as a separate classifier. Next section describes
our approach based on Support Vector Machines (SVM)
(Vapnik, 1995).
4.1 Automatic Predicate-Argument extraction
Given a sentence in natural language, all the predicates
associated with its verbs have to be identified along with
their arguments. This problem can be divided in two sub-
tasks: (a) detection of the target argument boundaries,
i.e. all its compounding words, and (b) classification of
the argument type, e.g. Arg0 or ArgM.
A direct approach to learn both detection and classifi-
cation of predicate arguments is summarized by the fol-
lowing steps:
1. Given a sentence from the training-set, generate a
full syntactic parse-tree;
2. let P and A be the set of predicates and the set of
parse-tree nodes (i.e. the potential arguments), re-
spectively;
3. for each pair <p, a> ? P ?A:
? extract the feature representation set, Fp,a;
? if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
0.68
0.71
0.74
0.77
0.8
0.83
1 2 3 4 5
Polynomial Degree
(a)
F1
Arg0
Arg1
ArgM
0.65
0.68
0.71
0.74
0.77
0.8
1 2 3 4 5
Polynomial Degree
(b)
F1
Figure 6: Single classifiers and Multi-classifier performance for argument extraction.
The above T+ and T? sets can be re-organized as pos-
itive T+argi and negative T?argi examples for each argu-
ment i. In this way, an individual ONE-vs-ALL SVM
classifier for each argument i can be trained. We adopted
this solution as it is simple and effective (Pradhan et al,
2003). In the classification phase, given a sentence of the
test-set, all its Fp,a are generated and classified by each
individual SVM classifier. As a final decision, we select
the argument associated with the maximum value among
the scores provided by the SVMs2, i.e. argmaxi?S Ci,
where S is the target set of arguments.
The discovering of relevant features is a complex task.
Nevertheless there is a common consensus on the basic
features that should be adopted. These standard features,
first proposed in (Gildea and Jurafsky, 2002), are derived
from parse trees as illustrated by Table 2.
4.2 Parsing Sentence into Predicate Argument
Structures
For the experiments, we used PropBank
(www.cis.upenn.edu/?ace) along with Penn-
TreeBank3 2 (www.cis.upenn.edu/?treebank)
(Echihabi and Marcu, 2003). This corpus contains about
53,700 sentences and a fixed split between training and
testing which has been used in other researches (Gildea
and Jurafsky, 2002; Surdeanu et al, 2003; Hacioglu et
al., 2003; Chen and Rambow, 2003; Gildea and Hock-
enmaier, 2003; Gildea and Palmer, 2002; Pradhan et al,
2003). In this split, Sections from 02 to 21 are used for
training, section 23 for testing and sections 1 and 22 as
developing set. We considered all PropBank arguments
from Arg0 to Arg9, ArgA and ArgM even if only Arg0
from Arg4 and ArgM contain enough training/testing
2This is a basic method to pass from binary categorization
into a multi-class categorization problem; several optimization
have been proposed, e.g. (Goh et al, 2001).
3We point out that we removed from the Penn TreeBank the
special tags of noun phrases like Subj and TMP as parsers usu-
ally are not able to provide this information.
data to affect the global performance.
The classifier evaluations were carried out using
the SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/ with the de-
fault polynomial kernel according to a degree d ?
{1, 2, 3, 4, 5}. The performances were evaluated using
the F1 measure for both single argument classifiers and
the multi-class classifier.
- PHRASE TYPE (pt): This feature indicates the syntactic
type of the phrase labeled as a predicate argument.
- PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the argu-
ment phrase, expressed as a sequence of nonterminal labels
linked by direction (up or down).
- POSITION (pos) Indicates if the constituent appears be-
fore or after the predicate in the sentence.
- VOICE (voice) This feature distinguishes between active
or passive voice for the predicate phrase.
- HEAD WORD (hw) This feature contains the head word
of the evaluated phrase. Case and morphological informa-
tion are preserved.
- GOVERNING CATEGORY (gov) This feature applies to
noun phrases only, and it indicates if the NP is dominated by
a sentence phrase (typical for subject arguments with active
voice predicates), or by a verb phrase (typical for object
arguments).
- PREDICATE WORD In our implementation this feature
consists of two components: (1) VERB: the word itself
with the case and morphological information preserved; and
(2) LEMMA which represents the verb normalized to lower
case and infinitive form.
Table 2: Standard Features used in Predicate Argument
Extraction.
Figure 6 illustrates the F1 measures for the overall ar-
gument extraction task (i.e. identification and classifica-
tion) according to different polynomial degrees. Figure
6(a) illustrates the F1-performance of single classifiers
for the arguments Arg0, Arg1 and ArgM. Figure 6(b) il-
lustrates the performance for all the arguments (i.e. the
multi-classifier). In general, we were able to recognize
predicate argument structures with an F1-score of 80%.
4.3 Using Predicate-Argument Structures in
Question Answering.
Predicate-argument structures are useful for identifying
candidate answers. Since they recognize long-distance
dependencies between a predicate and one its arguments,
they enable (1) the identification of the exact boundaries
of an answer; and (2) they unify the predicate-argument
relation sought by question with those recognized in can-
didate passages.
Moreover, they are very useful in situations when the
expected answer type of the question could not be recog-
nized. There are two causes when the expected answer
type cannot be identified:
Case1: the answer class is a name that cannot be correctly
classified by an available Named Entity Recognizer, be-
cause its class name is not encoded.
Case2: the answer class cannot be found in the Answer
Type hierarchy. The example from Figure 7 shows an in-
stance of case 1. In this figure, the TREC question Q2054
has a predicate that can be unified with PREDICATES
from the answer passage. The Arg1 of the predicate is
the expected answer, which is identified as ?the Declara-
tion of Independence?. The Arg0 in the question is But-
ton Gwinnett, whereas in the answer, it is underspecified,
and should be resolved to who. This relative pronoun has
Button Gwinnett as one of its antecedents.
In Figure 8 the second case is illustrated. The question
asked about the first argument of the predicate ?measure?,
when its Arg2 = ?a theodolite?. In the answer, Predicate
2, with its infinite form, has as Arg 2 the same ?theodo-
lite?. However, the predicates are lexicalized by different
verbs. In WordNet, the first sense of the verb ?measure?
as the verb ?determine? as a hypernym, therefore Arg1 =
?wind speeds? is the correct answer.
5 Intentional Structures
The correct interpretation of many questions requires
the inference of implicit information, that is not directly
stated in the question, but merely implied. The mecha-
nisms of recognizing the intentions of the questioner are
helpful means of identifying the implied information. For
example, in the question QI :?Will Prime Minister Mori
survive the crisis??, the user does not literally mean ?Will
Prime Minister Mori be still alive when the political cri-
sis is over ??, but rather (s)he implies her/his belief that
the current political crisis might cost the Japanese Prime
Minister his job. It is very unlikely that any expert knowl-
edge base covering Japanese politics will encode knowl-
edge covering all situations of political crisis and the pos-
sible outcomes of the prime minister. However, this prag-
matic knowledge is essential for the correct interpretation
of the question.
Q2054:
Answer: Button Gwinnett, George Walton and Lyman Hall were
Georgians who could have been hanged as traitors for
What document did Button Gwinnett sign on the upper left
hand side?
signing the Declaration of Independence on July 4, 1776.
Predicate?argument structure:
PREDICATE1: were
ARG1(PREDICATE1): Georgians
who
PREDICATE2: could have been hanged
ARG2(PREDICATE2): as traitors
PREDICATE3: signing
ARG1(PREDICATE3): The Declaration of Independence
ARGM?LOC(PREDICATE3): on July 4, 1776
ARG0: Button Gwinnett, George Walton and Lyman Hall
ARGM?LOC: on the upper left hand side
ARG0: Button Gwinnett
PREDICATE: sign
ARG1: What document Question Type
Predicate?argument structure:
Figure 7: Answer extraction from predicate-argument struc-
tures: Case1
measurePREDICATE:
What does a theodolite measure?
Predicate?argument structure:
ARG1: What
ARG2: a theodolite
Answer: The theodolite ? a 1940s gadget, no longer in production,
wind speeds.
that uses a helium balloon and trigonometry to determine
Predicate?argument structure:
a 1940s gadget
that
PREDICATE1: uses
PREDICATE2: to determine
ARG1(PREDICATE2): wind speeds
Q2145:
ARG1(PREDICATE1): The theodolite
ARG2(PREDICATE1): a helium balloon and trigonometry
Figure 8: Answer extraction from predicate-argument struc-
tures: Case 2
The design of advanced Question&Answering systems
capable of grasping the intention of a professional analyst
when (s)he poses a question depends both on the knowl-
edge of the domain referred by the question as well as on
a variety of rules and conventions that allow the commu-
nication of intentions and beliefs in addition to the literary
meaning of the question. Access to domain knowledge is
granted by a combination of retrieval mechanisms that
bring forward relevant document passages from unstruc-
tured collections of documents, specialized knowledge
Text Information RetrievalEngine
QUERY: Prime & Minister & Mori &DANGER (word)
Japanese Factual PoliticsKnowledge Base
Text Retrieval
"vote of non-confidence against 
Prime Minister Mori"
political crisis
survive crisis
adversity DANGER
WordNet 1.6
resignation
removal
strike
vote
vote of non-confidence =
   DANGER(Position)
continue in existence
Question: Will Prime Minister Mori survive the political crisis ?
DANGER ( Prime Minister Mori continues in Position)
Figure 9: Intentional Structure derived from Lexico-Semantic Knowledge.
bases and/or database access mechanisms. The research
proposed in this project focuses on the derivation and us-
age of pragmatic knowledge that supports the recognition
of question implications, also known as implicatures (cf.
(Grice, 1975b)).
5.1 Intentional structures Derived from
Lexico-Semantic Knowledge
The novel idea of this research is to link computa-
tional implicatures, similar to those defined by Grice
(Grice, 1975b), to inferences that can be drawn from
general lexico-semantic knowledge bases such as Word-
Net of FrameNet. Incipient work was described in
(Sanda Harabagiu and Yukawa, 1996), where a method
of using lexico-semantic path for recognizing textual im-
plicatures was presented. To our knowledge, this is the
only computational model of implicatures that was de-
veloped and tested on a large lexico-semantic knowledge
base (e.g. WordNet), enabling successful recognition of
implicatures.
The model proposed in (Sanda Harabagiu and Yukawa,
1996) uncovered a relationship between (a) the coherence
of a text segment; (b) its cohesion expressed by the lexical
paths and (c) the implicatures that can be drawn, mostly
to account for pragmatic knowledge. This relationship
can be extended across documents and across topics, to
learn patterns of textual and Q&A implicatures and the
methods of deriving knowledge that enables their recog-
nition.
The derivation of pragmatic knowledge combines in-
formation from three different sources:
(1) lexical knowledge bases (e.g. WordNet),
(2) expert knowledge bases that can be rapidly formatted
for many domains (e.g. Japanese political knowledge);
and
(3) knowledge supported from the textual information
available from documents. The methodology of combin-
ing these three sources of information is novel.
For question QI , the starting point is the concept iden-
tified as a cue for the expected answer type through meth-
ods described in (Harabagiu et al, 2000). This con-
cept is lexicalized by the verb-object pair survive-crisis.
Verb survive has four distinct senses in the WordNet 1.6
database, whereas noun crisis has two senses. The poly-
semy of the expected answer type increases the difficulty
of the derivation of pragmatic knowledge, but it does not
presupposes the word sense disambiguation of the ex-
pression. The information available in the glosses defin-
ing the WordNet synsets provides helpful information for
expanding the multi-word term defining the expected an-
swer type. By measuring the similarity between the two
senses of the noun crisis and the words encountered as
objects or prepositional attachments in the glosses of the
various senses of the verb survive, we distinguish the
noun adversity and the example cancer as expressing the
closest semantic orientation to the first sense of noun cri-
sis. The similarity is measured by counting the number
of common hypernyms and gloss concepts of hypernyms
of two synsets. Figure 9 illustrates the concepts related
to the question QI , as derived from WordNet lexico-
semantic knowledge base.
The fact that surviving a political crisis has a dangerous
component, indicated by the noun adversity, may also be
supported by inferences drawn from an expert knowledge
base, showing that a political crisis may be dangerous for
political figures in power. However, at this point, the ob-
ject of the dangerous situation is not specified. But sev-
eral concepts indicating dangerous political situations can
be inferred from the expert knowledge base and used in
the query for text evidence. Only when text passages in-
volving Prime Minister Mori are retrieved, clarifications
of the situation are brought to attention: a vote of non-
confidence against the prime minister is considered. This
new information helps inference from the expert knowl-
edge base. The expert knowledge base modeling the
 Intentional Structure of Questions
0* Evidence (     1-possess      (          2-Iraq,      3-biological weapons   ))
4* Means of finding (0)
a. reports 
b. inspections
c. assessments ? patterns of inspections
5* Source (0)
a. authority
b. reliability ? may, would
6* Consequence (0)
a. Enablement
b. Hiding/Presenting finding evidence 
Question: Does Iraq   have biological weapons  ?
x                           y
: have( Iraq, biological weapons )
Predicate-
Argument
Structure
Question
pattern
Does x have y ?
possess (x, y) 
Topic (3)
biological  weapons
a. Types of topic
b. Components
- chemical agents
- mustard gas, VX, sarin
c. Usage
- rockets, artillery shells
a. discover(1,2,3)
b. stockpile(2,3)
c. use(2,3)
d. 1-possess
a. develop(2,3)
b. acquire(2,3)
a. inspections( _,2,3)
b. ban( _,2,3)
Source/      fact/          reliability
reporter     evidence
5.a              0              5.b
coercion
Structure
Figure 10: Intentional structure derived from predicate-argument structures.
Japanese factional politics confirms that this is a danger-
ous situation for the Prime Minister and that in fact his
position is in jeopardy. Due to this inference from the
expert knowledge base, the concept POSITION replaces
noun existence from the gloss of the second sense of verb
survive, and the pragmatic knowledge required for the in-
terpretation of the implicature is assembled:
The interactions between the three information sources
derives the pragmatic knowledge on which relies the im-
plication of the question. The user had an inherent belief
that Prime Minister Mori might be replaced, and (s)he
queries the Q&A system not only to find information but
also to find support for his/her belief. The intentional
structure is represented as a set of concepts and the re-
lations that span them, as illustrated in Figure 9.
5.2 Coercion of Intentions
A second method of deriving the intentional structure of a
question is based on the predicate-argument structure that
is derived from the question and the candidate answers.
Figure 10 illustrates the Intentional Structure of one
such question. The structure of the intentions is deter-
mined by the predicate-argument structure of the ques-
tion and by its pattern. Generally, when asking whether
X posses Y, we want to find (1) evidence of this fact;
(2) we explore different means of finding the informa-
tion; (3) we are interested in the source of information
and (4) the enablers or inhibitors of finding the informa-
tion as well as the consequences of knowing it are of in-
terest. We assign a different index to each object from
the predicate-argument structure, and do the same for
each element of the intentional structure. For instance,
in Figure 2, source(0) is interpreted as source(index=0)
= source(evidence). Another feature of the intentional
structure is determined by the coercions that are associ-
ated with both forms of indexed objects. For example,
the coercion of evidence shows the most typical ways
of finding evidence in the context of the topic of the
question. Figure 2 lists such possibilities as (a) discov-
ering, (b) stockpiling, (c) using and even (d) possess-
ing. These possibilities are inserted in the context of the
topic, since they make use of the indexes for associat-
ing meaning to their representations. In fact, option (a)
discover(1,2,3) reads as discover(index=1, index=2, in-
dex=3) =discover(possesses(Iraq, biological weapons)).
Whereas option (b) stockpile(2,3) can be similarly inter-
preted as stockpile(Iraq, biological weapons). Note that
one of the indexed objects is the topic. The structure of
the topic is define along three semantic dimensions: (1)
hyponyms or examples of other types of the same cate-
gory as the topic; (2) the meronyms or components; and
(3) the functionality or the usage. The derivation of such
a large set of intentional structures helped us learn how
to coerce pragmatic knowledge. We have developed a
probabilistic approach extending the metonymy work of
(Lapata and Lascarides, 2003).
Lapata and Lascarides report a model of interpretation
of verbal metonymy as the point distribution P (e, o, v) of
three variables: the metonymy verb v, its object, and the
sought after interpretation i. For example a verb ? ob-
ject relation that needs to be metonymycally interpreted,
is enjoy ? movie. In this case v = enjoy, o = movie
and i ? {making, watching, directing}. The variables
of the distribution re ordered as <i, v, o> to help factor-
ing P (i, v, o) = P (i) ? P (v|i) ? P (o|i, v). Each of the
probabilities P (i), P (v|i) and P (o|i, v) can be estimated
using maximum likelihood. As it is illustrated in Fig-
ure 10, we have extended this model to account for: (1)
coercion of topic information; (2) coercion of evidence
of a fact; (3) interpretation of predicate and (4) inter-
pretation of arguments. Since the verb ? object rela-
tion translates in one of the predicate-argument relations,
we have coerced the predicate interpretations in the same
way as (Lapata and Lascarides, 2003), but we allowed
for any predicate-argument relation. Argument coercions
were produced by searching the most likely predicates
that used the same arguments. The topic model also in-
corporated topic signatures, similar to these reported in
(E.H. Hovy and Ravichandran, 2002).
6 Conclusions
In this paper we have described the problem of interpret-
ing the question intentions and proposed two methods of
generating the intentional structure of questions. The first
method is based on lexico-semantic chains between con-
cepts that are related to the question. The second method
generates intentional structures by using the predicate-
argument structures of questions and the topic represen-
tation of questions. To derive both forms of intentional
structures, we have relied on information available from
WordNet and on the parsing of questions and answers
in predicate-argument structures. Our experiments show
that the intentional structure may determine a different in-
terpretation of the question, and thus different keywords
can be used to retrieve the answers. Answer extraction
also depends on the semantic relations between the co-
erced interpretations of predicates and arguments. By
selecting a set of 100 questions for test, we have eval-
uated the correctness of the extracted answers when (1)
no intentional knowledge was coerced; (2) implicatures
were derived from lexico-semantic knowledge and (3)
intentional structures were derived based on predicate-
argument structures. An increase of 8structures and one
of 22the impact of each element of the intentional struc-
ture on the Q/A processing.
References
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In Pro-
ceedings of the 41st Annual Meeting of the ACL, Sap-
poro, Japan.
Chin-Yew Lin E.H. Hovy, U. Hermjakob and Deepak
Ravichandran. 2002. Using knowledge to facilitate
pinpointing of factoid answers. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING 2002).
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):254?288.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
King-Shy Goh, Edward Chang, and Kwang-Ting Cheng.
2001. SVM binary classifier ensembles for image clas-
sification. In Proceedings of the tenth international
conference on Information and knowledge manage-
ment, pages 395?402.
Paul H. Grice. 1975a. Logic and conversation. In P. Cole
and New York J.L. Morgan (ed.), Academic Press, edi-
tors, Syntax and Sematics Vol.3:Speech Acts, pages 41?
58.
Paul J. Grice. 1975b. Syntax and Semantics Vol.3:Speech
Acts. P. Cole and J. Morgan, editors.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing
using support vector machines. Technical report.
Sanda Harabagiu, Marius Pas?ca, and Steven Maiorano.
2000. Experiments with open-domain textual question
answering. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING-
2000), pages 292?298, Saarbrucken, Germany,.
Sanda M. Harabagiu, Dan I. Moldovan, Marius Pasca,
Rada Mihalcea, Mihai Surdeanu, Razvan C. Bunescu,
Roxana Girju, Vasile Rus, and Paul Morarescu. 2001.
The role of lexico-semantic feedback in open-domain
textual question-answering. In Meeting of the ACL,
pages 274?281.
S. Harabagiu, D. Moldovan, C. Clark, M. Bodwen,
J. Williams, and J. Bensley. 2003. Answer mining by
combining extraction techniques with abductive rea-
soning. In Notebook of the Twelveth Text REtrieval
Converence (TREC-2003), pages 46?53.
A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi.
2000. IBM?s statistical question answering system.
In Proceedings of the 9th Text REtrieval Conference,
Gaithersburg, MD.
T. Joachims. 1999. T. Joachims, Making large-Scale
SVM Learning Practical. In B. Scho?lkopf and C.
Burges and A. Smola (ed.), MIT-Press., editor, Ad-
vances in Kernel Methods - Support Vector Learning.
Maria Lapata and Alex Lascarides. 2003. A probabilistic
account of logical metonymy. Computational Linguis-
tics, 29:2:263?317.
Wendy Lehnert. 1978. The process of question answer-
ing. In Lawrence Erlbaum Assoc., Hillsdale.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured
text. In Proceedings of the International Conference
on Data Mining (ICDM-2003).
Dan Moldovan Sanda Harabagiu and Takashi Yukawa.
1996. Testing gricean constraints on a wordnet-based
coherence evaluation system. In Working Notes of the
AAAI-96 Spring Symposium on Computational Impli-
cature, Stanford, CA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of the
41th Annual Conference of the Association for Compu-
tational Linguistics (ACL-03), pages 8?15.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
A Novel Approach to Focus Identification in Question/Answering Systems
Alessandro Moschitti and Sanda Harabagiu
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
alessandro.moschitti@utdallas.edu
sanda@utdallas.edu
Abstract
Modern Question/Answering systems rely on
expected answer types for processing ques-
tions. The answer type is a semantic cate-
gory provided by Named Entity recognizer or
by semantic hierarchies. We argue in this pa-
per that Q/A systems should take advantage
of the topic information by exploiting several
models of question and answer categorization.
The matching of the question category with the
answer category allows the system to filter out
many incorrect answers.
1 Introduction
One method of retrieving information from vast docu-
ment collections is by using textual Question/Answering.
Q/A is an Information Retrieval (IR) paradigm that re-
turns a short list of answers, extracted from relevant doc-
uments, to a question formulated in natural language. An-
other, different method to find the desired information is
by navigating along subject categories assigned hierar-
chically to groups of documents, in a style made popular
by Yahoo.com among others. When the defined category
is reached, documents are inspected and the information
is eventually retrieved.
Q/A systems incorporate a paragraph retrieval engine,
to find paragraphs that contain candidate answers, as re-
ported in (Clark et al, 1999; Pasca and Harabagiu, 2001).
To our knowledge no information on the text categories of
these paragraphs is currently employed in any of the Q/A
systems. Instead, another semantic information, such as
the semantic classes of the expected answers, derived
from the question processing, is used to retrieve para-
graphs and later to extract answers. Typically, the se-
mantic classes of answers are organized in hierarchical
ontologies and do not relate in any way to the categories
associated with documents.
The ontology of expected answer classes contains
concepts like PERSON, LOCATION or PRODUCT,
whereas categories associated with documents are more
similar to topics than concepts, e.g., acquisitions, trading
or earnings. Given that text categories indicate different
semantic information than the classes of the expected an-
swers, we argue in this paper that text categories can be
used to improve the quality of textual Q/A. In fact, by as-
signing text categories to both questions and answers, we
have additional information on their similarity, which al-
lows systems to perform a first level of word disambigua-
tion. For example, if a user asks about the Apple charac-
teristics, two type of answers may be retrieved: (a) about
the apple company and (b) related to the agricultural do-
main. Instead, if the computer subject is selected, only
the answers involving the Apple company will be consid-
ered. Thus, topic categories allows Q/A systems to detect
the correct focus and consequently filter out many incor-
rect answers.
In order to assign categories to questions and answers,
the set of documents, on which the Q/A systems oper-
ate, has to be pre-categorized. For our experiments we
trained our basic Q/A system on the well-known text cat-
egorization benchmark, Reuters-21578. This allows us to
assume as categories of an answer the categories of the
documents, which contain such answer. More difficult,
instead, is assigning categories to questions as: (a) they
are not known in advance and (b) their reduced size (in
term of number of words) often prevents the detection of
their categories.
The article is organized as follows: Section 2 describes
our Q/A system whereas Section 3 shows the question
categorization problem and the solutions adopted. Sec-
tion 4 presents the filtering and the re-ranking methods
that combine the basic Q/A with the question classifica-
tion models. Section 5 reports the experiments on ques-
tion categorization, basic Question Answering and Ques-
tion Answering based on Text Categorization (TC). Fi-
nally, Section 6 derives the conclusions.
2 Textual Question Answering
The typical architecture of a Q/A system is illustrated in
Figure 1:
First the target question is processed to derive (a) the
semantic class of the expected answer and (b) what key-
words constitute the queries used to retrieve relevant
paragraphs. Question processing relies on external re-
sources to identify the class of the expected answer, typ-
ically in the form of semantic ontologies (Answer Type
Ontology).
Second, the semantic class of the expected answer is
later used to (1) filter out paragraphs that do not contain
any word that can be cast in the same class as the expected
answer, and (2) locate and extract the answers from the
paragraphs. Finally, the answers are extracted and ranked
based on their unification with the question.
Question Query Relevant Passag s Answer 
Answer Type Ontologies 
Semantic Class of expected Answers 
 Question Processing Paragraph Retrieval Answer extraction and formulation 
Document Collection 
Figure 1: Architecture of a Q/A system.
2.1 Question Processing
To determine what a question asks about, several forms of
information can be used. Since questions are expressed
in natural language, sometimes their stems, e.g., who,
what or where indicate the semantic class of the expected
answer, i.e. PERSON, ORGANIZATION or LO-
CATION, respectively. To identify words that belong
to such semantic classes, Name Entity Recognizers are
used, since most of these words represent names. Name
Entity (NE) recognition is a natural language technology
that identifies names of people, organizations, locations
and dates or monetary values.
However, most of the time the question stems are ei-
ther ambiguous or they simply do not exist. For example,
questions having what as their stem may ask about any-
thing. In this case another word from the question needs
to be used to determine the semantic class of the expected
answer. In particular, the additional word is semanti-
cally classified against an ontology of semantic classes.
To determine which word indicates the semantic class
of the expected answer, the syntactic dependencies1 be-
tween the question words may be employed (Harabagiu
1Syntactic parsers publicly available, e.g., (Charniak, 2000;
et al, 2000; Pasca and Harabagiu, 2001; Harabagiu et al,
2001).
Sometimes the semantic class of the expected answers
cannot be identified or is erroneously identified causing
the selection of erroneous answers. The use of text clas-
sification aims to filter out the incorrect set of answers
that Q/A systems provide.
2.2 Paragraph Retrieval
Once the question processing has chosen the relevant
keywords of questions, some term expansion techniques
are applied: all nouns and adjectives as well as morpho-
logical variations of nouns are inserted in a list. To find
the morphological variations of the nouns, we used the
CELEX (Baayen et al, 1995) database. The list of ex-
panded keywords is then used in the boolean version of
the SMART system to retrieve paragraphs relevant to the
target question. Paragraph retrieval is preferred over full
document retrieval because (a) it is assumed that the an-
swer is more likely to be found in a small text containing
the question keywords and at least one other word that
may be the exact answer; and (b) it is easier to process
syntactically and semantically a small text window for
unification with the question than processing a full docu-
ment.
2.3 Answer Extraction
The procedure for answer extraction that we used is re-
ported in (Pasca and Harabagiu, 2001), it has 3 steps:
Step 1) Identification of Relevant Sentences:
The Knowledge about the semantic class of the expected
answer generates two cases: (a) When the semantic class
of the expected answers is known, all sentences from each
paragraph, that contain a word identified by the Named
Entity recognizer as having the same semantic classes as
the expected answers, are extracted. (b) The semantic
class of the expected answer is not known, all sentences,
that contain at least one of the keywords used for para-
graph retrieval, are selected.
Step 2) Sentence Ranking:
We compute the sentence ranks as a by product of sorting
the selected sentences. To sort the sentences, we may use
any sorting algorithm, e.g., the quicksort, given that we
provide a comparison function between each pair of sen-
tences. To learn the comparison function we use a sim-
ple neural network, namely, the perceptron, to compute
a relative comparison between any two sentences. This
score is computed by considering four different features
for each sentence as explained in (Pasca and Harabagiu,
2001).
Step 3) Answer Extraction:
We select the top 5 ranked sentences and return them as
Collins, 1997), can be used to capture the binary dependencies
between the head of each phrase.
answers. If we lead fewer than 5 sentences to select from,
we return all of them.
Once the answers are extracted we can apply an addi-
tional filter based on text categories. The idea is to match
the categories of the answers against those of the ques-
tions. Next section addresses the problem of question and
answer categorization.
3 Text and Question Categorization
To exploit category information for Q/A we categorize
both answers and questions. For the former, we define as
categories of an answer a the categories of the document
that contain a. For the latter, the problem is more critical
as it is not clear what can be considered as categories of
a question.
To define question categories we assume that users
have a specific domain in mind when they formulate
their requests. Although, this can be considered a strong
assumption, it is verified in practical cases. In fact, to
formulate a sound question about a topic, the questioner
needs to know some basic concepts about that topic. As
an example consider a random question from TREC-92:
"How much folic acid should an expectant
mother get daily?"
The folic acid and get daily concepts are related to
the expectant mother concept since medical experts
prescribe such substance to pregnant woman with a
certain frequency. The hypothesis that the question
was generated without knowing the relations among
the above concepts is unlikely. Additionally, such
specific relations are frequent and often they characterize
domains. Thus, the user, by referring to some relations,
automatically determines specific domains or categories.
In summary, the idea of question categorization is: (a)
users cannot formulate a consistent question on a domain
that do not know, and (b) specific questions that express
relation among concepts automatically define domains.
It is worth noting that the specificity of the questions
depends on the categorization schemes which documents
are divided in. For example the following TREC ques-
tion:
"What was the name of the first Russian
astronaut to do a spacewalk?"
may be considered generic, but if a categorization
scheme includes categories like Space Conquest History
or Astronaut and Spaceship the above question is clearly
specific on the above categories.
The same rationale cannot be applied to very short
questions like: Where is Belize located?, Who
2TREC-9 questions are available at http://
trec.nist.gov/qa questions 201-893.
invented the paper clip? or How far away is
the moon? In these cases we cannot assume that a
question category exists. However, our aim is to provide
an additional answer filtering mechanism for stand-alone
Q/A systems. This means that when question categoriza-
tion is not applicable, we can deactivate such a mecha-
nism.
The automatic models that we have study to classify
questions and answers are: Rocchio (Ittner et al, 1995)
and SVM (Vapnik, 1995) classifiers. The former is a very
efficient TC that can be used for real scenario applica-
tions. This is a very appealing property considering that
Q/A systems are designed to operate on the web. The
second is one of the best figure TC that provides good
accuracy with a few training data.
3.1 Rocchio and SVM Text Classifiers
Rocchio and Support Vector Machines are both based on
the Vector Space Model. In this approach, the document
d is described as a vector ~d =<wdf1 , .., wdf|F |> in a |F |-
dimensional vector space, where F is the adopted set of
features. The axes of the space, f1, .., f|F | ? F , are the
features extracted from the training documents and the
vector components wdfj ? < are weights that can be eval-
uated as described in (Salton, 1989).
The weighing methods that we adopted are based on
the following quantities: M , the number of documents in
the training-set, Mf , the number of documents in which
the features f appears and ldf , the logarithm of the term
frequency defined as:
ldf =
{ 0 if odf = 0
log(odf ) + 1 otherwise
(1)
where, odf are the occurrences of the features f in the
document d (TF of features f in document d).
Accordingly, the document weights is:
wdf =
ldf ? IDF (f)??
r?F (ldr ? IDF (r))2
where the IDF (f) (the Inverse Document Frequency) is
defined as log( MMf ).
Given a category C and a set of positive and negative
examples, P and P? , Rocchio and SVM learning algo-
rithms use the document vector representations to derive
a hyperplane3, ~a ? ~d + b = 0. This latter separates the
documents that belong to C from those that do not be-
long to C in the training-set. More precisely, ?~d positive
examples (~d ? P ), ~a ? ~d + b ? 0, otherwise (~d ? P? )
~a ? ~d + b < 0. ~d is the equation variable, while the gra-
dient ~a and the constant b are determined by the target
learning algorithm. Once the above parameters are avail-
able, it is possible to define the associated classification
3The product between vectors is the usual scalar product.
function, ?c : D ? {C, ?}, from the set of documents D
to the binary decision (i.e., belonging or not to C). Such
decision function is described by the following equation:
?c(d) =
{
C ~a? ~d+ b ? 0
? otherwise (2)
Eq. 2 shows that a category is accepted only if the product
~a ? ~d overcomes the threshold ?b. Rocchio and SVM
are characterized by the same decision function4. Their
difference is the learning algorithm to evaluate the b and
the~a parameters: the former uses a simple heuristic while
the second solves an optimization problem.
3.1.1 Rocchio Learning
The learning algorithm of the Rocchio text classifier is
the simple application of the Rocchio?s formula (Eq. 3)
(Rocchio, 1971). The parameters ~a is evaluated by the
equation:
~af = max
{
0, 1|P |
?
d?P
wdf ?
?
|P? |
?
d?P?
wdf
}
(3)
where P is the set of training documents that belongs to
C and ? is a parameter that emphasizes the negative in-
formation. This latter can be estimated by picking-up the
value that maximizes the classifier accuracy on a train-
ing subset called evaluation-set. A method, named the
Parameterized Rocchio Classifier, to estimate good pa-
rameters has been given in (Moschitti, 2003b).
The above learning algorithm is based on a simple and
efficient heuristic but it does not ensure the best separa-
tion of the training documents. Consequently, the accu-
racy is lower than other TC algorithms.
3.1.2 Support Vector Machine Learning
The major advantage of SVM model is that the param-
eters ~a and b are evaluated applying the Structural Risk
Minimization principle (Vapnik, 1995), stated in the sta-
tistical learning theory. This principle provides a bound
for the error on the test-set. Such bound is minimized if
the SVMs are chosen in a way that |~a| is minimal. More
precisely the parameters ~a and b are a solution of the fol-
lowing optimization problem:
?
?
?
Minimize |~a|
~a? ~d+ b ? 1 ?d ? P
~a? ~d+ b < ?1 ?d ? P?
(4)
It can be proven that the minimum |~a| leads to a maxi-
mal margin5 (i.e. distance) between negative and positive
examples.
4This is true only for linear SVM. In the polynomial version
the decision function is a polynomial of support vectors.
5The software to carry out both the learning and clas-
sification algorithm for SVM is described in (Joachims,
1999) and it can be downloaded from the web site
http://svmlight.joachims.org/.
In summary, SVM provides a better accuracy than
Rocchio but this latter is better suited for real applica-
tions.
3.2 Question Categorization
In (Moschitti, 2003b; Joachims, 1999), Rocchio and
SVM text classifiers have reported to generate good ac-
curacy. Therefore, we use the same models to classify
questions. These questions can be considered as a partic-
ular case of documents, in which the number of words is
small. Due to the small number of words, a large collec-
tion of questions needs to be used for training the clas-
sifiers when reaching a reliable statistical word distribu-
tion. Practically, large number of training questions is not
available. Consequently, we approximate question word
statistics using document statistics and we learn question
categorization functions on category documents.
We define for each question q a vector ~q =
<wq1, .., wq|Fq|>, where w
q
i ? < are the weights associ-
ated to the question features in the feature set Fq , e.g.
the set of question words. Then, we evaluate four differ-
ent methods computing the weights of question features,
which in turn determine five models of question catego-
rization:
Method 1: We use lqf , the logarithm (evaluated simi-
larly to Eq. 1) of the word frequency f in the question
q, together with the IDF derived from training documents
as follows:
wqf =
lqf ? IDF (f)??
r?Fq (l
q
r ? IDF (r))2
(5)
This weighting mechanism uses the Inverse Document
Frequency (IDF) of features instead of computing the In-
verse Question Frequency. The rationale is that ques-
tion word statistics can be estimated from the word doc-
ument distributions. When this method is applied to the
Rocchio-based Text Categorization model, by substitut-
ing wdf with wqf we obtain a model call the RTC0 model.
When it is applied to the SVM model, by substituting wdf
with wqf , we call it SVM0.
Method 2: The weights of the question features are
computed by the formula 5 employed in Method 1, but
they are used in the Parameterized Rocchio Model (Mos-
chitti, 2003b). This entails that ? from formula 3 as well
as the threshold b are chosen to maximize the catego-
rization accuracy of the training questions. We call this
model of categorization PRTC.
Method 3: The weights of the question features are
computed by formula 5 employed in Method 1, but they
are used in an extended SVM model, in which two ad-
ditional conditions enhance the optimization problem ex-
pressed by Eq. 4. The two new conditions are:
?
?
?
Minimize |~a|
~a? ~q + b ? 1 ?q ? Pq
~a? ~q + b < ?1 ?q ? P?q
(6)
where Pq and P?q are the set of positive and negative ex-
amples of training questions for the target category C.
We call this question categorization model QSVM.
Method 4: We use the output of the basic Q/A system
to assign a category to questions. Each question has as-
sociated up to five answer sentences. In turn, each of the
answers is extracted from a document, which is catego-
rized. The category of the question is chosen as the most
frequent category of the answers. In case that more than
one category has the maximal frequency, the set of cat-
egories with maximal frequency is returned. We named
this ad-hoc question categorization method QATC (Q/A
and TC based model).
4 Answer Filtering and Re-Ranking Based
on Text Categorization
Many Q/A systems extract and rank answers successfully,
without employing any TC information. For such sys-
tems, it is interesting to evaluate if TC information im-
proves the ranking of the answers they generate. The
question category can be used in two ways: (1) to re-rank
the answers by pushing down in the list any answer that
is labeled with a different category than the question; or
(2) to simply eliminate answers labeled with categories
different than the question category.
First, a basic Q/A system has to be trained on docu-
ments that are categorized (automatically or manually)
in a predefined categorization scheme. Then, the target
questions as well as the answers provided by the basic
Q/A system are categorized. The answers receive the
categorization directly from the categorization scheme,
as they are extracted from categorized documents. The
questions are categorized using one of the models de-
scribed in the previous section. Two different impacts
of question categorization on Q/A are possible:
? Answers that do not match at least one of the cate-
gories of the target questions are eliminated. In this
case the precision of the system should increase if
the question categorization models are enough accu-
rate. The drawback is that some important answers
could be lost because of categorization errors.
? Answers that do not match the target questions (as
before) get lowered ranks. For example, if the first
answer has categories different from the target ques-
tion, it could shift to the last position in case of all
other answers have (at least) one category in com-
mon with the question. In any case, all questions
will be shown to the final users, preventing the lost
of relevant answers.
An example of the answer elimination and answer re-
ranking is given in the following. As basic Q/A system
we adopted the model described in Section 2. We trained6
it with the entire Reuters-21578 corpus7. In particular
we adopted the collection Apte? split. It includes 12,902
documents for 90 classes, with a fixed splitting between
test-set and learning data (3,299 vs. 9,603). A description
of some categories of this corpus is given in Table 1.
Table 1: Description of some Reuters categories
Category Description
Acq Acquisition of shares and companies
Earn Earns derived by acquisitions or sells
Crude Crude oil events: market, Opec decision,..
Grain News about grain production
Trade Trade between companies
Ship Economic events that involve ships
Cocoa Market and events related to Cocoa plants
Nat-gas Natural Gas market
Veg-oil Vegetal Oil market
Table 2 shows the five answers generated (with their
corresponding rank) by the basic Q/A system, for one
example question. The category of the document from
which the answer was extracted is displayed in column
1. The question classification algorithm automatically as-
signed the Crude category to the question.
The processing of the question identifies the word
say as indicating the semantic class of the expected an-
swer and for paragraph retrieval it used the keywords
k1 = Director, k2 = General, k3 = energy,
k4 = floating, k5 = production and k6 = plants
as well as all morphological variations for the nouns.
For each answer from Table 2, we have underlined the
words matched against the keywords and emphasized
the word matched in the class of the expected answer,
whenever such a word was recognized (e.g., for an-
swers 1 and 3 only). For example, the first answer
was extracted because words producers, product and
directorate general could be matched against the key-
words production, Director and General from the ques-
tion and moreover, the word said has the same semantic
class as the word say, which indicates the semantic class
of the expected answer.
The ambiguity of the word plants cause the basic
Q/A system to rank the answers related to Cocoa and
Grain plantations higher than the correct answer, which is
ranked as the third one. If the answer re-ranking or elim-
ination methods are adopted, the correct answer reaches
6We could not use the TREC conference data-set because
texts and questions are not categorized.
7Available at
http://kdd.ics.uci.edu/databases/reuters21578/.
Table 2: Example of question labeled in the Crude category and its five answers.
Rank Category Question: What did the Director General say about the energy floating production plants?
1 Cocoa ? Leading cocoa producers are trying to protect their market from our product , ? said a spokesman for Indonesia
?s directorate general of plantations.
2 Grain Hideo Maki , Director General of the ministry ?s Economic Affairs Bureau , quoted Lyng as telling Agriculture
Minister Mutsuki Kato that the removal of import restrictions would help Japan as well as the United States.
3 Crude Director General of Mineral and Energy Affairs Louw Alberts announced the strike earlier but said it was
uneconomic .
4 Veg-oil Norbert Tanghe, head of division of the Commission?s Directorate General for Agriculture, told the 8th Antwerp
Oils and Fats Contact Days ? the Commission firmly believes that the sacrifices which would be undergone by
Community producers in the oils and fats sector...
5 Nat-gas Youcef Yousfi, director - general of Sonatrach , the Algerian state petroleum agency , indicated in a television
interview in Algiers that such imports.
the top as it was assigned the same category as the ques-
tion, namely the Crude category.
Next section describes in detail our experiments to
prove that question categorization add some important in-
formation to select relevant answers.
5 Experiments
The aim of the experiments is to prove that category in-
formation used, as described in the previous section, is
useful for Q/A systems. For this purpose we have to show
that the performance of a basic Q/A system is improved
when the question classification is adopted. To imple-
ment our Q/A and filtering system we used: (1) A state
of the art Q/A system: improving low accurate systems is
not enough to prove that TC is useful for Q/A. The basic
Q/A system that we employed is based on the architec-
ture described in (Pasca and Harabagiu, 2001), which is
the current state-of-the-art. (2) The Reuters collection of
categorized documents on which training our basic Q/A
system. (3) A set of questions categorized according to
the Reuters categories. A portion of this set is used to
train PRTC and QSVM models, the other disjoint portion
is used to measure the performance of the Q/A systems.
Next section, describes the technique used to produce
the question corpus.
5.1 Question Set Generation
The idea of PRTC and QSVM models is to exploit a
set of questions for each category to improve the learn-
ing of the PRC and SVM classifiers. Given the com-
plexity of producing any single question, we decided to
test our algorithms on only 5 categories. We chose Acq,
Earn, Crude, Grain, Trade and Ship categories since
for them is available the largest number of training doc-
uments. To generate questions we randomly selected a
number of documents from each category, then we tried
to formulate questions related to the pairs <document,
category>. Three cases were found: (a) The document
Table 3: Some training/testing Questions
Acq Which strategy aimed activities on core busi-
nesses?
How could the transpacific telephone cable be-
tween the U.S. and Japan contribute to forming
a join venture?
Earn What was the most significant factor for the lack
of the distribution of assets?
What do analysts think about public compa-
nies?
Crude What is Kuwait known for?
What supply does Venezuela give to another oil
producer?
Grain Why do certain exporters fear that China may
renounce its contract?
Why did men in port?s grain sector stop work?
Trade How did the trade surplus and the reserves
weaken Taiwan?s position?
What are Spain?s plans for reaching European
Community export level?
Ship When did the strikes start in the ship sector?
Who attacked the Saudi Arabian supertanker in
the United Arab Emirates sea?
does not contain general questions about the target cat-
egory. (b) The document suggests general questions, in
this case some of the question words that are in the an-
swers are replaced with synonyms to formulate a new
(more general) question. (c) The document suggests gen-
eral questions that are not related to the target category.
We add these questions in our data-set associated with
their true categories.
Table 3 lists a sample of the questions we derived from
the target set of categories. It is worth noting that we
included short queries also to maintain general our ex-
perimental set-up.
We generated 120 questions and we used 60 for the
learning and the other 60 for testing. To measure the im-
pact that TC has on Q/A, we first evaluated the question
categorization models presented in Section 3.1. Then we
compared the performance of the basic Q/A system with
the extended Q/A systems that adopt the answer elimina-
tion and re-ranking methods.
5.2 Performance Measurements
In sections 3 and 4 we have introduced several models.
From the point of view of the accuracy, we can divided
them in two categories: the (document and question) cat-
egorization models and the Q/A models. The former
are usually measured by using Precision, Recall, and f-
measure (Yang, 1999); note that questions can be con-
sidered as small documents. The latter often provide as
output a list of ranked answers. In this case, a good mea-
sure of the system performance should take into account
the order of the correct and incorrect questions.
One method employed in TREC is the reciprocal value
of the rank (RAR) of the highest-ranked correct answer
generated by the Q/A system. Its value is 1 if the first
answer is correct, 0.5 if the second answer is correct
but not the first one, 0.33 when the correct answer was
on the third position, 0.25 if the fourth answer was cor-
rect, and 0.1 when the fifth answer was correct and so
on. If none of the answers are corrects, RAR=0. The
Mean Reciprocal Answer Rank (MRAR) is used to com-
pute the overall performance of Q/A systems8, defined as
MRAR = 1n
?
i
1
ranki , where n is the number of ques-
tions and ranki is the rank of the answer i.
Since we believe that TC information is meaningful to
prefer out incorrect answers, we defined a second mea-
sure to evaluate Q/A. For this purpose we designed the
Signed Reciprocal Answer Rank (SRAR), which is de-
fined as 1n
?
j?A
1
srankj , where A is the set of answers
given for the test-set questions, |srankj | is the rank posi-
tion of the answer j and srankj is positive if j is correct
and negative if it is not correct. The SRAR can be evalu-
ated over a set of questions as well as over only one ques-
tion. SRAR for a single question is 0 only if no answer
was provided for it.
For example, given the answer ranking of Table 2 and
considering that we have just one question for testing, the
MRAR score is 0.33 while the SRAR is -1 -.5 +.33 -.25 -
.1 = -1.52. If the answer re-ranking is adopted the MRAR
improve to 1 and the SRAR becomes +1 -.5 -.33 -.25 -.1
= -.18. The answer elimination produces a MRAR and a
SRAR of 1.
5.3 Evaluation of Question Categorization
Table 4 lists the performance of question categorization
for each of the models described in Section 3.1. We no-
ticed better results when the PRTC and QSVM models
were used. In the overall, we find that the performance of
8The same measure was used in all TREC Q/A evaluations.
question categorization is not as good as the one obtained
for TC in (Moschitti, 2003b).
Table 4: f1 performances of question categorization.
RTC0 SVM0 PRTC QSVM QATC
f1 f1 f1 f1 f1
acq 18.19 54.02 62.50 56.00 46.15
crude 33.33 54.05 53.33 66.67 66.67
earn 0.00 55.32 40.00 13.00 26.67
grain 50.00 52.17 75.00 66.67 50.00
ship 80.00 47.06 75.00 90.00 85.71
trade 40.00 57.13 66.67 58.34 45.45
5.4 Evaluation of Question Answering
To evaluate the impact of our filtering methods on Q/A
we first scored the answers of a basic Q/A system for the
test set, by using both the MRAR and the SRAR mea-
sures. Additionally, we evaluated (1) the MRAR when
answers were re-ranked based on question and answer
category information; and (2) the SRAR in the case when
answers extracted from documents with different cate-
gories were eliminated. Rows 1 and 2 of Table 5 report
the MRAR and SRAR performances of the basic Q/A.
Column 2,3,4,5 and 6 show the MRAR and SRAR accu-
racies (rows 4 and 5) of Q/A systems that eliminate or
re-rank the answer by using the RTC0, SVM0, PRTC,
QSVM and QATC question categorization models.
The basic Q/A results show that answering the Reuters
based questions is a quite difficult task9 as the MRAR is
.662, about 15 percent points under the best system result
obtained in the 2003 TREC competition. Note that the
basic Q/A system, employed in these experiments, uses
the same techniques adopted by the best figure Q/A sys-
tem of TREC 2003.
The quality of the Q/A results is strongly affected by
the question classification accuracy. In fact, RTC0 and
QATC that have the lowest classification f1 (see Table
4) produce very low MRAR (i.e. .622% and .607%) and
SRAR (i.e. -.189 and -.320). When the best question
classification model QSVM is used, the basic Q/A perfor-
mance improves with respect to both the MRAR (66.35%
vs 66.19%) and the SRAR (-.077% vs -.372%) scores.
In order to study how the number of answers impacts
the accuracy of the proposed models, we have evaluated
the MRAR and the SRAR score varying the maximum
number of answers, provided by the basic Q/A system.
We adopted as filtering policy the answer re-ranking.
Figure 2 shows that as the number of answers increases
the MRAR score for QSVM, PRTC and the basic Q/A in-
9Past TREC competition results have shown that Q/A per-
formances strongly depend on the questions/domains used for
the evaluation. For example, the more advanced systems of
2001 performed lower than the systems of 1999 as they were
evaluate on a more difficult test-set.
Table 5: Performance comparisons between basic Q/A and
Q/A using answer re-ranking or elimination policies.
MRAR .662
SRAR -.372
Model RTC0 SVM0 PRTC QSVM QATC
MRAR .622 .649 .658 .664 .607
(re-rank.)
SRAR -.189 -.135 -.036 -.077 -.320
(elimin.)
0.57
0.59
0.61
0.63
0.65
0.67
1 2 3 4 5 6 7 8 9 10# Answers
MR
AR
 
sc
ore
basic Q/A
PRTCQSVM
Figure 2: The MRAR results for basic Q/A and Q/A with an-
swer re-ranking based on question categorization via the PRTC
and QSVM models.
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
1 2 3 4 5 6 7 8 9 10# Answers
SRA
R s
co
re
basic Q/APRTCQSVM)
Figure 3: The SRAR results for basic Q/A and Q/A with an-
swer re-ranking based on question categorization via the PRTC
and QSVM models.
creases, for the first four answers and it reaches a plateau
afterwards. We also notice that the QSVM outperforms
both PRTC and the basic Q/A. This figure also shows that
question categorization per se does not greatly impact the
MRAR score of Q/A.
Figure 3 illustrates the SRAR curves by considering
the answer elimination policy. The figure clearly shows
that the QSVM and PRTC models for question catego-
rization determine a higher SRAR score, thus indicating
that fewer irrelevant answers are left. Figure 3 shows that
question categorization can greatly improve the quality
of Q/A when irrelevant answers are considered. It also
shows that perhaps, when evaluating Q/A systems with
the MRAR scoring method, the ?optimistic? view of Q/A
is taken, in which erroneous results are ignored for the
sake of emphasizing that an answer was obtained after
all, even if it was ranked below several incorrect answers.
In contrast, the SRAR score that we have described
in Section 5.2 produce a ?harsher? score, in which errors
are given the same weight as the correct results, but affect
negatively the overall score. This explains why, even for a
baseline Q/A, we obtained a negative score, as illustrated
in Table 5. This shows that the Q/A system generates
more erroneous answers then correct answers. If only
the MRAR scores would be considered we may assess
that TC does not bring significant information to Q/A for
precision enhancement by re-ranking answers. However,
the results obtained with the SRAR scoring scheme, in-
dicate that text categorization impacts on Q/A results, by
eliminating incorrect answers. We plan to further study
the question categorization methods and empirically find
which weighting scheme is ideal.
6 Conclusions
Question/Answering and Text Categorization have been,
traditionally, applied separately, even if category infor-
mation should be used to improve the answer search-
ing. In this paper, it has been, firstly, presented a Ques-
tion Answering system that exploits the category infor-
mation. The methods that we have designed are based on
the matching between the question and the answer cate-
gories. Depending on positive or negative matching two
strategies allow to affect the Q/A performances: answer
re-ranking and answer elimination.
We have studied five question categorization models
based on two traditional TC approaches: Rocchio and
Support Vector Machines. Their evaluation confirms the
difficulty of automated question categorization as the ac-
curacies are lower than those reachable for document cat-
egorization.
The impact of question classification in Q/A has been
evaluated using the MRAR and the SRAR scores. When
the SRAR, which considers the number of incorrect an-
swers, is used to evaluate the enhanced Q/A system as
well as the basic Q/A system, the results show a great
improvement.
References
R. H. Baayen, R. Piepenbrock, and L. Gulikers, editors.
1995. The CELEX Lexical Database (Release 2) [CD-
ROM]. Philadelphia, PA: Linguistic Data Consortium, Uni-
versity of Pennsylvania.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In In Proceedings of the 1st Meeting of the North American
Chapter of the ACL, pages 132?139.
P. Clark, J. Thompson, and B. Porter. 1999. A knowledge-
based approach to question-answering. In proceeding of
AAAI?99 Fall Symposium on Question-Answering Systems.
AAAI.
Michael Collins. 1997. Three generative, lexicalized models
for statistical parsing. In Proceedings of the ACL and EA-
CLinguistics, pages 16?23, Somerset, New Jersey.
S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Experiments
with open-domain textual question answering. In Proceed-
ings of the COLING-2000.
Sanda M. Harabagiu, Dan I. Moldovan, Marius Pasca, Rada
Mihalcea, Mihai Surdeanu, Razvan C. Bunescu, Roxana
Girju, Vasile Rus, and Paul Morarescu. 2001. The role of
lexico-semantic feedback in open-domain textual question-
answering. In Meeting of the ACL, pages 274?281.
David J. Ittner, David D. Lewis, and David D. Ahn. 1995.
Text categorization of low quality images. In Proceedings
of SDAIR-95, pages 301?315, Las Vegas, US.
T. Joachims. 1999. T. joachims, making large-scale svm learn-
ing practical. In B. Schlkopf, C. Burges, and MIT-Press.
A. Smola (ed.), editors, Advances in Kernel Methods - Sup-
port Vector Learning.
Alessandro Moschitti. 2003a. Natural Language Processing
and Automated Text Categorization: a study on the recipro-
cal beneficial interactions. Ph.D. thesis, Computer Science
Department, Univ. of Rome ?Tor Vergata?.
Alessandro Moschitti. 2003b. A study on optimal parameter
tuning for Rocchio text classifier. In Fabrizio Sebastiani, ed-
itor, Proceedings of ECIR-03, 25th European Conference on
Information Retrieval, Pisa, IT. Springer Verlag.
Marius A. Pasca and Sandra M. Harabagiu. 2001. High per-
formance question/answering. In Proceedings ACM SIGIR
2001, pages 366?374. ACM Press.
J.J. Rocchio. 1971. Relevance feedback in information re-
trieval. In G. Salton, editor, The SMART Retrieval System?
Experiments in Automatic Document Processing, pages 313-
323 Englewood Cliffs, NJ, Prentice Hall, Inc.
G. Salton. 1989. Automatic text processing: the transfor-
mation, analysis and retrieval of information by computer.
Addison-Wesley.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer.
Y. Yang. 1999. An evaluation of statistical approaches to text
categorization. Information Retrieval Journal.
Making Tree Kernels practical for Natural Language Learning
Alessandro Moschitti
Department of Computer Science
University of Rome ?Tor Vergata?
Rome, Italy
moschitti@info.uniroma2.it
Abstract
In recent years tree kernels have been pro-
posed for the automatic learning of natural
language applications. Unfortunately, they
show (a) an inherent super linear complex-
ity and (b) a lower accuracy than tradi-
tional attribute/value methods.
In this paper, we show that tree kernels
are very helpful in the processing of nat-
ural language as (a) we provide a simple
algorithm to compute tree kernels in linear
average running time and (b) our study on
the classification properties of diverse tree
kernels show that kernel combinations al-
ways improve the traditional methods. Ex-
periments with Support Vector Machines
on the predicate argument classification
task provide empirical support to our the-
sis.
1 Introduction
In recent years tree kernels have been shown to
be interesting approaches for the modeling of syn-
tactic information in natural language tasks, e.g.
syntactic parsing (Collins and Duffy, 2002), rela-
tion extraction (Zelenko et al, 2003), Named En-
tity recognition (Cumby and Roth, 2003; Culotta
and Sorensen, 2004) and Semantic Parsing (Mos-
chitti, 2004).
The main tree kernel advantage is the possibility
to generate a high number of syntactic features and
let the learning algorithm to select those most rel-
evant for a specific application. In contrast, their
major drawback are (a) the computational time
complexity which is superlinear in the number of
tree nodes and (b) the accuracy that they produce is
often lower than the one provided by linear models
on manually designed features.
To solve problem (a), a linear complexity al-
gorithm for the subtree (ST) kernel computation,
was designed in (Vishwanathan and Smola, 2002).
Unfortunately, the ST set is rather poorer than the
one generated by the subset tree (SST) kernel de-
signed in (Collins and Duffy, 2002). Intuitively,
an ST rooted in a node n of the target tree always
contains all n?s descendants until the leaves. This
does not hold for the SSTs whose leaves can be
internal nodes.
To solve the problem (b), a study on different
tree substructure spaces should be carried out to
derive the tree kernel that provide the highest ac-
curacy. On the one hand, SSTs provide learn-
ing algorithms with richer information which may
be critical to capture syntactic properties of parse
trees as shown, for example, in (Zelenko et al,
2003; Moschitti, 2004). On the other hand, if the
SST space contains too many irrelevant features,
overfitting may occur and decrease the classifica-
tion accuracy (Cumby and Roth, 2003). As a con-
sequence, the fewer features of the ST approach
may be more appropriate.
In this paper, we aim to solve the above prob-
lems. We present (a) an algorithm for the eval-
uation of the ST and SST kernels which runs in
linear average time and (b) a study of the impact
of diverse tree kernels on the accuracy of Support
Vector Machines (SVMs).
Our fast algorithm computes the kernels be-
tween two syntactic parse trees in O(m + n) av-
erage time, where m and n are the number of
nodes in the two trees. This low complexity al-
lows SVMs to carry out experiments on hundreds
of thousands of training instances since it is not
higher than the complexity of the polynomial ker-
113
nel, widely used on large experimentation e.g.
(Pradhan et al, 2004). To confirm such hypothe-
sis, we measured the impact of the algorithm on
the time required by SVMs for the learning of
about 122,774 predicate argument examples anno-
tated in PropBank (Kingsbury and Palmer, 2002)
and 37,948 instances annotated in FrameNet (Fill-
more, 1982).
Regarding the classification properties, we stud-
ied the argument labeling accuracy of ST and SST
kernels and their combinations with the standard
features (Gildea and Jurafsky, 2002). The re-
sults show that, on both PropBank and FrameNet
datasets, the SST-based kernel, i.e. the richest
in terms of substructures, produces the highest
SVM accuracy. When SSTs are combined with the
manual designed features, we always obtain the
best figure classifier. This suggests that the many
fragments included in the SST space are relevant
and, since their manual design may be problem-
atic (requiring a higher programming effort and
deeper knowledge of the linguistic phenomenon),
tree kernels provide a remarkable help in feature
engineering.
In the remainder of this paper, Section 2 de-
scribes the parse tree kernels and our fast algo-
rithm. Section 3 introduces the predicate argument
classification problem and its solution. Section 4
shows the comparative performance in term of the
execution time and accuracy. Finally, Section 5
discusses the related work whereas Section 6 sum-
marizes the conclusions.
2 Fast Parse Tree Kernels
The kernels that we consider represent trees in
terms of their substructures (fragments). These
latter define feature spaces which, in turn, are
mapped into vector spaces, e.g. <n. The asso-
ciated kernel function measures the similarity be-
tween two trees by counting the number of their
common fragments. More precisely, a kernel func-
tion detects if a tree subpart (common to both
trees) belongs to the feature space that we intend
to generate. For such purpose, the fragment types
need to be described. We consider two important
characterizations: the subtrees (STs) and the sub-
set trees (SSTs).
2.1 Subtrees and Subset Trees
In our study, we consider syntactic parse trees,
consequently, each node with its children is asso-
ciated with a grammar production rule, where the
symbol at left-hand side corresponds to the parent
node and the symbols at right-hand side are asso-
ciated with its children. The terminal symbols of
the grammar are always associated with the leaves
of the tree. For example, Figure 1 illustrates the
syntactic parse of the sentence "Mary brought a
cat to school".
S ? N VP
VP ? V NP PP
PP ? IN N
N ? school
N
school
The root
A leaf 
S
N
NP
D N
VP
VMary
to
brought 
a cat
PP
IN
A subtree
Figure 1: A syntactic parse tree.
We define as a subtree (ST) any node of a tree
along with all its descendants. For example, the
line in Figure 1 circles the subtree rooted in the NP
node. A subset tree (SST) is a more general struc-
ture. The difference with the subtrees is that the
leaves can be associated with non-terminal sym-
bols. The SSTs satisfy the constraint that they are
generated by applying the same grammatical rule
set which generated the original tree. For exam-
ple, [S [N VP]] is a SST of the tree in Figure
1 which has two non-terminal symbols, N and VP,
as leaves.
S
N
NP
D N
VP
VMary
brought
a
   cat
NP
D N
a
   cat
N
   cat
D
a
V
brought 
N
Mary
NP
D N
VP
V
brought 
a
   cat
Figure 2: A syntactic parse tree with its subtrees (STs).
NP
D N
a
  cat
NP
D N
NP
D N
a
NP
D N
NP
D N
VP
V
brought
a
   cat
  cat
NP
D N
VP
V
a
   cat
NP
D N
VP
V
N
   cat
D
a
V
brought 
N
Mary
?
Figure 3: A tree with some of its subset trees (SSTs).
Given a syntactic tree we can use as feature rep-
resentation the set of all its STs or SSTs. For ex-
ample, Figure 2 shows the parse tree of the sen-
tence "Mary brought a cat" together with its 6
STs, whereas Figure 3 shows 10 SSTs (out of
17) of the subtree of Figure 2 rooted in VP. The
114
high different number of substructures gives an in-
tuitive quantification of the different information
level between the two tree-based representations.
2.2 The Tree Kernel Functions
The main idea of tree kernels is to compute the
number of the common substructures between two
trees T1 and T2 without explicitly considering
the whole fragment space. For this purpose, we
slightly modified the kernel function proposed in
(Collins and Duffy, 2002) by introducing a param-
eter ? which enables the ST or the SST evaluation.
Given the set of fragments {f1, f2, ..} = F , we
defined the indicator function Ii(n) which is equal
1 if the target fi is rooted at node n and 0 other-
wise. We define
K(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2) (1)
where NT1 and NT2 are the sets of the T1?s
and T2?s nodes, respectively and ?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2). This latter is equal to the
number of common fragments rooted in the n1 and
n2 nodes. We can compute ? as follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the
same, and n1 and n2 have only leaf children
(i.e. they are pre-terminals symbols) then
?(n1, n2) = 1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) =
nc(n1)
?
j=1
(? + ?(cjn1 , c
j
n2)) (2)
where ? ? {0, 1}, nc(n1) is the number of the
children of n1 and cjn is the j-th child of the node
n. Note that, since the productions are the same,
nc(n1) = nc(n2).
When ? = 0, ?(n1, n2) is equal 1 only if
?j ?(cjn1 , cjn2) = 1, i.e. all the productions as-
sociated with the children are identical. By recur-
sively applying this property, it follows that the
subtrees in n1 and n2 are identical. Thus, Eq. 1
evaluates the subtree (ST) kernel. When ? = 1,
?(n1, n2) evaluates the number of SSTs common
to n1 and n2 as proved in (Collins and Duffy,
2002).
Additionally, we study some variations of the
above kernels which include the leaves in the frag-
ment space. For this purpose, it is enough to add
the condition:
0. if n1 and n2 are leaves and their associated
symbols are equal then ?(n1, n2) = 1,
to the recursive rule set for the ? evaluation
(Zhang and Lee, 2003). We will refer to such ex-
tended kernels as ST+bow and SST+bow (bag-of-
words).
Moreover, we add the decay factor ? by modi-
fying steps (2) and (3) as follows1:
2. ?(n1, n2) = ?,
3. ?(n1, n2) = ?
?nc(n1)
j=1 (? + ?(cjn1 , cjn2)).
The computational complexity of Eq. 1 is
O(|NT1 | ? |NT2 |). We will refer to this basic im-
plementation as the Quadratic Tree Kernel (QTK).
However, as observed in (Collins and Duffy, 2002)
this worst case is quite unlikely for the syntactic
trees of natural language sentences, thus, we can
design algorithms that run in linear time on aver-
age.
function Evaluate Pair Set(Tree T1, T2) returns NODE PAIR SET;
LIST L1,L2;
NODE PAIR SET Np;
begin
L1 = T1.ordered list;
L2 = T2.ordered list; /*the lists were sorted at loading time*/
n1 = extract(L1); /*get the head element and*/
n2 = extract(L2); /*remove it from the list*/
while (n1 and n2 are not NULL)
if (production of(n1) > production of(n2))
then n2 = extract(L2);
else if (production of(n1) < production of(n2))
then n1 = extract(L1);
else
while (production of(n1) == production of(n2))
while (production of(n1) == production of(n2))
add(?n1, n2?, Np);
n2=get next elem(L2); /*get the head element
and move the pointer to the next element*/
end
n1 = extract(L1);
reset(L2); /*set the pointer at the first element*/
end
end
return Np ;
end
Table 1: Pseudo-code for fast evaluation of the node pair
sets used in the fast Tree Kernel.
2.3 A Fast Tree Kernel (FTK)
To compute the kernels defined in the previous
section, we sum the ? function for each pair
?n1, n2?? NT1 ? NT2 (Eq. 1). When the pro-
ductions associated with n1 and n2 are different,
we can avoid to evaluate ?(n1, n2) since it is 0.
1To have a similarity score between 0 and 1, we also ap-
ply the normalization in the kernel space, i.e. K?(T1, T2) =
K(T1,T2)?
K(T1,T1)?K(T2,T2)
.
115
SN
NP
D N
VP
VMary
  to
brought 
a cat
PP
IN N
school
Arg. 0
Arg. MArg. 1
Predicate 
NP
D N
VP
V
brought 
a cat
SArg1 VP
V
  to
brought 
PP
IN N
school
S
N
VMary
brought 
VP
SArg0 SArgM
Figure 4: Tree substructure space for predicate argument classification.
Thus, we look for a node pair set Np ={?n1, n2??
NT1 ? NT2 : p(n1) = p(n2)}, where p(n) returns
the production rule associated with n.
To efficiently build Np, we (i) extract the L1 and
L2 lists of the production rules from T1 and T2,
(ii) sort them in the alphanumeric order and (iii)
scan them to find the node pairs ?n1, n2? such that
(p(n1) = p(n2)) ? L1?L2. Step (iii) may require
only O(|NT1 | + |NT2 |) time, but, if p(n1) appears
r1 times in T1 and p(n2) is repeated r2 times in
T2, we need to consider r1 ? r2 pairs. The formal
algorithm is given in Table 1.
Note that:
(a) The list sorting can be done only once at the
data preparation time (i.e. before training) in
O(|NT1 | ? log(|NT1 |)).
(b) The algorithm shows that the worst case oc-
curs when the parse trees are both generated us-
ing only one production rule, i.e. the two inter-
nal while cycles carry out |NT1 |?|NT2 | iterations.
In contrast, two identical parse trees may generate
a linear number of non-null pairs if there are few
groups of nodes associated with the same produc-
tion rule.
(c) Such approach is perfectly compatible with the
dynamic programming algorithm which computes
?. In fact, the only difference with the original
approach is that the matrix entries corresponding
to pairs of different production rules are not con-
sidered. Since such entries contain null values
they do not affect the application of the original
dynamic programming. Moreover, the order of
the pair evaluation can be established at run time,
starting from the root nodes towards the children.
3 A Semantic Application of Parse Tree
Kernels
An interesting application of the SST kernel is
the classification of the predicate argument struc-
tures defined in PropBank (Kingsbury and Palmer,
2002) or FrameNet (Fillmore, 1982). Figure
4 shows the parse tree of the sentence: "Mary
brought a cat to school" along with the pred-
icate argument annotation proposed in the Prop-
Bank project. Only verbs are considered as pred-
icates whereas arguments are labeled sequentially
from ARG0 to ARG9.
Also in FrameNet predicate/argument informa-
tion is described but for this purpose richer seman-
tic structures called Frames are used. The Frames
are schematic representations of situations involv-
ing various participants, properties and roles in
which a word may be typically used. Frame el-
ements or semantic roles are arguments of pred-
icates called target words. For example the fol-
lowing sentence is annotated according to the AR-
REST frame:
[Time One Saturday night] [ Authorities police
in Brooklyn ] [Target apprehended ] [ Suspect
sixteen teenagers].
The roles Suspect and Authorities are specific to
the frame.
The common approach to learn the classifica-
tion of predicate arguments relates to the extrac-
tion of features from the syntactic parse tree of
the target sentence. In (Gildea and Jurafsky, 2002)
seven different features2, which aim to capture the
relation between the predicate and its arguments,
were proposed. For example, the Parse Tree Path
of the pair ?brought, ARG1? in the syntactic tree
of Figure 4 is V ? VP ? NP. It encodes the depen-
dency between the predicate and the argument as a
sequence of nonterminal labels linked by direction
symbols (up or down).
An alternative tree kernel representation, pro-
posed in (Moschitti, 2004), is the selection of the
minimal tree subset that includes a predicate with
only one of its arguments. For example, in Figure
4, the substructures inside the three frames are the
semantic/syntactic structures associated with the
three arguments of the verb to bring, i.e. SARG0,
SARG1 and SARGM .
Given a feature representation of predicate ar-
2Namely, they are Phrase Type, Parse Tree Path, Pred-
icate Word, Head Word, Governing Category, Position and
Voice.
116
guments, we can build an individual ONE-vs-ALL
(OVA) classifier Ci for each argument i. As a fi-
nal decision of the multiclassifier, we select the ar-
gument type ARGt associated with the maximum
value among the scores provided by the Ci, i.e.
t = argmaxi?S score(Ci), where S is the set
of argument types. We adopted the OVA approach
as it is simple and effective as showed in (Pradhan
et al, 2004).
Note that the representation in Figure 4 is quite
intuitive and, to conceive it, the designer requires
much less linguistic knowledge about semantic
roles than those necessary to define relevant fea-
tures manually. To understand such point, we
should make a step back before Gildea and Juraf-
sky defined the first set of features for Semantic
Role Labeling (SRL). The idea that syntax may
have been useful to derive semantic information
was already inspired by linguists, but from a ma-
chine learning point of view, to decide which tree
fragments may have been useful for semantic role
labeling was not an easy task. In principle, the de-
signer should have had to select and experiment
all possible tree subparts. This is exactly what the
tree kernels can automatically do: the designer just
need to roughly select the interesting whole sub-
tree (correlated with the linguistic phenomenon)
and the tree kernel will generate all possible syn-
tactic features from it. The task of selecting the
most relevant substructures is carried out by the
kernel machines themselves.
4 The Experiments
The aim of the experiments is twofold. On the one
hand, we show that the FTK running time is linear
on the average case and is much faster than QTK.
This is accomplished by measuring the learning
time and the average kernel computation time. On
the other hand, we study the impact of the differ-
ent tree based kernels on the predicate argument
classification accuracy.
4.1 Experimental Set-up
We used two different corpora: PropBank
(www.cis.upenn.edu/?ace) along with Pen-
nTree bank 2 (Marcus et al, 1993) and FrameNet.
PropBank contains about 53,700 sentences and
a fixed split between training and testing which has
been used in other researches, e.g. (Gildea and
Palmer, 2002; Pradhan et al, 2004). In this split,
sections from 02 to 21 are used for training, sec-
tion 23 for testing and sections 1 and 22 as devel-
oping set. We considered a total of 122,774 and
7,359 arguments (from ARG0 to ARG9, ARGA
and ARGM) in training and testing, respectively.
Their tree structures were extracted from the Penn
Treebank. It should be noted that the main contri-
bution to the global accuracy is given by ARG0,
ARG1 and ARGM.
From the FrameNet corpus (http://www.icsi
.berkeley.edu/?framenet), we extracted all
24,558 sentences of the 40 Frames selected for
the Automatic Labeling of Semantic Roles task of
Senseval 3 (www.senseval.org). We mapped to-
gether the semantic roles having the same name
and we considered only the 18 most frequent roles
associated with verbal predicates, for a total of
37,948 arguments. We randomly selected 30% of
sentences for testing and 70% for training. Addi-
tionally, 30% of training was used as a validation-
set. Note that, since the FrameNet data does not
include deep syntactic tree annotation, we pro-
cessed the FrameNet data with Collins? parser
(Collins, 1997), consequently, the experiments on
FrameNet relate to automatic syntactic parse trees.
The classifier evaluations were carried out
with the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes ST and SST kernels in the SVM-
light software (Joachims, 1999). We used the
default linear (Linear) and polynomial (Poly)
kernels for the evaluations with the standard
features defined in (Gildea and Jurafsky, 2002).
We adopted the default regularization parameter
(i.e., the average of 1/||~x||) and we tried a few
cost-factor values (i.e., j ? {1, 3, 7, 10, 30, 100})
to adjust the rate between Precision and Recall on
the validation-set.
For the ST and SST kernels, we derived that the
best ? (see Section 2.2) were 1 and 0.4, respec-
tively. The classification performance was eval-
uated using the F1 measure3 for the single argu-
ments and the accuracy for the final multiclassi-
fier. This latter choice allows us to compare our
results with previous literature work, e.g. (Gildea
and Jurafsky, 2002; Pradhan et al, 2004).
4.2 Time Complexity Experiments
In this section we compare our Fast Tree Kernel
(FTK) approach with the Quadratic Tree Kernel
(QTK) algorithm. The latter refers to the naive
evaluation of Eq. 1 as presented in (Collins and
Duffy, 2002).
3F1 assigns equal importance to Precision P and Recall
R, i.e. f1 = 2P?RP+R .
117
Figure 5 shows the learning time4 of the SVMs
using QTK and FTK (over the SST structures)
for the classification of one large argument (i.e.
ARG0), according to different percentages of
training data. We note that, with 70% of the train-
ing data, FTK is about 10 times faster than QTK.
With all the training data FTK terminated in 6
hours whereas QTK required more than 1 week.
y = 0.0006x 2  - 0.001x
y = 0.0045x 2  + 0.1004x
0
5
10
15
20
25
30
35
0 10 20 30 40 50 60 70 80 90 100
% Training Data
H
o
u
rs
FTK
QTK
Figure 5: ARG0 classifier learning time according to dif-
ferent training percentages.
y = 0.04x 2  - 0.05x
y = 0.14x
0
20
40
60
80
100
120
10 15 20 25 30 35 40 45 50 55 60
Number of Tree Nodes
?? ??s
ec
o
n
ds
FTK
QTK
Figure 6: Average time in seconds for the QTK and FTK
evaluations.
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
0 10 20 30 40 50 60 70 80 90 100
% Training Data
A
c
c
u
ra
c
y
ST
SST
ST+bow
SST+bow
Linear
Poly
Figure 7: Multiclassifier accuracy according to different
training set percentages.
4We run the experiments on a Pentium 4, 2GHz, with 1
Gb ram.
The above results are quite interesting because
they show that (1) we can use tree kernels with
SVMs on huge training sets, e.g. on 122,774 in-
stances and (2) the time needed to converge is ap-
proximately the one required by SVMs when us-
ing polynomial kernel. This latter shows the mini-
mal complexity needed to work in the dual space.
To study the FTK running time, we extracted
from PennTree bank the first 500 trees5 containing
exactly n nodes, then, we evaluated all 25,000 pos-
sible tree pairs. Each point of the Figure 6 shows
the average computation time on all the tree pairs
of a fixed size n.
In the figures, the trend lines which best inter-
polates the experimental values are also shown. It
clearly appears that the training time is quadratic
as SVMs have quadratic learning time complexity
(see Figure 5) whereas the FTK running time has
a linear behavior (Figure 6). The QTK algorithm
shows a quadratic running time complexity, as ex-
pected.
4.3 Accuracy of the Tree Kernels
In these experiments, we investigate which ker-
nel is the most accurate for the predicate argument
classification.
First, we run ST, SST, ST+bow, SST+bow, Lin-
ear and Poly kernels over different training-set size
of PropBank. Figure 7 shows the learning curves
associated with the above kernels for the SVM-
based multiclassifier. We note that (a) SSTs have
a higher accuracy than STs, (b) bow does not im-
prove either ST or SST kernels and (c) in the fi-
nal part of the plot SST shows a higher gradient
than ST, Linear and Poly. This latter produces
the best accuracy 90.5% in line with the litera-
ture findings using standard features and polyno-
mial SVMs, e.g. 87.1%6 in (Pradhan et al, 2004).
Second, in tables 2 and 3, we report the results
using all available training data, on PropBank and
FrameNet test sets, respectively. Each row of the
two tables shows the F1 measure of the individ-
ual classifiers using different kernels whereas the
last column illustrates the global accuracy of the
multiclassifier.
5We measured also the computation time for the incom-
plete trees associated with the predicate argument structures
(see Section 3); we obtained the same results.
6The small difference (2.4%) is mainly due to the differ-
ent treatment of ARGMs: we built a single ARGM class for
all subclasses, e.g. ARGM-LOC and ARGM-TMP, whereas
in (Pradhan et al, 2004), the ARGMs, were evaluated sepa-
rately.
118
We note that, the F1 of the single arguments
across the different kernels follows the same be-
havior of the global multiclassifier accuracy. On
FrameNet, the bow impact on the ST and SST
accuracy is higher than on PropBank as it pro-
duces an improvement of about 1.5%. This sug-
gests that (1) to detect semantic roles, lexical in-
formation is very important, (2) bow give a higher
contribution as errors in POS-tagging make the
word + POS fragments less reliable and (3) as the
FrameNet trees are obtained with the Collins? syn-
tactic parser, tree kernels seem robust to incorrect
parse trees.
Third, we point out that the polynomial ker-
nel on flat features is more accurate than tree ker-
nels but the design of such effective features re-
quired noticeable knowledge and effort (Gildea
and Jurafsky, 2002). On the contrary, the choice
of subtrees suitable to syntactically characterize a
target phenomenon seems a easier task (see Sec-
tion 3 for the predicate argument case). More-
over, by combining polynomial and SST kernels,
we can improve the classification accuracy (Mos-
chitti, 2004), i.e. tree kernels provide the learn-
ing algorithm with many relevant fragments which
hardly can be designed by hand. In fact, as many
predicate argument structures are quite large (up
to 100 nodes) they contain many fragments.
ARGs ST SST ST+bow SST+bow Linear Poly
ARG0 86.5 88.0 86.9 88.4 88.6 90.6
ARG1 83.1 87.4 82.8 86.7 85.9 90.8
ARG2 58.0 67.6 58.9 66.7 65.5 80.4
ARG3 35.7 37.5 39.3 41.2 51.9 60.4
ARG4 62.7 65.6 63.3 63.9 66.2 70.0
ARGM 92.0 94.2 92.0 93.7 94.9 95.3
Acc. 84.6 87.7 84.8 87.5 87.6 90.7
Table 2: Evaluation of Kernels on PropBank.
Roles ST SST ST+bow SST+bow Linear Poly
agent 86.9 87.8 89.2 90.2 89.8 91.7
theme 76.1 79.2 78.5 80.7 82.9 90.4
goal 77.9 78.9 78.2 80.1 80.2 85.8
path 82.8 84.4 83.7 85.1 81.3 85.5
manner 79.9 82.0 81.3 82.5 70.8 80.5
source 85.6 87.7 86.9 87.8 86.5 89.8
time 76.3 78.3 77.0 79.1 61.8 68.3
reason 75.9 77.3 78.9 81.4 82.9 86.4
Acc. 80.0 81.2 81.3 82.9 82.3 85.6
18 roles
Table 3: Evaluation of the Kernels on FrameNet semantic
roles.
Finally, to study the combined kernels, we ap-
plied the K1 + ?K2 formula, where K1 is either
the Linear or the Poly kernel and K2 is the ST
Corpus Poly ST+Linear SST+Linear ST+Poly SST+Poly
PropBank 90.7 88.6 89.4 91.1 91.3
FrameNet 85.6 85.3 85.8 87.5 87.2
Table 4: Multiclassifier accuracy using Kernel Combina-
tions.
or the SST kernel. Table 4 shows the results of
four kernel combinations. We note that, (a) STs
and SSTs improve Poly (about 0.5 and 2 percent
points on PropBank and FrameNet, respectively)
and (b) the linear kernel, which uses fewer fea-
tures than Poly, is more enhanced by the SSTs than
STs (for example on PropBank we have 89.4% and
88.6% vs. 87.6%), i.e. Linear takes advantage by
the richer feature set of the SSTs. It should be
noted that our results of kernel combinations on
FrameNet are in contrast with (Moschitti, 2004),
where no improvement was obtained. Our expla-
nation is that, thanks to the fast evaluation of FTK,
we could carry out an adequate parameterization.
5 Related Work
Recently, several tree kernels have been designed.
In the following, we highlight their differences and
properties.
In (Collins and Duffy, 2002), the SST tree ker-
nel was experimented with the Voted Perceptron
for the parse-tree reranking task. The combination
with the original PCFG model improved the syn-
tactic parsing. Additionally, it was alluded that the
average execution time depends on the number of
repeated productions.
In (Vishwanathan and Smola, 2002), a linear
complexity algorithm for the computation of the
ST kernel is provided (in the worst case). The
main idea is the use of the suffix trees to store par-
tial matches for the evaluation of the string kernel
(Lodhi et al, 2000). This can be used to compute
the ST fragments once the tree is converted into a
string. To our knowledge, ours is the first applica-
tion of the ST kernel for a natural language task.
In (Kazama and Torisawa, 2005), an interesting
algorithm that speeds up the average running time
is presented. Such algorithm looks for node pairs
that have in common a large number of trees (ma-
licious nodes) and applies a transformation to the
trees rooted in such nodes to make faster the kernel
computation. The results show an increase of the
speed similar to the one produced by our method.
In (Zelenko et al, 2003), two kernels over syn-
tactic shallow parser structures were devised for
the extraction of linguistic relations, e.g. person-
affiliation. To measure the similarity between two
119
nodes, the contiguous string kernel and the sparse
string kernel (Lodhi et al, 2000) were used. In
(Culotta and Sorensen, 2004) such kernels were
slightly generalized by providing a matching func-
tion for the node pairs. The time complexity for
their computation limited the experiments on data
set of just 200 news items. Moreover, we note that
the above tree kernels are not convolution kernels
as those proposed in this article.
In (Shen et al, 2003), a tree-kernel based on
Lexicalized Tree Adjoining Grammar (LTAG) for
the parse-reranking task was proposed. Since
QTK was used for the kernel computation, the
high learning complexity forced the authors to
train different SVMs on different slices of train-
ing data. Our FTK, adapted for the LTAG tree ker-
nel, would have allowed SVMs to be trained on
the whole data.
In (Cumby and Roth, 2003), a feature descrip-
tion language was used to extract structural fea-
tures from the syntactic shallow parse trees asso-
ciated with named entities. The experiments on
the named entity categorization showed that when
the description language selects an adequate set of
tree fragments the Voted Perceptron algorithm in-
creases its classification accuracy. The explana-
tion was that the complete tree fragment set con-
tains many irrelevant features and may cause over-
fitting.
6 Conclusions
In this paper, we have shown that tree kernels
can effectively be adopted in practical natural lan-
guage applications. The main arguments against
their use are their efficiency and accuracy lower
than traditional feature based approaches. We
have shown that a fast algorithm (FTK) can evalu-
ate tree kernels in a linear average running time
and also that the overall converging time re-
quired by SVMs is compatible with very large
data sets. Regarding the accuracy, the experiments
with Support Vector Machines on the PropBank
and FrameNet predicate argument structures show
that: (a) the richer the kernel is in term of substruc-
tures (e.g. SST), the higher the accuracy is, (b)
tree kernels are effective also in case of automatic
parse trees and (c) as kernel combinations always
improve traditional feature models, the best ap-
proach is to combine scalar-based and structured
based kernels.
Acknowledgments
I would like to thank the AI group at the University of Rome
?Tor Vergata?. Many thanks to the EACL 2006 anonymous
reviewers, Roberto Basili and Giorgio Satta who provided
me with valuable suggestions. This research is partially sup-
ported by the Presto Space EU Project#: FP6-507336.
References
Michael Collins and Nigel Duffy. 2002. New ranking al-
gorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In ACL02.
Michael Collins. 1997. Three generative, lexicalized mod-
els for statistical parsing. In proceedings of the ACL97,
Madrid, Spain.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In proceedings of ACL04,
Barcelona, Spain.
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. In proceedings of ICML 2003. Washing-
ton, US.
Charles J. Fillmore. 1982. Frame semantics. In Linguistics
in the Morning Calm.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistic,
28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The necessity of
parsing for predicate argument recognition. In proceed-
ings of ACL02, Philadelphia, PA.
T. Joachims. 1999. Making large-scale SVM learning prac-
tical. In B. Scho?lkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods - Support Vector Learning.
Junichi Kazama and Kentaro Torisawa. 2005. Speeding up
training with tree kernels for node relation labeling. In
proceedings of EMNLP 2005, Toronto, Canada.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to
PropBank. In proceedings of LREC-2002, Spain.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Christopher Watkins. 2000. Text clas-
sification using string kernels. In NIPS02, Vancouver,
Canada.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn
Treebank. Computational Linguistics, 19:313?330.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow semantic parsing. In proceedings ACL04,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005. Sup-
port vector learning for semantic argument classification.
Machine Learning Journal.
Libin Shen, Anoop Sarkar, and Aravind Joshi. 2003. Using
LTAG based features in parse reranking. In proceedings
of EMNLP 2003, Sapporo, Japan.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
proceedings of EMNLP 2004 Barcelona, Spain.
S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on
strings and trees. In proceedings of Neural Information
Processing Systems.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel methods for relation extraction. Journal of Machine
Learning Research.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In proceedings of SI-
GIR?03, ACM Press.
120
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 97?100,
New York, June 2006. c?2006 Association for Computational Linguistics
Syntactic Kernels for Natural Language Learning:
the Semantic Role Labeling Case
Alessandro Moschitti
Department of Computer Science
University of Rome ?Tor Vergata?
Rome, Italy
moschitti@info.uniroma2.it
Abstract
In this paper, we use tree kernels to exploit
deep syntactic parsing information for nat-
ural language applications. We study the
properties of different kernels and we pro-
vide algorithms for their computation in
linear average time. The experiments with
SVMs on the task of predicate argument
classification provide empirical data that
validates our methods.
1 Introduction
Recently, several tree kernels have been applied to
natural language learning, e.g. (Collins and Duffy,
2002; Zelenko et al, 2003; Cumby and Roth, 2003;
Culotta and Sorensen, 2004; Moschitti, 2004). De-
spite their promising results, three general objec-
tions against kernel methods are raised: (1) only a
subset of the dual space features are relevant, thus,
it may be possible to design features in the primal
space that produce the same accuracy with a faster
computation time; (2) in some cases the high num-
ber of features (substructures) of the dual space can
produce overfitting with a consequent accuracy de-
crease (Cumby and Roth, 2003); and (3) the compu-
tation time of kernel functions may be too high and
prevent their application in real scenarios.
In this paper, we study the impact of the sub-
tree (ST) (Vishwanathan and Smola, 2002), subset
tree (SST) (Collins and Duffy, 2002) and partial tree
(PT) kernels on Semantic Role Labeling (SRL). The
PT kernel is a new function that we have designed
to generate larger substructure spaces. Moreover,
to solve the computation problems, we propose al-
gorithms which evaluate the above kernels in linear
average running time.
We experimented such kernels with Support Vec-
tor Machines (SVMs) on the classification of seman-
tic roles of PropBank (Kingsbury and Palmer, 2002)
and FrameNet (Fillmore, 1982) data sets. The re-
sults show that: (1) the kernel approach provides the
same accuracy of the manually designed features.
(2) The overfitting problem does not occur although
the richer space of PTs does not provide better ac-
curacy than the one based on SST. (3) The average
running time of our tree kernel computation is linear.
In the remainder of this paper, Section 2 intro-
duces the different tree kernel spaces. Section 3 de-
scribes the kernel functions and our fast algorithms
for their evaluation. Section 4 shows the compara-
tive performance in terms of execution time and ac-
curacy.
2 Tree kernel Spaces
We consider three different tree kernel spaces: the
subtrees (STs), the subset trees (SSTs) and the novel
partial trees (PTs).
An ST of a tree is rooted in any node and includes
all its descendants. For example, Figure 1 shows the
parse tree of the sentence "Mary brought a cat"
together with its 6 STs. An SST is a more general
structure since its leaves can be associated with non-
terminal symbols. The SSTs satisfy the constraint
that grammatical rules cannot be broken. For exam-
ple, Figure 2 shows 10 SSTs out of 17 of the sub-
tree of Figure 1 rooted in VP. If we relax the non-
breaking rule constraint we obtain a more general
form of substructures, i.e. the PTs. For example,
97
Figure 3 shows 10 out of the total 30 PTs, derived
from the same tree as before.
S 
N 
NP 
D N 
VP 
V Mary 
brought 
a 
   cat 
NP 
D N 
a 
   cat 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
NP 
D N 
VP 
V 
brought 
a 
   cat 
Figure 1: A syntactic parse tree with its subtrees (STs).
NP 
D N 
a 
  cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a 
   cat 
  cat NP D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary ? 
Figure 2: A tree with some of its subset trees (SSTs).
NP 
D N 
VP 
V 
brought 
a 
   cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
a 
   cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
Figure 3: A tree with some of its partial trees (PTs).
3 Fast Tree Kernel Functions
The main idea of tree kernels is to compute the
number of common substructures between two trees
T1 and T2 without explicitly considering the whole
fragment space. We designed a general function
to compute the ST, SST and PT kernels. Our fast
algorithm is inspired by the efficient evaluation of
non-continuous subsequences (described in (Shawe-
Taylor and Cristianini, 2004)). To further increase
the computation speed, we also applied the pre-
selection of node pairs which have non-null kernel.
3.1 Generalized Tree Kernel function
Given a tree fragment space F = {f1, f2, .., fF}, we
use the indicator function Ii(n) which is equal to 1 if
the target fi is rooted at node n and 0 otherwise. We
define the general kernel as:
K(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), (1)
where NT1 and NT2 are the sets of nodes in T1 and
T2, respectively and ?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2),
i.e. the number of common fragments rooted at the
n1 and n2 nodes. We can compute it as follows:
- if the node labels of n1 and n2 are different then
?(n1, n2) = 0;
- else:
?(n1, n2) = 1 +
?
~J1, ~J2,l(~J1)=l(~J2)
l(~J1)?
i=1
?(cn1 [ ~J1i], cn2 [ ~J2i])
(2)
where ~J1 = ?J11, J12, J13, ..? and ~J2 = ?J21, J22, J23, ..?
are index sequences associated with the ordered
child sequences cn1 of n1 and cn2 of n2, respectively,
~J1i and ~J2i point to the i-th children in the two se-
quences, and l(?) returns the sequence length. We
note that (1) Eq. 2 is a convolution kernel accord-
ing to the definition and the proof given in (Haus-
sler, 1999). (2) Such kernel generates a feature
space richer than those defined in (Vishwanathan
and Smola, 2002; Collins and Duffy, 2002; Zelenko
et al, 2003; Culotta and Sorensen, 2004; Shawe-
Taylor and Cristianini, 2004). Additionally, we add
the decay factor as follows: ?(n1, n2) =
?
(
?2+
?
~J1, ~J2,l(~J1)=l(~J2)
?d(~J1)+d(~J2)
l(~J1)?
i=1
?(cn1 [ ~J1i], cn2 [ ~J2i])
)
(3)
where d( ~J1) = ~J1l(~J1) ? ~J11 and d( ~J2) = ~J2l(~J2) ? ~J21.
In this way, we penalize subtrees built on child
subsequences that contain gaps. Moreover, to
have a similarity score between 0 and 1, we also
apply the normalization in the kernel space, i.e.
K?(T1, T2) = K(T1,T2)?K(T1,T1)?K(T2,T2) . As the summation
in Eq. 3 can be distributed with respect to different
types of sequences, e.g. those composed by p
children, it follows that
?(n1, n2) = ?
(
?2 +?lmp=1 ?p(n1, n2)
)
, (4)
where ?p evaluates the number of common subtrees
rooted in subsequences of exactly p children (of n1
and n2) and lm = min{l(cn1), l(cn2)}. Note also that if
we consider only the contribution of the longest se-
quence of node pairs that have the same children, we
implement the SST kernel. For the STs computation
we need also to remove the ?2 term from Eq. 4.
Given the two child sequences c1a = cn1 and
c2b = cn2 (a and b are the last children), ?p(c1a, c2b) =
?(a, b)?
|c1|?
i=1
|c2|?
r=1
?|c1|?i+|c2|?r ??p?1(c1[1 : i], c2[1 : r]),
where c1[1 : i] and c2[1 : r] are the child subse-
quences from 1 to i and from 1 to r of c1 and c2. If
we name the double summation term as Dp, we can
rewrite the relation as:
98
?p(c1a, c2b) =
{
?(a, b)Dp(|c1|, |c2|) if a = b;
0 otherwise.
Note that Dp satisfies the recursive relation:
Dp(k, l) = ?p?1(s[1 : k], t[1 : l]) + ?Dp(k, l ? 1)
+?Dp(k ? 1, l) + ?2Dp(k ? 1, l ? 1).
By means of the above relation, we can compute
the child subsequences of two sets c1 and c2 in
O(p|c1||c2|). This means that the worst case com-
plexity of the PT kernel is O(p?2|NT1 ||NT2 |), where
? is the maximum branching factor of the two trees.
Note that the average ? in natural language parse
trees is very small and the overall complexity can be
reduced by avoiding the computation of node pairs
with different labels. The next section shows our fast
algorithm to find non-null node pairs.
3.2 Fast non-null node pair computation
To compute the kernels defined in the previous sec-
tion, we sum the ? function for each pair ?n1, n2??
NT1 ? NT2 (Eq. 1). When the labels associated
with n1 and n2 are different, we can avoid evaluating
?(n1, n2) since it is 0. Thus, we look for a node pair
set Np ={?n1, n2?? NT1 ?NT2 : label(n1) = label(n2)}.
To efficiently build Np, we (i) extract the L1 and
L2 lists of nodes from T1 and T2, (ii) sort them in
alphanumeric order and (iii) scan them to find Np.
Step (iii) may require only O(|NT1 |+ |NT2 |) time, but,
if label(n1) appears r1 times in T1 and label(n2) is re-
peated r2 times in T2, we need to consider r1 ? r2
pairs. The formal can be found in (Moschitti, 2006).
4 The Experiments
In these experiments, we study tree kernel perfor-
mance in terms of average running time and accu-
racy on the classification of predicate arguments. As
shown in (Moschitti, 2004), we can label seman-
tic roles by classifying the smallest subtree that in-
cludes the predicate with one of its arguments, i.e.
the so called PAF structure.
The experiments were carried out with
the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes the fast tree kernels in the SVM-light
software (Joachims, 1999). The multiclassifiers
were obtained by training an SVM for each class
in the ONE-vs.-ALL fashion. In the testing phase,
we selected the class associated with the maximum
SVM score.
For the ST, SST and PT kernels, we found that the
best ? values (see Section 3) on the development set
were 1, 0.4 and 0.8, respectively, whereas the best ?
was 0.4.
4.1 Kernel running time experiments
To study the FTK running time, we extracted from
the Penn Treebank several samples of 500 trees con-
taining exactly n nodes. Each point of Figure 4
shows the average computation time1 of the kernel
function applied to the 250,000 pairs of trees of size
n. It clearly appears that the FTK-SST and FTK-PT
(i.e. FTK applied to the SST and PT kernels) av-
erage running time has linear behavior whereas, as
expected, the na??ve SST algorithm shows a quadratic
curve.
0
20
40
60
80
100
120
5 10 15 20 25 30 35 40 45 50 55Number of Tree Nodes
?? ??sec
ond
s
FTK-SST
naive-SSTFTK-PT
Figure 4: Average time in ?seconds for the na??ve SST kernel,
FTK-SST and FTK-PT evaluations.
4.2 Experiments on SRL dataset
We used two different corpora: PropBank
(www.cis.upenn.edu/?ace) along with Penn
Treebank 2 (Marcus et al, 1993) and FrameNet.
PropBank contains about 53,700 sentences and
a fixed split between training and testing used in
other researches. In this split, sections from 02 to
21 are used for training, section 23 for testing and
section 22 as development set. We considered a
total of 122,774 and 7,359 arguments (from Arg0
to Arg5, ArgA and ArgM) in training and testing,
respectively. The tree structures were extracted
from the Penn Treebank.
From the FrameNet corpus (www.icsi.
berkeley.edu/?framenet) we extracted all
1We run the experiments on a Pentium 4, 2GHz, with 1 Gb
ram.
99
0.75
0.78
0.80
0.83
0.85
0.88
0 10 20 30 40 50 60 70 80 90 100% Training Data
Accu
rac
y
ST SSTLinear PT
Figure 5: Multiclassifier accuracy according to different train-
ing set percentage.
24,558 sentences of the 40 Frames selected for
the Automatic Labeling of Semantic Roles task of
Senseval 3 (www.senseval.org). We considered
the 18 most frequent roles, for a total of 37,948
examples (30% of the sentences for testing and
70% for training/validation). The sentences were
processed with the Collins? parser (Collins, 1997)
to generate automatic parse trees.
We run ST, SST and PT kernels along with
the linear kernel of standard features (Carreras and
Ma`rquez, 2005) on PropBank training sets of dif-
ferent size. Figure 5 illustrates the learning curves
associated with the above kernels for the SVM mul-
ticlassifiers.
The tables 1 and 2 report the results, using all
available training data, on PropBank and FrameNet
test sets, respectively. We note that: (1) the accu-
racy of PTs is almost equal to the one produced by
SSTs as the PT space is a hyperset of SSTs. The
small difference is due to the poor relevance of the
substructures in the PT ? SST set, which degrade
the PT space. (2) The high F1 measures of tree ker-
nels on FrameNet suggest that they are robust with
respect to automatic parse trees.
Moreover, the learning time of SVMs using FTK
for the classification of one large argument (Arg 0)
is much lower than the one required by na??ve algo-
rithm. With all the training data FTK terminated in
6 hours whereas the na??ve approach required more
than 1 week. However, the complexity burden of
working in the dual space can be alleviated with re-
cent approaches proposed in (Kudo and Matsumoto,
2003; Suzuki et al, 2004).
Finally, we carried out some experiments with the
combination between linear and tree kernels and we
found that tree kernels improve the models based on
manually designed features by 2/3 percent points,
thus they can be seen as a useful tactic to boost sys-
tem accuracy.
Args Linear ST SST PT
Acc. 87.6 84.6 87.7 86.7
Table 1: Evaluation of kernels on PropBank data and gold
parse trees.
Roles Linear ST SST PT
Acc. 82.3 80.0 81.2 79.9
Table 2: Evaluation of kernels on FrameNet data encoded in
automatic parse trees.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL05.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL02.
Michael Collins. 1997. Three generative, lexicalized models
for statistical parsing. In Proceedings of the ACL97.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of ACL04.
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. In Proceedings of ICML03.
Charles J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm.
D. Haussler. 1999. Convolution kernels on discrete struc-
tures. Technical report ucs-crl-99-10, University of Califor-
nia Santa Cruz.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC02.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL03.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn Tree-
bank. Computational Linguistics.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In proceedings of ACL04.
Alessandro Moschitti. 2006. Making tree kernels practical for
natural language learning. In Proceedings of EACL06.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Jun Suzuki, Hideki Isozaki, and Eisaku Maeda. 2004. Con-
volution kernels with feature selection for natural language
processing tasks. In Proceedings of ACL04.
S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on
strings and trees. In Proceedings of NIPS02.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. JMLR.
100
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 401?408,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic learning of textual entailments with cross-pair similarities
Fabio Massimo Zanzotto
DISCo
University of Milano-Bicocca
Milan, Italy
zanzotto@disco.unimib.it
Alessandro Moschitti
Department of Computer Science
University of Rome ?Tor Vergata?
Rome, Italy
moschitti@info.uniroma2.it
Abstract
In this paper we define a novel similarity
measure between examples of textual en-
tailments and we use it as a kernel func-
tion in Support Vector Machines (SVMs).
This allows us to automatically learn the
rewrite rules that describe a non trivial set
of entailment cases. The experiments with
the data sets of the RTE 2005 challenge
show an improvement of 4.4% over the
state-of-the-art methods.
1 Introduction
Recently, textual entailment recognition has been
receiving a lot of attention. The main reason is
that the understanding of the basic entailment pro-
cesses will allow us to model more accurate se-
mantic theories of natural languages (Chierchia
and McConnell-Ginet, 2001) and design important
applications (Dagan and Glickman, 2004), e.g.,
Question Answering and Information Extraction.
However, previous work (e.g., (Zaenen et al,
2005)) suggests that determining whether or not
a text T entails a hypothesis H is quite complex
even when all the needed information is explic-
itly asserted. For example, the sentence T1: ?At
the end of the year, all solid companies pay divi-
dends.? entails the hypothesis H1: ?At the end of
the year, all solid insurance companies pay divi-
dends.? but it does not entail the hypothesis H2:
?At the end of the year, all solid companies pay
cash dividends.?
Although these implications are uncontrover-
sial, their automatic recognition is complex if we
rely on models based on lexical distance (or sim-
ilarity) between hypothesis and text, e.g., (Corley
and Mihalcea, 2005). Indeed, according to such
approaches, the hypotheses H1 and H2 are very
similar and seem to be similarly related to T1. This
suggests that we should study the properties and
differences of such two examples (negative and
positive) to derive more accurate entailment mod-
els. For example, if we consider the following en-
tailment:
T3 ? H3?
T3 ?All wild animals eat plants that have
scientifically proven medicinal proper-
ties.?
H3 ?All wild mountain animals eat plants
that have scientifically proven medici-
nal properties.?
we note that T3 is structurally (and somehow lex-
ically similar) to T1 and H3 is more similar to H1
than to H2. Thus, from T1 ? H1 we may extract
rules to derive that T3 ? H3.
The above example suggests that we should rely
not only on a intra-pair similarity between T and
H but also on a cross-pair similarity between two
pairs (T ?,H ?) and (T ??,H ??). The latter similarity
measure along with a set of annotated examples al-
lows a learning algorithm to automatically derive
syntactic and lexical rules that can solve complex
entailment cases.
In this paper, we define a new cross-pair similar-
ity measure based on text and hypothesis syntactic
trees and we use such similarity with traditional
intra-pair similarities to define a novel semantic
kernel function. We experimented with such ker-
nel using Support Vector Machines (Vapnik, 1995)
on the test tests of the Recognizing Textual En-
tailment (RTE) challenges (Dagan et al, 2005;
Bar Haim et al, 2006). The comparative results
show that (a) we have designed an effective way
to automatically learn entailment rules from ex-
amples and (b) our approach is highly accurate and
exceeds the accuracy of the current state-of-the-art
401
models (Glickman et al, 2005; Bayer et al, 2005)
by about 4.4% (i.e. 63% vs. 58.6%) on the RTE 1
test set (Dagan et al, 2005).
In the remainder of this paper, Sec. 2 illustrates
the related work, Sec. 3 introduces the complexity
of learning entailments from examples, Sec. 4 de-
scribes our models, Sec. 6 shows the experimental
results and finally Sec. 7 derives the conclusions.
2 Related work
Although the textual entailment recognition prob-
lem is not new, most of the automatic approaches
have been proposed only recently. This has been
mainly due to the RTE challenge events (Dagan et
al., 2005; Bar Haim et al, 2006). In the following
we report some of such researches.
A first class of methods defines measures of
the distance or similarity between T and H ei-
ther assuming the independence between words
(Corley and Mihalcea, 2005; Glickman et al,
2005) in a bag-of-word fashion or exploiting syn-
tactic interpretations (Kouylekov and Magnini,
2005). A pair (T,H) is then in entailment when
sim(T,H) > ?. These approaches can hardly
determine whether the entailment holds in the ex-
amples of the previous section. From the point of
view of bag-of-word methods, the pairs (T1,H1)
and (T1,H2) have both the same intra-pair simi-
larity since the sentences of T1 and H1 as well as
those of T1 and H2 differ by a noun, insurance and
cash, respectively. At syntactic level, also, we can-
not capture the required information as such nouns
are both noun modifiers: insurance modifies com-
panies and cash modifies dividends.
A second class of methods can give a solution
to the previous problem. These methods generally
combine a similarity measure with a set of possi-
ble transformations T applied over syntactic and
semantic interpretations. The entailment between
T and H is detected when there is a transformation
r ? T so that sim(r(T ),H) > ?. These trans-
formations are logical rules in (Bos and Markert,
2005) or sequences of allowed rewrite rules in (de
Salvo Braz et al, 2005). The disadvantage is that
such rules have to be manually designed. More-
over, they generally model better positive implica-
tions than negative ones and they do not consider
errors in syntactic parsing and semantic analysis.
3 Challenges in learning from examples
In the introductory section, we have shown that,
to carry out automatic learning from examples, we
need to define a cross-pair similarity measure. Its
definition is not straightforward as it should detect
whether two pairs (T ?,H ?) and (T ??,H ??) realize
the same rewrite rules. This measure should con-
sider pairs similar when: (1) T ? and H ? are struc-
turally similar to T ?? and H ??, respectively and (2)
the lexical relations within the pair (T ?,H ?) are
compatible with those in (T ??,H ??). Typically, T
and H show a certain degree of overlapping, thus,
lexical relations (e.g., between the same words)
determine word movements from T to H (or vice
versa). This is important to model the syntac-
tic/lexical similarity between example pairs. In-
deed, if we encode such movements in the syntac-
tic parse trees of texts and hypotheses, we can use
interesting similarity measures defined for syntac-
tic parsing, e.g., the tree kernel devised in (Collins
and Duffy, 2002).
To consider structural and lexical relation simi-
larity, we augment syntactic trees with placehold-
ers which identify linked words. More in detail:
- We detect links between words wt in T that are
equal, similar, or semantically dependent on words
wh in H . We call anchors the pairs (wt, wh) and
we associate them with placeholders. For exam-
ple, in Fig. 1, the placeholder 2? indicates the
(companies,companies) anchor between T1 and
H1. This allows us to derive the word movements
between text and hypothesis.
- We align the trees of the two texts T ? and T ?? as
well as the tree of the two hypotheses H ? and H ??
by considering the word movements. We find a
correct mapping between placeholders of the two
hypothesis H ? and H ?? and apply it to the tree of
H ?? to substitute its placeholders. The same map-
ping is used to substitute the placeholders in T ??.
This mapping should maximize the structural sim-
ilarity between the four trees by considering that
placeholders augment the node labels. Hence, the
cross-pair similarity computation is reduced to the
tree similarity computation.
The above steps define an effective cross-pair
similarity that can be applied to the example in
Fig. 1: T1 and T3 share the subtree in bold start-
ing with S ? NP VP. The lexicals in T3 and H3
are quite different from those T1 and H1, but we
can rely on the structural properties expressed by
their bold subtrees. These are more similar to the
subtrees of T1 and H1 than those of T1 and H2,
respectively. Indeed, H1 and H3 share the pro-
duction NP ? DT JJ NN NNS while H2 and H3 do
402
T1 T3
S
PP
IN
At
NP 0
NP 0
DT
the
NN 0
end
0
PP
IN
of
NP 1
DT
the
NN 1
year
1
,
,
NP 2
DT
all
JJ 2
solid
2?
NNS 2
companies
2?
VP 3
VBP 3
pay
3
NP 4
NNS 4
dividends
4
S
NP a
DT
All
JJ a
wild
a?
NNS a
animals
a?
VP b
VBP b
eat
b
NP c
plants
c ... properties
H1 H3
S
PP
IN
At
NP 0
NP 0
DT
the
NN 0
end
0
PP
IN
of
NP 1
DT
the
NN 1
year
1
,
,
NP 2
DT
all
JJ 2
solid
2?
NN
insurance
NNS 2
companies
2?
VP 3
VBP 3
pay
3
NP 4
NNS 4
dividends
4
S
NP a
DT
All
JJ a
wild
a?
NN
mountain
NNS a
animals
a?
VP b
VBP b
eat
b
NP c
plants
c ... properties
H2 H3
S
PP
At ... year
NP 2
DT
all
JJ 2
solid
2?
NNS 2
companies
2?
VP 3
VBP 3
pay
3
NP 4
NN
cash
NNS 4
dividends
4
S
NP a
DT
All
JJ a
wild
a?
NN
mountain
NNS a
animals
a?
VP b
VBP b
eat
b
NP c
plants
c ... properties
Figure 1: Relations between (T1,H1), (T1,H2), and (T3,H3).
not. Consequently, to decide if (T3,H3) is a valid
entailment, we should rely on the decision made
for (T1,H1). Note also that the dashed lines con-
necting placeholders of two texts (hypotheses) in-
dicate structurally equivalent nodes. For instance,
the dashed line between 3 and b links the main
verbs both in the texts T1 and T3 and in the hy-
potheses H1 and H3. After substituting 3 with b
and 2 with a , we can detect if T1 and T3 share
the bold subtree S ? NP 2 VP 3 . As such subtree
is shared also by H1 and H3, the words within the
pair (T1,H1) are correlated similarly to the words
in (T3,H3).
The above example emphasizes that we need
to derive the best mapping between placeholder
sets. It can be obtained as follows: let A? and A??
be the placeholders of (T ?,H ?) and (T ??,H ??), re-
spectively, without loss of generality, we consider
|A?| ? |A??| and we align a subset of A? to A??. The
best alignment is the one that maximizes the syn-
tactic and lexical overlapping of the two subtrees
induced by the aligned set of anchors.
More precisely, let C be the set of all bijective
mappings from a? ? A? : |a?| = |A??| to A??, an
element c ? C is a substitution function. We
define as the best alignment the one determined
by cmax = argmaxc?C(KT (t(H ?, c), t(H ??, i))+
KT (t(T ?, c), t(T ??, i)) (1)
where (a) t(S, c) returns the syntactic tree of the
hypothesis (text) S with placeholders replaced by
means of the substitution c, (b) i is the identity
substitution and (c) KT (t1, t2) is a function that
measures the similarity between the two trees t1
and t2 (for more details see Sec. 4.2). For ex-
ample, the cmax between (T1,H1) and (T3,H3)
is {( 2? , a? ), ( 2? , a? ), ( 3 , b ), ( 4 , c )}.
4 Similarity Models
In this section we describe how anchors are found
at the level of a single pair (T,H) (Sec. 4.1). The
anchoring process gives the direct possibility of
403
implementing an inter-pair similarity that can be
used as a baseline approach or in combination with
the cross-pair similarity. This latter will be imple-
mented with tree kernel functions over syntactic
structures (Sec. 4.2).
4.1 Anchoring and Lexical Similarity
The algorithm that we design to find the anchors
is based on similarity functions between words or
more complex expressions. Our approach is in line
with many other researches (e.g., (Corley and Mi-
halcea, 2005; Glickman et al, 2005)).
Given the set of content words (verbs, nouns,
adjectives, and adverbs) WT and WH of the two
sentences T and H , respectively, the set of anchors
A ? WT ?WH is built using a similarity measure
between two words simw(wt, wh). Each element
wh ? WH will be part of a pair (wt, wh) ? A if:
1) simw(wt, wh) 6= 0
2) simw(wt, wh) = maxw?t?WT simw(w?t, wh)
According to these properties, elements in WH
can participate in more than one anchor and con-
versely more than one element in WH can be
linked to a single element w ? WT .
The similarity simw(wt, wh) can be defined us-
ing different indicators and resources. First of all,
two words are maximally similar if these have the
same surface form wt = wh. Second, we can use
one of the WordNet (Miller, 1995) similarities in-
dicated with d(lw, lw?) (in line with what was done
in (Corley and Mihalcea, 2005)) and different rela-
tion between words such as the lexical entailment
between verbs (Ent) and derivationally relation
between words (Der). Finally, we use the edit dis-
tance measure lev(wt, wh) to capture the similar-
ity between words that are missed by the previous
analysis for misspelling errors or for the lack of
derivationally forms not coded in WordNet.
As result, given the syntactic category
cw ? {noun, verb, adjective, adverb} and
the lemmatized form lw of a word w, the simi-
larity measure between two words w and w? is
defined as follows:
simw(w,w?) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1 if w = w??
lw = lw? ? cw = cw??
((lw, cw), (lw? , cw? )) ? Ent?
((lw, cw), (lw? , cw? )) ? Der?
lev(w, w?) = 1
d(lw, lw? ) if cw = cw? ? d(lw, lw? ) > 0.2
0 otherwise
(2)
It is worth noticing that, the above measure is not
a pure similarity measure as it includes the entail-
ment relation that does not represent synonymy or
similarity between verbs. To emphasize the contri-
bution of each used resource, in the experimental
section, we will compare Eq. 2 with some versions
that exclude some word relations.
The above word similarity measure can be used
to compute the similarity between T and H . In
line with (Corley and Mihalcea, 2005), we define
it as:
s1(T,H) =
?
(wt,wh)?A
simw(wt, wh)? idf(wh)
?
wh?WH
idf(wh)
(3)
where idf(w) is the inverse document frequency
of the word w. For sake of comparison, we
consider also the corresponding more classical
version that does not apply the inverse document
frequency
s2(T,H) =
?
(wt,wh)?A
simw(wt, wh)/|WH | (4)
?From the above intra-pair similarities, s1
and s2, we can obtain the baseline cross-pair
similarities based on only lexical information:
Ki((T ?, H ?), (T ??,H ??)) = si(T ?, H ?)? si(T ??,H ??), (5)
where i ? {1, 2}. In the next section we define a
novel cross-pair similarity that takes into account
syntactic evidence by means of tree kernel func-
tions.
4.2 Cross-pair syntactic kernels
Section 3 has shown that to measure the syn-
tactic similarity between two pairs, (T ?,H ?)
and (T ??,H ??), we should capture the number of
common subtrees between texts and hypotheses
that share the same anchoring scheme. The best
alignment between anchor sets, i.e. the best
substitution cmax, can be found with Eq. 1. As the
corresponding maximum quantifies the alignment
degree, we could define a cross-pair similarity as
follows:
Ks((T ?,H ?), (T ??,H ??)) = max
c?C
(
KT (t(H ?, c), t(H ??, i))
+KT (t(T ?, c), t(T ??, i)
)
, (6)
where as KT (t1, t2) we use the tree kernel func-
tion defined in (Collins and Duffy, 2002). This
evaluates the number of subtrees shared by t1 and
t2, thus defining an implicit substructure space.
Formally, given a subtree space F =
{f1, f2, . . . , f|F|}, the indicator function Ii(n)
is equal to 1 if the target fi is rooted at
node n and equal to 0 otherwise. A tree-
kernel function over t1 and t2 is KT (t1, t2) =
?
n1?Nt1
?
n2?Nt2
?(n1, n2), where Nt1 and Nt2
are the sets of the t1?s and t2?s nodes, respectively.
In turn ?(n1, n2) =
?|F|
i=1 ?l(fi)Ii(n1)Ii(n2),
404
where 0 ? ? ? 1 and l(fi) is the number of lev-
els of the subtree fi. Thus ?l(fi) assigns a lower
weight to larger fragments. When ? = 1, ? is
equal to the number of common fragments rooted
at nodes n1 and n2. As described in (Collins and
Duffy, 2002), ? can be computed in O(|Nt1 | ?
|Nt2 |).
The KT function has been proven to be a valid
kernel, i.e. its associated Gram matrix is positive-
semidefinite. Some basic operations on kernel
functions, e.g. the sum, are closed with respect
to the set of valid kernels. Thus, if the maximum
held such property, Eq. 6 would be a valid ker-
nel and we could use it in kernel based machines
like SVMs. Unfortunately, a counterexample il-
lustrated in (Boughorbel et al, 2004) shows that
the max function does not produce valid kernels in
general.
However, we observe that: (1)
Ks((T ?,H ?), (T ??,H ??)) is a symmetric func-
tion since the set of transformation C are always
computed with respect to the pair that has the
largest anchor set; (2) in (Haasdonk, 2005), it
is shown that when kernel functions are not
positive semidefinite, SVMs still solve a data
separation problem in pseudo Euclidean spaces.
The drawback is that the solution may be only
a local optimum. Therefore, we can experiment
Eq. 6 with SVMs and observe if the empirical
results are satisfactory. Section 6 shows that the
solutions found by Eq. 6 produce accuracy higher
than those evaluated on previous automatic textual
entailment recognition approaches.
5 Refining cross-pair syntactic similarity
In the previous section we have defined the intra
and the cross pair similarity. The former does not
show relevant implementation issues whereas the
latter should be optimized to favor its applicability
with SVMs. The Eq. 6 improvement depends on
three factors: (1) its computation complexity; (2)
a correct marking of tree nodes with placeholders;
and, (3) the pruning of irrelevant information in
large syntactic trees.
5.1 Controlling the computational cost
The computational cost of cross-pair similarity be-
tween two tree pairs (Eq. 6) depends on the size of
C . This is combinatorial in the size of A? and A??,
i.e. |C| = (|A?|? |A??|)!|A??|! if |A?| ? |A??|. Thus
we should keep the sizes of A? and A?? reasonably
small.
To reduce the number of placeholders, we con-
sider the notion of chunk defined in (Abney, 1996),
i.e., not recursive kernels of noun, verb, adjective,
and adverb phrases. When placeholders are in a
single chunk both in the text and hypothesis we
assign them the same name. For example, Fig. 1
shows the placeholders 2? and 2? that are substi-
tuted by the placeholder 2 . The placeholder re-
duction procedure also gives the possibility of re-
solving the ambiguity still present in the anchor
set A (see Sec. 4.1). A way to eliminate the am-
biguous anchors is to select the ones that reduce
the final number of placeholders.
5.2 Augmenting tree nodes with placeholders
Anchors are mainly used to extract relevant syn-
tactic subtrees between pairs of text and hypoth-
esis. We also use them to characterize the syn-
tactic information expressed by such subtrees. In-
deed, Eq. 6 depends on the number of common
subtrees between two pairs. Such subtrees are
matched when they have the same node labels.
Thus, to keep track of the argument movements,
we augment the node labels with placeholders.
The larger number of placeholders two hypothe-
ses (texts) match the larger the number of their
common substructures is (i.e. higher similarity).
Thus, it is really important where placeholders are
inserted.
For example, the sentences in the pair (T1,H1)
have related subjects 2 and related main verbs
3 . The same occurs in the sentences of the pair
(T3,H3), respectively a and b . To obtain such
node marking, the placeholders are propagated in
the syntactic tree, from the leaves1 to the target
nodes according to the head of constituents. The
example of Fig. 1 shows that the placeholder 0
climbs up to the node governing all the NPs.
5.3 Pruning irrelevant information in large
text trees
Often only a portion of the parse trees is relevant
to detect entailments. For instance, let us consider
the following pair from the RTE 2005 corpus:
1To increase the generalization capacity of the tree ker-
nel function we choose not to assign any placeholder to the
leaves.
405
T ? H (id: 929)
T ?Ron Gainsford, chief executive of the
TSI, said: ?It is a major concern to us
that parents could be unwittingly expos-
ing their children to the risk of sun dam-
age, thinking they are better protected
than they actually are.?
H ?Ron Gainsford is the chief executive of
the TSI.?
Only the bold part of T supports the implication;
the rest is useless and also misleading: if we used
it to compute the similarity it would reduce the im-
portance of the relevant part. Moreover, as we nor-
malize the syntactic tree kernel (KT ) with respect
to the size of the two trees, we need to focus only
on the part relevant to the implication.
The anchored leaves are good indicators of rel-
evant parts but also some other parts may be very
relevant. For example, the function word not plays
an important role. Another example is given by the
word insurance in H1 and mountain in H3 (see
Fig. 1). They support the implication T1 ? H1
and T1 ? H3 as well as cash supports T1 ; H2.
By removing these words and the related struc-
tures, we cannot determine the correct implica-
tions of the first two and the incorrect implication
of the second one. Thus, we keep all the words that
are immediately related to relevant constituents.
The reduction procedure can be formally ex-
pressed as follows: given a syntactic tree t, the set
of its nodes N(t), and a set of anchors, we build
a tree t? with all the nodes N ? that are anchors or
ancestors of any anchor. Moreover, we add to t?
the leaf nodes of the original tree t that are direct
children of the nodes in N ?. We apply such proce-
dure only to the syntactic trees of texts before the
computation of the kernel function.
6 Experimental investigation
The aim of the experiments is twofold: we show
that (a) entailment recognition rules can be learned
from examples and (b) our kernel functions over
syntactic structures are effective to derive syntac-
tic properties. The above goals can be achieved by
comparing the different intra and cross pair simi-
larity measures.
6.1 Experimental settings
For the experiments, we used the Recognizing
Textual Entailment Challenge data sets, which we
name as follows:
- D1, T1 and D2, T2, are the development and
the test sets of the first (Dagan et al, 2005) and
second (Bar Haim et al, 2006) challenges, respec-
tively. D1 contains 567 examples whereas T1,
D2 and T2 have all the same size, i.e. 800 train-
ing/testing instances. The positive examples con-
stitute the 50% of the data.
- ALL is the union of D1, D2, and T1, which we
also split in 70%-30%. This set is useful to test if
we can learn entailments from the data prepared in
the two different challenges.
- D2(50%)? and D2(50%)?? is a random split of
D2. It is possible that the data sets of the two com-
petitions are quite different thus we created this
homogeneous split.
We also used the following resources:
- The Charniak parser (Charniak, 2000) and the
morpha lemmatiser (Minnen et al, 2001) to carry
out the syntactic and morphological analysis.
- WordNet 2.0 (Miller, 1995) to extract both the
verbs in entailment, Ent set, and the derivation-
ally related words, Der set.
- The wn::similarity package (Pedersen et
al., 2004) to compute the Jiang&Conrath (J&C)
distance (Jiang and Conrath, 1997) as in (Corley
and Mihalcea, 2005). This is one of the best fig-
ure method which provides a similarity score in
the [0, 1] interval. We used it to implement the
d(lw, lw?) function.
- A selected portion of the British National Cor-
pus2 to compute the inverse document frequency
(idf ). We assigned the maximum idf to words not
found in the BNC.
- SVM-light-TK3 (Moschitti, 2006) which en-
codes the basic tree kernel function, KT , in SVM-
light (Joachims, 1999). We used such software
to implement Ks (Eq. 6), K1, K2 (Eq. 5) and
Ks + Ki kernels. The latter combines our new
kernel with traditional approaches (i ? {1, 2}).
6.2 Results and analysis
Table 1 reports the results of different similarity
kernels on the different training and test splits de-
scribed in the previous section. The table is orga-
nized as follows:
The first 5 rows (Experiment settings) report the
intra-pair similarity measures defined in Section
4.1, the 6th row refers to only the idf similarity
metric whereas the following two rows report the
cross-pair similarity carried out with Eq. 6 with
(Synt Trees with placeholders) and without (Only
Synt Trees) augmenting the trees with placehold-
ers, respectively. Each column in the Experiment
2http://www.natcorp.ox.ac.uk/
3SVM-light-TK is available at http://ai-nlp.info
.uniroma2.it/moschitti/
406
Experiment Settings
w = w? ? lw = lw? ? cw = cw?
? ? ? ? ? ? ? ?
cw = cw? ? d(lw, lw? ) > 0.2
? ? ? ? ? ?
((lw , cw), (lw? , cw? )) ? Der
? ? ? ?
((lw , cw), (lw? , cw? )) ? Ent
? ? ? ?
lev(w, w?) = 1 ? ? ?
idf ? ? ? ? ? ?
Only Synt Trees
?
Synt Trees with placeholders
?
Datasets
?Train:D1-Test:T1? 0.5388 0.5813 0.5500 0.5788 0.5900 0.5888 0.6213 0.6300
?Train:T1-Test:D1? 0.5714 0.5538 0.5767 0.5450 0.5591 0.5644 0.5732 0.5838
?Train:D2(50%)?-Test:D2(50%)??? 0.6034 0.5961 0.6083 0.6010 0.6083 0.6083 0.6156 0.6350
?Train:D2(50%)?? -Test:D2(50%)?? 0.6452 0.6375 0.6427 0.6350 0.6324 0.6272 0.5861 0.6607
?Train:D2-Test:T2? 0.6000 0.5950 0.6025 0.6050 0.6050 0.6038 0.6238 0.6388
Mean 0.5918 0.5927 0.5960 0.5930 0.5990 0.5985 0.6040 0.6297
(? 0.0396 ) (? 0.0303 ) (? 0.0349 ) (? 0.0335 ) (? 0.0270 ) (? 0.0235 ) (? 0.0229 ) (? 0.0282 )
?Train:ALL(70%)-Test:ALL(30%)? 0.5902 0.6024 0.6009 - 0.6131 0.6193 0.6086 0.6376
?Train:ALL-Test:T2? 0.5863 0.5975 0.5975 0.6038 - - 0.6213 0.6250
Table 1: Experimental results of the different methods over different test settings
settings indicates a different intra-pair similarity
measure built by means of a combination of basic
similarity approaches. These are specified with the
check sign ?. For example, Column 5 refers to a
model using: the surface word form similarity, the
d(lw, lw?) similarity and the idf .
The next 5 rows show the accuracy on the data
sets and splits used for the experiments and the
next row reports the average and Std. Dev. over
the previous 5 results. Finally, the last two rows
report the accuracy on ALL dataset split in 70/30%
and on the whole ALL dataset used for training
and T2 for testing.
?From the table we note the following aspects:
- First, the lexical-based distance kernels K1 and
K2 (Eq. 5) show accuracy significantly higher than
the random baseline, i.e. 50%. In all the datasets
(except for the first one), the simw(T,H) simi-
larity based on the lexical overlap (first column)
provides an accuracy essentially similar to the best
lexical-based distance method.
- Second, the dataset ?Train:D1-Test:T1? allows
us to compare our models with the ones of the first
RTE challenge (Dagan et al, 2005). The accuracy
reported for the best systems, i.e. 58.6% (Glick-
man et al, 2005; Bayer et al, 2005), is not signif-
icantly different from the result obtained with K1
that uses the idf .
- Third, the dramatic improvement observed in
(Corley and Mihalcea, 2005) on the dataset
?Train:D1-Test:T1? is given by the idf rather than
the use of the J&C similarity (second vs. third
columns). The use of J&C with the idf decreases
the accuracy of the idf alone.
- Next, our approach (last column) is significantly
better than all the other methods as it provides the
best result for each combination of training and
test sets. On the ?Train:D1-Test:T1? test set, it
exceeds the accuracy of the current state-of-the-
art models (Glickman et al, 2005; Bayer et al,
2005) by about 4.4 absolute percent points (63%
vs. 58.6%) and 4% over our best lexical simi-
larity measure. By comparing the average on all
datasets, our system improves on all the methods
by at least 3 absolute percent points.
- Finally, the accuracy produced by Synt Trees with
placeholders is higher than the one obtained with
Only Synt Trees. Thus, the use of placeholders
is fundamental to automatically learn entailments
from examples.
6.2.1 Qualitative analysis
Hereafter we show some instances selected
from the first experiment ?Train:T1-Test:D1?.
They were correctly classified by our overall
model (last column) and miss-classified by the
models in the seventh and in the eighth columns.
The first is an example in entailment:
T ? H (id: 35)
T ?Saudi Arabia, the biggest oil pro-
ducer in the world, was once a sup-
porter of Osama bin Laden and his
associates who led attacks against the
United States.?
H ?Saudi Arabia is the world?s biggest oil
exporter.?
It was correctly classified by exploiting examples
like these two:
T ? H (id: 929)
T ?Ron Gainsford, chief executive of the
TSI, said: ...?
H ?Ron Gainsford is the chief executive of
the TSI.?
T ? H (id: 976)
T ?Harvey Weinstein, the co-chairman of
Miramax, who was instrumental in pop-
ularizing both independent and foreign
films with broad audiences, agrees.?
H ?Harvey Weinstein is the co-chairman
of Miramax.?
407
The rewrite rule is: ?X, Y, ...? implies ?X is Y?.
This rule is also described in (Hearst, 1992).
A more interesting rule relates the following
two sentences which are not in entailment:
T ; H (id: 2045)
T ?Mrs. Lane, who has been a Director
since 1989, is Special Assistant to the
Board of Trustees and to the President
of Stanford University.?
H ?Mrs. Lane is the president of Stanford
University.?
It was correctly classified using instances like the
following:
T ; H (id: 2044)
T ?Jacqueline B. Wender is Assistant to
the President of Stanford University.?
H ?Jacqueline B. Wender is the President
of Stanford University.?
T ; H (id: 2069)
T ?Grieving father Christopher Yavelow
hopes to deliver one million letters to
the queen of Holland to bring his chil-
dren home.?
H ?Christopher Yavelow is the queen of
Holland.?
Here, the implicit rule is: ?X (VP (V ...) (NP (to Y)
...)? does not imply ?X is Y?.
7 Conclusions
We have presented a model for the automatic
learning of rewrite rules for textual entailments
from examples. For this purpose, we devised a
novel powerful kernel based on cross-pair simi-
larities. We experimented with such kernel us-
ing Support Vector Machines on the RTE test
sets. The results show that (1) learning entailments
from positive and negative examples is a viable ap-
proach and (2) our model based on kernel meth-
ods is highly accurate and improves on the current
state-of-the-art entailment systems.
In the future, we would like to study approaches
to improve the computational complexity of our
kernel function and to design approximated ver-
sions that are valid Mercer?s kernels.
References
Steven Abney. 1996. Part-of-speech tagging and partial pars-
ing. In G.Bloothooft K.Church, S.Young, editor, Corpus-
based methods in language and speech. Kluwer academic
publishers, Dordrecht.
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.
The II PASCAL RTE challenge. In RTE Workshop,
Venice, Italy.
Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and
Alexander Yeh. 2005. MITRE?s submissions to the eu
PASCAL RTE challenge. In Proceedings of the 1st RTE
Workshop, Southampton, UK.
Johan Bos and Katja Markert. 2005. Recognising textual en-
tailment with logical inference. In Proc. of HLT-EMNLP
Conference, Canada.
S. Boughorbel, J-P. Tarel, and F. Fleuret. 2004. Non-mercer
kernel for svm object recognition. In Proceedings of
BMVC 2004.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the 1st NAACL,Seattle, Washington.
Gennaro Chierchia and Sally McConnell-Ginet. 2001.
Meaning and Grammar: An introduction to Semantics.
MIT press, Cambridge, MA.
Michael Collins and Nigel Duffy. 2002. New ranking al-
gorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of
ACL02.
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and En-
tailment, Ann Arbor, Michigan.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In Proceedings of the Workshop on Learning
Methods for Text Understanding and Mining, Grenoble,
France.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The PASCAL RTE challenge. In RTE Workshop,
Southampton, U.K.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Punyakanok,
Dan Roth, and Mark Sammons. 2005. An inference
model for semantic entailment in natural language. In
Proc. of the RTE Workshop, Southampton, U.K.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings of
the 1st RTE Workshop, Southampton, UK.
Bernard Haasdonk. 2005. Feature space interpretation of
SVMs with indefinite kernels. IEEE Trans Pattern Anal
Mach Intell.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 15th CoLing,
Nantes, France.
Jay J. Jiang and David W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy. In
Proc. of the 10th ROCLING, Tapei, Taiwan.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In Advances in Kernel Methods-Support Vector
Learning. MIT Press.
Milen Kouylekov and Bernardo Magnini. 2005. Tree edit
distance for textual entailment. In Proc. of the RANLP-
2005, Borovets, Bulgaria.
George A. Miller. 1995. WordNet: A lexical database for
English. Communications of the ACM, November.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL?06,
Trento, Italy.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of 5th NAACL, Boston, MA.
Vladimir Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Annie Zaenen, Lauri Karttunen, and Richard Crouch. 2005.
Local textual inference: Can it be defined or circum-
scribed? In Proc. of the ACL Workshop on Empirical
Modeling of Semantic Equivalence and Entailment, Ann
Arbor, Michigan.
408
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 929?936,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Role Labeling via FrameNet, VerbNet and PropBank
Ana-Maria Giuglea and Alessandro Moschitti
Department of Computer Science
University of Rome ?Tor Vergata?
Rome, Italy
agiuglea@gmail.com
moschitti@info.uniroma2.it
Abstract
This article describes a robust seman-
tic parser that uses a broad knowledge
base created by interconnecting three ma-
jor resources: FrameNet, VerbNet and
PropBank. The FrameNet corpus con-
tains the examples annotated with seman-
tic roles whereas the VerbNet lexicon pro-
vides the knowledge about the syntac-
tic behavior of the verbs. We connect
VerbNet and FrameNet by mapping the
FrameNet frames to the VerbNet Intersec-
tive Levin classes. The PropBank corpus,
which is tightly connected to the VerbNet
lexicon, is used to increase the verb cov-
erage and also to test the effectiveness of
our approach. The results indicate that our
model is an interesting step towards the
design of more robust semantic parsers.
1 Introduction
During the last years a noticeable effort has been
devoted to the design of lexical resources that
can provide the training ground for automatic se-
mantic role labelers. Unfortunately, most of the
systems developed until now are confined to the
scope of the resource used for training. A very
recent example in this sense was provided by the
CONLL 2005 shared task (Carreras and Ma`rquez,
2005) on PropBank (PB) (Kingsbury and Palmer,
2002) role labeling. The systems that participated
in the task were trained on the Wall Street Jour-
nal corpus (WSJ) and tested on portions of WSJ
and Brown corpora. While the best F-measure
recorded on WSJ was 80%, on the Brown cor-
pus, the F-measure dropped below 70%. The
most significant causes for this performance decay
were highly ambiguous and unseen predicates (i.e.
predicates that do not have training examples).
The same problem was again highlighted by the
results obtained with and without the frame infor-
mation in the Senseval-3 competition (Litkowski,
2004) of FrameNet (Johnson et al, 2003) role la-
beling task. When such information is not used
by the systems, the performance decreases by 10
percent points. This is quite intuitive as the se-
mantics of many roles strongly depends on the fo-
cused frame. Thus, we cannot expect a good per-
formance on new domains in which this informa-
tion is not available.
A solution to this problem is the automatic
frame detection. Unfortunately, our preliminary
experiments showed that given a FrameNet (FN)
predicate-argument structure, the task of identify-
ing the associated frame can be performed with
very good results when the verb predicates have
enough training examples, but becomes very chal-
lenging otherwise. The predicates belonging to
new application domains (i.e. not yet included in
FN) are especially problematic since there is no
training data available.
Therefore, we should rely on a semantic context
alternative to the frame (Giuglea and Moschitti,
2004). Such context should have a wide coverage
and should be easily derivable from FN data. A
very good candidate seems to be the Intersective
Levin class (ILC) (Dang et al, 1998) that can be
found as well in other predicate resources like PB
and VerbNet (VN) (Kipper et al, 2000).
In this paper we have investigated the above
claim by designing a semi-automatic algorithm
that assigns ILCs to FN verb predicates and by
carrying out several semantic role labeling (SRL)
experiments in which we replace the frame with
the ILC information. We used support vector ma-
929
chines (Vapnik, 1995) with (a) polynomial ker-
nels to learn the semantic role classification and
(b) Tree Kernels (Moschitti, 2004) for learning
both frame and ILC classification. Tree kernels
were applied to the syntactic trees that encode the
subcategorization structures of verbs. This means
that, although FN contains three types of predi-
cates (nouns, adjectives and verbs), we only con-
centrated on the verb predicates and their roles.
The results show that: (1) ILC can be derived
with high accuracy for both FN and Probank and
(2) ILC can replace the frame feature with almost
no loss in the accuracy of the SRL systems. At the
same time, ILC provides better predicate coverage
as it can also be learned from other corpora (e.g.
PB).
In the remainder of this paper, Section 2 sum-
marizes previous work done on FN automatic role
detection. It also explains in more detail why mod-
els based exclusively on this corpus are not suit-
able for free-text parsing. Section 3 focuses on VN
and PB and how they can enhance the robustness
of our semantic parser. Section 4 describes the
mapping between frames and ILCs whereas Sec-
tion 5 presents the experiments that support our
thesis. Finally, Section 6 summarizes the conclu-
sions.
2 Automatic Semantic Role Labeling
One of the goals of the FN project is to design a
linguistic ontology that can be used for the auto-
matic processing of semantic information. The as-
sociated hierarchy contains an extensive semantic
analysis of verbs, nouns, adjectives and situations
in which they are used, called frames. The basic
assumption on which the frames are built is that
each word evokes a particular situation with spe-
cific participants (Fillmore, 1968). The word that
evokes a particular frame is called target word or
predicate and can be an adjective, noun or verb.
The participant entities are defined using semantic
roles and they are called frame elements.
Several models have been developed for the
automatic detection of the frame elements based
on the FN corpus (Gildea and Jurafsky, 2002;
Thompson et al, 2003; Litkowski, 2004). While
the algorithms used vary, almost all the previous
studies divide the task into: 1) the identification of
the verb arguments to be labeled and 2) the tag-
ging of each argument with a role. Also, most
of the models agree on the core features as be-
ing: Predicate, Headword, Phrase Type, Govern-
ing Category, Position, Voice and Path. These are
the initial features adopted by Gildea and Jurafsky
(2002) (henceforth G&J) for both frame element
identification and role classification.
One difference among previous machine-
learning models is whether they used the frame in-
formation or not. The impact of the frame feature
over unseen predicates and words is particularly
interesting for us. The results obtained by G&J
provide some interesting insights in this direction.
In one of their experiments, they used the frame to
generalize from predicates seen in the training data
to unseen predicates, which belonged to the same
frame. The overall performance increased show-
ing that when no training data is available for a
target word we can use data from the same frame.
Other studies suggest that the frame is cru-
cial when trying to eliminate the major sources
of errors. In their error analysis, (Thompson et
al., 2003) pinpoints that the verb arguments with
headwords that are rare in a particular frame but
not rare over the whole corpus are especially hard
to classify. For these cases the frame is very im-
portant because it provides the context informa-
tion needed to distinguish between different word
senses.
Overall, the experiments presented in G&J?s
study correlated with the results obtained in the
Senseval-3 competition show that the frame fea-
ture increases the performance and decreases the
amount of annotated examples needed in training
(i.e. frame usage improves the generalization abil-
ity of the learning algorithm). On the other hand,
the results obtained without the frame information
are very poor.
These results show that having broader frame
coverage is very important for robust semantic
parsing. Unfortunately, the 321 frames that con-
tain at least one verb predicate cover only a small
fraction of the English verb lexicon and of the
possible domains. Also from these 321 frames
only 100 were considered to have enough training
data and were used in Senseval-3 (see (Litkowski,
2004) for more details).
Our approach for solving such problems in-
volves the usage of a frame-like feature, namely
the Intersective Levin class (ILC). We show that
the ILC can replace the frame with almost no loss
in performance. At the same time, ILC provides
better coverage as it can be learned also from other
930
corpora (e.g. PB).
The next section provides the theoretical sup-
port for the unified usage of FN, VN and PB, ex-
plaining why and how it is possible to link them.
3 Linking FrameNet to VerbNet and
PropBank
In general, predicates belonging to the same FN
frame have a coherent syntactic behavior that is
also different from predicates pertaining to other
frames (G&J). This finding is consistent with the-
ories of linking that claim that the syntactic behav-
ior of a verb can be predicted from its semantics
(Levin, 1993). This insight justifies the attempt to
use ILCs instead of the frame feature when clas-
sifying FN semantic roles (Giuglea and Moschitti,
2004).
The main advantage of using Levin classes
comes from the fact that other resources like PB
and the VN lexicon contain this kind of informa-
tion. Thus, we can train an ILC classifier also on
the PB corpus, considerably increasing the verb
knowledge base at our disposal. Another advan-
tage derives from the syntactic criteria that were
applied in defining the Levin?s clusters. As shown
later in this article, the syntactic nature of these
classes makes them easier to classify than frames
when using only syntactic and lexical features.
More precisely, Levin?s clusters are formed ac-
cording to diathesis alternation criteria which are
variations in the way verbal arguments are gram-
matically expressed when a specific semantic phe-
nomenon arises. For example, two different types
of diathesis alternations are the following:
(a) Middle Alternation
[Subject, Agent The butcher] cuts [Direct
Object, Patient the meat].
[Subject, Patient The meat] cuts easily.
(b) Causative/inchoative Alternation
[Subject, Agent Janet] broke [Direct Object,
Patient the cup].
[Subject, Patient The cup] broke.
In both cases, what is alternating is the grammati-
cal function that the Patient role takes when chang-
ing from the transitive use of the verb to the intran-
sitive one. The semantic phenomenon accompa-
nying these types of alternations is the change of
focus from the entity performing the action to the
theme of the event.
Levin documented 79 alternations which con-
stitute the building blocks for the verb classes.
Although alternations are chosen as the primary
means for identifying the classes, additional prop-
erties related to subcategorization, morphology
and extended meanings of verbs are taken into ac-
count as well. Thus, from a syntactic point of
view, the verbs in one Levin class have a regu-
lar behavior, different from the verbs pertaining to
other classes. Also, the classes are semantically
coherent and all verbs belonging to one class share
the same participant roles.
This constraint of having the same semantic
roles is further ensured inside the VN lexicon
which is constructed based on a more refined ver-
sion of the Levin?s classification, called Intersec-
tive Levin classes (ILCs) (Dang et al, 1998). The
lexicon provides a regular association between the
syntactic and semantic properties of each of the
described classes. It also provides information
about the syntactic frames (alternations) in which
the verbs participate and the set of possible seman-
tic roles.
One corpus associated with the VN lexicon is
PB. The annotation scheme of PB ensures that
the verbs belonging to the same Levin class share
similarly labeled arguments. Inside one ILC, to
one argument corresponds one semantic role num-
bered sequentially from ARG0 to ARG5. The ad-
junct roles are labeled ARGM.
Levin classes were constructed based on regu-
larities exhibited at grammatical level and the re-
sulting clusters were shown to be semantically co-
herent. As opposed, the FN frames were built on
semantic bases, by putting together verbs, nouns
and adjectives that evoke the same situations. Al-
though different in conception, the FN verb clus-
ters and VN verb clusters have common proper-
ties1:
1. Different syntactic properties between dis-
tinct verb clusters (as proven by the experi-
ments in G&J)
2. A shared set of possible semantic roles for all
verbs pertaining to the same cluster.
Having these insights, we have assigned a corre-
spondent VN class not to each verb predicate but
rather to each frame. In doing this we have ap-
plied the simplifying assumption that a frame has a
1See section 4.4 for more details
931
unique corresponding Levin class. Thus, we have
created a one-to-many mapping between the ILCs
and the frames. In order to create a pair ?FN frame,
VN class?, our mapping algorithm checks both the
syntactic and semantic consistency by comparing
the role frequency distributions on different syn-
tactic positions for the two candidates. The algo-
rithm is described in detail in the next section.
4 Mapping FrameNet frames to VerbNet
classes
The mapping algorithm consists of three steps: (a)
we link the frames and ILCs that have the largest
number of verbs in common and we create a set of
pairs ?FN frame, VN class? (see Table 1); (b) we
refine the pairs obtained in the previous step based
on diathesis alternation criteria, i.e. the verbs per-
taining to the FN frame have to undergo the same
diathesis alternation that characterize the corre-
sponding VN class (see Table 2) and (c) we man-
ually check the resulting mapping.
4.1 The mapping algorithm
Given a frame, F , we choose as candidate for the
mapping the ILC, C, that has the largest number of
verbs in common with it (see Table 1, line (I)). If
the number is greater or equal than three we form
a pair ?F , C? that will be tested in the second step
of the algorithm. Only the frames that have more
than 3 verb lexical units are candidates for this step
(frames with less than 3 members cannot pass con-
dition (II)). This excludes a number of 60 frames
that will be subsequently manually mapped.
In order to assign a VN class to a frame, we
have to verify that the verbs belonging to the FN
frame participate in the same diathesis alternation
criteria used to define the VN class. Thus, the
pairs ?F,C? formed in step 1 of the mapping al-
gorithm have to undergo a validation step that ver-
ifies the similarity between the enclosed FN frame
and VN class. This validation process has several
sub-steps:
First, we make use of the property (2) of the
Levin classes and FN frames presented in the pre-
vious section. According to this property, all verbs
pertaining to one frame or ILC have the same par-
ticipant roles. Thus, a first test of compatibility
between a frame and a Levin class is that they
share the same participant roles. As FN is anno-
tated with frame-specific semantic roles, we man-
ually mapped these roles into the VN set of the-
INPUT
V N = {C|C is a V erbNet class}
V N Class C = {v|c is a verb of C}
FN = {F |F is a FrameNet frame}
FN frame F = {v|v is a verb of F}
OUTPUT
Pairs = {?F, C? |F ? FN,C ? V N : F maps to C }
COMPUTE PAIRS:
Let Pairs = ?
for each F ? FN
(I) compute C? = argmaxC?V N |F ? C|
(II) if |F ? C?| ? 3 then Pairs = Pairs ? ?F,C??
Table 1: Linking FrameNet frames and VerbNet
classes.
TR = {?i : ?i is the i? th theta role of VerbNet }
for each ?F, C? ? Pairs
??AF = ?o1, .., on?, oi = #??i, F, pos =adjacent???DF = ?o1, .., on?, oi = #??i, F, pos =distant???AC = ?o1, .., on?, oi = #??i, C, pos =adjacent???DC = ?o1, .., on?, oi = #??i, C, pos =distant?
ScoreF,C = 23 ?
??AF ???AC???
?????AF
???
????
???
?????AC
???
???
+ 13 ?
??DF ???DC???
?????DF
???
????
???
?????DC
???
???
Table 2: Mapping algorithm - refining step.
matic roles. Given a frame, we assigned thematic
roles to all frame elements that are associated with
verbal predicates. For example the Speaker, Ad-
dressee, Message and Topic roles from the Telling
frame were respectively mapped into the Agent,
Recipient, Theme and Topic theta roles.
Second, we build a frequency distribution of
VN thematic roles on different syntactic positions.
Based on our observation and previous studies
(Merlo and Stevenson, 2001), we assume that each
ILC has a distinct frequency distribution of roles
on different grammatical slots. As we do not have
matching grammatical functions in FN and VN,
we approximate that subjects and direct objects
are more likely to appear on positions adjacent
to the predicate, while indirect objects appear on
more distant positions. The same intuition is suc-
cessfully used by G&J to design the Position fea-
ture.
For each thematic role ?i we acquired from VN
and FN data the frequencies with which ?i appears
on an adjacent A or distant D positions in a given
frame or VN class (i.e. #??i , class, position?).
Therefore, for each frame and class, we obtain two
vectors with thematic role frequencies correspond-
ing respectively to the adjacent and distant posi-
tions (see Table 2). We compute a score for each
932
 Score No. of Frames 
Not 
mapped Correct 
Overall 
Correct 
[0,0.5] 118 48.3% 82.5% 
(0.5,0.75] 69 0 84% 
(0.75,1] 72 0 100% 
89.6% 
 
Table 3: Results of the mapping algorithm.
pair ?F,C? using the normalized scalar product.
The core arguments, which tend to occupy adja-
cent positions, show a minor syntactic variability
and are more reliable than adjunct roles. To ac-
count for this in the overall score, we multiply the
adjacent and the distant scores by 2/3 and 1/3, re-
spectively. This limits the impact of adjunct roles
like Temporal and Location.
The above frequency vectors are computed for
FN directly from the corpus of predicate-argument
structure examples associated with each frame.
The examples associated with the VN lexicon are
extracted from the PB corpus. In order to do this
we apply a preprocessing step in which each la-
bel Arg0..5 is replaced with its corresponding the-
matic role given the ILC of the predicate. We
assign the same roles to the adjuncts all over PB
as they are general for all verb classes. The only
exception is ARGM-DIR that can correspond to
Source, Goal or Path. We assign different roles to
this adjunct based on the prepositions. We ignore
some adjuncts like ARGM-ADV or ARGM-DIS
because they cannot bear a thematic role.
4.2 Mapping Results
We found that only 133 VN classes have corre-
spondents among FN frames. Moreover, from the
frames mapped with an automatic score smaller
than 0.5 almost a half did not match any of the
existing VN classes2. A summary of the results
is depicted in Table 3. The first column contains
the automatic score provided by the mapping al-
gorithm when comparing frames with ILCs. The
second column contains the number of frames for
each score interval. The third column contains the
percentage of frames that did not have a corre-
sponding VN class and finally the fourth and fifth
columns contain the accuracy of the mapping al-
gorithm for each interval score and for the whole
task, respectively.
We mention that there are 3,672 distinct verb
senses in PB and 2,351 distinct verb senses in
2The automatic mapping is improved by manually assign-
ing the FN frames of the pairs that receive a score lower than
0.5.
FN. Only 501 verb senses are in common between
the two corpora which means 13.64% of PB and
21.31% of FN. Thus, by training an ILC classifier
on both PB and FN we extend the number of avail-
able verb senses to 5,522.
4.3 Discussion
In the literature, other studies compared the Levin
classes with the FN frames, e.g. (Baker and Rup-
penhofer, 2002; Giuglea and Moschitti, 2004; Shi
and Mihalcea, 2005). Their findings suggest that
although the two set of clusters are roughly equiv-
alent there are also several types of mismatches:
1. Levin classes that are narrower than the cor-
responding frames,
2. Levin classes that are broader that the corre-
sponding frames and
3. Overlapping groups.
For our task, point 2 does not pose a problem.
Points 1 and 3 however suggest that there are cases
in which to one FN frame corresponds more than
one Levin class. By investigating such cases, we
noted that the mapping algorithm consistently as-
signs scores below 75% to cases that match prob-
lem 1 (two Levin classes inside one frame) and
below 50% to cases that match problem 3 (more
than two Levin classes inside one frame). Thus,
to increase the accuracy of our results, a first step
should be to assign independently an ILC to each
of the verbs pertaining to frames with score lower
than 0.75%.
Nevertheless the current results are encourag-
ing as they show that the algorithm is achieving its
purpose by successfully detecting syntactic inco-
herences that can be subsequently corrected man-
ually. Also, in the next section we will show that
our current mapping achieves very good results,
giving evidence for the effectiveness of the Levin
class feature.
5 Experiments
In the previous sections we have presented the
algorithm for annotating the verb predicates of
FrameNet (FN) with Intersective Levin classes
(ILCs). In order to show the effectiveness of this
annotation and of the ILCs in general we have per-
formed several experiments.
First, we trained (1) an ILC multiclassifier from
FN, (2) an ILC multiclassifier from PB and (3) a
933
  
 Run 
51.3.2 
Cooking 
45.3 
Characterize 
29.2 
Other_cos 
45.4 
Say 
37.7 
Correspond 
36.1 Multiclassifier 
PB #Train Instances 
PB #Test Instances 
262 
5 
6 
5 
2,945 
134 
2,207 
149 
9,707 
608 
259 
20 
52,172 
2,742 
PB Results 75 33.33 96.3 97.24 100 88.89 92.96 
FN #Train Instances 
FN #Test Instances 
5,381 
1,343 
138 
35 
765 
40 
721 
184 
1,860 
1,343 
557 
111 
46,734 
11,650 
FN Results 96.36 72.73 95.73 92.43 94.43 78.23 92.63 
Table 4: F1s of some individual ILC classifiers and the overall multiclassifier accuracy (180 classes on
PB and 133 on FN).
 
 Body_part Crime Degree Agent Multiclassifier 
FN #Train Instances 
FN #Test Instances 
1,511 
356 
39 
5 
765 
187 
6,441 
1,643 
102,724 
25,615 
LF+Gold Frame 90.91 88.89 70.51 93.87 90.8 
LF+Gold ILC 90.80 88.89 71.52 92.01 88.23 
LF+Automatic Frame 84.87 88.89 70.10 87.73 85.64 
LF+Automatic ILC 85.08 88.89 69.62 87.74 84.45 
LF 79.76 75.00 64.17 80.82 80.99 
 
Table 5: F1s of some individual FN role classifiers and the overall multiclassifier accuracy (454 roles).
frame multiclassifier from FN. We compared the
results obtained when trying to classify the VN
class with the results obtained when classifying
frame. We show that ILCs are easier to detect than
FN frames.
Our second set of experiments regards the auto-
matic labeling of FN semantic roles on FN corpus
when using as features: gold frame, gold ILC, au-
tomatically detected frame and automatically de-
tected ILC. We show that in all situations in which
the VN class feature is used, the accuracy loss,
compared to the usage of the frame feature, is neg-
ligible. This suggests that the ILC can success-
fully replace the frame feature for the task of se-
mantic role labeling.
Another set of experiments regards the gener-
alization property of the ILC. We show the impact
of this feature when very few training data is avail-
able and its evolution when adding more and more
training examples. We again perform the exper-
iments for: gold frame, gold ILC, automatically
detected frame and automatically detected ILC.
Finally, we simulate the difficulty of free text
by annotating PB with FN semantic roles. We
used PB because it covers a different set of ver-
bal predicates and also because it is very different
from FN at the level of vocabulary and sometimes
even syntax. These characteristics make PB a dif-
ficult testbed for the semantic role models trained
on FN.
In the following section we present the results
obtained for each of the experiments mentioned
above.
5.1 Experimental setup
The corpora available for the experiments were PB
and FN. PB contains about 54,900 predicates and
gold parse trees. We used sections from 02 to 22
(52,172 predicates) to train the ILC classifiers and
Section 23 (2,742 predicates) for testing purposes.
The number of ILCs is 180 in PB and 133 on FN,
i.e. the classes that we were able to map.
For the experiments on FN corpus, we extracted
58,384 sentences from the 319 frames that contain
at least one verb annotation. There are 128,339
argument instances of 454 semantic roles. In our
evaluation we use only verbal predicates. More-
over, as there is no fixed split between training and
testing, we randomly selected 20% of sentences
for testing and 80% for training. The sentences
were processed using Charniak?s parser (Char-
niak, 2000) to generate parse trees automatically.
The classification models were implemented by
means of the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti
which encodes tree kernels in the SVM-light
software (Joachims, 1999). We used the default
parameters. The classification performance was
evaluated using the F1 measure for the individual
role and ILC classifiers and the accuracy for the
multiclassifiers.
934
5.2 Automatic VerbNet class vs. automatic
FrameNet frame detection
In these experiments, we classify ILCs on PB and
frames on FN. For the training stage we use SVMs
with Tree Kernels.
The main idea of tree kernels is the modeling
of a KT (T1,T2) function which computes the num-
ber of common substructures between two trees T1
and T2. Thus, we can train SVMs with structures
drawn directly from the syntactic parse tree of the
sentence. The kernel that we employed in our ex-
periments is based on the SCF structure devised
in (Moschitti, 2004). We slightly modified SCF
by adding the headwords of the arguments, useful
for representing the selectional preferences (more
details are given in (Giuglea and Moschitti, 2006).
For frame detection on FN, we trained our clas-
sifier on 46,734 training instances and tested on
11,650 testing instances, obtaining an accuracy of
91.11%. For ILC detection the results are depicted
in Table 4. The first six columns report the F1
measure of some verb class classifiers whereas the
last column shows the global multiclassifier accu-
racy. We note that ILC detection is more accurate
than the frame detection on both FN and PB. Ad-
ditionally, the ILC results on PB are similar with
those obtained for the ILCs on FN. This suggests
that the training corpus does not have a major in-
fluence. Also, the SCF-based tree kernel seems to
be robust in what concerns the quality of the parse
trees. The performance decay is very small on FN
that uses automatic parse trees with respect to PB
that contains gold parse trees.
5.3 Automatic semantic role labeling on
FrameNet
In the experiments involving semantic role label-
ing, we used SVMs with polynomial kernels. We
adopted the standard features developed for se-
mantic role detection by Gildea and Jurafsky (see
Section 2). Also, we considered some of the fea-
tures designed by (Pradhan et al, 2005): First and
Last Word/POS in Constituent, Subcategorization,
Head Word of Prepositional Phrases and the Syn-
tactic Frame feature from (Xue and Palmer, 2004).
For the rest of the paper, we will refer to these fea-
tures as being literature features (LF). The results
obtained when using the literature features alone
or in conjunction with the gold frame feature, gold
ILC, automatically detected frame feature and au-
tomatically detected ILC are depicted in Table 5.
30
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90 100
% Training Data
Acc
ura
cy
LF+ILC
LF
LF+Automatic ILC Trained on PB
LF+Automatic ILC Trained on FN
Figure 1: Semantic role learning curve.
The first four columns report the F1 measure
of some role classifiers whereas the last column
shows the global multiclassifier accuracy. The first
row contains the number of training and testing in-
stances and each of the other rows contains the
performance obtained for different feature com-
binations. The results are reported for the label-
ing task as the argument-boundary detection task
is not affected by the frame-like features (G&J).
We note that automatic frame produces an accu-
racy very close to the one obtained with automatic
ILC suggesting that this is a very good candidate
for replacing the frame feature. Also, both auto-
matic features are very effective and they decrease
the error rate by 20%.
To test the impact of ILC on SRL with different
amount of training data, we additionally draw the
learning curves with respect to different features:
LF, LF+ (gold) ILC, LF+automatic ILC trained on
PB and LF+automatic ILC trained on FN. As can
be noted, the automatic ILC information provided
by the ILC classifiers (trained on FN or PB) per-
forms almost as good as the gold ILC.
5.4 Annotating PB with FN semantic roles
To show that our approach can be suitable for
semantic role free-text annotation, we have au-
tomatically classified PB sentences3 with the FN
semantic-role classifiers. In order to measure
the quality of the annotation, we randomly se-
lected 100 sentences and manually verified them.
We measured the performance obtained with and
without the automatic ILC feature. The sentences
contained 189 arguments from which 35 were in-
correct when ILC was used compared to 72 incor-
rect in the absence of this feature, i.e. an accu-
racy of 81% with ILC versus 62% without it. This
demonstrates the importance of the ILC feature
3The results reported are only for role classification.
935
outside the scope of FN where the frame feature
is not available.
6 Conclusions
In this paper we have shown that the ILC feature
can successfully replace the FN frame feature. By
doing that we could interconnect FN to VN and
PB obtaining better verb coverage and a more ro-
bust semantic parser. Our good results show that
we have defined an effective framework which is
a promising step toward the design of more robust
semantic parsers.
In the future, we intend to measure the effec-
tiveness of our system by testing FN SRL on a
larger portion of PB or on other corpora containing
a larger verb set.
References
Collin Baker and Josef Ruppenhofer. 2002. Framenets
frames vs. levins verb classes. In 28th Annual Meet-
ing of the Berkeley Linguistics Society.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NACL00, Seattle,
Washington.
Hoa Trang Dang, Karin Kipper, Martha Palmer, and
Joseph Rosenzweig. 1998. Investigating regular
sense extensions based on intersective levin classes.
In Coling-ACL98.
Charles J. Fillmore. 1968. The case for case. In Uni-
versals in Linguistic Theory.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tic.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge discovering using FrameNet, VerbNet
and PropBank. In Proceedings of Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Shallow semantic parsing based on FrameNet, Verb-
Net and PropBank. In Proceedings of the 17th Euro-
pean Conference on Artificial Intelligence, Riva del
Garda, Italy.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vec-
tor Learning.
Christopher Johnson, Miriam Petruck, Collin Baker,
Michael Ellsworth, Josef Ruppenhofer, and Charles
Fillmore. 2003. Framenet: Theory and practice.
Berkeley, California.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In LREC02).
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In AAAI00.
Beth Levin. 1993. English Verb Classes and Alterna-
tions A Preliminary Investigation. Chicago: Univer-
sity of Chicago Press.
Kenneth Litkowski. 2004. Senseval-3 task automatic
labeling of semantic roles. In Senseval-3.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distribution of
argument structure. CL Journal.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In ACL04,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support vector learning for semantic argu-
ment classification. Machine Learning Journal.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings of
Cicling 2005, Mexico.
Cynthia A. Thompson, Roger Levy, and Christopher
Manning. 2003. A generative model for semantic
role labeling. In 14th European Conference on Ma-
chine Learning.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP 2004, Barcelona, Spain. Association for
Computational Linguistics.
936
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 48?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Engineering of Syntactic Features for Shallow Semantic Parsing
Alessandro Moschitti?
? DISP - University of Rome ?Tor Vergata?, Rome, Italy
{moschitti, pighin, basili}@info.uniroma2.it
? ITC-Irst, ? DIT - University of Trento, Povo-Trento, Italy
coppolab@itc.it
Bonaventura Coppola?? Daniele Pighin? Roberto Basili?
Abstract
Recent natural language learning research
has shown that structural kernels can be
effectively used to induce accurate models
of linguistic phenomena.
In this paper, we show that the above prop-
erties hold on a novel task related to predi-
cate argument classification. A tree kernel
for selecting the subtrees which encodes
argument structures is applied. Experi-
ments with Support Vector Machines on
large data sets (i.e. the PropBank collec-
tion) show that such kernel improves the
recognition of argument boundaries.
1 Introduction
The design of features for natural language process-
ing tasks is, in general, a critical problem. The inher-
ent complexity of linguistic phenomena, often char-
acterized by structured data, makes difficult to find
effective linear feature representations for the target
learning models.
In many cases, the traditional feature selection
techniques (Kohavi and Sommerfield, 1995) are not
so useful since the critical problem relates to feature
generation rather than selection. For example, the
design of features for a natural language syntactic
parse-tree re-ranking problem (Collins, 2000) can-
not be carried out without a deep knowledge about
automatic syntactic parsing. The modeling of syn-
tactic/semantic based features should take into ac-
count linguistic aspects to detect the interesting con-
text, e.g. the ancestor nodes or the semantic depen-
dencies (Toutanova et al, 2004).
A viable alternative has been proposed in (Collins
and Duffy, 2002), where convolution kernels were
used to implicitly define a tree substructure space.
The selection of the relevant structural features was
left to the voted perceptron learning algorithm. An-
other interesting model for parsing re-ranking based
on tree kernel is presented in (Taskar et al, 2004).
The good results show that tree kernels are very
promising for automatic feature engineering, espe-
cially when the available knowledge about the phe-
nomenon is limited.
Along the same line, automatic learning tasks that
rely on syntactic information may take advantage of
a tree kernel approach. One of such tasks is the au-
tomatic boundary detection of predicate arguments
of the kind defined in PropBank (Kingsbury and
Palmer, 2002). For this purpose, given a predicate p
in a sentence s, we can define the notion of predicate
argument spanning trees (PAST s) as those syntac-
tic subtrees of s which exactly cover all and only
the p?s arguments (see Section 4.1). The set of non-
spanning trees can be then associated with all the
remaining subtrees of s.
An automatic classifier which recognizes the
spanning trees can potentially be used to detect the
predicate argument boundaries. Unfortunately, the
application of such classifier to all possible sen-
tence subtrees would require an exponential execu-
tion time. As a consequence, we can use it only to
decide for a reduced set of subtrees associated with
a corresponding set of candidate boundaries. Notice
how these can be detected by previous approaches
48
(e.g. (Pradhan et al, 2004)) in which a traditional
boundary classifier (tbc) labels the parse-tree nodes
as potential arguments (PA). Such classifiers, gen-
erally, are not sensitive to the overall argument struc-
ture. On the contrary, a PAST classifier (pastc) can
consider the overall argument structure encoded in
the associated subtree. This is induced by the PA
subsets.
The feature design for the PAST representation
is not simple. Tree kernels are a viable alternative
that allows the learning algorithm to measure the
similarity between two PAST s in term of all pos-
sible tree substructures.
In this paper, we designed and experimented a
boundary classifier for predicate argument labeling
based on two phases: (1) a first annotation of po-
tential arguments by using a high recall tbc and
(2) a PAST classification step aiming to select the
correct substructures associated with potential argu-
ments. Both classifiers are based on Support Vector
Machines learning. The pastc uses the tree kernel
function defined in (Collins and Duffy, 2002). The
results show that the PAST classification can be
learned with high accuracy (the f-measure is about
89%) and the impact on the overall boundary detec-
tion accuracy is good.
In the remainder of this paper, Section 2 intro-
duces the Semantic Role Labeling problem along
with the boundary detection subtask. Section 3 de-
fines the SVMs using the linear kernel and the parse
tree kernel for boundary detection. Section 4 de-
scribes our boundary detection algorithm. Section 5
shows the preliminary comparative results between
the traditional and the two-step boundary detection.
Finally, Section 7 summarizes the conclusions.
2 Automated Semantic Role Labeling
One of the largest resources of manually annotated
predicate argument structures has been developed in
the PropBank (PB) project. The PB corpus contains
300,000 words annotated with predicative informa-
tion on top of the Penn Treebank 2 Wall Street Jour-
nal texts. For any given predicate, the expected ar-
guments are labeled sequentially from Arg0 to Arg9,
ArgA and ArgM. Figure 1 shows an example of
the PB predicate annotation of the sentence: John
rented a room in Boston.
Predicates in PB are only embodied by verbs
whereas most of the times Arg0 is the subject, Arg1
is the direct object and ArgM indicates locations, as
in our example.
 
 
 
 
 
 
 
 
 
Predicate 
Arg. 0 
Arg. M 
S 
N 
NP 
D N 
VP 
V John 
in 
 rented 
a 
  room 
PP 
IN N 
Boston 
Arg. 1 
Figure 1: A predicate argument structure in a parse-tree rep-
resentation.
Several machine learning approaches for auto-
matic predicate argument extraction have been de-
veloped, e.g. (Gildea and Jurasfky, 2002; Gildea and
Palmer, 2002; Gildea and Hockenmaier, 2003; Prad-
han et al, 2004). Their common characteristic is
the adoption of feature spaces that model predicate-
argument structures in a flat feature representation.
In the next section, we present the common parse
tree-based approach to this problem.
2.1 Predicate Argument Extraction
Given a sentence in natural language, all the predi-
cates associated with the verbs have to be identified
along with their arguments. This problem is usually
divided in two subtasks: (a) the detection of the tar-
get argument boundaries, i.e. the span of its words
in the sentence, and (b) the classification of the argu-
ment type, e.g. Arg0 or ArgM in PropBank or Agent
and Goal in FrameNet.
The standard approach to learn both the detection
and the classification of predicate arguments is sum-
marized by the following steps:
1. Given a sentence from the training-set, gener-
ate a full syntactic parse-tree;
2. let P and A be the set of predicates and the
set of parse-tree nodes (i.e. the potential argu-
ments), respectively;
3. for each pair < p, a >? P ?A:
? extract the feature representation set, Fp,a;
49
? if the subtree rooted in a covers exactly
the words of one argument of p, put Fp,a
in T+ (positive examples), otherwise put
it in T? (negative examples).
For instance, in Figure 1, for each combination of
the predicate rent with the nodes N, S, VP, V, NP,
PP, D or IN the instances Frent,a are generated. In
case the node a exactly covers ?John?, ?a room? or
?in Boston?, it will be a positive instance otherwise
it will be a negative one, e.g. Frent,IN .
The T+ and T? sets are used to train the bound-
ary classifier. To train the multi-class classifier T+
can be reorganized as positive T+argi and negative
T?argi examples for each argument i. In this way,
an individual ONE-vs-ALL classifier for each argu-
ment i can be trained. We adopted this solution, ac-
cording to (Pradhan et al, 2004), since it is simple
and effective. In the classification phase, given an
unseen sentence, all its Fp,a are generated and clas-
sified by each individual classifier Ci. The argument
associated with the maximum among the scores pro-
vided by the individual classifiers is eventually se-
lected.
2.2 Standard feature space
The discovery of relevant features is, as usual, a
complex task. However, there is a common con-
sensus on the set of basic features. These stan-
dard features, firstly proposed in (Gildea and Juras-
fky, 2002), refer to unstructured information de-
rived from parse trees, i.e. Phrase Type, Predicate
Word, Head Word, Governing Category, Position
and Voice. For example, the Phrase Type indicates
the syntactic type of the phrase labeled as a predicate
argument, e.g. NP for Arg1 in Figure 1. The Parse
Tree Path contains the path in the parse tree between
the predicate and the argument phrase, expressed as
a sequence of nonterminal labels linked by direction
(up or down) symbols, e.g. V ? VP ? NP for Arg1 in
Figure 1. The Predicate Word is the surface form of
the verbal predicate, e.g. rent for all arguments.
In the next section we describe the SVM approach
and the basic kernel theory for the predicate argu-
ment classification.
3 Learning predicate structures via
Support Vector Machines
Given a vector space in <n and a set of positive and
negative points, SVMs classify vectors according to
a separating hyperplane, H(~x) = ~w ? ~x + b = 0,
where ~w ? <n and b ? < are learned by applying
the Structural Risk Minimization principle (Vapnik,
1995).
To apply the SVM algorithm to Predicate Argu-
ment Classification, we need a function ? : F ? <n
to map our features space F = {f1, .., f|F|} and our
predicate/argument pair representation, Fp,a = Fz ,
into <n, such that:
Fz ? ?(Fz) = (?1(Fz), .., ?n(Fz))
From the kernel theory we have that:
H(~x) =
( ?
i=1..l
?i~xi
)
? ~x+ b =
?
i=1..l
?i~xi ? ~x+ b =
?
i=1..l
?i?(Fi) ? ?(Fz) + b.
where, Fi ?i ? {1, .., l} are the training instances
and the product K(Fi, Fz) =<?(Fi) ??(Fz)> is the
kernel function associated with the mapping ?.
The simplest mapping that we can apply is
?(Fz) = ~z = (z1, ..., zn) where zi = 1 if fi ? Fz
and zi = 0 otherwise, i.e. the characteristic vector
of the set Fz with respect to F . If we choose the
scalar product as a kernel function we obtain the lin-
ear kernel KL(Fx, Fz) = ~x ? ~z.
An interesting property is that we do not need to
evaluate the ? function to compute the above vector.
Only the K(~x, ~z) values are in fact required. This al-
lows us to derive efficient classifiers in a huge (pos-
sible infinite) feature space, provided that the ker-
nel is processed in an efficient way. This property
is also exploited to design convolution kernel like
those based on tree structures.
3.1 The tree kernel function
The main idea of the tree kernels is the modeling of
a KT (T1, T2) function which computes the number
of common substructures between two trees T1 and
T2.
Given the set of substructures (fragments)
{f1, f2, ..} = F extracted from all the trees of the
training set, we define the indicator function Ii(n)
50
 S 
NP VP 
VP VP CC 
VB NP 
took DT NN 
the book 
and VB NP 
read PRP$ NN 
its title 
PRP 
John 
S 
NP VP 
VP 
VB NP 
read 
Sentence Parse-Tree 
S 
NP VP 
VP 
VB NP 
  took 
took{ARG0, ARG1} 
PRP 
John 
PRP 
John 
DT NN 
the book 
PRP$ NN 
its title 
read{ARG0, ARG1} 
Figure 2: A sentence parse tree with two predicative tree structures (PAST s)
which is equal 1 if the target fi is rooted at node n
and 0 otherwise. It follows that:
KT (T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2) (1)
where NT1 and NT2 are the sets of the T1?s
and T2?s nodes, respectively and ?(n1, n2) =?|F|
i=1 Ii(n1)Ii(n2). This latter is equal to the num-
ber of common fragments rooted at the n1 and n2
nodes. We can compute ? as follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminals symbols) then ?(n1, n2) =
1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) =
nc(n1)?
j=1
(1 + ?(cjn1 , cjn2)) (2)
where nc(n1) is the number of the children of n1
and cjn is the j-th child of the node n. Note that, as
the productions are the same, nc(n1) = nc(n2).
The above kernel has the drawback of assigning
higher weights to larger structures1. In order to over-
come this problem we scale the relative importance
of the tree fragments imposing a parameter ? in con-
ditions 2 and 3 as follows: ?(nx, nz) = ? and
?(nx, nz) = ?
?nc(nx)
j=1 (1 + ?(cjn1 , cjn2)).
1In order to approach this problem and to map similarity
scores in the [0,1] range, a normalization in the kernel space,
i.e. K?T (T1, T2) = KT (T1,T2)?KT (T1,T1)?KT (T2,T2) . is always applied
4 Boundary detection via argument
spanning
Section 2 has shown that traditional argument
boundary classifiers rely only on features extracted
from the current potential argument node. In or-
der to take into account a complete argument struc-
ture information, the classifier should select a set of
parse-tree nodes and consider them as potential ar-
guments of the target predicate. The number of all
possible subsets is exponential in the number of the
parse-tree nodes of the sentence, thus, we need to
cut the search space. For such purpose, a traditional
boundary classifier can be applied to select the set
of potential arguments PA. The reduced number of
PA subsets can be associated with sentence subtrees
which in turn can be classified by using tree kernel
functions. These measure if a subtree is compatible
or not with the subtree of a correct predicate argu-
ment structure.
4.1 The Predicate Argument Spanning Trees
(PAST s)
We consider the predicate argument structures an-
notated in PropBank along with the corresponding
TreeBank data as our object space. Given the target
predicate p in a sentence parse tree T and a subset
s = {n1, .., nk} of the T?s nodes, NT , we define as
the spanning tree root r the lowest common ancestor
of n1, .., nk. The node spanning tree (NST ), ps is
the subtree rooted in r, from which the nodes that
are neither ancestors nor descendants of any ni are
removed.
Since predicate arguments are associated with
tree nodes, we can define the predicate argu-
51
 S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
Arg. 1 
Arg. 0 
S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
S 
NP-0 VP 
John 
PP 
DT NN 
the book 
NP IN 
of 
S 
NP-0 VP 
VB NP 
read 
John 
DT NN 
the title 
NP-1 PP-2 
DT NN 
the book 
IN 
of 
NP 
(a) (b) (c) 
Correct PAST 
Incorrect  PAST 
Correct PAST 
Incorrect  PAST 
DT NN 
the title 
NP 
NP-1 VB 
read 
 
 
 
Figure 3: Two-step boundary classifier.
ment spanning tree (PAST ) of a predicate ar-
gument set, {a1, .., an}, as the NST over such
nodes, i.e. p{a1,..,an}. A PAST corresponds
to the minimal subparse tree whose leaves are
all and only the word sequence compounding
the arguments. For example, Figure 2 shows
the parse tree of the sentence "John took the
book and read its title". took{ARG0,ARG1}
and read{ARG0,ARG1} are two PAST structures
associated with the two predicates took and read,
respectively. All the other NST s are not valid
PAST s.
Notice that, labeling ps, ?s ? NT with a PAST
classifier (pastc) corresponds to solve the boundary
problem. The critical points for the application of
this strategy are: (1) how to design suitable features
for the PAST characterization. This new problem
requires a careful linguistic investigation about the
significant properties of the argument spanning trees
and (2) how to deal with the exponential number of
NST s.
For the first problem, the use of tree kernels over
the PAST s can be an alternative to the manual fea-
tures design as the learning machine, (e.g. SVMs)
can select the most relevant features from a high di-
mensional feature space. In other words, we can use
Eq. 1 to estimate the similarity between two PAST s
avoiding to define explicit features. The same idea
has been successfully applied to the parse-tree re-
ranking task (Taskar et al, 2004; Collins and Duffy,
2002) and predicate argument classification (Mos-
chitti, 2004).
For the second problem, i.e. the high computa-
tional complexity, we can cut the search space by us-
ing a traditional boundary classifier (tbc), e.g. (Prad-
han et al, 2004), which provides a small set of po-
tential argument nodes. Let PA be the set of nodes
located by tbc as arguments. We may consider the
set P of the NST s associated with any subset of
PA, i.e. P = {ps : s ? PA}. However, also
the classification ofP may be computationally prob-
lematic since theoretically there are |P| = 2|PA|
members.
In order to have a very efficient procedure, we
applied pastc to only the PA sets associated with
incorrect PAST s. A way to detect such incor-
rect NST s is to look for a node pair <n1, n2>?
PA ? PA of overlapping nodes, i.e. n1 is ances-
tor of n2 or viceversa. After we have detected such
nodes, we create two node sets PA1 = PA? {n1}
and PA2 = PA ? {n2} and classify them with the
pastc to select the correct set of argument bound-
aries. This procedure can be generalized to a set of
overlapping nodes O greater than 2 as reported in
Appendix 1.
Note that the algorithm selects a maximal set of
non-overlapping nodes, i.e. the first that is gener-
ated. Additionally, the worst case is rather rare thus
the algorithm is very fast on average.
The Figure 3 shows a working example of the
multi-stage classifier. In Frame (a), tbc labels as
potential arguments (gray color) three overlapping
nodes (in Arg.1). The overlap resolution algorithm
proposes two solutions (Frame (b)) of which only
one is correct. In fact, according to the second so-
lution the propositional phrase ?of the book? would
incorrectly be attached to the verbal predicate, i.e.
in contrast with the parse tree. The pastc, applied
52
to the two NST s, should detect this inconsistency
and provide the correct output. Note that, during the
learning, we generate the non-overlapping structures
in the same way to derive the positive and negative
examples.
4.2 Engineering Tree Fragment Features
In the Frame (b) of Figure 3, we show one of the
possible cases which pastc should deal with. The
critical problem is that the two NST s are perfectly
identical, thus, it is not possible to discern between
them using only their parse-tree fragments.
The solution to engineer novel features is to sim-
ply add the boundary information provided by the
tbc to the NST s. We mark with a progressive num-
ber the phrase type corresponding to an argument
node, starting from the leftmost argument. For ex-
ample, in the first NST of Frame (c), we mark
as NP-0 and NP-1 the first and second argument
nodes whereas in the second NST we have an hy-
pothesis of three arguments on the NP, NP and PP
nodes. We trasform them in NP-0, NP-1 and
PP-2.
This simple modification enables the tree ker-
nel to generate features useful to distinguish be-
tween two identical parse trees associated with dif-
ferent argument structures. For example, for the first
NST the fragments [NP-1 [NP][PP]], [NP
[DT][NN]] and [PP [IN][NP]] are gener-
ated. They do not match anymore with the [NP-0
[NP][PP]], [NP-1 [DT][NN]] and [PP-2
[IN][NP]] fragments of the second NST .
In order to verify the relevance of our model, the
next section provides empirical evidence about the
effectiveness of our approach.
5 The Experiments
The experiments were carried out with
the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes the tree kernels in the SVM-light
software (Joachims, 1999). For tbc, we used the
linear kernel with a regularization parameter (option
-c) equal to 1 and a cost-factor (option -j) of 10 to
have a higher Recall. For the pastc we used ? = 0.4
(see (Moschitti, 2004)).
As referring dataset, we used the PropBank cor-
pora available at www.cis.upenn.edu/?ace,
along with the Penn TreeBank 2
(www.cis.upenn.edu/?treebank) (Marcus et
al., 1993). This corpus contains about 53,700
sentences and a fixed split between training and
testing which has been used in other researches, e.g.
(Pradhan et al, 2004; Gildea and Palmer, 2002).
We did not include continuation and co-referring
arguments in our experiments.
We used sections from 02 to 07 (54,443 argu-
ment nodes and 1,343,046 non-argument nodes) to
train the traditional boundary classifier (tbc). Then,
we applied it to classify the sections from 08 to
21 (125,443 argument nodes vs. 3,010,673 non-
argument nodes). As results we obtained 2,988
NST s containing at least an overlapping node pair
out of the total 65,212 predicate structures (accord-
ing to the tbc decisions). From the 2,988 over-
lapping structures we extracted 3,624 positive and
4,461 negative NST s, that we used to train the
pastc.
The performance was evaluated with the F1 mea-
sure2 over the section 23. This contains 10,406 ar-
gument nodes out of 249,879 parse tree nodes. By
applying the tbc classifier we derived 235 overlap-
ping NSTs, from which we extracted 204 PAST s
and 385 incorrect predicate argument structures. On
such test data, the performance of pastc was very
high, i.e. 87.08% in Precision and 89.22% in Recall.
Using the pastc we removed from the tbc the PA
that cause overlaps. To measure the impact on the
boundary identification performance, we compared
it with three different boundary classification base-
lines:
? tbc: overlaps are ignored and no decision is
taken. This provides an upper bound for the
recall as no potential argument is rejected for
later labeling. Notice that, in presence of over-
lapping nodes, the sentence cannot be anno-
tated correctly.
? RND: one among the non-overlapping struc-
tures with maximal number of arguments is
randomly selected.
2F1 assigns equal importance to Precision P and Recall R,
i.e. F1 = 2P?RP+R .
53
tbc tbc+RND tbc+Heu tbc+pastc
P R F P R F P R F P R F
All Struct. 92.21 98.76 95.37 93.55 97.31 95.39 92.96 97.32 95.10 94.40 98.42 96.36
Overl. Struct. 98.29 65.8 78.83 74.00 72.27 73.13 68.12 75.23 71.50 89.61 92.68 91.11
Table 1: Two-steps boundary classification performance using the traditional boundary classifier tbc, the random selection of
non-overlapping structures (RND), the heuristic to select the most suitable non-overlapping node set (Heu) and the predicate
argument spanning tree classifier (pastc).
? Heu (heuristic): one of the NST s which con-
tain the nodes with the lowest overlapping
score is chosen. This score counts the number
of overlapping node pairs in the NST . For ex-
ample, in Figure 3.(a) we have a NP that over-
laps with two nodes NP and PP, thus it is as-
signed a score of 2.
The third row of Table 1 shows the results of tbc,
tbc + RND, tbc + Heu and tbc + pastc in the
columns 2,3,4 and 5, respectively. We note that:
? The tbc F1 is slightly higher than the result ob-
tained in (Pradhan et al, 2004), i.e. 95.37%
vs. 93.8% on same training/testing conditions,
i.e. (same PropBank version, same training and
testing split and same machine learning algo-
rithm). This is explained by the fact that we
did not include the continuations and the co-
referring arguments that are more difficult to
detect.
? Both RND and Heu do not improve the tbc re-
sult. This can be explained by observing that in
the 50% of the cases a correct node is removed.
? When, to select the correct node, the pastc is
used, the F1 increases of 1.49%, i.e. (96.86 vs.
95.37). This is a very good result considering
that to increase the very high baseline of tbc is
hard.
In order to give a fairer evaluation of our approach
we tested the above classifiers on the overlapping
structures only, i.e. we measured the pastc improve-
ment on all and only the structures that required its
application. Such reduced test set contains 642 ar-
gument nodes and 15,408 non-argument nodes. The
fourth row of Table 1 reports the classifier perfor-
mance on such task. We note that the pastc im-
proves the other heuristics of about 20%.
6 Related Work
Recently, many kernels for natural language applica-
tions have been designed. In what follows, we high-
light their difference and properties.
The tree kernel used in this article was proposed
in (Collins and Duffy, 2002) for syntactic parsing re-
ranking. It was experimented with the Voted Percep-
tron and was shown to improve the syntactic parsing.
A refinement of such technique was presented in
(Taskar et al, 2004). The substructures produced by
the proposed tree kernel were bound to local prop-
erties of the target parse tree and more lexical infor-
mation was added to the overall kernel function.
In (Zelenko et al, 2003), two kernels over syn-
tactic shallow parser structures were devised for
the extraction of linguistic relations, e.g. person-
affiliation. To measure the similarity between two
nodes, the contiguous string kernel and the sparse
string kernel (Lodhi et al, 2000) were used. The
former can be reduced to the contiguous substring
kernel whereas the latter can be transformed in the
non-contiguous string kernel. The high running time
complexity, caused by the general form of the frag-
ments, limited the experiments on data-set of just
200 news items.
In (Cumby and Roth, 2003), it is proposed a de-
scription language that models feature descriptors
to generate different feature type. The descriptors,
which are quantified logical prepositions, are instan-
tiated by means of a concept graph which encodes
the structural data. In the case of relation extraction
the concept graph is associated with a syntactic shal-
low parse and the extracted propositional features
express fragments of a such syntactic structure. The
experiments over the named entity class categoriza-
tion show that when the description language selects
an adequate set of tree fragments the Voted Percep-
tron algorithm increases its classification accuracy.
In (Culotta and Sorensen, 2004) a dependency
54
tree kernel is used to detect the Named Entity classes
in natural language texts. The major novelty was
the combination of the contiguous and sparse ker-
nels with the word kernel. The results show that
the contiguous outperforms the sparse kernel and the
bag-of-words.
7 Conclusions
The feature design for new natural language learn-
ing tasks is difficult. We can take advantage from
the kernel methods to model our intuitive knowledge
about the target linguistic phenomenon. In this pa-
per we have shown that we can exploit the properties
of tree kernels to engineer syntactic features for the
predicate argument boundary detection task.
Preliminary results on gold standard trees suggest
that (1) the information related to the whole predi-
cate argument structure is important and (2) tree ker-
nel can be used to generate syntactic features.
In the future, we would like to use an approach
similar to the PAST classifier on parses provided
by different parsing models to detect boundary and
to classify semantic role more accurately .
Acknowledgements
We wish to thank Ana-Maria Giuglea for her help in
the design and implementation of the basic Seman-
tic Role Labeling system that we used in the experi-
ments.
References
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In ACL02.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML 2000.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 423?429,
Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel methods for
relational learning. In Proceedings of the Twentieth
International Conference (ICML 2003), Washington,
DC, USA.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, Sapporo,
Japan.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA, USA.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2002), Las Palmas, Spain.
Ron Kohavi and Dan Sommerfield. 1995. Feature sub-
set selection using the wrapper model: Overfitting and
dynamic search space topology. In The First Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 192?197. AAAI Press, Menlo Park,
California, August. Journal version in AIJ.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Christopher Watkins. 2000. Text clas-
sification using string kernels. In NIPS, pages 563?
569.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The Penn Treebank. Computational Linguistics,
19:313?330.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow semantic parsing. In proceedings of
the 42th Conference on Association for Computational
Linguistic (ACL-2004), Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. to appear in Machine Learning Journal.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Dekang Lin and Dekai Wu, editors, Proceedings of
EMNLP 2004, pages 1?8, Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Kristina Toutanova, Penka Markova, and Christopher D.
Manning. 2004. The leaf projection path view of
parse trees: Exploring string kernels for hpsg parse se-
lection. In Proceedings of EMNLP 2004.
55
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research.
Appendix 1: Generalized Boundary
Selection Algorithm
Let O be the set of overlapping nodes of PA, and
NO the set of non overlapping nodes of PA.
Let subs(?1)(A) = {B|B ? 2A, |B| = |A| ? 1}.
Let O? = subs(?1)(O).
while(true)
begin
1. H = ?
2. ?o ? O?:
(a) If o does not include any overlapping node
pair
then H = H ? {o}
3. If H 6= ? then:
(a) Let s? =argmaxo?H pastc(pNO?o),
where pNO?o represents the node span-
ning tree compatible with o, and the
pastc(pNO?o) is the score provided by the
PAST SVM categorizer on it
(b) If pastc(s?) > 0 then RETURN( s?)
4. If O? = {?} then RETURN( NO )
5. Else:
(a) O? = O? ?H
(b) O? = ?o?O? subs(?1)(o)
end
56
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 1?8, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Effective use of WordNet semantics via kernel-based learning
Roberto Basili and Marco Cammisa and Alessandro Moschitti
Department of Computer Science
University of Rome ?Tor Vergata?, Rome, Italy
{basili,cammisa,moschitti}@info.uniroma2.it
Abstract
Research on document similarity has
shown that complex representations are
not more accurate than the simple bag-of-
words. Term clustering, e.g. using latent
semantic indexing, word co-occurrences
or synonym relations using a word ontol-
ogy have been shown not very effective.
In particular, when to extend the similar-
ity function external prior knowledge is
used, e.g. WordNet, the retrieval system
decreases its performance. The critical is-
sues here are methods and conditions to
integrate such knowledge.
In this paper we propose kernel func-
tions to add prior knowledge to learn-
ing algorithms for document classifica-
tion. Such kernels use a term similarity
measure based on the WordNet hierarchy.
The kernel trick is used to implement such
space in a balanced and statistically co-
herent way. Cross-validation results show
the benefit of the approach for the Support
Vector Machines when few training data is
available.
1 Introduction
The large literature on term clustering, term sim-
ilarity and weighting schemes shows that docu-
ment similarity is a central topic in Information Re-
trieval (IR). The research efforts have mostly been
directed in enriching the document representation
by using clustering (term generalization) or adding
compounds (term specifications). These studies are
based on the assumption that the similarity between
two documents can be expressed as the similarity be-
tween pairs of matching terms. Following this idea,
term clustering methods based on corpus term dis-
tributions or on external prior knowledge (e.g. pro-
vided by WordNet) were used to improve the basic
term matching.
An example of statistical clustering is given in
(Bekkerman et al, 2001). A feature selection tech-
nique, which clusters similar features/words, called
the Information Bottleneck (IB), was applied to Text
Categorization (TC). Such cluster based representa-
tion outperformed the simple bag-of-words on only
one out of the three experimented collections. The
effective use of external prior knowledge is even
more difficult since no attempt has ever been suc-
cessful to improve document retrieval or text classi-
fication accuracy, (e.g. see (Smeaton, 1999; Sussna,
1993; Voorhees, 1993; Voorhees, 1994; Moschitti
and Basili, 2004)).
The main problem of term cluster based represen-
tations seems the unclear nature of the relationship
between the word and the cluster information lev-
els. Even if (semantic) clusters tend to improve the
system recall, simple terms are, on a large scale,
more accurate (e.g. (Moschitti and Basili, 2004)).
To overcome this problem, hybrid spaces containing
terms and clusters were experimented (e.g. (Scott
and Matwin, 1999)) but the results, again, showed
that the mixed statistical distributions of clusters and
terms impact either marginally or even negatively on
the overall accuracy.
In (Voorhees, 1993; Smeaton, 1999), clusters of
synonymous terms as defined in WordNet (WN)
(Fellbaum, 1998) were used for document retrieval.
The results showed that the misleading information
due to the wrong choice of the local term senses
causes the overall accuracy to decrease. Word sense
disambiguation (WSD) was thus applied beforehand
by indexing the documents by means of disam-
biguated senses, i.e. synset codes (Smeaton, 1999;
1
Sussna, 1993; Voorhees, 1993; Voorhees, 1994;
Moschitti and Basili, 2004). However, even the
state-of-the-art methods for WSD did not improve
the accuracy because of the inherent noise intro-
duced by the disambiguation mistakes. The above
studies suggest that term clusters decrease the pre-
cision of the system as they force weakly related or
unrelated (in case of disambiguation errors) terms to
give a contribution in the similarity function. The
successful introduction of prior external knowledge
relies on the solution of the above problem.
In this paper, a model to introduce the semantic
lexical knowledge contained in the WN hierarchy
in a supervised text classification task has been pro-
posed. Intuitively, the main idea is that the docu-
ments d are represented through the set of all pairs
in the vocabulary < t, t? >? V ? V originating by
the terms t ? d and all the words t? ? V , e.g. the
WN nouns. When the similarity between two docu-
ments is evaluated, their matching pairs are used to
account for the final score. The weight given to each
term pair is proportional to the similarity that the two
terms have in WN. Thus, the term t of the first docu-
ment contributes to the document similarity accord-
ing to its relatedness with any of the terms of the
second document and the prior external knowledge,
provided by WN, quantifies the single term to term
relatedness. Such approach has two advantages: (a)
we obtain a well defined space which supports the
similarity between terms of different surface forms
based on external knowledge and (b) we avoid to
explicitly define term or sense clusters which in-
evitably introduce noise.
The class of spaces which embeds the above pair
information may be composed by O(|V |2) dimen-
sions. If we consider only the WN nouns (about
105), our space contains about 1010 dimensions
which is not manageable by most of the learning al-
gorithms. Kernel methods, can solve this problem as
they allow us to use an implicit space representation
in the learning algorithms. Among them Support
Vector Machines (SVMs) (Vapnik, 1995) are kernel
based learners which achieve high accuracy in pres-
ence of many irrelevant features. This is another im-
portant property as selection of the informative pairs
is left to the SVM learning.
Moreover, as we believe that the prior knowledge
in TC is not so useful when there is a sufficient
amount of training documents, we experimented our
model in poor training conditions (e.g. less equal
than 20 documents for each category). The improve-
ments in the accuracy, observed on the classification
of the well known Reuters and 20 NewsGroups cor-
pora, show that our document similarity model is
very promising for general IR tasks: unlike previous
attempts, it makes sense of the adoption of semantic
external resources (i.e. WN) in IR.
Section 2 introduces the WordNet-based term
similarity. Section 3 defines the new document simi-
larity measure, the kernel function and its use within
SVMs. Section 4 presents the comparative results
between the traditional linear and the WN-based
kernels within SVMs. In Section 5 comparative dis-
cussion against the related IR literature is carried
out. Finally Section 6 derives the conclusions.
2 Term similarity based on general
knowledge
In IR, any similarity metric in the vector space mod-
els is driven by lexical matching. When small train-
ing material is available, few words can be effec-
tively used and the resulting document similarity
metrics may be inaccurate. Semantic generaliza-
tions overcome data sparseness problems as con-
tributions from different but semantically similar
words are made available.
Methods for the induction of semantically in-
spired word clusters have been widely used in lan-
guage modeling and lexical acquisition tasks (e.g.
(Clark and Weir, 2002)). The resource employed
in most works is WordNet (Fellbaum, 1998) which
contains three subhierarchies: for nouns, verbs and
adjectives. Each hierarchy represents lexicalized
concepts (or senses) organized according to an ?is-
a-kind-of ? relation. A concept s is described by
a set of words syn(s) called synset. The words
w ? syn(s) are synonyms according to the sense
s.
For example, the words line, argumentation, logi-
cal argument and line of reasoning describe a synset
which expresses the methodical process of logical
reasoning (e.g. ?I can?t follow your line of reason-
ing?). Each word/term may be lexically related to
more than one synset depending on its senses. The
word line is also a member of the synset line, divid-
ing line, demarcation and contrast, as a line denotes
2
also a conceptual separation (e.g. ?there is a nar-
row line between sanity and insanity?). The Wordnet
noun hierarchy is a direct acyclic graph1 in which
the edges establish the direct isa relations between
two synsets.
2.1 The Conceptual Density
The automatic use of WordNet for NLP and IR tasks
has proved to be very complex. First, how the topo-
logical distance among senses is related to their cor-
responding conceptual distance is unclear. The per-
vasive lexical ambiguity is also problematic as it im-
pacts on the measure of conceptual distances be-
tween word pairs. Second, the approximation of a
set of concepts by means of their generalization in
the hierarchy implies a conceptual loss that affects
the target IR (or NLP) tasks. For example, black
and white are colors but are also chess pieces and
this impacts on the similarity score that should be
used in IR applications. Methods to solve the above
problems attempt to map a priori the terms to spe-
cific generalizations levels, i.e. to cuts in the hier-
archy (e.g. (Li and Abe, 1998; Resnik, 1997)), and
use corpus statistics for weighting the resulting map-
pings. For several tasks (e.g. in TC) this is unsatis-
factory: different contexts of the same corpus (e.g.
documents) may require different generalizations of
the same word as they independently impact on the
document similarity.
On the contrary, the Conceptual Density (CD)
(Agirre and Rigau, 1996) is a flexible semantic simi-
larity which depends on the generalizations of word
senses not referring to any fixed level of the hier-
archy. The CD defines a metrics according to the
topological structure of WordNet and can be seem-
ingly applied to two or more words. The measure
formalized hereafter adapt to word pairs a more gen-
eral definition given in (Basili et al, 2004).
We denote by s? the set of nodes of the hierarchy
rooted in the synset s, i.e. {c ? S|c isa s}, where S
is the set of WN synsets. By definition ?s ? S, s ?
s?. CD makes a guess about the proximity of the
senses, s1 and s2, of two words u1 and u2, accord-
ing to the information expressed by the minimal sub-
hierarchy, s?, that includes them. Let Si be the set of
1As only the 1% of its nodes own more than one parent in
the graph, most of the techniques assume the hierarchy to be a
tree, and treat the few exception heuristically.
generalizations for at least one sense si of the word
ui, i.e. Si = {s ? S|si ? s?, ui ? syn(si)}. The
CD of u1 and u2 is:
CD(u1, u2) =
?
???
???
0 iff S1 ? S2 = ?
maxs?S1?S2
?h
i=0(?(s?))i
|s?|
otherwise
(1)
where:
? S1?S2 is the set of WN shared generalizations
(i.e. the common hypernyms) of u1 and u2
? ?(s?) is the average number of children per node
(i.e. the branching factor) in the sub-hierarchy
s?. ?(s?) depends on WordNet and in some cases
its value can approach 1.
? h is the depth of the ideal, i.e. maximally
dense, tree with enough leaves to cover the
two senses, s1 and s2, according to an average
branching factor of ?(s?). This value is actually
estimated by:
h =
{ blog?(s?)2c iff ?(s?) 6= 1
2 otherwise (2)
When ?(s)=1, h ensures a tree with at least 2
nodes to cover s1 and s2 (height = 2).
? |s?| is the number of nodes in the sub-hierarchy
s?. This value is statically measured on WN and
it is a negative bias for the higher level general-
izations (i.e. larger s?).
CD models the semantic distance as the density
of the generalizations s ? S1 ? S2. Such density is
the ratio between the number of nodes of the ideal
tree and |s?|. The ideal tree should (a) link the two
senses/nodes s1 and s2 with the minimal number
of edges (isa-relations) and (b) maintain the same
branching factor (bf ) observed in s?. In other words,
this tree provides the minimal number of nodes (and
isa-relations) sufficient to connect s1 and s2 accord-
ing to the topological structure of s?. For example, if
s? has a bf of 2 the ideal tree connects the two senses
with a single node (their father). If the bf is 1.5, to
replicate it, the ideal tree must contain 4 nodes, i.e.
the grandfather which has a bf of 1 and the father
which has bf of 2 for an average of 1.5. When bf is
1 the Eq. 1 degenerates to the inverse of the number
of nodes in the path between s1 and s2, i.e. the sim-
ple proximity measure used in (Siolas and d?Alch
Buc, 2000).
3
It is worth noting that for each pair CD(u1, u2)
determines the similarity according to the closest
lexical senses, s1, s2 ? s?: the remaining senses of u1
and u2 are irrelevant, with a resulting semantic dis-
ambiguation side effect. CD has been successfully
applied to semantic tagging ((Basili et al, 2004)).
As the WN hierarchies for other POS classes (i.e.
verb and adjectives) have topological properties dif-
ferent from the noun hyponimy network, their se-
mantics is not suitably captured by Eq. 1. In this
paper, Eq. 1 has thus been only applied to noun
pairs. As the high number of such pairs increases
the computational complexity of the target learn-
ing algorithm, efficient approaches are needed. The
next section describes how kernel methods can make
practical the use of the Conceptual Density in Text
Categorization.
3 A WordNet Kernel for document
similarity
Term similarities are used to design document simi-
larities which are the core functions of most TC al-
gorithms. The term similarity proposed in Eq. 1
is valid for all term pairs of a target vocabulary and
has two main advantages: (1) the relatedness of each
term occurring in the first document can be com-
puted against all terms in the second document, i.e.
all different pairs of similar (not just identical) to-
kens can contribute and (2) if we use all term pair
contributions in the document similarity we obtain a
measure consistent with the term probability distri-
butions, i.e. the sum of all term contributions does
not penalize or emphasize arbitrarily any subset of
terms. The next section presents more formally the
above idea.
3.1 A semantic vector space
Given two documents d1 and d2 ? D (the document-
set) we define their similarity as:
K(d1, d2) =
?
w1?d1,w2?d2
(?1?2)? ?(w1, w2) (3)
where ?1 and ?2 are the weights of the words (fea-
tures) w1 and w2 in the documents d1 and d2, re-
spectively and ? is a term similarity function, e.g.
the conceptual density defined in Section 2. To
prove that Eq. 3 is a valid kernel is enough to
show that it is a specialization of the general defi-
nition of convolution kernels formalized in (Haus-
sler, 1999). Hereafter, we report such definition. Let
X,X1, .., Xm be separable metric spaces, x ? X
a structure and ~x = x1, ..., xm its parts, where
xi ? Xi ?i = 1, ..,m. Let R be a relation on
the set X?X1? ..?Xm such that R(~x, x) is ?true?
if ~x are the parts of x. We indicate with R?1(x) the
set {~x : R(~x, x)}. Given two objects x and y ? X
their similarity K(x, y) is defined as:
K(x, y) =
?
~x?R?1(x)
?
~y?R?1(y)
m?
i=1
Ki(xi, yi) (4)
If X defines the document set (i.e. D = X),
and X1 the vocabulary of the target document corpus
(X1 = V ), it follows that: x = d (a document), ~x =
x1 = w ? V (a word which is a part of the document
d) and R?1(d) defines the set of words in the doc-
ument d. As ?mi=1 Ki(xi, yi) = K1(x1, y1), then
K1(x1, y1) = K(w1, w2) = (?1?2) ? ?(w1, w2),
i.e. Eq. 3.
The above equation can be used in support vector
machines as illustrated by the next section.
3.2 Support Vector Machines and Kernel
methods
Given the vector space in R? and a set of positive
and negative points, SVMs classify vectors accord-
ing to a separating hyperplane, H(~x) = ~??~x+b = 0,
where ~x and ~? ? R? and b ? R are learned by apply-
ing the Structural Risk Minimization principle (Vap-
nik, 1995). From the kernel theory we have that:
H(~x) =
( ?
h=1..l
?h ~xh
)
?~x+b =
?
h=1..l
?h~xh?~x+b =
?
h=1..l
?h?(dh) ? ?(d) + b =
?
h=1..l
?hK(dh, d) + b
(5)
where, d is a classifying document and dh are all the
l training instances, projected in ~x and ~xh respec-
tively. The product K(d, dh) =<?(d) ? ?(dh)> is
the Semantic WN-based Kernel (SK) function asso-
ciated with the mapping ?.
Eq. 5 shows that to evaluate the separating hy-
perplane in R? we do not need to evaluate the entire
vector ~xh or ~x. Actually, we do not know even the
mapping ? and the number of dimensions, ?. As
it is sufficient to compute K(d, dh), we can carry
out the learning with Eq. 3 in the Rn, avoiding to
4
use the explicit representation in the R? space. The
real advantage is that we can consider only the word
pairs associated with non-zero weight, i.e. we can
use a sparse vector computation. Additionally, to
have a uniform score across different document size,
the kernel function can be normalized as follows:
SK(d1,d2)?
SK(d1,d1)?SK(d2,d2)
4 Experiments
The use of WordNet (WN) in the term similarity
function introduces a prior knowledge whose impact
on the Semantic Kernel (SK) should be experimen-
tally assessed. The main goal is to compare the tradi-
tional Vector Space Model kernel against SK, both
within the Support Vector learning algorithm.
The high complexity of the SK limits the size
of the experiments that we can carry out in a fea-
sible time. Moreover, we are not interested to large
collections of training documents as in these train-
ing conditions the simple bag-of-words models are
in general very effective, i.e. they seems to model
well the document similarity needed by the learning
algorithms. Thus, we carried out the experiments
on small subsets of the 20NewsGroups2 (20NG)
and the Reuters-215783 corpora to simulate critical
learning conditions.
4.1 Experimental set-up
For the experiments, we used the SVM-
light software (Joachims, 1999) (available at
svmlight.joachims.org) with the default linear
kernel on the token space (adopted as the baseline
evaluations). For the SK evaluation we imple-
mented the Eq. 3 with ?(?, ?) = CD(?, ?) (Eq. 1)
inside SVM-light. As Eq. 1 is only defined for
nouns, a part of speech (POS) tagger has been previ-
ously applied. However, also verbs, adjectives and
numerical features were included in the pair space.
For these tokens a CD = 0 is assigned to pairs
made by different strings. As the POS-tagger could
introduce errors, in a second experiment, any token
with a successful look-up in the WN noun hierarchy
was considered in the kernel. This approximation
has the benefit to retrieve useful information even
2Available at www.ai.mit.edu/people/jrennie/
20Newsgroups/.
3The Apte? split available at kdd.ics.uci.edu/
databases/reuters21578/reuters21578.html.
for verbs and capture the similarity between verbs
and some nouns, e.g. to drive (via the noun drive)
has a common synset with parkway.
For the evaluations, we applied a careful SVM
parameterization: a preliminary investigation sug-
gested that the trade-off (between the training-set er-
ror and margin, i.e. c option in SVM-light) parame-
ter optimizes the F1 measure for values in the range
[0.02,0.32]4. We noted also that the cost-factor pa-
rameter (i.e. j option) is not critical, i.e. a value of
10 always optimizes the accuracy. The feature se-
lection techniques and the weighting schemes were
not applied in our experiments as they cannot be ac-
curately estimated from the small available training
data.
The classification performance was evaluated by
means of the F1 measure5 for the single category and
the MicroAverage for the final classifier pool (Yang,
1999). Given the high computational complexity of
SK we selected 8 categories from the 20NG6 and 8
from the Reuters corpus7
To derive statistically significant results with few
training documents, for each corpus, we randomly
selected 10 different samples from the 8 categories.
We trained the classifiers on one sample, parameter-
ized on a second sample and derived the measures
on the other 8. By rotating the training sample we
obtained 80 different measures for each model. The
size of the samples ranges from 24 to 160 documents
depending on the target experiment.
4.2 Cross validation results
The SK (Eq. 3) was compared with the linear kernel
which obtained the best F1 measure in (Joachims,
1999). Table 1 reports the first comparative results
for 8 categories of 20NG on 40 training documents.
The results are expressed as the Mean and the Std.
Dev. over 80 runs. The F1 are reported in Column 2
for the linear kernel, i.e. bow, in Column 3 for SK
without applying POS information and in Column 4
4We used all the values from 0.02 to 0.32 with step 0.02.
5F1 assigns equal importance to Precision P and Recall R,
i.e. F1 = 2P ?RP+R .
6We selected the 8 most different categories (in terms of
their content) i.e. Atheism, Computer Graphics, Misc Forsale,
Autos, Sport Baseball, Medicine, Talk Religions and Talk Poli-
tics.
7We selected the 8 largest categories, i.e. Acquisition, Earn,
Crude, Grain, Interest, Money-fx, Trade and Wheat.
5
for SK with the use of POS information (SK-POS).
The last row shows the MicroAverage performance
for the above three models on all 8 categories. We
note that SK improves bow of 3%, i.e. 34.3% vs.
31.5% and that the POS information reduces the im-
provement of SK, i.e. 33.5% vs. 34.3%.
To verify the hypothesis that WN information is
useful in low training data conditions we repeated
the evaluation over the 8 categories of Reuters with
samples of 24 and 160 documents, respectively. The
results reported in Table 2 shows that (1) again SK
improves bow (41.7% - 37.2% = 4.5%) and (2) as
the number of documents increases the improvement
decreases (77.9% - 75.9% = 2%). It is worth noting
that the standard deviations tend to assume high val-
ues. In general, the use of 10 disjoint training/testing
samples produces a higher variability than the n-
fold cross validation which insists on the same docu-
ment set. However, this does not affect the t-student
confidence test over the differences between the Mi-
croAverage of SK and bow since the former has a
higher accuracy at 99% confidence level.
The above findings confirm that SK outperforms
the bag-of-words kernel in critical learning condi-
tions as the semantic contribution of the SK recov-
ers useful information. To complete this study we
carried out experiments with samples of different
size, i.e. 3, 5, 10, 15 and 20 documents for each
category. Figures 1 and 2 show the learning curves
for 20NG and Reuters corpora. Each point refers to
the average on 80 samples.
As expected the improvement provided by SK
decreases when more training data is available.
However, the improvements are not negligible yet.
The SK model (without POS information) pre-
serves about 2-3% of improvement with 160 training
documents. The matching allowed between noun-
verb pairs still captures semantic information which
is useful for topic detection. In particular, during
the similarity estimation, each word activates 60.05
pairs on average. This is particularly useful to in-
crease the amount of information available to the
SVMs.
Finally, we carried out some experiments with
160 Reuters documents by discarding the string
matching from SK. Only words having different
surface forms were allowed to give contributions to
the Eq. 3.
Category bow SK SK-POS
Atheism 29.5?19.8 32.0?16.3 25.2?17.2
Comp.Graph 39.2?20.7 39.3?20.8 29.3?21.8
Misc.Forsale 61.3?17.7 51.3?18.7 49.5?20.4
Autos 26.2?22.7 26.0?20.6 33.5?26.8
Sport.Baseb. 32.7?20.1 36.9?22.5 41.8?19.2
Sci.Med 26.1?17.2 18.5?17.4 16.6?17.2
Talk.Relig. 23.5?11.6 28.4?19.0 27.6?17.0
Talk.Polit. 28.3?17.5 30.7?15.5 30.3?14.3
MicroAvg. F1 31.5?4.8 34.3?5.8 33.5?6.4
Table 1: Performance of the linear and Semantic Kernel with
40 training documents over 8 categories of 20NewsGroups col-
lection.
Category 24 docs 160 docs
bow SK bow SK
Acq. 55.3?18.1 50.8?18.1 86.7?4.6 84.2?4.3
Crude 3.4?5.6 3.5?5.7 64.0?20.6 62.0?16.7
Earn 64.0?10.0 64.7?10.3 91.3?5.5 90.4?5.1
Grain 45.0?33.4 44.4?29.6 69.9?16.3 73.7?14.8
Interest 23.9?29.9 24.9?28.6 67.2?12.9 59.8?12.6
Money-fx 36.1?34.3 39.2?29.5 69.1?11.9 67.4?13.3
Trade 9.8?21.2 10.3?17.9 57.1?23.8 60.1?15.4
Wheat 8.6?19.7 13.3?26.3 23.9?24.8 31.2?23.0
Mic.Avg. 37.2?5.9 41.7?6.0 75.9?11.0 77.9?5.7
Table 2: Performance of the linear and Semantic Kernel with
40 and 160 training documents over 8 categories of the Reuters
corpus.
30.0
33.0
36.0
39.0
42.0
45.0
48.0
51.0
54.0
40 60 80 100 120 140 160
# Training Documents
Mic
ro-
Ave
rag
e F1
bowSKSK-POS
Figure 1: MicroAverage F1 of SVMs using bow, SK and
SK-POS kernels over the 8 categories of 20NewsGroups.
The important outcome is that SK converges to a
MicroAverage F1 measure of 56.4% (compare with
Table 2). This shows that the word similarity pro-
vided by WN is still consistent and, although in the
worst case, slightly effective for TC: the evidence
is that a suitable balancing between lexical ambigu-
ity and topical relatedness is captured by the SVM
learning.
6
35.0
40.0
45.0
50.0
55.0
60.0
65.0
70.0
75.0
80.0
20 40 60 80 100 120 140 160
# Training Documents
Mic
ro-
Ave
rag
e F
1
bowSK
Figure 2: MicroAverage F1 of SVMs using bow and SK over
the 8 categories of the Reuters corpus.
5 Related Work
The IR studies in this area focus on the term similar-
ity models to embed statistical and external knowl-
edge in document similarity.
In (Kontostathis and Pottenger, 2002) a Latent Se-
mantic Indexing analysis was used for term cluster-
ing. Such approach assumes that values xij in the
transformed term-term matrix represents the simi-
larity (> 0) and anti-similarity between terms i and
j. By extension, a negative value represents an anti-
similarity between i and j enabling both positive and
negative clusters of terms. Evaluation of query ex-
pansion techniques showed that positive clusters can
improve Recall of about 18% for the CISI collection,
2.9% for MED and 3.4% for CRAN. Furthermore,
the negative clusters, when used to prune the result
set, improve the precision.
The use of external semantic knowledge seems
to be more problematic in IR. In (Smeaton, 1999),
the impact of semantic ambiguity on IR is stud-
ied. A WN-based semantic similarity function be-
tween noun pairs is used to improve indexing and
document-query matching. However, the WSD al-
gorithm had a performance ranging between 60-
70%, and this made the overall semantic similarity
not effective.
Other studies using semantic information for im-
proving IR were carried out in (Sussna, 1993) and
(Voorhees, 1993; Voorhees, 1994). Word seman-
tic information was here used for text indexing and
query expansion, respectively. In (Voorhees, 1994)
it is shown that semantic information derived di-
rectly from WN without a priori WSD produces
poor results.
The latter methods are even more problematic in
TC (Moschitti and Basili, 2004). Word senses tend
to systematically correlate with the positive exam-
ples of a category. Different categories are better
characterized by different words rather than differ-
ent senses. Patterns of lexical co-occurrences in the
training data seem to suffice for automatic disam-
biguation. (Scott and Matwin, 1999) use WN senses
to replace simple words without word sense disam-
biguation and small improvements are derived only
for a small corpus. The scale and assessment pro-
vided in (Moschitti and Basili, 2004) (3 corpora us-
ing cross-validation techniques) showed that even
the accurate disambiguation of WN senses (about
80% accuracy on nouns) did not improve TC.
In (Siolas and d?Alch Buc, 2000) was proposed
an approach similar to the one presented in this ar-
ticle. A term proximity function is used to design
a kernel able to semantically smooth the similarity
between two document terms. Such semantic ker-
nel was designed as a combination of the Radial Ba-
sis Function (RBF) kernel with the term proximity
matrix. Entries in this matrix are inversely propor-
tional to the length of the WN hierarchy path link-
ing the two terms. The performance, measured over
the 20NewsGroups corpus, showed an improvement
of 2% over the bag-of-words. Three main differ-
ences exist with respect to our approach. First, the
term proximity does not fully capture the WN topo-
logical information. Equidistant terms receive the
same similarity irrespectively from their generaliza-
tion level. For example, Sky and Location (direct
hyponyms of Entity) receive a similarity score equal
to knife and gun (hyponyms of weapon). More ac-
curate measures have been widely discussed in lit-
erature, e.g. (Resnik, 1997). Second, the kernel-
based CD similarity is an elegant combination of
lexicalized and semantic information. In (Siolas and
d?Alch Buc, 2000) the combination of weighting
schemes, the RBF kernel and the proximitry matrix
has a much less clear interpretation. Finally, (Siolas
and d?Alch Buc, 2000) selected only 200 features
via Mutual Information statistics. In this way rare
or non statistically significant terms are neglected
while being source of often relevant contributions in
the SK space modeled over WN.
Other important work on semantic kernel for re-
trieval has been developed in (Cristianini et al,
7
2002; Kandola et al, 2002). Two methods for in-
ferring semantic similarity from a corpus were pro-
posed. In the first a system of equations were de-
rived from the dual relation between word-similarity
based on document-similarity and viceversa. The
equilibrium point was used to derive the semantic
similarity measure. The second method models se-
mantic relations by means of a diffusion process on
a graph defined by lexicon and co-occurrence in-
formation. The major difference with our approach
is the use of a different source of prior knowledge.
Similar techniques were also applied in (Hofmann,
2000) to derive a Fisher kernel based on a latent class
decomposition of the term-document matrix.
6 Conclusions
The introduction of semantic prior knowledge in
IR has always been an interesting subject as the
examined literature suggests. In this paper, we
used the conceptual density function on the Word-
Net (WN) hierarchy to define a document similar-
ity metric. Accordingly, we defined a semantic
kernel to train Support Vector Machine classifiers.
Cross-validation experiments over 8 categories of
20NewsGroups and Reuters over multiple samples
have shown that in poor training data conditions, the
WN prior knowledge can be effectively used to im-
prove (up to 4.5 absolute percent points, i.e. 10%)
the TC accuracy.
These promising results enable a number of future
researches: (1) larger scale experiments with differ-
ent measures and semantic similarity models (e.g.
(Resnik, 1997)); (2) improvement of the overall ef-
ficiency by exploring feature selection methods over
the SK, and (3) the extension of the semantic sim-
ilarity by a general (i.e. non binary) application of
the conceptual density model.
References
E. Agirre and G. Rigau. 1996. Word sense disambiguation
using conceptual density. In Proceedings of COLING?96,
Copenhagen, Danmark.
R. Basili, M. Cammisa, and F. M. Zanzotto. 2004. A similar-
ity measure for unsupervised semantic disambiguation. In
In Proceedings of Language Resources and Evaluation Con-
ference, Lisbon, Portugal.
Ron Bekkerman, Ran El-Yaniv, Naftali Tishby, and Yoad Win-
ter. 2001. On feature distributional clustering for text cat-
egorization. In Proceedings of SIGIR?01 , New Orleans,
Louisiana, US.
Stephen Clark and David Weir. 2002. Class-based probability
estimation using a semantic hierarchy. Comput. Linguist.,
28(2):187?206.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2002.
Latent semantic kernels. J. Intell. Inf. Syst., 18(2-3):127?
152.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
D. Haussler. 1999. Convolution kernels on discrete struc-
tures. Technical report ucs-crl-99-10, University of Califor-
nia Santa Cruz.
Thomas Hofmann. 2000. Learning probabilistic models of
the web. In Research and Development in Information Re-
trieval.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
J. Kandola, J. Shawe-Taylor, and N. Cristianini. 2002. Learn-
ing semantic similarity. In NIPS?02) - MIT Press.
A. Kontostathis and W. Pottenger. 2002. Improving retrieval
performance with positive and negative equivalence classes
of terms.
Hang Li and Naoki Abe. 1998. Generalizing case frames using
a thesaurus and the mdl principle. Computational Linguis-
tics, 23(3).
Alessandro Moschitti and Roberto Basili. 2004. Complex
linguistic features for text classification: a comprehensive
study. In Proceedings of ECIR?04, Sunderland, UK.
P. Resnik. 1997. Selectional preference and sense disambigua-
tion. In Proceedings of ACL Siglex Workshop on Tagging
Text with Lexical Semantics, Why, What and How?, Wash-
ington, 1997.
Sam Scott and Stan Matwin. 1999. Feature engineering for
text classification. In Proceedings of ICML?99, Bled, SL.
Morgan Kaufmann Publishers, San Francisco, US.
Georges Siolas and Florence d?Alch Buc. 2000. Support vector
machines based on a semantic kernel for text categorization.
In Proceedings of IJCNN?00. IEEE Computer Society.
Alan F. Smeaton. 1999. Using NLP or NLP resources for in-
formation retrieval tasks. In Natural language information
retrieval, Kluwer Academic Publishers, Dordrecht, NL.
M. Sussna. 1993. Word sense disambiguation for free-text in-
dexing using a massive semantic network. In CKIM?93,.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer.
Ellen M. Voorhees. 1993. Using wordnet to disambiguate word
senses for text retrieval. In Proceedings SIGIR?93 Pitts-
burgh, PA, USA.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR?94,
ACM/Springer.
Y. Yang. 1999. An evaluation of statistical approaches to text
categorization. Information Retrieval Journal.
8
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 10?17,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Verb subcategorization kernels for automatic semantic labeling
Alessandro Moschitti and Roberto Basili
Department of Computer Science
University of Rome ?Tor Vergata?
Rome, Italy
{moschitti,basili}@info.uniroma2.it
Abstract
Recently, many researches in natural lan-
guage learning have considered the repre-
sentation of complex linguistic phenom-
ena by means of structural kernels. In
particular, tree kernels have been used to
represent verbal subcategorization frame
(SCF) information for predicate argument
classification. As the SCF is a relevant
clue to learn the relation between syn-
tax and semantic, the classification algo-
rithm accuracy was remarkable enhanced.
In this article, we extend such work by
studying the impact of the SCF tree kernel
on both PropBank and FrameNet seman-
tic roles. The experiments with Support
Vector Machines (SVMs) confirm a strong
link between the SCF and the semantics of
the verbal predicates as well as the bene-
fit of using kernels in diverse and complex
test conditions, e.g. classification of un-
seen verbs.
1 Introduction
Some theories of verb meaning are based on syn-
tactic properties, e.g. the alternations of verb argu-
ments (Levin, 1993). In turn, Verb Subcategoriza-
tion Frame (SCF) characterizes different syntactic
alternations, thus, it plays a central role in the link-
ing theory between verb semantics and their syntac-
tic structures.
Figure 1 shows the parse tree for the sentence
"John rented a room in Boston" along
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Predicate 
Arg. 0 
Arg. M 
S 
N 
NP 
D N 
VP 
V John 
in 
 rented 
a 
  room 
PP 
IN N 
Boston 
Arg. 1 
Figure 1: A predicate argument structure in a parse-tree rep-
resentation.
with the semantic shallow information embodied by
the verbal predicate to rent and its three arguments:
Arg0, Arg1 and ArgM. The SCF of such verb, i.e.
NP-PP, provides a synthesis of the predicate argu-
ment structure.
Currently, the systems which aim to derive se-
mantic shallow information from texts recognize the
SCF of a target verb and represent it as a flat feature
(e.g. (Xue and Palmer, 2004; Pradhan et al, 2004))
in the learning algorithm. To achieve this goal, a lex-
icon which describes the SCFs for each verb, is re-
quired. Such a resource is difficult to find especially
for specific domains, thus, several methods to auto-
matically extract SCF have been proposed (Korho-
nen, 2003). In (Moschitti, 2004), an alternative to
the SCF extraction was proposed, i.e. the SCF ker-
nel (SK). The subcategorization frame of verbs was
implicitly represented by means of the syntactic sub-
trees which include the predicate with its arguments.
The similarity between such syntactic structures was
evaluated by means of convolution kernels.
Convolution kernels are machine learning ap-
proaches which aim to describe structured data in
10
terms of its substructures. The similarity between
two structures is carried out by kernel functions
which determine the number of common substruc-
tures without evaluating the overall substructure
space. Thus, if we associate two SCFs with two
subtrees, we can measure their similarity with such
functions applied to the two trees. This approach
determines a more syntactically motivated verb par-
tition than the traditional method based on flat SCF
representations (e.g. the NP-PP of Figure 1). The
subtrees associated with SCF group the verbs which
have similar syntactic realizations, in turn, accord-
ing to Levin?s theories, this would suggest that they
are semantically related.
A preliminary study on the benefit of such ker-
nels was measured on the classification accuracy of
semantic arguments in (Moschitti, 2004). In such
work, the improvement on the PropBank arguments
(Kingsbury and Palmer, 2002) classification sug-
gests that SK adds information to the prediction
of semantic structures. On the contrary, the perfor-
mance decrease on the FrameNet data classification
shows the limit of such approach, i.e. when the syn-
tactic structures are shared among several semantic
roles SK seems to be useless.
In this article, we use Support Vector Machines
(SVMs) to deeply analyze the role of SK in the au-
tomatic predicate argument classification. The ma-
jor novelty of the article relates to the extensive ex-
perimentation carried out on the PropBank (Kings-
bury and Palmer, 2002) and FrameNet (Fillmore,
1982) corpora with diverse levels of task complex-
ity, e.g. test instances of unseen predicates (typi-
cal of free-text processing). The results show that:
(1) once a structural representation of a linguistic
object, e.g. SCF, is available we can use convolu-
tion kernels to study its connections with another
linguistic phenomenon, e.g. the semantic predicate
arguments. (2) The tree kernels automatically derive
the features (structures) which support also a sort of
back-off estimation in case of unseen verbs. (3) The
structural features are in general robust in all testing
conditions.
The remainder of this article is organized as fol-
lows: Section 2 defines the Predicate Argument Ex-
traction problem and the standard solution to solve
it. In Section 3 we present our kernels whereas
in Section 4 we show comparative results among
SVMs using standard features and the proposed ker-
nels. Finally, Section 5 summarizes the conclusions.
2 Parsing of Semantic Roles and Semantic
Arguments
There are two main resources that relate to predicate
argument structures: PropBank (PB) and FrameNet
(FN). PB is a 300,000 word corpus annotated with
predicative information on top of the Penn Treebank
2 Wall Street Journal texts. For any given pred-
icate, the expected arguments are labeled sequen-
tially from Arg 0 to Arg 9, ArgA and ArgM. The
Figure 1 shows an example of the PB predicate an-
notation. Predicates in PB are only embodied by
verbs whereas most of the times Arg 0 is the subject,
Arg 1 is the direct object and ArgM may indicate lo-
cations, as in our example.
FrameNet alo describes predicate/argument
structures but for this purpose it uses richer se-
mantic structures called frames. These latter are
schematic representations of situations involving
various participants, properties and roles, in which
a word may be typically used. Frame elements or
semantic roles are arguments of target words that
can be verbs or nouns or adjectives. In FrameNet,
the argument names are local to the target frames.
For example, assuming that attach is the target word
and Attaching is the target frame, a typical sentence
annotation is the following.
[Agent They] attachTgt [Item themselves]
[Connector with their mouthparts] and then
release a digestive enzyme secretion which
eats into the skin.
Several machine learning approaches for argu-
ment identification and classification have been de-
veloped, e.g. (Gildea and Jurasfky, 2002; Gildea and
Palmer, ; Gildea and Hockenmaier, 2003; Pradhan et
al., 2004). Their common characteristic is the adop-
tion of feature spaces that model predicate-argument
structures in a flat feature representation. In the next
section we present the common parse tree-based ap-
proach to this problem.
2.1 Predicate Argument Extraction
Given a sentence in natural language, all the predi-
cates associated with the verbs have to be identified
11
along with their arguments. This problem can be
divided into two subtasks: (a) the detection of the
target argument boundaries, i.e. all its compound-
ing words, and (b) the classification of the argument
type, e.g. Arg0 or ArgM in PropBank or Agent and
Goal in FrameNet.
The standard approach to learn both the detection
and the classification of predicate arguments is sum-
marized by the following steps:
1. Given a sentence from the training-set, gener-
ate a full syntactic parse-tree;
2. let P and A be the set of predicates and the
set of parse-tree nodes (i.e. the potential argu-
ments), respectively;
3. for each pair <p, a> ? P ?A:
? extract the feature representation set, Fp,a;
? if the subtree rooted in a covers exactly
the words of one argument of p, put Fp,a
in T+ (positive examples), otherwise put
it in T? (negative examples).
For instance, in Figure 1, for each combination of
the predicate rent with the nodes N, S, VP, V, NP,
PP, D or IN the instances Frent,a are generated. In
case the node a exactly covers ?Paul?, ?a room? or
?in Boston?, it will be a positive instance otherwise
it will be a negative one, e.g. Frent,IN .
The T+ and T? sets can be re-organized as posi-
tive T+argi and negative T?argi examples for each argu-
ment i. In this way, an individual ONE-vs-ALL clas-
sifier for each argument i can be trained. We adopted
this solution as it is simple and effective (Pradhan et
al., 2004). In the classification phase, given a sen-
tence of the test-set, all its Fp,a are generated and
classified by each individual classifier Ci. As a final
decision, we select the argument associated with the
maximum value among the scores provided by the
individual classifiers.
2.2 Standard feature space
The discovery of relevant features is, as usual, a
complex task, nevertheless, there is a common con-
sensus on the basic features that should be adopted.
These standard features, firstly proposed in (Gildea
and Jurasfky, 2002), refer to a flat information de-
rived from parse trees, i.e. Phrase Type, Predicate
Word, Head Word, Governing Category, Position
and Voice. For example, the Phrase Type indicates
the syntactic type of the phrase labeled as a predi-
cate argument, e.g. NP for Arg1 in Figure 1. The
Parse Tree Path contains the path in the parse tree
between the predicate and the argument phrase, ex-
pressed as a sequence of non-terminal labels linked
by direction (up or down) symbols, e.g. V ? VP ?
NP for Arg1 in Figure 1. The Predicate Word is the
surface form of the verbal predicate, e.g. rent for all
arguments.
In the next section we describe the SVM approach
and the basic kernel theory for the predicate argu-
ment classification.
2.3 Learning with Support Vector Machines
Given a vector space in <n and a set of positive and
negative points, SVMs classify vectors according to
a separating hyperplane, H(~x) = ~w ? ~x + b = 0,
where ~w ? <n and b ? < are learned by applying
the Structural Risk Minimization principle (Vapnik,
1995).
To apply the SVM algorithm to Predicate Argu-
ment Classification, we need a function ? : F ? <n
to map our features space F = {f1, .., f|F|} and our
predicate/argument pair representation, Fp,a = Fz ,
into <n, such that:
Fz ? ?(Fz) = (?1(Fz), .., ?n(Fz))
From the kernel theory we have that:
H(~x) =
( ?
i=1..l
?i~xi
)
? ~x+ b =
?
i=1..l
?i~xi ? ~x+ b =
?
i=1..l
?i?(Fi) ? ?(Fz) + b.
where, Fi ?i ? {1, .., l} are the training instances
and the product KT (Fi, Fz) =<?(Fi) ? ?(Fz)> is
the kernel function associated with the mapping ?.
The simplest mapping that we can apply is
?(Fz) = ~z = (z1, ..., zn) where zi = 1 if fi ? Fz
and zi = 0 otherwise, i.e. the characteristic vector
of the set Fz with respect to F . If we choose the
scalar product as a kernel function we obtain the lin-
ear kernel KL(Fx, Fz) = ~x ? ~z.
Another function that has shown high ac-
curacy for the predicate argument classification
(Pradhan et al, 2004) is the polynomial kernel:
12
SNP VP
VP VPCC
VB NP
took DT NN
the book
and VB NP
read PRP$ NN
its title
PRP
John
S
NP VP
VP
VB NP
took
S
NP VP
VP
VB NP
read
Arg. 0 Arg. 0
Arg. 1 Arg. 1
Sentence Parse-Tree Ftook Fread
Figure 2: Subcategorization frame structure for two predicate
argument structures.
 
 
 
 
 
 
 
 
 
 
S 
NP VP 
VP 
VB NP 
took S 
NP VP 
VP 
VB NP 
took 
VP 
VB NP 
S 
NP VP 
VP 
VB NP S 
NP VP 
VP 
VB 
took 
VP 
VP 
VB NP 
took 
VP 
VP 
VB NP 
VP 
VP 
Figure 3: All 10 valid fragments of the SCFS associated with
the arguments of Ftook of Figure 2.
KPoly(Fx, Fz) = (c+ ~x ? ~z)d, where c is a constant
and d is the degree of the polynom.
The interesting property is that we do not need to
evaluate the ? function to compute the above vector;
only the K(~x, ~z) values are required. This allows
us to define efficient classifiers in a huge (possible
infinite) feature set, provided that the kernel is pro-
cessed in an efficient way. In the next section, we
introduce the convolution kernel that we used to rep-
resent subcategorization structures.
3 Subcategorization Frame Kernel (SK)
The convolution kernel that we have experimented
was devised in (Moschitti, 2004) and is character-
ized by two aspects: the semantic space of the sub-
categorization structures and the kernel function that
measure their similarities.
3.1 Subcategorization Frame Structure (SCFS)
We consider the predicate argument structures an-
notated in PropBank or FrameNet as our semantic
space. As we assume that semantic structures are
correlated to syntactic structures, we used a ker-
nel that selects semantic information according to
the syntactic structure of a predicate. The subparse
tree which describes the subcategorization frame of
the target verbal predicate defines the target Sub-
categorization Frame Structure (SCFS). For exam-
ple, Figure 2 shows the parse tree of the sentence
"John took the book and read its title" to-
gether with two SCFS structures, Ftook and Fread
associated with the two predicates took and read, re-
spectively. Note that SCFS includes also the external
argument (i.e. the subject) although some linguistic
theories do not consider it being part of the SCFs.
Once the semantic representation is defined, we
need to design a tree kernel function to estimate the
similarity between our objects.
3.2 The tree kernel function
The main idea of tree kernels is to model a
K(T1, T2) function which computes the number of
the common substructures between two trees T1 and
T2. For example, Figure 3 shows all the fragments
of the argument structure Ftook (see Figure 2) which
will be matched against the fragment of another
SCFS.
Given the set of fragments {f1, f2, ..} = F ex-
tracted from all SCFSs of the training set, we define
the indicator function Ii(n) which is equal 1 if the
target fi is rooted at node n and 0 otherwise. It fol-
lows that:
K(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2) (1)
where NT1 and NT2 are the sets of the T1?s
and T2?s nodes, respectively and ?(n1, n2) =?|F|
i=1 Ii(n1)Ii(n2). This latter is equal to the num-
ber of common fragments rooted in the n1 and n2
nodes. We can compute ? as follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminals symbols) then ?(n1, n2) =
1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) =
nc(n1)?
j=1
(1 + ?(cjn1 , cjn2)) (2)
where ? ? {0, 1}, nc(n1) is the number of the chil-
dren of n1 and cjn is the j-th child of the node n.
13
Note that, as the productions are the same nc(n1) =
nc(n2).
The above kernel has the drawback of assigning
higher weights to larger structures1. To overcome
this problem we can scale the relative importance of
the tree fragments using a parameter ? in the con-
ditions 2 and 3 as follows: ?(nx, nz) = ? and
?(nx, nz) = ?
?nc(nx)
j=1 (? +?(cjn1 , cjn2)).
The set of fragments that belongs to SCFs are
derived by human annotators according to seman-
tic considerations, thus they generate a semantic
subcategorization frame kernel (SK). We also
note that SK estimates the similarity between
two SCFSs by counting the number of fragments
that are in common. For example, in Figure 2,
KT (?(Ftook), ?(Fread)) is quite high (i.e. 6 out 10
substructures) as the two verbs have the same syn-
tactic realization.
In other words the fragments encode semantic in-
formation which is measured by SK. This provides
the argument classifiers with important clues about
the possible set of arguments suited for a target ver-
bal predicate. To support this hypothesis the next
section presents the experiments on the predicate ar-
gument type of FrameNet and ProbBank.
4 The Experiments
A clustering algorithm which uses SK would group
together verbs that show a similar syntactic struc-
ture. To study the properties of such clusters we ex-
perimented SK in combination with the traditional
kernel used for the predicate argument classification.
As the polynomial kernel with degree 3 was shown
to be the most accurate for the argument classifica-
tion (Pradhan et al, 2004; Moschitti, 2004) we use
it to build two kernel combinations:
? Poly + SK: KPoly|KPoly| + ?
KT
|KT | , i.e. the sum be-
tween the normalized polynomial kernel (see
Section 2.3) and the normalized SK2.
? Poly ? SK: KPoly?KT|KPoly |?|KT | , i.e. the normal-
ized product between the polynomial kernel
1With a similar aim and to have a similarity score between 0
and 1, we also apply the normalization in the kernel space, i.e.
K?(T1, T2) = K(T1,T2)?K(T1,T1)?K(T2,T2) .
2To normalize a kernel K(~x, ~z) we can divide it by?
K(~x, ~x)?K(~z, ~z).
and SK.
For the experiments we adopted two corpora
PropBank (PB) and FrameNet (FN). PB, avail-
able at www.cis.upenn.edu/?ace, is used along
with the Penn TreeBank 2 (www.cis.upenn.edu
/?treebank) (Marcus et al, 1993). This corpus
contains about 53,700 sentences and a fixed split be-
tween training and testing which has been used in
other researches, e.g. (Pradhan et al, 2004; Gildea
and Palmer, ). In this split, Sections from 02 to 21
are used for training, section 23 for testing and sec-
tions 1 and 22 as development set. We considered all
12 arguments from Arg0 to Arg9, ArgA and ArgM for
a total of 123,918 and 7,426 arguments in the train-
ing and test sets, respectively. It is worth noting that
in the experiments we used the gold standard parsing
from the Penn TreeBank, thus our kernel structures
are derived with high precision.
The second corpus was obtained by extract-
ing from FrameNet (www.icsi.berkeley.edu/
?framenet/) all 24,558 sentences from 40 frames
of the Senseval 3 (http://www.senseval.org) Au-
tomatic Labeling of Semantic Role task. We con-
sidered 18 of the most frequent roles for a total of
37,948 arguments3. Only verbs are selected to be
predicates in our evaluations. Moreover, as there is
no fixed split between training and testing, we ran-
domly selected 30% of the sentences for testing and
30% for validation-set, respectively. Both training
and testing sentences were processed using Collins?
parser (Collins, 1997) to generate parse-tree auto-
matically. This means that our shallow semantic
parser for FrameNet is fully automated.
4.1 The Classification set-up
The evaluations were carried out with the SVM-
light-TK software (Moschitti, 2004) available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes the tree kernels in the SVM-light
software (Joachims, 1999).
The classification performance was measured us-
ing the F1 measure4 for the individual arguments
and the accuracy for the final multi-class classifier.
This latter choice allows us to compare the results
3We mapped together roles having the same name
4F1 assigns equal importance to Precision P and Recall R,
i.e. F1 = 2P?RP+R .
14
with previous literature works, e.g. (Gildea and
Jurasfky, 2002; Pradhan et al, 2004; Gildea and
Palmer, ).
For the evaluation of SVMs, we used the default
regularization parameter (e.g., C = 1 for normal-
ized kernels) and we tried a few cost-factor values
(i.e., j ? {1, 2, 3, 5, 7, 10, 100}) to adjust the rate
between Precision and Recall. We chose the pa-
rameters by evaluating the SVMs using the KPoly
kernel (degree = 3) over the validation-set. Both ?
(see Section 3.2) and ? parameters were evaluated
in a similar way by maximizing the performance of
SVM using Poly+SK. We found that the best values
were 0.4 and 0.3, respectively.
4.2 Comparative results
To study the impact of the subcategorization frame
kernel we experimented the three models Poly,
Poly + SK and Poly ? SK on different training
conditions.
First, we run the above models using all the verbal
predicates available in the training and test sets. Ta-
bles 1 and 2 report the F1 measure and the global
accuracy for PB and FN, respectively. Column 2
shows the accuracy of Poly (90.5%) which is sub-
stantially equal to the accuracy obtained in (Prad-
han et al, 2004) on the same training and test sets
with the same SVM model. Columns 3 and 4
show that the kernel combinations Poly + SK and
Poly ? SK remarkably improve Poly accuracy,
i.e. 2.7% (93.2% vs. 90.5%) whereas on FN only
Poly+SK produces a small accuracy increase, i.e.
0.7% (86.2% vs. 85.5%).
This outcome is lower since the FN classification
requires dealing with a higher variability of its se-
mantic roles. For example, in ProbBank most of the
time, the PB Arg0 and Arg1 corresponds to the log-
ical subject and logical direct object, respectively.
On the contrary, the FN Cause and Agent roles are
often both associated with the logical subject and
share similar syntactic realizations, making SCFS
less effective to distinguish between them. More-
over, the training data available for FrameNet is
smaller than that used for PropBank, thus, the tree
kernel may not have enough examples to generalize,
correctly.
Second, we carried out other experiments using
a subset of the total verbs for training and another
Args All Verbs Disjoint Verbs
Poly +SK ?SK Poly +SK ?SK
Arg0 90.8 94.6 94.7 86.8 90.9 91.1
Arg1 91.1 92.9 94.1 81.7 86.8 88.3
Arg2 80.0 77.4 82.0 49.9 49.5 47.6
Arg3 57.9 56.2 56.4 20.3 22.9 20.6
Arg4 70.5 69.6 71.1 0 0 0
ArgM 95.4 96.1 96.3 90.3 93.4 93.7
Acc. 90.5 92.4 93.2 82.1 86.3 86.9
Table 1: Kernel accuracies on PropBank.
Role All Verbs Disjoint Verbs
Poly +SK ?SK Poly +SK ?SK
agent 91.7 94.4 94.0 82.5 84.8 84.7
cause 57.4 60.6 56.4 29.1 28.1 26.9
degree 77.1 77.2 60.9 40.6 44.6 22.6
depict. 85.8 86.2 85.9 73.6 74.0 71.2
instrum. 67.1 69.1 64.6 13.3 13.0 12.8
manner 80.5 79.7 77.7 74.8 74.3 72.3
Acc. 85.5 86.2 85.0 72.8 74.6 74.2
Table 2: Kernel accuracies on 18 FrameNet semantic roles.
disjoint subset for testing. In these conditions, the
impact of SK is amplified: on PB, SK ?Poly out-
performs Poly by 4.8% (86.9% vs. 82.1%), whereas,
on FN, SK increases Poly of about 2%, i.e. 74.6%
vs. 72.8%. These results suggest that (a) when test-
set verbs are not observed during training, the clas-
sification task is harder, e.g. 82.1% vs. 90.5% on
PB and (b) the syntactic structures of the verbs, i.e.
the SCFSs, allow the SVMs to better generalize on
unseen verbs.
To verify that the kernel representation is supe-
rior to the traditional representation we carried out
an experiment using a flat feature representation of
the SCFs, i.e. we used the syntactic frame feature
described (Xue and Palmer, 2004) in place of SK.
The result as well as other literature findings, e.g.
(Pradhan et al, 2004) show an improvement on PB
of about 0.7% only. Evidently flat features cannot
derive the same information of a convolution kernel.
Finally, to study how the verb complexity impacts
on the usefulness of SK, we carried out additional
experiments with different verb sets. One dimension
of complexity is the frequency of the verbs in the
target corpus. Infrequent verbs are associated with
predicate argument structures poorly represented in
the training set thus they are more difficult to clas-
sify. Another dimension of the verb complexity is
the number of different SCFs that they show in dif-
ferent contexts. Intuitively, the higher is the number
15
(a) PropBank Multi-classifier
0.83
0.85
0.87
0.89
0.91
0.93
0.95
0.97
1-5 6-10 11-15 16-30 31-60 61-100 101-
300
301-
400
401-
600
601-
700
701-
1000
>1000
Verb Frequency
A
cc
u
ra
cy
Poly
Poly+SK
(b) Arg0 Classifier
0.85
0.87
0.89
0.91
0.93
0.95
0.97
1-5 6-10 11-15 16-30 31-60 61-100 101-300 301-400 401-600 601-700 701-
1000
>1000
Verb Frequency
F1
Poly
Poly+SK
(c) PropBank Multi-classifier
0.80
0.82
0.84
0.86
0.88
0.90
0.92
0.94
1 2-3 4-5 6-10 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 >50
# SCF Type
Ac
cu
ra
cy
Poly
Poly+SK
(d) Arg0 Classifier
0.80
0.82
0.84
0.86
0.88
0.90
0.92
0.94
1 2-3 4-5 6-10 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 >50
# SCF Type 
F1
Poly
Poly+SK
(e) FrameNet Multi-classifier
0.60
0.65
0.70
0.75
0.80
0.85
0.90
1 2-3 4-5 6-10 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 >50
# SCF Type
Ac
cu
ra
cy
Poly
Poly+SK
(f) Agent Classifier
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1 2-3 4-5 6-10 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 >50
# SCF Type 
F1
Poly
Poly+SK
Figure 4: The impact of SCF on the classification accuracy of the semantic arguments and semantic roles according to the verb
complexity.
of verb?s SCF types the more difficult is the classifi-
cation of its arguments.
Figure 4.a, reports the accuracy along with the
trend line plot of Poly and SK + Poly according
to subsets of different verb frequency. For example,
the label 1-5 refers to the class of verbal predicates
whose frequency ranges from 1 to 5. The associated
accuracy is evaluated on the portions of the training
and test-sets which contain only the verbs in such
class. We note that SK improves Poly for any verb
frequency. Such improvement decreases when the
frequency becomes very high, i.e. when there are
many training instances that can suggest the correct
classification. A similar behavior is shown in Figure
4.b where the F1 measure for Arg0 of PB is reported.
Figures 4.c and 4.d illustrate the accuracy and the
F1 measure for all arguments and Arg0 of PB ac-
cording to the number of SCF types, respectively.
We observe that the Semantic Kernel does not pro-
duce any improvement on the verbs which are syn-
tactically expressed by only one type of SCF. As the
number of SCF types increases (> 1) Poly + SK
outperforms Poly for any verb class, i.e. when the
verb is enough complex SK always produces use-
ful information independently of the number of the
training set instances. On the one hand, a high num-
ber of verb instances reduces the complexity of the
classification task. On the other hand, as the num-
ber of verb type increases the complexity of the task
increases as well.
A similar behavior can be noted on the FN data
(Figure 4.e) even if the not so strict correlation be-
tween syntax and semantics prevents SK to produce
high improvements. Figure 4.f shows the impact of
SK on the Agent role. We note that, the F1 increases
more than the global accuracy (Figure 4.e) as the
Agent most of the time corresponds to Arg0. This is
confirmed by the Table 2 which shows an improve-
ment for the Agent of up to 2% when SK is used
along with the polynomial kernel.
5 Conclusive Remarks
In this article, we used Support Vector Machines
(SVMs) to deeply analyze the role of the subcat-
egorization frame kernel (SK) in the automatic
predicate argument classification of PropBank and
16
FrameNet corpora. To study the SK?s verb clas-
sification properties we have combined it with the
polynomial kernel on standard flat features.
We run the SVMs on diverse levels of task com-
plexity. The results show that: (1) in general SK
remarkably improves the classification accuracy. (2)
When there are no training instances of the test-
set verbs the improvement of SK is almost double.
This suggests that tree kernels automatically derive
features which support also a sort of back-off esti-
mation in case of unseen verbs. (3) In all complexity
conditions the structural features are in general very
robust, maintaining a high improvement over the ba-
sic accuracy. (4) The semantic role classification in
FrameNet is affected with more noisy data as it is
based on the output of a statistical parser. As a con-
sequence the improvement is lower. Anyway, the
systematic superiority of SK suggests that it is less
sensible to parsing errors than other models. This
opens promising direction for a more weakly super-
vised application of the statistical semantic tagging
supported by SK.
In summary, the extensive experimentation has
shown that the SK provides information robust with
respect to the complexity of the task, i.e. verbs with
richer syntactic structures and sparse training data.
An important observation on the use of tree ker-
nels has been pointed out in (Cumby and Roth,
2003). Both computational efficiency and classifi-
cation accuracy can often be superior if we select
the most informative tree fragments and carry out
the learning in the feature space. Nevertheless, the
case studied in this paper is well suited for using ker-
nels as: (1) it is difficult to guess which fragment
from an SCF should be retained and which should
be discarded, (2) it may be the case that all frag-
ments are useful as SCFs are small structures and all
theirs substructures may serve as different back-off
levels and (3) small structures do not heavily penal-
ize efficiency.
Future research may be addressed to (a) the use
of SK kernel to explicitly generate verb clusters and
(b) the use of convolution kernels to study other lin-
guistic phenomena: we can use tree kernels to in-
vestigate which syntactic features are suited for an
unknown phenomenon.
Acknowledgement
We would like to thank the anonymous reviewers for
their competence and commitment showed in the re-
view of this paper.
References
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
ACL?97,Somerset, New Jersey.
Chad Cumby and Dan Roth. 2003. Kernel methods for
relational learning. In Proceedings of ICML?03, Wash-
ington, DC, USA.
Charles J. Fillmore. 1982. Frame semantics. In Linguis-
tics in the Morning Calm, pages 111?137.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of EMNLP?03, Sapporo, Japan.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
Daniel Gildea and Martha Palmer. The necessity of pars-
ing for predicate argument recognition. In Proceed-
ings of ACL?02.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of LREC?02.
Anna Korhonen. 2003. Subcategorization Acquisi-
tion. Ph.D. thesis, Techical Report UCAM-CL-TR-
530. Computer Laboratory, University of Cambridge.
Beth Levin. 1993. English Verb Classes and Alterna-
tions A Preliminary Investigation. Chicago: Univer-
sity of Chicago Press.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The Penn Treebank. Computational Linguistics,
19:313?330.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In proceedings
ACL?04, Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. to appear in the Machine Learning Journal.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP?04,
Barcelona, Spain, July.
17
Tree Kernel Engineering in Semantic Role Labeling Systems
Alessandro Moschitti and Daniele Pighin and Roberto Basili
University of Rome, Tor Vergata
{moschitti,basili}@info.uniroma2.it
daniele.pighin@gmail.com
Abstract
Recent work on the design of automatic
systems for semantic role labeling has
shown that feature engineering is a com-
plex task from a modeling and implemen-
tation point of view. Tree kernels alleviate
such complexity as kernel functions gener-
ate features automatically and require less
software development for data extraction.
In this paper, we study several tree kernel
approaches for both boundary detection
and argument classification. The compar-
ative experiments on Support Vector Ma-
chines with such kernels on the CoNLL
2005 dataset show that very simple tree
manipulations trigger automatic feature
engineering that highly improves accuracy
and efficiency in both phases. Moreover,
the use of different classifiers for internal
and pre-terminal nodes maintains the same
accuracy and highly improves efficiency.
1 Introduction
A lot of attention has been recently devoted to
the design of systems for the automatic label-
ing of semantic roles (SRL) as defined in two
important projects: FrameNet (Johnson and Fill-
more, 2000), inspired by Frame Semantics, and
PropBank (Kingsbury and Palmer, 2002) based
on Levin?s verb classes. In general, given a sen-
tence in natural language, the annotation of a pred-
icate?s semantic roles requires (1) the detection of
the target word that embodies the predicate and
(2) the detection and classification of the word se-
quences constituting the predicate?s arguments. In
particular, step (2) can be divided into two differ-
ent phases: (a) boundary detection, in which the
words of the sequence are detected and (b) argu-
ment classification, in which the type of the argu-
ment is selected.
Most machine learning models adopted for the
SRL task have shown that (shallow or deep) syn-
tactic information is necessary to achieve a good
labeling accuracy. This research brings a wide
empirical evidence in favor of the linking theories
between semantics and syntax, e.g. (Jackendoff,
1990). However, as no theory provides a sound
and complete treatment of such issue, the choice
and design of syntactic features for the automatic
learning of semantic structures requires remark-
able research efforts and intuition.
For example, the earlier studies concerning lin-
guistic features suitable for semantic role labeling
were carried out in (Gildea and Jurasfky, 2002).
Since then, researchers have proposed diverse syn-
tactic feature sets that only slightly enhance the
previous ones, e.g. (Xue and Palmer, 2004) or
(Carreras and Ma`rquez, 2005). A careful analy-
sis of such features reveals that most of them are
syntactic tree fragments of training sentences, thus
a natural way to represent them is the adoption of
tree kernels as described in (Moschitti, 2004). The
idea is to associate with each argument the mini-
mal subtree that includes the target predicate with
one of its arguments, and to use a tree kernel func-
tion to evaluate the number of common substruc-
tures between two such trees. Such approach is in
line with current research on the use of tree kernels
for natural language learning, e.g. syntactic pars-
ing re-ranking (Collins and Duffy, 2002), relation
extraction (Zelenko et al, 2003) and named entity
recognition (Cumby and Roth, 2003; Culotta and
Sorensen, 2004).
Regarding the use of tree kernels for SRL, in
(Moschitti, 2004) two main drawbacks have been
49
pointed out:
? Highly accurate boundary detection cannot
be carried out by a tree kernel model since
correct and incorrect arguments may share a
large portion of the encoding trees, i.e. they
may share many substructures.
? Manually derived features (extended with a
polynomial kernel) have been shown to be su-
perior to tree kernel approaches.
Nevertheless, we believe that modeling a com-
pletely kernelized SRL system is useful for the fol-
lowing reasons:
? We can implement it very quickly as the fea-
ture extractor module only requires the writ-
ing of the subtree extraction procedure. Tra-
ditional SRL systems are, in contrast, based
on the extraction of more than thirty features
(Pradhan et al, 2005), which require the writ-
ing of at least thirty different procedures.
? Combining it with a traditional attribute-
value SRL system allows us to obtain a more
accurate system. Usually the combination of
two traditional systems (based on the same
machine learning model) does not result in
an improvement as their features are more
or less equivalent as shown in (Carreras and
Ma`rquez, 2005).
? The study of the effective structural features
can inspire the design of novel linear fea-
tures which can be used with a more efficient
model (i.e. linear SVMs).
In this paper, we carry out tree kernel engineer-
ing (Moschitti et al, 2005) to increase both ac-
curacy and speed of the boundary detection and
argument classification phases. The engineering
approach relates to marking the nodes of the en-
coding subtrees in order to generate substructures
more strictly correlated with a particular argu-
ment, boundary or predicate. For example, mark-
ing the node that exactly covers the target ar-
gument helps tree kernels to generate different
substructures for correct and incorrect argument
boundaries.
The other technique that we applied to engineer
different kernels is the subdivision of internal and
pre-terminal nodes. We show that designing dif-
ferent classifiers for these two different node types
slightly increases the accuracy and remarkably de-
creases the learning and classification time.
An extensive experimentation of our tree ker-
nels with Support Vector Machines on the CoNLL
2005 data set provides interesting insights on the
design of performant SRL systems entirely based
on tree kernels.
In the remainder of this paper, Section 2 intro-
duces basic notions on SRL systems and tree ker-
nels. Section 3 illustrates our new kernels for both
boundary and classification tasks. Section 4 shows
the experiments of SVMs with the above tree ker-
nel based classifiers.
2 Preliminary Concepts
In this section we briefly define the SRL model
that we intend to design and the kernel function
that we use to evaluate the similarity between sub-
trees.
2.1 Basic SRL approach
The SRL approach that we adopt is based on the
deep syntactic parse (Charniak, 2000) of the sen-
tence that we intend to annotate semantically. The
standard algorithm is to classify the tree node pair
?p, a?, where p and a are the nodes that exactly
cover the target predicate and a potential argu-
ment, respectively. If ?p, a? is labeled with an ar-
gument, then the terminal nodes dominated by a
will be considered as the words constituting such
argument. The number of pairs for each sentence
can be hundreds, thus, if we consider training cor-
pora of thousands of sentences, we have to deal
with millions of training instances.
The usual solution to limit such complexity is to
divide the labeling task in two subtasks:
? Boundary detection, in which a single clas-
sifier is trained on many instances to detect
if a node is an argument or not, i.e. if the
sequence of words dominated by the target
node constitutes a correct boundary.
? Argument classification: only the set of
nodes corresponding to correct boundaries
are considered. These can be used to train a
multiclassifier that, for such nodes, only de-
cides the type of the argument. For example,
we can train n classifiers in the style One-vs-
All. At classification time, for each argument
node, we can select the argument type asso-
ciated with the maximum among the n scores
provided by the single classifiers.
50
We adopt this solution as it enables us to use
only one computationally expensive classifier, i.e.
the boundary detection one. This, as well as the
argument classifiers, requires a feature represen-
tation of the predicate-argument pair. Such fea-
tures are mainly extracted from the parse trees of
the target sentence, e.g. Phrase Type, Predicate
Word, Head Word, Governing Category, Position
and Voice proposed in (Gildea and Jurasfky, 2002).
As most of the features proposed in literature
are subsumed by tree fragments, tree-kernel func-
tions are a natural way to produce them automati-
cally.
2.2 Tree kernel functions
Tree-kernel functions simply evaluate the number
of substructures shared between two trees T1 and
T2. Such functions can be seen as a scalar product
in the huge vector space constituted by all possi-
ble substructures of the training set. Thus, kernel
functions implicitly define a large feature space.
Formally, given a tree fragment space
{f1, f2, ..} = F , we can define an indica-
tor function Ii(n), which is equal to 1 if the
target fi is rooted at node n and equal to
0 otherwise. Therefore, a tree-kernel func-
tion K over T1 and T2 can be defined as
K(T1, T2) =
?
n1?NT1
?
n2?NT2 ?(n1, n2),
where NT1 and NT2 are the sets of the
T1?s and T2?s nodes, respectively and
?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2). This latter
is equal to the number of common fragments
rooted at nodes n1 and n2 and, according to
(Collins and Duffy, 2002), it can be computed as
follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the
same, and n1 and n2 have only leaf chil-
dren (i.e. they are pre-terminal symbols) then
?(n1, n2) = ?;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminal then
?(n1, n2) = ?
?nc(n1)
j=1 (1 + ?(cjn1 , cjn2)).
where ? is the decay factor to scale down the im-
pact of large structures, nc(n1) is the number of
the children of n1 and cjn is the j-th child of the
node n. Note that, as the productions are the same,
nc(n1) = nc(n2). Additionally, to map similar-
ity scores in the [0,1] range, we applied a nor-
Figure 1: The PAF subtree associated with A1.
Figure 2: Example of CMST.
malization in the kernel space, i.e. K ?(T1, T2) =
K(T1,T2)?
K(T1,T1)?K(T2,T2)
.
Once a kernel function is defined, we need to
characterize the predicate-argument pair with a
subtree. This allows kernel machines to generate a
large number of syntactic features related to such
pair. The approach proposed in (Moschitti, 2004)
selects the minimal subtree that includes a predi-
cate with its argument. We follow such approach
by studying and proposing novel, interesting solu-
tions.
3 Novel Kernels for SRL
The basic structure used to characterize the predi-
cate argument relation is the smallest subtree that
includes a predicate with one of its argument. For
example, in Figure 1, the dashed line encloses a
predicate argument feature (PAF) over the parse
tree of the sentence: ?Paul delivers a talk in for-
mal style?. This PAF is a subtree that characterizes
the predicate to deliver with its argument a talk.
In this section, we improve PAFs, propose dif-
ferent kernels for internal and pre-terminal nodes
and new kernels based on complete predicate ar-
51
Figure 3: Differences between PAF (a) and MPAF (b) structures.
gument structures.
3.1 Improving PAF
PAFs have shown to be very effective for argu-
ment classification but not for boundary detection.
The reason is that two nodes that encode correct
and incorrect boundaries may generate very sim-
ilar PAFs. For example, Figure 3.A shows two
PAFs corresponding to a correct (PAF+) and an
incorrect (PAF-) choice of the boundary for A1:
PAF+ from the NP vs. PAF- from the N nodes. The
number of their common substructures is high, i.e.
the four subtrees shown in Frame C. This prevents
the algorithm from making different decisions for
such cases.
To solve this problem, we specify which is the
node that exactly covers the argument (also called
argument node) by simply marking it with the la-
bel B denoting the boundary property. Figure 3.B
shows the two new marked PAFs (MPAFs). The
features generated from the two subtrees are now
very different so that there is only one substructure
in common (see Frame D). Note that, each markup
strategy impacts on the output of a kernel function
in terms of the number of structures common to
two trees. The same output can be obtained us-
ing unmarked trees and redefining consistently the
kernel function, e.g. the algorithm described in
Section 2.2.
An alternative way to partially solve the struc-
ture overlapping problem is the use of two differ-
ent classifiers, one for the internal nodes and one
for the pre-terminal nodes, and combining their
decisions. In this way, the negative example of
Figure 3 would not be used to train the same clas-
sifier that uses PAF+. Of course, similar structures
can both be rooted on internal nodes, therefore
they can belong to the training data of the same
classifier. However, the use of different classi-
fiers is motivated also by the fact that many ar-
gument types can be found mostly in pre-terminal
nodes, e.g. modifier or negation arguments, and
do not necessitate training data extracted from in-
ternal nodes. Consequently, it is more convenient
(at least from a computational point of view) to
use two different boundary classifiers, hereinafter
referred to as combined classifier.
3.2 Kernels on complete predicate argument
structures
The type of a target argument strongly depends on
the type and number of the predicate?s arguments1
(Punyakanok et al, 2005; Toutanova et al, 2005).
Consequently, to correctly label an argument, we
should extract features from the complete predi-
cate argument structure it belongs to. In contrast,
PAFs completely neglect the information (i.e. the
tree portions) related to non-target arguments.
One way to use this further information with
tree kernels is to use the minimum subtree that
spans all the predicate?s arguments. The whole
parse tree in Figure 1 is an example of such Min-
imum Spanning Tree (MST) as it includes all and
only the argument structures of the predicate ?to
deliver?. However, MSTs pose some problems:
? We cannot use them for the boundary detec-
tion task since we do not know the predi-
cate?s argument structure yet. However, we
can derive the MST (its approximation) from
the nodes selected by a boundary classifier,
i.e. the nodes that correspond to potential ar-
guments. Such approximated MSTs can be
easily used in the argument type classifica-
tion phase. They can also be used to re-rank
the most probable m sequences of arguments
for both labeling phases.
? Obviously, an MST is the same for all the
arguments it includes, thus we need a way
to differentiate it for each target argument.
1This is true at least for core arguments.
52
Again, we can mark the node that exactly
covers the target argument as shown in the
previous section. We refer to this subtree as
marked MST (MMST). However, for large
arguments (i.e. spread on a large part of the
sentence tree) the substructures? likelihood of
being part of other arguments is quite high.
To address this latter problem, we can mark all
nodes that descend from the target argument node.
Figure 2 shows a MST in which the subtree as-
sociated with the target argument (AM) has the
nodes marked. We refer to this structure as a
completely marked MST (CMST). CMSTs may
be seen as PAFs enriched with new information
coming from the other arguments (i.e. the non-
marked subtrees). Note that if we consider only
the PAF subtree from a CMST we obtain a differ-
ently marked subtree which we refer to as CPAF.
In the next section we study the impact of the
proposed kernels on the boundary detection and
argument classification performance.
4 Experiments
In these experiments we evaluate the impact of our
proposed kernels in terms of accuracy and effi-
ciency. The accuracy improvement confirms that
the node marking approach enables the automatic
engineering of effective SRL features. The effi-
ciency improvement depends on (a) the less train-
ing data used when applying two distinct type clas-
sifiers for internal and pre-terminal nodes and (b) a
more adequate feature space which allows SVMs
to converge faster to a model containing a smaller
number of support vectors, i.e. faster training and
classification.
4.1 Experimental set up
The empirical evaluations were carried out within
the setting defined in the CoNLL-2005 Shared
Task (Carreras and Ma`rquez, 2005). We
used as a target dataset the PropBank corpus
available at www.cis.upenn.edu/?ace, along
with the Penn TreeBank 2 for the gold trees
(www.cis.upenn.edu/?treebank) (Marcus et al,
1993), which includes about 53,700 sentences.
Since the aim of this study was to design a real
SRL system we adopted the Charniak parse trees
from the CoNLL 2005 Shared Task data (available
at www.lsi.upc.edu/?srlconll/).
We used Section 02, 03 and 24 from the Penn
TreeBank in most of the experiments. Their char-
acteristics are shown in Table 1. Pos and Neg in-
dicate the number of nodes corresponding or not
to a correct argument boundary. Rows 3 and 4 re-
port such number for the internal and pre-terminal
nodes separately. We note that the latter are much
fewer than the former; this results in a very fast
pre-terminal classifier.
As the automatic parse trees contain errors,
some arguments cannot be associated with any
covering node. This prevents us to extract a tree
representation for them. Consequently, we do not
consider them in our evaluation. In sections 2, 3
and 24 there are 454, 347 and 731 such cases, re-
spectively.
The experiments were carried out with
the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes fast tree kernel evaluation (Mos-
chitti, 2006) in the SVM-light software (Joachims,
1999). We used a regularization parameter (option
-c) equal to 1 and ? = 0.4 (see (Moschitti,
2004)).
4.2 Boundary Detection Results
In these experiments, we used Section 02 for train-
ing and Section 24 for testing. The results using
the PAF and the MPAF based kernels are reported
in Table 2 in rows 2 and 3, respectively. Columns
3 and 4 show the CPU testing time (in seconds)
and the F1 of the monolithic boundary classifier.
The next 3 columns show the CPU time for the in-
ternal (Int) and pre-terminal (Pre) node classifiers,
as well as their total (All). The F1 measures are
reported in the 3 rightmost columns. In particular,
the third column refers to the F1 of the combined
classifier. This has been computed by summing
correct, incorrect and not retrieved examples of the
two distinct classifiers.
We note that: first, the monolithic classifier ap-
plied to MPAF improves both the efficiency, i.e.
about 3,131 seconds vs. 5,179, of PAF and the
F1, i.e. 82.07 vs. 75.24. This suggests that mark-
ing the argument node simplifies the generaliza-
tion process.
Second, by dividing the boundary classifica-
tion in two tasks, internal and pre-terminal nodes,
we furthermore improve the classification time for
both PAF and MPAF kernels, i.e. 5,179 vs. 1,851
(PAF) and 3,131 vs. 1,471 (MPAF). The sepa-
rated classifiers are much faster, especially the pre-
terminal one (about 61 seconds to classify 81,075
nodes).
53
Section 2 Section 3 Section 24
Nodes pos neg tot pos neg tot pos neg tot
Internal 11,847 71,126 82,973 6,403 53,591 59,994 7,525 50,123 57,648
Pre-terminal 894 114,052 114,946 620 86,232 86,852 709 80,366 81,075
Both 12,741 185,178 197,919 7,023 139,823 146,846 8,234 130,489 138,723
Table 1: Tree nodes of the sentences from sections 2, 3 and 24 of the PropBank. pos and neg are the
nodes that exactly cover arguments and all the other nodes, respectively.
Monolithic Combined
Tagging strategy CPUtime F1 CPUtime F1Int Pre All Int Pre All
PAF 5,179.18 75.24 1,794.92 56.72 1,851.64 79.93 79.39 79.89
MPAF 3,131.56 82.07 1,410.10 60.99 1,471.09 82.20 79.14 81.96
Table 2: F1 comparison between PAF and MPAF based kernels using different classification strategies.
Int, Pre and ALL are the internal, pre-terminal and combined classifiers. The CPU time refers to the
classification time in seconds of all Section 24.
Figure 4: Learning curve comparison between the
PAF and MPAF F1 measures using the combined
classifier.
Third, the combined classifier approach seems
quite feasible as its F1 is almost equal to the mono-
lithic one (81.96 vs. 82.07) in case of MPAF and
even superior when using PAF (79.89 vs. 75.34).
This result confirms the observation given in Sec-
tion 3.1 about the importance of reducing the num-
ber of substructures common to PAFs associated
with correct and incorrect boundaries.
Finally, we trained the combined boundary clas-
sifiers with sets of increasing size to derive the
learning curves of the PAF and MPAF models.
To have more significant results, we increased the
training set by using also sections from 03 to 07.
Figure 4 shows that the MPAF approach is con-
stantly over the PAF. Consider also that the mark-
ing strategy has a lesser impact on the combined
classifier.
4.3 Argument Classification Results
In these experiments we tested different kernels
on the argument classification task. As some ar-
guments have a very small number of training in-
stances in a single section, we also used Section
03 for training and we continued to test on only
Section 24.
The results of the multiclassifiers on 59 argu-
ment types2 (e.g. constituted by 59 binary clas-
sifiers in the monolithic approach) are reported in
Table 3. The rows from 3 to 5 report the accuracy
when using the PAF,MPAF and CPAFwhereas the
rows from 6 to 8 show the accuracy for the com-
plete argument structure approaches, i.e. MST,
MMST and CMST.
More in detail, Column 2 shows the accuracy of
the monolithic multi-argument classifiers whereas
Columns 3, 4 and 5 report the accuracy of the in-
ternal, pre-terminal and combined multi-argument
classifiers, respectively.
We note that:
First, the two classifier approach does not im-
prove the monolithic approach accuracy. Indeed,
the subtrees describing different argument types
are quite different and this property holds also for
the pre-terminal nodes. However, we still mea-
sured a remarkable improvement in efficiency.
Second, MPAF is the best kernel. This con-
firms the outcome on boundary detection ex-
periments. The fact that it is more accu-
rate than CPAF reveals that we need to distin-
27 for the core arguments (A0...AA), 13 for the adjunct
arguments (AM-*), 19 for the argument references (R-*) and
20 for the continuations (C-*).
54
Monolithic CombinedTagging strategy Internal nodes Pre-terminals Overall
PAF 75.06 74.16 85.61 75.15
MPAF 77.17 76.25 85.76 77.07
CPAF 76.79 75.68 85.76 76.54
MST 34.80 36.52 78.14 40.10
MMST 72.55 71.59 86.32 72.86
CMST 73.21 71.93 86.32 73.17
Table 3: Accuracy produced by different tree kernels on argument classification. We trained on sections
02 and 03 and tested on Section 24.
guish the argument node from the other nodes.
To explain this, suppose that two argument
nodes, NP1 and NP2, dominate the follow-
ing structures: [NP1 [NP [DT NN]][PP]]
and [NP2 [DT NN]]. If we mark only the
argument node we obtain [NP-B [NP [DT
NN]][PP]] and [NP-B [DT NN]] which
have no structure in common. In contrast, if
we mark them completely, i.e. [NP-B [NP-B
[DT-B NN-B]][PP-B]] and [NP-B [DT-B
NN-B]], they will share the subtree [NP-B
[DT-B NN-B]]. Thus, although it may seem
counterintuitive, by marking only one node, we
obtain more specific substructures. Of course, if
we use different labels for the argument nodes and
their descendants, we obtain the same specializa-
tion effect.
Finally, if we do not mark the target argument
in the MSTs, we obtain a very low result (i.e.
40.10%) as expected. When we mark the cover-
ing node or the complete argument subtree we ob-
tain an acceptable accuracy. Unfortunately, such
accuracy is lower than the one produced by PAFs,
e.g. 73.17% vs. 77.07%, thus it may seem that
the additional information provided by the whole
argument structure is not effective. A more care-
ful analysis can be carried out by considering a
CMST as composed by a PAF and the rest of the
argument structure. We observe that some pieces
of information provided by a PAF are not deriv-
able by a CMST (or a MMST). For example, Fig-
ure 1 shows that the PAF contains the subtree [VP
[V NP]] while the associated CMST (see Figure
2) contains [VP [V NP PP]]. The latter struc-
ture is larger and more sparse and consequently,
the learning machine applied to CMSTs (or MM-
STs) performs a more difficult generalization task.
This problem is emphasized by our use of the ad-
juncts in the design of MSTs. As adjuncts tend to
be the same for many predicates they do not pro-
vide a very discriminative information.
5 Discussions and Conclusions
The design of automatic systems for the labeling
of semantic roles requires the solution of complex
problems. Among others, feature engineering is
made difficult by the structural nature of the data,
i.e. features should represent information con-
tained in automatic parse trees. This raises two
problems: (1) the modeling of effective features,
partially solved in the literature work and (2) the
implementation of the software for the extraction
of a large number of such features.
A system completely based on tree kernels al-
leviate both problems as (1) kernel functions au-
tomatically generate features and (2) only a pro-
cedure for subtree extraction is needed. Although
some of the manual designed features seem to be
superior to those derived with tree kernels, their
combination seems still worth applying.
In this paper, we have improved tree kernels
by studying different strategies: MPAF and the
combined classifier (for internal and pre-terminal
nodes) highly improve efficiency and accuracy in
both the boundary detection and argument classi-
fication tasks. In particular, MPAF improves the
old PAF-based tree kernel of about 8 absolute per-
cent points in the boundary classification task, and
when used along the combined classifier approach
the speed of the model increases of 3.5 times. In
case of argument classification the improvement is
less evident but still consistent, about 2%.
We have also studied tree representations based
on complete argument structures (MSTs). Our
preliminary results seem to suggest that additional
information extracted from other arguments is not
effective. However, such findings are affected by
two main problems: (1) We used adjuncts in the
tree representation. They are likely to add more
noise than useful information for the recognition
of the argument type. (2) The traditional PAF
contains subtrees that cannot be derived by the
55
MMSTs, thus we should combine these structures
rather than substituting one with the other.
In the future, we plan to extend this study as
follows:
First, our results are computed individually for
boundary and classification tasks. Moreover, in
our experiments, we removed arguments whose
PAF or MST could not be extracted due to errors
in parse trees. Thus, we provided only indicative
accuracy to compare the different tree kernels. A
final evaluation of the most promising structures
using the CoNLL 2005 evaluator should be carried
out to obtain a sound evaluation.
Second, as PAFs and MSTs should be com-
bined to generate more information, we are go-
ing to carry out a set of experiments that com-
bine different kernels associated with different
subtrees. Moreover, as shown in (Basili and Mos-
chitti, 2005; Moschitti, 2006), there are other tree
kernel functions that generate different fragment
types. The combination of such functions with the
marking strategies may provide more general and
effective kernels.
Third, once the final set of the most promising
kernels is established, we would like to use all the
available CoNLL 2005 data. This would allow us
to study the potentiality of our approach by exactly
comparing with literature work.
Next, our fast tree kernel function along with
the combined classification approach and the im-
proved tree representation make the learning and
classification much faster so that the overall run-
ning time is comparable with polynomial kernels.
However, when these latter are used with SVMs
the running time is prohibitive when very large
datasets (e.g. millions of instances) are targeted.
Exploiting tree kernel derived features in a more
efficient way is thus an interesting line of future
research.
Finally, as CoNLL 2005 has shown that the
most important contribution relates on re-ranking
predicate argument structures based on one single
tree (Toutanova et al, 2005) or several trees (Pun-
yakanok et al, 2005), we would like to use tree
kernels for the re-ranking task.
Acknowledgments
This research is partially supported by the Euro-
pean project, PrestoSpace (FP6-IST-507336).
References
Roberto Basili and Alessandro Moschitti. 2005. Automatic
Text Categorization: from Information Retrieval to Sup-
port Vector Learning. Aracne Press, Rome, Italy.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL?05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the NACL?00.
Michael Collins and Nigel Duffy. 2002. New ranking al-
gorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In ACL?02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. In Proceedings of ICML?03.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic
labeling of semantic roles. Computational Linguistic,
28(3):496?530.
R. Jackendoff. 1990. Semantic Structures, Current Studies in
Linguistics series. Cambridge, Massachusetts: The MIT
Press.
T. Joachims. 1999. Making large-scale SVM learning prac-
tical. In B. Scho?lkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods - Support Vector Learning.
Christopher R. Johnson and Charles J. Fillmore. 2000. The
framenet tagset for frame-semantic and syntactic coding
of predicate-argument structure. In In the Proceedings
ANLP-NAACL.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC?02.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn
Treebank. Computational Linguistics, 19:313?330.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of ACL?04,
Barcelona, Spain.
Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,
and Roberto Basili. 2005. Engineering of syntactic fea-
tures for shallow semantic parsing. In of the ACL05 Work-
shop on Feature Engineering for Machine Learning in
Natural Language Processing, USA.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL?06,
Trento, Italy.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005. Sup-
port vector learning for semantic argument classification.
Machine Learning Journal.
V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity of
syntactic parsing for semantic role labeling. In Proceed-
ings of IJCAI?05.
Kristina Toutanova, Aria Haghighi, and Christopher Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL?05.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP
2004.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel methods for relation extraction. Journal of Machine
Learning Research.
56
Towards Free-text Semantic Parsing: A Unified Framework Based on  
FrameNet, VerbNet and PropBank 
Ana-Maria Giuglea and Alessandro Moschitti 
University of Rome ?Tor Vergata?,  
Rome, Italy 
ana-maria.giuglea@topex.ro  
moschitti@info.uniroma2.it 
Abstract 
This article describes a robust semantic 
parser that uses a broad knowledge base 
created by interconnecting three major 
resources: FrameNet, VerbNet and 
PropBank. The FrameNet corpus con-
tains the examples annotated with se-
mantic roles whereas the VerbNet lexi-
con provides the knowledge about the 
syntactic behavior of the verbs. We 
connect VerbNet and FrameNet by 
mapping the FrameNet frames to the 
VerbNet Intersective Levin classes. The 
PropBank corpus, which is tightly con-
nected to the VerbNet lexicon, is used to 
increase the verb coverage and also to 
test the effectiveness of our approach. 
The results indicate that our model is an 
interesting step towards the design of 
free-text semantic parsers. 
1 Introduction 
During the last years a noticeable effort has been 
devoted to the design of lexical resources that 
can provide the training ground for automatic 
semantic role labelers. Unfortunately, most of the 
systems developed until now are confined to the 
scope of the resource that they use during the 
learning stage.  A very recent example in this 
sense was provided by the CONLL 2005 Shared 
Task on PropBank (Kingsbury and Palmer, 
2002) role labeling (Carreras and M?rquez, 
2005). While the best F-measure recorded on a 
test set selected from the training corpus (WSJ) 
was 80%, on the Brown corpus, the F-measure 
dropped below 70%. The most significant causes 
for this performance decay were highly ambigu-
ous and unseen predicates (i.e. predicates that do 
not have training examples, unseen in the train-
ing set). 
On the FrameNet (Johnson et al, 2003) role 
labeling task, the Senseval-3 competition (Lit-
kowski, 2004) registered similar results (~80%) 
by using the gold frame information as a given 
feature. No tests were performed outside Frame-
Net. In this paper, we show that when the frame 
feature is not used, the performance decay on 
different corpora reaches 30 points. Thus, the 
context knowledge provided by the frame is very 
important and a free-text semantic parser using 
FrameNet roles depends on the accurate auto-
matic detection of this information.  
In order to test the feasibility of such a task, 
we have trained an SVM (Support Vector Ma-
chine) Tree Kernel model for the automatic ac-
quisition of the frame information. Although Fra-
meNet contains three types of predicates (nouns, 
adjectives and verbs), we concentrated on the 
verb predicates and the roles associated with 
them. Therefore, we considered only the frames 
that have at least one verb lexical unit. Our 
experiments show that given a FrameNet 
predicate-argument structure, the task of identi-
fying the originating frame can be performed 
with very good results when the verb predicates 
have enough training examples, but becomes 
very challenging otherwise. The predicates not 
yet included in FrameNet and the predicates be-
longing to new application domains (that require 
new frames) are especially problematic as for 
them there is no available training data.  
We have thus studied new means of captur-
ing the semantic context, other than the frame, 
which can be easily annotated on FrameNet and 
are available on a larger scale (i.e. have a better 
coverage). A very good candidate seems to be 
the Intersective Levin classes (Dang et al, 1998) 
that can be found as well in other predicate re-
sources like PropBank and VerbNet (Kipper et 
al., 2000).  Thus, we have designed a semi-
automatic algorithm for assigning an Intersective 
Levin class to each FrameNet verb predicate. 
78
The algorithm creates a mapping between Fra-
meNet frames and the Intersective Levin classes. 
By doing that we could connect FrameNet to 
VerbNet and PropBank and obtain an increased 
training set for the Intersective Levin class. This 
leads to better verb coverage and a more robust 
semantic parser. The newly created knowledge 
base allows us to surpass the shortcomings that 
arise when FrameNet, VerbNet and PropBank 
are used separately while, at the same time, we 
benefit from the extensive research involving 
each of them (Pradhan et al, 2004; Gildea and 
Jurafsky, 2002; Moschitti, 2004). 
We mention that there are 3,672 distinct 
verb senses1 in PropBank and 2,351 distinct verb 
senses in FrameNet. Only 501 verb senses are in 
common between the two corpora which mean 
13.64% of PropBank and 21.31% of FrameNet. 
Thus, by training an Intersective Levin class 
classifier on both PropBank and FrameNet we 
extend the number of available verb senses to 
5,522.
In the remainder of this paper, Section 2 
summarizes previous work done on FrameNet 
automatic role detection. It also explains in more 
detail why models based exclusively on this cor-
pus are not suitable for free-text parsing. Section 
3 focuses on VerbNet and PropBank and how 
they can enhance the robustness of our semantic 
parser. Section 4 describes the mapping between 
frames and Intersective Levin classes whereas 
Section 5 presents the experiments that support 
our thesis. Finally, Section 6 summarizes the 
conclusions. 
2 Automatic semantic role detection on 
FrameNet 
One of the goals of the FrameNet project is to 
design a linguistic ontology that can be used for 
automatic processing of semantic information. 
This hierarchy contains an extensive semantic 
analysis of verbs, nouns, adjectives and situa-
tions in which they are used, called frames. The 
basic assumption on which the frames are built is 
that each word evokes a particular situation with 
specific participants (Fillmore, 1968). The situa-
tions can be fairly simple depicting the entities 
involved and the roles they play or can be very 
complex and in this case they are called scenar-
ios. The word that evokes a particular frame is 
called target word or predicate and can be an 
                                                
1 A verb sense is an Intersective Levin class in which 
the verb is listed. 
adjective, noun or verb. The participant entities 
are defined using semantic roles and they are 
called frame elements.
Several models have been developed for the 
automatic detection of the frame elements based 
on the FrameNet corpus (Gildea and Jurafsky, 
2002; Thompson et al, 2003; Litkowski, 2004). 
While the algorithms used vary, almost all the 
previous studies divide the task into 1) the identi-
fication of the verb arguments to be labeled and 
2) the tagging of each argument with a role. 
Also, most of the models agree on the core fea-
tures as being: Predicate, Headword, Phrase 
Type, Governing Category, Position, Voice and 
Path. These are the initial features adopted by 
Gildea and Jurafsky (2002) (henceforth G&J) for 
both frame element identification and role classi-
fication.  
A difference among the previous machine-
learning models is whether the frame information 
was used as gold feature. Of particular interest 
for us is the impact of the frame over unseen 
predicates and unseen words in general.  The 
results obtained by G&J are relevant in this 
sense; especially, the experiment that uses the 
frame to generalize from predicates seen in the 
training data to other predicates (i.e. when no 
data is available for a target word, G&J use data 
from the corresponding frame). The overall per-
formance induced by the frame usage increased. 
Other studies suggest that the frame is cru-
cial when trying to eliminate the major sources 
of errors. In their error analysis, (Thompson et 
al., 2003) pinpoints that the verb arguments with 
headwords that are ?rare? in a particular frame 
but not rare over the whole corpus are especially 
hard to classify. For these cases the frame is very 
important because it provides the context infor-
mation needed to distinguish between different 
word senses. 
Overall, the experiments presented in G&J?s 
study correlated with the results obtained in the 
Senseval-3 competition show that the frame fea-
ture increases the performance and decreases the 
amount of annotated examples needed in training 
(i.e. frame usage improves the generalization 
ability of the learning algorithm). On the other 
hand the results obtained without the frame in-
formation are very poor.  
This behavior suggests that predicates in the 
same frame behave similarly in terms of their 
argument structure and that they differ with re-
spect to other frames. From this perspective, hav-
ing a broader verb knowledge base becomes of 
major importance for free-text semantic parsing. 
79
Unfortunately, the 321 frames that contain at 
least one verb predicate cover only a small frac-
tion of the English verb lexicon and of possible 
domains. Also from these 321 frames only 100 
were considered to have enough training data 
and were used in Senseval-3 (see Litkowski, 
2004 for more details). 
Our approach for solving such problems in-
volves the usage of a frame-like feature, namely 
the Intersective Levin class. We show that the 
Levin class is similar in many aspects to the 
frame and can replace it with almost no loss in 
performance. At the same time, Levin class pro-
vides better coverage as it can be learned also 
from other corpora (i.e. PropBank). We annotate 
FrameNet with Intersective Levin classes by us-
ing a mapping algorithm that exploits current 
theories of linking. Our extensive experimenta-
tion shows the validity of our technique and its 
effectiveness on corpora different from Frame-
Net. The next section provides the theoretical 
support for the unified usage of FrameNet, 
VerbNet and PropBank, explaining why and how 
is possible to link them. 
3 Linking FrameNet to VerbNet and 
PropBank 
In general, predicates belonging to the same 
FrameNet frame have a coherent syntactic be-
havior that is also different from predicates per-
taining to other frames (G&J). This finding is 
consistent with theories of linking that claim that 
the syntactic behavior of a verb can be predicted 
from its semantics (Levin 1993, Levin and Rap-
paport Hovav, 1996). This insight determined us 
to study the impact of using a feature based on 
Intersective Levin classes instead of the frame 
feature when classifying FrameNet semantic 
roles. The main advantage of using Levin classes 
comes from the fact that other resources like 
PropBank and the VerbNet lexicon contain this 
kind of information. Thus, we can train a Levin 
class classifier also on the PropBank corpus, 
considerably increasing the verb knowledge base 
at our disposal. Another advantage derives from 
the syntactic criteria that were applied in defin-
ing the Levin clusters. As shown later in this ar-
ticle, the syntactic nature of these classes makes 
them easier to classify than frames, when using 
only syntactic and lexical features. 
More precisely, the Levin clusters are 
formed according to diathesis alternation criteria 
which are variations in the way verbal arguments 
are grammatically expressed when a specific se-
mantic phenomenon arises. For example, two 
different types of diathesis alternations are the 
following: 
(a) Middle Alternation
[Subject, Agent The butcher] cuts [Direct Object, Patient the meat]. 
[Subject, Patient The meat] cuts easily.
(b) Causative/inchoative Alternation
[Subject, Agent Janet] broke [Direct Object, Patient the cup]. 
[Subject, Patient The cup] broke.
In both cases, what is alternating is the 
grammatical function that the Patient role takes 
when changing from the transitive use of the 
verb to the intransitive one. The semantic phe-
nomenon accompanying these types of alterna-
tions is the change of focus from the entity per-
forming the action to the theme of the event.  
Levin documented 79 alternations which 
constitute the building blocks for the verb 
classes. Although alternations are chosen as the 
primary means for identifying the classes, addi-
tional properties related to subcategorization, 
morphology and extended meanings of verbs are 
taken into account as well. Thus, from a syntactic 
point of view, the verbs in one Levin class have a 
regular behavior, different from the verbs per-
taining to other classes. Also, the classes are se-
mantically coherent and all verbs belonging to 
one class share the same participant roles. 
This constraint of having the same semantic 
roles is further ensured inside the VerbNet lexi-
con that is constructed based on a more refined 
version of the Levin classification called Inter-
sective Levin classes (Dang et al, 1998). The 
lexicon provides a regular association between 
the syntactic and semantic properties of each of 
the described classes. It also provides informa-
tion about the syntactic frames (alternations) in 
which the verbs participate and the set of possi-
ble semantic roles.   
One corpus associated with the VerbNet 
lexicon is PropBank. The annotation scheme of 
PropBank ensures that the verbs belonging to the 
same Levin class share similarly labeled argu-
ments. Inside one Intersective Levin class, to one 
argument corresponds one semantic role num-
bered sequentially from Arg0 to Arg5. Higher 
numbered argument labels are less consistent and 
assigned per-verb basis.  
The Levin classes were constructed based on 
regularities exhibited at grammatical level and 
the resulting clusters were shown to be semanti-
cally coherent. As opposed, the FrameNet frames 
were build on semantic bases, by putting together 
verbs, nouns and adjectives that evoke the same 
situations. Although different in conception, the 
80
FrameNet verb clusters and VerbNet verb clus-
ters have common properties2: 
(1) Coherent syntactic behavior of verbs inside one 
cluster,  
(2) Different syntactic properties between any two 
distinct verb clusters,  
(3) Shared set of possible semantic roles for all verbs 
pertaining to the same cluster.  
Having these insights, we have assigned a corre-
spondent VerbNet class not to each verb predi-
cate but rather to each frame. In doing this we 
have applied the simplifying assumption that a 
frame has a unique corresponding Levin class. 
Thus, we have created a one-to-many mapping 
between the Intersective Levin classes and the 
frames. In order to create a pair ?FrameNet 
frame, VerbNet class?, our mapping algorithm 
checks both the syntactic and semantic consis-
tency by comparing the role frequency distribu-
tions on different syntactic positions for the two 
candidates. The algorithm is described in detail 
in the next section. 
4 Mapping FrameNet frames to 
VerbNet classes 
The mapping algorithm consists of three steps: 
(a) we link the frames and Intersective Levin 
verb classes that have the largest number of 
verbs in common and we create a set of pairs 
?FrameNet frame, VerbNet class? (see Figure 1); 
(b) we refine the pairs obtained in the previous 
step based on diathesis alternation criteria, i.e. 
the verbs pertaining to the FrameNet frame have 
to undergo the same diathesis alternation that 
characterize the corresponding VerbNet class 
(see Figure 2) and (c) we manually check and 
correct the resulting mapping. In the next sec-
tions we will explain in more detail each step of 
the mapping algorithm. 
4.1 Linking frames and Intersective Levin 
classes based on common verbs 
During the first phase of the algorithm, given a 
frame, we compute its intersection with each 
VerbNet class. We choose as candidate for the 
mapping the Intersective Levin class that has the 
largest number of verbs in common with the 
given frame (Figure 1, line (I)). If the size of the 
intersection between the FrameNet frame and the 
candidate VerbNet class is bigger than or equal 
                                                
2 For FrameNet, properties 1 and 2 are true for most 
of the frames but not for all. See section 4.4 for more 
details.  
to 3 elements then we form a pair ?FrameNet 
frame, VerbNet class? that qualifies for the 
second step of the algorithm.  
Only the frames that have more than three 
verb lexical units are candidates for this step 
(frames with less than 3 members cannot pass 
condition (II)). This excludes a number of 60 
frames that will subsequently be mapped 
manually.
Figure 1. Linking FrameNet frames and VerbNet 
classes 
4.2 Refining the mapping based on verb 
alternations 
In order to assign a VerbNet class to a frame, we 
have to check that the verbs belonging to that 
frame respect the diathesis alternation criteria 
used to define the VerbNet class. Thus, the pairs 
?FrameNet frame, VerbNet class? formed in step 
(I) of the mapping algorithm have to undergo a 
validation step that verifies the similarity be-
tween the enclosed FrameNet frame and VerbNet 
class. This validation process has several sub-
steps. 
First, we make use of the property (3) of the 
Levin classes and FrameNet frames presented in 
the previous section. According to this property, 
all verbs pertaining to one frame or Levin class 
have the same participant roles. Thus, a first test 
of compatibility between a frame and a Levin 
class is that they share the same participant roles. 
As FrameNet is annotated with frame-specific 
semantic roles we manually mapped these roles 
into the VerbNet set of thematic roles. Given a 
frame, we assigned thematic roles to all frame 
elements that are associated with verbal predi-
cates. For example the roles Speaker, Addressee, 
Message and Topic from the Telling frame were 
respectively mapped into Agent, Recipient, 
Theme and Topic.
)({ }
( )**
*
,3)(
maxarg)(
:,|,
}|{
}|{
}|{
}|{
CFPairsPairsthenCFifII
                            CFCcomputeI
FNFeachfor
PairsLet
:PAIRSCOMPUTE
CtomappedisFVNCFNFCFPairs
OUTPUT
FofverbaisvvFFrameFN
frameFrameNetaisFFFN
CofverbaisvvCClassVN
classVerbNetaisCCVN
INPUT
VNC
?=??
?=
?
?=
??=
=
=
=
=
?
81
)(
||||||||3
1
||||||||3
2
),,(#),,..,(
),,(#),,..,(
),,(#),,..,(
),,(#),,..,(
,
}:{
,
1
1
1
1
CF
CF
CF
CF
CF
iin
C
iin
C
iin
F
iin
F
th
ii
DSTDST
DSTDST
ADJADJ
ADJADJScore
positionCowhereooDST
positionCowhereooADJ
positionFowhereooDST
positionFowhereooADJ
PairsCFeachfor
a role setrbNet thete of theVe theta rolis the iTR
?
?+
?
?=
===
===
===
===
?
=
??
distant  
adjacent  
distant  
adjacent  
  
?
?
?
?
??
Second, we build a frequency distribution of 
VerbNet thematic roles on different syntactic 
position. Based on our observation and previous 
studies (Merlo and Stevenson, 2001), we assume 
that each Levin class has a distinct frequency 
distribution of roles on different grammatical 
slots. As we do not have matching grammatical 
function in FrameNet and VerbNet, we approxi-
mate that subjects and direct objects are more 
likely to appear on positions adjacent to the 
predicate, while indirect objects appear on more 
distant positions. The same intuition is used suc-
cessfully by G&J in the design of the Position
feature. 
We will acquire from the corpus, for each 
thematic role ?i, the frequencies with which it 
appears on an adjacent (ADJ) or distant (DST) 
position in a given frame or VerbNet class (i.e. 
#(?i, class, position)). Therefore, for each frame 
and class, we obtain two vectors with thematic 
role frequencies corresponding respectively to 
the adjacent and distant positions (see Figure 2). 
We compute a score for each pair ?FrameNet 
frame, VerbNet class? using the normalized sca-
lar product. We give a bigger weight to the adja-
cent dot product multiplying its score by 2/3 with 
respect to the distant dot product that is multi-
plied by 1/3. We do this to minimize the impact 
that adjunct roles like Temporal and Location 
(that appear mostly on the distant positions) 
could have on the final outcome.  
Figure 2. Mapping algorithm ? refining step 
The above frequency vectors are computed 
for FrameNet directly from the corpus of predi-
cate-argument structure examples associated 
with each frame. The examples associated with 
the VerbNet lexicon are extracted from the 
PropBank corpus.  In order to do this we apply a 
preprocessing step in which each label ARG0..N 
is replaced with its corresponding thematic role 
given the Intersective Levin class of the predi-
cate. We assign the same roles to the adjuncts all 
over PropBank as they are general for all verb 
classes. The only exception is ARGM-DIR that 
can correspond to Source, Goal or Path. We as-
sign different roles to this adjunct based on the 
prepositions. We ignore some adjuncts like 
ARGM-ADV or ARGM-DIS because they can-
not bear a thematic role. 
4.3 Mapping Results 
We found that only 133 VerbNet classes have 
correspondents among FrameNet frames. Also, 
from the frames mapped with an automatic score 
smaller than 0.5 points almost a half did not 
match any of the existing VerbNet classes3. A 
summary of the results is depicted in Table 1. 
The first column contains the automatic score 
provided by the mapping algorithm when com-
paring frames with Intersective Levin classes. 
The second column contains the number of 
frames for each score interval. The third column 
contains the percentage of frames, per each score 
interval, that did not have a corresponding 
VerbNet class and finally the forth column con-
tains the accuracy of the mapping algorithm.  
Score No. of Frames 
Not 
mapped Correct 
Overall 
Correct 
[0,0.5] 118 48.3% 82.5% 
(0.5,0.75] 69 0 84% 
(0.75,1] 72 0 100% 
89.6% 
Table 1. Results of the mapping algorithm 
4.4 Discussion 
In the literature, other studies compared the 
Levin classes to the FrameNet frames (Baker and 
Ruppenhofer, 2002). Their findings suggest that 
although the two set of clusters are roughly 
equivalent  there are also several types of 
mistmaches: 1) Levin classes that are narrower 
than  the corresponding frames, 2) Levin classes 
that are broader that the corresponding frames 
and 3) overlapping groupings. For our task, point 
2 does not pose a problem. Points 1 and 3 
however suggest that there are cases in which to 
one FrameNet frame corresponds more than one 
Levin class. By investigating such cases we 
noted that the mapping algorithm consistently 
assigns scores below 75% to cases that match 
problem 1 (two Levin classes inside one frame) 
and below 50% to cases that match problem 3 
(more than two Levin classes inside one frame). 
Thus, in order to increase the accuracy of our 
results a first step should be to assign an 
                                                
3 The automatic mapping  can be improved by manu-
ally assigning the FrameNet frames of the pairs that 
receive a score lower than 0.5. 
82
Intersective Levin class to each of the verbs 
pertaining to frames with score lower than 0.75. 
Nevertheless the current results are encouraging 
as they show that the algorithm is achiving its 
purpose by successfully detecting syntactic 
incoherencies that can be subsequently corrected 
manually. Also, in the next section we will show 
that our current mapping achieves very good 
results, giving evidence for  the effectivenes of 
the Levin class feature.  
5 Experiments 
In the previous section we have presented the 
algorithm for annotating the verb predicates of 
FrameNet with Intersective Levin classes. In or-
der to show the effectiveness of this annotation 
and of the Intersective Levin class in general we 
have performed several experiments. 
First, we trained (1) an ILC multiclassifier 
from FrameNet, (2) an ILC multiclassifier from 
PropBank and (3) a frame multiclassifier from 
FrameNet. We compared the results obtained 
when trying to classify the VerbNet class with 
the results obtained when classifying frame. We 
show that Intersective Levin classes are easier to 
detect than FrameNet frames.  
Our second set of experiments regards the 
automatic labeling of FrameNet semantic roles 
on FrameNet corpus when using as features: gold 
frame, gold Intersective Levin class, automati-
cally detected frame and automatically detected 
Intersective Levin class. We show that in all 
situations in which the VerbNet class feature is 
used, the accuracy loss, compared to the usage of 
the frame feature, is negligible. We thus show 
that the Intersective Levin class can successfully 
replace the frame feature for the task of semantic 
role labeling.  
Another set of experiments regards the gen-
eralization property of the Intersective Levin 
class. We show the impact of this feature when 
very few training data is available and its evolu-
tion when adding more and more training exam-
ples. We again perform the experiments for: gold 
frame, gold Intersective Levin class, automati-
cally detected frame and automatically detected 
Intersective Levin class.  
Finally, we simulate the difficulty of free 
text by annotating PropBank with FrameNet se-
mantic roles. We use PropBank because it is dif-
ferent from FrameNet from a domain point of 
view. This characteristic makes PropBank a dif-
ficult test bed for semantic role models trained 
on FrameNet.  
In the following section we present the re-
sults obtained for each of the experiments men-
tioned above. 
5.1 Experimental setup 
The corpora available for the experiments were 
PropBank and FrameNet. PropBank contains 
about 54,900 sentences and gold parse trees. We 
used sections from 02 to 22 (52,172 sentences) to 
train the Intersective Levin class classifiers and 
section 23 (2,742 sentences) for testing purposes. 
For the experiments on FrameNet corpus we 
extracted 58,384 sentences from the 319 frames 
that contain at least one verb annotation. There 
are 128,339 argument instances of 454 semantic 
roles. Only verbs are selected to be predicates in 
our evaluations. Moreover, as there is no fixed 
split between training and testing, we randomly 
selected 20% of sentences for testing and 80% 
for training. The sentences were processed using 
Charniak?s parser (Charniak, 2000) to generate 
parse trees automatically. 
For classification, we used the SVM-light-
TK software available at http://ai-nlp. 
info.uniroma2.it/moschitti which en-
codes tree kernels in the SVM-light software 
(Joachims, 1999). The classification performance 
was evaluated using the F1 measure for the sin-
gle-argument classifiers and the accuracy for the 
multiclassifiers. 
5.2 Automatic VerbNet vs. automatic Fra-
meNet frame detection 
In these experiments we classify Intersective 
Levin classes (ILC) on PropBank (PB) and 
FrameNet (FN) and frame on FrameNet. For the 
training stage we use SVMs with Tree Kernels. 
The main idea of tree kernels is the modeling 
of a KT(T1,T2) function which computes the 
number of common substructures between two 
trees T1 and T2. Thus, we can train SVMs with 
structures drawn directly from the syntactic parse 
tree of the sentence.  
The kernel that we employed in our 
experiments is based on the SCF structure 
devised in (Moschitti, 2004). We slightly 
modified SCF by adding the headwords of the 
arguments, useful for representing the selectional 
preferences.
  For frame detection on FrameNet, we trained 
our classifier on 46,734 training instances and 
tested on 11,650 testing instances, obtaining an 
accuracy of 91.11%. For ILC detection the 
results are depicted in Table  2. The first six 
columns report the F1 measure of some verb 
83
class classifiers whereas the last column shows 
the global multiclassifier accuracy.  
We note that ILC detection is performed better 
than frame detection on both FrameNet and 
PropBank. Also, the results obtained on ILC on 
PropBank are similar with the ones obtained on 
ILC on FrameNet. This suggests that the training 
corpus does not have a major influence. Also, the 
SCF-based tree kernel seems to be robust in what 
concerns the quality of the parse trees. The 
performance decay is very small on FrameNet 
that uses automatic parse trees with respect to 
PropBank that contains gold parse trees. These 
properties suggest that ILC are very suitable for 
free text.  
Table 2 . F1 and accuracy of the argument classifiers and the overall multiclassifier for Intersective Levin class  
5.3 Automatic semantic role labeling on 
FrameNet 
In the experiments involving semantic role 
labelling, we used a SVM with a polynomial 
kernel. We adopted the standard features 
developed for semantic role detection by Gildea 
and Jurafsky (see Section 2). Also, we 
considered some of the features designed by 
(Pradhan et al, 2004): First and Last Word/POS 
in Constituent, Subcategorization, Head Word of 
Prepositional Phrases and the Syntactic Frame
feature from (Xue and Palmer, 2004). For the 
rest of the paper we will refer to these features as 
being literature features (LF). The results 
obtained when using the literature features alone 
or in conjunction with the gold frame feature, 
gold ILC, automatically detected frame feature 
and automatically detected ILC are depicted in 
Table 3. The first four columns report the F1
measure of some role classifiers whereas the last 
column shows the global multiclassifier 
accuracy. The first row contains the number of 
training and testing instances and each of the 
other rows contains the performance obtained for 
different feature combinations. The results are 
reported for the labeling task as the argument-
boundary detection task is not affected by the 
frame-like features (G&J). 
We note that automatic frame results are 
very similar to automatic ILC results suggesting 
that ILC feature is a very good candidate for 
replacing the frame feature. Also, both automatic 
features are very effective, decreasing the error 
rate of 20%. 
 Body_part Crime Degree Agent Multiclassifier 
FN #Train Instances 
FN #Test Instances 
1,511 
356 
39 
5 
765 
187 
6,441 
1,643 
102,724 
25,615 
LF+Gold Frame 90.91 88.89 70.51 93.87 90.8 
LF+Gold ILC 90.80 88.89 71.52 92.01 88.23 
LF+Automatic Frame 84.87 88.89 70.10 87.73 85.64 
LF+Automatic ILC 85.08 88.89 69.62 87.74 84.45 
LF 79.76 75.00 64.17 80.82 80.99 
Table 3. F1 and accuracy of the argument classifiers and the overall multiclassifier for  
FrameNet semantic roles 
5.4 Semantic role learning curve when us-
ing Intersective Levin classes 
The next set of experiments show the impact of 
the ILC feature on semantic role labelling when 
few training data is available (Figure 3). As can 
be noted, the automatic ILC features (i.e. derived 
with classifers trained on FrameNet or PB) 
produce accuracy almost as good as the gold ILC 
one. Another observation is that the SRL 
classifiers are not saturated and more training 
examples would improve their accuracy. 
 run-
51.3.2 
cooking-
45.3 
characterize-
29.2 
other_cos-
45.4 
say-
37.7 
correspond-
36.1 Multiclassifier 
PB #Train Instances 
PB #Test Instances 
262 
5 
6 
5 
2,945 
134 
2,207 
149 
9,707 
608 
259 
20 
52,172 
2,742 
PB Results 75 33.33 96.3 97.24 100 88.89 92.96 
FN #Train Instances 
FN #Test Instances 
5,381 
1,343 
138 
35 
765 
40 
721 
184 
1,860 
1,343 
557 
111 
46,734 
11,650 
FN Results 96.36 72.73 95.73 92.43 94.43 78.23 92.63 
84
30
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90 100
% Training Data
A
cc
ur
ac
y 
   
 --
LF+ILC
LF
LF+Automatic ILC Trained on PB
LF+Automatic ILC Trained on FN
Figure 3. Semantic Role learning curve 
5.5 Annotating PropBank with FrameNet 
semantic roles 
To show that our approach can be suitable for 
semantic role free-text annotation, we have 
automatically classified PropBank sentences with 
the FrameNet semantic-role classifiers. In order 
to measure the quality of the annotation, we ran-
domly selected 100 sentences and manually veri-
fied them. We measured the performance ob-
tained with and without the automatic ILC fea-
ture. The sentences contained 189 arguments 
from which 35 were incorrect when ILC was 
used compared to 72 incorrect in the absence of 
this feature. This corresponds to an accuracy of 
81% with Intersective Levin class versus 62% 
without it.  
6 Conclusions 
In this paper we have shown that the Intersective 
Levin class feature can successfully replace the 
FrameNet frame feature. By doing that we could 
interconnect FrameNet to VerbNet and Prop-
Bank obtaining better verb coverage and a more 
robust semantic parser. Our good results show 
that we have defined an effective framework 
which is a promising step toward the design of 
free-text semantic parsers.  
In the future, we intend to measure the effective-
ness of our system by testing on larger, more 
comprehensive corpora and without relying on 
any manual annotation. 
Reference 
Collin Baker and Josef Ruppenhofer. 2002. Frame-
Net?s frames vs. Levin?s verb classes. 28th Annual 
Meeting of the Berkeley Linguistics Society. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. CONLL?05. 
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. ANLP?00 
Hoa Trang Dang, Karin Kipper, Martha Palmer and 
Joseph Rosenzweig. 1998. Investigating regular 
sense extensions based on Intersective Levin 
classes. Coling-ACL?98. 
Charles Fillmore. 1968. The case for case. Universals 
in Linguistic Theory. 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. CL Journal. 
Christopher Johnson, Miriam Petruck, Collin Baker, 
Michael Ellsworth, Josef Ruppenhofer, and Charles 
Fillmore. 2003. FrameNet: Theory and Practice. 
Berkeley, California. 
Paul Kingsbury, Martha Palmer. 2002. From Tree-
Bank to PropBank. LREC?02. 
Karin Kipper, Hoa Trang Dang and Martha Palmer. 
2000. Class-based construction of a verb lexicon. 
AAAI?00. 
 Beth Levin. 1993. English Verb Classes and Alterna-
tions A Preliminary Investigation. Chicago: Uni-
versity of Chicago Press. 
Kenneth Litkowski. 2004. Senseval-3 task automatic 
labeling of semantic roles. Senseval-3. 
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb classification based on statistical distri-
bution of argument structure. CL Journal. 
Alessandro Moschitti. 2004. A study on convolution 
kernel for shallow semantic parsing. ACL?04. 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, 
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2004. Support vector learning for semantic ar-
gument classification. Machine Learning Journal. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A Generative Model for FrameNet 
Semantic Role Labeling. ECML?03. 
Thorsten Joachims. 1999. Making large-scale SVM 
learning practical.. Advances in Kernel Methods - 
Support Vector Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP?04. 
85
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 61?68, New York City, June 2006. c?2006 Association for Computational Linguistics
Semantic Role Labeling via Tree Kernel Joint Inference
Alessandro Moschitti, Daniele Pighin and Roberto Basili
Department of Computer Science
University of Rome ?Tor Vergata?
00133 Rome, Italy
{moschitti,basili}@info.uniroma2.it
daniele.pighin@gmail.com
Abstract
Recent work on Semantic Role Labeling
(SRL) has shown that to achieve high
accuracy a joint inference on the whole
predicate argument structure should be ap-
plied. In this paper, we used syntactic sub-
trees that span potential argument struc-
tures of the target predicate in tree ker-
nel functions. This allows Support Vec-
tor Machines to discern between correct
and incorrect predicate structures and to
re-rank them based on the joint probabil-
ity of their arguments. Experiments on the
PropBank data show that both classifica-
tion and re-ranking based on tree kernels
can improve SRL systems.
1 Introduction
Recent work on Semantic Role Labeling (SRL)
(Carreras and Ma`rquez, 2005) has shown that to
achieve high labeling accuracy a joint inference on
the whole predicate argument structure should be
applied. For this purpose, we need to extract fea-
tures from the sentence?s syntactic parse tree that
encodes the target semantic structure. This task is
rather complex since we do not exactly know which
are the syntactic clues that capture the relation be-
tween the predicate and its arguments. For exam-
ple, to detect the interesting context, the modeling
of syntax/semantics-based features should take into
account linguistic aspects like ancestor nodes or se-
mantic dependencies (Toutanova et al, 2004).
A viable approach to generate a large number of
features has been proposed in (Collins and Duffy,
2002), where convolution kernels were used to im-
plicitly define a tree substructure space. The selec-
tion of the relevant structural features was left to the
Voted Perceptron learning algorithm. Such success-
ful experimentation shows that tree kernels are very
promising for automatic feature engineering, espe-
cially when the available knowledge about the phe-
nomenon is limited.
In a similar way, we can model SRL systems with
tree kernels to generate large feature spaces. More
in detail, most SRL systems split the labeling pro-
cess into two different steps: Boundary Detection
(i.e. to determine the text boundaries of predicate
arguments) and Role Classification (i.e. labeling
such arguments with a semantic role, e.g. Arg0 or
Arg1 as defined in (Kingsbury and Palmer, 2002)).
The former relates to the detection of syntactic parse
tree nodes associated with constituents that corre-
spond to arguments, whereas the latter considers the
boundary nodes for the assignment of the suitable
label. Both steps require the design and extraction
of features from parse trees. As capturing the tightly
interdependent relations among a predicate and its
arguments is a complex task, we can apply tree ker-
nels on the subtrees that span the whole predicate
argument structure to generate the feature space of
all the possible subtrees.
In this paper, we apply the traditional bound-
ary (TBC) and role (TRC) classifiers (Pradhan
et al, 2005a), which are based on binary predi-
cate/argument relations, to label all parse tree nodes
corresponding to potential arguments. Then, we ex-
61
tract the subtrees which span the predicate-argument
dependencies of such arguments, i.e. Argument
Spanning Trees (AST s). These are used in a tree
kernel function to generate all possible substructures
that encode n-ary argument relations, i.e. we carry
out an automatic feature engineering process.
To validate our approach, we experimented with
our model and Support Vector Machines for the clas-
sification of valid and invalid AST s. The results
show that this classification problem can be learned
with high accuracy. Moreover, we modeled SRL as a
re-ranking task in line with (Toutanova et al, 2005).
The large number of complex features provided by
tree kernels for structured learning allows SVMs to
reach the state-of-the-art accuracy.
The paper is organized as follows: Section 2 intro-
duces the Semantic Role Labeling based on SVMs
and the tree kernel spaces; Section 3 formally de-
fines the AST s and the algorithm for their classifi-
cation and re-ranking; Section 4 shows the compara-
tive results between our approach and the traditional
one; Section 5 presents the related work; and finally,
Section 6 summarizes the conclusions.
2 Semantic Role Labeling
In the last years, several machine learning ap-
proaches have been developed for automatic role
labeling, e.g. (Gildea and Jurasfky, 2002; Prad-
han et al, 2005a). Their common characteristic is
the adoption of attribute-value representations for
predicate-argument structures. Accordingly, our ba-
sic system is similar to the one proposed in (Pradhan
et al, 2005a) and it is hereby described.
We use a boundary detection classifier (for any
role type) to derive the words compounding an ar-
gument and a multiclassifier to assign the roles (e.g.
Arg0 or ArgM) described in PropBank (Kingsbury
and Palmer, 2002)). To prepare the training data for
both classifiers, we used the following algorithm:
1. Given a sentence from the training-set, generate
a full syntactic parse tree;
2. Let P and A be respectively the set of predicates
and the set of parse-tree nodes (i.e. the potential ar-
guments);
3. For each pair ?p, a? ? P ?A:
- extract the feature representation set, Fp,a;
- if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in the T+
set (positive examples), otherwise put it in the
T? set (negative examples).
The outputs of the above algorithm are the T+ and
T? sets. These sets can be directly used to train a
boundary classifier (e.g. an SVM). Regarding the
argument type classifier, a binary labeler for a role r
(e.g. an SVM) can be trained on the T+r , i.e. its pos-
itive examples and T?r , i.e. its negative examples,
where T+ = T+r ? T?r , according to the ONE-vs-
ALL scheme. The binary classifiers are then used
to build a general role multiclassifier by simply se-
lecting the argument associated with the maximum
among the SVM scores.
Regarding the design of features for predicate-
argument pairs, we can use the attribute-values de-
fined in (Gildea and Jurasfky, 2002) or tree struc-
tures (Moschitti, 2004). Although we focus on
the latter approach, a short description of the for-
mer is still relevant as they are used by TBC and
TRC. They include the Phrase Type, Predicate
Word, Head Word, Governing Category, Position
and Voice features. For example, the Phrase Type
indicates the syntactic type of the phrase labeled as
a predicate argument and the Parse Tree Path con-
tains the path in the parse tree between the predicate
and the argument phrase, expressed as a sequence of
nonterminal labels linked by direction (up or down)
symbols, e.g. V ? VP ? NP.
A viable alternative to manual design of syntac-
tic features is the use of tree-kernel functions. These
implicitly define a feature space based on all possi-
ble tree substructures. Given two trees T1 and T2, in-
stead of representing them with the whole fragment
space, we can apply the kernel function to evaluate
the number of common fragments.
Formally, given a tree fragment space F =
{f1, f2, . . . , f|F|}, the indicator function Ii(n)
is equal to 1 if the target fi is rooted at
node n and equal to 0 otherwise. A tree-
kernel function over t1 and t2 is Kt(t1, t2) =?
n1?Nt1
?
n2?Nt2 ?(n1, n2), where Nt1 and Nt2
are the sets of the t1?s and t2?s nodes, respectively. In
turn ?(n1, n2) =
?|F|
i=1 ?l(fi)Ii(n1)Ii(n2), where
0 ? ? ? 1 and l(fi) is the height of the subtree
fi. Thus ?l(fi) assigns a lower weight to larger frag-
62
S
NP VP
PRP
John
VP CC VP
VB NP
and
VB NP
took
DT NN
the book read
PRP$ NN
its title
Sentence Parse-Tree
S
NP VP
PRP
John
VP
VB NP
took
DT NN
the book
took{ARG0, ARG1}
S
NP VP
PRP
John
VP
VB NP
read
PRP$ NN
its title
read{ARG0, ARG1}
Figure 1: A sentence parse tree with two argument spanning trees (AST s)
ments. When ? = 1, ? is equal to the number of
common fragments rooted at nodes n1 and n2. As
described in (Collins and Duffy, 2002), ? can be
computed in O(|Nt1 | ? |Nt2 |).
3 Tree kernel-based classification of
Predicate Argument Structures
Traditional semantic role labeling systems extract
features from pairs of nodes corresponding to a
predicate and one of its argument, respectively.
Thus, they focus on only binary relations to make
classification decisions. This information is poorer
than the one expressed by the whole predicate ar-
gument structure. As an alternative we can select
the set of potential arguments (potential argument
nodes) of a predicate and extract features from them.
The number of the candidate argument sets is ex-
ponential, thus we should consider only those cor-
responding to the most probable correct argument
structures.
The usual approach (Toutanova et al, 2005) uses
a traditional boundary classifier (TBC) to select the
set of potential argument nodes. Such set can be as-
sociated with a subtree which in turn can be classi-
fied by means of a tree kernel function. This func-
tion intuitively measures to what extent a given can-
didate subtree is compatible with the subtree of a
correct predicate argument structure. We can use it
to define two different learning problems: (a) the
simple classification of correct and incorrect pred-
icate argument structures and (b) given the best m
structures, we can train a re-ranker algorithm able to
exploit argument inter-dependencies.
3.1 The Argument Spanning Trees (AST s)
We consider predicate argument structures anno-
tated in PropBank along with the corresponding
TreeBank data as our object space. Given the target
predicate node p and a node subset s = {n1, .., nk}
of the parse tree t, we define as the spanning tree
root r the lowest common ancestor of n1, .., nk and
p. The node set spanning tree (NST ) ps is the sub-
tree of t rooted in r from which the nodes that are
neither ancestors nor descendants of any ni or p are
removed.
Since predicate arguments are associated with
tree nodes (i.e. they exactly fit into syntactic
constituents), we can define the Argument Span-
ning Tree (AST ) of a predicate argument set,
{p, {a1, .., an}}, as the NST over such nodes,
i.e. p{a1,..,an}. An AST corresponds to the min-
imal subtree whose leaves are all and only the
words compounding the arguments and the predi-
cate. For example, Figure 1 shows the parse tree
of the sentence "John took the book and read
its title". took{Arg0,Arg1} and read{Arg0,Arg1}
are two AST structures associated with the two
predicates took and read, respectively. All the other
possible subtrees, i.e. NST s, are not valid AST s
for these two predicates. Note that classifying ps in
AST or NST for each node subset s of t is equiva-
lent to solve the boundary detection problem.
The critical points for the AST classification are:
(1) how to design suitable features for the charac-
terization of valid structures. This requires a careful
linguistic investigation about their significant prop-
erties. (2) How to deal with the exponential number
of NST s.
The first problem can be addressed by means of
tree kernels over the AST s. Tree kernel spaces are
an alternative to the manual feature design as the
learning machine, (e.g. SVMs) can select the most
relevant features from a high dimensional space. In
other words, we can use a tree kernel function to
estimate the similarity between two AST s (see Sec-
63
Figure 2: Two-step boundary classification. a) Sentence tree; b) Two candidate ASTs; c) Extended AST -
Ord labeling
tion 2), hence avoiding to define explicit features.
The second problem can be approached in two
ways:
(1) We can increase the recall of TBC to enlarge the
set of candidate arguments. From such set, we can
extract correct and incorrect argument structures. As
the number of such structures will be rather small,
we can apply the AST classifier to detect the cor-
rect ones.
(2) We can consider the classification probability
provided by TBC and TRC (Pradhan et al, 2005a)
and select the m most probable structures. Then, we
can apply a re-ranking approach based on SVMs and
tree kernels.
The re-ranking approach is the most promising
one as suggested in (Toutanova et al, 2005) but it
does not clearly reveal if tree kernels can be used
to learn the difference between correct or incorrect
argument structures. Thus it is interesting to study
both the above approaches.
3.2 NST Classification
As we cannot classify all possible candidate argu-
ment structures, we apply the AST classifier just to
detect the correct structures from a set of overlap-
ping arguments. Given two nodes n1 and n2 of an
NST , they overlap if either n1 is ancestor of n2 or
vice versa. NST s that contain overlapping nodes
are not valid AST s but subtrees of NSTs may be
valid ASTs. Assuming this, we define s as the set
of potential argument nodes and we create two node
sets s1 = s ? {n1} and s2 = s ? {n2}. By classi-
fying the two new NST s ps1 and ps2 with the AST
classifier, we can select the correct structures. Of
course, this procedure can be generalized to a set of
overlapping nodes greater than 2. However, consid-
ering that the Precision of TBC is generally high,
the number of overlapping nodes is usually small.
Figure 2 shows a working example of the multi-
stage classifier. In Frame (a), TBC labels as po-
tential arguments (circled nodes) three overlapping
nodes related to Arg1. This leads to two possible
non-overlapping solutions (Frame (b)) but only the
first one is correct. In fact, according to the second
one the propositional phrase ?of the book? would be
incorrectly attached to the verbal predicate, i.e. in
contrast with the parse tree. The AST classifier, ap-
plied to the two NST s, is expected to detect this
inconsistency and provide the correct output.
3.3 Re-ranking NST s with Tree Kernels
To implement the re-ranking model, we follow the
approach described in (Toutanova et al, 2005).
First, we use SVMs to implement the boundary
TBC and role TRC local classifiers. As SVMs do
not provide probabilistic output, we use the Platt?s
algorithm (Platt, 2000) and its revised version (Lin
et al, 2003) to trasform scores into probabilities.
Second, we combine TBC and TRC probabil-
ities to obtain the m most likely sequences s of
tree nodes annotated with semantic roles. As argu-
ment constituents of the same verb cannot overlap,
we generate sequences that respect such node con-
straint. We adopt the same algorithm described in
(Toutanova et al, 2005). We start from the leaves
and we select the m sequences that respect the con-
straints and at the same time have the highest joint
probability of TBC and TRC.
Third, we extract the following feature represen-
tation:
(a) The AST s associated with the predicate argu-
ment structures. To make faster the learning process
and to try to only capture the most relevant features,
we also experimented with a compact version of the
64
AST which is pruned at the level of argument nodes.
(b) Attribute value features (standard features) re-
lated to the whole predicate structure. These include
the features for each arguments (Gildea and Juras-
fky, 2002) and global features like the sequence of
argument labels, e.g. ?Arg0, Arg1, ArgM?.
Finally, we prepare the training examples for the
re-ranker considering the m best annotations of each
predicate structure. We use the approach adopted
in (Shen et al, 2003), which generates all possible
pairs from the m examples, i.e. (m2
)
pairs. Each pair
is assigned to a positive example if the first mem-
ber of the pair has a higher score than the second
member. The score that we use is the F1 measure
of the annotated structure with respect to the gold
standard. More in detail, given training/testing ex-
amples ei = ?t1i , t2i , v1i , v2i ?, where t1i and t2i are two
AST s and v1i and v2i are two feature vectors associ-
ated with two candidate predicate structures s1 and
s2, we define the following kernels:
1) Ktr(e1, e2) = Kt(t11, t12) +Kt(t21, t22)
?Kt(t11, t22)?Kt(t21, t12),
where tji is the j-th AST of the pair ei, Kt is the
tree kernel function defined in Section 2 and i, j ?
{1, 2}.
2) Kpr(e1, e2) = Kp(v11, v12) +Kp(v21, v22)
?Kp(v11, v22)?Kp(v21, v12),
where vji is the j-th feature vector of the pair ei and
Kp is the polynomial kernel applied to such vectors.
The final kernel that we use for re-ranking is the
following:
K(e1, e2) = Ktr(e1, e2)|Ktr(e1, e2)| +
Kpr(e1, e2)
|Kpr(e1, e2)|
Regarding tree kernel feature engineering, the
next section show how we can generate more effec-
tive features given an established kernel function.
3.4 Tree kernel feature engineering
Consider the Frame (b) of Figure 2, it shows two
perfectly identical NST s, consequently, their frag-
ments will also be equal. This prevents the algorithm
to learn something from such examples. To solve the
problem, we can enrich the NSTs by marking their
argument nodes with a progressive number, starting
from the leftmost argument. For example, in the first
NST of Frame (c), we mark as NP-0 and NP-1 the
first and second argument nodes whereas in the sec-
ond NST we trasform the three argument node la-
bels in NP-0, NP-1 and PP-2. We will refer to the
resulting structure as a AST -Ord (ordinal number).
This simple modification allows the tree kernel to
generate different argument structures for the above
NST s. For example, from the first NST in Fig-
ure 2.c, the fragments [NP-1 [NP][PP]], [NP
[DT][NN]] and [PP [IN][NP]] are gener-
ated. They do not match anymore with the [NP-0
[NP][PP]], [NP-1 [DT][NN]] and [PP-2
[IN][NP]] fragments generated from the second
NST in Figure 2.c.
Additionally, it should be noted that the semantic
information provided by the role type can remark-
ably help the detection of correct or incorrect predi-
cate argument structures. Thus, we can enrich the ar-
gument node label with the role type, e.g. the NP-0
and NP-1 of the correct AST of Figure 2.c become
NP-Arg0 and NP-Arg1 (not shown in the figure).
We refer to this structure as AST -Arg. Of course,
to apply the AST -Arg classifier, we need that TRC
labels the arguments detected by TBC.
4 The experiments
The experiments were carried out within the set-
ting defined in the CoNLL-2005 Shared Task
(Carreras and Ma`rquez, 2005). In particular,
we adopted the Charniak parse trees available at
www.lsi.upc.edu/?srlconll/ along with the of-
ficial performance evaluator.
All the experiments were performed with
the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes ST and SST kernels in SVM-light
(Joachims, 1999). For TBC and TRC, we used the
linear kernel with a regularization parameter (option
-c) equal to 1. A cost factor (option -j) of 10 was
adopted for TBC to have a higher Recall, whereas
for TRC, the cost factor was parameterized accord-
ing to the maximal accuracy of each argument class
on the validation set. For the AST -based classifiers
we used a ? equal to 0.4 (see (Moschitti, 2004)).
65
Section 21 Section 23
AST Class. P. R. F1 P. R. F1
? 69.8 77.9 73.7 62.2 77.1 68.9
Ord 73.7 81.2 77.3 63.7 80.6 71.2
Arg 73.6 84.7 78.7 64.2 82.3 72.1
Table 1: AST , AST -Ord, and AST -Arg perfor-
mance on sections 21 and 23.
4.1 Classification of whole predicate argument
structures
In these experiments, we trained TBC on sections
02-08 whereas, to achieve a very accurate role clas-
sifier, we trained TRC on all sections 02-21. To
train the AST , AST -Ord (AST with ordinal num-
bers in the argument nodes), and AST -Arg (AST
with argument type in the argument nodes) clas-
sifiers, we applied the TBC and TRC over sec-
tions 09-20. Then, we considered all the structures
whose automatic annotation showed at least an ar-
gument overlap. From these, we extracted 30,220
valid AST s and 28,143 non-valid AST s, for a total
of 183,642 arguments.
First, we evaluate the accuracy of the AST -based
classifiers by extracting 1,975 AST s and 2,220 non-
AST s from Section 21 and the 2,159 AST s and
3,461 non-AST s from Section 23. The accuracy
derived on Section 21 is an upperbound for our clas-
sifiers since it is obtained using an ideal syntactic
parser (the Charniak?s parser was trained also on
Section 21) and an ideal role classifier.
Table 1 shows Precision, Recall and F1 mea-
sures of the AST -based classifiers over the above
NSTs. Rows 2, 3 and 4 report the performance of
AST , AST -Ord, and AST -Arg classifiers, respec-
tively. We note that: (a) The impact of parsing ac-
curacy is shown by the gap of about 6% points be-
tween sections 21 and 23. (b) The ordinal number-
ing of arguments (Ord) and the role type informa-
tion (Arg) provide tree kernels with more meaning-
ful fragments since they improve the basic model
of about 4%. (c) The deeper semantic information
generated by the Arg labels provides useful clues to
select correct predicate argument structures since it
improves the Ord model on both sections.
Second, we measured the impact of the AST -
based classifiers on the accuracy of both phases of
semantic role labeling. Table 2 reports the results
on sections 21 and 23. For each of them, Precision,
Recall and F1 of different approaches to bound-
ary identification (bnd) and to the complete task,
i.e. boundary and role classification (bnd+class)
are shown. Such approaches are based on differ-
ent strategies to remove the overlaps, i.e. with the
AST , AST -Ord and AST -Arg classifiers and using
the baseline (RND), i.e. a random selection of non-
overlapping structures. The baseline corresponds to
the system based on TBC and TRC1.
We note that: (a) for any model, the boundary de-
tection F1 on Section 21 is about 10 points higher
than the F1 on Section 23 (e.g. 87.0% vs. 77.9%
for RND). As expected the parse tree quality is very
important to detect argument boundaries. (b) On the
real test (Section 23) the classification introduces la-
beling errors which decrease the accuracy of about
5% (77.9 vs 72.9 for RND). (c) The Ord and Arg
approaches constantly improve the baseline F1 of
about 1%. Such poor impact does not surprise as
the overlapping structures are a small percentage of
the test set, thus the overall improvement cannot be
very high.
Third, the comparison with the CoNLL 2005 re-
sults (Carreras and Ma`rquez, 2005) can only be
carried out with respect to the whole SRL task
(bnd+class in table 2) since boundary detection ver-
sus role classification is generally not provided in
CoNLL 2005. Moreover, our best global result, i.e.
73.9%, was obtained under two severe experimental
factors: a) the use of just 1/3 of the available train-
ing set, and b) the usage of the linear SVM model
for the TBC classifier, which is much faster than the
polynomial SVMs but also less accurate. However,
we note the promising results of the AST meta-
classifier, which can be used with any of the best
figure CoNLL systems.
Finally, the overall results suggest that the tree
kernel model is robust to parse tree errors since pre-
serves the same improvement across trees derived
with different accuracy, i.e. the semi-automatic trees
of Section 21 and the automatic tree of Section 23.
Moreover, it shows a high accuracy for the classi-
fication of correct and incorrect AST s. This last
property is quite interesting as the best SRL systems
1We needed to remove the overlaps from the baseline out-
come in order to apply the CoNLL evaluator.
66
(Punyakanok et al, 2005; Toutanova et al, 2005;
Pradhan et al, 2005b) were obtained by exploit-
ing the information on the whole predicate argument
structure.
Next section shows our preliminary experiments
on re-ranking using the AST kernel based approach.
4.2 Re-ranking based on Tree Kernels
In these experiments, we used the output of TBC
and TRC2 to provide an SVM tree kernel with a
ranked list of predicate argument structures. More in
detail, we applied a Viterbi-like algorithm to gener-
ate the 20 most likely annotations for each predicate
structure, according to the joint probabilistic model
of TBC and TRC. We sorted such structures based
on their F1 measure and used them to learn the SVM
re-ranker described in 3.3.
For training, we used Sections 12, 14, 15, 16
and 24, which contain 24,729 predicate structures.
For each of them, we considered the 5 annotations
having the highest F1 score (i.e. 123,674 NST s)
on the span of the 20 best annotations provided by
Viterbi algorithm. With such structures, we ob-
tained 294,296 pairs used to train the SVM-based
re-ranker. As the number of such structures is very
large the SVM training time was very high. Thus,
we sped up the learning process by using only the
AST s associated with the core arguments. From the
test sentences (which contain 5,267 structures), we
extracted the 20 best Viterbi annotated structures,
i.e. 102,343 (for a total of 315.531 pairs), which
were used for the following experiments:
First, we selected the best annotation (according
to the F1 provided by the gold standard annotations)
out of the 20 provided by the Viterbi?s algorithm.
The resulting F1 of 88.59% is the upperbound of our
approach.
Second, we selected the top ranked annotation in-
dicated by the Viterbi?s algorithm. This provides our
baseline F1 measure, i.e. 75.91%. Such outcome is
slightly higher than our official CoNLL result (Mos-
chitti et al, 2005) obtained without converting SVM
scores into probabilities.
Third, we applied the SVM re-ranker to select
2With the aim of improving the state-of-the-art, we applied
the polynomial kernel for all basic classifiers, at this time.
We used the models developed during our participation to the
CoNLL 2005 shared task (Moschitti et al, 2005).
the best structures according to the core roles. We
achieved 80.68% which is practically equal to the
result obtained in (Punyakanok et al, 2005; Car-
reras and Ma`rquez, 2005) for core roles, i.e. 81%.
Their overall F1 which includes all the arguments
was 79.44%. This confirms that the classification of
the non-core roles is more complex than the other
arguments.
Finally, the high computation time of the re-
ranker prevented us to use the larger structures
which include all arguments. The major complexity
issue was the slow training and classification time
of SVMs. The time needed for tree kernel function
was not so problematic as we could use the fast eval-
uation proposed in (Moschitti, 2006). This roughly
reduces the computation time to the one required by
a polynomial kernel. The real burden is therefore the
learning time of SVMs that is quadratic in the num-
ber of training instances. For example, to carry out
the re-ranking experiments required approximately
one month of a 64 bits machine (2.4 GHz and 4Gb
Ram). To solve this problem, we are going to study
the impact on the accuracy of fast learning algo-
rithms such as the Voted Perceptron.
5 Related Work
Recently, many kernels for natural language applica-
tions have been designed. In what follows, we high-
light their difference and properties.
The tree kernel used in this article was proposed
in (Collins and Duffy, 2002) for syntactic parsing
re-ranking. It was experimented with the Voted
Perceptron and was shown to improve the syntac-
tic parsing. In (Cumby and Roth, 2003), a feature
description language was used to extract structural
features from the syntactic shallow parse trees asso-
ciated with named entities. The experiments on the
named entity categorization showed that when the
description language selects an adequate set of tree
fragments the Voted Perceptron algorithm increases
its classification accuracy. The explanation was that
the complete tree fragment set contains many irrel-
evant features and may cause overfitting. In (Pun-
yakanok et al, 2005), a set of different syntactic
parse trees, e.g. the n best trees generated by the
Charniak?s parser, were used to improve the SRL
accuracy. These different sources of syntactic infor-
mation were used to generate a set of different SRL
67
Section 21 Section 23
bnd bnd+class bnd bnd+class
AST Classifier RND AST Classifier RND AST Classifier RND AST Classifier RND
- Ord Arg - Ord Arg - Ord Arg - Ord Arg
P. 87.5 88.3 88.3 86.9 85.5 86.3 86.4 85.0 78.6 79.0 79.3 77.8 73.1 73.5 73.4 72.3
R. 87.3 88.1 88.3 87.1 85.7 86.5 86.8 85.6 78.1 78.4 78.7 77.9 73.8 74.1 74.4 73.6
F1 87.4 88.2 88.3 87.0 85.6 86.4 86.6 85.3 78.3 78.7 79.0 77.9 73.4 73.8 73.9 72.9
Table 2: Semantic Role Labeling performance on automatic trees using AST -based classifiers.
outputs. A joint inference stage was applied to re-
solve the inconsistency of the different outputs. In
(Toutanova et al, 2005), it was observed that there
are strong dependencies among the labels of the se-
mantic argument nodes of a verb. Thus, to approach
the problem, a re-ranking method of role sequences
labeled by a TRC is applied. In (Pradhan et al,
2005b), some experiments were conducted on SRL
systems trained using different syntactic views.
6 Conclusions
Recent work on Semantic Role Labeling has shown
that to achieve high labeling accuracy a joint in-
ference on the whole predicate argument structure
should be applied. As feature design for such task is
complex, we can take advantage from kernel meth-
ods to model our intuitive knowledge about the n-
ary predicate argument relations.
In this paper we have shown that we can exploit
the properties of tree kernels to engineer syntactic
features for the semantic role labeling task. The ex-
periments suggest that (1) the information related
to the whole predicate argument structure is impor-
tant as it can improve the state-of-the-art and (2)
tree kernels can be used in a joint model to gen-
erate relevant syntactic/semantic features. The real
drawback is the computational complexity of work-
ing with SVMs, thus the design of fast algorithm is
an interesting future work.
Acknowledgments
This research is partially supported by the
PrestoSpace EU Project#: FP6-507336.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL05.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL02.
Chad Cumby and Dan Roth. 2003. Kernel methods for re-
lational learning. In Proceedings of ICML03, Washington,
DC, USA.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic label-
ing of semantic roles. Computational Linguistic, 28(3):496?
530.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC?02), Las Palmas, Spain.
H.T. Lin, C.J. Lin, and R.C. Weng. 2003. A note on platt?s
probabilistic outputs for support vector machines. Technical
report, National Taiwan University.
Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,
and Roberto Basili. 2005. Hierarchical semantic role label-
ing. In Proceedings of CoNLL05 shared task, Ann Arbor
(MI), USA.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of ACL?04,
Barcelona, Spain.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL?06,
Trento, Italy.
J. Platt. 2000. Probabilistic outputs for support vector ma-
chines and comparison to regularized likelihood methods.
MIT Press.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005a. Support vec-
tor learning for semantic argument classification. Machine
Learning Journal.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin,
and Daniel Jurafsky. 2005b. Semantic role labeling using
different syntactic views. In Proceedings ACL?05.
V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity of
syntactic parsing for semantic role labeling. In Proceedings
of IJCAI 2005.
Libin Shen, Anoop Sarkar, and Aravind Joshi. 2003. Using
ltag based features in parse reranking. In Conference on
EMNLP03, Sapporo, Japan.
Kristina Toutanova, Penka Markova, and Christopher D. Man-
ning. 2004. The leaf projection path view of parse trees:
Exploring string kernels for hpsg parse selection. In In Pro-
ceedings of EMNLP04.
Kristina Toutanova, Aria Haghighi, and Christopher Manning.
2005. Joint learning improves semantic role labeling. In
Proceedings of ACL05.
68
Workshop on TextGraphs, at HLT-NAACL 2006, pages 33?36,
New York City, June 2006. c?2006 Association for Computational Linguistics
Similarity between Pairs of Co-indexed Trees
for Textual Entailment Recognition
Fabio Massimo Zanzotto
DISCo
University Of Milan-Bicocca
Milano, Italy
zanzotto@disco.unimib.it
Alessandro Moschitti
DISP
University Of Rome ?Tor Vergata?
Roma, Italy
moschitti@info.uniroma2.it
Abstract
In this paper we present a novel similarity
between pairs of co-indexed trees to auto-
matically learn textual entailment classi-
fiers. We defined a kernel function based
on this similarity along with a more clas-
sical intra-pair similarity. Experiments
show an improvement of 4.4 absolute per-
cent points over state-of-the-art methods.
1 Introduction
Recently, a remarkable interest has been devoted to
textual entailment recognition (Dagan et al, 2005).
The task requires to determine whether or not a text
T entails a hypothesis H . As it is a binary classifica-
tion task, it could seem simple to use machine learn-
ing algorithms to learn an entailment classifier from
training examples. Unfortunately, this is not. The
learner should capture the similarities between dif-
ferent pairs, (T ?,H ?) and (T ??,H ??), taking into ac-
count the relations between sentences within a pair.
For example, having these two learning pairs:
T1 ? H1
T1 ?At the end of the year, all solid compa-
nies pay dividends?
H1 ?At the end of the year, all solid
insurance companies pay dividends.?
T1 ; H2
T1 ?At the end of the year, all solid compa-
nies pay dividends?
H2 ?At the end of the year, all solid compa-
nies pay cash dividends.?
determining whether or not the following implica-
tion holds:
T3 ? H3?
T3 ?All wild animals eat plants that have
scientifically proven medicinal proper-
ties.?
H3 ?All wild mountain animals eat plants
that have scientifically proven medici-
nal properties.?
requires to detect that:
1. T3 is structurally (and somehow lexically) sim-
ilar to T1 and H3 is more similar to H1 than to
H2;
2. relations between the sentences in the pairs
(T3,H3) (e.g., T3 and H3 have the same noun
governing the subject of the main sentence) are
similar to the relations between sentences in the
pairs (T1,H1) and (T1,H2).
Given this analysis we may derive that T3 ? H3.
The example suggests that graph matching tec-
niques are not sufficient as these may only detect
the structural similarity between sentences of textual
entailment pairs. An extension is needed to consider
also if two pairs show compatible relations between
their sentences.
In this paper, we propose to observe textual entail-
ment pairs as pairs of syntactic trees with co-indexed
nodes. This shuold help to cosider both the struc-
tural similarity between syntactic tree pairs and the
similarity between relations among sentences within
a pair. Then, we use this cross-pair similarity with
more traditional intra-pair similarities (e.g., (Corley
and Mihalcea, 2005)) to define a novel kernel func-
tion. We experimented with such kernel using Sup-
port Vector Machines on the Recognizing Textual
Entailment (RTE) challenge test-beds. The compar-
ative results show that (a) we have designed an ef-
fective way to automatically learn entailment rules
33
from examples and (b) our approach is highly accu-
rate and exceeds the accuracy of the current state-of-
the-art models.
In the remainder of this paper, Sec. 2 introduces
the cross-pair similarity and Sec. 3 shows the exper-
imental results.
2 Learning Textual Entailment from
examples
To carry out automatic learning from exam-
ples, we need to define a cross-pair similarity
K((T ?,H ?), (T ??,H ??)). This function should con-
sider pairs similar when: (1) texts and hypotheses
are structurally and lexically similar (structural sim-
ilarity); (2) the relations between the sentences in
the pair (T ?,H ?) are compatible with the relations
in (T ??,H ??) (intra-pair word movement compatibil-
ity). We argue that such requirements could be met
by augmenting syntactic trees with placeholders that
co-index related words within pairs. We will then
define a cross-pair similarity over these pairs of co-
indexed trees.
2.1 Training examples as pairs of co-indexed
trees
Sentence pairs selected as possible sentences in en-
tailment are naturally co-indexed. Many words (or
expressions) wh in H have a referent wt in T . These
pairs (wt, wh) are called anchors. Possibly, it is
more important that the two words in an anchor are
related than the actual two words. The entailment
could hold even if the two words are substitued with
two other related words. To indicate this we co-
index words associating placeholders with anchors.
For example, in Fig. 1, 2? indicates the (compa-
nies,companies) anchor between T1 and H1. These
placeholders are then used to augment tree nodes. To
better take into account argument movements, place-
holders are propagated in the syntactic trees follow-
ing constituent heads (see Fig. 1).
In line with many other researches (e.g., (Cor-
ley and Mihalcea, 2005)), we determine these an-
chors using different similarity or relatedness dec-
tors: the exact matching between tokens or lemmas,
a similarity between tokens based on their edit dis-
tance, the derivationally related form relation and
the verb entailment relation in WordNet, and, fi-
nally, a WordNet-based similarity (Jiang and Con-
rath, 1997). Each of these detectors gives a different
weight to the anchor: the actual computed similarity
for the last and 1 for all the others. These weights
will be used in the final kernel.
2.2 Similarity between pairs of co-indexed
trees
Pairs of syntactic trees where nodes are co-indexed
with placeholders allow the design a cross-pair simi-
larity that considers both the structural similarity and
the intra-pair word movement compatibility.
Syntactic trees of texts and hypotheses permit to
verify the structural similarity between pairs of sen-
tences. Texts should have similar structures as well
as hypotheses. In Fig. 1, the overlapping subtrees
are in bold. For example, T1 and T3 share the sub-
tree starting with S ? NP VP. Although the lexicals
in T3 and H3 are quite different from those T1 and
H1, their bold subtrees are more similar to those of
T1 and H1 than to T1 and H2, respectively. H1 and
H3 share the production NP ? DT JJ NN NNS while
H2 and H3 do not. To decide on the entailment for
(T3,H3), we can use the value of (T1,H1).
Anchors and placeholders are useful to verify if
two pairs can be aligned as showing compatible
intra-pair word movement. For example, (T1,H1)
and (T3,H3) show compatible constituent move-
ments given that the dashed lines connecting place-
holders of the two pairs indicates structurally equiv-
alent nodes both in the texts and the hypotheses. The
dashed line between 3 and b links the main verbs
both in the texts T1 and T3 and in the hypotheses H1
and H3. After substituting 3 to b and 2 to a , T1
and T3 share the subtree S ? NP 2 VP 3 . The same
subtree is shared between H1 and H3. This implies
that words in the pair (T1,H1) are correlated like
words in (T3,H3). Any different mapping between
the two anchor sets would not have this property.
Using the structural similarity, the placeholders,
and the connection between placeholders, the over-
all similarity is then defined as follows. Let A? and
A?? be the placeholders of (T ?,H ?) and (T ??,H ??),
respectively. The similarity between two co-indexed
syntactic tree pairs Ks((T ?,H ?), (T ??,H ??)) is de-
fined using a classical similarity between two trees
KT (t1, t2) when the best alignment between the A?
and A?? is given. Let C be the set of all bijective
34
T1 T3
S
PP
IN
At
NP 0
NP 0
DT
the
NN 0
end
0
PP
IN
of
NP 1
DT
the
NN 1
year
1
,
,
NP 2
DT
all
JJ 2
solid
2?
NNS 2
companies
2?
VP 3
VBP 3
pay
3
NP 4
NNS 4
dividends
4
S
NP a
DT
All
JJ a
wild
a?
NNS a
animals
a?
VP b
VBP b
eat
b
NP c
plants
c ... properties
H1 H3
S
PP
IN
At
NP 0
NP 0
DT
the
NN 0
end
0
PP
IN
of
NP 1
DT
the
NN 1
year
1
,
,
NP 2
DT
all
JJ 2
solid
2?
NN
insurance
NNS 2
companies
2?
VP 3
VBP 3
pay
3
NP 4
NNS 4
dividends
4
S
NP a
DT
All
JJ a
wild
a?
NN
mountain
NNS a
animals
a?
VP b
VBP b
eat
b
NP c
plants
c ... properties
H2 H3
S
PP
At ... year
NP 2
DT
all
JJ 2
solid
2?
NNS 2
companies
2?
VP 3
VBP 3
pay
3
NP 4
NN
cash
NNS 4
dividends
4
S
NP a
DT
All
JJ a
wild
a?
NN
mountain
NNS a
animals
a?
VP b
VBP b
eat
b
NP c
plants
c ... properties
Figure 1: Relations between (T1,H1), (T1,H2), and (T3,H3).
mappings from a? ? A? : |a?| = |A??| to A??, an
element c ? C is a substitution function. The co-
indexed tree pair similarity is then defined as:
Ks((T ?, H ?), (T ??,H ??)) =
maxc?C(KT (t(H ?, c), t(H ??, i)) +KT (t(T ?, c), t(T ??, i))
where (1) t(S, c) returns the syntactic tree of the
hypothesis (text) S with placeholders replaced by
means of the substitution c, (2) i is the identity sub-
stitution and (3) KT (t1, t2) is a function that mea-
sures the similarity between the two trees t1 and t2.
2.3 Enhancing cross-pair syntactic similarity
As the computation cost of the similarity measure
depends on the number of the possible sets of corre-
spondences C and this depends on the size of the
anchor sets, we reduce the number of placehold-
ers used to represent the anchors. Placeholders will
have the same name if these are in the same chunk
both in the text and the hypothesis, e.g., the place-
holders 2? and 2? are collapsed to 2 .
3 Experimental investigation
The aim of the experiments is twofold: we show that
(a) entailments can be learned from examples and
(b) our kernel function over syntactic structures is
effective to derive syntactic properties. The above
goals can be achieved by comparing our cross-pair
similarity kernel against (and in combination with)
other methods.
3.1 Experimented kernels
We compared three different kernels: (1) the ker-
nel Kl((T ?,H ?), (T ??,H ??)) based on the intra-pair
35
Datasets Kl Kl +Kt Kl +Ks
Train:D1 Test:T1 0.5888 0.6213 0.6300
Train:T1 Test:D1 0.5644 0.5732 0.5838
Train:D2(50%)? Test:D2(50%)?? 0.6083 0.6156 0.6350
Train:D2(50%)?? Test:D2(50%)? 0.6272 0.5861 0.6607
Train:D2 Test:T2 0.6038 0.6238 0.6388
Mean 0.5985 0.6040 0.6297
(? 0.0235 ) (? 0.0229 ) (? 0.0282 )
Table 1: Experimental results
lexical similarity siml(T,H) as defined in (Cor-
ley and Mihalcea, 2005). This kernel is de-
fined as Kl((T ?,H ?), (T ??,H ??)) = siml(T ?,H ?) ?
siml(T ??,H ??). (2) the kernel Kl+Ks that combines
our kernel with the lexical-similarity-based kernel;
(3) the kernel Kl + Kt that combines the lexical-
similarity-based kernel with a basic tree kernel.
This latter is defined as Kt((T ?,H ?), (T ??,H ??)) =
KT (T ?, T ??)+KT (H ?,H ??). We implemented these
kernels within SVM-light (Joachims, 1999).
3.2 Experimental settings
For the experiments, we used the Recognizing Tex-
tual Entailment (RTE) Challenge data sets, which
we name as D1, T1 and D2, T2, are the develop-
ment and the test sets of the first and second RTE
challenges, respectively. D1 contains 567 examples
whereas T1, D2 and T2 have all the same size, i.e.
800 instances. The positive examples are the 50%
of the data. We produced also a random split of D2.
The two folds are D2(50%)? and D2(50%)??.
We also used the following resources: the Char-
niak parser (Charniak, 2000) to carry out the syntac-
tic analysis; the wn::similarity package (Ped-
ersen et al, 2004) to compute the Jiang&Conrath
(J&C) distance (Jiang and Conrath, 1997) needed to
implement the lexical similarity siml(T,H) as de-
fined in (Corley and Mihalcea, 2005); SVM-light-
TK (Moschitti, 2004) to encode the basic tree kernel
function, KT , in SVM-light (Joachims, 1999).
3.3 Results and analysis
Table 1 reports the accuracy of different similar-
ity kernels on the different training and test split de-
scribed in the previous section. The table shows
some important result.
First, as observed in (Corley and Mihalcea, 2005)
the lexical-based distance kernel Kl shows an accu-
racy significantly higher than the random baseline,
i.e. 50%. This accuracy (second line) is comparable
with the best systems in the first RTE challenge (Da-
gan et al, 2005). The accuracy reported for the best
systems, i.e. 58.6% (Glickman et al, 2005; Bayer
et al, 2005), is not significantly far from the result
obtained with Kl, i.e. 58.88%.
Second, our approach (last column) is signifi-
cantly better than all the other methods as it pro-
vides the best result for each combination of train-
ing and test sets. On the ?Train:D1-Test:T1? test-
bed, it exceeds the accuracy of the current state-of-
the-art models (Glickman et al, 2005; Bayer et al,
2005) by about 4.4 absolute percent points (63% vs.
58.6%) and 4% over our best lexical similarity mea-
sure. By comparing the average on all datasets, our
system improves on all the methods by at least 3 ab-
solute percent points.
Finally, the accuracy produced by our kernel
based on co-indexed trees Kl + Ks is higher than
the one obtained with the plain syntactic tree ker-
nel Kl + Kt. Thus, the use of placeholders and co-
indexing is fundamental to automatically learn en-
tailments from examples.
References
Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and
Alexander Yeh. 2005. MITRE?s submissions to the eu pas-
cal rte challenge. In Proceedings of the 1st Pascal Challenge
Workshop, Southampton, UK.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of the 1st NAACL, pages 132?139, Seattle, Wash-
ington.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, pages 13?18, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The
PASCAL RTE challenge. In PASCAL Challenges Workshop,
Southampton, U.K.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings of the
1st Pascal Challenge Workshop, Southampton, UK.
Jay J. Jiang and David W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In Proc. of
the 10th ROCLING, pages 132?139, Tapei, Taiwan.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods-Support Vector Learning. MIT
Press.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In proceedings of the ACL,
Barcelona, Spain.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of 5th NAACL, Boston, MA.
36
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 201?204, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Hierarchical Semantic Role Labeling
Alessandro Moschitti?
moschitti@info.uniroma2.it
? DISP - University of Rome ?Tor Vergata?, Rome, Italy
? ITC-Irst, ? DIT - University of Trento, Povo, Trento, Italy
Ana-Maria Giuglea?
ana-maria.giuglea@topex.ro
Bonaventura Coppola??
coppolab@itc.it
Roberto Basili?
basili@info.uniroma2.it
Abstract
We present a four-step hierarchical SRL
strategy which generalizes the classical
two-level approach (boundary detection
and classification). To achieve this, we
have split the classification step by group-
ing together roles which share linguistic
properties (e.g. Core Roles versus Ad-
juncts). The results show that the non-
optimized hierarchical approach is com-
putationally more efficient than the tradi-
tional systems and it preserves their accu-
racy.
1 Introduction
For accomplishing the CoNLL 2005 Shared Task
on Semantic Role Labeling (Carreras and Ma`rquez,
2005), we capitalized on our experience on the se-
mantic shallow parsing by extending our system,
widely experimented on PropBank and FrameNet
(Giuglea and Moschitti, 2004) data, with a two-
step boundary detection and a hierarchical argument
classification strategy.
Currently, the system can work in both basic and
enhanced configuration. Given the parse tree of an
input sentence, the basic system applies (1) a bound-
ary classifier to select the nodes associated with cor-
rect arguments and (2) a multi-class labeler to assign
the role type. For such models, we used some of the
linear (e.g. (Gildea and Jurasfky, 2002; Pradhan et
al., 2005)) and structural (Moschitti, 2004) features
developed in previous studies.
In the enhanced configuration, the boundary an-
notation is subdivided in two steps: a first pass in
which we label argument boundary and a second
pass in which we apply a simple heuristic to elimi-
nate the argument overlaps. We have also tried some
strategies to learn such heuristics automatically. In
order to do this we used a tree kernel to classify the
subtrees associated with correct predicate argument
structures (see (Moschitti et al, 2005)). The ratio-
nale behind such an attempt was to exploit the cor-
relation among potential arguments.
Also, the role labeler is divided into two steps:
(1) we assign to the arguments one out of four possi-
ble class labels: Core Roles, Adjuncts, Continuation
Arguments and Co-referring Arguments, and (2) in
each of the above class we apply the set of its spe-
cific classifiers, e.g. A0,..,A5 within the Core Role
class. As such grouping is relatively new, the tradi-
tional features may not be sufficient to characterize
each class. Thus, to generate a large set of features
automatically, we again applied tree kernels.
Since our SRL system exploits the PropBank for-
malism for internal data representation, we devel-
oped ad-hoc procedures to convert back and forth
to the CoNLL Shared Task format. This conversion
step gave us useful information about the amount
and the nature of the parsing errors. Also, we could
measure the frequency of the mismatches between
syntax and role annotation.
In the remainder of this paper, Section 2 describes
the basic system configuration whereas Section 3 il-
lustrates its enhanced properties and the hierarchical
structure. Section 4 describes the experimental set-
ting and the results. Finally, Section 5 summarizes
201
our conclusions.
2 The Basic Semantic Role Labeler
In the last years, several machine learning ap-
proaches have been developed for automatic role la-
beling, e.g. (Gildea and Jurasfky, 2002; Pradhan
et al, 2005). Their common characteristic is the
adoption of flat feature representations for predicate-
argument structures. Our basic system is similar to
the one proposed in (Pradhan et al, 2005) and it is
described hereafter.
We divided the predicate argument labeling in two
subtasks: (a) the detection of the arguments related
to a target, i.e. all the compounding words of such
argument, and (b) the classification of the argument
type, e.g. A0 or AM. To learn both tasks we used the
following algorithm:
1. Given a sentence from the training-set, generate
a full syntactic parse-tree;
2. Let P and A be respectively the set of predicates
and the set of parse-tree nodes (i.e. the potential ar-
guments);
3. For each pair <p, a> ? P ?A:
- extract the feature representation set, Fp,a;
- if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
We trained the SVM boundary classifier on T+ and
T? sets and the role labeler i on the T+i , i.e. its pos-
itive examples and T?i , i.e. its negative examples,
where T+ = T+i ? T?i , according to the ONE-vs.-
ALL scheme. To implement the multi-class clas-
sifiers we select the argument associated with the
maximum among the SVM scores.
To represent the Fp,a pairs we used the following
features:
- the Phrase Type, Predicate Word, Head Word,
Governing Category, Position and Voice defined in
(Gildea and Jurasfky, 2002);
- the Partial Path, Compressed Path, No Direction
Path, Constituent Tree Distance, Head Word POS,
First and Last Word/POS in Constituent, SubCate-
gorization and Head Word of Prepositional Phrases
proposed in (Pradhan et al, 2005); and
- the Syntactic Frame designed in (Xue and Palmer,
2004).
Figure 1: Architecture of the Hierarchical Semantic Role La-
beler.
3 Hierarchical Semantic Role Labeler
Having two phases for argument labeling provides
two main advantages: (1) the efficiency is increased
as the negative boundary examples, which are al-
most all parse-tree nodes, are used with one clas-
sifier only (i.e. the boundary classifier), and (2) as
arguments share common features that do not occur
in the non-arguments, a preliminary classification
between arguments and non-arguments advantages
the boundary detection of roles with fewer training
examples (e.g. A4). Moreover, it may be simpler
to classify the type of roles when the not-argument
nodes are absent.
Following this idea, we generalized the above two
level strategy to a four-step role labeling by group-
ing together the arguments sharing similar proper-
ties. Figure 1, shows the hierarchy employed for ar-
gument classification:
During the first phase, we select the parse tree
nodes which are likely predicate arguments. An
SVM with moderately high recall is applied for such
purpose.
In the second phase, a simple heuristic which se-
lects non-overlapping nodes from those derived in
the previous step is applied. Two nodes n1 and n2
do not overlap if n1 is not ancestor of n2 and vicev-
ersa. Our heuristic simply eliminates the nodes that
cause the highest number of overlaps. We have also
studied how to train an overlap resolver by means of
tree kernels; the promising approach and results can
be found in (Moschitti et al, 2005).
In the third phase, we classify the detected argu-
ments in the following four classes: AX, i.e. Core
202
Arguments, AM, i.e. Adjuncts, CX, i.e. Continua-
tion Arguments and RX, i.e. the Co-referring Argu-
ments. The above classification relies on linguistic
reasons. For example Core arguments class contains
the arguments specific to the verb frames while Ad-
junct Arguments class contains arguments that are
shared across all verb frames.
In the fourth phase, we classify the members
within the classes of the previous level, e.g. A0 vs.
A1, ..., A5.
4 The Experiments
We experimented our approach with the CoNLL
2005 Shared Task standard dataset, i.e. the Pen-
nTree Bank, where sections from 02 to 21 are used
as training set, Section 24 as development set (Dev)
and Section 23 as the test set (WSJ). Additionally,
the Brown corpus? sentences were also used as the
test set (Brown). As input for our feature extractor
we used only the Charniak?s parses with their POSs.
The evaluations were carried out with the SVM-
light-TK software (Moschitti, 2004) available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes the tree kernels in the SVM-light
software (Joachims, 1999). We used the default
polynomial kernel (degree=3) for the linear feature
representations and the tree kernels for the structural
feature processing.
As our feature extraction module was designed
to work on the PropBank project annotation format
(i.e. the prop.txt index file), we needed to generate
it from the CoNLL data. Each PropBank annota-
tion refers to a parse tree node which exactly cov-
ers the target argument but when using automatic
parses such node may not exist. For example, on
the CoNLL Charniak?s parses, (sections 02-21 and
24), we discovered that this problem affects 10,293
out of the 241,121 arguments (4.3%) and 9,741 sen-
tences out of 87,257 (11.5%). We have found out
that most of the errors are due to wrong parsing at-
tachments. This observation suggests that the capa-
bility of discriminating between correct and incor-
rect parse trees is a key issue in the boundary de-
tection phase and it must be properly taken into ac-
count.
4.1 Basic System Evaluation
For the boundary classifier we used a SVM with
the polynomial kernel of degree 3. We set the reg-
ularization parameter, c, to 1 and the cost factor,
j to 7 (to have a slightly higher recall). To re-
duce the learning time, we applied a simple heuristic
which removes the nodes covering the target predi-
cate node. From the initial 4,683,777 nodes (of sec-
tions 02-21), the heuristic removed 1,503,100 nodes
with a loss of 2.6% of the total arguments. How-
ever, as we started the experiments in late, we used
only the 992,819 nodes from the sections 02-08. The
classifier took about two days and half to converge
on a 64 bits machine (2.4 GHz and 4Gb Ram).
The multiclassifier was built with 52 binary ar-
gument classifiers. Their training on all arguments
from sec 02-21, (i.e. 242,957), required about a half
day on a machine with 8 processors (32 bits, 1.7
GHz and overll 4Gb Ram).
We run the role multiclassifier on the output of the
boundary classifier. The results on the Dev, WSJ and
Brown test data are shown in Table 1. Note that, the
overlapping nodes cause the generation of overlap-
ping constituents in the sentence annotation. This
prevents us to use the CoNLL evaluator. Thus, we
used the overlap resolution algorithm also for the ba-
sic system.
4.2 Hierarchical Role Labeling Evaluation
As the first two phases of the hierarchical labeler are
identical to the basic system, we focused on the last
two phases. We carried out our studies over the Gold
Standard boundaries in the presence of arguments
that do not have a perfect-covering node in the Char-
niak trees.
To accomplish the third phase, we re-organized
the flat arguments into the AX, AM, CX and RX
classes and we built a single multi-classifier. For
the fourth phase, we built a multi-classifier for each
of the above classes: only the examples related to
the target class were used, e.g. the AX mutliclas-
sifier was designed with the A0,..,A5 ONE-vs-ALL
binary classifiers.
In rows 2 and 3, Table 2 shows the numbers of
training and development set instances. Row 4 con-
tains the F1 of the binary classifiers of the third
phase whereas Row 5 reports the F1 of the result-
ing multi-classifier. Row 6 presents the F1s of the
multi-classifiers of the fourth phase.
Row 7 illustrates the F1 measure of the fourth
phase classifier applied to the third phase output. Fi-
203
Precision Recall F?=1
Development 74.95% 73.10% 74.01
Test WSJ 76.55% 75.24% 75.89
Test Brown 65.92% 61.83% 63.81
Test WSJ+Brown 75.19% 73.45% 74.31
Test WSJ Precision Recall F?=1
Overall 76.55% 75.24% 75.89
A0 81.05% 84.37% 82.67
A1 77.21% 74.12% 75.63
A2 67.02% 68.11% 67.56
A3 69.63% 54.34% 61.04
A4 74.75% 72.55% 73.63
A5 100.00% 40.00% 57.14
AM-ADV 55.23% 55.34% 55.28
AM-CAU 66.07% 50.68% 57.36
AM-DIR 50.62% 48.24% 49.40
AM-DIS 77.71% 78.44% 78.07
AM-EXT 68.00% 53.12% 59.65
AM-LOC 59.02% 63.09% 60.99
AM-MNR 67.67% 52.33% 59.02
AM-MOD 98.65% 92.56% 95.51
AM-NEG 97.37% 96.52% 96.94
AM-PNC 42.28% 45.22% 43.70
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 81.90% 74.52% 78.03
R-A0 79.50% 84.82% 82.07
R-A1 62.23% 75.00% 68.02
R-A2 100.00% 31.25% 47.62
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 100.00% 100.00% 100.00
R-AM-LOC 85.71% 85.71% 85.71
R-AM-MNR 22.22% 33.33% 26.67
R-AM-TMP 67.69% 84.62% 75.21
V 97.34% 97.30% 97.32
Table 1: Overall results (top) and detailed results on
the WSJ test (bottom).
nally, in Row 8, we report the F1 of the basic system
on the gold boundary nodes. We note that the basic
system shows a slightly higher F1 but is less compu-
tational efficient than the hierarchical approach.
5 Final Remarks
In this paper we analyzed the impact of a hierarchi-
cal categorization on the semantic role labeling task.
The results show that such approach produces an ac-
curacy similar to the flat systems with a higher ef-
ficiency. Moreover, some preliminary experiments
show that each node of the hierarchy requires differ-
ent features to optimize the associated multiclassi-
fier. For example, we found that the SCF tree kernel
(Moschitti, 2004) improves the AX multiclassifier
AX AM CX RX
# train. examples 172,457 59,473 2,954 7,928
# devel. examples 5,930 2,132 105 284
Phase III: binary class. 97.29 97.35 70.86 93.15
Phase III 95.99
Phase IV 92.50 85.88 91.43 91.55
Phase III & IV 88.15
Basic System 88.61
Table 2: Hierarchical Semantic Role Labeler Results
whereas the PAF tree kernel seems more suited for
the classification within the other classes, e.g. AM.
Future work on the optimization of each phase is
needed to study the potential accuracy limits of the
proposed hierarchical approach.
Acknowledgements
We wish to thank Daniele Pighin for his valuable
support in the development of the SRL system.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In pro-
ceedings of CoNLL?05.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic labeling
of semantic roles. Computational Linguistic.
Ana-Maria Giuglea and Alessandro Moschitti. 2004. Knowl-
edge Discovering using FrameNet, VerbNet and PropBank.
In proceedings of the Workshop on Ontology and Knowledge
Discovering at ECML?04, Pisa, Italy.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,
and Roberto Basili. 2005. Engineering of syntactic features
for shallow semantic parsing. In proceedings of the Feature
Engineering Workshop at ACL?05, Ann Arbor, USA.
Alessandro Moschitti. 2004. A study on convolution kernel
for shallow semantic parsing. In proceedings of ACL-2004,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Support vector
learning for semantic argument classification. to appear in
Machine Learning Journal.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP?04,
Barcelona, Spain.
204
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961?968
Manchester, August 2008
Coreference Systems based on Kernels Methods
Yannick Versley
SFB 441
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Alessandro Moschitti
DISI
University of Trento
moschitti@disi.unitn.it
Massimo Poesio
DISI
University of Trento
massimo.poesio@unitn.it
Xiaofeng Yang
Data Mining Department
Institute for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Abstract
Various types of structural information -
e.g., about the type of constructions in
which binding constraints apply, or about
the structure of names - play a central role
in coreference resolution, often in combi-
nation with lexical information (as in ex-
pletive detection). Kernel functions ap-
pear to be a promising candidate to capture
structure-sensitive similarities and com-
plex feature combinations, but care is re-
quired to ensure they are exploited in the
best possible fashion. In this paper we
propose kernel functions for three subtasks
of coreference resolution - binding con-
straint detection, expletive identification,
and aliasing - together with an architec-
ture to integrate them within the standard
framework for coreference resolution.
1 Introduction
Information about coreference relations?i.e.,
which noun phrases are mentions of the same
entity?has been shown to be beneficial in a great
number of NLP tasks, including information
extraction (McCarthy and Lehnert 1995), text
planning (Barzilay and Lapata 2005) and sum-
marization (Steinberger et al 2007). However,
the performance of coreference resolvers on
unrestricted text is still quite low. One reason
for this is that coreference resolution requires a
great deal of information, ranging from string
matching to syntactic constraints to semantic
knowledge to discourse salience information to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
full common sense reasoning (Sidner 1979; Hobbs
1978, 1979; Grosz et al 1995; Vieira and Poesio
2000; Mitkov 2002). Much of this information
won?t be available to robust coreference resolvers
until better methods are found to represent and
encode common sense knowledge; but part of
the problem is also the need for better methods
to encode information that is in part structural,
in part lexical. Enforcing binding constraints
?e.g., ruling out Peter as antecedent of him in (1a)
requires recognizing that the anaphor occurs in a
particular type of construction (Chomsky 1981;
Lappin and Leass 1994; Yang et al 2006) whose
exact definition however has not yet been agreed
upon by linguists (indeed, it may only be definable
in a graded sense (Sturt 2003; Yang et al 2006)),
witness examples like (1b). Parallelism effects are
a good example of structural information inducing
preferences rather than constraints. Recognizing
that It in examples such as (1c,d) are expletives
requires a combination of structural information
and lexical information (Lappin and Leass 1994;
Evans 2001). But some sort of structure also
underlies our interpretation of other types of
coreference: e.g., knowledge about the structure
of names certainly plays a role in recognizing
that BJ Habibie is a possible antecedent for Mr.
Habibie.
(1) a. John thinks that Peter hates him.
b. John hopes that Jane is speaking only to
himself.
c. It?s lonely here.
d. It had been raining all day.
The need to capture such information suggests
a role for kernel methods (Vapnik 1995) in coref-
erence resolution. Kernel functions make it pos-
sible to capture the similarity between structures
961
without explicitly enumerating all the substruc-
tures, and have therefore been shown to be a vi-
able approach to feature engineering for natural
language processing for any task in which struc-
tural information plays a role, e.g. (Collins and
Duffy 2002; Zelenko et al 2003; Giuglea and Mos-
chitti 2006; Zanzotto and Moschitti 2006; Mos-
chitti et al 2007). Indeed, they have already been
used in NLP to encode the type of structural in-
formation that plays a role in binding constraints
(Yang et al 2006); however, the methods used in
this previous work do not make it possible to ex-
ploit the full power of kernel functions. In this
work, we extend the use of kernel functions for
coreference by designing and testing kernels for
three subtasks of the coreference task:
? Binding constraints
? Expletive detection
? Aliasing
and developing distinct classifiers for each of these
tasks. We show that our developed kernels produce
high accuracy for both distinct classifiers for these
subtasks as well as for the complete coreference
system.
In the remainder: Section 2, briefly describes
the basic kernel functions that we used; Section
3 illustrates our new kernels for expletive, binding
and name alias detection along with a coreference
context kernel; Section 4 reports the experiments
on individual classifiers on expletives, binding and
names whereas Section 5 shows the results on the
complete coreference task; Finally, Section 6 de-
rives the conclusions.
2 Kernel for Structured Data
We used three kernel functions in this work: the
String Kernel (SK) proposed in Shawe-Taylor and
Cristianini (2004) to evaluate the number of sub-
sequences between two sequences, the Syntactic
Tree Kernel (STK; see Collins and Duffy 2002)
which computes the number of syntactic tree frag-
ments and the Partial Tree Kernel (PTK; see Mos-
chitti 2006) which provides a more general repre-
sentation of trees in terms of tree fragments. We
discuss each in turn.
2.1 String Kernels (SK)
The string kernels that we consider count the num-
ber of substrings shared by two sequences contain-
ing gaps, i.e. some of the characters of the original
NP 
D N 
a 
  cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a 
   cat 
  cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
Figure 1: A tree with some of its STFs .
NP 
D N 
VP 
V 
brought 
a 
   cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
a 
   cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
Figure 2: A tree with some of its PTFs.
string are skipped. Gaps penalize the weight asso-
ciated with the matched substrings. More in detail,
(a) longer subsequences receive lower weights.
(b) Valid substrings are sequences of the original
string with some characters omitted, i.e. gaps. (c)
Gaps are accounted by weighting functions and (d)
symbols of a string can also be whole words, i.e.
the word sequence kernel Cancedda et al (2003).
2.2 Tree Kernels
The main idea underlying tree kernels is to com-
pute the number of common tree fragments be-
tween two trees without explicitly considering the
whole fragment space. The type of fragments char-
acterize different kernel functions. We consider
syntactic tree fragments (STFs) and partial tree
fragments (PTFs)
2.2.1 Syntactic Tree Kernels (STK)
An STF is a connected subset of the nodes and
edges of the original tree, with the constraint that
any node must have all or none of its children. This
is equivalent to stating that the production rules
contained in the STF cannot be partial. For ex-
ample, Figure 1 shows a tree with its PTFs: [VP [V
NP]] is an STF, [VP [V]] or [VP [NP]] are not STFs.
2.2.2 Partial Tree Kernel (PTK)
If we relax the production rule constraint over
the STFs, we obtain a more general substructure
type, i.e. PTF, generated by the application of par-
tial production rules, e.g. Figure 2 shows that [VP
[NP[D]]] is indeed a valid fragment. Note that
PTK can be seen as a STK applied to all possible
child sequences of the tree nodes, i.e. a string ker-
nel combined with a STK.
2.3 Kernel Engineering
The Kernels of previous section are basic functions
that can be applied to feature vectors, strings and
962
trees. In order to make them effective for a specific
task, e.g. for coreference resolution: (a) we can
combine them with additive or multiplicative op-
erators and (b) we can design specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
It is worth noting that a basic kernel applied to
an innovative view of a structure yields a new ker-
nel (e.g. Moschitti and Bejan (2004); Moschitti
et al (2006)), as we show below:
Let K(t
1
, t
2
) = ?(t
1
) ? ?(t
2
) be a basic ker-
nel, where t
1
and t
2
are two trees. If we map t
1
and t
2
into two new structures s
1
and s
2
with a
mapping ?
M
(?), we obtain: K(s
1
, s
2
) = ?(s
1
) ?
?(s
2
) = ?(?
M
(t
1
)) ? ?(?
M
(t
2
)) = ?
?
(t
1
) ?
?
?
(t
2
)=K?(t
1
, t
2
), which is a noticeably different
kernel induced by the mapping ?? = ? ? ?
M
.
3 Kernels for Coreference Resolution
In this paper we follow the standard learning ap-
proach to coreference developed by Soon et al
(2001) and also used the few variants in Ng and
Cardie (2002). In this framework, training and
testing instances consist of a pair (anaphor, an-
tecedent). During training, a positive instance is
created for each anaphor encountered by pairing
the anaphor with its closest antecedent; each of the
non-coreferential mentions between anaphor and
antecedent is used to produce a negative instance.
During resolution, every mention to be resolved is
paired with each preceding antecedent candidate
to form a testing instance. This instance is pre-
sented to the classifier which then returns a class
label with a confidence value indicating the likeli-
hood that the candidate is the antecedent.
The nearest candidate with a positive classifica-
tion will be selected as the antecedent of the pos-
sible anaphor. The crucial point is that in this ap-
proach, the classifier is trained to identify positive
and negative instances of the resolution process. In
previous work on using kernel functions for coref-
erence (Yang et al 2006), structural information
in the form of tree features was included in the
instances. This approach is appropriate for iden-
tifying contexts in which the binding constraints
apply, but not, for instance, to recognize exple-
tives. In this work we adopted therefore a more
general approach, in which separate classifiers are
used to recognize each relevant configuration, and
their output is then used as an input to the coref-
erence classifier. In this section we discuss the
types of structures and kernel functions we used
for three different kinds of classifiers: expletive,
binding and alias classifiers. We then present the
results of these classifiers, and finally the results
with the coreference resolver as a whole.
3.1 Expletive Kernels
In written text, about a third of the occurrences
of the pronoun it are not coreferent to a previ-
ous mention, but either refer to a general discourse
topic (it?s a shame) or do not refer at all, as in the
case of extraposed subjects (it is thought that . . . )
or weather verbs (it?s raining). It is desirable to
minimize the impact that these non-anaphoric pro-
nouns have on the accuracy of a anaphora resolu-
tion: Lappin and Leass (1994), for example, use
several heuristics to filter out expletive pronouns,
including a check for patterns including modal ad-
jectives (it is good/necessary/. . . that . . . ), and cog-
nitive verbs (it is thought/believed/. . . that . . . ).
Newer approaches to the problem use machine-
learning on hand-annotated examples: Evans
(2001) compares a shallow approach based on
surrounding lemmas, part-of-speech tags, and the
presence of certain elements such as modal adjec-
tives and cognitive verbs, trained on 3171 exam-
ples from Susanne and the BNC to a reimplemen-
tation of a pattern-based approach due to Paice and
Husk (1987) and finds that the shallower machine-
learning approach compares favorably to it. Boyd
et al (2005) use an approach that combines some
of Evans? shallow features with hand-crafted pat-
terns in a memory based learning approach and
find that the more informative features are ben-
eficial for the system?s performance (88% accu-
racy against 71% for their reimplementation using
Evans? shallow features).
Evans? study also mentions that incorporating
the expletive classifier as a filter for a pronoun re-
solver gives a gain between 2.86% (for manually
determined weights) and 1% (for automatically op-
timized weights).
Tree kernels are a good fit for expletive classi-
fication since they can naturally represent the lex-
ical and structural context around a word. Our fi-
nal classifier uses the combination of an unmodi-
fied tree (UT) (where the embedding clause or verb
phrase of the pronoun is used as a tree), and a tree
that only preserves the most salient structural fea-
tures (ST).
The reduced representation prunes all nodes that
963
would not be seen as indicative in a pattern ap-
proach, essentially keeping verb argument struc-
ture and important lexical items, such as the gov-
erning verb and, in the case of copula construc-
tions, the predicate. For example, the phrase
(S (NP (PRP It))
(VP (VBZ has)
(NP (NP (DT no) (NN bearing))
(PP (IN on)
(NP (NP (PRP$ our)
(NN work)
(NN force))
(NP (NN today)))))
(. .))
would be reduced to the ST:
(S-I (NP-I (PRP-I It))
(VP (VBX have)
(NP))
(.))
or, in a similar fashion,
(S (NP (PRP it))
(VP (VBZ ?s)
(NP (NP (NN time))
(PP (IN for)
(NP (PRP$ their)
(JJ biannual)
(NN powwow))))))
would just be represented as the ST:
(S-I (NP-I (PRP-I it))
(VP (BE VBZ)
(NP-PRD (NN time))))
3.2 Binding Kernels
The resolution of pronominal anaphora heavily re-
lies on the syntactic information and relationships
between the anaphor and the antecedent candi-
dates, including binding and other constraints, but
also context-induced preferences in sub-clauses.
Some researchers (Lappin and Leass 1994;
Kennedy and Boguraev 1996) use manually de-
signed rules to take into account the grammati-
cal role of the antecedent candidates as well as
the governing relations between the candidate and
the pronoun, while others use features determined
over the parse tree in a machine-learning approach
(Aone and Bennett 1995; Yang et al 2004; Luo
and Zitouni 2005). However, such a solution has
limitations, since the syntactic features have to be
selected and defined manually, and it is still partly
an open question which syntactic properties should
be considered in anaphora resolution.
We follow (Yang et al 2006; Iida et al 2006) in
using a tree kernel to represent structural informa-
tion using the subtree that covers a pronoun and its
antecedent candidate. Given a sentence like ?The
Figure 3: The structure for binding detection for
the instance inst(?the man?, ?him?) in the sentence
?the man in the room saw him?
man in the room saw him.?, we represent the syn-
tactic relation between ?The man? and ?him?, by
the shortest node path connecting the pronoun and
the candidate, along with the first-level of the node
children in the path.
Figure 3 graphically shows such tree highlighted
with dash lines. More in detail we operate the fol-
lowing tree transformation:
(a) To distinguish from other words, we explic-
itly mark up in the structured feature the pronoun
and the antecedent candidate under consideration,
by appending a string tag ?ANA? and ?CANDI?
in their respective nodes, i.e. ?NN-CANDI? for
?man? and ?PRP-ANA? for ?him?.
(b) To reduce the data sparseness, the leaf nodes
representing the words are not incorporated in the
feature, except that the word is the word node of
the ?DET? type (this is to indicate the lexical prop-
erties of an expression, e.g., whether it is a definite,
indefinite or bare NP).
(c) If the pronoun and the candidate are not in the
same sentence, we do not include the nodes denot-
ing the sentences (i.e., ?S? nodes) before the can-
didate or after the pronoun.
The above tree structures will be jointly used
with the basic STK which extracts tree fragments
able to characterize the following information: (a)
the candidate is post-modified by a preposition
phrase, (the node ?PP? for ?in the room? is in-
cluded), (b) the candidate is a definite noun phrase
(the article word ?the? is included), (c) the candi-
date is in a subject position (NP-S-VP structure),
(d) the anaphor is an object of a verb (the node
?VB? for ?saw? is included) and (e) the candidate
is c-commanding the anaphor (the parent of the
NP node for ?the main in the room? is dominat-
ing the anaphor (?him?), which are important for
reference determination in the pronoun resolution.
964
3.3 Encoding Context via Word Sequence
Kernel
The previous structures aim at describing the in-
teraction between one referential and one referent;
if such interaction is observed on another mention
pair, an automatic algorithm can establish if they
corefer or not. This kind of information is the most
useful to characterize the target problem, however,
the context in which such interaction takes place is
also very important. Indeed, natural language pro-
poses many exceptions to linguistic rules and these
can only be detect by looking at the context. To be
able to represent context words or phrases, we use
context word windows around the mentions and
the subsequence kernel function (see section 2.1)
to extract many features from it.
For example, in the context of ?and so Bill
Gates says that?, a string kernel would ex-
tract features including: Bill Gates says that,
says that, Gates, Gates says that, Bill says that,
so Gates says that, and so that and so on.
Name Alias
BJ Habibie Mr. Habibie
Federal Express Fedex
Ju Rong Zhi Ju
Table 1: Examples of coreferent named entities
(aliases) taken from the MUC 6 corpus.
3.4 Kernels for Alias Resolution
Most methods currently employed by coreference
resolution (CR) systems for identifying coreferent
named entities, i.e. aliases, are fairly simplistic in
nature, relying on simple surface features such as
the edit distance between two strings representing
names. We investigate the potential of using the
structure contained within names. This can be very
useful to solve complex cases like those shown in
Table 1, taken from the MUC 6 corpus (Chinchor
and Sundheim 2003). For this purpose, we add
syntactic information to the feature set by tagging
the parts of a name (e.g. first name, last name, etc.)
as illustrated in Figure 4.
To automatically extract such structure we used
the High Accuracy Parsing of Name Internal Struc-
ture (HAPNIS) script1. HAPNIS takes a name as
input and returns a tagged name like what is shown
in Figure 4. It uses a series of heuristics in making
its classifications based on information such as the
1The script is freely available at
http://www.cs.utah.edu/ hal/HAPNIS/.
Figure 4: A proper name labeled with syntactic in-
formation.
serial positions of tokens in a name, the total num-
ber of tokens, the presence of meaningful punctua-
tion such as periods and dashes, as well as a library
of common first names which can be arbitrarily ex-
tended to any size. The tag set consists of the fol-
lowing: surname, forename, middle, link, role, and
suffix2.
Once the structure for a name has been de-
rived, we can apply tree kernels to represent it in
the learning algorithms thus avoiding the manual
feature design. Such structures are not based on
any particular grammar, therefore, any tree sub-
part may be relevant. In this case the most suitable
kernel is PTK, which extracts any tree subpart. It
is worth to note that the name tree structure can
be improved by inserting a separate node for each
name character and exploiting the string matching
approximation carried out by PTK. For example,
Microsoft Inc. will have a large match with Mi-
crosoft Incorporated whereas the standard string
matching would be null.
4 Experiments with Coreference Subtask
Classifiers
In these experiments we test the kernels devised for
expletive (see Section 3.1), binding (see Section
3.2) and alias detection (see Section 3.4), to study
the level of accuracy reachable by our kernel-based
classifiers. The baseline framework is constituted
by SVMs along with a polynomial kernel over the
Soon et al?s features.
4.1 Experiments on Expletive Classification
We used the BBN Pronoun corpus3 as a source of
examples, with the training set consisting of sec-
tions 00-19, yielding more than 5800 instances of
2Daume? reports a 99.1% accuracy rate on his test data set.
We therefore concluded that it was sufficient for our purposes.
3Ralph Weischedel and Ada Brunstein (2005): BBN Pro-
noun Coreference and Entity Type Corpus, LDC2005T33
965
it, with the testing set consisting of sections 20 and
21, using the corresponding parses from the Penn
Treebank for the parse trees. Additionally, we re-
port on the performance of the classifier learnt on
only the first 1000 instances to verify that our ap-
proach also works for small datasets. The results
in Table 2 show that full tree (UT) achieves good
results whereas the salient tree (ST) leads to a bet-
ter ability to generalize, and the combination ap-
proach outperforms both individual trees.
BBN large BBN small
Prec Recl Acc Prec Recl Acc
UT 83.87 61.54 84.35 78.76 52.66 80.85
ST 78.08 67.46 83.98 77.61 61.54 82.50
UT+ST 81.12 68.64 85.27 80.74 64.50 84.16
Table 2: Results for kernel-based expletive detec-
tion (using STK)
Note that the accuracy we get by training on
1000 examples (84% accuracy; see the small col-
umn in Table 2) is better than Boyd?s replication of
Evans (76% accuracy) or their decision tree clas-
sifier (81% accuracy) even though Boyd et al?s
dataset is three times bigger. On the other hand,
Boyd et als full system, which uses substantial
hand-crafted knowledge, gets a still better result
(88% accuracy), which is also higher than the ac-
curacy of our classifier even when trained on the
full 5800 instances.
MUC-6
Prec Recl F
Soon et al 51.25 55.51 53.29
STK 71.93 55.41 62.59
Table 3: Binding classifier: coreference classifica-
tion on same-sentence pronouns
4.2 Experiments with the Binding Classifier
To assess the effect of the binding classifier on
same-sentence pronoun links, we extracted 1398
mention pairs from the MUC-6 training data where
both mentions were in the same sentence and at
least one item of the pair included a pronoun, us-
ing the first 1000 for training and the remaining
398 examples for testing. The results (see Table 3)
show that the syntactic tree kernel (STK) consider-
ably improves the precision of classification of the
Soon et al?s features.
4.3 Experiments on Alias Classification
For our preliminary experiments, we extracted
only pairs in the MUC 6 testing set in which both
mentions were proper names, as determined by
the coreference resolver?s named entity recognizer.
This set of proper names contained about 37,000
pairs of proper names of which about 600 were
positive instances. About 5,500 pairs were ran-
domly selected as test instances and the rest were
used for training.
In the first experiment, we trained a decision
tree classifier to detect if two names are aliases.
For this task, we used either the string kernel score
over the sequence of characters or the edit distance.
The results in Table 4 show that the string kernel
score performs better by 21.6 percentage points in
F-measure.
In the second experiments we used SVMs
trained with the string kernel over the name-
character sequences and with PTK, which takes
into account the structure of names. The re-
sults in Table 5 show that the structure improves
alias detection by almost 5 absolute percent points.
This suggests that an effective coreference sys-
tem should embed PTK and name structures in the
coreference pair representation.
Recall Precision F-measure
String kernel 49.5% 60.8% 54.6%
Edit distance 23.9% 53.1% 33.0%
Table 4: Decision-tree based classification of name
aliases using string kernels and edit distance.
Recall Precision F-measure
String kernel 58.4% 67.5% 62.6%
PTK 64.8% 70.0% 67.3%
Table 5: SVM-based classification of name aliases
using string kernels and tree-based feature.
5 Experiments on Coreference Systems
In this section we evaluate the contribution in the
whole coreference task of the expletive classifier
and the binding kernel. The predictions of the for-
mer are used as a feature of our basic coreference
system whereas the latter is used directly in the
coreference classifier by adding it to the polyno-
mial kernel of the basic system.
Our basic system is based on the standard learn-
ing approach to coreference developed by Soon
et al (2001). It uses the features from Soon et
al?s work, including lexical properties, morpho-
logic type, distance, salience, parallelism, gram-
matical role and so on. The main difference with
966
Soon et al (2001) is the use of SVMs along with a
polynomial kernel.
MUC-6
Prec Recl F
plain 65.2 66.9 66.0
plain+expletive 66.1 66.9 66.5
upper limit 70.0 66.9 68.4
Table 6: Expletive classification: influence on pro-
noun resolution
5.1 Influence of Expletive classification
To see how useful a classifier for expletives can
be, we conducted experiments using the expletive
classifier learned on the BBN pronoun corpus on
the MUC-6 corpus. Preliminary experiments indi-
cated that perfect detection of expletives (i.e. using
gold standard annotation) could raise the precision
of pronoun resolution from 65.2% to 70.0%, yield-
ing a 2.4% improvement in the F-score for pronoun
resolution alone, or 0.6% improvement in the over-
all coreference F-score (see Table 6).
For a more realistic assessment, we used the
classifier learned on the BBN pronoun corpus ex-
amples as an additional feature to gauge the im-
provement that could be achieved using it. While
the gain in precision is small even in comparison
to the achievable error reduction, we need to keep
in mind that our baseline is in fact a well-tuned
system.
MUC-6 ACE02-BNews
R P F R P F
PK 64.3 63.1 63.7 58.9 68.1 63.1
PK+TK 65.2 80.1 71.9 65.6 69.7 67.6
Table 7: Results of the pronoun resolution
5.2 Binding and Context Kernels
In these experiments, we compared our corefer-
ence system based on Polynomial Kernel (PK)
against its combinations with Syntactic Tree Ker-
nels (STK) over the binding structures (Sec. 3.2)
and Word Sequence Kernel (WSK) on context
windows (Sec. 3.3). We experimented with
both the only pronoun and the complete corefer-
ence resolution tasks on the standard MUC-6 and
ACE03-BNews data sets.
On the validation set, the best kernel combina-
tion between PK and STK was STK(T1, T2) ?
PK(~x
1
, ~x
2
)+PK(~x
1
, ~x
2
). Then an improvement
arises when simply summing WSK.
Table 7 lists the results for the pronoun resolu-
tion. We used PK on the Soon et al?s features as
the baseline. On MUC-6, the system achieves a
recall of 64.3% and precision 63.1% and an over-
all F-measure of 63.7%. On ACE02-BNews, the
recall is lower 58.9% but the precision is higher,
i.e. 68.1%, for a resulting F-measure of 63.1%.
In contrast, adding the binding kernel (PK+STK)
leads to a significant improvement in 17% preci-
sion for MUC-6 with a small gain (1%) in recall,
whereas on the ACE data set, it also helps to in-
crease the recall by 7%. Overall, we can see an
increase in F-measure of around 8% for MUC and
4.5% for ACE02-BNews. These results suggest
that the structured feature is very effective for pro-
noun resolution.
MUC-6 ACE02-BNews
R P F R P F
PK 61.5 67.2 64.2 54.8 66.1 59.9
PK+STK 63.4 67.5 65.4 56.6 66.0 60.9
PK+STK+WSK 64.4 67.8 66.0 57.1 65.4 61.0
Table 8: Results of the coreference resolution
Table 8 lists the results on the coreference res-
olution. We note that adding the structured fea-
ture to the polynomial kernel, i.e. using the model
PK+STK, improves the recall of 1.9% for MUC-
6 and 1.8% for ACE-02-BNews and keeps invari-
ant the precision. Compared to pronoun resolu-
tion, the improvement of the overall F-measure is
smaller (about 1%). This occurs since the resolu-
tion of non-pronouns case does not require a mas-
sive use of syntactic knowledge as in the pronoun
resolution problem. WSK further improves the
system?s F1 suggesting that adding structured fea-
tures of different types helps in solving the coref-
erece task.
6 Conclusions
We presented four examples of using kernel-based
methods to take advantage of a structured repre-
sentation for learning problems that arise in coref-
erence systems, presenting high-accuracy classi-
fiers for expletive detection, binding constraints
and same-sentence pronoun resolution, and name
alias matching. We have shown the accuracy
of the individual classifiers for the above tasks
and the impact of expletives and binding classi-
fiers/kernels in the complete coreference resolu-
tion system. The improvement over the individual
and complete tasks suggests that kernel methods
967
are a promising research direction to achieve state-
of-the-art coreference resolution systems.
Future work is devoted on making the use of ker-
nels for coreference more efficient since the size of
the ACE-2 corpora prevented us to directly use the
combination of all kernels that we designed. In this
paper, we have also studied a solution which re-
lates to factoring out decisions into separate clas-
sifiers and using the decisions as binary features.
However, this solution shows some loss in terms of
accuracy. We are currently investigating methods
that allow us to combine the accuracy and flexibil-
ity of the integrated approach with the speed of the
separate classifier approach.
Acknowledgements Y. Versley was funded by the
Deutsche Forschungsgemeinschaft as part of SFB (Collabora-
tive Research Centre) 441. A. Moschitti has been partly sup-
ported by the FP6 IST LUNA project (contract No. 33549).
Part of the work reported in this paper was done at the Johns
Hopkins Summer Workshop in 2007, funded by NSF and
DARPA. We are especially grateful for Alan Jern?s implemen-
tation help for name structure identification.
References
Aone, C. and Bennett, S. W. (1995). Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proc. ACL 1995, pages 122?129.
Barzilay, R. and Lapata, M. (2005). Modelling local coher-
ence: An entity-based approach. In Proc. of ACL, Ann
Arbor, MI.
Boyd, A., Gegg-Harrison, W., and Byron, D. (2005). Iden-
tifying non-referential it: a machine learning approach in-
corporating linguistically motivated features. In Proc. ACL
WS on Feature Engineering for Machine Learning in Nat-
ural Language Processing.
Cancedda, N., Gaussier, E., Goutte, C., and Renders, J. M.
(2003). Word sequence kernels. JMLR, 3:1059?1082.
Chinchor, N. and Sundheim, B. (2003). Muc 6 corpus. Mes-
sage Understanding Conference (MUC) 6.
Chomsky, N. (1981). Lectures on government and binding.
Foris, Dordrecht, The Netherlands.
Collins, M. and Duffy, N. (2002). New ranking algorithms for
parsing and tagging: kernels over discrete structures and
the voted perceptron. In Proc. ACL 2002, pages 263?270.
Evans, R. (2001). Applying machine learning toward an au-
tomatic classification of it. Literary and Linguistic Com-
puting, 16(1):45?57.
Giuglea, A.-M. and Moschitti, A. (2006). Semantic role la-
beling via framenet, verbnet and propbank. In Proceedings
of Coling-ACL, Sydney, Australia.
Grosz, B., Joshi, A., and Weinstein, S. (1995). Centering: a
framework for modeling the local coherence of discourse.
CL, 21(2):203?225.
Hobbs, J. (1978). Resolving pronoun references. Lingua,
44:339?352.
Hobbs, J. (1979). Resolving pronoun references. Coherence
and Coreference, 3(1):67?90.
Iida, R., Inui, K., and Matsumoto, Y. (2006). Exploiting syn-
tactic patterns as clues in zero-anaphora resolution. In
Proc. Coling/ACL 2006, pages 625?632.
Kennedy, C. and Boguraev, B. (1996). Anaphora for every-
one: pronominal anaphora resolution without a parser. In
Proc. Coling 1996.
Lappin, S. and Leass, H. (1994). An algorithm for pronominal
anaphora resolution. CL, 20(4):525?561.
Luo, X. and Zitouni, I. (2005). Multi-lingual coreference res-
olution with syntactic features. In Proc. HLT/EMNLP 05.
McCarthy, J. and Lehnert, W. (1995). Using decision trees for
coreference resolution. In Proc. IJCAI 1995.
Mitkov, R. (2002). Anaphora resolution. Longman.
Moschitti, A. (2006). Efficient convolution kernels for depen-
dency and constituent syntactic trees. Proc. ECML 2006.
Moschitti, A. and Bejan, C. A. (2004). A semantic kernel for
predicate argument classification. In CoNLL-2004, USA.
Moschitti, A., Pighin, D., and Basili, R. (2006). Semantic
Role Labeling via Tree Kernel Joint Inference. In Pro-
ceedings of CoNLL-X.
Moschitti, A., Quarteroni, S., Basili, R., and Manandhar, S.
(2007). Exploiting syntactic and shallow semantic kernels
for question answer classification. In Proceedings ACL,
Prague, Czech Republic.
Ng, V. and Cardie, C. (2002). Improving machine learning
approaches to coreference resolution. In Proc. ACL 2002.
Paice, C. D. and Husk, G. D. (1987). Towards an automatic
recognition of anaphoric features in english text: The im-
personal pronoun ?it?. Computer Speech and Language,
2:109?132.
Shawe-Taylor, J. and Cristianini, N. (2004). Kernel Methods
for Pattern Analysis. Cambridge University Press.
Sidner, C. (1979). Toward a computational theory of definite
anaphora comprehension in english. Technical report AI-
TR-537, MIT, Cambridge, MA.
Soon, W., Ng, H., and Lim, D. (2001). A machine learning
approach to coreference resolution of noun phrases. CL,
27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K.
(2007). Two uses of anaphora resolution in summarization.
Information Processing and Management, 43:1663?1680.
Special issue on Summarization.
Sturt, P. (2003). The time-course of the application of binding
constraints in reference resolution. Journal of Memory and
Language.
Vapnik, V. (1995). The Nature of Statistical Learning Theory.
Springer.
Vieira, R. and Poesio, M. (2000). An empirically based sys-
tem for processing definite descriptions. CL, 27(4):539?
592.
Yang, X., Su, J., and Tan, C. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
COLING-ACL 06.
Yang, X., Su, J., Zhou, G., and Tan, C. (2004). Improving pro-
noun resolution by incorporating coreferential information
of candidates. In Proc. ACL 2004.
Zanzotto, F. M. and Moschitti, A. (2006). Automatic learn-
ing of textual entailments with cross-pair similarities. In
Proceedings of Coling-ACL, Sydney, Australia.
Zelenko, D., Aone, C., and Richardella, A. (2003). Kernel
methods for relation extraction. JMLR, 3(6):1083 ? 1106.
968
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111?120,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Reverse Engineering of Tree Kernel Feature Spaces
Daniele Pighin
FBK-Irst, HLT
Via di Sommarive, 18 I-38100 Povo (TN) Italy
pighin@fbk.eu
Alessandro Moschitti
University of Trento, DISI
Via di Sommarive, 14 I-38100 Povo (TN) Italy
moschitti@disi.unitn.it
Abstract
We present a framework to extract the
most important features (tree fragments)
from a Tree Kernel (TK) space according
to their importance in the target kernel-
based machine, e.g. Support Vector Ma-
chines (SVMs). In particular, our min-
ing algorithm selects the most relevant fea-
tures based on SVM estimated weights
and uses this information to automatically
infer an explicit representation of the in-
put data. The explicit features (a) improve
our knowledge on the target problem do-
main and (b) make large-scale learning
practical, improving training and test time,
while yielding accuracy in line with tradi-
tional TK classifiers. Experiments on se-
mantic role labeling and question classifi-
cation illustrate the above claims.
1 Introduction
The last decade has seen a massive use of Support
Vector Machines (SVMs) for carrying out NLP
tasks. Indeed, their appealing properties such as
1) solid theoretical foundations, 2) robustness to
irrelevant features and 3) outperforming accuracy
have been exploited to design state-of-the-art lan-
guage applications.
More recently, kernel functions, which im-
plicitly represent data in some high dimensional
space, have been employed to study and fur-
ther improve many natural language systems, e.g.
(Collins and Duffy, 2002), (Kudo and Matsumoto,
2003), (Cumby and Roth, 2003), (Cancedda et al,
2003), (Culotta and Sorensen, 2004), (Toutanova
et al, 2004), (Kazama and Torisawa, 2005), (Shen
et al, 2003), (Gliozzo et al, 2005), (Kudo et al,
2005), (Moschitti et al, 2008), (Diab et al, 2008).
Unfortunately, the benefit to easily and effectively
model the target linguistic phenomena is reduced
by the the implicit nature of the kernel space,
which prevents to directly observe the most rele-
vant features. As a consequence, even very accu-
rate models generally fail in providing useful feed-
back for improving our understanding of the prob-
lems at study. Moreover, the computational bur-
den induced by high dimensional kernels makes
the application of SVMs to large corpora still more
problematic.
In (Pighin and Moschitti, 2009), we proposed a
feature extraction algorithm for Tree Kernel (TK)
spaces, which selects the most relevant features
(tree fragments) according to the gradient compo-
nents (weight vector) of the hyperplane learnt by
an SVM, in line with current research, e.g. (Rako-
tomamonjy, 2003; Weston et al, 2003; Kudo and
Matsumoto, 2003). In particular, we provided al-
gorithmic solutions to deal with the huge dimen-
sionality and, consequently, high computational
complexity of the fragment space. Our experimen-
tal results showed that our approach reduces learn-
ing and classification processing time leaving the
accuracy unchanged.
In this paper, we present a new version of such
algorithm which, under the same parameteriza-
tion, is almost three times as fast while produc-
ing the same results. Most importantly, we ex-
plored tree fragment spaces for two interesting
natural language tasks: Semantic Role Labeling
(SRL) and Question Classification (QC). The re-
sults show that: (a) on large data sets, our ap-
proach can improve training and test time while
yielding almost unaffected classification accuracy,
and (b) our framework can effectively exploit the
ability of TKs and SVMs to, respectively, gener-
ate and recognize relevant structured features. In
particular, we (i) study in more detail the relevant
fragments identfied for the boundary classification
task of SRL, (ii) closely observe the most relevant
fragments for each QC class and (iii) look at the di-
verse syntactic patterns characterizing each ques-
111
tion category.
The rest of the paper is structured as follows:
Section 2 will briefly review SVMs and TK func-
tions; Section 3 will detail our proposal for the lin-
earization of a TK feature space; Section 4 will
review previous work on related subjects; Section
5 will detail the outcome of our experiments, and
Section 6 will discuss some relevant aspects of the
evaluation; finally, in Section 7 we will draw our
conclusions.
2 Tree Kernel Functions
The decision function of an SVM is:
f(~x) = ~w ? ~x + b =
n
?
i=1
?
i
y
i
~x
i
? ~x + b (1)
where ~x is a classifying example and ~w and b are
the separating hyperplane?s gradient and its bias,
respectively. The gradient is a linear combination
of the training points ~x
i
, their labels y
i
and their
weights ?
i
. Applying the so-called kernel trick it
is possible to replace the scalar product with a ker-
nel function defined over pairs of objects:
f(o) =
n
?
i=1
?
i
y
i
k(o
i
, o) + b
with the advantage that we do not need to provide
an explicit mapping ?(?) of our examples in a vec-
tor space.
A Tree Kernel function is a convolution ker-
nel (Haussler, 1999) defined over pairs of trees.
Practically speaking, the kernel between two trees
evaluates the number of substructures (or frag-
ments) they have in common, i.e. it is a measure
of their overlap. The function can be computed re-
cursively in closed form, and quite efficient imple-
mentations are available (Moschitti, 2006). Dif-
ferent TK functions are characterized by alterna-
tive fragment definitions, e.g. (Collins and Duffy,
2002) and (Kashima and Koyanagi, 2002). In the
context of this paper we will be focusing on the
SubSet Tree (SST) kernel described in (Collins
and Duffy, 2002), which relies on a fragment defi-
nition that does not allow to break production rules
(i.e. if any child of a node is included in a frag-
ment, then also all the other children have to). As
such, it is especially indicated for tasks involving
constituency parsed texts.
Implicitly, a TK function establishes a corre-
spondence between distinct fragments and dimen-
sions in some fragment space, i.e. the space of all
Fragment space
A
B A
A
B A
B A
A
B A
C
A
B A
B A
C
A
C
D
B A
D
B A
C
1 2 3 4 5 6 7
T1
A
B A
B A
C
T2
D
B A
C
?(T1) = [2, 1, 1, 1, 1, 0, 0]
?(T2) = [0, 0, 0, 0, 1, 1, 1]
K(T1, T2) = ??(T1), ?(T2)? = 1
Figure 1: Esemplification of a fragment space and
the kernel product between two trees.
the possible fragments. To simplify, a tree t can
be represented as a vector whose attributes count
the occurrences of each fragment within the tree.
The kernel between two trees is then equivalent to
the scalar product between pairs of such vectors,
as exemplified in Figure 1.
3 Linearization of a TK function
Our objective is to efficiently mine the most rele-
vant fragments from the huge fragment space, so
that we can explicitly represent our input trees in
terms of these fragments and learn fast and accu-
rate linear classifiers.
The framework defines five distinct activities,
detailed in the following paragraphs.
3.1 Kernel Space Learning (KSL)
The first step involves the generation of an approx-
imation of the whole fragment space, i.e. we can
consider only the trees that encode the most rele-
vant fragments. To this end, we can partition our
training data into S smaller sets, and use the SVM
and the SST kernel to learn S models. We will
only consider the fragments encoded by the sup-
port vectors of the S models. In the next stage, we
will use the SVM estimated weights to drive our
feature selection process.
Since time complexity of SVM training is ap-
proximately quadratic in the number of examples,
by breaking training data into smaller sets we
can considerably accelerate the process of filtering
trees and estimating support vector weights. Ac-
cording to statistical learning theory, being trained
on smaller subsets of the available data these mod-
els will be less robust with respect to the min-
imization of the empirical risk (Vapnik, 1998).
112
Algorithm 3.1: MINE MODEL(M,L, ?)
global maxexp
prev? ? ; CLEAR INDEX()
for each ??y, t? ?M
do
?
?
?
?
?
?
?
T
i
? ? ? y/?t?
for each n ? N
t
do
{
f ? FRAG(n) ; rel = ? ? T
i
prev? prev ? {f, rel}
PUT(f, rel)
best pr? BEST(L) ;
while true
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
next? ?
for each ?f, rel? ? prev if f ? best pr
do
?
?
?
?
?
?
?
?
?
?
X = EXPAND(f,maxexp)
rel exp? ? ? rel
for each frag ? X
do
{
temp = {frag, rel exp}
next? next ? temp
PUT(frag, rel exp)
best? BEST(L)
if not CHANGED()
then break
best pr ? best
prev? next
F
L
? best pr
return (F
L
)
Nonetheless, since we do not need to employ them
for classification (but just to direct our feature se-
lection process, as we will describe shortly), we
can accept to rely on sub-optimal weights. Fur-
thermore, research results in the field of SVM par-
allelization using cascades of SVMs (Graf et al,
2004) suggest that support vectors collected from
locally learnt models can encode many of the rel-
evant features retained by models learnt globally.
Henceforth, let M
s
be the model associated with
the s-th split, and F
s
the fragment space that can
describe all the trees in M
s
.
3.2 Fragment Mining and Indexing (FMI)
In Equation 1 it is possible to isolate the gradient
~w =
?
n
i=1
?
i
y
i
~x
i
, with ~x
i
= [x
(1)
i
, . . . , x
(N)
i
], N
being the dimensionality of the feature space. For
a tree kernel function, we can rewrite x(j)
i
as:
x
(j)
i
=
t
i,j
?
?(f
j
)
?t
i
?
=
t
i,j
?
?(f
j
)
?
?
N
k=1
(t
i,k
?
?(f
k
)
)
2
(2)
where: t
i,j
is the number of occurrences of the
fragment f
j
, associated with the j-th dimension of
the feature space, in the tree t
i
; ? is the kernel de-
cay factor; and ?(f
j
) is the depth of the fragment.
The relevance |w(j)| of the fragment f
j
can be
measured as:
|w
(j)
| =
?
?
?
?
?
n
?
i=1
?
i
y
i
x
(j)
i
?
?
?
?
?
=
?
?
?
?
n
i=1
?
i
y
i
t
i,j
?
?(f
j
)
?
?
?
?t
i
?
.
(3)
We fix a threshold L and from each model M
s
(learnt during KSL) we select the L most relevant
fragments, i.e. we build the set F
s,L
= ?
k
{f
k
} so
that:
|F
s,L
| = L and |w(k)| ? |w(i)|?f
i
? F \ F
s,L
.
To generate all the fragments encoded in a
model, we adopt the greedy strategy described in
Algorithm 3.1. Its arguments are: an SVM model
M represented as ??y, t? pairs, where t is a tree
structure; the threshold value L; and the kernel de-
cay factor ?.
The function FRAG(n) generates the smallest
fragment rooted in node n (i.e. for an SST kernel,
the fragment consisting of n and its direct chil-
dren). We call such fragment a base fragment. The
function EXPAND(f,maxexp) generates all the
fragments that can be derived from the fragment
f by expanding, i.e. including in the fragment the
direct children of some of its nodes. These frag-
ments are derived from f . The parameter maxexp
limits fragment proliferation by setting the maxi-
mum number of nodes which can be expanded in
a fragment expansion operation. For example, if
there are 10 nodes which can be expanded in frag-
ment f , then only the fragments where at most 3
of the 10 nodes are expanded will be generated by
a call to EXPAND(f, 3).
Every time we generate a fragment f , the func-
tion PUT(f, rel) saves the fragment along with its
relevance rel in an index. The index keeps track
of the cumulative relevance of a fragment, and its
implementation has been optimized for fast inser-
tions and spatial compactness.
A whole cycle of expansions is considered as
an iteration of the mining process: we take into
account all the fragments that have undergone k
expansions and produce all the fragments that re-
sult from a further expansion, i.e. all the fragments
expanded k + 1 times.
We keep iterating until we reach a stop crite-
rion, which we base on the threshold value L, i.e.
the limit on the number of fragments that we are
interested in mining from a model. During each it-
eration k+1, we only expand the best L fragments
identified during the previous iteration k. When
113
the iteration is complete we re-evaluate the set of
L best fragments in the index, and we stop only if
the worst of them, i.e. the L-th ranked fragment
at the step k + 1, and its score are the same as at
the end of the previous iteration. That is, we as-
sume that if none of the fragments mined during
the (k + 1)-th iteration managed to affect the bot-
tom of the pool of the L most relevant fragments,
then none of their expansions is likely to succeed.
In the algorithm, N
t
is the set of nodes of the tree
t; BEST(L) returns the L highest ranked fragments
in the index; CHANGED() verifies whether the bot-
tom of the L-best set has been affected by the last
iteration or not.
We call MINE MODEL(?) on each of the mod-
els M
s
that we learnt from the S initial splits. For
each model, the function returns the set of L-best
fragments in the model. The union of all the frag-
ments harvested from each model is then saved
into a dictionary D
L
which will be used by the next
stage.
3.2.1 Discussion on FMI algorithm
With respect to the algorithm presented in (Pighin
and Moschitti, 2009), the one presented here has
the following advantages:
? the process of building fragments is strictly
small-to-large: fragments that span n+1 lev-
els of the tree may be generated only after all
those spanning n levels;
? the threshold value L is a parameter of the
mining process, and it is used to prevent the
algorithm from generating more fragments
than necessary, thus making it more efficient;
? it has one less parameter (maxdepth) which
was used to force fragments to span at-most
a given number of levels. The new algorithm
does not need it since the maximum number
of iterations is implicitly set via L.
These differences result in improved efficiency for
the FMI stage. For example, on the data for the
boundary classification task (see Section 5), using
comparable parameters the old algorithm required
85 minutes to mine the most relevant fragments,
whereas the new one only takes 31, i.e. it is 2.74
times as fast.
3.3 Tree Fragment Extraction (TFX)
During this phase we actually linearize our data:
a file encoding label-tree pairs ?y
i
, t
i
? is trans-
formed to encode label-vector pairs ?y
i
, ~v
i
?. To
do so, we generate the fragment space of t
i
, us-
ing a variant of the mining algorithm described in
Algorithm 3.1, and encode in ~v
i
all and only the
fragments t
i,j
so that t
i,j
? D
L
. The algorithm
exploits labels and production rules found in the
fragments listed in the dictionary to generate only
the fragments that may be in the dictionary. For
example, if the dictionary does not contain a frag-
ment whose root is labeled N , then if a node N is
encountered during TFX neither its base fragment
nor its expansions are generated. The process is
applied to the whole training (TFX-train) and test
(TFX-test) sets. The fragment space is now ex-
plicit, as there is a mapping between the input vec-
tors and the fragments they encode.
3.4 Explicit Space Learning (ESL)
Linearized training data is used to learn a very fast
model by using all the available data and a linear
kernel.
3.5 Explicit Space Classification (ESC)
The linear model is used to classify linearized test
data and evaluate the accuracy of the resulting
classifier.
4 Previous work
A rather comprehensive overview of feature se-
lection techniques is carried out in (Guyon and
Elisseeff, 2003). Non-filter approaches for SVMs
and kernel machines are often concerned with
polynomial and Gaussian kernels, e.g. (Weston et
al., 2001) and (Neumann et al, 2005). Weston
et al (2003) use the ?
0
norm in the SVM opti-
mizer to stress the feature selection capabilities
of the learning algorithm. In (Kudo and Mat-
sumoto, 2003), an extension of the PrefixSpan al-
gorithm (Pei et al, 2001) is used to efficiently
mine the features in a low degree polynomial ker-
nel space. The authors discuss an approximation
of their method that allows them to handle high
degree polynomial kernels.
Suzuki and Isozaki (2005) present an embed-
ded approach to feature selection for convolution
kernels based on ?2-driven relevance assessment.
To our knowledge, this is the only published work
clearly focusing on feature selection for tree ker-
nel functions, and indeed has been one of the
major sources of inspiration for our methodol-
ogy. With respect to their work, the difference
114
in our approach is that we want to exploit the
SVM optimizer to select the most relevant fea-
tures instead of a relevance assessment measure
that moves from different statistical assumptions
than the learning algorithm.
In (Graf et al, 2004), an approach to SVM
parallelization is presented which is based on a
divide-et-impera strategy to reduce optimization
time. The idea of using a compact graph rep-
resentation to represent the support vectors of a
TK function is explored in (Aiolli et al, 2006),
where a Direct Acyclic Graph (DAG) is employed.
In (Moschitti, 2006; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b; Mos-
chitti et al, 2007), the SST kernel along with other
tree and combined kernels are employed for ques-
tion classification and semantic role labeling with
interesting results.
5 Experiments
We evaluated the capability of our model to ex-
tract relevant features on two data sets: the
CoNLL 2005 shared task on Semantic Role Label-
ing (SRL) (Carreras and Ma`rquez, 2005), and the
Question Classification (QC) task based on data
from the TREC 10 QA competition (Voorhees,
2001). The next sections will detail the setup and
outcome of the two sets of experiments.
All the experiments were run on a machine
equipped with 4 Intel R? Xeon R? CPUs clocked at
1.6 GHz and 4 GB of RAM. As a supervised learn-
ing framework we used SVM-Light-TK1, which
extends the SVM-Light optimizer (Joachims,
2000) with tree kernel support. For each classi-
fication task, we compare the accuracy of a vanilla
SST classifier against the corresponding linearized
SST classifier (SST
?
). For KSL and SST training
we used the default decay factor ? = 0.4. For
ESL, we use a non-normalized, linear kernel. No
further parametrization of the learning algorithms
is carried out. Indeed, our focus is on showing
that, under the same conditions, our linearized tree
kernel can be as accurate as the original kernel,
and choosing of parameters may just bias such
test.
5.1 Semantic Role Labeling
For our experiments on semantic role labeling we
used PropBank annotations (Palmer et al, 2005)
1http://disi.unitn.it/
?
moschitt/
Tree-Kernel.htm
S
NP
NNP
Mary
VP
VB
bought
NP
D
a
NN
cat
(A1)
(A0)
?
VP
VB-P
bought
NP
D-B
a
VP
VB-P
bought
NP-B
D
a
NN
cat
-1: BC +1: BC,A1
-1: A0,A2,A3,A4,A5
Figure 2: Examples of AST
m
structured features.
and automatic Charniak parse trees (Charniak,
2000) as provided for the CoNLL 2005 evaluation
campaign (Carreras and Ma`rquez, 2005). SRL can
be decomposed into two tasks: boundary detec-
tion, where the word sequences that are arguments
of a predicate word w are identified, and role clas-
sification, where each argument is assigned the
proper role. The former task requires a binary
Boundary Classifier (BC), whereas the second in-
volves a Role Multi-class Classifier (RM).
5.1.1 Setup
If the constituency parse tree t of a sentence s
is available, we can look at all the pairs ?p, n
i
?,
where n
i
is any node in the tree and p is the node
dominating w, and decide whether n
i
is an argu-
ment node or not, i.e. whether it exactly dominates
all and only the words encoding any of w?s argu-
ments. The objects that we classify are subsets
of the input parse tree that encompass both p and
n
i
. Namely, we use the AST
m
structure defined
in (Moschitti et al, 2008), which is the minimal
tree that covers all and only the words of p and n
i
.
In the AST
m
, p and n
i
are marked so that they can
be distinguished from the other nodes. An AST
m
is regarded as a positive example for BC if n
i
is an
argument node, otherwise it is considered a nega-
tive example. Positive BC examples can be used to
train an efficient RM: for each role r we can train
a classifier whose positive examples are argument
nodes whose label is exactly r, whereas negative
examples are argument nodes labeled r? 6= r. Two
AST
m
s extracted from an example parse tree are
shown in Figure 2: the first structure is a negative
example for BC and is not part of the data set of
RM, whereas the second is a positive instance for
BC and A1.
To train BC we used PropBank sections 1
through 6, extracting AST
m
structures out of the
first 1 million ?p, n
i
? pairs from the corresponding
parse trees. As a test set we used the 149,140 in-
stance collected from the annotations in Section
24. There are 61,062 positive examples in the
training set (i.e. 6.1%) and 8,515 in the test set
115
(i.e. 5.7%).
For RM we considered all the argument nodes
of any of the six PropBank core roles (i.e. A0,
. . . , A5) from all the available training sections,
i.e. 2 through 21, for a total of 179,091 train-
ing instances. Similarly, we collected 5,928 test
instances from the annotations of Section 24.
Columns Tr+ and Te+ of Table 1 show the num-
ber of positive training and test examples, respec-
tively, for BC and the role classifiers.
For all the linearized classifiers, we used 50
splits for the FMI stage and we set the threshold
value L = 50k and maxexp = 1 during FMI and
TFX. We did not validate these parameters, which
we know to be sub-optimal. These values were
selected during the development of the software
because, on a very small test bed, they resulted in
a responsive and accurate system.
We should point out that other experiments have
shown that linearization is very robust with re-
spect to parametrization: due to the huge num-
ber and variety of fragments in the TK space, dif-
ferent choices of the parameters result in differ-
ent explicit spaces and more or less efficient solu-
tions, but in most cases the final accuracy of the
linearized classifiers is affected only marginally.
For example, it could be expected that reducing
the number of splits during KSL would improve
the final accuracy of a linearized classifier, as the
weights used for FMI would then converge to the
global optimum. Instead, we have observed that
increasing the number of splits does not necessar-
ily decrease the accuracy of the linearized classi-
fier.
The evaluation on the whole SRL task using
the official CoNLL?05 evaluator was not carried
out because producing complete annotations re-
quires several steps (e.g. overlap resolution, OvA
or Pairwise combination of individual role classi-
fiers) that would shade off the actual impact of the
methodology on classification.
5.1.2 Results
The left side of Table 1 shows the distribution of
positive data points in the training and test sets of
each classifier. Columns SST and SST
?
compare
side by side the F
1
measure of the non-linearized
and linearized classifier for each class. The accu-
racy of the RM classifier is the percentage of cor-
rect class assignments.
We can see that the accuracy of linearized clas-
sifiers is always in line with vanilla SST, even
Data set Accuracy
Class Tr+ Te+ SST SST
?
BC 61,062 8,515 81.8 81.3
A0 60,900 2,014 91.6 91.1
A1 90,636 3,041 89.0 89.4
A2 21,291 697 73.1 73.0
A3 3,481 105 56.8 53.0
A4 2,713 69 69.1 67.9
A5 69 2 66.7 0.0
RM 87.8 87.8
Table 1: Number of positive training (Tr+) and test
(Te+) examples in the SRL dataset. Accuracy of
the non-linearized (SST) and linearized (SST
?
) bi-
nary classifiers (i.e. BC, A0, . . . A5) is F
1
measure.
Accuracy of RM is the percentage of correct class
assignments.
if the selected linearization parameters generate
a very rough approximation of the original frag-
ment space, generally consisting of billions of
fragments. BC
?
(i.e. the linearized BC) has an
F
1
of 81.3, just 0.5% less than BC, i.e. 81.8. Con-
cerning RM
?
, its accuracy is the same as the non
linearized classifier, i.e. 87.8.
We should consider that the linearization frame-
work can drastically improve the efficiency of
learning and classification when dealing with large
amounts of data. For a linearized classifier, we
consider training time to be the overall time re-
quired to carry out the following activities: KSL,
FMI, TFX on training data and ESL. Similarly,
we consider test time the time necessary to per-
form TFX on test data and ESC. Training BC took
more than two days of CPU time and testing about
4 hours, while training and testing the linearized
boundary classifier required only 381 and 25 min-
utes, respectively. That is, on the same amount
of data we can train a linearized classifier about
8 times as fast, and test it in about 1 tenth of the
time. Concerning RM, sequential training of the
6 models took 2,596 minutes, while testing took
27 minutes. The linearized role multi classifier re-
quired 448 and 24 minutes for training and test-
ing, respectively, i.e. training is about 5 times as
fast while testing time is about the same. If com-
pared with the boundary classifier, the improve-
ment in efficiency is less evident: indeed, the rel-
atively small size of the role classifiers data sets
limits the positive effect of splitting training data
into smaller chunks.
SRL fragment space. Table 3 lists the best frag-
ments identified for the Boundary Classifier. We
should remember that we are using AST
m
struc-
116
tures as input to our classifiers: nodes whose la-
bel end with ?-P? are predicate nodes, while nodes
whose label ends with ?-B? are candidate argu-
ment nodes.
All the most relevant fragments encode the min-
imum sub-tree encompassing the predicate and the
argument node. This kind of structured feature
subsumes several features traditionally employed
for explicit SRL models: the Path (i.e. the se-
quence of nodes connecting the predicate and the
candidate argument node), Phrase Type (i.e. the
label of the candidate argument node), Predicate
POS (i.e. the POS of the predicate word), Posi-
tion (i.e. whether the argument is to the left or to
the right of the predicate) and Governing Category
(i.e. the label of the common ancestor) defined
in (Gildea and Jurafsky, 2002).
The linearized model for BC contains about 160
thousand fragments. Of these, about 70 and 33
thousand encompass the candidate argument or the
predicate node, respectively. About 16 thousand
fragments contain both.
5.2 Question Classification
For question classification we used the data set
from the TREC 10 QA evaluation campaign2, con-
sisting of 5,500 training and 500 test questions.
5.2.1 Setup
Given a question, the QC task consists in selecting
the most appropriate expected answer type from a
given set of possibilities. We adopted the question
taxonomy known as coarse grained, which has
been described in (Zhang and Lee, 2003) and (Li
and Roth, 2006), consisting of six non overlap-
ping classes: Abbreviations (ABBR), Descrip-
tions (DESC, e.g. definitions or explanations), En-
tity (ENTY, e.g. animal, body or color), Human
(HUM, e.g. group or individual), Location (LOC,
e.g. cities or countries) and Numeric (NUM, e.g.
amounts or dates).
For each question, we generate the full parse
of the sentence and use it to train SST and (lin-
earized) SST
?
models. The automatic parses are
obtained with the Stanford parser3 (Klein and
Manning, 2003). We actually have only 5,483 sen-
tences in our training set, due to parsing issues
with a few of them.
2http://l2r.cs.uiuc.edu/cogcomp/Data/
QA/QC/
3http://nlp.stanford.edu/software/
lex-parser.shtml
Data set Accuracy
Class Tr+ Te+ SST SST
?
ABBR 89 9 80.0 87.5
DESC 1,164 138 96.0 94.5
ENTY 1,269 94 63.9 63.5
HUM 1,231 65 88.1 87.2
LOC 834 81 77.6 77.9
NUM 896 113 80.4 80.8
Overall 86.2 86.6
Table 2: Number of positive training (Tr+) and test
(Te+) examples in the QA dataset. Accuracy of
the non-linearized (SST) and linearized (SST
?
) bi-
nary classifiers is F
1
measure. Overall accuracy is
the percentage of correct class assignments.
The classifiers are arranged in a one-vs.-all
(OvA) configuration, where each sentence is a
positive example for one of the six classes, and
negative for the other five. Given the very small
size of the data set, we used S = 1 during KSL
for the linearized classifier (i.e. we didn?t parti-
tion training data). We carried out no validation of
the parameters, and we used maxexp = 4 and
L = 50k in order to generate a rich fragment
space.
5.2.2 Results
Table 2 shows the number of positive examples
in the training and test set of each individual bi-
nary classifiers. Columns SST and SST
?
compare
the F
1
measure of the vanilla and linearized classi-
fiers on the individual classes, and the accuracy of
the complete QC task (Row Overall) in terms of
percentage of correct class assignments. Also in
this case, we can notice that the accuracy of the
linearized classifiers is always in line with non-
linearized ones, e.g. 86.6 vs. 86.2 for the multi-
classifiers. These results are lower than those de-
rived in (Moschitti, 2006; Moschitti et al, 2007),
i.e. 88.2 and 90.4, respectively, where the param-
eters for each classifier were carefully optimized.
QC Fragment space. Tables from 4 to 9 list the
top fragments identified for each class 4.
As expected, for all the categories the domain
lexical information is very relevant. For example,
film, color, book, novel and sport for ENTY or
city, country, state and capital for LOC. Of the six
classes, ENTY (Table 6) is mostly characterized
by lexical features. Interestingly, function words,
which would have been eliminated by a pure In-
formation Retrieval approach (i.e. by means of
4Some categories show meaningful syntactic fragments
after the first 10, so for them we report more subtrees.
117
standard stop-list), are in the top positions, e.g.:
why and how for DESC, what for ENTY, who for
HUM, where for LOC and when for NUM. For the
latter, also how seems to be important suggesting
that features may strongly characterize more than
one given class.
Characteristic syntactic features appear in the
top positions for each class, for example: (VP (VB
(stand)) (PP)), which suggests that stand should
be followed by a prepositional phrase to character-
ize ABBR; or (NP (NP (DT) (NN (abbreviation)))
(PP)), which suggests that, to be in a relevant pat-
tern, abbreviation should be preceded by an article
and followed by a PP. Also, the syntactic struc-
ture is useful to differentiate the use of the same
important words, e.g. (SBARQ (WHADVP (WRB
(How))) (SQ) (.)) for DESC better characterizes
the use of how with respect to NUM, in which a
relevant use is (WHADJP (WRB (How)) (JJ)).
In (Moschitti et al, 2007) it was shown that the
use of TK improves QC of 1.2 percent points, i.e.
from 90.6 to 91.8: further analysis of these frag-
ments may help us to device compact, less sparse
syntactic features and design more accurate mod-
els for the task.
6 Discussion
The fact that our model doesn?t always improve
the accuracy of a standard SST model might be
related to the process of splitting training data and
employing locally estimated weights during FMI.
Concerning the experiments presented in this
paper, this objection might apply to the results on
SRL, where we used 50 splits to identify the most
relevant fragments, but not to those on QC, where
given the limited size of the data set we decided
not to split training data at all as explained in Sec-
tion 5.2. Furthermore, as we already discussed,
we have evidence that there is no direct correlation
between the number of splits used for KSL and
the accuracy of the resulting classifier. After all,
the optimization carried out during ESL is global,
and we can assume that, if we mined enough frag-
ments during FMI, than those actually retained by
the global linear model would be by and large the
same, regardless of the split configuration.
More in general, feature selection may give an
improvement to some learning algorithm but if it
can help SVMs is debatable, since its related the-
ory show that they are robust to irrelevant fea-
tures. In our specific case, we remove features
(ADJP(RB-B)(VBN-P))
(NP(VBN-P)(NNS-B))
(S(NP-B)(VP))
(VP(VBD-P(said))(SBAR))
(VP(VB-P)(NP-B))
(NP(VBG-P)(NNS-B))
(VP(VBD-P)(NP-B))
(VP(VBG-P)(NP-B))
(VP(VBZ-P)(NP-B))
(VP(VBN-P)(NP-B))
(VP(VBP-P)(NP-B))
(NP(NP-B)(VP))
(NP(VBG-P)(NN-B))
(S(S(VP(VBG-P)))(NP-B))
Table 3: Best fragments for SRL BC.
(NN(abbreviation))
(NP(DT)(NN(abbreviation)))
(NP(DT(the))(NN(abbreviation)))
(IN(for))
(VB(stand))
(VBZ(does))
(PP(IN))
(VP(VB(stand))(PP))
(NP(NP(DT)(NN(abbreviation)))(PP))
(SQ(VBZ)(NP)(VP(VB(stand))(PP)))
(SBARQ(WHNP)(SQ(VBZ)(NP)(VP(VB(stand))(PP)))(.))
(SQ(VBZ(does))(NP)(VP(VB(stand))(PP)))
(VP(VBZ)(NP(NP(DT)(NN(abbreviation)))(PP)))
Table 4: Best fragments for the ABBR class.
(WRB(Why))
(WHADVP(WRB(Why)))
(WHADVP(WRB(How)))
(WHADVP(WRB))
(VB(mean))
(VBZ(causes))
(VB(do))
(ROOT(SBARQ(WHADVP(WRB(How)))(SQ)(.)))
(ROOT(SBARQ(WHADVP(WRB(How)))(SQ)(.(?))))
(SBARQ(WHADVP(WRB(How)))(SQ))
(WRB(How))
(SBARQ(WHADVP(WRB(How)))(SQ)(.))
(SBARQ(WHADVP(WRB(How)))(SQ)(.(?)))
(SBARQ(WHADVP(WRB(Why)))(SQ))
(ROOT(SBARQ(WHADVP(WRB(Why)))(SQ)))
(SBARQ(WHADVP(WRB))(SQ))
Table 5: Best fragments for the DESC class.
(NN(film))
(NN(color))
(NN(book))
(NN(novel))
(NN(sport))
(WP(What))
(NN(fear))
(NN(movie))
(NN(word))
(VP(VBN(called)))
(NN(game))
(NP(DT)(NN(fear)))
(NP(NP(DT)(NN(fear)))(PP))
Table 6: Best fragments for the ENTY class.
118
(NN(company))
(WP(Who))
(WHNP(WP(Who)))
(NN(name))
(NN(team))
(NN(baseball))
(WHNP(WP))
(NN(character))
(NNP(President))
(NN(leader))
(NN(actor))
(NN(president))
(JJ(Whose))
(VP(VBD)(NP))
(NP(NP)(JJ)(NN(name)))
(VP(VBD)(VP))
(NN(organization))
(VP(VBD)(NP)(PP(IN)(NP)))
(SBARQ(WHNP(WP(Who)))(SQ)(.))
(ROOT(SBARQ(WHNP(WP(Who)))(SQ)(.)))
(ROOT(SBARQ(WHNP(WP(Who)))(SQ)(.(?))))
(SBARQ(WHNP(WP(Who)))(SQ)(.(?)))
Table 7: Best fragments for the HUM class.
(NN(city))
(NN(country))
(WRB(Where))
(NN(state))
(WHADVP(WRB(Where)))
(NN(capital))
(NP(NN(city)))
(NNS(countries))
(NP(NN(state)))
(PP(IN(in)))
(SBARQ(WHADVP(WRB(Where)))(SQ)(.(?)))
(SBARQ(WHADVP(WRB(Where)))(SQ)(.))
(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)(.)))
(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)(.(?))))
(NN(island))
(NN(address))
(NN(river))
(NN(mountain))
(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)))
(SBARQ(WHADVP(WRB(Where)))(SQ))
Table 8: Best fragments for the LOC class.
(WRB(How))
(WHADVP(WRB(When)))
(WRB(When))
(JJ(many))
(NN(year))
(WHADJP(WRB)(JJ))
(NP(NN(year)))
(WHADJP(WRB(How))(JJ))
(NN(date))
(SBARQ(WHADVP(WRB(When)))(SQ)(.(?)))
(SBARQ(WHADVP(WRB(When)))(SQ)(.))
(NN(day))
(NN(population))
(ROOT(SBARQ(WHADVP(WRB(When)))(SQ)(.)))
(ROOT(SBARQ(WHADVP(WRB(When)))(SQ)(.(?))))
(JJ(average))
(NN(number))
Table 9: Best fragments for the NUM class.
whose SVM weights are the lowest, i.e. those
that are (almost) irrelevant for the SVM. There-
fore, the chance of this resulting in an improve-
ment is rather low.
With respect to cases where our model is less
accurate than a standard SST, we should consider
that our choice of parameters is sub-optimal and
we adopt a very aggressive feature selection strat-
egy, that only retains a few thousand features from
a space where there are hundreds of millions of
different features.
7 Conclusions
We introduced a novel framework for support vec-
tor classification that combines advantages of con-
volution kernels, i.e. the generation of a very high
dimensional structure space, with the efficiency
and clarity of explicit representations in a linear
space.
For this paper, we focused on the SubSet Tree
kernel and verified the potential of the proposed
solution on two NLP tasks, i.e. semantic role
labeling and question classification. The exper-
iments show that our framework drastically re-
duces processing time, e.g. boundary classifica-
tion for SRL, while preserving the accuracy.
We presented a selection of the most relevant
fragments identified for the SRL boundary classi-
fier as well as for each class of the coarse grained
QC task. Our analysis shows that our frame-
work can discover state-of-the-art features, e.g.
the Path feature for SRL. We believe that shar-
ing these fragments with the NLP community and
studying them in more depth will be useful to
identify new, relevant features for the character-
ization of several learning problems. For this
purpose, we made available the fragment spaces
at http://danielepighin.net and we will keep
them updated with new set of experiments on new
tasks, e.g. SRL based on FrameNet and VerbNet,
e.g. (Giuglea and Moschitti, 2004).
In our future work, we plan to widen the list
of covered tasks and to extend our algorithm to
cope with different kernel families, such as the
partial tree kernel and kernels defined over pairs
of trees, e.g. the ones used for textual entailment
in (Moschitti and Zanzotto, 2007). We also plan to
move from mining fragments to mining classes of
fragments, i.e. to identify prototypical fragments
in the fragment space that generalize topological
sub-classes of the most relevant fragments.
119
References
Fabio Aiolli, Giovanni Da San Martino, Alessandro Sper-
duti, and Alessandro Moschitti. 2006. Fast on-line kernel
learning for trees. In Proceedings of ICDM?06.
Stephan Bloehdorn and Alessandro Moschitti. 2007a. Com-
bined syntactic and semantic kernels for text classification.
In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b. Struc-
ture and semantics for expressive text kernels. In In Pro-
ceedings of CIKM ?07.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL?05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Michael Collins and Nigel Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over Discrete
Structures, and the Voted Perceptron. In Proceedings of
ACL?02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceedings of
ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for Re-
lational Learning. In Proceedings of ICML 2003.
Mona Diab, Alessandro Moschitti, and Daniele Pighin. 2008.
Semantic role labeling systems for Arabic using kernel
methods. In Proceedings of ACL-08: HLT, pages 798?
806.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics, 28:245?
288.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge discovery using framenet, verbnet and prop-
bank. In A. Meyers, editor, Workshop on Ontology and
Knowledge Discovering at ECML 2004, Pisa, Italy.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation. In
Proceedings of ACL?05, pages 403?410.
Hans P. Graf, Eric Cosatto, Leon Bottou, Igor Durdanovic,
and Vladimir Vapnik. 2004. Parallel support vector ma-
chines: The cascade svm. In Neural Information Process-
ing Systems.
Isabelle Guyon and Andre? Elisseeff. 2003. An introduc-
tion to variable and feature selection. Journal of Machine
Learning Research, 3:1157?1182.
David Haussler. 1999. Convolution kernels on discrete struc-
tures. Technical report, Dept. of Computer Science, Uni-
versity of California at Santa Cruz.
T. Joachims. 2000. Estimating the generalization perfor-
mance of a SVM efficiently. In Proceedings of ICML?00.
Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for
semi-structured data. In Proceedings of ICML?02.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding up
training with tree kernels for node relation labeling. In
Proceedings of HLT-EMNLP?05.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL?03, pages
423?430.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-
based parse reranking with subtree features. In Proceed-
ings of ACL?05.
Xin Li and Dan Roth. 2006. Learning question classifiers:
the role of semantic information. Natural Language En-
gineering, 12(3):229?249.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from
texts. In ICML?07.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and
Suresh Manandhar. 2007. Exploiting syntactic and shal-
low semantic kernels for question/answer classification.
In Proceedings of ACL?07.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2008. Tree kernels for semantic role labeling. Compu-
tational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Pro-
ceedings of ECML?06, pages 318?329.
Julia Neumann, Christoph Schnorr, and Gabriele Steidl.
2005. Combined SVM-Based Feature Selection and Clas-
sification. Machine Learning, 61(1-3):129?150.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of semantic
roles. Comput. Linguist., 31(1):71?106.
J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal,
and M. C. Hsu. 2001. PrefixSpan Mining Sequential Pat-
terns Efficiently by Prefix Projected Pattern Growth. In
Proceedings of ICDE?01.
Daniele Pighin and Alessandro Moschitti. 2009. Efficient
linearization of tree kernel functions. In Proceedings of
CoNLL?09.
Alain Rakotomamonjy. 2003. Variable selection using SVM
based criteria. Journal of Machine Learning Research,
3:1357?1370.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Us-
ing LTAG Based Features in Parse Reranking. In Proceed-
ings of EMNLP?06.
Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree
Kernels with Statistical Feature Mining. In Proceedings
of NIPS?05.
Kristina Toutanova, Penka Markova, and Christopher Man-
ning. 2004. The Leaf Path Projection View of Parse
Trees: Exploring String Kernels for HPSG Parse Selec-
tion. In Proceedings of EMNLP 2004.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.
Ellen M. Voorhees. 2001. Overview of the trec 2001 ques-
tion answering track. In In Proceedings of the Tenth Text
REtrieval Conference (TREC, pages 42?51.
Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimil-
iano Pontil, Tomaso Poggio, and Vladimir Vapnik. 2001.
Feature Selection for SVMs. In Proceedings of NIPS?01.
Jason Weston, Andre? Elisseeff, Bernhard Scho?lkopf, and
Mike Tipping. 2003. Use of the zero norm with lin-
ear models and kernel methods. J. Mach. Learn. Res.,
3:1439?1461.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of SI-
GIR?03, pages 26?32.
120
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076?1085,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Re-Ranking Models Based-on Small Training Data for Spoken Language
Understanding
Marco Dinarelli
University of Trento
Italy
dinarelli@disi.unitn.it
Alessandro Moschitti
University of Trento
Italy
moschitti@disi.unitn.it
Giuseppe Riccardi
University of Trento
Italy
riccardi@disi.unitn.it
Abstract
The design of practical language applica-
tions by means of statistical approaches
requires annotated data, which is one of
the most critical constraint. This is par-
ticularly true for Spoken Dialog Systems
since considerably domain-specific con-
ceptual annotation is needed to obtain ac-
curate Language Understanding models.
Since data annotation is usually costly,
methods to reduce the amount of data are
needed. In this paper, we show that bet-
ter feature representations serve the above
purpose and that structure kernels pro-
vide the needed improved representation.
Given the relatively high computational
cost of kernel methods, we apply them to
just re-rank the list of hypotheses provided
by a fast generative model. Experiments
with Support Vector Machines and differ-
ent kernels on two different dialog cor-
pora show that our re-ranking models can
achieve better results than state-of-the-art
approaches when small data is available.
1 Introduction
Spoken Dialog Systems carry out automatic
speech recognition and shallow natural language
understanding by heavily relying on statistical
models. These in turn need annotated data de-
scribing the application domain. Such annotation
is far the most expensive part of the system de-
sign. Therefore, methods reducing the amount of
labeled data can speed up and lower the overall
amount of work.
Among others, Spoken Language Understand-
ing (SLU) is an important component of the sys-
tems above, which requires training data to trans-
late a spoken sentence into its meaning repre-
sentation based on semantic constituents. These
are conceptual units instantiated by sequences of
words.
In the last decade two major approaches have
been proposed to automatically map words in con-
cepts: (i) generative models, whose parameters re-
fer to the joint probability of concepts and con-
stituents; and (ii) discriminative models, which
learn a classification function based on conditional
probabilities of concepts given words.
A simple but effective generative model is the
one based on Finite State Transducers. It performs
SLU as a translation process from words to con-
cepts using Finite State Transducers (FST). An ex-
ample of discriminative model used for SLU is the
one based on Support Vector Machines (SVMs)
(Vapnik, 1995), as shown in (Raymond and Ric-
cardi, 2007). In this approach, data is mapped into
a vector space and SLU is performed as a clas-
sification problem using Maximal Margin Clas-
sifiers (Vapnik, 1995). A relatively more recent
approach for SLU is based on Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001). CRFs
are undirected graphical and discriminative mod-
els. They use conditional probabilities to account
for many feature dependencies without the need of
explicitly representing such dependencies.
Generative models have the advantage to be
more robust to overfitting on training data, while
discriminative models are more robust to irrele-
vant features. Both approaches, used separately,
have shown good accuracy (Raymond and Ric-
cardi, 2007), but they have very different charac-
teristics and the way they encode prior knowledge
is very different, thus designing models that take
into account characteristics of both approaches are
particularly promising.
In this paper, we propose a method for SLU
based on generative and discriminative models:
the former uses FSTs to generate a list of SLU
hypotheses, which are re-ranked by SVMs. To
effectively design our re-ranker, we use all pos-
1076
sible word/concept subsequences with gaps of the
spoken sentence as features (i.e. all possible n-
grams). Gaps allow for encoding long distance de-
pendencies between words in relatively small se-
quences. Since the space of such features is huge,
we adopted kernel methods, i.e. sequence kernels
(Shawe-Taylor and Cristianini, 2004) and tree ker-
nels (Collins and Duffy, 2002; Moschitti, 2006a)
to implicitly encode them along with other struc-
tural information in SVMs.
We experimented with different approaches for
training the discriminative models and two differ-
ent corpora: the french MEDIA corpus (Bonneau-
Maynard et al, 2005) and a corpus made available
by the European project LUNA
1
(Dinarelli et al,
2009b). In particular, the new contents with re-
spect to our previous work (Dinarelli et al, 2009a)
are:
? We designed a new sequential structure
(SK2) and two new hierarchical tree struc-
tures (MULTILEVEL and FEATURES) for
re-ranking models (see Section 4.2). The lat-
ter combined with two different tree kernels
originate four new different models.
? We experimented with automatic speech
transcriptions thus assessing the robustness to
noise of our models.
? We compare our models against Conditional
Random Field (CRF) approaches described
in (Hahn et al, 2008), which are the cur-
rent state-of-the-art in SLU. Learning curves
clearly show that our models improve CRF,
especially when small data sets are used.
The remainder of the paper is organized as fol-
lows: Section 2 introduces kernel methods for
structured data, Section 3 describes the generative
model producing the initial hypotheses whereas
Section 4 presents the discriminative models for
re-ranking them. The experiments and results
are reported in Section 5 and the conclusions are
drawn in Section 6.
2 Feature Engineering via Structure
Kernels
Kernel methods are viable approaches to engi-
neer features for text processing, e.g. (Collins and
Duffy, 2002; Kudo and Matsumoto, 2003; Cumby
1
Contract n. 33549
and Roth, 2003; Cancedda et al, 2003; Culotta
and Sorensen, 2004; Toutanova et al, 2004; Kudo
et al, 2005; Moschitti, 2006a; Moschitti et al,
2007; Moschitti, 2008; Moschitti et al, 2008;
Moschitti and Quarteroni, 2008). In the follow-
ing, we describe structure kernels, which will be
used to engineer features for our discriminative re-
ranker.
2.1 String Kernels
The String Kernels that we consider count the
number of substrings containing gaps shared by
two sequences, i.e. some of the symbols of the
original string are skipped. We adopted the ef-
ficient algorithm described in (Shawe-Taylor and
Cristianini, 2004; Lodhi et al, 2000). More
specifically, we used words and markers as sym-
bols in a style similar to (Cancedda et al, 2003;
Moschitti, 2008). For example, given the sen-
tence: How may I help you ? sample substrings,
extracted by the Sequence Kernel (SK), are: How
help you ?, How help ?, help you, may help you,
etc.
2.2 Tree kernels
Tree kernels represent trees in terms of their sub-
structures (fragments). The kernel function detects
if a tree subpart (common to both trees) belongs to
the feature space that we intend to generate. For
such purpose, the desired fragments need to be de-
scribed. We consider two important characteriza-
tions: the syntactic tree (STF) and the partial tree
(PTF) fragments.
2.2.1 Tree Fragment Types
An STF is a general subtree whose leaves can
be non-terminal symbols (also called SubSet Tree
(SST) in (Moschitti, 2006a)). For example, Fig-
ure 1(a) shows 10 STFs (out of 17) of the sub-
tree rooted in VP (of the left tree). The STFs sat-
isfy the constraint that grammatical rules cannot
be broken. For example, [VP [V NP]] is an
STF, which has two non-terminal symbols, V and
NP, as leaves whereas [VP [V]] is not an STF.
If we relax the constraint over the STFs, we ob-
tain more general substructures called partial trees
fragments (PTFs). These can be generated by the
application of partial production rules of the gram-
mar, consequently [VP [V]] and [VP [NP]]
are valid PTFs. Figure 1(b) shows that the num-
ber of PTFs derived from the same tree as before
is still higher (i.e. 30 PTs).
1077
(a) Syntactic Tree fragments (STF) (b) Partial Tree fragments (PTF)
Figure 1: Examples of different classes of tree fragments.
2.3 Counting Shared Subtrees
The main idea of tree kernels is to compute the
number of common substructures between two
trees T
1
and T
2
without explicitly considering the
whole fragment space. To evaluate the above ker-
nels between two T
1
and T
2
, we need to define a
set F = {f
1
, f
2
, . . . , f
|F|
}, i.e. a tree fragment
space and an indicator function I
i
(n), equal to 1
if the target f
i
is rooted at node n and equal to 0
otherwise. A tree-kernel function over T
1
and T
2
is TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
),
where N
T
1
and N
T
2
are the sets of the T
1
?s
and T
2
?s nodes, respectively and ?(n
1
, n
2
) =
?
|F|
i=1
I
i
(n
1
)I
i
(n
2
). The latter is equal to the num-
ber of common fragments rooted in the n
1
and n
2
nodes.
The algorithm for the efficient evaluation of ?
for the syntactic tree kernel (STK) has been widely
discussed in (Collins and Duffy, 2002) whereas its
fast evaluation is proposed in (Moschitti, 2006b),
so we only describe the equations of the partial
tree kernel (PTK).
2.4 The Partial Tree Kernel (PTK)
PTFs have been defined in (Moschitti, 2006a).
Their computation is carried out by the following
? function:
1. if the node labels of n
1
and n
2
are different
then ?(n
1
, n
2
) = 0;
2. else ?(n
1
, n
2
) =
1 +
?
~
I
1
,
~
I
2
,l(
~
I
1
)=l(
~
I
2
)
?
l(
~
I
1
)
j=1
?(c
n
1
(
~
I
1j
), c
n
2
(
~
I
2j
))
where
~
I
1
= ?h
1
, h
2
, h
3
, ..? and
~
I
2
=
?k
1
, k
2
, k
3
, ..? are index sequences associated with
the ordered child sequences c
n
1
of n
1
and c
n
2
of
n
2
, respectively,
~
I
1j
and
~
I
2j
point to the j-th child
in the corresponding sequence, and, again, l(?) re-
turns the sequence length, i.e. the number of chil-
dren.
Furthermore, we add two decay factors: ? for
the depth of the tree and ? for the length of the
child subsequences with respect to the original se-
quence, i.e. we account for gaps. It follows that
?(n
1
, n
2
) =
?
(
?
2
+
?
~
I
1
,
~
I
2
,l(
~
I
1
)=l(
~
I
2
)
?
d(
~
I
1
)+d(
~
I
2
)
l(
~
I
1
)
?
j=1
?(c
n
1
(
~
I
1j
), c
n
2
(
~
I
2j
))
)
,
(1)
where d(
~
I
1
) =
~
I
1l(
~
I
1
)
?
~
I
11
and d(
~
I
2
) =
~
I
2l(
~
I
2
)
?
~
I
21
. This way, we penalize both larger trees and
child subsequences with gaps. Eq. 1 is more gen-
eral than the ? equation for STK. Indeed, if we
only consider the contribution of the longest child
sequence from node pairs that have the same chil-
dren, we implement STK.
3 Generative Model: Stochastic
Conceptual Language Model (SCLM)
The first step of our approach is to produce a list
of SLU hypotheses using a Stochastic Conceptual
Language Model. This is the same described in
(Raymond and Riccardi, 2007) with the only dif-
ference that we train the language model using the
SRILM toolkit (Stolcke, 2002) and we then con-
vert it into a Stochastic Finite State Transducer
(SFST). Such method allows us to use a wide
group of language models, backed-off or inter-
polated with many kind of smoothing techniques
(Chen and Goodman, 1998).
To exemplify our SCLM let us consider the
following input italian sentence taken from the
LUNA corpus along with its English translation:
Ho un problema col monitor.
(I have a problem with my screen).
A possible semantic annotation is:
null{ho} PROBLEM{un problema} HARD-
WARE{col monitor},
where PROBLEM and HARDWARE are two
domain concepts and null is the label used for
words not meaningful for the task. To associate
word sequences with concepts, we use begin
1078
(B) and inside (I) markers after each word of a
sequence, e.g.:
null{ho} PROBLEM-B{un} PROBLEM-
I{problema} HARDWARE-B{col} HARD-
WARE-I{monitor}
This annotation is automatically performed
by a model based on a combination of three
transducers:
?
SLU
= ?
W
? ?
W2C
? ?
SLM
,
where ?
W
is the transducer representation of the
input sentence, ?
W2C
is the transducer mapping
words to concepts and ?
SLM
is the Stochastic
Conceptual Language Model trained with SRILM
toolkit and converted in FST. The SCLM repre-
sents joint probability of word and concept se-
quences by using the joint probability:
P (W,C) =
k
?
i=1
P (w
i
, c
i
|h
i
),
where W = w
1
..w
k
, C = c
1
..c
k
and h
i
=
w
i?1
c
i?1
..w
1
c
1
.
4 Discriminative re-ranking
Our discriminative re-ranking is based on SVMs
trained with pairs of conceptually annotated sen-
tences produced by the FST-based generative
model described in the previous section. An SVM
learn to classify which annotation has an error rate
lower than the others so that it can be used to sort
the m-best annotations based on their correctness.
While for SVMs details we remaind to the wide
literature available, for example (Vapnik, 1995) or
(Shawe-Taylor and Cristianini, 2004), in this sec-
tion we focus on hypotheses generation and on the
kernels used to implement our re-ranking model.
4.1 Generation of m-best concept labeling
Using the FST-based model described in Section
3, we can generate the list of m best hypotheses
ranked by the joint probability of the Stochastic
Conceptual Language Model (SCLM). The Re-
ranking model proposed in this paper re-ranks
such list.
After an analysis of the m-best hypothesis list,
we noticed that many times the first hypothesis
ranked by SCLM is not the most accurate, i.e.
the error rate evaluated with its Levenshtein dis-
tance from the manual annotation is not the low-
est among the m hypotheses. This means that re-
ranking hypotheses could improve the SLU ac-
curacy. Intuitively, to achieve satisfactory re-
sults, different features from those used by SCLM
should be considered to exploit in a different way
the information encoded in the training data.
4.2 Structural features for re-ranking
The kernels described in previous sections pro-
vide a powerful technology for exploiting features
of structured data. These kernels were originally
designed for data annotated with syntactic parse
trees. In Spoken Language Understanding the data
available are text sentences with their semantic
annotation based on basic semantic constituents.
This kind of data has a rather flat structure with
respect to syntactic parse trees. Thus, to exploit
the power of kernels, a careful design of the struc-
tures used to represent data must be carried out,
where the goal is to build tree-like annotation from
the semantic annotation. For this purpose, we
note that the latter is made upon sentence chunks,
which implicitly define syntactic structures as long
as the annotation is consistent in the corpus.
We took into account the characteristics of the
presented kernels and the structure of semantic an-
notated data. As a result we designed the tree
structures shown in figures 2(a), 2(b) and 3 for
STK and PTK and sequential structures for SK
defined in the following (where all the structures
refer to the same example presented in Section 3,
i.e. Ho un problema col monitor). The structures
used with SK are:
(SK1) NULL ho PROBLEM-B un
PROBLEM-I problema HARDWARE-B col
HARDWARE-I monitor
(SK2) NULL ho PROBLEM B un PROB-
LEM I problema HARDWARE B col HARD-
WARE I monitor,
For simplicity, from now on, the two structures
will be referred as SK1 and SK2 (String Kernel 1
and 2). They differer in the use of chunk mark-
ers B and I. In SK1, markers are part of the con-
cept, thus they increase the number of semantic
tags in the data whereas in SK2 markers are put
apart as separated words so that they can mark ef-
fectively the beginning and the end of a concept,
but for the same reason they can add noise in the
sentence. Notice that the order of words and con-
cepts is meaningful since each word is preceded
by its corresponding concepts.
The structures shown in Figure 2(a), 2(b) and 3
1079
have been designed for STK and PTK. They pro-
vide trees with increasing structure complexity as
described in the following.
The first structure (FLAT) is a simple tree
providing direct dependency between words and
chunked concepts. From it, STK and PTK can ex-
tract relevant features (tree fragments).
The second structure (MULTILEVEL) has one
more level of nodes and yields the same separation
of concepts and markers shown in SK1. Notice
that the same separation can be carried out putting
the markers B and I as features at the same level of
the words. This would increase exponentially (in
the number of leaves) the number of subtrees taken
into account by the STK computation. Since STK
doesn?t separate children, as described in Section
2.3, the structure we chose is lighter but also more
rigid.
The third structure (FEATURES) is a more
complex structure. It allows to use a wide num-
ber of features (like Word categories, POS tags,
morpho-syntactic features), which are commonly
used in this kind of task. As described above, the
use of features exponentially increases the num-
ber of subtrees taken into account by kernel com-
putations but they also increase the robustness of
the model. In this work we only used Word Cate-
gories as features. They are domain independent,
e.g. ?Months?, ?Dates?, ?Number? etc. or POS
tags, which are useful to generalize target words.
Note also that the features in common between
two trees must appear in the same child-position,
hence we sort them based on their indices, e.g.?F0?
for words and ?F1? for word categories.
4.3 Re-ranking models using sequences
The FST generates the m most likely concept an-
notations. These are used to build annotation
pairs,
?
s
i
, s
j
?
, which are positive instances if s
i
has a lower concept annotation error than s
j
, with
respect to the manual annotation. Thus, a trained
binary classifier can decide if s
i
is more accurate
than s
j
. Each candidate annotation s
i
is described
by a word sequence with its concept annotation.
Considering the example in the previous section, a
pair of annotations
?
s
i
, s
j
?
could be
s
i
: NULL ho PROBLEM-B un PROBLEM-
I problema HARDWARE-B col HARDWARE-I
monitor
s
j
: NULL ho ACTION-B un ACTION-I prob-
lema HARDWARE-B col HARDWARE-B moni-
tor
where NULL, ACTION and HARDWARE are
the assigned concepts. The second annotation is
less accurate than the first since problema is erro-
neously annotated as ACTION and ?col monitor?
is split in two different concepts.
Given the above data, the sequence kernel
is used to evaluate the number of common n-
grams between s
i
and s
j
. Since the string ker-
nel skips some elements of the target sequences,
the counted n-grams include: concept sequences,
word sequences and any subsequence of words
and concepts at any distance in the sentence.
Such counts are used in our re-ranking function
as follows: let e
k
be the pair
?
s
1
k
, s
2
k
?
we evaluate
the kernel:
K
R
(e
1
, e
2
) = SK(s
1
1
, s
1
2
) + SK(s
2
1
, s
2
2
) (2)
? SK(s
1
1
, s
2
2
)? SK(s
2
1
, s
1
2
)
This schema, consisting in summing four different
kernels, has been already applied in (Collins and
Duffy, 2002; Shen et al, 2003) for syntactic pars-
ing re-ranking, where the basic kernel was a tree
kernel instead of SK. It was also used also in (Shen
et al, 2004) to re-rank different candidates of the
same hypothesis for machine translation. Notice
that our goal is different from the one tackled in
such paper and, in general, it is more difficult: we
try to learn which is the best annotation of a given
input sentence, while in (Shen et al, 2004), they
learn to distinguish between ?good? and ?bad?
translations of a sentence. Even if our goal is more
difficult, our approach is very effective, as shown
in (Dinarelli et al, 2009a). It is more appropriate
since in parse re-ranking there is only one best hy-
pothesis, while in machine translation a sentence
can have more than one correct translations.
Additionally, in (Moschitti et al, 2006; Mos-
chitti et al, 2008) a tree kernel was applied to se-
mantic trees similar to the one introduced in the
next section to re-rank Semantic Role Labeling an-
notations.
4.4 Re-ranking models using trees
Since the aim of concept annotation re-ranking is
to exploit innovative and effective source of infor-
mation, we can use, in addition to sequence ker-
nels, the power of tree kernels to generate correla-
tion between concepts and word structures.
Figures 2(a), 2(b) and 3 describe the struc-
tural association between the concept and the word
1080
(a) FLAT Tree (b) MULTILEVEL Tree
Figure 2: Examples of structures used for STK and PTK
Figure 3: The FEATURES semantic tree used for STK or PTK
Corpus Train set Test set
LUNA words concepts words concepts
Dialogs 183 67
Turns 1.019 373
Tokens 8.512 2.887 2.888 984
Vocab. 1.172 34 - -
OOV rate - - 3.2% 0.1%
Table 1: Statistics on the LUNA corpus
Corpus Train set Test set
Media words concepts words concepts
Turns 12,922 3,518
# of tokens 94,912 43,078 26,676 12,022
Vocabulary 5,307 80 - -
OOV rate - - 0.01% 0.0%
Table 2: Statistics on the MEDIA corpus
level. This kind of trees allows us to engineer new
kernels and consequently new features (Moschitti
et al, 2008), e.g. their subparts extracted by STK
or PTK, like the tree fragments in figures 1(a) and
1(b). These can be used in SVMs to learn the clas-
sification of words in concepts.
More specifically, in our approach, we use tree
fragments to establish the order of correctness
between two alternative annotations. Therefore,
given two trees associated with two annotations, a
re-ranker based on tree kernel can be built in the
same way of the sequence-based kernel by substi-
tuting SK in Eq. 2 with STK or PTK. The major
advantage of using trees is the hierarchical depen-
dencies between its nodes, allowing for the use of
richer n-grams with back-off models.
5 Experiments
In this section, we describe the corpora, parame-
ters, models and results of our experiments on re-
ranking for SLU. Our baseline is constituted by the
error rate of systems solely based on either FST
or SVMs. The re-ranking models are built on the
FST output, which in turn is applied to both man-
ual or automatic transcriptions.
5.1 Corpora
We used two different speech corpora:
The LUNA corpus, produced in the homony-
mous European project, is the first Italian dataset
of spontaneous speech on spoken dialogs. It is
based on help-desk conversations in a domain
of software/hardware repairing (Dinarelli et al,
2009b). The data is organized in transcriptions
and annotations of speech based on a new multi-
level protocol. Although data acquisition is still in
progress, 250 dialogs have been already acquired
with a WOZ approach and other 180 Human-
Human (HH) dialogs have been annotated. In this
work, we only use WOZ dialogs, whose statistics
are reported in Table 1.
The corpus MEDIA was collected within
the French project MEDIA-EVALDA (Bonneau-
Maynard et al, 2005) for development and evalu-
ation of spoken understanding models and linguis-
tic studies. The corpus is composed of 1257 di-
alogs (from 250 different speakers) acquired with
a Wizard of Oz (WOZ) approach in the context
of hotel room reservations and tourist information.
1081
Statistics on transcribed and conceptually anno-
tated data are reported in Table 2.
5.2 Experimental setup
Given the small size of LUNA corpus, we did not
carried out any parameterization thus we used de-
fault or a priori parameters. We experimented with
LUNA and three different re-rankers obtained with
the combination of SVMs with STK, PTK and SK,
described in Section 4. The initial annotation to be
re-ranked is the list of the ten best hypotheses out-
put by an FST model.
We point out that, on the large Media dataset the
processing time is considerably high
2
so we could
not run all the models.
We trained all the SCLMs used in our experi-
ments with the SRILM toolkit (Stolcke, 2002) and
we used an interpolated model for probability es-
timation with the Kneser-Ney discount (Chen and
Goodman, 1998). We then converted the model in
an FST again with SRILM toolkit.
The model used to obtain the SVM baseline for
concept classification was trained using YamCHA
(Kudo and Matsumoto, 2001). As re-ranking
models based on structure kernels and SVMs,
we used the SVM-Light-TK toolkit (available at
disi.unitn.it/moschitti). For ? (see Section 3), cost-
factor and trade-off parameters, we used, 0.4, 1
and 1, respectively (i.e. the default parameters).
The number m of hypotheses was always set to 10.
The CRF model we compare with was
trained with the CRF++ tool, available at
http://crfpp.sourceforge.net/. The model is equiva-
lent to the one described in (Hahn et al, 2008). As
features, we used word and morpho-syntactic cat-
egories in a window of [-2, +2] with respect to the
current token, plus bigrams of concept tags (see
(Hahn et al, 2008) and the CRF++ web site for
more details).
Such model is very effective for SLU. In (Hahn
et al, 2008), it is compared with other four models
(Stochastic Finite State Transducers, Support Vec-
tor Machines, Machine Translation, Positional-
Based Log-linear model) and it is by far the best
on MEDIA. Additionally, in (Raymond and Ric-
cardi, 2007), a similar CRF model was compared
with FST and SVMs on ATIS and on a different
2
The number of parameters of the models and the number
of training approaches make the exhaustive experimentation
very expensive in terms of processing time, which would be
roughly between 2 and 3 months of a typical workstation.
Structure STK PTK SK
FLAT 18.5 19.3 -
MULTILEVEL 20.6 19.1 -
FEATURES 19.9 18.4 -
SK1 - - 16.2
SK2 - - 18.5
Table 3: CER of SVMs using STK, PTK and SK
on LUNA (manual transcriptions). The Baselines,
FST and SVMs alone, show a CER of 23.2% and
26.3%, respectively.
Model MEDIA (CER) LUNA (CER)
FST 13.7% 23.2%
CRF 11.5% 20.4%
SVM-RR (PTK) 12.1% 18.4%
Table 4: Results of SLU experiments on MEDIA
and LUNA test set (manual transcriptions).
version of MEDIA, showing again to be very ef-
fective.
We ran SLU experiments on manual and auto-
matic transcriptions. The latter are produced by
a speech recognizer with a WER of 41.0% and
31.4% on the LUNA and the MEDIA test sets, re-
spectively.
5.3 Training approaches
The FST model generates the 10-best annotations,
i.e. the data used to train the re-ranker based on
SVMs. Different training approaches can be car-
ried out based on the use of the data. We divided
the training set in two parts. We train FSTs on
part 1 and generate the 10-best hypotheses using
part 2, thus providing the first chunk of re-ranking
data. Then, we re-apply these steps inverting part
1 with part 2 to provide the second data chunk.
Finally, we train the re-ranker on the merged data.
For classification, we generate the 10-best hy-
potheses of the whole test set using the FST
trained on all training data.
5.4 Re-ranking results
In Tables 3, 4 and 5 and Figures 4(a) and 4(b) we
report the results of our experiments, expressed in
terms of concept error rate (CER). CER is a stan-
dard measure based on the Levensthein alignment
of sentences and it is computed as the ratio be-
tween inserted, deleted and confused concepts and
the number of concepts in the reference sentence.
Table 3 shows the results on the LUNA cor-
pus using the different training approaches, ker-
nels and structures described in this paper. The
1082
15 
20 
25 
30 
35 
40 
45 
50 
55 
60 
100 500 1000 2000 3000 4000 5000 6000 
CER
 
Training Sentences 
FST CRF RR 
(a) Learning Curve on MEDIA corpus using the RR model
based on SVMs and STK
15 
20 
25 
30 
35 
40 
45 
50 
55 
100 300 500 700 1000 
CER
 
Training Sentences 
FST CRF RR 
(b) Learning Curve on LUNA corpus using the RR model
based on SVMs and SK
Figure 4: Learning curves on MEDIA and LUNA corpora using FST, CRF and RR on the FST hypotheses
Model MEDIA (CER) LUNA (CER)
FST 28.6% 42.7%
CRF 24.0% 41.8%
SVM-RR (PTK) 25.0% 38.9%
Table 5: Results of SLU experiments on MEDIA
and LUNA test set (automatic transcriptions with
a WER 31.4% on MEDIA and 41% on LUNA)
dash symbol means that the structure cannot be
applied to the corresponding kernel. We note that
our re-rankers significantly improve our baselines,
i.e. 23.2% CER for FST and 26.3% CER for SVM
concept classifiers. For example, SVM re-ranker
using SK, in the best case, improves FST concept
classifier of 23.2-16.2 = 7 points.
Note also that the structures designed for trees
yield quite different results depending on which
kernel is used. We can see in Table 3 that the
best result using STK is obtained with the simplest
structure (FLAT), while with PTK the best result
is achieved with the most complex structure (FEA-
TURES). This is due to the fact that STK does
not split the children of each node, as explained in
Section 2.2, and so structures like MULTILEVEL
and FEATURES are too rigid and prevent the STK
to be effective.
For lack of space we do not report all the results
using different kernels and structures on MEDIA,
but we underline that as MEDIA is a more com-
plex task (34 concepts in LUNA, 80 in MEDIA),
the more complex structures are more effective to
capture word-concept dependencies and the best
results were obtained using the FEATURES tree.
Table 4 shows the results of the SLU exper-
iments on the MEDIA and LUNA test sets us-
ing the manual transcriptions of spoken sentences
and a re-ranker based on PTK and the FEATURES
structure (already reported in the previous table).
We used PTK since it is enough efficient to carry
out the computation on the much larger Media cor-
pus although as previously shown it is less accu-
rate than SK.
We note that on a big corpus like MEDIA, the
baseline models (FST and CRF) can be accurately
learned thus less errors can be ?corrected?. As
a consequence, our re-ranking approach does not
improve CRF but it still improves the FSTs base-
line of 1.6% points (11.7% of relative improve-
ment).
The same behavior is reproduced for the SLU
experiments on automatic transcriptions, shown in
Table 5. We note that, on the LUNA corpus, CRFs
are more accurate than FSTs (0.9% points), but
they are significantly improved by the re-ranking
model (2.9% points), which also improves the
FSTs baseline by 3.8% points. On the MEDIA
corpus, the re-ranking model is again very accu-
rate improving the FSTs baseline of 3.6% points
(12.6% relative improvement) on attribute anno-
tation, but the most accurate model is again CRF
(1% points better than the re-ranking model).
5.5 Discussion
The different behavior of the re-ranking model in
the LUNA and MEDIA corpora is due partially to
the task complexity, but it is mainly due to the fact
that CRFs have been deeply studied and experi-
mented (see (Hahn et al, 2008)) on MEDIA. Thus
CRF parameters and features have been largely
optimized. We believe that the re-ranking model
can be relevantly improved by carrying out param-
eter optimization and new structural feature de-
1083
sign.
Moreover, our re-ranking models achieve the
highest accuracy for automatic concept annota-
tion when small data sets are available. To show
this, we report in Figure 4(a) and 4(b) the learning
curves according to an increasing number of train-
ing sentences on the MEDIA and LUNA corpora,
respectively. To draw the first plot, we used a re-
ranker based on STK (and the FLAT tree), which
is less accurate than the other kernels but also the
most efficient in terms of training time. In the sec-
ond plot, we report the re-ranker accuracy using
SK applied to SK1 structure.
In these figures, the FST baseline performance
is compared with our re-ranking (RR) and a Con-
ditional Random Field (CRF) model. The above
curves clearly shows that for small datasets our
RR model is better than CRF whereas when the
data increases, CRF accuracy approaches the one
of the RR.
Regarding the use of kernels two main findings
can be derived:
? Kernels producing a high number of features,
e.g. SK, produce accuracy higher than ker-
nels less rich in terms of features, i.e. STK. In
particular STK is improved by 18.5-16.2=2.3
points (Table 3). This is an interesting re-
sult since it shows that (a) a kernel producing
more features also produces better re-ranking
models and (b) kernel methods give a remark-
able help in feature design.
? Although the training data is small, the re-
rankers based on kernels appear to be very
effective. This may also alleviate the burden
of annotating large amount of data.
6 Conclusions
In this paper, we propose discriminative re-
ranking of concept annotation to jointly exploit
generative and discriminative models. We im-
prove the FST-based generative approach, which
is a state-of-the-art model in LUNA, by 7 points,
where the more limited availability of annotated
data leaves a larger room for improvement. Our
re-ranking model also improves FST and CRF on
MEDIA when small data sets are used.
Kernel methods show that combinations of fea-
ture vectors, sequence kernels and other structural
kernels, e.g. on shallow or deep syntactic parse
trees, appear to be a promising future research
line
3
. Finally, the experimentation with automatic
speech transcriptions revealed that to test the ro-
bustness of our models to transcription errors.
In the future we would like to extend this re-
search by focusing on advanced shallow semantic
approaches such as predicate argument structures,
e.g. (Giuglea and Moschitti, 2004; Moschitti and
Cosmin, 2004; Moschitti et al, 2008). Addition-
ally, term similarity kernels, e.g. (Basili et al,
2005; Bloehdorn et al, 2006), will be likely im-
prove our models, especially when combined syn-
tactic and semantic kernels are used, i.e. (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b).
References
Roberto Basili, Alessandro Moschitti, and
Maria Teresa Pazienza. 1999. A text classifier
based on linguistic processing. In Proceedings
of IJCAI 99, Machine Learning for Information
Filtering.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet seman-
tics via kernel-based learning. In Proceedings of
CoNLL-2005, Ann Arbor, Michigan.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Proceedings of ECIR 2007, Rome,
Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In In proceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of ICDM 06,
Hong Kong, 2006.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of In-
terspeech2005, Lisbon, Portugal.
N. Cancedda, E. Gaussier, C. Goutte, and J. M. Ren-
ders. 2003. Word sequence kernels. J. Mach.
Learn. Res., 3.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techniques for language modeling. In
Technical Report of Computer Science Group, Har-
vard, USA.
3
A basic approach is the use of part-of-speech tags like for
example in text categorization (Basili et al, 1999) but given
the high efficiency of modern syntactic parsers we can use the
complete parse tree.
1084
M. Collins and N. Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over
Discrete structures, and the voted perceptron. In
ACL02, pages 263?270.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceed-
ings of ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2009a. Re-ranking models for spoken lan-
guage understanding. In Proceedings of EACL2009,
Athens, Greece.
Marco Dinarelli, Silvia Quarteroni, Sara Tonelli,
Alessandro Moschitti, and Giuseppe Riccardi.
2009b. Annotating spoken dialogs: from speech
segments to dialog acts and frame semantics. In
Proceedings of SRSL 2009 Workshop of EACL,
Athens, Greece.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovery using Framenet, Verbnet and
Propbank. In A. Meyers, editor, Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
Stefan Hahn, Patrick Lehnen, Christian Raymond, and
Hermann Ney. 2008. A comparison of various
methods for concept tagging for spoken language
understanding. In Proceedings of LREC, Mar-
rakech, Morocco.
T. Kudo and Y. Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
NAACL2001, Pittsburg, USA.
Taku Kudo and Yuji Matsumoto. 2003. Fast meth-
ods for kernel-based text analysis. In Proceedings
of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In Proceedings of ACL?05.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML2001, US.
Huma Lodhi, John S. Taylor, Nello Cristianini, and
Christopher J. C. H. Watkins. 2000. Text classifi-
cation using string kernels. In NIPS.
Alessandro Moschitti and Adrian Bejan Cosmin. 2004.
A semantic kernel for predicate argument classifica-
tion. In CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In Proceedings of ACL-08: HLT, Short Papers,
Columbus, Ohio.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question/answer classification. In Proceedings of
ACL?07, Prague, Czech Republic.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006a. Efficient Convolution
Kernels for Dependency and Constituent Syntactic
Trees. In Proceedings of ECML 2006, pages 318?
329, Berlin, Germany.
Alessandro Moschitti. 2006b. Making Tree Kernels
Practical for Natural Language Learning. In Pro-
ceedings of EACL2006.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
Proceeding of CIKM ?08, NY, USA.
C. Raymond and G. Riccardi. 2007. Generative and
discriminative algorithms for spoken language un-
derstanding. In Proceedings of Interspeech2007,
Antwerp,Belgium.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Proceedings of EMNLP?06.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
HLT-NAACL, pages 177?184.
A. Stolcke. 2002. Srilm: an extensible language mod-
eling toolkit. In Proceedings of SLP2002, Denver,
USA.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View
of Parse Trees: Exploring String Kernels for HPSG
Parse Selection. In Proceedings of EMNLP 2004.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
1085
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378?1387,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Convolution Kernels on Constituent, Dependency and Sequential
Structures for Relation Extraction
Truc-Vien T. Nguyen and Alessandro Moschitti and Giuseppe Riccardi
nguyenthi,moschitti,riccardi@disi.unitn.it
Department of Information Engineering and Computer Science
University of Trento
38050 Povo (TN), Italy
Abstract
This paper explores the use of innovative
kernels based on syntactic and semantic
structures for a target relation extraction
task. Syntax is derived from constituent
and dependency parse trees whereas se-
mantics concerns to entity types and lex-
ical sequences. We investigate the effec-
tiveness of such representations in the au-
tomated relation extraction from texts. We
process the above data by means of Sup-
port Vector Machines along with the syn-
tactic tree, the partial tree and the word
sequence kernels. Our study on the ACE
2004 corpus illustrates that the combina-
tion of the above kernels achieves high ef-
fectiveness and significantly improves the
current state-of-the-art.
1 Introduction
Relation Extraction (RE) is defined in ACE as the
task of finding relevant semantic relations between
pairs of entities in texts. Figure 1 shows part
of a document from ACE 2004 corpus, a collec-
tion of news articles. In the text, the relation be-
tween president and NBC?s entertainment division
describes the relationship between the first entity
(person) and the second (organization) where the
person holds a managerial position.
Several approaches have been proposed for au-
tomatically learning semantic relations from texts.
Among others, there has been increased interest in
the application of kernel methods (Zelenko et al,
2002; Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al, 2005; Wang, 2008). Their main prop-
erty is the ability of exploiting a huge amount of
This work has been partially funded by the LiveMemo-
ries project (http://www.livememories.org/) and Expert Sys-
tem (http://www.expertsystem.net/) research grant.
Jeff Zucker, the longtime executive producer of
NBC?s ?Today? program, will be named Friday
as the new president of NBC?s entertainment
division, replacing Garth Ancier, NBC execu-
tives said.
Figure 1: A document from ACE 2004 with all
entity mentions in bold.
features without an explicit feature representation.
This can be done by computing a kernel function
between a pair of linguistic objects, where such
function is a kind of similarity measure satisfy-
ing certain properties. An example is the sequence
kernel (Lodhi et al, 2002), where the objects are
strings of characters and the kernel function com-
putes the number of common subsequences of
characters in the two strings. Such substrings are
then weighted according to a decaying factor pe-
nalizing longer ones. In the same line, Tree Ker-
nels count the number of subtree shared by two in-
put trees. An example is that of syntactic (or sub-
set) tree kernel (SST) (Collins and Duffy, 2001),
where trees encode grammatical derivations.
Previous work on the use of kernels for RE
has exploited some similarity measures over di-
verse features (Zelenko et al, 2002; Culotta and
Sorensen, 2004; Zhang et al, 2005) or subse-
quence kernels over dependency graphs (Bunescu
and Mooney, 2005a; Wang, 2008). More specif-
ically, (Bunescu and Mooney, 2005a; Culotta
and Sorensen, 2004) use kernels over depen-
dency trees, which showed much lower accuracy
than feature-based methods (Zhao and Grishman,
2005). One problem of the dependency kernels
above is that they do not exploit the overall struc-
tural aspects of dependency trees. A more effec-
tive solution is the application of convolution ker-
nels to constituent parse trees (Zhang et al, 2006)
but this is not satisfactory from a general per-
1378
spective since dependency structures offer some
unique advantages, which should be exploited by
an appropriate kernel.
Therefore, studying convolution tree kernels for
dependency trees is worthwhile also considering
that, to the best of our knowledge, these models
have not been previously used for relation extrac-
tion
1
task. Additionally, sequence kernels should
be included in such global study since some of
their forms have not been applied to RE.
In this paper, we study and evaluate diverse con-
volution and sequence kernels for the RE problem
by providing several kernel combinations on con-
stituent and dependency trees and sequential struc-
tures. To fully exploit the potential of dependency
trees, in addition to the SST kernel, we applied
the partial tree (PT) kernel proposed in (Moschitti,
2006), which is a general convolution tree kernel
adaptable for dependency structures. We also in-
vestigate various sequence kernels (e.g. the word
sequence kernel (WSK) (Cancedda et al, 2003))
by incorporating dependency structures into word
sequences. These are also enriched by including
information from constituent parse trees.
We conduct experiments on the standard ACE
2004 newswire and broadcast news domain. The
results show that although some kernels are less
effective than others, they exhibit properties that
are complementary to each other. In particu-
lar, we found that relation extraction can benefit
from increasing the feature space by combining
kernels (with a simple summation) exploiting the
two different parsing paradigms. Our experiments
on RE show that the current composite kernel,
which is constituent-based is more effective than
those based on dependency trees and individual
sequence kernel but at the same time their com-
binations, i.e. dependency plus constituent trees,
improve the state-of-the-art in RE. More interest-
ingly, also the combinations of various sequence
kernels gain significant better performance than
the current state-of-the-art (Zhang et al, 2005).
Overall, these results are interesting for the
computational linguistics research since they show
that the above two parsing paradigms provide dif-
ferent and important information for a semantic
task such as RE. Regarding sequence-based ker-
nels, the WSK gains better performance than pre-
vious sequence and dependency models for RE.
1
The function defined on (Culotta and Sorensen, 2004),
although on dependency trees, is not a convolution tree ker-
nel.
A review of previous work on RE is described
in Section 2. Section 3 introduces support vec-
tor machines and kernel methods whereas our spe-
cific kernels for RE are described is Section 4. The
experiments and conclusions are presented in sec-
tions 5 and 6, respectively.
2 Related Work
To identify semantic relations using machine
learning, three learning settings have mainly been
applied, namely supervised methods (Miller et
al., 2000; Zelenko et al, 2002; Culotta and
Sorensen, 2004; Kambhatla, 2004; Zhou et al,
2005), semi supervised methods (Brin, 1998;
Agichtein and Gravano, 2000), and unsupervised
method (Hasegawa et al, 2004). In a supervised
learning setting, representative related work can
be classified into generative models (Miller et al,
2000), feature-based (Roth and tau Yih, 2002;
Kambhatla, 2004; Zhao and Grishman, 2005;
Zhou et al, 2005) or kernel-based methods (Ze-
lenko et al, 2002; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005a; Zhang et al, 2005;
Wang, 2008; Zhang et al, 2006).
The learning model employed in (Miller et al,
2000) used statistical parsing techniques to learn
syntactic parse trees. It demonstrated that a lexi-
calized, probabilistic context-free parser with head
rules can be used effectively for information ex-
traction. Meanwhile, feature-based approaches
often employ various kinds of linguistic, syntac-
tic or contextual information and integrate into
the feature space. (Roth and tau Yih, 2002) ap-
plied a probabilistic approach to solve the prob-
lems of named entity and relation extraction with
the incorporation of various features such as word,
part-of-speech, and semantic information from
WordNet. (Kambhatla, 2004) employed maximum
entropy models with diverse features including
words, entity and mention types and the number
of words (if any) separating the two entities.
Recent work on Relation Extraction has mostly
employed kernel-based approaches over syntac-
tic parse trees. Kernels on parse trees were pi-
oneered by (Collins and Duffy, 2001). This
kernel function counts the number of common
subtrees, weighted appropriately, as the measure
of similarity between two parse trees. (Culotta
and Sorensen, 2004) extended this work to cal-
culate kernels between augmented dependency
trees. (Zelenko et al, 2002) proposed extracting
1379
relations by computing kernel functions between
parse trees. (Bunescu and Mooney, 2005a) pro-
posed a shortest path dependency kernel by stipu-
lating that the information to model a relationship
between two entities can be captured by the short-
est path between them in the dependency graph.
Although approaches in RE have been domi-
nated by kernel-based methods, until now, most
of research in this line has used the kernel as some
similarity measures over diverse features (Zelenko
et al, 2002; Culotta and Sorensen, 2004; Bunescu
and Mooney, 2005a; Zhang et al, 2005; Wang,
2008). These are not convolution kernels and pro-
duce a much lower number of substructures than
the PT kernel. A recent approach successfully em-
ploys a convolution tree kernel (of type SST) over
constituent syntactic parse tree (Zhang et al, 2006;
Zhou et al, 2007), but it does not capture gram-
matical relations in dependency structure. We be-
lieve that an efficient and appropriate kernel can
be used to solve the RE problem, exploiting the
advantages of dependency structures, convolution
tree kernels and sequence kernels.
3 Support Vector Machines and Kernel
Methods
In this section we give a brief introduction to sup-
port vector machines, kernel methods, diverse tree
and sequence kernel spaces, which can be applied
to the RE task.
3.1 Support Vector Machines (SVMs)
Support Vector Machines refer to a supervised ma-
chine learning technique based on the latest results
of the statistical learning theory (Vapnik, 1998).
Given a vector space and a set of training points,
i.e. positive and negative examples, SVMs find a
separating hyperplane H(~x) = ~? ? ~x + b = 0
where ? ? R
n
and b ? R are learned by applying
the Structural Risk Minimization principle (Vap-
nik, 1995). SVMs is a binary classifier, but it can
be easily extended to multi-class classifier, e.g. by
means of the one-vs-all method (Rifkin and Pog-
gio, 2002).
One strong point of SVMs is the possibility to
apply kernel methods (robert Mller et al, 2001)
to implicitly map data in a new space where the
examples are more easily separable as described
in the next section.
3.2 Kernel Methods
Kernel methods (Schlkopf and Smola, 2001) are
an attractive alternative to feature-based methods
since the applied learning algorithm only needs
to compute a product between a pair of objects
(by means of kernel functions), avoiding the ex-
plicit feature representation. A kernel function
is a scalar product in a possibly unknown feature
space. More precisely, The object o is mapped in
~x with a feature function ? : O ? <
n
, whereO is
the set of the objects.
The kernel trick allows us to rewrite the deci-
sion hyperplane as:
H(~x) =
(
?
i=1..l
y
i
?
i
~x
i
)
? ~x+ b =
?
i=1..l
y
i
?
i
~x
i
? ~x+ b =
?
i=1..l
y
i
?
i
?(o
i
) ? ?(o) + b,
where y
i
is equal to 1 for positive and -1 for neg-
ative examples, ?
i
? < with ?
i
? 0, o
i
?i ?
{1, .., l} are the training instances and the product
K(o
i
, o) = ??(o
i
) ? ?(o)? is the kernel function
associated with the mapping ?.
Kernel engineering can be carried out by com-
bining basic kernels with additive or multiplica-
tive operators or by designing specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
Regarding NLP applications, kernel methods
have attracted much interest due to their ability
of implicitly exploring huge amounts of structural
features automatically extracted from the origi-
nal object representation. The kernels for struc-
tured natural language data, such as parse tree
kernel (Collins and Duffy, 2001) and string ker-
nel (Lodhi et al, 2002) are examples of the well-
known convolution kernels used in many NLP ap-
plications.
Tree kernels represent trees in terms of their
substructures (called tree fragments). Such frag-
ments form a feature space which, in turn, is
mapped into a vector space. Tree kernels mea-
sure the similarity between pair of trees by count-
ing the number of fragments in common. There
are three important characterizations of fragment
type (Moschitti, 2006): the SubTrees (ST), the
SubSet Trees (SST) and the Partial Trees (PT). For
sake of space, we do not report the mathematical
description of them, which is available in (Vish-
wanathan and Smola, 2002), (Collins and Duffy,
1380
2001) and (Moschitti, 2006), respectively. In con-
trast, we report some descriptions in terms of fea-
ture space that may be useful to understand the
new engineered kernels.
In principle, a SubTree (ST) is defined by tak-
ing any node along with its descendants. A Sub-
Set Tree (SST) is a more general structure which
does not necessarily include all the descendants. It
must be generated by applying the same grammat-
ical rule set, which generated the original tree. A
Partial Tree (PT) is a more general form of sub-
structures obtained by relaxing constraints over
the SST.
4 Kernels for Relation Extraction
In this section we describe the previous kernels
based on constituent trees as well as new kernels
based on diverse types of trees and sequences for
relation extraction. As mentioned in the previ-
ous section, we can engineer kernels by combin-
ing tree and sequence kernels. Thus we focus on
the problem to define structure embedding the de-
sired syntactic relational information between two
named entities (NEs).
4.1 Constituent and Dependency Structures
Syntactic parsing (or syntactic analysis) aims at
identifying grammatical structures in a text. A
parser thus captures the hidden hierarchy of the
input text and processes it into a form suitable for
further processing. There are two main paradigms
for representing syntactic information: constituent
and dependency parsing, which produces two dif-
ferent tree structures.
Constituent tree encodes structural properties
of a sentence. The parse tree contains constituents,
such as noun phrases (NP) and verb phrases (VP),
as well as terminals/part-of-speech tags, such as
determiners (DT) or nouns (NN). Figure 2.a shows
the constituent tree of the sentence: In Washing-
ton, U.S. officials are working overtime.
Dependency tree encodes grammatical rela-
tions between words in a sentence with the words
as nodes and dependency types as edges. An edge
from a word to another represents a grammatical
relation between these two. Every word in a de-
pendency tree has exactly one parent except the
root. Figure 2.b shows and example of the depen-
dency tree of the previous sentence.
Given two NEs, such as Washington and offi-
cials, both the above trees can encode the syntactic
dependencies between them. However, since each
parse tree corresponds to a sentence, there may be
more than two NEs and many relations expressed
in a sentence. Thus, the use of the entire parse
tree of the whole sentence holds two major draw-
backs: first, it may be too computationally expen-
sive for kernel calculation since the size of a com-
plete parse tree may be very large (up to 300 nodes
in the Penn Treebank (Marcus et al, 1993)); sec-
ond, there is ambiguity on the target pairs of NEs,
i.e. different NEs associated with different rela-
tions are described by the same parse tree. There-
fore, it is necessary to identify the portion of the
parse tree that best represent the useful syntactic
information.
Let e
1
and e
2
be two entity mentions in the same
sentence such that they are in a relationship R.
For the constituent parse tree, we used the path-
enclosed tree (PET), which was firstly proposed
in (Moschitti, 2004) for Semantic Role Labeling
and then adapted by (Zhang et al, 2005) for re-
lation extraction. It is the smallest common sub-
tree including the two entities of a relation. The
dashed frame in Figure 2.a surrounds PET associ-
ated with the two mentions, officials and Washing-
ton. Moreover, to improve the representation, two
extra nodes T1-PER, denoting the type PERSON,
and T2-LOC, denoting the type LOCATION, are
added to the parse tree, above the two target NEs,
respectively. In this example, the above PET is de-
signed to capture the relation Located-in between
the entities ?officials? and ?Washington? from the
ACE corpus. Note that, a third NE, U.S., is char-
acterized by the node GPE (GeoPolitical Entity),
where the absence of the prefix T1 or T2 before
the NE type (i.e. GPE), denotes that the NE does
not take part in the target relation.
In previous work, some dependency trees have
been used (Bunescu and Mooney, 2005a; Wang,
2008) but the employed kernel just exploited the
syntactic information concentrated in the path be-
tween e
1
and e
2
. In contrast, we defined and stud-
ied three different dependency structures whose
potential can be fully exploited by our convolution
partial tree kernel:
- Dependency Words (DW) tree is similar to
PET adapted for dependency tree constituted
by simple words. We select the minimal sub-
tree which includes e
1
and e
2
, and we insert
an extra node as father of the NEs, labeled
with the NE category. For example, given
1381
Figure 2: The constituent and dependency parse trees integrated with entity information
the tree in Figure 2.b, we design the tree in
Figure 2.c surrounded by the dashed frames,
where T1-PER, T2-LOC and GPE are the ex-
tra nodes inserted as fathers of Washington,
soldier and U.S..
- Grammatical Relation (GR) tree, i.e. the DW
tree in which words are replaced by their
grammatical functions, e.g. prep, pobj and
nsubj. For example, Figure 2.d, shows the
GR tree for the previous relation: In is re-
placed by prep , U.S. by nsubj and so on.
- Grammatical Relation and Words (GRW)
tree, words and grammatical functions are
both used in the tree, where the latter are in-
serted as a father node of the former. For
example, Figure 2.e, shows such tree for the
previous relation.
4.2 Sequential Structures
Some sequence kernels have been used on depen-
dency structures (Bunescu and Mooney, 2005b;
Wang, 2008). These kernels just used lexical
words with some syntactic information. To fully
exploit syntactic and semantic information, we de-
fined and studied six different sequences (in a style
similar to what proposed in (Moschitti, 2008)),
which include features from constituent and de-
pendency parse trees and NEs:
1. Sequence of terminals (lexical words) in the
PET (SK
1
), e.g.:
T2-LOC Washington , U.S. T1-PER officials.
2. Sequence of part-of-speech (POS) tags in the
PET (SK
2
), i.e. the SK
1
in which words are
replaced by their POS tags, e.g.:
T2-LOC NN , NNP T1-PER NNS.
3. Sequence of grammatical relations in the
1382
PET (SK
3
), i.e. the SK
1
in which words are
replaced by their grammatical functions, e.g.:
T2-LOC pobj , nn T1-PER nsubj.
4. Sequence of words in the DW (SK
4
), e.g.:
Washington T2-LOC In working T1-PER of-
ficials GPE U.S..
5. Sequence of grammatical relations in the GR
(SK
5
), i.e. the SK
4
in which words are re-
placed by their grammatical functions, e.g.:
pobj T2-LOC prep ROOT T1-PER nsubj GPE
nn.
6. Sequence of POS tags in the DW (SK
6
), i.e.
the SK
4
in which words are replaced by their
POS tags, e.g.:
NN T2-LOC IN VBP T1-PER NNS GPE
NNP.
It is worth noting that the potential information
contained in such sequences can be fully exploited
by the word sequence kernel.
4.3 Combining Kernels
Given that syntactic information from different
parse trees may have different impact on relation
extraction (RE), the viable approach to study the
role of dependency and constituent parsing is to
experiment with different syntactic models and
measuring the impact in terms of RE accuracy.
For this purpose we compared the composite ker-
nel described in (Zhang et al, 2006) with the par-
tial tree kernels applied to DW , GR, and GRW
and sequence kernels based on six sequences de-
scribed above. The composite kernels include
polynomial kernel applied to entity-related feature
vector. The word sequence kernel (WSK) is al-
ways applied to sequential structures. The used
kernels are described in more detail below.
4.3.1 Polynomial Kernel
The basic kernel between two named entities of
the ACE documents is defined as:
K
P
(R
1
, R
2
) =
?
i=1,2
K
E
(R
1
.E
i
, R
2
.E
i
),
where R
1
and R
2
are two relation instances, E
i
is
the i
th
entity of a relation instance. K
E
(?, ?) is a
kernel over entity features, i.e.:
K
E
(E
1
, E
2
) = (1 + ~x
1
? ~x
2
)
2
,
where ~x
1
and ~x
2
are two feature vectors extracted
from the two NEs.
For the ACE 2004, the features used include:
entity headword, entity type, entity subtype, men-
tion type, and LDC
2
mention type. The last four
attributes are taken from the ACE corpus 2004. In
ACE, each mention has a head annotation and an
extent annotation.
4.3.2 Kernel Combinations
1. Polynomial kernel plus a tree kernel:
CK
1
= ? ?K
P
+ (1? ?) ?K
x
,
where ? is a coefficient to give more impact
to K
P
and K
x
is either the partial tree ker-
nel applied to one the possible dependency
structures, DW, GR or GRW or the SST ker-
nel applied to PET, described in the previous
section.
2. Polynomial kernel plus constituent plus de-
pendency tree kernels:
CK
2
= ? ?K
P
+ (1? ?) ? (K
SST
+K
PT
)
where K
SST
is the SST kernel and K
PT
is
the partial tree kernel (applied to the related
structures as in point 1).
3. Constituent tree plus square of polynomial
kernel and dependency tree kernel:
CK
3
= ? ?K
SST
+ (1??) ? (K
P
+K
PT
)
2
4. Dependency word tree plus grammatical re-
lation tree kernels:
CK
4
= K
PT?DW
+K
PT?GR
where K
PT?DW
and K
PT?GR
are the par-
tial tree kernels applied to dependency struc-
tures DW and GR.
5. Polynomial kernel plus dependency word
plus grammatical relation tree kernels:
CK
5
= ??K
P
+(1??)?(K
PT?DW
+K
PT?GR
)
Some preliminary experiments on a validation set
showed that the second, the fourth and the fifth
combinations yield the best performance with ? =
0.4 while the first and the third combinations yield
the best performance with ? = 0.23.
Regarding WSK, the following combinations
are applied:
2
Linguistic Data Consortium (LDC):
http://www.ldc.upenn.edu/Projects/ACE/
1383
1. SK
3
+ SK
4
2. SK
3
+ SK
6
3. SSK =
?
i=1,..,6
SK
i
4. K
SST
+ SSK
5. CSK = ? ?K
P
+ (1??) ? (K
SST
+SSK)
Preliminary experiments showed that the last com-
bination yields the best performance with ? =
0.23.
We used a polynomial expansion to explore the
bi-gram features of i) the first and the second en-
tity participating in the relation, ii) grammatical
relations which replace words in the dependency
tree. Since the kernel function set is closed un-
der normalization, polynomial expansion and lin-
ear combination (Schlkopf and Smola, 2001), all
the illustrated composite kernels are also proper
kernels.
5 Experiments
Our experiments aim at investigating the effec-
tiveness of convolution kernels adapted to syntac-
tic parse trees and various sequence kernels for
the RE task. For this purpose, we use the sub-
set and partial tree kernel over different kinds of
trees, namely constituent and dependency syntac-
tic parse trees. Diverse sequences are applied indi-
vidually and in combination together. We consider
our task of relation extraction as a classification
problem where categories are relation types. All
pairs of entity mentions in the same sentence are
taken to generate potential relations, which will be
processed as positive and negative examples.
5.1 Experimental setup
We use the newswire and broadcast news domain
in the English portion of the ACE 2004 corpus
provided by LDC. This data portion includes 348
documents and 4400 relation instances. It defines
seven entity types and seven relation types. Every
relation is assigned one of the seven types: Phys-
ical, Person/Social, Employment/Membership/-
Subsidiary, Agent-Artifact, PER/ORG Affiliation,
GPE Affiliation, and Discourse. For sake of space,
we do not explain these relationships here, never-
theless, they are explicitly described in the ACE
document guidelines. There are 4400 positive and
38,696 negative examples when generating pairs
of entity mentions as potential relations.
Documents are parsed using Stanford
Parser (Klein and Manning, 2003) to pro-
duce parse trees. Potential relations are generated
by iterating all pairs of entity mentions in the same
sentence. Entity information, namely entity type,
is integrated into parse trees. To train and test our
binary relation classifier, we used SVMs. Here,
relation detection is formulated as a multiclass
classification problem. The one vs. rest strategy
is employed by selecting the instance with largest
margin as the final answer. For experimentation,
we use 5-fold cross-validation with the Tree
Kernel Tools (Moschitti, 2004) (available at
http://disi.unitn.it/?moschitt/Tree-Kernel.htm).
5.2 Results
In this section, we report the results of different
kernels setup over constituent (CT) and depen-
dency (DP) parse trees and sequences taken from
these parse trees. The tree kernel (TK), compos-
ite kernel (CK
1
, CK
2
, CK
3
, CK
4
, and CK
5
corresponding to five combination types in Sec-
tion 4.3.2) were employed over these two syntactic
trees. For the tree kernel, we apply the SST kernel
for the path-enclosed tree (PET) of the constituent
tree and the PT kernel for three kinds of depen-
dency tree DW, GR, and GRW, described in the
previous section. The two composite kernels CK
2
and CK
3
are applied over both two parse trees.
The word sequence kernels are applied over six
sequences SK
1
, SK
2
, SK
3
, SK
4
, SK
5
, and SK
6
(described in Section 4.3).
The results are shown in Table 1 and Table 2.
In the first table, the first column indicates the
structure used in the combination shown in the
second column, e.g. PET associated with CK
1
means that the SST kernel is applied on PET (a
portion of the constituent tree) and combined with
the CK
1
schema whereas PET and GR associated
with CK
5
means that SST kernel is applied to
PET and PT kernel is applied to GR in CK
5
. The
remaining three columns report Precision, Recall
and F1 measure. The interpretation of the second
table is more immediate since the only tree ker-
nel involved is the SST kernel applied to PET and
combined by means of CK
1
.
We note that: first, the dependency kernels,
i.e. the results on the rows from 3 to 6 are be-
low the composite kernel CK
1
, i.e. 68.9. This
is the state-of-the-art in RE, designed by (Zhang
et al, 2006), where our implementation provides
1384
Parse Tree Kernel P R F
PET CK
1
69.5 68.3 68.9
DW CK
1
53.2 59.7 56.3
GR CK
1
58.8 61.7 60.2
GRW CK
1
56.1 61.2 58.5
DW and GR CK
5
59.7 64.1 61.8
PET and GR
CK
2
70.7 69.0 69.8
CK
3
70.8 70.2 70.5
Table 1: Results on the ACE 2004 evaluation test
set. Six structures were experimented over the
constituent and dependency trees.
Kernel P R F
CK
1
69.5 68.3 68.9
SK
1
72.0 52.8 61.0
SK
2
61.7 60.0 60.8
SK
3
62.6 60.7 61.6
SK
4
73.1 50.3 59.7
SK
5
59.0 60.7 59.8
SK
6
57.7 61.8 59.7
SK
3
+ SK
4
75.0 63.4 68.8
SK
3
+ SK
6
66.8 65.1 65.9
SSK =
?
i
SK
i
73.8 66.2 69.8
CSK 75.6 66.6 70.8
CK
1
+ SSK 76.6 67.0 71.5
(Zhou et al, 2007)
82.2 70.2 75.8
CK
1
with Heuristics
Table 2: Performance comparison on the ACE
2004 data with different kernel setups.
a slightly smaller result than the original version
(i.e. an F1 of about 72 using a different syntactic
parser).
Second, CK
1
improves to 70.5, when the con-
tribution of PT kernel applied to GR (dependency
tree built using grammatical relations) is added.
This suggests that dependency structures are effec-
tively exploited by PT kernel and that such infor-
mation is somewhat complementary to constituent
trees.
Third, in the second table, the model CK
1
+
SSK, which adds to CK
1
the contribution of di-
verse sequence kernels, outperforms the state-of-
the-art by 2.6%. This suggests that the sequential
information encoded by several sequence kernels
can better represents the dependency information.
Finally, we also report in the last row (in italic)
the superior RE result by (Zhou et al, 2007).
However, to achieve this outcome the authors used
the composite kernel CK
1
with several heuristics
to define an effective portion of constituent trees.
Such heuristics expand the tree and remove unnec-
essary information allowing a higher improvement
on RE. They are tuned on the target RE task so al-
though the result is impressive, we cannot use it to
compare with pure automatic learning approaches,
such us our models.
6 Conclusion and Future Work
In this paper, we study the use of several types
of syntactic information: constituent and depen-
dency syntactic parse trees. A relation is repre-
sented by taking the path-enclosed tree (PET) of
the constituent tree or of the path linking two enti-
ties of the dependency tree. For the design of auto-
matic relation classifiers, we have investigated the
impact of dependency structures to the RE task.
Our novel composite kernels, which account for
the two syntactic structures, are experimented with
the appropriate convolution kernels and show sig-
nificant improvement with respect to the state-of-
the-art in RE.
Regarding future work, there are many research
line that may be followed:
i) Capturing more features by employing ex-
ternal knowledge such as ontological, lexical re-
source or WordNet-based features (Basili et al,
2005a; Basili et al, 2005b; Bloehdorn et al, 2006;
Bloehdorn and Moschitti, 2007) or shallow se-
mantic trees, (Giuglea and Moschitti, 2004; Giu-
glea and Moschitti, 2006; Moschitti and Bejan,
2004; Moschitti et al, 2007; Moschitti, 2008;
Moschitti et al, 2008).
ii) Design a new tree-based structures, which
combines the information of both constituent and
dependency parses. From dependency trees we
can extract more precise but also more sparse
relationships (which may cause overfit). From
constituent trees, we can extract subtrees consti-
tuted by non-terminal symbols (grammar sym-
bols), which provide a better generalization (with
a risk of underfitting).
iii) Design a new kernel which can integrate the
advantages of the constituent and dependency tree.
The new tree kernel should inherit the benefits of
the three available tree kernels: ST, SST or PT.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
1385
lections. In Proceedings of the 5th ACM Interna-
tional Conference on Digital Libraries.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005a. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 1?8, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005b. A semantic kernel to classify texts
with very few training examples. In In Proceedings
of the Workshop on Learning in Web Search, at the.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Structure and semantics for expressive text ker-
nels. In CIKM ?07: Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 861?864, New York,
NY, USA. ACM.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining (ICDM
06), Hong Kong, 18-22 December 2006, DEC.
Sergey Brin. 1998. Extracting patterns and relations
from world wide web. In Proceeding of WebDB
Workshop at 6th International Conference on Ex-
tending Database Technology, pages 172?183.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of EMNLP, pages 724?731.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of EMNLP.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence ker-
nels. Journal of Machine Learning Research, pages
1059?1082.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neu-
ral Information Processing Systems (NIPS?2001).
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting on ACL, Barcelona,
Spain.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge discovery using framenet, verbnet and
propbank. In A. Meyers, editor, Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic Role Labeling via Framenet, Verbnet and
Propbank. In Proceedings of ACL 2006, Sydney,
Australia.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named en-
tities from large corpora. In Proceedings of the 42nd
Annual Meeting on ACL, Barcelona, Spain.
Nanda Kambhatla. 2004. Combining lexical, syntactic
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions, Barcelona, Spain.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the ACL, pages 423?430.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, , and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, pages 419?444.
Mitchell P. Marcus, Beatrice Santorini, , and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: the penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Scott Miller, Heidi Fox, Lance Ramshaw, , and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st conference on North American chapter of the
ACL, pages 226?233, Seattle, USA.
Alessandro Moschitti and Cosmin Bejan. 2004. A se-
mantic kernel for predicate argument classification.
In CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question/answer classification. In Proceedings of
ACL?07, Prague, Czech Republic.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Meeting of the ACL, Barcelona,
Spain.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning, Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM ?08: Proceeding of the 17th ACM conference
on Information and knowledge management, pages
253?262, New York, NY, USA. ACM.
Ryan Michael Rifkin and Tomaso Poggio. 2002. Ev-
erything old is new again: a fresh look at historical
approaches in machine learning. PhD thesis, Mas-
sachusetts Institute of Technology.
1386
Klaus robert Mller, Sebastian Mika, Gunnar Rtsch,
Koji Tsuda, , and Bernhard Schlkopf. 2001. An
introduction to kernel-based learning algorithms.
IEEE Transactions on Neural Networks, 12(2):181?
201.
Dan Roth and Wen tau Yih. 2002. Probabilistic rea-
soning for entity and relation recognition. In Pro-
ceedings of the COLING-2002, Taipei, Taiwan.
Bernhard Schlkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer?Verlag, New York.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. John Wiley and Sons, New York.
S.V.N. Vishwanathan and Alexander J. Smola. 2002.
Fast kernels on strings and trees. In Proceedings of
Neural Information Processing Systems.
Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In Pro-
ceedings of the 3rd International Joint Conference
on Natural Language Processing-IJCNLP.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations be-
tween named entities from a large raw corpus using
tree similarity-based clustering. In Proceedings of
IJCNLP?2005, Lecture Notes in Computer Science
(LNCS 3651), pages 378?389, Jeju Island, South
Korea.
Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of COLING-ACL 2006, pages 825?
832.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Meeting of the
ACL, pages 419?426, Ann Arbor, Michigan, USA.
GuoDong Zhou, Jian Su, Jie Zhang, , and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Meeting of the
ACL, pages 427?434, Ann Arbor, USA, June.
GuoDong Zhou, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse
tree information. In Proceedings of EMNLP-CoNLL
2007, pages 728?736.
1387
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 202?210,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Re-Ranking Models For Spoken Language Understanding
Marco Dinarelli
University of Trento
Italy
dinarelli@disi.unitn.it
Alessandro Moschitti
University of Trento
Italy
moschitti@disi.unitn.it
Giuseppe Riccardi
University of Trento
Italy
riccardi@disi.unitn.it
Abstract
Spoken Language Understanding aims at
mapping a natural language spoken sen-
tence into a semantic representation. In
the last decade two main approaches have
been pursued: generative and discrimi-
native models. The former is more ro-
bust to overfitting whereas the latter is
more robust to many irrelevant features.
Additionally, the way in which these ap-
proaches encode prior knowledge is very
different and their relative performance
changes based on the task. In this pa-
per we describe a machine learning frame-
work where both models are used: a gen-
erative model produces a list of ranked hy-
potheses whereas a discriminative model
based on structure kernels and Support
Vector Machines, re-ranks such list. We
tested our approach on the MEDIA cor-
pus (human-machine dialogs) and on a
new corpus (human-machine and human-
human dialogs) produced in the Euro-
pean LUNA project. The results show a
large improvement on the state-of-the-art
in concept segmentation and labeling.
1 Introduction
In Spoken Dialog Systems, the Language Under-
standing module performs the task of translating
a spoken sentence into its meaning representation
based on semantic constituents. These are the
units for meaning representation and are often re-
ferred to as concepts. Concepts are instantiated by
sequences of words, therefore a Spoken Language
Understanding (SLU) module finds the association
between words and concepts.
In the last decade two major approaches have
been proposed to find this correlation: (i) gener-
ative models, whose parameters refer to the joint
probability of concepts and constituents; and (ii)
discriminative models, which learn a classifica-
tion function to map words into concepts based
on geometric and statistical properties. An ex-
ample of generative model is the Hidden Vector
State model (HVS) (He and Young, 2005). This
approach extends the discrete Markov model en-
coding the context of each state as a vector. State
transitions are performed as stack shift operations
followed by a push of a preterminal semantic cat-
egory label. In this way the model can capture se-
mantic hierarchical structures without the use of
tree-structured data. Another simpler but effec-
tive generative model is the one based on Finite
State Transducers. It performs SLU as a transla-
tion process from words to concepts using Finite
State Transducers (FST). An example of discrim-
inative model used for SLU is the one based on
Support Vector Machines (SVMs) (Vapnik, 1995),
as shown in (Raymond and Riccardi, 2007). In
this approach, data are mapped into a vector space
and SLU is performed as a classification problem
using Maximal Margin Classifiers (Shawe-Taylor
and Cristianini, 2004).
Generative models have the advantage to be
more robust to overfitting on training data, while
discriminative models are more robust to irrele-
vant features. Both approaches, used separately,
have shown a good performance (Raymond and
Riccardi, 2007), but they have very different char-
acteristics and the way they encode prior knowl-
edge is very different, thus designing models able
to take into account characteristics of both ap-
proaches are particularly promising.
In this paper we propose a method for SLU
based on generative and discriminative models:
the former uses FSTs to generate a list of SLU hy-
potheses, which are re-ranked by SVMs. These
exploit all possible word/concept subsequences
(with gaps) of the spoken sentence as features (i.e.
all possible n-grams). Gaps allow for the encod-
202
ing of long distance dependencies between words
in relatively small n-grams. Given the huge size
of this feature space, we adopted kernel methods
and in particular sequence kernels (Shawe-Taylor
and Cristianini, 2004) and tree kernels (Raymond
and Riccardi, 2007; Moschitti and Bejan, 2004;
Moschitti, 2006) to implicitly encode n-grams and
other structural information in SVMs.
We experimented with different approaches for
training the discriminative models and two dif-
ferent corpora: the well-known MEDIA corpus
(Bonneau-Maynard et al, 2005) and a new corpus
acquired in the European project LUNA1 (Ray-
mond et al, 2007). The results show a great
improvement with respect to both the FST-based
model and the SVM model alone, which are the
current state-of-the-art for concept classification
on such corpora. The rest of the paper is orga-
nized as follows: Sections 2 and 3 show the gener-
ative and discriminative models, respectively. The
experiments and results are reported in Section 4
whereas the conclusions are drawn in Section 5.
2 Generative approach for concept
classification
In the context of Spoken Language Understanding
(SLU), concept classification is the task of asso-
ciating the best sequence of concepts to a given
sentence, i.e. word sequence. A concept is a class
containing all the words carrying out the same se-
mantic meaning with respect to the application do-
main. In SLU, concepts are used as semantic units
and are represented with concept tags. The associ-
ation between words and concepts is learned from
an annotated corpus.
The Generative model used in our work for con-
cept classification is the same used in (Raymond
and Riccardi, 2007). Given a sequence of words
as input, a translation process based on FST is
performed to output a sequence of concept tags.
The translation process involves three steps: (1)
the mapping of words into classes (2) the mapping
of classes into concepts and (3) the selection of the
best concept sequence.
The first step is used to improve the generaliza-
tion power of the model. The word classes at this
level can be both domain-dependent, e.g. ?Hotel?
in MEDIA or ?Software? in the LUNA corpus, or
domain-independent, e.g. numbers, dates, months
1Contract n. 33549
etc. The class of a word not belonging to any class
is the word itself.
In the second step, classes are mapped into con-
cepts. The mapping is not one-to-one: a class
may be associated with more than one concept, i.e.
more than one SLU hypothesis can be generated.
In the third step, the best or the m-best hy-
potheses are selected among those produced in the
previous step. They are chosen according to the
maximum probability evaluated by the Conceptual
Language Model, described in the next section.
2.1 Stochastic Conceptual Language Model
(SCLM)
An SCLM is an n-gram language model built on
semantic tags. Using the same notation proposed
in (Moschitti et al, 2007) and (Raymond and Ric-
cardi, 2007), our SCLM trains joint probability
P (W,C) of word and concept sequences from an
annotated corpus:
P (W,C) =
k
?
i=1
P (wi, ci|hi),
where W = w1..wk, C = c1..ck and
hi = wi?1ci?1..w1c1. Since we use a 3-gram
conceptual language model, the history hi is
{wi?1ci?1, wi?2ci?2}.
All the steps of the translation process described
here and above are implemented as Finite State
Transducers (FST) using the AT&T FSM/GRM
tools and the SRILM (Stolcke, 2002) tools. In
particular the SCLM is trained using SRILM tools
and then converted to an FST. This allows the use
of a wide set of stochastic language models (both
back-off and interpolated models with several dis-
counting techniques like Good-Turing, Witten-
Bell, Natural, Kneser-Ney, Unchanged Kneser-
Ney etc). We represent the combination of all the
translation steps as a transducer ?SLU (Raymond
and Riccardi, 2007) in terms of FST operations:
?SLU = ?W ? ?W2C ? ?SLM ,
where ?W is the transducer representation of the
input sentence, ?W2C is the transducer mapping
words to classes and ?SLM is the Semantic Lan-
guage Model (SLM) described above. The best
SLU hypothesis is given by
C = projectC(bestpath1(?SLU )),
where bestpathn (in this case n is 1 for the 1-best
hypothesis) performs a Viterbi search on the FST
203
and outputs the n-best hypotheses and projectC
performs a projection of the FST on the output la-
bels, in this case the concepts.
2.2 Generation of m-best concept labeling
Using the FSTs described above, we can generate
m best hypotheses ranked by the joint probability
of the SCLM.
After an analysis of the m-best hypotheses of
our SLU model, we noticed that many times the
hypothesis ranked first by the SCLM is not the
closest to the correct concept sequence, i.e. its er-
ror rate using the Levenshtein alignment with the
manual annotation of the corpus is not the low-
est among the m hypotheses. This means that
re-ranking the m-best hypotheses in a convenient
way could improve the SLU performance. The
best choice in this case is a discriminative model,
since it allows for the use of informative features,
which, in turn, can model easily feature dependen-
cies (also if they are infrequent in the training set).
3 Discriminative re-ranking
Our discriminative re-ranking is based on SVMs
or a perceptron trained with pairs of conceptually
annotated sentences. The classifiers learn to select
which annotation has an error rate lower than the
others so that the m-best annotations can be sorted
based on their correctness.
3.1 SVMs and Kernel Methods
Kernel Methods refer to a large class of learning
algorithms based on inner product vector spaces,
among which Support Vector Machines (SVMs)
are one of the most well known algorithms. SVMs
and perceptron learn a hyperplane H(~x) = ~w~x +
b = 0, where ~x is the feature vector represen-
tation of a classifying object o, ~w ? Rn (a
vector space) and b ? R are parameters (Vap-
nik, 1995). The classifying object o is mapped
into ~x by a feature function ?. The kernel trick
allows us to rewrite the decision hyperplane as
?
i=1..l yi?i?(oi)?(o) + b = 0, where yi is equal
to 1 for positive and -1 for negative examples,
?i ? R+, oi?i ? {1..l} are the training instances
and the product K(oi, o) = ??(oi)?(o)? is the ker-
nel function associated with the mapping ?. Note
that we do not need to apply the mapping ?, we
can use K(oi, o) directly (Shawe-Taylor and Cris-
tianini, 2004). For example, next section shows a
kernel function that counts the number of word se-
quences in common between two sentences, in the
space of n-grams (for any n).
3.2 String Kernels
The String Kernels that we consider count the
number of substrings containing gaps shared by
two sequences, i.e. some of the symbols of the
original string are skipped. Gaps modify the
weight associated with the target substrings as
shown in the following.
Let ? be a finite alphabet, ?? = ??n=0 ?n is the
set of all strings. Given a string s ? ??, |s| denotes
the length of the strings and si its compounding
symbols, i.e s = s1..s|s|, whereas s[i : j] selects
the substring sisi+1..sj?1sj from the i-th to the
j-th character. u is a subsequence of s if there
is a sequence of indexes ~I = (i1, ..., i|u|), with
1 ? i1 < ... < i|u| ? |s|, such that u = si1 ..si|u|
or u = s[~I] for short. d(~I) is the distance between
the first and last character of the subsequence u in
s, i.e. d(~I) = i|u| ? i1 + 1. Finally, given s1, s2
? ??, s1s2 indicates their concatenation.
The set of all substrings of a text corpus forms a
feature space denoted by F = {u1, u2, ..} ? ??.
To map a string s in R? space, we can use the
following functions: ?u(s) =
P
~I:u=s[~I] ?
d(~I) for
some ? ? 1. These functions count the num-
ber of occurrences of u in the string s and assign
them a weight ?d(~I) proportional to their lengths.
Hence, the inner product of the feature vectors for
two strings s1 and s2 returns the sum of all com-
mon subsequences weighted according to their
frequency of occurrences and lengths, i.e.
SK(s1, s2) =
X
u???
?u(s1) ??u(s2) =
X
u???
X
~I1:u=s1[~I1]
?d( ~I1)
X
~I2:u=s2[~I2]
?d( ~I2) =
X
u???
X
~I1:u=s1[~I1]
X
~I2:u=s2[~I2]
?d( ~I1)+d( ~I2),
where d(.) counts the number of characters in the
substrings as well as the gaps that were skipped in
the original string. It is worth noting that:
(a) longer subsequences receive lower weights;
(b) some characters can be omitted, i.e. gaps;
and
(c) gaps determine a weight since the exponent
of ? is the number of characters and gaps be-
tween the first and last character.
204
Characters in the sequences can be substituted
with any set of symbols. In our study we pre-
ferred to use words so that we can obtain word
sequences. For example, given the sentence: How
may I help you ? sample substrings, extracted by
the Sequence Kernel (SK), are: How help you ?,
How help ?, help you, may help you, etc.
3.3 Tree kernels
Tree kernels represent trees in terms of their sub-
structures (fragments). The kernel function de-
tects if a tree subpart (common to both trees) be-
longs to the feature space that we intend to gen-
erate. For such purpose, the desired fragments
need to be described. We consider two important
characterizations: the syntactic tree (STF) and the
partial tree (PTF) fragments.
3.3.1 Tree Fragment Types
An STF is a general subtree whose leaves can be
non-terminal symbols. For example, Figure 1(a)
shows 10 STFs (out of 17) of the subtree rooted in
VP (of the left tree). The STFs satisfy the con-
straint that grammatical rules cannot be broken.
For example, [VP [V NP]] is an STF, which
has two non-terminal symbols, V and NP, as leaves
whereas [VP [V]] is not an STF. If we relax
the constraint over the STFs, we obtain more gen-
eral substructures called partial trees fragments
(PTFs). These can be generated by the application
of partial production rules of the grammar, con-
sequently [VP [V]] and [VP [NP]] are valid
PTFs. Figure 1(b) shows that the number of PTFs
derived from the same tree as before is still higher
(i.e. 30 PTs).
3.4 Counting Shared SubTrees
The main idea of tree kernels is to compute the
number of common substructures between two
trees T1 and T2 without explicitly considering the
whole fragment space. To evaluate the above ker-
nels between two T1 and T2, we need to define a
set F = {f1, f2, . . . , f|F|}, i.e. a tree fragment
space and an indicator function Ii(n), equal to 1
if the target fi is rooted at node n and equal to 0
otherwise. A tree-kernel function over T1 and T2
is TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2),
where NT1 and NT2 are the sets of the T1?s
and T2?s nodes, respectively and ?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2). The latter is equal to the num-
ber of common fragments rooted in the n1 and
n2 nodes. In the following sections we report the
equation for the efficient evaluation of ? for ST
and PT kernels.
3.5 Syntactic Tree Kernels (STK)
The ? function depends on the type of fragments
that we consider as basic features. For example,
to evaluate the fragments of type STF, it can be
defined as:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the
same, and n1 and n2 have only leaf children
(i.e. they are pre-terminals symbols) then
?(n1, n2) = 1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) =
nc(n1)
?
j=1
(? + ?(cjn1 , c
j
n2)) (1)
where ? ? {0, 1}, nc(n1) is the number of chil-
dren of n1 and cjn is the j-th child of the node
n. Note that, since the productions are the same,
nc(n1) = nc(n2). ?(n1, n2) evaluates the num-
ber of STFs common to n1 and n2 as proved in
(Collins and Duffy, 2002).
Moreover, a decay factor ? can be added by
modifying steps (2) and (3) as follows2:
2. ?(n1, n2) = ?,
3. ?(n1, n2) = ?
?nc(n1)
j=1 (? + ?(c
j
n1 , c
j
n2)).
The computational complexity of Eq. 1 is
O(|NT1 | ? |NT2 |) but as shown in (Moschitti,
2006), the average running time tends to be lin-
ear, i.e. O(|NT1 | + |NT2 |), for natural language
syntactic trees.
3.6 The Partial Tree Kernel (PTK)
PTFs have been defined in (Moschitti, 2006).
Their computation is carried out by the following
? function:
1. if the node labels of n1 and n2 are different
then ?(n1, n2) = 0;
2. else ?(n1, n2) =
1+?~I1,~I2,l(~I1)=l(~I2)
?l(~I1)
j=1 ?(cn1(~I1j), cn2(~I2j))
2To have a similarity score between 0 and 1, we also apply
the normalization in the kernel space, i.e.:
K?(T1, T2) = TK(T1 ,T2)?TK(T1 ,T1)?TK(T2 ,T2) .
205
NP 
D N 
a 
  cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a 
   cat 
  cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
(a) Syntactic Tree fragments (STF)
NP 
D N 
VP 
V 
brought 
a 
   cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
a 
   cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
(b) Partial Tree fragments (PTF)
Figure 1: Examples of different classes of tree fragments.
where ~I1 = ?h1, h2, h3, ..? and ~I2 =
?k1, k2, k3, ..? are index sequences associated with
the ordered child sequences cn1 of n1 and cn2 of
n2, respectively, ~I1j and ~I2j point to the j-th child
in the corresponding sequence, and, again, l(?) re-
turns the sequence length, i.e. the number of chil-
dren.
Furthermore, we add two decay factors: ? for
the depth of the tree and ? for the length of the
child subsequences with respect to the original se-
quence, i.e. we account for gaps. It follows that
?(n1, n2) =
?
(
?2+
?
~I1,~I2,l(~I1)=l(~I2)
?d(~I1)+d(~I2)
l(~I1)
?
j=1
?(cn1(~I1j), cn2(~I2j))
)
,
(2)
where d(~I1) = ~I1l(~I1) ? ~I11 and d(~I2) = ~I2l(~I2) ?
~I21. This way, we penalize both larger trees and
child subsequences with gaps. Eq. 2 is more gen-
eral than Eq. 1. Indeed, if we only consider the
contribution of the longest child sequence from
node pairs that have the same children, we imple-
ment the STK kernel.
3.7 Re-ranking models using sequences
The FST generates the m most likely concept an-
notations. These are used to build annotation
pairs,
?
si, sj
?
, which are positive instances if si
has a lower concept annotation error than sj , with
respect to the manual annotation in the corpus.
Thus, a trained binary classifier can decide if si
is more accurate than sj . Each candidate anno-
tation si is described by a word sequence where
each word is followed by its concept annotation.
For example, given the sentence:
ho (I have) un (a) problema (problem) con
(with) la (the) scheda di rete (network card) ora
(now)
a pair of annotations
?
si, sj
?
could be
si: ho NULL un NULL problema PROBLEM-B con
NULL la NULL scheda HW-B di HW-I rete HW-I ora
RELATIVETIME-B
sj: ho NULL un NULL problema ACTION-B con
NULL la NULL scheda HW-B di HW-B rete HW-B ora
RELATIVETIME-B
where NULL, ACTION, RELATIVETIME,
and HW are the assigned concepts whereas B and
I are the usual begin and internal tags for concept
subparts. The second annotation is less accurate
than the first since problema is annotated as an ac-
tion and ?scheda di rete? is split in three different
concepts.
Given the above data, the sequence kernel
is used to evaluate the number of common n-
grams between si and sj . Since the string ker-
nel skips some elements of the target sequences,
the counted n-grams include: concept sequences,
word sequences and any subsequence of words
and concepts at any distance in the sentence.
Such counts are used in our re-ranking function
as follows: let ei be the pair
?
s1i , s2i
?
we evaluate
the kernel:
KR(e1, e2) = SK(s11, s12) + SK(s21, s22) (3)
? SK(s11, s22)? SK(s21, s12)
This schema, consisting in summing four differ-
ent kernels, has been already applied in (Collins
and Duffy, 2002) for syntactic parsing re-ranking,
where the basic kernel was a tree kernel instead of
SK and in (Moschitti et al, 2006), where, to re-
rank Semantic Role Labeling annotations, a tree
kernel was used on a semantic tree similar to the
one introduced in the next section.
3.8 Re-ranking models using trees
Since the aim in concept annotation re-ranking is
to exploit innovative and effective source of infor-
mation, we can use the power of tree kernels to
generate correlation between concepts and word
structures.
Fig. 2 describes the structural association be-
tween the concept and the word level. This kind of
trees allows us to engineer new kernels and con-
sequently new features (Moschitti et al, 2008),
206
Figure 2: An example of the semantic tree used for STK or PTK
Corpus Train set Test set
LUNA words concepts words concepts
Dialogs WOZ 183 67
Dialogs HH 180 -
Turns WOZ 1.019 373
Turns HH 6.999 -
Tokens WOZ 8.512 2.887 2.888 984
Tokens WOZ 62.639 17.423 - -
Vocab. WOZ 1.172 34 - -
Vocab. HH 4.692 49 - -
OOV rate - - 3.2% 0.1%
Table 1: Statistics on the LUNA corpus
Corpus Train set Test set
Media words concepts words concepts
Turns 12,922 3,518
# of tokens 94,912 43,078 26,676 12,022
Vocabulary 5,307 80 - -
OOV rate - - 0.01% 0.0%
Table 2: Statistics on the MEDIA corpus
e.g. their subparts extracted by STK or PTK, like
the tree fragments in figures 1(a) and 1(b). These
can be used in SVMs to learn the classification of
words in concepts.
More specifically, in our approach, we use tree
fragments to establish the order of correctness
between two alternative annotations. Therefore,
given two trees associated with two annotations, a
re-ranker based on tree kernel, KR, can be built
in the same way of the sequence-based kernel by
substituting SK in Eq. 3 with STK or PTK.
4 Experiments
In this section, we describe the corpora, param-
eters, models and results of our experiments of
word chunking and concept classification. Our
baseline relates to the error rate of systems based
on only FST and SVMs. The re-ranking models
are built on the FST output. Different ways of
producing training data for the re-ranking models
determine different results.
4.1 Corpora
We used two different speech corpora:
The corpus LUNA, produced in the homony-
mous European project is the first Italian corpus
of spontaneous speech on spoken dialog: it is
based on the help-desk conversation in the domain
of software/hardware repairing (Raymond et al,
2007). The data are organized in transcriptions
and annotations of speech based on a new multi-
level protocol. Data acquisition is still in progress.
Currently, 250 dialogs acquired with a WOZ ap-
proach and 180 Human-Human (HH) dialogs are
available. Statistics on LUNA corpus are reported
in Table 1.
The corpus MEDIA was collected within
the French project MEDIA-EVALDA (Bonneau-
Maynard et al, 2005) for development and evalu-
ation of spoken understanding models and linguis-
tic studies. The corpus is composed of 1257 di-
alogs, from 250 different speakers, acquired with
a Wizard of Oz (WOZ) approach in the context
of hotel room reservations and tourist information.
Statistics on transcribed and conceptually anno-
tated data are reported in Table 2.
4.2 Experimental setup
We defined two different training sets in the
LUNA corpus: one using only the WOZ train-
ing dialogs and one merging them with the HH
dialogs. Given the small size of LUNA corpus, we
did not carried out parameterization on a develop-
ment set but we used default or a priori parameters.
We experimented with LUNA WOZ and six re-
rankers obtained with the combination of SVMs
and perceptron (PCT) with three different types
of kernels: Syntactic Tree Kernel (STK), Partial
Tree kernels (PTK) and the String Kernel (SK) de-
scribed in Section 3.3.
Given the high number and the cost of these ex-
periments, we ran only one model, i.e. the one
207
Corpus LUNA WOZ+HH MEDIA
Approach (STK) MT ST MT
FST 18.2 18.2 12.6
SVM 23.4 23.4 13.7
RR-A 15.6 17.0 11.6
RR-B 16.2 16.5 11.8
RR-C 16.1 16.4 11.7
Table 3: Results of experiments (CER) using FST
and SVMs with the Sytntactic Tree Kernel (STK)
on two different corpora: LUNA WOZ + HH, and
MEDIA.
based on SVMs and STK3 , on the largest datasets,
i.e. WOZ merged with HH dialogs and Media.
We trained all the SCLMs used in our experiments
with the SRILM toolkit (Stolcke, 2002) and we
used an interpolated model for probability esti-
mation with the Kneser-Ney discount (Chen and
Goodman, 1998). We then converted the model in
an FST as described in Section 2.1.
The model used to obtain the SVM baseline
for concept classification was trained using Yam-
CHA (Kudo and Matsumoto, 2001). For the re-
ranking models based on structure kernels, SVMs
or perceptron, we used the SVM-Light-TK toolkit
(available at dit.unitn.it/moschitti). For ? (see Sec-
tion 3.2), cost-factor and trade-off parameters, we
used, 0.4, 1 and 1, respectively.
4.3 Training approaches
The FST model generates the m-best annotations,
i.e. the data used to train the re-ranker based
on SVMs and perceptron. Different training ap-
proaches can be carried out based on the use of the
corpus and the method to generate the m-best. We
apply two different methods for training: Mono-
lithic Training and Split Training.
In the former, FSTs are learned with the whole
training set. The m-best hypotheses generated by
such models are then used to train the re-ranker
classifier. In Split Training, the training data are
divided in two parts to avoid bias in the FST gen-
eration step. More in detail, we train FSTs on part
1 and generate the m-best hypotheses using part 2.
Then, we re-apply these procedures inverting part
1 with part 2. Finally, we train the re-ranker on the
merged m-best data. At the classification time, we
generate the m-best of the test set using the FST
trained on all training data.
3The number of parameters, models and training ap-
proaches make the exhaustive experimentation expensive in
terms of processing time, which approximately requires 2 or
3 months.
Monolithic Training
WOZ SVM PCT
STK PTK SK STK PTK SK
RR-A 18.5 19.3 19.1 24.2 28.3 23.3
RR-B 18.5 19.3 19.0 29.4 23.7 20.3
RR-C 18.5 19.3 19.1 31.5 30.0 20.2
Table 4: Results of experiments, in terms of Con-
cept Error Rate (CER), on the LUNA WOZ corpus
using Monolithic Training approach. The baseline
with FST and SVMs used separately are 23.2%
and 26.7% respectively.
Split Training
WOZ SVM PCT
STK PTK SK STK PTK SK
RR-A 20.0 18.0 16.1 28.4 29.8 27.8
RR-B 19.0 19.0 19.0 26.3 30.0 25.6
RR-C 19.0 18.4 16.6 27.1 26.2 30.3
Table 5: Results of experiments, in terms of Con-
cept Error Rate (CER), on the LUNA WOZ cor-
pus using Split Training approach. The baseline
with FST and SVMs used separately are 23.2%
and 26.7% respectively.
Regarding the generation of the training in-
stances
?
si, sj
?
, we set m to 10 and we choose one
of the 10-best hypotheses as the second element of
the pair, sj , thus generating 10 different pairs.
The first element instead can be selected accord-
ing to three different approaches:
(A): si is the manual annotation taken from the
corpus;
(B) si is the most accurate annotation, in terms
of the edit distance from the manual annotation,
among the 10-best hypotheses of the FST model;
(C) as above but si is selected among the 100-
best hypotheses. The pairs are also inverted to
generate negative examples.
4.4 Re-ranking results
All the results of our experiments, expressed in
terms of concept error rate (CER), are reported in
Table 3, 4 and 5.
In Table 3, the corpora, i.e. LUNA (WOZ+HH)
and Media, and the training approaches, i.e.
Monolithic Training (MT) and Split Training (ST),
are reported in the first and second row. Column
1 shows the concept classification model used, i.e.
the baselines FST and SVMs, and the re-ranking
models (RR) applied to FST. A, B and C refer
to the three approaches for generating training in-
stances described above. As already mentioned
for these large datasets, SVMs only use STK.
208
We note that our re-rankers relevantly improve
our baselines, i.e. the FST and SVM concept clas-
sifiers on both corpora. For example, SVM re-
ranker using STK, MT and RR-A improves FST
concept classifier of 23.2-15.6 = 7.6 points.
Moreover, the monolithic training seems the
most appropriate to train the re-rankers whereas
approach A is the best in producing training in-
stances for the re-rankers. This is not surprising
since method A considers the manual annotation
as a referent gold standard and it always allows
comparing candidate annotations with the perfect
one.
Tables 4 and 5 have a similar structure of Ta-
ble 3 but they only show experiments on LUNA
WOZ corpus with respect to the monolithic and
split training approach, respectively. In these ta-
bles, we also report the result for SVMs and per-
ceptron (PCT) using STK, PTK and SK. We note
that:
First, the small size of WOZ training set (only
1,019 turns) impacts on the accuracy of the sys-
tems, e.g. FST and SVMs, which achieved a
CER of 18.2% and 23.4%, respectively, using also
HH dialogs, with only the WOZ data, they obtain
23.2% and 26.7%, respectively.
Second, the perceptron algorithm appears to be
ineffective for re-ranking. This is mainly due to
the reduced size of the WOZ data, which clearly
prevents an on line algorithm like PCT to ade-
quately refine its model by observing many exam-
ples4.
Third, the kernels which produce higher number
of substructures, i.e. PTK and SK, improves the
kernel less rich in terms of features, i.e. STK. For
example, using split training and approach A, STK
is improved by 20.0-16.1=3.9. This is an interest-
ing result since it shows that (a) richer structures
do produce better ranking models and (b) kernel
methods give a remarkable help in feature design.
Next, although the training data is small, the re-
rankers based on kernels appear to be very effec-
tive. This may also alleviate the burden of anno-
tating a lot of data.
Finally, the experiments of MEDIA show a not
so high improvement using re-rankers. This is due
to: (a) the baseline, i.e. the FST model is very
accurate since MEDIA is a large corpus thus the
re-ranker can only ?correct? small number of er-
rors; and (b) we could only experiment with the
4We use only one iteration of the algorithm.
less expensive but also less accurate models, i.e.
monolithic training and STK.
Media also offers the possibility to compare
with the state-of-the-art, which our re-rankers
seem to improve. However, we need to consider
that many Media corpus versions exist and this
makes such comparisons not completely reliable.
Future work on the paper research line appears
to be very interesting: the assessment of our best
models on Media and WOZ+HH as well as other
corpora is required. More importantly, the struc-
tures that we have proposed for re-ranking are
just two of the many possibilities to encode both
word/concept statistical distributions and linguis-
tic knowledge encoded in syntactic/semantic parse
trees.
5 Conclusions
In this paper, we propose discriminative re-
ranking of concept annotation to capitalize from
the benefits of generative and discriminative ap-
proaches. Our generative approach is the state-
of-the-art in concept classification since we used
the same FST model used in (Raymond and Ric-
cardi, 2007). We could improve it by 1% point
in MEDIA and 7.6 points (until 30% of relative
improvement) on LUNA, where the more limited
availability of annotated data leaves a larger room
for improvement.
It should be noted that to design the re-ranking
model, we only used two different structures,
i.e. one sequence and one tree. Kernel meth-
ods show that combinations of feature vectors, se-
quence kernels and other structural kernels, e.g.
on shallow or deep syntactic parse trees, appear
to be a promising research line (Moschitti, 2008).
Also, the approach used in (Zanzotto and Mos-
chitti, 2006) to define cross pair relations may be
exploited to carry out a more effective pair re-
ranking. Finally, the experimentation with auto-
matic speech transcriptions is interesting to test the
robustness of our models to transcription errors.
Acknowledgments
This work has been partially supported by the Eu-
ropean Commission - LUNA project, contract n.
33549.
209
References
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of In-
terspeech2005, Lisbon, Portugal.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techniques for language modeling. In
Technical Report of Computer Science Group, Har-
vard, USA.
M. Collins and N. Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over
Discrete structures, and the voted perceptron. In
ACL02, pages 263?270.
Y. He and S. Young. 2005. Semantic processing us-
ing the hidden vector state model. Computer Speech
and Language, 19:85?106.
T. Kudo and Y. Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
NAACL2001, Pittsburg, USA.
A. Moschitti and C. Bejan. 2004. A semantic ker-
nel for predicate argument classification. In CoNLL-
2004, Boston, MA, USA.
A. Moschitti, D. Pighin, and R. Basili. 2006. Seman-
tic role labeling via tree kernel joint inference. In
Proceedings of CoNLL-X, New York City.
A. Moschitti, G. Riccardi, and C. Raymond. 2007.
Spoken language understanding with kernels for
syntactic/semantic structures. In Proceedings of
ASRU2007, Kyoto, Japan.
A. Moschitti, D. Pighin, and R. Basili. 2008. Tree
kernels for semantic role labeling. Computational
Linguistics, 34(2):193?224.
A. Moschitti. 2006. Efficient Convolution Kernels
for Dependency and Constituent Syntactic Trees. In
Proceedings of ECML 2006, pages 318?329, Berlin,
Germany.
A. Moschitti. 2008. Kernel methods, syntax and se-
mantics for relational text categorization. In CIKM
?08: Proceeding of the 17th ACM conference on In-
formation and knowledge management, pages 253?
262, New York, NY, USA. ACM.
C. Raymond and G. Riccardi. 2007. Generative and
discriminative algorithms for spoken language un-
derstanding. In Proceedings of Interspeech2007,
Antwerp,Belgium.
C. Raymond, G. Riccardi, K. J. Rodrigez, and J. Wis-
niewska. 2007. The luna corpus: an annotation
scheme for a multi-domain multi-lingual dialogue
corpus. In Proceedings of Decalog2007, Trento,
Italy.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
A. Stolcke. 2002. Srilm: an extensible language mod-
eling toolkit. In Proceedings of SLP2002, Denver,
USA.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
F. M. Zanzotto and A. Moschitti. 2006. Automatic
learning of textual entailments with cross-pair simi-
larities. In Proceedings of the 21st Coling and 44th
ACL, pages 401?408, Sydney, Australia, July.
210
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 576?584,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Syntactic and Semantic Kernels for Short Text Pair Categorization
Alessandro Moschitti
Department of Computer Science and Engineering
University of Trento
Via Sommarive 14
38100 POVO (TN) - Italy
moschitti@disi.unitn.it
Abstract
Automatic detection of general relations
between short texts is a complex task that
cannot be carried out only relying on lan-
guage models and bag-of-words. There-
fore, learning methods to exploit syntax
and semantics are required. In this pa-
per, we present a new kernel for the repre-
sentation of shallow semantic information
along with a comprehensive study on ker-
nel methods for the exploitation of syntac-
tic/semantic structures for short text pair
categorization. Our experiments with Sup-
port Vector Machines on question/answer
classification show that our kernels can be
used to greatly improve system accuracy.
1 Introduction
Previous work on Text Categorization (TC) has
shown that advanced linguistic processing for doc-
ument representation is often ineffective for this
task, e.g. (Lewis, 1992; Furnkranz et al, 1998;
Allan, 2000; Moschitti and Basili, 2004). In con-
trast, work in question answering suggests that
syntactic and semantic structures help in solving
TC (Voorhees, 2004; Hickl et al, 2006). From
these studies, it emerges that when the categoriza-
tion task is linguistically complex, syntax and se-
mantics may play a relevant role. In this perspec-
tive, the study of the automatic detection of re-
lationships between short texts is particularly in-
teresting. Typical examples of such relations are
given in (Giampiccolo et al, 2007) or those hold-
ing between question and answer, e.g. (Hovy et
al., 2002; Punyakanok et al, 2004; Lin and Katz,
2003), i.e. if a text fragment correctly responds to
a question.
In Question Answering, the latter problem is
mostly tackled by using different heuristics and
classifiers, which aim at extracting the best an-
swers (Chen et al, 2006; Collins-Thompson et
al., 2004). However, for definitional questions, a
more effective approach would be to test if a cor-
rect relationship between the answer and the query
holds. This, in turns, depends on the structure of
the two text fragments. Designing language mod-
els to capture such relation is too complex since
probabilistic models suffer from (i) computational
complexity issues, e.g. for the processing of large
bayesian networks, (ii) problems in effectively es-
timating and smoothing probabilities and (iii) high
sensitiveness to irrelevant features and processing
errors. In contrast, discriminative models such as
Support Vector Machines (SVMs) have theoreti-
cally been shown to be robust to noise and irrele-
vant features (Vapnik, 1995). Thus, partially cor-
rect linguistic structures may still provide a rel-
evant contribution since only the relevant infor-
mation would be taken into account. Moreover,
such a learning approach supports the use of kernel
methods which allow for an efficient and effective
representation of structured data.
SVMs and Kernel Methods have recently been
applied to natural language tasks with promising
results, e.g. (Collins and Duffy, 2002; Kudo and
Matsumoto, 2003; Cumby and Roth, 2003; Shen
et al, 2003; Moschitti and Bejan, 2004; Culotta
and Sorensen, 2004; Kudo et al, 2005; Toutanova
et al, 2004; Kazama and Torisawa, 2005; Zhang
et al, 2006; Moschitti et al, 2006). In particular,
in question classification, tree kernels, e.g. (Zhang
and Lee, 2003), have shown accuracy comparable
to the best models, e.g. (Li and Roth, 2005).
Moreover, (Shen and Lapata, 2007; Moschitti
et al, 2007; Surdeanu et al, 2008; Chali and Joty,
576
2008) have shown that shallow semantic informa-
tion in the form of Predicate Argument Structures
(PASs) (Jackendoff, 1990; Johnson and Fillmore,
2000) improves the automatic detection of cor-
rect answers to a target question. In particular,
in (Moschitti et al, 2007) kernels for the process-
ing of PASs (in PropBank1 format (Kingsbury and
Palmer, 2002)) extracted from question/answer
pairs were proposed. However, the relatively high
kernel computational complexity and the limited
improvement on bag-of-words (BOW) produced
by this approach do not make the use of such tech-
nique practical for real world applications.
In this paper, we carry out a complete study on
the use of syntactic/semantic structures for rela-
tional learning from questions and answers. We
designed sequence kernels for words and Part of
Speech Tags which capture basic lexical seman-
tics and basic syntactic information. Then, we de-
sign a novel shallow semantic kernel which is far
more efficient and also more accurate than the one
proposed in (Moschitti et al, 2007).
The extensive experiments carried out on two
different corpora of questions and answers, de-
rived from Web documents and the TREC corpus,
show that:
? Kernels based on PAS, POS-tag sequences and
syntactic parse trees improve the BOW approach
on both datasets. On the TREC data the improve-
ment is interestingly high, e.g. about 61%, making
its application worthwhile.
? The new kernel for processing PASs is more ef-
ficient and effective than previous models so that
it can be practically used in systems for short text
pair categorization, e.g. question/answer classifi-
cation.
In the remainder of this paper, Section 2
presents well-known kernel functions for struc-
tural information whereas Section 3 describes our
new shallow semantic kernel. Section 4 reports
on our experiments with the above models and, fi-
nally, a conclusion is drawn in Section 5.
2 String and Tree Kernels
Feature design, especially for modeling syntactic
and semantic structures, is one of the most dif-
ficult aspects in defining a learning system as it
requires efficient feature extraction from learning
objects. Kernel methods are an interesting rep-
resentation approach as they allow for the use of
1www.cis.upenn.edu/?ace
all object substructures as features. In this per-
spective, String Kernel (SK) proposed in (Shawe-
Taylor and Cristianini, 2004) and the Syntactic
Tree Kernel (STK) (Collins and Duffy, 2002) al-
low for modeling structured data in high dimen-
sional spaces.
2.1 String Kernels
The String Kernels that we consider count the
number of substrings containing gaps shared by
two sequences, i.e. some of the symbols of the
original string are skipped. Gaps modify the
weight associated with the target substrings as
shown in the following.
Let? be a finite alphabet, ?? =
??
n=0 ?n is the
set of all strings. Given a string s ? ??, |s| denotes
the length of the strings and si its compounding
symbols, i.e s = s1..s|s|, whereas s[i : j] selects
the substring sisi+1..sj?1sj from the i-th to the
j-th character. u is a subsequence of s if there
is a sequence of indexes !I = (i1, ..., i|u|), with
1 ? i1 < ... < i|u| ? |s|, such that u = si1 ..si|u|
or u = s[!I] for short. d(!I) is the distance between
the first and last character of the subsequence u in
s, i.e. d(!I) = i|u| ? i1 + 1. Finally, given s1, s2
? ??, s1s2 indicates their concatenation.
The set of all substrings of a text corpus forms a
feature space denoted by F = {u1, u2, ..} ? ??.
To map a string s in R? space, we can use the
following functions: ?u(s) =
P
!I:u=s[!I] ?
d(!I) for
some ? ? 1. These functions count the num-
ber of occurrences of u in the string s and assign
them a weight ?d(!I) proportional to their lengths.
Hence, the inner product of the feature vectors for
two strings s1 and s2 returns the sum of all com-
mon subsequences weighted according to their
frequency of occurrences and lengths, i.e.
SK(s1, s2) =
X
u???
?u(s1) ??u(s2) =
X
u???
X
!I1:u=s1[!I1]
?d(
!I1)
X
!I2:u=t[!I2]
?d(
!I2) =
X
u???
X
!I1:u=s1[!I1]
X
!I2:u=t[!I2]
?d(
!I1)+d( !I2),
where d(.) counts the number of characters in the
substrings as well as the gaps that were skipped in
the original string.
2.2 Syntactic Tree Kernel (STK)
Tree kernels compute the number of common sub-
structures between two trees T1 and T2 without
explicitly considering the whole fragment space.
Let F = {f1, f2, . . . , f|F|} be the set of tree
577
SNP
NNP
Anxiety
VP
VBZ
is
NP
D
a
N
disease
?
VP
VBZ
is
NP
D
a
N
disease
VP
VBZ NP
D
a
N
disease
VP
VBZ
is
NP
D N
disease
VP
VBZ
is
NP
D N
VP
VBZ
is
NP
VP
VBZ NP
NP
D
a
N
disease
NP
NNP
Anxiety
NNP
Anxiety
VBZ
is
D
a
N
disease . . .
Figure 1: A tree for the sentence ?Anxiety is a disease? with some of its syntactic tree fragments.
fragments and ?i(n) be an indicator function,
equal to 1 if the target fi is rooted at node n
and equal to 0 otherwise. A tree kernel func-
tion over T1 and T2 is defined as TK(T1, T2) =?
n1?NT1
?
n2?NT2
?(n1, n2), where NT1 and
NT2 are the sets of nodes in T1 and T2, respec-
tively and ?(n1, n2) =
?|F|
i=1 ?i(n1)?i(n2).
? function counts the number of subtrees
rooted in n1 and n2 and can be evaluated as fol-
lows (Collins and Duffy, 2002):
1. if the productions at n1 and n2 are different then
?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminal symbols) then ?(n1, n2) = ?;
3. if the productions at n1 and n2 are the same, and
n1 and n2 are not pre-terminals then ?(n1, n2) =
?
Ql(n1)
j=1 (1 + ?(cn1(j), cn2(j))), where l(n1) is the
number of children of n1, cn(j) is the j-th child
of node n and ? is a decay factor penalizing larger
structures.
Figure 1 shows some fragments of the subtree
on the left part. These satisfy the constraint that
grammatical rules cannot be broken. For exam-
ple, [VP [VBZ NP]] is a valid fragment which has
two non-terminal symbols, VBZ and NP, as leaves
whereas [VP [VBZ]] is not a valid feature.
3 Shallow Semantic Kernels
The extraction of semantic representations from
text is a very complex task. For it, tradition-
ally used models are based on lexical similarity
and tends to neglect lexical dependencies. Re-
cently, work such as (Shen and Lapata, 2007; Sur-
deanu et al, 2008; Moschitti et al, 2007; Mos-
chitti and Quarteroni, 2008; Chali and Joty, 2008),
uses PAS to consider such dependencies but only
the latter three researches attempt to completely
exploit PAS with Shallow Semantic Tree Kernels
(SSTKs). Unfortunately, these kernels result com-
putational expensive for real world applications.
In the remainder of this section, we present our
new kernel for PASs and compare it with the pre-
vious SSTK.
PAS
A1
Disorder
rel
characterize
A0
fear
(a)
PAS
R-A0
that
rel
causes
A1
anxiety
(b)
Figure 2: Predicate Argument Structure trees associated
with the sentence: ?Panic disorder is characterized by unex-
pected and intense fear that causes anxiety.?.
PAS
rel
characterize
A0
fear
PAS
rel
characterize
PAS
A1 rel A0
PAS
A1 rel
characterize
PAS
rel
characterize
A0
Figure 3: Some of the tree substructures useful to capture
shallow semantic properties.
3.1 Shallow Semantic Structures
Shallow approaches to semantic processing are
making large strides in the direction of efficiently
and effectively deriving tacit semantic informa-
tion from text. Large data resources annotated
with levels of semantic information, such as in the
FrameNet (Johnson and Fillmore, 2000) and Prop-
Bank (PB) (Kingsbury and Palmer, 2002) projects,
make it possible to design systems for the auto-
matic extraction of predicate argument structures
(PASs) (Carreras and Ma`rquez, 2005). PB-based
systems produce sentence annotations like:
[A1 Panic disorder] is [rel characterized] [A0 by unexpected
and intense fear] [R?A0 that] [relcauses] [A1 anxiety].
A tree representation of the above semantic in-
formation is given by the two PAS trees in Fig-
ure 2, where the argument words are replaced by
the head word to reduce data sparseness. Hence,
the semantic similarity between sentences can be
measured in terms of the number of substructures
between the two trees. The required substructures
violate the STK constraint (about breaking pro-
duction rules), i.e. since we need any set of nodes
linked by edges of the initial tree. For example,
interesting semantic fragments of Figure 2.a are
shown in Figure 3.
Unfortunately, STK applied to PAS trees cannot
generate such fragments. To overcome this prob-
lem, a Shallow Semantic Tree Kernel (SSTK) was
designed in (Moschitti et al, 2007).
3.2 Shallow Semantic Tree Kernel (SSTK)
SSTK is obtained by applying two different steps:
first, the PAS tree is transformed by adding a layer
578
of SLOT nodes as many as the number of possi-
ble argument types, where each slot is assigned to
an argument following a fixed ordering (e.g. rel,
A0, A1, A2, . . . ). For example, if an A1 is found
in the sentence annotation it will be always posi-
tioned under the third slot. This is needed to ?arti-
ficially? allow SSTK to generate structures con-
taining subsets of arguments. For example, the
tree in Figure 2.a is transformed into the first tree
of Fig. 4, where ?null? just states that there is no
corresponding argument type.
Second, to discard fragments only containing
slot nodes, in the STK algorithm, a new step 0 is
added and the step 3 is modified (see Sec. 2.2):
0. if n1 (or n2) is a pre-terminal node and its child
label is null, ?(n1, n2) = 0;
3. ?(n1, n2) =
Ql(n1)
j=1 (1 +?(cn1(j), cn2(j))) ? 1.
For example, Fig. 4 shows the fragments gen-
erated by SSTK. The comparison with the ideal
fragments in Fig. 3 shows that SSTK well approx-
imates the semantic features needed for the PAS
representation. The computational complexity of
SSTK is O(n2), where n is the number of the PAS
nodes (leaves excluded). Considering that the tree
including all the PB arguments contains 52 slot
nodes, the computation becomes very expensive.
To overcome this drawback, in the next section,
we propose a new kernel to efficiently process PAS
trees with no addition of slot nodes.
3.3 Semantic Role Kernel (SRK)
The idea of SRK is to produce all child subse-
quences of a PAS tree, which correspond to se-
quences of predicate arguments. For this purpose,
we can use a string kernel (SK) (see Section 2.1)
for which efficient algorithms have been devel-
oped. Once a sequence of arguments is output by
SK, for each argument, we account for the poten-
tial matches of its children, i.e. the head of the
argument (or more in general the argument word
sequence).
More formally, given two sequences of argu-
ment nodes, s1 and s2, in two PAS trees and
considering the string kernel in Sec 2.1, the
SRK(s1, s2) is defined as:
X
!I1:u=s1[ !I1]
!I2:u=s2[!I2]
Y
l=1..|u|
(1 + ?(s1[$I1l], s2[$I2l]))?
d( !I1)+d(!I2), (1)
where u is any subsequence of argument nodes,
!Il is the index of the l-th argument node, s[!Il] is
the corresponding argument node in the sequence
s and ?(s1[!I1l], s2[!I2l]) is 1 if the heads of the ar-
guments are identical, otherwise is 0.
Proposition 1 SRK computes the number of all
possible tree substructures shared by the two eval-
uating PAS trees, where the considered substruc-
tures of a tree T are constituted by any set of nodes
(at least two) linked by edges of T .
Proof The PAS trees only contain three node lev-
els and, according to the proposition?s thesis, sub-
structures contain at least two nodes. The num-
ber of substructures shared by two trees, T1 and
T2, constituted by the root node (PAS) and the
subsequences of argument nodes is evaluated by
?
!I1:u=s1[!I1],!I2:u=s2[!I2]
?d(!I1)+d(!I2) (when ? = 1).
Given a node in a shared subsequence u, its child
(i.e. the head word) can be both in T1 and T2,
originating two different shared structures (with
or without such head node). The matches on the
heads (for each shared node of u) are combined
together generating different substructures. Thus
the number of substructures originating from u is
the product,
?
l=1..|u|(1+?(s1[!I1l], s2[!I2l])). This
number multiplied by all shared subsequences
leads to Eq. 1. !
We can efficiently compute SRK by following a
similar approach to the string kernel evaluation in
(Shawe-Taylor and Cristianini, 2004) by defining
the following dynamic matrix:
Dp(k, l) =
kX
i=1
lX
r=1
?k?i+l?r ? ?p?1(s1[1 : i], s2[1 : r]),
(2)
where ?p(s1, s2) counts the number of shared sub-
structures of exactly p argument nodes between s1
and s2 and again, s[1 : i] indicates the sequence
portion from argument 1 to i. The above matrix is
then used to evaluate ?p(s1a, s2b) =
(
?2(1 + ?(h(a), h(b)))Dp(|s1|, |s2|) if a = b;
0 otherwise.
(3)
where s1a and s2b indicate the concatenation of
the sequences s and t with the argument nodes, a
and b, respectively and ?(h(a), h(b)) is 1 if the
children of a and b are identical (e.g. same head).
The interesting property is that:
Dp(k, l) = ?p?1(s1[1 : k], s2[1 : l]) + ?Dp(k, l ? 1)
+ ?Dp(k ? 1, l)? ?2Dp(k ? 1, l ? 1).
(4)
To obtain the final kernel, we need to con-
sider all possible subsequence lengths. Let
m be the minimum between |s1| and |s2|,
SRK(s1, s2) =
mX
p=1
?p(s1, s2).
579
PAS
SLOT
rel
characterize
SLOT
A0
fear
*
SLOT
A1
disorder
*
SLOT
null
. . .
PAS
SLOT
rel
characterize
SLOT
A0
fear
*
SLOT
null
. . .
PAS
SLOT
rel
characterize
SLOT
null
SLOT
null
. . .
PAS
SLOT
rel
SLOT
A0
SLOT
A1
. . .
PAS
SLOT
rel
characterize
SLOT
A1
SLOT
null
. . .
Figure 4: Fragments of Fig. 2.a produced by the SSTK (similar to those of Fig. 3).
Regarding the processing time, if ? is the max-
imum number of arguments in a predicate struc-
ture, the worst case computational complexity of
SRK is O(?3).
3.4 SRK vs. SSTK
A comparison between SSTK and SRK suggests
the following points: first, although the computa-
tional complexity of SRK is larger than the one
of SSTK, we will show in the experiment section
that the running time (for both training and test-
ing) is much lower. The worse case is not really
informative since as shown in (Moschitti, 2006),
we can design fast algorithm with a linear average
running time (we use such algorithm for SSTK).
Second, although SRK uses trees with only
three levels, in Eq.1, the function ? (defined to
give 1 or 0 if the heads match or not) can be sub-
stituted by any kernel function. Thus, ? can re-
cursively be an SRK (and evaluate Nested PASs
(Moschitti et al, 2007)) or any other potential ker-
nel (over the arguments). The very interesting as-
pect is that the efficient algorithm that we provide
(Eqs 2, 3 and 4) can be accordingly modified to
efficiently evaluate new kernels obtained with the
? substitution2.
Third, the interesting difference between SRK
and SSTK (in addition to efficiency) is that SSTK
requires an ordered sequence of arguments to eval-
uate the number of argument subgroups (argu-
ments are sorted before running the kernel). This
means that the natural order is lost. SRK instead is
based on subsequence kernels so it naturally takes
into account the order which is very important:
without it, syntactic/semantic properties of pred-
icates cannot be captured, e.g. passive and active
forms have the same argument order for SSTK.
Finally, SRK gives a weight to the predicate
substructures by considering their length, which
also includes gaps, e.g. the sequence (A0, A1) is
more similar to (A0, A1) than (A0, A-LOC, A1),
in turn, the latter produces a heavier match than
(A0, A-LOC, A2, A1) (please see Section 2.1).
2For space reasons we cannot discuss it here.
This is another important property for modeling
shallow semantics similarity.
4 Experiments
Our experiments aim at studying the impact of our
kernels applied to syntactic/semantic structures for
the detection of relations between short texts. In
particular, we first show that our SRK is far more
efficient and effective than SSTK. Then, we study
the impact of the above kernels as well as se-
quence kernels based on words and Part of Speech
Tags and tree kernels for the classification of ques-
tion/answer text pairs.
4.1 Experimental Setup
The task used to test our kernels is the classifi-
cation of the correctness of ?q, a? pairs, where a
is an answer for the query q. The text pair ker-
nel operates by comparing the content of ques-
tions and the content of answers in a separate fash-
ion. Thus, given two pairs p1 = ?q1, a1? and
p2 = ?q2, a2?, a kernel function is defined as
K(p1, p2) =
?
? K? (q1, q2) +
?
? K? (a1, a2),
where ? varies across different kernel functions
described hereafter.
As a basic kernel machine, we used our
SVM-Light-TK toolkit, available at disi.unitn.
it/moschitti (which is based on SVM-Light
(Joachims, 1999) software). In it, we imple-
mented: the String Kernel (SK), the Syntactic Tree
Kernel (STK), the Shallow Semantic Tree Kernel
(SSTK) and the Semantic Role Kernel (SRK) de-
scribed in sections 2 and 3. Each kernel is associ-
ated with the above linguistic objects: (i) the linear
kernel is used with the bag-of-words (BOW) or the
bag-of-POS-tags (POS) features. (ii) SK is used
with word sequences (i.e. the Word Sequence Ker-
nel, WSK) and POS sequences (i.e. the POS Se-
quence Kernel, PSK). (iii) STK is used with syn-
tactic parse trees automatically derived with Char-
niak?s parser; (iv) SSTK and SRK are applied to
two different PAS trees (see Section 3.1), automat-
ically derived with our SRL system.
It is worth noting that, since answers often con-
580
0 
20 
40 
60 
80 
100 
120 
140 
160 
180 
200 
220 
240 
200 400 600 800 1000 1200 1400 1600 1800 
Ti
m
e 
in
 S
ec
on
ds
 
Training Set Size 
SRK (training) SRK (test) 
SSTK (test) SSTK (training) 
Figure 5: Efficiency of SRK and SSTK
tain more than one PAS, we applied SRK or SSTK
to all pairs P1 ? P2 and sum the obtained contri-
bution, where P1 and P2 are the set of PASs of the
first and second answer3. Although different ker-
nels can be used for questions and for answers, we
used (and summed together) the same kernels ex-
cept for those based on PASs, which are only used
on answers.
4.1.1 Datasets
To train and test our text QA classifiers, we
adopted the two datasets of question/answer pairs
available at disi.unitn.it/?silviaq, contain-
ing answers to only definitional questions. The
datasets are based on the 138 TREC 2001 test
questions labeled as ?description? in (Li and Roth,
2005). Each question is paired with all the top
20 answer paragraphs extracted by two basic QA
systems: one trained with the web documents and
the other trained with the AQUAINT data used in
TREC?07.
The WEB corpus (Moschitti et al, 2007) of QA
pairs contains 1,309 sentences, 416 of which are
positive4 answers whereas the TREC corpus con-
tains 2,256 sentences, 261 of which are positive.
4.1.2 Measures and Parameterization
The accuracy of the classifiers is provided by the
average F1 over 5 different samples using 5-fold
cross-validation whereas each plot refers to a sin-
gle fold. We carried out some preliminary experi-
ments of the basic kernels on a validation set and
3More formally, let Pt and Pt? be the sets of PASs ex-
tracted from text fragments t and t?; the resulting kernel will
be Kall(Pt, Pt?) =
P
p?Pt
P
p??Pt?
SRK(p, p?).
4For instance, given the question ?What are inverte-
brates??, the sentence ?At least 99% of all animal species
are invertebrates, comprising . . . ? was labeled ?-1? , while
?Invertebrates are animals without backbones.? was labeled
?+1?.
we noted that the F1 was maximized by using the
default cost parameters (option -c of SVM-Light),
? = 0.04 (see Section 2). The trade-off parame-
ter varied according to different kernels on WEB
data (so it needed an ad-hoc estimation) whereas
a value of 10 was optimal for any kernel on the
TREC corpus.
4.2 Shallow Semantic Kernel Efficiency
Section 2 has illustrated that SRK is applied to
more compact PAS trees than SSTK, which runs
on large structures containing as many slots as
the number of possible predicate argument types.
This impacts on the memory occupancy as well
as on the kernel computation speed. To empiri-
cally verify our analytical findings (Section 3.3),
we divided the training (TREC) data in 9 bins of
increasing size (200 instances between two con-
tiguous bins) and we measured the learning and
test time5 for each bin. Figure 5 shows that in
both the classification and learning phases, SRK
is much faster than SSTK. With all training data,
SSTK employs 487.15 seconds whereas SRK only
uses 12.46 seconds, i.e. it is about 40 times faster,
making the experimentation of SVMs with large
datasets feasible. It is worth noting that to imple-
ment SSTK we used the fast version of STK and
that, although the PAS trees are smaller than syn-
tactic trees, they may still contain more than half
million of substructures (when they are formed by
seven arguments).
4.3 Results for Question/Answer
Classification
In these experiments, we tested different kernels
and some of their most promising combinations,
which are simply obtained by adding the different
kernel contributions6 (this yields the joint feature
space of the individual kernels).
Table 1 shows the average F1? the standard de-
viation7 over 5-folds on Web (and TREC) data of
SVMs using different kernels. We note that: (a)
BOW achieves very high accuracy, comparable to
the one produced by STK, i.e. 65.3 vs 65.1; (b)
the BOW+STK combination achieves 66.0, im-
5Processing time in seconds of a Mac-Book Pro 2.4 Ghz.
6All adding kernels are normalized to have a sim-
ilarity score between 0 and 1, i.e. K?(X1,X2) =
K(X1,X2)?
K(X1,X1)?K(X2,X2)
.
7The Std. Dev. of the difference between two classifier
F1s is much lower making statistically significant almost all
our system ranking in terms of performance.
581
WEB Corpus
BOW POS PSK WSK STK SSTK SRK BOW+POS BOW+STK PSK+STK WSK+STK STK+SSTK STK+SRK
65.3?2.9 56.8?0.8 62.5?2.3 65.7?6.0 65.1?3.9 52.9?1.7 50.8?1.2 63.7?1.6 66.0?2.7 65.3?2.4 66.6?3.0 (+WSK) 68.0?2.7 (+WSK) 68.2?4.3
TREC Corpus
24.2?5.0 26.5?7.9 31.6?6.8 14.0?4.2 33.1?3.8 21.8?3.7 23.6?4.7 31.9?7.1 30.3?4.1 36.4?7.0 23.7?3.9 (+PSK) 37.2?6.9 (+PSK) 39.1?7.3
Table 1: F1 ? Std. Dev. of the question/answer classifier according to several kernels on the WEB and
TREC corpora.
proving both BOW and STK; (c) WSK (65.7) im-
proves BOW and it is enhanced by WSK+STK
(66.6), demonstrating that word sequences and
STKs are very relevant for this task; and fi-
nally, WSK+STK+SSTK is slightly improved by
WSK+STK+SRK, 68.0% vs 68.2% (not signifi-
cantly) and both improve on WSK+STK.
The above findings are interesting as the syntac-
tic information provided by STK and the semantic
information brought by WSK and SRK improve
on BOW. The high accuracy of BOW is surprising
if we consider that at classification time, instances
of the training models (e.g. support vectors) are
compared with different test examples since ques-
tions cannot be shared between training and test
set8. Therefore the answer words should be dif-
ferent and useless to generalize rules for answer
classification. However, error analysis reveals that
although questions are not shared between train-
ing and test set, there are common words in the
answers due to typical Web page patterns which
indicate if a retrieved passage is an incorrect an-
swer, e.g. Learn more about X.
Although the ability to detect these patterns is
beneficial for a QA system as it improves its over-
all accuracy, it is slightly misleading for the study
that we are carrying out. Thus, we experimented
with the TREC corpus which does not contain
Web extra-linguistic texts and it is more complex
from a QA task viewpoint (it is more difficult to
find a correct answer).
Table 1 also shows the classification results on
the TREC dataset. A comparative analysis sug-
gests that: (a) the F1 of all models is much lower
than for the Web dataset; (b) BOW shows the low-
est accuracy (24.2) and also the accuracy of its
combination with STK (30.3) is lower than the
one of STK alone (33.1); (c) PSK (31.6) improves
POS (26.5) information and PSK+STK (36.4) im-
proves on PSK and STK; and (d) PAS adds further
8Sharing questions between test and training sets would
be an error from a machine learning viewpoint as we cannot
expect new questions to be identical to those in the training
set.
information as the best model is PSK+STK+SRK,
which improves BOW from 24.2 to 39.1, i.e. 61%.
Finally, it is worth noting that SRK provides a
higher improvement (39.1-36.4) than SSTK (37.2-
36.4).
4.4 Precision/Recall Curves
To better study the benefit of the proposed linguis-
tic structures, we also plotted the Precision/Recall
curves (one fold for each corpus). Figure 6 shows
the curve of some interesting kernels applied to
the Web dataset. As expected, BOW shows the
lowest curves, although, its relevant contribution
is evident. STK improves BOW since it pro-
vides a better model generalization by exploit-
ing syntactic structures. Also, WSK can gener-
ate a more accurate model than BOW since it uses
n-grams (with gaps) and when it is summed to
STK, a very accurate model is obtained9. Finally,
WSK+STK+SRK improves all the models show-
ing the potentiality of PASs.
Such curves show that there is no superior
model. This is caused by the high contribution
of BOW, which de-emphasize all the other mod-
els?s result. In this perspective, the results on
TREC are more interesting as shown by Figure 7
since the contribution of BOW is very low making
the difference in accuracy with the other linguis-
tic models more evident. PSK+STK+SRK, which
encodes the most advanced syntactic and semantic
information, shows a very high curve which out-
performs all the others.
The analysis of the above results suggests that:
first as expected, BOW does not prove very rel-
evant to capture the relations between short texts
from examples. In the QA classification, while
BOW is useful to establish the initial ranking by
measuring the similarity between question and an-
swer, it is almost irrelevant to capture typical rules
suggesting if a description is valid or not. Indeed,
since test questions are not in the training set, their
words as well as those of candidate answers will
be different, penalizing BOW models. In these
9Some of the kernels have been removed from the figures
so that the plots result more visible.
582
0 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
30 40 50 60 70 80 90 100 
Pr
ec
is
io
n 
Recall 
STK 
WSK+STK 
WSK+STK+SRK 
BOW 
WSK 
Figure 6: Precision/Recall curves of some kernel
combinations on the WEB dataset.
0 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
10 15 20 25 30 35 40 45 50 55 60 
Pr
ec
is
io
n 
Recall 
STK 
PSK+STK 
"PSK+STK+SRK" 
BOW 
PSK 
Figure 7: Precision/Recall curves of some kernel
combinations on the TREC dataset.
conditions, we need to rely on syntactic structures
which at least allow for detecting well formed de-
scriptions.
Second, the results show that STK is important
to detect typical description patterns but also POS
sequences provide additional information since
they are less sparse than tree fragments. Such pat-
terns improve on the bag of POS-tags by about 6%
(see POS vs PSK). This is a relevant result consid-
ering that in standard text classification bigrams or
trigrams are usually ineffective.
Third, although PSK+STK generates a very rich
feature set, SRK significantly improves the classi-
fication F1 by about 3%, suggesting that shallow
semantics can be very useful to detect if an an-
swer is well formed and is related to a question.
Error analysis revealed that PAS can provide pat-
terns like:
- A0(X) R-A0(that) rel(result) A1(Y)
- A1(X) rel(characterize) A0(Y),
where X and Y need not necessarily be matched.
Finally, the best model, PSK+STK+SRK, im-
proves on BOW by 61%. This is strong evidence
that complex natural language tasks require ad-
vanced linguistic information that should be ex-
ploited by powerful algorithms such as SVMs
and using effective feature engineering techniques
such as kernel methods.
5 Conclusion
In this paper, we have studied several types
of syntactic/semantic information: bag-of-words
(BOW), bag-of-POS tags, syntactic parse trees
and predicate argument structures (PASs), for the
design of short text pair classifiers. Our learn-
ing framework is constituted by Support Vector
Machines (SVMs) and kernel methods applied
to automatically generated syntactic and semantic
structures.
In particular, we designed (i) a new Semantic
Role Kernel (SRK) based on a very fast algorithm;
(ii) a new sequence kernel over POS tags to en-
code shallow syntactic information; (iii) many ker-
nel combinations (to our knowledge no previous
work uses so many different kernels) which allow
for the study of the role of several linguistic levels
in a well defined statistical framework.
The results on two different question/answer
classification corpora suggest that (a) SRK for pro-
cessing PASs is more efficient and effective than
previous models, (b) kernels based on PAS, POS-
tag sequences and syntactic parse trees improve on
BOW on both datasets and (c) on the TREC data
the improvement is remarkably high, e.g. about
61%.
Promising future work concerns the definition
of a kernel on the entire argument information
(e.g. by means of lexical similarity between all the
words of two arguments) and the design of a dis-
course kernel to exploit the relational information
gathered from different sentence pairs. A closer
relationship between questions and answers can be
exploited with models presented in (Moschitti and
Zanzotto, 2007; Zanzotto and Moschitti, 2006).
Also the use of PAS derived from FrameNet and
PropBank (Giuglea and Moschitti, 2006) appears
to be an interesting research line.
Acknowledgments
I would like to thank Silvia Quarteroni for her
work on extracting linguistic structures. This
work has been partially supported by the European
Commission - LUNA project, contract n. 33549.
583
References
J. Allan. 2000. Natural Language Processing for Information
Retrieval. In NAACL/ANLP (tutorial notes).
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: SRL. In CoNLL-2005.
Y. Chali and S. Joty. 2008. Improving the performance of
the random walk model for answering complex questions.
In Proceedings of ACL-08: HLT, Short Papers, Columbus,
Ohio.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking answers
from definitional QA using language models. In ACL?06.
M. Collins and N. Duffy. 2002. New ranking algorithms for
parsing and tagging: Kernels over discrete structures, and
the voted perceptron. In ACL?02.
K. Collins-Thompson, J. Callan, E. Terra, and C. L.A. Clarke.
2004. The effect of document retrieval quality on factoid
QA performance. In SIGIR?04.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree
Kernels for Relation Extraction. In ACL04, Barcelona,
Spain.
C. Cumby and D. Roth. 2003. Kernel Methods for Rela-
tional Learning. In Proceedings of ICML 2003, Washing-
ton, DC, USA.
J. Furnkranz, T. Mitchell, and E. Rilof. 1998. A case study
in using linguistic phrases for text categorization on the
www. In Working Notes of the AAAI/ICML, Workshop on
Learning for Text Categorization.
D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan. 2007.
The third pascal recognizing textual entailment challenge.
In Proceedings of the ACL-PASCAL Workshop, Prague.
A.-M. Giuglea and A. Moschitti. 2006. Semantic role label-
ing via framenet, verbnet and propbank. In Proceedings
of ACL 2006, Sydney, Australia.
A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and
B. Rink. 2006. Question answering with lccs chaucer at
trec 2006. In Proceedings of TREC?06.
E. Hovy, U. Hermjakob, C.-Y. Lin, and D. Ravichandran.
2002. Using knowledge to facilitate factoid answer pin-
pointing. In Proceedings of Coling, Morristown, NJ,
USA.
R. Jackendoff. 1990. Semantic Structures. MIT Press.
T. Joachims. 1999. Making large-scale SVM learning prac-
tical. In B. Scho?lkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods.
C. R. Johnson and C. J. Fillmore. 2000. The framenet tagset
for frame-semantic and syntactic coding of predicate-
argument structures. In ANLP-NAACL?00, pages 56?62.
J. Kazama and K. Torisawa. 2005. Speeding up Train-
ing with Tree Kernels for Node Relation Labeling. In
Proceedings of EMNLP 2005, pages 137?144, Toronto,
Canada.
P. Kingsbury and M. Palmer. 2002. From Treebank to Prop-
Bank. In LREC?02.
T. Kudo and Y. Matsumoto. 2003. Fast Methods for Kernel-
Based Text Analysis. In Erhard Hinrichs and Dan Roth,
editors, Proceedings of ACL.
T. Kudo, J. Suzuki, and H .Isozaki. 2005. Boosting-based
parse reranking with subtree features. In Proceedings of
ACL?05, US.
D. D. Lewis. 1992. An evaluation of phrasal and clustered
representations on a text categorization task. In Proceed-
ings of SIGIR-92.
X. Li and D. Roth. 2005. Learning question classifiers: the
role of semantic information. JNLE.
J. Lin and B. Katz. 2003. Question answering from the web
using knowledge annotation and knowledge mining tech-
niques. In CIKM ?03.
A. Moschitti and R. Basili. 2004. Complex linguistic fea-
tures for text classification: A comprehensive study. In
ECIR, Sunderland, UK.
A.Moschitti and C. Bejan. 2004. A semantic kernel for pred-
icate argument classification. In CoNLL-2004, Boston,
MA, USA.
A. Moschitti and S. Quarteroni. 2008. Kernels on linguistic
structures for answer extraction. In Proceedings of ACL-
08: HLT, Short Papers, Columbus, Ohio.
A. Moschitti and F. Zanzotto. 2007. Fast and effective ker-
nels for relational learning from texts. In Zoubin Ghahra-
mani, editor, Proceedings of ICML 2007.
A. Moschitti, D. Pighin, and R. Basili. 2006. Semantic role
labeling via tree kernel joint inference. In Proceedings of
CoNLL-X, New York City.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar.
2007. Exploiting syntactic and shallow semantic kernels
for question/answer classification. In ACL?07, Prague,
Czech Republic.
A. Moschitti. 2006. Making Tree Kernels Practical for Nat-
ural Language Learning. In Proceedings of EACL2006.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping depen-
dencies trees: An application to question answering. In
Proceedings of AI&Math 2004.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods
for Pattern Analysis. Cambridge University Press.
D. Shen and M. Lapata. 2007. Using semantic roles to im-
prove question answering. In Proceedings of EMNLP-
CoNLL.
L. Shen, A. Sarkar, and A. k. Joshi. 2003. Using LTAG
Based Features in Parse Reranking. In EMNLP, Sapporo,
Japan.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008. Learn-
ing to rank answers on large online QA collections. In
Proceedings of ACL-08: HLT, Columbus, Ohio.
K. Toutanova, P. Markova, and C. Manning. 2004. The Leaf
Path Projection View of Parse Trees: Exploring String
Kernels for HPSG Parse Selection. In Proceedings of
EMNLP 2004, Barcelona, Spain.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer.
E. M. Voorhees. 2004. Overview of the trec 2001 question
answering track. In Proceedings of the Thirteenth Text
REtreival Conference (TREC 2004).
F. M. Zanzotto and A. Moschitti. 2006. Automatic learning
of textual entailments with cross-pair similarities. In Pro-
ceedings of the 21st Coling and 44th ACL, Sydney, Aus-
tralia.
D. Zhang and W. Lee. 2003. Question classification using
support vector machines. In SIGIR?03, Toronto, Canada.
ACM.
M. Zhang, J. Zhang, and J. Su. 2006. Exploring Syntactic
Features for Relation Extraction using a Convolution tree
kernel. In Proceedings of NAACL, New York City, USA.
584
Tree Kernels for Semantic Role Labeling
Alessandro Moschitti?
University of Trento
Daniele Pighin??
University of Trento
Roberto Basili?
University of Rome ?Tor Vergata?
The availability of large scale data sets of manually annotated predicate?argument struc-
tures has recently favored the use of machine learning approaches to the design of automated
semantic role labeling (SRL) systems. The main research in this area relates to the design choices
for feature representation and for effective decompositions of the task in different learning models.
Regarding the former choice, structural properties of full syntactic parses are largely employed as
they represent ways to encode different principles suggested by the linking theory between syntax
and semantics. The latter choice relates to several learning schemes over global views of the
parses. For example, re-ranking stages operating over alternative predicate?argument sequences
of the same sentence have shown to be very effective.
In this article, we propose several kernel functions to model parse tree properties in kernel-
based machines, for example, perceptrons or support vector machines. In particular, we define
different kinds of tree kernels as general approaches to feature engineering in SRL. Moreover, we
extensively experiment with such kernels to investigate their contribution to individual stages
of an SRL architecture both in isolation and in combination with other traditional manually
coded features. The results for boundary recognition, classification, and re-ranking stages provide
systematic evidence about the significant impact of tree kernels on the overall accuracy, especially
when the amount of training data is small. As a conclusive result, tree kernels allow for a general
and easily portable feature engineering method which is applicable to a large family of natural
language processing tasks.
1. Introduction
Much attention has recently been devoted to the design of systems for the automatic
labeling of semantic roles (SRL) as defined in two important projects: FrameNet (Baker,
Fillmore, and Lowe 1998), based on frame semantics, and PropBank (Palmer, Gildea,
? Department of Information Engineering and Computer Science, Via Sommarive, 14 I-38050 Povo (TN).
E-mail: moschitti@dit.unitn.it.
?? Fondazione Bruno Kessler, Center for Scientific and Technological Research, Department of Information
Engineering and Computer Science, Via Sommarive, 18 I-38050 Povo (TN). E-mail: pighin@itc.it.
? Department of Computer Science, Systems and Production, Via del Politecnico, 1 I-00133 RM.
E-mail: basili@info.uniroma2.it.
Submission received: 15 July 2006; revised submission received: 1 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
and Kingsbury 2005), inspired by Levin?s verb classes. To annotate natural language
sentences, such systems generally require (1) the detection of the target word em-
bodying the predicate and (2) the detection and classification of the word sequences
constituting the predicate?s arguments.
Previous work has shown that these steps can be carried out by applying machine
learning techniques (Carreras and Ma`rquez 2004, 2005; Litkowski 2004), for which the
most important features encoding predicate?argument relations are derived from (shal-
low or deep) syntactic information. The outcome of this research brings wide empirical
evidence in favor of the linking theories between semantics and syntax, for example,
Jackendoff (1990). Nevertheless, as no such theory provides a sound and complete
treatment, the choice and design of syntactic features to represent semantic structures
requires remarkable research effort and intuition.
For example, earlier studies on feature design for semantic role labeling were car-
ried out by Gildea and Jurafsky (2002) and Thompson, Levy, and Manning (2003). Since
then, researchers have proposed several syntactic feature sets, where the more recent
sets slightly enhanced the older ones.
A careful analysis of such features reveals that most of them are syntactic tree
fragments of training sentences, thus a viable way to alleviate the feature design com-
plexity is the adoption of syntactic tree kernels (Collins and Duffy 2002). For example, in
Moschitti (2004), the predicate?argument relation is represented by means of the min-
imal subtree that includes both of them. The similarity between two instances is eval-
uated by a tree kernel function in terms of common substructures. Such an approach
is in line with current research on kernels for natural language learning, for example,
syntactic parsing re-ranking (Collins and Duffy 2002), relation extraction (Zelenko,
Aone, and Richardella 2003), and named entity recognition (Cumby and Roth 2003;
Culotta and Sorensen 2004).
Furthermore, recent work (Haghighi, Toutanova, and Manning 2005; Punyakanok
et al 2005) has shown that, to achieve high labeling accuracy, joint inference should
be applied on the whole predicate?argument structure. For this purpose, we need to
extract features from the sentence syntactic parse tree that encodes the relationships
governing complex semantic structures. This task is rather difficult because we do
not exactly know which syntactic clues effectively capture the relation between the
predicate and its arguments. For example, to detect the interesting context, themodeling
of syntax-/semantics-based features should take into account linguistic aspects like
ancestor nodes or semantic dependencies (Toutanova, Markova, and Manning 2004).
In this scenario, the automatic feature generation/selection carried out by tree kernels
can provide useful insights into the underlying linguistic phenomena. Other advantages
coming from the use of tree kernels are the following.
First, we can implement them very quickly as the feature extractor module only
requires the writing of a general procedure for subtree extraction. In contrast, traditional
SRL systems use more than thirty features (e. g., Pradhan, Hacioglu, Krugler et al 2005),
each of which requires the writing of a dedicated procedure.
Second, their combination with traditional attribute?value models produces more
accurate systems, also when using the same machine learning algorithm in the combi-
nation, because the feature spaces are very different.
Third, we can carry out feature engineering using kernel combinations andmarking
strategies (Moschitti et al 2005a; Moschitti, Pighin, and Basili 2006). This allows us to
boost the SRL accuracy in a relatively simple way.
Next, tree kernels generate large tree fragment sets which constitute back-off
models for important syntactic features. Using them, the learning algorithm generalizes
194
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
better and produces a more accurate classifier, especially when the amount of training
data is scarce.
Finally, once the learning algorithm using tree kernels has converged, we can iden-
tify the most important structured features of the generated model. One approach for
such a reverse engineering process relies on the computation of the explicit feature
space, at least for the highest-weighted features (Kudo and Matsumoto 2003). Once
the most relevant fragments are available, they can be used to design novel effective
attribute?value features (which in turn can be used to design more efficient classifiers,
e. g., with linear kernels) and inspire new linguistic theories.
These points suggest that tree kernels should always be applied, at least for an initial
study of the problem. Unfortunately, they suffer from two main limitations: (a) poor
impact on boundary detection as, in this task, correct and incorrect arguments may
share a large portion of the encoding trees (Moschitti 2004); and (b) more expensive
running time and limited contribution to the overall accuracy if compared with manu-
ally derived features (Cumby and Roth 2003). Point (a) has been addressed byMoschitti,
Pighin, and Basili (2006) by showing that a strategy ofmarking relevant parse-tree nodes
makes correct and incorrect subtrees for boundary detection quite different. Point (b)
can be tackled by studying approaches to kernel engineering that allow for the design
of efficient and effective kernels.
In this article, we provide a comprehensive study of the use of tree kernels for se-
mantic role labeling. For this purpose, we define tree kernels based on the composition
of two different feature functions: canonical mappings, which map sentence-parse trees
in tree structures encoding semantic information, and feature extraction functions,
which encode these trees in the actual feature space. The latter functions explode the
canonical trees into all their substructures and, in the literature, are usually referred to as
tree kernels. For instance, in Collins and Duffy (2002), Vishwanathan and Smola (2002),
and Moschitti (2006a) different tree kernels extract different types of tree fragments.
Given the heuristic nature of canonical mappings, we studied their properties
by experimenting with them within support vector machines and with the data set
provided by CoNLL shared tasks (Carreras and Ma`rquez 2005). The results show that
carefully engineered tree kernels always boost the accuracy of the basic systems. Most
importantly, in complex tasks such as the re-ranking of semantic role annotations, they
provide an easy way to engineer new features which enhance the state-of-the-art in SRL.
In the remainder of this article, Section 2 presents traditional architectures for SRL
and the typical features proposed in literature. Tree kernels are formally introduced
in Section 3, and Section 4 describes our modular architecture employing support
vector machines along with manually designed features, tree kernels (feature extraction
functions), and their combinations. Section 5 presents our structured features (canonical
mappings) inducing different kernels that we used for different SRL subtasks. The
extensive experimental results obtained on the boundary recognition, role classification,
and re-ranking stages are presented in Section 6. Finally, Section 7 summarizes the
conclusions.
2. Automatic Shallow Semantic Parsing
The recognition of semantic structures within a sentence relies on lexical and syntactic
information provided by early stages of anNLP process, such as lexical analysis, part-of-
speech tagging, and syntactic parsing. The complexity of the SRL task mostly depends
on two aspects: (a) the information is generally noisy, that is, in a real-world scenario
the accuracy and reliability of NLP subsystems are generally not very high; and (b) the
195
Computational Linguistics Volume 34, Number 2
lack of a sound and complete linguistic or cognitive theory about the links between
syntax and semantics does not allow an informed, deductive approach to the problem.
Nevertheless, the large amount of available lexical and syntactic information favors the
application of inductive approaches to the SRL task, which indeed is generally treated
as a combination of statistical classification problems.
The next sections define the SRL task more precisely and summarize the most
relevant work carried out to address these two problems.
2.1 Problem Definition
The most well-known shallow semantic theories are studied in two different projects:
PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and
Lowe 1998). The former is based on a linguistic model inspired by Levin?s verb classes
(Levin 1993), focusing on the argument structure of verbs and on the alternation pat-
terns that describe movements of verbal arguments within a predicate structure. The
latter refers to the application of frame semantics (Fillmore 1968) in the annotation of
predicate?argument structures based on frame elements (semantic roles). These theories
have been investigated in two CoNLL shared tasks (Carreras and Ma`rquez 2004, 2005)
and a Senseval-3 evaluation (Litkowski 2004), respectively.
Given a sentence and a predicate word, an SRL system outputs an annotation of the
sentence in which the sequences of words that make up the arguments of the predicate
are properly labeled, for example:
[Arg0 He] got [Arg1 his money] [C-V back]
1
in response to the input He got his money back. This processing requires that: (1) the
predicates within the sentence are identified and (2) the word sequences that span the
boundaries of each predicate argument are delimited and assigned the proper role label.
The first sub-task can be performed either using statistical methods or hand-crafted
lexical and syntactic rules. In the case of verbal predicates, it is quite easy to write
simple rules matching regular expressions built on POS tags. The second task is more
complex and is generally viewed as a combination of statistical classification problems:
The learning algorithms are trained to recognize the extension of predicate arguments
and the semantic role they play.
2.2 Models for Semantic Role Labeling
An SRL model and the resulting architecture are largely influenced by the kind of data
available for the task. As an example, a model relying on a shallow syntactic parser
would assign roles to chunks, whereas with a full syntactic parse of the sentence it
would be straightforward to establish a correspondence between nodes of the parse tree
and semantic roles. We focused on the latter as it has been shown to be more accurate
by the CoNLL 2005 shared task results.
According to the deep syntactic formulation, the classifying instances are pairs
of parse-tree nodes which dominate the exact span of the predicate and the target
argument. Such pairs are usually represented in terms of attribute?value vectors, where
1 In PropBank notation, Arg0 and Arg1 represent the logical subject and the logical object of the target
verbal predicate, respectively. C-V represents the particle of a phrasal-verb predicate.
196
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
the attributes describe properties of predicates, arguments, and theway they are related.
There is large agreement on an effective set of linguistic features (Gildea and Jurafsky
2002; Pradhan, Hacioglu, Krugler, et al 2005) that have been employed in the vast
majority of SRL systems. The most relevant features are summarized in Table 1.
Once the representation for the predicate?argument pairs is available, a multi-
classifier is used to recognize the correct node pairs, namely, nodes associated with
correct arguments (given a predicate), and assign them a label (which is the label of the
argument). This can be achieved by training a multi-classifier on n+ 1 classes, where
the first n classes correspond to the different roles and the (n+ 1)th is a NARG (non-
argument) class to which non-argument nodes are assigned.
A more efficient solution consists in dividing the labeling process into two steps:
boundary detection and argument classification. A Boundary Classifier (BC) is a binary
classifier that recognizes the tree nodes that exactly cover a predicate argument, that
is, that dominate all and only the words that belong to target arguments. Then, such
nodes are classified by a Role Multi-classifier (RM) that assigns to each example the
most appropriate label. This two-step approach (Gildea and Jurafsky 2002) has the
advantage of only applying BC on all parse-tree nodes. RM can ignore non-boundary
nodes, resulting in a much faster classification. Other approaches have extended this
solution and suggested other multi-stage classification models (e. g., Moschitti et al
2005b in which a four-step hierarchical SRL architecture is described).
After node labeling has been carried out, it is possible that the output of the argu-
ment classifier does not result in a consistent annotation, as the labeling scheme may
not be compatible with the underlying linguistic model. As an example, PropBank-style
annotations do not allow arguments to be nested. This happens when two or more
Table 1
Standard linguistic features employed by most SRL systems.
Feature Name Description
Predicate Lemmatization of the predicate word
Path Syntactic path linking the predicate and an argument,
e. g., NN?NP?VP?VBX
Partial path Path feature limited to the branching of the argument
No-direction path Like Path, but without traversal directions
Phrase type Syntactic type of the argument node
Position Relative position of the argument with respect to the predicate
Voice Voice of the predicate, i. e., active or passive
Head word Syntactic head of the argument phrase
Verb subcategorization Production rule expanding the predicate parent node
Named entities Classes of named entities that appear in the argument node
Head word POS POS tag of the argument node head word (less sparse than
Head word)
Verb clustering Type of verb? direct object relation
Governing Category Whether the candidate argument is the verb subject or object
Syntactic Frame Position of the NPs surrounding the predicate
Verb sense Sense information for polysemous verbs
Head word of PP Enriched POS of prepositional argument nodes (e. g., PP-for, PP-in)
First and last word/POS First and last words and POS tags of candidate argument phrases
Ordinal position Absolute offset of a candidate argument within a proposition
Constituent tree distance Distance from the predicate with respect to the parse tree
Constituent features Description of the constituents surrounding the argument node
Temporal Cue Words Temporal markers which are very distinctive of some roles
197
Computational Linguistics Volume 34, Number 2
overlapping tree nodes, namely, one dominating the other, are classified as positive
boundaries.
The simplest solution relies on the application of heuristics that take into account
the whole predicate?argument structure to remove the incorrect labels (e. g., Moschitti
et al 2005a; Tjong Kim Sang et al 2005). A much more complex solution consists in the
application of some joint inference model to the whole predicate?argument structure,
as in Pradhan et al (2004). As an example, Haghighi, Toutanova, and Manning (2005)
associate a posterior probability with each argument node role assignment, estimate the
likelihood of the alternative labeling schemes, and employ a re-ranking mechanism to
select the best annotation.
Additionally, the most accurate systems participating in CoNLL 2005 shared task
(Pradhan, Hacioglu, Ward et al 2005; Punyakanok et al 2005) use different syntactic
views of the same input sentence. This allows the SRL system to recover from syntactic
parser errors; for example, a prepositional phrase specifying the direct object of the
predicate would be attached to the verb instead of the argument. This kind of error
prevents some arguments of the proposition from being recognized, as: (1) there may
not be a node of the parse tree dominating (all and only) the words of the correct se-
quence; (2) a badly attached tree node may invalidate other argument nodes, generating
unexpected overlapping situations.
The manual design of features which capture important properties of complete
predicate?argument structures (also coming from different syntactic views) is quite
complex. Tree kernels are a valid alternative to manual design as the next section
points out.
3. Tree Kernels
Tree kernels have been applied to reduce the feature design effort in the context of
several natural language tasks, for example, syntactic parsing re-ranking (Collins and
Duffy 2002), relation extraction (Zelenko, Aone, and Richardella 2003), named entity
recognition (Cumby and Roth 2003; Culotta and Sorensen 2004), and semantic role
labeling (Moschitti 2004).
On the one hand, these studies show that the kernel ability to generate large feature
sets is useful to quickly model new and not well understood linguistic phenomena
in learning machines. On the other hand, they show that sometimes it is possible to
manually design features for linear kernels that produce higher accuracy and faster
computation time. One of the most important causes of such mixed behavior is the
inappropriate choice of kernel functions. For example, in Moschitti, Pighin, and Basili
(2006) andMoschitti (2006a), several kernels have been designed and shown to produce
different impacts on the training algorithms.
In the next sections, we briefly introduce the kernel trick and describe the subtree
(ST) kernel devised in Vishwanathan and Smola (2002), the subset tree (SST) kernel
defined in Collins and Duffy (2002), and the partial tree (PT) kernel proposed in
Moschitti (2006a).
3.1 Kernel Trick
The main concept underlying machine learning for classification tasks is the automatic
learning of classification functions based on examples labeled with the class informa-
tion. Such examples can be described by means of feature vectors in an n dimensional
198
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
space over the real numbers, namely, n. The learning algorithm uses space metrics
over vectors, for example, the scalar product, to learn an abstract representation of all
instances belonging to the target class.
For example, support vector machines (SVMs) are linear classifiers which learn a
hyperplane f (x) = w? x+ b = 0, separating positive from negative examples. x is the
feature vector representation of a classifying object o, whereas w ? n and b ?  are
parameters learned from the data by applying the Structural Risk Minimization principle
(Vapnik 1998). The object o is mapped to x via a feature function ? : O ? n, O being
the set of the objects that we want to classify. o is categorized in the target class only if
f (x) ? 0.
The kernel trick allows us to rewrite the decision hyperplane as:
f (x) =
(
?
i=1..l
yi?ixi
)
? x+ b =
?
i=1..l
yi?ixi ? x+ b =
?
i=1..l
yi?i?(oi) ??(o)+ b = 0
where yi is equal to 1 for positive examples and ?1 for negative examples, ?i ?  with
?i ? 0, oi ?i ? {1, .., l} are the training instances and the product K(oi, o) = ??(oi) ??(o)?
is the kernel function associated with the mapping ?.
Note that we do not need to apply the mapping ?; we can use K(oi, o) directly.
This allows us, underMercer?s conditions (Shawe-Taylor and Cristianini 2004), to define
abstract kernel functions which generate implicit feature spaces. A traditional example
is given by the polynomial kernel: Kp(o1, o2) = (c+ x1 ? x2)d, where c is a constant and d
is the degree of the polynomial. This kernel generates the space of all conjunctions of
feature groups up to d elements.
Additionally, we can carry out two interesting operations:
 kernel combinations, for example, K1 + K2 or K1 ? K2
 feature mapping compositions, for example,
K(o1, o2) = ??(o1) ??(o2)? = ??B(?A(o1)) ??B(?A(o2))?
Kernel combinations are very useful for integrating the knowledge provided by the
manually defined features with the knowledge automatically obtained with structural
kernels; feature mapping compositions are useful methods to describe diverse kernel
classes (see Section 5). In this perspective, we propose to split themapping? by defining
our tree kernel as follows:
 Canonical Mapping, ?M(), in which a linguistic object (e. g., a syntactic
parse tree) is transformed into a more meaningful structure (e. g., the
subtree corresponding to a verb subcategorization frame).
 Feature Extraction, ?S(), which maps the canonical structure in all its
fragments according to different fragment spaces S (e. g., ST, SST, and PT).
For example, given the kernel KST = ?ST(o1) ??ST(o2), we can apply a canonical
mapping ?M(), obtaining K
M
ST = ?ST(?M(o1)) ??ST(?M(o2)) =
(
?ST ??M
)
(o1) ?
(
?ST ?
?M
)
(o2), which is a noticeably different kernel, which is induced by the mapping
(
?ST ??M
)
.
199
Computational Linguistics Volume 34, Number 2
In the remainder of this section we start the description of our engineered kernels
by defining three different feature extraction mappings based on three different kernel
spaces (i. e., ST, SST, and PT).
3.2 Tree Kernel Spaces
The kernels that we consider represent trees in terms of their substructures (fragments).
The kernel function detects if a tree subpart (common to both trees) belongs to the fea-
ture space that we intend to generate. For this purpose, the desired fragments need to be
described. We consider three main characterizations: the subtrees (STs) (Vishwanathan
and Smola 2002), the subset trees (SSTs) or all subtrees (Collins and Duffy 2002), and the
partial trees (PTs) (Moschitti 2006a).
As we consider syntactic parse trees, each node with its children is associated with
a grammar production rule, where the symbol on the left-hand side corresponds to the
parent and the symbols on the right-hand side are associated with the children. The
terminal symbols of the grammar are always associated with tree leaves.
A subtree (ST) is defined as a tree rooted in any non-terminal node along with
all its descendants. For example, Figure 1a shows the parse tree of the sentence Mary
brought a cat together with its six STs. A subset tree (SST) is a more general structure
because its leaves can be non-terminal symbols. For example, Figure 1(b) shows ten
SSTs (out of 17) of the subtree in Figure 1a rooted in VP. SSTs satisfy the constraint that
grammatical rules cannot be broken. For example, [VP [V NP]] is an SST which has
two non-terminal symbols, V and NP, as leaves. On the contrary, [VP [V]] is not an
SST as it violates the production VP?V NP. If we relax the constraint over the SSTs, we
obtain a more general form of substructures that we call partial trees (PTs). These can be
generated by the application of partial production rules of the grammar; consequently
[VP [V]] and [VP [NP]] are valid PTs. It is worth noting that PTs consider the position
of the children as, for example, [A [B][C][D]] and [A [D][C][B]] only share single
children, i.e., [A [B]], [A [C]], and [A [D]].
Figure 1c shows that the number of PTs derived from the same tree as before is still
higher (i. e., 30 PTs). These numbers provide an intuitive quantification of the different
degrees of information encoded by each representation.
Figure 1
Example of (a) ST, (b) SST, and (c) PT fragments.
200
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
3.3 Feature Extraction Functions
The main idea underlying tree kernels is to compute the number of common substruc-
tures between two trees T1 and T2 without explicitly considering the whole fragment
space. In the following, we report on the Subset Tree (SST) kernel proposed in Collins
and Duffy (2002). The algorithms to efficiently compute it along with the ST and PT
kernels can be found in Moschitti (2006a).
Given two trees T1 and T2, let { f1, f2, ..} = F be the set of substructures (fragments)
and Ii(n) be equal to 1 if fi is rooted at node n, 0 otherwise. Collins and Duffy?s kernel is
defined as
K(T1,T2) =
?
n1?NT1
?
n2?NT2
?(n1,n2) (1)
where NT1 and NT2 are the sets of nodes in T1 and T2, respectively, and ?(n1,n2) =
?|F|
i=1 Ii(n1)Ii(n2). The latter is equal to the number of common fragments rooted in nodes
n1 and n2.? can be computed as follows:
1. If the productions (i.e. the nodes with their direct children) at n1 and n2
are different, then?(n1,n2) = 0.
2. If the productions at n1 and n2 are the same, and n1 and n2 only have
leaf children (i.e., they are pre-terminal symbols), then ?(n1,n2) = 1.
3. If the productions at n1 and n2 are the same, and n1 and n2 are
not pre-terminals, then?(n1,n2) =
?nc(n1 )
j=1 (1+?(c
j
n1 , c
j
n2 )), where
nc(n1) is the number of children of n1 and c
j
n is the j-th child of n.
Such tree kernels can be normalized and a ? factor can be added to reduce the
weight of large structures (refer to Collins and Duffy [2002] for a complete description).
3.4 Related Work
Although the literature on SRL is extensive, there is almost no study of the use of tree
kernels for its solution. Consequently, the reported research is mainly based on diverse
natural language learning problems tackled by means of tree kernels.
In Collins and Duffy (2002), the SST kernel was experimented with using the voted
perceptron for the parse tree re-ranking task. A combination with the original PCFG
model improved the syntactic parsing. Another interesting kernel for re-ranking was
defined in Toutanova, Markova, andManning (2004). This represents parse trees as lists
of paths (leaf projection paths) from leaves to the top level of the tree. It is worth noting
that the PT kernel includes tree fragments identical to such paths.
In Kazama and Torisawa (2005), an interesting algorithm that speeds up the average
running time is presented. This algorithm looks for node pairs in which the rooted
subtrees share many substructures (malicious nodes) and applies a transformation to
the trees rooted in such nodes to make the kernel computation faster. The results show
a several-hundred-fold speed increase with respect to the basic implementation.
201
Computational Linguistics Volume 34, Number 2
In Zelenko, Aone, and Richardella (2003), two kernels over syntactic shallow
parser structures were devised for the extraction of linguistic relations, for example,
person-affiliation. To measure the similarity between two nodes, the contiguous string
kernel and the sparse string kernelwere used. In Culotta and Sorensen (2004) such kernels
were slightly generalized by providing a matching function for the node pairs. The
time complexity for their computation limited the experiments to a data set of just
200 news items.
In Shen, Sarkar, and Joshi (2003), a tree kernel based on lexicalized tree adjoining
grammar (LTAG) for the parse re-ranking task was proposed. The subtrees induced by
this kernel are built using the set of elementary trees as defined by LTAG.
In Cumby and Roth (2003), a feature description language was used to extract struc-
tured features from the syntactic shallow parse trees associated with named entities.
Their experiments on named entity categorization showed that when the description
language selects an adequate set of tree fragments the voted perceptron algorithm
increases its classification accuracy. The explanationwas that the complete tree fragment
set contains many irrelevant features and may cause overfitting.
In Zhang, Zhang, and Su (2006), convolution tree kernels for relation extraction
were applied in a way similar to the one proposed in Moschitti (2004). The combina-
tion of standard features along with several tree subparts, tailored according to their
importance for the task, produced again an improvement on the state of the art.
Such previous work, as well as that described previously, show that tree kernels
can efficiently represent syntactic objects, for example, constituent parse trees, in huge
feature spaces. The next section describes our SRL system adopting tree kernels within
SVMs.
4. A State-of-the-Art Architecture for Semantic Role Labeling
Ameaningful study of tree kernels for SRL cannot be carried out without a comparison
with a state-of-the-art architecture: Kernel models that improve average performing
systems are just a technical exercise whose findings would have a reduced value. A
state-of-the-art architecture, instead, can be used as a basic system upon which tree
kernels should improve. Because kernel functions in general introduce a sensible slow-
down with respect to the linear approach, we also have to consider efficiency issues.
These aims drove us in choosing the following components for our SRL system:
 SVMs as our learning algorithm; these provide both a state-of-the-art
learning model (in terms of accuracy) and the possibility of using
kernel functions
 a two-stage role labeling module to improve learning and classification
efficiency; this comprises:
? a feature extractor that can represent candidate arguments using
both linear and structured features
? a boundary classifier (BC)
? a role multi-classifier (RM), which is obtained by applying the OVA
(One vs. All) approach
 a conflict resolution module, that is, a software component that resolves
inconsistencies in the annotations using either a rule-based approach
or a tree kernel classifier; the latter allows experimentation with
202
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
the classification of complete predicate?argument annotations in correct
and incorrect structures
 a joint inference re-ranking module, which employs a combination of
standard features and tree kernels to rank alternative candidate labeling
schemes for a proposition; this module, as shown in Gildea and Jurafsky
(2002), Pradhan et al (2004), and Haghighi, Toutanova, and Manning
(2005), is mandatory in order to achieve state-of-the-art accuracy
We point out that we did not use any heuristic to filter out the nodes which are
likely to be incorrect boundaries, for example, as done in Xue and Palmer (2004). On the
one hand, this makes the learning and classification phases more complex because they
involve more instances. On the other hand, our results are not biased by the quality of
the heuristics, leading to more meaningful findings.
In the remainder of this section, we describe the main functional modules of our
architecture for SRL and introduce some basic concepts about the use of structured
features for SRL. Specific feature engineering for the above SRL subtasks is described
and discussed in Section 5.
4.1 A Basic Two-Stage Role Labeling System
Given a sentence in natural language, our SRL system identifies all the verb predicates
and their respective arguments. We divide this step into three subtasks: (a) predicate
detection, which can be carried out by simple heuristics based on part-of-speech infor-
mation, (b) the detection of predicate?argument boundaries (i. e., the span of their words
in the sentence), and (c) the classification of the argument type (e. g., Arg0 or ArgM in
PropBank).
The standard approach to learning both the detection and the classification of
predicate arguments is summarized by the following steps:
1. Given a sentence from the training set, generate a full syntactic parse tree;
2. let P and A be the set of predicates and the set of parse-tree nodes (i. e., the
potential arguments), respectively;
3. for each pair ?p, a? ? P ?A:
 extract the feature representation, ?(p, a), (e. g., attribute?values or
tree fragments [see Section 3.1]);
 if the leaves of the subtree rooted in a correspond to all and
only the words of one argument of p (i. e., a exactly covers an
argument), add ?(p, a) in E+ (positive examples), otherwise
add it in E? (negative examples).
For instance, given the example in Figure 2(a), we would consider all the pairs ?p, a?
where p is the node associated with the predicate took and a is any other tree node not
overlapping with p. If the node a exactly covers the word sequences John or the book,
then ?(p, a) is added to the set E+, otherwise it is added to E?, as in the case of the node
(NN book).
The E+ and E? sets are used to train the boundary classifier. To train the role
multiclassifier, the elements of E+ can be reorganized as positive E+argi and negative E
?
argi
examples for each role type i. In this way, a binary OVA classifier for each argument
203
Computational Linguistics Volume 34, Number 2
Figure 2
Positive (framed) and negative (unframed) examples of candidate argument nodes for the
propositions (a) [Arg0 John] took [Arg1 the book] and read its title and (b) [Arg0 John] took the
book and read [Arg1 its title].
i can be trained. We adopted this solution following Pradhan, Hacioglu, Krugler et al
(2005) because it is simple and effective. In the classification phase, given an unseen
sentence, all the pairs ?p, a? are generated and classified by each individual role classifier
Ci. The argument label associated with the maximum among the scores provided by Ci
is eventually selected.
The feature extraction function ? can be implemented according to different lin-
guistic theories and intuitions. From a technical point of view, we can use ? to map
?p, a? in feature vectors or in structures to be used in a tree kernel function. The next
section describes our choices in more detail.
4.2 Linear and Structured Representation
Our feature extractor module and our learning algorithms are designed to cope with
both linear and structured features, used for the different stages of the SRL process.
The standard features that we adopted are shown in Table 1. They include:
 the Phrase Type, Predicate Word, Head Word, Governing Category, Position,
and Voice defined in Gildea and Jurafsky (2002);
 the Partial Path, No Direction Path, Constituent Tree Distance, Head Word
POS, First and Last Word/POS, Verb Subcategorization, and Head Word of the
Noun Phrase in the Prepositional Phrase proposed in Pradhan, Hacioglu,
Krugler et al (2005); and
 the Syntactic Frame defined in Xue and Palmer (2004).
We indicate with structured features the basic syntactic structures extracted from
the sentence-parse tree or their canonical transformation (see Section 3.1). In particular,
we focus on the minimal spanning tree that includes the predicate along with all of its
arguments.
More formally, given a parse tree t, a node set spanning tree (NST) over a set of
nodes Nt = {n1, . . . ,nk} is a partial tree of t that (1) is rooted at the deepest level and (2)
contains all and only the nodes ni ? Nt, along with their ancestors and descendants. An
NST can be built as follows. For any choice of Nt, we call r the lowest common ancestor
204
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Figure 3
(a) A sentence parse tree, the correct ASTns associated with two different predicates (b,c), and (d)
a correct AST1 relative to the argument Arg1 its title of the predicate read.
of n1, . . . ,nk. Then, from the set of all the descendants of r, we remove all the nodes nj
that: (1) do not belong to Nt and (2) are neither ancestors nor descendants of any node
belonging to Nt.
Because predicate arguments are associatedwith tree nodes, we can define the pred-
icate argument spanning tree (ASTn) of a predicate argument node setAp = {a1, . . . , an}
as the NST over these nodes and the predicate node, that is, the node exactly covering
the predicate p.2 An ASTn corresponds to the minimal parse subtree whose leaves are
all and only the word sequences belonging to the arguments and the predicate. For
example, Figure 3a shows the parse tree of the sentence: John took the book and read its
title. took{ARG0,ARG1} and read{ARG0,ARG1} are two ASTn structures associated with the two
predicates took and read, respectively, and are shown in Figure 3b and 3c.
For each predicate, only one NST is a valid ASTn. Careful manipulations of an ASTn
can be employed for those tasks that require a representation of the whole predicate?
argument structure, for example, overlap resolution or proposition re-ranking.
It is worth noting that the predicate?argument feature, or PAF in Moschitti (2004),
is a canonical transformation of the ASTn in the subtree including the predicate p and
only one of its arguments. For the sake of uniform notation, PAF will be referred to as
AST1 (argument spanning tree), the subscript 1 stressing the fact that the structure only
encompasses one of the predicate arguments. An example AST1 is shown in Figure 3d.
Manipulations of an AST1 structure can lead to interesting tree kernels for local learning
tasks, such as boundary detection and argument classification.
Regardless of the adopted feature space, our multiclassification approach suffers
from the problem of selecting both boundaries and argument roles independently of
the whole structures. Thus, it is possible that (a) two labeled nodes refer to the same
arguments (node overlaps) and (b) invalid role sequences are generated (e. g., Arg0,
Arg0, Arg0, . . . ). Next, we describe our approach to solving such problems.
4.3 Conflict Resolution
We call a conflict, or ambiguity, or overlap resolution a stage of the SRL process
which resolves annotation conflicts that invalidate the underlying linguistic model. This
2 The ASTn of a predicate p and its argument nodes {a1, . . . , an}, will also be referred to as p{a1,..., an}.
205
Computational Linguistics Volume 34, Number 2
happens, for example, when both a node and one of its descendants are classified as
positive boundaries, namely, they received a role label. We say that such nodes are
overlapping as their leaf (i. e., word) sequences overlap. Because this situation is not
allowed by the PropBank annotation definition, we need a method to select the most
appropriate word sequence. Our system architecture can employ one of three different
disambiguation strategies:
 a basic solution which, given two overlapping nodes, randomly selects
one to be removed;
 the following heuristics:
1. The node causing the major number of overlaps is removed, for
example, a node which dominates two nodes labeled as arguments
2. Core arguments (i. e., arguments associated with the
subcategorization frame of the target verb) are always preferred
over adjuncts (i. e., arguments that are not specific to verbs or
verb senses)
3. In case the two previous rules do not eliminate all conflicts, the
nodes located deeper in the tree are discarded; and
 a tree kernel?based overlap resolution strategy consisting of an SVM
trained to recognize non-clashing configurations that often correspond
to correct propositions.
The latter approach consists of: (1) a softwaremodule that generates all the possible non-
overlapping configurations of nodes. These are built using the output of the local node
classifiers by generating all the permutations of argument nodes of a predicate and re-
moving the configurations that contain at least one overlap; (2) an SVM trained on such
non-overlapping configurations, where the positive examples are correct predicate?
argument structures (although eventually not complete) and negative ones are not. At
testing time, we classify all the alternative non-clashing configurations. In case more
than one structure is selected as correct, we choose the one associated with the highest
SVM score.
These disambiguationmodules can be invoked after either the BC or the RM classifi-
cation. The different information available after each phase can be used to design differ-
ent kinds of features. For example, the knowledge of the candidate role of an argument
node can be a key issue in the design of effective conflict resolution methodologies, for
example, by eliminating ArgX, ArgX, ArgX, . . . sequences. These different approaches
are discussed in Section 5.2.
The next section describes a more advanced approach that can eliminate overlaps
and choose the most correct annotation for a proposition among a set of alternative
labeling schemes.
4.4 A Joint Model for Re-Ranking
The heuristics considered in the previous sections only act when a conflict is detected.
In a real situation, many incorrect annotations are generated with no overlaps. To deal
with such cases, we need a re-ranking module based on a joint BC and RM model as
suggested in Haghighi, Toutanova, and Manning (2005). Such a model is based on (1)
206
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
an algorithm to evaluate the most likely labeling schemes for a given predicate, and (2)
a re-ranker that sorts the labeling schemes according to their correctness.
Step 1 uses the probabilities associated with each possible annotation of parse tree
nodes, hence requiring a probabilistic output from BC and RM. As the SVM learning
algorithm produces metric values, we applied Platt?s algorithm (Platt 1999) to convert
them into probabilities, as already proposed in Pradhan, Ward et al (2005). These
posterior probabilities are then combined to generate the n labelings that maximize a
likelihood measure. Step 2 requires the training of an automatic re-ranker. This can be
designed using a binary classifier that, given two annotations, decides which one is
more accurate. We modeled such a classifier by means of three different kernels based
on standard features, structured features, and their combination.
4.4.1 Evaluation of the N-best Annotations. First, we converted the output of each node-
classifier into a posterior probability conditioned by its output scores (Platt 1999).
This method uses a parametric model to fit onto a sigmoid distribution the posterior
probability P(y = 1, f ), where f is the output of the classifier and the parameters are
dynamically adapted to give the best probability output.3 Second, we selected the n
most likely sequences of node labelings. Given a predicate, the likelihood of a labeling
scheme (or state) s for the K candidate argument nodes is given by:
p(s) =
K
?
i=1
p?i (l), p
?
i (l) =
{
pi(li)pi(ARG) if li = NARG
(1? pi(ARG))2 otherwise
(2)
where pi(l) is the probability of node i being assigned the label l, and p
?
i (l) is the same
probability weighted by the probability pi(ARG) of the node being an argument. If l =
NARG (not an argument) then both terms evaluate to (1? pi(ARG)) and the likelihood
of the NARG label assignment is given by (1? pi(ARG))2.
To select the n states associated with the highest probability, we cannot evaluate
the likelihood of all possible states because they are exponential in number. In order
to reduce the search space we (a) limit the number of possible labelings of each node
to m and (b) avoid traversing all the states by applying a Viterbi algorithm to search
for the most likely labeling schemes. From each state we generate the states in which
a candidate argument is assigned different labels. This operation is bound to output at
most n states which are generated by traversing a maximum of n?m states. Therefore,
in the worst case scenario the number of traversed states is V = n?m? k, k being the
number of candidate argument nodes in the tree.
During the search we also enforce overlap resolution policies. Indeed, for any given
state in which a node nj is assigned a label l = NARG, we generate all; and only the
states in which all the nodes that are dominated by nj are assigned the NARG label.
4.4.2 Modeling an Automatic Re-Ranker. The Viterbi algorithm generates the nmost likely
annotations for the proposition associated with a predicate p. These can be used to build
annotation pairs, ?si, sj?, which, in turn, are used to train a binary classifier that decides if
3 We actually implemented the pseudo-code proposed in Lin, Lin, and Weng (2003) which, with respect
to Platt?s original formulation, is theoretically demonstrated to converge and avoids some numerical
difficulties that may arise.
207
Computational Linguistics Volume 34, Number 2
si is more accurate that sj. Each candidate proposition si can be described by a structured
feature ti and a vector of standard features vi. As a whole, an example ei is described by
the tuple ?t1i , t
2
i , v
1
i , v
2
i ?, where t
1
i and v
1
i refer to the first candidate annotation, whereas t
2
i
and v2i refer to the second one. Given such data, we can define the following re-ranking
kernels:
Ktr(e1, e2) = Kt(t
1
1, t
1
2)+ Kt(t
2
1, t
2
2)? Kt(t
1
1, t
2
2)? Kt(t
2
1, t
1
2)
Kpr(e1, e2) = Kp(v
1
1, v
1
2)+ Kp(v
2
1, v
2
2)? Kp(v
1
1, v
2
2)? Kp(v
2
1, v
1
2)
where Kt is one of the tree kernel functions defined in Section 3 and Kp is a polynomial
kernel applied to the feature vectors. The final kernel that we use is the following
combination:
K(e1, e2) =
Ktr(e1, e2)
|Ktr(e1, e2)|
+
Kpr(e1, e2)
|Kpr(e1, e2)|
Previous sections have shown how our SRL architecture exploits tree kernel func-
tions to a large extent. In the next section, we describe in more detail our structured
features and the engineering methods applied for the different subtasks of the SRL
process.
5. Structured Feature Engineering
Structured features are an effective alternative to standard features in many aspects. An
important advantage is that the target feature space can be completely changed even
by small modifications of the applied kernel function. This can be exploited to identify
features relevant to learning problems lacking a clear and sound linguistic or cognitive
justification.
As shown in Section 3.1, a kernel function is a scalar product ?(o1) ??(o2), where
? is a mapping in an Euclidean space, and o1 and o2 are the target data, for example,
parse trees. To make the engineering process easier, we decompose ? into a canonical
mapping, ?M, and a feature extraction function, ?S, over the set of incoming parse
trees. ?M transforms a tree into a canonical structure equivalent to an entire class of
input parses and ?S shatters an input tree into its subparts (e. g., subtrees, subset trees,
or partial trees as described in Section 3). A large number of different feature spaces can
thus be explored by suitable combinations ? = ?S ??M of mappings.
We study different canonical mappings to capture syntactic/semantic aspects useful
for SRL. In particular, we define structured features for the different phases of the SRL
process, namely, boundary detection, argument classification, conflict resolution, and
proposition re-ranking.
5.1 Structures for Boundary Detection and Argument Classification
The AST1 or PAF structures, already mentioned in Section 4.2, have shown to be very
effective for argument classification but not for boundary detection. The reason is that
two nodes that encode correct and incorrect boundaries may generate very similar
AST1s and, consequently, have many fragments in common. To solve this problem, we
208
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Figure 4
Parse tree of the example proposition [Arg0 Paul] delivers [Arg1 a talk in formal style].
Figure 5
(a) AST1, (b) AST
m
1 , and (c) AST
cm
1 structures relative to the argument Arg1 a talk in formal style of
the predicate delivers of the example parse tree shown in Figure 4.
specify the node that exactly covers the target argument node by simply marking it (or
marking all its descendants) with the label B, denoting the boundary property.
For example, Figure 4 shows the parse tree of the sentence Paul delivers a talk in
formal style, highlighting the predicate with its two arguments, that is, Arg0 and Arg1.
Figure 5 shows the AST1, AST
m
1 , and AST
cm
1 , that is, the basic structure, the structure
with the marked argument node, and the completely marked structure, respectively.
To understand the usefulness of node-marking strategies, we can examine Figure 6.
This reports the case in which a correct and an incorrect argument node are chosen by
also showing the corresponding AST1 and AST
m
1 representations ((a) and (b)). Figure 6c
shows that the number of common fragments of two AST1 structures is 14. This is
much larger than the number of common ASTm1 fragments, that is, only 3 substructures
(Figure 6d).
Additionally, because the type of a target argument strongly depends on the type
and number of the other predicate arguments4 (Punyakanok et al 2005; Toutanova,
4 This is true at least for core arguments.
209
Computational Linguistics Volume 34, Number 2
Figure 6
(a) AST1s and (b) AST
m
1 s extracted for the same target argument with their respective (c,b)
common fragment spaces.
Haghighi, and Manning 2005), we should extract features from the whole predicate
argument structure. In contrast, AST1s completely neglect the information (i. e., the tree
portions) related to non-target arguments.
One way to use this further information with tree kernels is to use the minimum
subtree that spans all the predicate?argument structures, that is, the ASTn defined in
Section 4.2.
However, ASTns pose two problems. First, we cannot use them for the boundary
detection task since we do not know the predicate?argument structure yet. We can
derive the ASTn (its approximation) from the nodes selected by a boundary classifier,
that is, the nodes that correspond to potential arguments. Such approximated ASTns
can be easily used in the argument classification stage.
Second, an ASTn is the same for all the arguments in a proposition, thus we need a
way to differentiate it for each target argument. Again, we canmark the target argument
node as shown in the previous section. We refer to this subtree as a marked target
ASTn (AST
mt
n ). However, for large arguments (i. e., spread over a large part of the
sentence tree) the substructures? likelihood of being part of different arguments is quite
high.
To address this problem, we can mark all the nodes that descend from the target
argument node. We refer to this structure as a completely marked targetASTn (AST
cmt
n ).
ASTcmtn s may be seen as AST1s enriched with new information coming from the other
arguments (i. e., the non-marked subtrees). Note that if we only consider the AST1
subtree from a ASTcmtn , we obtain AST
cm
1 .
210
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
5.2 Structured Features for Conflict Resolution
This section describes structured features employed by the tree kernel?based conflict
resolution module of the SRL architecture described in Section 4.3. This subtask is
performed by means of:
1. A first annotation of potential arguments using a high recall boundary
classifier and, eventually, the role information provided by a role
multiclassifier (RM).
2. An ASTn classification step aiming at selecting, among the substructures
that do not contain overlaps, those that are more likely to encode the
correct argument set.
The set of argument nodes recognized by BC can be associated with a subtree of the
corresponding sentence parse, which can be classified using tree kernel functions. These
should evaluate whether a subtree encodes a correct predicate?argument structure or
not. As it encodes features from the whole predicate?argument structure, the ASTn that
we introduced in Section 4.2 is a structure that can be employed for this task.
Let Ap be the set of potential argument nodes for the predicate p output by BC; the
classifier examples are built as follows: (1) we look for node pairs ?n1,n2? ? Ap ? Ap
where n1 is the ancestor of n2 or vice versa; (2) we create two node sets A1 = A? {n1}
and A2 = A? {n2} and classify the two NSTs associated with A1 and A2 with the tree
kernel classifier to select the most correct set of argument boundaries. This procedure
can be generalized to a set of overlapping nodes Owith more than two elements, as we
simply need to generate all and only the permutations of A?s nodes that do not contain
overlapping pairs.
Figure 7 shows a working example of such amulti-stage classifier. In (Figure 7a), the
BC labels as potential arguments four nodes (circled), three of which are overlapping
Figure 7
An overlap situation (a) and the candidate solutions resulting from the employment of the
different marking strategies.
211
Computational Linguistics Volume 34, Number 2
(in bold circles). The overlap resolution algorithm proposes two solutions (Figure 7b)
of which only one is correct. In fact, according to the second solution, the preposi-
tional phrase of the book would incorrectly be attached to the verbal predicate, that
is, in contrast with the parse tree. The ASTn classifier, applied to the two NSTs,
should detect this inconsistency and provide the correct output. Figure 7 also high-
lights a critical problem the ASTn classifier has to deal with: as the two NSTs are
perfectly identical, it is not possible to distinguish between them using only their
fragments.
In order to engineer novel features, we simply add the boundary information pro-
vided by BC to the NSTs. We mark with a progressive number the phrase type cor-
responding to an argument node, starting from the leftmost argument. We call the
resulting structure an ordinal predicate?argument spanning tree (ASTordn ). For example,
in the first NST of Figure 7c, we mark as NP-0 and NP-1 the first and second argument
nodes, whereas in the second NST, we have a hypothesis of three arguments on three
nodes that we transform as NP-0, NP-1, and PP-2.
This simple modification enables the tree kernel to generate features useful for dis-
tinguishing between two identical parse trees associated with different argument struc-
tures. For example, for the first NST the fragments [NP-1 [NP PP]], [NP [DT NN]], and
[PP [IN NP]] are generated. They no longer match with the fragments of the second
NST [NP-0 [NP PP]], [NP-1 [DT NN]], and [PP-2 [IN NP]].
We also experimented with another structure, the marked predicate?argument
spanning tree (ASTmn ), in which each argument node is marked with a role label as-
signed by a role multi-classifier (RM). Of course, this model requires a RM to classify all
the nodes recognized by BC first. An example ASTmn is shown in Figure 7d.
5.3 Structures for Proposition Re-Ranking
In Section 4.4, we presented our re-ranking mechanism, which is inspired by the joint
inference model described in Haghighi, Toutanova, and Manning (2005). Designing
structured features for the re-ranking classifier is complex in many aspects. Unlike
the other structures that we have discussed so far, the defined mappings should:
(1) preserve as much information as possible about the whole predicate?argument
structure; (2) focus the learning algorithm on the whole structure; and (3) be able
to identify those small differences that distinguish more or less accurate labeling
schemes. Among the possible solutions that we have explored, three are especially
interesting in terms of accuracy improvement or linguistic properties, and are described
hereinafter.
The ASTcmn (completely marked ASTn, see Figure 8a) is an ASTn in which each
argument node label is enriched with the role assigned to the node by RM. The la-
bels of the descendants of each argument node are modified accordingly, down to
pre-terminal nodes. The ASTcmtn is a variant of AST
cm
n in which only the target is
marked. Marking a node descendant is meant to force substructures matching only
among homogeneous argument types. This representation should provide rich syn-
tactic and lexical information about the parse tree encoding the predicate?argument
structure.
The PAS (predicate?argument structure, see Figure 8b) is a completely different
structure that preserves the parse subtrees associated with each argument node while
discarding the intra-argument syntactic parse information. Indeed, the syntactic links
between the argument nodes are represented as a dummy 1-level tree, which appears
in any PAS and therefore does not influence the evaluation of similarity between pairs
212
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Figure 8
Different representations of the same proposition.
of structures. This structure accommodates the predicate and all the arguments of an
annotation in a sequence of seven slots.5 To each slot is attached an argument label to
which in turn is attached the subtree rooted in the argument node. The predicate is
represented by means of a pre-terminal node labeled rel to which the lemmatization of
the predicate word is attached as a leaf node. In general, a proposition consists of m
arguments, with m ? 6, where m varies according to the predicate and the context. To
guarantee that predicate structures with a different number of arguments are matched
in the SST kernel function, we attach a dummy descendant marked null to the slots not
filled by an argument.
The PAStl (type-only, lemmatized PAS, see Figure 8c) is a specialization of the PAS
that only focuses on the syntax of the predicate?argument structure, namely, the type
and relative position of each argument, minimizing the amount of lexical and syntactic
information derived from the parse tree. The differences with the PAS are that: (1) each
slot is attached to a pre-terminal node representing the argument type and a terminal
node whose label indicates the syntactic type of the argument; and (2) the predicate
word is lemmatized.
The next section presents the experiments used to evaluate the effectiveness of the
proposed canonical structures in SRL.
5 We assume that predicate?argument structures cannot be composed by more than six arguments, which
is generally true.
213
Computational Linguistics Volume 34, Number 2
6. Experiments
The experiments aim to measure the contribution and the effectiveness of our proposed
kernel engineering models and of the diverse structured features that we designed
(Section 5). From this perspective, the role of feature extraction functions is not
fundamental because the study carried out in Moschitti (2006a) strongly suggests that
the SST (Collins and Duffy 2002) kernel produces higher accuracy than the PT kernel
when dealing with constituent parse trees, which are adopted in our study.6 We then
selected the SST kernel and designed the following experiments:
(a) A study of canonical functions based on node marking for boundary detection
and argument classification, that is, ASTm1 (Section 6.2). Moreover, as the standard
features have shown to be effective, we combined them with ASTm1 based kernels on
the boundary detection and classification tasks (Section 6.2).
(b) We varied the amount of training data to demonstrate the higher generalization
ability of tree kernels (Section 6.3).
(c) Given the promising results of kernel engineering, we also applied it to solve a more
complex task, namely, conflict resolution in SRL annotations (see Section 6.4). As this
involves the complete predicate?argument structure, we could test advanced canonical
functions generating ASTn, AST
ord
n , and AST
m
n .
(d) Previous work has shown that re-ranking is very important in boosting the accuracy
of SRL. Therefore, we tested advanced canonical mappings, that is, those based on
ASTcmn , PAS, and PAS
tl, on such tasks (Section 6.5).
6.1 General Setup
The empirical evaluations were mostly carried out within the setting defined in the
CoNLL 2005 shared task (Carreras and Ma`rquez 2005). As a target data set, we
used the PropBank7 and the automatic Charniak parse trees of the sentences of Penn
TreeBank 2 corpus8 (Marcus, Santorini, and Marcinkiewicz 1993) from the CoNLL 2005
shared-task data.9 We employed the SVM-light-TK software10, which encodes fast tree
kernel evaluation (Moschitti 2006b), and combinations betweenmultiple feature vectors
and trees in the SVM-light software (Joachims 1999). We used the default regularization
parameter (option -c) and ? = 0.4 (see Moschitti [2004]).
6.2 Testing Canonical Functions Based on Node Marking
In these experiments, we measured the impact of node marking strategies on boundary
detection (BD) and the complete SRL task, that is, BD and role classification (RC). We
employed a configuration of the architecture described in Section 4 and previously
6 Of course the PT kernel may be much more accurate in processing PAS and PAStl because these are not
simply constituent parse trees. Nevertheless, a study of the PT kernel potential is beyond the purpose
of this article.
7 http://www.cis.upenn.edu/?ace.
8 http://www.cis.upenn.edu/?treebank.
9 http://www.lsi.upc.edu/?srlconll/.
10 http://ai-nlp.info.uniroma2.it/moschitti/.
214
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Table 2
Number of arguments (Arguments) and of unrecoverable arguments (Unrecoverable) due to
parse tree errors in Sections 2, 3, and 24 of the Penn TreeBank/PropBank.
Sec. Arguments Unrecoverable
2 198,373 454 (0.23%)
3 147,193 347 (0.24%)
24 139,454 731 (0.52%)
Table 3
Comparison between different models on Boundary Detection and the complete Semantic Role
Labeling tasks. The training set is constituted by the first 1 million instances from Sections 02?06
for the boundary classifier and all arguments from Sections 02?21 for the role multiclassifier
(253,129 instances). The performance is measured against Section 24 (149,140 instances).
Boundary Detection Semantic Role Labeling
Kernels P R F1 P R F1
AST1 75.75% 71.68% 73.66 64.71% 61.71% 63.17
ASTm1 77.32% 74.80% 76.04 66.58% 64.87% 65.71
Poly 82.18% 79.19% 80.66 75.86% 72.60% 73.81
Poly+AST1 81.74% 80.71% 81.22 74.23% 73.62% 73.92
Poly+ASTm1 81.64% 80.73% 81.18 74.36% 73.87% 74.11
adopted in Moschitti et al (2005b), in which the simple conflict resolution heuristic is
applied. The results were derived within the CoNLL setting by means of the related
evaluator.
In more detail, in the BD experiments, we used the first million instances from the
Penn TreeBank Sections 2?6 for training11 and Section 24 for testing. Our classification
model applied to this data replicates the results obtained in the CoNLL 2005 shared
task, that is, the highest accuracy in BD among the systems using only one parse
tree and one learning algorithm. For the complete SRL task, we used the previous
BC and all the available data, that is, the sections from 2 to 21, for training the role
multiclassifier.
It is worth mentioning that, as the automatic parse trees contain errors, some
arguments cannot be associated with any covering node; thus we cannot extract a
tree representation for them. In particular, Table 2 shows the number of arguments
(column 2) for sections 2, 3, and 24 as well as the number of arguments that we could not
take into account (Unrecoverable) due to the lack of parse tree nodes exactly covering
their word spans. Note how Section 24 of the Penn TreeBank (which is not part of the
Charniak training set) is much more affected by this problem.
Given this setting, the impact of node marking can be measured by comparing the
AST1 and the AST
m
1 based kernels. The results are reported in the rows AST1 and AST
m
1
of Table 3. Columns 2, 3, and 4 show their Precision, Recall, and F1 measure on BD and
columns 5, 6, and 7 report the performance on SRL. We note that marking the argument
11 This was the most expensive process in terms of training time, requiring more than one week.
215
Computational Linguistics Volume 34, Number 2
node simplifies the generalization process as it improves both tasks by about 3.5 and 2.5
absolute percentage points, respectively.
However, Row Poly shows that the polynomial kernel using state-of-the-art fea-
tures (Moschitti et al 2005b) outperforms ASTm1 by about 4.5 percentage points in BD
and 8 points in the SRL task. The main reason is that the employed tree structures
do not explicitly encode very important features like the passive voice or predicate
position. In Moschitti (2004), these are shown to be very effective especially when used
in polynomial kernels. Of course, it is possible to engineer trees including these and
other standard features with a canonical mapping, but the aim here is to provide new
interesting representations rather than to abide by the simple exercise of representing
already designed features within tree kernel functions. In other words, we follow the
idea presented in Moschitti (2004), where tree kernels were suggested as a means to
derive new features rather than generate a stand-alone feature set.
Rows Poly+AST1 and Poly+AST
m
1 investigate this possibility by presenting the
combination of polynomial and tree kernels. Unfortunately, the results on both BD and
SRL do not show enough improvement to justify the use of tree kernels; for example,
Poly+ASTm1 improves Poly by only 0.52 in BD and 0.3 in SRL. The small improvement
is intuitively due to the use of (1) a state-of-the-art model as a baseline and (2) a very
large amount of training data which decreases the contribution of tree features. In the
next section an analysis in terms of training data will shed some light on the role of tree
kernels for BD and RC in SRL.
6.3 The Role of Tree Kernels for Boundary Detection and Argument Classification
The previous section has shown that if a state-of-the-art model12 is adopted, then the
tree kernel contribution is marginal. On the contrary, if a non state-of-the-art model is
adopted tree kernels can play a significant role. To verify this hypothesis, we tested the
polynomial kernel over the standard feature vector proposed in Gildea and Jurafsky
(2002) obtaining an F1 of 67.3, which is comparable with the ASTm1 model, that is 65.71.
Moreover, a kernel combination produced a significant improvement of both models
reaching an F1 of 70.4.
Thus, the role of tree kernels relates to the design of features for novel linguistic
tasks for which the optimal data representation has not yet been developed. For exam-
ple, although SRL has been studied for many years and many effective features have
been designed, representations for languages like Arabic are still not very well under-
stood and raise challenges in the design of effective predicate?argument descriptions.
However, this hypothesis on the usefulness of tree kernels is not completely satis-
factory as the huge feature space produced by them should play a more important role
in predicate?argument representation. For example, the many fragments extracted by
an AST1 provide a very promising back-off model for the Path feature, which should
improve the generalization process of SVMs.
As back-off models show their advantages when the amount of training data
is small, we experimented with Poly, AST1, AST
m
1 , Poly+AST1, and Poly+AST
m
1 and
12 The adopted model is the same as used in Moschitti et al (2005b), which is the most accurate among the
systems that use a single learning model, a single source of syntactic information, and no accurate
inference mechanism. If tree kernels improved this basic model they would likely improve the
accuracy of more complex systems as well.
216
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
different bins of training data, starting from a very small set, namely, 10,000 instances
(1%) to 1 million (100%) of instances. The results from the BD classifiers and the
complete SRL task are very interesting and are illustrated by Figure 9. We note several
things.
First, Figure 9a shows that with only 1% of data (i.e., 640 arguments) as positive
examples, the F1 on BD of the ASTm1 kernel is surprisingly about 3 percentage points
higher than the one obtained by the polynomial kernel (Poly) (i. e., the state of the art).
When ASTm1 is combined with Poly the improvement reaches 5 absolute percentage
points. This suggests that tree kernels should always be used when small training data
sets are available.
Second, although the performance of AST1 is much lower than all the other models,
its combination with Poly produces results similar to Poly+ASTm1 , especially when
the amount of training data increases. This, in agreement with the back-off property,
indicates that the number of tree fragments is more relevant than their quality.
Third, Figure 9b shows that as we increase training data, the advantage of using
tree kernels decreases. This is rather intuitive as (i) in general less accurate data machine
learning models trained with enough data can reach the accuracy of the most accurate
models, and (ii) if the hypothesis that tree kernels provide back-off models is true, a lot
of training data makes them less critical, for example, the probability of finding the Path
feature of a test instance in the training set becomes high.
Figure 9
Learning curves for BD (a and b) and the SRL task (c and d), where 100% of data corresponds to
1 million candidate argument nodes for boundary detection and 64,000 argument nodes for role
classification.
217
Computational Linguistics Volume 34, Number 2
Table 4
Boundary detection accuracy (F1) on gold-standard parse trees and ambiguous structures
employing the different conflict resolution methodologies described in Section 4.3.
RND HEU ASTordn
73.13 71.50 91.11
Finally, Figures 9c and 9d show learning curves13 similar to Figures 9a and 9b, but
with a reduced impact of tree kernels on the Poly model. This is due to the reduced
impact of ASTm1 on role classification. Such findings are in agreement with the results
in Moschitti (2004), which show that for argument classification the SCF structure (a
variant of the ASTmn ) is more effective. Thus a comparison between learning curves of
Poly and SCF on RC may show a behavior similar to Poly and ASTm1 for BD.
6.4 Conflict Resolution Results
In these experiments, we are interested in (1) the evaluation of the accuracy of our
tree kernel?based conflict resolution strategy and (2) studying the most appropriate
structured features for the task.
A first evaluation was carried out over gold-standard Penn TreeBank parses and
PropBank annotations. We compared the alternative conflict resolution strategies imple-
mented by our architecture (see Section 4.3), namely the random (RND), the heuristic
(HEU), and a tree kernel?based disambiguator working with ASTordn structures. The
disambiguators were run on the output of BC, that is, without any information about the
candidate arguments? roles. BC was trained on Sections 2 to 7 with a high-recall linear
kernel. We applied it to classify Sections 8 to 21 and obtained 2,988 NSTs containing at
least one overlapping node. These structures generated 3,624 positive NSTs (i. e., correct
structures) and 4,461 negative NSTs (incorrect structures) in which no overlap is present.
We used them to train the ASTordn classifier. The F1 measure on the boundary detection
task was evaluated on the 385 overlapping annotations of Section 23, consisting of 642
argument and 15,408 non-argument nodes.
The outcome of this experiment is summarized in Table 4. We note two points.
(1) The RND disambiguator (slightly) outperforms the HEU. This suggests that the
heuristics that we implemented were inappropriate for solving the problem. It also
underlines how difficult it is to explicitly choose the aspects that are relevant for a
complex, non-local task such as overlap resolution. (2) The ASTordn classifier outperforms
the other strategies by about 20 percentage points, that is, 91.11 vs. 73.13 and 71.50.
This datum along with the previous one is a good demonstration of how tree kernels
can be effectively exploited to describe phenomena whose relevant features are largely
unknown or difficult to represent explicitly. It should be noted that a more accurate
baseline can be provided by using the Viterbi-style search (see Section 4.4.1). However,
the experiments in Section 6.5 show that the heuristics produce the same accuracy (at
least when the complete task is carried out).
13 Note that using all training data, all the models reach lower F1s than the respective values shown in
Table 3. This happens because the data for training the role multiclassifier is restricted to the first
million instances, in other words, about 64,000 out of the total 253,129 arguments.
218
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Table 5
SRL accuracy on different PropBank target sections in terms of F1 measure of the different
structured features employed for conflict resolution.
Target section ASTn AST
ord
n AST
m
n
21 73.7 77.3 78.7
23 68.9 71.2 72.1
These experiments suggest that tree kernels are promising methods for resolving
annotation conflicts; thus, we tried to also select the most representative structured
features (i. e., ASTn, AST
ord
n , or AST
m
n ) when automatic parse trees are used. We trained
BC on Sections 2?8, whereas, to achieve a very accurate argument classifier, we trained
a role multi-classifier (RM) on Sections 2?21. Then, we trained the ASTn, AST
ord
n , and
ASTmn classifiers on the output of BC. To test BC, RM, and the tree kernel classifiers, we
ran two evaluations on Section 23 and Section 21.14
Table 5 shows the F1 measure for the different tree kernels (columns 2, 3, and 4) for
conflict resolution over the NSTs of Sections 21 and 23. Several points should be noted.
(1) The general performance is much lower than that achieved on gold-standard
trees, as shown in Table 4. This datum and the gap of about 6 percentage points between
Sections 21 and 23 confirm the impact of parsing accuracy on the subtasks of the SRL
process.
(2) The ordinal numbering of arguments (ASTordn ) and the role type information
(ASTmn ) provide tree kernels with more meaningful fragments because they improve
the basic model by about 4 percentage points.
(3) The deeper semantic information generated by the argument labels provides
useful clues for selecting correct predicate?argument structures because the ASTmn
model improves ASTordn performance on both sections.
6.5 Proposition Re-Ranking Results
In these experiments, Section 23 was used for testing our proposition re-ranking. We
employed a BC trained on Sections 2 to 8, whereas RMwas trained on Sections 2?12.15 In
order to provide a probabilistic interpretation of the SVM output (see Section 4.4.1), we
evaluated each classifier distribution parameter based on its output on Section 12. For
computational complexity reasons, we decided to consider the five most likely labelings
for each node and the five first alternatives output by the Viterbi algorithm (i. e., m = 5
and n = 5).
With this set-up, we evaluated the accuracy lower and upper bounds of our system.
As our baseline, we consider the accuracy of a re-ranker that always chooses the first
alternative output from the Viterbi algorithm, that is, the most likely according to the
joint inference model. This accuracy has been measured as 75.91 F1 percentage points;
this is practically identical to the 75.89 obtained by applying heuristics to remove
overlaps generated by BC.
14 As Section 21 of the Penn TreeBank is part of the Charniak parser training set, the performance derived
on its parse trees represents an upper bound for our classifiers, i. e., the results using a nearly ideal
syntactic parser and role multiclassifier.
15 In these experiments we did not use tree kernels for BC and RM as we wanted to measure the impact of
tree kernels only on the re-ranking stage.
219
Computational Linguistics Volume 34, Number 2
This does not depend on the bad quality of the five top labelings. Indeed, we
selected the best alternative produced by the Viterbi algorithm according to the gold-
standard score, and we obtained an F1 of 84.76 for n = 5. Thus, the critical aspect resides
in the selection of the best annotations, which should be carried out by an automatic
re-ranker.
Rows 2 and 3 of Table 6 show the number of distinct propositions and alternative
annotations output by the Viterbi algorithm for each of the employed sections. In row
3, the number of pair comparisons (i. e., the number of training/test examples for the
classifier) is shown.
Using this data, we carried out a complete SRL experiment, which is summarized in
Table 7. First, we compared the accuracy of the ASTcmn , PAS, and PAS
tlclassifiers trained
on Section 24 (in row 3, columns 2, 3, and 4) and discovered that the latter structure
produces a noticeable F1 improvement, namely, 78.15 vs. 76.47 and 76.77, whereas the
accuracy gap between the PAS and the ASTcmn classifiers is very small, namely, 76.77
vs. 76.47 percentage points. We selected the most interesting structured feature, that
is, the PAStl, and extended it with the local (to each argument node) standard features
commonly employed for the boundary detection and argument classification tasks, as
in Haghighi, Toutanova, and Manning (2005). This richer kernel (PAStl+STD, column 5)
was compared with the PAStl one. The comparison was performed on two different
training sets (rows 2 and 3): In both cases, the introduction of the standard features
produced a performance decrement, most notably in the case of Section 12 (i. e., 82.07
vs. 75.06). Our best re-ranking kernel (i. e., the PAStl) was then employed in a larger
experiment, using both Sections 12 and 24 for testing (row 4), achieving an F1 measure
of 78.44.
First, we note that the accuracy of the ASTcmn and PAS classifiers is very similar (i. e.,
76.77 vs. 76.47). This datum suggests that the intra-argument syntactic information is
not critical for the re-ranking task, as including it or not in the learning algorithm does
not lead to noticeable differences.
Second, we note that the PAStl kernel is much more effective than those based on
ASTcmn and PAS, which are always outperformed. This may be due to the fact that
Table 6
Number of propositions, alternative annotations (as output by the Viterbi algorithm), and pair
comparisons (i. e., re-ranker input examples) for the PropBank sections used for the experiments.
Section 12 Section 23 Section 24
Propositions 4,899 5,267 3,248
Alternatives 24,494 26,325 16,240
Comparisons 74,650 81,162 48,582
Table 7
Summary of the proposition re-ranking experiments with different training sets.
Training Section ASTcmn PAS PAS
tl PAStl+STD
12 ? ? 78.27 77.61
24 76.47 76.77 78.15 77.77
12+24 ? ? 78.44 ?
220
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
two ASTcmn s (or PASs) always share a large number of substructures, because most
alternative annotations tend to be very similar and the small differences among them
only affect a small part of the encoding of syntactic information; on the other hand,
the small amount of local parsing information encoded in the PAStls enables a good
generalization process.
Finally, the introduction of the standard, local standard features in our re-ranking
model caused a performance loss of about 0.5 percentage points on both Sections 12 and
24. This fact, which is in contrast with what has been shown in Haghighi, Toutanova,
and Manning (2005), might be the consequence of the small training sets that we
employed. Indeed, local standard features tend to be very sparse and their effectiveness
should be evaluated against a larger data set.
7. Discussions and Conclusions
The design of automatic systems for the labeling of semantic roles requires the solution
of complex problems. Among other issues, feature engineering is made difficult by the
structured nature of the data, that is, features should represent information expressed
by automatically generated parse trees. This raises two main problems: (1) the mod-
eling of effective features, partially solved for some subtasks in previous works, and
(2) the implementation of the software for the extraction of a large number of such
features.
A system completely (or largely) based on tree kernels alleviates both problems
as (1) kernel functions automatically generate features and (2) only a procedure for the
extraction of subtrees is needed. Although some of themanually designed features seem
to be superior to those derived with tree kernels, their combination still seems worth
applying. Moreover, tree kernels provide a back-off model that greatly outperforms
state-of-the-art SRL models when the amount of training data is small.
To demonstrate these points, we carried out a comprehensive study of the use of
tree kernels for semantic role labeling by designing several canonical mappings. These
correspond to the application of innovative tree kernel engineering techniques tailored
to different stages of an SRL process. The experiments with these methods and SVMs on
the data set provided by the CoNLL 2005 shared task (Carreras andMa`rquez 2005) show
that, first, tree kernels are a valid support tomanually designed features for many stages
of the SRL process. We have shown that our improved tree kernel (i.e., the one based
on ASTm1 ) highly improves accuracy in both boundary detection and the SRL task when
the amount of training data is small (e.g., 5 absolute percentage points over a state-of-
the-art boundary classifier). In the case of argument classification the improvement is
less evident but still consistent, at about 3%.
Second, appropriately engineered tree kernels can replace standard features in
many SRL subtasks. For example, in complex tasks such as conflict resolution or re-
ranking, they provide an easy way to build new features that would be difficult to
describe explicitly. More generally, tree kernels can be used to combine different sources
of information for the design of complex learning models.
Third, in the specific re-ranking task, our structured features show a noticeable im-
provement over our baseline (i. e., about 2.5 percentage points). This could be increased
considering that we have not been able to fully exploit the potential of our re-ranking
model, whose theoretical upper bound is 6 percentage points away. Still, although we
only used a small fraction of the available training data (i. e., only 2 sections out of 22
were used to train the re-ranker) our system?s accuracy is in line with state-of-the-art
systems (Carreras and Ma`rquez 2005) that do not employ tree kernels.
221
Computational Linguistics Volume 34, Number 2
Finally, although the study carried out in this article is quite comprehensive, several
issues should be considered in more depth in the future:
(a) The tree feature extraction functions ST, SST, and PT should be studied in com-
bination with the proposed canonical mappings. For example, as the PT kernel seems
more suitable for the processing of dependency information, it would be interesting
to apply it in an architecture using these kinds of syntactic parse trees (e. g., Chen
and Rambow 2003). In particular, the combination of different extraction functions on
different syntactic views may lead to very good results.
(b) Once the set of the most promising kernels is established, it would be interesting
to use all the available CoNLL 2005 data. This would allow us to estimate the potential
of our approach by comparing it with previous work on a fairer basis.
(c) The use of fast tree kernels (Moschitti 2006a) along with the proposed tree repre-
sentations makes the learning and classification much faster, so that the overall running
time is comparable with polynomial kernels. However, when used with SVMs their
running time on very large data sets (e. g., millions of instances) becomes prohibitive.
Exploiting tree kernel?derived features in a more efficient way (e. g., by selecting the
most relevant fragments and using them in an explicit space) is thus an interesting
line of future research. Note that such fragments would be the product of a reverse
engineering process useful to derive linguistic insights on semantic role theory.
(d) As CoNLL 2005 (Punyakanok et al 2005) has shown that multiple parse trees
provide the most important boost to the accuracy of SRL systems, we would like to
extend our model to work with multiple syntactic views of each input sentence.
Acknowledgments
This article is the result of research on kernel
methods for Semantic Role Labeling which
started in 2003 and went under the review of
several program committees of different
scientific communities, from which it highly
benefitted. In this respect, we would like to
thank the reviewers of the SRL special issue
as well as those of the ACL, CoNLL, EACL,
ECAI, ECML, HLT-NAACL, and ICML
conferences. We are indebted to Silvia
Quarteroni for her help in reviewing the
English formulation of an earlier version of
this article.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In COLING-ACL ?98:
Proceedings of the Conference, pages 86?90,
Montre?al, Canada.
Carreras, Xavier and Llu??s Ma`rquez.
2004. Introduction to the CoNLL-2004
shared task: Semantic role labeling.
In HLT-NAACL 2004 Workshop: Eighth
Conference on Computational Natural
Language Learning (CoNLL-2004),
pages 89?97, Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez.
2005. Introduction to the CoNLL-2005
shared task: Semantic role labeling.
In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 152?164,
Ann Arbor, MI.
Chen, John and Owen Rambow. 2003.
Use of deep linguistic features for
the recognition and labeling of
semantic arguments. In Proceedings
of the 2003 Conference on Empirical
Methods in Natural Language Processing,
pages 41?48, Sapporo, Japan.
Collins, Michael and Nigel Duffy. 2002.
New ranking algorithms for parsing and
tagging: Kernels over discrete structures,
and the voted perceptron. In ACL02,
pages 263?270, Philadelphia, PA.
Culotta, Aron and Jeffrey Sorensen. 2004.
Dependency tree kernels for relation
extraction. In ACL04, pages 423?429,
Barcelona, Spain.
Cumby, Chad and Dan Roth. 2003. Kernel
methods for relational learning. In
Proceedings of ICML 2003, pages 107?114,
Washington, DC.
Fillmore, Charles J. 1968. The case for case. In
Emmon Bach and Robert T. Harms,
222
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
editors, Universals in Linguistic Theory.
Holt, Rinehart, and Winston, New York,
pages 1?210.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3): 245?288.
Haghighi, Aria, Kristina Toutanova, and
Christopher Manning. 2005. A joint model
for semantic role labeling. In Proceedings of
the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 173?176, Ann Arbor, MI.
Jackendoff, Ray. 1990. Semantic Structures,
Current Studies in Linguistics Series. The
MIT Press, Cambridge, MA.
Joachims, Thorsten. 1999. Making large-scale
SVM learning practical. In B. Scho?lkopf,
C. Burges, and A. Smola, editors,
Advances in Kernel Methods?Support Vector
Learning. MIT Press, Cambridge, MA,
pages 169?184.
Kazama, Jun?ichi and Kentaro Torisawa.
2005. Speeding up training with tree
kernels for node relation labeling.
In Proceedings of EMNLP 2005,
pages 137?144, Toronto, Canada.
Kudo, Taku and Yuji Matsumoto. 2003. Fast
methods for kernel-based text analysis. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 24?31, Sapporo, Japan.
Levin, Beth. 1993. English Verb Classes
and Alternations. The University of
Chicago Press, Chicago, IL.
Lin, H.-T., C.-J. Lin, and R. C. Weng. 2003.
A note on Platt?s probabilistic outputs
for support vector machines. Technical
report, National Taiwan University.
Litkowski, Kenneth. 2004. Senseval-3 task:
Automatic labeling of semantic roles.
In Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9?12,
Barcelona, Spain.
Marcus, M. P., B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn
treebank. Computational Linguistics,
19:313?330.
Moschitti, Alessandro. 2004. A study
on convolution kernels for shallow
semantic parsing. In Proceedings of
the 42th Conference on Association for
Computational Linguistic (ACL-2004),
pages 335?342, Barcelona, Spain.
Moschitti, Alessandro. 2006a. Efficient
convolution kernels for dependency
and constituent syntactic trees.
In Proceedings of The 17th European
Conference on Machine Learning,
pages 318?329, Berlin, Germany.
Moschitti, Alessandro. 2006b. Making tree
kernels practical for natural language
learning. In Proceedings of 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL2006),
pages 113?120, Treato, Italy.
Moschitti, Alessandro, Bonaventura
Coppola, Daniele Pighin, and Roberto
Basili. 2005a. Engineering of syntactic
features for shallow semantic parsing.
In Proceedings of the ACL Workshop on
Feature Engineering for Machine Learning in
Natural Language Processing, pages 48?56,
Ann Arbor, MI.
Moschitti, Alessandro, Ana-Maria Giuglea,
Bonaventura Coppola, and Roberto
Basili. 2005b. Hierarchical semantic
role labeling. In Proceedings of the
Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 201?204, Ann Arbor, MI.
Moschitti, Alessandro, Daniele Pighin,
and Roberto Basili. 2006. Tree kernel
engineering in semantic role labeling
systems. In Proceedings of the Workshop on
Learning Structured Information in Natural
Language Applications, EACL 2006,
pages 49?56, Trento, Italy.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1): 71?106.
Platt, J. 1999. Probabilistic outputs
for support vector machines and
comparison to regularized likelihood
methods. In A. J. Smola, P. Bartlett,
B. Schoelkopf, and D. Schuurmans,
editors, Advances in Large Margin
Classifiers. MIT Press, Cambridge, MA,
pages 61?74.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Daniel Jurafsky. 2005a. Support
vector learning for semantic argument
classification.Machine Learning,
60(1?3):11?39.
Pradhan, Sameer, Kadri Hacioglu, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2005b. Semantic role chunking
combining complementary syntactic
views. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 217?220,
Ann Arbor, MI.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Daniel
Jurafsky. 2005c. Semantic role labeling
223
Computational Linguistics Volume 34, Number 2
using different syntactic views. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 581?588,
Ann Arbor, MI.
Pradhan, Sameer S., Wayne H. Ward,
Kadri Hacioglu, James H. Martin, and
Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines.
In HLT-NAACL 2004: Main Proceedings,
pages 233?240, Boston, MA.
Punyakanok, Vasin, Peter Koomen,
Dan Roth, and Wen-tau Yih. 2005.
Generalized inference with multiple
semantic role labeling systems. In
Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 181?184,
Ann Arbor, MI.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern
Analysis. Cambridge University Press,
Cambridge, UK.
Shen, Libin, Anoop Sarkar, and Aravind K.
Joshi. 2003. Using LTAG based features in
parse reranking. In Empirical Methods for
Natural Language Processing (EMNLP),
pages 89?96, Sapporo, Japan.
Thompson, Cynthia A., Roger Levy, and
Christopher Manning. 2003. A generative
model for semantic role labeling. In 14th
European Conference on Machine Learning,
pages 397?408, Cavtat, Croatia.
Tjong Kim Sang, Erik, Sander Canisius,
Antal van den Bosch, and Toine Bogers.
2005. Applying spelling error correction
techniques for improving semantic
role labelling. In Proceedings of the
Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 229?232, Ann Arbor, MI.
Toutanova, Kristina, Aria Haghighi, and
Christopher Manning. 2005. Joint learning
improves semantic role labeling. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 589?596,
Ann Arbor, MI.
Toutanova, Kristina, Penka Markova, and
Christopher Manning. 2004. The leaf path
projection view of parse trees: Exploring
string kernels for HPSG parse selection. In
Proceedings of EMNLP 2004, pages 166?173,
Barcelona, Spain.
Vapnik, Vladimir N. 1998. Statistical Learning
Theory. John Wiley and Sons, New York.
Vishwanathan, S. V. N. and A. J. Smola.
2002. Fast kernels on strings and trees.
In Proceedings of Neural Information
Processing Systems, pages 569?576,
Vancouver, British Columbia.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of EMNLP 2004,
pages 88?94, Barcelona, Spain.
Zelenko, D., C. Aone, and A. Richardella.
2003. Kernel methods for relation
extraction. Journal of Machine Learning
Research, 3:1083?1106.
Zhang, Min, Jie Zhang, and Jian Su. 2006.
Exploring syntactic features for relation
extraction using a convolution tree
kernel. In Proceedings of the Human
Language Technology Conference of the
NAACL, Main Conference, pages 288?295,
New York, NY.
224
Proceedings of NAACL HLT 2009: Short Papers, pages 85?88,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Shallow Semantic Parsing for Spoken Language Understanding
Bonaventura Coppola and Alessandro Moschitti and Giuseppe Riccardi
Department of Information Engineering and Computer Science - University of Trento, Italy
{coppola,moschitti,riccardi}@disi.unitn.it
Abstract
Most Spoken Dialog Systems are based on
speech grammars and frame/slot semantics.
The semantic descriptions of input utterances
are usually defined ad-hoc with no ability to
generalize beyond the target application do-
main or to learn from annotated corpora. The
approach we propose in this paper exploits
machine learning of frame semantics, bor-
rowing its theoretical model from computa-
tional linguistics. While traditional automatic
Semantic Role Labeling approaches on writ-
ten texts may not perform as well on spo-
ken dialogs, we show successful experiments
on such porting. Hence, we design and eval-
uate automatic FrameNet-based parsers both
for English written texts and for Italian dia-
log utterances. The results show that disflu-
encies of dialog data do not severely hurt per-
formance. Also, a small set of FrameNet-like
manual annotations is enough for realizing ac-
curate Semantic Role Labeling on the target
domains of typical Dialog Systems.
1 Introduction
Commercial services based on spoken dialog sys-
tems have consistently increased both in number and
in application scenarios (Gorin et al, 1997). De-
spite its success, current Spoken Language Under-
standing (SLU) technology is mainly based on sim-
ple conceptual annotation, where just very simple
semantic composition is attempted. In contrast, the
availability of richer semantic models as FrameNet
(Baker et al, 1998) is very appealing for the de-
sign of better dialog managers. The first step to en-
able the exploitation of frame semantics is to show
that accurate automatic semantic labelers can be de-
signed for processing conversational speech.
In this paper, we face the problem of perform-
ing shallow semantic analysis of speech transcrip-
tions from real-world dialogs. In particular, we ap-
ply Support Vector Machines (SVMs) and Kernel
Methods to the design of a semantic role labeler
(SRL) based on FrameNet. Exploiting Tree Kernels
(Collins and Duffy, 2002; Moschitti et al, 2008), we
can quickly port our system to different languages
and domains. In the experiments, we compare
results achieved on the English FrameNet against
those achieved on a smaller Italian FrameNet-like
corpus of spoken dialog transcriptions. They show
that the system is robust enough to disfluencies and
noise, and that it can be easily ported to new do-
mains and languages.
In the remainder of the paper, Section 2 presents
our basic Semantic Role Labeling approach, Sec-
tion 3 describes the experiments on the English
FrameNet and on our Italian dialog corpus, and Sec-
tion 4 draws the conclusions.
2 FrameNet-based Semantic Role Labeling
Semantic frames represent prototypical events or
situations which individually define their own set
of actors, or frame participants. For example,
the COMMERCE SCENARIO frame includes partic-
ipants as SELLER, BUYER, GOODS, and MONEY.
The task of FrameNet-based shallow semantic pars-
ing can be implemented as a combination of multi-
ple specialized semantic labelers as those in (Car-
reras and Ma`rquez, 2005), one for each frame.
Therefore, the general semantic parsing work-flow
includes 4 main steps: (i) Target Word Detec-
tion, where the semantically relevant words bringing
predicative information (the frame targets) are de-
tected, e.g. the verb to purchase for the above exam-
ple; (ii) Frame Disambiguation, where the correct
frame for every target word (which may be ambigu-
ous) is determined, e.g. COMMERCE SCENARIO;
(iii) Boundary Detection (BD), where the sequences
of words realizing the frame elements (or predicate
85
arguments) are detected; and (iv) Role Classification
(RC) (or argument classification), which assigns se-
mantic labels to the frame elements detected in the
previous step, e.g. GOODS. Therefore, we imple-
ment the full task of FrameNet-based parsing by a
combination of multiple specialized SRL-like label-
ers, one for each frame (Coppola et al, 2008). For
the design of each single labeler, we use the state-of-
the-art strategy developed in (Pradhan et al, 2005;
Moschitti et al, 2008).
2.1 Standard versus Structural Features
In machine learning tasks, the manual engineering
of effective features is a complex and time con-
suming process. For this reason, our SVM-based
SRL approach exploits the combination of two dif-
ferent models. We first used Polynomial Kernels
over handcrafted, linguistically-motivated, ?stan-
dard? SRL features (Gildea and Jurafsky, 2002;
Pradhan et al, 2005; Xue and Palmer, 2004).
Nonetheless, since we aim at modeling an SRL sys-
tem for a new language (Italian) and a new domain
(dialog transcriptions), the above features may re-
sult ineffective. Thus, to achieve independence on
the application domain, we exploited Tree Kernels
(Collins and Duffy, 2002) over automatic structural
features proposed in (Moschitti et al, 2005; Mos-
chitti et al, 2008). These are complementary to stan-
dard features and are obtained by applying Tree Ker-
nels (Collins and Duffy, 2002; Moschitti et al, 2008)
to basic tree structures expressing the syntactic rela-
tion between arguments and predicates.
3 Experiments
Our purpose is to show that an accurate automatic
FrameNet parser can be designed with reasonable
effort for Italian conversational speech. For this pur-
pose, we designed and evaluated both a semantic
parser for the English FrameNet (Section 3.1) and
one for a corpus of Italian spoken dialogs (Section
3.2). The accuracy of the latter and its comparison
against the former can provide evidence to sustain
out thesis or not.
3.1 Evaluation on the English FrameNet
In this experiment we trained and tested boundary
detectors (BD) and role classifiers (RC) as described
in Section 2. More in detail, (a) we trained 5 BDs
according to the syntactic categories of the possi-
ble target predicates, namely nouns, verbs, adjec-
tives, adverbs and prepositions; (b) we trained 782
one-versus-all multi-role classifiers RC, one for each
available frame and predicate syntactic category, for
a total of 5,345 binary classifiers; and (c) we ap-
plied the above models for recognizing predicate ar-
guments and their associated semantic labels in sen-
tences, where the frame label and the target predi-
cate were considered as given.
3.1.1 Data Set
We exploited the FrameNet 1.3 data base. After
preprocessing and parsing the sentences with Char-
niak?s parser, we obtained 135,293 semantically-
annotated and syntactically-parsed sentences.
The above dataset was partitioned into three sub-
sets: 2% of data (2,782 sentences) for training the
BDs, 90% (121,798 sentences) for training RC, and
1% (1,345 sentences) as test set. The remaining data
were discarded. Accordingly, the number of pos-
itive and negative training examples for BD were:
2,764 positive and 37,497 negative examples for ver-
bal, 1,189 and 35,576 for nominal, 615 and 14,544
for adjectival, 0 and 40 for adverbial, and 7 and 177
for prepositional predicates (for a total of 4,575 and
87,834). For RC, the total numbers were 207,662
and 1,960,423, which divided by the number of role
types show the average number of 39 positive versus
367 negative examples per role label.
3.1.2 Results
We tested several kernels over standard fea-
tures (Gildea and Jurafsky, 2002; Pradhan et al,
2005) and structured features (Moschitti et al,
2008): the Polynomial Kernel (PK, with a degree of
3), the Tree Kernel (TK) and its combination with
the bag of word kernel on the tree leaves (TKL).
Also, the combinations PK+TK and PK+TKL were
tested.
The 4 rows of Table 1 report the performance of
different classification tasks. They show in turn: (1)
the ?pure? performance of the BD classifiers, i.e.
considering correct the classification decisions also
when a correctly classified tree node does not ex-
actly correspond to its argument?s word boundaries.
Such mismatch frequently happens when the parse
tree (which is automatically generated) contains in-
86
PK TK PK+TK TKL PK+TKL
Eval sett. P R F1 P R F1 P R F1 P R F1 P R F1
BD .887 .675 .767 .949 .652 .773 .915 .698 .792 .938 .659 .774 .908 .701 .791
BD pj .850 .647 .735 .919 .631 .748 .875 .668 .758 .906 .636 .747 .868 .670 .757
BD+RC .654 .498 .565 .697 .479 .568 .680 .519 .588 .689 .484 .569 .675 .521 .588
BD+RC pj .625 .476 .540 .672 .462 .548 .648 .495 .561 .663 .466 .547 .644 .497 .561
Table 1: Results on FrameNet dataset: Polynomial Kernel, two different Tree Kernels, and their combinations (see
Section 3.1.2) with 2% training for BD and 90% for RC.
correct node attachments; (2) the real performance
of the BD classification when actually ?projected?
(?pj?) on the tree leaves, i.e. when matching not
only the constituent node as in 1, but also exactly
matching the selected words (leaves) with those in
the FrameNet gold standard. This also implies the
exact automatic syntactic analysis for the subtree;
(3) the same as in (1), with the argument role classi-
fication (RC) also performed (frame element labels
must also match); (4) the same as in (2), with RC
also performed. For each classification task, the Pre-
cision, Recall and F1 measure achieved by means
of different kernel combinations are shown in the
columns of the table. Only for the best configuration
in Table 1 (PK+TK, results in bold) the amount of
training data for the BD model was increased from
2% to 90%, resulting in a popular splitting for this
task(Erk and Pado, 2006). Results are shown in Ta-
ble 2: the PK+TK kernel achieves 1.0 Precision,
0.732 Recall, and 0.847 F1. These figures can be
compared to 0.855 Precision, 0.669 Recall and 0.751
F1 of the system described in (Erk and Pado, 2006)
and trained over the same amount of data. In con-
clusion, our best learning scheme is currently capa-
ble of tagging FrameNet data with exact boundaries
and role labels at 63% F1. Our next steps will be (1)
further improving the RC models using FrameNet-
specific information (such as Frame and role inheri-
tance), and (2) introducing an effective Frame clas-
sifier to automatically choose Frame labels.
Enhanced PK+TK
Eval Setting P R F1
BD (nodes) 1.0 .732 .847
BD (words) .963 .702 .813
BD+RC (nodes) .784 .571 .661
BD+RC (words) .747 .545 .630
Table 2: Results on the FrameNet dataset. Best configu-
ration from Table 1, raised to 90% of training data for BD
and RC.
Eval Setting P R F1 P R F1
PK
BD - - - .900 .869 .884
BD+RC - - - .769 .742 .756
TK PK+TK
BD .887 .856 .871 .905 .873 .889
BD+RC .765 .738 .751 .774 .747 .760
Table 3: Experiment Results on the Italian dialog corpus
for different learning schemes and kernel combinations.
3.2 Evaluation on Italian Spoken Dialogs
In this section, we present the results of BD and RC
of our FrameNet parser on the smaller Italian spoken
dialog corpus. We assume here as well that the target
word (i.e. the predicate for which arguments have to
be extracted) along with the correct frame are given.
3.2.1 Data Set
The Italian dialog corpus includes 50 real human-
human dialogs recorded and manually transcribed at
the call center of the help-desk facility of an Ital-
ian Consortium for Information Systems. The di-
alogs are fluent and spontaneous conversations be-
tween a caller and an operator, concerning hard-
ware and software problems. The dialog turns con-
tain 1,677 annotated frame instances spanning 154
FrameNet frames and 20 new ad hoc frames spe-
cific for the domain. New frames mostly con-
cern data processing such as NAVIGATION, DIS-
PLAY DATA, LOSE DATA, CREATE DATA. Being
intended as a reference resource, this dataset in-
cludes partially human-validated syntactic analysis,
i.e. lower branches corrected to fit arguments. We
divided such dataset into 90% training (1,521 frame
instances) and 10% testing (156 frame instances).
Each frame instance brings its own set of frame par-
ticipant (or predicate argument) instances.
For BD, the very same approach as in Section 3.1
was followed. For RC, we also followed the same
approach but, in order to cope with data sparse-
87
ness, we also attempted a different RC strategy by
merging data related to different syntactic predicates
within the same frame. So, within each frame, we
merged data related to verbal predicates, nominal
predicates, and so on. Due to the short space avail-
able, we will just report results for this latter ap-
proach, which performed sensitively better.
3.2.2 Results
The results are reported in Table 3. Each ta-
ble block shows Precision, Recall and F1 for ei-
ther PK, TK, or PK+TK. The rows marked as BD
show the results for the task of marking the exact
constituent boundaries of every frame element (ar-
gument) found. The rows marked as BD+RC show
the results for the two-stage pipeline of bothmarking
the exact constituent boundaries and also assigning
the correct semantic label. A few observations hold.
First, the highest F1 has been achieved using the
PK+TK combination. On this concern, we under-
line that kernel combinations always gave the best
performance in any experiment we run.
Second, we emphasize that the F1 of PK is sur-
prisingly high, since it exploits the set of standard
SRL feature (Gildea and Jurafsky, 2002; Pradhan
et al, 2005), originally developed for English and
left unmodified for Italian. Nonetheless, their per-
formance is comparable to the Tree Kernels and,
as we said, their combination improves the result.
Concerning the structured features exploited by Tree
Kernels, we note that they work as well without any
tuning when ported to Italian dialogs.
Finally, the achieved F1 is extremely good. In
fact, our corresponding result on the FrameNet cor-
pus (Table 2) is P=0.784, R=0.571, F1=0.661,
where the corpus contains much more data, its sen-
tences come from a standard written text (no dis-
fluencies are present) and it is in English language,
which is morphologically simpler than Italian. On
the other hand, the Italian corpus includes optimal
syntactic annotation which exactly fits the frame se-
mantics, and the number of frames is lower than in
the FrameNet experiment.
4 Conclusions
The good performance achieved for Italian dialogs
shows that FrameNet-based parsing is viable for la-
beling conversational speech in any language us-
ing a few training data. Moreover, the approach
works well for very specific domains, like help-
desk/customer conversations. Nonetheless, addi-
tional tests based on fully automatic transcription
and syntactic parsing are needed. However, our cur-
rent results show that future research on complex
spoken dialog systems is enabled to exploit automat-
ically generated frame semantics, which is our very
direction.
Acknowledgments
The authors wish to thank Daniele Pighin for the SRL subsys-
tem and Sara Tonelli for the Italian corpus. This work has been
partially funded by the European Commission - LUNA project
(contract n.33549), and by the Marie Curie Excellence Grant
for the ADAMACH project (contract n.022593).
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL ?98, pages 86?90.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2005.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete structures, and the voted perceptron. In ACL02.
Bonaventura Coppola, Alessandro Moschitti, and
Daniele Pighin. 2008. Generalized framework for
syntax-based relation mining. In IEEE-ICDM 2008.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser - a
flexible toolbox for semantic role assignment. In Pro-
ceedings of LREC 2006, Genoa, Italy.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic La-
beling of Semantic Roles. Computational Linguistics.
A. L. Gorin, G. Riccardi, and J. H. Wright. 1997. How
may i help you? Speech Communication.
Alessandro Moschitti, Bonaventura Coppola, Daniele
Pighin, and Roberto Basili. 2005. Engineering of syn-
tactic features for shallow semantic parsing. In ACL
WS on Feature Engineering for ML in NLP.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. Machine Learning.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
EMNLP 2004.
88
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776?783,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Exploiting Syntactic and Shallow Semantic Kernels
for Question/Answer Classification
Alessandro Moschitti
University of Trento
38050 Povo di Trento
Italy
moschitti@dit.unitn.it
Silvia Quarteroni
The University of York
York YO10 5DD
United Kingdom
silvia@cs.york.ac.uk
Roberto Basili
?Tor Vergata? University
Via del Politecnico 1
00133 Rome, Italy
basili@info.uniroma2.it
Suresh Manandhar
The University of York
York YO10 5DD
United Kingdom
suresh@cs.york.ac.uk
Abstract
We study the impact of syntactic and shallow
semantic information in automatic classifi-
cation of questions and answers and answer
re-ranking. We define (a) new tree struc-
tures based on shallow semantics encoded
in Predicate Argument Structures (PASs)
and (b) new kernel functions to exploit the
representational power of such structures
with Support Vector Machines. Our ex-
periments suggest that syntactic information
helps tasks such as question/answer classifi-
cation and that shallow semantics gives re-
markable contribution when a reliable set of
PASs can be extracted, e.g. from answers.
1 Introduction
Question answering (QA) is as a form of informa-
tion retrieval where one or more answers are re-
turned to a question in natural language in the form
of sentences or phrases. The typical QA system ar-
chitecture consists of three phases: question pro-
cessing, document retrieval and answer extraction
(Kwok et al, 2001).
Question processing is often centered on question
classification, which selects one of k expected an-
swer classes. Most accurate models apply super-
vised machine learning techniques, e.g. SNoW (Li
and Roth, 2005), where questions are encoded us-
ing various lexical, syntactic and semantic features.
The retrieval and answer extraction phases consist in
retrieving relevant documents (Collins-Thompson et
al., 2004) and selecting candidate answer passages
from them. A further answer re-ranking phase is op-
tionally applied. Here, too, the syntactic structure
of a sentence appears to provide more useful infor-
mation than a bag of words (Chen et al, 2006), al-
though the correct way to exploit it is still an open
problem.
An effective way to integrate syntactic structures
in machine learning algorithms is the use of tree ker-
nel (TK) functions (Collins and Duffy, 2002), which
have been successfully applied to question classifi-
cation (Zhang and Lee, 2003; Moschitti, 2006) and
other tasks, e.g. relation extraction (Zelenko et al,
2003; Moschitti, 2006). In more complex tasks such
as computing the relatedness between questions and
answers in answer re-ranking, to our knowledge no
study uses kernel functions to encode syntactic in-
formation. Moreover, the study of shallow semantic
information such as predicate argument structures
annotated in the PropBank (PB) project (Kingsbury
and Palmer, 2002) (www.cis.upenn.edu/?ace) is a
promising research direction. We argue that seman-
tic structures can be used to characterize the relation
between a question and a candidate answer.
In this paper, we extensively study new structural
representations, encoding parse trees, bag-of-words,
POS tags and predicate argument structures (PASs)
for question classification and answer re-ranking.
We define new tree representations for both simple
and nested PASs, i.e. PASs whose arguments are
other predicates (Section 2). Moreover, we define
new kernel functions to exploit PASs, which we au-
tomatically derive with our SRL system (Moschitti
et al, 2005) (Section 3).
Our experiments using SVMs and the above ker-
776
nels and data (Section 4) shows the following: (a)
our approach reaches state-of-the-art accuracy on
question classification. (b) PB predicative structures
are not effective for question classification but show
promising results for answer classification on a cor-
pus of answers to TREC-QA 2001 description ques-
tions. We created such dataset by using YourQA
(Quarteroni and Manandhar, 2006), our basic Web-
based QA system1. (c) The answer classifier in-
creases the ranking accuracy of our QA system by
about 25%.
Our results show that PAS and syntactic parsing
are promising methods to address tasks affected by
data sparseness like question/answer categorization.
2 Encoding Shallow Semantic Structures
Traditionally, information retrieval techniques are
based on the bag-of-words (BOW) approach aug-
mented by language modeling (Allan et al, 2002).
When the task requires the use of more complex se-
mantics, the above approaches are often inadequate
to perform fine-level textual analysis.
An improvement on BOW is given by the use of
syntactic parse trees, e.g. for question classification
(Zhang and Lee, 2003), but these, too are inadequate
when dealing with definitional answers expressed by
long and articulated sentences or even paragraphs.
On the contrary, shallow semantic representations,
bearing a more ?compact? information, could pre-
vent the sparseness of deep structural approaches
and the weakness of BOW models.
Initiatives such as PropBank (PB) (Kingsbury
and Palmer, 2002) have made possible the design of
accurate automatic Semantic Role Labeling (SRL)
systems (Carreras and Ma`rquez, 2005). Attempting
an application of SRL to QA hence seems natural,
as pinpointing the answer to a question relies on a
deep understanding of the semantics of both.
Let us consider the PB annotation: [ARG1
Antigens] were [AM?TMP originally] [rel
defined] [ARG2 as non-self molecules].
Such annotation can be used to design a shallow
semantic representation that can be matched against
other semantically similar sentences, e.g. [ARG0
Researchers] [rel describe] [ARG1 antigens]
[ARG2 as foreign molecules] [ARGM?LOC in
1Demo at: http://cs.york.ac.uk/aig/aqua.
PAS
rel
define
ARG1
antigens
ARG2
molecules
ARGM-TMP
originally
PAS
rel
describe
ARG0
researchers
ARG1
antigens
ARG2
molecules
ARGM-LOC
body
Figure 1: Compact predicate argument structures of
two different sentences.
the body].
For this purpose, we can represent the above anno-
tated sentences using the tree structures described in
Figure 1. In this compact representation, hereafter
Predicate-Argument Structures (PAS), arguments
are replaced with their most important word ? often
referred to as the semantic head. This reduces
data sparseness with respect to a typical BOW
representation.
However, sentences rarely contain a single pred-
icate; it happens more generally that propositions
contain one or more subordinate clauses. For
instance let us consider a slight modification of the
first sentence: ?Antigens were originally defined
as non-self molecules which bound specifically to
antibodies2 .? Here, the main predicate is ?defined?,
followed by a subordinate predicate ?bound?. Our
SRL system outputs the following two annotations:
(1) [ARG1 Antigens] were [ARGM?TMP
originally] [rel defined] [ARG2 as non-self
molecules which bound specifically to
antibodies].
(2) Antigens were originally defined as
[ARG1 non-self molecules] [R?A1 which] [rel
bound] [ARGM?MNR specifically] [ARG2 to
antibodies].
giving the PASs in Figure 2.(a) resp. 2.(b).
As visible in Figure 2.(a), when an argument node
corresponds to an entire subordinate clause, we label
its leaf with PAS, e.g. the leaf of ARG2. Such PAS
node is actually the root of the subordinate clause
in Figure 2.(b). Taken as standalone, such PASs do
not express the whole meaning of the sentence; it
is more accurate to define a single structure encod-
ing the dependency between the two predicates as in
2This is an actual answer to ?What are antibodies?? from
our question answering system, YourQA.
777
PAS
rel
define
ARG1
antigens
ARG2
PAS
AM-TMP
originally
(a)
PAS
rel
bound
ARG1
molecules
R-ARG1
which
AM-ADV
specifically
ARG2
antibodies
(b)
PAS
rel
define
ARG1
antigens
ARG2
PAS
rel
bound
ARG1
molecules
R-ARG1
which
AM-ADV
specifically
ARG2
antibodies
AM-TMP
originally
(c)
Figure 2: Two PASs composing a PASN
Figure 2.(c). We refer to nested PASs as PASNs.
It is worth to note that semantically equivalent
sentences syntactically expressed in different ways
share the same PB arguments and the same PASs,
whereas semantically different sentences result in
different PASs. For example, the sentence: ?Anti-
gens were originally defined as antibodies which
bound specifically to non-self molecules?, uses the
same words as (2) but has different meaning. Its PB
annotation:
(3) Antigens were originally defined
as [ARG1 antibodies] [R?A1 which] [rel
bound] [ARGM?MNR specifically] [ARG2 to
non-self molecules],
clearly differs from (2), as ARG2 is now non-
self molecules; consequently, the PASs are also
different.
Once we have assumed that parse trees and PASs
can improve on the simple BOW representation, we
face the problem of representing tree structures in
learning machines. Section 3 introduces a viable ap-
proach based on tree kernels.
3 Syntactic and Semantic Kernels for Text
As mentioned above, encoding syntactic/semantic
information represented by means of tree structures
in the learning algorithm is problematic. A first so-
lution is to use all its possible substructures as fea-
tures. Given the combinatorial explosion of consid-
ering subparts, the resulting feature space is usually
very large. A tree kernel (TK) function which com-
putes the number of common subtrees between two
syntactic parse trees has been given in (Collins and
Duffy, 2002). Unfortunately, such subtrees are sub-
ject to the constraint that their nodes are taken with
all or none of the children they have in the original
tree. This makes the TK function not well suited for
the PAS trees defined above. For instance, although
the two PASs of Figure 1 share most of the subtrees
rooted in the PAS node, Collins and Duffy?s kernel
would compute no match.
In the next section we describe a new kernel de-
rived from the above tree kernel, able to evaluate the
meaningful substructures for PAS trees. Moreover,
as a single PAS may not be sufficient for text rep-
resentation, we propose a new kernel that combines
the contributions of different PASs.
3.1 Tree kernels
Given two trees T1 and T2, let {f1, f2, ..} = F be
the set of substructures (fragments) and Ii(n) be
equal to 1 if fi is rooted at node n, 0 otherwise.
Collins and Duffy?s kernel is defined as
TK(T1, T2) =
?
n1?NT1
?
n2?NT2 ?(n1, n2), (1)
where NT1 and NT2 are the sets of nodes
in T1 and T2, respectively and ?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2). The latter is equal to the number
of common fragments rooted in nodes n1 and n2. ?
can be computed as follows:
(1) if the productions (i.e. the nodes with their
direct children) at n1 and n2 are different then
?(n1, n2) = 0;
(2) if the productions at n1 and n2 are the same, and
n1 and n2 only have leaf children (i.e. they are pre-
terminal symbols) then ?(n1, n2) = 1;
(3) if the productions at n1 and n2 are the same, and
n1 and n2 are not pre-terminals then ?(n1, n2) =
?nc(n1)
j=1 (1+?(cjn1 , cjn2)), where nc(n1) is the num-
ber of children of n1 and cjn is the j-th child of n.
Such tree kernel can be normalized and a ? factor
can be added to reduce the weight of large structures
(refer to (Collins and Duffy, 2002) for a complete
description). The critical aspect of steps (1), (2) and
(3) is that the productions of two evaluated nodes
have to be identical to allow the match of further de-
scendants. This means that common substructures
cannot be composed by a node with only some of its
778
PAS
SLOT
rel
define
SLOT
ARG1
antigens
*
SLOT
ARG2
PAS
*
SLOT
ARGM-TMP
originally
*
(a)
PAS
SLOT
rel
define
SLOT
ARG1
antigens
*
SLOT
null
SLOT
null
(b)
PAS
SLOT
rel
define
SLOT
null
SLOT
ARG2
PAS
*
SLOT
null
(c)
Figure 3: A PAS with some of its fragments.
children as an effective PAS representation would
require. We solve this problem by designing the
Shallow Semantic Tree Kernel (SSTK) which allows
to match portions of a PAS.
3.2 The Shallow Semantic Tree Kernel (SSTK)
The SSTK is based on two ideas: first, we change
the PAS, as shown in Figure 3.(a) by adding SLOT
nodes. These accommodate argument labels in a
specific order, i.e. we provide a fixed number of
slots, possibly filled with null arguments, that en-
code all possible predicate arguments. For simplic-
ity, the figure shows a structure of just 4 arguments,
but more can be added to accommodate the max-
imum number of arguments a predicate can have.
Leaf nodes are filled with the wildcard character *
but they may alternatively accommodate additional
information.
The slot nodes are used in such a way that the
adopted TK function can generate fragments con-
taining one or more children like for example those
shown in frames (b) and (c) of Figure 3. As pre-
viously pointed out, if the arguments were directly
attached to the root node, the kernel function would
only generate the structure with all children (or the
structure with no children, i.e. empty).
Second, as the original tree kernel would generate
many matches with slots filled with the null label,
we have set a new step 0:
(0) if n1 (or n2) is a pre-terminal node and its child
label is null, ?(n1, n2) = 0;
and subtract one unit to ?(n1, n2), in step 3:
(3) ?(n1, n2) =
?nc(n1)
j=1 (1 + ?(cjn1 , cjn2))? 1,
The above changes generate a new ? which,
when substituted (in place of the original ?) in Eq.
1, gives the new Shallow Semantic Tree Kernel. To
show that SSTK is effective in counting the number
of relations shared by two PASs, we propose the fol-
lowing:
Proposition 1 The new ? function applied to the
modified PAS counts the number of all possible k-
ary relations derivable from a set of k arguments,
i.e.
?k
i=1
(k
i
)
relations of arity from 1 to k (the pred-
icate being considered as a special argument).
Proof We observe that a kernel applied to a tree and
itself computes all its substructures, thus if we eval-
uate SSTK between a PAS and itself we must obtain
the number of generated k-ary relations. We prove
by induction the above claim.
For the base case (k = 0): we use a PAS with no
arguments, i.e. all its slots are filled with null la-
bels. Let r be the PAS root; since r is not a pre-
terminal, step 3 is selected and ? is recursively ap-
plied to all r?s children, i.e. the slot nodes. For the
latter, step 0 assigns ?(cjr, cjr) = 0. As a result,
?(r, r) = ?nc(r)j=1 (1 + 0)? 1 = 0 and the base case
holds.
For the general case, r is the root of a PAS with k+1
arguments. ?(r, r) = ?nc(r)j=1 (1 + ?(cjr, cjr)) ? 1
=
?k
j=1(1+?(cjr , cjr))?(1+?(ck+1r , ck+1r ))?1. For
k arguments, we assume by induction that?kj=1(1+
?(cjr, cjr))? 1 =
?k
i=1
(k
i
)
, i.e. the number of k-ary
relations. Moreover, (1 + ?(ck+1r , ck+1r )) = 2, thus
?(r, r) = ?ki=1
(k
i
)
? 2 = 2k ? 2 = 2k+1 = ?k+1i=1
(k+1
i
)
, i.e. all the relations until arity k + 1 2
TK functions can be applied to sentence parse
trees, therefore their usefulness for text processing
applications, e.g. question classification, is evident.
On the contrary, the SSTK applied to one PAS ex-
tracted from a text fragment may not be meaningful
since its representation needs to take into account all
the PASs that it contains. We address such problem
779
by defining a kernel on multiple PASs.
Let Pt and Pt? be the sets of PASs extracted from
the text fragment t and t?. We define:
Kall(Pt, Pt?) =
?
p?Pt
?
p??Pt?
SSTK(p, p?), (2)
While during the experiments (Sect. 4) the Kall
kernel is used to handle predicate argument struc-
tures, TK (Eq. 1) is used to process parse trees and
the linear kernel to handle POS and BOW features.
4 Experiments
The purpose of our experiments is to study the im-
pact of the new representations introduced earlier for
QA tasks. In particular, we focus on question clas-
sification and answer re-ranking for Web-based QA
systems.
In the question classification task, we extend pre-
vious studies, e.g. (Zhang and Lee, 2003; Moschitti,
2006), by testing a set of previously designed ker-
nels and their combination with our new Shallow Se-
mantic Tree Kernel. In the answer re-ranking task,
we approach the problem of detecting description
answers, among the most complex in the literature
(Cui et al, 2005; Kazawa et al, 2001).
The representations that we adopt are: bag-of-
words (BOW), bag-of-POS tags (POS), parse tree
(PT), predicate argument structure (PAS) and nested
PAS (PASN). BOW and POS are processed by
means of a linear kernel, PT is processed with TK,
PAS and PASN are processed by SSTK. We imple-
mented the proposed kernels in the SVM-light-TK
software available at ai-nlp.info.uniroma2.it/
moschitti/ which encodes tree kernel functions in
SVM-light (Joachims, 1999).
4.1 Question classification
As a first experiment, we focus on question classi-
fication, for which benchmarks and baseline results
are available (Zhang and Lee, 2003; Li and Roth,
2005). We design a question multi-classifier by
combining n binary SVMs3 according to the ONE-
vs-ALL scheme, where the final output class is the
one associated with the most probable prediction.
The PASs were automatically derived by our SRL
3We adopted the default regularization parameter (i.e., the
average of 1/||~x||) and tried a few cost-factor values to adjust
the rate between Precision and Recall on the development set.
system which achieves a 76% F1-measure (Mos-
chitti et al, 2005).
As benchmark data, we use the question train-
ing and test set available at: l2r.cs.uiuc.edu/
?cogcomp/Data/QA/QC/, where the test set are the
500 TREC 2001 test questions (Voorhees, 2001).
We refer to this split as UIUC. The performance of
the multi-classifier and the individual binary classi-
fiers is measured with accuracy resp. F1-measure.
To collect statistically significant information, we
run 10-fold cross validation on the 6,000 questions.
Features Accuracy (UIUC) Accuracy (c.v.)
PT 90.4 84.8?1.2
BOW 90.6 84.7?1.2
PAS 34.2 43.0?1.9
POS 26.4 32.4?2.1
PT+BOW 91.8 86.1?1.1
PT+BOW+POS 91.8 84.7?1.5
PAS+BOW 90.0 82.1?1.3
PAS+BOW+POS 88.8 81.0?1.5
Table 1: Accuracy of the question classifier with dif-
ferent feature combinations
Question classification results Table 1 shows the
accuracy of different question representations on the
UIUC split (Column 1) and the average accuracy ?
the corresponding confidence limit (at 90% signifi-
cance) on the cross validation splits (Column 2).(i)
The TK on PT and the linear kernel on BOW pro-
duce a very high result, i.e. about 90.5%. This is
higher than the best outcome derived in (Zhang and
Lee, 2003), i.e. 90%, obtained with a kernel combin-
ing BOW and PT on the same data. Combined with
PT, BOW reaches 91.8%, very close to the 92.5%
accuracy reached in (Li and Roth, 2005) using com-
plex semantic information from external resources.
(ii) The PAS feature provides no improvement. This
is mainly because at least half of the training and
test questions only contain the predicate ?to be?, for
which a PAS cannot be derived by a PB-based shal-
low semantic parser.
(iii) The 10-fold cross-validation experiments con-
firm the trends observed in the UIUC split. The
best model (according to statistical significance) is
PT+BOW, achieving an 86.1% average accuracy4.
4This value is lower than the UIUC split one as the UIUC
test set is not consistent with the training set (it contains the
780
4.2 Answer classification
Question classification does not allow to fully ex-
ploit the PAS potential since questions tend to be
short and with few verbal predicates (i.e. the only
ones that our SRL system can extract). A differ-
ent scenario is answer classification, i.e. deciding
if a passage/sentence correctly answers a question.
Here, the semantics to be generated by the classi-
fier are not constrained to a small taxonomy and an-
swer length may make the PT-based representation
too sparse.
We learn answer classification with a binary SVM
which determines if an answer is correct for the tar-
get question: here, the classification instances are
?question, answer? pairs. Each pair component can
be encoded with PT, BOW, PAS and PASN repre-
sentations (processed by previous kernels).
As test data, we collected the 138 TREC 2001 test
questions labeled as ?description? and for each, we
obtained a list of answer paragraphs extracted from
Web documents using YourQA. Each paragraph sen-
tence was manually evaluated based on whether it
contained an answer to the corresponding question.
Moreover, to simplify the classification problem, we
isolated for each paragraph the sentence which ob-
tained the maximal judgment (in case more than one
sentence in the paragraph had the same judgment,
we chose the first one). We collected a corpus con-
taining 1309 sentences, 416 of which ? labeled ?+1?
? answered the question either concisely or with
noise; the rest ? labeled ?-1?? were either irrele-
vant to the question or contained hints relating to the
question but could not be judged as valid answers5.
Answer classification results To test the impact
of our models on answer classification, we ran 5-fold
cross-validation, with the constraint that two pairs
?q, a1? and ?q, a2? associated with the same ques-
tion q could not be split between training and test-
ing. Hence, each reported value is the average over 5
different outcomes. The standard deviations ranged
TREC 2001 questions) and includes a larger percentage of eas-
ily classified question types, e.g. the numeric (22.6%) and de-
scription classes (27.6%) whose percentage in training is 16.4%
resp. 16.2%.
5For instance, given the question ?What are invertebrates??,
the sentence ?At least 99% of all animal species are inverte-
brates, comprising . . . ? was labeled ?-1? , while ?Invertebrates
are animals without backbones.? was labeled ?+1?.
  
  
   
   
    
    
   
   
  
  
      	  	                 

         







Proceedings of ACL-08: HLT, pages 798?806,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semantic Role Labeling Systems for Arabic using Kernel Methods
Mona Diab
CCLS, Columbia University
New York, NY 10115, USA
mdiab@ccls.columbia.edu
Alessandro Moschitti
DISI, University of Trento
Trento, I-38100, Italy
moschitti@disi.unitn.it
Daniele Pighin
FBK-irst; DISI, University of Trento
Trento, I-38100, Italy
pighin@fbk.eu
Abstract
There is a widely held belief in the natural lan-
guage and computational linguistics commu-
nities that Semantic Role Labeling (SRL) is
a significant step toward improving important
applications, e.g. question answering and in-
formation extraction. In this paper, we present
an SRL system for Modern Standard Arabic
that exploits many aspects of the rich mor-
phological features of the language. The ex-
periments on the pilot Arabic Propbank data
show that our system based on Support Vector
Machines and Kernel Methods yields a global
SRL F1 score of 82.17%, which improves the
current state-of-the-art in Arabic SRL.
1 Introduction
Shallow approaches to semantic processing are mak-
ing large strides in the direction of efficiently and
effectively deriving tacit semantic information from
text. Semantic Role Labeling (SRL) is one such ap-
proach. With the advent of faster and more power-
ful computers, more effective machine learning al-
gorithms, and importantly, large data resources an-
notated with relevant levels of semantic information,
such as the FrameNet (Baker et al, 1998) and Prob-
Bank (Kingsbury and Palmer, 2003), we are seeing
a surge in efficient approaches to SRL (Carreras and
Ma`rquez, 2005).
SRL is the process by which predicates and their
arguments are identified and their roles are defined
in a sentence. For example, in the English sen-
tence, ?John likes apples.?, the predicate is ?likes?
whereas ?John? and ?apples?, bear the semantic role
labels agent (ARG0) and theme (ARG1). The cru-
cial fact about semantic roles is that regardless of
the overt syntactic structure variation, the underly-
ing predicates remain the same. Hence, for the sen-
tence ?John opened the door? and ?the door opened?,
though ?the door? is the object of the first sentence
and the subject of the second, it is the ?theme? in
both sentences. Same idea applies to passive con-
structions, for example.
There is a widely held belief in the NLP and com-
putational linguistics communities that identifying
and defining roles of predicate arguments in a sen-
tence has a lot of potential for and is a significant
step toward improving important applications such
as document retrieval, machine translation, question
answering and information extraction (Moschitti et
al., 2007).
To date, most of the reported SRL systems are for
English, and most of the data resources exist for En-
glish. We do see some headway for other languages
such as German and Chinese (Erk and Pado, 2006;
Sun and Jurafsky, 2004). The systems for the other
languages follow the successful models devised for
English, e.g. (Gildea and Jurafsky, 2002; Gildea and
Palmer, 2002; Chen and Rambow, 2003; Thompson
et al, 2003; Pradhan et al, 2003; Moschitti, 2004;
Xue and Palmer, 2004; Haghighi et al, 2005). In the
same spirit and facilitated by the release of the Se-
mEval 2007 Task 18 data1, based on the Pilot Arabic
Propbank, a preliminary SRL system exists for Ara-
bic2 (Diab and Moschitti, 2007; Diab et al, 2007a).
However, it did not exploit some special character-
istics of the Arabic language on the SRL task.
In this paper, we present an SRL system for MSA
that exploits many aspects of the rich morphological
features of the language. It is based on a supervised
model that uses support vector machines (SVM)
technology (Vapnik, 1998) for argument boundary
detection and argument classification. It is trained
and tested using the pilot Arabic Propbank data re-
leased as part of the SemEval 2007 data. Given the
lack of a reliable Arabic deep syntactic parser, we
1http://nlp.cs.swarthmore.edu/semeval/
2We use Arabic to refer to Modern Standard Arabic (MSA).
798
use gold standard trees from the Arabic Tree Bank
(ATB) (Maamouri et al, 2004).
This paper is laid out as follows: Section 2
presents facts about the Arabic language especially
in relevant contrast to English; Section 3 presents
the approach and system adopted for this work; Sec-
tion 4 presents the experimental setup, results and
discussion. Finally, Section 5 draws our conclu-
sions.
2 Arabic Language and Impact on SRL
Arabic is a very different language from English in
several respects relevant to the SRL task. Arabic is a
semitic language. It is known for its templatic mor-
phology where words are made up of roots and af-
fixes. Clitics agglutinate to words. Clitics include
prepositions, conjunctions, and pronouns.
In contrast to English, Arabic exhibits rich mor-
phology. Similar to English, Arabic verbs explic-
itly encode tense, voice, Number, and Person fea-
tures. Additionally, Arabic encodes verbs with Gen-
der, Mood (subjunctive, indicative and jussive) in-
formation. For nominals (nouns, adjectives, proper
names), Arabic encodes syntactic Case (accusative,
genitive and nominative), Number, Gender and Def-
initeness features. In general, many of the morpho-
logical features of the language are expressed via
short vowels also known as diacritics3 .
Unlike English, syntactically Arabic is a pro-drop
language, where the subject of a verb may be im-
plicitly encoded in the verb morphology. Hence, we
observe sentences such as ?A?KQ. ? @ ?? @ Akl AlbrtqAl
?ate-[he] the-oranges?, where the verb Akl encodes
the third Person Masculine Singular subject in the
verbal morphology. It is worth noting that in the
ATB 35% of all sentences are pro-dropped for sub-
ject (Maamouri et al, 2006). Unless the syntactic
parse is very accurate in identifying the pro-dropped
case, identifying the syntactic subject and the under-
lying semantic arguments are a challenge for such
pro-drop cases.
Arabic syntax exhibits relative free word order.
Arabic allows for both subject-verb-object (SVO)
and verb-subject-object (VSO) argument orders.4 In
3Diacritics encode the vocalic structure, namely the short
vowels, as well as the gemmination marker for consonantal dou-
bling, among other markers.
4MSA less often allows for OSV, or OVS.
the VSO constructions, the verb agrees with the syn-
tactic subject in Gender only, while in the SVO con-
structions, the verb agrees with the subject in both
Number and Gender. Even though, in the ATB, an
equal distribution of both VSO and SVO is observed
(each appearing 30% of the time), it is known that
in general Arabic is predominantly in VSO order.
Moreover, the pro-drop cases could effectively be
perceived as VSO orders for the purposes of SRL.
Syntactic Case is very important in the cases of VSO
and pro-drop constructions as they indicate the syn-
tactic roles of the object arguments with accusative
Case. Unless the morphology of syntactic Case is
explicitly present, such free word order could run
the SRL system into significant confusion for many
of the predicates where both arguments are semanti-
cally of the same type.
Arabic exhibits more complex noun phrases than
English mainly to express possession. These con-
structions are known as idafa constructions. Mod-
ern standard Arabic does not have a special parti-
cle expressing possession. In these complex struc-
tures a surface indefinite noun (missing an explicit
definite article) may be followed by a definite noun
marked with genitive Case, rendering the first noun
syntactically definite. For example, I
J. ? @ ?g. P rjl
Albyt ?man the-house? meaning ?man of the house?,
?g. P becomes definite. An adjective modifying the
noun ?g. P will have to agree with it in Number,
Gender, Definiteness, and Case. However, with-
out explicit morphological encoding of these agree-
ments, the scope of the arguments would be con-
fusing to an SRL system. In a sentence such as
?K
???@ I
J. ? @ ?g. P rjlu Albyti AlTwylu meaning ?the
tall man of the house?: ?man? is definite, masculine,
singular, nominative, corresponding to Definiteness,
Gender, Number and Case, respectively; ?the-house?
is definite, masculine, singular, genitive; ?the-tall? is
definite, masculine, singular, nominative. We note
that ?man? and ?tall? agree in Number, Gender, Case
and Definiteness. Syntactic Case is marked using
short vowels u, and i at the end of the word. Hence,
rjlu and AlTwylu agree in their Case ending5 With-
out the explicit marking of the Case information,
5The presence of the Albyti is crucial as it renders rjlu defi-
nite therefore allowing the agreement with AlTwylu to be com-
plete.
799
SVP
VBDpredicate
@YK.
started
NPARG0
NP
NN
?
KP
president
NP
NN
Z @P 	P??@
ministers
JJ
?

	?J
??@
Chinese
NP
NNP
? 	P
Zhu
NNP
?
m.
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 113?116,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Kernels on Linguistic Structures for Answer Extraction
Alessandro Moschitti and Silvia Quarteroni
DISI, University of Trento
Via Sommarive 14
38100 POVO (TN) - Italy
{moschitti,silviaq}@disi.unitn.it
Abstract
Natural Language Processing (NLP) for Infor-
mation Retrieval has always been an interest-
ing and challenging research area. Despite the
high expectations, most of the results indicate
that successfully using NLP is very complex.
In this paper, we show how Support Vector
Machines along with kernel functions can ef-
fectively represent syntax and semantics. Our
experiments on question/answer classification
show that the above models highly improve on
bag-of-words on a TREC dataset.
1 Introduction
Question Answering (QA) is an IR task where the
major complexity resides in question processing
and answer extraction (Chen et al, 2006; Collins-
Thompson et al, 2004) rather than document re-
trieval (a step usually carried out by off-the shelf IR
engines). In question processing, useful information
is gathered from the question and a query is created.
This is submitted to an IR module, which provides
a ranked list of relevant documents. From these, the
QA system extracts one or more candidate answers,
which can then be re-ranked following various crite-
ria. Although typical methods are based exclusively
on word similarity between query and answer, recent
work, e.g. (Shen and Lapata, 2007) has shown that
shallow semantic information in the form of predi-
cate argument structures (PASs) improves the auto-
matic detection of correct answers to a target ques-
tion. In (Moschitti et al, 2007), we proposed the
Shallow Semantic Tree Kernel (SSTK) designed to
encode PASs1 in SVMs.
1in PropBank format, (www.cis.upenn.edu/
?
ace).
In this paper, similarly to our previous approach,
we design an SVM-based answer extractor, that se-
lects the correct answers from those provided by a
basic QA system by applying tree kernel technol-
ogy. However, we also provide: (i) a new kernel
to process PASs based on the partial tree kernel al-
gorithm (PAS-PTK), which is highly more efficient
and more accurate than the SSTK and (ii) a new ker-
nel called Part of Speech sequence kernel (POSSK),
which proves very accurate to represent shallow syn-
tactic information in the learning algorithm.
To experiment with our models, we built two
different corpora, WEB-QA and TREC-QA by us-
ing the description questions from TREC 2001
(Voorhees, 2001) and annotating the answers re-
trieved from Web resp. TREC data (available at
disi.unitn.it/
?
silviaq). Comparative exper-
iments with re-ranking models of increasing com-
plexity show that: (a) PAS-PTK is far more efficient
and effective than SSTK, (b) POSSK provides a re-
markable further improvement on previous models.
Finally, our experiments on the TREC-QA dataset,
un-biased by the presence of typical Web phrasings,
show that BOW is inadequate to learn relations be-
tween questions and answers. This is the reason
why our kernels on linguistic structures improve it
by 63%, which is a remarkable result for an IR task
(Allan, 2000).
2 Kernels for Q/A Classification
The design of an answer extractor basically depends
on the design of a classifier that decides if an an-
swer correctly responds to the target question. We
design a classifier based on SVMs and different ker-
nels applied to several forms of question and answer
113
PAS
A1
autism
rel
characterize
A0
spectrum
PAS
A0
behavior
R-A0
that
rel
characterize
A1
inattention
(a)
PAS
A1
disorder
rel
characterize
A0
anxiety
(b)
PAS
rel
characterize
PAS
A1 rel A0
PAS
A1 rel
characterize
PAS
rel
characterize
A0
rel
characterize
(c)
Figure 1: Compact PAS-PTK structures of s1 (a) and s2 (b) and some fragments they have in common as produced by
the PTK (c). Arguments are replaced with their most important word (or semantic head) to reduce data sparseness.
representations:
(1) linear kernels on the bag-of-words (BOW) or
bag-of-POS-tags (POS) features,
(2) the String Kernel (SK) (Shawe-Taylor and Cris-
tianini, 2004) on word sequences (WSK) and POS-
tag sequences (POSSK),
(3) the Syntactic Tree Kernel (STK) (Collins and
Duffy, 2002) on syntactic parse trees (PTs),
(4) the Shallow Semantic Tree Kernel (SSTK) (Mos-
chitti et al, 2007) and the Partial Tree Kernel (PTK)
(Moschitti, 2006) on PASs.
In particular, POS-tag sequences and PAS trees
used with SK and PTK yield to two innovative ker-
nels, i.e. POSSK and PAS-PTK2. In the next sec-
tions, we describe in more detail the data structures
on which we applied the above kernels.
2.1 Syntactic Structures
The POSSK is obtained by applying the String Ker-
nel on the sequence of POS-tags of a question or
a answer. For example, given sentence s0: What
is autism?, the associated POS sequence is WP
AUX NN ? and some of the substrings extracted by
POSSK are WP NN or WP AUX. A more complete
structure is the full parse tree (PT) of the sentence,
that constitutes the input of the STK. For instance,
the STK accepts the syntactic parse: (SBARQ (WHNP
(WP What))(SQ (VP (AUX is)(NP (NN autism))))(. ?)).
2.2 Semantic Structures
The intuition behind our semantic representation is
the idea that when we ignore the answer to a def-
inition question we check whether such answer is
formulated as a ?typical? definition and whether an-
swers defining similar concepts are expressed in a
2For example, let PTK(t1, t2) = ?(t1) ? ?(t2), where t1
and t2 are two syntactic parse trees. If we map t1 and t2
into two new shallow semantic trees s1 and s2 with a map-
ping ?M (?), we obtain: PTK(s1, s2) = ?(s1) ? ?(s2) =
?(?M (t1)) ? ?(?M (t2)) = ??(t1) ? ??(t2)=PAS-PTK(t1, t2),
which is a noticeably different kernel induced by the mapping
?? = ? ? ?M .
similar way.
To take advantage of semantic representations, we
work with two types of semantic structures; first,
the Word Sequence Kernel applied to both ques-
tion and answer; given s0, sample substrings are:
What is autism, What is, What autism, is autism,
etc. Then, two PAS-based trees: Shallow Seman-
tic Trees for SSTK and Shallow Semantic Trees for
PTK, both based on PropBank structures (Kings-
bury and Palmer, 2002) are automatically generated
by our SRL system (Moschitti et al, 2005). As an
example, let us consider an automatically annotated
sentence from our TREC-QA corpus:
s1: [A1 Autism] is [rel characterized] [A0 by a broad
spectrum of behavior] [R?A0 that] [relincludes] [A1 ex-
treme inattention to surroundings and hypersensitivity to
sound and other stimuli].
Such annotation can be used to design a shallow se-
mantic representation that can be matched against
other semantically similar sentences, e.g.
s2: [A1 Panic disorder] is [rel characterized] [A0 by un-
realistic or excessive anxiety].
It can be observed here that, although autism is a
different disease from panic disorder, the structure
of both definitions and the latent semantics they con-
tain (inherent to behavior, disorder, anxiety) are sim-
ilar. So for instance, s2 appears as a definition even
to someone who only knows what the definition of
autism looks like.
The above annotation can be compactly repre-
sented by predicate argument structure trees (PASs)
such as those in Figure 1. Here, we can notice that
the semantic similarity between sentences is explic-
itly visible in terms of common fragments extracted
by the PTK from their respective PASs. Instead,
the similar PAS-SSTK representation in (Moschitti
et al, 2007) does not take argument order into ac-
count, thus it fails to capture the linguistic ratio-
nale expressed above. Moreover, it is much heavier,
causing large memory occupancy and, as shown by
our experiments, much longer processing time.
114
3 Experiments
In our experiments we show that (a) the PAS-PTK
shallow semantic tree kernel is more efficient and ef-
fective than the SSTK proposed in (Moschitti et al,
2007), and (b) our POSSK jointly used with PAS-
PTK and STK greatly improves on BOW.
3.1 Experimental Setup
In our experiments, we implemented the BOW and
POS kernels, WSK, POSSK, STK (on syntactic
PTs derived automatically with Charniak?s parser),
SSTK and PTK (on PASs derived automatically with
our SRL system) as well as their combinations in
SVM-light-TK3. Since answers often contain more
than one PAS (see Figure 1), we sum PTK (or SSTK)
applied to all pairs P1?P2, P1 and P2 being the sets
of PASs of the first two answers.
The experimental datasets were created by sub-
mitting the 138 TREC 2001 test questions labeled as
?description? in (Li and Roth, 2002) to our basic QA
system, YourQA (Quarteroni and Manandhar, 2008)
and by gathering the top 20 answer paragraphs.
YourQA was run on two sources: Web docu-
ments by exploiting Google (code.google.com/
apis/) and the AQUAINT data used for TREC?07
(trec.nist.gov/data/qa) by exploiting Lucene
(lucene.apache.org), yielding two different cor-
pora: WEB-QA and TREC-QA. Each sentence of
the returned paragraphs was manually evaluated
based on whether it contained a correct answer to
the corresponding question. To simplify our task,
we isolated for each paragraph the sentence with the
maximal judgment (such as s1 and s2 in Sec. 2.2)
and labeled it as positive if it answered the question
either concisely or with noise, negative otherwise.
The resulting WEB-QA corpus contains 1309 sen-
tences, 416 of which positive; the TREC-QA corpus
contains 2256 sentences, 261 of which positive.
3.2 Results
In a first experiment, we compared the learning and
classification efficiency of SVMs on PASs by apply-
ing either solely PAS-SSTK or solely PAS-PTK on
the WEB-QA and TREC-QA sets. We divided the
training data in 9 bins of increasing size (with a step
3Toolkit available at dit.unitn.it/moschitti/, based
on SVM-light (Joachims, 1999)
0
20
40
60
80
100
120
140
160
180
200
220
240
200 400 600 800 1000 1200 1400 1600 1800
Training Set Size
Ti
m
e 
in
 S
ec
on
ds
PTK (training) PTK (test)
SSTK (test) SSTK (training)
Figure 2: Efficiency of PTK and SSTK
60
61
62
63
64
65
66
67
68
69
1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0
cost-factor
F1
-m
ea
su
re
PT+WSK+PAS(PTK) PT
PT+BOW PT+POS
PT+WSK WSK
BOW PT+WSK+PAS(SSTK)
Figure 3: Impact of different kernels on WEB-QA
20
22
24
26
28
30
32
34
36
38
40
4 6 8 10 12 14 16 18 20
cost-factor
F1
-m
ea
su
re
PT POS+PT
POSSK+PT POSSK+PT+PAS-PTK
BOW+PT BOW+POS+PT
BOW POSSK+PT+PAS-SSTK
Figure 4: Impact of different kernels on TREC-QA
of 200) and measured the training and test time4 for
each bin. Figure 2 shows that in both the test and
training phases, PTK is much faster than SSTK. In
training, PTK is 40 times faster, enabling the exper-
imentation of SVMs with large datasets. This differ-
ence is due to the combination of our lighter seman-
tic structures and the PTK?s ability to extract from
these at least the same information that SSTK de-
rives from much larger structures.
Further interesting experiments regard the accu-
4Processing time in seconds of a Mac-Book Pro 2.4 Ghz.
115
racy tests of different kernels and some of their most
promising combinations. As a kernel operator, we
applied the sum between kernels5 that yields the
joint feature space of the individual kernels (Shawe-
Taylor and Cristianini, 2004).
Figure 3 shows the F1-plots of several kernels ac-
cording to different cost-factor values (i.e. different
Precision/Recall rates). Each F1 value is the average
of 5 fold cross-validation. We note that (a) BOW
achieves very high accuracy, comparable to the one
produced by PT; (b) the BOW+PT combination im-
proves on both single models; (c) WSK improves on
BOW and it is enhanced by WSK+PT, demonstrat-
ing that word sequences and PTs are very relevant
for this task; (d) both PAS-SSTK and PAS-PTK im-
prove on previous models yielding the highest result.
The high accuracy of BOW is surprising as sup-
port vectors are compared with test examples which
are in general different (there are no questions
shared between training and test set). The explana-
tion resides in the fact that WEB-QA contains com-
mon BOW patterns due to typical Web phrasings,
e.g. Learn more about X, that facilitate the de-
tection of incorrect answers.
Hence, to have un-biased results, we experi-
mented with the TREC corpus which is cleaner from
a linguistic viewpoint and also more complex from
a QA perspective. A comparative analysis of Fig-
ure 4 suggests that: (a) the F1 of all models is much
lower than for the WEB-QA dataset; (b) BOW de-
notes the lowest accuracy; (c) POS combined with
PT improves on PT; (d) POSSK+PT improves on
POS+PT; (f) finally, PAS adds further information
as the best model is POSSK+PT+PAS-PTK (or PAS-
SSTK).
4 Conclusions
With respect to our previous findings, experimenting
with TREC-QA allowed us to show that BOW is not
relevant to learn re-ranking functions from exam-
ples; indeed, while it is useful to establish an initial
ranking by measuring the similarity between ques-
tion and answer, BOW is almost irrelevant to grasp
typical rules that suggest if a description is valid or
not. Moreover, using the new POSSK and PAS-PTK
5All adding kernels are normalized to have a similarity score
between 0 and 1, i.e. K?(X1,X2) = K(X1,X2)?K(X1,X1)?K(X2,X2) .
kernels provides an improvement of 5 absolute per-
cent points wrt our previous work.
Finally, error analysis revealed that PAS-PTK can
provide patterns like A1(X) R-A1(that) rel(result)
A1(Y) and A1(X) rel(characterize) A0(Y), where X
and Y need not necessarily be matched.
Acknowledgments
This work was partly supported by the FP6 IST LUNA
project (contract No. 33549) and by the European
Commission Marie Curie Excellence Grant for the
ADAMACH project (contract No. 022593).
References
J. Allan. 2000. Natural language processing for informa-
tion retrieval. In Proceedings of NAACL/ANLP (tuto-
rial notes).
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
ACL?06.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL?02.
K. Collins-Thompson, J. Callan, E. Terra, and C. L.A.
Clarke. 2004. The effect of document retrieval quality
on factoid QA performance. In SIGIR?04.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In LREC?02.
X. Li and D. Roth. 2002. Learning question classifiers.
In ACL?02.
A. Moschitti, B. Coppola, A. Giuglea, and R. Basili.
2005. Hierarchical semantic role labeling. In CoNLL
2005 shared task.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting syntactic and shallow semantic
kernels for question/answer classification. In ACL?07.
A. Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In
ECML?06.
S. Quarteroni and S. Manandhar. 2008. Designing an
interactive open domain question answering system.
Journ. of Nat. Lang. Eng. (in press).
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In EMNLP-CoNLL.
E. M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. In TREC?01.
116
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 9?12,
Columbus, June 2008. c?2008 Association for Computational Linguistics
BART: A Modular Toolkit for Coreference Resolution
Yannick Versley
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Simone Paolo Ponzetto
EML Research gGmbH
ponzetto@eml-research.de
Massimo Poesio
University of Essex
poesio@essex.ac.uk
Vladimir Eidelman
Columbia University
vae2101@columbia.edu
Alan Jern
UCLA
ajern@ucla.edu
Jason Smith
Johns Hopkins University
jsmith@jhu.edu
Xiaofeng Yang
Inst. for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Alessandro Moschitti
University of Trento
moschitti@dit.unitn.it
Abstract
Developing a full coreference system able
to run all the way from raw text to seman-
tic interpretation is a considerable engineer-
ing effort, yet there is very limited avail-
ability of off-the shelf tools for researchers
whose interests are not in coreference, or for
researchers who want to concentrate on a
specific aspect of the problem. We present
BART, a highly modular toolkit for de-
veloping coreference applications. In the
Johns Hopkins workshop on using lexical
and encyclopedic knowledge for entity dis-
ambiguation, the toolkit was used to ex-
tend a reimplementation of the Soon et al
(2001) proposal with a variety of additional
syntactic and knowledge-based features, and
experiment with alternative resolution pro-
cesses, preprocessing tools, and classifiers.
1 Introduction
Coreference resolution refers to the task of identify-
ing noun phrases that refer to the same extralinguis-
tic entity in a text. Using coreference information
has been shown to be beneficial in a number of other
tasks, including information extraction (McCarthy
and Lehnert, 1995), question answering (Morton,
2000) and summarization (Steinberger et al, 2007).
Developing a full coreference system, however, is
a considerable engineering effort, which is why a
large body of research concerned with feature en-
gineering or learning methods (e.g. Culotta et al
2007; Denis and Baldridge 2007) uses a simpler but
non-realistic setting, using pre-identified mentions,
and the use of coreference information in summa-
rization or question answering techniques is not as
widespread as it could be. We believe that the avail-
ability of a modular toolkit for coreference will sig-
nificantly lower the entrance barrier for researchers
interested in coreference resolution, as well as pro-
vide a component that can be easily integrated into
other NLP applications.
A number of systems that perform coreference
resolution are publicly available, such as GUITAR
(Steinberger et al, 2007), which handles the full
coreference task, and JAVARAP (Qiu et al, 2004),
which only resolves pronouns. However, literature
on coreference resolution, if providing a baseline,
usually uses the algorithm and feature set of Soon
et al (2001) for this purpose.
Using the built-in maximum entropy learner
with feature combination, BART reaches 65.8%
F-measure on MUC6 and 62.9% F-measure on
MUC7 using Soon et al?s features, outperforming
JAVARAP on pronoun resolution, as well as the
Soon et al reimplementation of Uryupina (2006).
Using a specialized tagger for ACE mentions and
an extended feature set including syntactic features
(e.g. using tree kernels to represent the syntactic
relation between anaphor and antecedent, cf. Yang
et al 2006), as well as features based on knowledge
extracted from Wikipedia (cf. Ponzetto and Smith, in
preparation), BART reaches state-of-the-art results
on ACE-2. Table 1 compares our results, obtained
using this extended feature set, with results from
Ng (2007). Pronoun resolution using the extended
feature set gives 73.4% recall, coming near special-
ized pronoun resolution systems such as (Denis and
Baldridge, 2007).
9
Figure 1: Results analysis in MMAX2
2 System Architecture
The BART toolkit has been developed as a tool to
explore the integration of knowledge-rich features
into a coreference system at the Johns Hopkins Sum-
mer Workshop 2007. It is based on code and ideas
from the system of Ponzetto and Strube (2006), but
also includes some ideas from GUITAR (Steinberger
et al, 2007) and other coreference systems (Versley,
2006; Yang et al, 2006). 1
The goal of bringing together state-of-the-art ap-
proaches to different aspects of coreference res-
olution, including specialized preprocessing and
syntax-based features has led to a design that is very
modular. This design provides effective separation
of concerns across several several tasks/roles, in-
cluding engineering new features that exploit dif-
ferent sources of knowledge, designing improved or
specialized preprocessing methods, and improving
the way that coreference resolution is mapped to a
machine learning problem.
Preprocessing To store results of preprocessing
components, BART uses the standoff format of the
MMAX2 annotation tool (Mu?ller and Strube, 2006)
with MiniDiscourse, a library that efficiently imple-
ments a subset of MMAX2?s functions. Using a
generic format for standoff annotation allows the use
of the coreference resolution as part of a larger sys-
tem, but also performing qualitative error analysis
using integrated MMAX2 functionality (annotation
1An open source version of BART is available from
http://www.sfs.uni-tuebingen.de/?versley/BART/.
diff, visual display).
Preprocessing consists in marking up noun
chunks and named entities, as well as additional in-
formation such as part-of-speech tags and merging
these information into markables that are the start-
ing point for the mentions used by the coreference
resolution proper.
Starting out with a chunking pipeline, which
uses a classical combination of tagger and chun-
ker, with the Stanford POS tagger (Toutanova et al,
2003), the YamCha chunker (Kudoh and Mat-
sumoto, 2000) and the Stanford Named Entity Rec-
ognizer (Finkel et al, 2005), the desire to use richer
syntactic representations led to the development of
a parsing pipeline, which uses Charniak and John-
son?s reranking parser (Charniak and Johnson, 2005)
to assign POS tags and uses base NPs as chunk
equivalents, while also providing syntactic trees that
can be used by feature extractors. BART also sup-
ports using the Berkeley parser (Petrov et al, 2006),
yielding an easy-to-use Java-only solution.
To provide a better starting point for mention de-
tection on the ACE corpora, the Carafe pipeline
uses an ACE mention tagger provided by MITRE
(Wellner and Vilain, 2006). A specialized merger
then discards any base NP that was not detected to
be an ACE mention.
To perform coreference resolution proper, the
mention-building module uses the markables cre-
ated by the pipeline to create mention objects, which
provide an interface more appropriate for corefer-
ence resolution than the MiniDiscourse markables.
These objects are grouped into equivalence classes
by the resolution process and a coreference layer is
written into the document, which can be used for de-
tailed error analysis.
Feature Extraction BART?s default resolver goes
through all mentions and looks for possible an-
tecedents in previous mentions as described by Soon
et al (2001). Each pair of anaphor and candi-
date is represented as a PairInstance object,
which is enriched with classification features by fea-
ture extractors, and then handed over to a machine
learning-based classifier that decides, given the fea-
tures, whether anaphor and candidate are corefer-
ent or not. Feature extractors are realized as sepa-
rate classes, allowing for their independent develop-
10
Figure 2: Example system configuration
ment. The set of feature extractors that the system
uses is set in an XML description file, which allows
for straightforward prototyping and experimentation
with different feature sets.
Learning BART provides a generic abstraction
layer that maps application-internal representations
to a suitable format for several machine learning
toolkits: One module exposes the functionality of
the the WEKA machine learning toolkit (Witten
and Frank, 2005), while others interface to special-
ized state-of-the art learners. SVMLight (Joachims,
1999), in the SVMLight/TK (Moschitti, 2006) vari-
ant, allows to use tree-valued features. SVM Classi-
fication uses a Java Native Interface-based wrapper
replacing SVMLight/TK?s svm classify pro-
gram to improve the classification speed. Also in-
cluded is a Maximum entropy classifier that is
based upon Robert Dodier?s translation of Liu and
Nocedal?s (1989) L-BFGS optimization code, with
a function for programmatic feature combination.2
Training/Testing The training and testing phases
slightly differ from each other. In the training phase,
the pairs that are to be used as training examples
have to be selected in a process of sample selection,
whereas in the testing phase, it has to be decided
which pairs are to be given to the decision function
and how to group mentions into equivalence rela-
tions given the classifier decisions.
This functionality is factored out into the en-
2see http://riso.sourceforge.net
coder/decoder component, which is separate from
feature extraction and machine learning itself. It
is possible to completely change the basic behav-
ior of the coreference system by providing new
encoders/decoders, and still rely on the surround-
ing infrastructure for feature extraction and machine
learning components.
3 Using BART
Although BART is primarily meant as a platform for
experimentation, it can be used simply as a corefer-
ence resolver, with a performance close to state of
the art. It is possible to import raw text, perform
preprocessing and coreference resolution, and either
work on the MMAX2-format files, or export the re-
sults to arbitrary inline XML formats using XSL
stylesheets.
Adapting BART to a new coreferentially anno-
tated corpus (which may have different rules for
mention extraction ? witness the differences be-
tween the annotation guidelines of MUC and ACE
corpora) usually involves fine-tuning of mention cre-
ation (using pipeline and MentionFactory settings),
as well as the selection and fine-tuning of classi-
fier and features. While it is possible to make rad-
ical changes in the preprocessing by re-engineering
complete pipeline components, it is usually possi-
ble to achieve the bulk of the task by simply mix-
ing and matching existing components for prepro-
cessing and feature extraction, which is possible by
modifying only configuration settings and an XML-
11
BNews NPaper NWire
Recl Prec F Recl Prec F Recl Prec F
basic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533
extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627
Ng 2007? 0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633
?: ?expanded feature set? in Ng 2007; Ng trains on the entire ACE training corpus.
Table 1: Performance on ACE-2 corpora, basic vs. extended feature set
based description of the feature set and learner(s)
used.
Several research groups focusing on coreference
resolution, including two not involved in the ini-
tial creation of BART, are using it as a platform
for research including the use of new information
sources (which can be easily incorporated into the
coreference resolution process as features), different
resolution algorithms that aim at enhancing global
coherence of coreference chains, and also adapting
BART to different corpora. Through the availability
of BART as open source, as well as its modularity
and adaptability, we hope to create a larger com-
munity that allows both to push the state of the art
further and to make these improvements available to
users of coreference resolution.
Acknowledgements We thank the CLSP at Johns
Hopkins, NSF and the Department of Defense for
ensuring funding for the workshop and to EML
Research, MITRE, the Center for Excellence in
HLT, and FBK-IRST, that provided partial support.
Yannick Versley was supported by the Deutsche
Forschungsgesellschaft as part of SFB 441 ?Lin-
guistic Data Structures?; Simone Paolo Ponzetto has
been supported by the Klaus Tschira Foundation
(grant 09.003.2004).
References
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc. ACL
2005.
Culotta, A., Wick, M., and McCallum, A. (2007). First-order
probabilistic models for coreference resolution. In Proc.
HLT/NAACL 2007.
Denis, P. and Baldridge, J. (2007). A ranking approach to pro-
noun resolution. In Proc. IJCAI 2007.
Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorpo-
rating non-local information into information extraction sys-
tems by Gibbs sampling. In Proc. ACL 2005, pages 363?370.
Joachims, T. (1999). Making large-scale SVM learning prac-
tical. In Scho?lkopf, B., Burges, C., and Smola, A., editors,
Advances in Kernel Methods - Support Vector Learning.
Kudoh, T. and Matsumoto, Y. (2000). Use of Support Vector
Machines for chunk identification. In Proc. CoNLL 2000.
Liu, D. C. and Nocedal, J. (1989). On the limited memory
method for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
McCarthy, J. F. and Lehnert, W. G. (1995). Using decision trees
for coreference resolution. In Proc. IJCAI 1995.
Morton, T. S. (2000). Coreference for NLP applications. In
Proc. ACL 2000.
Moschitti, A. (2006). Making tree kernels practical for natural
language learning. In Proc. EACL 2006.
Mu?ller, C. and Strube, M. (2006). Multi-level annotation of
linguistic data with MMAX2. In Braun, S., Kohn, K., and
Mukherjee, J., editors, Corpus Technology and Language
Pedagogy: New Resources, New Tools, New Methods. Peter
Lang, Frankfurt a.M., Germany.
Ng, V. (2007). Shallow semantics for coreference resolution. In
Proc. IJCAI 2007.
Petrov, S., Barett, L., Thibaux, R., and Klein, D. (2006). Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL 2006.
Ponzetto, S. P. and Strube, M. (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. HLT/NAACL 2006.
Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004). A public reference
implementation of the RAP anaphora resolution algorithm.
In Proc. LREC 2004.
Soon, W. M., Ng, H. T., and Lim, D. C. Y. (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K. (2007).
Two uses of anaphora resolution in summarization. Informa-
tion Processing and Management, 43:1663?1680. Special
issue on Summarization.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proc. NAACL 2003, pages 252?259.
Uryupina, O. (2006). Coreference resolution with and without
linguistic knowledge. In Proc. LREC 2006.
Versley, Y. (2006). A constraint-based approach to noun phrase
coreference resolution in German newspaper text. In Kon-
ferenz zur Verarbeitung Natu?rlicher Sprache (KONVENS
2006).
Wellner, B. and Vilain, M. (2006). Leveraging machine read-
able dictionaries in discriminative sequence models. In Proc.
LREC 2006.
Witten, I. and Frank, E. (2005). Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
Yang, X., Su, J., and Tan, C. L. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
CoLing/ACL-2006.
12
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 72?77,
Prague, June 2007. c?2007 Association for Computational Linguistics
Shallow Semantics in Fast Textual Entailment Rule Learners
Fabio Massimo Zanzotto
DISP
University of Rome ?Tor Vergata?
Roma, Italy
zanzotto@info.uniroma2.it
Marco Pennacchiotti
Computerlinguistik
Universita?t des Saarlandes,
Saarbru?cken, Germany
pennacchiotti@coli.uni-sb.de
Alessandro Moschitti
DIT
University of Trento
Povo di Trento, Italy
moschitti@dit.unitn.it
Abstract
In this paper, we briefly describe two
enhancements of the cross-pair similarity
model for learning textual entailment rules:
1) the typed anchors and 2) a faster compu-
tation of the similarity. We will report and
comment on the preliminary experiments
and on the submission results.
1 Introduction
Results of the second RTE challenge (Bar Haim et
al., 2006) have suggested that both deep semantic
models and machine learning approaches can suc-
cessfully be applied to solve textual entailment. The
only problem seems to be the size of the knowledge
bases. The two best systems (Tatu et al, 2005; Hickl
et al, 2005), which are significantly above all the
others (more than +10% accuracy), use implicit or
explicit knowledge bases larger than all the other
systems. In (Tatu et al, 2005), a deep semantic
representation is paired with a large amount of gen-
eral and task specific semantic rules (explicit knowl-
edge). In (Hickl et al, 2005), the machine learning
model is trained over a large amounts of examples
(implicit knowledge).
In contrast, Zanzotto&Moschitti (2006) proposed
a machine-learning based approach which reaches a
high accuracy by only using the available RTE data.
The key idea is the cross-pair similarity, i.e. a simi-
larity applied to two text and hypothesis pairs which
considers the relations between the words in the two
texts and between the words in the two hypotheses.
This is obtained by using placeholders to link the re-
lated words. Results in (Bar Haim et al, 2006) are
comparable with the best machine learning system
when this latter is trained only on the RTE exam-
ples.
Given the high potential of the cross-pair similar-
ity model, for the RTE3 challenge, we built on it by
including some features of the two best systems: 1)
we go towards a deeper semantic representation of
learning pairs including shallow semantic informa-
tion in the syntactic trees using typed placeholders;
2) we reduce the computational cost of the cross-pair
similarity computation algorithm to allow the learn-
ing over larger training sets.
The paper is organized as follows: in Sec. 2 we
review the cross-pair similarity model and its limits;
in Sec. 3, we introduce our model for typed anchors;
in Sec. 4 we describe how we limit the computa-
tional cost of the similarity; in Sec. 5 we present the
two submission experiments, and in Sec. 6 we draw
some conclusions.
2 Cross-pair similarity and its limits
2.1 Learning entailment rules with syntactic
cross-pair similarity
The cross-pair similarity model (Zanzotto and
Moschitti, 2006) proposes a similarity measure
aiming at capturing rewrite rules from train-
ing examples, computing a cross-pair similarity
KS((T ?,H ?), (T ??,H ??)). The rationale is that if two
pairs are similar, it is extremely likely that they have
the same entailment value. The key point is the use
of placeholders to mark the relations between the
sentence words. A placeholder co-indexes two sub-
structures in the parse trees of text and hypothesis,
72
indicating that such substructures are related. For
example, the sentence pair, ?All companies file an-
nual reports? implies ?All insurance companies file
annual reports?, is represented as follows:
T1 (S (NP: 1 (DT All) (NNS: 1 compa-
nies)) (VP: 2 (VBP: 2 file) (NP: 3 (JJ: 3
annual) (NNS: 3 reports))))
H1 (S (NP: 1 (DT All) (NNP Fortune)
(CD 50) (NNS: 1 companies)) (VP: 2
(VBP: 2 file) (NP: 3 (JJ: 3 annual)
(NNS: 3 reports))))
(E1)
where the placeholders 1 , 2 , and 3 indicate the rela-
tions between the structures of T and of H .
Placeholders help to determine if two pairs share
the same rewriting rule by looking at the subtrees
that they have in common. For example, suppose
we have to determine if ?In autumn, all leaves fall?
implies ?In autumn, all maple leaves fall?. The re-
lated co-indexed representation is:
T2 (S (PP (IN In) (NP (NN: a automn)))
(, ,) (NP: b (DT all) (NNS: b leaves))
(VP: c (VBP: c fall)))
H2 (S (PP (IN In) (NP: a (NN: a automn)))
(, ,) (NP: b (DT all) (NN maple)
(NNS: a leaves)) (VP: c (VBP: c fall)))
(E2)
E1 and E2 share the following subtrees:
T3 (S (NP: x (DT all) (NNS: x )) (VP: y
(VBP: y )))
H3 (S (NP: x (DT all) (NN) (NNS: x ))
(VP: x (VBP: x )))
(R3)
This is the rewrite rule they have in common. Then,
E2 can be likely classified as a valid entailment, as
it shares the rule with the valid entailment E1.
The cross-pair similarity model uses: (1) a tree
similarity measure KT (?1, ?2) (Collins and Duffy,
2002) that counts the subtrees that ?1 and ?2 have
in common; (2) a substitution function t(?, c) that
changes names of the placeholders in a tree accord-
ing to a set of correspondences between placehold-
ers c. Given C as the collection of all correspon-
dences between the placeholders of (T ?,H ?) and
(T ??,H ??), the cross-pair similarity is computed as:
KS((T ?, H ?), (T ??, H ??)) =
maxc?C(KT (t(T ?, c), t(T ??, c)) + KT (t(H ?, c), t(H ??, c)))
(1)
The cross-pair similarity KS , used in a kernel-based
learning model as the support vector machines, al-
lows the exploitation of implicit true and false en-
tailment rewrite rules described in the examples.
2.2 Limits of the syntactic cross-pair similarity
Learning from examples using cross-pair similarity
is an attractive and effective approach. However,
the cross-pair strategy, as any machine learning ap-
proach, is highly sensitive on how the examples are
represented in the feature space, as this can strongly
bias the performance of the classifier.
Consider for example the following text-
hypothesis pair, which can lead to an incorrect rule,
if misused.
T4 ?For my younger readers, Chapman
killed John Lennon more than twenty
years ago.?
H4 ?John Lennon died more than twenty
years ago.?
(E4)
In the basic cross-pair similarity model, the learnt
rule would be the following:
T5 (S (NP: x ) (VP: y (VBD: y ) (NP: z )
(ADVP: k )))
H5 (S (NP: z ) (VP: y (VBD: y )
(ADVP: k )))
(R5)
where the verbs kill and die are connected by the y
placeholder. This rule is useful to classify examples
like:
T6 ?Cows are vegetarian but, to save
money on mass-production, farmers fed
cows animal extracts.?
H6 ?Cows have eaten animal extracts.?
(E6)
but it will clearly fail when used for:
T7 ?FDA warns migraine medicine makers
that they are illegally selling migraine
medicines without federal approval.?
H7 ?Migraine medicine makers declared
that their medicines have been ap-
proved.?
(E7)
where warn and declare are connected as generically
similar verbs.
The problem of the basic cross-pair similarity
measure is that placeholders do not convey the
semantic knowledge needed in cases such as the
above, where the semantic relation between con-
nected verbs is essential.
2.3 Computational cost of the cross-similarity
measure
Let us go back to the computational cost of KS (eq.
1). It heavily depends on the size of C. We de-
fine p? and p?? as the placeholders of, respectively,
(T ?,H ?) and (T ??,H ??). As C is combinatorial with
respect to |p?| and |p??|, |C| rapidly grows. Assigning
placeholders only to chunks helps controlling their
73
number. For example, in the RTE data the number
of placeholders hardly goes beyond 7, as hypothe-
ses are generally short sentences. But, even in these
cases, the number of KT computations grows. As
the trees t(?, c) are obtained from a single tree ?
(containing placeholder) applying different c ? C,
it is reasonable to think that they will share com-
mon subparts. Then, during the iterations of c ?
C, KT (t(??, c), t(???, c)) will compute the similarity
between subtrees that have already been evaluated.
The reformulation of the cross-pair similarity func-
tion we present takes advantage of this.
3 Adding semantic information to
cross-pair similarity
The examples in the previous section show that
the cross-pairs approach lacks the lexical-semantic
knowledge connecting the words in a placeholder.
In the examples, the missed knowledge is the type
of semantic relation between the main verbs. The
relation that links kill and die is not a generic sim-
ilarity, as a WordNet based similarity measure can
suggest, but a more specific causal relation. The
learnt rewrite rule R5 holds only for verbs in such
relation. In facts, it is correctly applied in example
E6, as feed causes eat, but it gives a wrong sugges-
tion in example E7, since warn and declare are only
related by a generic similarity relation.
We then need to encode this information in the
syntactic trees in order to learn correct rules.
3.1 Defining anchor types
The idea of introducing anchor types should be in
principle very simple and effective. Yet, this may be
not the case: simpler attempts to introduce semantic
information in RTE systems have often failed. To
investigate the validity of our idea, we then need to
focus on a small set of relevant relation types, and to
carefully control ambiguity for each type.
A valuable source of relation types among words
is WordNet. We choose to integrate in our system
three important relation standing at the word level:
part-of, antinomy, and verb entailment. We also de-
fine two more general anchor types: similarity and
the surface matching. The first type links words
which are similar according to some WordNet simi-
larity measure. Specifically, this type is intended to
Rank Relation Type Symbol
1. antinomy ?
2. part-of ?
3. verb entailment ?
4. similarity ?
5. surface matching =
Table 1: Ranked anchor types
capture the semantic relations of synonymy and hy-
peronymy. The second type is activated when words
or lemmas match: then, it captures cases in which
words are semantically equivalent. The complete set
of relation types used in the experiments is given in
Table 1.
3.2 Type anchors in the syntactic tree
To learn more correct rewrite rules by using the an-
chor types defined in the previous section, we need
to add this information to syntactic trees. The best
position would be in the same nodes of the anchors.
Also, to be more effective, this information should
be inserted in as many subtrees as possible. Thus we
define the typed-anchor climbing-up rules. We then
implement in our model the following climbing up
rule:
if two typed anchors climb up to the same
node, give precedence to that with the high-
est ranking in Tab. 1.
This rule can be easily showed to be consistent with
common sense intuitions. For an example like ?John
is a tall boy? that does not entail ?John is a short
boy?, our strategy will produce these trees:
(E8)
T8 H8
S ? 3
NP = 1
NNP = 1
John
VP ? 2
AUX
is
NP ? 3
DT
a
JJ ? 2
tall
NN = 3
boy
S ? 3
NP = 1
NNP = 1
John
VP ? 2
AUX
is
NP ? 3
DT
a
JJ ? 2
short
NN = 3
boy
This representation can be used to derive a correct
rewrite rule, such as:
if two fragments have the same syntactic struc-
ture S(NP1, V P (AUX,NP2)), and there is an
antonym type (?) on the S and NP2 , then the
74
c1 = {(a, 1), (b, 2), (c, 3)} c2 = {(a, 1), (b, 2), (d, 3)}
?1 t(?1, c1) t(?1, c2)
X1 a
A2 a
B3 a
w1
a
C4 b
w2
b
D5 d
D6 c
w3
c
C7 d
w4
d
X1 a:1
A2 a:1
B3 a:1
w1
a:1
C4 b:2
w2
b:2
D5 d
D6 c:3
w3
c:3
C7 d
w4
d
X1 a:1
A2 a:1
B3 a:1
w1
a:1
C4 b:2
w2
b:2
D5 d:3
D6 c
w3
c
C7 d:3
w4
d:3
?2 t(?2, c1) t(?2, c2)
X1 1
A2 1
B3 1
m1
1
C4 2
m2
2
D5
D6 3
m3
3
C7
m4
X1 a:1
A2 a:1
B3 a:1
m1
a:1
C4 b:2
m2
b:2
D5
D6 c:3
m3
c:3
C7
m4
X1 a:1
A2 a:1
B3 a:1
m1
a:1
C4 b:2
m2
b:2
D5
D6 d:3
m3
d:3
C7
m4
Figure 1: Tree pairs with placeholders and t(T, c) transformation
entailment does not hold.
4 Reducing computational cost of the
cross-pair similarity computation
4.1 The original kernel function
In this section, we describe more in detail the simi-
larity function KS (Eq. 1). To simplify, we focus on
the computation of only one KT of the kernel sum.
KS(??,???) = maxc?C KT (t(?
?, c), t(???, c)), (2)
where the (??,???) pair can be either (T ?, T ??) or
(H ?,H ??). We apply this simplification since we
are interested in optimizing the evaluation of the
KT with respect to different sets of correspondences
c ? C.
To better explain KS , we need to analyze the role
of the substitution function t(?, c) and to review the
tree kernel function KT .
The aim of t(?, c) is to coherently replace place-
holders in two trees ?? and ??? so that these two trees
can be compared. The substitution is carried out
according to the set of correspondences c. Let p?
and p?? be placeholders of ?? and ???, respectively,
if p?? ? p? then c is a bijection between a subset
p?? ? p? and p??. For example (Fig. 1), the trees ?1
has p1 ={ a , b , c , d } as placeholder set and ?2 has
p2 ={ 1 , 2 , 3 }. In this case, a possible set of corre-
spondence is c1 = {(a, 1), (b, 2), (c, 3)}. In Fig. 1
the substitution function replaces each placeholder
a of the tree ?1with the new placeholder a:1 by
t(?, c) obtaining the transformed tree t(?1, c1), and
each placeholder 1 of ?2 with a:1 . After these sub-
stitutions, the labels of the two trees can be matched
and the similarity function KT is applicable.
KT (? ?, ? ??), as defined in (Collins and Duffy,
2002), computes the number of common subtrees
between ? ? and ? ??.
4.2 An observation to reduce the
computational cost
The above section has shown that the similarity
function KS firstly applies the transformation t(?, c)
and then computes the tree kernel KT . The overall
process can be optimized by factorizing redundant
KT computations.
Indeed, two trees, t(?, c?) and t(?, c??), obtained
by applying two sets of correspondences c?, c?? ? C,
may partially overlap since c? and c?? can share a non-
empty set of common elements. Let us consider the
subtree set S shared by t(?, c?) and t(?, c??) such
that they contain placeholders in c? ? c?? = c, then
t(?, c) = t(?, c?) = t(?, c??) ?? ? S. Therefore if
we apply a tree kernel function KT to a pair (??,???),
we can find a c such that subtrees of ?? and subtrees
of ??? are invariant with respect to c? and c??. There-
fore, KT (t(??, c), t(???, c)) = KT (t(??, c?), t(???, c?))
= KT (t(??, c??), t(???, c??)). This implies that it is
possible to refine the dynamic programming algo-
rithm used to compute the ? matrices while com-
75
puting the kernel KS(??,???).
To better explain this idea let us consider
Fig. 1 that represents two trees, ?1 and ?2,
and the application of two different transforma-
tions c1 = {(a, 1), (b, 2), (c, 3)} and c2 =
{(a, 1), (b, 2), (d, 3)}. Nodes are generally in the
form Xi z where X is the original node label, z is
the placeholder, and i is used to index nodes of the
tree. Two nodes are equal if they have the same node
label and the same placeholder. The first column of
the figure represents the original trees ?1 and ?2.
The second and third columns contain respectively
the transformed trees t(?, c1) and t(?, c2)
Since the subtree of ?1 starting from A2 a con-
tains only placeholders that are in c, in the trans-
formed trees, t(?1, c1) and t(?1, c2), the subtrees
rooted in A2 a:1 are identical. The same happens
for ?2 with the subtree rooted in A2 1 . In the trans-
formed trees, t(?2, c1) and t(?2, c2), subtrees rooted
in A2 a:1 are identical. The computation of KT
applied to the above subtrees gives an identical re-
sult. Then, this computation can be avoided. If cor-
rectly used in a dynamic programming algorithm,
the above observation can produce an interesting de-
crease in the time computational cost. More de-
tails on the algorithm and the decrease in computa-
tional cost may be found in (Moschitti and Zanzotto,
2007).
5 Experimental Results
5.1 Experimental Setup
We implemented the novel cross-similarity kernel
in the SVM-light-TK (Moschitti, 2006) that en-
codes the basic syntactic kernel KT in SVM-light
(Joachims, 1999).
To assess the validity of the typed anchor model
(tap), we evaluated two sets of systems: the plain
and lexical-boosted systems. The plain systems are:
-tap: our tree-kernel approach using typed place-
holders with climbing in the syntactic tree;
-tree: the cross-similarity model described in Sec.2.
Its comparison with tap indicates the effectiveness
of our approaches;
The lexical-boosted systems are:
-lex: a standard approach based on lexical over-
lap. The classifier uses as the only feature the lexi-
cal overlap similarity score described in (Corley and
Mihalcea, 2005);
-lex+tap: these configurations mix lexical overlap
and our typed anchor approaches;
-lex+tree: the comparison of this configuration with
lex+tap should further support the validity of our in-
tuition on typed anchors;
Preliminary experiments have been performed us-
ing two datasets: RTE2 (the 1600 entailment pairs
from the RTE-2 challenge) and RTE3d (the devel-
opment dataset of this challenge). We randomly
divided this latter in two halves: RTE3d0 and
RTE3d1.
5.2 Investigatory Results Analysis and
Submission Results
Table 2 reports the results of the experiments. The
first column indicates the training set whereas the
second one specifies the used test set. The third and
the forth columns represent the accuracy of basic
models: the original tree model and the enhanced
tap model. The latter three columns report the basic
lex model and the two combined models, lex+tree
and lex+tap. The second and the third rows repre-
sent the accuracy of the models with respect to the
first randomly selected half of RTE3d whilst the
last two rows are related to the second half.
The experimental results show some interesting
facts. In the case of the plain systems (tree and tap),
we have the following observations:
- The use of the typed anchors in the model seems
to be effective. All the tap model results are higher
than the corresponding tree model results. This sug-
gests that the method used to integrate this kind of
information in the syntactic tree is effective.
- The claim that using more training material helps
seems not to be supported by these experiments. The
gap between tree and tap is higher when learn-
ing with RTE2 + RTE3d0 than when learning
with RTE30. This supports the claim. How-
ever, the result is not kept when learning with
RTE2 + RTE3d1 with respect to when learning
with RTE31. This suggests that adding not very
specific information, i.e. derived from corpora dif-
ferent from the target one (RTE3), may not help the
learning of accurate rules.
On the other hand, in the case of the lexical-
boosted systems (lex, lex+tree, and lex+tap), we
see that:
76
Train Test tree tap lex lex+tree lex+tap
RTE3d0 RTE3d1 62.97 64.23 69.02 68.26 69.02
RTE2 +RTE3d0 RTE3d1 62.22 62.47 71.03 71.28 71.79
RTE3d1 RTE3d0 62.03 62.78 70.22 70.22 71.22
RTE2 +RTE3d0 RTE3d0 63.77 64.76 71.46 71.22 72.95
Table 2: Accuracy of the systems on two folds of RTE3 development
- There is an extremely high accuracy result for the
pure lex model. This result is counterintuitive. A
model like lex has been likely used by QA or IE
systems to extract examples for the RTE3d set. If
this is the case we may expect that positive and
negative examples should have similar values for
this lex distance indicator. It is then not clear why
this model results in so high accuracy.
- Given the high results of the lex model, the model
lex+tree does not increase the performances.
- On the contrary, the model lex+tap is always better
(or equal) than the lex model. This suggests that
for this particular set of examples the typed anchors
are necessary to effectively use the rewriting rules
implicitly encoded in the examples.
- When the tap model is used in combination with
the lex model, it seems that the claim ?the more
training examples the better? is valid. The gaps
between lex and lex+tap are higher when the RTE2
is used in combination with the RTE3d related set.
Given this analysis we submitted two systems
both based on the lex+tap model. We did two differ-
ent training: one using RTE3d and the other using
RTE2 +RTE3d. Results are reported in the Table
below:
Train Accuracy
RTE3d 66.75%
RTE2 +RTE3d 65.75%
Such results seem too low to be statistically consis-
tent with our development outcome. This suggests
that there is a clear difference between the content
of RTE3d and the RTE3 test set. Moreover, in
contrast with what expected, the system trained with
only the RTE3d data is more accurate than the oth-
ers. Again, this suggests that the RTE corpora (from
all the challenges) are most probably very different.
6 Conclusions and final remarks
This paper demonstrates that it is possible to ef-
fectively include shallow semantics in syntax-based
learning approaches. Moreover, as it happened in
RTE2, it is not always true that more learning ex-
amples increase the accuracy of RTE systems. This
claim is still under investigation.
References
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. 2006.
The II PASCAL RTE challenge. In PASCAL Challenges
Workshop, Venice, Italy.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of ACL02.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, pages 13?18, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2005. Recognizing textual en-
tailment with LCCs GROUNDHOG system. In Proceedings
of the Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Thorsten Joachims. 1999. Making large-scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods-Support Vector Learning. MIT
Press.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from texts.
In Proceedings of the International Conference of Machine
Learning (ICML), Corvallis, Oregon.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL?06,
Trento, Italy.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and
Dan Moldovan. 2005. COGEX at the second recognizing
textual entailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy.
Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Au-
tomatic learning of textual entailments with cross-pair sim-
ilarities. In Proceedings of the 21st Coling and 44th ACL,
pages 401?408, Sydney, Australia, July.
77
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 133?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
CUNIT: A Semantic Role Labeling System for Modern Standard Arabic
Mona Diab
Columbia University
mdiab@cs.columbia.edu
Alessandro Moschitti
University of Trento, DIT
moschitti@dit.unitn.it
Daniele Pighin
FBK-irst; University of Trento, DIT
pighin@itc.it
Abstract
In this paper, we present a system for Ara-
bic semantic role labeling (SRL) based on
SVMs and standard features. The system is
evaluated on the released SEMEVAL 2007
development and test data. The results show
an F?=1 score of 94.06 on argument bound-
ary detection and an overall F?=1 score of
81.43 on the complete semantic role label-
ing task using gold parse trees.
1 Introduction
There is a widely held belief in the computational
linguistics field that identifying and defining the
roles of predicate arguments, semantic role label-
ing (SRL), in a sentence has a lot of potential for
and is a significant step towards the improvement of
important applications such as document retrieval,
machine translation, question answering and infor-
mation extraction. However, effective ways for see-
ing this belief come to fruition require a lot more
research investment.
Since most of the available data resources are for
the English language, most of the reported SRL sys-
tems to date only deal with English. Nevertheless,
we do see some headway for other languages, such
as German and Chinese (Erk and Pado, 2006; Sun
and Jurafsky, 2004; Xue and Palmer, 2005). The
systems for non-English languages follow the suc-
cessful models devised for English, e.g. (Gildea and
Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et
al., 2003). However, no SRL system exists for Ara-
bic.
In this paper, we present a system for semantic
role labeling for modern standard Arabic. To our
knowledge, it is the first SRL system for a semitic
language in the literature. It is based on a supervised
model that uses support vector machines (SVM)
technology for argument boundary detection and ar-
gument classification. It is trained and tested using
the pilot Arabic PropBank data released as part of
the SEMEVAL 2007 data. Given the lack of a re-
liable deep syntactic parser, in this research we use
gold trees.
The system yields an F-score of 94.06 on the sub
task of argument boundary detection and an F-score
of 81.43 on the complete task, i.e. boundary plus
classification.
2 SRL system for Arabic
The design of an optimal model for an Arabic SRL
systems should take into account specific linguis-
tic aspects of the language. However, a remarkable
amount of research has already been done in SRL
and we can capitalize from it to design a basic and
effective SRL system. The idea is to use the technol-
ogy developed for English and verify if it is suitable
for Arabic.
Our adopted SRL models use Support Vector Ma-
chines (SVM) to implement a two steps classifica-
tion approach, i.e. boundary detection and argument
classification. Such models have already been in-
vestigated in (Pradhan et al, 2003; Moschitti et al,
2005) and their description is hereafter reported.
2.1 Predicate Argument Extraction
The extraction of predicative structures is carried out
at the sentence level. Given a predicate within a
natural language sentence, its arguments have to be
properly labeled. This problem is usually divided
in two subtasks: (a) the detection of the boundaries,
i.e. the word spans of the arguments, and (b) the
classification of their type, e.g. Arg0 and ArgM in
133
SNP
NN
  
 /project
NP
NNP

  	/nations
JJ


 




	/United
VP
VBP




 /instated
NP
NN



 /grace-period
JJ




  Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 288?291,
Prague, June 2007. c?2007 Association for Computational Linguistics
RTV: Tree Kernels for Thematic Role Classification
Daniele Pighin
FBK-irst; University of Trento, DIT
pighin@itc.it
Alessandro Moschitti
University of Trento, DIT
moschitti@dit.unitn.it
Roberto Basili
University of Rome Tor Vergata, DISP
basili@info.uniroma2.it
Abstract
We present a simple, two-steps supervised
strategy for the identification and classifica-
tion of thematic roles in natural language
texts. We employ no external source of in-
formation but automatic parse trees of the in-
put sentences. We use a few attribute-value
features and tree kernel functions applied to
specialized structured features. The result-
ing system has an F1 of 75.44 on the Se-
mEval2007 closed task on semantic role la-
beling.
1 Introduction
In this paper we present a system for the labeling
of semantic roles that produces VerbNet (Kipper et
al., 2000) like annotations of free text sentences us-
ing only full syntactic parses of the input sentences.
The labeling process is modeled as a cascade of two
distinct classification steps: (1) boundary detection
(BD), in which the word sequences that encode a
thematic role for a given predicate are recognized,
and (2) role classification (RC), in which the type
of thematic role with respect to the predicate is as-
signed. After role classification, a set of simple
heuristics are applied in order to ensure that only
well formed annotations are output.
We designed our system on a per-predicate basis,
training one boundary classifier and a battery of role
classifiers for each predicate word. We clustered all
the senses of the same verb together and ended up
with 50 distinct boundary classifiers (one for each
target predicate word) and 619 role classifiers to rec-
ognize the 47 distinct role labels that appear in the
training set.
The remainder of this paper is structured as fol-
lows: Section 2 describes in some detail the archi-
tecture of our labeling system; Section 3 describes
the features that we use to represent the classifier
examples; Section 4 describes the experimental set-
ting and reports the accuracy of the system on the
SemEval2007 semantic role labeling closed task; fi-
nally, Section 5 discusses the results and presents
our conclusions.
2 System Description
Given a target predicate word in a natural language
sentence, a SRL system is meant to correctly iden-
tify all the arguments of the predicate. This problem
is usually divided in two sub-tasks: (a) the detection
of the boundaries (i. e. the word span) of each argu-
ment and (b) the classification of the argument type,
e.g. Arg0 or ArgM in PropBank or Agent and Goal
in FrameNet or VerbNet.
The standard approach to learn both the detection
and the classification of predicate arguments is sum-
marized by the following steps:
1 Given a sentence from the training-set, gener-
ate a full syntactic parse-tree;
2 let P and A be the set of predicates and the
set of parse-tree nodes (i.e. the potential argu-
ments), respectively;
3 for each pair ?p, a? ? P ?A:
3.1 extract the feature representation set, Fp,a;
3.2 if the sub-tree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
For instance, in Figure 1.a, for each combination
of the predicate approve with any other tree node a
288
that does not overlap with the predicate, a classifier
example Fapprove,a is generated. If a exactly covers
one of the predicate arguments (in this case: ?The
charter?, ?by the EC Commission? or ?on Sept. 21?)
it is regarded as a positive instance, otherwise it will
be a negative one, e. g. Fapprove,(NN charter).
The T+ and T? sets are used to train the bound-
ary classifier. To train the role multi-class classifier,
T+ can be reorganized as positive T+argi and nega-
tive T?argi examples for each argument i. In this way,
an individual ONE-vs-ALL classifier for each argu-
ment i can be trained. We adopted this solution, ac-
cording to (Pradhan et al, 2005), since it is simple
and effective. In the classification phase, given an
unseen sentence, all its Fp,a are generated and clas-
sified by each individual role classifier. The role la-
bel associated with the maximum among the scores
provided by the individual classifiers is eventually
selected.
To make the annotations consistent with the un-
derlying linguistic model, we employ a few simple
heuristics to resolve the overlap situations that may
occur, e. g. both ?charter? and ?the charter? in Figure
1 may be assigned a role:
? if more than two nodes are involved, i. e. a node
d and two or more of its descendants ni are
classified as arguments, then assume that d is
not an argument. This choice is justified by pre-
vious studies (Moschitti et al, 2006b) showing
that the accuracy of classification is higher for
lower nodes;
? if only two nodes are involved, i. e. they dom-
inate each other, then keep the one with the
highest classification score.
3 Features for Semantic Role Labeling
We explicitly represent as attribute-value pairs the
following features of each Fp,a pair:
? Phrase Type, Predicate Word, Head Word, Po-
sition and Voice as defined in (Gildea and Juras-
fky, 2002);
? Partial Path, No Direction Path, Head Word
POS, First and Last Word/POS in Constituent
and SubCategorization as proposed in (Pradhan
et al, 2005);
a) S
NP
DT
The
NN
charter
VP
AUX
was
VP
VBN
approved
PP
IN
by
NP
DT
the
NNP
EC
NNP
Commission
PP
IN
on
NP
NNP
Sept.
CD
21
.
.
b) S
NP-B
DT
The
NN
charter
VP
VP
VBN-P
approved
VP
VBN-P
approved
PP-B
IN
by
NP
DT
the
NNP
EC
NNP
Commission
Cause
Experiencer ARGM-TMP
Figure 1: A sentence parse tree (a) and two example ASTm1
structures relative to the predicate approve (b).
Set Props T T+ T?
Train 15,838 793,104 45,157 747,947
Dev 1,606 75,302 4,291 71,011
Train - Dev 14,232 717,802 40,866 676,936
Table 1: Composition of the dataset in terms of: number of
annotations (Props); number of candidate argument nodes (T );
positive (T+) and negative (T?) boundary classifier examples.
? Syntactic Frame as designed in (Xue and
Palmer, 2004).
We also employ structured features derived by the
full parses in an attempt to capture relevant aspects
that may not be emphasized by the explicit feature
representation. (Moschitti et al, 2006a) and (Mos-
chitti et al, 2006b) defined several classes of struc-
tured features that were successfully employed with
tree kernels for the different stages of an SRL pro-
cess. Figure 1 shows an example of the ASTm1 struc-
tures that we used for both the boundary detection
and the role classification stages.
4 Experiments
In this section we discuss the setup and the results
of the experiments carried out on the dataset of the
SemEval2007 closed task on SRL.
289
Task Kernel(s) Precision Recall F?=1
BD poly 94.34% 71.26% 81.19poly + TK 92.89% 76.09% 83.65
BD + RC poly 88.72% 68.76% 77.47poly + TK 86.60% 72.40% 78.86
Table 2: SRL accuracy on the development test for the bound-
ary detection (BD) and the complete SRL task (BD+RC) using
the polynomial kernel alone (poly) or combined with a tree ker-
nel function (poly + TK).
4.1 Setup
The training set comprises 15,8381 training annota-
tions organized on a per-verb basis. In order to build
a development set (Dev), we sampled about one
tenth, i. e. 1,606 annotations, of the original train-
ing set. For the final evaluation on the test set (Test),
consisting of 3,094 annotations, we trained our clas-
sifiers on the whole training data. Statistics on the
dataset composition are shown in Table 1.
The evaluations were carried out with the SVM-
Light-TK2 software (Moschitti, 2004) which ex-
tends the SVM-Light package (Joachims, 1999)
with tree kernel functions. We used the default
polynomial kernel (degree=3) for the linear features
and a SubSet Tree (SST) kernel (Collins and Duffy,
2002) for the comparison of ASTm1 structured fea-
tures. The kernels are normalized and summed by
assigning a weight of 0.3 to the TK contribution.
Training all the 50 boundary classifiers and the
619 role classifiers on the whole dataset took about
4 hours on a 64 bits machine (2.2GHz, 1GB RAM)3.
4.2 Evaluation
All the evaluations were carried out using
the CoNLL2005 evaluator tool available at
http://www.lsi.upc.es/?srlconll/soft.html.
Table 2 shows the aggregate results on boundary
detection (BD) and the complete SRL task (BD+RC)
on the development set using the polynomial kernel
alone (poly) or in conjunction with the tree kernels
and structured features (poly+TK). For both tasks,
tree kernel functions do trigger automatic feature se-
1A bunch of unaligned annotations were removed from the
dataset.
2http://ai-nlp.info.uniroma2.it/moschitti/
3In order to have a faster development cycle, we only used
60k training examples to train the boundary classifier of the verb
say. The accuracy on this relation is still very high, as we mea-
sured an overall F1 of 87.18 on the development set and of 85.13
on the test set.
Role #TI Precision Recall F?=1
Ov(BD) 6931 87.09% 72.96% 79.40Ov(BD+RC) 81.58% 70.16% 75.44
ARG2 4 100.00% 25.00% 40.00
ARG3 17 61.11% 64.71% 62.86
ARG4 4 0.00% 0.00% 0.00
ARGM-ADV 188 55.14% 31.38% 40.00
ARGM-CAU 13 50.00% 23.08% 31.58
ARGM-DIR 4 100.00% 25.00% 40.00
ARGM-EXT 3 0.00% 0.00% 0.00
ARGM-LOC 151 51.66% 51.66% 51.66
ARGM-MNR 85 41.94% 15.29% 22.41
ARGM-PNC 28 38.46% 17.86% 24.39
ARGM-PRD 9 83.33% 55.56% 66.67
ARGM-REC 1 0.00% 0.00% 0.00
ARGM-TMP 386 55.65% 35.75% 43.53
Actor1 12 85.71% 50.00% 63.16
Actor2 1 100.00% 100.00% 100.00
Agent 2551 91.38% 77.34% 83.78
Asset 21 42.42% 66.67% 51.85
Attribute 17 60.00% 70.59% 64.86
Beneficiary 24 65.00% 54.17% 59.09
Cause 48 75.56% 70.83% 73.12
Experiencer 132 86.49% 72.73% 79.01
Location 12 83.33% 41.67% 55.56
Material 7 100.00% 14.29% 25.00
Patient 37 76.67% 62.16% 68.66
Patient1 20 72.73% 40.00% 51.61
Predicate 181 63.75% 56.35% 59.82
Product 106 70.79% 59.43% 64.62
R-ARGM-LOC 2 0.00% 0.00% 0.00
R-ARGM-MNR 2 0.00% 0.00% 0.00
R-ARGM-TMP 4 0.00% 0.00% 0.00
R-Agent 74 70.15% 63.51% 66.67
R-Experiencer 5 100.00% 20.00% 33.33
R-Patient 2 0.00% 0.00% 0.00
R-Predicate 1 0.00% 0.00% 0.00
R-Product 2 0.00% 0.00% 0.00
R-Recipient 8 100.00% 87.50% 93.33
R-Theme 7 75.00% 42.86% 54.55
R-Theme1 7 100.00% 85.71% 92.31
R-Theme2 1 50.00% 100.00% 66.67
R-Topic 14 66.67% 42.86% 52.17
Recipient 48 75.51% 77.08% 76.29
Source 25 65.22% 60.00% 62.50
Stimulus 21 33.33% 19.05% 24.24
Theme 650 79.22% 68.62% 73.54
Theme1 69 77.42% 69.57% 73.28
Theme2 60 74.55% 68.33% 71.30
Topic 1867 84.26% 82.27% 83.25
Table 3: Evaluation of the semantic role labeling accuracy on
the SemEval2007 - Task 17 test set using the poly + TK kernel.
Column #TI reports the number of instances of each role label
in the test set. Rows Ov(BD) and Ov(BD + RC) show the overall
accuracy on the boundary detection and the complete SRL task,
respectively.
lection and improve the polynomial kernel by 2.46
and 1.39 F1 points, respectively.
The SRL accuracy for each one of the 47 dis-
tinct role labels is shown in Table 3. Column 2 lists
290
the number of instances of each role in the test set.
Many roles have very few positive examples both in
the training and the test sets, and therefore have little
or no impact on the overall accuracy which is domi-
nated by the few roles which are very frequent, such
as Theme, Agent, Topic and ARGM-TMP which ac-
count for almost 80% of all the test roles.
5 Final Remarks
In this paper we presented a system that employs
tree kernels and a basic set of flat features for the
classification of thematic roles.
We adopted a very simple approach that is meant
to be as general and fast as possible. The issue
of generality is addressed by training the bound-
ary and role classifiers on a per-predicate basis and
by employing tree kernel and structured features in
the learning algorithm. The resulting architecture
can indeed be used to learn the classification of
roles of non-verbal predicates as well, and the au-
tomatic feature selection triggered by the tree kernel
should compensate for the lack of ad-hoc, well es-
tablished explicit features for some classes of non-
verbal predicates, e. g. adverbs or prepositions.
Splitting the learning problem also has the clear
advantage of noticeably improving the efficiency of
the classifiers, thus reducing training and classifica-
tion time. On the other hand, this split results in
some classifiers having too few training instances
and therefore being very inaccurate. This is espe-
cially true for the boundary classifiers, which con-
versely need to be very accurate in order to posi-
tively support the following stages of the SRL pro-
cess. The solution of a monolithic boundary classi-
fier that we previously employed (Moschitti et al,
2006b) is noticeably more accurate though much
less efficient, especially for training. Indeed, after
the SemEval2007 evaluation period was over, we
ran another experiment using a monolithic boundary
classifier. On the test set, we measured F1 values of
82.09 vs 79.40 and 77.17 vs 75.44 for the boundary
detection and the complete SRL tasks, respectively.
Although it was provided as part of both the train-
ing and test data, we chose not to use the verb sense
information. This choice is motivated by our in-
tention to depend on as less external resources as
possible in order to be able to port our SRL system
to other linguistic models and languages, for which
such resources may not exist. Still, identifying the
predicate sense is a key issue especially for role clas-
sification, as the argument structure of a predicate is
largely determined by its sense. In the near feature
we plan to use larger structured features, i. e. span-
ning all the potential arguments of a predicate, to
improve the accuracy of our role classifiers.
Acknowledgments
The development of the SRL system was carried out
at the University of Rome Tor Vergata and financed
by the EU project PrestoSpace4 (FP6-507336).
References
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL02.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic label-
ing of semantic roles. Computational Linguistic, 28(3):496?
530.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In Proceedings
of AAAI-2000 Seventeenth National Conference on Artificial
Intelligence, Austin, TX.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2006a. Semantic role labeling via tree kernel joint inference.
In Proceedings of the Tenth Conference on Computational
Natural Language Learning, CoNLL-X.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2006b. Tree kernel engineering in semantic role labeling
systems. In Proceedings of the Workshop on Learning Struc-
tured Information in Natural Language Applications, EACL
2006, pages 49?56, Trento, Italy, April. European Chapter
of the Association for Computational Linguistics.
Alessandro Moschitti. 2004. A study on convolution kernel
for shallow semantic parsing. In proceedings of ACL-2004,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Support vector
learning for semantic argument classification. to appear in
Machine Learning Journal.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP 2004,
pages 88?94, Barcelona, Spain, July.
4http://www.prestospace.org
291
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 34?41,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Annotating Spoken Dialogs: from Speech Segments to Dialog Acts and
Frame Semantics
Marco Dinarelli, Silvia Quarteroni, Sara Tonelli, Alessandro Moschitti, Giuseppe Riccardi?
University of Trento
38050 Povo - Trento, Italy
{dinarelli,silviaq,moschitti,riccardi}@disi.unitn.it, satonelli@fbk.eu
Abstract
We are interested in extracting semantic
structures from spoken utterances gener-
ated within conversational systems. Cur-
rent Spoken Language Understanding sys-
tems rely either on hand-written seman-
tic grammars or on flat attribute-value se-
quence labeling. While the former ap-
proach is known to be limited in coverage
and robustness, the latter lacks detailed re-
lations amongst attribute-value pairs. In
this paper, we describe and analyze the hu-
man annotation process of rich semantic
structures in order to train semantic statis-
tical parsers. We have annotated spoken
conversations from both a human-machine
and a human-human spoken dialog cor-
pus. Given a sentence of the transcribed
corpora, domain concepts and other lin-
guistic features are annotated, ranging
from e.g. part-of-speech tagging and con-
stituent chunking, to more advanced anno-
tations, such as syntactic, dialog act and
predicate argument structure. In particu-
lar, the two latter annotation layers appear
to be promising for the design of complex
dialog systems. Statistics and mutual in-
formation estimates amongst such features
are reported and compared across corpora.
1 Introduction
Spoken language understanding (SLU) addresses
the problem of extracting and annotating the
meaning structure from spoken utterances in the
context of human dialogs (De Mori et al, 2008).
In spoken dialog systems (SDS) most used models
of SLU are based on the identification of slots (en-
?This work was partially funded by the European Com-
mission projects LUNA (contract 33549) and ADAMACH
(contract 022593).
tities) within one or more frames (frame-slot se-
mantics) that is defined by the application. While
this model is simple and clearly insufficient to
cope with interpretation and reasoning, it has sup-
ported the first generation of spoken dialog sys-
tems. Such dialog systems are thus limited by the
ability to parse semantic features such as predi-
cates and to perform logical computation in the
context of a specific dialog act (Bechet et al,
2004). This limitation is reflected in the type of
human-machine interactions which are mostly di-
rected at querying the user for specific slots (e.g.
?What is the departure city??) or implementing
simple dialog acts (e.g. confirmation). We believe
that an important step in overcoming such limita-
tion relies on the study of models of human-human
dialogs at different levels of representation: lexi-
cal, syntactic, semantic and discourse.
In this paper, we present our results in address-
ing the above issues in the context of the LUNA
research project for next-generation spoken dialog
interfaces (De Mori et al, 2008). We propose
models for different levels of annotation of the
LUNA spoken dialog corpus, including attribute-
value, predicate argument structures and dialog
acts. We describe the tools and the adaptation of
off-the-shelf resources to carry out annotation of
the predicate argument structures (PAS) of spoken
utterances. We present a quantitative analysis of
such semantic structures for both human-machine
and human-human conversations.
To the best of our knowledge this is the first
(human-machine and human-human) SDS corpus
denoting a multilayer approach to the annotation
of lexical, semantic and dialog features, which al-
lows us to investigate statistical relations between
the layers such as shallow semantic and discourse
features used by humans or machines. In the fol-
lowing sections we describe the corpus, as well as
a quantitative analysis and statistical correlations
between annotation layers.
34
2 Annotation model
Our corpus is planned to contain 1000 equally
partitioned Human-Human (HH) and Human-
Machine (HM) dialogs. These are recorded by
the customer care and technical support center of
an Italian company. While HH dialogs refer to
real conversations of users engaged in a problem
solving task in the domain of software/hardware
troubleshooting, HM dialogs are acquired with a
Wizard of Oz approach (WOZ). The human agent
(wizard) reacts to user?s spontaneous spoken re-
quests following one of ten possible dialog scenar-
ios inspired by the services provided by the com-
pany.
The above data is organized in transcrip-
tions and annotations of speech based on a new
multi-level protocol studied specifically within the
project, i.e. the annotation levels of words, turns1,
attribute-value pairs, dialog acts, predicate argu-
ment structures. The annotation at word level
is made with part-of-speech and morphosyntac-
tic information following the recommendations of
EAGLES corpora annotation (Leech and Wilson,
2006). The attribute-value annotation uses a pre-
defined domain ontology to specify concepts and
their relations. Dialog acts are used to annotate in-
tention in an utterance and can be useful to find
relations between different utterances as the next
section will show. For predicate structure annota-
tion, we followed the FrameNet model (Baker et
al., 1998) (see Section 2.2).
2.1 Dialog Act annotation
Dialog act annotation is the task of identifying
the function or goal of a given utterance (Sinclair
and Coulthard, 1975): thus, it provides a comple-
mentary information to the identification of do-
main concepts in the utterance, and a domain-
independent dialog act scheme can be applied.
For our corpus, we used a dialog act taxonomy
which follows initiatives such as DAMSL (Core
and Allen, 1997), TRAINS (Traum, 1996) and
DIT++ (Bunt, 2005). Although the level of granu-
larity and coverage varies across such taxonomies,
a careful analysis leads to identifying three main
groups of dialog acts:
1. Core acts, which represent the fundamen-
tal actions performed in the dialog, e.g. re-
1A turn is defined as the interval when a speaker is active,
between two pauses in his/her speech flow.
questing and providing information, or exe-
cuting a task. These include initiatives (often
called forward-looking acts) and responses
(backward-looking acts);
2. Conventional/Discourse management acts,
which maintain dialog cohesion and delimit
specific phases, such as opening, continua-
tion, closing, and apologizing;
3. Feedback/Grounding acts,used to elicit and
provide feedback in order to establish or re-
store a common ground in the conversation.
Our taxonomy, following the same three-fold
partition, is summarized in Table 1.
Table 1: Dialog act taxonomy
Core dialog acts
Info-request Speaker wants information from ad-
dressee
Action-request Speaker wants addressee to perform
an action
Yes-answer Affirmative answer
No-answer Negative answer
Answer Other kinds of answer
Offer Speaker offers or commits to perform
an action
ReportOnAction Speaker notifies an action is being/has
been performed
Inform Speaker provides addressee with in-
formation not explicitly required (via
an Info-request)
Conventional dialog acts
Greet Conversation opening
Quit Conversation closing
Apology Apology
Thank Thanking (and down-playing)
Feedback/turn management dialog acts
Clarif-request Speaker asks addressee for confirma-
tion/repetition of previous utterance
for clarification.
Ack Speaker expresses agreement with
previous utterance, or provides feed-
back to signal understanding of what
the addressee said
Filler Utterance whose main goal is to man-
age conversational time (i.e. dpeaker
taking time while keeping the turn)
Non-interpretable/non-classifiable dialog acts
Other Default tag for non-interpretable and
non-classifiable utterances
It can be noted that we have decided to retain
only the most frequent dialog act types from the
schemes that inspired our work. Rather than as-
piring to the full discriminative power of possible
conversational situations, we have opted for a sim-
ple taxonomy that would cover the vast majority
35
of utterances and at the same time would be able
to generalize them. Its small number of classes is
meant to allow a supervised classification method
to achieve reasonable performance with limited
data. The taxonomy is currently used by the sta-
tistical Dialogue Manager in the ADAMACH EU
project (Varges et al, 2008); the limited number
of classes allows to reduce the number of hypoth-
esized current dialogue acts, thus reducing the di-
alogue state space.
Dialog act annotation was performed manually
by a linguist on speech transcriptions previously
segmented into turns as mentioned above. The an-
notation unit for dialog acts, is the utterance; how-
ever, utterances are complex semantic entities that
do not necessarily correspond to turns. Hence, a
segmentation of the dialog transcription into ut-
terances was performed by the annotator before
dialog act labeling. Both utterance segmentation
and dialog act labeling were performed through
the MMAX tool (Mu?ller and Strube, 2003).
The annotator proceeded according to the fol-
lowing guidelines:
1. by default, a turn is also an utterance;
2. if more than one tag is applicable to an ut-
terance, choose the tag corresponding to its
main function;
3. in case of doubt among several tags, give pri-
ority to tags in core dialog acts group;
4. when needed, split the turn into several utter-
ances or merge several turns into one utter-
ance.
Utterance segmentation provides the basis not
only for dialog act labeling but also for the other
semantic annotations. See Fig. 1 for a dialog sam-
ple where each line represents an utterance anno-
tated according to the three levels.
2.2 Predicate Argument annotation
We carried out predicate argument structure an-
notation applying the FrameNet paradigm as de-
scribed in (Baker et al, 1998). This model
comprises a set of prototypical situations called
frames, the frame-evoking words or expressions
called lexical units and the roles or participants in-
volved in these situations, called frame elements.
The latter are typically the syntactic dependents of
the lexical units. All lexical units belonging to
the same frame have similar semantics and show
                                              PERSON-NAME 
Info: Buongiorno, sono   Paola.  
  
          GREETING    B._NAMED Name 
Good morning, this is Paola. 
 
Info-req: Come la posso aiutare? 
                      
                    Benefitted_party   ASSISTANCE 
How may I help you? 
 
                                                       CONCEPT         HARDWARE-COMPONENT 
Info: Buongiorno. Ho un problema con la stampante.  
 
          GREETING            PR._DESCRIPTION     Affected_device 
Good morning. I have a problem with the printer. 
 
           PART-OF-DAY   NEGAT. ACTION                ACTION 
Info: Da stamattina non   riesco pi? a  stampare 
                                       
                                    Problem 
Since this morning I can?t print. 
 
Info-req:   Mi  pu?  dire   nome e cognome per favore? 
 
              Addressee      TELLING               Message 
Can you tell me your name and surname, please? 
 
                                       PERSON-NAME  PERSON-SURNAME 
Answer: Mi chiamo  Alessandro  Manzoni. 
 
               Entity B._NAMED                   Name 
My name is Alessandro Manzoni. 
Figure 1: Annotated dialog extract. Each utterance
is preceded by dialog act annotation. Attribute-
value annotation appears above the text, PAS an-
notation below the text.
the same valence. A particular feature of the
FrameNet project both for English and for other
languages is its corpus-based nature, i.e. every el-
ement described in the resource has to be instanti-
ated in a corpus. To annotate our SDS corpus, we
adopted where possible the already existing frame
and frame element descriptions defined for the En-
glish FrameNet project, and introduced new def-
initions only in case of missing elements in the
original model.
Figure 1 shows a dialog sample with PAS an-
notation reported below the utterance. All lexi-
cal units are underlined and the frame is written in
capitals, while the other labels refer to frame el-
ements. In particular, ASSISTANCE is evoked by
the lexical unit aiutare and has one attested frame
element (Benefitted party), GREETING has no
frame element, and PROBLEM DESCRIPTION
and TELLING have two frame elements each.
Figure 2 gives a comprehensive view of the an-
notation process, from audio file transcription to
the annotation of three semantic layers. Whereas
36
Figure 2: The annotation process
Audio file 
Turn segmentation & 
Transcription 
Utterance segmentation 
POS tagging Domain attribute 
annotation 
PAS annotation 
Dialog Act 
annotation 
Syntactic parsing 
attribute-value and DA annotation are carried
out on the segmented dialogs at utterance level,
PAS annotation requires POS-tagging and syntac-
tic parsing (via Bikel?s parser trained for Italian
(Corazza et al, 2007)). Finally, a shallow manual
correction is carried out to make sure that the tree
nodes that may carry semantic information have
correct constituent boundaries. For the annotation
of frame information, we used the Salto tool (Bur-
chardt et al, 2006), that stores the dialog file in
TIGER-XML format and allows to easily intro-
duce word tags and frame flags. Frame informa-
tion is recorded on top of parse trees, with target
information pointing to terminal words and frame
elements pointing to tree nodes.
3 Quantitative comparison of the
Annotation
We evaluated the outcome of dialog act and
PAS annotation levels on both the human-human
(henceforth HH) and human-machine (HM) cor-
pora by not only analyzing frequencies and occur-
rences in the separate levels, but also their interac-
tion, as discussed in the following sections.
3.1 Dialog Act annotation
Analyzing the annotation of 50 HM and 50 HH
dialogs at the dialog act level, we note that an
HH dialog is composed in average by 48.9?17.4
(standard deviation) dialog acts, whereas a HM
dialog is composed of 18.9?4.4. The difference
between average lengths shows how HH sponta-
neous speech can be redundant, while HM dialogs
are more limited to an exchange of essential infor-
mation. The standard deviation of a conversation
in terms of dialog acts is considerably higher in
the HH corpus than in the HM one. This can be ex-
plained by the fact that the WOZ follows a unique,
previously defined task-solving strategy that does
not allow for digressions. Utterance segmentation
was also performed differently on the two corpora.
In HH we performed 167 turn mergings and 225
turn splittings; in HM dialogs, only turn splittings
(158) but no turn mergings were performed.
Tables 2 and 3 report the dialog acts occurring
in the HM and HH corpora, respectively, ranked
by their frequencies.
Table 2: Dialog acts ranked by frequency in the
human-machine (HM) corpus
human-machine (HM)
DA count rel. freq.
Info-request 249 26.3%
Answer 171 18.1%
Inform 163 17.2%
Yes-answer 70 7.4%
Quit 60 6.3%
Thank 56 5.9%
Greet 50 5.3%
Offer 49 5.2%
Clarification-request 26 2.7%
Action-request 25 2.6%
Ack 12 1.3%
Filler 6 0.6%
No-answer 5 0.5%
Other, ReportOnAction 2 0.2%
Apology 1 0.1%
TOTAL 947
From a comparative analysis, we note that:
1. info-request is by far the most common dia-
log act in HM, whereas in HH ack and info
share the top ranking position;
2. the most frequently occurring dialog act in
HH, i.e. ack, is only ranked 11th in HM;
3. the relative frequency of clarification-request
(4,7%) is considerably higher in HH than in
HM.
We also analyzed the ranking of the most fre-
quent dialog act bigrams in the two corpora. We
can summarize our comparative analysis, reported
in Table 4, to the following: in both corpora,
most bigram types contain info and info-request,
37
Table 3: Dialog acts ranked by frequency in the
human-human (HH) corpus
human-human (HH)
DA count rel. freq.
Ack 582 23.8%
Inform 562 23.0%
Info-request 303 12.4%
Answer 192 7.8%
Clarification-request 116 4.7%
Offer 114 4.7%
Yes-answer 112 4.6%
Quit 101 4.1%
ReportOnAction 91 3.7%
Other 70 2.9%
Action-request 69 2.8%
Filler 61 2.5%
Thank 33 1.3%
No-answer 26 1.1%
Greet, Apology 7 0.3%
TOTAL 2446
as expected in a troubleshooting system. How-
ever, the bigram info-request answer, which we
expected to form the core of a task-solving dia-
log, is only ranked 5th in the HH corpus, while 5
out of the top 10 bigram types contain ack. We
believe that this is because HH dialogs primarily
contain spontaneous information-providing turns
(e.g. several info info by the same speaker) and
acknowledgements for the purpose of backchan-
nel. Instead, HM dialogs, structured as sequences
of info-request answers pairs, are more minimal
and brittle, showing how users tend to avoid re-
dundancy when addressing a machine.
Table 4: The 10 most frequent dialog act bigrams
human-machine (HM) human-human (HH)
info-req answer ack info
answer info-req info ack
info info-req info info
info-req y-answer ack ack
sentence beginning greet info-req answer
greet info info info-req
info quit info-req y-answer
offer info ack info-req
thank info answer ack
y-answer thank quit sentence end
3.2 Predicate Argument annotation
We annotated 50 HM and 50 HH dialogs with
frame information. Differently from the English
FrameNet database, we didn?t annotate one frame
per sentence. On the contrary, we identified all
lexical units corresponding to ?semantically rele-
vant? verbs, nouns and adjectives with a syntac-
tic subcategorization pattern, eventually skipping
the utterances with empty semantics (e.g. dis-
fluencies). In particular, we annotated all lexical
units that imply an action, introduce the speaker?s
opinion or describe the office environment. We
introduced 20 new frames out of the 174 iden-
tified in the corpus because the original defini-
tion of frames related to hardware/software, data-
handling and customer assistance was sometimes
too coarse-grained. Few new frame elements were
introduced as well, mostly expressing syntactic re-
alizations that are typical of spoken Italian.
Table 5 shows some statistics about the cor-
pus dimension and the results of our annotation.
The human-human dialogs contain less frame in-
stances in average than the human-machine group,
meaning that speech disfluencies, not present in
turns uttered by the WOZ, negatively affect the se-
mantic density of a turn. For the same reason, the
percentage of turns in HH dialogs that were manu-
ally corrected in the pre-processing step (see Sec-
tion 2.2) is lower than for HM turns, since HH di-
alogs have more turns that are semantically empty
and that were skipped in the correction phase. Be-
sides, HH dialogs show a higher frame variabil-
ity than HM, which can be explained by the fact
that spontaneous conversation may concern mi-
nor topics, whereas HM dialogs follow a previ-
ously defined structure, designed to solve soft-
ware/hardware problems.
Tables 6 and 7 report the 10 most frequent
frames occurring in the human-machine resp.
human-human dialogs. The relative frame fre-
quency in HH dialogs is more sparse than in HM
dialogs, meaning that the task-solving strategy fol-
lowed by the WOZ limits the number of digres-
sions, whereas the semantics of HH dialogs is
richer and more variable.
As mentioned above, we had to introduce and
define new frames which were not present in the
original FrameNet database for English in order to
capture all relevant situations described in the di-
alogs. A number of these frames appear in both
tables, suggesting that the latter are indeed rel-
38
Table 5: Dialog turn and frame statistics for the
human-machine (HM) resp. human-human (HH)
corpus
HM HH
Total number of turns 662 1,997
Mean dialog length (turns) 13.2 39.9
Mean turn length (tokens) 11.4 10.8
Corrected turns (%) 50 39
Total number of annotations 923 1951
Mean number of frame annota-
tions per dialog
18.5 39.0
Mean number of frame elements
per frame annotation
1.6 1.7
evant to model the general semantics of the di-
alogs we are approaching. The most frequent
frame group comprises frames relating to infor-
mation exchange that is typical of the help-desk
activity, including Telling, Greeting, Contacting,
Statement, Recording, Communication. Another
relevant group encompasses frames related to the
operational state of a device, for example Be-
ing operational, Change operational state, Oper-
ational testing, Being in operation.
The two groups also show high variability of
lexical units. Telling, Change operational state
and Greeting have the richest lexical unit set,
with 11 verbs/nouns/adjectives each. Arriving
and Awareness are expressed by 10 different lexi-
cal units, while Statement, Being operational, Re-
moving and Undergo change of operational state
have 9 different lexical units each. The informal
nature of the spoken dialogs influences the com-
position of the lexical unit sets. In fact, they are
rich in verbs and multiwords used only in collo-
quial contexts, for which there are generally few
attestations in the English FrameNet database.
Similarly to the dialog act statistics, we also
analyzed the most frequent frame bigrams and
trigrams in HM and HH dialogs. Results are
reported in Tables 8 and 9. Both HH bigrams
and trigrams show a more sparse distribution and
lower relative frequency than HM ones, implying
that HH dialogs follow a more flexible structure
with a richer set of topics, thus the sequence of
themes is less predictable. In particular, 79%
of HH bigrams and 97% of HH trigrams occur
only once (vs. 68% HM bigrams and 82% HM
trigrams). On the contrary, HM dialogs deal with
Table 6: The 10 most frequent frames in the HM
corpus (* =newly introduced)
HM corpus
Frame count freq-%
Greeting* 146 15.8
Telling 134 14.5
Recording 83 8.9
Being named 74 8.0
Contacting 52 5.6
Usefulness 50 5.4
Being operational 28 3.0
Problem description* 24 2.6
Inspecting 24 2.6
Perception experience 21 2.3
Table 7: The 10 most frequent frames in the HH
corpus (* =newly introduced)
HH corpus
Frame count freq-%
Telling 143 7.3
Greeting* 124 6.3
Awareness 74 3.8
Contacting 63 3.2
Giving 62 3.2
Navigation* 61 3.1
Change operational state 51 2.6
Perception experience 46 2.3
Insert data* 46 2.3
Come to sight* 38 1.9
a fix sequence of topics driven by the turns uttered
by the WOZ. For instance, the most frequent
HM bigram and trigram both correspond to the
opening utterance of the WOZ:
Help desk buongiornoGREETING, sonoBEING NAMED
Paola, in cosa posso esserti utileUSEFULNESS?
(Good morning, help-desk service, Paola speaking, how can
I help you?)
3.3 Mutual information between PAS and
dialog acts
A unique feature of our corpus is the availabil-
ity of both a semantic and a dialog act annota-
tion level: it is intuitive to seek relationships in
the purpose of improving the recognition and un-
derstanding of each level by using features from
the other. We considered a subset of 20 HH and
50 HM dialogs and computed an initial analysis
39
Table 8: The 5 most frequent frame bigrams
human-machine (HM) freq-%
Greeting Being named 17.1
Being named Usefulness 15.3
Telling Recording 12.9
Recording Contacting 10.9
Contacting Greeting 10.6
human-human (HH) freq-%
Greeting Greeting 4.7
Navigation Navigation 1.2
Telling Telling 1.0
Change op. state Change op. state 0.9
Telling Problem description 0.8
Table 9: The 5 most frequent frame trigrams
human-machine (HM) freq-%
Greeting Being named Usefulness 9.5
Recording Contacting Greeting 5.7
Being named Usefulness Greeting 3.7
Telling Recording Contacting 3.5
Telling Recording Recording 2.2
human-human (HH) freq-%
Greeting Greeting Greeting 1.6
Greeting Being named Greeting 0.5
Contacting Greeting Greeting 0.3
Navigation Navigation Navigation 0.2
Working on Greeting Greeting 0.2
of the co-occurrences of dialog acts and PAS. We
noted that each PAS tended to co-occur only with a
limited subset of the available dialog act tags, and
moreover in most cases the co-occurrence hap-
pened with only one dialog act. For a more thor-
ough analysis, we computed the weighted condi-
tional entropy between PAS and dialog acts, which
yields a direct estimate of the mutual information
between the two levels of annotation2.
2Let H(yj |xi) be the weighted conditional entropy of ob-
servation yj of variable Y given observation xi of variable
X:
H(yj |xi) = ?p(xi; yj)log
p(xi; yj)
p(xi)
,
where p(xi; yj) is the probability of co-occurrence of xi and
yj , and p(xi) and p(yj) are the marginal probabilities of oc-
currence of xi resp. yj in the corpus. There is an obvious re-
lation with the weighted mutual information between xi and
yj , defined following e.g. (Bechet et al, 2004) as:
wMI(xi; yj) = p(xi; yj)log
p(xi; yj)
p(xi)p(yj)
.
(a) human-machine dialogs (filtering co-occurrences below 3)
(b) human-human dialogs (filtering co-occurrences below 5)
Figure 3: Weighted conditional entropy between
PAS and dialog acts in the HM (a) and HH corpus
(b). To lower entropies correspond higher values
of mutual information (darker color in the scale)
Our results are illustrated in Figure 3. In the
HM corpus (Fig. 3(a)), we noted some interesting
associations between dialog acts and PAS. First,
info-req has the maximal MI with PAS like Be-
ing in operation and Being attached, as requests
are typically used by the operator to get informa-
tion about the status of device. Several PAS de-
note a high MI with the info dialog act, includ-
ing Activity resume, Information, Being named,
Contacting, and Resolve problem. Contacting
refers to the description of the situation and of the
speaker?s point of view (usually the caller). Be-
ing named is primarily employed when the caller
introduces himself, while Activity resume usually
refers to the operator?s description of the sched-
Indeed, the higher is H(yj |xi), the lower is wMI(xi; yj).
We approximate all probabilities using frequency of occur-
rence.
40
uled interventions.
As for the remaining acts, clarif has the high-
est MI with Perception experience and Statement,
used to warn the addressee about understanding
problems and asking him to repeat/rephrase an ut-
terance, respectively. The two strategies can be
combined in the same utterance, as in the utter-
ance: Non ho sentito bene: per favore ripeti cer-
cando di parlare piu` forte. (I haven?t quite heard
that, please repeat trying to speak up.).
The answer tag is highly informative with Suc-
cessful action, Change operational state, Becom-
ing nonfunctional, Being detached, Read data.
These PAS refer to the exchange of infor-
mation (Read data) or to actions performed
by the user after a suggestion of the system
(Change operational state). Action requests (act-
req) seem to be correlated to Replacing as it usu-
ally occurs when the operator requests the caller
to carry out an action to solve a problem, typically
to replace a component with another. Another fre-
quent request may refer to some device that the
operator has to test.
In the HH corpus (Fig. 3(b)), most of the PAS
are highly mutually informative with info: in-
deed, as shown in Table 3, this is the most fre-
quently occurring act in HH except for ack, which
rarely contain verbs that can be annotated by a
frame. As for the remaining acts, there is an easily
explainable high MI between quit and Greeting;
moreover, info-req denote its highest MI with
Giving, as in requests to give information, while
rep-action denotes a strong co-occurrence with
Inchoative attaching: indeed, interlocutors often
report on the action of connecting a device.
These results corroborate our initial observation
that for most PAS, the mutual information tends
to be very high in correspondence of one dialog
act type: this suggests the beneficial effect of in-
cluding shallow semantic information as features
for dialog act classification. The converse is less
clear as the same dialog act can relate to a span
of words covered by multiple PAS and generally,
several PAS co-occur with the same dialog act.
4 Conclusions
In this paper we have proposed an approach to
the annotation of spoken dialogs using seman-
tic and discourse features. Such effort is crucial
to investigate the complex dependencies between
the layers of semantic processing. We have de-
signed the annotation model to incorporate fea-
tures and models developed both in the speech
and language research community and bridging
the gap between the two communities. Our multi-
layer annotation corpus allows the investigation
of cross-layer dependencies and across human-
machine and human-human dialogs as well as
training of semantic models which accounts for
predicate interpretation.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
ACL/Coling?98, pages 86?90.
F. Bechet, G. Riccardi, and D. Hakkani-Tur. 2004.
Mining spoken dialogue corpora for system evalu-
ation and modeling. In Proceedings of EMNLP?04,
pages 134?141.
H. Bunt. 2005. A framework for dialogue act specica-
tion. In Proceedings of SIGSEM WG on Represen-
tation of Multimodal Semantic Information.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. Salto - a versatile multi-
level annotation tool. In Proceedings of LREC 2006,
pages 517?520, Genoa, Italy.
A. Corazza, A. Lavelli, and G. Satta. 2007. Anal-
isi sintattica-statistica basata su costituenti. Intelli-
genza Artificiale, 4(2):38?39.
M. G. Core and J. F. Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communicative
Actions in Humans and Machines.
R. De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,
G. Riccardi, and G. Tur. 2008. Spoken language
understanding: A survey. IEEE Signal Processing
magazine, 25(3):50?58.
G. Leech and A. Wilson. 2006. EAGLES recommen-
dations for the morphosyntactic annotation of cor-
pora. Technical report, ILC-CNR.
C. Mu?ller and M. Strube. 2003. Multi-level annotation
in MMAX. In Proceedings of SIGDIAL?03.
J. M. Sinclair and R. M. Coulthard. 1975. Towards an
Analysis of Discourse: The English Used by Teach-
ers and Pupils. Oxford University Press, Oxford.
D. Traum. 1996. Conversational agency: The
TRAINS-93 dialogue manager. In Proceedings of
TWLT 11: Dialogue Management in Natural Lan-
guage Systems, pages 1?11, June.
S. Varges, G. Riccardi, and S. Quarteroni. 2008. Per-
sistent information state in a data-centric architec-
ture. In Proceedings of SIGDIAL?08.
41
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 30?38,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Linearization of Tree Kernel Functions
Daniele Pighin
FBK-Irst, HLT
Via di Sommarive, 18 I-38100 Povo (TN) Italy
pighin@fbk.eu
Alessandro Moschitti
University of Trento, DISI
Via di Sommarive, 14 I-38100 Povo (TN) Italy
moschitti@disi.unitn.it
Abstract
The combination of Support Vector Machines
with very high dimensional kernels, such as
string or tree kernels, suffers from two ma-
jor drawbacks: first, the implicit representa-
tion of feature spaces does not allow us to un-
derstand which features actually triggered the
generalization; second, the resulting compu-
tational burden may in some cases render un-
feasible to use large data sets for training. We
propose an approach based on feature space
reverse engineering to tackle both problems.
Our experiments with Tree Kernels on a Se-
mantic Role Labeling data set show that the
proposed approach can drastically reduce the
computational footprint while yielding almost
unaffected accuracy.
1 Introduction
The use of Support Vector Machines (SVMs)
in supervised learning frameworks is spreading
across different communities, including Computa-
tional Linguistics and Natural Language Processing,
thanks to their solid mathematical foundations, ef-
ficiency and accuracy. Another important reason
for their success is the possibility of using kernel
functions to implicitly represent examples in some
high dimensional kernel space, where their similar-
ity is evaluated. Kernel functions can generate a very
large number of features, which are then weighted
by the SVM optimization algorithm obtaining a fea-
ture selection side-effect. Indeed, the weights en-
coded by the gradient of the separating hyperplane
learnt by the SVM implicitly establish a ranking be-
tween features in the kernel space. This property has
been exploited in feature selection models based on
approximations or transformations of the gradient,
e.g. (Rakotomamonjy, 2003), (Weston et al, 2003)
or (Kudo and Matsumoto, 2003).
However, kernel based systems have two major
drawbacks: first, new features may be discovered
in the implicit space but they cannot be directly ob-
served. Second, since learning is carried out in the
dual space, it is not possible to use the faster SVM or
perceptron algorithms optimized for linear spaces.
Consequently, the processing of large data sets can
be computationally very expensive, limiting the use
of large amounts of data for our research or applica-
tions.
We propose an approach that tries to fill in the
gap between explicit and implicit feature represen-
tations by 1) selecting the most relevant features in
accordance with the weights estimated by the SVM
and 2) using these features to build an explicit rep-
resentation of the kernel space. The most innovative
aspect of our work is the attempt to model and im-
plement a solution in the context of structural ker-
nels. In particular we focus on Tree Kernel (TK)
functions, which are especially interesting for the
Computational Linguistics community as they can
effectively encode rich syntactic data into a kernel-
based learning algorithm. The high dimensionality
of a TK feature space poses interesting challenges in
terms of computational complexity that we need to
address in order to come up with a viable solution.
We will present a number of experiments carried
out in the context of Semantic Role Labeling, show-
ing that our approach can noticeably reduce training
time while yielding almost unaffected classification
accuracy, thus allowing us to handle larger data sets
at a reasonable computational cost.
The rest of the paper is structured as follows: Sec-
30
Fragment space
A
B A
A
B A
B A
A
B A
C
A
B A
B A
C
A
C
D
B A
D
B A
C
1 2 3 4 5 6 7
T1
A
B A
B A
C
T2
D
B A
C
?(T1) = [2, 1, 1, 1, 1, 0, 0]
?(T2) = [0, 0, 0, 0, 1, 1, 1]
K(T1, T2) = ??(T1), ?(T2)? = 1
Figure 1: Esemplification of a fragment space and the
kernel product between two trees.
tion 2 will briefly review SVMs and Tree Kernel
functions; Section 3 will detail our proposal for the
linearization of a TK feature space; Section 4 will
review previous work on related subjects; Section 5
will describe our experiments and comment on their
results; finally, in Section 6 we will draw our con-
clusions.
2 Tree Kernel Functions
The decision function of an SVM is:
f(~x) = ~w ? ~x+ b =
n?
i=1
?iyi ~xi ? ~x+ b (1)
where ~x is a classifying example and ~w and b are
the separating hyperplane?s gradient and its bias,
respectively. The gradient is a linear combination
of the training points ~xi, their labels yi and their
weights ?i. These and the bias are optimized at
training time by the learning algorithm. Applying
the so-called kernel trick it is possible to replace the
scalar product with a kernel function defined over
pairs of objects:
f(o) =
n?
i=1
?iyik(oi, o) + b
with the advantage that we do not need to provide
an explicit mapping ?(?) of our examples in a vector
space.
A Tree Kernel function is a convolution ker-
nel (Haussler, 1999) defined over pairs of trees.
Practically speaking, the kernel between two trees
evaluates the number of substructures (or fragments)
they have in common, i.e. it is a measure of their
overlap. The function can be computed recursively
in closed form, and quite efficient implementations
are available (Moschitti, 2006). Different TK func-
tions are characterized by alternative fragment defi-
nitions, e.g. (Collins and Duffy, 2002) and (Kashima
and Koyanagi, 2002). In the context of this paper
we will be focusing on the SubSet Tree (SST) ker-
nel described in (Collins and Duffy, 2002), which
relies on a fragment definition that does not allow to
break production rules (i.e. if any child of a node is
included in a fragment, then also all the other chil-
dren have to). As such, it is especially indicated for
tasks involving constituency parsed texts.
Implicitly, a TK function establishes a correspon-
dence between distinct fragments and dimensions in
some fragment space, i.e. the space of all the pos-
sible fragments. To simplify, a tree t can be repre-
sented as a vector whose attributes count the occur-
rences of each fragment within the tree. The ker-
nel between two trees is then equivalent to the scalar
product between pairs of such vectors, as exempli-
fied in Figure 1.
3 Mining the Fragment Space
If we were able to efficiently mine and store in a
dictionary all the fragments encoded in a model,
we would be able to represent our objects explicitly
and use these representations to train larger models
and very quick and accurate classifiers. What we
need to devise are strategies to make this approach
convenient in terms of computational requirements,
while yielding an accuracy comparable with direct
tree kernel usage.
Our framework defines five distinct activities,
which are detailed in the following paragraphs.
Fragment Space Learning (FSL) First of all, we
can partition our training data into S smaller sets,
and use the SVM and the SST kernel to learn S mod-
els. We will use the estimated weights to drive our
feature selection process. Since the time complexity
of SVM training is approximately quadratic in the
number of examples, this way we can considerably
accelerate the process of estimating support vector
weights.
According to statistical learning theory, being
trained on smaller subsets of the available data
these models will be less robust with respect to the
31
minimization of the empirical risk (Vapnik, 1998).
Nonetheless, since we do not need to employ them
for classification (but just to direct our feature se-
lection process, as we will describe shortly), we can
accept to rely on sub-optimal weights. Furthermore,
research results in the field of SVM parallelization
using cascades of SVMs (Graf et al, 2004) suggest
that support vectors collected from locally learnt
models can encode many of the relevant features re-
tained by models learnt globally. Henceforth, letMs
be the model associated with the s-th split, and Fs
the fragment space that can describe all the trees in
Ms.
Fragment Mining and Indexing (FMI) In Equa-
tion 1 it is possible to isolate the gradient ~w =?n
i=1 ?iyi ~xi, with ~xi = [x(1)i , . . . , x(N)i ], N being
the dimensionality of the feature space. For a tree
kernel function, we can rewrite x(j)i as:
x(j)i = ti,j?
`(fj)
?ti?
= ti,j?
`(fj)
??N
k=1(ti,k?`(fk))2
(2)
where: ti,j is the number of occurrences of the frag-
ment fj , associated with the j-th dimension of the
feature space, in the tree ti; ? is the kernel decay
factor; and `(fj) is the depth of the fragment.
The relevance |w(j)| of the fragment fj can be
measured as:
|w(j)| =
?????
n?
i=1
?iyix(j)i
????? . (3)
We fix a threshold L and from each model Ms
(learnt during FSL) we select the L most relevant
fragments, i.e. we build the set Fs,L = ?k{fk} so
that:
|Fs,L| = L and |w(k)| ? |w(i)|?fi ? F \ Fs,L .
In order to do so, we need to harvest all the frag-
ments with a fast extraction function, store them in
a compact data structure and finally select the frag-
ments with the highest relevance. Our strategy is ex-
emplified in Figure 2. First, we represent each frag-
ment as a sequence as described in (Zaki, 2002). A
sequence contains the labels of the fragment nodes
in depth-first order. By default, each node is the
child of the previous node in the sequence. A spe-
cial symbol (?) indicates that the next node in the
R1
A B
Z W
R2
X Y B
Z W
B
Z W
weight: w1 weight: w2 weight: w3
R1, A, ?, B, Z, ?, W
R2, X, ?, Y, ?, B, Z, ?, W
B, Z, ?, W
R1
w1A
? W
X R2w2
Z
Y
B
w3
1,1
1,2
2,11,1
1,1 1,-
1,1
1,3
3,1
1,2
1
2
Figure 2: Fragment indexing. Each fragment is repre-
sented as a sequence 1 and then encoded as a path in the
index 2 which keeps track of its cumulative relevance.
sequence should be attached after climbing one level
in the tree. For example, the tree (B (Z W)) in figure
is represented as the sequence [B, Z, ?, W]. Then, we
add the elements of the sequence to a graph (which
we call an index of fragments) where each sequence
becomes a path. The nodes of the index are the la-
bels of the fragment nodes, and each arc is associ-
ated with a pair of values ?d, n?: d is a node identi-
fier, which is unique with respect to the source node;
n is the identifier of the arc that must be selected at
the destination node in order to follow the path as-
sociated with the sequence. Index nodes associated
with a fragment root also have a field where the cu-
mulative relevance of the fragment is stored.
As an example, the index node labeled B in fig-
ure has an associated weight of w3, thus identify-
ing the root of a fragment. Each outgoing edge
univocally identifies an indexed fragment. In this
case, the only outgoing edge is labeled with the pair
?d = 1, n = 1?, meaning that we should follow it
to the next node, i.e. Z, and there select the edge la-
beled 1, as indicated by n. The edge with d = 1 in Z
is ?d = 1, n = 1?, so we browse to ? where we se-
lect the edge ?d = 1, n = ??. The missing value for
n tells us that the next node, W, is the last element
of the sequence. The complete sequence is then [B,
Z, ?, W], which encodes the fragment (B (Z W)).
The index implementation has been optimized for
fast insertions and has the following features: 1)
each node label is represented exactly once; 2) each
distinct sequence tail is represented exactly once.
The union of all the fragments harvested from each
model is then saved into a dictionary DL which will
be used by the next stage.
To mine the fragments, we apply to each tree in
each model the algorithm shown in Algorithm 3.1.
In this context, we call fragment expansion the pro-
32
Algorithm 3.1: MINE TREE(tree)
global maxdepth,maxexp
main
mined? ?; indexed? ?; MINE(FRAG(tree), 0)
procedure MINE(frag, depth)
if frag ? indexed
then return
indexed? indexed ? {frag}
INDEX(frag)
for each node ? TO EXPAND(frag)
do
?
?
?
if node 6? mined
then
{
mined? mined ? {node}
MINE(FRAG(node), 0)
if depth < maxdepth
then
{for each fragment ? EXPAND(frag,maxexp)
do MINE(fragment, depth+ 1)
cess by which tree nodes are included in a frag-
ment. Fragment expansion is achieved via node ex-
pansions, where expanding a node means includ-
ing its direct children in the fragment. The func-
tion FRAG(n) builds the basic fragment rooted in a
given node n, i.e. the fragment consisting only of n
and its direct children. The function TO EXPAND(f)
returns the set of nodes in a fragment f that can
be expanded (i.e. internal nodes in the origin tree),
while the function EXPAND(f,maxexp) returns all
the possible expansions of a fragment f . The pa-
rameter maxexp is a limit to the number of nodes
that can be expanded at the same time when a new
fragment is generated, while maxdepth sets a limit
on the number of times that a base fragment can be
expanded. The function INDEX(f) adds the frag-
ment f to the index. To keep the notation simple,
here we assume that a fragment f contains all the
necessary information to calculate its relevance (i.e.
the weight, label and norm of the support vector ?i,
yi, and ?ti?, the depth of the fragment `(f) and the
decay factor ?, see equations 2 and 3).
Performing in a different order the same node ex-
pansions on the same fragment f results in the same
fragment f ?. To prevent the algorithm from entering
circular loops, we use the set indexed so that the
very same fragment in each tree cannot be explored
more than once. Similarly, the mined set is used
so that the base fragment rooted in a given node is
considered only once.
Tree Fragment Extraction (TFX) During this
phase, a data file encoding label-tree pairs ?yi, ti? is
S
NP
NNP
Mary
VP
VB
bought
NP
D
a
NN
cat
(A1)
(A0)
?
VP
VB-P
bought
NP
D-B
a
VP
VB-P
bought
NP-B
D
a
NN
cat
-1: BC +1: BC,A1
-1: A0,A2,A3,A4,A5
Figure 3: Examples of ASTm structured features.
transformed to encode label-vector pairs ?yi, ~vi?. To
do so, we generate the fragment space of ti, using
a variant of the mining algorithm described in Fig-
ure 3.1, and encode in ~vi all and only the fragments
ti,j so that ti,j ? DL, i.e. we perform feature extrac-
tion based on the indexed fragments. The process is
applied to the whole training and test sets. The al-
gorithm exploits labels and production rules found
in the fragments listed in the dictionary to generate
only the fragments that may be in the dictionary. For
example, if the dictionary does not contain a frag-
ment whose root is labeled N , then if a node N is
encountered during TFX neither its base fragment
nor its expansions are generated.
Explicit Space Learning (ESL) After linearizing
the training data, we can learn a very fast model by
using all the available data and a linear kernel. The
fragment space is now explicit, as there is a mapping
between the input vectors and the fragments they en-
code.
Explicit Space Classification (ESC) After learn-
ing the linear model, we can classify our linearized
test data and evaluate the accuracy of the resulting
classifier.
4 Previous work
A rather comprehensive overview of feature selec-
tion techniques is carried out in (Guyon and Elis-
seeff, 2003). Non-filter approaches for SVMs and
kernel machines are often concerned with polyno-
mial and Gaussian kernels, e.g. (Weston et al, 2001)
and (Neumann et al, 2005). Weston et al (2003) use
the `0 norm in the SVM optimizer to stress the fea-
ture selection capabilities of the learning algorithm.
In (Kudo and Matsumoto, 2003), an extension of the
PrefixSpan algorithm (Pei et al, 2001) is used to ef-
ficiently mine the features in a low degree polyno-
mial kernel space. The authors discuss an approx-
imation of their method that allows them to handle
high degree polynomial kernels.
33
Data set Non-linearized classifiers Linearized classifiers (Thr=10k)
Task Pos Neg Train Test P R F1 Train Test P R F1
A0 60,900 118,191 521 7 90.26 92.95 91.59 209 3 88.95 91.91 90.40
A1 90,636 88,455 1,206 11 89.45 88.62 89.03 376 3 89.39 88.13 88.76
A2 21,291 157,800 692 7 84.56 64.42 73.13 248 3 81.23 68.29 74.20
A3 3,481 175,610 127 2 97.67 40.00 56.76 114 3 97.56 38.10 54.79
A4 2,713 176,378 47 1 92.68 55.07 69.10 92 2 95.00 55.07 69.72
A5 69 179,022 3 0 100.00 50.00 66.67 63 2 100.00 50.00 66.67
BC 61,062 938,938 3,059 247 82.57 80.96 81.76 916 39 83.36 78.95 81.10
RM - - 2,596 27 89.37 86.00 87.65 1,090 16 88.50 85.81 87.13
Table 1: Accuracy (P, R, F1), training (Train) and test (Test) time of non-linearized (center) and linearized (right)
classifiers. Times are in minutes. For each task, columns Pos and Neg list the number of positive and negative training
examples, respectively. The accuracy of the role multiclassifiers is the micro-average of the individual classifiers
trained to recognize core PropBank roles.
Suzuki and Isozaki (2005) present an embedded
approach to feature selection for convolution ker-
nels based on ?2-driven relevance assessment. To
our knowledge, this is the only published work
clearly focusing on feature selection for tree ker-
nel functions. In (Graf et al, 2004), an approach
to SVM parallelization is presented which is based
on a divide-et-impera strategy to reduce optimiza-
tion time. The idea of using a compact graph rep-
resentation to represent the support vectors of a TK
function is explored in (Aiolli et al, 2006), where a
Direct Acyclic Graph (DAG) is employed.
Concerning the use of kernels for NLP, inter-
esting models and results are described, for exam-
ple, in (Collins and Duffy, 2002), (Moschitti et al,
2008), (Kudo and Matsumoto, 2003), (Cumby and
Roth, 2003), (Shen et al, 2003), (Cancedda et al,
2003), (Culotta and Sorensen, 2004), (Daume? III
and Marcu, 2004), (Kazama and Torisawa, 2005),
(Kudo et al, 2005), (Titov and Henderson, 2006),
(Moschitti et al, 2006), (Moschitti and Bejan, 2004)
or (Toutanova et al, 2004).
5 Experiments
We tested our model on a Semantic Role La-
beling (SRL) benchmark, using PropBank annota-
tions (Palmer et al, 2005) and automatic Charniak
parse trees (Charniak, 2000) as provided for the
CoNLL 2005 evaluation campaign (Carreras and
Ma`rquez, 2005). SRL can be decomposed into
two tasks: boundary detection, where the word se-
quences that are arguments of a predicate word w
are identified, and role classification, where each ar-
gument is assigned the proper role. The former task
requires a binary Boundary Classifier (BC), whereas
the second involves a Role Multi-class Classifier
(RM).
Setup. If the constituency parse tree t of a sen-
tence s is available, we can look at all the pairs
?p, ni?, where ni is any node in the tree and p is
the node dominating w, and decide whether ni is an
argument node or not, i.e. whether it exactly dom-
inates all and only the words encoding any of w?s
arguments. The objects that we classify are sub-
sets of the input parse tree that encompass both p
and ni. Namely, we use the ASTm structure defined
in (Moschitti et al, 2008), which is the minimal tree
that covers all and only the words of p and ni. In
the ASTm, p and ni are marked so that they can be
distinguished from the other nodes. An ASTm is
regarded as a positive example for BC if ni is an ar-
gument node, otherwise it is considered a negative
example. Positive BC examples can be used to train
an efficient RM: for each role r we can train a clas-
sifier whose positive examples are argument nodes
whose label is exactly r, whereas negative examples
are argument nodes labeled r? 6= r. Two ASTms
extracted from an example parse tree are shown in
Figure 3: the first structure is a negative example for
BC and is not part of the data set of RM, whereas
the second is a positive instance for BC and A1.
To train BC we used PropBank sections 1 through
6, extracting ASTm structures out of the first 1 mil-
lion ?p, ni? pairs from the corresponding parse trees.
As a test set we used the 149,140 instance collected
from the annotations in Section 24. There are 61,062
positive examples in the training set (i.e. 6.1%) and
8,515 in the test set (i.e. 5.7%).
For RM we considered all the argument nodes of
any of the six PropBank core roles (i.e. A0, . . . ,
34
1k 2k 5k 10k 20k30k 50k 100k
0
200
400
600
800
1,000
1,200
929 916 1,037
1,104
Threshold (log10)
Le
arn
ing
tim
e(
mi
nu
tes
)
Overall TFX ESL
FMI FSL
Figure 4: Training time decomposition for the linearized
BC with respect to its main components when varying the
threshold value.
A5) from all the available training sections, i.e. 2
through 21, for a total of 179,091 training instances.
Similarly, we collected 5,928 test instances from the
annotations of Section 24.
In the remainder, we will mark with an ` the lin-
earized classifiers, i.e. BC` and RM` will refer to
the linearized boundary and role classifiers, respec-
tively. Their traditional, vanilla SST counterparts
will be simply referred to as BC and RM.
We used 10 splits for the FMI stage and we set
maxdepth = 4 and maxexp = 5 during FMI and
TFX. We didn?t carry out an extensive validation of
these parameters. These values were selected dur-
ing the development of the software because, on a
very small development set, they resulted in a very
responsive system.
Since the main topic of this paper is the assess-
ment of the efficiency and accuracy of our lineariza-
tion technique, we did not carry out an evaluation
on the whole SRL task using the official CoNLL?05
evaluator. Indeed, producing complete annotations
requires several steps (e.g. overlap resolution, OvA
or Pairwise combination of individual role classi-
fiers) that would shade off the actual impact of the
methodology on classification.
Platform. All the experiments were run on a ma-
chine equipped with 4 Intel R? Xeon R? CPUs clocked
at 1.6 GHz and 4 GB of RAM running on a Linux
2.6.9 kernel. As a supervised learning framework
we used SVM-Light-TK 1, which extends the SVM-
Light optimizer (Joachims, 2000) with tree kernel
1http://disi.unitn.it/?moschitt/Tree-Kernel.htm
1k 2k 5k 10k 20k30k 50k 100k
72
74
76
78
80
82
84
Threshold (log10)
Ac
cu
rac
y
BC` Prec BC Prec
BC` Rec BC Rec
BC` F1 BC F1
Figure 5: BC` accuracy for different thresholds.
support. During FSL, we learn the models using a
normalized SST kernel and the default decay factor
? = 0.4. The same parameters are used to train
the models of the non linearized classifiers. During
ESL, the classifier is trained using a linear kernel.
We did not carry out further parametrization of the
learning algorithm.
Results. The left side of Table 1 shows the distri-
bution of positive (Column Pos) and negative (Neg)
data points in each classifier?s training set. The cen-
tral group of columns lists training and test effi-
ciency and accuracy of BC and RM, i.e. the non-
linearized classifiers, along with figures for the indi-
vidual role classifiers that make up RM.
Training BC took more than two days of CPU
time and testing about 4 hours. The classifier
achieves an F1 measure of 81.76, with a good bal-
ance between precision and recall. Concerning RM,
sequential training of the 6 models took 2,596 min-
utes, while classification took 27 minutes. The slow-
est of the individual role classifiers happens to be
A1, which has an almost 1:1 ratio between posi-
tive and negative examples, i.e. they are 90,636 and
88,455 respectively.
We varied the threshold value (i.e. the number of
fragments that we mine from each model, see Sec-
tion 3) to measure its effect on the resulting classi-
fier accuracy and efficiency. In this context, we call
training time all the time necessary to obtain a lin-
earized model, i.e. the sum of FSL, FMI and TFX
time for every split, plus the time for ESL. Similarly,
we call test time the time necessary to classify a lin-
earized test set, i.e. the sum of TFX and ESC on test
data.
In Figure 4 we plot the efficiency of BC` learn-
35
ing with respect to different threshold values. The
Overall training time is shown alongside with par-
tial times coming from FSL (which is the same for
every threshold value and amounts to 433 minutes),
FMI, training data TFX and ESL. The plot shows
that TFX has a logarithmic behaviour, and that quite
soon becomes the main player in total training time
after FSL. For threshold values lower than 10k, ESL
time decreases as the threshold increases: too few
fragments are available and adding new ones in-
creases the probability of including relevant frag-
ments in the dictionary. After 10k, all the relevant
fragments are already there and adding more only
makes computation harder. We can see that for a
threshold value of 100k total training time amounts
to 1,104 minutes, i.e. 36% of BC. For a threshold
value of 10k, learning time further decreases to 916
minutes, i.e. less than 30%. This threshold value
was used to train the individual linearized role clas-
sifiers that make up RM`.
These considerations are backed by the trend of
classification accuracy shown in Figure 5, where the
Precision, Recall and F1 measure of BC`, evaluated
on the test set, are shown in comparison with BC.
We can see that BC` precision is almost constant,
while its recall increases as we increase the thresh-
old, reaches a maximum of 78.95% for a threshold
of 10k and then settles around 78.8%. The F1 score
is maximized for a threshold of 10k, where it mea-
sures 81.10, i.e. just 0.66 points less than BC. We
can also see that BC` is constantly more conserva-
tive than BC, i.e. it always has higher precision and
lower recall.
Table 1 compares side to side the accuracy
(columns P, R and F1), training (Train) and test
(Test) times of the different classifiers (central block
of columns) and their linearized counterparts (block
on the right). Times are measured in minutes. For
the linearized classifiers, test time is the sum of
TFX and ESC time, but the only relevant contribu-
tion comes from TFX, as the low dimensional linear
space and fast linear kernel allow us to classify test
instances very efficiently 2. Overall, BC` test time is
39 minutes, which is more than 6 times faster than
BC (i.e. 247 minutes). It should be stressed that we
2Although ESC is not shown in table, the classification of all
149k test instances with BC` took 5 seconds with a threshold of
1k and 17 seconds with a threshold of 100k.
Learning parallelization
Task Non Lin. Linearized (Thr=10k)1 cpu 5 cpus 10 cpus
BC 3,059 916 293 215
RM 2,596 1,090 297 198
Table 2: Learning time when exploiting the framework?s
parallelization capabilities. Column Non Lin. lists non-
linearized training time.
are comparing against a fast TK implementation that
is almost linear in time with respect to the number of
tree nodes (Moschitti, 2006).
Concerning RM`, we can see that the accuracy
loss is even less than with BC`, i.e. it reaches an F1
measure of 87.13 which is just 0.52 less than RM.
It is also interesting to note how the individual lin-
earized role classifiers manage to perform accurately
regardless of the distribution of examples in the data
set: for all the six classifiers the final accuracy is
in line with that of the corresponding non-linearized
classifier. In two cases, i.e. A2 and A4, the accuracy
of the linearized classifier is even higher, i.e. 74.20
vs. 73.13 and 69.72 vs. 69.10, respectively. As for
the efficiency, total training time for RM` is 37% of
RM, i.e. 1,190 vs. 2,596 minutes, while test time
is reduced to 60%, i.e. 16 vs 27 minutes. These
improvements are less evident than those measured
for boundary detection. The main reason is that
the training set for boundary classification is much
larger, i.e. 1 million vs. 179k instances: therefore,
splitting training data during FSL has a reduced im-
pact on the overall efficiency of RM`.
Parallelization. All the efficiency improvements
that have been discussed so far considered a com-
pletely sequential process. But one of the advan-
tages of our approach is that it allows us to paral-
lelize some aspect of SVM training. Indeed, every
activity (but ESL) can exploit some degree of par-
allelism: during FSL, all the models can be learnt
at the same time (for this activity, the maximum de-
gree of parallelization is conditioned by the number
of training data splits); during FMI, models can be
mined concurrently; during TFX, the data-set to be
linearized can be split arbitrarily and individual seg-
ments can be processed in parallel. Exploiting this
possibility we can drastically improve learning ef-
ficiency. As an example, in Table 2 we show how
the total learning of the BC` can be cut to as low as
215 seconds when exploiting ten CPUs and using a
36
1 2 3 4 5 6 7 8 9 10
20
40
60
80
100
Models
Cu
mu
lat
ive
co
ntr
ibu
tio
n(
%)
1k 5k 10k
50k 100k
Figure 6: Growth of dictionary size when including frag-
ments from more splits at different threshold values.
When a low threshold is used, the contribution of indi-
vidual dictionaries tends to be more marginal.
threshold of 10k. Even running on just 5 CPUs, the
overall computational cost of BC` is less than 10%
of BC (Column Non Lin.). Similar considerations
can be drawn concerning the role multi-classifier.
Fragment space. In this section we take a look at
the fragments included in the dictionary of the BC`
classifier. During FMI, we incrementally merge the
fragments mined from each of the models learnt dur-
ing FSL. Figure 6 plots, for different threshold val-
ues, the percentage of new fragments (on the y axis)
that the i-th model (on the x axis) contributes with
respect to the number of fragments mined from each
model (i.e. the threshold value).
If we consider the curve for a threshold equal to
100k, we can see that each model after the first ap-
proximately contributes with the same number of
fragments. On the other hand, if the threshold is set
to 1k than the contribution of subsequent models is
increasingly more marginal. Eventually, less than
10% of the fragments mined from the last model are
new ones. This behaviour suggests that there is a
core set of very relevant fragments which is com-
mon across models learnt on different data, i.e. they
are relevant for the task and do not strictly depend
on the training data that we use. When we increase
the threshold value, the new fragments that we index
are more and more data specific.
The dictionary compiled with a threshold of 10k
lists 62,760 distinct fragments. 15% of the frag-
ments contain the predicate node (which generally
is the node encoding the predicate word?s POS tag),
more than one third contain the candidate argument
node and, of these, about one third are rooted in it.
This last figure strongly suggests that the internal
structure of an argument is indeed a very powerful
feature not only for role classification, as we would
expect, but also for boundary detection. About 10%
of the fragments contain both the predicate and the
argument node, while about 1% encode the Path fea-
ture traditionally used in explicit semantic role label-
ing models (Gildea and Jurafsky, 2002). About 5%
encode a sort of extended Path feature, where the ar-
gument node is represented together with its descen-
dants. Overall, about 2/3 of the fragments contain at
least some terminal symbol (i.e. words), generally a
preposition or an adverb.
6 Conclusions
We presented a supervised learning framework for
Support Vector Machines that tries to combine the
power and modeling simplicity of convolution ker-
nels with the advantages of linear kernels and ex-
plicit feature representations. We tested our model
on a Semantic Role Labeling benchmark and ob-
tained very promising results in terms of accuracy
and efficiency. Indeed, our linearized classifiers
manage to be almost as accurate as non linearized
ones, while drastically reducing the time required to
train and test a model on the same amounts of data.
To our best knowledge, the main points of nov-
elty of this work are the following: 1) it addresses
the problem of feature selection for tree kernels, ex-
ploiting SVM decisions to guide the process; 2) it
provides an effective way to make the kernel space
observable; 3) it can efficiently linearize structured
data without the need for an explicit mapping; 4) it
combines feature selection and SVM parallelization.
We began investigating the fragments generated
by a TK function for SRL, and believe that study-
ing them in more depth will be useful to identify
new, relevant features for the characterization of
predicate-argument relations.
In the months to come, we plan to run a set of ex-
periments on a wider list of tasks so as to consolidate
the results we obtained so far. We will also test the
generality of the approach by testing with different
high-dimensional kernel families, such as sequence
and polynomial kernels.
37
References
Fabio Aiolli, Giovanni Da San Martino, Alessandro Sper-
duti, and Alessandro Moschitti. 2006. Fast on-line
kernel learning for trees. In Proceedings of ICDM?06.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL?05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceedings
of ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245?288.
Hans P. Graf, Eric Cosatto, Leon Bottou, Igor Dur-
danovic, and Vladimir Vapnik. 2004. Parallel support
vector machines: The cascade svm. In Neural Infor-
mation Processing Systems.
Isabelle Guyon and Andre? Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157?1182.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, Dept. of Computer Sci-
ence, University of California at Santa Cruz.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML?00.
Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for
semi-structured data. In Proceedings of ICML?02.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP?05.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Alessandro Moschitti and Cosmin Bejan. 2004. A se-
mantic kernel for predicate argument classification. In
CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree ker-
nel joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proccedings of
EACL?06.
Julia Neumann, Christoph Schnorr, and Gabriele Steidl.
2005. Combined SVM-Based Feature Selection and
Classification. Machine Learning, 61(1-3):129?150.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Comput. Linguist., 31(1):71?106.
J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U.
Dayal, and M. C. Hsu. 2001. PrefixSpan Mining Se-
quential Patterns Efficiently by Prefix Projected Pat-
tern Growth. In Proceedings of ICDE?01.
Alain Rakotomamonjy. 2003. Variable selection using
SVM based criteria. Journal of Machine Learning Re-
search, 3:1357?1370.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Proceedings of EMNLP?06.
Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree
Kernels with Statistical Feature Mining. In Proceed-
ings of the 19th Annual Conference on Neural Infor-
mation Processing Systems (NIPS?05).
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.
Jason Weston, Sayan Mukherjee, Olivier Chapelle, Mas-
similiano Pontil, Tomaso Poggio, and Vladimir Vap-
nik. 2001. Feature Selection for SVMs. In Proceed-
ings of NIPS?01.
Jason Weston, Andre? Elisseeff, Bernhard Scho?lkopf, and
Mike Tipping. 2003. Use of the zero norm with lin-
ear models and kernel methods. J. Mach. Learn. Res.,
3:1439?1461.
Mohammed J Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of KDD?02.
38
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 519?527,
Beijing, August 2010
Reranking Models in Fine-grained Opinion Analysis
Richard Johansson and Alessandro Moschitti
University of Trento
{johansson, moschitti}@disi.unitn.it
Abstract
We describe the implementation of
reranking models for fine-grained opinion
analysis ? marking up opinion expres-
sions and extracting opinion holders. The
reranking approach makes it possible
to model complex relations between
multiple opinions in a sentence, allowing
us to represent how opinions interact
through the syntactic and semantic
structure. We carried out evaluations on
the MPQA corpus, and the experiments
showed significant improvements over a
conventional system that only uses local
information: for both tasks, our system
saw recall boosts of over 10 points.
1 Introduction
Recent years have seen a surge of interest in the
automatic processing of subjective language. The
technologies emerging from this research have ob-
vious practical uses, either as stand-alone appli-
cations or supporting other NLP tools such as
information retrieval or question answering sys-
tems. While early efforts in subjectivity analysis
focused on coarse-grained tasks such as retriev-
ing the subjective documents from a collection,
most recent work on this topic has focused on fine-
grained tasks such as determining the attitude of a
particular person on a particular topic. The devel-
opment and evaluation of such systems has been
made possible by the release of manually anno-
tated resources using fairly fine-grained represen-
tations to describe the structure of subjectivity in
language, for instance the MPQA corpus (Wiebe
et al, 2005).
A central task in the automatic analysis of sub-
jective language is the indentification of subjective
expressions: the text pieces that allow us to draw
the conclusion that someone has a particular feel-
ing about something. This is necessary for fur-
ther analysis, such as the determination of opin-
ion holder and the polarity of the opinion. The
MPQA corpus defines two types of subjective ex-
pressions: direct subjective expressions (DSEs),
which are explicit mentions of attitude, and ex-
pressive subjective elements (ESEs), which signal
the attitude of the speaker by the choice of words.
The prototypical example of a DSE would be a
verb of statement or categorization such as praise
or disgust, and the opinion holder would typi-
cally be a direct semantic argument of this verb.
ESEs, on the other hand, are less easy to cate-
gorize syntactically; prototypical examples would
include value-expressing adjectives such as beau-
tiful and strongly charged words like appease-
ment, while the relation between the expression
and the opinion holder is typically less clear-cut
than for DSEs. In addition to DSEs and ESEs, the
MPQA corpus also contains annotation for non-
subjective statements, which are referred to as ob-
jective speech events (OSEs).
Examples (1) and (2) show two sentences from
the MPQA corpus where DSEs and ESEs have
been manually annotated.
(1) He [made such charges]DSE [despite the
fact]ESE that women?s political, social and cul-
tural participation is [not less than that]ESE of
men.
(2) [However]ESE , it is becoming [rather
fashionable]ESE to [exchange harsh words]DSE
with each other [like kids]ESE .
The task of marking up these expressions has
usually been approached using straightforward
sequence labeling techniques using using simple
features in a small contextual window (Choi et
al., 2006; Breck et al, 2007). However, due to
519
the simplicity of the feature sets, this approach
fails to take into account the fact that the semantic
and pragmatic interpretation of sentences is not
only determined by words but also by syntactic
and shallow-semantic relations. Crucially, taking
grammatical relations into account allows us to
model how expressions interact in various ways
that influence their interpretation as subjective
or not. Consider, for instance, the word said in
examples (3) and (4) below, where the interpre-
tation as a DSE or an OSE is influenced by the
subjective content of the enclosed statement.
(3) ?We will identify the [culprits]ESE of these
clashes and [punish]ESE them,? he [said]DSE .
(4) On Monday, 80 Libyan soldiers disembarked
from an Antonov transport plane carrying military
equipment, an African diplomat [said]OSE .
In addition, the various opinions expressed in
a sentence are very interdependent when it comes
to the resolution of their holders, i.e. determining
the entity that harbors the sentiment manifested
textually in the opinion expression. Clearly, the
structure of the sentence is influential also for this
task: an ESE will be quite likely to be linked to
the same opinion holder as a DSE directly above
it in the syntactic tree.
In this paper, we demonstrate how syntactic
and semantic structural information can be used
to improve the detection of opinion expressions
and the extraction of opinion holders. While this
feature model makes it impossible to use the stan-
dard sequence labeling method, we show that with
a simple strategy based on reranking, incorporat-
ing structural features results in a significant im-
provement. In an evaluation on the MPQA corpus,
the best system we evaluated, a reranker using the
Passive?Aggressive learning algorithm, achieved
a 10-point absolute improvement in soft recall,
and a 5-point improvement in F-measure, over the
baseline sequence labeler. Similarly, the recall is
boosted by almost 11 points for the holder extrac-
tion (3 points in F-measure) by modeling the inter-
action of opinion expressions with respect to hold-
ers.
2 Related Work
Since the most significant body of work in sub-
jectivity analysis has been dedicated to coarse-
grained tasks such as document polarity classi-
fication, most approaches to analysing the senti-
ment of natural-language text have relied funda-
mentally on purely lexical information (see (Pang
et al, 2002; Yu and Hatzivassiloglou, 2003), in-
ter alia) or low-level grammatical information
such as part-of-speech tags and functional words
(Wiebe et al, 1999). This is not unexpected since
these problems have typically been formulated as
text categorization problems, and it has long been
agreed in the information retrieval community that
very little can be gained by complex linguistic
processing for tasks such as text categorization
and search (Moschitti and Basili, 2004).
As the field moves towards increasingly sophis-
ticated tasks requiring a detailed analysis of the
text, the benefit of syntactic and semantic analy-
sis becomes more clear. For the task of subjec-
tive expression detection, Choi et al (2006) and
Breck et al (2007) used syntactic features in a se-
quence model. In addition, syntactic and shallow-
semantic relations have repeatedly proven useful
for subtasks of subjectivity analysis that are in-
herently relational, above all for determining the
holder or topic of a given opinion. Choi et al
(2006) is notable for the use of a global model
based on hand-crafted constraints and an integer
linear programming optimization step to ensure a
globally consistent set of opinions and holders.
Works using syntactic features to extract top-
ics and holders of opinions are numerous (Bethard
et al, 2005; Kobayashi et al, 2007; Joshi and
Penstein-Rose?, 2009; Wu et al, 2009). Seman-
tic role analysis has also proven useful: Kim
and Hovy (2006) used a FrameNet-based seman-
tic role labeler to determine holder and topic of
opinions. Similarly, Choi et al (2006) success-
fully used a PropBank-based semantic role labeler
for opinion holder extraction. Ruppenhofer et al
(2008) argued that semantic role techniques are
useful but not completely sufficient for holder and
topic identification, and that other linguistic phe-
nomena must be studied as well. One such lin-
guistic pheonomenon is the discourse structure,
520
which has recently attracted some attention in the
subjectivity analysis community (Somasundaran
et al, 2009).
3 Modeling Interaction over Syntactic
and Semantic Structure
Previous systems for opinion expression markup
have typically used simple feature sets which have
allowed the use of efficient off-the-shelf sequence
labeling methods based on Viterbi search (Choi et
al., 2006; Breck et al, 2007). This is not pos-
sible in our case since we would like to extract
structural, relational features that involve pairs of
opinion expressions and may apply over an arbi-
trarily long distance in the sentence.
While it is possible that search algorithms for
exact or approximate inference can be construc-
tured for the argmax problem in this model, we
sidestepped this issue by using a reranking de-
composition of the problem:
? Apply a standard Viterbi-based sequence la-
beler based on local context features but no
structural interaction features. Generate a
small candidate set of size k.
? Generate opinion holders for every proposed
opinion expression.
? Apply a complex model using interaction
features to pick the top candidate from the
candidate set.
The advantages of a reranking approach com-
pared to more complex approaches requiring ad-
vanced search techniques are mainly simplicity
and efficiency: this approach is conceptually sim-
ple and fairly easy to implement provided that k-
best output can be generated efficiently, and fea-
tures can be arbitrarily complex ? we don?t have to
think about how the features affect the algorithmic
complexity of the inference step. A common ob-
jection to reranking is that the candidate set may
not be diverse enough to allow for much improve-
ment unless it is very large; the candidates may
be trivial variations that are all very similar to the
top-scoring candidate.
3.1 Syntactic and Semantic Structures
We used the syntactic?semantic parser by Johans-
son and Nugues (2008) to annnotate the sen-
tences with dependency syntax (Mel?c?uk, 1988)
and shallow semantic structures in the PropBank
(Palmer et al, 2005) and NomBank (Meyers et al,
2004) frameworks. Figure 1 shows an example
of the annotation: The sentence they called him a
liar, where called is a DSE and liar is an ESE, has
been annotated with dependency syntax (above
the text) and PropBank-based semantic role struc-
ture (below the text). The predicate called, which
is an instance of the PropBank frame call.01,
has three semantic arguments: the Agent (A0), the
Theme (A1), and the Predicate (A2), which are re-
alized on the surface-syntactic level as a subject,
a direct object, and an object predicative comple-
ment, respectively.
]ESEThey called
call.01
SBJ
OPRD
liarhim[ [a
A1A0 A2
]DSE
NMODOBJ
Figure 1: Syntactic and shallow semantic struc-
ture.
3.2 Base Sequence Labeling Model
To solve the first subtask, we implemented a stan-
dard sequence labeler for subjective expression
markup, similar to the approach by Breck et al
(2007). We encoded the opinionated expression
brackets using the IOB2 encoding scheme (Tjong
Kim Sang and Veenstra, 1999) and trained the
model using the metod by Collins (2002).
The sequence labeler used word, POS tag, and
lemma features in a window of size 3. In addi-
tion, we used prior polarity and intensity features
derived from the lexicon created by Wilson et al
(2005). It is important to note that prior subjec-
tivity does not always imply subjectivity in a par-
ticular context; this is why contextual features are
essential for this task.
This sequence labeler was used to generate the
candidate set for the reranker. To generate rerank-
ing training data, we carried out a 5-fold hold-out
procedure: We split the training set into 5 pieces,
521
trained a sequence labeler on pieces 1 to 4, applied
it to piece 5 and so on.
3.3 Base Opinion Holder Extractor
For every opinion expression, we extracted opin-
ion holders, i.e. mentions of the entity holding
the opinion denoted by the opinion expression.
Since the problem of holder extraction is in many
ways similar to semantic argument detection ?
when the opinion expression is a verb, finding the
holder typically entails finding a SPEAKER argu-
ment ? we approached this problem using meth-
ods inspired by semantic role labeling. We thus
trained support vector machines using the LIB-
LINEAR software (Fan et al, 2008), and applied
them to the noun phrases in the same sentence
as the holder. Separate classifiers were trained to
extract holders for DSEs, ESEs, and OSEs. The
classifiers used the following feature set:
SYNTACTIC PATH. Similarly to the path fea-
ture widely used in SRL, we extract a feature
representing the path in the dependency tree
between the expression and the holder (Jo-
hansson and Nugues, 2008). For instance,
the path from the DSE called to the holder
They is SBJ?.
SHALLOW-SEMANTIC RELATION. If there is a
direct shallow-semantic relation between the
expression and the holder, use a feature rep-
resenting its semantic role, such as A0 for
They with respect to called.
EXPRESSION HEAD WORD AND POS.
HOLDER HEAD WORD AND POS.
DOMINATING EXPRESSION TYPE.
CONTEXT WORDS AND POS FOR HOLDER.
EXPRESSION VERB VOICE.
However, there are also differences compared
to typical argument extraction in SRL. First, it is
important to note that the MPQA corpus does not
annotate direct links from opinions to a holders,
but from opinions to holder coreference chains.
To handle this issue, we created positive training
instances for allmembers of the coreference chain
in the same sentence as the opinion, and negative
instances for the other noun phrases.
Secondly, an opinion may be linked not to an
overt noun phrase in a sentence, but to an im-
plicit holder; a special case of implicit holder is
the writer of the text. We trained separate clas-
sifiers to detect these situations. These classifiers
did not use the features requiring a holder phrase.
Finally, there is a restriction that every expres-
sion may have at most one holder, so at test time
we select only the highest-scoring opinion holder
candidate.
3.4 Opinion Expression Reranker Features
The rerankers use two types of structural fea-
tures: syntactic features extracted from the depen-
dency tree, and semantic features extracted from
the predicate?argument (semantic role) graph.
The syntactic features are based on paths
through the dependency tree. This creates a small
complication for multiword opinion expressions;
we select the shortest possible path in such cases.
For instance, in example (1) above, the path will
be computed betweenmade and despite, and in (2)
between fashionable and exchange.
We used the following syntactic interaction fea-
tures:
SYNTACTIC PATH. Given a pair opinion ex-
pressions, we use a feature representing
the labels of the two expressions and the
path between them through the syntactic
tree. For instance, for the DSE called
and the ESE liar in Figure 1, we represent
the syntactic configuration using the feature
DSE:OPRD?:ESE, meaning that the path
from the DSE to the ESE follows an OPRD
link downward.
LEXICALIZED PATH. Same as above,
but with lexical information attached:
DSE/called:OPRD?:ESE/liar.
DOMINANCE. In addition to the features based
on syntactic paths, we created a more generic
feature template describing dominance re-
lations between expressions. For instance,
from the graph in Figure 1, we extract the
feature DSE/called?ESE/liar, mean-
ing that a DSE with the word called domi-
nates an ESE with the word liar.
The semantic features were the following:
522
PREDICATE SENSE LABEL. For every pred-
icate found inside an opinion expression,
we add a feature consisting of the expres-
sion label and the predicate sense identi-
fier. For instance, the verb call which is
also a DSE is represented with the feature
DSE/call.01.
PREDICATE AND ARGUMENT LABEL. For ev-
ery argument of a predicate inside an opin-
ion expression, we also create a feature
representing the predicate?argument pair:
DSE/call.01:A0.
CONNECTING ARGUMENT LABEL. When a
predicate inside some opinion expression is
connected to some argument inside another
opinion expression, we use a feature con-
sisting of the two expression labels and the
argument label. For instance, the ESE liar
is connected to the DSE call via an A2 la-
bel, and we represent this using a feature
DSE:A2:ESE.
Apart from the syntactic and semantic features,
we also used the score output from the base se-
quence labeler as a feature. We normalized the
scores over the k candidates so that their expo-
nentials summed to 1.
3.5 Opinion Holder Reranker Features
In addition, we modeled the interaction between
different opinions with respect to their holders.
We used the following two features to represent
this interaction:
SHARED HOLDERS. A feature representing
whether or not two opinion expressions have
the same holder. For instance, if a DSE
dominates an ESE and they have the same
holder as in Figure 1 where the holder
is They, we represent this by the feature
DSE:ESE:true.
HOLDER TYPES + PATH. A feature repre-
senting the types of the holders, combined
with the syntactic path between the expres-
sions. The types take the following pos-
sible values: explicit, implicit, writer. In
Figure 1, we would thus extract the feature
DSE/Expl:OPRD?:ESE/Expl.
Similar to base model feature for the expression
detection, we also used a feature for the output
score from the holder extraction classifier.
3.6 Training the Reranker
We trained the reranker using the method em-
ployed by many rerankers following Collins
(2002), which learns a scoring function that is
trained to maximize performance on the rerank-
ing task. While there are batch learning algo-
rithms that work in this setting (Tsochantaridis
et al, 2005), online learning methods have been
more popular for performance reasons. We inves-
tigated two online learning algorithms: the popu-
lar structured perceptron (Collins, 2002) and the
Passive?Aggressive (PA) algorithm (Crammer et
al., 2006). To increase robustness, we used an
averaged implementation (Freund and Schapire,
1999) of both algorithms.
The difference between the two algorithms is
the way the weight vector is incremented in each
step. In the perceptron, for a given input x, we
update based on the difference between the correct
output y and the predicted output y?, where? is the
feature representation function:
y? ? argmaxh w ? ?(x, h)
w ? w + ?(x, y)? ?(x, y?)
In the PA algorithm, which is based on the the-
ory of large-margin learning, we instead find the
y? that violates the margin constraints maximally.
The update step length ? is computed based on the
margin; this update is bounded by a regularization
constant C:
y? ? argmaxh w ? ?(x, h) +
?
?(y, h)
? ? min
(
C, w(?(x,y?)??(x,y))+
?
?(y,y?)
??(x,y?)??(x,y)?2
)
w ? w + ?(?(x, y)? ?(x, y?))
The algorithm uses a cost function ?. We used
the function ?(y, y?) = 1 ? F (y, y?), where F is
the soft F-measure described in Section 4.1. With
this approach, the learning algorithm thus directly
optimizes the measure we are interested in, i.e. the
F-measure.
4 Experiments
We carried out the experiments on version 2 of
the MPQA corpus (Wiebe et al, 2005), which we
523
split into a test set (150 documents, 3,743 sen-
tences) and a training set (541 documents, 12,010
sentences).
4.1 Evaluation Metrics
Since expression boundaries are hard to define ex-
actly in annotation guidelines (Wiebe et al, 2005),
we used soft precision and recall measures to
score the quality of the system output. To de-
rive the soft precision and recall, we first define
the span coverage c of a span s with respect to
another span s?, which measures h ow well s? is
covered by s:
c(s, s?) = |s ? s
?|
|s?|
In this formula, the operator | ? | counts tokens, and
the intersection ? gives the set of tokens tha t two
spans have in common. Since our evaluation takes
span labels (DSE, ESE, OSE) into account, we set
c(s, s?) to zero if the labels associated with s and
s? are different.
Using the span coverage, we define the span set
coverage C of a set of spans S with respect to a
set S?:
C(S,S?) =
?
sj?S
?
s?k?S?
c(sj , s?k)
We now define the soft precision P and recall
R of a proposed set of spans S? with respect to a
gold standard set S as follows:
P (S, S?) = C(S,S?)|S?| R(S, S?) =
C(S?,S)
|S|
Note that the operator | ? | counts spans in this for-
mula.
Conventionally, when measuring the quality of
a system for an information extraction task, a pre-
dicted entity is counted as correct if it exactly
matches the boundaries of a corresponding en-
tity in the gold standard; there is thus no reward
for close matches. However, since the boundaries
of the spans annotated in the MPQA corpus are
not strictly defined in the annotation guidelines
(Wiebe et al, 2005), measuring precision and re-
call using exact boundary scoring will result in
figures that are too low to be indicative of the
usefulness of the system. Therefore, most work
using this corpus instead use overlap-based preci-
sion and recall measures, where a span is counted
as correctly detected if it overlaps with a span in
the gold standard (Choi et al, 2006; Breck et al,
2007). As pointed out by Breck et al (2007), this
is problematic since it will tend to reward long
spans ? for instance, a span covering the whole
sentence will always be counted as correct if the
gold standard contains any span for that sentence.
The precision and recall measures proposed
here correct the problem with overlap-based mea-
sures: If the system proposes a span covering the
whole sentence, the span coverage will be low
and result in a low soft precision. Note that our
measures are bounded below by the exact mea-
sures and above by the overlap-based measures:
replacing c(s, s?) with ?c(s, s?)? gives the exact
measures and replacing c(s, s?) with ?c(s, s?)? the
overlap-based measures.
To score the extraction of opinion holders, we
started from the same basic approach. However,
the evaluation of this task is more complex be-
cause a) we only want to give credit for holders
for correctly extracted opinion expressions; b) the
gold standard links opinion expressions to coref-
erence chains rather than individual mentions of
holders; c) the holder may be the writer or im-
plicit (see 3.3). We therefore used the following
method: Given a holder h linked to an expres-
sion e, we first located the expression e? in the
gold standard that most closely corresponds to e,
that is e? = argmaxx c(x, e), regardless of the
labels of e and e?. We then located the gold stan-
dard holder h? by finding the closest correspond-
ing holder in the coreference chain H linked to e?:
h? = argmaxx?H c(x, h). If h is proposed as the
writer, we score it as perfectly detected (coverage
1) if the coreference chain H contains the writer,
and a full error (coverage 0) otherwise, and simi-
lar if h is implicit.
4.2 Machine Learning Methods
We compared the machine learning methods de-
scribed in Section 3. In these experiments, we
used a candidate set size k of 8. Table 1 shows
the results of the evaluations using the precision
and recall measures described above. The base-
line is the result of taking the top-scoring labeling
524
from the base model.
System P R F
Baseline 63.36 46.77 53.82
Perceptron 62.84 48.13 54.51
PA 63.50 51.79 57.04
Table 1: Evaluation of reranking learning meth-
ods.
We note that the best performance was obtained
using the PA algorithm. While these results are
satisfactory, it is possible that they could be im-
proved further if we would use a batch learning
method such as SVMstruct (Tsochantaridis et al,
2005) instead of the online learning methods used
here.
4.3 Candidate Set Size
In any method based on reranking, it is important
to study the influence of the candidate set size on
the quality of the reranked output. In addition, an
interesting question is what the upper bound on
reranker performance is ? the oracle performance.
Table 2 shows the result of an experiment that in-
vestigates these questions. We used the reranker
based on the Passive?Aggressive method in this
experiment since this reranker gave the best re-
sults in the previous experiment.
Reranked Oracle
k P R F P R F
1 63.36 46.77 53.82 63.36 46.77 53.82
2 63.70 48.17 54.86 72.66 55.18 62.72
4 63.57 49.78 55.84 79.12 62.24 69.68
8 63.50 51.79 57.04 83.72 68.14 75.13
16 63.00 52.94 57.54 86.92 72.79 79.23
32 62.15 54.50 58.07 89.18 76.76 82.51
64 61.02 55.67 58.22 91.08 80.19 85.28
128 60.22 56.45 58.27 92.63 83.00 87.55
256 59.87 57.22 58.51 94.01 85.27 89.43
Table 2: Oracle and reranker performance as a
function of candidate set size.
As is common in reranking tasks, the reranker
can exploit only a fraction of the potential im-
provement ? the reduction of the F-measure error
is between 10 and 15 percent of the oracle error
reduction for all candidate set sizes.
The most visible effect of the reranker is that
the recall is greatly improved. However, this does
not seem to have an adverse effect on the precision
until the candidate set size goes above 16 ? in fact,
the precision actually improves over the baseline
for small candidate set sizes. After the size goes
above 16, the recall (and the F-measure) still rises,
but at the cost of decreased precision.
4.4 Syntactic and Semantic Features
We studied the impact of syntactic and seman-
tic structural features on the performance of the
reranker. Table 3 shows the result of the investi-
gation for syntactic features. Using all the syntac-
tic features (and no semantic features) gives an F-
measure roughly 4 points above the baseline, us-
ing the PA reranker with a k of 64. We then mea-
sured the F-measure obtained when each one of
the three syntactic features has been removed. It
is clear that the unlexicalized syntactic path is the
most important syntactic feature; the effect of the
two lexicalized features seems to be negligible.
System P R F
Baseline 63.36 46.77 53.82
All syntactic 62.45 53.19 57.45
No SYN PATH 64.40 48.69 55.46
No LEX PATH 62.62 53.19 57.52
No DOMINANCE 62.32 52.92 57.24
Table 3: Effect of syntactic features.
A similar result was obtained when studying the
semantic features (Table 4). Removing the con-
necting labels feature, which is unlexicalized, has
a greater effect than removing the other two se-
mantic features, which are lexicalized.
System P R F
Baseline 63.36 46.77 53.82
All semantic 61.26 53.85 57.31
No PREDICATE SL 61.28 53.81 57.30
No PRED+ARGLBL 60.96 53.61 57.05
No CONN ARGLBL 60.73 50.47 55.12
Table 4: Effect of semantic features.
4.5 Opinion Holder Extraction
Table 5 shows the performance of the opinion
holder extractor. The baseline applies the holder
525
classifier (3.3) to the opinions extracted by the
base sequence labeler (3.2), without modeling any
interactions between opinions. A large perfor-
mance boost is then achieved simply by applying
the opinion expression reranker (k = 64); this is
simply the consequence of improved expression
detection, since a correct expression is required to
get credit for a holder).
However, we can improve on this by adding
the holder interaction features: both the SHARED
HOLDERS and HOLDER TYPES + PATH features
contribute to improving the recall even further.
System P R F
Baseline 57.66 45.14 50.64
Reranked expressions 52.35 52.54 52.45
SHARED HOLDERS 52.43 55.21 53.78
HTYPES + PATH 52.22 54.41 53.30
Both 52.28 55.99 54.07
Table 5: Opinion holder extraction experiments.
5 Conclusion
We have shown that features derived from gram-
matical and semantic role structure can be used
to improve two fundamental tasks in fine-grained
opinion analysis: the detection of opinionated ex-
pressions in subjectivity analysis, and the extrac-
tion of opinion holders. Our feature sets are based
on interaction between opinions, which makes ex-
act inference intractable. To overcome this issue,
we used an implementation based on reranking:
we first generated opinion expression sequence
candidates using a simple sequence labeler sim-
ilar to the approach by Breck et al (2007). We
then applied SRL-inspired opinion holder extrac-
tion classifiers, and finally a global model apply-
ing to all opinions and holders.
Our experiments show that the interaction-
based models result in drastic improvements. Sig-
nificantly, we see significant boosts in recall (10
points for both tasks) while the precision de-
creases only slightly, resulting in clear F-measure
improvements. This result compares favorably
with previously published results, which have
been precision-oriented and scored quite low on
recall.
We analyzed the impact of the syntactic and se-
mantic features and saw that the best model is the
one that makes use of both types of features. The
most effective features we have found are purely
structural, i.e. based on tree fragments in a syn-
tactic or semantic tree. Features involving words
did not seem to have the same impact.
There are multiple opportunities for future
work in this area. An important issue that we have
left open is the coreference problem for holder ex-
traction, which has been studied by Stoyanov and
Cardie (2006). Similarly, recent work has tried to
incorporate complex, high-level linguistic struc-
ture such as discourse representations (Somasun-
daran et al, 2009); it is clear that these structures
are very relevant for explaining the way humans
organize their expressions of opinions rhetori-
cally. However, theoretical depth does not nec-
essarily guarantee practical applicability, and the
challenge is as usual to find a middle ground that
balances our goals: explanatory power in theory,
significant performance gains in practice, compu-
tational tractability, and robustness in difficult cir-
cumstances.
6 Acknowledgements
The research described in this paper has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant 231126: LivingKnowledge ? Facts, Opin-
ions and Bias in Time, and under grant 247758:
Trustworthy Eternal Systems via Evolving Soft-
ware, Data and Knowledge (EternalS).
References
Bethard, Steven, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extract-
ing opinion propositions and opinion holders using
syntactic and lexical cues. In Shanahan, James G.,
Yan Qu, and Janyce Wiebe, editors, Computing At-
titude and Affect in Text: Theory and Applications.
Breck, Eric, Yejin Choi, and Claire Cardie. 2007.
Identifying expressions of opinion in context. In
Proceedings of IJCAI-2007, Hyderabad, India.
Choi, Yejin, Eric Breck, and Claire Cardie. 2006.
Joint extraction of entities and relations for opinion
recognition. In Proceedings of EMNLP 2006.
526
Collins, Michael. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2002),
pages 1?8.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 2006(7):551?585.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Freund, Yoav and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277?296.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
Joshi, Mahesh and Carolyn Penstein-Rose?. 2009.
Generalizing dependency features for opinion min-
ing. In Proceedings of ACL/IJCNLP 2009, Short
Papers Track.
Kim, Soo-Min and Eduard Hovy. 2006. Extract-
ing opinions, opinion holders, and topics expressed
in online news media text. In Proceedings of
ACL/COLING Workshop on Sentiment and Subjec-
tivity in Text.
Kobayashi, Nozomi, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Extracting aspect-evaluation and
aspect-of relations in opinion mining. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-CoNLL-2007).
Mel?c?uk, Igor A. 1988. Dependency Syntax: Theory
and Practice. State University Press of New York,
Albany.
Meyers, Adam, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, United States.
Moschitti, Alessandro and Roberto Basili. 2004.
Complex linguistic features for text classification:
A comprehensive study. In Proceedings of ECIR.
Palmer, Martha, Dan Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?105.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP.
Ruppenhofer, Josef, Swapna Somasundaran, and
Janyce Wiebe. 2008. Finding the sources and tar-
gets of subjective expressions. In Proceedings of
LREC.
Somasundaran, Swapna, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP.
Stoyanov, Veselin and Claire Cardie. 2006. Partially
supervised coreference resolution for opinion sum-
marization through structured rule learning. In Pro-
ceedings of EMNLP 2006.
Tjong Kim Sang, Erik F. and Jorn Veenstra. 1999.
Representing text chunks. In Proceedings of
EACL99, pages 173?179, Bergen, Norway.
Tsochantaridis, Iannis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6(Sep):1453?1484.
Wiebe, Janyce, Rebecca Bruce, and Thomas O?Hara.
1999. Development and use of a gold standard data
set for subjectivity classifications. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Wilson, Theresa, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLT/EMNLP 2005.
Wu, Yuanbin, Qi Zhang, Xuanjing Huang, and Lide
Wu. 2009. Phrase dependency parsing for opinion
mining. In Proceedings of EMNLP.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 129?136, Sap-
poro, Japan.
527
Coling 2010: Poster Volume, pages 901?909,
Beijing, August 2010
Kernel-based Reranking for Named-Entity Extraction
Truc-Vien T. Nguyen and Alessandro Moschitti and Giuseppe Riccardi
Department of Information Engineering and Computer Science
University of Trento
nguyenthi,moschitti,riccardi@disi.unitn.it
Abstract
We present novel kernels based on struc-
tured and unstructured features for rerank-
ing the N-best hypotheses of conditional
random fields (CRFs) applied to entity ex-
traction. The former features are gener-
ated by a polynomial kernel encoding en-
tity features whereas tree kernels are used
to model dependencies amongst tagged
candidate examples. The experiments on
two standard corpora in two languages,
i.e. the Italian EVALITA 2009 and the En-
glish CoNLL 2003 datasets, show a large
improvement on CRFs in F-measure, i.e.
from 80.34% to 84.33% and from 84.86%
to 88.16%, respectively. Our analysis re-
veals that both kernels provide a compara-
ble improvement over the CRFs baseline.
Additionally, their combination improves
CRFs much more than the sum of the indi-
vidual contributions, suggesting an inter-
esting kernel synergy.
1 Introduction
Reranking is a promising computational frame-
work, which has drawn special attention in the
Natural Language Processing (NLP) community.
Basically, this method first employs a probabilis-
tic model to generate a list of top-n candidates and
then reranks this n-best list with additional fea-
tures. One appeal of this approach is its flexibility
of incorporating arbitrary features into a model.
These features help in discriminating good from
bad hypotheses and consequently their automatic
learning. Various algorithms have been applied
for reranking in NLP applications (Huang, 2008;
Shen et al, 2004; Collins, 2002b; Collins and
Koo, 2000), including parsing, name tagging and
machine translation. This work has exploited the
disciminative property as one of the key criterion
of the reranking algorithm.
Reranking appears extremely interesting if cou-
pled with kernel methods (Dinarelli et al, 2009;
Moschitti, 2004; Collins and Duffy, 2001), as the
latter allow for extracting from the ranking hy-
potheses a huge amount of features along with
their dependencies. Indeed, while feature-based
learning algorithms involve only the dot-product
between feature vectors, kernel methods allow
for a higher generalization by replacing the dot-
product with a function between pairs of linguis-
tic objects. Such functions are a kind of similarity
measure satisfying certain properties. An exam-
ple is the tree kernel (Collins and Duffy, 2001),
where the objects are syntactic trees that encode
grammatical derivations and the kernel function
computes the number of common subtrees. Simi-
larly, sequence kernels (Lodhi et al, 2002) count
the number of common subsequences shared by
two input strings.
Named-entities (NEs) are essential for defin-
ing the semantics of a document. NEs are ob-
jects that can be referred by names (Chinchor and
Robinson, 1998), such as people, organizations,
and locations. The research on NER has been
promoted by the Message Understanding Con-
ferences (MUCs, 1987-1998), the shared task of
the Conference on Natural Language Learning
(CoNLL, 2002-2003), and the Automatic Content
Extraction program (ACE, 2002-2005). In the lit-
erature, there exist various learning approaches
to extract named-entities from text. A NER sys-
901
tem often builds some generative/discriminative
model, then, either uses only one classifier (Car-
reras et al, 2002) or combines many classifiers us-
ing some heuristics (Florian et al, 2003).
To the best of our knowledge, reranking has
not been applied to NER except for the rerank-
ing algorithms defined in (Collins, 2002b; Collins,
2002a), which only targeted the entity detection
(and not entity classification) task. Besides, since
kernel methods offer a natural way to exploit lin-
guistic properties, applying kernels for NE rerank-
ing is worthwhile.
In this paper, we describe how kernel methods
can be applied for reranking, i.e. detection and
classification of named-entities, in standard cor-
pora for Italian and English. The key aspect of
our reranking approach is how structured and flat
features can be employed in discriminating candi-
date tagged sequences. For this purpose, we apply
tree kernels to a tree structure encoding NE tags of
a sentence and combined them with a polynomial
kernel, which efficiently exploits global features.
Our main contribution is to show that (a) tree
kernels can be used to define general features (not
merely syntactic) and (b) using appropriate al-
gorithms and features, reranking can be very ef-
fective for named-entity recognition. Our study
demonstrates that the composite kernel is very
effective for reranking named-entity sequences.
Without the need of producing and heuristically
combining learning models like previous work on
NER, the composite kernel not only captures most
of the flat features but also efficiently exploits
structured features. More interestingly, this kernel
yields significant improvement when applied to
two corpora of two different languages. The eval-
uation in the Italian corpus shows that our method
outperforms the best reported methods whereas on
the English data it reaches the state-of-the-art.
2 Background
2.1 The data
Different languages exhibit different linguistic
phenomena and challenges. A robust NER sys-
tem is expected to be well-adapted to multiple
domains and languages. Therefore, we experi-
mented with two datasets: the EVALITA 2009
Italian corpus and the well-known CoNLL 2003
English shared task corpus.
The EVALITA 2009 Italian dataset is based
on I-CAB, the Italian Content Annotation
Bank (Magnini et al, 2006), annotated with four
entity types: Person (PER), Organization (ORG),
Geo-Political Entity (GPE) and Location (LOC).
The training data, taken from the local newspa-
per ?L?Adige?, consists of 525 news stories which
belong to five categories: News Stories, Cultural
News, Economic News, Sports News and Local
News. Test data, on the other hand, consist of
completely new data, taken from the same news-
paper and consists of 180 news stories.
The CoNLL 2003 English dataset is created
within the shared task of CoNLL-2003 (Sang
and Meulder, 2003). It is a collection of news
wire articles from the Reuters Corpus, annotated
with four entity types: Person (PER), Location
(LOC), Organization (ORG) and Miscellaneous
name (MISC). The training and the development
datasets are news feeds from August 1996, while
the test set contains news feeds from December
1996. Accordingly, the named entities in the test
dataset are considerably different from those that
appear in the training or the development set.
Italian GPE LOC ORG PER
Train 2813 362 3658 457724.65% 3.17% 32.06% 40.11%
Test 1143 156 1289 237823.02% 3.14% 25.96% 47.89%
English LOC MISC ORG PER
Train 7140 3438 6321 660030.38% 14.63% 26.90% 28.09%
Dev 1837 922 1341 184230.92% 15.52% 22.57% 31.00%
Test 1668 702 1661 161729.53% 12.43% 29.41% 28.63%
Table 1: Statistics on the Italian EVALITA 2009
and English CoNLL 2003 corpora.
2.2 The baseline algorithm
We selected Conditional Random Fields (Lafferty
et al, 2001) as the baseline model. Conditional
902
random fields (CRFs) are a probabilistic frame-
work for labeling and segmenting sequence data.
They present several advantages over other purely
generative models such as Hidden Markov models
(HMMs) by relaxing the independence assump-
tions required by HMMs. Besides, HMMs and
other discriminative Markov models are prone to
the label bias problem, which is effectively solved
by CRFs.
The named-entity recognition (NER) task is
framed as assigning label sequences to a set of
observation sequences. We follow the IOB nota-
tion where the NE tags have the format B-TYPE,
I-TYPE or O, which mean that the word is a be-
ginning, a continuation of an entity, or not part of
an entity at all. For example, consider the sentence
with their corresponding NE tags, each word is la-
beled with a tag indicating its appropriate named-
entity, resulting in annotated text, such as:
Il/O presidente/O della/O Fifa/B-ORG Sepp/B-PER
Blatter/I-PER affermando/O che/O il/O torneo/O era/O
stato/O ottimo/O (FIFA president Sepp Blatter says that the
tournament was excellent)
For our experiments, we used CRF++ 1 to build
our recognizer, which is a model trained discrim-
inatively with the unigram and bigram features.
These are extracted from a window at k words
centered in the target word w (i.e. the one we want
to classify with the B, O, I tags). More in detail
such features are:
? The word itself, its prefixes, suffixes, and
part-of-speech
? Orthographic/Word features. These are
binary and mutually exclusive features that
test whether a word contains all upper-cased,
initial letter upper-cased, all lower-cased,
roman-number, dots, hyphens, acronym,
lonely initial, punctuation mark, single-char,
and functional-word.
? Gazetteer features. Class (geographical,
first name, surname, organization prefix, lo-
cation prefix) of words in the window.
? Left Predictions. The predicted tags on the
left of the word in the current classification.
1http://crfpp.sourceforge.net
The gazetteer lists are built with names im-
ported from different sources. For English, the
geographic features are imported from NIMA?s
GEOnet Names Server (GNS)2, The Alexandria
Digital Library (ADL) gazetteer3. The company
data is included with all the publicly traded com-
panies listed in Google directory4, the European
business directory5. For Italian, the generic proper
nouns are extracted from Wikipedia and various
Italian sites.
2.3 Support Vector Machines (SVMs)
Support Vector Machines refer to a supervised
machine learning technique based on the latest re-
sults of the statistical learning theory. Given a
vector space and a set of training points, i.e. posi-
tive and negative examples, SVMs find a separat-
ing hyperplane H(~x) = ~? ? ~x + b = 0 where
? ? Rn and b ? R are learned by applying the
Structural Risk Minimization principle (Vapnik,
1998). SVMs are a binary classifier, but they can
be easily extended to multi-class classifier, e.g. by
means of the one-vs-all method (Rifkin and Pog-
gio, 2002).
One strong point of SVMs is the possibility to
apply kernel methods to implicitly map data in
a new space where the examples are more easily
separable as described in the next section.
2.4 Kernel methods
Kernel methods (Scho?lkopf and Smola, 2001) are
an attractive alternative to feature-based methods
since the applied learning algorithm only needs
to compute a product between a pair of objects
(by means of kernel functions), avoiding the ex-
plicit feature representation. A kernel function
is a scalar product in a possibly unknown feature
space. More precisely, The object o is mapped in
~x with a feature function ? : O ? <n, where O
is the set of the objects.
The kernel trick allows us to rewrite the deci-
sion hyperplane as:
H(~x) =
( ?
i=1..l
yi?i~xi
)
? ~x+ b =
2http://www.nima.mil/gns/html
3http://www.alexandria.ucsb.edu
4http://directory.google.com/Top/Business
5http://www.europages.net
903
?i=1..l
yi?i~xi ? ~x+ b =
?
i=1..l
yi?i?(oi) ? ?(o) + b,
where yi is equal to 1 for positive and -1 for
negative examples, ?i ? < with ?i ? 0, oi
?i ? {1, .., l} are the training instances and the
product K(oi, o) = ??(oi) ? ?(o)? is the kernel
function associated with the mapping ?.
Kernel engineering can be carried out by com-
bining basic kernels with additive or multiplica-
tive operators or by designing specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
Regarding NLP applications, kernel methods
have attracted much interest due to the ability of
implicitly exploring huge amounts of structural
features. The parse tree kernel (Collins and Duffy,
2001) and string kernel (Lodhi et al, 2002) are
examples of the well-known convolution kernels
used in various NLP tasks.
2.5 Tree Kernels
Tree kernels represent trees in terms of their sub-
structures (called tree fragments). Such fragments
form a feature space which, in turn, is mapped into
a vector space. Tree kernels measure the similar-
ity between pair of trees by counting the number
of fragments in common. There are three impor-
tant characterizations of fragment type: the Sub-
Trees (ST), the SubSet Trees (SST) and the Partial
Trees (PT). For sake of space, we do not report the
mathematical description of them, which is avail-
able in (Vishwanathan and Smola, 2002), (Collins
and Duffy, 2001) and (Moschitti, 2006), respec-
tively. In contrast, we report some descriptions in
terms of feature space that may be useful to un-
derstand the new engineered kernels.
In principle, a SubTree (ST) is defined by tak-
ing any node along with its descendants. A SubSet
Tree (SST) is a more general structure which does
not necessarily include all the descendants. The
distinction is that an SST must be generated by ap-
plying the same grammatical rule set which gen-
erated the original tree, as pointed out in (Collins
and Duffy, 2001). A Partial Tree (PT) is a more
general form of sub-structures obtained by relax-
ing constraints over the SSTs. Figure 1 shows the
overall fragment set of the ST, SST and PT kernels
for the syntactic parse tree of the sentence frag-
Figure 1: Three kinds of tree kernels.
ment: gives a talk .
In the next section, we will define new struc-
tures for tagged sequences of NEs which along
with the application of the PT kernel produce in-
novative tagging kernels for reranking.
3 Reranking Method
3.1 Reranking Strategy
As a baseline we trained the CRFs model to gen-
erate 10-best candidates per sentence, along with
their probabilities. Each candidate was then rep-
resented by a semantic tree together with a feature
vector. We consider our reranking task as a binary
classification problem where examples are pairs
of hypotheses < Hi, Hj >.
Given a sentence ?South African Breweries Ltd
bought stakes in the Lech and Tychy brewers? and three
of its candidate tagged sequences:
H1 B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O
B-ORG O (the correct sequence)
H2 B-MISC I-MISC B-ORG I-ORG O O O O B-ORG
I-ORG I-ORG O
H3 B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O
B-LOC O
where B-ORG, I-ORG, B-LOC, O are the gen-
erated NE tags according to IOB notation as de-
scribed in Section 3.2.
With the above data (an original sentence to-
gether with a list of candidate tagged sequences),
the following pairs of hypotheses will be gener-
904
ated < H1, H2 >, < H1, H3 >,< H2, H1 > and
< H3, H1 >, where the first two pairs are positive
and the latter pairs are negative instances. Then a
binary classifier based on SVMs and kernel meth-
ods can be trained to discriminate between the
best hypothesis, i.e. < H1 > and the others. At
testing time the hypothesis receiving the highest
score is selected (Collins and Duffy, 2001).
3.2 Representation of Tagged Sequences in
Semantic Trees
We now consider the representation that exploits
the most discriminative aspects of candidate struc-
tures. As in the case of NER, an input can-
didate is a sequence of word/tag pairs x =
{w1/t1...wn/tn} where wi is the i?th word and
ti is the i?th NE tag for that word. The first repre-
sentation we consider is the tree structure. See fig-
ure 2 as an example of candidate tagged sequence
and its semantic tree.
With the sentence ?South African Breweries Ltd
bought stakes in the Lech and Tychy brewers? and three
of its candidate tagged sequences in the previous
section, the training algorithm considers to con-
struct a tree for each sequence, with the named-
entity tags as pre-terminals and the words as
leaves. See figure 2 for an example of the seman-
tic tree for the first tagged sequence.
With this tree representation, for a word wi, the
target NE tag would be set at parent and the fea-
tures for this word are at child nodes. This allows
us to best exploit the inner product between com-
peting candidates. Indeed, in the kernel space,
the inner product counts the number of common
subtrees thus sequences with similar NE tags are
likely to have higher score. For example, the sim-
ilarity between H1 and H3 will be higher than the
similarity of the previous hypotheses withH2; this
is reasonable since these two also have higher F1.
It is worth noting that another useful modifica-
tion is the flexibility of incorporate diverse, ar-
bitrary features into this tree structure by adding
children to the parent node that contains entity tag.
These characteristics can be exploited efficiently
with the PT kernel, which relaxes constraints of
production rules. The inner product can implicitly
include these features and deal better with sparse
data.
3.3 Global features
Mixed n-grams features
In previous works, some global features have
been used (Collins, 2002b; Collins, 2002a) but the
employed algorithm just exploited arbitrary infor-
mation regarding word types and linguistic pat-
terns. In contrast, we define and study diverse
features by also considering n-grams patterns pre-
ceding, and following the target entity.
Complementary context
In supervised learning, NER systems often suf-
fer from low recall, which is caused by lack of
both resource and context. For example, a word
like ?Arkansas? may not appear in the training set
and in the test set, there may not be enough con-
text to infer its NE tag. In such cases, neither
global features (Chieu and Ng, 2002) nor aggre-
gated contexts (Chieu and Ng, 2003) can help.
To overcome this deficiency, we employed the
following unsupervised procedure: first, the base-
line NER is applied to the target un-annotated cor-
pus. Second, we associate each word of the corpus
with the most frequent NE category assigned in
the previous step. Finally, the above tags are used
as features during the training of the improved
NER and also for building the feature represen-
tation for a new classification instance.
This way, for any unknown word w of the test
set, we can rely on the most probable NE category
as feature. The advantage is that we derived it by
using the average over many possible contexts of
w, which are in the different instances of the un-
nanotated corpus.
The unlabeled corpus for Italian was collected
from La Repubblica 6 and it contains over 20 mil-
lions words. Whereas the unlabeled corpus for
English was collected mainly from The New York
Times 7 and BBC news stories 8 with more than
35 millions words.
Head word
As the head word of an entity plays an impor-
tant role in information extraction (Bunescu and
Mooney, 2005a; Surdeanu et al, 2003), it is in-
6http://www.repubblica.it/
7http://www.nytimes.com/
8http://news.bbc.co.uk/
905
Figure 2: Semantic structure of the first sequence
cluded in the global set together with its ortho-
graphic feature. We now describe some primitives
for our global feature framework.
1. wi for i = 1 . . . n is the i?th word
2. ti is the NE tag of wi
3. gi is the gazetteer feature of the word wi
4. fi is the most frequent NE tag seen in a large
corpus of wi
5. hi is the head word of the entity. We nor-
mally set the head word of an entity as its last
word. However, when a preposition exists in
the entity string, its head word is set as the
last word before the preposition. For exam-
ple, the head word of the entity ?University
of Pennsylvania? is ?University?.
6. Mixed n-grams features of the words and
their gazetteers/frequent-tag before/after the
start/end of an entity. In addition to the
normal n-grams solely based on words, we
mixed words with gazetteers/frequent-tag
seen from a large corpus and create mixed
n-grams features.
Table 2 shows the full set of global features in
our reranking framework. Features are anchored
to each entity instance and adapted to entity types.
This helps to discriminate different entities with
the same surface forms. Moreover, they can be
combined with n-grams patterns to learn and ex-
plicitly push the score of the correct sequence
above the score of competing sequences.
3.4 Reranking with Composite Kernel
In this section we describe our novel tagging ker-
nels based on diverse global features as well as
semantic trees for reranking candidate tagged se-
quences. As mentioned in the previous section,
we can engineer kernels by combining tree and
entity kernels. Thus we focus on the problem to
define structure embedding the desired relational
information among tagged sequences.
The Partial Tree Kernel
Let F = f1, f2, . . . , f|F | be a tree fragment
space of type PTs and let the indicator function
Ii(n) be equal to 1 if the target f1 is rooted at node
n and 0 otherwise, we define the PT kernel as:
K(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2)
where NT1 and NT2 are the set of nodes
in T1 and T2 respectively and ?(n1, n2) =?|F |
i=1 Ii(n1)Ii(n2), i.e. the number of common
fragments rooted at the n1 and n2 nodes of the
type shown in Figure 1.c.
The Polynomial Kernel
The polynomial kernel between two candidate
tagged sequences is defined as:
K(x, y) = (1 + ~x1 ? ~x2)2,
where ~x1 and ~x2 are two feature vectors extracted
from the two sequences with the global feature
template.
The Tagging Kernels
In our reranking framework, we incorporate the
probability from the original model with the tree
structure as well as the feature vectors. Let us con-
sider the following notations:
906
Feature Description
ws ws+1 . . . we Entity string
gs gs+1 . . . ge The gazetteer feature within the entity
fs fs+1 . . . fe The most frequent NE tag feature (seen from a
large corpus) within the entity
hw The head word of the entity
lhw Indicates whether the head word is lower-cased
ws?1 ws; ws?1 gs; gs?1 ws; gs?1 gs Mixed bigrams of the words/gazetteer features
before/after the start of the entity
we we+1; we ge+1; ge we+1; ge ge+1 Mixed bigrams of the words/gazetteer features
before/after the end of the entity
ws?1 ws; ws?1 fs; fs?1 ws; fs?1 fs Mixed bigrams of the words/frequent-tag fea-
tures before/after the start of the entity
we we+1; we fe+1; fe we+1; fe fe+1 Mixed bigrams of the words/frequent-tag fea-
tures before/after the end of the entity
ws?2 ws?1 ws; ws?1 ws ws+1; we?1 we we+1; we?2 we?1 we Trigram features of the words before/after the
start/end of the entity
ws?2 ws?1 gs; ws?2 gs?1 ws; ws?2 gs?1 gs;
gs?2 ws?1 ws; gs?2 ws?1 gs; gs?2 gs?1 ws; gs?2 gs?1 gs;
ws?1 ws gs+1; ws?1 gs ws+1; ws?1 gs gs+1;
gs?1 ws ws+1; gs?1 ws gs+1; gs?1 gs ws+1; gs?1 gs gs+1
Mixed trigrams of the words/gazetteer features
before/after the start of the entity
we?1 we ge+1; we?1 ge we+1; we?1 ge ge+1;
ge?1 we we+1; ge?1 we ge+1; ge?1 ge we+1; ge?1 ge ge+1;
we?2 we?1 ge; we?2 ge?1 we; we?2 ge?1 ge;
ge?2 we?1 we; ge?2 we?1 ge; ge?2 ge?1 we; ge?2 ge?1 ge
Mixed trigrams of the words/gazetteer features
before/after the end of the entity
ws?2 ws?1 fs; ws?2 fs?1 ws; ws?2 fs?1 fs;
fs?2 ws?1 ws; fs?2 ws?1 fs; fs?2 fs?1 ws; fs?2 fs?1 fs;
ws?1 ws fs+1; ws?1 fs ws+1; ws?1 fs fs+1;
fs?1 ws ws+1; fs?1 ws fs+1; fs?1 fs ws+1; fs?1 fs fs+1
Mixed trigrams of the words/frequent-tag fea-
tures before/after the start of the entity
we?1 we fe+1; we?1 fe we+1; we?1 fe fe+1;
fe?1 we we+1; fe?1 we fe+1; fe?1 fe we+1; fe?1 fe fe+1;
we?2 we?1 fe; we?2 fe?1 we; we?2 fe?1 fe;
fe?2 we?1 we; fe?2 we?1 fe; fe?2 fe?1 we; fe?2 fe?1 fe
Mixed trigrams of the words/frequent-tag fea-
tures before/after the end of the entity
Table 2: Global features in the entity kernel for reranking. These features are anchored for each entity
instance and adapted to entity categories. For example, the entity string (first feature) of the entity
?United Nations? with entity type ?ORG? is ?ORG United Nations?.
? K(x, y) = L(x) ? L(y) is the basic kernel
where L(x) is the log probability of a can-
didate tagged sequence x under the original
probability model.
? TK(x, y) = t(x) ? t(y) is the partial tree ker-
nel under the structure representation
? FK(x, y) = f(x) ? f(y) is the polynomial
kernel under the global features
The tagging kernels between two tagged se-
quences are defined in the following combina-
tions:
1. CTK = ? ?K + (1? ?) ? TK
2. CFK = ? ?K + (1? ?) ? FK
3. CTFK = ? ?K + (1? ?) ? (TK + FK)
where ?, ?, ? are parameters weighting the two
participating terms. Experiments on the validation
set showed that these combinations yield the best
performance with ? = 0.2 for both languages,
? = 0.4 for English and ? = 0.3 for and Italian,
? = 0.24 for English and ? = 0.2 for Italian.
4 Experimens and Results
4.1 Experimental Setup
As a baseline we trained the CRFs classifier on
the full training portion (11,227 sentences in the
Italian and 14,987 sentences in the English cor-
pus). In developing a reranking strategy for both
English and Italian, the training data was split into
5 sections, and in each case the baseline classifier
was trained on 4/5 of the data, then used to decode
the remaining 1/5.
907
The top 10 hypotheses together with their log
probabilities were recovered for each training sen-
tence. Similarly, a model trained on the whole
training data was used to produce 10 hypotheses
for each sentence in the development set. For the
reranking experiments, we applied different ker-
nel setups to the two corpora described in Section
2.1. The three kernels were trained on the training
portion.
Italian Test P R F
CRFs 83.43 77.48 80.34
CTK 84.97 78.03 81.35
CFK 84.93 79.13 81.93
CTFK 85.99 82.73 84.33
(Zanoli et al, 2009) 84.07 80.02 82.00
English Test P R F
CRFs 85.37 84.35 84.86
CTK 87.19 84.79 85.97
CFK 86.53 86.75 86.64
CTFK 88.07 88.25 88.16
(Ratinov and Roth, ) N/A N/A 90.57
Table 3: Reranking results of the three tagging
kernels on the Italian and English testset.
4.2 Discussion
Table 3 presents the reranking results on the test
data of both corpora. The results show a 20.29%
relative improvement in F-measure for Italian and
21.79% for English.
CFK based on unstructured features achieves
higher accuracy than CTK based on structured
features. However, the huge amount of subtrees
generated by the PT kernel may limit the expres-
sivity of some structural features, e.g. many frag-
ments may only generate noise. This problem is
less important with the polynomial kernel where
global features are tailored for individual entities.
In any case, the experiments demonstrate that
both tagging kernels CTK and CFK give im-
provement over the CRFs baseline in both lan-
guages. This suggests that structured and unstruc-
tured features are effective in discriminating be-
tween competing NE annotations.
Furthermore, the combination of the two tag-
ging kernels on both standard corpora shows a
large improvement in F-measure from 80.34% to
84.33% for Italian and from 84.86% to 88.16%
for English data. This suggests that these two ker-
nels, corresponding to two kinds of feature, com-
plement each other.
To better collocate our results with previous
work, we report the best NER outcome on the
Italian (Zanoli et al, 2009) and the English (Rati-
nov and Roth, ) datasets, in the last row (in italic)
of each table. This shows that our model outper-
forms the best Italian NER system and it is close
to the state-of-art model for English, which ex-
ploits many complex features9. Also note that we
are very close to the F1 achieved by the best sys-
tem of CoNLL 2003, i.e. 88.8.
5 Conclusion
We analyzed the impact of kernel-based ap-
proaches for modeling dependencies between
tagged sequences for NER. Our study illustrates
that each individual kernel, either with structured
or with flat features clearly gives improvement to
the base model. Most interestingly, as we showed,
these contributions are independent and, the ap-
proaches can be used together to yield better re-
sults. The composite kernel, which combines both
kinds of features, can outperform the state-of-the-
art.
In the future, it will be very interesting to
use syntactic/semantic kernels, as for example in
(Basili et al, 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). An-
other promising direction is the use of syntactic
trees, feature sequences and pairs of instances,
e.g. (Nguyen et al, 2009; Moschitti, 2008).
Acknowledgments
We would like to thank Roberto Zanoli and
Marco Dinarelli for helpful explanation
about their work. This work has been par-
tially funded by the LiveMemories project
(http://www.livememories.org/) and Expert
System (http://www.expertsystem.net/) research
grant.
9In the future we will be able to integrate them with the
authors collaboration.
908
References
Basili, Roberto, Marco Cammisa, and Alessandro
Moschitti. 2005. Effective use of WordNet seman-
tics via kernel-based learning. In CoNLL.
Bloehdorn, Stephan and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In ECIR.
Bloehdorn, Stephan and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In CIKM.
Bunescu, Razvan C. and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In EMNLP.
Carreras, Xavier, Llu??s Ma`rques, and Llus Padro?.
2002. Named entity extraction using Adaboost. In
CoNLL.
Chieu, Hai Leong and Hwee Tou Ng. 2002. Named
entity recognition: A maximum entropy approach
using global information. In COLING.
Chieu, Hai Leong and Hwee Tou Ng. 2003. Named
entity recognition with a maximum entropy ap-
proach. In CoNLL.
Chinchor, Nancy and Patricia Robinson. 1998. Muc-7
named entity task definition. In the MUC.
Collins, Michael and Nigel Duffy. 2001. Convolution
kernels for natural language. In NIPS.
Collins, Michael and Terry Koo. 2000. Discriminative
reranking for natural language parsing. In ICML.
Collins, Michael. 2002a. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL.
Collins, Michael. 2002b. Ranking algorithms for
named-entity extraction boosting and the voted per-
ceptron. In ACL.
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-ranking models based on small
training data for spoken language understanding. In
EMNLP.
Florian, Radu, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In CoNLL.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL-HLT.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Lodhi, Huma, Craig Saunders, John Shawe Taylor,
Nello Cristianini, , and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, pages 419?444.
Magnini, Bernardo, Emmanuele Pianta, Christian Gi-
rardi, Matteo Negri, Lorenza Romano, Manuela
Speranza, Valentina Bartalesi Lenzi, and Rachele
Sprugnoli. 2006. I-CAB: the italian content anno-
tation bank. In LREC.
Moschitti, Alessandro. 2004. A study on convolution
kernels for shallow semantic parsing. In ACL.
Moschitti, Alessandro. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ICML.
Moschitti, Alessandro. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Nguyen, Truc-Vien T., Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In EMNLP.
Ratinov, Lev and Dan Roth. Design challenges and
misconceptions in named entity recognition. In
CoNLL.
Rifkin, Ryan Michael and Tomaso Poggio. 2002. Ev-
erything old is new again: a fresh look at historical
approaches in machine learning. PhD thesis, MIT.
Sang, Erik F. Tjong Kim and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition.
In CoNLL.
Scho?lkopf, Bernhard and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA.
Shen, Libin, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative reranking for machine transla-
tion. In HLT-NAACL, Boston, Massachusetts, USA.
Surdeanu, Mihai, Sanda Harabagiu, John Williams,
and Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In ACL.
Vapnik, Vladimir N. 1998. Statistical Learning The-
ory. John Wiley and Sons, New York.
Vishwanathan, S.V.N. and Alexander J. Smola. 2002.
Fast kernels on strings and trees. In NIPS.
Zanoli, Roberto, Emanuele Pianta, and Claudio Giu-
liano. 2009. Named entity recognition through re-
dundancy driven classifiers. In EVALITA.
909
Coling 2008: Kernel Engineering for Fast and Easy Design of Natural Language Applications?Tutorial notes, pages 1?91,
Beijing, August 2010
Kernel Engineering for Fast and Easy 
Design of Natural Language Applications 
 Alessandro Moschitti 
Department of Information Engineering and Computer Science 
University of Trento 
Email: moschitti@disi.unitn.it 
The 23rd International Conference on Computational Linguistics August 22, 2010 Beijing, China 
Schedule 
 ? 14:00 - 15:30 First part 
 ? 15:30 - 16:00 Coffee break   
 ? 16:00 - 17:30 Second part 
1
Outline (1) 
 ? Motivation 
 ? Kernel-Based Machines 
 ? Perceptron 
 ? Support Vector Machines 
 ? Kernel Definition 
 ? Kernel Trick 
 ? Mercer?s conditions 
 ? Kernel operators 
 ? Basic Kernels 
 ? Linear Kernel 
 ? Polynomial Kernel 
 ? Lexical Kernel 
Outline (2) 
 ? Structural Kernels 
 ? String and Word Sequence Kernels  
 ?  Tree Kernels 
 ? Subtree, Syntactic, Partial Tree Kernels 
 ? Applied Examples of Structural Kernels 
 ? Semantic Role Labeling (SRL) 
 ? Question Classification (QC)  
 ? SVM-Light-TK 
 ? Experiments in classroom with SRL and QC 
 ? Inspection of the input, output, and model files 
2
Outline (3) 
 ? Kernel Engineering 
 ? Structure Transformation 
 ? Syntactic Semantic Tree kernels 
 ? Kernel Combinations 
 ? Kernels on Object Pairs 
 ? Kernels for re-ranking 
 ? Practical Question and Answer Classifier based on     SVM-Light-TK 
 ? Combining Kernels 
 ? Conclusion and Future Work 
Motivation (1) 
 ? Feature design most difficult aspect in designing a 
learning system 
 ? complex and difficult phase, e.g., structural feature 
representation: 
 ? deep knowledge and intuitions are required 
 ? design problems when the phenomenon is 
described by many features 
3
Motivation (2) 
 ? Kernel methods alleviate such problems 
 ? Structures represented in terms of substructures 
 ? High dimensional feature spaces 
 ? Implicit and abstract feature spaces 
 ? Generate high number of features 
 ? Support Vector Machines ?select? the relevant features 
 ? Automatic Feature engineering side-effect 
Part I: Kernel Methods Theory 
4
A simple classification problem: Text Categorization 
Sport   Cn 
Politic     C1 
Economic 
            C2 
. . . . . . . . . . . 
Bush 
declares 
war 
Wonderful 
Totti 
Yesterday 
match 
Berlusconi 
acquires 
Inzaghi 
before 
elections 
l i 
i  
i 
 
l ti  
Text Classification Problem 
 ? Given: 
 ? a set of target categories: 
 ? the set T of documents,  
     define 
       f : T  ?   2C 
 ? VSM (Salton89?) 
 ? Features are dimensions of a Vector Space. 
 ? Documents and Categories are vectors of feature weights. 
 ? d is assigned to        if  
  
? 
? 
d ?
? 
C 
i
> th
? 
C = C
1
,..,C
n
{ }
i
C
5
More in detail 
 ? In Text Categorization documents are word 
vectors 
 ? The dot product            counts the number of 
features in common 
 ? This provides a sort of similarity 
  
? 
?(d
x
) =
? 
x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1)
                         buy       acquisition     stocks          sell     market
zx
?
?
?
  
? 
?(d
z
) =
? 
z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0)
                         buy   company            stocks          sell     
Linear Classifier 
  
? 
f (
? 
x ) =
? 
x ?
? 
w + b = 0,   
? 
x ,
? 
w ? ?
n
,b ? ?
 ? The equation of a hyperplane is 
 ?    is the vector representing the classifying example 
 ?    is the gradient of the hyperplane 
 ? The classification function is 
x
?
w
?
( ) sign( ( ))h x f x=
6
 ? Mapping vectors in a space where they are linearly separable 
x
x
x
x
o
o
o
o
The main idea of Kernel Functions 
)(xx
?? ??
)x(?
)x(?)x(?)x(?)(o? )(o?
)(o?
)(o??
A mapping example 
 ? Given two masses m1 and m2 , one is constrained 
 ? Apply a force fa to the mass m1   
 ? Experiments 
 ? Features m1 , m2 and  fa 
 ? We want to learn a classifier that tells when a mass m1 will get far away from m2  
2
21
21
),,(
r
mm
Crmmf =
 ? If we consider the Gravitational Newton Law 
 ? we need to find when f(m1 , m2 , r) < fa 
7
A mapping example (2) 
))(),...,(()(),...,(
11
xxxxxx
nn
???? ??? =?=
 ? The gravitational law is not linear so we need to change space 
)ln,ln,ln,(ln),,,(),,,(
2121
rmmfzyxkrmmf
aa
=?
zyxcrmmCrmmf 2ln2lnlnln),,(ln
2121
?++=?++=
(ln m1,ln m2,-2ln r)? (x,y,z)- ln fa + ln C = 0, we can decide without error if the mass will get far away or not 
 ? As 
0lnln2lnlnln
21
=?+?? Crmmf
a
 ? We need the hyperplane 
A kernel-based Machine Perceptron training 
  
? 
? 
w 
0
?
? 
0 ;b
0
? 0;k ? 0;R? max
1? i? l
||
? 
x 
i
||
do
       for i =  1 to ?
         if y
i
(
? 
w 
k
?
? 
x 
i
+ b
k
) ? 0 then
                  
? 
w 
k +1
=
? 
w 
k
+?y
i
? 
x 
i
                  b
k +1
= b
k
+?y
i
R
2
                 k = k +1
        endif
      endfor
while an error is found
return k,(
? 
w 
k
,b
k
) 
8
9
Novikoff?s Theorem 
Let S be a non-trivial training-set and let 
Let us suppose there is a vector           and 
with ? > 0. Then the maximum number of errors of the perceptron is: 
* *
, || || 1 =w w
* *
( , ) , 1,..., ,
i i
y b i l?+ ?      =w x
2
*
2
,
R
t ?? ?= ? ?
? ?
1
max || || .
i
i l
R x
? ?
=
10
 ? In each step of perceptron only training data is added with a certain weight 
 ? So the classification function 
 ? Note that data only appears in the scalar product 
Dual Representation for Classification 
  
? 
? 
w = ?
j
j=1..?
?
y
j
? 
x 
j
  
? 
sgn(
? 
w ?
? 
x + b) = sgn ?
j
j=1..?
?
y
j
? 
x 
j
?
? 
x + b
? 
? 
? 
? 
? 
? 
? 
? 
Dual Representation for Learning 
 ? as well as the updating function  
 ? The learning rate      only affects the re-scaling of the hyperplane, it does not affect the algorithm, so we can fix 1.? = ?  ? if yi( ? jj=1..?? y j ? x j ? ? x i + b) ? 0 then ? i =? i +?
11
 ? We can rewrite the classification function as 
 ? As well as the updating function 
Dual Perceptron algorithm and Kernel functions 
  
? 
h(x) = sgn(
? 
w ? ? ?(? x ) + b? ) = sgn( ? j
j=1..?
?
y
j
?(? x 
j
) ? ?(? x ) + b? ) =
= sgn( ?
j
i=1..?
?
y
j
k(
? 
x 
j
,
? 
x ) + b? )
  
? 
if y
i
?
j
j=1..?
?
y
j
k(
? 
x 
j
,
? 
x 
i
) + b?? 
? 
? 
? 
? 
? 
? 
? 
? 0 allora ?
i
=?
i
+?
Support Vector Machines 
 ? Hard-margin SVMs 
 ? Soft-margin SVMs 
12
Which hyperplane do we choose? 
Classifier with a Maximum Margin 
Var1 
Var2 
Margin 
Margin 
IDEA 1: Select the hyperplane with maximum margin 
13
Support Vectors 
Var1 
Var2 
Margin 
Support Vectors 
Support Vector Machines 
Var1 
Var2 kbxw ?=+? ??
kbxw =+?
??
0=+? bxw
??
k
k
w
?
The margin is equal to 2 k
w
14
Support Vector Machines 
Var1 
Var2 kbxw ?=+? ??
kbxw =+?
??
0=+? bxw
??
k
k
w
?
The margin is equal to 2 k
w
We need to solve 
  
? 
max
2 k
||
? 
w ||
? 
w ?
? 
x + b ? +k,   if 
? 
x is positive  
? 
w ?
? 
x + b ? ?k,   if 
? 
x is negative 
Support Vector Machines 
Var1 
Var2 1w x b? + = ?? ?
1w x b? + =
? ?
0=+? bxw
??
1
1
w
?
There is a scale for which k=1.  
The problem transforms in: 
  
?  
max
2
||
? 
w ||
? 
w ?
? 
x + b ? +1,  if 
? 
x is positive  
? 
w ?
? 
x + b ? ?1,  if 
? 
x is negative 
15
Final Formulation 
? 
?
  
? 
max
2
||
? 
w ||
? 
w ?
? 
x 
i
+ b ? +1,  y
i
=1
? 
w ?
? 
x 
i
+ b ? ?1,  y
i
 = -1
  
? 
max
2
||
? 
w ||
y
i
(
? 
w ?
? 
x 
i
+ b) ?1
  
? 
min
||
? 
w ||
2
y
i
(
? 
w ?
? 
x 
i
+ b) ?1
  
?  
min
||
? 
w ||
2
2
y
i
(
? 
w ?
? 
x 
i
+ b) ?1
? 
?
? 
?
? 
?
Optimization Problem 
 ? Optimal Hyperplane: 
 ? Minimize 
 ? Subject to 
 ? The dual problem is simpler 
   
libxwy
ww
ii
,...,1,1))((
2
1
)(
2
=?+?
=
??
???
16
Lagrangian Definition 
Dual Optimization Problem 
17
Dual Transformation 
 ? To solve the dual problem we need to evaluate: 
 ? Given the Lagrangian associated with our problem 
 ? Let us impose the derivatives to 0, with respect to   w?
Dual Transformation (cont?d) 
 ? and wrt b 
 ? Then we substituted them in the objective function 
18
The Final Dual Optimization Problem 
Khun-Tucker Theorem 
 ? Necessary and sufficient conditions to optimality 
19
Properties coming from constraints 
 ? Lagrange constraints: 
 ? Karush-Kuhn-Tucker constraints 
 ? Support Vectors have     not null 
 ? To evaluate b, we can apply the following equation 
  
? 
a
i
i=1
l
?
y
i
= 0,
? 
w = ?
i
i=1
l
?
y
i
? 
x 
i
libwxy
iii
,...,1,0]1)([ ==?+??
???
i
?
Soft Margin SVMs 
Var1 
Var2 1w x b? + = ?? ?
1w x b? + =
? ?
0=+? bxw
??
1
1
w
?
i
?    slack variables are added 
Some errors are allowed but they should penalize the objective function 
i
?
20
Soft Margin SVMs 
Var1 
Var2 1w x b? + = ?? ?
1w x b? + =
? ?
0=+? bxw
??
1
1
w
?
i
? The new constraints are 
The objective function penalizes the incorrect classified examples 
C is the trade-off between margin and the error 
  
? 
y
i
(
? 
w ?
? 
x 
i
+ b) ?1??
i
   
?
? 
x 
i
  where  ?
i
? 0
  
?  
min
1
2
||
? 
w ||
2
+C ?
i
i
?
Dual formulation 
 ? By deriving wrt 
  
? 
? 
w ,
? ? and b
21
Partial Derivatives 
Substitution in the objective function 
 ?      of Kronecker  
ij
?
22
Final dual optimization problem 
Soft Margin Support Vector Machines 
 ? The algorithm tries to keep ?i low and maximize the margin 
 ? NB: The number of error is not directly minimized (NP-complete 
problem); the distances from the hyperplane are minimized 
 ? If C??, the solution tends to the one of the hard-margin algorithm 
 ? Attention !!!: if C = 0 we get          = 0, since  
 ? If C increases the number of error decreases. When C tends to 
infinite the number of errors must be 0, i.e. the hard-margin 
formulation 
|||| w
?
  
? 
min
1
2
||
? 
w ||
2
+C ?
i
i
?
  
? 
y
i
(
? 
w ?
? 
x 
i
+ b) ?1??
i
   ?
? 
x 
i?
i
? 0
  
? 
y
i
b ?1??
i
   ?
? 
x 
i
23
Robusteness of Soft vs. Hard Margin SVMs 
i
?Var1 
Var2 
0=+? bxw
??
?i 
Var1 
Var2 
0=+? bxw
??
Soft Margin SVM Hard Margin SVM 
Kernels in Support Vector Machines  
 ? In Soft Margin SVMs we maximize: 
 ? By using kernel functions we rewrite the problem as: 
24
Kernel Function Definition 
 ? Kernels are the product of mapping functions 
such as 
  
? 
? 
x ? ?
n
,     
? ? (? x ) = (?
1
(
? 
x ),?
2
(
? 
x ),...,?
m
(
? 
x )) ? ?
m
The Kernel Gram Matrix 
 ? With KM-based learning, the sole information used from the training data set is the Kernel Gram Matrix 
 ? If the kernel is valid, K is symmetric definite-positive . 
25
Valid Kernels 
Valid Kernels cont?d 
 ? If the matrix is positive semi-definite then we can find a mapping ? implementing the kernel function 
26
Mercer?s Theorem (finite space) 
 ? Let us consider 
  
? 
K =  K(
? 
x 
i
,
? 
x 
j
)
( )
i, j=1
n
 ? K symmetric ? ? V:                      for Takagi factorization of a 
complex-symmetric matrix, where:  
 ? ? is the diagonal matrix of the eigenvalues ?t of K  
 ?                        are the eigenvectors, i.e. the columns of V 
 ? Let us assume lambda values non-negative 
? 
K = V?
? 
V
  
? 
? 
v 
t
 =  v
ti
( )
i =1
n
  
? 
? : ? x 
i
 ?  ?
t
v
ti
( )
t =1
n
? ?
n
, i =1,..,n
Mercer?s Theorem (sufficient conditions) 
  
? 
?(
? 
x 
i
) ? ?(
? 
x 
j
) = ?
t
v
ti
t=1
n
?
v
tj
= V?
? 
V 
( )
ij
= K
ij
= K(
? 
x 
i
,
? 
x 
j
)
     
 ? Therefore 
                                                                    ,  
 ? which implies that K is a kernel function       
27
Mercer?s Theorem (necessary conditions) 
  
? 
? 
z 
2
=
? 
z ?
? 
z = ?
? 
V 
? 
v 
s
?
? 
V 
? 
v 
s
=
? 
v 
s
' V ? ?
? 
V 
? 
v 
s
=
 
? 
v 
s
' K
? 
v 
s
=  
? 
v 
s
' ?
s
? 
v 
s
= ?
s
? 
v 
s
2
< 0
 ? Suppose we have negative eigenvalues ?s and eigenvectors       the following point 
 ? has the following norm: 
this contradicts the geometry of the space. 
  
? 
? 
v
s
  
? 
? 
z = v
si
?(
? 
x 
i
)
i=1
n
?
= v
si
?
t
v
ti
( )
t
=
i=1
n
?
?
? 
V 
? 
v 
s
   
Is it a valid kernel? 
 ? It may not be a kernel so we can use M??M 
28
Valid Kernel operations 
 ? k(x,z) = k1(x,z)+k2(x,z) 
 ? k(x,z) = k1(x,z)*k2(x,z) 
 ? k(x,z) = ? k1(x,z) 
 ? k(x,z) = f(x)f(z) 
 ? k(x,z) = k1(?(x),?(z)) 
 ? k(x,z) = x'Bz 
Basic Kernels for unstructured data 
 ? Linear Kernel 
 ? Polynomial Kernel 
 ? Lexical kernel 
 ? String Kernel 
29
Linear Kernel 
 ? In Text Categorization documents are word 
vectors 
 ? The dot product            counts the number of 
features in common 
 ? This provides a sort of similarity 
  
? 
?(d
x
) =
? 
x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1)
                         buy       acquisition     stocks          sell     market
zx
?
?
?
  
? 
?(d
z
) =
? 
z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0)
                         buy   company            stocks          sell     
Feature Conjunction (polynomial Kernel) 
 ? The initial vectors are mapped in a higher space 
 ? More expressive, as            encodes  
      Stock+Market vs. Downtown+Market features 
 ? We can smartly compute the scalar product as 
)1,2,2,2,,(),(
2121
2
2
2
121
xxxxxxxx ?><?
),()1()1(        
1222        
)1,2,2,2,,()1,2,2,2,,(         
)()(
22
2211
22112121
2
2
2
2
2
1
2
1
2121
2
2
2
12121
2
2
2
1
zxKzxzxzx
zxzxzzxxzxzx
zzzzzzxxxxxx
zx
Poly
?
?
?
?
?
?
=+?=++=
=+++++=
=?=
=???
)(
21
xx
30
Document Similarity 
industry 
telephone 
 market 
company 
product 
Doc 1 Doc 2 
Lexical Semantic Kernel [CoNLL 2005] 
 ? The document similarity is the SK function: 
 ? where s is any similarity function between words, 
e.g. WordNet [Basili et al,2005] similarity or LSA 
[Cristianini et al, 2002] 
 ? Good results when training data is small ? 
SK(d
1
,d
2
) = s(w
1
,w
2
)
w
1
?d
1
,w
2
?d
2
?
31
Using character sequences 
zx
?
?
?
  
? 
?("bank") = ? x = (0,..,1,..,0,..,1,..,0,......1,..,0,..,1,..,0,..,1,..,0)
 ?          counts the number of common substrings 
 bank       ank           bnk          bk          b 
  
? 
?("rank") = ? z = (1,..,0,..,0,..,1,..,0,......0,..,1,..,0,..,1,..,0,..,1)
 rank               ank                  rnk          rk            r 
  
? 
? 
x ?
? 
z = ?("bank") ? ?("rank") = k("bank","rank")
String Kernel 
 ? Given two strings, the number of matches 
between their substrings is evaluated 
 ? E.g. Bank and Rank 
 ? B, a, n, k, Ba, Ban, Bank, Bk, an, ank, nk,.. 
 ? R, a , n , k, Ra, Ran, Rank, Rk, an, ank, nk,.. 
 ? String kernel over sentences and texts 
 ? Huge space but there are efficient algorithms 
32
Formal Definition 
,  where 
,  where 
i1 +1 
Kernel between Bank and Rank 
33
An example of string kernel computation 
Efficient Evaluation 
 ? Dynamic Programming technique 
 ? Evaluate the spectrum string kernels 
 ? Substrings of size p 
 ? Sum the contribution of the different spectra 
34
Efficient Evaluation 
An example: SK(?Gatta?,?Cata?) 
 ? First, evaluate the SK with size p=1, i.e. ?a?, 
?a?,?t?,?t?,?a?,?a? 
 ? Store this in the table 
? 
  SK
p=1
  
35
Evaluating DP2 
 ? Evaluate the weight of the string of size p in case 
a character will be matched  
 ? This is done by multiplying the double summation 
by the number of substrings of size p-1 
Evaluating the Predictive DP on strings of size 2 (second row) 
 ? Let?s consider substrings of size 2 and suppose that: 
 ? we have matched the first ?a? 
 ? we will match the next character that we will add to the two strings 
 ? We compute the weights of matches above at different string 
positions with some not-yet known character ??? 
 ? If the match occurs immediately after ?a? the weight will be ?1+1 
x ?1+1 = ?4 and we store just ?2 in the DP entry in [?a?,?a?] 
36
Evaluating the DP wrt different positions (second row) 
 ? If the match for ?gatta? occurs after ?t? the weight will be ?1+2 
(x ?2 = ?5) since the substring for it will be with ?a???  
 ?  We write such prediction in the entry [?a?,?t?] 
 ? Same rationale for a match after the second ?t?: we have 
the substring ?a????  (matching with ?a?? from ?catta?) for 
a weight of ?3+1  (x ?2) 
Evaluating the DP wrt different positions (third row) 
 ? If the match occurs after ?t? of ?cata?, the weight will be ?2+1  
(x ?2 = ?5 ) since it will be with the string ?a???, with a weight 
of ?3  
 ? If the match occurs after ?t? of both ?gatta? and ?cata?, there 
are two ways to compose substring of size two: ?a??? with 
weight ?4 or ?t?? with weight ?2 ? the total is ?2+?4  
37
Evaluating the DP wrt different positions (third row) 
 ? The final case is a match after the last ?t? of both ?cat? and 
?gatta? 
 ?  There are three possible substrings of ?gatta?: 
 ? ?a????, ?t???, ?t?? for ?gatta? with weight ?3 , ?2 or ?, respectively. 
 ? There are two possible substrings of ?cata? 
 ?  ?a???, ?t?? with weight ?2 and ? 
 ? Their match gives weights: ?5 , ?3, ?2  ? by summing: ?5 + ?3 + ?2 
Evaluating SK of size 2 using DP2 
 ? The number (weight) of 
substrings of size 2 between 
?gat? and ?cat? is ?4 = ?2 
([?a?,?a?] entry of DP) x ?2(cost 
of one character), where a = 
?t? and   b = ?t?. 
 ? Between ?gatta? and ?cata? is 
?7 + ?5 + ?4, i.e the matches of 
?a??a?, ?t?a?, ?ta? with 
?a?a? and ?ta?. ? 
  SK
p= 2
  
38
Tree kernels 
 ? Subtree, Subset Tree, Partial Tree kernels 
 ? Efficient computation 
Example of a parse tree 
 ? ?John delivers a talk in Rome? 
S ? N VP 
VP ? V NP PP 
PP ? IN N 
N ? Rome 
N 
Rome 
S 
N 
NP 
D N 
VP 
V John 
in 
 delivers  
a talk 
PP 
IN 
39
The Syntactic Tree Kernel (STK)  
[Collins and Duffy, 2002] 
NP 
D N 
VP 
V 
delivers 
a    talk 
The overall fragment set 
40
The overall fragment set 
NP 
D 
VP 
a Children are not divided 
Explicit kernel space 
zx
?
?
?
  
? 
?(T
x
) =
? 
x = (0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0)
 ?          counts the number of common substructures 
  
? 
?(T
z
) =
? 
z = (1,..,0,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,0,..,1,..,0,..,0)
41
Efficient evaluation of the scalar product 
  
? 
? 
x ?
? 
z = ?(T
x
) ? ?(T
z
) = K(T
x
,T
z
) =
                    =
n
x
?T
x
?
?(n
x
,n
z
)
n
z
?T
z
?
Efficient evaluation of the scalar product 
 ? [Collins and Duffy, ACL 2002] evaluate ? in O(n2): 
? 
?(n
x
,n
z
) = 0,  if the productions are different else
?(n
x
,n
z
) =1,   if pre - terminals else
?(n
x
,n
z
) = (1+ ?(ch(n
x
, j),ch(n
z
, j)))
j=1
nc(n
x
)
?
  
? 
? 
x ?
? 
z = ?(T
x
) ? ?(T
z
) = K(T
x
,T
z
) =
                    =
n
x
?T
x
?
?(n
x
,n
z
)
n
z
?T
z
?
42
Other Adjustments 
 ? Normalization 
? 
?(n
x
,n
z
) = ?,    if pre - terminals else
?(n
x
,n
z
) = ? (1+ ?(ch(n
x
, j),ch(n
z
, j)))
j=1
nc(n
x
)
?
? 
? 
K (T
x
,T
z
) =
K(T
x
,T
z
)
K(T
x
,T
x
) ?K(T
z
,T
z
)
 
 ? Decay factor 
SubTree (ST) Kernel [Vishwanathan and Smola, 2002] 
 
NP 
D 
N 
a 
  talk  
D N 
a   talk  
NP 
D N 
VP 
V 
delivers  
a 
   talk  
V 
delivers  
43
Evaluation 
? 
?(n
x
,n
z
) = 0,  if the productions are different else
?(n
x
,n
z
) =1,   if pre - terminals else
?(n
x
,n
z
) = (1+ ?(ch(n
x
, j),ch(n
z
, j)))
j=1
nc(n
x
)
?
 ? Given the equation for the SST kernel 
Evaluation 
? 
?(n
x
,n
z
) = 0,  if the productions are different else
?(n
x
,n
z
) =1,   if pre - terminals else
?(n
x
,n
z
) = ?(ch(n
x
, j),ch(n
z
, j))
j=1
nc(n
x
)
?
 ? Given the equation for the SST kernel 
44
Fast Evaluation of STK [Moschitti, EACL 2006] 
where P(nx) and P(nz) are the production rules used 
at nodes nx and nz 
? 
K(T
x
,T
z
) =  ?(n
x
,n
z
)
n
x
,n
z
?NP
?
NP = n
x
,n
z
? T
x
?T
z
:?(n
x
,n
z
) ? 0
{ }
=
      = n
x
,n
z
? T
x
?T
z
: P(n
x
) = P(n
z
)
{ }
,
Algorithm 
45
Observations 
 ? We order the production rules used in Tx and Tz,  at loading time 
 ? At learning time we may evaluate NP in  
    |Tx|+|Tz | running time 
 ? If Tx and Tz are generated by only one production 
rule ? O(|Tx|?|Tz | )? 
Observations 
 ? We order the production rules used in Tx and Tz,  at loading time 
 ? At learning time we may evaluate NP in  
    |Tx|+|Tz | running time 
 ? If Tx and Tz are generated by only one production 
rule ? O(|Tx|?|Tz | )?Very Unlikely!!!! 
46
Labeled Ordered Tree Kernel 
NP 
D N 
VP 
V 
   gives 
a   talk 
NP 
D N 
VP 
V 
a    talk 
NP 
D N 
VP 
a    talk 
NP 
D N 
VP 
a 
NP 
D
VP 
a 
NP 
D
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D
NP ? 
VP 
 ? SST satisfies the constraint ?remove 0 or all 
children at a time?. 
 ? If we relax such constraint we get more general 
substructures [Kashima and Koyanagi, 2002] 
Weighting Problems 
 ? Both matched pairs give the 
same contribution. 
 ? Gap based weighting is 
needed. 
 ? A novel efficient evaluation 
has to be defined 
NP 
D N 
VP 
V 
   gives 
a   talk 
NP 
D N 
VP 
V 
a    talk 
NP 
D N 
VP 
V 
   gives 
a   talk 
   gives 
JJ 
  good 
NP 
D N 
VP 
V 
   gives 
a   talk 
JJ 
  bad 
47
Partial Trees, [Moschitti, ECML 2006] 
NP 
D N 
VP 
V 
brought 
a    cat 
NP 
D N 
VP 
V 
a    cat 
NP 
D N 
VP 
a    cat 
NP 
D N 
VP 
a 
NP 
D
VP 
a 
NP 
D
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D
NP ? 
VP 
 ? SST + String Kernel with weighted gaps on 
Nodes? children 
Partial Tree Kernel 
 ? By adding two decay factors we obtain: 
48
Efficient Evaluation (1) 
 ? In [Taylor and Cristianini, 2004 book], sequence kernels with 
weighted gaps are factorized with respect to different 
subsequence sizes. 
 ? We treat children as sequences and apply the same theory 
Dp 
Efficient Evaluation (2) 
 ? The complexity of finding the subsequences is             
 ? Therefore the overall complexity is 
    where ?  is the maximum branching factor (p = ?) 
49
Running Time of Tree Kernel Functions 
SVM-light-TK Software 
 ? Encodes ST, SST and combination kernels  
    in SVM-light [Joachims, 1999] 
 ? Available at http://dit.unitn.it/~moschitt/ 
 ? Tree forests, vector sets 
 ? The new SVM-Light-TK toolkit will be released 
asap 
50
Data Format 
 ? ?What does Html stand for?? 
 ? 1  |BT| (SBARQ (WHNP (WP What))(SQ (AUX does)(NP (NNP S.O.S.))(VP (VB stand)(PP (IN for))))(. ?))  
|BT|    (BOW (What *)(does *)(S.O.S. *)(stand *)(for *)(? *))  
|BT|    (BOP (WP *)(AUX *)(NNP *)(VB *)(IN *)(. *))  
|BT|   (PAS (ARG0 (R-A1 (What *)))(ARG1 (A1 (S.O.S. NNP)))(ARG2 (rel stand)))  
|ET| 1:1 21:2.742439465642236E-4 23:1 30:1 36:1 39:1 41:1 46:1 49:1 66:1 152:1 274:1 333:1  
|BV| 2:1 21:1.4421347148614654E-4 23:1 31:1 36:1 39:1 41:1 46:1 49:1 52:1 66:1 152:1 246:1 333:1 392:1 |EV|  
Basic Commands 
 ? Training and classification 
 ? ./svm_learn -t 5 -C T train.dat model 
 ? ./svm_classify test.dat model 
 ? Learning with a vector sequence 
 ? ./svm_learn -t 5 -C V train.dat model 
 ? Learning with the sum of vector and kernel 
sequences 
 ? ./svm_learn -t 5 -C + train.dat model 
51
Part II: Kernel Methods for Practical Applications 
Kernel Engineering approaches 
 ? Basic Combinations 
 ? Canonical Mappings, e.g. object transformations  
 ? Merging of Kernels 
52
Kernel Combinations an example 
 ? Kernel Combinations: 
3
3
3
3
33
          , 
              , 
pTree
pTree
PTree
p
p
Tree
Tree
PTree
pTreePTreepTreePTree
KK
KK
K
K
K
K
K
K
KKKKKK
?
?
=+?=
?=+?=
?+
?+ ??kernel Tree featuresflat    of  kernel  polynomial  3TreepKK
Object Transformation [Moschitti et al CLJ 2008] 
 ? Canonical Mapping, ?M()  
 ? object transformation, 
 ? e. g. a syntactic parse tree into a verb subcategorization frame tree. 
 ? Feature Extraction, ?E() 
 ? maps the canonical structure in all its fragments 
 ? different fragment spaces, e. g. ST, SST and PT. 
                    
),()()(                
))(())(()()(),(
2121
212121
SSKSS
OOOOOOK
EEE
MEME
=?=
?=?= ?? ??????
53
Predicate Argument Classification 
 ? In an event: 
 ? target words describe relation among different entities 
 ? the participants are often seen as predicate's arguments. 
 ? Example: 
Paul gives a talk in Rome 
Predicate Argument Classification 
 ? In an event: 
 ? target words describe relation among different entities 
 ? the participants are often seen as predicate's arguments. 
 ? Example: 
[ Arg0 Paul] [ predicate gives ] [ Arg1 a talk] [ ArgM in Rome] 
54
Predicate-Argument Feature Representation 
Given a sentence, a predicate p: 
1.? Derive the sentence parse tree 
2.? For each node pair <Np,Nx>  
a.? Extract a feature representation set F 
b.? If Nx exactly covers the Arg-i, F is one of its positive examples 
c.? F is a negative example otherwise 
Vector Representation for the linear kernel 
Phrase Type 
Predicate 
Word 
Head Word 
Parse Tree 
Path 
Voice Active 
osition Right 
55
Kernel Engineering: Tree Tailoring 
PAT Kernel [Moschitti, ACL 2004] 
S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a    talk 
PP 
IN   NP 
jj 
Fv,arg.0 
 formal 
 N 
      style 
Arg. 0 
a) S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a    talk 
PP 
IN   NP 
jj 
 formal 
 N 
      style 
Fv,arg.1 b) S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a    talk 
PP 
IN   NP 
jj 
 formal 
 N 
      style Arg. 1 
Fv,arg.M 
c) 
Arg.M 
 ? These are Semantic Structures 
 ? Given the sentence: 
  [ Arg0 Paul] [ predicate delivers] [ Arg1 a talk] [ ArgM in formal Style] 
56
In other words we consider? 
NP 
D N 
VP 
V 
delivers 
a    talk 
S 
N 
Paul 
in 
PP 
IN   NP 
jj 
 formal 
 N 
     style Arg. 1 
Sub-Categorization Kernel (SCF) [Moschitti, ACL 2004] 
S 
N 
NP 
D N 
VP 
V Paul 
in 
delivers 
a    talk 
PP 
IN   NP 
jj 
 formal 
 N 
     style 
Arg. 1 
Arg. M 
Arg. 0 
Predicate 
57
Experiments on Gold Standard Trees 
 ? PropBank and PennTree bank 
 ? about 53,700 sentences 
 ? Sections from 2 to 21 train., 23 test., 1 and 22 dev. 
 ? Arguments from Arg0 to Arg5, ArgA and ArgM for 
    a total of 122,774 and 7,359 
 ? FrameNet and Collins? automatic trees 
 ? 24,558 sentences from the 40 frames of Senseval 3 
 ? 18 roles (same names are mapped together) 
 ? Only verbs  
 ? 70% for training and 30% for testing 
Argument Classification with Poly Kernel 
58
PropBank Results 
Argument Classification on PAT using different Tree Fragment Extractor 
0.75
0.78
0.80
0.83
0.85
0.88
0 10 20 30 40 50 60 70 80 90 100
% Training Data
A
c
c
u
r
a
c
y
 
 
 
-
-
-
ST SST
Linear PT
59
FrameNet Results 
 ? ProbBank arguments vs. Semantic Roles  
Kernel Engineering: Node marking 
60
Marking Boundary nodes 
Node Marking Effect  
61
Different tailoring and marking 
CMST 
MMST 
Experiments 
 ? PropBank and PennTree bank 
 ? about 53,700 sentences 
 ? Charniak trees from CoNLL 2005 
 ? Boundary detection: 
 ? Section 2 training 
 ? Section 24 testing 
 ? PAF and MPAF 
62
Number of examples/nodes of Section 2 
Predicate Argument Feature (PAF) vs. Marked PAF (MPAF) [Moschitti et al ACL-ws-2005] 
63
Merging of Kernels [ECIR 2007]: Question/Answer Classification 
 ? Syntactic/Semantic Tree Kernel 
 ? Kernel Combinations 
 ? Experiments 
Merging of Kernels [Bloehdorn & Moschitti, ECIR 
2007 & CIKM 2007] 
64
Merging of Kernels 
NP 
D N 
VP 
V 
   gives 
a   talk 
N 
 good  
NP 
D N 
VP 
V 
   gives 
a   talk 
N 
  solid 
Delta Evaluation is very simple 
65
Question Classification 
 ? Definition: What does HTML stand for?    
 ? Description: What's the final line in the Edgar Allan Poe poem "The Raven"?   
 ? Entity: What foods can cause allergic reaction in people? 
 ? Human: Who won the Nobel Peace Prize in 1992?   
 ? Location: Where is the Statue of Liberty?    
 ? Manner: How did Bob Marley die?     
 ? Numeric: When was Martin Luther King Jr. born?   
 ? Organization: What company makes Bentley cars?  
Question Classifier based on Tree Kernels 
 ? Question dataset (http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/)   
[Lin and Roth, 2005]) 
 ? Distributed on 6 categories: Abbreviations, Descriptions, Entity, Human, Location, and Numeric. 
 ? Fixed split 5500 training and 500 test questions  
 ? Cross-validation (10-folds) 
 ? Using the whole question parse trees 
 ? Constituent parsing 
 ? Example 
        ?What is an offer of direct stock purchase plan ?? 
66
Kernels 
 ? BOW, POS are obtained with a simple tree, e.g. 
 ? PT (parse tree) 
 ? PAS (predicate argument structure) 
? 
BOX 
is What an offer an 
* * * * * 
67
Question classification 
Similarity based on WordNet 
68
Question Classification with S/STK 
Multiple Kernel Combinations 
69
TASK: Question/Answer Classification [Moschitti, CIKM 2008] 
 ? The classifier detects if a pair (question and 
answer) is correct or not 
 ? A representation for the pair is needed 
 ? The classifier can be used to re-rank the output of 
a basic QA system 
Dataset 2: TREC data 
 ? 138 TREC 2001 test questions labeled as 
?description?  
 ? 2,256 sentences, extracted from the best ranked 
paragraphs (using a basic QA system based on 
Lucene search engine on TREC dataset) 
 ?  216 of which labeled as correct by one annotator 
70
Dataset 2: TREC data 
 ? 138 TREC 2001 test questions labeled as 
?description?  
 ? 2,256 sentences, extracted from the best ranked 
paragraphs (using a basic QA system based on 
Lucene search engine on TREC dataset) 
 ?  216 of which labeled as correct by one annotator 
A question is linked to many answers: all its derived 
pairs cannot be shared by training and test sets 
Bags of words (BOW) and POS-tags (POS) 
 ? To save time, apply STK to these trees: 
? 
BOX 
is What an offer of 
* * * * * 
? 
BOX 
VBZ WHNP DT NN IN 
* * * * * 
71
Word and POS Sequences 
 ? What is an offer of?? (word sequence, WSK) 
 ? What_is_offer 
 ? What_is 
 ? WHNP VBZ DT NN IN?(POS sequence, POSSK) 
 ? WHNP_VBZ_NN 
 ? WHNP_NN_IN 
Syntactic Parse Trees (PT) 
72
Predicate Argument Structure for Partial Tree Kernel (PASPTK) 
 ? [ARG1 Antigens] were [AM?TMP originally] [rel defined] [ARG2 as non-
self molecules]. 
 ? [ARG0 Researchers] [rel describe] [ARG1 antigens][ARG2 as foreign 
molecules] [ARGM?LOC in the body] 
Kernels and Combinations 
 ? Exploiting the property: k(x,z) = k1(x,z)+k2(x,z) 
 ? BOW, POS, WSK, POSSK, PT, PASPTK 
? BOW+POS, BOW+PT, PT+POS, ? 
73
Results on TREC Data (5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW POS POS_
SK WSK PT 
PAS_S
STK PAS_P
TK 
BOW+
POS BOW+
PT 
POS_
SK+P
T 
WSK+
PT 
POS_
SK+P
T+PAS
_SST
K 
POS_
SK+P
T+PAS
_PTK
 
F1-
me
asu
re 
Kernel Type 
Results on TREC Data (5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW POS POS_
SK WSK PT 
PAS_S
STK PAS_P
TK 
BOW+
POS BOW+
PT 
POS_
SK+P
T 
WSK+
PT 
POS_
SK+P
T+PAS
_SST
K 
POS_
SK+P
T+PAS
_PTK
 
F1-
me
asu
re 
Kernel Type 
74
Results on TREC Data (5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW POS POS_
SK WSK PT 
PAS_S
STK PAS_P
TK 
BOW+
POS BOW+
PT 
POS_
SK+P
T 
WSK+
PT 
POS_
SK+P
T+PAS
_SST
K 
POS_
SK+P
T+PAS
_PTK
 
F1-
me
asu
re 
Kernel Type 
Results on TREC Data (5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW POS POS_
SK WSK PT 
PAS_S
STK PAS_P
TK 
BOW+
POS BOW+
PT 
POS_
SK+P
T 
WSK+
PT 
POS_
SK+P
T+PAS
_SST
K 
POS_
SK+P
T+PAS
_PTK
 
F1-
me
asu
re 
Kernel Type 
75
Results on TREC Data (5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW POS POS_
SK WSK PT 
PAS_S
STK PAS_P
TK 
BOW+
POS BOW+
PT 
POS_
SK+P
T 
WSK+
PT 
POS_
SK+P
T+PAS
_SST
K 
POS_
SK+P
T+PAS
_PTK
 
F1-
me
asu
re 
Kernel Type 
Results on TREC Data (5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW POS POS_
SK WSK PT 
PAS_S
STK PAS_P
TK 
BOW+
POS BOW+
PT 
POS_
SK+P
T 
WSK+
PT 
POS_
SK+P
T+PAS
_SST
K 
POS_
SK+P
T+PAS
_PTK
 
F1-
me
asu
re 
Kernel Type 
76
Results on TREC Data (5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW POS POS_
SK WSK PT 
PAS_S
STK PAS_P
TK 
BOW+
POS BOW+
PT 
POS_
SK+P
T 
WSK+
PT 
POS_
SK+P
T+PAS
_SST
K 
POS_
SK+P
T+PAS
_PTK
 
F1-
me
asu
re 
Kernel Type 
BOW ? 24 POSSK+STK+PAS_PTK? 39 
?62 % of improvement 
Kernels for Re-ranking 
77
Re-ranking Framework 
 ? Local classifier generates the most likely set of 
hypotheses. 
 ?  These are used to build annotation pairs,           . 
 ? positive instances if hi more correct than hj, 
 ? A binary classifier decides if hi is more accurate 
than hj.  
 ? Each candidate annotation hi is described by a 
structural representation 
? 
h
i
, h
j
Re-ranking framework 
Local Model 
78
Syntactic Parsing Re-ranking 
 ? Pairs of parse trees (Collins and Duffy, 2002) 
Re-ranking concept labeling 
[Dinarelli et al 2009] 
 ? I have a problem with my monitor 
hi: I NULL have NULL a NULL problem PROBLEM-
B with NULL my NULL monitor HW-B 
hj: I NULL have NULL a NULL problem HW-B 
with NULL my NULL monitor 
79
Flat tree representation  (cross-language structure) 
Multilevel Tree 
80
Enriched Multilevel Tree 
 ? FST CER from 23.2 to 16.01 
Re-ranking for Named-Entity Recognition [Vien et al 2010] 
 ? CRF F1 from 84.86 to 88.16 
81
Re-ranking Predicate Argument Structures 
[Moschitti et al CoNLL 2006] 
 ? SVMs F1 from 75.89 to 77.25 
Conclusions 
 ? Kernel methods and SVMs are useful tools to design 
language applications 
 ? Kernel design still requires some level of expertise 
 ? Engineering approaches to tree kernels 
 ? Basic Combinations 
 ? Canonical Mappings, e.g. 
 ? Node Marking 
 ? Merging of kernels in more complex kernels 
 ? Easy modeling produces state-of-the-art accuracy in many 
tasks, RTE, SRL, QC, NER, RE 
 ? SVM-Light-TK efficient tool to use them 
82
Future (on going work) 
 ? Once we have found the right kernel, are we satisfied? 
 ? What about knowing the most relevant features? 
 ? Can we speed up learning/classification at real-application 
scenario level? 
 ? The answer is reverse kernel engineering: 
 ? [Pighin&Moschitti, CoNLL2009, EMNLP2009, CoNLL2010] 
 ? Mine the most relevant fragments according to SVMs gradient 
 ? Use the linear space 
 ? Software for reverse kernel engineering available in the 
next  months 
Thank you 
83
References 
 ? Alessandro Moschitti and Silvia Quarteroni, Linguistic Kernels for Answer Re-ranking in 
Question Answering Systems, Information and Processing Management, ELSEVIER,
2010. 
 ? Yashar Mehdad, Alessandro Moschitti and Fabio Massimo Zanzotto. Syntactic/
Semantic Structures for Textual Entailment Recognition. Human Language Technology 
- North American chapter of the Association for Computational Linguistics (HLT-
NAACL), 2010, Los Angeles, Calfornia. 
 ? Daniele Pighin and Alessandro Moschitti. On Reverse Feature Engineering of Syntactic 
Tree Kernels. In Proceedings of the 2010 Conference on Natural Language Learning, 
Upsala, Sweden, July 2010. Association for Computational Linguistics.  
 ? Thi Truc Vien Nguyen, Alessandro Moschitti and Giuseppe Riccardi. Kernel-based 
Reranking for Entity Extraction. In proceedings of the 23rd International Conference on 
Computational Linguistics (COLING), August 2010, Beijing, China. 
References 
 ? Alessandro Moschitti. Syntactic and semantic kernels for short text pair categorization. 
In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 
2009), pages 576?584, Athens, Greece, March 2009. Association for Computational 
Linguistics.  
 ? Truc-Vien Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. Convolution kernels 
on constituent, dependency and sequential structures for relation extraction. In 
Proceedings of the Conference on Empirical Methods in Natural Language Processing, 
pages 1378?1387, Singapore, August 2009. Association for Computational Linguistics.  
 ? Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. Re-ranking models 
based-on small training data for spoken language understanding. In Proceedings of the 
Conference on Empirical Methods in Natural Language Processing, pages 1076?1085, 
Singapore, August 2009. Association for Computational Linguistics.  
 ? Alessandra Giordani and Alessandro Moschitti. Syntactic Structural Kernels for Natural 
Language Interfaces to Databases. In ECML/PKDD, pages 391?406, Bled, Slovenia, 
2009. 
84
References 
 ? Alessandro Moschitti, Daniele Pighin and Roberto Basili. Tree Kernels for Semantic 
Role Labeling, Special Issue on Semantic Role Labeling, Computational Linguistics 
Journal. March 2008. 
 ? Fabio Massimo Zanzotto, Marco Pennacchiotti and Alessandro Moschitti, A Machine 
Learning Approach to Textual Entailment Recognition, Special Issue on Textual 
Entailment Recognition, Natural Language Engineering, Cambridge University Press., 
2008 
 ? Mona Diab, Alessandro Moschitti, Daniele Pighin, Semantic Role Labeling Systems for 
Arabic Language using Kernel Methods. In proceedings of the 46th Conference of the 
Association for Computational Linguistics (ACL'08). Main Paper Section. Columbus, 
OH, USA, June 2008.  
 ? Alessandro Moschitti, Silvia Quarteroni, Kernels on Linguistic Structures for Answer 
Extraction. In proceedings of the 46th Conference of the Association for Computational 
Linguistics (ACL'08). Short Paper Section. Columbus, OH, USA, June 2008.  
References 
 ? Yannick Versley, Simone Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, 
Jason Smith, Xiaofeng Yang and Alessandro Moschitti, BART: A Modular Toolkit for 
Coreference Resolution, In Proceedings of the Conference on Language Resources 
and Evaluation, Marrakech, Marocco, 2008.  
 ? Alessandro Moschitti, Kernel Methods, Syntax and Semantics for Relational Text 
Categorization. In proceeding of ACM 17th Conference on Information and Knowledge 
Management (CIKM). Napa Valley, California, 2008.  
 ? Bonaventura Coppola, Alessandro Moschitti, and Giuseppe Riccardi. Shallow semantic 
parsing for spoken language understanding. In Proceedings of HLT-NAACL Short 
Papers, pages 85?88, Boulder, Colorado, June 2009. Association for Computational 
Linguistics.  
 ? Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for 
Relational Learning from Texts, Proceedings of The 24th Annual International 
Conference on Machine Learning  (ICML 2007). 
85
References 
 ? Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification, Proceedings of the 45th Conference of the Association for Computational Linguistics (ACL), Prague, June 2007. 
 ? Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for Relational Learning from Texts, Proceedings of The 24th Annual International Conference on Machine Learning  (ICML 2007), Corvallis, OR, USA. 
 ? Daniele Pighin, Alessandro Moschitti and Roberto Basili, RTV: Tree Kernels for Thematic Role Classification, Proceedings of the 4th International Workshop on Semantic Evaluation (SemEval-4), English Semantic Labeling, Prague, June 2007. 
 ? Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semanitc Kernels for Text Classification, to appear in the 29th European Conference on Information Retrieval (ECIR), April 2007, Rome, Italy. 
 ? Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007 
References 
 ? Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, 
Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification, 
Proceedings of the 45th Conference of the Association for Computational Linguistics 
(ACL), Prague, June 2007. 
 ? Alessandro Moschitti, Giuseppe Riccardi, Christian Raymond, Spoken Language 
Understanding with Kernels for Syntactic/Semantic Structures, Proceedings of IEEE 
Automatic Speech Recognition and Understanding Workshop (ASRU2007), Kyoto, 
Japan, December 2007  
 ? Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semantic 
Kernels for Text Classification, to appear in the 29th European Conference on 
Information Retrieval (ECIR), April 2007, Rome, Italy.  
 ? Stephan Bloehdorn, Alessandro Moschitti: Structure and semantics for expressive text 
kernels. In proceeding of ACM 16th Conference on Information and   Knowledge 
Management (CIKM-short paper) 2007: 861-864, Portugal.  
86
References 
 ? Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, 
Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on 
Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.  
 ? Alessandro Moschitti, Efficient Convolution Kernels for Dependency and Constituent 
Syntactic Trees. In Proceedings of the 17th European Conference on Machine 
Learning, Berlin, Germany, 2006. 
 ? Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, 
Fast On-line Kernel Learning for Trees, International Conference on Data Mining 
(ICDM) 2006 (short paper).  
 ? Stephan Bloehdorn, Roberto Basili, Marco Cammisa, Alessandro Moschitti, Semantic 
Kernels for Text Classification based on Topological Measures of Feature Similarity. In 
Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 06), Hong 
Kong, 18-22 December 2006. (short paper). 
References 
 ? Roberto Basili, Marco Cammisa and Alessandro Moschitti, A Semantic Kernel to 
classify texts with very few training examples, in Informatica, an international journal of 
Computing and Informatics, 2006.  
 ? Fabio Massimo Zanzotto and Alessandro Moschitti, Automatic learning of textual 
entailments with cross-pair similarities. In Proceedings of COLING-ACL, Sydney, 
Australia, 2006.  
 ? Ana-Maria Giuglea and Alessandro Moschitti, Semantic Role Labeling via FrameNet, 
VerbNet and PropBank. In Proceedings of COLING-ACL, Sydney, Australia, 2006.  
 ? Alessandro Moschitti, Making tree kernels practical for natural language learning. In 
Proceedings of the Eleventh International Conference on European Association for 
Computational Linguistics, Trento, Italy, 2006. 
 ? Alessandro Moschitti, Daniele Pighin and Roberto Basili. Semantic Role Labeling via 
Tree Kernel joint inference. In Proceedings of the 10th Conference on Computational 
Natural Language Learning, New York, USA, 2006.  
87
References 
 ? Roberto Basili, Marco Cammisa and Alessandro Moschitti, Effective use of Wordnet 
semantics via kernel-based learning. In Proceedings of the 9th Conference on 
Computational Natural Language Learning (CoNLL 2005), Ann Arbor (MI), USA, 2005  
 ? Alessandro Moschitti, A study on Convolution Kernel for Shallow Semantic Parsing. In 
proceedings of the 42-th Conference on Association for Computational Linguistic 
(ACL-2004), Barcelona, Spain, 2004. 
 ? Alessandro Moschitti and Cosmin Adrian Bejan, A Semantic Kernel for Predicate 
Argument Classification. In proceedings of the Eighth Conference on Computational 
Natural Language Learning (CoNLL-2004), Boston, MA, USA, 2004.  
An introductory book on SVMs, Kernel methods and Text Categorization 
88
Non-exhaustive reference list from other authors 
 ? V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995. 
 ? P. Bartlett and J. Shawe-Taylor, 1998. Advances in Kernel Methods -Support Vector Learning, chapter Generalization Performance of Support Vector Machines and other Pattern Classifiers. MIT Press. 
 ? David Haussler. 1999. Convolution kernels on discrete structures. 
Technical report, Dept. of Computer Science, University of California at 
Santa Cruz. 
 ? Lodhi, Huma, Craig Saunders, John Shawe Taylor, Nello Cristianini, and Chris Watkins. Text classification using string kernels. JMLR,2000 
 ? Sch?lkopf, Bernhard and Alexander J. Smola. 2001. Learning with 
Kernels: Support Vector Machines, Regularization, Optimization, and 
Beyond. MIT Press, Cambridge, MA, USA. 
Non-exhaustive reference list from other authors 
 ? N. Cristianini and J. Shawe-Taylor, An introduction to support vector 
machines (and other kernel-based learning methods) Cambridge 
University Press, 2002 
 ? M. Collins and N. Duffy, New ranking algorithms for parsing and 
tagging: Kernels over discrete structures, and the voted perceptron. In 
ACL02, 2002. 
 ? Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for semi-
structured data. In Proceedings of ICML?02. 
 ? S.V.N. Vishwanathan and A.J. Smola. Fast kernels on strings and trees. In Proceedings of NIPS, 2002. 
 ? Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, 3:1059?1082. D. Zelenko, C. Aone, and A. Richardella. Kernel methods for relation extraction. JMLR, 3:1083?1106, 2003. 
89
Non-exhaustive reference list from other authors 
 ? Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL?03. 
 ? Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In Proceedings of SIGIR?03, pages 26?32. 
 ? Libin Shen, Anoop Sarkar, and Aravind k. Joshi. Using LTAG Based Features in Parse Reranking. In Proceedings of EMNLP?03, 2003 
 ? C. Cumby and D. Roth. Kernel Methods for Relational Learning. In Proceedings of ICML 2003, pages 107?114, Washington, DC, USA, 2003. 
 ? J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. 
 ? A. Culotta and J. Sorensen. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting on ACL, Barcelona, Spain, 2004. 
Non-exhaustive reference list from other authors 
 ? Kristina Toutanova, Penka Markova, and Christopher Manning. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004. 
 ? Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree Kernels with Statistical Feature Mining. In Proceedings of NIPS?05. 
 ? Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting based parse reranking with subtree features. In Proceedings of ACL?05. 
 ? R. C. Bunescu and R. J. Mooney. Subsequence kernels for relation extraction. In Proceedings of NIPS, 2005. 
 ? R. C. Bunescu and R. J. Mooney. A shortest path dependency kernel for relation extraction. In Proceedings of EMNLP, pages 724?731, 2005. 
 ? S. Zhao and R. Grishman. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Meeting of the ACL, pages 419?426, Ann Arbor, Michigan, USA, 2005. 
90
Non-exhaustive reference list from other authors 
 ? J. Kazama and K. Torisawa. Speeding up Training with Tree Kernels for Node Relation Labeling. In Proceedings of EMNLP 2005, pages 137?144, Toronto, Canada, 2005. 
 ? M. Zhang, J. Zhang, J. Su, , and G. Zhou. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of COLING-ACL 2006, pages 825?832, 2006. 
 ? M. Zhang, G. Zhou, and A. Aw. Exploring syntactic structured features over parse trees for relation extraction using kernel methods. Information Processing and Management, 44(2):825?832, 2006. 
 ? G. Zhou, M. Zhang, D. Ji, and Q. Zhu. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of EMNLP-CoNLL 2007, pages 728?736, 2007. 
Non-exhaustive reference list from other authors 
 ? Ivan Titov and James Henderson. Porting statistical parsers with data-defined kernels. In Proceedings of CoNLL-X, 2006 
 ? Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. In Proceedings of NAACL. 
 ? M. Wang. A re-examination of dependency path kernels for relation extraction. In Proceedings of the 3rd International Joint Conference on Natural Language Processing-IJCNLP, 2008. 
91
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 193?202, Dublin, Ireland, August 23-29 2014.
A Study of using Syntactic and Semantic Structures
for Concept Segmentation and Labeling
Iman Saleh
?
, Shafiq Joty, Llu??s M
`
arquez,
Alessandro Moschitti, Preslav Nakov
ALT Research Group
Qatar Computing Research Institute
{sjoty,lmarquez,amoschitti,pnakov}
@qf.org.qa
Scott Cyphers, Jim Glass
MIT CSAIL
Cambridge, Massachusetts 02139
USA
{cyphers,glass}@mit.edu
Abstract
This paper presents an empirical study on using syntactic and semantic information for Concept
Segmentation and Labeling (CSL), a well-known component in spoken language understand-
ing. Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser. We
perform extensive experimentation by comparing different tree-based kernels with a variety of
representations of the available linguistic information, including semantic concepts, words, POS
tags, shallow and full syntax, and discourse trees. The results show that the structured representa-
tion with the semantic concepts yields significant improvement over the base CSL parser, much
larger compared to learning with an explicit feature vector representation. We also show that
shallow syntax helps improve the results and that discourse relations can be partially beneficial.
1 Introduction
Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms,
or, equivalently, database queries, which can then be used to satisfy the user?s information needs. This
process is known as Concept Segmentation and Labeling (CSL): it maps utterances into meaning repre-
sentations based on semantic constituents. The latter are basically sequences of semantic entities, often
referred to as concepts, attributes or semantic tags. Traditionally, grammar-based methods have been
used for CSL, but more recently machine learning approaches to semantic structure computation have
been shown to yield higher accuracy. However, most previous work did not exploit syntactic/semantic
structures of the utterances, and the state-of-the-art is represented by conditional models for sequence la-
beling, such as Conditional Random Fields (Lafferty et al., 2001) trained with simple morphological and
lexical features. In our study, we measure the impact of syntactic and discourse structures by also com-
bining them with innovative features. In the following subsections, we present the application context
for our CSL task and then we outline the challenges and the findings of our research.
1.1 Semantic parsing for the ?restaurant? domain
We experiment with the dataset of McGraw et al. (2012), containing spoken and typed questions about
restaurants, which are to be answered using a database of free text such as reviews, categorical data such
as names and locations, and semi-categorical data such as user-reported cuisines and amenities.
Semantic parsing, in the form of sequential segmentation and labeling, makes it easy to convert spoken
and typed questions such as ?cheap lebanese restaurants in doha with take out? into database queries.
First, a language-specific semantic parser tokenizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in] [
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to the segments, with the option to possibly relabel mis-
labeled segments; at this point, discourse history may be incorporated as well.
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
?
Iman Saleh (iman.saleh@fci-cu.edu.eg) is affiliated to Faculty of Computers and Information, Cairo University.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
193
Finally, a database query is formed from the list of labels and values, and is then executed against the
database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed.
{$and [{cuisine:"lebanese"}, {city:"doha"}, {price:"low"}, {amenity:"carry out"}]}
1.2 Related work on CSL
Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were
word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein
and Hastie, 1997; Santaf?e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other genera-
tive models were applied, which model the joint probability of a word sequence and a concept sequence,
as well as discriminative models, which directly model a conditional probability over the concepts in the
input text.
Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied
stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local
syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al.
(1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty
et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an
approach for CSL that is specific to query understanding for web applications. A general survey of CSL
approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on
shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview.
Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels
for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used
explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins,
2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al.,
2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL
hypotheses using structures built on top of concepts, words and features that are simpler than those
studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar
to ours, as it models the extraction of semantics as a reranking task using string kernels.
1.3 Syntactic and semantic structures for CSL
The related work has highlighted that automatic CSL is mostly based on powerful machine learning al-
gorithms and simple feature representations based on word and tag n-grams. In this paper, we study the
impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and
discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts
derived by a local model, where the hypotheses are represented as trees enriched with semantic con-
cepts similarly to (Dinarelli et al., 2011). These tree-based structures can capture dependencies between
sentence constituents and concepts. However, extracting features from them is rather difficult as their
number is exponentially large. Thus, we rely on structural kernels (e.g., see (Moschitti, 2006)) for au-
tomatically encoding tree fragments, which represent syntactic and semantic dependencies from words
and concepts, and we train the reranking functions with Support Vector Machines (e.g., see (Joachims,
1999)). Additionally, we experiment with several types of kernels and newly designed feature vectors.
We test our models on the above-mentioned Restaurant domain. The results show that (i) the basic
CRF model, in fact semi-CRF (see below), is very accurate, achieving more than 83% in F
1
-score, which
indicates that improving over the semi-CRF approach is very hard; (ii) the upper-bound performance
of the reranking approach is very high as well, i.e., the correct annotation is generated in the first 100
hypotheses in 98.72% of the cases; (iii) our feature vectors show improvement only when all feature
groups are used together; otherwise, we only observe marginal improvement; (iv) structural kernels yield
a 10% relative error reduction from the semi-CRF baseline, which is more than double the feature vector
result; (v) syntactic information significantly improves on the best model, but only when using shallow
syntax; and finally, (vi) although, discourse structures provide good improvement over the semi-CRF
model, they perform lower than shallow syntax (thus, a valuable use of discourse features is still an open
problem that we plan to pursue in future work).
194
2 CSL reranking
Reranking is based on a list of N annotation hypotheses, which are generated and sorted by probability
using local classifiers. Then a reranker, typically a meta-classifier, tries to select the best hypothesis from
the list. The reranker can exploit global information, and, specifically, the dependencies between the
different concepts that are made available by the local model. We use semi-CRF as our local model since
it yields the highest accuracy in CSL (when using a single model), and preference reranking with kernel
machines to rerank the N hypotheses generated by the semi-CRF.
2.1 Basic parser using semi-CRF
We use a semi-Markov CRF (Sarawagi and Cohen, 2004), or semi-CRF, a variation of a linear-chain
CRF (Lafferty et al., 2001), to produce the N -best list of labeled segment hypotheses that serve as the
input to reranking. In a linear-chain CRF, with a sequence of tokens x and labels y, we approximate
p(y|x) as a product of factors of the form p(y
i
|y
i?1
, x), which corresponds to features of the form
f
j
(y
i?1
, y
i
, i, x), where i iterates over the token/label positions. This supports a Viterbi search for the
approximateN best values of y. WithM label values, if for each label y
m
we know the bestN sequences
of labels y
1
, y
2
, . . . , y
i?1
= y
m
, then we can use p(y
i
|y
i?1
, x) to get the probability for extending each
path by each possible label y
i
= y
?
m
. Then for each label y
?
m
, we will have MN paths and scores, one
from each of the paths of length i? 1 ending with y
m
. For each y
?
m
, we pick the N best extended paths.
With semi-CRF, we want a labeled segmentation s rather than a sequence of labels. Each segment
s
i
= (y
i
, t
i
, u
i
) has a label y
i
as well as a starting and ending token position for the segment, t
i
and
u
i
respectively, where u
i
+ 1 = t
i+1
. We approximate p(s|x), with factors of the form p(s
i
|s
i?1
, x),
which we simplify to p(y
i
, u
i
|y
i?1
, t
i
), so features take the form f
j
(y
i?1
, y
i
, t
i
, u
i
), i.e., they can use the
previous segment?s label and the current segment?s label and endpoints. The Viterbi search is extended
to search for a pair of label and segment end. Whereas for M labels we kept track of MN paths, we
must keep track of MLN paths, where L is the maximum segment length.
We use token n-gram features relative to the segment boundaries, n-grams within the segment, token
regular expression and lexicon features within a segment. Each of these features also includes the labels
of the previous and current segment, and the segment length.
2.2 Preference reranking with kernel machines
Preference reranking (PR) uses a classifier C of pairs of hypotheses ?H
i
, H
j
?, which decides if H
i
is
better thanH
j
. Given each training question Q, positive and negative examples are generated for training
the classifier. We adopt the following approach for example generation: the pairs ?H
1
, H
i
? constitute
positive examples, where H
1
has the lowest error rate with respect to the gold standard among the
hypotheses for Q, and vice versa, ?H
i
, H
1
? are considered as negative examples. At testing time, given
a new question Q
?
, C classifies all pairs ?H
i
, H
j
? generated from the annotation hypotheses of Q
?
: a
positive classification is a vote for H
i
, otherwise the vote is for H
j
. Also, the classifier score can be used
as a weighted vote. H
k
are then ranked according to the number (sum) of the (weighted) votes they get.
We build our reranker with kernel machines. The latter, e.g., SVMs, classify an input object o using
the following function: C(o) =
?
i
?
i
y
i
K(o, o
i
), where ?
i
are model parameters estimated from the
training data, o
i
are support objects and y
i
are the labels of the support objects. K(?, ?) is a kernel
function, which computes the scalar product between the two objects in an implicit vector space. In the
case of the reranker, the objects o are ?H
i
, H
j
?, and the kernel is defined as follow:
K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) = S(H
1
, H
?
1
) + S(H
2
, H
?
2
)? S(H
1
, H
?
2
)? S(H
2
, H
?
1
).
Our reranker also includes traditional feature vectors in addition to the trees. Therefore, we define each
hypothesis H as a tuple ?T,~v? composed of a tree T and a feature vector ~v. We then define a structural
kernel (similarity) between two hypotheses H and H
?
as follows: S(H,H
?
) = S
TK
(T, T
?
) + S
v
(~v,~v
?
),
where S
TK
is one of the tree kernel functions defined in Section 3.1, and S
v
is a kernel over feature
vectors (see Section 3.3), e.g., linear, polynomial, gaussian, etc.
195
(a) Basic Tree (BT). (b) Discourse Tree (DT).
(c) Shallow Syntactic Tree (ShT).
(d) Syntactic Tree (ST).
(e) BT with POS (BTP).
Figure 1: Syntactic/semantic trees. The numeric semantic tagset is defined in Table 7.
3 Structural kernels for semantic parsing
In this section, we briefly describe the kernels we use in S(H,H
?
) for preference reranking. We engineer
them by combining three aspects: (i) different types of existing tree kernels, (ii) new syntactic/semantic
structures for representing CSL, and (iii) new feature vectors.
3.1 Tree kernels
Structural kernels, e.g., tree and sequence kernels, measure the similarity between two structures in terms
of their shared substructures. One interesting aspect is that these kernels correspond to a scalar product
in the fragment space, where each substructure is a feature. Therefore, they can be used in the training
and testing algorithms of kernel machines (see Section 2.2). Below, we briefly describe different types of
kernels we tested in our study, which are made available in the SVM-Light-TK toolkit (Moschitti, 2006).
Subtree Kernel (K0) is one of the simplest tree kernels, as it only generates complete subtrees, i.e., tree
fragments that, given any arbitrary starting node, necessarily include all its descendants.
Syntactic Tree Kernel (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob-
jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannot
be separated from their parents. In other words, substructures are composed of atomic building blocks
corresponding to nodes, along with all of their direct children. In the case of a syntactic parse tree, these
are complete production rules for the associated parser grammar.
Syntactic Tree Kernel + BOW (K2) extends ST by allowing leaf nodes to be part of the feature space.
The leaves of the trees correspond to words, i.e., we allow bag-of-words (BOW).
Partial Tree Kernel (K3) can be effectively applied to both constituency and dependency parse trees.
It generates all possible connected tree fragments, e.g., sibling nodes can be also separated and be part
of different tree fragments. In other words, a fragment is any possible tree path from whose nodes other
tree paths can depart. Thus, it can generate a very rich feature space.
Sequence Kernel (K4) is the traditional string kernel applied to the words of a sentence. In our case, we
apply it to the sequence of concepts.
3.2 Semantic/syntactic structures
As mentioned before, tree kernels allow us to compute structural similarities between two trees without
explicitly representing them as feature vectors. For the CSL task, we experimented with a number of tree
representations that incorporate different levels of syntactic and semantic information.
To capture the structural dependencies between the semantic tags, we use a basic tree (Figure 1a)
where the words of a sentence are tagged with their semantic tags. More specifically, the words in the
sentence constitute the leaves of the tree, which are in turn connected to the pre-terminals containing the
semantic tags in BIO notation (?B?=begin, ?I?=inside, ?O?=outside). The BIO tags are then generalized
in the upper level, and so on. The basic tree does not include any syntactic information.
196
However, part-of-speech (POS) and phrasal information could be informative for both segmentation
and labeling in semantic parsing. To incorporate this information, we use two extensions of the basic
tree: one that includes the POS tags of the words (Figure 1e), and another one that includes both POS
tags and syntactic chunks (Figure 1c). The POS tags are children of the semantic tags, whereas the
chunks (i.e., phrasal information) are included as parents of the semantic tags.
We also experiment with full syntactic trees (Figure 1d) to see the impact of deep syntactic informa-
tion. The semantic tags are attached to the pre-terminals (i.e., POS tags) in the syntactic tree. We use the
Stanford POS tagger and syntactic parser and the Twitter NLP tool
1
for the shallow trees.
A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example,
the first clause ?along my route tell me the next steak house? is elaborated by the second clause ?that is
within a mile?. The relations by which clauses in a text are linked are called coherence relations (e.g.,
Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide addi-
tional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse
structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates
discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson,
1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a
nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is.
3.3 New features
In order to compare to the structured representation, we also devoted significant effort towards engineer-
ing a set of features to be used in a flat feature-vector representation; they can be used in isolation or in
combination with the kernel-based approach (as a composite kernel using a linear combination):
CRF-based: these include the basic features used to train the initial semi-CRF model (cf. Section 2.1).
n-gram based: we collected 3- and 4-grams of the output label sequence at the level of concepts, with
artificial tags inserted to identify the start (?S?) and end (?E?) of the sequence.
2
Probability-based: two features computing the probability of the label sequence as an average of the
probabilities at the word level p(l
i
|w
i
) (i.e., assuming independence between words). The unigram prob-
abilities are estimated by frequency counts using maximum likelihood in two ways: (i) from the complete
100-best list of hypotheses; (ii) from the training set (according to the gold standard annotation).
DB-based: a single feature encoding the number of results returned from the database when constructing
a query using the conjunction of all semantic segments in the hypothesis. Three possible values are
considered by using a threshold t: 0 (if the query result is void), 1 (if the number of results is in [1, t]),
and 2 (if the number of results is greater than t). In our case, t is empirically set to 10,000.
4 Experiments
The experiments aim at investigating which structures, and thus which linguistic models and combination
with other models, are the most appropriate for our reranker. We first calculate the oracle accuracy in
order to compute an upper bound of the reranker. Then we present experiments with the feature vectors,
tree kernels, and representations of linguistic information introduced in the previous sections.
4.1 Experimental setup
In our experiments, we use questions annotated with semantic tags in the restaurant domain,
3
which were
collected by McGraw et al. (2012) through crowdsourcing on Amazon Mechanical Turk.
4
We split the
dataset into training, development and test sets. Table 1 shows statistics about the dataset and about the
size of the parts we used for training, development and testing (see the semi-CRF line).
We subsequently split the training data randomly into ten folds. We generated the N -best lists on
the training set in a cross-validation fashion, i.e., iteratively training on nine folds and annotating the
remaining fold. We computed the 100-best hypotheses for each example.
1
Available from http://nlp.stanford.edu/software/index.shtml and https://github.com/aritter/twitter nlp, respectively.
2
For instance, if the output sequence is Other-Rating-Other-Amenity the 3-gram patterns would be: S-Other-Rating, Other-
Rating-Other, Rating-Other-Amenity, and Other-Amenity-E.
3
http://www.sls.csail.mit.edu/downloads/restaurant
4
We could not use the datasets used by Dinarelli et al. (2011), because they use French and Italian corpora for which there
are no reliable syntactic and discourse parsers.
197
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 28,482 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
-score for N -best lists
of different lengths.
We used the development set to experiment and tune the hyper-parameters of the reranking model. The
results on the development set presented in Section 4.2 were obtained by semi-CRF and reranking models
learned on the training set. The results on the test set were obtained by models trained on the training
plus development sets. Similarly, the N -best lists for the development and test sets were generated using
a single semi-CRF model trained on the training set and the training+development sets, respectively.
Each generated hypothesis is represented using a semantic tree and a feature vector (explained in
Section 3) and two extra features accounting for (i) the semi-CRF probability of the hypothesis, and
(ii) the hypothesis reciprocal rank in the N -best list. SVM-Light-TK
5
is used to train the reranker with
a combination of tree kernels and feature vectors (Moschitti, 2006; Joachims, 1999). Although we
tried several parameters on the validation set, we observed that the default values yielded the highest
results. Thus, we used the default c (trade-off) and tree kernel parameters and a linear kernel for the
feature vectors. Table 1 shows the sizes of the train, the development and the test sets used for the
semi-CRF as well as the number of pairs generated for the reranker. As a baseline, we picked the best-
scored hypothesis in the list, according to the semi-CRF tagger. The evaluation measure used in all
the experiments is the harmonic mean of precision and recall, i.e., the F
1
-score (van Rijsbergen, 1979),
computed at the token level and micro-averaged over the different semantic types.
6
We used paired t-test
to measure the statistical significance of the improvements: we split the test set into 31 equally-sized
samples and performed t-tests based on the F
1
-scores of different models on the resulting samples.
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
-score for N -best lists of different lengths, i.e., which
can be achieved by picking the best candidate of the N -best list for various values of N . We can see that
going to 5-best increases the oracle F
1
-score by almost ten points absolute. Going down to 10-best only
adds 2.5 extra F
1
points absolute, and a 100-best list adds 3.5 F
1
points more to yield a respectable F
1
-
score of 98.72. This high result can be explained considering that the size of the complete hypothesis set
is smaller than 100 for most questions. Thus, we can conclude that theN -best lists do include many good
options and do offer quite a large space for potential improvement. We can further observe that going to
5-best lists offers a good balance between the length of the list and the possibility to improve F
1
-score:
generally, we do not want too long N -best lists since they slow down computation and also introduce
more opportunities to make the wrong choice for a reranker (since there are just more candidates to
choose from). In our experiments with larger N , we observed improvements only for 10 and only on the
development set; thus, we will focus on 5-best lists in our experiments below.
K0 K1 K2 K3 K4
Dev 84.21 82.92 83.07 85.07 83.78
Test 84.08 83.19 83.20 84.61 82.93
Table 3: Results for using different tree kernels on the basic tree (BT) representation.
Choosing the best tree kernel. We first select the most appropriate tree kernel to limit the number
of experiment variables. Table 3 shows the results of different tree kernels using the basic tree (BT)
representation (see Figure 1a). We can observe that for both the development set and the test set, kernel
K3 (see Section 3.1) yields the highest F
1
-score.
Impact of feature vectors. Table 4 presents the results for the feature vector experiments in terms
of F
1
-scores and relative error reductions (row RER). The first column shows the baseline, when no
reranking is used; the following four columns contain the results when using vectors including different
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
?Other? is not considered a semantic type, thus ?Other? tokens are not included in the F
1
calculation.
198
Baseline n-grams CRF features Count DB ProbBased AllFeat
Dev 83.86 83.79 83.96 83.80 83.86 83.87 84.49
RER -0.4 0.6 -0.4 0.0 0.0 3.9
Test 83.03 82.90 83.44 82.90 83.01 83.09 83.86
RER -0.7 2.4 -0.7 -0.1 0.3 4.8
Table 4: Feature vector experiments: F
1
score and relative error reduction (in %).
Combining AllFeat and
Baseline BT BTP ShT ST AllFeat +BT +ShT +ShT +BT
Dev 83.86 85.07 85.41 85.06 84.30 84.49 85.57 85.58 85.33
RER 7.5 9.6 7.4 2.8 3.9 10.6 10.7 9.1
Test 83.03 84.61 84.63 84.07 83.81 83.86 84.67 84.79 84.76
RER 9.3 9.4 6.1 4.5 4.8 9.6 10.2 10.2
p.v. 0.00049 0.0002 0.012 0.032 0.00018 0.00028 0.00004 0.000023
Table 5: Tree kernel experiments: F
1
-score, relative error reduction (in %), and p-values.
kinds of features: (i) n-gram features, (ii) all features used by the semi-CRF, (iii) count features, and
(iv) database (DB) features. In each case, we include two additional features: the semi-CRF score
(i.e., the probability) and the reciprocal rank of the hypothesis in the N -best list. Among (i)?(iv), only
the semi-CRF features seem to help; the rest either show no improvements or degrade the performance.
However, putting all these features together (AllFeat) yields sizable gains in terms of F
1
-score and a
relative error reduction of 4-5%; the improvement is statistically significant, and it is slightly larger on
the test dataset compared to the development dataset.
Impact of structural kernels and combinations. Table 5 shows the results when experimenting with
various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with
part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We
can see that the basic tree works rather well, yielding +1.6 F
1
-score on the test dataset, but adding POS
information can help a bit more, especially for the tuning dataset. Interestingly, the syntactic tree kernels,
ShT and ST, perform worse than BT and BTP, especially on the test dataset. The last three columns in the
table show the results when we combine the AllFeat feature vector (see Table 4) with BT and ShT. We can
see that combining AllFeat with ShT works better, on both development and test sets, than combining it
with BT or with both ShT and BT. Also note the big jump in performance from AllFeat to AllFeat+ShT.
Overall, we can conclude that shallow syntax has a lot to offer over AllFeat, and it is preferable over BT
in the combination with AllFeat. The improvements reported in Tables 5 and 6 are statistically significant
when compared to the semi-CRF baseline as shown by the p.v. (value) row. Moreover, the improvement
of AllFeat + ShT over BT is also statistically significant (p.v.<0.05).
Combining AllFeat and
Baseline DS +DS +DS +BT +DS +ShT
Dev 83.86 84.61 85.14 85.43 85.46
RER 4.7 7.9 9.7 9.9
Test 83.03 84.38 84.55 84.63 84.67
RER 7.9 8.9 9.4 9.6
p.v. 0.0005 0.0001 0.00066 0.00015
Table 6: Experiments with discourse kernels: F
1
score, relative error reduction (in %), and p-values.
Discourse structure. Finally, Table 6 shows the results for the discourse tree kernel (DS), which we
designed and experimented with for the first time in this paper. We see that DS yields sizable improve-
ments over the baseline. We also see that further gains can be achieved by combining DS with AllFeat,
and also with BT and ShT, the best combination being AllFeat+DS+ShT (see last column). However,
comparing to Table 5, we see that it is better to use just AllFeat+ShT and leave DS out. We would like
to note though that the discourse parser produced non-trivial trees for only 30% of the hypotheses (due
to the short, simple nature of the questions); in the remaining cases, it probably hurt rather than helped.
We conclude that discourse structure has clear potential, but how to make best use of it, especially in the
case of short simple questions, remains an open question that deserves further investigation.
199
Tag ID Other Rating Restaurant Amenity Cuisine Dish Hours Location Price
0 Other 8260 35 43 110 15 19 55 113 9
1 Rating 29 266 0 14 3 6 0 0 8
2 Restaurant 72 6 657 20 19 15 0 5 0
3 Amenity 117 9 10 841 27 27 7 12 7
4 Cuisine 36 2 12 26 543 44 3 1 0
5 Dish 23 0 4 20 33 324 1 4 0
6 Hours 61 0 1 2 6 1 426 9 1
7 Location 104 1 14 20 2 1 1 1457 0
8 Price 22 1 0 7 0 2 0 1 204
Table 7: Confusion matrix for the output of the best performing system.
4.3 Error analysis and discussion
Table 7 shows the confusion matrix for our best-performing model AllFeat+ShT (rows = gold standard
tags; columns = system predicted tags). Given the good results of the semantic parser, the numbers in the
diagonal are clearly dominating the weight of the matrix. The largest errors correspond to missed (first
column) and over-generated (first row) entity tokens. Among the proper confusions between semantic
types, Dish and Cuisine tend to mislead each other most. This is due to the fact that these two tags
are semantically similar, thus making them hard to distinguish. We can also notice that it is difficult to
identify Amenity correctly, and the model mistakenly tags many other tags as Amenity. We looked into
some examples to further investigate the errors. Our findings are as follow:
Inaccuracies and inconsistencies in human annotations. Since the annotations were done in Me-
chanical Turk, they have many inaccuracies and inconsistencies. For example, the word good with
exactly the same sense was tagged as both Other and Rating by the Turkers in the following examples:
Gold: [
Other
any good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Model: [
Other
any] [
Rating
good] [
Price
cheap] [
Cuisine
german] [
Other
restaurants] [
Location
nearby]
Gold: [
Other
any place] [
Location
along the road] [
Other
has a] [
Rating
good] [
Dish
beer] [
Other
selection that also serves] ...
Requires lexical semantics and more coverage. In some cases our model fails to generalize well. For
instance, it fails to correctly tag establishments and tameles for the following examples. This suggests
that we need to consider other forms of semantic information, e.g., distributional and compositional
semantics computed from large corpora and/or using Web resources such as Wikipedia.
Gold: [
Other
any] [
Location
dancing establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Model: [
Other
any] [
Amenity
dancing] [
Other
establishments] [
Other
with] [
Price
reasonable] [
Other
pricing]
Gold: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Dish
tameles] [
Amenity
special today]
Model: [
Other
any] [
Cuisine
mexican] [
Other
places have a] [
Amenity
tameles] [
Other
special] [
Hours
today]
5 Conclusions
We have presented a study on the usage of syntactic and semantic structured information for improved
Concept Segmentation and Labeling (CSL). Our approach is based on reranking a set of N -best se-
quences generated by a state-of-the-art semi-CRF model for CSL. The syntactic and semantic informa-
tion was encoded in tree-based structures, which we used to train a reranker with kernel-based Support
Vector Machines. We empirically compared several variants of syntactic/semantic structured representa-
tions and kernels, including also a vector of manually engineered features.
The first and foremost conclusion from our study is that structural kernels yield significant improve-
ment over the strong baseline system, with a relative error reduction of ?10%. This more than doubles
the improvement when using the explicit feature vector. Second, we observed that shallow syntactic
information also improves results significantly over the best model. Unfortunately, the results obtained
using full syntax and discourse trees are not so clear. This is probably explained by the fact that user
queries are rather short and linguistically not very complex. We also observed that the upper bound per-
formance for the reranker still leaves large room for improvement. Thus, it remains to be seen whether
some alternative kernel representations can be devised to make better use of discourse and other syntac-
tic/semantic information. Also, we think that some innovative features based on analyzing the results
obtained from our database (or the Web) when querying with the segments represented in each hypothe-
ses have the potential to improve the results. All these concerns will be addressed in future work.
200
Acknowledgments
This research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Re-
search Institute (QCRI) within the Qatar Foundation in collaboration with MIT. It is part of the Interactive
sYstems for Answer Search (Iyas) project.
References
Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 263?270, Philadelphia, PA, USA.
Renato De Mori, Dilek Hakkani-T?ur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur. 2008. Spoken
language understanding: a survey. IEEE Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2011. Discriminative reranking for spoken lan-
guage understanding. IEEE Transactions on Audio, Speech and Language Processing, 20(2):526?539.
Ruifang Ge and Raymond Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the
21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL?06, pages 263?270, Sydney, Australia.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Thorsten Joachims. 1999. Advances in kernel methods. In Bernhard Sch?olkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Making Large-scale Support Vector Machine Learning Practical, pages 169?184,
Cambridge, MA, USA. MIT Press.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level dis-
course analysis. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages 904?915, Jeju Island, Korea.
Rohit Kate and Raymond Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of
the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association
for Computational Linguistics, COLING-ACL ?06, pages 913?920, Sydney, Australia.
Terry Koo and Michael Collins. 2005. Hidden-variable models for discriminative reranking. In Proceedings
of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,
HLT-EMNLP ?05, pages 507?514, Vancouver, British Columbia, Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 189?
196, Ann Arbor, MI, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, ICML ?01, pages 282?289, Williamstown, MA, USA.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Llu??s M`arquez, Xavier Carreras, Kenneth Litkowski, and Suzanne Stevenson. 2008. Semantic Role Labeling: An
Introduction to the Special Issue. Computational Linguistics, 34(2):145?159.
Ian McGraw, Scott Cyphers, Panupong Pasupat, Jingjing Liu, and Jim Glass. 2012. Automating crowd-supervised
learning for spoken language systems. In Proceedings of 13th Annual Conference of the International Speech
Communication Association, INTERSPEECH ?12, Portland, OR, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and Robert Ingria. 1994. Statistical language processing using
hidden understanding models. In Proceedings of the workshop on Human Language Technology, HLT ?94,
pages 278?282, Morristown, NJ, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint
inference. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),
pages 61?68, New York City, June.
201
Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Machine Learning, ECML?06, pages 318?329, Berlin, Hei-
delberg. Springer-Verlag.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2012. Structural reranking models for named entity recognition.
Intelligenza Artificiale, 6(2):177?190.
Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maximum likelihood and discriminative training of
direct translation models. In Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing, volume 1, pages 189?192, Seattle, WA, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee. 1991. Stochastic representation of conceptual structure in
the ATIS task. In Proceedings of the Workshop on Speech and Natural Language, HLT ?91, pages 121?124,
Pacific Grove, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language
understanding. In Proceedings of 8th Annual Conference of the International Speech Communication Associa-
tion, INTERSPEECH ?07, pages 1605?1608, Antwerp, Belgium.
Yigal Dan Rubinstein and Trevor Hastie. 1997. Discriminative vs informative learning. In Proceedings of the
Third International Conference on Knowledge Discovery and Data Mining, KDD ?97, pages 49?53, Newport
Beach, CA, USA.
Guzm?an Santaf?e, Jose Lozano, and Pedro Larra?naga. 2007. Discriminative vs. generative learning of Bayesian
network classifiers. Lecture Notes in Computer Science, 4724:453?464.
Sunita Sarawagi and William Cohen. 2004. Semi-Markov conditional random fields for information extraction.
In Proceedings of the 18th Annual Conference on Neural Information Processing Systems, NIPS ?04, pages
1185?1192, Vancouver, British Columbia, Canada.
Stephanie Seneff. 1989. TINA: A probabilistic syntactic parser for speech understanding systems. In Proceedings
of the Workshop on Speech and Natural Language, HLT ?89, pages 168?178, Philadelphia, PA, USA.
Manfred Stede. 2011. Discourse Processing. Synthesis Lectures on Human Language Technologies. Morgan and
Claypool Publishers.
Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Butterworth.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Jakub Szymanski. 2009. Semi-supervised learning of semantic
classes for query understanding: from the web and for the web. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management, CIKM ?09, pages 37?46, New York, NY, USA.
202
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 712?724,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Using Syntactic and Semantic Structural Kernels for
Classifying Definition Questions in Jeopardy!
Alessandro Moschitti? Jennifer Chu-Carroll? Siddharth Patwardhan?
James Fan? Giuseppe Riccardi?
?Department of Information Engineering and Computer Science
University of Trento, 38123 Povo (TN), Italy
{moschitti,riccardi}@disi.unitn.it
?IBM T.J. Watson Research Center P.O. Box 704, Yorktown Heights, NY 10598, U.S.A.
{jencc,siddharth,fanj}@us.ibm.com
Abstract
The last decade has seen many interesting ap-
plications of Question Answering (QA) tech-
nology. The Jeopardy! quiz show is certainly
one of the most fascinating, from the view-
points of both its broad domain and the com-
plexity of its language. In this paper, we study
kernel methods applied to syntactic/semantic
structures for accurate classification of Jeop-
ardy! definition questions. Our extensive em-
pirical analysis shows that our classification
models largely improve on classifiers based on
word-language models. Such classifiers are
also used in the state-of-the-art QA pipeline
constituting Watson, the IBM Jeopardy! sys-
tem. Our experiments measuring their impact
on Watson show enhancements in QA accu-
racy and a consequent increase in the amount
of money earned in game-based evaluation.
1 Introduction
Question Answering (QA) is an important research
area of Information Retrieval applications, which re-
quires the use of core NLP capabilities, such as syn-
tactic and semantic processing for a more effective
user experience. While the development of most
existing QA systems are driven by organized eval-
uation efforts such as TREC (Voorhees and Dang,
2006), CLEF (Giampiccolo et al, 2007), and NT-
CIR (Sasaki et al, 2007), there exist efforts that
leverage data from popular quiz shows, such as Who
Wants to be a Millionaire (Clarke et al, 2001; Lam
et al, 2003) and Jeopardy! (Ferrucci et al, 2010), to
demonstrate the generality of the technology.
Jeopardy! is a popular quiz show in the US which
has been on the air for 27 years. In each game, three
contestants compete for the opportunity to answer
60 questions in 12 categories of 5 questions each.
Jeopardy! questions cover an incredibly broad do-
main, from science, literature, history, to popular
culture. We are drawn to Jeopardy! as a test bed
for open-domain QA technology due to its broad do-
main, complex language, as well as the emphasis on
accuracy, confidence, and speed during game play.
While the vast majority of Jeopardy! questions
are factoid questions, we find several other types
of questions in the Jeopardy! data, which can ben-
efit from specialized processing in the QA system.
The additional processing in these questions com-
plements that of the factoid questions to achieve im-
proved overall QA performance. Among the various
types of questions handled by the system are defini-
tion questions shown in the examples below:
(1) GON TOMORROW: It can be the basket
below a hot-air balloon or a flat-bottomed
boat used on a canal (answer: gondola);
(2) I LOVE YOU, ?MIN?: Overbearing (an-
swer: domineering);
(3) INVEST: From the Latin for ?year?, it?s
an investment or retirement fund that pays
out yearly (answer: an annuity)
where the upper case text indicates the Jeop-
ardy! category for each question1.
Several characteristics of this class of questions
warrant special processing: first, the clue (question)
1A Jeopardy! category indicates a theme is common among
its 5 questions.
712
often aligns well with dictionary entries, making
dictionary resources potentially effective. Second,
these clues often do not indicate an answer type,
which is an important feature for identifying cor-
rect answers in factoid questions (in the examples
above, only (3) provided an answer type, ?fund?).
Third, definition questions are typically shorter in
length than the average factoid question. These dif-
ferences, namely the shorter clue length and the lack
of answer types, make the use of a specialized ma-
chine learning model potentially promising for im-
proving the overall system accuracy. The first step
for handling definitions is, of course, the automatic
separation of definitions from other question types,
which is not a simple task in the Jeopardy! domain.
For instance, consider the following example which
is a variation of (3) above:
(4) INVEST: From the Latin for ?year?,
an annuity is an investment or retirement
fund that pays out this often (answer:
yearly)
Even though the clue is nearly identical to (3), the
clue does not provide a definition for the answer
yearly, although at first glance we may have been
misled. The source of complexity is given by the fact
that Jeopardy! clues are not phrased in interrogative
form as questions typically are. This complicates the
design of definition classifiers since we cannot di-
rectly use either typical structural patterns that char-
acterize definition/description questions, or previous
approaches, e.g. (Ahn et al, 2004; Kaisser and Web-
ber, 2007; Blunsom et al, 2006). Given the com-
plexity and the novelty of the task, we found it use-
ful to exploit the kernel methods technology. This
has shown state-of-the-art performance in Question
Classification (QC), e.g. (Zhang and Lee, 2003;
Suzuki et al, 2003; Moschitti et al, 2007) and it
is very well suited for engineering feature represen-
tations for novel tasks.
In this paper, we apply SVMs and kernel meth-
ods to syntactic/semantic structures for modeling
accurate classification of Jeopardy! definition ques-
tions. For this purpose, we use several levels of lin-
guistic information: word and POS tag sequences,
dependency, constituency and predicate argument
structures and we combined them using state-of-
the-art structural kernels, e.g. (Collins and Duffy,
2002; Shawe-Taylor and Cristianini, 2004; Mos-
chitti, 2006). The extensive empirical analysis of
several advanced models shows that our best model,
which combines different kernels, improves the F1
of our baseline model by 67% relative, from 40.37
to 67.48. Surprisingly, with respect to previous find-
ings on standard QC, e.g. (Zhang and Lee, 2003;
Moschitti, 2006), the Syntactic Tree Kernel (Collins
and Duffy, 2002) is not effective whereas the ex-
ploitation of partial tree patterns proves to be es-
sential. This is due to the different nature of Jeop-
ardy! questions, which are not expressed in the usual
interrogative form.
To demonstrate the benefit of our question clas-
sifier, we integrated it into our Watson by coupling
it with search and candidate generation against spe-
cialized dictionary resources. We show that in end-
to-end evaluations, Watson with kernel-based defi-
nition classification and specialized definition ques-
tion processing achieves statistically significant im-
provement compared to our baseline systems.
In the reminder of this paper, Section 2 describes
Watson by focusing on the problem of definition
question classification, Section 3 describes our mod-
els for such classifiers, Section 4 presents our exper-
iments on QC, whereas Section 5 shows the final im-
pact on Watson. Finally, Section 6 discusses related
work and Section 7 derives the conclusions.
2 Watson: The IBM Jeopardy! System
This section gives a quick overview of Watson and
the problem of classification of definition questions,
which is the focus of this paper.
2.1 Overview
Watson is a massively parallel probabilistic
evidence-based architecture for QA (Ferrucci et
al., 2010). It consists of several major stages for
underlying sub-tasks, including analysis of the
question, retrieval of relevant content, scoring and
ranking of candidate answers, as depicted in Figure
1. In the rest of this section, we provide an overview
of Watson, focusing on the task of answering
definitional questions.
Question Analysis: The first stage of the pipeline,
it applies several analytic components to identify
key characteristics of the question (such as answer
713
Figure 1: Overview of Watson
type, question classes, etc.) used by later stages of
the Watson pipeline. Various general purpose NLP
components, such as a parser and named entity de-
tector, are combined with task-specific modules for
this analysis.
The task-specific analytics include several QC
components, which determine if the question be-
longs to one or more broad ?question classes?.
These question classes can influence later stages of
the Watson pipeline. For instance, a question de-
tected as an abbreviation question can invoke spe-
cialized candidate generators to produce possible ex-
pansions of the abbreviated term in the clue. Simi-
larly, the question classes can impact the methods
for answer scoring and the machine learning mod-
els used for ranking candidate answers. The focus
of this paper is on the definition class, which is de-
scribed in the next section.
Hypothesis Generation: Following question anal-
ysis, the Watson pipeline searches its document col-
lection for relevant documents and passages that are
likely to contain the correct answer to the question.
This stage of the pipeline generates search queries
based on question analysis results, and obtains a
ranked list of documents and passages most relevant
to the search queries. A variety of candidate gen-
eration techniques are then applied to the retrieved
results to produce a set of candidate answers.
Information obtained from question analysis can
be used to influence the search and candidate gener-
ation processes. The question classes detected dur-
ing question analysis can focus the search towards
specific subsets of the corpus. Similarly, during can-
didate generation, strategies used to generate the set
of candidate answers are selected based on the de-
tected question classes.
Hypothesis and Evidence Scoring: A wide variety
of answer scorers are then used to gather evidence
supporting each candidate answer as the correct an-
swer to the given question. The scorers include both
context dependent as well as context independent
scorers, relying on various structured and unstruc-
tured resources for their supporting evidence.
Candidate Ranking: Finally, machine learning
models are used to weigh the gathered evidence and
rank the candidate answers. The models generate a
ranked list of answers each with an associated con-
fidence. The system can also choose to refrain from
answering a question if it has low confidence in all
candidates. This stage of the pipeline employs sev-
eral machine learning models specially trained to
handle various types of questions. These models are
trained using selected feature sets based on question
classes and candidate answers are ?routed? to the
appropriate model according to the question classes
detected during question analysis.
2.2 Answering Definition Questions
Among the many question classes that Watson iden-
tifies and leverages for special processing, of partic-
ular interest for this paper is the class we refer to
as definition questions. These are questions whose
clue texts contain one or more definitions of the cor-
rect answer. For instance, in example (3), the main
clause in the question corresponds to a dictionary
definition of the correct answer (annuity). Looking
up this definition in dictionary resources could en-
able us to answer this question correctly and with
high confidence. This suggests that special process-
714
ing of such definition questions could allow us to
hone in on the correct answer through processes dif-
ferent from those used for other types of questions.
This paper explores strategies for definition ques-
tion processing to improve overall question answer-
ing performance. A key challenge we have to ad-
dress is that of accurate recognition of such ques-
tions. Given an input question the Watson question
analysis stage uses a definition question recognizer
to detect this specific class of questions. We explore
several approaches for recognition, including a rule
based approach and a variety of statistical models.
Questions that are recognized as definition ques-
tions invoke search processes targeted towards
dictionary-like sources in our system. We use a va-
riety of such sources, such as standard English dic-
tionaries, Wiktionary, WordNet, etc. After gather-
ing supporting evidence for candidate answers ex-
tracted from these sources, our system routes the
candidates to definition-specific candidate ranking
models, which have been trained with selected fea-
ture sets.
The following sections present a description and
evaluation of our approach for identifying and an-
swering definition questions.
3 Kernel Models for Question
Classification
Previous work (Zhang and Lee, 2003; Suzuki et al,
2003; Blunsom et al, 2006; Moschitti et al, 2007)
as shown that syntactic structures are essential for
QC. Given the novelty of both the domain and the
type of our classification items, we rely on kernel
methods to study and design effective representa-
tions. Indeed, these are excellent tools for auto-
matic feature engineering, especially for unknown
tasks and domains. Our approach consists of using
SVMs and kernels for structured data applied to sev-
eral types of structural lexical, syntactic and shallow
semantic information.
3.1 Tree and Sequence Kernels
Kernel functions are implicit scalar products be-
tween data examples (i.e. questions in our case)
in the very high dimensional space of substructures,
where each of the latter is a component of the im-
plicit vectors associated with the examples.
ROOT
SBARQ
WHADVP
WRB
When
S
VP
VBN
hit
PP
IN
by
NP
NNS
electrons
,
,
NP
DT
a
NN
phosphor
VP
VBZ
gives
PRP
RP
off
NP
NP
JJ
electromagnetic
NN
energy
PP
IN
in
NP
DT
this
NN
form
1
Figure 2: Constituency Tree
PASS
P
A1
phosphor
A0
electron
PR
hit
P
PR
energy
A2
electromag.
A1
phosphor
P
A1
energy
PR
give
AM-TMP
hit
A0
phosphor
ROOT
VBZ
OBJ
NN
NMOD
IN
PMOD
NN
formNMOD
DT
this
in
energyNMOD
JJ
electromag.
PRT
RP
off
givesSBJ
NN
phosphorNMOD
DT
a
P
,
TMP
VBN
LGS
IN
PMOD
NNS
electrons
by
hitTMP
WRB
when
1
Figure 3: Dependency Tree
negative mistake STK, ok PTK
NP
ADJP
JJ
conceited
CC
or
JJ
arrogant
NP
NN
meaning
NN
adjective
NN
5-letter
NN
fowl
positive mistake STK, ok PTK
NP
VP
PP
NP
NN
field
VBG
playing
DT
a
IN
on
VBN
used
NP
NN
grass
JJ
green
JJ
artificial
NP
VP
PP
NP
NN
canal
DT
a
IN
on
VBN
used
NP
NN
boat
JJ
flat-bottomed
DT
a
PASS
P
A1
phosphor
A0
electron
PR
hit
P
PR
energy
AM-MNR
electromag.
A1
phosphor
P
A1
energy
PR
give
AM-TMP
hit
A0
phosphor
1
Figure 4: A tree encoding a Predicate Argument Structure Set
Although several kernels for structured data have
been developed (see Section 6), the main distinc-
tions in terms of feature spaces is given by the fol-
lowing three different kernels:
? Sequence Kernels (SK); we implemented the
discontinuous string kernels described in (Shawe-
Taylor and Cristianini, 2004). This allows for rep-
resenting a string of symbols in terms of its possi-
ble substrings with gaps, i.e. an arbitrary number of
symbols can be skipped during the generation of a
substring. The symbols we used in the sequential de-
scriptions of questions are words and part-of-speech
tags (in two separate sequences). Consequently, all
possible multiwords with gaps are features of the im-
plicitly generated vector space.
715
? Syntactic Tree Kernel (STK) (Collins and Duffy,
2002) applied to constituency parse trees. This gen-
erates all possible tree fragments as features with
the conditions that sibling nodes from the original
trees cannot be separated. In other words, substruc-
tures are composed by atomic building blocks cor-
responding to nodes along with all their direct chil-
dren. These, in case of a syntactic parse tree, are
complete production rules of the associated parser
grammar2.
? Partial Tree Kernel (PTK) (Moschitti, 2006) ap-
plied to both constituency and dependency parse
trees. This generates all possible tree fragments, as
above, but sibling nodes can be separated (so they
can be part of different tree fragments). In other
words, a fragment is any possible tree path, from
whose nodes other tree paths can depart. Conse-
quently, an extremely rich feature space is gener-
ated. Of course, PTK subsumes STK but sometimes
the latter provides more effective solutions as the
number of irrelevant features is smaller as well.
When applied to sequences and tree structures, the
kernels discussed above produce many different
kinds of features. Therefore, the design of appro-
priate syntactic/semantic structures determines the
representational power of the kernels. Hereafter, we
show the models we used.
3.2 Syntactic Semantic Structures
We applied the above kernels to different structures.
These can be divided in sequences of words (WS)
and part of speech tags (PS) and different kinds of
trees. For example, given the non-definition Jeop-
ardy! question:
(5) GENERAL SCIENCE: When hit by elec-
trons, a phosphor gives off electromag-
netic energy in this form. (answer: light
or photons),
we use the following sequences:
WS: [when][hit][by][electrons][,][a][phosphor][gives]
[off][electromagnetic][energy][in][this][form]
PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in]
[dt][nn]
Additionally, we use constituency trees (CTs), see
2From here the name syntactic tree kernels
Figure 2 and dependency structures converted into
the dependency trees (DTs), e.g. shown in Figure
3. Note that, the POS-tags are central nodes, the
grammatical relation label is added as a father
node and all the relations with the other nodes are
described by means of the connecting edges. Words
are considered additional children of the POS-tag
nodes (in this case the connecting edge just serves
to add a lexical feature to the target POS-tag node).
Finally, we also use predicate argument structures
generated by verbal and nominal relations accord-
ing to PropBank (Palmer et al, 2005) and NomBank
(Meyers et al, 2004). Given the target sentence, the
set of its predicates are extracted and converted into
a forest, then a fake root node, PAS, is used to con-
nect these trees. For example, Figure 4 illustrates a
Predicate Argument Structures Set (PASS) encoding
two relations, give and hit, as well as the nominaliza-
tion energy along with all their arguments.
4 Experiments on Definition Question
Classification
In these experiments, we study the role of kernel
technology for the design of accurate classification
of definition questions. We build several classifiers
based on SVMs and kernel methods. Each classi-
fier uses advanced syntactic/semantic structural fea-
tures and their combination. We carry out an exten-
sive comparison in terms of F1 between the different
models on the Jeopardy! datasets.
4.1 Experimental Setup
Corpus: the data for our QC experiments consists
of a randomly selected set of 33 Jeopardy! games3.
These questions were manually annotated based on
whether or not they are considered definitional. This
resulted in 306 definition and 4964 non-definition
clues. Each test set is stored in a separate file con-
sisting of one line per question, which contains tab-
separated clue information and the Jeopardy! cate-
gory, e.g. INVEST in example (4).
Tools: for SVM learning, we used the SVMLight-
TK software4, which includes structural kernels in
SVMLight (Joachims, 1999)5. For generating con-
3Past Jeopardy! games can be downloaded from
http://www.j-archive.com.
4Available at http://dit.unitn.it/?moschitt
5http://svmlight.joachims.org
716
stituency trees, we used the Charniak parser (Char-
niak, 2000). We also used the syntactic?semantic
parser by Johansson and Nugues (2008) to gener-
ate dependency trees (Mel?c?uk, 1988) and predicate
argument trees according to the PropBank (Palmer
et al, 2005) and NomBank (Meyers et al, 2004)
frameworks.
Baseline Model: the first model that we used as a
baseline is a rule-based classifier (RBC). The RBC
leverages a set of rules that matches against lexical
and syntactic information in the clue to make a bi-
nary decision on whether or not the clue is consid-
ered definitional. The rule set was manually devel-
oped by a human expert, and consists of rules that
attempt to identify roughly 70 different constructs
in the clues. For instance, one of the rules matches
the parse tree structure for ?It?s X or Y?, which will
identify example (1) as a definition question.
Kernel Models: we apply the kernels described
in Section 3 to the structures extracted from Jeop-
ardy! clues. In particular, we design the following
models: BOW, i.e. linear kernel on bag-of-words
from the clues; WSK, PSK and CSK, i.e. SK applied
to the word and POS-tag sequences from the clues,
and the word sequence taken from the question cat-
egories, respectively; STK-CT, i.e. STK applied to
CTs of the clue; PTK-CT and PTK-DT, i.e. PTK
applied to CTs and DTs of the clues, respectively;
PASS, i.e. PTK applied to the Predicate Argument
Structure Set extracted from the clues; and RBC, i.e.
a linear kernel applied to the vector only constituted
by the 1/0 output of RBC.
Learning Setting: there is no particular parameteri-
zation. Since there is an imbalance between positive
and negative examples, we used a Precision/Recall
trade-off parameter in SVM-Light-TK equal to 5.6
Measures: the performance is measured with Pre-
cision, Recall and F1-measure. We estimated them
by means of Leave-One-Out7 (LOO) on the question
set.
4.2 Results and Discussion
Table 1 shows the performance obtained using dif-
ferent kernels (feature spaces) with SVMs. We note
6We have selected 5 as a reasonable value, which kept bal-
anced Precision and Recall on a validation set.
7LOO applied to a corpus ofN instances consists in training
on N ? 1 examples and testing on the single held-out example.
This process is repeated for all instances.
Kernel Space Prec. Rec. F1
RBC 28.27 70.59 40.38
BOW 47.67 46.73 47.20
WSK 47.11 50.65 48.82
STK-CT 50.51 32.35 39.44
PTK-CT 47.84 57.84 52.37
PTK-DT 44.81 57.84 50.50
PASS 33.50 21.90 26.49
PSK 39.88 45.10 42.33
CSK 39.07 77.12 51.86
Table 1: Kernel performance using leave-one-out cross-
validation.
that: first, RBC has good Recall but poor Precision.
This is interesting since, on one hand, these results
validate the complexity of the task: in order to cap-
ture the large variability of the positive examples,
the rules developed by a skilled human designer are
unable to be sufficiently precise to limit the recog-
nition to those examples. On the other hand, RBC,
being a rather different approach from SVMs, can be
successfully exploited in a joint model with them.
Second, BOW yields better F1 than RBC but it
does not generalize well since its F1 is still low.
When n-grams are also added to the model by
means of WSK, the F1 improves by about 1.5 ab-
solute points. As already shown in (Zhang and Lee,
2003; Moschitti et al, 2007), syntactic structures are
needed to improve generalization.
Third, surprisingly with respect to previous work,
STK applied to CT8 provides accuracy lower than
BOW, about 8 absolute points. The reason is due to
the different nature of the Jeopardy! questions: large
syntactic variability reduces the probability of find-
ing general and well formed patterns, i.e. structures
generated by entire production rules. This suggests
that PTK, which can capture patterns derived from
partial production rules, can be more effective. In-
deed, PTK-CT achieves the highest F1, outperform-
ing WSK also when used with a different syntactic
paradigm, i.e. PTK-DT.
Next, PSK and PASS provide a lower accuracy
but they may be useful in kernel combinations as
they can complement the information captured by
the other models. Interestingly, CSK alone is rather
effective for classifying definition questions. We be-
8Applying it to DT does not make much sense as already
pointed out in (Moschitti, 2006).
717
?Figure 5: Similarity according to PTK and STK
lieve this is because definition questions are some-
times clustered into categories such as 4-LETTER
WORDS or BEGINS WITH ?B?.
Moreover, we carried out qualitative error analy-
sis on the PTK and STK outcome, which supported
our initial hypothesis. Let us consider the bottom
tree in Figure 5 in the training set. The top tree is
a test example correctly classified by PTK but in-
correctly classified by STK. The dashed line in the
top tree contains the largest subtree matched by PTK
(against the bottom tree), whereas the dashed line in
the bottom tree indicates the largest subtree matched
by STK (against the top tree). As the figure shows,
PTK can exploit a larger number of partial patterns.
Finally, the above points suggest that different
kernels produce complementary information. It is
thus promising to experiment with their combina-
tions. The joint models can be simply built by
summing kernel functions together. The results are
shown in Table 2. We note that: (i) CSK comple-
ments the WSK information, achieving a substan-
tially better result, i.e. 62.95; (ii) PTK-CT+CSK
performs even better than WSK+CSK (as PTK out-
performs WSK); and (iii) adding RBC improves
further on the above combinations, i.e. 68.11 and
67.32, respectively. This evidently demonstrates
that RBC captures complementary information. Fi-
nally, more complex kernels, especially the overall
kernel summation, do not seem to improve the per-
Kernel Space Prec. Rec. F1
WSK+CSK 70.00 57.19 62.95
PTK-CT+CSK 69.43 60.13 64.45
PTK-CT+WSK+CSK 68.59 62.09 65.18
CSK+RBC 47.80 74.51 58.23
PTK-CT+CSK+RBC 59.33 74.84 65.79
BOW+CSK+RBC 60.65 73.53 66.47
PTK-CT+WSK+CSK+RBC 67.66 66.99 67.32
PTK-CT+PASS+CSK+RBC 62.46 71.24 66.56
WSK+CSK+RBC 69.26 66.99 68.11
ALL 61.42 67.65 64.38
Table 2: Performance of Kernel Combinations using
leave-one-out cross-validation.
formance. This is also confirmed by the PASS re-
sults derived in (Moschitti et al, 2007) on TREC
QC.
5 Experiments on the Jeopardy System
Since the kernel-based classifiers perform substan-
tially better than RBC, we incorporate the PTK-
CT+WSK+CSK model9 into Watson for definition
classification and evaluated the QA performance
against two baseline systems. For the end-to-end ex-
periments, we used Watson?s English Slot Grammar
parser (McCord, 1980) to generate the constituency
trees. The component level evaluation shows that
we achieved comparable performance as previously
discussed with ESG.
5.1 Experimental Setup
We integrated the classifier into the question analy-
sis module, and incorporated additional components
to search against dictionary resources and extract
candidate answers from these search results when a
question is classified as definitional. In the final ma-
chine learning models, a separate model is trained
for definition questions to enable scoring tailored to
the specific characteristics of those questions.
Based on our manually annotated gold standard,
less than 10% of Jeopardy! questions are classified
as definition questions. Due to their relatively low
frequency we conduct two types of evaluations. The
first is definition-only evaluation, in which we apply
our definition question classifier to identify a large
9Since we aim to compare a purely statistical approach to
the rule-based approach, we did not experiment with the model
that uses RBC as a feature in our end-to-end experiments.
718
set of definition questions and evaluate the end-to-
end system?s performance on this large set of ques-
tions. These results enable us to draw statistically
significant conclusions about our approach to ad-
dressing definition questions.
The second type of evaluation is game-based
evaluation, which assesses the impact of our defi-
nition question processing on Watson performance
while preserving the natural distribution of these
question types in Jeopardy! data. Game-based eval-
uations situate the system?s performance on defini-
tion questions relative to other types of questions,
and enable us to gauge the component?s contribu-
tions in a game-based setting.
For both evaluation settings, three configurations
of Watson are used as follows:
? the NoDef system, in which Watson is config-
ured without definition classification and pro-
cessing, thereby treating all definition ques-
tions as regular factoid questions;
? the StatDef system, which leverages the sta-
tistical classifier and subsequent definition spe-
cific search and candidate generation compo-
nents as described above; and
? the RuleDef system, in which Watson adopts
RBC and employs the same additional defini-
tion search and candidate generation compo-
nents as the StatDef system.
For the definition-only evaluation, we selected all
questions recognized as definitional by the statistical
classifier from roughly 1000 unseen games (60000
questions), resulting in a test set of 1606 questions.
Due to the size of the initial set, it is impractical to
manually create a gold standard for measuring Pre-
cision and Recall of the classifier. Instead, we com-
pare the StatDef system against the NoDef on these
1606 questions using two metrics: accuracy, defined
as the percentage of questions correctly answered,
and p@70, the system?s Precision when answering
only the top 70% most confident questions. P@70 is
an important metric in Jeopardy! game play as well
as in real world applications where the system may
refrain from answering a question when it is not con-
fident about any of its answers. Since RBC identifies
significantly more definition questions, we started
NoDef StatDef NoDef RuleDef
# Questions 1606 1606 1875 1875
Accuracy 63.76% 65.57% 56.64% 57.51%
P@70 82.22% 84.53% 72.73% 74.87%
Table 3: Definition-Only Evaluation Results
with an initial set of roughly 300 games, from which
the RBC identified 1875 questions as definitional.
We compared the RuleDef system?s performance on
these questions against the NoDef baseline using the
accuracy and p@70 metrics.
For the game-based evaluation, we randomly se-
lected 66 unseen Jeopardy! games, consisting of
3546 questions after excluding audio/visual ques-
tions.10 We contrast the StatDef system perfor-
mance against that of NoDef and RuleDef along
several dimensions: accuracy and p@70, described
above, as well as earnings, the average amount of
money earned for each game.
5.2 Definition-Only Evaluation
For the definition-only evaluation, we compared the
StatDef system against the NoDef system on a set of
1606 questions that the StatDef system classified as
definitional. The results are shown in the first two
columns in Table 3. To contrast the gain obtained
by the StatDef system against that achieved by the
RuleDef system, we ran the RuleDef system over
the 1875 questions identified as definitional by the
rule-based classifier. We contrast the RuleDef sys-
tem performance with that of the NoDef system, as
shown in the last two columns in Table 3.
Our results show that based on both evaluation
metrics, StatDef improved upon the NoDef baseline
more than RuleDef improved on the same baseline
system. Furthermore, for the accuracy metric where
all samples are paired and independent, the differ-
ence in performance between the StatDef and NoDef
systems is statistically significant at p<0.05, while
that between the RuleDef and NoDef systems is not.
5.3 Game-Based Evaluation
The game-based evaluation was carried out on 66
unseen games (roughly 3500 questions). Of these
10Audio/visual questions are those accompanied by either an
image or an audio clip. The text portions of these questions are
often insufficient for identifying the correct answers.
719
# Def Q?s Accuracy P@70 Earnings
NoDef 0 69.71% 86.79% $24,818
RuleDef 480 69.23% 86.31% $24,397
StatDef 131 69.85% 87.19% $25,109
Table 4: Game-Based Evaluation Results
questions, the StatDef system classified 131 of them
as definitional while the RuleDef system identified
480 definition questions. Both systems were com-
pared against the NoDef system using the accuracy,
p@70, and earnings metric computed over all ques-
tions, as shown in Table 4.
Our results show that even though in the
definition-only evaluation both the RuleDef and
StatDef systems outperformed the NoDef baseline,
in our game-based evaluation, the RuleDef system
performed worse than the NoDef baseline. The low-
ered performance is due to the fact that the Preci-
sion of the RBC is much lower than that of the sta-
tistical classifier, and the special definition process-
ing applied to questions that are erroneously clas-
sified as definitional was harmful. Our evaluation
of this false positive set showed that its accuracy
dropped by 6% compared to the NoDef system. On
the other hand, the StatDef system outperformed the
two other systems, and its accuracy improvement
upon the RuleDef system is statistically significant
at p<0.05.
6 Related Work
Our paper studies the use of advanced representa-
tion for QC in the Jeopardy! domain. As previously
mentioned Jeopardy! questions are stated as affir-
mative sentences, which are different from the typ-
ical QA questions. For the design of our models,
we have carefully taken into account previous work.
This shows that semantics and syntax are essential
to retrieve precise answers, e.g (Hickl et al, 2006;
Voorhees, 2004; Small et al, 2004).
We focus on definition questions, which typically
require more complex processing than factoid ques-
tions (Blair-Goldensohn et al, 2004; Chen et al,
2006; Shen and Lapata, 2007; Bilotti et al, 2007;
Moschitti et al, 2007; Surdeanu et al, 2008; Echi-
habi and Marcu, 2003). For example, language mod-
els were applied to definitional QA in (Cui et al,
2005) to learn soft pattern models based on bigrams.
Other related work, such as (Sasaki, 2005; Suzuki
et al, 2002), was also very tied to bag-of-words
features. Predicate argument structures have been
mainly used for reranking (Shen and Lapata, 2007;
Bilotti et al, 2007; Moschitti et al, 2007; Surdeanu
et al, 2008).
Our work and methods are similar to (Zhang and
Lee, 2003; Moschitti et al, 2007), which achieved
the state-of-the-art in QC by applying SVMs along
with STK-CT. The results were derived by experi-
menting with a TREC dataset11(Li and Roth, 2002),
reaching an accuracy of 91.8%. However, such data
refers to typical instances from QA, whose syntactic
patterns can be easily generalized by STK. In con-
trast, we have shown that STK-CT is not effective
for our domain, as it presents very innovative ele-
ments: questions in affirmative and highly variable
format. Thus, we employed new methods such as
PTK, dependency structures, multiple sequence ker-
nels including category information and many com-
binations.
Regarding the use of Kernel Methods, there is
a considerably large body of work in Natural Lan-
guage Processing, e.g. regarding syntactic parsing
(Collins and Duffy, 2002; Kudo et al, 2005; Shen
et al, 2003; Kudo and Matsumoto, 2003; Titov and
Henderson, 2006; Toutanova et al, 2004), named
entity recognition and chunking (Cumby and Roth,
2003; Daume? III and Marcu, 2004), relation extrac-
tion (Zelenko et al, 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005; Zhang et al,
2005; Bunescu, 2007; Nguyen et al, 2009a), text
categorization (Cancedda et al, 2003), word sense
disambiguation (Gliozzo et al, 2005) and seman-
tic role labeling (SRL), e.g. (Kazama and Torisawa,
2005; Che et al, 2006a; Moschitti et al, 2008).
However, ours is the first study on the use of sev-
eral combinations of kernels applied to several struc-
tures on very complex data from the Jeopardy! do-
main.
7 Final Remarks and Conclusion
In this paper we have experimented with advanced
structural kernels applied to several kinds of syntac-
tic/semantic linguistic structures for the classifica-
tion of questions in a new application domain, i.e.
Jeopardy!. Our findings are summarized hereafter:
11Available at http://cogcomp.cs.illinois.
edu/Data/QA/QC/
720
First, it should be noted that basic kernels, such
as STK, PTK and SK, when applied to new repre-
sentations, i.e. syntactic/semantic structures, con-
stitute new kernels. Thus structural representations
play a major role and, from this perspective, our pa-
per makes a significant contribution.
Second, the experimental results show that the
higher variability of Jeopardy! questions prevents us
from achieving generalization with typical syntactic
patterns even if they are derived by powerful meth-
ods such as STK. In contrast, partial patterns, such
as those provided by PTK applied to constituency
(or dependency) trees, prove to be effective.
In particular, STK has been considered as the best
kernel for exploiting syntactic information in con-
stituency trees, e.g. it is state-of-the-art in: QC
(Zhang and Lee, 2003; Moschitti et al, 2007; Mos-
chitti, 2008); SRL, (Moschitti et al, 2008; Mos-
chitti et al, 2005; Che et al, 2006b); pronominal
coreference resolution (Yang et al, 2006; Versley
et al, 2008) and Relation Extraction (Zhang et al,
2006; Nguyen et al, 2009b). We showed that, in
the complex domain of Jeopardy!, STK surprisingly
provides low accuracy whereas PTK is rather ef-
fective and greatly outperforms STK. We have also
provided an explanation of such behavior by means
of error analysis: in contrast with traditional ques-
tion classification, which focuses on basic syntactic
patterns (e.g. ?what?, ?where?, ?who? and ?how?).
Figure 5 shows that PTK captures partial patterns
that are important for more complex questions like
those in Jeopardy!
Third, we derived other interesting findings for
NLP related to this novel domain, e.g.: (i) the im-
pact of dependency trees is similar to the one of
constituency trees. (ii) A simple computational rep-
resentation of shallow semantics, i.e. PASS (Mos-
chitti, 2008), does not work in Jeopardy!. (iii) Se-
quence kernels on category cues, i.e., higher level of
lexical semantics, improve question classification.
(iv) RBC jointly used with statistical approaches is
helpful to tackle the Jeopardy! complexity.
Next, our kernel models improve up to 20 abso-
lute percent points over n-grams based approaches,
reaching a significant accuracy of about 70%. Wat-
son, exploiting such a classifier, improved previ-
ous versions using RBC and no definition classifica-
tion both in definition-only evaluations and in game-
based evaluations.
Finally, we point out that:
? Jeopardy! has a variety of different special ques-
tion types that are handled differently. We focus on
kernel methods for definition question for two rea-
sons. First, their recognition relies heavily on parse
structures and is therefore more amenable to the ap-
proach proposed in the paper than the recognition
of other question types. Second, definition is by far
the most frequent special question type in Jeopardy!;
therefore, we can obtain sufficient data for training
and testing.
? We were unable to address the whole QC prob-
lem using a statistical model due to the lack of suffi-
cient training data for most special question classes.
Furthermore, we focused only on the definition clas-
sification and its impact on system performance due
to space reasons.
? Our RBC has a rather imbalanced trade-off be-
tween Precision and Recall. This may not be the
best operating point, but the optimal point is diffi-
cult to obtain empirically for an RBC, which is a
strong motivation of the work in this paper. We ex-
perimented with tuning the trade-off between Preci-
sion and Recall with the RBC, but since RBC uses
hand-crafted rules and does not have a parameter for
that, ultimately the statistical approach proved more
effective.
In future work, we plan to extend the current re-
search by investigating models capable of exploit-
ing predicate argument structures for question clas-
sification and answer reranking. The use of syntac-
tic/semantic kernels is a promising research direc-
tion (Basili et al, 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). In this
perspective kernel learning is a very interesting re-
search line, considering the complexity of represen-
tation and classification problems in which our ker-
nels operate.
Acknowledgements
This work has been supported by the IBM?s Open
Collaboration Research (OCR) awards program. We
are deeply in debt with Richard Johansson, who pro-
duced the earlier syntactic/semantic representations
of the Jeopardy! questions from the text format.
721
References
Kisuh Ahn, Johan Bos, Stephen Clark, James R. Cur-
ran, Tiphaine Dalmas, Jochen L. Leidner, Matthew B.
Smillie, and Bonnie Webber. 2004. Question an-
swering with qed and wee at trec-2004. In E. M.
Voorhees and L. P. Buckland, editors, The Thirteenth
Text REtrieval Conference, TREC 2004, pages 595?
599, Gaitersburg, MD.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of CoNLL-
2005, pages 1?8, Ann Arbor, Michigan. Association
for Computational Linguistics.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of ACM SIGIR.
S. Blair-Goldensohn, K. R. McKeown, and A. H.
Schlaikjer. 2004. Answering definitional questions:
A hybrid approach. In M. Maybury, editor, Proceed-
ings of AAAI 2004. AAAI Press.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ?07.
Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear models.
In SIGIR ?06: Proceedings of the 29th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 615?616, New
York, NY, USA. ACM.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006a. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 73?
80, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006b. A hybrid convolution tree kernel for semantic
role labeling. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 73?80, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
Proceedings of ACL.
Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358?365.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern
models for definitional QA. In Proceedings of SIGIR,
Salvador, Brazil. ACM.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423?429, Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proceedings of ACL.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).
Danilo Giampiccolo, Pamela Froner, Anselmo Pen?as,
Christelle Ayache, Dan Cristea, Valentin Jijkoun,
Petya Osenova, Paulo Rocha, Bogdan Sacaleanu, and
Richard Suteliffe. 2007. Overview of the CLEF 2007
multilingual question anwering track. In Proceedings
of the Cross Language Evaluation Forum.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and
B. Rink. 2006. Question answering with lcc chaucer
at trec 2006. In Proceedings of TREC.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods ? Sup-
port Vector Learning, 13.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
722
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings of
the Workshop on Deep Linguistic Processing, DeepLP
?07, pages 41?48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP?05.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Shyong Lam, David Pennock, Dan Cosley, and Steve
Lawrence. 2003. 1 billion pages = 1 milllion dollars?
mining the web to pay ?who wants to be a millionaire?
In Proceedings of the 19th Conference on Uncertainty
in AI.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL.
Michael C. McCord. 1980. Slot grammars. Computa-
tional Linguistics.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31, Boston, United
States.
Alessandro Moschitti, Bonaventura Coppola, Ana-Maria
Giuglea, and Roberto Basili. 2005. Hierarchical se-
mantic role labeling. In CoNLL 2005 shared task.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL?07.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ?08, NY, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009a. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378?1387, Singapore, August.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009b. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1378?1387, Morristown,
NJ, USA. Association for Computational Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?105.
Yutaka Sasaki, Chuan-Jie Lin, Kuang-hua Chen, and
Hsin-Hsi Chen. 2007. Overview of the NTCIR-6
cross-lingual question answering (CLQA) task. In
Proceedings of the 6th NTCIR Workshop on Evalua-
tion of Information Access Technologies.
Y. Sasaki. 2005. Question answering as question-biased
term extraction: A new approach toward multilingual
qa. In Proceedings of ACL, pages 215?222.
John Shawe-Taylor and Nello Cristianini. 2004. LaTeX
User?s Guide and Document Reference Manual. Ker-
nel Methods for Pattern Analysis, Cambridge Univer-
sity Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL.
L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG
Based Features in Parse Reranking. In Proceedings of
EMNLP, Sapporo, Japan.
S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin,
N. Shimizu, P. Kantor, D. Kelly, and N. Wacholder.
2004. Hitiqa: Towards analytical question answering.
In Proceedings of COLING.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT, Columbus, Ohio.
J. Suzuki, Y. Sasaki, and E. Maeda. 2002. Svm an-
swer selection for open-domain question answering.
In Proceedings of Coling, pages 974?980.
Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku
Maeda. 2003. Question classification using hdag ker-
nel. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answering,
pages 61?68, Sapporo, Japan, July. Association for
Computational Linguistics.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In The 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing?08), Manchester, England.
723
Ellen M. Voorhees and Hoa Trang Dang. 2006.
Overview of the TREC 2005 question answering track.
In Proceedings of the TREC 2005 Conference.
E. M. Voorhees. 2004. Overview of the trec 2004 ques-
tion answering track. In Proceedings of TREC 2004.
Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proc. COLING-ACL 06.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM Press.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP?2005, Lecture Notes in Computer Science (LNCS
3651), pages 378?389, Jeju Island, South Korea.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
724
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1034?1046,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Lexical Similarity via Convolution Kernels on Dependency Trees
Danilo Croce
DII
University of Tor Vergata
00133 Roma, Italy
croce@info.uniroma2.it
Alessandro Moschitti
DISI
University of Trento
38123 Povo (TN), Italy
moschitti@disi.unitn.it
Roberto Basili
DII
University of Tor Vergata
00133 Roma, Italy
basili@info.uniroma2.it
Abstract
A central topic in natural language process-
ing is the design of lexical and syntactic fea-
tures suitable for the target application. In this
paper, we study convolution dependency tree
kernels for automatic engineering of syntactic
and semantic patterns exploiting lexical simi-
larities. We define efficient and powerful ker-
nels for measuring the similarity between de-
pendency structures, whose surface forms of
the lexical nodes are in part or completely dif-
ferent. The experiments with such kernels for
question classification show an unprecedented
results, e.g. 41% of error reduction of the for-
mer state-of-the-art. Additionally, semantic
role classification confirms the benefit of se-
mantic smoothing for dependency kernels.
1 Introduction
A central topic in Natural Language Processing is
the design of lexical and syntactic features suitable
for the target application. The selection of effective
patterns composed of syntactic dependencies and
lexical constraints is typically a complex task.
Additionally, the availability of training data is
usually scarce. This requires the development of
generalized features or the definition of seman-
tic similarities between them, e.g. as proposed in
(Resnik, 1995; Jiang and Conrath, 1997; Schtze,
1998; Pedersen et al, 2004a; Bloehdorn and Mos-
chitti, 2007b; Davis et al, 2007) or in semi-
supervised settings, e.g. (Chapelle et al, 2006).
A semantic similarity can be defined at structural
level over a graph, e.g. (Freeman, 1977; Bunke and
Shearer, 1998; Brandes, 2001; Zhao et al, 2009), as
well as combining structural and lexical similarity
over semantic networks, e.g. (Cowie et al, 1992; Wu
and Palmer, 1994; Resnik, 1995; Jiang and Conrath,
1997; Schtze, 1998; Leacock and Chodorow, 1998;
Pedersen et al, 2004a; Budanitsky and Hirst, 2006).
More recent research also focuses on mechanisms
to define if two structures, e.g. graphs, are enough
similar, as explored in (Mihalcea, 2005; Zhao et al,
2009; Fu?rstenau and Lapata, 2009; Navigli and La-
pata, 2010).
On one hand, previous work shows that there is
a substantial lack of automatic methods for engi-
neering lexical/syntactic features (or more in gen-
eral syntactic/semantic similarity). On the other
hand, automatic feature engineering of syntactic or
shallow semantic structures has been carried out
by means of structural kernels, e.g. (Collins and
Duffy, 2002; Kudo and Matsumoto, 2003; Cumby
and Roth, 2003; Cancedda et al, 2003; Daume? III
and Marcu, 2004; Toutanova et al, 2004; Shen et al,
2003; Gliozzo et al, 2005; Kudo et al, 2005; Titov
and Henderson, 2006; Zelenko et al, 2002; Bunescu
and Mooney, 2005; Zhang et al, 2006). The main
idea of structural kernels is to generate structures
that in turn represent syntactic or shallow semantic
features. Most notably, the work in (Bloehdorn and
Moschitti, 2007b) encodes lexical similarity in such
kernels. This is essentially the syntactic tree ker-
nel (STK) proposed in (Collins and Duffy, 2002) in
which syntactic fragments from constituency trees
can be matched even if they only differ in the leaf
nodes (i.e. they have different surface forms). This
implies matching scores lower than 1, depending on
the semantic similarity of the corresponding leaves
in the syntactic fragments.
Although this kernel achieves state-of-the-art per-
formance in NLP tasks, such as Question Classifica-
1034
tion (Bloehdorn and Moschitti, 2007b) and Textual
Entailment (Mehdad et al, 2010), it offers clearly
possibility of improvement: (i) better possibility to
exploit semantic smoothing since, e.g., trivially STK
only matches the syntactic structure apple/orange
when comparing the big beautiful apple to a nice
large orange; and (ii) STK cannot be effectively ap-
plied to dependency structures, e.g. see experiments
and motivation in (Moschitti, 2006a). Additionally,
to our knowledge, there is no previous study that
clearly describes how dependency structures should
be converted in trees to be fully and effectively ex-
ploitable by convolution kernels. Indeed, although
the work in (Culotta and Sorensen, 2004) defines a
dependency tree also using node similarity, it is not
a convolution kernel: this results in a much poorer
feature space.
In this paper, we propose a study of convolution
kernels for dependency structures aiming at jointly
modeling syntactic and lexical semantic similarity.
More precisely, we define several dependency trees
exploitable by the Partial Tree Kernel (PTK) (Mos-
chitti, 2006a) and compared them with STK over
constituency trees. Most importantly, we define
an innovative and efficient class of kernels, i.e. the
Smoothed Partial Tree Kernels (SPTKs), which can
measure the similarity of structural similar trees
whose nodes are associated with different but re-
lated lexicals. Given the convolution nature of such
kernels any possible node path of lexicals provide
a contribution smoothed by the similarity accounted
by its nodes.
The extensive experimentation on two datasets of
question classification (QC) and semantic role label-
ing (SRL), shows that: (i) PTK applied to our depen-
dency trees outperforms STK, demonstrating that
dependency parsers are fully exploitable for feature
engineering based on structural kernels; (ii) SPTK
outperforms any previous kernels achieving an un-
precedented result of 41% of error reduction with re-
spect to the former state-of-the-art on QC; and (iii)
the experiments on SRL confirm that the approach
can be applied to different tasks without any tuning
and again achieving state-of-the-art accuracy.
In the reminder of this paper, Section 2 provides
the background for structural and lexical similar-
ity kernels. Section 3 introduces SPTK. Section 4
provides our representation models for dependency
trees. Section 5 presents the experimental evaluation
for QC and SRL. Section 6 derives the conclusions.
2 Kernel Background
In kernel-based machines, both learning and classi-
fication algorithms only depend on the inner prod-
uct between instances. This in several cases can be
efficiently and implicitly computed by kernel func-
tions by exploiting the following dual formulation:?
i=1..l yi?i?(oi)?(o) + b = 0, where oi and o are
two objects, ? is a mapping from the objects to fea-
ture vectors ~xi and ?(oi)?(o) = K(oi, o) is a ker-
nel function implicitly defining such mapping. In
case of structural kernels,K determines the shape of
the substructures describing the objects above. The
most general kind of kernels used in NLP are string
kernels, e.g. (Shawe-Taylor and Cristianini, 2004),
the Syntactic Tree Kernels (Collins and Duffy, 2002)
and the Partial Tree Kernels (Moschitti, 2006a).
2.1 String Kernels
The String Kernels (SK) that we consider count
the number of subsequences shared by two strings
of symbols, s1 and s2. Some symbols during the
matching process can be skipped. This modifies
the weight associated with the target substrings as
shown by the following SK equation:
SK(s1, s2) =
?
u???
?u(s1) ? ?u(s2) =
?
u???
?
~I1:u=s1[~I1]
?
~I2:u=s2[~I2]
?d(~I1)+d(~I2)
where, ?? = ??n=0 ?n is the set of all strings, ~I1 and
~I2 are two sequences of indexes ~I = (i1, ..., i|u|),
with 1 ? i1 < ... < i|u| ? |s|, such that u = si1 ..si|u| ,
d(~I) = i|u| ? i1 + 1 (distance between the first and
last character) and ? ? [0, 1] is a decay factor.
It is worth noting that: (a) longer subsequences
receive lower weights; (b) some characters can be
omitted, i.e. gaps; (c) gaps determine a weight since
the exponent of ? is the number of characters and
gaps between the first and last character; and (c)
the complexity of the SK computation is O(mnp)
(Shawe-Taylor and Cristianini, 2004), where m and
n are the lengths of the two strings, respectively and
p is the length of the largest subsequence we want to
consider.
1035
2.2 Tree Kernels
Convolution Tree Kernels compute the number
of common substructures between two trees T1
and T2 without explicitly considering the whole
fragment space. For this purpose, let the set
F = {f1, f2, . . . , f|F|} be a tree fragment space and
?i(n) be an indicator function, equal to 1 if the
target fi is rooted at node n and equal to 0 oth-
erwise. A tree-kernel function over T1 and T2 is
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), NT1
and NT2 are the sets of the T1?s and T2?s nodes,
respectively and ?(n1, n2) = ?|F|i=1 ?i(n1)?i(n2).
The latter is equal to the number of common frag-
ments rooted in the n1 and n2 nodes. The ? func-
tion determines the richness of the kernel space and
thus different tree kernels. Hereafter, we consider
the equation to evaluate STK and PTK 1.
2.2.1 Syntactic Tree Kernels (STK)
To compute STK is enough to compute
?STK(n1, n2) as follows (recalling that since
it is a syntactic tree kernels, each node can be
associated with a production rule): (i) if the
productions at n1 and n2 are different then
?STK(n1, n2) = 0; (ii) if the productions at
n1 and n2 are the same, and n1 and n2 have
only leaf children then ?STK(n1, n2) = ?; and
(iii) if the productions at n1 and n2 are the
same, and n1 and n2 are not pre-terminals then
?STK(n1, n2) = ?
?l(n1)
j=1 (1 + ?STK(c
j
n1 , cjn2)),
where l(n1) is the number of children of n1 and cjn
is the j-th child of the node n. Note that, since the
productions are the same, l(n1) = l(n2) and the
computational complexity of STK is O(|NT1 ||NT2 |)
but the average running time tends to be linear,
i.e.O(|NT1 |+ |NT2 |), for natural language syntactic
trees (Moschitti, 2006a).
2.2.2 The Partial Tree Kernel (PTK)
The computation of PTK is carried out by the
following ?PTK function: if the labels of n1
and n2 are different then ?PTK(n1, n2) = 0; else
?PTK(n1, n2) =
?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(~I1)+d(~I2)
l(~I1)?
j=1
?PTK(cn1(~I1j), cn2(~I2j))
)
1To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
where d(~I1) = ~I1l(~I1)?~I11+1 and d(~I2) = ~I2l(~I2)?
~I21 + 1. This way, we penalize both larger trees and
child subsequences with gaps. PTK is more general
than the STK as if we only consider the contribu-
tion of shared subsequences containing all children
of nodes, we implement the STK kernel. The com-
putational complexity of PTK is O(p?2|NT1 ||NT2 |)
(Moschitti, 2006a), where p is the largest subse-
quence of children that we want consider and ? is the
maximal outdegree observed in the two trees. How-
ever the average running time again tends to be lin-
ear for natural language syntactic trees (Moschitti,
2006a).
2.3 Lexical Semantic Kernel
Given two text fragments d1 and d2 ? D (the text
fragment set), a general lexical kernel (Basili et al,
2005) defines their similarity as:
K(d1, d2) =
?
w1?d1,w2?d2
(?1?2)? ?(w1, w2) (1)
where ?1 and ?2 are the weights of the words (fea-
tures) w1 and w2 in the documents d1 and d2, re-
spectively, and ? is a term similarity function, e.g.
(Pedersen et al, 2004b; Sahlgren, 2006; Corley and
Mihalcea, 2005; Mihalcea et al, 2005). Technically,
any ? can be used, provided that the resulting Gram
matrix, G = K(d1, d2) ?d1, d2 ? D is positive
semi-definite (Shawe-Taylor and Cristianini, 2004)
(D is typically the training text set).
We determine the term similarity function through
distributional analysis (Pado and Lapata, 2007), ac-
cording to the idea that the meaning of a word can
be described by the set of textual contexts in which it
appears (Distributional Hypothesis, (Harris, 1964)).
The contexts are words appearing in a n-window
with target words: such a space models a generic
notion of semantic relatedness, i.e. two words
close in the space are likely to be either in paradig-
matic or syntagmatic relation as in (Sahlgren, 2006).
The original word-by-word context matrix M is de-
composed through Singular Value Decomposition
(SVD) (Golub and Kahan, 1965) into the product
of three new matrices: U , S, and V so that S is di-
agonal and M = USV T . M is approximated by
Ml = UlSlV Tl in which only the first l columns of
U and V are used, and only the first l greatest singu-
lar values are considered. This approximation sup-
plies a way to project a generic term wi into the l-
1036
dimensional space using W = UlS1/2l , where eachrow corresponds to the representation vectors ~wi.
Therefore, given two words w1 and w2, the term
similarity function ? is estimated as the cosine simi-
larity between the corresponding projections ~w1, ~w2,
i.e ?(w1, w2) = ~w1? ~w2? ~w1?? ~w2? . The latent semantic ker-nels (Siolas and d?Alch Buc, 2000; Cristianini et al,
2001) derive G by applying LSA, resulting in a valid
kernel.
Another methods to design a valid kernel is to rep-
resent words as word vectors and compute ? as their
scalar product between such vectors. For example,
in (Bloehdorn et al, 2006), bag of hyponyms and
hypernyms (up to a certain level of WordNet hierar-
chy) were used to build such vectors. We will refer
to such similarity as WL (word list).
3 Smoothing Partial Tree Kernel (SPTK)
Combining lexical and structural kernels provides
clear advantages on all-vs-all words similarity,
which tends to semantically diverge. Indeed syn-
tax provides the necessary restrictions to com-
pute an effective semantic similarity. Following
this idea, Bloedhorn & Moschitti (2007a) mod-
ified step (i) of ?STK computation as follows:
(i) if n1 and n2 are pre-terminal nodes with
the same number of children, ?STK(n1, n2) =
?
?nc(n1)
j=1 ?(lex(n1), lex(n2)), where lex returns
the node label. This allows to match fragments hav-
ing same structure but different leaves by assigning a
score proportional to the product of the lexical sim-
ilarities of each leaf pair. Although it is an inter-
esting kernel, the fact that lexicals must belong to
the leaf nodes of exactly the same structures limits
its applications. Trivially, it cannot work on depen-
dency trees. Hereafter, we define a much more gen-
eral smoothed tree kernel that can be applied to any
tree and exploit any combination of lexical similari-
ties, respecting the syntax enforced by the tree.
3.1 SPTK Definition
If n1 and n2 are leaves then ??(n1, n2) =
???(n1, n2); else
??(n1, n2) = ??(n1, n2)?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(~I1)+d(~I2)
l(~I1)?
j=1
??(cn1(~I1j), cn2(~I2j))
)
, (2)
where ? is any similarity between nodes, e.g. be-
tween their lexical labels, and the other variables are
the same of PTK.
3.2 Soundness
A completely formal proof of the validity of the
Eq. 2 is beyond the purpose of this paper (mainly
due to space reason). Here we give a first sketch:
let us consider ? as a string matching between
node labels and ? = ? = 1. Each recursive
step of Eq. 2 can be seen as a summation of (1 +
?l(~I1)
j=1 ?STK(cn1(~I1j), cn2(~I2j))), i.e. the ?STK
recursive equation (see Sec. 2.2.1), for all subse-
quences of children cn1(~I1j). In other words, PTK
is a summation of an exponential number of STKs,
which are valid kernels. It follows that PTK is a ker-
nel. Note that the multiplication by ? and ? elevated
to any power only depends on the target fragment.
Thus, it just gives an additional weight to the frag-
ment and does not violate the Mercer?s conditions.
In contrast, the multiplication by ?(n1, n2) does de-
pend on both comparing examples, i.e. on n1 and n2.
However, if the matrix [?(n1, n2)
]
?n1, n2 ? f ? F
is positive semi-definite, a decomposition exists
such that ?(n1, n2) = ?(n1)?(n2) ? ??(n1, n2)
can be written as ?|F|i=1 ?(n1)?i(n1)?(n2)?i(n2)
= ?|F|i=1 ??(n1)??(n2) (see Section 2.2), which
proves SPTK to be a valid kernel.
3.3 Efficient Evaluation
We followed the idea in (Moschitti, 2006a) for effi-
ciently computing SPTK. We consider Eq. 2 evalu-
ated with respect to sequences of different length p;
it follows that
?(n1, n2) = ??(n1, n2)
(
?2 +
m?
p=1
?p(cn1 , cn2)
)
,
where ?p evaluates the number of common sub-
trees rooted in subsequences of exactly p children
(of n1 and n2) and m = min{l(cn1), l(cn2)}.
Given the two child sequences s1a = cn1 and
s2b = cn2 (a and b are the last children)
?p(s1a, s2b) = ?(a, b)?
|s1|?
i=1
|s2|?
r=1
?|s1|?i+|s2|?r ?
??p?1(s1[1 : i], s2[1 : r])
where s1[1 : i] and s2[1 : r] are the child subse-
quences from 1 to i and from 1 to r of s1 and s2. If
we name the double summation term as Dp, we can
1037
S1
SBARQ
.
?::.
SQ
VP
NP
PP
NP
NN
field::n
NN
football::n
DT
a::d
IN
of::i
NP
NN
width::n
DT
the::d
AUX
be::v
WHNP
WP
what::w
Figure 1: Constituent Tree (CT)
rewrite the relation as:
?p(s1a, s2b) =
{
?(a, b)Dp(|s1|, |s2|) if ?(a, b) > 0;
0 otherwise.
Note that Dp satisfies the recursive relation:
Dp(k, l) = ?p?1(s1[1 : k], s2[1 : l]) + ?Dp(k, l ? 1)
+?Dp(k ? 1, l)? ?2Dp(k ? 1, l ? 1)
By means of the above relation, we can compute the
child subsequences of two sequences s1 and s2 in
O(p|s1||s2|). Thus the worst case complexity of the
SPTK is identical to PTK, i.e. O(p?2|NT1 ||NT2 |),
where ? is the maximum branching factor of the two
trees. The latter is very small in natural language
parse trees and we also avoid the computation of
node pairs with non similar labels.
We note that PTK generalizes both (i) SK, allow-
ing the similarity between sequences (node children)
structured in a tree and (ii) STK, allowing the com-
putation of STK over any possible pair of subtrees
extracted from the original tree. For this reason,
we do not dedicate additional space on the defini-
tion of the smoothed SK or smoothed STK, which
are in any case important corollary findings of our
research.
3.4 Innovative Features of SPTK
The most similar kernel to SPTK is the Syntactic
Semantic Tree Kernel (SSTK) proposed in (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b). However, the following aspects show
the remarkable innovativeness of SPTK:
? SSTK can only work on constituency trees
and not on dependency trees (see (Moschitti,
2006a)).
? The lexical similarity in SSTK is only applied
to leaf nodes in exactly the same syntactic
constituents. Only complete matching of the
structure of subtrees is allowed: there is abso-
lutely no flexibility, e.g. the NP structure ?ca-
ble television system? has no match with the
NP ?video streaming system?. SPTK provides
matches between all possible relevant subparts,
e.g. ?television system? and ?video system? (so
also exploiting the meaningful similarity be-
tween ?video? and ?television?).
? The similarity in the PTK equation is added
such that SPTK still corresponds to a scalar
product in the semantic/structure space2.
? We have provided a fast evaluation of SPTK
with dynamic programming (otherwise the
computation would have required exponential
time).
4 Dependency Tree Structures
The feature space generated by the structural ker-
nels, presented in the previous section, obviously de-
pends on the input structures. In case of PTK and
SPTK different tree representations may lead to en-
gineer more or less effective syntactic/semantic fea-
ture spaces. The next two sections provide our repre-
sentation models for dependency trees and their dis-
cussion.
4.1 Proposed Computational Structures
Given the following sentence:
(s1) What is the width of a football field?
The representation tree for a phrase structure
paradigm leaves little room for variations as shown
by the constituency tree (CT) in Figure 1. We ap-
ply lemmatization to the lexicals to improve gener-
alization and, at the same time, we add a generalized
PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), de-
terminer (::d) and so on, to them. This is useful to
measure similarity between lexicals belonging to the
same grammatical category.
In contrast, the conversion of dependency struc-
tures in computationally effective trees (for the
above kernels) is not straightforward. We need to
decide the role of lexicals, their grammatical func-
tions (GR), PoS-tags and dependencies. It is natural
2This is not trivial: for example if sigma is added in Eq. 2 by
only multiplying the ?d1+d2 term, no valid space is generated.
1038
ROOT
VBZ
P
.
?::.
PRD
NN
NMOD
IN
PMOD
NN
field::nNMOD
NN
football::n
NMOD
DT
a::d
of::i
width::nNMOD
DT
the::d
be::vSBJ
WP
what::w
Figure 2: PoS-Tag Centered Tree (PCT)
ROOT
P
.
?::.
PRD
NMOD
PMOD
NN
field::n
NMOD
NN
football::n
NMOD
DT
a::d
IN
of::i
NN
width::n
NMOD
DT
the::d
VBZ
be::v
SBJ
WP
what::w
Figure 3: Grammatical Relation Centered Tree (GRCT)
be::v
VBZROOT?::.
.P
width::n
NNPRDof::i
INNMODfield::n
the::d
DTNMOD
what::w
WPSBJ
NNPMODfootball::n
NNNMOD
a::d
DTNMOD
Figure 4: Lexical Centered Tree (LCT)
to associate edges with dependencies but, since our
kernels cannot process labels on the arcs, they must
be associated with tree nodes. The basic idea of our
structures is to use (i) one of the three kinds of infor-
mation above as central node, from which depen-
be::v
?::.width::n
of::i
field::n
football::na::d
the::d
what::w
Figure 5: Lexical Only Centered Tree (LOCT)
TOP
.
?::.
NN
field::n
NN
football::n
DT
a::d
IN
of::i
NN
width::n
DT
the::d
VBZ
be::v
WP
what::w
Figure 6: Lexical and PoS-Tag Sequences Tree (LPST)
TOP
?::.field::nfootball::na::dof::iwidth::nthe::dbe::vwhat::w
Figure 7: Lexical Sequences Tree (LST)
dencies are drawn and (ii) all the other information
as features (in terms of additional nodes) attached to
the central nodes.
We define three main trees: the PoS-Tag Centered
Tree (PCT), e.g. see Figure 2, where the GR is added
as father and the lexical as a child; the GR Centered
Tree (GRCT), e.g. see Figure 3, where the PoS-Tags
are children of GR nodes and fathers of their associ-
ated lexicals; and the Lexical Centered Tree (LCT),
e.g. see Figure 4, in which both GR and PoS-Tag are
added as the rightmost children.
TOP
ROOT
P
.
?::.
PRD
NMOD
PMOD
NN
goal::n
NMOD
NN
hockey::n
NMOD
NN
ice::n
NMOD
DT
an::d
IN
of::i
NN
dimension::n
NMOD
DT
the::d
VBP
be::v
SBJ
WP
what::w
Figure 8: Grammatical Relation Centered Tree of (s2)
4.2 Comparative Structures
To better study the role of the above dependency
structures, especially from a performance perspec-
tive, we define additional structures: the Lexical
Only Centered Tree (LOCT), e.g. see Figure 5,
which is an LCT only containing lexical nodes; the
Lexical and PoS-Tag Sequences Tree (LPST), e.g.
see Figure 6, which ignores the syntactic structure
of the sentence being a simple sequence of PoS-Tag
nodes, where lexicals are simply added as children;
and the Lexical Sequence Tree (LST), where only
lexical items are leaves of a single root node. PTK
1039
and PSTK applied to it simulates a standard SK and
an SK with smoothing, respectively.
4.3 Structural Features
Section 2 has already described the kind of features
generated by SK, STK and PTK. However, it is
interesting to analyze what happens when SPTK is
applied. For example, given the following sentence
syntactically and semantically similar to s1:
(s2) What is the dimension of an ice hockey goal?
Figure 8 shows the corresponding GRCT, whose
largest PTK fragment shared with the GRTC of s1
(Fig. 3) is: (ROOT (SBJ (WP (what::w))) (PRD (NMOD
(DT (the::d))) (NN) (NMOD (IN (of::i)) (PMOD (NMOD (DT))
(NMOD (NN)) (NN)))) (P (. (?::.)))). If smoothing is ap-
plied the matching is almost total, i.e. also the chil-
dren: width::n/dimension::n, football::n/hockey::n
and field::n/goal::n will be matched (with a smooth-
ing equal to the product of their similarities).
The matching using LCT is very interesting:
without smoothing, the largest subtree is: (be::v
(what::w (SBJ) (WP)) (ROOT)); when smoothing is used
only the fragment (NMOD (NN (ice::n)) will not be part
of the match. This suggests that LCT will probably
receive the major benefit from smoothing. Addition-
ally, with respect to all the above structures, LCT is
the only one that can produce only lexical fragments,
i.e. paths only composed by similar lexical nodes
constrained by syntactic dependencies. All the other
trees produce fragments in which lexicals play the
role of features of GR or PoS-Tag nodes.
5 Experiments
The aim of the experiments is to analyze different
levels of representation, i.e. structure, for syntactic
dependency parses. At the same time, we compare
with the constituency trees and different kernels to
derive the best syntactic paradigm for convolution
kernels. Most importantly, the role of lexical simi-
larity embedded in syntactic structures will be inves-
tigated. For this purpose, we first carry out extensive
experiments on coarse and fine grained QC and then
we verify our findings on a completely different task,
i.e. Argument Classification in SRL.
5.1 General experimental setup
Tools: for SVM learning, we extended the SVM-
LightTK software3 (Moschitti, 2006a) (which in-
3http://disi.unitn.it/moschitti/Tree-Kernel.htm
cludes structural kernels in SVMLight (Joachims,
2000)) with the smooth match between tree nodes.
For generating constituency trees, we used the Char-
niak parser (Charniak, 2000) whereas we applied
LTH syntactic parser (described in (Johansson and
Nugues, 2008a)) to generate dependency trees.
Lexical Similarity: we used the Eq. 1 with ?1 =
?2 = 1 and ? is derived with both approaches de-
scribed in Sec. 2.3. The first approach is LSA-based:
LSA was applied to ukWak (Baroni et al, 2009),
which is a large scale document collection made by
2 billion tokens. More specifically, to build the ma-
trix M, POS tagging is first applied to build rows
with pairs ?lemma, ::POS?, or lemma::POS in brief.
The contexts of such items are the columns of M
and are short windows of size [?3,+3], centered on
the items. This allows for better capturing syntactic
properties of words. The most frequent 20,000 items
are selected along with their 20k contexts. The en-
tries of M are the point-wise mutual information be-
tween them. The SVD reduction is then applied to
M, with a dimensionality cut of l = 250. The sec-
ond approach uses the similarity based on word list
(WL) as provided in (Li and Roth, 2002).
Models: SVM-LightTK is applied to the different
tree representations discussed in Section 4. Since
PTK and SPTK are typically used in our experi-
ments, to have a more compact acronym for each
model, we associate the latter with the name of the
structure, i.e. this indicates that PTK is applied to
it. Then the presence of the subscript WL and LSA
indicates that SPTK is applied along with the corre-
sponding similarity, e.g. LCTWL is the SPTK ker-
nel applied to LCT structure, using WL similarity.
We experiment with multi-classification, which we
model through one-vs-all scheme by selecting the
category associated with the maximum SVM mar-
gin. The quality of such classification is measured
with accuracy. We determine the statistical signi-
cance by using the model described in (Yeh, 2000)
and implemented in (Pado?, 2006).
The parameterization of each classifier is carried on
a held-out set (30% of the training) and concerns
with the setting of the trade-off parameter (option -
c) and the Leaf Weight (LeW ) (see Sec. 5.2), which
is used to linearly scale the contribution of the leaf
nodes. In contrast, the cost-factor parameter of the
SVM-LightTK is set as the ratio between the num-
1040
80% 
82% 
84% 
86% 
88% 
90% 
92% 
0 1000 2000 3000 4000 5000 
Accu
racy
 
Number of Examples 
PCT LPST CT LOCT GRCT LCT BOW 
Figure 9: Learning curves: comparison with no similarity
80% 
82% 
84% 
86% 
88% 
90% 
92% 
94% 
0 1000 2000 3000 4000 5000 
Accu
racy
 
Number of Examples 
PCT-WL LPST-WL CT-WL LOCT-WL GRCT-WL LCT-WL PCT 
Figure 10: Learning curves: comparison with similarity
ber of negative and positive examples for attempting
to have a balanced Precision/Recall.
5.2 QC experiments
For these experiments, we used the UIUC dataset
(Li and Roth, 2002). It is composed by a training
set of 5,452 questions and a test set of 500 ques-
tions4. Question classes are organized in two levels:
6 coarse-grained classes (like ENTITY or HUMAN)
and 50 fine-grained sub-classes (e.g. Plant, Food
as subclasses of ENTITY).
The outcome of the several kernels applied to sev-
eral structures for the coarse and fine grained QC
is reported in Table 1. The first column shows
the experimented models, obtained by applying
PTK/SPTK to the structures described in Sec. 4. The
last two rows are: CT-STK, i.e. STK applied to a
constituency tree and BOW, which is a linear ker-
4http://cogcomp.cs.illinois.edu/Data/QA/QC/
nel applied to lexical vectors. Column 2, 3 and 4
report the accuracy using no, LSA and WL similar-
ity, where LeW is the amplifying parameter, i.e. a
weight associated with the leaves in the tree. The
last three columns refer to the fine grained task.
It is worth nothing that when no similarity is ap-
plied: (i) BOW produces high accuracy, i.e. 88.8%
but it is improved by STK (the current state-of-the-
art5 in QC (Zhang and Lee, 2003; Moschitti et al,
2007)); (ii) PTK applied to the same tree of STK
produces a slightly lower value (non-statistically
significant difference); (iii) interestingly, when PTK
is instead applied to dependency structures, it im-
proves STK, i.e. 91.60% vs 91.40% (although not
significantly); and (iv) LCT, strongly based on lexi-
cal nodes, is the least accurate, i.e 90.80% since it is
obviously subject to data sparseness (fragments only
composed by lexicals are very sparse).
The very important results can be noted when lex-
ical similarity is used, i.e. SPTK is applied: (a) all
the syntactic-base structures using both LSA or WL
improve the classification accuracy. (b) CT gets the
lowest improvement whereas LCT achieves an im-
pressive result of 94.80%, i.e more than 41% of rel-
ative error reduction. It seems that the lexical similar
paths when driven by syntax produces accurate fea-
tures. Indeed, when syntax is missing such as for the
unstructured lexical path of LSTLSA, the accuracy
does not highly improve or may also decrease. Ad-
ditionally, the result of our best model is so high that
its errors only refer to questions like What did Jesse
Jackson organize ?, where the classifier selected En-
tity instead ofHuman category. These refer to clear
cases where a huge amount of background knowl-
edge is needed for deriving the exact solution.
Finally, on the fine grained experiments LCT
still produces the most accurate outcome again ex-
ceeding the state-of-the-art (Zhang and Lee, 2003),
where WL significantly improves on all models (CT
included).
5.3 Learning curves
It is interesting to study the impact of syntac-
tic/semantic kernels on the learning generalization.
For this purpose, Fig. 9 reports the learning curve
5Note that in (Bloehdorn and Moschitti, 2007b), higher ac-
curacy values for smoothed STK are shown for different param-
eters but the best according to a validation set is not highlighted.
1041
COARSE FINE
NO LSA WL NO LSA WL
LeW Acc. LeW Acc. LeW Acc. LeW Acc. LeW Acc. LeW Acc.
CT 4 90.80% 2 91.00% 5 92.20% 4 84.00% 5 83.00% 7 86.60%
GRCT 3 91.60% 4 92.60% 2 94.20% 3 83.80% 4 83.20% 2 85.00%
LCT 1 90.80% 1 94.80% 1 94.20% 0.33 85.40% 1 86.20% 0.33 87.40%
LOCT 1 89.20% 1 93.20% 1 91.80% 1 85.40% 1 86.80% 1 87.00%
LST 1 88.20% 1 85.80% 1 89.60% 1 84.00% 1 80.00% 1 85.00%
LPST 3 89.40% 1 89.60% 1 92.40% 3 84.20% 4 82.20% 1 84.60%
PCT 4 91.20% 4 92.20% 5 93.40% 4 84.80% 5 84.00% 5 85.20%
CT-STK - 91.20% - - - - - 82.20% - - - -
BOW - 88.80% - - - - - 83.20% - - - -
Table 1: Accuracy of structural several kernels on different structures for coarse and fine grained QC
y = 0.051x2.005 
y = 0.030x1.609 
y = 0.068x1.213 
y = 0.081x1.705 
0 
20 
40 
60 
80 
100 
120 
0 10 20 30 40 50 60 
micr
osec
onds
 
Number of Nodes 
LPST-WL GRCT-WL GRCT LCT-WL LCT LPST 
Figure 11: Micro-seconds for each kernel computation
of the previous models without lexical similarity
whereas Fig. 10 shows the complete SPTK behavior
through the different structures. We note that when
no similarity is used the dependency trees better
generalize than constituency trees or non-syntactic
structures like LPST or BOW. When WL is acti-
vated, all models outperform the best kernel of the
previous pool, i.e. PCT (see dashed line of Fig. 10
or the top curve in Fig. 9).
5.4 Kernel Efficiency
We plotted the average running time of each compu-
tation of PTK/SPTK applied to the different struc-
tures. We divided the examples from QC based
on the number of nodes in each example. Fig-
ure 11 shows the elapsed time in function of the
number of nodes for different tree representations.
We note that: (i) when the WL is not active, LCT
and GRCT are very fast as they impose hierarchical
matching of subtrees; (ii) when the similarity is ac-
tivated, LCTWL and GRCTWL tend to match many
more tree fragments thus their complexity increases.
However, the equations of the curve fit, shown in the
figure, suggests that the trend is sub-quadratic (x1.7).
Only LPSTWL, which has no structure, matches a
very large number of sequences of nodes, when the
similarity is active. This increases the complexity,
which results in an order higher than 2.
5.5 FrameNet Role Classification Experiments
To verify that our findings are general and that our
syntactic/semantic dependency kernels can be effec-
tively exploited for diverse NLP tasks, we experi-
mented with a completely different application, i.e.
FrameNet SRL classification (gold standard bound-
aries). We used the FrameNet version 1.3 with
the 90/10% split between training and test set (i.e
271,560 and 30,173 examples respectively), as de-
fined in (Johansson and Nugues, 2008b), one of the
best system for FrameNet parsing. We used the LTH
dependency parser. LSA was applied to the BNC
corpus, the source of the FrameNet annotations.
For each of 648 frames, we applied SVM along
with the best models for QC, i.e. GRCT and LCT, to
learn its associated binary role classifiers (RC) for
a total of 4,254 classifiers. For example, Figure 12
shows the LCT representation of the first two roles
of the following sentence:
[Bootleggers]CREATOR, then copy [the film]ORIGINAL
[onto hundreds of V HS tapes]GOAL
Table 2 shows the results of the different multi-
classifiers. GRCT and LCT show a large ac-
curacy, i.e. 87.60. This improves up to 88.74
by activating the LSA similarity. The combina-
tion GRCTLSA+LCTLSA significantly improves the
above model, achieving 88.91%. This is very close
to the state-of-the-art of SRL for classification (us-
ing a single classifier, i.e. no joint model), i.e.
89.6%, achieved in (Johansson and Nugues, 2008b).
1042
copy::v
VBPROOTbootlegger::n
NNSSBJ
copy::v
VBPROOTfilm::n
NNOBJthe::d
DTNMOD
Figure 12: LCT Examples for argument roles
Kernel Accuracy
GRCT 87.60%
GRCTLSA 88,61%
LCT 87.61%
LCTLSA 88.74%
GRCT + LCT 87.99%
GRCTLSA + LCTLSA 88.91%
Table 2: Argument Classification Accuracy
Finally, it should be noted that, to learn and test the
SELF MOTION multi-classifier, containing 14,584
examples, distributed on 22 roles, SVM-SPTK em-
ployed 1.5 h and 10 minutes, respectively6.
6 Final Remarks and Conclusion
In this paper, we have proposed a study on repre-
sentation of dependency structures for the design of
effective structural kernels. Most importantly, we
have defined a new class of kernel functions, i.e. SP-
TKs, that carry out syntactic and lexical similarities
on the above structures. SPTK exploits the latter
by providing generalization trough lexical similar-
ities constrained in them. This allows for automat-
ically generating feature spaces of generalized syn-
tactic/semantic dependency substructures.
To test our models, we carried out experiments
on QC and SRL. These show that by exploiting the
similarity between two sets of words carried out ac-
cording to their dependency structure leads to an un-
precedented result for QC, i.e. 94.8% of accuracy.
In contrast, when no structure is used the accuracy
does not significantly improves. We have also pro-
vided a fast algorithm for the computation of SPTK
and empirically shown that it can easily scale.
It should be noted that our models are not abso-
lutely restricted to QC and SRL. Indeed, since most
of the NLP applications are based on syntactic and
lexical representations, SPTK will have a major im-
pact in most of them, e.g.:
6Using one of the 8 processors of an Intel(R) Xeon(R) CPU
E5430 @ 2.66GHz machine, 32Gb Ram.
? Question Answering, the high results for QC
will positively impact on the overall task.
? SRL, SPTK alone reaches the state-of-the-art
(SOA) (only 0.7% less) in FrameNet role clas-
sification. This is very valuable as previous
work showed that tree kernels (TK) alone per-
form lower than models based on manually en-
gineered features for SRL tasks, e.g., (Mos-
chitti, 2004; Giuglea and Moschitti, 2004; Giu-
glea and Moschitti, 2006; Moschitti, 2006b;
Che et al, 2006; Moschitti et al, 2008). Thus
for the first time in an SRL task, a general
tree kernel reaches the same accuracy of heavy
manual feature design. This also suggests an
improvement when used in combinations with
manual feature vectors.
? Relation Extraction and Pronominal Corefer-
ence, whose state-of-the-art for some tasks is
achieved with the simple STK-CT (see (Zhang
et al, 2006) and (Yang et al, 2006; Versley et
al., 2008), respectively).
? In word sense disambiguation tasks, SPTK can
generalize context according to syntactic and
semantic constraints (selectional restrictions)
making very effective distributional semantic
approaches.
? In Opinion Mining SPTK will allow to match
sentiment words within their corresponding
syntactic counterparts and improve the state-
of-the-art (Johansson and Moschitti, 2010b; Jo-
hansson and Moschitti, 2010a).
? Experiments on Recognizing Textual Entail-
ment (RTE) tasks, the use of SSTK (in-
stead of STK-CT) improved the state-of-the-art
(Mehdad et al, 2010). SPTK may provide fur-
ther enhancement and innovative and effective
dependency models.
The above points also suggest many promising fu-
ture research directions, which we would like to ex-
plore.
Acknowledgements
This work has been partially supported by the EC
project FP247758: Trustworthy Eternal Systems via
Evolving Software, Data and Knowledge (EternalS).
1043
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of CoNLL-
2005, pages 1?8, Ann Arbor, Michigan. Association
for Computational Linguistics.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and
Alessandro Moschitti. 2006. Semantic kernels for text
classification based on topological measures of feature
similarity. In Proceedings of ICDM 06, Hong Kong,
2006.
Ulrik Brandes. 2001. A Faster Algorithm for Between-
ness Centrality. Journal of Mathematical Sociology,
25:163?177.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Horst Bunke and Kim Shearer. 1998. A graph distance
metric based on the maximal common subgraph. Pat-
tern Recogn. Lett., 19(3-4):255?259, March.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
O. Chapelle, B. Schlkopf, and A. Zien. 2006. Semi-
Supervised Learning. Adaptive computation and ma-
chine learning. MIT Press, Cambridge, MA, USA, 09.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for semantic
role labeling. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 73?80, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 13?18, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Jim Cowie, Joe Guthrie, and Louise Guthrie. 1992. Lex-
ical disambiguation using simulated annealing. In in
COLING, pages 359?365.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2001. Latent semantic kernels. In Carla Brodley and
Andrea Danyluk, editors, Proceedings of ICML-01,
18th International Conference on Machine Learning,
pages 66?73, Williams College, US. Morgan Kauf-
mann Publishers, San Francisco, US.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423?429, Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and
Inderjit S. Dhillon. 2007. Information-theoretic met-
ric learning. In Proceedings of the 24th international
conference on Machine learning, ICML ?07, pages
209?216, New York, NY, USA. ACM.
Linton C. Freeman. 1977. A Set of Measures of Central-
ity Based on Betweenness. Sociometry, 40(1):35?41.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
In Proceedings of EMNLP ?09, pages 11?20, Morris-
town, NJ, USA.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovering using FrameNet, VerbNet and
PropBank. In In Proceedings of the Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
A.-M. Giuglea and A. Moschitti. 2006. Semantic role
labeling via framenet, verbnet and propbank. In Pro-
ceedings of ACL, Sydney, Australia.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
G. Golub and W. Kahan. 1965. Calculating the singular
values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics: Se-
ries B, Numerical Analysis, 2(2):pp. 205?224.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics. Oxford University Press.
1044
J. J. Jiang and D. W. Conrath. 1997. Semantic Similarity
Based on Corpus Statistics and Lexical Taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X).
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML?00.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis. In
Proceedings of the 23rd International Conference of
Computational Linguistics (Coling 2010), pages 519?
527, Beijing, China.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic?semantic analysis with
PropBank and NomBank. In CoNLL 2008: Proceed-
ings of the Twelfth Conference on Natural Language
Learning, pages 183?187, Manchester, United King-
dom.
Richard Johansson and Pierre Nugues. 2008b. The effect
of syntactic representation on semantic role labeling.
In Proceedings of COLING, Manchester, UK, August
18-22.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Claudia Leacock and Martin Chodorow, 1998. Combin-
ing Local Context and WordNet Similarity for Word
Sense Identification, chapter 11, pages 265?283. The
MIT Press.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL?02.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In HLT-NAACL,
pages 1020?1028.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2005. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence (AAAI
2006), Boston, July.
Rada Mihalcea. 2005. unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In In HLT/EMNLP
2005, pages 411?418.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL?07.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
A. Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of ACL,
Barcelona, Spain.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proccedings of
EACL?06.
Roberto Navigli and Mirella Lapata. 2010. An Experi-
mental Study of Graph Connectivity for Unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 32(4):678?
692.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004a. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004b. Wordnet::similarity - measuring the re-
latedness of concepts. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence, pages 448?453.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Hinrich Schtze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89?96, Sapporo, Japan.
Georges Siolas and Florence d?Alch Buc. 2000. Sup-
port vector machines based on a semantic kernel for
1045
text categorization. In Proceedings of the IEEE-INNS-
ENNS International Joint Conference on Neural Net-
works (IJCNN?00)-Volume 5, page 5205. IEEE Com-
puter Society.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In The 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing?08), Manchester, England.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In 32nd. Annual Meeting of the
Association for Computational Linguistics, pages 133
?138, New Mexico State University, Las Cruces, New
Mexico.
Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proc. COLING-ACL 06.
Alexander S. Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In COLING,
pages 947?953.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM Press.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
Peixiang Zhao, Jiawei Han, and Yizhou Sun. 2009. P-
Rank: a comprehensive structural similarity measure
over information networks. In CIKM ?09: Proceed-
ing of the 18th ACM conference on Information and
knowledge management, pages 553?562, New York,
NY, USA. ACM.
1046
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 458?467,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Feature Engineering for Answer Selection and Extraction
Aliaksei Severyn
DISI, University of Trento
38123 Povo (TN), Italy
severyn@disi.unitn.it
Alessandro Moschitti
Qatar Computing Research Institue
5825 Doha, Qatar
amoschitti@qf.org.qa
Abstract
This paper proposes a framework for automat-
ically engineering features for two important
tasks of question answering: answer sentence
selection and answer extraction. We represent
question and answer sentence pairs with lin-
guistic structures enriched by semantic infor-
mation, where the latter is produced by auto-
matic classifiers, e.g., question classifier and
Named Entity Recognizer. Tree kernels ap-
plied to such structures enable a simple way to
generate highly discriminative structural fea-
tures that combine syntactic and semantic in-
formation encoded in the input trees. We con-
duct experiments on a public benchmark from
TREC to compare with previous systems for
answer sentence selection and answer extrac-
tion. The results show that our models greatly
improve on the state of the art, e.g., up to 22%
on F1 (relative improvement) for answer ex-
traction, while using no additional resources
and no manual feature engineering.
1 Introduction
Question Answering (QA) systems are typically
built from three main macro-modules: (i) search and
retrieval of candidate passages; (ii) reranking or se-
lection of the most promising passages; and (iii) an-
swer extraction. The last two steps are the most in-
teresting from a Natural Language Processing view-
point since deep linguistic analysis can be carried
out as the input is just a limited set of candidates.
Answer sentence selection refers to the task of se-
lecting the sentence containing the correct answer
among the different sentence candidates retrieved by
a search engine.
Answer extraction is a final step, required for
factoid questions, consisting in extracting multi-
words constituting the synthetic answer, e.g., Barack
Obama for a question: Who is the US president?
The definition of rules for both tasks is conceptually
demanding and involves the use of syntactic and se-
mantic properties of the questions and its related an-
swer passages.
For example, given a question from TREC QA1:
Q: What was Johnny Appleseed?s real
name?
and a relevant passage, e.g., retrieved by a search
engine:
A: Appleseed, whose real name was John
Chapman, planted many trees in the early
1800s.
a rule detecting the semantic links between Johnny
Appleseed?s real name and the correct answer
John Chapman in the answer sentence has to
be engineered. This requires the definition of
other rules that associate the question pattern
real name ?(X) with real name is(X) of
the answer sentence. Although this can be done by
an expert NLP engineer, the effort for achieving the
necessary coverage and a reasonable accuracy is not
negligible.
An alternative to manual rule definition is the use
of machine learning, which often shifts the problem
1We use it as our running example in the rest of the paper.
458
to the easier task of feature engineering. Unfortu-
nately, when the learning task is semantically dif-
ficult such as in QA, e.g., features have to encode
combinations of syntactic and semantic properties.
Thus their extraction modules basically assume the
shape of high-level rules, which are, in any case, es-
sential to achieve state-of-the-art accuracy. For ex-
ample, the great IBM Watson system (Ferrucci et
al., 2010) uses a learning to rank algorithm fed with
hundreds of features. The extraction of some of the
latter requires articulated rules/algorithms, which,
in terms of complexity, are very similar to those
constituting typical handcrafted QA systems. An
immediate consequence is the reduced adaptability
to new domains, which requires a substantial re-
engineering work.
In this paper, we show that tree kernels (Collins
and Duffy, 2002; Moschitti, 2006) can be applied to
automatically learn complex structural patterns for
both answer sentence selection and answer extrac-
tion. Such patterns are syntactic/semantic structures
occurring in question and answer passages. To make
such information available to the tree kernel func-
tions, we rely on the shallow syntactic trees enriched
with semantic information (Severyn et al, 2013b;
Severyn et al, 2013a), e.g., Named Entities (NEs)
and question focus and category, automatically de-
rived by machine learning modules, e.g., question
classifier (QC) or focus classifier (FC).
More in detail, we (i) design a pair of shallow
syntactic trees (one for the question and one for the
answer sentence); (ii) connect them with relational
nodes (i.e., those matching the same words in the
question and in the answer passages); (iii) label the
tree nodes with semantic information such as ques-
tion category and focus and NEs; and (iv) use the NE
type to establish additional semantic links between
the candidate answer, i.e., an NE, and the focus word
of the question. Finally, for the task of answer ex-
traction we also connect such semantic information
to the answer sentence trees such that we can learn
factoid answer patterns.
We show that our models are very effective in pro-
ducing features for both answer selection and ex-
traction by experimenting with TREC QA corpora
and directly comparing with the state of the art,
e.g., (Wang et al, 2007; Yao et al, 2013). The re-
sults show that our methods greatly improve on both
tasks yielding a large improvement in Mean Average
Precision for answer selection and in F1 for answer
extraction: up to 22% of relative improvement in F1,
when small training data is used. Moreover, in con-
trast to the previous work, our model does not rely
on external resources, e.g., WordNet, or complex
features in addition to the structural kernel model.
The reminder of this paper is organized as fol-
lows, Sec. 2 describes our kernel-based classifiers,
Sec. 3 illustrates our question/answer relational
structures also enriched with semantic information,
Sec. 4 describes our model for answer selection and
extraction, Sec. 5 illustrates our comparative exper-
iments on TREC data, Sec. 6 reports on our error
analysis, Sec. 7 discusses the related work, and fi-
nally, Sec. 8 derives the conclusions.
2 Structural Kernels for classification
This section describes a kernel framework where the
input question/answer pairs are handled directly in
the form of syntactic/semantic structures.
2.1 Feature vector approach to object pair
classification
A conventional approach to represent a ques-
tion/answer pairs in linear models consists in defin-
ing a set of similarity features {xi} and computing
the simple scalar product h(x) = w ? x =
?
iwixi,
where w is the model weight vector learned on the
training data. Hence, the learning problem boils
down to estimating individual weights of each of
the similarity features xi. Such features often en-
code various types of lexical, syntactic and semantic
similarities shared between a question and its can-
didate. Previous work used a rich number of distri-
butional semantic, knowledge-based, translation and
paraphrase resources to build explicit feature vector
representations. One evident potential downside of
using feature vectors is that a great deal of structural
information encoded in a given text pair is lost.
2.2 Pair Classification using Structural Kernels
A more versatile approach in terms of the input
representation relies on kernels. A typical ker-
nel machine, e.g., SVM, classifies a test input x
using the following prediction function: h(x) =
?
i ?iyiK(x,xi), where ?i are the model parame-
ters estimated from the training data, yi are target
459
variables, xi are support vectors, andK(?, ?) is a ker-
nel function. The latter can measure the similarity
between question and answer pairs.
We define each question/answer pair x as a triple
composed of a question treeT q and answer sentence
tree T s and a similarity feature vector v , i.e., x =
?T q,T s, v?. Given two triples xi and xj , we define
the following kernel:
K(xi,xj) = KTK(T iq,T
j
q)
+ KTK(T is,T
j
s)
+ Kv(v i, vj),
(1)
where KTK computes a structural kernel, e.g., tree
kernel, and Kv is a kernel over feature vectors, e.g.,
linear, polynomial, gaussian, etc. Structural kernels
can capture the structural representation of a ques-
tion/answer pair whereas traditional feature vectors
can encode some sort of similarity, e.g., lexical, syn-
tactic, semantic, between a question and its candi-
date answer.
We prefer to split the kernel computation over a
question/answer pair into two terms since tree ker-
nels are very efficient and there are no efficient
graph kernels that can encode exhaustively all graph
fragments. It should be noted that the tree kernel
sum does not capture feature pairs. Theoretically,
for such purpose, a kernel product should be used.
However, our experiments revealed that using the
product is actually worse in practice. In contrast,
we solve the lack of feature pairing by annotating
the trees with relational tags which are supposed
to link the question tree fragments with the related
fragments from the answer sentence.
Such relational information is very important to
improve the quality of the pair representation as well
as the implicitly generated features. In the next sec-
tion, we show simple structural models that we used
in our experiments for question and answer pair clas-
sification.
2.3 Partial Tree Kernels
The above framework can use any kernel for
structural data. We use the Partial Tree Kernel
(PTK) (Moschitti, 2006) to compute KTK(?, ?) as it
is the most general convolution tree kernel, which
at the same time shows rather good efficiency. PTK
can be effectively applied to both constituency and
dependency parse trees. It generalizes the syntactic
tree kernel (STK) (Collins and Duffy, 2002), which
maps a tree into the space of all possible tree frag-
ments constrained by the rule that sibling nodes can-
not be separated. In contrast, the PTK fragments
can contain any subset of siblings, i.e., PTK allows
for breaking the production rules in syntactic trees.
Consequently, PTK generates an extremely rich fea-
ture space, which results in higher generalization
ability.
3 Relational Structures
This section introduces relational structures de-
signed to encode syntactic and shallow semantic
properties of question/answer pairs. We first define a
simple to construct shallow syntactic tree represen-
tation derived from a shallow parser. Next, we in-
troduce a relational linking scheme based on a plain
syntactic matching and further augment it with ad-
ditional semantic information.
3.1 Shallow syntactic tree
Our shallow tree structure is a two-level syntactic
hierarchy built from word lemmas (leaves), part-of-
speech tags that organized into chunks identified by
a shallow syntactic parser (Fig. 1). We defined a
similar structure in (Severyn and Moschitti, 2012)
for answer passage reranking, which improved on
feature vector baselines.
This simple linguistic representation is suitable
for building a rather expressive answer sentence se-
lection model. Moreover, the use of a shallow parser
is motivated by the need to generate text spans to
produce candidate answers required by an answer
extraction system.
3.2 Tree pairs enriched with relational links
It is important to establish a correspondence be-
tween question and answer sentence aligning related
concepts from both. We take on a two-level ap-
proach, where we first use plain lexical matching to
connect common lemmas from the question and its
candidate answer sentence. Secondly, we establish
semantic links between NEs extracted from the an-
swer sentence and the question focus word, which
encodes the expected lexical answer type (LAT). We
use the question categories to identify NEs that have
460
Figure 1: Shallow tree representation of the example q/a pair from Sec. 1. Dashed arrows (red) indicate the tree
fragments (red dashed boxes) in the question and its answer sentence linked by the relational REL tag, which is
established via syntactic match on the word lemmas. Solid arrows (blue) connect a question focus word name with the
related named entities of type Person corresponding to the question category (HUM) via a relational tag REL-HUM.
Additional ANS tag is used to mark chunks containing candidate answer (here the correct answer John Chapman).
higher probability to be correct answers following a
mapping defined in Table 1.
Next, we briefly introduce our tree kernel-based
models for building question focus and category
classifiers.
Lexical Answer Type. Question Focus represents
a central entity or a property asked by a question
(Prager, 2006). It can be used to search for semanti-
cally compatible candidate answers, thus greatly re-
ducing the search space (Pinchak, 2006). While sev-
eral machine learning approaches based on manual
features and syntactic structures have been recently
explored, e.g. (Quarteroni et al, 2012; Damljanovic
et al, 2010; Bunescu and Huang, 2010), we opt for
the latter approach where tree kernels handle auto-
matic feature engineering.
To build an automatic Question Focus detector we
use a tree kernel approach as follows: we (i) parse
each question; (ii) create a set of positive trees by
labeling the node exactly covering the focus with
FC tag; (iii) build a set of negative trees by labeling
any other constituent node with FC; (iii) we train
the FC node classifier with tree kernels. At the test
time, we try to label each constituent node with FC
generating a set of candidate trees. Finally, we select
the tree and thus the constituent associated with the
highest SVM score.
Question classification. Our question classification
model is simpler than before: we use an SVM multi-
classifier with tree kernels to automatically extract
the question class. To build a multi-class classifier
we train a binary SVM for each of the classes and
apply a one-vs-all strategy to obtain the predicted
Table 1: Expected Answer Type (EAT) ? named entity
types.
EAT Named Entity types
HUM Person
LOCATION Location
ENTITY Organization, Person, Misc
DATE Date, Time, Number
QUANTITY Number, Percentage
CURRENCY Money, Number
class. We use constituency trees as our input repre-
sentation.
Our question taxonomy is derived from the
UIUIC dataset (Li and Roth, 2002) which defines
6 coarse and 50 fine grain classes. In particular,
our set of question categories is formed by adopt-
ing 3 coarse classes: HUM (human), LOC (loca-
tion), ENTY (entities) and replacing the NUM (nu-
meric) coarse class with 3 fine-grain classes: CUR-
RENCY, DATE, QUANTITY2. This set of question
categories is sufficient to capture the coarse seman-
tic answer type of the candidate answers found in
TREC. Also using fewer question classes results in
a more accurate multi-class classifier.
Semantic tagging. Question focus word specifies
the lexical answer type capturing the target informa-
tion need posed by a question, but to make this piece
of information effective, the focus word needs to
be linked to the target candidate answer. The focus
word can be lexically matched with words present in
2This class is composed by including all the fine-grain
classes from NUMERIC coarse class except for CURRENCY
and DATE.
461
the answer sentence, or the match can be established
using semantic information. Clearly, the latter ap-
proach is more appealing since it helps to alleviate
the lexical gap problem, i.e., it improves the cover-
age of the na?ive string matching of words between a
question and its answer.
Hence, we propose to exploit a question focus
along with the related named entities (according to
the mapping from Table 1) of the answer sentence
to establish relational links between the tree frag-
ments. In particular, once the question focus and
question category are determined, we link the fo-
cus word wfocus in the question, with all the named
entities whose type matches the question class (Ta-
ble 1). We perform tagging at the chunk level and
use a relational tag typed with a question class, e.g.,
REL-HUM. Fig. 1 shows an example q/a pair where
the typed relational tag is used in the shallow syntac-
tic tree representation to link the chunk containing
the question focus name with the named entities of
the corresponding type Person, i.e., Appleseed and
John Chapman.
4 Answer Sentence Selection and Answer
Keyword Extraction
This section describes our approach to (i) answer
sentence selection used to select the most promising
answer sentences; and (ii) answer extraction which
returns the answer keyword (for factoid questions).
4.1 Answer Sentence Selection
We cast the task of answer sentence selection as
a classification problem. Considering a supervised
learning scenario, we are given a set of questions
{qi}Ni=1 where each question qi is associated with
a list of candidate answer sentences {(ri, si)}Ni=1,
with ri ? {?1,+1} indicating if a given candidate
answer sentence si contains a correct answer (+1)
or not (?1). Using this labeled data, our goal is to
learn a classifier model to predict if a given pair of
a question and an answer sentence is correct or not.
We train a binary SVM with tree kernels3 to train an
answer sentence classifier. The prediction scores ob-
tained from a classifier are used to rerank the answer
candidates (pointwise reranking), s.t. the sentences
that are more likely to contain correct answers will
3disi.unitn.it/moschitti/Tree-Kernel.htm
be ranked higher than incorrect candidates. In addi-
tion to the structural representation, we augment our
model with basic bag-of-word features (unigram and
bigrams) computed over lemmas.
4.2 Answer Sentence Extraction
The goal of answer extraction is to extract a text span
from a given candidate answer sentence. Such span
represents a correct answer phrase for a given ques-
tion. Different from previous work that casts the an-
swer extraction task as a tagging problem and apply
a CRF to learn an answer phrase tagger (Yao et al,
2013), we take on a simpler approach using a kernel-
based classifier.
In particular, we rely on the shallow tree represen-
tation, where text spans identified by a shallow syn-
tactic parser serve as a source of candidate answers.
Algorithm 1 specifies the steps to generate training
data for our classifier. In particular, for each ex-
ample representing a triple ?a, Tq, Ts? composed of
the answer a, the question and the answer sentence
trees, we generate a set of training examples E with
every candidate chunk marked with an ANS tag (one
at a time). To reduce the number of generated exam-
ples for each answer sentence, we only consider NP
chunks, since other types of chunks, e.g., VP, ADJP,
typically do not contain factoid answers. Finally, an
original untagged tree is used to generate a positive
example (line 8), when the answer sentence contains
a correct answer, and a negative example (line 10),
when it does not contain a correct answer.
At the classification time, given a question and a
candidate answer sentence, all NP nodes of the sen-
tence are marked with ANS (one at a time) as the
possible answer, generating a set of tree candidates.
Then, such trees are classified (using the kernel from
Eq. 1) and the one with the highest score is selected.
If no tree is classified as positive example we do not
extract any answer.
5 Experiments
We provide the results on two related yet different
tasks: answer sentence selection and answer extrac-
tion. The goal of the former is to learn a model
scoring correct question and answer sentence pairs
to bring in the top positions sentences containing the
correct answers. Answer extraction derives the cor-
462
Algorithm 1 Generate training data for answer ex-
traction
1: for all ?a, Tq, Ts? ?D do
2: E ? ?
3: for all chunk ? extract chunks(Ts) do
4: if not chunk == NP then
5: continue
6: T ?s ? tagAnswerChunk(Ts, chunk)
7: if contains answer(a, chunk) then
8: label? +1
9: else
10: label? ?1
11: e? build example(Tq, T ?s, label)
12: E ? E ? {e}
13: return E
rect answer keywords, i.e., a text span such as multi-
words or constituents, from a given sentence.
5.1 Semantic Annotation
We briefly describe the experiments of training auto-
matic question category and focus classifiers, which
are more extensively described in (Severyn et al,
2013b).
Question Focus detection. We used three datasets
for training and evaluating the performance of our
focus detector: SeCo-600 (Quarteroni et al, 2012),
Mooney GeoQuery (Damljanovic et al, 2010) and
the dataset from (Bunescu and Huang, 2010). The
SeCo dataset contains 600 questions. The Mooney
GeoQuery contains 250 question targeted at ge-
ographical information in the U.S. The first two
datasets are very domain specific, while the dataset
from (Bunescu and Huang, 2010) is more generic
containing the first 2,000 questions from the answer
type dataset from Li and Roth annotated with fo-
cus words. We removed questions with implicit and
multiple focuses.
Question Classification. We used the UIUIC
dataset (Li and Roth, 2002) which contains 5,952
factoid questions 4 to train a multi-class question
classifier.
Table 2 summarizes the results of question focus
and category classification.
4We excluded questions from TREC to ensure there is no
overlap with the data used for testing models trained on TREC
QA.
Table 2: Accuracy (%) of focus (FC) and question classi-
fiers (QC) using PTK.
TASK SET PTK
FC
MOONEY 80.5
SECO-600 90.0
BUNESCU 96.9
QC
UIUIC 85.9
TREC 11-12 78.1
5.2 Answer Sentence Selection
We used the train and test data from (Wang et al,
2007) to enable direct comparison with previous
work on answer sentence selection. The training
data is composed by questions drawn from TREC
8-12 while questions from TREC 13 are used for
testing. The data provided for training comes as
two sets: a small set of 94 questions (TRAIN) that
were manually curated for errors5 and 1,229 ques-
tions from the entire TREC 8-12 that contain at least
one correct answer sentence (ALL). The latter set
represents a more noisy setting, since many answer
sentences are marked erroneously as correct as they
simply match a regular expression. Table 3 summa-
rizes the data used for training and testing.
Table 4 compares our kernel-based structural
model with the previous state-of-the-art systems for
answer sentence selection. In particular, we com-
pare with four most recent state of the art answer
sentence reranker models (Wang et al, 2007; Heil-
man and Smith, 2010; Wang and Manning, 2010;
Yao et al, 2013), which report their performance on
the same questions and candidate sets from TREC
13 as provided by (Wang et al, 2007).
Our simple shallow tree representation (Severyn
and Moschitti, 2012) delivers state-of-the-art ac-
curacy largely improving on previous work. Fi-
nally, augmenting the structure with semantic link-
ing (Severyn et al, 2013b) yields additional im-
provement in MAP and MRR. This suggests the
utility of using supervised components, e.g., ques-
tion focus and question category classifiers coupled
with NERs, to establish semantic mapping between
words in a q/a pair.
5In TREC correct answers are identified by regex matching
using the provided answer pattern files
463
Table 3: Summary of TREC data for answer extraction
used in (Yao et al, 2013).
data questions candidates correct
TRAIN 94 4718 348
ALL 1229 53417 6410
TEST 89 1517 284
Table 4: Answer sentence reranking on TREC 13.
System MAP MRR
Wang et al (2007) 0.6029 0.6852
Heilman & Smith (2010) 0.6091 0.6917
Wang & Manning (2010) 0.5951 0.6951
Yao et al (2013) 0.6319 0.7270
+ WN 0.6371 0.7301
shallow tree (S&M, 2012) 0.6485 0.7244
+ semantic tagging 0.6781 0.7358
It is worth noting that our kernel-based classifier
is conceptually simpler than approaches in the previ-
ous work, as it relies on the structural kernels, e.g.,
PTK, to automatically extract salient syntactic pat-
terns relating questions and answers. Our model
only includes the most basic feature vector (uni- and
bi-grams) and does not rely on external sources such
as WordNet.
5.3 Answer Extraction
Our experiments on answer extraction replicate the
setting of (Yao et al, 2013), which is the most recent
work on answer extraction reporting state-of-the-art
results.
Table 5 reports the accuracy of our model in re-
covering correct answers from a set of candidate an-
swer sentences for a given question. Here the fo-
cus is on the ability of an answer extraction system
to recuperate as many correct answers as possible
from each answer sentence candidate. The set of
extracted candidate answers can then be used to se-
lect a single best answer, which is the final output
of the QA system for factoid questions. Recall (R)
encodes the percentage of correct answer sentences
for which the system correctly extracts an answer
(for TREC 13 there are a total of 284 correct answer
sentences), while Precision (P) reflects how many
answers extracted by the system are actually correct.
Clearly, having a high recall system, allows for cor-
rectly answering more questions. On the other hand,
a high precision system would attempt to answer less
questions (extracting no answers at all) but get them
right.
We compare our results to a CRF model of (Yao et
al., 2013) augmented with WordNet features (with-
out forced voting) 6. Unlike the CRF model which
obtains higher values of precision, our system acts
as a high recall system able to recover most of the
answers from the correct answer sentences. Having
higher recall is favorable to high precision in answer
extraction since producing more correct answers can
help in the final voting scheme to come up with a
single best answer. To solve the low recall problem
of their CRF model, Yao et al (2013) apply fairly
complex outlier resolution techniques to force an-
swer predictions, thus aiming at increasing the num-
ber of extracted answers.
To further boost the number of answers produced
by our system we exclude negative examples (an-
swer sentences not containing the correct answer)
from training, which slightly increases the number
of pairs with correctly recovered answers. Never-
theless, it has a substantial effect on the number of
questions that can be answered correctly (assuming
perfect single best answer selection). Clearly, our
system is able to recover a large number of answers
from the correct answer sentences, while low pre-
cision, i.e., extracting answer candidates from sen-
tences that do not contain a correct answer, can be
overcome by further applying various best answer
selection strategies, which we explore in the next
section.
5.4 Best Answer Selection
Since the final step of the answer extraction module
is to select for each question a single best answer
from a set of extracted candidate answers, an answer
selection scheme is required.
We adopt a simple majority voting strategy, where
we aggregate the extracted answers produced by our
answer extraction model. Answers sharing simi-
lar lemmas (excluding stop words) are grouped to-
gether. The prediction scores obtained by the an-
6We could not replicate the results obtained in (Yao et al,
2013) with the forced voting strategy. Thus such result is not
included in Table 5.
464
Table 5: Results on answer extraction. P/R - precision
and recall; pairs - number of QA pairs with a correctly ex-
tracted answer, q - number of questions with at least one
correct answer extracted, F1 sets an upper bound on the
performance assuming the selected best answer among
extracted candidates is always correct. *-marks the set-
ting where we exclude incorrect question answer pairs
from training.
set P R pairs q F1
Yao et al (2013) 25.7 23.4 73 33 -
+ WN 26.7 24.3 76 35 -
TRAIN 29.6 64.4 183 58 65.2
TRAIN* 15.7 71.8 204 66 74.1
Yao et al (2013) 35.2 35.1 100 38 -
+ WN 34.5 34.7 98 38 -
ALL 29.4 74.6 212 69 77.5
ALL* 15.8 76.7 218 73 82.0
Table 6: Results on finding the best answer with voting.
system set P R F1
Yao et al (2013)
TRAIN
55.7 43.8 49.1
+ forced 54.5 53.9 54.2
+ WN 55.2 53.9 54.5
this work 66.2 66.2 66.2
Yao et al (2013)
ALL
67.2 50.6 57.7
+ forced 60.9 59.6 60.2
+ WN 63.6 62.9 63.3
this work 70.8 70.8 70.8
swer extraction classifier are used as votes to decide
on the final rank to select the best single answer.
Table 6 shows the results after the majority vot-
ing is applied to select a single best answer for each
candidate. A rather na??ve majority voting scheme
already produces satisfactory outcome demonstrat-
ing better results than the previous work. Our vot-
ing scheme is similar to the one used by (Yao et al,
2013), yet it is much simpler since we do not per-
form any additional hand tuning to account for the
weight of the ?forced? votes or take any additional
steps to catch additional answers using outlier detec-
tion techniques applied in the previous work.
6 Discussion and Error Analysis
There are several sources of errors affecting the fi-
nal performance of our answer extraction system: (i)
chunking, (ii) named entity recognition and seman-
tic linking, (iii) answer extraction, (iv) single best
answer selection.
Chunking. Our system uses text spans identified by
a chunker to extract answer candidates, which makes
it impossible to extract answers that lie outside the
chunk boundaries. Nevertheless, we found this to
be a minor concern since for 279 out of total 284
candidate sentences from TREC 13 the answers are
recoverable within the chunk spans.
Semantic linking. Our structural model relies heav-
ily on the ability of NER to identify the relevant en-
tities in the candidate sentence that can be further
linked to the focus word of the question. While
our answer extraction model is working on all the
NP chunks, the semantic tags from NER serve as a
strong cue for the classifier that a given chunk has
a high probability of containing an answer. Typical
off-the-shelf NER taggers have good precision and
low recall, s.t. many entities as potential answers are
missed. In this respect, a high recall entity linking
system, e.g., linking to wikipedia entities (Ratinov
et al, 2011), is required to boost the quality of can-
didates considered for answer extraction. Finally,
improving the accuracy of question and focus clas-
sifiers would allow for having more accurate input
representations fed to the learning algorithm.
Answer Extraction. Our answer extraction model
acts as a high recall system, while it suffers from
low precision in extracting answers for many incor-
rect sentences. Improving the precision without sac-
rificing the recall would ease the successive task of
best answer selection, since having less incorrect an-
swer candidates would result in a better final per-
formance. Introducing additional constraints in the
form of semantic tags to allow for better selection of
answer candidates could also improve our system.
Best Answer Selection. We apply a na??ve majority
voting scheme to select a single best answer from
a set of extracted answer candidates. This step has
a dramatic impact on the final performance of the
answer extraction system resulting in a large drop
of recall, i.e., from 82.0 to 70.8 before and after vot-
ing respectively. Hence, a more involved model, i.e.,
465
performing joint answer sentence re-ranking and an-
swer extraction, is required to yield a better perfor-
mance.
7 Related Work
Tree kernel methods have found many applications
for the task of answer reranking which are reported
in (Moschitti, 2008; Moschitti, 2009; Moschitti and
Quarteroni, 2008; Severyn and Moschitti, 2012).
However, their methods lack the use of important
relational information between a question and a can-
didate answer, which is essential to learn accurate
relational patterns. In this respect, a solution based
on enumerating relational links was given in (Zan-
zotto and Moschitti, 2006; Zanzotto et al, 2009) for
the textual entailment task but it is computationally
too expensive for the large dataset of QA. A few so-
lutions to overcome computational issues were sug-
gested in (Zanzotto et al, 2010).
In contrast, this paper relies on structures directly
encoding the output of question and focus classifiers
to connect focus word and good candidate answer
keywords (represented by NEs) of the answer pas-
sage. This provides more effective relational infor-
mation, which allows our model to significantly im-
prove on previous rerankers. Additionally, previous
work on kernel-based approaches does not target an-
swer extraction.
One of the best models for answer sentence selec-
tion has been proposed in (Wang et al, 2007). They
use the paradigm of quasi-synchronous grammar to
model relations between a question and a candidate
answer with syntactic transformations. (Heilman
and Smith, 2010) develop an improved Tree Edit
Distance (TED) model for learning tree transforma-
tions in a q/a pair. They search for a good sequence
of tree edit operations using complex and com-
putationally expensive Tree Kernel-based heuristic.
(Wang and Manning, 2010) develop a probabilistic
model to learn tree-edit operations on dependency
parse trees. They cast the problem into the frame-
work of structured output learning with latent vari-
ables. The model of (Yao et al, 2013) has reported
an improvement over the Wang?s et al (2007) sys-
tem. It applies linear chain CRFs with features de-
rived from TED and WordNet to automatically learn
associations between questions and candidate an-
swers.
Different from previous approaches that use tree-
edit information derived from syntactic trees, our
kernel-based learning approach also use tree struc-
tures but with rather different learning methods, i.e.,
SVMs and structural kernels, to automatically ex-
tract salient syntactic patterns relating questions and
answers. In (Severyn et al, 2013c), we have shown
that such relational structures encoding input text
pairs can be directly used within the kernel learning
framework to build state-of-the-art models for pre-
dicting semantic textual similarity. Furthermore, se-
mantically enriched relational structures, where au-
tomatic have been previously explored for answer
passage reranking in (Severyn et al, 2013b; Sev-
eryn et al, 2013a). This paper demonstrates that this
model also works for building a reranker on the sen-
tence level, and extends the previous work by apply-
ing the idea of automatic feature engineering with
tree kernels to answer extraction.
8 Conclusions
Our paper demonstrates the effectiveness of han-
dling the input structures representing QA pairs di-
rectly vs. using explicit feature vector representa-
tions, which typically require substantial feature en-
gineering effort. Our approach relies on a kernel-
based learning framework, where structural kernels,
e.g., tree kernels, are used to handle automatic fea-
ture engineering. It is enough to specify the desired
type of structures, e.g., shallow, constituency, de-
pendency trees, representing question and its can-
didate answer sentences and let the kernel learning
framework learn to use discriminative tree fragments
for the target task.
An important feature of our approach is that it
can effectively combine together different types of
syntactic and semantic information, also generated
by additional automatic classifiers, e.g., focus and
question classifiers. We augment the basic struc-
tures with additional relational and semantic infor-
mation by introducing special tag markers into the
tree nodes. Using the structures directly in the ker-
nel learning framework makes it easy to integrate
additional relational constraints and semantic infor-
mation directly in the structures.
The comparison with previous work on a public
466
benchmark from TREC suggests that our approach
is very promising as we can improve the state of the
art in both answer selection and extraction by a large
margin (up to 22% of relative improvement in F1 for
answer extraction). Our approach makes it relatively
easy to integrate other sources of semantic informa-
tion, among which the use of Linked Open Data can
be the most promising to enrich the structural repre-
sentation of q/a pairs.
To achieve state-of-the-art results in answer sen-
tence selection and answer extraction, it is sufficient
to provide our model with a suitable tree structure
encoding relevant syntactic information, e.g., using
shallow, constituency or dependency formalisms.
Moreover, additional semantic and relational infor-
mation can be easily plugged in by marking tree
nodes with special tags. We believe this approach
greatly eases the task of tedious feature engineering
that will find its applications well beyond QA tasks.
Acknowledgements
This research is partially supported by the EU?s
7th Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and an Open Collaborative Re-
search (OCR) award from IBM Research. The first
author is supported by the Google Europe Fellow-
ship 2013 award in Machine Learning.
References
Razvan Bunescu and Yunfeng Huang. 2010. Towards a
general model of answer typing: Question focus iden-
tification. In CICLing.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete Structures, and the Voted Perceptron. In ACL.
Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Identification of the question fo-
cus: Combining syntactic analysis and ontology-based
lookup through the user interaction. In LREC.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In NAACL.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In COLING.
A. Moschitti and S. Quarteroni. 2008. Kernels on Lin-
guistic Structures for Answer Extraction. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In CIKM.
Alessandro Moschitti. 2009. Syntactic and semantic ker-
nels for short text pair categorization. In EACL.
Christopher Pinchak. 2006. A probabilistic answer type
model. In In EACL.
John M. Prager. 2006. Open-domain question-
answering. Foundations and Trends in Information
Retrieval, 1(2):91?231.
Silvia Quarteroni, Vincenzo Guerrisi, and Pietro La
Torre. 2012. Evaluating multi-focus natural language
queries over data services. In LREC.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In SIGIR.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from classifiers
for passage reranking. In CIKM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In CoNLL.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013c. Learning semantic textual similar-
ity with structural representations. In ACL.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In ACL.
Mengqiu Wang, Noah A. Smith, and Teruko Mitaura.
2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as se-
quence tagging with tree edit distance. In NAACL.
F. M. Zanzotto and A. Moschitti. 2006. Automatic
Learning of Textual Entailments with Cross-Pair Sim-
ilarities. In COLING.
F. M. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A Machine Learning Approach to Recognizing
Textual Entailment. Natural Language Engineering,
Volume 15 Issue 4, October 2009:551?582.
F. M. Zanzotto, L. Dell?Arciprete, and A. Moschitti.
2010. Efficient graph kernels for textual entail-
ment recognition. FUNDAMENTA INFORMATICAE,
2010.
467
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214?220,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning to Differentiate Better from Worse Translations
Francisco Guzm
?
an Shafiq Joty Llu??s M
`
arquez
Alessandro Moschitti Preslav Nakov Massimo Nicosia
ALT Research Group
Qatar Computing Research Institute ? Qatar Foundation
{fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa
Abstract
We present a pairwise learning-to-rank
approach to machine translation evalua-
tion that learns to differentiate better from
worse translations in the context of a given
reference. We integrate several layers
of linguistic information encapsulated in
tree-based structures, making use of both
the reference and the system output simul-
taneously, thus bringing our ranking closer
to how humans evaluate translations. Most
importantly, instead of deciding upfront
which types of features are important, we
use the learning framework of preference
re-ranking kernels to learn the features au-
tomatically. The evaluation results show
that learning in the proposed framework
yields better correlation with humans than
computing the direct similarity over the
same type of structures. Also, we show
our structural kernel learning (SKL) can
be a general framework for MT evaluation,
in which syntactic and semantic informa-
tion can be naturally incorporated.
1 Introduction
We have seen in recent years fast improvement
in the overall quality of machine translation (MT)
systems. This was only possible because of the
use of automatic metrics for MT evaluation, such
as BLEU (Papineni et al., 2002), which is the de-
facto standard; and more recently: TER (Snover et
al., 2006) and METEOR (Lavie and Denkowski,
2009), among other emerging MT evaluation met-
rics. These automatic metrics provide fast and in-
expensive means to compare the output of differ-
ent MT systems, without the need to ask for hu-
man judgments each time the MT system has been
changed.
As a result, this has enabled rapid develop-
ment in the field of statistical machine translation
(SMT), by allowing to train and tune systems as
well as to track progress in a way that highly cor-
relates with human judgments.
Today, MT evaluation is an active field of re-
search, and modern metrics perform analysis at
various levels, e.g., lexical (Papineni et al., 2002;
Snover et al., 2006), including synonymy and
paraphrasing (Lavie and Denkowski, 2009); syn-
tactic (Gim?enez and M`arquez, 2007; Popovi?c
and Ney, 2007; Liu and Gildea, 2005); semantic
(Gim?enez and M`arquez, 2007; Lo et al., 2012);
and discourse (Comelles et al., 2010; Wong and
Kit, 2012; Guzm?an et al., 2014; Joty et al., 2014).
Automatic MT evaluation metrics compare the
output of a system to one or more human ref-
erences in order to produce a similarity score.
The quality of such a metric is typically judged
in terms of correlation of the scores it produces
with scores given by human judges. As a result,
some evaluation metrics have been trained to re-
produce the scores assigned by humans as closely
as possible (Albrecht and Hwa, 2008). Unfortu-
nately, humans have a hard time assigning an ab-
solute score to a translation. Hence, direct hu-
man evaluation scores such as adequacy and flu-
ency, which were widely used in the past, are
now discontinued in favor of ranking-based eval-
uations, where judges are asked to rank the out-
put of 2 to 5 systems instead. It has been shown
that using such ranking-based assessments yields
much higher inter-annotator agreement (Callison-
Burch et al., 2007).
While evaluation metrics still produce numeri-
cal scores, in part because MT evaluation shared
tasks at NIST and WMT ask for it, there has also
been work on a ranking formulation of the MT
evaluation task for a given set of outputs. This
was shown to yield higher correlation with human
judgments (Duh, 2008; Song and Cohn, 2011).
214
Learning automatic metrics in a pairwise set-
ting, i.e., learning to distinguish between two al-
ternative translations and to decide which of the
two is better (which is arguably one of the easiest
ways to produce a ranking), emulates closely how
human judges perform evaluation assessments in
reality. Instead of learning a similarity function
between a translation and the reference, they learn
how to differentiate a better from a worse trans-
lation given a corresponding reference. While the
pairwise setting does not provide an absolute qual-
ity scoring metric, it is useful for most evaluation
and MT development scenarios.
In this paper, we propose a pairwise learning
setting similar to that of Duh (2008), but we extend
it to a new level, both in terms of feature represen-
tation and learning framework. First, we integrate
several layers of linguistic information encapsu-
lated in tree-based structures; Duh (2008) only
used lexical and POS matches as features. Second,
we use information about both the reference and
two alternative translations simultaneously, thus
bringing our ranking closer to how humans rank
translations. Finally, instead of deciding upfront
which types of features between hypotheses and
references are important, we use a our structural
kernel learning (SKL) framework to generate and
select them automatically.
The structural kernel learning (SKL) framework
we propose consists in: (i) designing a struc-
tural representation, e.g., using syntactic and dis-
course trees of translation hypotheses and a refer-
ences; and (ii) applying structural kernels (Mos-
chitti, 2006; Moschitti, 2008), to such representa-
tions in order to automatically inject structural fea-
tures in the preference re-ranking algorithm. We
use this method with translation-reference pairs
to directly learn the features themselves, instead
of learning the importance of a predetermined set
of features. A similar learning framework has
been proven to be effective for question answer-
ing (Moschitti et al., 2007), and textual entailment
recognition (Zanzotto and Moschitti, 2006).
Our goals are twofold: (i) in the short term, to
demonstrate that structural kernel learning is suit-
able for this task, and can effectively learn to rank
hypotheses at the segment-level; and (ii) in the
long term, to show that this approach provides a
unified framework that allows to integrate several
layers of linguistic analysis and information and to
improve over the state-of-the-art.
Below we report the results of some initial ex-
periments using syntactic and discourse structures.
We show that learning in the proposed framework
yields better correlation with humans than apply-
ing the traditional translation?reference similarity
metrics using the same type of structures. We
also show that the contributions of syntax and dis-
course information are cumulative. Finally, de-
spite the limited information we use, we achieve
correlation at the segment level that outperforms
BLEU and other metrics at WMT12, e.g., our met-
ric would have been ranked higher in terms of cor-
relation with human judgments compared to TER,
NIST, and BLEU in the WMT12 Metrics shared
task (Callison-Burch et al., 2012).
2 Kernel-based Learning from Linguistic
Structures
In our pairwise setting, each sentence s in
the source language is represented by a tuple
?t
1
, t
2
, r?, where t
1
and t
2
are two alternative
translations and r is a reference translation. Our
goal is to develop a classifier of such tuples that
decides whether t
1
is a better translation than t
2
given the reference r.
Engineering features for deciding whether t
1
is
a better translation than t
2
is a difficult task. Thus,
we rely on the automatic feature extraction en-
abled by the SKL framework, and our task is re-
duced to choosing: (i) a meaningful structural rep-
resentation for ?t
1
, t
2
, r?, and (ii) a feature func-
tion ?
mt
that maps such structures to substruc-
tures, i.e., our feature space. Since the design
of ?
mt
is complex, we use tree kernels applied
to two simpler structural mappings ?
M
(t
1
, r) and
?
M
(t
2
, r). The latter generate the tree representa-
tions for the translation-reference pairs (t
1
, r) and
(t
2
, r). The next section shows such mappings.
2.1 Representations
To represent a translation-reference pair (t, r), we
adopt shallow syntactic trees combined with RST-
style discourse trees. Shallow trees have been
successfully used for question answering (Severyn
and Moschitti, 2012) and semantic textual sim-
ilarity (Severyn et al., 2013b); while discourse
information has proved useful in MT evaluation
(Guzm?an et al., 2014; Joty et al., 2014). Com-
bined shallow syntax and discourse trees worked
well for concept segmentation and labeling (Saleh
et al., 2014a).
215
DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
VP NP-REL NP VP-REL o-REL o-REL
RB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
not to give them the time to think . "
VP NP-REL NP VP-REL o-REL o-REL
TO-REL `` VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-REL
to " give them no time to think . "
a) Hypothesis
b) Reference DIS:ELABORATION
EDU:NUCLEUS EDU:SATELLITE-REL
Bag-of-words relations 
rela
tion
 pro
pag
atio
n di
rect
ion
Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS.
Figure 1 shows two example trees combining
discourse, shallow syntax and POS: one for a
translation hypothesis (top) and the other one for
the reference (bottom). To build such structures,
we used the Stanford POS tagger (Toutanova et
al., 2003), the Illinois chunker (Punyakanok and
Roth, 2001), and the discourse parser
1
of (Joty et
al., 2012; Joty et al., 2013).
The lexical items constitute the leaves of the
tree. The words are connected to their respec-
tive POS tags, which are in turn grouped into
chunks. Then, the chunks are grouped into el-
ementary discourse units (EDU), to which the
nuclearity status is attached (i.e., NUCLEUS or
SATELLITE). Finally, EDUs and higher-order dis-
course units are connected by discourse relations
(e.g., DIS:ELABORATION).
2.2 Kernels-based modeling
In the SKL framework, the learning objects are
pairs of translations ?t
1
, t
2
?. Our objective is to
automatically learn which pair features are impor-
tant, independently of the source sentence. We
achieve this by using kernel machines (KMs) over
two learning objects ?t
1
, t
2
?, ?t
?
1
, t
?
2
?, along with
an explicit and structural representation of the
pairs (see Fig. 1).
1
The discourse parser can be downloaded from
http://alt.qcri.org/tools/
More specifically, KMs carry out learning using
the scalar product
K
mt
(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) = ?
mt
(t
1
, t
2
) ??
mt
(t
?
1
, t
?
2
),
where ?
mt
maps pairs into the feature space.
Considering that our task is to decide whether
t
1
is better than t
2
, we can conveniently rep-
resent the vector for the pair in terms of the
difference between the two translation vectors,
i.e., ?
mt
(t
1
, t
2
) = ?
K
(t
1
) ? ?
K
(t
2
). We can
approximate K
mt
with a preference kernel PK to
compute this difference in the kernel space K:
PK(?t
1
, t
2
?, ?t
?
1
, t
?
2
?) (1)
= K(t
1
)? ?
K
(t
2
)) ? (?
K
(t
?
1
)? ?
K
(t
?
2
))
= K(t
1
, t
?
1
) +K(t
2
, t
?
2
)?K(t
1
, t
?
2
)?K(t
2
, t
?
1
)
The advantage of this is that now K(t
i
, t
?
j
) =
?
K
(t
i
) ? ?
K
(t
?
j
) is defined between two transla-
tions only, and not between two pairs of transla-
tions. This simplification enables us to map trans-
lations into simple trees, e.g., those in Figure 1,
and then to apply them tree kernels, e.g., the Par-
tial Tree Kernel (Moschitti, 2006), which carry out
a scalar product in the subtree space.
We can further enrich the representation ?
K
, if
we consider all the information available to the
human judges when they are ranking translations.
That is, the two alternative translations along with
their corresponding reference.
216
In particular, let r and r
?
be the references for
the pairs ?t
1
, t
2
? and ?t
?
1
, t
?
2
?, we can redefine all
the members of Eq. 1, e.g., K(t
1
, t
?
1
) becomes
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
, r), ?
M
(t
?
1
, r
?
))
+ PTK(?
M
(r, t
1
), ?
M
(r
?
, t
?
1
)),
where ?
M
maps a pair of texts to a single tree.
There are several options to produce the bitext-
to-tree mapping for ?
M
. A simple approach is
to only use the tree corresponding to the first ar-
gument of ?
M
. This leads to the basic model
K(?t
1
, r?, ?t
?
1
, r
?
?) = PTK(?
M
(t
1
), ?
M
(t
?
1
)) +
PTK(?
M
(r), ?
M
(r
?
)), i.e., the sum of two tree
kernels applied to the trees constructed by ?
M
(we
previously informally mentioned it).
However, this simple mapping may be ineffec-
tive since the trees within a pair, e.g., (t
1
, r), are
treated independently, and no meaningful features
connecting t
1
and r can be derived from their
tree fragments. Therefore, we model ?
M
(r, t
1
) by
using word-matching relations between t
1
and r,
such that connections between words and con-
stituents of the two trees are established using
position-independent word matching. For exam-
ple, in Figure 1, the thin dashed arrows show the
links connecting the matching words between t
1
and r. The propagation of these relations works
from the bottom up. Thus, if all children in a con-
stituent have a link, their parent is also linked.
The use of such connections is essential as it en-
ables the comparison of the structural properties
and relations between two translation-reference
pairs. For example, the tree fragment [ELABORA-
TION [SATELLITE]] from the translation is con-
nected to [ELABORATION [SATELLITE]] in the
reference, indicating a link between two entire dis-
course units (drawn with a thicker arrow), and pro-
viding some reliability to the translation
2
.
Note that the use of connections yields a graph
representation instead of a tree. This is problem-
atic as effective models for graph kernels, which
would be a natural fit to this problem, are not cur-
rently available for exploiting linguistic informa-
tion. Thus, we simply use K, as defined above,
where the mapping ?
M
(t
1
, r) only produces a tree
for t
1
annotated with the marker REL represent-
ing the connections to r. This marker is placed on
all node labels of the tree generated from t
1
that
match labels from the tree generated from r.
2
Note that a non-pairwise model, i.e., K(t
1
, r), could
also be used to match the structural information above, but
it would not learn to compare it to a second pair (t
2
, r).
In other words, we only consider the trees en-
riched by markers separately, and ignore the edges
connecting both trees.
3 Experiments and Discussion
We experimented with datasets of segment-level
human rankings of system outputs from the
WMT11 and the WMT12 Metrics shared tasks
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012): we used the WMT11 dataset for training
and the WMT12 dataset for testing. We focused
on translating into English only, for which the
datasets can be split by source language: Czech
(cs), German (de), Spanish (es), and French (fr).
There were about 10,000 non-tied human judg-
ments per language pair per dataset. We scored
our pairwise system predictions with respect to
the WMT12 human judgments using the Kendall?s
Tau (? ), which was official at WMT12.
Table 1 presents the ? scores for all metric vari-
ants introduced in this paper: for the individual
language pairs and overall. The left-hand side of
the table shows the results when using as sim-
ilarity the direct kernel calculation between the
corresponding structures of the candidate transla-
tion and the reference
3
, e.g., as in (Guzm?an et al.,
2014; Joty et al., 2014). The right-hand side con-
tains the results for structured kernel learning.
We can make the following observations:
(i) The overall results for all SKL-trained metrics
are higher than the ones when applying direct sim-
ilarity, showing that learning tree structures is bet-
ter than just calculating similarity.
(ii) Regarding the linguistic representation, we see
that, when learning tree structures, syntactic and
discourse-based trees yield similar improvements
with a slight advantage for the former. More in-
terestingly, when both structures are put together
in a combined tree, the improvement is cumula-
tive and yields the best results by a sizable margin.
This provides positive evidence towards our goal
of a unified tree-based representation with multi-
ple layers of linguistic information.
(iii) Comparing to the best evaluation metrics
that participated in the WMT12 Metrics shared
task, we find that our approach is competitive and
would have been ranked among the top 3 partici-
pants.
3
Applying tree kernels between the members of a pair to
generate one feature (for each different kernel function) has
become a standard practice in text similarity tasks (Severyn et
al., 2013b) and in question answering (Severyn et al., 2013a).
217
Similarity Structured Kernel Learning
Structure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all
1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.198
2 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.184
3 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.183
4 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231
Table 1: Kendall?s (? ) correlation with human judgements on WMT12 for each language pair.
Furthermore, our result (0.237) is ahead of the
correlation obtained by popular metrics such as
TER (0.217), NIST (0.214) and BLEU (0.185) at
WMT12. This is very encouraging and shows the
potential of our new proposal.
In this paper, we have presented only the first
exploratory results. Our approach can be easily
extended with richer linguistic structures and fur-
ther combined with some of the already existing
strong evaluation metrics.
Testing
Train cs-en de-en es-en fr-en all
1 cs-en 0.210 0.204 0.217 0.204 0.209
2 de-en 0.196 0.251 0.203 0.202 0.213
3 es-en 0.218 0.204 0.240 0.223 0.221
4 fr-en 0.203 0.218 0.224 0.223 0.217
5 all 0.231 0.258 0.226 0.232 0.237
Table 2: Kendall?s (? ) on WMT12 for cross-
language training with DIS+SYN.
Note that the results in Table 1 were for train-
ing on WMT11 and testing on WMT12 for each
language pair in isolation. Next, we study the im-
pact of the choice of training language pair. Ta-
ble 2 shows cross-language evaluation results for
DIS+SYN: lines 1-4 show results when training on
WMT11 for one language pair, and then testing for
each language pair of WMT12.
We can see that the overall differences in perfor-
mance (see the last column: all) when training on
different source languages are rather small, rang-
ing from 0.209 to 0.221, which suggests that our
approach is quite independent of the source lan-
guage used for training. Still, looking at individ-
ual test languages, we can see that for de-en and
es-en, it is best to train on the same language; this
also holds for fr-en, but there it is equally good
to train on es-en. Interestingly, training on es-en
improves a bit for cs-en.
These somewhat mixed results have motivated
us to try tuning on the full WMT11 dataset; as line
5 shows, this yielded improvements for all lan-
guage pairs except for es-en. Comparing to line
4 in Table 1, we see that the overall Tau improved
from 0.231 to 0.237.
4 Conclusions and Future Work
We have presented a pairwise learning-to-rank ap-
proach to MT evaluation, which learns to differen-
tiate good from bad translations in the context of
a given reference. We have integrated several lay-
ers of linguistic information (lexical, syntactic and
discourse) in tree-based structures, and we have
used the structured kernel learning to identify rel-
evant features and learn pairwise rankers.
The evaluation results have shown that learning
in the proposed SKL framework is possible, yield-
ing better correlation (Kendall?s ? ) with human
judgments than computing the direct kernel sim-
ilarity between translation and reference, over the
same type of structures. We have also shown that
the contributions of syntax and discourse informa-
tion are cumulative, indicating that this learning
framework can be appropriate for the combination
of different sources of information. Finally, de-
spite the limited information we used, we achieved
better correlation at the segment level than BLEU
and other metrics in the WMT12 Metrics task.
In the future, we plan to work towards our long-
term goal, i.e., including more linguistic informa-
tion in the SKL framework and showing that this
can help. This would also include more semantic
information, e.g., in the form of Brown clusters or
using semantic similarity between the words com-
posing the structure calculated with latent seman-
tic analysis (Saleh et al., 2014b).
We further want to show that the proposed
framework is flexible and can include information
in the form of quality scores predicted by other
evaluation metrics, for which a vector of features
would be combined with the structured kernel.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
218
References
Joshua Albrecht and Rebecca Hwa. 2008. Regression
for machine translation evaluation at the sentence
level. Machine Translation, 22(1-2):1?27.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, WMT ?07, pages 136?158,
Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64, Edin-
burgh, Scotland, UK.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
?12, pages 10?51, Montr?eal, Canada.
Elisabet Comelles, Jes?us Gim?enez, Llu??s M`arquez,
Irene Castell?on, and Victoria Arranz. 2010.
Document-level automatic MT evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 333?
338, Uppsala, Sweden.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the Third
Workshop on Statistical Machine Translation, WMT
?08, pages 191?194, Columbus, Ohio, USA.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, WMT ?07,
pages 256?264, Prague, Czech Republic.
Francisco Guzm?an, Shafiq Joty, Llu??s M`arquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL ?14, pages 687?
698, Baltimore, Maryland, USA.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 904?915, Jeju Island, Korea.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?13, pages 486?496, Sofia,
Bulgaria.
Shafiq Joty, Francisco Guzm?an, Llu??s M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using discourse
structure for machine translation evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT ?14, pages 402?408, Balti-
more, Maryland, USA.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25?32, Ann Ar-
bor, Michigan, USA.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic MT evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, WMT ?12, pages 243?252,
Montr?eal, Canada.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion answer classification. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, ACL ?07, pages 776?783, Prague,
Czech Republic.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of 17th European Conference on Ma-
chine Learning and the 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases, ECML/PKDD ?06, pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syn-
tax and semantics for relational text categorization.
In Proceedings of the 17th ACM Conference on In-
formation and Knowledge Management, CIKM ?08,
pages 253?262, Napa Valley, California, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meting of the Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Philadelphia, Pennsylvania, USA.
Maja Popovi?c and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
WMT ?07, pages 48?55, Prague, Czech Republic.
Vasin Punyakanok and Dan Roth. 2001. The use of
classifiers in sequential inference. In Advances in
Neural Information Processing Systems 14, NIPS
?01, pages 995?1001, Vancouver, Canada.
219
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014a. A study of using syntactic and se-
mantic structures for concept segmentation and la-
beling. In Proceedings of the 25th International
Conference on Computational Linguistics, COLING
?14, pages 193?202, Dublin, Ireland.
Iman Saleh, Alessandro Moschitti, Preslav Nakov,
Llu??s M`arquez, and Shafiq Joty. 2014b. Semantic
kernels for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?14, Doha, Qatar.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?12,
pages 741?750, Portland, Oregon, USA.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?13, pages 75?83, Sofia,
Bulgaria.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning semantic textual sim-
ilarity with structural representations. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), ACL ?13, pages 714?718, Sofia, Bulgaria.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ?06, Cambridge, Massachusetts, USA.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence-level MT
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, WMT ?11, pages
123?129, Edinburgh, Scotland, UK.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, HLT-NAACL ?03, pages 173?180, Ed-
monton, Canada.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1060?1068, Jeju Island, Korea.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, COLING-ACL
?06, pages 401?408, Sydney, Australia.
220
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 436?442,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semantic Kernels for Semantic Parsing
Iman Saleh
Faculty of Computers and Information
Cairo University
iman.saleh@fci-cu.edu.eg
Alessandro Moschitti, Preslav Nakov,
Llu??s M
`
arquez, Shafiq Joty
ALT Research Group
Qatar Computing Research Institute
{amoschitti,pnakov,lmarquez,sjoty}@qf.org.qa
Abstract
We present an empirical study on the use
of semantic information for Concept Seg-
mentation and Labeling (CSL), which is
an important step for semantic parsing.
We represent the alternative analyses out-
put by a state-of-the-art CSL parser with
tree structures, which we rerank with a
classifier trained on two types of seman-
tic tree kernels: one processing structures
built with words, concepts and Brown
clusters, and another one using semantic
similarity among the words composing the
structure. The results on a corpus from the
restaurant domain show that our semantic
kernels exploiting similarity measures out-
perform state-of-the-art rerankers.
1 Introduction
Spoken Language Understanding aims to inter-
pret user utterances and to convert them to logical
forms or, equivalently, to database queries, which
can then be used to satisfy the user?s information
needs. This process is known as Concept Segmen-
tation and Labeling (CSL), also called semantic
parsing in the speech community: it maps utter-
ances into meaning representations based on se-
mantic constituents. The latter are basically word
sequences, often referred to as concepts, attributes
or semantic tags. CSL makes it easy to convert
spoken questions such as ?cheap lebanese restau-
rants in doha with take out? into database queries.
First, a language-specific semantic parser tok-
enizes, segments and labels the question:
[
Price
cheap] [
Cuisine
lebanese] [
Other
restaurants in]
[
City
doha] [
Other
with] [
Amenity
take out]
Then, label-specific normalizers are applied to
the segments, with the option to possibly relabel
mislabeled segments:
[
Price
low] [
Cuisine
lebanese] [
City
doha] [
Amenity
carry out]
Finally, a database query is formed from the list
of labels and values, and is then executed against
the database, e.g., MongoDB; a backoff mecha-
nism may be used if the query has not succeeded.
{$and [{cuisine:"lebanese"},{city:"doha"},
{price:"low"},{amenity:"carry out"}]}
The state-of-the-art of CSL is represented by
conditional models for sequence labeling such as
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) trained with simple morphological and
lexical features. The basic CRF model was im-
proved by means of reranking (Moschitti et al.,
2006; Dinarelli et al., 2012) using structural ker-
nels (Moschitti, 2006). Although these meth-
ods exploited sentence structure, they did not use
syntax at all. More recently, we applied shal-
low syntactic structures and discourse parsing with
slightly better results (Saleh et al., 2014). How-
ever, the most obvious models for semantic pars-
ing, i.e., rerankers based on semantic structural
kernels (Bloehdorn and Moschitti, 2007b), had not
been applied to semantic structures yet.
In this paper, we study the impact of semantic
information conveyed by Brown Clusters (BCs)
(Brown et al., 1992) and semantic similarity, while
also combining them with innovative features. We
use reranking, similarly to (Saleh et al., 2014),
to select the best hypothesis annotated with con-
cepts predicted by a local model. The competing
hypotheses are represented as innovative trees en-
riched with the semantic concepts and BC labels.
The trees can capture dependencies between sen-
tence constituents, concepts and BCs. However,
extracting explicit features from them is rather
difficult as their number is exponentially large.
Thus, we rely on (i) Support Vector Machines
(Joachims, 1999) to train the reranking functions
and on (ii) structural kernels (Moschitti, 2010;
Moschitti, 2012; Moschitti, 2013) to automatically
encode tree fragments that represent syntactic and
semantic dependencies from words and concepts.
436
(a) Semantic Kernel Structure (SKS)
(b) SKS with Brown Clusters
Figure 1: CSL structures: standard and with Brown Clusters.
We further apply a semantic kernel (SK),
namely the Smoothed Partial Tree Kernel (Croce
et al., 2011), which uses the lexical similarity be-
tween the tree nodes, while computing the sub-
structure space. This is the first time that SKs are
applied to reranking hypotheses. This (i) makes
the global sentence structure along with concepts
available to the learning algorithm, and (ii) enables
computing the similarity between lexicals in syn-
tactic patterns that are enriched by concepts.
We tested our models on the Restaurant do-
main. Our results show that: (i) The basic CRF
parser, which uses semi-Markov CRF, or semi-
CRF (Sarawagi and Cohen, 2004), is already very
accurate; it achieves F
1
scores over 83%, mak-
ing any further improvement very hard. (ii) The
upper-bound performance of the reranker is very
high as well, i.e., the correct annotation is gen-
erated in the list of the first 100 hypotheses in
98.72% of the cases. (iii) SKs significantly im-
prove over the semi-CRF baseline and our pre-
vious state-of-the-art reranker exploiting shallow
syntactic patterns (Saleh et al., 2014), as shown
by extensive comparisons using several systems.
(iv) Making BCs effective requires a deeper study.
2 Related Work
One of the early approaches to CSL was that
of Pieraccini et al. (1991), where the word se-
quences and concepts were modeled using Hid-
den Markov Models (HMMs) as observations and
hidden states, respectively. Generative models
were exploited by Seneff (1989) and Miller et
al. (1994), who used stochastic grammars for
CSL. Other discriminative models followed such
preliminary work, e.g., (Rubinstein and Hastie,
1997; Santaf?e et al., 2007; Raymond and Riccardi,
2007). CRF-based models are considered to be the
state of the art in CSL (De Mori et al., 2008).
Another relevant line of research are the seman-
tic kernels, i.e., kernels that use lexical similarity
between features. One of the first that applyed
LSA was (Cristianini et al., 2002), whereas (Bloe-
hdorn et al., 2006; Basili et al., 2006) used Word-
Net. Semantic structural kernels of the type we
use in this paper were first introduced in (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b). The most advanced model based on
tree kernels, which we also use in this paper, is the
Smoothed PTK (Croce et al., 2011).
3 Reranking for CSL
Reranking is applied to a list of N annotation hy-
potheses, which are generated and sorted by the
probability to be globally correct as estimated us-
ing local classifiers or global classifiers that only
use local features. Then, a reranker, typically a
meta-classifier, tries to select the best hypothe-
sis from the list. The reranker can exploit global
information, and specifically, the dependencies
between the different concepts, which are made
available by the local model. We use semi-CRFs
for the local model as they yield the highest ac-
curacy in CSL (when using a single model) and
preference reranking for the global reranker.
3.1 Preference Reranking (PR)
PR uses a classifier C, which takes a pair of hy-
potheses ?H
i
, H
j
? and decides whether H
i
is bet-
ter than H
j
. Given a training question Q, posi-
tive and negative examples are built for training
the classifier. Let H
1
be the hypothesis with the
lowest error rate with respect to the gold standard
among all hypotheses generated for question Q.
We adopt the following approach for example gen-
eration: the pairs ?H
1
, H
i
? (i = 2, 3, . . . , N ) are
positive examples, while ?H
i
, H
1
? are considered
negative.
437
At testing time, given a new question Q
?
, C clas-
sifies all pairs ?H
i
, H
j
? generated from the anno-
tation hypotheses of Q
?
: a positive classification is
a vote for H
i
, otherwise the vote is for H
j
, where
the classifier score can be used as a weighted vote.
H
k
are then ranked according to the number (sum)
of the votes (weighted by score) they receive.
We build our reranker with SVMs using the
following kernel: K(?H
1
, H
2
?, ?H
?
1
, H
?
2
?) =
?(?H
1
, H
2
?) ? ?(?H
?
1
, H
?
2
?) ,
(
?(H
1
) ?
?(H
2
)
)
?
(
?(H
?
1
) ? ?(H
?
2
)
)
= ?(H
1
)?(H
?
1
) +
?(H
2
)?(H
?
2
) ? ?(H
1
)?(H
?
2
) ? ?(H
2
)?(H
?
1
) =
S(H
1
, H
?
1
) + S(H
2
, H
?
2
) ? S(H
1
, H
?
2
) ?
S(H
2
, H
?
1
). We consider H as a tuple ?T,~v? com-
posed of a tree T and a feature vector ~v. Then, we
define S(H,H
?
) = S
TK
(T, T
?
)+S
v
(~v,~v
?
), where
S
TK
computes one of the tree kernel functions
defined in 3.2 and 3.3; and S
v
is a kernel (see 3.4),
e.g., linear, polynomial, Gaussian, etc.
3.2 Tree kernels (TKs)
TKs measure the similarity between two structures
in terms of the number of substructures they share.
We use two types of tree kernels: (i) Partial Tree
Kernel (PTK), which can be effectively applied
to both constituency and dependency parse trees
(Moschitti, 2006). It generates all possible con-
nected tree fragments, e.g., sibling nodes can be
also separated and can be part of different tree
fragments: a fragment is any possible tree path,
and other tree paths are allowed to depart from its
nodes. Thus, it can generate a very rich feature
space. (ii) The smoothed PTK or semantic kernel
(SK) (Croce et al., 2011), which extends PTK by
allowing soft matching (i.e., via similarity compu-
tation) between nodes associated with different but
related lexical items. The node similarity can be
derived from manually annotated resources, e.g.,
WordNet or Wikipedia, as well as using corpus-
based clustering approaches, e.g., latent semantic
analysis (LSA), as we do in this paper.
3.3 Semantic structures
Tree kernels allow us to compute structural simi-
larities between two trees; thus, we engineered a
special structure for the CSL task. In order to cap-
ture the structural dependencies between the se-
mantic tags,
1
we use a basic tree (see for exam-
ple Figure 1a), where the words of a sentence are
tagged with their semantic tags.
1
They are associated with the following IDs: 0-Other,
1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6-
Hours, 7-Location, and 8-Price.
More specifically, the words in the sentence
constitute the leaves of the tree, which are in
turn connected to the pre-terminals containing
the semantic tags in BIO notation (?B?=begin,
?I?=inside, ?O?=outside). The BIO tags are then
generalized in the upper level, and joined to the
Root node. Additionally, part-of-speech (POS)
tags
2
are added to each word by concatenating
it with the string ?::L?, where L is the first let-
ter of the POS-tags of the words, e.g., along, my
and route, receive i, p and n, which are the first
letters of the POS-tags IN, PRN and NN, respec-
tively. SK applied to the above structure can gen-
erate powerful semantic patterns such as [Root
[4-Cuisine [similar to(stake house)]][7-Loc [simi-
lar to(within a mile)]]], e.g., for correctly labeling
new clauses like Pizza Parlor in three kilometers.
The BC labels, represented as cluster IDs, are sim-
ply added as siblings of words as shown in Fig. 1b.
3.4 Feature Vectors
For the sake of comparison, we also devoted
some effort towards engineering a set of features
to be used in a flat feature-vector representation.
These features can be used in isolation to learn
the reranking function, or in combination with the
kernel-based approach (as a composite kernel us-
ing a linear combination). They belong to the fol-
lowing four categories: (i) CRF-based: these in-
clude the basic features used to train the initial
semi-CRF model; (ii) n-gram based: we collected
3- and 4-grams of the output label sequence at
the level of concepts, with artificial tags inserted
to identify the start (?S?) and end (?E?) of the se-
quence.
3
(iii) Probability-based, computing the
probability of the label sequence as an average of
the probabilities at the word level in the N -best
list; and (iv) DB-based: a single feature encoding
the number of results returned from the database
when constructing a query using the conjunction
of all semantic segments in the hypothesis.
4 Experiments
The experiments aim at investigating the role of
feature vectors, PTK, SK and BCs in reranking.
We first describe the experimental setting and then
we move into the analysis of the results.
2
We use the Stanford tagger (Toutanova et al., 2003).
3
For instance, if the output sequence is Other-Rating-
Other-Amenity the 3-gram patterns would be: S-Other-
Rating, Other-Rating-Other, Rating-Other-Amenity, and
Other-Amenity-E.
438
Train Devel. Test Total
semi-CRF 6,922 739 1,521 9,182
Reranker 7,000 3,695 7,605 39,782
Table 1: Number of instances and pairs used to
train the semi-CRF and rerankers, respectively.
4.1 Experimental setup
Dataset. In our experiments, we used questions
annotated with semantic tags, which were col-
lected through crowdsourcing on Amazon Me-
chanical Turk and made available
4
by McGraw et
al. (2012). We split the dataset into training, de-
velopment and test sets. Table 1 shows the num-
ber of examples and example pairs we used for
the semi-CRF and the reranker, respectively. We
subsequently split the training data randomly into
10 folds. We used cross-validation, i.e., iteratively
training with 9 folds and annotating the remaining
fold, in order to generate the N -best lists of hy-
potheses for the entire training dataset. We com-
puted the 100-best hypotheses for each example.
We then used the development dataset to test and
tune the hyper-parameters of our reranking model.
The results on the development set, which we will
present in Section 4.2 below, were obtained us-
ing semi-CRF and reranking models trained on the
training set.
Data representation. Each hypothesis is repre-
sented by a semantic tree, a feature vector (ex-
plained in Section 3), and two extra features:
(i) the semi-CRF probability of the hypothesis,
and (ii) its reciprocal rank in the N -best list.
Learning algorithm. We used the SVM-Light-
TK
5
to train the reranker with a combination of
tree kernels and feature vectors (Moschitti, 2006;
Joachims, 1999). We used the default parameters
and a linear kernel for the feature vectors. As a
baseline, we picked the best-scoring hypothesis in
the list, i.e., the output by the regular semi-CRF
parser. The setting is exactly the same as that de-
scribed in (Saleh et al., 2014).
Evaluation measure. In all experiments, we used
the harmonic mean of precision and recall (F
1
)
(van Rijsbergen, 1979), computed at the token
level and micro-averaged across the different se-
mantic types.
6
4
http://groups.csail.mit.edu/sls/downloads/restaurant/
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
6
We do not consider ?Other? to be a semantic type; thus,
we did not include it in the F
1
calculation.
N 1 2 5 10 100
F
1
83.03 87.76 92.63 95.23 98.72
Table 2: Oracle F
1
score for N -best lists.
Brown Clusters. Clustering groups of similar
words together provides a way of generalizing
them. In this work, we explore the use of Brown
clusters (Brown et al., 1992) in both feature vec-
tors and tree kernels. The Brown clustering al-
gorithm uses an n-gram class model. It first as-
signs each word to a distinct cluster, and then it
merges different clusters in a bottom-up fashion.
The merge step is done in a way that minimizes the
loss in average mutual information between clus-
ters. The outcome is hierarchical clustering, which
we use in our reranking algorithm. To create the
Brown clusters, we used the Yelp dataset of re-
views.
7
It contains 335,022 reviews about 15,585
businesses; 5,575 of the businesses and 233,839 of
the reviews are restaurant-related. This dataset is
very similar to the dataset of queries about restau-
rants we use in our experiments.
Similarity matrix for SK. We compute the lexi-
cal similarity for SK by applying LSA (Furnas et
al., 1988) to Tripadvisor data. The dataset and the
exact procedure for creating the LSA matrix are
described in (Castellucci et al., 2013; Croce and
Previtali, 2010).
4.2 Results
Oracle accuracy. Table 2 shows the oracle F
1
score for N -best lists of different lengths, i.e., the
F
1
that is achieved by picking the best candidate
in the N -best list for various values of N . Con-
sidering 5-best lists yields an increase in oracle F
1
of almost ten absolute points. Going up to 10-best
lists only adds 2.5 extra F
1
points. The complete
100-best lists add 3.5 extra F
1
points, for a total
of 98.72. This very high value is explained by the
fact that often the total number of different anno-
tations for a given question is smaller than 100. In
our experiments, we will focus on 5-best lists.
Baseline accuracy. We computed F
1
for the semi-
CRF model on both the development and the test
sets, obtaining 83.86 and 83.03, respectively.
Learning Curves. The semantic information in
terms of BCs or semantic similarity derived by
LSA can have a major impact in case of data
scarcity. Therefore, we trained our reranking mod-
els with increasing sizes of training data.
7
http://www.yelp.com/dataset challenge/
439
Development set
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?PTK+BC	 ? PTK+all	 ?PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Test set
74	 ?
76	 ?
78	 ?
80	 ?
82	 ?
84	 ?
86	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?
PTK+BC	 ? PTK+all	 ?
PTK+BC+all	 ? Baseline	 ?
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
0	 ? 1000	 ? 2000	 ? 3000	 ? 4000	 ? 5000	 ? 6000	 ? 7000	 ?
F1-??m
easu
re	 ?
Training	 ?data	 ?size	 ?
PTK	 ? SK	 ?SK+BC	 ? PTK+all	 ?SK+all	 ? SK+BC+all	 ?Baseline	 ?
Figure 2: Learning curves for different reranking models on the development and on the testing sets.
The first two graphs in Fig. 2 show the plots
on the development set whereas the last two are
computed on the test set. The reranking models
reported are Baseline, PTK, PTK+BC, PTK+all
(features), PTK+BC+all, SK, SK+BC, SK+all and
SK+BC+all.
8
We can see that: (i) PTK alone, i.e.,
without semantic information, has the lowest ac-
curacy; (ii) BCs do not improve significantly any
model; (iii) SK almost always achieves the high-
est accuracy; (iv) PTK+all (i.e., the model also us-
ing features) improves on PTK, but its accuracy
is lower than for any model using SK, i.e., us-
ing semantic similarity; and (v) all features pro-
vide an initial boost to SK, but as soon as the data
increases, their impact decreases.
5 Conclusion and Future Work
In summary, the learning curves clearly show the
good generalization ability of SK, which improve
the CRF baseline using little data (?3,000). The
semantic kernel significantly improves over the
semi-CRF baseline and our previous state-of-the-
art reranker exploiting shallow syntactic patterns
(Saleh et al., 2014), which corresponds to PTK+all
in the above comparison.
8
Models are split between 2 plots in order to ease reading.
The improvement falls between 1-2 absolute
percent points. This is remarkable as (i) it corre-
sponds to ?10% relative error reduction, and (ii)
the state-of-the-art baseline system is very difficult
to beat, as confirmed by the low impact of tradi-
tional features and BCs. Although the latter can
generalize over concepts and words, their use is
not straightforward, resulting in no improvement.
In the future, we plan to investigate the use of
semantic similarity from distributional and other
sources (Mihalcea et al., 2006; Pad?o and Lapata,
2007), e.g., Wikipedia (Strube and Ponzetto, 2006;
Mihalcea and Csomai, 2007), Wiktionary (Zesch
et al., 2008), WordNet (Pedersen et al., 2004;
Agirre et al., 2009), FrameNet, VerbNet (Shi and
Mihalcea, 2005), BabelNet (Navigli and Ponzetto,
2010), and LSA, and for different domains.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation. We would like to
thank Danilo Croce, Roberto Basili and Giuseppe
Castellucci for helping and providing us with the
similarity matrix for the semantic kernels.
440
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Boulder, Colorado, June.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica (Slovenia),
30(2):163?172.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Advances in Information Retrieval
- Proceedings of the 29th European Conference on
Information Retrieval (ECIR 2007), pages 307?318,
Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In Proceedings of the 16th ACM Conference on
Information and Knowledge Management (CIKM
2007), pages 861?864, Lisbon, Portugal.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining (ICDM
06), pages 808?812, Hong Kong.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Giuseppe Castellucci, Simone Filice, Danilo Croce,
and Roberto Basili. 2013. UNITOR: Combining
Syntactic and Semantic Kernels for Twitter Senti-
ment Analysis. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
369?374, Atlanta, Georgia, USA.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent Semantic Kernels. Journal
of Intelligent Information Systems, 18(2):127?152.
Danilo Croce and Daniele Previtali. 2010. Mani-
fold learning for the semi-supervised induction of
framenet predicates: An empirical investigation. In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, pages 7?16,
Uppsala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 1034?1046,
Edinburgh, Scotland, UK.
Renato De Mori, Frederic B?echet, Dilek Hakkani-T?ur,
Michael McTear, Giuseppe Riccardi, and Gokhan
Tur. 2008. Spoken Language Understanding. IEEE
Signal Processing Magazine, 25:50?58.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2012. Discriminative reranking for
spoken language understanding. IEEE Transac-
tions on Audio, Speech and Language Processing,
20(2):526?539.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proceedings of the 11th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval (SIGIR ?88),
pages 465?480, New York, USA.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press, Cambridge,
MA, USA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), pages 282?289, Williamstown, MA, USA.
Ian McGraw, Scott Cyphers, Panupong Pasupat,
Jingjing Liu, and Jim Glass. 2012. Automating
crowd-supervised learning for spoken language sys-
tems. In Proceedings of the 13th Annual Conference
of the International Speech Communication Asso-
ciation (INTERSPEECH 2012), pages 2473?2476,
Portland, OR, USA.
Rada Mihalcea and Andras Csomai. 2007. Wikify!
linking documents to encyclopedic knowledge. In
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment (CIKM 2007), pages 233?242, Lisbon, Portu-
gal.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st National Conference on Artificial In-
telligence - Volume 1 (AAAI 2006), pages 775?780,
Boston, MA, USA.
Scott Miller, Richard Schwartz, Robert Bobrow, and
Robert Ingria. 1994. Statistical Language Process-
ing using Hidden Understanding Models. In Pro-
ceedings of the workshop on Human Language Tech-
nology (HLT 1994), pages 278?282, Plainsboro, NJ,
USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
441
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 61?68, New York City, USA.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML 2006), pages 318?329,
Berlin, Germany.
Alessandro Moschitti. 2010. Kernel engineering
for fast and easy design of natural language ap-
plications. In Coling 2010: Kernel Engineering
for Fast and Easy Design of Natural Language
Applications?Tutorial notes, pages 1?91, Beijing,
China.
Alessandro Moschitti. 2012. State-of-the-art kernels
for natural language processing. In Tutorial Ab-
stracts of ACL 2012, page 2, Jeju Island, Korea.
Alessandro Moschitti. 2013. Kernel-based learning to
rank with syntactic and semantic structures. In Tu-
torial abstracts of the 36th Annual ACM SIGIR Con-
ference, page 1128, Dublin, Ireland.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th annual meet-
ing of the association for computational linguistics
(ACL 2010), pages 216?225, Uppsala, Sweden.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the
relatedness of concepts. In HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA.
Roberto Pieraccini, Esther Levin, and Chin-Hui Lee.
1991. Stochastic Representation of Conceptual
Structure in the ATIS Task. In Proceedings of the
Fourth Joint DARPA Speech and Natural Language
Workshop, pages 121?124, Los Altos, CA, USA.
Christian Raymond and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proceedings
of the 8th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2007), pages 1605?1608, Antwerp, Bel-
gium, August.
Y. Dan Rubinstein and Trevor Hastie. 1997. Discrimi-
native vs Informative Learning. In Proceedings of
the Third International Conference on Knowledge
Discovery and Data Mining (KDD-1997), pages 49?
53, Newport Beach, CA, USA.
Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,
Llu??s M`arquez, Alessandro Moschitti, and Preslav
Nakov. 2014. A study of using syntactic and seman-
tic structures for concept segmentation and labeling.
In Proceedings of the 25th International Conference
on Computational Linguistics, COLING ?14, pages
193?202, Dublin, Ireland.
G. Santaf?e, J.A. Lozano, and P. Larra?naga. 2007.
Discriminative vs. Generative Learning of Bayesian
Network Classifiers. In Proceedings of the 9th Euro-
pean Conference on Symbolic and Quantitative Ap-
proaches to Reasoning with Uncertainty (ECSQARU
2007), pages 453?546, Hammamet, Tunisia.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems 17 (NIPS 2004), Vancouver, British
Columbia, Canada.
Stephanie Seneff. 1989. TINA: A Probabilistic Syn-
tactic Parser for Speech Understanding Systems.
In Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing (ICASSP-
89), pages 711?714, Glasgow, UK.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Computational Lin-
guistics and Intelligent Text Processing, pages 100?
111. Springer Berlin Heidelberg.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence (AAAI?06), pages
1419?1424, Boston, Massachusetts, USA.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 173?180, Edmon-
ton, Canada.
Cornelis J. van Rijsbergen. 1979. Information
Retrieval. Butterworth-Heinemann Newton, MA,
USA.
Torsten Zesch, Christof M?uller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In Proceedings of the 23rd National Con-
ference on Artificial Intelligence (AAAI?08), pages
861?866, Chicago, Illinois,USA.
442
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2049?2060,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Discriminative Reranking of Discourse Parses Using Tree Kernels
Shafiq Joty and Alessandro Moschitti
ALT Research Group
Qatar Computing Research Institute
{sjoty,amoschitti}@qf.org.qa
Abstract
In this paper, we present a discrimina-
tive approach for reranking discourse trees
generated by an existing probabilistic dis-
course parser. The reranker relies on tree
kernels (TKs) to capture the global depen-
dencies between discourse units in a tree.
In particular, we design new computa-
tional structures of discourse trees, which
combined with standard TKs, originate
novel discourse TKs. The empirical evalu-
ation shows that our reranker can improve
the state-of-the-art sentence-level parsing
accuracy from 79.77% to 82.15%, a rel-
ative error reduction of 11.8%, which in
turn pushes the state-of-the-art document-
level accuracy from 55.8% to 57.3%.
1 Introduction
Clauses and sentences in a well-written text are
interrelated and exhibit a coherence structure.
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) represents the coherence struc-
ture of a text by a labeled tree, called discourse
tree (DT) as shown in Figure 1. The leaves cor-
respond to contiguous clause-like units called ele-
mentary discourse units (EDUs). Adjacent EDUs
and larger discourse units are hierarchically con-
nected by coherence relations (e.g., ELABORA-
TION, CAUSE). Discourse units connected by a re-
lation are further distinguished depending on their
relative importance: nuclei are the core parts of the
relation while satellites are the supportive ones.
Conventionally, discourse analysis in RST in-
volves two subtasks: (i) discourse segmentation:
breaking the text into a sequence of EDUs, and
(ii) discourse parsing: linking the discourse units
to form a labeled tree. Despite the fact that dis-
course analysis is central to many NLP appli-
cations, the state-of-the-art document-level dis-
course parser (Joty et al., 2013) has an f -score
of only 55.83% using manual discourse segmen-
tation on the RST Discourse Treebank (RST-DT).
Although recent work has proposed rich lin-
guistic features (Feng and Hirst, 2012) and pow-
erful parsing models (Joty et al., 2012), discourse
parsing remains a hard task, partly because these
approaches do not consider global features and
long range structural dependencies between DT
constituents. For example, consider the human-
annotated DT (Figure 1a) and the DT generated by
the discourse parser of Joty et al. (2013) (Figure
1b) for the same text. The parser makes a mistake
in finding the right structure: it considers only e
3
as the text to be attributed to e
2
, where all the text
spans from e
3
to e
6
(linked by CAUSE and ELAB-
ORATION) compose the statement to be attributed.
Such errors occur because existing systems do not
encode long range dependencies between DT con-
stituents such as those between e
3
and e
4?6
.
Reranking models can make the global struc-
tural information available to the system as fol-
lows: first, a base parser produces several DT
hypotheses; and then a classifier exploits the en-
tire information in each hypothesis, e.g., the com-
plete DT with its dependencies, for selecting the
best DT. Designing features capturing such global
properties is however not trivial as it requires the
selection of important DT fragments. This means
selecting subtree patterns from an exponential fea-
ture space. An alternative approach is to implicitly
generate the whole feature space using tree kernels
(TKs) (Collins and Duffy, 2002; Moschitti, 2006).
In this paper, we present reranking models for
discourse parsing based on Support Vector Ma-
chines (SVMs) and TKs. The latter allows us
to represent structured data using the substructure
space thus capturing structural dependencies be-
tween DT constituents, which is essential for ef-
fective discourse parsing. Specifically, we made
the following contributions. First, we extend the
2049
Topic-Comment
Attribution
Cause
Elaboration
Elaboration
e
e
e
e
e e
2
3
4
5 6
1
(a) A human-annotated discourse tree.
Background
Attribution
Cause
Elaboration
Elaboration
e
e e e
e e
2 3 4
5 6
1
(b) A discourse tree generated by Joty et al. (2013).
Figure 1: Example of human-annotated and system-generated discourse trees for the text [what?s more,]
e
1
[he believes]
e
2
[seasonal swings in the auto industry this year aren?t occurring at the same time in the past,]
e
3
[because of production and pric-
ing differences]
e
4
[that are curbing the accuracy of seasonal adjustments]
e
5
] [built into the employment data.]
e
6
Horizontal
lines indicate text segments; satellites are connected to their nuclei by curved arrows.
existing discourse parser
1
(Joty et al., 2013) to
produce a list of k most probable parses for each
input text, with associated probabilities that define
the initial ranking.
Second, we define a set of discourse tree ker-
nels (DISCTK) based on the functional composi-
tion of standard TKs with structures representing
the properties of DTs. DISCTK can be used for
any classification task involving discourse trees.
Third, we use DISCTK to define kernels for
reranking and use them in SVMs. Our rerankers
can exploit the complete DT structure using TKs.
They can ascertain if portions of a DT are compat-
ible, incompatible or simply not likely to coexist,
since each substructure is an exploitable feature.
In other words, problematic DTs are expected to
be ranked lower by our reranker.
Finally, we investigate the potential of our ap-
proach by computing the oracle f -scores for both
document- and sentence-level discourse parsing.
However, as demonstrated later in Section 6, for
document-level parsing, the top k parses often
miss the best parse. For example, the oracle f -
scores for 5- and 20-best document-level parsing
are only 56.91% and 57.65%, respectively. Thus
the scope of improvement for the reranker is rather
narrow at the document level. On the other hand,
the oracle f -score for 5-best sentence-level dis-
course parsing is 88.09%, where the base parser
(i.e., 1-best) has an oracle f -score of 79.77%.
Therefore, in this paper we address the following
two questions: (i) how far can a reranker improve
the parsing accuracy at the sentence level? and
(ii) how far can this improvement, if at all, push
the (combined) document-level parsing accuracy?
To this end, our comparative experiments on
1
Available from http://alt.qcri.org/tools/
RST-DT show that the sentence-level reranker can
improve the f -score of the state-of-the-art from
79.77% to 82.15%, corresponding to a relative
error reduction of 11.8%, which in turn pushes
the state-of-the-art document-level f -score from
55.8% to 57.3%, an error reduction of 3.4%.
In the rest of the paper, after introducing the TK
technology in Section 2, we illustrate our novel
structures, and how they lead to the design of
novel DISCTKs in Section 3. We present the k-
best discourse parser in Section 4. In Section 5, we
describe our reranking approach using DISCTKs.
We report our experiments in Section 6. We briefly
overview the related work in Section 7, and finally,
we summarize our contributions in Section 8.
2 Kernels for Structural Representation
Tree kernels (Collins and Duffy, 2002; Shawe-
Taylor and Cristianini, 2004; Moschitti, 2006) are
a viable alternative for representing arbitrary sub-
tree structures in learning algorithms. Their ba-
sic idea is that kernel-based learning algorithms,
e.g., SVMs or perceptron, only need the scalar
product between the feature vectors representing
the data instances to learn and classify; and kernel
functions compute such scalar products in an effi-
cient way. In the following subsections, we briefly
describe the kernel machines and three types of
tree kernels (TKs), which efficiently compute the
scalar product in the subtree space, where the vec-
tor components are all possible substructures of
the corresponding trees.
2.1 Kernel Machines
Kernel Machines (Cortes and Vapnik, 1995), e.g.,
SVMs, perform binary classification by learning
a hyperplane H(~x) = ~w ? ~x + b = 0, where
2050
Chapter 2. Background 15
a
b
c e
g
)
a
b
c e
g
b
c e
g c e
Figure 2.4: A tree (left) and all of its proper subtrees (right).
a
b
c e
g
)
a
b
c e
g
a
b g
c e
g
b
c e
Figure 2.5: A tree (left) and all of its subset trees (right).
Proper Subtree A proper subtree t
i
comprises node v
i
along with all of its de-
scendants (see figure 2.4 for an example of a tree along with all its proper subtrees).
Subset Tree A subset tree is a subtree for which the following constraint is sat-
isfied: either all of the children of a node belong to the subset tree or none of them.
The reason for adding such a constraint can be understood by considering the fact
that subset trees were defined for measuring the similarity of parse trees in natural
language applications. In that context a node along with all of its children represent
a grammar production. Figure 2.5 gives an example of a tree along with some of its
subset trees.
Figure 2: A tree with its STK subtrees; STK
b
also includes
leaves as features.
~x ? R
n
is the feature vector representation of an
object o ? O to be classified and ~w ? R
n
and
b ? R are parameters learned from the training
data. One can train such machines in the dual
space by rewri ing the model parameter ~w as a l n-
ear combination of training examples, i.e., ~w =
?
i=1..l
y
i
?
i
~x
i
, where y
i
is equal to 1 for positive
exam les and ?1 for negative example , ?
i
? R
+
and ~x
i
?i ? {1, .., l} are the training instances.
Then, we can use the data object o
i
? O directly
in the hyperplane equation considering their map-
ping function ? : O ? R
n
, as follows: H(o) =
?
i=1..l
y
i
?
i
~x
i
?~x+b =
?
i=1..l
y
i
?
i
?(o
i
) ??(o)+
b =
?
i=1..l
y
i
?
i
K(o
i
, o) + b, where the product
K(o
i
, o) = ??(o
i
) ? ?(o)? is the kernel function
(e.g., TK) associated with the mapping ?.
2.2 Tree Kernels
Convolution TKs compute the number of com-
mon tree fragments between two trees T
1
and T
2
without explicitly considering the whole fragment
space. A TK function over T
1
and T
2
is defined as:
TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
),
where N
T
1
and N
T
2
are the sets of the nodes of
T
1
and T
2
, respectively, and ?(n
1
, n
2
) is equal
to the number of common fragments rooted in
the n
1
and n
2
nodes.
2
The computation of ?
function depends on the shape of fragments,
conversely, a different ? determines the richness
of the kernel space and thus different tree kernels.
In the following, we briefly describe two existing
and well-known tree kernels. Please see several
tutorials on kernels (Moschitti, 2013; Moschitti,
2012; Moschitti, 2010) for more details.
3
Syntactic Tree Kernels (STK) produce fragments
such that each of their nodes includes all or none
of its children. Figure 2 shows a tree T and its
three fragments (do not consider the single nodes)
in the STK space on the left and right of the ar-
2
To get a similarity score between 0 and 1, it is
common to apply a normalization in the kernel space,
i.e.
TK(T
1
,T
2
)
?
TK(T
1
,T
1
)?TK(T
2
,T
2
)
.
3
Tutorials notes available at http://disi.unitn.
it/moschitti/
14 Chapter 2. Background
a
b
c e
g
2 3
31
Figure 2.2: A positional Tree. The number over an arc represents the position of
the node with respect to its parent.
a
b
c e
g
)
a
b
c e
g
a
b g
c e
g a
gb
c e
a
b
c e
a
b
e
Figure 2.3: A tree (left) and some of its subtrees (right).
node. The maximum out-degree of a tree is the highest index of all the nodes of the
tree. The out-degree of a node for an ordered tree corresponds to the number of its
children. The depth of a node v
i
with respect to one of its ascendants v
j
is defined
as the number of nodes comprising the path from v
j
to v
i
. When not specified, the
node with respect to the depth is computed, is the root.
A tree can be decomposed in many types of substructures.
Subtree A subtree t is a subset of nodes in the tree T , with corresponding edges,
which forms a tree. A subtree rooted at node v
i
will be indicated with t
i
, while a
subtree rooted at a generic node v will be indicated by t(v). When t is used in a
context where a node is expected, t refers to the root node of the subtree t. The
set of subtrees of a tree will be indicated by N
T
. When clear from the context N
T
may refer to specific type of subtrees. Figure 2.3 gives an example of a tree together
with its subtrees. Various types of subtrees can be defined for a tree T .
Figure 3: A tree with its PTK fragments.
row, respectively. STK(T ,T ) counts the number
of co mon fragments, which in this case is the
umber of subtrees of T , i.e., three. In the figure,
we also show three single nodes, c, e, and g, i.e.,
the leaves of T , which are computed by a vari-
ant of the kernel, that we call STK
b
. The com-
putati nal complexity of STK is O(|N
T
1
||N
T
2
|),
but the average running time tends to be linear
(i.e. O(|N
T
1
| + |N
T
2
|)) for syntactic trees (Mos-
chitti, 2006).
Partial Tree Kernel (PTK) generates a richer set
of tree fragments. Given a target tree T , PTK
can generate any subset of connected nodes of T ,
whose edges are in T . For example, Figure 3
shows a tree with its nine fragments including all
single nodes (i.e., the leaves of T ). PTK is more
general than STK as its fragments can include any
subsequence of children of a target node. The time
complexity of PTK is O(p?
2
|N
T
1
||N
T
2
|), where
p is the largest subsequence of children that one
wants to consider and ? is the maximal out-degree
observed in the two trees. However, the average
running time again tends to be linear for syntactic
trees (Moschitti, 2006).
3 Discourse Tree Kernels (DISCTK)
Engineering features that can capture the depen-
dencies between DT constituents is a difficult task.
In principle, any dependency between words, rela-
tions and structures (see Figure 1) can be an im-
portant feature for discourse parsing. This may
lead to an exponential number of features, which
makes the feature engineering process very hard.
The standard TKs described in the previous sec-
tion serve as a viable option to get useful sub-
tree features automatically. However, the defini-
tion of the input to a TK, i.e., the tree represent-
ing a training instance, is extremely important as
it implicitly affects the subtree space generated by
the TK, where the target learning task is carried
out. This can be shown as follows. Let ?
M
()
be a mapping from linguistic objects o
i
, e.g., a
discourse parse, to a meaningful tree T
i
, and let
?
TK
() be a mapping into a tree kernel space us-
2051
(a) JRN
(b) SRN
Figure 4: DISCTK trees: (a) Joint Relation-Nucleus (JRN), and (b) Split Relation Nucleus (SRN).
ing one of the TKs described in Section 2.2, i.e.,
TK(T
1
, T
2
) = ?
TK
(T
1
) ? ?
TK
(T
2
). If we apply
TK to the objects o
i
transformed by ?
M
(), we
obtain TK(?
M
(o
1
), ?
M
(o
2
)) = ?
TK
(?
M
(o
1
)) ?
?
TK
(?
M
(o
2
))=
(
?
TK
??
M
)
(o
1
)?
(
?
TK
??
M
)
(o
2
)
= DiscTK(o
1
, o
2
), which is a new kernel
4
in-
duced by the mapping ?
DiscTK
=
(
?
TK
? ?
M
)
.
We define two different mappings ?
M
to trans-
form the discourse parses generated by the base
parser into two different tree structures: (i) the
Joint Relation-Nucleus tree (JRN), and (ii) the
Split Relation Nucleus tree (SRN).
3.1 Joint Relation-Nucleus Tree (JRN)
As shown in Figure 4a, JRN is a direct mapping
of the parser output, where the nuclearity statuses
(i.e., satellite or nucleus) of the connecting nodes
are attached to the relation labels.
5
For example,
the root BACKGROUND
Satellite?Nucleus
in Figure
4a denotes a Background relation between a satel-
lite discourse unit on the left and a nucleus unit on
the right. Text spans (i.e., EDUs) are represented
as sequences of Part-of-Speech (POS) tags con-
nected to the associated words, and are grouped
under dummy SPAN nodes. We experiment with
two lexical variations of the trees: (i) All includes
all the words in the EDU, and (ii) Bigram includes
only the first and last two words in the EDU.
When JRN is used with the STK kernel, an ex-
ponential number of fragments are generated. For
example, the upper row of Figure 5 shows two
4
People interested in algorithms may like it more design-
ing a complex algorithm to compute
(
?
TK
??
M
)
. However,
the design of ?
M
is conceptually equivalent and more effec-
tive from an engineering viewpoint.
5
This is a common standard followed by the parsers.
smallest (atomic) fragments and one subtree com-
posed of two atomic fragments. Note that much
larger structures encoding long range dependen-
cies are also part of the feature space. These frag-
ments can reveal if the discourse units are orga-
nized in a compatible way, and help the reranker
to detect the kind of errors shown earlier in Fig-
ure 1b. However, one problem with JRN repre-
sentation is that since the relation nodes are com-
posed of three different labels, the generated sub-
trees tend to be sparse. In the following, we de-
scribe SRN that attempts to solve this issue.
3.2 Split Relation Nucleus Tree (SRN)
SRN is not very different from JRN as shown in
Figure 4b. The only difference is that instead of
attaching the nuclearity statuses to the relation la-
bels, in this representation we assign them to their
respective discourse units. When STK kernel is
applied to SRN it again produces an exponential
number of fragments. For example, the lower row
of Figure 5 shows two atomic fragments and one
subtree composed of two atomic fragments. Com-
paring the two examples in Figure 5, it is easy
to understand that the space of subtrees extracted
from SRN is less sparse than that of JRN.
Note that, as described in Secion 2.2, when the
PTK kernel is applied to JRN and SRN trees, it can
generate a richer feature space, e.g., features that
are paths containing relation labels (e.g., BACK-
GROUND - CAUSE - ELABORATION or ATTRIBU-
TION - CAUSE - ELABORATION).
4 Generation of k-best Discourse Parses
In this section we describe the 1-best discourse
parser of Joty et al. (2013), and how we extend
2052
Figure 5: Fragments from JRN in Figure 4a (upper row) and SRN in Figure 4b (lower row).
it to k-best discourse parsing.
Joty et al. (2013) decompose the problem of
document-level discourse parsing into two stages
as shown in Figure 6. In the first stage, the intra-
sentential discourse parser produces discourse
subtrees for the individual sentences in a docu-
ment. Then the multi-sentential parser combines
the sentence-level subtrees and produces a DT for
the document. Both parsers have the same two
components: a parsing model and a parsing al-
gorithm. The parsing model explores the search
space of possible DTs and assigns a probability to
every possible DT. Then the parsing algorithm se-
lects the most probable DT(s). While two separate
parsing models are employed for intra- and multi-
sentential parsing, the same parsing algorithm is
used in both parsing conditions. The two-stage
parsing exploits the fact that sentence boundaries
correlate very well with discourse boundaries. For
example, more than 95% of the sentences in RST-
DT have a well-formed discourse subtree in the
full document-level discourse tree.
The choice of using two separate models for
intra- and multi-sentential parsing is well justified
for the following two reasons: (i) it has been ob-
served that discourse relations have different dis-
tributions in the two parsing scenarios, and (ii) the
models could independently pick their own infor-
mative feature sets. The parsing model used for
intra-sentential parsing is a Dynamic Conditional
Random Field (DCRF) (Sutton et al., 2007) shown
in Figure 7. The observed nodes U
j
at the bottom
layer represent the discourse units at a certain level
of the DT; the binary nodes S
j
at the middle layer
predict whether two adjacent units U
j?1
and U
j
should be connected or not; and the multi-class
nodes R
j
at the top layer predict the discourse
relation between U
j?1
and U
j
. Notice that the
model represents the structure and the label of a
DT constituent jointly, and captures the sequential
dependencies between the DT constituents. Since
the chain-structured DCRF model does not scale
up to multi-sentential parsing of long documents,
Model
A lgorithmSentences segmentedinto EDUs
Document-leveldiscourse treeModel
A lgorithm
Multi-sentential parserIntra-sentential parser
Figure 6: The two-stage document-level discourse parser
proposed by Joty et al. (2013).
U UU U U
2
2
2
3 j t-1 t
SS S S S
R R R R R
3
3 j
j t-1
t-1 t
U1
t
Relation sequenceStructuresequence
 Unit sequence at level i 
Figure 7: The intra-sentential parsing model.
the multi-sentential parsing model is a CRF which
breaks the chain structure of the DCRF model.
The parsing models are applied recursively at
different levels of the DT in their respective pars-
ing scenarios (i.e., intra- and multi-sentential),
and the probabilities of all possible DT con-
stituents are obtained by computing the posterior
marginals over the relation-structure pairs (i.e.,
P (R
j
, S
j
=1|U
1
, ? ? ? , U
t
,?), where ? are model
parameters). These probabilities are then used in
a CKY-like probabilistic parsing algorithm to find
the globally optimal DT for the given text.
Let U
b
x
and U
e
x
denote the beginning and
end EDU Ids of a discourse unit U
x
, and
R[U
b
i
, U
e
m
, U
e
j
] refers to a coherence relation
R that holds between the discourse unit con-
taining EDUs U
b
i
through U
e
m
and the unit
containing EDUs U
e
m
+1 through U
e
j
. Given n
discourse units, the parsing algorithm uses the
upper-triangular portion of the n?n dynamic
programming table A, where cell A[i, j] (for
i < j) stores:
A[i, j] = P (r
?
[U
b
i
, U
e
m
?
, U
e
j
]), where
(m
?
, r
?
) = argmax
i?m<j ; R
P (R[U
b
i
, U
e
m
, U
e
j
])?
A[i,m]?A[m+ 1, j] (1)
2053
1 1 2
2 2
3
B
r
1
r
3
r
2
r
2
r
3
r
4
C
r
2
r
1
e
1
e
2
r
4
e
3
e
4
Figure 8: The B and C dynamic programming tables (left), and the corresponding discourse tree (right).
In addition to A, which stores the probability of
the most probable constituents of a DT, the pars-
ing algorithm also simultaneously maintains two
other tables B and C for storing the best structure
(i.e., U
e
m
?
) and the relations (i.e., r
?
) of the corre-
sponding DT constituents, respectively. For exam-
ple, given 4 EDUs e
1
? ? ? e
4
, the B and C tables at
the left side in Figure 8 together represent the DT
shown at the right. More specifically, to generate
the DT, we first look at the top-right entries in the
two tables, and find B[1, 4] = 2 and C[1, 4] = r
2
,
which specify that the two discourse units e
1:2
and
e
3:4
should be connected by the relation r
2
(the
root in the DT). Then, we see how EDUs e
1
and
e
2
should be connected by looking at the entries
B[1, 2] and C[1, 2], and find B[1, 2] = 1 and
C[1, 2] = r
1
, which indicates that these two units
should be connected by the relation r
1
(the left
pre-terminal). Finally, to see how EDUs e
3
and e
4
should be linked, we look at the entriesB[3, 4] and
C[3, 4], which tell us that they should be linked by
the relation r
4
(the right pre-terminal).
It is straight-forward to generalize the above al-
gorithm to produce k most probable DTs. When
filling up the dynamic programming tables, rather
than storing a single best parse for each subtree,
we store and keep track of k-best candidates si-
multaneously. More specifically, each cell in the
dynamic programming tables (i.e., A, B and C)
should now contain k entries (sorted by their prob-
abilities), and for each such entry there should be a
back-pointer that keeps track of the decoding path.
The algorithm works in polynomial time. For
n discourse units and M number of relations, the
1-best parsing algorithm has a time complexity of
O(n
3
M) and a space complexity of O(n
2
), where
the k-best version has a time and space complexi-
ties ofO(n
3
Mk
2
log k) andO(n
2
k), respectively.
There are cleverer ways to reduce the complexity
(e.g., see (Huang and Chiang, 2005) for three such
ways). However, since the efficiency of the algo-
rithm did not limit us to produce k-best parses for
larger k, it was not a priority in this work.
5 Kernels for Reranking Discourse Trees
In Section 3, we described DISCTK, which essen-
tially can be used for any classification task involv-
ing discourse trees. For example, given a DT, we
can use DISCTK to classify it as correct vs. in-
correct. However, such classification is not com-
pletely aligned to our purpose, since our goal is
to select the best (i.e., the most correct) DT from
k candidate DTs; i.e., a ranking task. We adopt
a preference reranking technique as described in
(Moschitti et al., 2006; Dinarelli et al., 2011).
5.1 Preference Reranker
Preference reranking (PR) uses a classifier C of
pairs of hypotheses ?h
i
, h
j
?, which decides if h
i
(i.e., a candidate DT in our case) is better than
h
j
. We generate positive and negative examples to
train the classifier using the following approach.
The pairs ?h
1
, h
i
? constitute positive examples,
where h
1
has the highest f -score accuracy on the
Relation metric (to be described in Section 6) with
respect to the gold standard among the candidate
hypotheses, and vice versa, ?h
i
, h
1
? are considered
as negative examples. At test time, C classifies all
pairs ?h
i
, h
j
? generated from the k-best hypothe-
ses. A positive decision is a vote for h
i
, and a neg-
ative decision is a vote for h
j
. Also, the classifier
score can be used as a weighted vote. Hypotheses
are then ranked according to the number (sum) of
the (weighted) votes they get.
6
We build our reranker using simple SVMs.
7
6
As shown by Collins and Duffy (2002), only the classifi-
cation of k hypotheses (paired with the empty one) is needed
in practice, thus the complexity is only O(k).
7
Structural kernels, e.g., TKs, cannot be used in more ad-
vanced algorithms working in structured output spaces, e.g.,
SVM
struct
. Indeed, to our knowledge, no one could suc-
cessfully find a general and exact solution for the argmax
equation, typically part of such advanced models, when struc-
tural kernels are used. Some approximate solutions for sim-
ple kernels, e.g., polynomial or gaussian kernels, are given in
(Joachims and Yu, 2009), whereas (Severyn and Moschitti,
2011; Severyn and Moschitti, 2012) provide solutions for
using the cutting-plane algorithm (which requires argmax
computation) with structural kernels but in binary SVMs.
2054
Since in our problem a pair of hypotheses ?h
i
, h
j
?
constitutes a data instance, we now need to define
the kernel between the pairs. However, notice that
DISCTK only works on a single pair.
Considering that our task is to decide whether
h
i
is better than h
j
, it can be convenient to
represent the pairs in terms of differences be-
tween the vectors of the two hypotheses, i.e.,
?
K
(h
i
)? ?
K
(h
j
), where K (i.e., DISCTK) is de-
fined between two hypotheses (not on two pairs
of hypotheses). More specifically, to compute
this difference implicitly, we can use the follow-
ing kernel summation: PK(?h
1
, h
2
?, ?h
?
1
, h
?
2
?) =
(?
K
(h
1
) ? ?
K
(h
2
)) ? (?
K
(h
?
1
) ? ?
K
(h
?
2
)) =
K(h
1
, h
?
1
)+K(h
2
, h
?
2
)?K(h
1
, h
?
2
)?K(h
2
, h
?
1
).
In general, Preference Kernel (PK) works well
because it removes many identical features by tak-
ing differences between two huge implicit TK-
vectors. In our reranking framework, we also in-
clude traditional feature vectors in addition to the
trees. Therefore, each hypothesis h is represented
as a tuple ?T,~v? composed of a tree T and a fea-
ture vector ~v. We then define a structural kernel
(i.e., similarity) between two hypotheses h and
h
?
as follows: K(h, h
?
) = DiscTK(T, T
?
) +
FV (~v,~v
?
), where DISCTK maps the DTs T and
T
?
to JRN or SRN and then applies STK, STK
b
or
PTK defined in Sections 2.2 and 3, and FV is a
standard kernel, e.g., linear, polynomial, gaussian,
etc., over feature vectors (see next section).
5.2 Feature Vectors
We also investigate the impact of traditional
(i.e., not subtree) features for reranking discourse
parses. Our feature vector comprises two types of
features that capture global properties of the DTs.
Basic Features. This set includes eight global
features. The first two are the probability and
the (inverse) rank of the DT given by the base
parser. These two features are expected to help
the reranker to perform at least as good as the base
parser. The other six features encode the structural
properties of the DT, which include depth of the
DT, number of nodes connecting two EDUs (i.e.,
SPANs in Figure 4), number of nodes connecting
two relational nodes, number of nodes connecting
a relational node and an EDU, number of nodes
that connects a relational node as left child and an
EDU as right child, and vice versa.
Relation Features. We encode the relations in
the DT as bag-of-relations (i.e., frequency count).
This will allow us to assess the impact of a flat rep-
resentation of the DT. Note that more important
relational features would be the subtree patterns
extracted from the DT. However, they are already
generated by TKs in a simpler way. See (Pighin
and Moschitti, 2009; Pighin and Moschitti, 2010)
for a way to extract the most relevant features from
a model learned in the kernel space.
6 Experiments
Our experiments aim to show that reranking of
discourse parses is a promising research direction,
which can improve the state-of-the-art. To achieve
this, we (i) compute the oracle accuracy of the k-
best parser, (ii) test different kernels for reranking
discourse parses by applying standard kernels to
our new structures, (iii) show the reranking perfor-
mance using the best kernel for different number
of hypotheses, and (iv) show the relative impor-
tance of features coming from different sources.
6.1 Experimental Setup
Data. We use the standard RST-DT corpus (Carl-
son et al., 2002), which comes with discourse an-
notations for 385 articles (347 for training and 38
for testing) from the Wall Street Journal. We ex-
tracted sentence-level DTs from a document-level
DT by finding the subtrees that exactly span over
the sentences. This gives 7321 and 951 sentences
in the training and test sets, respectively. Follow-
ing previous work, we use the same 18 coarser re-
lations defined by Carlson and Marcu (2001).
We create the training data for the reranker in a
5-fold cross-validation fashion.
8
Specifically, we
split the training set into 5 equal-sized folds, and
train the parsing model on 4 folds and apply to the
rest to produce k most probable DTs for each text.
Then we generate and label the pairs (by compar-
ing with the gold) from the k most probable trees
as described in Section 5.1. Finally, we merge the
5 labeled folds to create the full training data.
SVM Reranker. We use SVM-light-TK to train
our reranking models,
9
which enables the use
of tree kernels (Moschitti, 2006) in SVM-light
(Joachims, 1999). We build our new kernels for
reranking exploiting the standard built-in TK func-
tions, such as STK, STK
b
and PTK. We applied
8
Note that our earlier experiments with a 2-fold cross vali-
dation process yielded only 50% of our current improvement.
9
http://disi.unitn.it/moschitti/Tree-Kernel.htm
2055
a linear kernel to standard feature vectors as it
showed to be the best on our development set.
Metrics. The standard procedure to evaluate dis-
course parsing performance is to compute Pre-
cision, Recall and f -score of the unlabeled and
labeled metrics proposed by Marcu (2000b).
10
Specifically, the unlabeled metric Span measures
how accurate the parser is in finding the right
structure (i.e., skeleton) of the DT, while the la-
beled metrics Nuclearity and Relation measure the
parser?s ability to find the right labels (nuclearity
and relation) in addition to the right structure. Op-
timization of the Relation metric is considered to
be the hardest and the most desirable goal in dis-
course parsing since it gives aggregated evaluation
on tree structure and relation labels. Therefore,
we measure the oracle accuracy of the k-best dis-
course parser based on the f -scores of the Relation
metric, and our reranking framework aims to op-
timize the Relation metric.
11
Specifically, the ora-
cle accuracy for k-best parsing is measured as fol-
lows: ORACLE =
?
N
i=1
max
k
j=1
f?score
r
(g
i
,h
j
i
)
N
, where
N is the total number of texts (sentences or docu-
ments) evaluated, g
i
is the gold DT annotation for
text i, h
j
i
is the j
th
parse hypothesis generated by
the k-best parser for text i, and f -score
r
(g
i
, h
j
i
) is
the f -score accuracy of hypothesis h
j
i
on the Re-
lation metric. In all our experiments we report the
f -scores of the Relation metric.
6.2 Oracle Accuracy
Table 1 presents the oracle scores of the k-
best intra-sentential parser PAR-S on the standard
RST-DT test set. The 1-best result corresponds
to the accuracy of the base parser (i.e., 79.77%).
The 2-best shows dramatic oracle-rate improve-
ment (i.e., 4.65% absolute), suggesting that the
base parser often generates the best tree in its
top 2 outputs. 5-best increases the oracle score
to 88.09%. Afterwards, the increase in accuracy
slows down, achieving, e.g., 90.37% and 92.57%
at 10-best and 20-best, respectively.
The results are quite different at the document
level as Table 2 shows the oracle scores of the k-
best document-level parser PAR-D.
12
The results
10
Precision, Recall and f -score are the same when the dis-
course parser uses manual discourse segmentation. Since all
our experiments in this paper are based on manual discourse
segmentation, we only report the f -scores.
11
It is important to note that optimizing Relation metric
may also result in improved Nuclearity scores.
12
For document-level parsing, Joty et al. (2013) pro-
k 1 2 5 10 15 20
PAR-S 79.77 84.42 88.09 90.37 91.74 92.57
Table 1: Oracle scores as a function of k of k-best sentence-
level parses on RST-DT test set.
k 1 2 5 10 15 20
PAR-D 55.83 56.52 56.91 57.23 57.54 57.65
Table 2: Oracle scores as a function of k of k-best
document-level parses on RST-DT test set.
suggest that the best tree is often missing in the
top k parses, and the improvement in oracle-rate is
very little as compared to the sentence-level pars-
ing. The 2-best and the 5-best improve over the
base accuracy by only 0.7% and 1.0%, respec-
tively. The improvement becomes even lower for
larger k. For example, the gain from 20-best to
30-best parsing is only 0.09%. This is not sur-
prising because generally document-level DTs are
big with many constituents, and only a very few
of them change from k-best to k+1-best parsing.
These small changes do not contribute much to
the overall f -score accuracy.
13
In summary, the
results in Tables 1 and 2 demonstrate that a k-best
reranker can potentially improve the parsing accu-
racy at the sentence level, but may not be a suit-
able option for improving parsing at the document
level. In the following, we report our results for
reranking sentence-level discourse parses.
6.3 Performance of Different DISCTKs
Section 3 has pointed out that different DISCTKs
can be obtained by specifying the TK type (e.g.,
STK, STK
b
, PTK) and the mapping ?
M
(i.e.,
JRN, SRN) in the overall kernel function
(
?
TK
?
?
M
)
(o
1
)?
(
?
TK
??
M
)
(o
2
). Table 3 reports the per-
formance of such model compositions using the 5-
best hypotheses on the RST-DT test set. Addition-
ally, it also reports the accuracy for the two ver-
sions of JRN and SRN, i.e., Bigram and All. From
these results, we can note the following.
Firstly, the kernels generally perform better on
Bigram than All lexicalization. This suggests that
using all the words from the text spans (i.e., EDUs)
produces sparse models.
pose two approaches to combine intra- and multi-sentential
parsers, namely 1S-1S (1 Sentence-1 Subtree) and Sliding
window. In this work we extend 1S-1S to k-best document-
level parser PAR-D since it is not only time efficient but it
also achieves better results on the Relation metric.
13
Note that Joty et al. (2012; 2013) report lower f -scores
both at the sentence level (i.e., 77.1% as opposed to our
79.77%) and at the document level (i.e., 55.73% as opposed
to our 55.83%). We fixed a crucial bug in their (1-best) pars-
ing algorithm, which accounts for the improved performance.
2056
?TK
? ?
M
JRN SRN
Bigram All Bigram All
STK 81.28 80.04 82.15 80.04
STK
b
81.35 80.28 82.18 80.25
PTK 81.63 78.50 81.42 78.25
Table 3: Reranking performance of different discourse tree
kernels on different representations.
Secondly, while the tree kernels perform sim-
ilarly on the JRN representation, STK performs
significantly better (p-value < 0.01) than PTK
on SRN.
14
This result is interesting as it pro-
vides indications of the type of DT fragments use-
ful for improving parsing accuracy. As pointed
out in Section 2.2, PTK includes all features
generated by STK, and additionally, it includes
fragments whose nodes can have any subsets of
the children they have in the original DT. Since
this does not improve the accuracy, we speculate
that complete fragments, e.g., [CAUSE [ATTRI-
BUTION][ELABORATION]] are more meaningful
than the partial ones, e.g., [CAUSE [ATTRIBU-
TION]] and [CAUSE [ELABORATION]], which
may add too much uncertainty on the signature
of the relations contained in the DT. We verified
this hypothesis by running an experiment with
PTK constraining it to only generate fragments
whose nodes preserve all or none of their children.
The accuracy of such fragments approached the
ones of STK, suggesting that relation information
should be used as a whole for engineering features.
Finally, STK
b
is slightly (but not significantly)
better than STK suggesting that the lexical infor-
mation is already captured by the base parser.
Note that the results in Table 3 confirms many
other experiments we carried out on several devel-
opment sets. For any run: (i) STK always performs
as well as STK
b
, (ii) STK is always better than
PTK, and (iii) SRN is always better than JRN. In
what follows, we show the reranking performance
based on STK applied to SRN with Bigram.
6.4 Insights on DISCTK-based Reranking
Table 4 reports the performance of our reranker
(RR) in comparison with the oracle (OR) accuracy
for different values of k, where we also show the
corresponding relative error rate reduction (ERR)
with respect to the baseline. To assess the general-
ity of our approach, we evaluated our reranker on
both the standard test set and the entire training set
using 5-fold cross validation.
15
14
Statistical significance is verified using paired t-test.
15
The reranker was trained on 4 folds and tested on the rest
Baseline Basic feat. + Rel. feat. + Tree
79.77 79.84 79.81 82.15
Table 5: Comparison of features from different sources for
5-best discourse reranking.
(Joty et al., 2013) With Reranker
PAR-D 55.8 57.3
Table 6: Document-level parsing results with 5-best
sentence-level discourse reranker.
We note that: (i) the best result on the standard
test set is 82.15% for k = 4 and 5, which gives
an ERR of 11.76%, and significantly (p-value <
0.01) outperforms the baseline, (ii) the improve-
ment is consistent when we move from standard
test set to 5-folds, (iii) the best result on the 5-folds
is 80.86 for k = 6, which is significantly (p-value
< 0.01) better than the baseline 78.57, and gives
an ERR of 11.32%. We also experimented with
other values of k in both training and test sets (also
increasing k only in the test set), but we could not
improve over our best result. This suggests that
outperforming the baseline (which in our case is
the state of the art) is rather difficult.
16
In this respect, we also investigated the im-
pact of traditional ranking methods based on fea-
ture vectors, and compared it with our TK-based
model. Table 5 shows the 5-best reranking accu-
racy for different feature subsets. The Basic fea-
tures (Section 5.2) alone do not significantly im-
prove over the Baseline. The only relevant fea-
tures are the probability and the rank of each hy-
pothesis, which condense all the information of
the local model (TKs models always used them).
Similarly, adding the relations as bag-of-
relations in the vector (Rel. feat.) does not pro-
vide any gain, whereas the relations encoded in
the tree fragments (Tree) gives improvement. This
shows the importance of using structural depen-
dencies for reranking discourse parses.
Finally, Table 6 shows that if we use our
sentence-level reranker in the document-level
parser of Joty et al. (2013), the accuracy of the lat-
ter increases from 55.8% to 57.3%, which is a sig-
nificant improvement (p < 0.01), and establishes
a new state-of-the-art for document-level parsing.
6.5 Error Analysis
We looked at some examples where our reranker
failed to identify the best DT. Unsurprisingly, it
16
The human agreement on sentence-level parsing is 83%.
2057
Standard test set 5-folds (average)
k=1 k=2 k=3 k=4 k=5 k=6 k=1 k=2 k=3 k=4 k=5 k=6
RR 79.77 81.08 81.56 82.15 82.15 82.11 78.57 79.76 80.28 80.68 80.80 80.86
ERR - 6.48 8.85 11.76 11.76 11.57 - 5.88 8.45 10.43 11.02 11.32
OR 79.77 84.42 86.55 87.68 88.09 88.75 78.57 83.20 85.13 86.49 87.35 88.03
Table 4: Reranking performance (RR) in comparison with oracle (OR) accuracy for different values of k on the standard
testset and 5-folds of RST-DT. Second row shows the relative error rate reduction (ERR).
happens many times for small DTs containing
only two or three EDUs, especially when the re-
lations are semantically similar. Figure 9 presents
such a case, where the reranker fails to rank the
DT with Summary ahead of the DT with Elabo-
ration. Although we understand that the reranker
lacks enough structural context to distinguish the
two relations in this example, we expected that in-
cluding the lexical items (e.g., (CFD)) in our DT
representation could help. However, similar short
parenthesized texts are also used to elaborate as
in Senate Majority Leader George Mitchell (D.,
Maine), where the text (D., Maine) (i.e., Democrat
from state Maine) elaborates its preceding text.
This confuses our reranker. We also found er-
ror examples where the reranker failed to distin-
guish between Background and Elaboration, and
between Cause and Elaboration. This suggests
that we need rich semantic representation of the
text to improve our reranker further.
7 Related Work
Early work on discourse parsing applied hand-
coded rules based on discourse cues and surface
patterns (Marcu, 2000a). Supervised learning was
first attempted by Marcu (2000b) to build a shift-
reduce discourse parser. This work was then con-
siderably improved by Soricut and Marcu (2003).
They presented probabilistic generative models for
sentence-level discourse parsing based on lexico-
syntactic patterns. Sporleder and Lapata (2005)
investigated the necessity of syntax in discourse
analysis. More recently, Hernault et al. (2010)
presented the HILDA discourse parser that itera-
tively employs two SVM classifiers in pipeline to
build a DT in a greedy way. Feng and Hirst (2012)
improved the HILDA parser by incorporating rich
linguistic features, which include lexical seman-
tics and discourse production rules.
Joty et al. (2013) achieved the best prior results
by (i) jointly modeling the structure and the la-
bel of a DT constituent, (ii) performing optimal
rather than greedy decoding, and (iii) discriminat-
ing between intra- and multi-sentential discourse
parsing. However, their model does not con-
Same-UnitSummary
begins trading today.On the Big Board, Crawford & Co., Atlanta, (CFD)
Elaboration
Figure 9: An error made by our reranker.
sider long range dependencies between DT con-
stituents, which are encoded by our kernels. Re-
garding the latter, our work is surely inspired by
(Collins and Duffy, 2002), which uses TK for syn-
tactic parsing reranking or in general discrimina-
tive reranking, e.g., (Collins and Koo, 2005; Char-
niak and Johnson, 2005; Dinarelli et al., 2011).
However, such excellent studies do not regard
discourse parsing, and in absolute they achieved
lower improvements than our methods.
8 Conclusions and Future Work
In this paper, we have presented a discriminative
approach for reranking discourse trees generated
by an existing discourse parser. Our reranker uses
tree kernels in SVM preference ranking frame-
work to effectively capture the long range struc-
tural dependencies between the constituents of a
discourse tree. We have shown the reranking per-
formance for sentence-level discourse parsing us-
ing the standard tree kernels (i.e., STK and PTK)
on two different representations (i.e., JRN and
SRN) of the discourse tree, and compare it with
the traditional feature vector-based approach. Our
results show that: (i) the reranker improves only
when it considers subtree features computed by
the tree kernels, (ii) SRN is a better representation
than JRN, (iii) STK performs better than PTK for
reranking discourse trees, and (iv) our best result
outperforms the state-of-the-art significantly.
In the future, we would like to apply our
reranker to the document-level parses. However,
this will require a better hypotheses generator.
Acknowledgments
This research is part of the Interactive sYstems
for Answer Search (Iyas) project, conducted by
the Arabic Language Technologies (ALT) group
at Qatar Computing Research Institute (QCRI)
within the Qatar Foundation.
2058
References
Lynn Carlson and Daniel Marcu. 2001. Discourse Tag-
ging Reference Manual. Technical Report ISI-TR-
545, University of Southern California Information
Sciences Institute.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002. RST Discourse Treebank (RST-
DT) LDC2002T07. Linguistic Data Consortium,
Philadelphia.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL?05, pages 173?180, NJ, USA. ACL.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
ACL.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Comput.
Linguist., 31(1):25?70, March.
Corinna Cortes and Vladimir Vapnik. 1995. Support
Vector Networks. Machine Learning, 20:273?297.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2011. Discriminative Reranking for
Spoken Language Understanding. IEEE Transac-
tions on Audio, Speech and Language Processing
(TASLP), 20:526539.
Vanessa Feng and Graeme Hirst. 2012. Text-level Dis-
course Parsing with Rich Linguistic Features. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?12,
pages 60?68, Jeju Island, Korea. ACL.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser Using Support Vector Machine Classification.
Dialogue and Discourse, 1(3):1?33.
Liang Huang and David Chiang. 2005. Better K-
best Parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
?05, pages 53?64, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Thorsten Joachims and Chun-Nam John Yu. 2009.
Sparse Kernel SVMs via Cutting-Plane Training.
Machine Learning, 76(2-3):179?193. ECML.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. In Advances in Kernel Methods
- Support Vector Learning.
Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 904?915, Jeju Island, Korea. ACL.
Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?13, Sofia, Bulgaria. ACL.
William Mann and Sandra Thompson. 1988. Rhetor-
ical Structure Theory: Toward a Functional Theory
of Text Organization. Text, 8(3):243?281.
Daniel Marcu. 2000a. The Rhetorical Parsing of Un-
restricted Texts: A Surface-based Approach. Com-
putational Linguistics, 26:395?448.
Daniel Marcu. 2000b. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press,
Cambridge, MA, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic Role Labeling via Tree Ker-
nel Joint Inference. In Proceedings of the Tenth
Conference on Computational Natural Language
Learning (CoNLL-X), pages 61?68, New York City,
June. Association for Computational Linguistics.
Alessandro Moschitti. 2006. Efficient Convolution
Kernels for Dependency and Constituent Syntactic
Trees. In 17th European Conference on Machine
Learning, pages 318?329. Springer.
Alessandro Moschitti. 2010. Kernel Engineering for
Fast and Easy Design of Natural Language Applica-
tions. In COLING (Tutorials), pages 1?91.
Alessandro Moschitti. 2012. State-of-the-Art Kernels
for Natural Language Processing. In Tutorial Ab-
stracts of ACL 2012, page 2, Jeju Island, Korea, July.
Association for Computational Linguistics.
Alessandro Moschitti. 2013. Kernel-based Learning
to Rank with Syntactic and Semantic Structures. In
SIGIR, page 1128.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse Engineering of Tree Kernel Feature Spaces. In
EMNLP, pages 111?120.
Daniele Pighin and Alessandro Moschitti. 2010. On
Reverse Feature Engineering of Syntactic Tree Ker-
nels. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 223?233, Uppsala, Sweden, July. Association
for Computational Linguistics.
Aliaksei Severyn and Alessandro Moschitti. 2011.
Fast Support Vector Machines for Structural Ker-
nels. In ECML/PKDD (3), pages 175?190.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Fast Support Vector Machines for Convolution Tree
Kernels. Data Min. Knowl. Discov., 25(2):325?357.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
2059
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL?03, pages 149?156,
Edmonton, Canada. ACL.
Caroline Sporleder and Mirella Lapata. 2005. Dis-
course Chunking and its Application to Sentence
Compression. In Proceedings of the conference
on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT-
EMNLP?05, pages 257?264, Vancouver, British
Columbia, Canada. ACL.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic Conditional Ran-
dom Fields: Factorized Probabilistic Models for La-
beling and Segmenting Sequence Data. Journal of
Machine Learning Research (JMLR), 8:693?723.
2060
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?672,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Encoding Semantic Resources in Syntactic Structures
for Passage Reranking
Kateryna Tymoshenko
Trento RISE
38123 Povo (TN), Italy
k.tymoshenko@trentorise.eu
Alessandro Moschitti
Qatar Computing Research Instit.
5825 Doha, Qatar
amoschitti@qf.org.qa
Aliaksei Severyn
University of Trento
38123 Povo (TN), Italy
severyn@disi.unitn.it
Abstract
In this paper, we propose to use seman-
tic knowledge from Wikipedia and large-
scale structured knowledge datasets avail-
able as Linked Open Data (LOD) for
the answer passage reranking task. We
represent question and candidate answer
passages with pairs of shallow syntac-
tic/semantic trees, whose constituents are
connected using LOD. The trees are pro-
cessed by SVMs and tree kernels, which
can automatically exploit tree fragments.
The experiments with our SVM rank algo-
rithm on the TREC Question Answering
(QA) corpus show that the added relational
information highly improves over the state
of the art, e.g., about 15.4% of relative im-
provement in P@1.
1 Introduction
Past work in TREC QA, e.g. (Voorhees, 2001),
and more recent work (Ferrucci et al., 2010) in
QA has shown that, to achieve human perfor-
mance, semantic resources, e.g., Wikipedia
1
,
must be utilized by QA systems. This requires
the design of rules or machine learning features
that exploit such knowledge by also satisfying
syntactic constraints, e.g., the semantic type of
the answer must match the question focus words.
The engineering of such rules for open domain
QA is typically very costly. For instance, for
automatically deriving the correctness of the
answer passage in the following question/answer
passage (Q/AP) pair (from the TREC corpus
2
):
Q: What company owns the soft drink brand ?Gatorade??
A: Stokely-Van Camp bought the formula and started
marketing the drink as Gatorade in 1967. Quaker Oats Co.
took over Stokely-Van Camp in 1983.
1
http://www.wikipedia.org
2
It will be our a running example for the rest of the paper.
we would need to write the following complex
rules:
is(Quaker Oats Co.,company),
own(Stokely-Van Camp,Gatorade),
took over(Quaker Oats Co.,Stokely-Van Camp),
took over(Y, Z)?own(Z,Y),
and carry out logic unification and resolution.
Therefore, approaches that can automatically
generate patterns (i.e., features) from syntactic
and semantic representations of the Q/AP are
needed. In this respect, our previous work, e.g.,
(Moschitti et al., 2007; Moschitti and Quarteroni,
2008; Moschitti, 2009), has shown that tree
kernels for NLP, e.g., (Moschitti, 2006), can
exploit syntactic patterns for answer passage
reranking significantly improving search engine
baselines. Our more recent work, (Severyn and
Moschitti, 2012; Severyn et al., 2013b; Severyn
et al., 2013a), has shown that using automatically
produced semantic labels in shallow syntactic
trees, such as question category and question
focus, can further improve passage reranking and
answer extraction (Severyn and Moschitti, 2013).
However, such methods cannot solve the class
of examples above as they do not use background
knowledge, which is essential to answer com-
plex questions. On the other hand, Kalyanpur
et al. (2011) and Murdock et al. (2012) showed
that semantic match features extracted from large-
scale background knowledge sources, including
the LOD ones, are beneficial for answer rerank-
ing.
In this paper, we tackle the candidate answer
passage reranking task. We define kernel func-
tions that can automatically learn structural pat-
terns enriched by semantic knowledge, e.g., from
LOD. For this purpose, we carry out the follow-
ing steps: first, we design a representation for the
Q/AP pair by engineering a pair of shallow syn-
tactic trees connected with relational nodes (i.e.,
664
  
NLPAnnotatorsFocus and Question classifiers
NLPAnnotatorsFocus and Question classifiers
syntactic/semantic graphsyntactic/semantic graph train/testdata
Kernel-based rerankerKernel-based reranker
RerankedAP
EvaluationEvaluation
CandidateAPQuestion
UIMA pipeline
Search engineSearch engine
q/a similarity featuresq/a similarity features
Wikipedia link annotatorWikipedia link annotator
WikipediaWikipedia
LOD type annotatorLOD type annotator
LOD datasetsLOD datasets
Figure 1: Kernel-based Answer Passage Reranking System
those matching the same words in the question and
in the answer passages).
Secondly, we use YAGO (Suchanek et al.,
2007), DBpedia (Bizer et al., 2009) and Word-
Net (Fellbaum, 1998) to match constituents from
Q/AP pairs and use their generalizations in our
syntactic/semantic structures. We employ word
sense disambiguation to match the right entities in
YAGO and DBpedia, and consider all senses of an
ambiguous word from WordNet.
Finally, we experiment with TREC QA and sev-
eral models combining traditional feature vectors
with automatic semantic labels derived by statis-
tical classifiers and relational structures enriched
with LOD relations. The results show that our
methods greatly improve over strong IR baseline,
e.g., BM25, by 96%, and on our previous state-
of-the-art reranking models, up to 15.4% (relative
improvement) in P@1.
2 Reranking with Tree Kernels
In contrast to ad-hoc document retrieval, struc-
tured representation of sentences and paragraphs
helps to improve question answering (Bilotti et al.,
2010). Typically, rules considering syntactic and
semantic properties of the question and its candi-
date answer are handcrafted. Their modeling is in
general time-consuming and costly. In contrast,
we rely on machine learning and automatic fea-
ture engineering with tree kernels. We used our
state-of-the-art reranking models, i.e., (Severyn et
al., 2013b; Severyn et al., 2013a) as a baseline.
Our major difference with such approach is that
we encode knowledge and semantics in different
ways, using knowledge from LOD. The next sec-
tions outline our new kernel-based framework, al-
though the detailed descriptions of the most inno-
vative aspects such as new LOD-based representa-
tions are reported in Section 3.
2.1 Framework Overview
Our QA system is based on a rather simple rerank-
ing framework as displayed in Figure 1: given a
question Q, a search engine retrieves a list of can-
didate APs ranked by their relevancy. Next, the
question together with its APs are processed by
a rich NLP pipeline, which performs basic tok-
enization, sentence splitting, lemmatization, stop-
word removal. Various NLP components, em-
bedded in the pipeline as UIMA
3
annotators, per-
form more involved linguistic analysis, e.g., POS-
tagging, chunking, NE recognition, constituency
and dependency parsing, etc.
Each Q/AP pair is processed by a Wikipedia
link annotator. It automatically recognizes n-
grams in plain text, which may be linked to
Wikipedia and disambiguates them to Wikipedia
URLs. Given that question passages are typically
short, we concatenate them with the candidate an-
swers to provide a larger disambiguation context
to the annotator.
These annotations are then used to produce
computational structures (see Sec. 2.2) input to the
reranker. The semantics of such relational struc-
tures can be further enriched by adding links be-
tween Q/AP constituents. Such relational links
can be also generated by: (i) matching lemmas
as in (Severyn and Moschitti, 2012); (ii) match-
ing the question focus type derived by the ques-
tion classifiers with the type of the target NE as
in (Severyn et al., 2013a); or (iii) by matching the
constituent types based on LOD (proposed in this
paper). The resulting pairs of trees connected by
semantic links are then used to train a kernel-based
reranker, which is used to re-order the retrieved
answer passages.
2.2 Relational Q/AP structures
We use the shallow tree representation that we
proposed in (Severyn and Moschitti, 2012) as a
baseline structural model. More in detail, each Q
and its candidate AP are encoded into two trees,
where lemmas constitute the leaf level, the part-
of-speech (POS) tags are at the pre-terminal level
and the sequences of POS tags are organized into
the third level of chunk nodes. We encoded struc-
tural relations using the REL tag, which links the
related structures in Q/AP, when there is a match
3
http://uima.apache.org/
665
Figure 2: Basic structural representations using a shallow chunk tree structure for the Q/AP in the running example. Curved
line indicates the tree fragments in the question and its answer passage linked by the relational REL tag.
between the lemmas in Q and AP. We marked the
parent (POS tags) and grand parent (chunk) nodes
of such lemmas by prepending a REL tag.
However, more general semantic relations, e.g.,
derived from the question focus and category, can
be encoded using the REL-FOCUS-<QC> tag,
where <QC> stands for the question class. In
(Severyn et al., 2013b; Severyn et al., 2013a), we
used statistical classifiers to derive question focus
and categories of the question and of the named
entities in the AP. We again mark (i) the focus
chunk in the question and (ii) the AP chunks con-
taining named entities of type compatible with the
question class, by prepending the above tags to
their labels. The compatibility between the cat-
egories of named entities and questions is evalu-
ated with a lookup to a manually predefined map-
ping (see Table 1 in (Severyn et al., 2013b)). We
also prune the trees by removing the nodes beyond
a certain distance (in terms of chunk nodes) from
the REL and REL-FOCUS nodes. This removes
irrelevant information and speeds up learning and
classification. We showed that such model outper-
forms bag-of-words and POS-tag sequence mod-
els (Severyn et al., 2013a).
An example of a Q/AP pair encoded using shal-
low chunk trees is given in Figure 2. Here, for ex-
ample, the lemma ?drink? occurs in both Q and AP
(we highlighted it with a solid line box in the fig-
ure). ?Company? was correctly recognized as a fo-
cus
4
, however it was misclassified as ?HUMAN?
(?HUM?). As no entities of the matching type
?PERSON? were found in the answer by a NER
system, no chunks were marked as REL-FOCUS
on the answer passage side.
We slightly modify the REL-FOCUS encod-
ing into the tree. Instead of prepending REL-
FOCUS-<QC>, we only prepend REL-FOCUS
to the target chunk node, and add a new node
QC as the rightmost child of the chunk node, e.g,
in Figure 2, the focus node would be marked as
REL-FOCUS and the sequence of its children
would be [WP NN HUM]. This modification in-
4
We used the same approach to focus detection and ques-
tion classification used in (Severyn et al., 2013b)
tends to reduce the feature sparsity.
3 LOD for Semantic Structures
We aim at exploiting semantic resources for build-
ing more powerful rerankers. More specifically,
we use structured knowledge about properties of
the objects referred to in a Q/AP pair. A large
amount of knowledge has been made available as
LOD datasets, which can be used for finding addi-
tional semantic links between Q/AP passages.
In the next sections, we (i) formally define novel
semantic links between Q/AP structures that we
introduce in this paper; (ii) provide basic notions
of Linked Open Data along with three of its most
widely used datasets, YAGO, DBpedia and Word-
Net; and, finally, (iii) describe our algorithm to
generate linked Q/AP structures.
3.1 Matching Q/AP Structures: Type Match
We look for token sequences (e.g., complex nomi-
nal groups) in Q/AP pairs that refer to entities and
entity classes related by isa (Eq. 1) and isSubclas-
sOf (Eq. 2) relations and then link them in the
structural Q/AP representations.
isa : entity ? class? {true, false} (1)
isSubclassOf : class? class? {true, false} (2)
Here, entities are all the objects in the world
both real or abstract, while classes are sets of en-
tities that share some common features. Informa-
tion about entities, classes and their relations can
be obtained from the external knowledge sources
such as the LOD resources. isa returns true if an
entity is an element of a class (false otherwise),
while isSubclassOf(class1,class2) returns true if
all elements of class1 belong also to class2.
We refer to the token sequences introduced
above as to anchors and the entities/classes they
refer to as references. We define anchors to be in
a Type Match (TM) relation if the entities/classes
they refer to are in isa or isSubclassOf relation.
More formally, given two anchors a
1
and a
2
be-
longing to two text passages, p
1
and p
2
, respec-
tively, and given an R(a, p) function, which re-
turns a reference of an anchor a in passage p, we
define TM (r
1
, r
2
) as
666
{
isa (r
1
, r
2
) : if isEntity (r
1
) ? isClass (r
2
)
subClassOf (r
1
, r
2
) : if isClass (r
1
) ? isClass (r
2
)
(3)
where r
1
= R(a
1
, p
1
), r
2
= R(a
2
, p
2
) and isEn-
tity(r) and isClass(r) return true if r is an entity or
a class, respectively, and false otherwise. It should
be noted that, due to the ambiguity of natural lan-
guage, the same anchor may have different refer-
ences depending on the context.
3.2 LOD for linking Q/A structures
LOD consists of datasets published online accord-
ing to the Linked Data (LD) principles
5
and avail-
able in open access. LOD knowledge is repre-
sented following the Resource Description Frame-
work (RDF)
6
specification as a set of statements.
A statement is a subject-predicate-object triple,
where predicate denotes the directed relation, e.g.,
hasSurname or owns, between subject and object.
Each object described by RDF, e.g., a class or
an entity, is called a resource and is assigned a
Unique Resource Identifier (URI).
LOD includes a number of common schemas,
i.e., sets of classes and predicates to be reused
when describing knowledge. For example, one
of them is RDF Schema (RDFS)
7
, which contains
predicates rdf:type and rdfs:SubClassOf
similar to the isa and subClassOf functions above.
LOD contains a number of large-scale cross-
domain datasets, e.g., YAGO (Suchanek et al.,
2007) and DBpedia (Bizer et al., 2009). Datasets
created before the emergence of LD, e.g., Word-
Net, are brought into correspondence with the LD
principles and added to the LOD as well.
3.2.1 Algorithm for detecting TM
Algorithm 1 detects n-grams in the Q/AP struc-
tures that are in TM relation and encodes TM
knowledge in the shallow chunk tree representa-
tions of Q/AP pairs. It takes two text passages, P
1
and P
2
, and a LOD knowledge source, LOD
KS
,
as input. We run the algorithm twice, first with
AP as P
1
and Q as P
2
and then vice versa. For
example, P
1
and P
2
in the first run could be, ac-
cording to our running example, Q and AP candi-
date, respectively, and LOD
KS
could be YAGO,
DBpedia or WordNet.
Detecting anchors. getAnchors(P
2
,LOD
KS
)
in line 1 of Algorithm 1 returns all anchors in the
5
http://www.w3.org/DesignIssues/
LinkedData.html
6
http://www.w3.org/TR/rdf-concepts/
7
http://www.w3.org/TR/rdf-schema/
Algorithm 1 Type Match algorithm
Input: P
1
, P
2
- text passages; LOD
KS
- LOD knowledge
source.
1: for all anchor ? getAnchors(P
2
,LOD
KS
) do
2: for all uri ? getURIs(anchor,P
2
,LOD
KS
) do
3: for all type ? getTypes(uri,LOD
KS
) do
4: for all ch ? getChunks(P
1
) do
5: matchedTokens ? checkMatch(ch,
type.labels)
6: if matchedTokens 6= ? then
7: markAsTM(anchor,P
2
.parseTree)
8: markAsTM(matchedTokens,
P
1
.parseTree)
given text passage, P
2
. Depending on LOD
KS
one may have various implementations of this pro-
cedure. For example, when LOD
KS
is Word-
Net, getAnchor returns token subsequences of the
chunks in P
2
of lengths n-k, where n is the number
of tokens in the chunk and k = [1, .., n? 1).
In case when LOD
KS
is YAGO or DBpedia,
we benefit from the fact that both YAGO and DB-
pedia are aligned with Wikipedia on entity level by
construction and we can use the so-called wikifica-
tion tools, e.g., (Milne and Witten, 2009), to detect
the anchors. The wikification tools recognize n-
grams that may denote Wikipedia pages in plain
text and disambiguate them to obtain a unique
Wikipedia page. Such tools determine whether
a certain n-gram may denote a Wikipedia page(s)
by looking it up in a precomputed vocabulary cre-
ated using Wikipedia page titles and internal link
network (Csomai and Mihalcea, 2008; Milne and
Witten, 2009).
Obtaining references. In line 2 of Algorithm 1
for each anchor, we determine the URIs of enti-
ties/classes it refers to in LOD
KS
. Here again,
we have different strategies for different LOD
KS
.
In case of WordNet, we use the all-senses strat-
egy, i.e., getURI procedure returns a set of URIs
of synsets that contain the anchor lemma.
In case when LOD
KS
is YAGO or DBpedia,
we use wikification tools to correctly disambiguate
an anchor to a Wikipedia page. Then, Wikipedia
page URLs may be converted to DBpedia URIs by
substituting the en.wikipedia.org/wiki/
prefix to the dbpedia.org/resource/; and
YAGO URIs by querying it for subjects of the
RDF triples with yago:hasWikipediaUrl
8
as a predicate and Wikipedia URL as an object.
For instance, one of the anchors detected in
the running example AP would be ?Quaker oats?,
8
yago: is a shorthand for the http prefix http://
yago-knowledge.org/resource/
667
a wikification tool would map it to wiki:
Quaker_Oats_Company
9
, and the respective
YAGO URI would be yago:Quaker_Oats_
Company.
Obtaining type information. Given a uri, if it
is an entity, we look for all the classes it belongs
to, or if it is a class, we look for all classes for
which it is a subclass. This process is incorpo-
rated in the getTypes procedure in line 3 of Algo-
rithm 1. We call such classes types. If LOD
KS
is WordNet, then our types are simply the URIs of
the hypernyms of uri. If LOD
KS
is DBpedia or
YAGO, we query these datasets for the values of
the rdf:type and rdfs:subClassOf prop-
erties of the uri (i.e., objects of the triples with uri
as subject and type/subClassOf as predicates) and
add their values (which are also URIs) to the types
set. Then, we recursively repeat the same queries
for each retrieved type URI and add their results to
the types. Finally, the getTypes procedure returns
the resulting types set.
The extracted URIs returned by getTypes are
HTTP ids, however, frequently they have human-
readable names, or labels, specified by the rdfs:
label property. If no label information for a
URI is available, we can create the label by re-
moving the technical information from the type
URI, e.g., http prefix and underscores. type.labels
denotes a set of type human-readable labels for
a specific type. For example, one of the types
extracted for yago:Quaker_Oats_Company
would have label ?company?.
Checking for TM. Further, the checkMatch
procedure checks whether any of the labels in the
type.labels matches any of the chunks in P
1
re-
turned by getChunks, fully or partially (line 5 of
Algorithm 1). Here, getChunks procedure returns
a list of chunks recognized in P
1
by an external
chunker.
More specifically, given a chunk, ch, and a type
label, type.label, checkMatch checks whether the
ch string matches
10
type.label or its last word(s).
If no match is observed, we remove the first to-
ken from ch and repeat the procedure. We stop
when the match is observed or when no tokens
in ch are left. If the match is observed, check-
Match returns all the tokens remaining in ch as
matchedTokens. Otherwise, it returns an empty
set. For example, the question of the running ex-
9
wiki: is a shorthand for the http prefix http://en.
wikipedia.org/wiki/
10
case-insensitive exact string match
ample contains the chunk ?what company?, which
partially matches the human readable ?company?
label of one of the types retrieved for the ?Quaker
oats? anchor from the answer. Our implemen-
tation of the checkMatch procedure would re-
turn ?company? from the question as one of the
matchedTokens.
If the matchedTokens set is not empty,
this means that TM
(
R
(
anchor, P
2
)
, R
(
matchedTokens, P
1
))
in Eq. 3 returns true.
Indeed, a
1
is an anchor and a
2
is the matched-
Tokens sequence (see Eq. 3), and their respective
references, i.e., URI assigned to the anchor and
URI of one of its types, are either in subClassOf
or in isa relation by construction. Naturally, this
is only one of the possible ways to evaluate the
TM function, and it may be noise-prone.
Marking TM in tree structures. Finally,
if the TM match is observed, i.e., matchedTo-
kens is not an empty set, we mark tree substruc-
tures corresponding to the anchor in the struc-
tural representation of P
2
(P
2
.parseTree) and
those corresponding to matchedTokens in that of
P
1
(P
1
.parseTree) as being in a TM relation. In
our running example, we would mark the substruc-
tures corresponding to ?Quaker oats? anchor in the
answer (our P
2
) and the ?company? matchedTo-
ken in the question (our P
1
) shallow syntactic tree
representations. We can encode TM match infor-
mation into a tree in a variety of ways, which we
describe below.
3.2.2 Encoding TM knowledge in the trees
a
1
and a
2
from Eq. 3 are n-grams, therefore they
correspond to the leaf nodes in the shallow syn-
tactic trees of p
1
and p
2
. We denote the set of
their preterminal parents as N
TM
. We consid-
ered the following strategies of encoding TM re-
lation in the trees: (i) TM node (TM
N
). Add leaf
sibling tagged with TM to all the nodes in N
TM
.
(ii) Directed TM node (TM
ND
). Add leaf sib-
ling tagged with TM-CHILD to all the nodes in
N
TM
corresponding to the anchor, and leaf sib-
lings tagged with TM-PARENT to the nodes cor-
responding to matchedTokens. (iii) Focus TM
(TM
NF
). Add leaf siblings to all the nodes in
N
TM
. If matchedTokens is a part of a question
focus label them as TM-FOCUS. Otherwise, la-
bel them as TM. (iv) Combo TM
NDF
. Encode
using the TM
ND
strategy. If matchedTokens is a
part of a question focus label then also add a child
labeled FOCUS to each of the TM labels. Intu-
668
Figure 3: Fragments of a shallow chunk parse tree anno-
tated in TM
ND
mode.
itively, TM
ND
, TM
NF
, TM
NDF
are likely to re-
sult in more expressive patterns. Fig. 3 shows an
example of the TM
ND
annotation.
3.3 Wikipedia-based matching
Lemma matching for detecting REL may result in
low coverage, e.g., it is not able to match differ-
ent variants for the same name. We remedy this
by using Wikipedia link annotation. We consider
two word sequences (in Q and AP, respectively)
that are annotated with the same Wikipedia link
to be in a matching relation. Thus, we add new
REL tags to Q/AP structural representations as de-
scribed in Sec. 2.2.
4 Experiments
We evaluated our different rerankers encoding sev-
eral semantic structures on passage retrieval task,
using a factoid open-domain TREC QA corpus.
4.1 Experimental Setup
TREC QA 2002/2003. In our experiments, we
opted for questions from years 2002 and 2003,
which totals to 824 factoid questions. The
AQUAINT corpus
11
is used for searching the sup-
porting passages.
Pruning. Following (Severyn and Moschitti,
2012) we prune the shallow trees by removing the
nodes beyond distance of 2 from the REL, REL-
FOCUS or TM nodes.
LOD datasets. We used the core RDF distribu-
tion of YAGO2
12
, WordNet 3.0 in RDF
13
, and the
datasets from the 3.9 DBpedia distribution
14
.
Feature Vectors. We used a subset of the sim-
ilarity functions between Q and AP described in
(Severyn et al., 2013b). These are used along
with the structural models. More explicitly: Term-
overlap features: i.e., a cosine similarity over
question/answer, sim
COS
(Q,AP ), where the in-
put vectors are composed of lemma or POS-tag
11
http://catalog.ldc.upenn.edu/
LDC2002T31
12
http://www.mpi-inf.mpg.de/yago-naga/
yago1_yago2/download/yago2/yago2core_
20120109.rdfs.7z
13
http://semanticweb.cs.vu.nl/lod/wn30/
14
http://dbpedia.org/Downloads39
n-grams with n = 1, .., 4. PTK score: i.e., out-
put of the Partial Tree Kernel (PTK), defined in
(Moschitti, 2006), when applied to the structural
representations of Q and AP, sim
PTK
(Q,AP ) =
PTK(Q,AP ) (note that, this is computed within
a pair). PTK defines similarity in terms of the
number of substructures shared by two trees.
Search engine ranking score: the ranking score of
our search engine assigned to AP divided by a nor-
malizing factor.
SVM re-ranker. To train our models, we use
SVM-light-TK
15
, which enables the use of struc-
tural kernels (Moschitti, 2006) in SVM-light
(Joachims, 2002). We use default parameters and
the preference reranking model described in (Sev-
eryn and Moschitti, 2012; Severyn et al., 2013b).
We used PTK and the polynomial kernel of degree
3 on standard features.
Pipeline. We built the entire processing pipeline
on top of the UIMA framework.We included many
off-the-shelf NLP tools wrapping them as UIMA
annotators to perform sentence detection, tok-
enization, NE Recognition, parsing, chunking and
lemmatization. Moreover, we used annotators
for building new sentence representations starting
from tools? annotations and classifiers for question
focus and question class.
Search engines. We adopted Terrier
16
using the
accurate BM25 scoring model with default param-
eters. We trained it on the TREC corpus (3Gb),
containing about 1 million documents. We per-
formed indexing at the paragraph level by splitting
each document into a set of paragraphs, which are
then added to the search index. We retrieve a list of
50 candidate answer passages for each question.
Wikipedia link annotators. We use the
Wikipedia Miner (WM) (Milne and Witten,
2009)
17
tool and the Machine Linking (ML)
18
web-service to annotate Q/AP pairs with links to
Wikipedia. Both tools output annotation confi-
dence. We use all WM and ML annotations with
confidence exceeding 0.2 and 0.05, respectively.
We obtained these figures heuristically, they are
low because we aimed to maximize the Recall of
the Wikipedia link annotators in order to maxi-
15
http://disi.unitn.it/moschitti/
Tree-Kernel.htm
16
http://terrier.org
17
http://sourceforge.net/projects/
wikipedia-miner/files/wikipedia-miner/
wikipedia-miner_1.1, we use only topic detector
module which detects and disambiguates anchors
18
http://www.machinelinking.com/wp
669
System MRR MAP P@1
BM25 28.02?2.94 0.22?0.02 18.17?3.79
CH+V (CoNLL, 2013) 37.45 0.3 27.91
CH+V+QC+TFC
(CoNLL, 2013)
39.49 0.32 30
CH + V 36.82?2.68 0.30?0.02 26.34?2.17
CH + V+ QC+TFC 40.20?1.84 0.33?0.01 30.85?2.35
CH+V+QC+TFC* 40.50?2.32 0.33?0.02 31.46?2.42
Table 1: Baseline systems
mize the number of TMs. In all the experiments,
we used a union of the sets of the annotations pro-
vided by WM and ML.
Metrics. We used common QA metrics: Precision
at rank 1 (P@1), i.e., the percentage of questions
with a correct answer ranked at the first position,
and Mean Reciprocal Rank (MRR). We also report
the Mean Average Precision (MAP). We perform
5-fold cross-validation and report the metrics aver-
aged across all the folds together with the std.dev.
4.2 Baseline Structural Reranking
In these experiments, we evaluated the accuracy
of the following baseline models: BM25 is the
BM25 scoring model, which also provides the ini-
tial ranking; CH+V is a combination of tree struc-
tures encoding Q/AP pairs using relational links
with the feature vector; and CH+V+QC+TFC is
CH+V extended with the semantic categorial links
introduced in (Severyn et al., 2013b).
Table 1 reports the performance of our base-
line systems. The lines marked with (CoNLL,
2013) contain the results we reported in (Sev-
eryn et al., 2013b). Lines four and five report
the performance of the same systems, i.e., CH+V
and CH+V+QC+TFC, after small improvement
and changes. Note that in our last version, we
have a different set of V features than in (CoNLL,
2013). Finally, CH+V+QC+TFC* refers to the
performance of CH+V+QC+TFC with question
type information of semantic REL-FOCUS links
represented as a distinct node (see Section 2.2).
The results show that this modification yields a
slight improvement over the baseline, thus, in
the next experiments, we add LOD knowledge to
CH+V+QC+TFC*.
4.3 Impact of LOD in Semantic Structures
These experiments evaluated the accuracy of the
following models (described in the previous sec-
tions): (i) a system using Wikipedia to establish
the REL links; and (ii) systems which use LOD
knowledge to find type matches (TM).
The first header line of the Table 2 shows which
baseline system was enriched with the TM knowl-
edge. Type column reports the TM encoding strat-
egy employed (see Section 3.2.2). Dataset column
reports which knowledge source was employed to
find TM relations. Here, yago is YAGO2, db is
DBpedia, and wn is WordNet 3.0. The first re-
sult line in Table 2 reports the performance of
the strong CH+V and CH+V+QC+TFC* base-
line systems. Line with the ?wiki? dataset re-
ports on CH+V and CH+V+QC+TFC* using
both Wikipedia link annotations provided by ML
and MW and hard lemma matching to find the re-
lated structures to be marked by REL (see Sec-
tion 3.3 for details of the Wikipedia-based REL
matching). The remainder of the systems is built
on top of the baselines using both hard lemma and
Wikipedia-based matching. We used bold font to
mark the top scores for each encoding strategy.
The tables show that all the systems ex-
ploiting LOD knowledge, excluding those us-
ing DBpedia only, outperform the strong CH+V
and CH+V+QC+TFC* baselines. Note that
CH+V enriched with TM tags performs com-
parably to, and in some cases even outper-
forms, CH+V+QC+TFC*. Compare, for exam-
ple, the outputs of CH+V+TM
NDF
using YAGO,
WordNet and DBpedia knowledge and those of
CH+V+QC+TFC* with no LOD knowledge.
Adding TM tags to the top-performing base-
line system, CH+V+QC+TFC*, typically re-
sults in further increase in performance. The
best-performing system in terms of MRR and
P@1 is CH+V+QC+TFC*+TM
NF
system us-
ing the combination of WordNet and YAGO2 as
source of TM knowledge and Wikipedia for REL-
matching. It outperforms the CH+V+QC+TFC*
baseline by 3.82% and 4.15% in terms of MRR
and P@1, respectively. Regarding MAP, a num-
ber of systems employing YAGO2 in combina-
tion with WordNet and Wikipedia-based REL-
matching obtain 0.37 MAP score thus outperform-
ing the CH+V+QC+TFC* baseline by 4%.
We used paired two-tailed t-test for evaluating
the statistical significance of the results reported in
Table 2. ? and ? correspond to the significance lev-
els of 0.05 and 0.1, respectively. We compared (i)
the results in the wiki line to those in the none line;
and (ii) the results for the TM systems to those in
the wiki line.
The table shows that we typically obtain bet-
ter results when using YAGO2 and/or WordNet.
In our intuition this is due to the fact that these
resources are large-scale, have fine-grained class
670
Type Dataset CH + V CH + V + QC + TFC*
MRR MAP P@1 MRR MAP P@1
- none 36.82?2.68 0.30?0.02 26.34?2.17 40.50?2.32 0.33?0.02 31.46?2.42
- wiki 39.17?1.29? 0.31?0.01? 28.66?1.43? 41.33?1.17 0.34?0.01 31.46?1.40
TM
N
db 40.60?1.88 0.33?0.01? 31.10?2.99? 40.80?1.01 0.34?0.01 30.37?1.90
TM
N
wn 41.39?1.96? 0.33?0.01? 31.34?2.94 42.43?0.56 0.35?0.01 32.80?0.67
TM
N
wn+db 40.85?1.52? 0.33?0.01? 30.37?2.34 42.37?1.12 0.35?0.01 32.44?2.64
TM
N
yago 40.71?2.07 0.33?0.03? 30.24?2.09? 43.28?1.91? 0.36?0.01? 33.90?2.75
TM
N
yago+db 41.25?1.57? 0.34?0.02? 31.10?1.88? 42.39?1.83 0.35?0.01 32.93?3.14
TM
N
yago+wn 42.01?2.26? 0.34?0.02? 32.07?3.04? 43.98?1.08? 0.36?0.01? 35.24?1.46?
TM
N
yago+wn+db 41.52?1.85? 0.34?0.02? 30.98?2.71? 43.13?1.38 0.36?0.01 33.66?2.77
TM
NF
db 40.67?1.94? 0.33?0.01? 30.85?2.22? 41.43?0.70 0.35?0.01 31.22?1.09
TM
NF
wn 40.95?2.27? 0.33?0.01? 30.98?3.74 42.37?0.98 0.35?0.01 32.56?1.76
TM
NF
wn+db 40.84?2.18? 0.34?0.01? 30.73?3.04 43.08?0.83? 0.36?0.01? 33.54?1.29?
TM
NF
yago 42.01?2.44? 0.34?0.02? 32.07?3.01? 43.82?2.36? 0.36?0.02? 34.88?3.35
TM
NF
yago+db 41.32?1.70? 0.34?0.02? 31.10?2.48? 43.19?1.17? 0.36?0.01? 33.90?1.86
TM
NF
yago+wn 41.69?1.66? 0.34?0.02? 31.10?2.44? 44.32?0.70? 0.36?0.01? 35.61?1.11?
TM
NF
yago+wn+db 41.56?1.41? 0.34?0.02? 30.85?2.22? 43.79?0.73? 0.37?0.01? 34.88?1.69?
TM
ND
db 40.37?1.87 0.33?0.01? 30.37?2.17 41.58?1.02 0.35?0.01? 31.46?1.59
TM
ND
wn 41.13?2.14? 0.33?0.01? 30.73?2.75 42.19?1.39 0.35?0.01 32.32?1.36
TM
ND
wn+db 41.28?1.03? 0.34?0.01? 30.73?0.82? 42.37?1.16 0.36?0.01 32.44?2.71
TM
ND
yago 42.11?3.24? 0.34?0.02? 32.07?4.06? 44.04?2.05? 0.36?0.01? 34.63?2.17?
TM
ND
yago+db 42.28?2.01? 0.35?0.01? 32.44?1.99? 43.77?2.02? 0.37?0.01? 34.27?2.42
TM
ND
yago+wn 42.96?1.45? 0.35?0.01? 33.05?2.04? 44.25?1.32? 0.37?0.00? 34.76?1.61?
TM
ND
yago+wn+db 42.56?1.25? 0.35?0.01? 32.56?1.91? 43.91?1.01? 0.37?0.01? 34.63?1.32?
TM
NDF
db 40.40?1.93? 0.33?0.01? 30.49?1.78? 41.85?1.05 0.35?0.01? 31.83?0.80
TM
NDF
wn 40.84?1.69? 0.33?0.01? 30.49?2.24 41.89?0.99 0.35?0.01 31.71?0.86
TM
NDF
wn+db 41.14?1.29? 0.34?0.01? 30.73?1.40? 42.31?0.92 0.36?0.01 32.32?2.36
TM
NDF
yago 42.31?2.57? 0.35?0.02? 32.68?3.01? 44.22?2.38? 0.37?0.02? 35.00?2.88?
TM
NDF
yago+db 41.96?1.82? 0.35?0.01? 32.32?2.24? 43.82?1.95? 0.37?0.01? 34.51?2.39?
TM
NDF
yago+wn 42.80?1.19? 0.35?0.01? 33.17?1.86? 43.91?0.98? 0.37?0.01? 34.63?0.90?
TM
NDF
yago+wn+db 43.15?0.93? 0.35?0.01? 33.78?1.59? 43.96?0.94? 0.37?0.01? 34.88?1.69?
Table 2: Results in 5-fold cross-validation on TREC QA corpus
taxonomy and contain many synonymous labels
per class/entity thus allowing us to have a good
coverage with TM-links. DBpedia ontology that
we employed in the db experiments is more shal-
low and contains fewer labels for classes, there-
fore the amount of discovered TM matches is
not always sufficient for increasing performance.
YAGO2 provides better coverage for TM relations
between entities and their classes, while Word-
Net contains more relations between classes
19
.
Note that in (Severyn and Moschitti, 2012), we
also used supersenses of WordNet (unsuccess-
fully) whereas here we use hypernymy relations
and a different technique to incorporate semantic
match into the tree structures.
Different TM-knowledge encoding strategies,
TM
N
, TM
ND
, TM
NF
, TM
NDF
produce small
changes in accuracy. We believe, that the differ-
ence between them would become more signifi-
cant when experimenting with larger corpora.
5 Conclusions
This paper proposes syntactic structures whose
nodes are enriched with semantic information
from statistical classifiers and knowledge from
LOD. In particular, YAGO, DBpedia and Word-
Net are used to match and generalize constituents
from QA pairs: such matches are then used in
19
We consider the WordNet synsets to be classes in the
scope of our experiments
syntactic/semantic structures. The experiments
with TREC QA and the above representations
also combined with traditional features greatly im-
prove over a strong IR baseline, e.g., 96% on
BM25, and on previous state-of-the-art rerank-
ing models, up to 15.4% (relative improvement)
in P@1. In particular, differently from previous
work, our models can effectively use semantic
knowledge in statistical learning to rank methods.
These promising results open interesting future
directions in designing novel semantic structures
and using innovative semantic representations in
learning algorithms.
Acknowledgments
This research is partially supported by the EU?s 7
th
Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and by a Shared University
Research award from the IBM Watson Research
Center - Yorktown Heights, USA and the IBM
Center for Advanced Studies of Trento, Italy. The
third author is supported by the Google Europe
Fellowship 2013 award in Machine Learning.
References
Matthew W. Bilotti, Jonathan L. Elsas, Jaime Car-
bonell, and Eric Nyberg. 2010. Rank learning
for factoid question answering with linguistic and
semantic constraints. In Proceedings of the 19th
671
ACM international Conference on Information and
Knowledge Management (CIKM), pages 459?468.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. Dbpedia - a crys-
tallization point for the web of data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, 7(3):154?165, September.
Andras Csomai and Rada Mihalcea. 2008. Linking
documents to encyclopedic knowledge. IEEE Intel-
ligent Systems, 23(5):34?41.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010.
Building watson: An overview of the deepqa
project. AI Magazine, 31(3).
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 133?142. ACM.
Aditya Kalyanpur, J William Murdock, James Fan, and
Christopher Welty. 2011. Leveraging community-
built knowledge for type coercion in question an-
swering. In The Semantic Web?ISWC 2011, pages
144?156. Springer.
David Milne and Ian H Witten. 2009. An open-source
toolkit for mining wikipedia. In New Zealand Com-
puter Science Research Student Conference (NZC-
SRSC).
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics on Human Language Technologies: Short Pa-
pers (ACL), pages 113?116.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion/answer classification. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 776?783.
Alessandro Moschitti. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In Proceedings of the 17th European Confer-
ence on Machine Learning (ECML), pages 318?329.
Springer.
Alessandro Moschitti. 2009. Syntactic and seman-
tic kernels for short text pair categorization. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 576?584. Association for
Computational Linguistics.
J William Murdock, Aditya Kalyanpur, Chris Welty,
James Fan, David A Ferrucci, DC Gondek, Lei
Zhang, and Hiroshi Kanayama. 2012. Typing can-
didate answers using type coercion. IBM Journal of
Research and Development, 56(3.4):7?1.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th in-
ternational ACM SIGIR conference on Research and
development in information retrieval (SIGIR), pages
741?750. ACM.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 458?467.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from clas-
sifiers for passage reranking. In Proceedings of the
22nd ACM international conference on Conference
on information & knowledge management (CIKM),
pages 969?978. ACM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL).
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web (WWW), pages 697?706.
ACM Press.
Ellen M Voorhees. 2001. Overview of the TREC
2001 Question Answering Track. In Proceedings of
TREC, pages 42?51.
672
Relational Features in Fine-Grained
Opinion Analysis
Richard Johansson?
University of Gothenburg
Alessandro Moschitti??
University of Trento
Fine-grained opinion analysis methods often make use of linguistic features but typically do
not take the interaction between opinions into account. This article describes a set of experiments
that demonstrate that relational features, mainly derived from dependency-syntactic and se-
mantic role structures, can significantly improve the performance of automatic systems for a
number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion
holders, and determining the polarities of opinion expressions. These features make it possible
to model the way opinions expressed in natural-language discourse interact in a sentence over
arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously,
which makes the search for the optimal analysis intractable. However, a reranker can be used as
a sufficiently accurate and efficient approximation.
A number of feature sets and machine learning approaches for the rerankers are evaluated.
For the task of opinion expression extraction, the best model shows a 10-point absolute improve-
ment in soft recall on the MPQA corpus over a conventional sequence labeler based on local
contextual features, while precision decreases only slightly. Significant improvements are also
seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,
respectively. In addition, the systems outperform previously published results for unlabeled (6
F-measure points) and polarity-labeled (10?15 points) opinion expression extraction. Finally,
as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical
opinion mining tasks. In all scenarios considered, the machine learning features derived from the
opinion expressions lead to statistically significant improvements.
1. Introduction
Automatic methods for the analysis of opinions (textual expressions of emotions, be-
liefs, and evaluations) have attracted considerable attention in the natural language
? Spra?kbanken, Department of Swedish, University of Gothenburg, Box 100, SE-40530 Gothenburg,
Sweden. E-mail: richard.johansson@gu.se. The work described here was mainly carried out at the
University of Trento.
?? DISI ? Department of Information Engineering and Computer Science, University of Trento,
Via Sommarive 14, 38123 Trento (TN), Italy. E-mail: moschitti@disi.unitn.it.
Submission received: 11 January 2012; revised version received: 11 May 2012; accepted for publication:
11 June 2012.
doi:10.1162/COLI a 00141
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
processing community in recent years (Pang and Lee 2008). Apart from their interest
from a linguistic and psychological point of view, the technologies emerging from this
research have obvious practical uses, either as stand-alone applications or supporting
other tools such as information retrieval or question answering systems.
The research community initially focused on high-level tasks such as retrieving doc-
uments or passages expressing opinion, or classifying the polarity of a given text, and
these coarse-grained problem formulations naturally led to the application of methods
derived from standard retrieval or text categorization techniques. The models under-
lying these approaches have used very simple feature representations such as purely
lexical (Pang, Lee, and Vaithyanathan 2002; Yu and Hatzivassiloglou 2003) or low-level
grammatical features such as part-of-speech tags and functional words (Wiebe, Bruce,
and O?Hara 1999). This is in line with the general consensus in the information retrieval
community that very little can be gained by complex linguistic processing for tasks such
as text categorization and search (Moschitti and Basili 2004). There are a few exceptions,
such as Karlgren et al (2010), who showed that construction features added to a bag-of-
words representation resulted in improved performance on a number of coarse-grained
opinion analysis tasks. Similarly, Greene and Resnik (2009) argued that a speaker?s
attitude can be predicted from syntactic features such as the selection of a transitive
or intransitive verb frame.
In contrast to the early work, recent years have seen a shift towards more detailed
problem formulations where the task is not only to find a piece of opinionated text,
but also to extract a structured representation of the opinion. For instance, we may
determine the person holding the opinion (the holder) and towards which entity or fact
it is directed (the topic), whether it is positive or negative (the polarity), and the strength
of the opinion (the intensity). The increasing complexity of representation leads us
from retrieval and categorization deep into natural language processing territory; the
methods used here have been inspired by information extraction and semantic role
labeling, combinatorial optimization, and structured machine learning. For such tasks,
deeper representations of linguistic structure have seen more use than in the coarse-
grained case. Syntactic and shallow-semantic relations have repeatedly proven useful
for subtasks of opinion analysis that are relational in nature, above all for determining
the holder or topic of a given opinion, in which case there is considerable similarity to
tasks such as semantic role labeling (Kim and Hovy 2006; Ruppenhofer, Somasundaran,
and Wiebe 2008).
There has been no systematic research, however, on the role played by linguistic
structure in the relations between opinions expressed in text, despite the fact that the
opinion expressions in a sentence are not independent but organized rhetorically to
achieve a communicative effect intended by the speaker. We therefore expect that the
interplay between opinion expressions can be exploited to derive information useful for
the analysis of opinions expressed in text. In this article, we start from this intuition and
propose several novel features derived from the interdependencies between opinion
expressions on the syntactic and shallow-semantic levels.
Based on these features, we devised structured prediction models for (1) extraction
of opinion expressions, (2) joint expression extraction and holder extraction, and (3) joint
expression extraction and polarity labeling. The models were trained using a number
of discriminative machine learning methods. Because the interdependency features
required us to consider more than one opinion expression at a time, the inference steps
carried out at training and prediction time could not rely on commonly used opinion
expression mark-up methods based on Viterbi search, but we show that an approximate
search method using reranking suffices for this purpose: In a first step a base system
474
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
using local features and efficient search generates a small set of hypotheses, and in a
second step a classifier using the complex features selects the final output from the
hypothesis set. This approach allows us to make use of arbitrary features extracted from
the complete set of opinion expressions in a sentence, without having to impose any
restriction on the expressivity of the features. An additional advantage is that it is fairly
easy to implement as long as the underlying system is able to generate k-best output.
The interaction-based reranking systems were evaluated on a test set extracted
from the MPQA corpus, and compared to strong baselines consisting of stand-alone
systems for opinion expression mark-up, opinion holder extraction, and polarity classi-
fication. Our evaluations showed that (1) the best opinion expression mark-up system
we evaluated achieved a 10-point absolute improvement in soft recall, and a 5-point im-
provement in F-measure, over the baseline sequence labeler. Our system outper-
formed previously described opinion expression mark-up tools by six points in overlap
F-measure. (2) The recall was boosted by almost 10 points for the holder extraction task
(over three points in F-measure) by modeling the interaction of opinion expressions
with respect to holders. (3) We saw an improvement for the extraction of polarity-
labeled expression of four F-measure points. Our result for opinion extraction and po-
larity labeling is especially striking when compared with the best previously published
end-to-end system for this task: 10?15 points in F-measure improvement. In addition to
the performance evaluations, we studied the impact of features on the subtasks, and the
effect of the choice of the machine learning method for training the reranker.
As a final extrinsic evaluation of the system, we evaluated the usefulness of its
output in a number of applications. Although there have been several publications
detailing the extraction of MPQA-style opinion expressions, as far as we are aware there
has been no attempt to use them in an application. In contrast, we show that the opinion
expressions as defined by the MPQA corpus may be used to derive machine learning
features that are useful in two practical opinion mining tasks; the addition of these
features leads to statistically significant improvements in all scenarios we evaluated.
First, we develop a system for the extraction of evaluations of product attributes
from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and
McDonald 2008), and we show that the features derived from opinion expressions lead
to significant improvement. Secondly, we show that fine-grained opinion structural
information can even be used to build features that improve a coarse-grained sentiment
task: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002;
Pang and Lee 2004).
After the present introduction, Section 2 gives a linguistic motivation and an
overview of the related work; Section 3 describes the MPQA opinion corpus and its
underlying representation; Section 4 illustrates the baseline systems: a sequence labeler
for the extraction of opinion expressions and classifiers for opinion holder extraction
and polarity labeling; Section 5 reports on the main contribution: the description of the
interaction models and their features; finally, Section 7 presents the experimental results
and Section 8 derives the conclusions.
2. Motivation and Related Work
Intuitively, interdependency features could be useful in the process of locating and
disambiguating expressions of opinion. These expressions tend to occur in patterns, and
the presence of one opinionated piece of text may influence our interpretation of another
as opinionated or not. Consider, for instance, the word said in sentences (a) and (b) in
Example (1), where the presence or non-presence of emotionally loaded words in the
475
Computational Linguistics Volume 39, Number 3
complement clause provides evidence for the interpretation as a subjective opinion or
an objective speech event. (In the example, opinionated expressions are marked S for
subjective and the non-opinionated speech event O for objective.)
Example (1)
(a) ?We will identify the [culprits]S of these clashes and [punish]S them,? he [said]S.
(b) On Monday, 80 Libyan soldiers disembarked from an Antonov transport plane
carrying military equipment, an African diplomat [said]O.
Moreover, opinions expressed in a sentence are interdependent when it comes to the
resolution of their holders?the person or entity having the attitude expressed in the
opinion expression. Clearly, the structure of the sentence is also influential for this task
because certain linguistic constructions provide evidence for opinion holder correlation.
In the most obvious case, shown in the two sentences in Example (2), pejorative words
share the opinion holder with the communication and categorization verbs dominating
them. (Here, opinions are marked S and holders H.)
Example (2)
(a) [Domestic observers]H [tended to side with]S the MDC, [denouncing]S the election
as [fraud-tainted]S and [unfair]S.
(b) [Bush]H [labeled]S North Korea, Iran and Iraq an ?[axis of evil]S.?
In addition, interaction is important when determining opinion polarity. Here, relations
that influence polarity interpretation include coordination, verb?complement, as well as
other types of discourse relations. In particular, the presence of a COMPARISON discourse
relation, such as contrast or concession (Prasad et al 2008), may allow us to infer that
opinion expressions have different polarities. In Example (3), we see how contrastive
discourse connectives (underlined) are used when there are contrasting polarities in the
surrounding opinion expressions. (Positive opinions are tagged ?+?, negative ?-?.)
Example (3)
(a) ?[This is no blind violence but rather targeted violence]-,? Annemie Neyts [said]-.
?However, the movement [is more than that]+.?
(b) ?[Trade alone will not save the world]-,? Neyts [said]-, but it constitutes an
[important]+ factor for economic development.
The problems we focus on in this article?extracting opinion expressions with holders
and polarity labeling?have naturally been studied previously, especially since the
release of the MPQA corpus (Wiebe, Wilson, and Cardie 2005). For the first subtask,
because the MPQA corpus uses span-based annotation to represent opinions, it is
natural to apply straightforward sequence labeling techniques to extract them. This idea
has resulted in a number of publications (Choi, Breck, and Cardie 2006; Breck, Choi, and
Cardie 2007). Such systems do not use any features describing the interaction between
opinions, and it would not be possible to add interaction features because a Viterbi-
based sequence labeler by construction is restricted to using local features in a small
contextual window.
Works using syntactic features to extract topics and holders of opinions are numer-
ous (Bethard et al 2005; Kobayashi, Inui, and Matsumoto 2007; Joshi and Penstein-Rose?
476
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
2009; Wu et al 2009). Semantic role analysis has also proven useful: Kim and Hovy
(2006) used a FrameNet-based semantic role labeler to determine holder and topic of
opinions. Similarly, Choi, Breck, and Cardie (2006) successfully used a PropBank-based
role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently ap-
plied tree kernel learning methods on a combination of syntactic and semantic role trees
for extracting holders, but did not consider their relations to the opinion expressions.
Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques
are useful but not completely sufficient for holder and topic identification, and that
other linguistic phenomena must be studied as well. Choi, Breck, and Cardie (2006)
built a joint model of opinion expression extraction and holder extraction and applied
integer linear programming to carry out the optimization step.
While the tasks of opinion expression detection and polarity classification of opin-
ion expressions (Wilson, Wiebe, and Hoffmann 2009) have mostly been studied in
isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously
extracted opinion expressions and assigned them polarity values and this is so far the
only published result on joint opinion segmentation and polarity classification. Their
experiment, however, lacked the obvious baseline: a standard pipeline consisting of an
expression tagger followed by a polarity classifier. In addition, although their model
is the first end-to-end system for opinion expression extraction and polarity classifica-
tion, it is still based on sequence labeling and thus by construction limited in feature
expressivity.
On a conceptual level, discourse-oriented approaches (Asher, Benamara, and
Mathieu 2009; Somasundaran et al 2009; Zirn et al 2011) applying interaction features
for polarity classification are arguably the most related because they are driven by
a vision similar to ours: Individual opinion expressions interplay in discourse and
thus provide information about each other. On a practical level there are obvious
differences, since our features are extracted from syntactic and shallow-semantic
linguistic representations, which we argue are reflections of discourse structure, while
they extract features directly from a discourse representation. It is doubtful whether
automatic discourse representation extraction in text is currently mature enough to
provide informative features, whereas syntactic parsing and shallow-semantic analysis
are today fairly reliable. Another related line of work is represented by Choi and Cardie
(2008), where interaction features based on compositional semantics were used in a joint
model for the assignment of polarity values to pre-segmented opinion expressions in a
sentence.
3. The MPQA Corpus and its Annotation of Opinion Expressions
The most detailed linguistic resource useful for developing automatic systems for opin-
ion analysis is theMPQA corpus (Wiebe,Wilson, and Cardie 2005). In this article, we use
the word opinion in its broadest sense, equivalent to the word private state used by the
MPQA annotators (page 2): ?opinions, emotions, sentiments, speculations, evaluations,
and other private states (Quirk et al 1985), i.e., internal states that cannot be directly
observed by others.?
The central building block in the MPQA annotation is the opinion expression
(or subjective expression): A text piece that expresses a private state, allowing us to
draw the conclusion that someone has a particular emotion or belief about some topic.
Identifying these units allows us to carry out further analysis, such as the determination
of opinion holder and the polarity of the opinion. The annotation scheme defines two
types of opinion expressions: direct subjective expressions (DSEs), which are explicit
477
Computational Linguistics Volume 39, Number 3
mentions of attitude or evaluation, and expressive subjective elements (ESEs), which
signal the attitude of the speaker by the choice of words. The prototypical example of
a DSE would be a verb of statement, feeling, or categorization such as praise or disgust.
ESEs, on the other hand, are less easy to categorize syntactically; prototypical examples
would include simple value-expressing adjectives such as beautiful and strongly charged
words like appeasement or big government. In addition to DSEs and ESEs, the corpus also
contains annotation for non-subjective statements, which are referred to as objective
speech events (OSEs). Some words such as say may appear as DSEs or OSEs depending
on the context, so for an automatic system there is a need for disambiguation.
Example (4) shows a number of sentences from the MPQA corpus where DSEs and
ESEs have been manually annotated.
Example (4)
(a) He [made such charges]DSE [despite the fact]ESE that women?s political, social, and
cultural participation is [not less than that]ESE of men.
(b) [However]ESE, it is becoming [rather fashionable]ESE to [exchange harsh words]DSE
with each other [like kids]ESE.
(c) For instance, he [denounced]DSE as a [human rights violation]ESE the banning and
seizure of satellite dishes in Iran.
(d) This [is viewed]DSE as the [main impediment]ESE to the establishment of political
order in the country.
Every expression in the corpus is connected to an opinion holder,1 an entity that
experiences the sentiment or utters the evaluation that appears textually in the opinion
expression. For DSEs, it is often fairly straightforward to find the opinion holders
because they tend to be realized as direct semantic arguments filling semantic roles
such as SPEAKER or EXPERIENCER?and the DSEs tend to be verbs or nominalizations.
For ESEs, the connection between the expression and the opinion holder is typically
less clear-cut than for DSEs; the holder is more frequently implicit or located outside
the sentence for ESEs than for DSEs.
The MPQA corpus does not annotate links directly from opinion expressions to
particular mentions of a holder entity. Instead, the opinions are connected to holder
coreference chains that may span the whole text. Some opinion expressions are linked
to entities that are not explicitly mentioned in the text. If this entity is the author of the
text, it is called the writer, otherwise implicit. Example (5) shows sentences annotated
with expressions and holders.
Example (5)
(a) For instance, [he]H1 [denounced]DSE/H1 as a [human rights violation]ESE/H1 the
banning and seizure of satellite dishes in Iran.
(b) [(writer)]H1: [He]H2 [made such charges]DSE/H2 [despite the fact]ESE/H1 that
women?s political, social, and cultural participation is [not less than that]ESE/H1 of men.
(c) [(implicit)]H1: This [is viewed]DSE/H1 as the [main impediment]ESE/H1 to the estab-
lishment of political order in the country.
1 The MPQA uses the term source but we prefer the term holder because it seems to be more common.
478
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Finally, MPQA associates opinion expressions (DSEs and ESEs, but not OSEs) with
a polarity feature taking the values POSITIVE, NEGATIVE, NEUTRAL, and BOTH, and
with an intensity feature taking the values LOW, MEDIUM, HIGH, and EXTREME. The
two sentences in Example (6) from the MPQA corpus show opinion expressions with
polarities. Positive polarity is represented with a ?+? and negative with a ?-?.
Example (6)
(a) We foresaw electoral [fraud]- but not [daylight robbery]-.
(b) Join in this [wonderful]+ event and help Jameson Camp continue to provide the
year-round support that gives kids a [chance to create dreams]+.
The corpus does not currently contain annotation of topics (evaluees) of opinions,
although there have been efforts to add this separately (Stoyanov and Cardie 2008).
4. Baseline Systems for Fine-Grained Opinion Analysis
The assessment of our reranking-based systems requires us to compare against strong
baselines. We developed (1) a sequence labeler for opinion expression extraction similar
to that by Breck, Choi, and Cardie (2007), (2) a set of classifiers to determine the
opinion holder, and (3) a multiclass classifier that assigns polarity to a given opinion
expression similar to that described by Wilson, Wiebe, and Hoffmann (2009). These
tools were also used to generate the hypothesis sets for the rerankers described in
Section 5.
4.1 Sequence Labeler for Opinion Expression Mark-up
To extract opinion expressions, we implemented a standard sequence labeler for sub-
jective expression mark-up similar to the approach by Breck, Choi, and Cardie (2007).
The sequence labeler extracted basic grammatical and lexical features (word, lemma,
and POS tag), as well as prior polarity and intensity features derived from the lexicon
created by Wilson, Wiebe, and Hoffmann (2005), which we refer to as subjectivity
clues. It is important to note that prior subjectivity does not always imply subjectivity
in a particular context; this is why contextual features are essential for this task. The
grammatical features and subjectivity clues were extracted in a window of size 3 around
the word in focus. We encoded the opinionated expression brackets by means of the
IOB2 scheme (Tjong Kim Sang and Veenstra 1999). When using this representation, we
are unable to handle overlapping opinion expressions, but they are fortunately rare.
To exemplify, Figure 1 shows an example of a sentence and how it is processed
by the sequence labeler. The ESE defenseless situation is encoded in IOB2 as two tags
B-ESE and I-ESE. There are four input columns (words, lemmas, POS tags, subjectivity
clues) and one output column (opinion expression tags in IOB2 encoding). The figure
also shows the sliding window from which the feature extraction function can extract
features when predicting an output tag (at the arrow).
We trained the model using the method by Collins (2002), with a Viterbi decoder
and the online Passive?Aggressive algorithm (Crammer et al 2006) to estimate the
model weights. The learning algorithm parameters were tuned on a development set.
When searching for the best value of the C parameter, we varied it along a log scale from
479
Computational Linguistics Volume 39, Number 3
HRW
has
denounced
the
situation
of
these
prisoners
HRW
have
denounce
the
defenseless
situation
of
this
prisoner
defenseless
NNP
VBZ
VBN
DT
JJ
NN
IN
DT
NNS
?
?
?
?
?
?
str/neg
?
weak/neg
O
O
B?ESE
I?ESE
B?DSE
O
Figure 1
Sequence labeling example.
0.001 to 100, and the best value was 0.1. We used the max-loss version of the algorithm
and ten iterations through the training set.
4.2 Classifiers for Opinion Holder Extraction
The problem of extracting opinion holders for a given opinion expression is in many
ways similar to argument detection in semantic role labeling (Choi, Breck, and Cardie
2006; Ruppenhofer, Somasundaran, and Wiebe 2008). For instance, in the simplest
case when the opinion expression is a verb of evaluation or categorization, finding
the holder would entail finding a semantic argument such as an EXPERIENCER or
COMMUNICATOR. We therefore approached this problem using methods inspired by
semantic role labeling: Given an opinion expression in a sentence, we define binary
classifiers that decide whether each noun phrase of the sentence is its holder or not.
Separate classifiers were trained to extract holders for DSEs, ESEs, and OSEs.
Hereafter, we describe the feature set used by the classifiers. Our walkthrough
example is given by the sentence in Figure 1. Some features are derived from the
syntactic and shallow semantic analysis of the sentence, shown in Figure 2 (Section 6.1
gives more details on this).
SYNTACTIC PATH. Similarly to the path feature widely used in semantic role labeling
(SRL), we extract a feature representing the path in the dependency tree between
the expression and the holder (Johansson and Nugues 2008). For instance, the path
from denounced to HRW in the example is VC?SBJ?.
SHALLOW-SEMANTIC RELATION. If there is a direct shallow-semantic relation between
the expression and the holder, we use a feature representing its semantic role, such
as A0 between denounced and HRW.
EXPRESSION HEAD WORD, POS, AND LEMMA. denounced, VBD, denounce for de-
nounced; situation, NN, situation for defenseless situation.
HEAD WORD AND POS OF HOLDER CANDIDATE. HRW, NNP for HRW.
DOMINATING EXPRESSION TYPE. When locating the holder for the ESE defenseless
situation, we extract a feature representing the fact that it is syntactically dominated
by a DSE. At test time, the expression and its type were extracted automatically.
480
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
CONTEXT WORDS AND POS FOR HOLDER CANDIDATE. Words adjacent to the left
and right; for HRW we extract Right:has, Right:VBZ.
EXPRESSION VERB VOICE. Similar to the common voice feature used in SRL. Takes
the values Active, Passive, and None (for non-verbal opinion expressions). In the
example, we get Active for denounced and None for defenseless situation.
EXPRESSION DEPENDENCY RELATION TO PARENT. VC and OBJ, respectively.
There are also differences compared with typical argument extraction in SRL, however.
First, as outlined in Section 3, it is important to note that the MPQA corpus does not
annotate direct links from opinions to holders, but from opinions to holder coreference
chains. To handle this issue, we used the following approach when training the classi-
fier: We created a positive training instance for each member of the coreference chain
occurring in the same sentence as the opinion, and negative training instances for all
other noun phrases in the sentence. We do not use coreference information at test time,
in order for the system not to rely on automatic coreference resolution.
A second complication is that in addition to the explicit noun phrases in the same
sentence, an opinion may be linked to an implicit holder; a special case of implicit holder
is the writer of the text. To detect implicit and writer links, we trained two separate
classifiers that did not use the features requiring a holder phrase. We did not try to
link opinion expressions to explicitly expressed holders outside the sentence; this is
an interesting open problem with some connections to inter-sentential semantic role
labeling, a problem whose study is in its infancy (Gerber and Chai 2010).
We implemented the classifiers as linear support vector machines (SVMs; Boser,
Guyon, and Vapnik 1992) using the LIBLINEAR software (Fan et al 2008). To handle
the restriction that every expression can have at most one holder, the classifier selects
only the highest-scoring opinion holder candidate at test time. We tuned the learning
parameters on a development set, and the best results were obtained with an L2-
regularized L2-loss SVM and a C value of 1.
4.3 Polarity Classifier
Given an expression, we use a classifier to assign a polarity value: POSITIVE, NEUTRAL,
or NEGATIVE. Following Choi and Cardie (2010), the polarity value BOTH was mapped
to NEUTRAL?the expressions having this value were in any case very few. In the cases
where the polarity value was empty or missing, we set the polarity to NEUTRAL. In
addition, the annotators of the MPQA corpus may use special uncertainty labels in the
case where the annotator was unsure of which polarity to assign, such as UNCERTAIN-
POSITIVE. In these cases, we just removed the uncertainty label.
We again trained SVMs to carry out this classification. The problem of polarity clas-
sification has been studied in detail by Wilson, Wiebe, and Hoffmann (2009), who used
a set of carefully devised linguistic features. Our classifier is simpler and is based on
fairly shallow features. Like the sequence labeler for opinion expressions, this classifier
made use of the lexicon of subjectivity clues.
The feature set used by the polarity classifier consisted of the following features.
The examples come from the opinion expression defenseless situation in Figure 1.
WORDS IN EXPRESSION: defenseless, situation.
POS TAGS IN EXPRESSION: JJ, NN
481
Computational Linguistics Volume 39, Number 3
SUBJECTIVITY CLUES OF WORDS IN EXPRESSION: None.
WORD BIGRAMS IN EXPRESSION: defenseless situation.
WORDS BEFORE AND AFTER EXPRESSION: B:the, A:of.
POS TAGS BEFORE AND AFTER EXPRESSION: B:DT, A:IN.
To train the support vector classifiers, we again used LIBLINEAR with the same param-
eters. The three-class classification problem was binarized using the one-versus-all
method.
5. Fine-Grained Opinion Analysis with Interaction Features
Because there is a combinatorial number of ways to segment a sentence into opin-
ion expressions, and label the opinion expressions with type labels (DSE, ESE, OSE)
as well as polarities and opinion holders, the tractability of the opinion expression
segmentation task will obviously depend on whether we impose restrictions on the
problem in a way that allows for efficient inference. Most previous work (Choi, Breck,
and Cardie 2006; Breck, Choi, and Cardie 2007; Choi and Cardie 2010) used Markov
factorizations and could thus apply standard sequence labeling techniques where the
argmax step was carried out using the Viterbi algorithm (as described in Section 4.1).
As we argued in the introduction, however, it makes linguistic sense that opinion
expression segmentation and other tasks could be improved if relations between possible
expressions were considered; these relations can be syntactic or semantic in nature,
for instance.
We will show that adding relational features causes the Markov assumption to
break down and the problem of finding the best analysis to become computationally
intractable. We thus have to turn to approximate inference methods based on reranking,
which can be trained efficiently.
5.1 Formalization of the Model
We formulate the problem of extracting the opinion structure (the set of opinion expres-
sions, and possibly also their holders or polarities) from a given sentence as a structured
prediction problem
y? = argmax
y
w ? ?(x, y) (1)
where w is a weight vector and ?(x, y) a feature extraction function representing a sen-
tence x and an opinion structure y as a high-dimensional feature vector. We now further
decompose the feature representation ? into a local part ?L and a nonlocal part ?NL:
? = ?L +?NL (2)
where ?L is a standard first-order Markov factorization, and ?NL represents the non-
local interactions between pairs of opinion expressions:
?NL(x, y) =
?
ei,ej?y,ei 6=ej
?p(ei, ej, x) (3)
482
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
The feature function ?p represents a pair of opinion expressions ei and ej and their
interaction in the sentence x, such as the syntactic and semantic relations connecting
them.
5.2 Approximate Inference with Interaction Features
It is easy to see that the inference step argmaxy w ? ?(x, y) is NP-hard for unrestricted
pairwise interaction feature representations ?: This class of models includes simpler
ones such as loopy Markov random fields, where inference is known to be NP-hard and
require the use of approximate approaches such as belief propagation. Although it is
possible that search algorithms for approximate inference in ourmodel could be devised
along similar lines, we sidestepped this issue by using a reranking decomposition of the
problem:
r Apply a simple model based on local context features ?L but no structural
interaction features. Generate a small hypothesis set of size k.
r Apply a complex model using interaction features ?NL to pick the top
hypothesis from the hypothesis set.
The advantages of a reranking approach compared with more complex approaches re-
quiring advanced search techniques are mainly simplicity and efficiency: This approach
is conceptually simple and fairly easy to implement provided that k-best output can
be generated efficiently, which is certainly true for the Viterbi-based sequence labeler
described in Section 4.1. The features can then be arbitrarily complex because we do
not have to think about how the problem structure affects the algorithmic complexity of
the inference step. Reranking has been used in a wide range of applications, starting in
speech recognition (Schwartz and Austin 1991) and very commonly in syntactic parsing
of natural language (Collins 2000).
The hypothesis generation procedure becomes slightly more complex when polar-
ity values and opinion holders of the opinion expressions enter the picture. In that case,
we not only need hypotheses generated by a sequence labeler, but also the outputs
of a secondary classifier: the holder extractor (Section 4.2) or the polarity classifier
(Section 4.3). The hypothesis set generation thus proceeds as follows:
r For a given sentence, let the base sequence labeler generate up to k1
sequences of unlabeled opinion expressions;
r for every sequence, apply the secondary classifier to generate up to k2
outputs.
The hypothesis set size is thus at most k1 ? k2.
To illustrate this process we give a hypothetical example, assuming k1 = k2 = 2
and the sentence The appeasement emboldened the terrorists. We first apply the opinion
expression extractor to generate a set of k1 possible segmentations of the sentence:
The [appeasement] emboldened the [terrorists]
The [appeasement] [emboldened] the [terrorists]
483
Computational Linguistics Volume 39, Number 3
In the second step, we add polarity values, up to k2 labelings for every segmentation
candidate:
The [appeasement]? emboldened the [terrorists]?
The [appeasement]? [emboldened]+ the [terrorists]?
The [appeasement]0 emboldened the [terrorists]?
The [appeasement]? [emboldened]0 the [terrorists]?
5.3 Training the Rerankers
In addition to the approximate inference method to carry out the maximization of Equa-
tion (1), we still need a machine learning method to assign weights to the vector w by
estimating on a training set. We investigated a number of machine learning approaches
to train the rerankers.
5.3.1 Structured SVM Learning. We first applied the method of large-margin estimation
for structured output spaces, also known as structured support vector machines. In
this method, we use quadratic optimization to find the smallest weight vector w that
satisfies the constraint that the difference in output score between the correct output y
and an incorrect output y? should be at least ?(y, y?), where ? is a loss function based
on the degree of error in the output y? with respect to the gold standard y. This is a
generalization of the well-known support vector machine from binary classification to
prediction of structured objects.
Formally, for a given training set T = {?xi, yi?} where the output space for the
input xi is Yi, we state the learning problem as a constrained quadratic optimization
program:
minimizew ?w?2
subject to w ? (?(xi, yi)? ?(xi, yij)) ? ?(yi, yij),
??xi, yi? ? T , yij ? Yi
(4)
Because real-world data tend to be noisy, this optimization problem is usually also
regularized to reduce overfitting, which leads to the introduction of a parameter C as in
regular support vector machines (see Taskar, Guestrin, and Koller [2004] inter alia for
details).
The optimization problem (4) is usually not solved directly because the number of
constraints makes a direct solution of the optimization program intractable for most
realistic types of problems. Instead, an approximation has to be used in practice, and
we used the SVMstructsoftware package (Tsochantaridis et al 2005; Joachims, Finley,
and Yu 2009), which finds a solution to the quadratic program by successively finding
its most violated constraints and adding them to a working set. We used the default
values for the learning parameters, except for the parameter C, which was determined
by optimizing on a development set. This resulted in a C value of 500.
We defined the loss function ? as 1 minus the intersection F-measure defined in
Section 7.1. When training rerankers for the complex extraction tasks (expressions +
holders or expressions + polarities), we used a weighted combination of F-measures
for the primary task (expressions) and the secondary task (holders or polarities, see
Sections 7.1.1 and 7.1.2, respectively).
484
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
5.3.2 On-line Learning. In addition to the structured SVM learning method, we trained
models using two variants of on-line learning. Such learning methods are a feasible
alternative for performance reasons. We investigated two on-line learning algorithms:
the popular structured perceptron (Collins 2002) and the Passive?Aggressive (PA)
algorithm (Crammer et al 2006). To increase robustness, we used an averaged imple-
mentation (Freund and Schapire 1999) of both algorithms.
The difference between the two algorithms is the way the weight vector is incre-
mented in each step. In the perceptron, for a given input x, we compute an update to
the current weight vector by computing the difference between the correct output y and
the predicted output y?. Pseudocode is given in Algorithm 1.
Algorithm 1 The structured perceptron algorithm.
function PERCEPTRON(T ,N)
input Training set T = {(xi, yi)}Ti=1
Number of iterations N
w0 ? (0, . . . , 0)
repeat N times
for (x, y) in T
y?? argmaxh w ? ?(x, h)
wi+1 ? wi +?(x, y)? ?(x, y?)
i? i+ 1
return 1NT
?NT
i=1 wi
The PA algorithm, with pseudocode in Algorithm 2, is based on the theory of large-
margin learning similar to the structured SVM. Here we instead base the update step
on the y? that violates the margin constraints maximally, also taking the loss function ?
into account. The update step length ? is computed based on the margin; this update
is bounded by a regularization constant C, which we set to 0.005 after tuning on a
development set. The number N of iterations through the training set was 8 for both
on-line algorithms.
Algorithm 2 The on-line passive?aggressive algorithm.
function PASSIVE?AGGRESSIVE(T ,N,C)
input Training set T = {(xi, yi)}Ti=1
Number of iterations N
Regularization parameter C
w0 ? (0, . . . , 0)
repeat N times
for (x, y) in T
y?? argmaxh w ? (?(x, h)? ?(x, y))+
?
?(y, h)
?? min
(
C, w?(?(x,y?)??(x,y))+
?
?(y,y?)
??(x,y?)??(x,y)?2
)
wi+1 ? wi + ?(?(x, y)? ?(x, y?))
i? i+ 1
return 1NT
?NT
i=1 wi
485
Computational Linguistics Volume 39, Number 3
6. Overview of the Interaction Features
The feature extraction function ?NL extracts three groups of interaction features: (1)
features considering the opinion expressions only; (2) features considering opinion
holders; and (3) features considering polarity values.
In addition to the interaction features ?NL, the rerankers used features representing
the scores output by the base models (opinion expression sequence labeler and sec-
ondary classifiers); they did not directly use the local features ?L. We normalized the
scores over the k candidates so that their exponentials summed to 1.
6.1 Syntactic and Shallow Semantic Analysis
The features used by the rerankers, as well as the opinion holder extractor in Section 4.2,
are to a large extent derived from syntactic and semantic role structures. To extract
them, we used the syntactic?semantic parser by Johansson and Nugues (2008), which
annotates the sentences with dependency syntax (Mel?c?uk 1988) and shallow semantics
in the PropBank (Palmer, Gildea, and Kingsbury 2005) and NomBank (Meyers et al
2004) frameworks, using the format of the CoNLL-2008 Shared Task (Surdeanu et al
2008). The system includes a sense disambiguator that assigns PropBank or NomBank
senses to the predicate words.
Figure 2 shows an example of the structure of the annotation: The sentence HRW
denounced the defenseless situation of these prisoners, where denounced is a DSE and de-
fenseless situation is an ESE, has been annotated with dependency syntax (above the
text) and semantic role structure (below the text). The predicate denounced, which is an
instance of the PropBank frame denounce.01, has two semantic arguments: the Speaker
(A0, or Agent in VerbNet terminology) and the Subject (A1, or Theme), which are
realized on the surface-syntactic level as a subject and a direct object, respectively. Sim-
ilarly, situation has the NomBank frame situation.01 and an EXPERIENCER semantic
argument (A0).
6.2 Opinion Expression Interaction Features
The rerankers use two types of structural features: syntactic features extracted from
the dependency tree, and semantic features extracted from the predicate?argument
(semantic role) graph.
The syntactic features are based on paths through the dependency tree. This leads to
a minor complication for multiword opinion expressions; we select the shortest possible
path in such cases. For instance, in the sentence in Figure 2, the path will be computed
between denounced and situation.
HRW thehas [denounced] defenseless[ situation]ESE
denounce.01
A0
NMOD
NMOD
OBJ
SBJ VC
DSE of these prisoners
NMOD NMOD
PMOD
situation.01
A0A1
Figure 2
Example sentence and its syntactic and shallow-semantic analysis.
486
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
We used the following syntactic interaction features. All examples refer to Figure 2.
SYNTACTIC PATH. Given a pair of opinion expressions, we use a feature representing
the labels of the two expressions and the path between them through the syntactic
tree, following standard practice from dependency-based semantic role labeling
(Johansson and Nugues 2008). For instance, for the DSE denounced and the ESE
defenseless situation in Figure 2, we represent the syntactic configuration using the
feature DSE:OBJ?:ESE, meaning that the syntactic relation between the DSE and the
ESE consists of a single link representing a grammatical object.
LEXICALIZED PATH. Same as above, but with lexical information attached: DSE/
denounced:OBJ?:ESE/situation.
DOMINANCE. In addition to the features based on syntactic paths, we created a
more generic feature template describing dominance relations between expres-
sions. For instance, from the graph in Figure 2, we extract the feature DSE/
denounced?ESE/situation, meaning that a DSE with the word denounced domi-
nates an ESE with the word situation.
The features based on semantic roles were the following:
PREDICATE SENSE LABEL. For every predicate found inside an opinion expression, we
add a feature consisting of the expression label and the predicate sense identifier.
For instance, the verb denounced, which is also a DSE, is representedwith the feature
DSE/denounce.01.
PREDICATE AND ARGUMENT LABEL. For every argument of a predicate inside an
opinion expression, we also create a feature representing the predicate?argument
pair: DSE/denounced.01:A0.
CONNECTING ARGUMENT LABEL. When a predicate inside some opinion expression
is connected to some argument inside another opinion expression, we use a feature
consisting of the two expression labels and the argument label. For instance, the
ESE defenseless situation is connected to the DSE denounced via an A1 label, and we
represent this using a feature DSE:A1:ESE.
6.3 Opinion Holder Interaction Features
In addition, we modeled the interaction between different opinions with respect to their
holders. We used the following two features to represent this interaction:
SHARED HOLDERS. A feature representing whether or not two opinion expressions
have the same holder. For instance, if a DSE dominates an ESE and they have the
same holder as in Figure 2, where the holder is HRW, we represent this by the
feature DSE:ESE:true.
HOLDER TYPES + PATH. A feature representing the types of the holders, combined with
the syntactic path between the expressions. The types take the following possible
values: explicit, implicit, writer. In Figure 2, we would thus extract the feature
DSE/Expl:OBJ?:ESE/Expl.
487
Computational Linguistics Volume 39, Number 3
6.4 Polarity Interaction Features
The model used the following features that take the polarities of the expressions into
account. These features are extracted from DSEs and ESEs only, because the OSEs have
no polarity values. The examples of extracted features are given with respect to the two
opinion expressions (denounced and defenseless situation) in Figure 2, both of which have
a negative polarity value.
POLARITY PAIR. For every pair of opinion expressions in the sentence, we create a
feature consisting of the pair of polarity values, such as NEGATIVE:NEGATIVE.
POLARITY PAIR AND SYNTACTIC PATH. NEGATIVE:OBJ?:NEGATIVE.
POLARITY PAIR AND SYNTACTIC DOMINANCE. NEGATIVE?NEGATIVE.
POLARITY PAIR AND WORD PAIR. NEGATIVE-denounced:NEGATIVE-situation.
POLARITY PAIR AND EXPRESSION TYPES. Adds the expression types (ESE or DSE) to
the polarity pair: DSE-NEGATIVE:ESE-NEGATIVE.
POLARITY PAIR AND TYPES AND SYNTACTIC PATH. Adds syntactic information to the
type and polarity combination: DSE-NEGATIVE:OBJ?:ESE-NEGATIVE.
POLARITY PAIR AND SHALLOW-SEMANTIC RELATION. When two opinion expres-
sions are directly connected through a link in the shallow-semantic structure, we
create a feature based on the semantic role label of the connecting link: NEGA-
TIVE:A1:NEGATIVE.
POLARITY PAIR AND WORDS ALONG SYNTACTIC PATH. We follow the syntactic path
between the two expressions and create a feature for every word we pass on
the way. In the example, no such feature is extracted because the expressions are
directly connected.
7. Experiments
We trained and evaluated the rerankers on version 2.0 of the MPQA corpus,2 which
contains 692 documents. We discarded one document whose annotation was garbled
and we split the remaining 691 into a training set (541 documents) and a test set (150
documents). We also set aside a development set of 90 documents from the training set
that we used when developing features and tuning learning algorithm parameters; all
experiments described in this article, however, usedmodels that were trained on the full
training set. Table 1 shows some statistics about the training and test sets: the number
of documents and sentences; the number of DSEs, ESEs, and OSEs; and the number of
expressions marked with the various polarity labels.
We considered three experimental settings: (1) opinion expression extraction;
(2) joint opinion expression and holder extraction; and (3) joint opinion expression and
polarity classification. Finally, the polarity-based opinion extraction system was used in
an extrinsic evaluation: document polarity classification of movie reviews.
To generate the training data for the rerankers, we carried out a 5-fold hold-out
procedure: We split the training set into five pieces, trained a sequence labeler and
secondary classifiers on pieces 1?4, applied them to piece 5, and so on.
2 http://www.cs.pitt.edu/mpqa/databaserelease/.
488
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 1
Statistics for the training and test splits of the MPQA collection.
Training Test
Documents 541 150
Sentences 12,010 3,743
DSE 8,389 2,442
ESE 10,279 3,370
OSE 3,048 704
POSITIVE 3,192 1,049
NEGATIVE 6,093 1,675
NEUTRAL 9,105 3,007
BOTH 278 81
7.1 Evaluation Metrics
Because expression boundaries are hard to define rigorously (Wiebe,Wilson, and Cardie
2005), our evaluations mainly used intersection-based precision and recallmeasures to
score the quality of the system output. The idea is to assign values between 0 and 1, as
opposed to traditional precision and recall where a span is counted as either correctly
or incorrectly detected. We thus define the span coverage c of a span s (a set of token
indices) with respect to another span s?, which measures how well s? is covered by s:
c(s, s?) = |s ? s
?|
|s?|
In this formula, |s|means the length of the span s, and the intersection ? gives the set of
token indices that two spans have in common. Because our evaluation takes span labels
(DSE, ESE, OSE) into account, we set c(s, s?) to zero if the labels associated with s and s?
are different.
Using the span coverage, we define the span set coverage C of a set of spans S with
respect to a set S? :
C(S,S? ) =
?
sj?S
?
s?k?S?
c(sj, s?k)
We now define the intersection-based precision P and recall R of a proposed set of spans
S? with respect to a gold standard set S as follows:
P(S, S?) = C(S, S?)|S?| R(S, S?) =
C(S?,S)
|S|
Note that in this formula, |S|means the number of spans in a set S.
Conventionally, when measuring the quality of a system for an information extrac-
tion task, a predicted entity is counted as correct if it exactly matches the boundaries of
a corresponding entity in the gold standard; there is thus no reward for close matches.
Because the boundaries of the spans annotated in the MPQA corpus are not strictly
489
Computational Linguistics Volume 39, Number 3
defined in the annotation guidelines (Wiebe, Wilson, and Cardie 2005), however, mea-
suring precision and recall using exact boundary scoring will result in figures that are
too low to be indicative of the usefulness of the system. Therefore, most work using
this corpus instead use overlap-based precision and recall measures, where a span
is counted as correctly detected if it overlaps with a span in the gold standard (Choi,
Breck, and Cardie 2006; Breck, Choi, and Cardie 2007). As pointed out by Breck, Choi,
and Cardie (2007), this is problematic because it will tend to reward long spans?for
instance, a span covering the whole sentence will always be counted as correct if the
gold standard contains any span for that sentence. Conversely, the overlap metric does
not give higher credit to a span that is perfectly detected than to one that has a very low
overlap with the gold standard.
The precision and recall measures proposed here correct the problem with overlap-
based measures: If the system proposes a span covering the whole sentence, the span
coverage will be low and result in a low soft precision, and a low soft recall will be
assigned if only a small part of a gold standard span is covered. Note that our measures
are bounded below by the exact measures and above by the overlap-based measures.
7.1.1 Opinion Holders. To score the extraction of opinion holders, we started from the
same basic idea: Assign a score based on intersection. The evaluation of this task
is more complex, however, because (1) we only want to give credit for holders for
correctly extracted opinion expressions; (2) the gold standard links opinion expressions
to coreference chains rather than individual mentions of holders; and (3) the holder
entity may be the writer or implicit (see Section 4.2).
We therefore used the following method: If the system has proposed an opinion
expression e and its holder h, we first located the expression e? in the gold standard that
most closely corresponds to e, that is e? = argmaxx c(x, e), regardless of the span labels
of e and e?. To assign a score to the proposed holder entity, we then selected the most
closely corresponding gold standard holder entity h? in the coreference chain H? linked
to e?: h? = argmaxx?H? c(x, h). Finally, we computed the precision and recall scores using
c(h?, h) and c(h, h?). We stress again that the gold standard coreference chains were used
for evaluation purposes only, and that our system did not make use of them at test time.
If the system guesses that the holder of some opinion is the writer entity, we score
it as perfectly detected (coverage 1) if the coreference chain H annotated in the gold
standard contains the writer, and a full error (coverage 0) otherwise, and similar if h is
implicit.
7.1.2 Polarity. In our experiments involving opinion expressions with polarities, we
report precision and recall values for polarity-labeled opinion expression segmentation:
In order to be assigned an intersection score above zero, a segment must be labeled with
the correct polarity. In the gold standard, all polarity labels were changed as described
in Section 4.3. In these evaluations, OSEs were ignored and DSEs and ESEs were not
distinguished.
7.2 Experiments in Opinion Expression Extraction
The first task we considered was the extraction of opinion expression (labeled with
expression types). We first studied the impact of the machine learning method and
hypothesis set size on the reranker performance. Then, we carried out an analysis of the
effectiveness of the features used by the reranker. We finally compared the performance
of the expression extraction system with previous work (Breck, Choi, and Cardie 2007).
490
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 2
Evaluation of reranking learning methods.
Learning method P R F
Baseline 63.4 ? 1.5 46.8 ? 1.2 53.8 ? 1.1
Structured SVM 61.8 ? 1.5 52.5 ? 1.3 56.8 ? 1.1
Perceptron 62.8 ? 1.5 48.1 ? 1.3 54.5 ? 1.2
Passive?Aggressive 63.5 ? 1.5 51.8 ? 1.3 57.0 ? 1.1
Table 3
Oracle and reranker performance as a function of candidate set size.
Reranked Oracle
k P R F P R F
1 63.36 46.77 53.82 63.36 46.77 53.82
2 63.70 48.17 54.86 72.66 55.18 62.72
4 63.57 49.78 55.84 79.12 62.24 69.68
8 63.50 51.79 57.05 83.72 68.14 75.13
16 63.00 52.94 57.54 86.92 72.79 79.23
32 62.15 54.50 58.07 89.18 76.76 82.51
64 61.03 55.67 58.23 91.09 80.19 85.29
128 60.22 56.45 58.27 92.63 83.00 87.55
256 59.87 57.22 58.51 94.01 85.27 89.43
7.2.1 Evaluation of Machine Learning Methods. We compared the machine learning meth-
ods described in Section 5. In these experiments, we used a hypothesis set size k of 8. All
features from Section 6.2 were used. Table 2 shows the results of the evaluations using
the precision and recall measures described earlier.3 The baseline is the result of taking
the top-scoring labeling from the base sequence labeler.
We note that the margin-based methods?structured SVM and the on-line PA
algorithm?outperform the perceptron soundly, which shows the benefit of learning
methods that make use of the cost function ?. Comparing the two best-performing
learning methods, we note that the reranker using the structured SVM is more recall-
oriented whereas the PA-based reranker more precision-oriented; the difference in
F-measure is not statistically significant. In the remainder of this article, all rerankers are
trained using the PA learning algorithm (with the same parameters) because its training
process is much faster than that of the structured SVM.
7.2.2 Candidate Set Size. In any method based on reranking, it is important to study the
influence of the hypothesis set size on the quality of the reranked output. In addition,
an interesting question is what the upper bound on reranker performance is?the
oracle performance. Table 3 shows the result of an experiment that investigates these
questions.
3 All confidence intervals in this article are at the 95% level and were estimated using a resampling method
(Hjorth 1993). The significance tests for differences were carried out using permutation tests.
491
Computational Linguistics Volume 39, Number 3
Table 4
Investigation of the contribution of syntactic features.
Feature set P R F
Baseline 63.4 ? 1.5 46.8 ? 1.2 53.8 ? 1.1
All syntactic features 62.5 ? 1.4 53.2 ? 1.2 57.5 ? 1.1
Removed SYNTACTIC PATH 64.4 ? 1.5 48.7 ? 1.2 55.5 ? 1.1
Removed LEXICAL PATH 62.6 ? 1.4 53.2 ? 1.2 57.5 ? 1.1
Removed DOMINANCE 62.3 ? 1.5 52.9 ? 1.2 57.2 ? 1.1
As is common in reranking tasks, the reranker can exploit only a fraction of the
potential improvement?the reduction of the F-measure error ranges between 10% and
15% of the oracle error reduction for all hypothesis set sizes.
The most visible effect of the reranker is that the recall is greatly improved. This
does not seem to have an adverse effect on the precision, however, until the candidate
set size goes above eight?in fact, the precision actually improves over the baseline for
small candidate set sizes. After the size goes above eight, the recall (and the F-measure)
still rises, but at the cost of decreased precision. In the remainder of this article, we used
a k value of 64, which we thought gave a good balance between processing time and
performance.
7.2.3 Feature Analysis. We studied the impact of syntactic and semantic structural fea-
tures on the performance of the reranker. Table 4 shows the result of an investigation
of the contribution of the syntactic features. Using all the syntactic features (and no
semantic features) gives an F-measure roughly four points above the baseline. We then
carried out an ablation test and measured the F-measure obtained when each one of
the three syntactic features has been removed. It is clear that the unlexicalized syntactic
path is the most important syntactic feature; this feature causes a two-point drop in
F-measure when removed, which is clearly statistically significant (p < 0.0001). The
effect of the two lexicalized features is smaller, with only DOMINANCE causing a sig-
nificant (p < 0.05) drop when removed.
A similar result was obtained when studying the semantic features (Table 5). Re-
moving the connecting label feature, which is unlexicalized, has a greater effect than
removing the other two semantic features, which are lexicalized. Only the connecting
label causes a statistically significant drop when removed (p < 0.0001).
Because our most effective structural features combine a pair of opinion expression
labels with a tree fragment, it is interesting to study whether the expression labels alone
would be enough. If this were the case, we could conclude that the improvement is
Table 5
Investigation of the contribution of semantic features.
Feature set P R F
Baseline 63.4 ? 1.5 46.8 ? 1.2 53.8 ? 1.1
All semantic features 61.3 ? 1.4 53.8 ? 1.3 57.3 ? 1.1
Removed PREDICATE SENSE LABEL 61.3 ? 1.4 53.8 ? 1.3 57.3 ? 1.1
Removed PREDICATE+ARGUMENT LABEL 61.0 ? 1.4 53.6 ? 1.3 57.0 ? 1.1
Removed CONNECTING ARGUMENT LABEL 60.7 ? 1.4 50.5 ? 1.2 55.1 ? 1.1
492
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 6
Structural features compared to label pairs.
Feature set P R F
Baseline 63.4 ? 1.5 46.8 ? 1.2 53.8 ? 1.1
Label pairs 62.0 ? 1.5 52.7 ? 1.2 57.0 ? 1.1
All syntactic features 62.5 ? 1.4 53.2 ? 1.2 57.5 ? 1.1
All semantic features 61.3 ? 1.4 53.8 ? 1.3 57.3 ? 1.1
Syntactic + semantic 61.0 ? 1.4 55.7 ? 1.2 58.2 ? 1.1
Syntactic + semantic + label pairs 61.6 ? 1.4 54.8 ? 1.3 58.0 ? 1.1
caused not by the structural features, but just by learning which combinations of labels
are common in the training set, such as that DSE+ESE would be more common than
OSE+ESE. We thus carried out an experiment comparing a reranker using label pair
features against rerankers based on syntactic features only, semantic features only, and
the full feature set. Table 6 shows the results. We see that the reranker using label pairs
indeed achieves a performance well above the baseline. Its performance is below that of
any reranker using structural features, however. In addition, we see no improvement
when adding label pair features to the structural feature set; this is to be expected
because the label pair information is subsumed by the structural features.
7.2.4 Analysis of the Performance Depending on Expression Type. In order to better under-
stand the performance details of the expression extraction, we analyzed how well it
extracted the three different classes of expressions. Table 7 shows the results of this
evaluation. The DSE row in the table thus shows the results of the performance on DSEs,
without taking ESEs or OSEs into account.
Apart from evaluations of the three different types of expressions, we evaluated
the performance for a number of combined classes that we think may be interesting
for applications: DSE & ESE, finding all opinionated expressions and ignoring objective
speech events; DSE & OSE, finding opinionated and non-opinionated speech and cat-
egorization events and ignoring expressive elements; and unlabeled evaluation of all
types of MPQA expressions. The same extraction system was used in all experiments,
and it was not retrained to maximize the different measures of performance.
Again, the strongest overall tendency is that the reranker boosts the recall. Going
into the details, we see that the reranker gives very large improvements for DSEs and
OSEs, but a smaller improvement for the combined DSE & OSE class. This shows that
Table 7
Performance depending on the type of expression.
Baseline Reranked
P R F P R F
DSE 68.5 ? 2.1 57.8 ? 2.0 62.7 ? 1.7 67.0 ? 2.0 64.9 ? 1.9 66.0 ? 1.6
ESE 63.0 ? 2.1 36.9 ? 1.5 46.5 ? 1.4 58.2 ? 1.9 46.2 ? 1.6 51.5 ? 1.3
OSE 53.5 ? 3.5 62.3 ? 3.5 57.5 ? 3.0 57.0 ? 3.3 73.9 ? 3.2 64.3 ? 2.7
DSE & ESE 71.1 ? 1.6 48.1 ? 1.2 57.4 ? 1.1 68.0 ? 1.5 57.6 ? 1.3 62.4 ? 1.0
DSE & OSE 76.6 ? 1.8 70.3 ? 1.6 73.3 ? 1.3 72.6 ? 1.8 75.8 ? 1.5 74.2 ? 1.2
Unlabeled 75.6 ? 1.5 55.3 ? 1.2 63.9 ? 1.0 71.0 ? 1.4 63.7 ? 1.2 67.2 ? 1.0
493
Computational Linguistics Volume 39, Number 3
one of the most clear benefits of the complex features is to help disambiguate these
expressions. This also affects the performance for general opinionated expressions (DSE
& ESE).
7.2.5 Comparison with Breck, Choi, and Cardie (2007). Comparison of systems in opinion
expression detection is often nontrivial because evaluation settings have differed
widely. Since our problem setting?marking up and labeling opinion expressions in the
MPQA corpus?is most similar to that described by Breck, Choi, and Cardie (2007), we
carried out an evaluation using the setting from their experiment.
For compatibility with their experimental set-up, this experiment differed from the
ones described in the previous sections in the following ways:
r The results were measured using the overlap-based precision and recall,
although this is problematic as pointed out in Section 7.1.
r The system did not need to distinguish DSEs and ESEs and did not have to
detect the OSEs.
r Instead of the training/test split used in the previous evaluations, the
systems were evaluated using a 10-fold cross-validation over the same set
of 400 documents and the same cross-validation split as used in Breck,
Choi, and Cardie?s experiment. Each of the 10 rerankers was evaluated on
one fold and trained on data generated in a cross-validation over the
remaining nine folds.
Again, our reranker uses the PA learning method with the full feature set (Section 6.2)
and a hypothesis set size k of 64. Table 8 shows the performance of our baseline (Section
4.1) and reranked system, along with the best results reported by Breck, Choi, and
Cardie (2007).
We see that the performance of our system is clearly higher?in both precision and
recall?than all results reported by Breck, Choi, and Cardie (2007). Note that our system
was optimized for the intersection metric rather than the overlap metric and that we
did not retrain it for this evaluation.
7.3 Opinion Holder Extraction
Table 9 shows the performance of our holder extraction systems, evaluated using
the scoring method described in Section 7.1.1. We compared the performance of the
reranker using opinion holder interaction features (Section 6.3) to two baselines: The
first of them consisted of the opinion expression sequence labeler (ES, Section 4.1) and
the holder extraction classifier (HC, Section 4.2), without modeling any interactions
between opinions. The second and more challenging baseline was implemented by
Table 8
Results using the evaluation setting from Breck, Choi, and Cardie (2007).
System P R F
Breck, Choi, and Cardie (2007) 71.64 74.70 73.05
Baseline 86.1 ? 1.0 66.7 ? 0.8 75.1 ? 0.7
Reranked 83.4 ? 1.0 75.0 ? 0.8 79.0 ? 0.6
494
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 9
Opinion holder extraction results.
System P R F
ES+HC 57.7 ? 1.7 45.3 ? 1.3 50.8 ? 1.3
ES+ER+HC 53.3 ? 1.5 52.0 ? 1.4 52.6 ? 1.3
ES+HC+EHR 53.2 ? 1.6 55.1 ? 1.5 54.2 ? 1.4
adding the opinion expression reranker (ER) without holder interaction features to
the pipeline. This results in a large performance boost simply as a consequence of
improved expression detection, because a correct expression is required to get credit
for a holder. However, both baselines are outperformed by the reranker using holder
interaction features, which we refer to as the expression/holder reranker (EHR); the
differences to the strong baseline in recall and F-measure are both statistically significant
(p < 0.0001).
We carried out an ablation test to gauge the impact of the two holder interaction
features; we see in Table 10 that both of them contribute to improving the recall, and the
effect on the precision is negligible. The statistical significance for the recall improve-
ment is highest for SHARED HOLDERS (p < 0.0001) and lower for HOLDER TYPES +
PATH (p < 0.02).
We omit a comparison with previous work in holder extraction because our formu-
lation of the opinion holder extraction problem is different from those used in previous
publications. Choi, Breck, and Cardie (2006) used the holders of a simplified set of
opinion expressions, whereas Wiegand and Klakow (2010) extracted every entity tagged
as ?source? inMPQA regardless of whether it was connected to any opinion expression.
Neither of them extracted implicit or writer holders.
Table 11 shows a detailed breakdown of the holder extraction results based on
opinion expression type (DSE, OSE, and ESE), and whether the holder is internally or
externally located; that is, whether or not the holder is textually realized in the same
sentence as the opinion expression. In addition, Table 12 shows the performance for the
two types of externally located holders.
As we noted in previous evaluations, the most obvious change between the baseline
system and the reranker is that the recall and F-measure are improved; this is the case
in every single evaluation. As previously, a large share of the improvement is explained
simply by improved expression detection, which can be seen by comparing the reranked
system to the strong baseline (ES+ER+HC). For the most important situations, however,
we see improvement when using the reranker with holder interaction features. In
those cases it outperforms the strong baseline significantly: DSE internal: p < 0.001,
ESE internal p < 0.001, ESE external p < 0.05 (Table 11), writer p < 0.05 (Table 12).
Table 10
Opinion holder reranker feature ablation test.
Feature set P R F
Both features 53.2 ? 1.6 55.1 ? 1.5 54.2 ? 1.4
Removed HOLDER TYPES + PATH 53.1 ? 1.6 54.6 ? 1.5 53.8 ? 1.3
Removed SHARED HOLDERS 53.1 ? 1.5 53.6 ? 1.5 53.3 ? 1.3
495
Computational Linguistics Volume 39, Number 3
Table 11
Detailed opinion holder extraction results.
DSE Internal External
P R F P R F
ES+HC 57.4 ? 2.4 48.9 ? 2.2 52.8 ? 1.9 32.3 ? 6.8 25.8 ? 5.8 28.7 ? 5.8
ES+ER+HC 56.7 ? 2.2 54.2 ? 2.2 55.5 ? 1.9 33.3 ? 5.9 34.2 ? 6.1 33.7 ? 5.5
ES+HC+EHR 55.6 ? 2.2 58.8 ? 2.3 57.2 ? 1.9 35.2 ? 6.2 32.1 ? 6.0 33.6 ? 5.6
OSE Internal External
P R F P R F
ES+HC 46.2 ? 3.6 57.2 ? 3.9 51.1 ? 3.3 39.7 ? 12.0 35.2 ? 11.2 37.3 ? 10.5
ES+ER+HC 48.6 ? 3.4 66.8 ? 3.7 56.2 ? 3.1 36.8 ? 11.0 39.4 ? 11.4 38.1 ? 10.2
ES+HC+EHR 50.4 ? 3.6 65.9 ? 3.9 57.1 ? 3.2 35.9 ? 10.9 39.4 ? 11.4 37.6 ? 10.1
ESE Internal External
P R F P R F
ES+HC 50.5 ? 4.7 19.2 ? 2.1 27.8 ? 2.7 45.1 ? 3.0 41.2 ? 2.5 43.0 ? 2.4
ES+ER+HC 48.3 ? 3.9 29.3 ? 2.8 36.4 ? 2.9 40.7 ? 2.6 48.4 ? 2.7 44.2 ? 2.3
ES+HC+EHR 40.4 ? 3.4 36.5 ? 3.2 39.8 ? 3.0 43.2 ? 2.8 47.7 ? 2.9 45.3 ? 2.4
The only common case where the improvement is not statistically significant is OSE
internal.
The improvements are most notable for internally located holders, and especially
for the ESEs. Extracting the opinion holder for ESEs is often complex because the ex-
pression and the holder are typically not directly connected on the syntactic or shallow-
semantic level, as opposed to the typical situation for DSEs. When we use the reranker,
however, the interaction features may help us make use of the holders of other opinion
expressions in the same sentence; for instance, the interaction features make it easier
to distinguish cases like ?the film was [awful]ESE? with an external (writer) holder from
cases such as ?I [thought]DSE the film was [awful]ESE? with an internal holder directly
connected to a DSE.
Table 12
Opinion holder extraction results for external holders.
Writer P R F
ES+HC 44.8?3.0 42.8?2.6 43.8?2.4
ES+ER+HC 40.6?2.6 50.3?2.7 44.9?2.3
ES+HC+EHR 42.7?2.8 49.7?2.9 45.9?2.4
Implicit P R F
ES+HC 41.2?6.4 28.3?4.8 33.6?4.9
ES+ER+HC 38.7?5.4 34.4?5.1 36.4?4.7
ES+HC+EHR 43.1?5.9 32.9?5.0 37.4?4.8
496
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 13
Overall evaluation of polarity-labeled opinion expression extraction.
System P R F
ES+PC 56.5 ? 1.7 38.4 ? 1.2 45.7 ? 1.2
ES+ER+PC 53.8 ? 1.6 44.5 ? 1.3 48.8 ? 1.2
ES+PC+EPR 54.7 ? 1.6 45.6 ? 1.3 49.7 ? 1.2
7.4 Polarity Classification
To evaluate the effect of the polarity-based reranker, we carried out experiments to
compare it with two baseline systems similarly to the evaluations of holder extraction
performance. Table 13 shows the precision, recall, and F-measures. The evaluation used
the polarity-based intersection metric (Section 7.1.2). The first baseline consisted of an
expression segmenter and a polarity classifier (ES+PC), and the second also included an
expression reranker (ER). The reranker using polarity interaction features is referred to
as the expression/polarity reranker (EPR).
The result shows that the polarity-based reranker gives a significant boost in recall,
which is in line with our previous results that also mainly improved the recall. The
precision shows a slight decrease from the ES+PC baseline but much lower than the
recall improvement. The differences between the polarity reranker and the strongest
baseline are all statistically significant (precision p < 0.02, recall and F-measure
p < 0.005).
In addition, we evaluated the performance for individual polarity values. The
figures are shown in Table 14. We see that the differences in performance when adding
the polarity reranker are concentrated to the more frequent polarity values (NEUTRAL
and NEGATIVE).
Table 14
Intersection-based evaluation for individual polarity values.
POSITIVE P R F
ES+PC 53.5 ? 3.7 37.3 ? 3.0 43.9 ? 2.8
ES+ER+PC 50.5 ? 3.4 41.8 ? 3.0 45.8 ? 2.6
ES+PC+EPR 51.0 ? 3.5 41.6 ? 3.1 45.8 ? 2.7
NEUTRAL P R F
ES+PC 56.4 ? 2.3 37.8 ? 1.7 45.3 ? 1.7
ES+ER+PC 54.0 ? 2.1 45.2 ? 1.8 49.2 ? 1.6
ES+PC+EPR 55.8 ? 2.1 46.1 ? 1.8 50.5 ? 1.6
NEGATIVE P R F
ES+PC 58.4 ? 2.8 40.1 ? 2.4 47.6 ? 2.2
ES+ER+PC 55.5 ? 2.7 45.0 ? 2.3 49.7 ? 2.0
ES+PC+EPR 54.9 ? 2.7 47.0 ? 2.4 50.6 ? 2.0
497
Computational Linguistics Volume 39, Number 3
Table 15
Overlap-based evaluation for individual polarity values, and comparison with the results
reported by Choi and Cardie (2010).
POSITIVE P R F
ES+PC 59.4 ? 2.6 46.1 ? 2.1 51.9 ? 2.0
ES+ER+PC 53.1 ? 2.3 50.9 ? 2.2 52.0 ? 1.9
ES+PC+EPR 58.2 ? 2.5 49.3 ? 2.2 53.4 ? 2.0
ES+PC+EPRp 63.6 ? 2.8 44.9 ? 2.2 52.7 ? 2.1
Choi and Cardie (2010) 67.1 31.8 43.1
NEUTRAL P R F
ES+PC 60.9 ? 1.4 49.2 ? 1.2 54.5 ? 1.0
ES+ER+PC 55.1 ? 1.2 57.7 ? 1.2 56.4 ? 1.0
ES+PC+EPR 60.3 ? 1.3 55.8 ? 1.2 58.0 ? 1.1
ES+PC+EPRp 68.3 ? 1.5 48.2 ? 1.2 56.5 ? 1.2
Choi and Cardie (2010) 66.6 31.9 43.1
NEGATIVE P R F
ES+PC 72.1 ? 1.8 52.0 ? 1.5 60.4 ? 1.4
ES+ER+PC 65.4 ? 1.7 58.2 ? 1.4 61.6 ? 1.3
ES+PC+EPR 67.6 ? 1.7 59.9 ? 1.5 63.5 ? 1.3
ES+PC+EPRp 75.4 ? 2.0 55.0 ? 1.5 63.6 ? 1.4
Choi and Cardie (2010) 76.2 40.4 52.8
Finally, we carried out an evaluation in the setting4 of Choi and Cardie (2010) and
the figures are shown in Table 15. The table shows our baseline and integrated systems
along with the figures5 from Choi and Cardie. Instead of a single value for all polarities,
we show the performance for every individual polarity value (POSITIVE, NEUTRAL,
NEGATIVE). This evaluation uses the overlap metric instead of the intersection-based
one. As we have pointed out, we use the overlap metric for compatibility although it is
problematic.
As can be seen from the table, the system by Choi and Cardie (2010) shows a large
precision bias despite being optimizedwith respect to the recall-promoting overlapmet-
ric. In recall and F-measure, their system is significantly outperformed for all polarity
values by our baseline consisting of a pipeline of opinion expression extraction and
polarity classifier. In addition, our joint model clearly outperforms the pipeline. The
precision is slightly lower overall, but this is offset by large boosts in recall in all cases.
In order to rule out the hypothesis that our F-measure improvement compared with
the Choi and Cardie system could be caused just by rebalancing precision and recall, we
additionally trained a precision-biased reranker EPRp by changing the loss function ?
(see Section 5.3) from 1? Fi to 1? 13Fi ? 23Po, where Fi is the intersection F-measure and
Po the overlap precision. When we use this reranker, we achieve almost the same levels
of precision as reported by Choi and Cardie, even outperforming their precision for the
4 In addition to polarity, their system also assigned opinion intensity, which we do not consider here.
5 Confidence intervals for Choi and Cardie (2010) are omitted because we had no access to their output.
498
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
NEUTRAL polarity value, while the recall values are still massively higher. The precision
bias causes slight drops in F-measure for the POSITIVE and NEUTRAL polarities.
7.5 First Extrinsic Evaluation: Extraction of Evaluations of Product Attributes
As an extrinsic evaluation of the opinion expression extraction system, we evaluated
the impact of the expressions on a practical application: extraction of evaluations of
attributes from product reviews. We first describe the collection we used and then the
implementation of the extractor.
We used the annotated data set by Hu and Liu (2004a, 2004b)6 for the experiments
in extraction of attribute evaluations from product reviews. The collection contains
reviews of five products: one DVD player, two cameras, one MP3 player, and one
cellular phone. In this data set, every sentence is associated with a set of attribute eval-
uations. An evaluation consists of an attribute name and an evaluation value between
?3 and +3, where ?3 means a strongly negative evaluation and +3 strongly positive.
For instance, the sentence this player boasts a decent size and weight, a relatively-intuitive
navigational system that categorizes based on id3 tags, and excellent sound is tagged with the
attribute evaluations size +2, weight +2, navigational system +2, sound +2. In this
work, we do not make use of the exact value of the evaluation but only its sign. We
removed the product attribute mentions in the form of anaphoric pronouns referring
to entities mentioned in previous sentences; these cases are directly marked in the
data set.
7.5.1 Implementation.We considered two problems: (1) extraction of attribute evaluations
without taking the polarity into account, and (2) extraction with polarity (positive or
negative). The former is modeled as a binary classifier that tags each word in the review
(except the punctuation) as an evaluation or not, and the latter requires the definition of
a three-class polarity classifier. For both tasks, we compared three feature sets: a baseline
using simple features, a stronger baseline using a lexicon, and finally a system using
features derived from opinion expressions.
Similarly to the opinion expression polarity classifier, we implemented the clas-
sifiers as SVMs that we trained using LIBLINEAR. For the extraction task without
polarities, the best results were obtained using an L2-regularized L2-loss SVM and a
C value of 0.1. For the polarity task, we used a multiclass SVM (Crammer and Singer
2001) with the same parameters. To handle the precision/recall tradeoff, we varied the
class weighting for the null class.
The baseline classifier used features based on lexical information (word, POS tag,
and lemma) in a window of size 3 around the word under consideration (the focus
word). In addition, it had two features representing the overall sentence polarities. To
compute the polarities, we trained bag-of-words classifiers following the implemen-
tation by Pang, Lee, and Vaithyanathan (2002). Two separate classifiers were used:
one for positive and one for negative polarity. Note that these classifiers detect the
presence of positive or negative polarity, which may thus occur in the same sentence. The
classifiers were trained on the MPQA corpus, where we counted a sentence as positive
if it contained a positive opinion expression with an intensity of at least MEDIUM, and
conversely for the negative polarity.
6 http://www.cs.uic.edu/?liub/FBS/CustomerReviewData.zip.
499
Computational Linguistics Volume 39, Number 3
7.5.2 Features Using a Sentiment Lexicon. Many previous implementations for several
opinion-related tasks make use of sentiment lexicons, so the stronger baseline system
used features based on the subjectivity lexicon by Wilson, Wiebe, and Hoffmann (2005),
which we previously used for opinion expression segmentation in Section 4.1 and for
polarity classification in Section 4.3. We created a classifier using a number of features
based on this lexicon.
These features make use of the syntactic and semantic structure of the sentence.
In the following examples, we use the sentence The software itself was not so easy to use,
presented in Figure 3. In this sentence, consider the focus word software. One word is
listed in the lexicon as associated with positive sentiment: easy. The system then extracts
the following features:
SENTIMENT LEXICON POLARITIES. For every word in the sentence that is listed in
the lexicon, we add a feature. Given the example sentence, we will thus add a
feature lex pol:positive because of the word easy, which is listed as positive in
the sentiment lexicon.
CLOSEST PREVIOUS AND FOLLOWING SENTIMENT WORD. If there are sentiment
words before or after the focus word, we add the closest of them to the feature
vector. In this case, there is no previous sentiment word, so we only extract fol-
lowing word:easy.
SYNTACTIC PATHS TO SENTIMENT WORDS. For every sentiment word in the sentence,
we extract a syntactic path similar to our previous feature sets. This represents a
syntactic pattern describing the relation between the sentiment word and the focus
words. For instance, in the example we extract the path SBJ?PRD?, representing a
copula construction: The word software is connected to the sentiment word easy first
up through a subject link and then down through a predicative complement link.
SEMANTIC LINKS TO SENTIMENT WORDS. When there is a direct semantic role link
between a sentiment word and the focus word, we add a feature for the semantic
role label. No such features are extracted in the example sentence. The focus word
is an argument but no sentiment word is also a predicate.
7.5.3 Extended Feature Set Based on MPQA Opinion Expressions. We finally created an
extended feature set incorporating the following features derived from MPQA-style
opinion expressions, which we extracted automatically. The features are similar in
construction to those extracted by means of the sentiment lexicon. The following list
describes the new features exemplified with the same sentence above, which contains a
negative opinion expression not so easy.
softwareThe itself was not so easy
A1
NMOD APPO
SBJ PRD
ADV AMOD
[ ]
ESE
useto
IM
use.01
AMOD
Figure 3
Example sentence for product feature evaluation extraction.
500
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Table 16
Product attribute evaluation extraction performance.
Feature representation Unlabeled Polarity-labeled
Baseline 49.8 ? 2.0 39.6 ? 2.0
Lexicon 53.8 ? 2.0 46.2 ? 2.0
Opinion expressions 54.8 ? 2.0 49.0 ? 2.0
OPINION EXPRESSION POLARITIES. For every opinion expression extracted by the
automatic system, we add a feature representing the polarity of the expression.
In the example, we get op expr:negative.
CLOSEST PREVIOUS AND FOLLOWING OPINION EXPRESSION WORD. We extract fea-
tures for the closest words before and after the focus word that are contained in
some opinion expression. In the example, there is an expression not so easy after the
focus word software, so we get a single feature following expr:not.
SYNTACTIC PATHS TO OPINION EXPRESSIONS. For every opinion expression in the
sentence, we extract a path from the expression to the focus word. Because opinion
expressions frequently consist of more than one word, we use the shortest path. In
this case, we will thus again get SBJ?PRD?.
SEMANTIC LINKS TO OPINION EXPRESSIONS. Finally, we extracted features in case
there were semantic role links. Again, we get no features based on the semantic
role structure in the example since the opinion expression contains no predicate or
argument.
7.5.4 Results. We evaluated the performance of the product attribute evaluation extrac-
tion using a 10-fold cross-validation procedure on the whole data set. We evaluated
three classifiers: a baseline that did not use the lexicon or the opinion expressions, a
classifier that adds the lexicon-based features, and finally the classifier that adds the
MPQA opinion expressions. The F-measures are shown in Table 16 for the extraction
task, and Figure 4 shows the precision/recall plots. There are clear improvements when
adding the lexicon features, but the highest performing system is the one that also
used the opinion expression features. The difference between the two top-performing
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.4 0.45 0.5 0.55 0.6 0.65 0.7
Re
ca
ll
Precision
Unlabeled
Baseline
Lexicon
Opinion Expressions
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65
Re
ca
ll
Precision
Polarity-labeled
Baseline
Lexicon
Opinion Expressions
Figure 4
Precision / recall curves for extraction of product attribute evaluations.
501
Computational Linguistics Volume 39, Number 3
classifiers is statistically significant (p < 0.001). For the extraction task where we also
consider the polarities, the difference is even greater: almost three F-measure points.
7.6 Second Extrinsic Evaluation: Document Polarity Classification Experiment
In a second extrinsic evaluation of the opinion expression extractor, we investigated
how expression-based features affect the performance of a document-level polarity
classifier of reviews as positive or negative. We followed the same evaluation protocol
as in the first extrinsic evaluation, where we compare three classifiers of increasing
complexity: (1) a baseline using a pure word-based representation, (2) a stronger base-
line adding features derived from a sentiment lexicon, and (3) a classifier with features
extracted from opinion expressions.
The task of categorizing a full document as positive or negative can be viewed
as a document categorization task, and this has led to the application of standard
text categorization techniques (Pang, Lee, and Vaithyanathan 2002). We followed this
approach and implemented the document polarity classifier as a binary linear SVM;
this learning method has a long tradition of successful application in text categorization
(Joachims 2002).
For these experiments, we used six collections. The first one consisted of movie
reviews written in English extracted from the Web by Pang and Lee (2004).7 This data
set is an extension of a smaller set (Pang, Lee, and Vaithyanathan 2002) that has been
used in a large number of experiments. The remaining five sets consisted of product
reviews gathered by Blitzer, Dredze, and Pereira (2007).8 We used five of the largest
subsets: reviews of DVDs, software, books, music, and cameras. In all six collections,
1,000 documents were labeled by humans as positive and 1,000 as negative.
Following Pang and Lee (2004), the documents were represented as bag-of-word
feature vectors based on presence features for individual words. No weighting such as
IDF was used. The vectors were normalized to unit length. Again, we trained the SVMs
using LIBLINEAR, and the best results were obtained using an L2-regularized L2-loss
version of the SVM with a C value of 1.
7.6.1 Features Based on the Subjectivity Lexicon. We used features based on the subjectivity
lexicon by Wilson, Wiebe, and Hoffmann (2005) that we used for opinion expression
segmentation in Section 4.1 and for polarity classification in Section 4.3. For every word
whose lemma is listed in the lexicon, we added a feature consisting of the word and its
prior polarity and intensity to the bag-of-words feature vector.
The feature examples are taken from the sentence HRW has denounced the defenseless
situation of these prisoners, where denounce is listed in the lexicon as strong/negative
and prisoner as weak/negative.
LEXICON POLARITY. negative.
LEXICON POLARITY AND INTENSITY. strong/negative, weak/negative.
LEXICON POLARITY AND WORD. denounced/negative, prisoners/negative.
7 http://www.cs.cornell.edu/people/pabo/movie-review-data/.
8 http://www.cs.jhu.edu/?mdredze/datasets/sentiment/unprocessed.tar.gz.
502
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
7.6.2 Features Extracted from Opinion Expressions. Finally, we created a feature set based
on the opinion expressions with polarities. We give examples from the same sentence;
here, denounced is a negative DSE and defenseless situation is a negative ESE.
EXPRESSION POLARITY. negative.
EXPRESSION POLARITY AND WORD. negative/denounced, negative/defenseless,
negative/situation.
EXPRESSION TYPE AND WORD. DSE/denounced, ESE/defenseless, ESE/situation.
7.6.3 Evaluation Results. To evaluate the performance of the document polarity classifiers,
we carried out a 10-fold cross-validation procedure for every review collection. We
evaluated three classifiers: one using only bag-of-words features (?Baseline?); one using
features extracted from the subjectivity lexicon (?Lexicon?); and finally one also using
the expression-based features (?Expressions?).
In order to abstract away from the tuning threshold, the performances were mea-
sured using AUC, the area under ROC curve. The AUC values are given in Table 17.
These evaluations show that the classifier adding features extracted from the opin-
ion expressions significantly outperforms the classifier using only a bag-of-words fea-
ture representation and also that using the lexicon-based features. This demonstrates
that the extraction and disambiguation of opinion expressions in their context is useful
for a coarse-grained task such as document polarity classification. The differences in
AUC values between the two best configurations are statistically significant (p < 0.005
for all six collections). In addition, we show the precision/recall plots in Figure 5; we
see that for all six collections, the expression-based set-up outperforms the other two
near the precision/recall breakeven point.
The collection where we can see the most significant difference is the movie review
set. The main difference of this collection compared with the other collections is that its
documents are larger: The average size of a document here is about four times larger
than in the other collections. In addition, its reviews often contain large sections that are
purely factual in nature, mainly plot descriptions. The opinion expression identification
may be seen as a way to process the document to highlight the interesting parts on
which the classifier should focus.
8. Conclusion
We have shown that features derived from grammatical and semantic role structure
can be used to improve three fundamental tasks in fine-grained opinion analysis: the
detection of opinionated expressions, the extraction of opinion holders, and finally the
Table 17
Document polarity classification evaluation (AUC values).
Feature set Movie DVD Software Books Music Cameras
Baseline 93.1 ? 1.0 85.1 ? 1.7 91.0 ? 1.3 85.7 ? 1.6 84.7 ? 1.7 91.9 ? 1.2
Lexicon 93.8 ? 1.0 86.6 ? 1.6 92.3 ? 1.2 87.4 ? 1.5 86.6 ? 1.5 92.9 ? 1.1
Expressions 94.7 ? 0.9 87.2 ? 1.5 92.9 ? 1.1 88.1 ? 1.5 87.5 ? 1.5 93.6 ? 1.1
503
Computational Linguistics Volume 39, Number 3
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Movies
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
DVDs
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Software
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Books
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Music
Baseline
Lexicon
Opinion Expressions
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Re
ca
ll
Precision
Cameras
Baseline
Lexicon
Opinion Expressions
Figure 5
Precision / recall curves for detection of positive reviews.
assignment of polarity labels to opinion expressions. The main idea is to use relational
features describing the interaction of opinion expressions through linguistic structures
such as syntax and semantics. This is not only interesting from a practical point of
view (improving performance) but also confirms our linguistic intuitions that surface-
linguistic structure phenomena such as syntax and shallow semantics are used in the
encoding of the rhetorical organization of the sentence, and that we can thus extract
useful information from those structures.
504
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
Because our feature sets are based on interaction between opinion expressions
that can appear anywhere in a sentence, exact inference in this model becomes in-
tractable. To overcome this issue, we used an approximate search strategy based on
reranking: In the first step, we used the baseline systems, which use only simple local
features, to generate a relatively small hypothesis set; we then applied a classifier using
interaction features to pick the final result. A common objection to reranking is that
the candidate set may not be diverse enough to allow for much improvement unless
it is very large; the candidates may be trivial variations that are all very similar to
the top-scoring candidate. Investigating inference methods that take a less brute-force
approach than plain reranking is thus another possible future direction. Interesting
examples of such inference methods include forest reranking (Huang 2008) and loopy
belief propagation (Smith and Eisner 2008). Nevertheless, although the development of
such algorithms is a fascinating research problem, it will not necessarily result in a more
usable system: Rerankers impose very few restrictions on feature expressivity andmake
it easy to trade accuracy for efficiency.
We investigated the effect of machine learning features, as well as other design
parameters such as the choice of machine learning method and the size of the
hypothesis set. For the features, we analyzed the impact of using syntax and semantics
and saw that the best models are those making use of both. The most effective features
we have found are purely structural, based on tree fragments in a syntactic or semantic
tree. Features involving words generally did not seem to have the same impact. Sparsity
may certainly be an issue for features defined in terms of tree fragments. Possible future
extensions in this area could include bootstrapping methods to mine for meaningful
fragments unseen in the training set, or methods to group such features into clusters to
reduce the sparsity.
In addition to the core results on fine-grained opinion analysis, we have described
experiments demonstrating that features extracted from opinion expressions can be
used to improve practical applications: extraction of evaluations of product attributes,
and document polarity classification. Although for the first task it may be fairly obvious
that it is useful to carry out a fine-grained analysis of the sentence opinion structure,
the second result is more unexpected because the document polarity classification task
is a high-level and coarse-grained task. For both tasks, we saw statistically significant
increases in performance compared not only to simple baselines, but also compared
to strong baselines using a lexicon of sentiment words. Although the lexicon leads to
clear improvements, the best classifiers also used the features extracted from the opinion
expressions.
It is remarkable that the opinion expressions as defined by the MPQA corpus are
useful for practical applications on reviews from several domains, because this corpus
mainly consists of news documents related to political topics; this shows that the expres-
sion identifier has been able to generalize from the specific domains. It would still be
relevant, however, to apply domain adaptation techniques (Blitzer, Dredze, and Pereira
2007). It could also be interesting to see how domain-specific opinion word lexicons
could improve over the generic lexicon we used here; especially if such a lexicon were
automatically constructed (Jijkoun, de Rijke, and Weerkamp 2010).
There are multiple additional opportunities for future work in this area. An impor-
tant issue that we have left open is the coreference problem for holder extraction, which
has been studied by Stoyanov and Cardie (2006). Similarly, recent work has tried to
incorporate complex, high-level linguistic structure such as discourse representations
(Asher, Benamara, and Mathieu 2009; Somasundaran et al 2009; Zirn et al 2011); it is
clear that these structures are very relevant for explaining the way humans organize
505
Computational Linguistics Volume 39, Number 3
their expressions of opinions rhetorically. Theoretical depth does not necessarily guar-
antee practical applicability, however, and the challenge is as usual to find a middle
ground that balances our goals: explanatory power in theory, significant performance
gains in practice, computational tractability, and robustness in difficult circumstances.
Acknowledgments
We would like to thank Eric Breck and
Yejin Choi for clarifying their results and
experimental set-up, and for sharing their
cross-validation split. In addition, we are
grateful to the anonymous reviewers,
whose feedback has helped to improve
the clarity and readability of this article.
The research described here has received
funding from the European Community?s
Seventh Framework Programme
(FP7/2007-2013) under grant 231126:
LivingKnowledge?Facts, Opinions and Bias
in Time, and under grant 247758:
Trustworthy Eternal Systems via Evolving
Software, Data and Knowledge (EternalS).
References
Asher, Nicholas, Farah Benamara, and
Yannick Mathieu. 2009. Appraisal of
opinion expressions in discourse.
Lingvisticae Investigations, 31(2):279?292.
Bethard, Steven, Hong Yu, Ashley Thornton,
Vasileios Hatzivassiloglou, and
Dan Jurafsky. 2005. Extracting opinion
propositions and opinion holders
using syntactic and lexical cues.
In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing
Attitude and Affect in Text: Theory and
Applications. Springer, New York,
chapter 11, pages 125?140.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics
(ACL-07), pages 440?447, Prague.
Boser, Bernhard, Isabelle Guyon, and
Vladimir Vapnik. 1992. A training
algorithm for optimal margin classifiers.
In Proceedings of the Fifth Annual Workshop
on Computational Learning Theory,
pages 144?152, Pittsburgh, PA.
Breck, Eric, Yejin Choi, and Claire Cardie.
2007. Identifying expressions of opinion
in context. In IJCAI 2007, Proceedings of
the 20th International Joint Conference on
Artificial Intelligence, pages 2,683?2,688,
Hyderabad.
Choi, Yejin, Eric Breck, and Claire Cardie.
2006. Joint extraction of entities and
relations for opinion recognition. In
Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 431?439, Sydney.
Choi, Yejin and Claire Cardie. 2008.
Learning with compositional semantics
as structural inference for subsentential
sentiment analysis. In Proceedings of the
2008 Conference on Empirical Methods
in Natural Language Processing,
pages 793?801, Honolulu, HI.
Choi, Yejin and Claire Cardie. 2010.
Hierarchical sequential learning for
extracting opinions and their attributes.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 269?274, Uppsala.
Collins, Michael. 2000. Discriminative
reranking for natural language
parsing. In Proceedings of the
Seventeenth International Conference on
Machine Learning, pages 175?182,
San Francisco, CA.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002),
pages 1?8, Philadelphia, PA.
Crammer, Koby, Ofer Dekel, Joseph Keshet,
Shai Shalev-Schwartz, and Yoram Singer.
2006. Online passive-aggressive
algorithms. Journal of Machine Learning
Research, 2006(7):551?585.
Crammer, Koby and Yoram Singer. 2001.
On the algorithmic implementation of
multiclass kernel-based vector machines.
Journal of Machine Learning Research,
2001(2):265?585.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A library for large linear
classification. Journal of Machine Learning
Research, 9:1871?1874.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277?296.
Gerber, Matthew and Joyce Chai. 2010.
Beyond NomBank: A study of implicit
506
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
arguments for nominal predicates.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 1,583?1,592, Uppsala.
Greene, Stephan and Philip Resnik.
2009. More than words: Syntactic
packaging and implicit sentiment.
In Proceedings of Human Language
Technologies: The 2009 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 503?511, Boulder, CO.
Hjorth, J. S. Urban. 1993. Computer Intensive
Statistical Methods. Chapman and Hall,
London.
Hu, Minqing and Bing Liu. 2004a. Mining
and summarizing customer reviews.
In Proceedings of the ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining (KDD-04),
pages 168?177, Seattle, WA.
Hu, Minqing and Bing Liu. 2004b. Mining
opinion features in customer reviews.
In Proceedings of the Nineteeth National
Conference on Artificial Intellgience
(AAAI-2004), pages 755?760,
San Jose, CA.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL-08: HLT,
pages 586?594, Columbus, OH.
Jijkoun, Valentin, Maarten de Rijke, and
Wouter Weerkamp. 2010. Generating
focused topic-specific sentiment lexicons.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 585?594, Uppsala.
Joachims, Thorsten. 2002. Learning to Classify
Text using Support Vector Machines.
Kluwer/Springer, Boston.
Joachims, Thorsten, Thomas Finley, and
Chun-Nam Yu. 2009. Cutting-plane
training of structural SVMs. Machine
Learning, 77(1):27?59.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based syntactic?semantic
analysis with PropBank and NomBank.
In CoNLL 2008: Proceedings of the Twelfth
Conference on Natural Language Learning,
pages 183?187, Manchester.
Joshi, Mahesh and Carolyn Penstein-Rose?.
2009. Generalizing dependency features
for opinion mining. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers,
pages 313?316, Singapore.
Karlgren, Jussi, Gunnar Eriksson, Magnus
Sahlgren, and Oscar Ta?ckstro?m. 2010.
Between bags and trees?Constructional
patterns in text used for attitude
identification. In Proceedings of ECIR 2010,
32nd European Conference on Information
Retrieval, pages 38?49, Milton Keynes.
Kim, Soo-Min and Eduard Hovy. 2006.
Extracting opinions, opinion holders, and
topics expressed in online news media
text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text,
pages 1?8, Sydney.
Kobayashi, Nozomi, Kentaro Inui, and
Yuji Matsumoto. 2007. Extracting
aspect-evaluation and aspect-of relations
in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 1,065?1,074,
Prague.
Mel?c?uk, Igor A. 1988. Dependency Syntax:
Theory and Practice. State University Press
of New York, Albany.
Meyers, Adam, Ruth Reeves, Catherine
Macleod, Rachel Szekely, Veronika
Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004
Workshop: Frontiers in Corpus Annotation,
pages 24?31, Boston, MA.
Moschitti, Alessandro and Roberto Basili.
2004. Complex linguistic features for
text classification: A comprehensive
study. In Proceedings of the 26th European
Conference on Information Retrieval
Research (ECIR 2004), pages 181?196,
Sunderland.
Palmer, Martha, Dan Gildea, and Paul
Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?105.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume,
pages 271?278, Barcelona.
Pang, Bo and Lillian Lee. 2008. Opinion
mining and sentiment analysis.
Foundations and Trends in Information
Retrieval, 2(1?2):1?135.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in
Natural Language Processing, pages 79?86,
Philadelphia, PA.
Popescu, Ana-Maria and Oren Etzioni. 2005.
Extracting product features and opinions
507
Computational Linguistics Volume 39, Number 3
from reviews. In Proceedings of Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 339?346,
Vancouver.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo,
Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International
Conference on Languages Resources
and Evaluations (LREC 2008),
pages 2,961?2,968, Marrakech.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1985.
A Comprehensive Grammar of the English
Language. Longman, New York.
Ruppenhofer, Josef, Swapna Somasundaran,
and Janyce Wiebe. 2008. Finding the
sources and targets of subjective
expressions. In Proceedings of the Sixth
International Language Resources and
Evaluation (LREC?08), pages 2,781?2,788,
Marrakech.
Schwartz, Richard and Steve Austin. 1991.
A comparison of several approximate
algorithms for finding multiple (n-best)
sentence hypotheses. In Proceedings of the
IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP),
pages 701?704, Toronto.
Smith, David and Jason Eisner. 2008.
Dependency parsing by belief
propagation. In Proceedings of the
Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 145?156, Honolulu, HI.
Somasundaran, Swapna, Galileo Namata,
Janyce Wiebe, and Lise Getoor. 2009.
Supervised and unsupervised methods
in employing discourse relations for
improving opinion polarity classification.
In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 170?179, Singapore.
Stoyanov, Veselin and Claire Cardie.
2006. Partially supervised coreference
resolution for opinion summarization
through structured rule learning.
In Proceedings of the 2006 Conference
on Empirical Methods in Natural
Language Processing, pages 336?344,
Sydney.
Stoyanov, Veselin and Claire Cardie.
2008. Annotating topics of opinions.
In Proceedings of the Sixth International
Language Resources and Evaluation
(LREC?08), pages 3,213?3,217,
Marrakech.
Surdeanu, Mihai, Richard Johansson,
Adam Meyers, Llu??s Ma`rquez, and
Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of
syntactic and semantic dependencies.
In Proceedings of CoNLL 2008,
pages 159?177, Manchester.
Taskar, Ben, Carlos Guestrin, and Daphne
Koller. 2004. Max-margin Markov
networks. In Advances in Neural Information
Processing Systems 16, pages 25?32,
Cambridge, MA, MIT Press.
Titov, Ivan and Ryan McDonald. 2008.
A joint model of text and aspect
ratings for sentiment summarization.
In Proceedings of ACL-08: HLT,
pages 308?316, Columbus, OH.
Tjong Kim Sang, Erik F., and Jorn Veenstra.
1999. Representing text chunks.
In Proceedings of the Ninth Conference
of the European Chapter of the Association
for Computational Linguistics,
pages 173?179, Bergen.
Tsochantaridis, Ioannis, Thorsten Joachims,
Thomas Hofmann, and Yasemin Altun.
2005. Large margin methods for
structured and interdependent
output variables. Journal of Machine
Learning Research, 6(Sep):1453?1484.
Wiebe, Janyce, Rebecca Bruce, and Thomas
O?Hara. 1999. Development and use of a
gold standard data set for subjectivity
classifications. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 246?253,
College Park, MD.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2-3):165?210.
Wiegand, Michael and Dietrich Klakow.
2010. Convolution kernels for opinion
holder extraction. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 795?803, Los Angeles, CA.
Wilson, Theresa, Janyce Wiebe, and
Paul Hoffmann. 2005. Recognizing
contextual polarity in phrase-level
sentiment analysis. In Proceedings of
Human Language Technology Conference
and Conference on Empirical Methods
in Natural Language Processing,
pages 347?354, Vancouver.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffmann. 2009. Recognizing contextual
polarity: An exploration of features
508
Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis
for phrase-level sentiment analysis.
Computational Linguistics, 35(3):399?433.
Wu, Yuanbin, Qi Zhang, Xuangjing
Huang, and Lide Wu. 2009. Phrase
dependency parsing for opinion
mining. In Proceedings of the 2009
Conference on Empirical Methods in
Natural Language Processing,
pages 1,533?1,541, Singapore.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2003), pages 129?136,
Sapporo.
Zirn, Ca?cilia, Mathias Niepert, Heiner
Stuckenschmidt, and Michael Strube. 2011.
Fine-grained sentiment analysis with
structural features. In Proceedings of 5th
International Joint Conference on Natural
Language Processing, pages 336?344,
Chiang Mai.
509

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1020?1028,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Syntactic/Semantic Structures for Textual Entailment Recognition
Yashar Mehdad
FBK-IRST, DISI
University of Trento
Povo (TN) - Italy
mehdad@fbk.eu
Alessandro Moschitti
DISI
University of Trento
Povo (TN) - Italy
moschitti@disi.unitn.it
Fabio Massimo Zanzotto
DISP
University of Rome ?Tor Vergata?
Roma - Italy
zanzotto@info.uniroma2.it
Abstract
In this paper, we describe an approach based
on off-the-shelf parsers and semantic re-
sources for the Recognizing Textual Entail-
ment (RTE) challenge that can be generally
applied to any domain. Syntax is exploited
by means of tree kernels whereas lexical se-
mantics is derived from heterogeneous re-
sources, e.g. WordNet or distributional se-
mantics through Wikipedia. The joint syn-
tactic/semantic model is realized by means of
tree kernels, which can exploit lexical related-
ness to match syntactically similar structures,
i.e. whose lexical compounds are related. The
comparative experiments across different RTE
challenges and traditional systems show that
our approach consistently and meaningfully
achieves high accuracy, without requiring any
adaptation or tuning.
1 Introduction
Recognizing Textual Entailment (RTE) is rather
challenging as effectively modeling syntactic and
semantic for this task is difficult. Early deep seman-
tic models (e.g., (Norvig, 1987)) as well as more re-
cent ones (e.g., (Tatu and Moldovan, 2005; Bos and
Markert, 2005; Roth and Sammons, 2007)) rely on
specific world knowledge encoded in rules for draw-
ing decisions. Shallower models exploit matching
methods between syntactic/semantic graphs of texts
and hypotheses (Haghighi et al, 2005). The match-
ing step is carried out after the application of some
lexical-syntactic rules that are used to transform the
text T or the hypothesis H (Bar-Haim et al, 2009)
at surface form level. For all these methods, the ef-
fective use of syntactic and semantic information de-
pends on the coverage and the quality of the specific
rules. Lexical-syntactic rules can be automatically
extracted from plain corpora (e.g., (Lin and Pantel,
2001; Szpektor and Dagan, 2008)) but the quality
(also in terms of little noise) and the coverage is low.
In contrast, rules written at the semantic level are
more accurate but their automatic design is difficult
and so they are typically hand-coded for the specific
phenomena.
In this paper, we propose models for effectively
using syntactic and semantic information in RTE,
without requiring either large automatic rule acqui-
sition or hand-coding. These models exploit lexi-
cal similarities to generalize lexical-syntactic rules
automatically derived by supervised learning meth-
ods. In more detail, syntax is encoded in the form of
parse trees whereas similarities are defined by means
of WordNet simlilarity measures or Latent Seman-
tic Analysis (LSA) applied to Wikipedia or to the
British National Corpus (BNC). The joint syntac-
tic/semantic model is realized by means of novel tree
kernels, which can match subtrees whose leaves are
lexically similar (so not just identical).
To assess the benefit of our approach, we carried
out comparative experiments with previous work:
especially with the method described in (Zanzotto
and Moschitti, 2006; Zanzotto et al, 2009). This
constitutes our strong baseline as, although it can
only exploit lexical-syntactic rules, it has achieved
top accuracy in all RTE challenges. The results,
across different RTE challenges, show that our ap-
proach constantly and significantly improves the
1020
baseline model. Moreover, our approach does not
require any adaptation or tuning and uses a compu-
tation for the similarity function based on Wikipedia
which is faster than the computation of tools based
on WordNet or other resources (Basili et al, 2006).
The remainder of the paper is organized as fol-
lows: Section 2 critically reviews the previous work
by highlighting the need of generalizing lexico-
syntactic rules. Section 3 describes lexical similar-
ity approaches, which can serve the generalization
purpose. Section 4 describes how to integrate lex-
ical similarity in syntactic structures using syntac-
tic/semantic tree kernels (SSTK) whereas Section 5
shows how to use SSTK in a kernel-based RTE sys-
tem. Section 6 describes the experiments and re-
sults. Section 7 discusses the efficiency and accu-
racy of our system compared with other RTE sys-
tems. Finally, we draw the conclusions in Section
8.
2 Related work
Lexical-syntactic rules are largely used in textual en-
tailment recognition systems (e.g., (Bar-Haim et al,
2007; Dinu and Wang, 2009)) as they conveniently
encode world knowledge into linguistic structures.
For example, to decide whether the simple sentences
are in the entailment relation:
T2 ??H2
T2 ?In 1980 Chapman killed Lennon.?
H2 ?John Lennon died in 1980.?
we need a lexical-syntactic rule such as:
?3 = XkilledY ? Ydied
along with such rules, the temporal information
should be taken into consideration.
Given the importance of lexical-syntactic rules in
RTE, many methods have been proposed for their
extraction from large corpora (e.g., (Lin and Pantel,
2001; Szpektor and Dagan, 2008)). Unfortunately,
these unsupervised methods in general produce rules
that can hardly be used: noise and coverage are the
most critical issues.
Supervised approaches were experimented in
(Zanzotto and Moschitti, 2006; Zanzotto et al,
2009), where lexical-syntactic rules were derived
from examples in terms of complex relational fea-
tures. This approach can easily miss some useful
information and rules. For example, given the pair
?T2,H2?, to derive the entailment value of the fol-
lowing case:
T4 ??H4
T4 ?In 1963 Lee Harvey Oswald mur-
dered JFK?
H4 ?JFK died in 1963?
we can only rely on this relatively interesting
lexical-syntactic rule (i.e. which is in common be-
tween the two examples):
?5 = (V P (V BZ)(NP X )) ? (S(NP X )(V P (V BZ died)))
Unfortunately, this can be extremely misleading
since it also derives similar decisions for the follow-
ing example:
T6 ??H6
T6 ?In 1956 JFK met Marilyn Monroe?
H6 ?Marilyn Monroe died in 1956?
The problem is that the pairs ?T2,H2? and
?T4,H4? share more meaningful features than the
rule ?5, which should make the difference with re-
spect to the relation between the pairs ?T2,H2? and
?T6,H6?. Indeed, the word ?kill? is more semanti-
cally related to ?murdered? than to ?meet?. Using
this information, it is possible to derive more effec-
tive rules from training examples.
There are several solutions for taking this infor-
mation into account, e.g. by using FrameNet se-
mantics (e.g., like in (Burchardt et al, 2007)), it is
possible to encode a lexical-syntactic rule using the
KILLING and the DEATH frames, i.e.:
?7 = KILLING(Killer :
X ,
V ictim : Y ) ?
DEATH(
Protagonist : Y )
However, to use this model, specific rules and a
semantic role labeler on the specific corpora are
needed.
3 Lexical similarities
Previous research in computational linguistics has
produced many effective lexical similarity mea-
sures based on many different resources or corpora.
For example, WordNet similarities (Pedersen et al,
2004) or Latent Semantic Analysis over a large cor-
pus are widely used in many applications and for
1021
the definition of kernel functions, e.g. (Basili et al,
2006; Basili et al, 2005; Bloehdorn et al, 2006).
In this section we present the main component of
our new kernel, i.e. a lexical similarity derived from
different resources. This is used inside the syntac-
tic/semantic tree kernel defined in (Bloehdorn and
Moschitti, 2007a; Bloehdorn and Moschitti, 2007b)
to enhance the basic tree kernel functions.
3.1 WordNet Similarities
WordNet similarities have been heavily used in pre-
vious NLP work (Chan and Ng, 2005; Agirre et al,
2009). All WordNet similarities apply to pairs of
synonymy sets (synsets) and return a value indicat-
ing their semantic relatedness. For example, the fol-
lowing measures, that we use in this study, are based
on path lengths between concepts in the Wordnet Hi-
erarchy:
Path the measure is equal to the inverse of the
shortest path length (path length) between two
synsets c1 and c2 in WordNet
SimPath =
1
path length(c1, c2)
(1)
WUP the Wu and Palmer (Wu and Palmer, 1994)
similarity metric is based on the depth of two given
synsets c1 and c2 in the WordNet taxonomy, and the
depth of their least common subsumer (lcs). These
are combined into a similarity score:
SimWUP =
2? depth(lcs)
depth(c1) + depth(c2)
(2)
Wordnet similarity measures on synsets can be
extended to similarity measures between words as
follows:
?S(w1, w2) = max(c1,c2)?C1?C2SimS(c1, c2)
(3)
where S is Path or WUP and Ci is the set of the
synsets related to the word wi.
3.2 Distributional Semantic Similarity
Latent Semantic Analysis (LSA) is one of the
corpus-based measure of distributional semantic
similarity, proposed by (Landauer et al, 1998).
Words ~wi are represented in a document space. Each
feauture is a document and its value is the frequency
of the word in the document. The similarity is gen-
erally computed as a cosine similarity:
?LSI(w1, w2) =
~w1 ~w2
|| ~w1|| ? || ~w2||
(4)
In our approach we define a proximity matrix P
where pi,j represents ?LSI(wi, wj) The core of our
approach lies on LSI (Latent Semantic Indexing)
over a large corpus. We used singular value de-
composition (SVD) to build the proximity matrix
P = DDT from a large corpus, represented by its
word-by-document matrix D.
SVD decomposes D (weighted matrix of term
frequencies in a collection of text) into three matri-
ces U?V T , where U (matrix of term vectors) and
V (matrix of document vectors) are orthogonal ma-
trices whose columns are the eigenvectors of DDT
and DTD respectively, and ? is the diagonal matrix
containing the singular value of D.
Given such decomposition, P can be obtained as
Uk?2kUTk , where Uk is the matrix containing the first
k columns of U and k is the dimensionality of the
latent semantic space. This is efficiently used to re-
duce the memory requirements while retaining the
information. Finally we computed the term simi-
larity using the cosine measure in the vector space
model (VSM).
Generally, LSA can be observed as a way to over-
come some of the drawbacks of the standard vector
space model, such as sparseness and dimensionality.
In other words, the LSA similarity is computed in
a lower dimensional space, in which second-order
relations among words and documents are exploited
(Mihalcea et al, 2006).
It is worth mentioning that the LSA similarity
measure depends on the selected corpus but it ben-
efits from a higher computation speed in compari-
son to the construction of the similarity matrix based
on the WordNet Similarity package (Pedersen et al,
2004).
4 Lexical similarity in Syntactic Tree
Kernels
Section 2 has shown that the role of the syntax is im-
portant in extracting generalized rules for RTE but it
is not enough. Therefore, the lexical similarity de-
scribed in the previous section should be taken into
1022
SPP
IN
CD
1963
NP
NNP
Lee
NNP
Harvey
NNP
Oswald
VP
VBN
murdered
NNP
JFK
?
S
PP
IN
CD
NP VP
VBN
murdered
NNP
S
PP
IN
CD
NP VP
VBN NNP
S
PP
IN
NP VP
VBN NNP
VP
VBN
murdered
NNP
JFK
VP
VBN
murdered
NNP
VP
VBN NNP
JFK
VP
VBN
killed
NNP
Kennedy
Figure 1: A syntactic parse tree (on the left) along with some of its fragments. After the bar there is an important
fragment from a semantically similar sentence, which cannot be matched by STK but it is matched by SSTK.
account in the model definition. Since tree kernels
have been shown to be very effective for exploit-
ing syntactic information in natural language tasks, a
promising idea is to merge together the two different
approaches, i.e. tree kernels and semantic similari-
ties.
4.1 Syntactic Tree Kernel (STK)
Tree kernels compute the number of common sub-
structures between two trees T1 and T2 without ex-
plicitly considering the whole fragment space. The
standard definition of the STK, given in (Collins and
Duffy, 2002), allows for any set of nodes linked by
one or more entire production rules to be valid sub-
structures. The formal characterization is given in
(Collins and Duffy, 2002) and is reported hereafter:
Let F = {f1, f2, . . . , f|F|} be the set of tree
fragments and ?i(n) be an indicator function,
equal to 1 if the target fi is rooted at node n
and equal to 0 otherwise. A tree kernel func-
tion over T1 and T2 is defined as TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), where NT1 and NT2
are the sets of nodes in T1 and T2, respectively and
?(n1, n2) =
?|F|
i=1 ?i(n1)?i(n2).
? function counts the number of subtrees rooted
in n1 and n2 and can be evaluated as follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminal symbols) then ?(n1, n2) = ?;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) = ?
?l(n1)
j=1 (1 + ?(cn1(j), cn2(j))),
where l(n1) is the number of children of n1,
cn(j) is the j-th child of node n and ? is a de-
cay factor penalizing larger structures.
Figure 1 shows some fragments (out of the over-
all 472) of the syntactic parse tree on the left, which
is derived from the text T4. These fragments sat-
isfy the constraint that grammatical rules cannot be
broken. For example, (VP (VBN (murdered) NNP
(JFK))) is a valid fragment whereas (VP (VBN (mur-
dered)) is not. One drawback of such kernel is that
two sentences expressing similar semantics but with
different lexicals produce structures which will not
be matched. For example, after the vertical bar
there is a fragment, extracted from the parse tree
of a semantically identical sentences: In 1963
Oswald killed Kennedy. In this case, much
less matches will be counted by the kernel function
applied to such parse trees and the one of T4. In par-
ticular, the complete VP subtree will not be matched.
To tackle this problem the Syntactic Semantic
Tree Kernel (SSTK) was defined in (Bloehdorn and
Moschitti, 2007a); hereafter, we report its definition.
4.2 Syntactic Semantic Tree kernels (SSTK)
An SSTK produces all the matches of STK. More-
over, the fragments, which are identical but for their
lexical nodes, produce a match proportional to the
product of the similarity between their correspond-
ing words. This is a sound definition. Indeed, since
the structures are the same, each word in position i
of the first fragment can be associated with a word
located in the same position i of the second frag-
ment. More formally, the fast evaluation of ? for
STK can be used for computing the semantic ? for
SSTK by simply adding the following step
0. if n1 and n2 are pre-terminals and label(n1) =
label(n2) then ?(n1, n2) = ??S(ch1n1 , ch1n2),
where label(ni) is the label of node ni and ?S is
a term similarity kernel, e.g. based on Wikipedia,
Wordnet or BNC, defined in Section 3. Note that:
(a) since n1 and n2 are pre-terminals of a parse tree
1023
they can have only one child (i.e. ch1n1 and ch1n2 )
and such children are words and (b) Step 2 of the
original ? evaluation is no longer necessary.
For example, the fragments: (VP (VBN (murdered)
NNP (JFK))) has a match with (VP (VBN (killed)
NNP (Kennedy))) equal to ?S(murdered, kill) ?
?S(JFK,Kennedy).
Beside the novelty of taking into account tree
fragments that are not identical it should be noted
that the lexical semantic similarity is constrained
in syntactic structures, which limit errors/noise due
to incorrect (or, as in our case, not provided) word
sense disambiguation.
Finally, it should be noted that when a valid ker-
nel is used in place of ?S , SSTK is a valid kernel for
definition of convolution kernels (Haussler, 1999).
Since the matrix P derived by applying LSA pro-
duces a semi-definite matrix (see (Cristianini and
Holloway, 2001)) we can always use the similarity
matrix derived by LSA in SSTK. In case of Wordnet,
the validity of the kernel will depend of the kind of
similarity used. In our experiments, we have carried
out single value decomposition and we have verified
that our Wordenet matrices, Path and WUP, are in-
deed positive semi-definite.
5 Kernels for Textual Entailment
Recognition
In this section, we describe how we use the syntac-
tic tree kernel (STK) and the semantic/syntactic tree
kernel (SSTK) for modeling lexical-syntactic ker-
nels for textual entailment recognition. We build
on the kernel described in (Zanzotto and Moschitti,
2006; Zanzotto et al, 2009) that can model lexical-
syntactic rules with variables (i.e., first-order rules).
5.1 Anchoring and pruning
Kernels for modeling lexical-syntactic rules with
variables presuppose that words in texts T are ex-
plicitly related to words in hypotheses H . This cor-
relation is generally called anchoring and it is imple-
mented with placeholders that co-index the syntactic
trees derived from T and H . Words and intermediate
nodes are co-indexed when they are equal or similar.
For example, in the pair:
T8 ??H8
T8 ?Lee Harvey Oswald was born in
New Orleans, Louisiana, and was
of English, German, French and
Irish ancestry. In 1963 1 Oswald
murdered JFK 2 ?
H8 ?JFK 2 died in 1963 1 ?
Moreover, the set of anchors also allows us to
prune fragments of the text T that are irrelevant
for the final decision: we can discard sentences
or phrases uncovered by placeholders. For exam-
ple, in the pair ?T8,H8?, we can infer that ?Lee
H. . . ancestry? is not a relevant fragment and remove
it. This allows us to focus on the critical part for de-
termining the entailment value.
5.2 Kernels for capturing lexical-syntactic
rules
Once placeholders are available in the entailment
pairs, we can apply the model proposed in (Zan-
zotto et al, 2009). This derives the maximal simi-
larity between pairs of T and H based on the lexico-
syntactic information encoded by the syntactic parse
trees of T and H enriched with placeholders. More
formally, the original kernel is based on the follow-
ing equation:
maxSTK(?T,H?, ?T ?, H ??) = maxc?C (5)
(STK(t(T, c), t(T ?, i)) + STK(t(H, c), t(H ?, i)),
where: (i) C is the set of all bijective mappings be-
tween the placeholders (i.e., the possible variables)
from ?T,H? into ?T ?,H ??; (ii) c ? C is a substitu-
tion function, which implements such mapping; (iii)
t(?, c) returns the syntactic tree enriched with place-
holders replaced by means of the substitution c; and
(iv) STK(?1, ?2) is a tree kernel function.
The new semantic-syntactic kernel for lexical-
syntactic rules, maxSSTK, substitutes STK with
SSTK in Eq. 5 thus enlarging the coverage of the
matching between the pairs of texts and the pairs of
hypotheses.
6 Experiments
The aim of the experiments is to investigate if our
RTE system exploiting syntactic semantic kernels
(SSTK) can effectively derive generalized lexico-
syntactic rules. In more detail, first, we determine
the best lexical similarity suitable for the task, i.e.
1024
No Semantic Wiki BNC Path WUP
RTE2 j = 1 63.12 63.5 62.75 62.88 63.88
j = 0.9 63.38 64.75 62.26 63.88 64.25
RTE3 j = 1 66.88 67.25 67.25 66.88 66.5
j = 0.9 67.25 67.75 67.5 67.12 67.38
RTE5 j = 1 65.5 66.5 65.83 66 66
j = 0.9 65.5 66.83 65.67 66 66.33
Table 1: Accuracy of plain (WOK+STK+maxSTK) and Semantic Lexico-Syntactic (WOK+SSTK+maxSSTK) Ker-
nels. The latter according to different similarities
distributional vs. Wordnet-based approaches. Sec-
ond, we derive qualitative and quantitative proper-
ties, which justify the selection of one with respect
to the other.
For this purpose, we tested four different version
of SSTK, i.e. using Path, WUP, BNC and WIKI
lexical similarities on three different RTE datasets.
These correspond to the three different challenges in
which the development set was provided.
6.1 Experimental Setup
We used the data from three recognizing textual en-
tailment challenge: RTE2 (Bar-Haim et al, 2006),
RTE3 (Giampiccolo et al, 2007), and RTE5, along
with the standard split between training and test sets.
We did not use RTE1 as it was differently built from
the others and RTE4 as it does not contain the devel-
opment set.
We used the following publicly available tools:
the Charniak Parser (Charniak, 2000) for pars-
ing sentences and SVM-light-TK (Moschitti, 2006;
Joachims, 1999), in which we coded our new kernels
for RTE. Additionally, we used the Jiang&Conrath
(J&C) distance (Jiang and Conrath, 1997) com-
puted with wn::similarity package (Pedersen
et al, 2004) to measure the similarity between T and
H . This similarity is also used to define the text-
hypothesis word overlap kernel (WOK).
The distributional semantics is captured by means
of LSA: we used the java Latent Semantic Indexing
(jLSI) tool (Giuliano, 2007). In particular, we pre-
computed the word-pair matrices for RTE2, RTE3,
and RTE5. We built different LSA matrices from
the British National Corpus (BNC) and Wikipedia
(Wiki). The British National Corpus (BNC) is a bal-
anced synchronic text corpus containing 100 mil-
lion words with morpho-syntactic annotation. For
Wikipedia, we created a model from the 200,000
most visited Wikipedia articles, after cleaning the
unnecessary markup tags. Articles are our doc-
uments for creating the term-by-document matrix.
Wikipedia provides the largest coverage knowledge
resource developed by a community, besides the no-
ticeable coverage of named entities. This further
motivates the design of a similarity measure. We
also consider two typical WordNet similarities (i.e.,
Path and WUP, respectively) as described in Sec.
3.1.
The main RTE model that we consider is consti-
tuted by three main kernels:
? WOK, i.e. the kernel based on only the text-
hypothesis lexical overlapping words (this is an
intra-pair similarity);
? STK, i.e. the sum of the standard tree kernel
(see Section 4.1) applied to the two text parse-
trees and the two hypothesis parse trees;
? SSTK, i.e. the same as STK with the use of
lexical similarities as explained in Section 4.2;
? maxSTK and maxSSTK, i.e. the kernel for
RTE, illustrated in Section 5.2, where the lat-
ter exploits similarity since it uses SSTK in Eq.
5.
Note that the model presented in (Zanzotto et al,
2009), our baseline, corresponds to the combination
kernel: WOK+maxSTK. In this paper, in addition to
the role of lexical similarities, we also study several
combinations (we just need to sum the separated ker-
nels), i.e. WOK+STK+maxSTK, SSTK+maxSSTK,
WOK+SSTK+maxSSTK and WOK+maxSSTK.
Finally, we measure the performance of our sys-
tem with the standard accuracy and then we deter-
mine the statistical significance by using the model
1025
STK SSTK maxSTK maxSSTK STK+maxSTK SSTK+maxSSTK ?
RTE2 +WOK 61.5 61.12 63.88 64.12 63.12 63.50 60.62
52.62 52.75 61.25 59.38 61.25 58.75 -
RTE3 +WOK 66.38 66.5 66.5 67.0 66.88 67.25 66.75
53.25 54.5 62.25 64.38 63.12 63.62 -
RTE5 +WOK 62.0 62.0 64.83 64.83 65.5 66.5 60.67
54.33 57.33 63.33 62.67 61.83 62.67 -
Table 2: Comparing different lexico-syntactic kernels with Wiki-based semantic kernels
described in (Yeh, 2000) and implemented in (Pado?,
2006).
6.2 Distributional vs. WordNet-based
Semantics
The first experiment compares the basic kernel, i.e.
WOK+STK+maxSTK, with the new semantic ker-
nel, i.e. WOK+SSTK+maxSSTK, where SSTK
and maxSSTK encode four different kinds of sim-
ilarities, BNC, WIKI, WUP and Path. The aim
is twofold: understanding if semantic similarities
can be effectively used to derive generalized lexico-
syntactic rules and to determine the best similarity
model.
Table 1 shows the results according to No Seman-
tics, Wiki, BNC, Path and WUP. The three pairs of
rows represent the results over the three different
datasets, i.e., RTE2, RTE3, and RTE5. For each
pair, we have two rows representing a different j
parameter of SVM. An increase of j augments the
weight of positive with respect to negative examples
and during learning it tunes-up the Recall/Precision
rate. We use two values j = 1 (the default value)
and j = 0.9 (selected during a preliminary experi-
ment on a validation set on RTE2). j = 0.9 was used
to minimally increase the Precision, considering that
the semantic model tends to improve the Recall.
The results show that:
? WIKI semantics constantly improves the basic
kernel (no Semantics) for any datasets or pa-
rameter.
? The distributional semantics is almost always
better than the WordNet-based one.
? In one case WUP improves WIKI, i.e. 63.88 vs
63.5 and in another case BNC reaches WIKI,
i.e. 67.25 but this happens for the default values
of the j parameters, i.e. j = 1, which was not
selected by our limited parameter validation.
Finally, the difference between the accuracy of the
best WIKI kernels and the No Semantic kernels are
statistically significant (p << 0.05).
6.3 Kernel Comparisons
The previous experiments (Sec. 6.2) show that
Wikipedia-based distributional semantics provides
an effective similarity to generalize lexico-syntactic
rules (features). As our RTE kernel is a composition
of other basic kernels, we experimented with dif-
ferent combinations to understand the role of each
component. Moreover, to obtain results independent
of parameterization we used the default parameter j.
Table 2 reports the accuracy of different kernels
and their combinations on different RTE datasets.
Each row describes the results for each dataset and
it is split in two according to the use of WOK or not
in the RTE model. In the each column, the different
kernels are reported. For example, the entry in the
4th column and the 2nd row refers to the accuracy of
SSTK in combination with WOK, i.e. WOK+SSTK
for the RTE2.
We observe that: first WOK produces a very high
accuracy in RTE challenges, i.e. 60.62, 66.75 and
60.67 and it is an essential component of RTE sys-
tems since its ablation always causes a large accu-
racy decrease. This is reasonable as the major source
of information to establish entailment between sen-
tences is their word overlap.
Second, STK and SSTK, when added to WOK,
improve it on RTE2 and RTE5 but do not improve
it on RTE3. This suggests a difficulty of exploiting
syntactic information for RTE3.
Third, maxSTK+WOK relevantly improves
WOK on RTE2 and RTE5 but fails in RTE3. Again,
the syntactic rules (with variables) which this kernel
1026
BNC WN WIKI
RTE2 0.55 0.42 0.83
RTE3 0.54 0.41 0.83
RTE5 0.45 0.34 0.82
Table 3: Coverage of the different resources for the words
of the three datasets
can provide are not enough general for RTE3. In
contrast, maxSSTK+WOK improves WOK on all
datasets thanks to its generalization ability.
Finally, STK and SSTK added to maxSTK+WOK
or to maxSSTK+WOK tend to produce an accuracy
increase, although not in every condition.
7 Discussion
7.1 Coverage and efficiency
As already mentioned, the practical use of
Wikipedia to design lexical similarities is motivated
by a large coverage. Deriving similarities from other
resources such as WordNet is more time-consuming.
To prove our claim, we performed an analysis on the
coverage and efficiency in computing the pair term
similarity.
Table 3 shows the coverage of the content words
of the three datasets. The coverage of Wikipedia is
about two times more than the other resources in all
experimented datasets.
Speed Milliseconds
LSA 0.54
WN with POS 5.3
WN without POS 15.2
Table 4: The comparison in terms of speed calculated
over 10000 pairs after loading the model.
Moreover, Table 4 shows that the computation
of the LSA matrix on Wikipedia is faster than us-
ing the WordNet similarity software (Pedersen et al,
2004). Even if the accuracy of some WordNet mod-
els can reach the one based on Wikipedia, the latter
is preferable for the smaller computational cost.
7.2 Comparison with previous work
The results of our models show that lexical se-
mantics for building more effective lexical-syntactic
rules is promising. Here, we compare our ap-
proaches with other RTE systems to show that our
Average Acc. Our rank # participants
RTE2 59.8 3rd 23
RTE3 64.5 4th 26
RTE5 61.5 4th 20
Table 5: Comparison with other approaches to RTE
results are indeed state-of-the-art. Unfortunately,
deriving a reasonable accuracy value to represent the
state-of-the-art is extremely difficult as many fac-
tors can determine the final score. For example, the
best systems in RTE2 and RTE3 (Giampiccolo et al,
2007) have an accuracy 10% higher than the others
but they generally use resources that are not publicly
available.
Table 5 shows the average accuracy of the partici-
pant systems, the rank of our system that we propose
in this paper and the number of participants. Our
model accuracy is absolutely above the average and
it is ranked at the top positions. We can also carry
out a finer comparison with respect to RTE2 (Bar-
Haim et al, 2006). Our system results are the best
when compared with systems using semantic mod-
els based on FrameNet, indeed the best ranked sys-
tem in this class, i.e., (Burchardt et al, 2007), scores
only 62.5. Among systems using logical inference,
our model is instead the 3rd out of 8 systems using
logical inference that perform worse than ours. Fi-
nally, it is the 2nd among systems using supervised
machine learning models.
8 Conclusion
In this paper we presented a model to effectively in-
clude semantics in lexical-syntactic features for tex-
tual entailment recognition. We have experimentally
shown that LSA-derived lexical semantics embed-
ded in syntactic structures is a promising approach.
The model that we have presented is one of the
best system in the RTE challenges. Additionally, in
contrast to many other methods it does not require
large sets of handcrafted or corpus extracted lexical-
syntactic rules.
Acknowledgements
The research of Alessandro Moschitti has been par-
tially supported by Trustworthy Eternal Systems via
Evolving Software, Data and Knowledge (EternalS,
project number FP7 247758).
1027
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca,
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and wordnet-based ap-
proaches. In NAACL ?09: Proc. HLT/NAACL.
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-
colo, and I. Magnini, B. Szpektor. 2006. The ii
PASCAL recognising textual entailment challenge. In
Proc. of the II PASCAL Challenges Workshop.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic level.
In AAAI?07: Proc. of the 22nd national conference on
Artificial intelligence.
R. Bar-Haim, J. Berant, and I. Dagan. 2009. A com-
pact forest for scalable inference over entailment and
paraphrase rules. In Proc. of EMNLP.
R. Basili, M. Cammisa, and A. Moschitti. 2005. Effec-
tive use of wordnet semantics via kernel-based learn-
ing. In CoNLL.
R. Basili, M. Cammisa, and A. Moschitti. 2006. A se-
mantic kernel to classify texts with very few training
examples. In Informatica.
S. Bloehdorn and A. Moschitti. 2007a. Combined syn-
tactic and semantic kernels for text classification. In
ECIR.
S. Bloehdorn and A. Moschitti. 2007b. Structure and se-
mantics for expressive text kernels. In Proc. of CIKM
?07.
S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.
2006. Semantic kernels for text classification based on
topological measures of feature similarity. In Proc. of
ICDM 06, Hong Kong, 2006.
J. Bos and K. Markert. 2005. Recognising textual entail-
ment with logical inference. In HLT ?05: Proc. of the
conference on HLT and EMNLP.
A. Burchardt, N. Reiter, S. Thater, and A. Frank. 2007.
Semantic Approach to Textual Entailment: System
Evaluation and Task Analysis. In Proc. of the 3rd-
PASCAL Workshop on Textual Entailment, Prague.
Y. S. Chan and H. T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proc. of IJCAI?05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the 1st NAACL conference.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: kernels over discrete struc-
tures, and the voted perceptron. In Proc. of ACL ?02.
N. Cristianini and R. Holloway. 2001. Latent semantic
kernels.
G. Dinu and R. Wang. 2009. Inference rules and their
application to recognizing textual entailment. In Proc.
of the EACL ?09.
D. Giampiccolo, B. Magnini, Ido Dagan, and B. Dolan.
2007. The third pascal recognizing textual entailment
challenge. In Proc. of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing.
Claudio Giuliano. 2007. jLSI a for latent se-
mantic indexing. http://tcc.itc.it/research/textec/tools-
resources/jLSI.html.
A. D. Haghighi, A. Y. Ng, and C. D. Manning. 2005.
Robust textual inference via graph matching. In HLT
?05: Proc. of the conference on HLT and EMNLP.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of the 10th ROCLING.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical.
Landauer, Foltz, and Laham. 1998. Introduction to latent
semantic analysis. In Discourse Processes 25.
D. Lin and P. Pantel. 2001. DIRT-discovery of inference
rules from text. In Proc. of the ACM KDD-01.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proc. of AAAI06.
Alessandro Moschitti. 2006. Making tree kernels practi-
cal for natural language learning. In Proc. of EACL.
Peter Norvig. 1987. A unified theory of inference for
text understanding. Technical report, USA.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of 5th NAACL.
D. Roth and M. Sammons. 2007. Semantic and logi-
cal inference model for textual entailment. In Proc.
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proc. of COLING ?08.
M. Tatu and D. Moldovan. 2005. A semantic approach
to recognizing textual entailment. In HLT ?05: Proc.
of HLT/EMNLP.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical
selection. In Proc. of ACL.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proc. of ACL
2000, Morristown, NJ, USA.
F. M. Zanzotto and A. Moschitti. 2006. Automatic learn-
ing of textual entailments with cross-pair similarities.
In Proc. of ACL ?06.
F. M. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine learning approach to textual en-
tailment recognition. NATURAL LANGUAGE ENGI-
NEERING.
1028
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 101?106,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Extracting Opinion Expressions and Their Polarities ? Exploration of
Pipelines and Joint Models
Richard Johansson and Alessandro Moschitti
DISI, University of Trento
Via Sommarive 14, 38123 Trento (TN), Italy
{johansson, moschitti}@disi.unitn.it
Abstract
We investigate systems that identify opinion
expressions and assigns polarities to the ex-
tracted expressions. In particular, we demon-
strate the benefit of integrating opinion ex-
traction and polarity classification into a joint
model using features reflecting the global po-
larity structure. The model is trained using
large-margin structured prediction methods.
The system is evaluated on the MPQA opinion
corpus, where we compare it to the only previ-
ously published end-to-end system for opinion
expression extraction and polarity classifica-
tion. The results show an improvement of be-
tween 10 and 15 absolute points in F-measure.
1 Introduction
Automatic systems for the analysis of opinions ex-
pressed in text on the web have been studied exten-
sively. Initially, this was formulated as a coarse-
grained task ? locating opinionated documents ?
and tackled using methods derived from standard re-
trieval or categorization. However, in recent years
there has been a shift towards a more detailed task:
not only finding the text expressing the opinion, but
also analysing it: who holds the opinion and to what
is addressed; it is positive or negative (polarity);
what its intensity is. This more complex formula-
tion leads us deep into NLP territory; the methods
employed here have been inspired by information
extraction and semantic role labeling, combinatorial
optimization and structured machine learning.
A crucial step in the automatic analysis of opinion
is to mark up the opinion expressions: the pieces of
text allowing us to infer that someone has a partic-
ular feeling about some topic. Then, opinions can
be assigned a polarity describing whether the feel-
ing is positive, neutral or negative. These two tasks
have generally been tackled in isolation. Breck et al
(2007) introduced a sequence model to extract opin-
ions and we took this one step further by adding a
reranker on top of the sequence labeler to take the
global sentence structure into account in (Johansson
and Moschitti, 2010b); later we also added holder
extraction (Johansson and Moschitti, 2010a). For
the task of classifiying the polarity of a given expres-
sion, there has been fairly extensive work on suitable
classification features (Wilson et al, 2009).
While the tasks of expression detection and polar-
ity classification have mostly been studied in isola-
tion, Choi and Cardie (2010) developed a sequence
labeler that simultaneously extracted opinion ex-
pressions and assigned polarities. This is so far
the only published result on joint opinion segmenta-
tion and polarity classification. However, their ex-
periment lacked the obvious baseline: a standard
pipeline consisting of an expression identifier fol-
lowed by a polarity classifier.
In addition, while theirs is the first end-to-end sys-
tem for expression extraction with polarities, it is
still a sequence labeler, which, by construction, is
restricted to use simple local features. In contrast, in
(Johansson and Moschitti, 2010b), we showed that
global structure matters: opinions interact to a large
extent, and we can learn about their interactions on
the opinion level by means of their interactions on
the syntactic and semantic levels. It is intuitive that
this should also be valid when polarities enter the
101
picture ? this was also noted by Choi and Cardie
(2008). Evaluative adjectives referring to the same
evaluee may cluster together in the same clause or
be dominated by a verb of categorization; opinions
with opposite polarities may be conjoined through a
contrastive discourse connective such as but.
In this paper, we first implement two strong base-
lines consisting of pipelines of opinion expression
segmentation and polarity labeling and compare
them to the joint opinion extractor and polarity clas-
sifier by Choi and Cardie (2010). Secondly, we ex-
tend the global structure approach and add features
reflecting the polarity structure of the sentence. Our
systems were superior by between 8 and 14 absolute
F-measure points.
2 The MPQA Opinion Corpus
Our system was developed using version 2.0 of the
MPQA corpus (Wiebe et al, 2005). The central
building block in the MPQA annotation is the opin-
ion expression. Opinion expressions belong to two
categories: Direct subjective expressions (DSEs)
are explicit mentions of opinion whereas expressive
subjective elements (ESEs) signal the attitude of the
speaker by the choice of words. Opinions have two
features: polarity and intensity, and most expres-
sions are also associated with a holder, also called
source. In this work, we only consider polarities,
not intensities or holders. The polarity takes the val-
ues POSITIVE, NEUTRAL, NEGATIVE, and BOTH;
for compatibility with Choi and Cardie (2010), we
mapped BOTH to NEUTRAL.
3 The Baselines
In order to test our hypothesis against strong base-
lines, we developed two pipeline systems. The first
part of each pipeline extracts opinion expressions,
and this is followed by a multiclass classifier assign-
ing a polarity to a given opinion expression, similar
to that described by Wilson et al (2009).
The first of the two baselines extracts opinion ex-
pressions using a sequence labeler similar to that by
Breck et al (2007) and Choi et al (2006). Sequence
labeling techniques such as HMMs and CRFs are
widely used for segmentation problems such as
named entity recognition and noun chunk extraction.
We trained a first-order labeler with the discrimi-
native training method by Collins (2002) and used
common features: words, POS, lemmas in a sliding
window. In addition, we used subjectivity clues ex-
tracted from the lexicon by Wilson et al (2005).
For the second baseline, we added our opinion ex-
pression reranker (Johansson and Moschitti, 2010b)
on top of the expression sequence labeler.
Given an expression, we use a classifier to assign
a polarity value: positive, neutral, or negative. We
trained linear support vector machines to carry out
this classification. The problem of polarity classi-
fication has been studied in detail by Wilson et al
(2009), who used a set of carefully devised linguis-
tic features. Our classifier is simpler and is based
on fairly shallow features: words, POS, subjectivity
clues, and bigrams inside and around the expression.
4 The Joint Model
We formulate the opinion extraction task as a struc-
tured prediction problem y? = arg maxy w ??(x, y).
where w is a weight vector and ? a feature extractor
representing a sentence x and a set y of polarity-
labeled opinions. This is a high-level formulation ?
we still need an inference procedure for the arg max
and a learner to estimate w on a training set.
4.1 Approximate Inference
Since there is a combinatorial number of ways to
segment a sentence and label the segments with po-
larities, the tractability of the arg max operation will
obviously depend on whether we can factorize the
problem for a particular ?.
Choi and Cardie (2010) used a Markov factor-
ization and could thus apply standard sequence la-
beling with a Viterbi arg max. However, in (Jo-
hansson and Moschitti, 2010b), we showed that a
large improvement can be achieved if relations be-
tween possible expressions are considered; these re-
lations can be syntactic or semantic in nature, for
instance. This representation breaks the Markov as-
sumption and the arg max becomes intractable. We
instead used a reranking approximation: a Viterbi-
based sequence tagger following Breck et al (2007)
generated a manageable hypothesis set of complete
segmentations, from which the reranking classifier
picked one hypothesis as its final output. Since the
set is small, no particular structure assumption (such
102
as Markovization) needs to be made, so the reranker
can in principle use features of arbitrary complexity.
We now adapt that approach to the problem of
joint opinion expression segmentation and polarity
classification. In that case, we not only need hy-
potheses generated by a sequence labeler, but also
the polarity labelings output by a polarity classifier.
The hypothesis generation thus proceeds as follows:
? For a given sentence, let the base sequence la-
beler generate up to ks sequences of unlabeled
opinion expressions;
? for every sequence, apply the base polarity
classifier to generate up to kp polarity labelings.
Thus, the hypothesis set size is at most ks ? kp. We
used a ks of 64 and a kp of 4 in all experiments.
To illustrate this process we give a hypothetical
example, assuming ks = kp = 2 and the sentence
The appeasement emboldened the terrorists. We
first generate the opinion expression sequence
candidates:
The [appeasement] emboldened the [terrorists]
The [appeasement] [emboldened] the [terrorists]
and in the second step we add polarity values:
The [appeasement]? emboldened the [terrorists]?
The [appeasement]? [emboldened]+ the [terrorists]?
The [appeasement]0 emboldened the [terrorists]?
The [appeasement]? [emboldened]0 the [terrorists]?
4.2 Features of the Joint Model
The features used by the joint opinion segmenter and
polarity classifier are based on pairs of opinions: ba-
sic features extracted from each expression such as
polarities and words, and relational features describ-
ing their interaction. To extract relations we used the
parser by Johansson and Nugues (2008) to annotate
sentences with dependencies and shallow semantics
in the PropBank (Palmer et al, 2005) and NomBank
(Meyers et al, 2004) frameworks.
Figure 1 shows the sentence the appeasement em-
boldened the terrorists, where appeasement and ter-
rorists are opinions with negative polarity, with de-
pendency syntax (above the text) and a predicate?
argument structure (below). The predicate em-
boldened, an instance of the PropBank frame
embolden.01, has two semantic arguments: the
Agent (A0) and the Theme (A1), realized syntacti-
cally as a subject and a direct object, respectively.
[appeasement] emboldened terroriststhe [
embolden.01
]The
NMOD SBJ OBJNMOD
A1A0
Figure 1: Syntactic and shallow semantic structure.
The model used the following novel features that
take the polarities of the expressions into account.
The examples are given with respect to the two ex-
pressions (appeasement and terrorists) in Figure 1.
Base polarity classifier score. Sum of the scores
from the polarity classifier for every opinion.
Polarity pair. For every pair of opinions in the
sentence, we add the pair of polarities: NEG-
ATIVE+NEGATIVE.
Polarity pair and syntactic path. For a pair
of opinions, we use the polarities and a
representation of the path through the syn-
tax tree between the expressions, follow-
ing standard practice from dependency-based
SRL (Johansson and Nugues, 2008): NEGA-
TIVE+SBJ?OBJ?+NEGATIVE.
Polarity pair and syntactic dominance. In addition
to the detailed syntactic path, we use a simpler
feature based on dominance, i.e. that one ex-
pression is above the other in the syntax tree. In
the example, no such feature is extracted since
neither of the expressions dominates the other.
Polarity pair and word pair. The polarity pair
concatenated with the words of the clos-
est nodes of the two expressions: NEGA-
TIVE+NEGATIVE+appeasement+terrorists.
Polarity pair and types and syntactic path. From
the opinion sequence labeler, we get the expres-
sion type as in MPQA (DSE or ESE): ESE-
NEGATIVE:+SBJ?OBJ?+ESE-NEGATIVE.
Polarity pair and semantic relation. When two
opinions are directly connected through a link
in the semantic structure, we add the role label
as a feature.
103
Polarity pair and words along syntactic path. We
follow the path between the expressions and
add a feature for every word we pass: NEG-
ATIVE:+emboldened+NEGATIVE.
We also used the features we developed in (Jo-
hansson and Moschitti, 2010b) to represent relations
between expressions without taking polarity into ac-
count.
4.3 Training the Model
To train the model ? find w ? we applied max-margin
estimation for structured outputs, a generalization of
the well-known support vector machine from binary
classification to prediction of structured objects.
Formally, for a training set T = {?xi, yi?}, where
the output space for the input xi is Yi, we state the
learning problem as a quadratic program:
minimize w ?w?
2
subject to w(?(xi, yi)? ?(xi, yij)) ? ?(yi, yij),
??xi, yi? ? T , yij ? Yi
Since real-world data tends to be noisy, we may
regularize to reduce overfitting and introduce a pa-
rameter C as in regular SVMs (Taskar et al, 2004).
The quadratic program is usually not solved directly
since the number of constraints precludes a direct
solution. Instead, an approximation is needed in
practice; we used SVMstruct (Tsochantaridis et al,
2005; Joachims et al, 2009), which finds a solu-
tion by successively finding the most violated con-
straints and adding them to a working set. The
loss ? was defined as 1 minus a weighted combi-
nation of polarity-labeled and unlabeled intersection
F-measure as described in Section 5.
5 Experiments
Opinion expression boundaries are hard to define
rigorously (Wiebe et al, 2005), so evaluations of
their quality typically use soft metrics. The MPQA
annotators used the overlap metric: an expression
is counted as correct if it overlaps with one in the
gold standard. This has also been used to evaluate
opinion extractors (Choi et al, 2006; Breck et al,
2007). However, this metric has a number of prob-
lems: 1) it is possible to ?fool? the metric by creat-
ing expressions that cover the whole sentence; 2) it
does not give higher credit to output that is ?almost
perfect? rather than ?almost incorrect?. Therefore,
in (Johansson and Moschitti, 2010b), we measured
the intersection between the system output and the
gold standard: every compared segment is assigned
a score between 0 and 1, as opposed to strict or over-
lap scoring that only assigns 0 or 1. For compatibil-
ity we present results in both metrics.
5.1 Evaluation of Segmentation with Polarity
We first compared the two baselines to the new
integrated segmentation/polarity system. Table 1
shows the performance according to the intersec-
tion metric. Our first baseline consists of an expres-
sion segmenter and a polarity classifier (ES+PC),
while in the second baseline we also add the ex-
pression reranker (ER) as we did in (Johansson and
Moschitti, 2010b). The new reranker described in
this paper is referred to as the expression/polarity
reranker (EPR). We carried out the evaluation using
the same partition of the MPQA dataset as in our
previous work (Johansson and Moschitti, 2010b),
with 541 documents in the training set and 150 in
the test set.
System P R F
ES+PC 56.5 38.4 45.7
ES+ER+PC 53.8 44.5 48.8
ES+PC+EPR 54.7 45.6 49.7
Table 1: Results with intersection metric.
The result shows that the reranking-based mod-
els give us significant boosts in recall, following
our previous results in (Johansson and Moschitti,
2010b), which also mainly improved the recall. The
precision shows a slight drop but much lower than
the recall improvement.
In addition, we see the benefit of the new reranker
with polarity interaction features. The system using
this reranker (ES+PC+EPR) outperforms the expres-
sion reranker (ES+ER+PC). The performance dif-
ferences are statistically significant according to a
permutation test: precision p < 0.02, recall and F-
measure p < 0.005.
5.2 Comparison with Previous Results
Since the results by Choi and Cardie (2010) are the
only ones that we are aware of, we carried out an
104
evaluation in their setting.1 Table 2 shows our fig-
ures (for the two baselines and the new reranker)
along with theirs, referred to as C & C (2010).
The table shows the scores for every polarity value.
For compatibility with their evaluation, we used the
overlap metric and carried out the evaluation us-
ing a 10-fold cross-validation procedure on a 400-
document subset of the MPQA corpus.
POSITIVE P R F
ES+PC 59.3 46.2 51.8
ES+ER+PC 53.1 50.9 52.0
ES+PC+EPR 58.2 49.3 53.4
C & C (2010) 67.1 31.8 43.1
NEUTRAL P R F
ES+PC 61.0 49.3 54.3
ES+ER+PC 55.1 57.7 56.4
ES+PC+EPR 60.3 55.8 58.0
C & C (2010) 66.6 31.9 43.1
NEGATIVE P R F
ES+PC 71.6 52.2 60.3
ES+ER+PC 65.4 58.2 61.6
ES+PC+EPR 67.6 59.9 63.5
C & C (2010) 76.2 40.4 52.8
Table 2: Results with overlap metric.
The C & C system shows a large precision
bias despite being optimized with respect to the
recall-promoting overlap metric. In recall and F-
measure, their system scores much lower than our
simplest baseline, which is in turn clearly outper-
formed by the stronger baseline and the polarity-
based reranker. The precision is lower than for C
& C overall, but this is offset by recall boosts for
all polarities that are much larger than the precision
drops. The polarity-based reranker (ES+PC+EPR)
soundly outperforms all other systems.
6 Conclusion
We have studied the implementation of end-to-end
systems for opinion expression extraction and po-
larity labeling. We first showed that it was easy to
1In addition to polarity, their system also assigned opinion
intensity which we do not consider here.
improve over previous results simply by combining
an opinion extractor and a polarity classifier; the im-
provements were between 7.5 and 11 points in over-
lap F-measure.
However, our most interesting result is that a joint
model of expression extraction and polarity label-
ing significantly improves over the sequential ap-
proach. This model uses features describing the in-
teraction of opinions through linguistic structures.
This precludes exact inference, but we resorted to
a reranker. The model was trained using approx-
imate max-margin learning. The final system im-
proved over the baseline by 4 points in intersection
F-measure and 7 points in recall. The improvements
over Choi and Cardie (2010) ranged between 10 and
15 in overlap F-measure and between 17 and 24 in
recall.
This is not only of practical value but also con-
firms our linguistic intuitions that surface phenom-
ena such as syntax and semantic roles are used in
encoding the rhetorical organization of the sentence,
and that we can thus extract useful information from
those structures. This would also suggest that we
should leave the surface and instead process the dis-
course structure, and this has indeed been proposed
(Somasundaran et al, 2009). However, automatic
discourse structure analysis is still in its infancy
while syntactic and shallow semantic parsing are rel-
atively mature.
Interesting future work should be devoted to ad-
dress the use of structural kernels for the proposed
reranker. This would allow to better exploit syn-
tactic and shallow semantic structures, e.g. as in
(Moschitti, 2008), also applying lexical similarity
and syntactic kernels (Bloehdorn et al, 2006; Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b; Moschitti, 2009).
Acknowledgements
The research described in this paper has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant 231126: LivingKnowledge ? Facts, Opin-
ions and Bias in Time, and under grant 247758:
Trustworthy Eternal Systems via Evolving Software,
Data and Knowledge (EternalS).
105
References
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and
Alessandro Moschitti. 2006. Semantic kernels for text
classification based on topological measures of feature
similarity. In Proceedings of ICDM 06, Hong Kong,
2006.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In IJCAI
2007, Proceedings of the 20th International Joint Con-
ference on Artificial Intelligence, pages 2683?2688,
Hyderabad, India.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 793?801, Honolulu, United
States.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
269?274, Uppsala, Sweden.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 431?439, Sydney, Australia.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 1?8.
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting-plane training of structural SVMs. Ma-
chine Learning, 77(1):27?59.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis. In
Proceedings of the 23rd International Conference of
Computational Linguistics (Coling 2010), pages 519?
527, Beijing, China.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31, Boston, United
States.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ?08, NY, USA.
Alessandro Moschitti. 2009. Syntactic and Seman-
tic Kernels for Short Text Pair Categorization. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 576?584,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?105.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
EMNLP 2009: conference on Empirical Methods in
Natural Language Processing.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin Markov networks. In Advances in Neu-
ral Information Processing Systems 16, Vancouver,
Canada.
Iannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin meth-
ods for structured and interdependent output variables.
Journal of Machine Learning Research, 6(Sep):1453?
1484.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of Human Lan-
guage Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3):399?433.
106
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 277?282,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
End-to-End Relation Extraction Using Distant Supervision
from External Semantic Repositories
Truc-Vien T. Nguyen and Alessandro Moschitti
Department of Information Engineering and Computer Science
University of Trento
38123 Povo (TN), Italy
{nguyenthi,moschitti}@disi.unitn.it
Abstract
In this paper, we extend distant supervision
(DS) based on Wikipedia for Relation Extrac-
tion (RE) by considering (i) relations defined
in external repositories, e.g. YAGO, and (ii)
any subset of Wikipedia documents. We show
that training data constituted by sentences
containing pairs of named entities in target re-
lations is enough to produce reliable supervi-
sion. Our experiments with state-of-the-art re-
lation extraction models, trained on the above
data, show a meaningful F1 of 74.29% on a
manually annotated test set: this highly im-
proves the state-of-art in RE using DS. Addi-
tionally, our end-to-end experiments demon-
strated that our extractors can be applied to
any general text document.
1 Introduction
Relation Extraction (RE) from text as defined in
ACE (Doddington et al, 2004) concerns the extrac-
tion of relationships between two entities. This is
typically carried out by applying supervised learn-
ing, e.g. (Zelenko et al, 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005) by using a hand-
labeled corpus. Although, the resulting models are
far more accurate than unsupervised approaches,
they suffer from the following drawbacks: (i) they
require labeled data, which is usually costly to pro-
duce; (ii) they are typically domain-dependent as
different domains involve different relations; and
(iii), even in case the relations do not change, they
result biased toward the text feature distributions of
the training domain.
The drawbacks above would be alleviated if data
from several different domains and relationships
were available. A form of weakly supervision,
specifically named distant supervision (DS) when
applied to Wikipedia, e.g. (Banko et al, 2007; Mintz
et al, 2009; Hoffmann et al, 2010) has been recently
developed to meet the requirement above. The main
idea is to exploit (i) relation repositories, e.g. the
Infobox, x, of Wikipedia to define a set of relation
types RT (x) and (ii) the text in the page associated
with x to produce the training sentences, which are
supposed to express instances of RT (x).
Previous work has shown that selecting the sen-
tences containing the entities targeted by a given re-
lation is enough accurate (Banko et al, 2007; Mintz
et al, 2009) to provide reliable training data. How-
ever, only (Hoffmann et al, 2010) used DS to de-
fine extractors that are supposed to detect all the re-
lation instances from a given input text. This is a
harder test for the applicability of DS but, at the
same time, the resulting extractor is very valuable:
it can find rare relation instances that might be ex-
pressed in only one document. For example, the re-
lation President(Barrack Obama, United States) can
be extracted from thousands of documents thus there
is a large chance of acquiring it. In contrast, Pres-
ident(Eneko Agirre, SIGLEX) is probably expressed
in very few documents, increasing the complexity
for obtaining it.
In this paper, we extend DS by (i) considering
relations from semantic repositories different from
Wikipedia, i.e. YAGO, and (2) using training in-
stances derived from any Wikipedia document. This
allows for (i) potentially obtaining training data
277
for many more relation types, defined in different
sources; (ii) meaningfully enlarging the size of the
DS data since the relation examples can be extracted
from any Wikipedia document 1.
Additionally, by following previous work, we
define state-of-the-art RE models based on kernel
methods (KM) applied to syntactic/semantic struc-
tures. We use tree and sequence kernels that can
exploit structural information and interdependencies
among labels. Experiments show that our models
are flexible and robust to Web documents as we
achieve the interesting F1 of 74.29% on 52 YAGO
relations. This is even more appreciable if we ap-
proximately compare with the previous result on RE
using DS, i.e. 61% (Hoffmann et al, 2010). Al-
though the experiment setting is different from ours,
the improvement of about 13 absolute percent points
demonstrates the quality of our model.
Finally, we also provide a system for extracting
relations from any text. This required the definition
of a robust Named Entity Recognizer (NER), which
is also trained on weakly supervised Wikipedia data.
Consequently, our end-to-end RE system is appli-
cable to any document. This is another major im-
provement on previous work. The satisfactory RE
F1 of 67% for 52 Wikipedia relations suggests that
our model is also successfully applicable in real sce-
narios.
1.1 Related Work
RE generally relates to the extraction of relational
facts, or world knowledge from the Web (Yates,
2009). To identify semantic relations using ma-
chine learning, three learning settings have been ap-
plied, namely supervised methods, e.g. (Zelenko
et al, 2002; Culotta and Sorensen, 2004; Kamb-
hatla, 2004), semi supervised methods, e.g. (Brin,
1998; Agichtein and Gravano, 2000), and unsuper-
vised method, e.g. (Hasegawa et al, 2004; Banko
et al, 2007). Work on supervised Relation Extrac-
tion has mostly employed kernel-based approaches,
e.g. (Zelenko et al, 2002; Culotta and Sorensen,
2004; Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005; Zhang et al, 2005; Bunescu, 2007;
Nguyen et al, 2009; Zhang et al, 2006). However,
1Previous work assumes the page related to the Infobox as
the only source for the training data.
Algorithm 2.1: ACQUIRE LABELED DATA()
DS = ?
Y AGO(R) : Instances of Relation R
for each ?Wikipedia article : W ? ? Freebase
do
?
?
?
?
?
??
?
?
?
?
??
S ? set of sentences fromW
for each s ? S
do
?
?
?
??
?
?
??
E ? set of entities from s
for each E1 ? E and E2 ? E and
R ? Y AGO
do
?
?
?
if R(E1, E2) ? YAGO(R)
then DS ? DS ? {s,R+}
else DS ? DS ? {s,R?}
return (DS)
such approaches can be applied to few relation types
thus distant supervised learning (Mintz et al, 2009)
was introduced to tackle such problem. Another so-
lution proposed in (Riedel et al, 2010) was to adapt
models trained in one domain to other text domains.
2 Resources and Dataset Creation
In this section, we describe the resources for the cre-
ation of an annotated dataset based on distant super-
vision. We use YAGO, a large knowledge base of
entities and relations, and Freebase, a collection of
Wikipedia articles. Our procedure uses entities and
facts from YAGO to provide relation instances. For
each pair of entities that appears in some YAGO re-
lation, we retrieve all the sentences of the Freebase
documents that contain such entities.
2.1 YAGO
YAGO (Suchanek et al, 2007) is a huge seman-
tic knowledge base derived from WordNet and
Wikipedia. It comprises more than 2 million entities
(like persons, organizations, cities, etc.) and 20 mil-
lion facts connecting these entities. These include
the taxonomic Is-A hierarchy as well as semantic re-
lations between entities.
We use the YAGO version of 2008-w40-2 with a
manually confirmed accuracy of 95% for 99 rela-
tions. However, some of them are (a) trivial, e.g.
familyNameOf ; (b) numerical attributes that change
over time, e.g. hasPopulation; (c) symmetric, e.g.
hasPredecessor; (d) used only for data management,
e.g. describes or foundIn. Therefore, we removed
those irrelevant relations and obtained 1,489,156 in-
stances of 52 relation types to be used with our DS
approach.
278
2.2 Freebase
To access to Wikipedia documents, we used Free-
base (March 27, 2010 (Metaweb Technologies,
2010)), which is a dump of the full text of all
Wikipedia articles. For our experiments, we used
100,000 articles. Out of them, only 28,074 articles
contain at least one relation for a total of 68,429 of
relation instances. These connect 744,060 entities,
97,828 dates and 203,981 numerical attributes.
Temporal and Numerical Expression
Wikipedia articles are marked with entities like Per-
son or Organization but not with dates or numeri-
cal attributes. This prevents to extract interesting
relations between entities and dates, e.g. John F.
Kennedy was born on May 29, 1917 or between en-
tities and numerical attributes, e.g. The novel Gone
with the wind has 1037 pages. Thus we designed
18 regular expressions to extract dates and other 25
to extract numerical attributes, which range from in-
teger number to ordinal number, percentage, mone-
tary, speed, height, weight, area, time, and ISBN.
2.3 Distant Supervision and generalization
Distant supervision (DS) for RE is based on the
following assumption: (i) a sentence is connected
in some way to a database of relations and (ii)
such sentence contains the pair of entities partic-
ipating in a target relation; (iii) then it is likely
that such sentence expresses the relation. In tra-
ditional DS the point (i) is implemented by the
Infobox, which is connected to the sentences by
a proximity relation (same page of the sentence).
In our extended DS, we relax (i) by allowing
for the use of an external DB of relations such
as YAGO and any document of Freebase (a col-
lection of Wikipedia documents). The alignment
between YAGO and Freebase is implemented by
the Wikipedia page link: for example the link
http://en.wikipedia.org/wiki/James Cameron refers
to the entity James Cameron.
We use an efficient procedure formally described
in Alg. 2.1: for each Wikipedia article in Free-
base, we scan all of its NEs. Then, for each pair
of entities2 seen in the sentence, we query YAGO to
2Our algorithm is robust to the lack of knowledge about the
existence of any relation between two entities. If the relation
retrieve the relation instance connecting these enti-
ties. Note that a simplified version of our approach
is the following: for any YAGO relation instance,
scan all the sentences of all Wikipedia articles to test
point (ii). Unfortunately, this procedure is impossi-
ble in practice due to millions of relation instances
in YAGO and millions of Wikipedia articles in Free-
base, i.e. an order of magnitude of 1014 iterations3.
3 Distant Supervised Learning with
Kernels
We model relation extraction (RE) using state-of-
the-art classifiers based on kernel methods. The
main idea is that syntactic/semantic structures are
used to represent relation instances. We followed the
model in (Nguyen et al, 2009) that has shown sig-
nificant improvement on the state-of-the-art. This
combines a syntactic tree kernel and a polynomial
kernel over feature extracted from the entities:
CK1 = ? ?KP + (1? ?) ? TK (1)
where ? is a coefficient to give more or less impact
to the polynomial kernel,KP , and TK is the syntac-
tic tree kernel (Collins and Duffy, 2001). The best
model combines the advantages of the two parsing
paradigms by adding the kernel above with six se-
quence kernels (described in (Nguyen et al, 2009)).
CSK = ? ?KP +(1??) ?(TK+
?
i=1,..,6
SKi) (2)
Such kernels cannot be applied to Wikipedia doc-
uments as the entity category, e.g. Person or Orga-
nization, is in general missing. Thus, we adapted
them by simply removing the category label in the
nodes of the trees and in the sequences. This data
transformation corresponds to different kernels (see
(Cristianini and Shawe-Taylor, 2000)).
4 Experiments
We carried out test to demonstrate that our DS ap-
proach produces reliable and practically usable re-
lation extractors. For this purpose, we test them on
instance is not in YAGO, it is simply assumed as a negative
instance even if such relation is present in other DBs.
3Assuming 100 sentences for each article.
279
DS data by also carrying out end-to-end RE evalua-
tion. This requires to experiment with a state-of-the-
art Named Entity Recognizer trained on Wikipedia
entities.
Class Precision Recall F-measure
bornOnDate 97.99 95.22 96.58
created 92.00 68.56 78.57
dealsWith 92.30 73.47 81.82
directed 85.19 51.11 63.89
hasCapital 93.69 61.54 74.29
isAffiliatedTo 86.32 71.30 78.10
locatedIn 87.85 78.33 82.82
wrote 82.61 42.22 55.88
Overall 91.42 62.57 74.29
Table 1: Performance of 8 out of 52 individual relations
with overall F1.
4.1 Experimental setting
We used the DS dataset generated from YAGO and
Wikipedia articles, as described in the algorithm
(Alg. 2.1). The candidate relations are generated
by iterating all pairs of entity mentions in the same
sentence. Relation detection is formulated as a mul-
ticlass classification problem. The One vs. Rest
strategy is employed by selecting the instance with
largest margin as the final answer. We carried out
5-fold cross-validation with the tree kernel toolkit4
(Moschitti, 2004; Moschitti, 2008).
4.2 Results on Wikipedia RE
We created a test set by sampling 200 articles from
Freebase (these articles are not used for training).
An expert annotator, for each sentence, labeled all
possible pairs of entities with one of the 52 rela-
tions from YAGO, where the entities were already
marked. This process resulted in 2,601 relation in-
stances.
Table 1 shows the performance of individual clas-
sifiers as well as the overall Micro-average F1 for
our adapted CSK: we note that it reaches an F1-
score of 74.29%. This can be compared with the
Micro-average F1 of CK1, i.e. 71.21%. The lower
result suggests that the combination of dependency
and constituent syntactic structures is very impor-
tant: +3.08 absolute percent points on CK1, which
only uses constituency trees.
4http://disi.unitn.it/ moschitt/Tree-Kernel.htm
Class Precision Recall F-measure
Entity Detection 68.84 64.56 66.63
End-to-End RE 82.16 56.57 67.00
Table 2: Entity Detection and End-to-end Relation Ex-
traction.
4.3 End-to-end Relation Extraction
Previous work in RE uses gold entities available in
the annotated corpus (i.e. ACE) but in real appli-
cations these are not available. Therefore, we per-
form experiments with automatic entities. For their
extraction, we follow the feature design in (Nguyen
et al, 2010), using CRF++ 5 with unigram/features
and Freebase as learning source. Dates and numer-
ical attributes required a different treatment, so we
use the patterns described in Section 2.3. The results
reported in Table 2 are rather lower than in standard
NE recognition. This is due to the high complexity
of predicting the boundaries of thousands of differ-
ent categories in YAGO.
Our end-to-end RE system can be applied to any
text fragment so we could experiment with it and
any Wikipedia document. This allowed us to carry
out an accurate evaluation. The results are shown in
Table 2. We note that, without gold entities, RE from
Wikipedia still achieves a satisfactory performance
of 67.00% F1.
5 Conclusion
This paper proposes two main contributions to Re-
lation Extraction: (i) a new approach to distant su-
pervision (DS) to create training data using relations
defined in different sources, i.e. YAGO, and poten-
tially using any Wikipedia document; and (ii) end-
to-end systems applicable both to Wikipedia pages
as well as to any natural language text.
The results show:
1. A high F1 of 74.29% on extracting 52 YAGO
relations from any Wikipedia document (not
only from Infobox related pages). This re-
sult improves on previous work by 13.29 abso-
lute percent points (approximated comparison).
This is a rough approximation since on one
hand, (Hoffmann et al, 2010) experimented
5http://crfpp.sourceforge.net
280
with 5,025 relations, which indicate that our re-
sults based on 52 relations cannot be compared
with it (i.e. our multi-classifier has two orders
of magnitude less of categories). On the other
hand, the only experiment that can give a re-
alistic measurement is the one on hand-labeled
test set (testing on data automatically labelled
by DS does not provide a realistic outcome).
The size of such test set is comparable with
ours, i.e. 100 documents vs. our set of 200
documents. Although, we do not know how
many types of relations were involved in the
test of (Hoffmann et al, 2010), it is clear that
only a small subset of the 5000 relations could
have been measured. Also, we have to consider
that, in (Hoffmann et al, 2010), only one rela-
tion extractor is supposed to be learnt from one
article (by using Infobox) whereas we can po-
tentially extract several relations even from the
same sentence.
2. The importance of using both dependency and
constituent structures (+3.08% when adding
dependency information to RE based on con-
stituent trees).
3. Our end-to-end system is useful for real appli-
cations as it shows a meaningful accuracy, i.e.
67% on 52 relations.
For this reason, we decided to make available the
DS dataset, the manually annotated test set and the
computational data (tree and sequential structures
with labels).
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries, pages 85?94.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of IJCAI, pages 2670?2676.
Sergey Brin. 1998. Extracting patterns and relations
from world wide web. In Proceedings of WebDB
Workshop at 6th International Conference on Extend-
ing Database Technology, pages 172?183.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT-EMNLP, pages 724?731, Vancou-
ver, British Columbia, Canada, October.
Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems (NIPS?2001), pages
625?632.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press, Cambridge, United Kingdom.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423?429, Barcelona, Spain, July.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) programtasks, data, and evaluation. In Proceed-
ings of LREC, pages 837?840, Barcelona, Spain.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of ACL, pages
415?422, Barcelona, Spain, July.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of ACL, pages 286?295, Uppsala, Sweden,
July.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for information extraction. In The Companion Volume
to the Proceedings of ACL, pages 178?181, Barcelona,
Spain, July.
Metaweb Technologies. 2010. Freebase wikipedia ex-
traction (wex), March.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-AFNLP,
pages 1003?1011, Suntec, Singapore, August.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL, pages 335?342, Barcelona, Spain, July.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceedings of CIKM, pages 253?262, New York, NY,
USA. ACM.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378?1387, Singapore, August.
281
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2010. Kernel-based re-ranking for
named-entity extraction. In Proceedings of COLING,
pages 901?909, China, August.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases, volume 6323 of Lecture
Notes in Computer Science, pages 148?163. Springer
Berlin / Heidelberg.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago - a core of semantic knowl-
edge. In 16th international World Wide Web confer-
ence, pages 697?706.
Alexander Yates. 2009. Extracting world knowledge
from the web. IEEE Computer, 42(6):94?97, June.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP?2005, Lecture Notes in Computer Science (LNCS
3651), pages 378?389, Jeju Island, South Korea.
Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou.
2006. A composite kernel to extract relations between
entities with both flat and structured features. In Pro-
ceedings of COLING-ACL 2006, pages 825?832.
282
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 263?272,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Verb Classification using Distributional Similarity
in Syntactic and Semantic Structures
Danilo Croce
University of Tor Vergata
00133 Roma, Italy
croce@info.uniroma2.it
Alessandro Moschitti
University of Trento
38123 Povo (TN), Italy
moschitti@disi.unitn.it
Roberto Basili
University of Tor Vergata
00133 Roma, Italy
basili@info.uniroma2.it
Martha Palmer
University of Colorado at Boulder
Boulder, CO 80302, USA
mpalmer@colorado.edu
Abstract
In this paper, we propose innovative repre-
sentations for automatic classification of verbs
according to mainstream linguistic theories,
namely VerbNet and FrameNet. First, syntac-
tic and semantic structures capturing essential
lexical and syntactic properties of verbs are
defined. Then, we design advanced similarity
functions between such structures, i.e., seman-
tic tree kernel functions, for exploiting distri-
butional and grammatical information in Sup-
port Vector Machines. The extensive empir-
ical analysis on VerbNet class and frame de-
tection shows that our models capture mean-
ingful syntactic/semantic structures, which al-
lows for improving the state-of-the-art.
1 Introduction
Verb classification is a fundamental topic of com-
putational linguistics research given its importance
for understanding the role of verbs in conveying se-
mantics of natural language (NL). Additionally, gen-
eralization based on verb classification is central to
many NL applications, ranging from shallow seman-
tic parsing to semantic search or information extrac-
tion. Currently, a lot of interest has been paid to
two verb categorization schemes: VerbNet (Schuler,
2005) and FrameNet (Baker et al, 1998), which
has also fostered production of many automatic ap-
proaches to predicate argument extraction.
Such work has shown that syntax is necessary
for helping to predict the roles of verb arguments
and consequently their verb sense (Gildea and Juras-
fky, 2002; Pradhan et al, 2005; Gildea and Palmer,
2002). However, the definition of models for opti-
mally combining lexical and syntactic constraints is
still far for being accomplished. In particular, the ex-
haustive design and experimentation of lexical and
syntactic features for learning verb classification ap-
pears to be computationally problematic. For exam-
ple, the verb order can belongs to the two VerbNet
classes:
? The class 60.1, i.e., order someone to do some-
thing as shown in: The Illinois Supreme Court or-
dered the commission to audit Commonwealth Edi-
son ?s construction expenses and refund any unrea-
sonable expenses .
? The class 13.5.1: order or request something like
in: ... Michelle blabs about it to a sandwich man
while ordering lunch over the phone .
Clearly, the syntactic realization can be used to dis-
cern the cases above but it would not be enough to
correctly classify the following verb occurrence: ..
ordered the lunch to be delivered .. in Verb class
13.5.1. For such a case, selectional restrictions are
needed. These have also been shown to be use-
ful for semantic role classification (Zapirain et al,
2010). Note that their coding in learning algorithms
is rather complex: we need to take into account syn-
tactic structures, which may require an exponential
number of syntactic features (i.e., all their possible
substructures). Moreover, these have to be enriched
with lexical information to trig lexical preference.
In this paper, we tackle the problem above
by studying innovative representations for auto-
matic verb classification according to VerbNet and
FrameNet. We define syntactic and semantic struc-
tures capturing essential lexical and syntactic prop-
erties of verbs. Then, we apply similarity between
263
such structures, i.e., kernel functions, which can also
exploit distributional lexical semantics, to train au-
tomatic classifiers. The basic idea of such functions
is to compute the similarity between two verbs in
terms of all the possible substructures of their syn-
tactic frames. We define and automatically extract
a lexicalized approximation of the latter. Then, we
apply kernel functions that jointly model structural
and lexical similarity so that syntactic properties are
combined with generalized lexemes. The nice prop-
erty of kernel functions is that they can be used
in place of the scalar product of feature vectors to
train algorithms such as Support Vector Machines
(SVMs). This way SVMs can learn the association
between syntactic (sub-) structures whose lexical ar-
guments are generalized and target verb classes, i.e.,
they can also learn selectional restrictions.
We carried out extensive experiments on verb
class and frame detection which showed that our
models greatly improve on the state-of-the-art (up
to about 13% of relative error reduction). Such re-
sults are nicely assessed by manually inspecting the
most important substructures used by the classifiers
as they largely correlate with syntactic frames de-
fined in VerbNet.
In the rest of the paper, Sec. 2 reports on related
work, Sec. 3 and Sec. 4 describe previous and our
models for syntactic and semantic similarity, respec-
tively, Sec. 5 illustrates our experiments, Sec. 6 dis-
cusses the output of the models in terms of error
analysis and important structures and finally Sec. 7
derives the conclusions.
2 Related work
Our target task is verb classification but at the same
time our models exploit distributional models as
well as structural kernels. The next three subsec-
tions report related work in such areas.
Verb Classification. The introductory verb classi-
fication example has intuitively shown the complex-
ity of defining a comprehensive feature representa-
tion. Hereafter, we report on analysis carried out in
previous work.
It has been often observed that verb senses tend
to show different selectional constraints in a specific
argument position and the above verb order is a clear
example. In the direct object position of the example
sentence for the first sense 60.1 of order, we found
commission in the role PATIENT of the predicate. It
clearly satisfies the +ANIMATE/+ORGANIZATION
restriction on the PATIENT role. This is not true
for the direct object dependency of the alternative
sense 13.5.1, which usually expresses the THEME
role, with unrestricted type selection. When prop-
erly generalized, the direct object information has
thus been shown highly predictive about verb sense
distinctions.
In (Brown et al, 2011), the so called dynamic
dependency neighborhoods (DDN), i.e., the set of
verbs that are typically collocated with a direct ob-
ject, are shown to be more helpful than lexical in-
formation (e.g., WordNet). The set of typical verbs
taking a noun n as a direct object is in fact a strong
characterization for semantic similarity, as all the
nounsm similar to n tend to collocate with the same
verbs. This is true also for other syntactic depen-
dencies, among which the direct object dependency
is possibly the strongest cue (as shown for example
in (Dligach and Palmer, 2008)).
In order to generalize the above DDN feature, dis-
tributional models are ideal, as they are designed
to model all the collocations of a given noun, ac-
cording to large scale corpus analysis. Their abil-
ity to capture lexical similarity is well established in
WSD tasks (e.g. (Schutze, 1998)), thesauri harvest-
ing (Lin, 1998), semantic role labeling (Croce et al,
2010)) as well as information retrieval (e.g. (Furnas
et al, 1988)).
Distributional Models (DMs). These models fol-
low the distributional hypothesis (Firth, 1957) and
characterize lexical meanings in terms of context of
use, (Wittgenstein, 1953). By inducing geometrical
notions of vectors and norms through corpus analy-
sis, they provide a topological definition of seman-
tic similarity, i.e., distance in a space. DMs can
capture the similarity between words such as dele-
gation, deputation or company and commission. In
case of sense 60.1 of the verb order, DMs can be
used to suggest that the role PATIENT can be inher-
ited by all these words, as suitable Organisations.
In supervised language learning, when few exam-
ples are available, DMs support cost-effective lexi-
cal generalizations, often outperforming knowledge
based resources (such as WordNet, as in (Pantel et
al., 2007)). Obviously, the choice of the context
264
type determines the type of targeted semantic prop-
erties. Wider contexts (e.g., entire documents) are
shown to suggest topical relations. Smaller con-
texts tend to capture more specific semantic as-
pects, e.g. the syntactic behavior, and better capture
paradigmatic relations, such as synonymy. In partic-
ular, word space models, as described in (Sahlgren,
2006), define contexts as the words appearing in a
n-sized window, centered around a target word. Co-
occurrence counts are thus collected in a words-by-
words matrix, where each element records the num-
ber of times two words co-occur within a single win-
dow of word tokens. Moreover, robust weighting
schemas are used to smooth counts against too fre-
quent co-occurrence pairs: Pointwise Mutual Infor-
mation (PMI) scores (Turney and Pantel, 2010) are
commonly adopted.
Structural Kernels. Tree and sequence kernels
have been successfully used in many NLP applica-
tions, e.g., parse reranking and adaptation, (Collins
and Duffy, 2002; Shen et al, 2003; Toutanova et
al., 2004; Kudo et al, 2005; Titov and Hender-
son, 2006), chunking and dependency parsing, e.g.,
(Kudo and Matsumoto, 2003; Daume? III and Marcu,
2004), named entity recognition, (Cumby and Roth,
2003), text categorization, e.g., (Cancedda et al,
2003; Gliozzo et al, 2005), and relation extraction,
e.g., (Zelenko et al, 2002; Bunescu and Mooney,
2005; Zhang et al, 2006).
Recently, DMs have been also proposed in in-
tegrated syntactic-semantic structures that feed ad-
vanced learning functions, such as the semantic
tree kernels discussed in (Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b; Mehdad et
al., 2010; Croce et al, 2011).
3 Structural Similarity Functions
In this paper we model verb classifiers by exploiting
previous technology for kernel methods. In particu-
lar, we design new models for verb classification by
adopting algorithms for structural similarity, known
as Smoothed Partial Tree Kernels (SPTKs) (Croce et
al., 2011). We define new innovative structures and
similarity functions based on LSA.
The main idea of SPTK is rather simple: (i) mea-
suring the similarity between two trees in terms of
the number of shared subtrees; and (ii) such number
also includes similar fragments whose lexical nodes
are just related (so they can be different). The con-
tribution of (ii) is proportional to the lexical similar-
ity of the tree lexical nodes, where the latter can be
evaluated according to distributional models or also
lexical resources, e.g., WordNet.
In the following, we define our models based on
previous work on LSA and SPTKs.
3.1 LSA as lexical similarity model
Robust representations can be obtained through
intelligent dimensionality reduction methods. In
LSA the original word-by-context matrix M is de-
composed through Singular Value Decomposition
(SVD) (Landauer and Dumais, 1997; Golub and Ka-
han, 1965) into the product of three new matrices:
U , S, and V so that S is diagonal and M = USV T .
M is then approximated by Mk = UkSkV Tk , where
only the first k columns of U and V are used,
corresponding to the first k greatest singular val-
ues. This approximation supplies a way to project
a generic term wi into the k-dimensional space us-
ing W = UkS
1/2
k , where each row corresponds to
the representation vectors ~wi. The original statisti-
cal information about M is captured by the new k-
dimensional space, which preserves the global struc-
ture while removing low-variant dimensions, i.e.,
distribution noise. Given two words w1 and w2,
the term similarity function ? is estimated as the
cosine similarity between the corresponding projec-
tions ~w1, ~w2 in the LSA space, i.e ?(w1, w2) =
~w1? ~w2
? ~w1?? ~w2?
. This is known as Latent Semantic Ker-
nel (LSK), proposed in (Cristianini et al, 2001),
as it defines a positive semi-definite Gram matrix
G = ?(w1, w2) ?w1, w2 (Shawe-Taylor and Cris-
tianini, 2004). ? is thus a valid kernel and can be
combined with other kernels, as discussed in the
next session.
3.2 Tree Kernels driven by Semantic Similarity
To our knowledge, two main types of tree kernels
exploit lexical similarity: the syntactic semantic tree
kernel defined in (Bloehdorn and Moschitti, 2007a)
applied to constituency trees and the smoothed
partial tree kernels (SPTKs) defined in (Croce et
al., 2011), which generalizes the former. We report
the definition of the latter as we modified it for our
purposes. SPTK computes the number of common
substructures between two trees T1 and T2 without
explicitly considering the whole fragment space. Its
265
SVP
S
-
NP-1
NN
commission::n
DT
the::d
VBD
TARGET-order::v
NP-SBJ
NNP
court::n
NNP
supreme::n
NNP
illinois::n
DT
the::d
Figure 1: Constituency Tree (CT) representation of verbs.
ROOT
OPRD
IM
VB
audit::v
TO
to::t
OBJ
NN
commission::n
NMOD
DT
the::d
VBD
TARGET-order::v
SBJ
NNP
court::n
NMOD
NNP
supreme::n
NMOD
NNP
illinois::n
NMOD
DT
the::d
Figure 2: Representation of verbs according to the Grammatical Relation Centered Tree (GRCT)
general equations are reported hereafter:
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), (1)
where NT1 and NT2 are the sets of the T1?s and T2?s
nodes, respectively and ?(n1, n2) is equal to the
number of common fragments rooted in the n1 and
n2 nodes1. The ? function determines the richness
of the kernel space and thus induces different tree
kernels, for example, the syntactic tree kernel (STK)
(Collins and Duffy, 2002) or the partial tree kernel
(PTK) (Moschitti, 2006).
The algorithm for SPTK?s ? is the follow-
ing: if n1 and n2 are leaves then ??(n1, n2) =
???(n1, n2); else
??(n1, n2) = ??(n1, n2)?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(
~I1)+d(~I2)
l(~I1)?
j=1
??(cn1(~I1j), cn2(~I2j))
)
, (2)
where (1) ? is any similarity between nodes, e.g., be-
tween their lexical labels; (2) ?, ? ? [0, 1] are decay
factors; (3) cn1(h) is the h
th child of the node n1;
(4) ~I1 and ~I2 are two sequences of indexes, i.e., ~I =
(i1, i2, .., l(I)), with 1 ? i1 < i2 < .. < il(I); and (5)
d(~I1) = ~I1l(~I1)?
~I11+1 and d(~I2) = ~I2l(~I2)?
~I21+1.
Note that, as shown in (Croce et al, 2011), the av-
erage running time of SPTK is sub-quadratic in the
number of the tree nodes. In the next section we
show how we exploit the class of SPTKs, for verb
classification.
1To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
4 Verb Classification Models
The design of SPTK-based algorithms for our verb
classification requires the modeling of two differ-
ent aspects: (i) a tree representation for the verbs;
and (ii) the lexical similarity suitable for the task.
We also modified SPTK to apply different similarity
functions to different nodes to introduce flexibility.
4.1 Verb Structural Representation
The implicit feature space generated by structural
kernels and the corresponding notion of similarity
between verbs obviously depends on the input struc-
tures. In the cases of STK, PTK and SPTK different
tree representations lead to engineering more or less
expressive linguistic feature spaces.
With the aim of capturing syntactic features, we
started from two different parsing paradigms: phrase
and dependency structures. For example, for repre-
senting the first example of the introduction, we can
use the constituency tree (CT) in Figure 1, where the
target verb node is enriched with the TARGET label.
Here, we apply tree pruning to reduce the computa-
tional complexity of tree kernels as it is proportional
to the number of nodes in the input trees. Accord-
ingly, we only keep the subtree dominated by the
target VP by pruning from it all the S-nodes along
with their subtrees (i.e, all nested sentences are re-
moved). To further improve generalization, we lem-
matize lexical nodes and add generalized POS-Tags,
i.e., noun (n::), verb (v::), adjective (::a), determiner
(::d) and so on, to them. This is useful for constrain-
ing similarity to be only contributed by lexical pairs
of the same grammatical category.
266
TARGET-order::v
VBDROOTto::t
TOOPRDaudit::v
VBIM
commission::n
NNOBJthe::d
DTNMOD
court::n
NNPSBJsupreme::n
NNPNMOD
illinois::n
NNPNMOD
the::d
DTNMOD
Figure 3: Representation of verbs according to the Lexical Centered Tree (LCT)
To encode dependency structure information in a
tree (so that we can use it in tree kernels), we use
(i) lexemes as nodes of our tree, (ii) their dependen-
cies as edges between the nodes and (iii) the depen-
dency labels, e.g., grammatical functions (GR), and
POS-Tags, again as tree nodes. We designed two
different tree types: (i) in the first type, GR are cen-
tral nodes from which dependencies are drawn and
all the other features of the central node, i.e., lexi-
cal surface form and its POS-Tag, are added as ad-
ditional children. An example of the GR Centered
Tree (GRCT) is shown in Figure 2, where the POS-
Tags and lexemes are children of GR nodes. (ii) The
second type of tree uses lexicals as central nodes on
which both GR and POS-Tag are added as the right-
most children. For example, Figure 3 shows an ex-
ample of a Lexical Centered Tree (LCT). For both
trees, the pruning strategy only preserves the verb
node, its direct ancestors (father and siblings) and
its descendants up to two levels (i.e., direct children
and grandchildren of the verb node). Note that, our
dependency tree can capture the semantic head of
the verbal argument along with the main syntactic
construct, e.g., to audit.
4.2 Generalized node similarity for SPTK
We have defined the new similarity ?? to be used in
Eq. 2, which makes SPTK more effective as shown
by Alg. 1. ?? takes two nodes n1 and n2 and applies
a different similarity for each node type. The latter is
derived by ? and can be: GR (i.e., SYNT), POS-Tag
(i.e., POS) or a lexical (i.e., LEX) type. In our exper-
iment, we assign 0/1 similarity for SYNT and POS
nodes according to string matching. For LEX type,
we apply a lexical similarity learned with LSA to
only pairs of lexicals associated with the same POS-
Tag. It should be noted that the type-based similarity
allows for potentially applying a different similarity
for each node. Indeed, we also tested an amplifica-
tion factor, namely, leaf weight (lw), which ampli-
fies the matching values of the leaf nodes.
Algorithm 1 ?? (n1, n2, lw)
?? ? 0,
if ?(n1) = ?(n2) = SYNT ? label(n1) = label(n2) then
?? ? 1
end if
if ?(n1) = ?(n2) = POS ? label(n1) = label(n2) then
?? ? 1
end if
if ?(n1) = ?(n2) = LEX ? pos(n1) = pos(n2) then
?? ? ?LEX(n1, n2)
end if
if leaf(n1) ? leaf(n2) then
?? ? ?? ? lw
end if
return ??
5 Experiments
In these experiments, we tested the impact of our dif-
ferent verb representations using different kernels,
similarities and parameters. We also compared with
simple bag-of-words (BOW) models and the state-
of-the-art.
5.1 General experimental setup
We consider two different corpora: one for VerbNet
and the other for FrameNet. For the former, we used
the same verb classification setting of (Brown et al,
2011). Sentences are drawn from the Semlink cor-
pus (Loper et al, 2007), which consists of the Prop-
Banked Penn Treebank portions of the Wall Street
Journal. It contains 113K verb instances, 97K of
which are verbs represented in at least one VerbNet
class. Semlink includes 495 verbs, whose instances
are labeled with more than one class (including one
single VerbNet class or none). We used all instances
of the corpus for a total of 45,584 instances for 180
verb classes. When instances labeled with the none
class are not included, the number of examples be-
comes 23,719.
The second corpus refers to FrameNet frame clas-
sification. The training and test data are drawn from
the FrameNet 1.5 corpus2, which consists of 135K
sentences annotated according the frame semantics
2http://framenet.icsi.berkeley.edu
267
(Baker et al, 1998). We selected the subset of
frames containing more than 100 sentences anno-
tated with a verbal predicate for a total of 62,813
sentences in 187 frames (i.e., very close to the Verb-
Net datasets). For both the datasets, we used 70% of
instances for training and 30% for testing.
Our verb (multi) classifier is designed with
the one-vs-all (Rifkin and Klautau, 2004) multi-
classification schema. This uses a set of binary
SVM classifiers, one for each verb class (frame) i.
The sentences whose verb is labeled with the class
i are positive examples for the classifier i. The sen-
tences whose verbs are compatible with the class i
but evoking a different class or labeled with none
(no current verb class applies) are added as negative
examples. In the classification phase the binary clas-
sifiers are applied by (i) only considering classes that
are compatible with the target verbs; and (ii) select-
ing the class associated with the maximum positive
SVM margin. If all classifiers provide a negative
score the example is labeled with none.
To learn the binary classifiers of the schema
above, we coded our modified SPTK in SVM-Light-
TK3 (Moschitti, 2006). The parameterization of
each classifier is carried on a held-out set (30% of
the training) and is concerned with the setting of the
trade-off parameter (option -c) and the leaf weight
(lw) (see Alg. 1), which is used to linearly scale
the contribution of the leaf nodes. In contrast, the
cost-factor parameter of SVM-Light-TK is set as the
ratio between the number of negative and positive
examples for attempting to have a balanced Preci-
sion/Recall.
Regarding SPTK setting, we used the lexical simi-
larity ? defined in Sec. 3.1. In more detail, LSA was
applied to ukWak (Baroni et al, 2009), which is a
large scale document collection made up of 2 billion
tokens. M is constructed by applying POS tagging to
build rows with pairs ?lemma, ::POS? (lemma::POS
in brief). The contexts of such items are the columns
of M and are short windows of size [?3,+3], cen-
tered on the items. This allows for better captur-
ing syntactic properties of words. The most frequent
20,000 items are selected along with their 20k con-
texts. The entries of M are the point-wise mutual
3(Structural kernels in SVMLight (Joachims, 2000)) avail-
able at http://disi.unitn.it/moschitti/Tree-Kernel.htm
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 83.83% 8 84.57% 8 84.46%
GRCT - 84.83% 8 85.15% 8 85.28%
LCT - 77.73% 0.1 86.03% 0.2 86.72%
Br. et Al. 84.64%
BOW 79.08%
SK 82.08%
Table 1: VerbNet accuracy with the none class
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
GRCT - 92.67% 6 92.97% 0.4 93.54%
LCT - 90.28% 6 92.99% 0.3 93.78%
BOW 91.13%
SK 91.84%
Table 2: FrameNet accuracy without the none class
information between them. SVD reduction is then
applied to M, with a dimensionality cut of l = 250.
For generating the CT, GRCT and LCT struc-
tures, we used the constituency trees generated by
the Charniak parser (Charniak, 2000) and the de-
pendency structures generated by the LTH syntactic
parser (described in (Johansson and Nugues, 2008)).
The classification performance is measured with
accuracy (i.e., the percentage of correct classifica-
tion). We also derive statistical significance of the
results by using the model described in (Yeh, 2000)
and implemented in (Pado?, 2006).
5.2 VerbNet and FrameNet Classification
Results
To assess the performance of our settings, we also
derive a simple baseline based on the bag-of-words
(BOW) model. For it, we represent an instance of
a verb in a sentence using all words of the sentence
(by creating a special feature for the predicate word).
We also used sequence kernels (SK), i.e., PTK ap-
plied to a tree composed of a fake root and only one
level of sentence words. For efficiency reasons4, we
only consider the 10 words before and after the pred-
icate with subsequence features of length up to 5.
Table 1 reports the accuracy of different mod-
els for VerbNet classification. It should be noted
that: first, SK produces a much higher accuracy than
BOW, i.e., 82.08 vs. 79.08. On one hand, this is
4The average running time of the SK is much higher than the
one of PTK. When a tree is composed by only one level PTK
collapses to SK.
268
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 91.14% 8 91.66% 6 91.66%
GRCT - 91.71% 8 92.38% 4 92.33%
LCT - 89.20% 0.2 92.54% 0.1 92.55%
BOW 88.16%
SK 89.86%
Table 3: VerbNet accuracy without the none class
generally in contrast with standard text categoriza-
tion tasks, for which n-gram models show accuracy
comparable to the simpler BOW. On the other hand,
it simply confirms that verb classification requires
the dependency information between words (i.e., at
least the sequential structure information provided
by SK).
Second, SK is 2.56 percent points below the state-
of-the-art achieved in (Brown et al, 2011) (BR), i.e,
82.08 vs. 84.64. In contrast, STK applied to our rep-
resentation (CT, GRCT and LCT) produces compa-
rable accuracy, e.g., 84.83, confirming that syntactic
representation is needed to reach the state-of-the-art.
Third, PTK, which produces more general struc-
tures, improves over BR by almost 1.5 (statistically
significant result) when using our dependency struc-
tures GRCT and LCT. CT does not produce the same
improvement since it does not allow PTK to directly
compare the lexical structure (lexemes are all leaf
nodes in CT and to connect some pairs of them very
large trees are needed).
Finally, the best model of SPTK (i.e, using LCT)
improves over the best PTK (i.e., using LCT) by al-
most 1 point (statistically significant result): this dif-
ference is only given by lexical similarity. SPTK im-
proves on the state-of-the-art by about 2.08 absolute
percent points, which, given the high accuracy of the
baseline, corresponds to 13.5% of relative error re-
duction.
We carried out similar experiments for frame clas-
sification. One interesting difference is that SK im-
proves BOW by only 0.70, i.e., 4 times less than in
the VerbNet setting. This suggests that word order
around the predicate is more important for deriving
the VerbNet class than the FrameNet frame. Ad-
ditionally, LCT or GRCT seems to be invariant for
both PTK and SPTK whereas the lexical similarity
still produces a relevant improvement on PTK, i.e.,
13% of relative error reduction, for an absolute accu-
racy of 93.78%. The latter improves over the state-
50% 
60% 
70% 
80% 
90% 
0% 20% 40% 60% 80% 100% 
Accu
racy 
Percentage of train examples 
SPTK 
BOW 
Brown et al 
Figure 4: Learning curves: VerbNet accuracy with the
none Class
of-the-art, i.e., 92.63% derived in (Giuglea and Mos-
chitti, 2006), by using STK on CT on 133 frames.
We also carried out experiments to understand
the role of the none class. Table 3 reports on the
VerbNet classification without its instances. This is
of course an unrealistic setting as it would assume
that the current VerbNet release already includes all
senses for English verbs. In the table, we note that
the overall accuracy highly increases and the differ-
ence between models reduces. The similarities play
no role anymore. This may suggest that SPTK can
help in complex settings, where verb class character-
ization is more difficult. Another important role of
SPTK models is their ability to generalize. To test
this aspect, Figure 4 illustrates the learning curves
of SPTK with respect to BOW and the accuracy
achieved by BR (with a constant line). It is impres-
sive to note that with only 40% of the data SPTK can
reach the state-of-the-art.
6 Model Analysis and Discussion
We carried out analysis of system errors and its in-
duced features. These can be examined by apply-
ing the reverse engineering tool5 proposed in (Pighin
and Moschitti, 2010; Pighin and Moschitti, 2009a;
Pighin and Moschitti, 2009b), which extracts the
most important features for the classification model.
Many mistakes are related to false positives and neg-
atives of the none class (about 72% of the errors).
This class also causes data imbalance. Most errors
are also due to lack of lexical information available
to the SPTK kernel: (i) in 30% of the errors, the
argument heads were proper nouns for which the
lexical generalization provided by the DMs was not
5http://danielepighin.net/cms/software/flink
269
VerbNet class 13.5.1
(IM(VB(target))(OBJ))
(VC(VB(target))(OBJ))
(VC(VBG(target))(OBJ))
(OPRD(TO)(IM(VB(target))(OBJ)))
(PMOD(VBG(target))(OBJ))
(VB(target))
(VC(VBN(target)))
(PRP(TO)(IM(VB(target))(OBJ)))
(IM(VB(target))(OBJ)(ADV(IN)(PMOD)))
(OPRD(TO)(IM(VB(target))(OBJ)(ADV(IN)(PMOD))))
VerbNet class 60
(VC(VB(target))(OBJ))
(NMOD(VBG(target))(OPRD))
(VC(VBN(target))(OPRD))
(NMOD(VBN(target))(OPRD))
(PMOD(VBG(target))(OBJ))
(ROOT(SBJ)(VBD(target))(OBJ)(P(,)))
(VC(VB(target))(OPRD))
(ROOT(SBJ)(VBZ(target))(OBJ)(P(,)))
(NMOD(SBJ(WDT))(VBZ(target))(OPRD))
(NMOD(SBJ)(VBZ(target))(OPRD(SBJ)(TO)(IM)))
Table 4: GRCT fragments
available; and (ii) in 76% of the errors only 2 or less
argument heads are included in the extracted tree,
therefore tree kernels cannot exploit enough lexical
information to disambiguate verb senses. Addition-
ally, ambiguity characterizes errors where the sys-
tem is linguistically consistent but the learned selec-
tional preferences are not sufficient to separate verb
senses. These errors are mainly due to the lack of
contextual information. While error analysis sug-
gests that further improvement is possible (e.g. by
exploiting proper nouns), the type of generalizations
currently achieved by SPTK are rather effective. Ta-
ble 4 and 5 report the tree structures characterizing
the most informative training examples of the two
senses of the verb order, i.e. the VerbNet classes
13.5.1 (make a request for something) and 60 (give
instructions to or direct somebody to do something
with authority).
In line with the method discussed in (Pighin and
Moschitti, 2009b), these fragments are extracted as
they appear in most of the support vectors selected
during SVM training. As easily seen, the two classes
are captured by rather different patterns. The typ-
ical accusative form with an explicit direct object
emerges as characterizing the sense 13.5.1, denot-
ing the THEME role. All fragments of the sense 60
emphasize instead the sentential complement of the
verb that in fact expresses the standard PROPOSI-
TION role in VerbNet. Notice that tree fragments
correspond to syntactic patterns. The a posteriori
VerbNet class 13.5.1
(VP(VB(target))(NP))
(VP(VBG(target))(NP))
(VP(VBD(target))(NP))
(VP(TO)(VP(VB(target))(NP)))
(S(NP-SBJ)(VP(VBP(target))(NP)))
VerbNet class 60
(VBN(target))
(VP(VBD(target))(S))
(VP(VBZ(target))(S))
(VBP(target))
(VP(VBD(target))(NP-1)(S(NP-SBJ)(VP)))
Table 5: CT fragments
analysis of the learned models (i.e. the underlying
support vectors) confirm very interesting grammati-
cal generalizations, i.e. the capability of tree kernels
to implicitly trigger useful linguistic inductions for
complex semantic tasks. When SPTK are adopted,
verb arguments can be lexically generalized into
word classes, i.e., clusters of argument heads (e.g.
commission vs. delegation, or gift vs. present). Au-
tomatic generation of such classes is an interesting
direction for future research.
7 Conclusion
We have proposed new approaches to characterize
verb classes in learning algorithms. The key idea is
the use of structural representation of verbs based on
syntactic dependencies and the use of structural ker-
nels to measure similarity between such representa-
tions. The advantage of kernel methods is that they
can be directly used in some learning algorithms,
e.g., SVMs, to train verb classifiers. Very interest-
ingly, we can encode distributional lexical similar-
ity in the similarity function acting over syntactic
structures and this allows for generalizing selection
restrictions through a sort of (supervised) syntactic
and semantic co-clustering.
The verb classification results show a large im-
provement over the state-of-the-art for both Verb-
Net and FrameNet, with a relative error reduction
of about 13.5% and 16.0%, respectively. In the fu-
ture, we plan to exploit the models learned from
FrameNet and VerbNet to carry out automatic map-
ping of verbs from one theory to the other.
Acknowledgements This research is partially sup-
ported by the European Community?s Seventh Frame-
work Programme (FP7/2007-2013) under grant numbers
247758 (ETERNALS), 288024 (LIMOSINE) and 231126
(LIVINGKNOWLEDGE). Many thanks to the reviewers
for their valuable suggestions.
270
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a collec-
tion of very large linguistically processed web-crawled
corpora. LRE, 43(3):209?226.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Gianni Amati, Claudio Carpineto, and
Gianni Romano, editors, Proceedings of ECIR, vol-
ume 4425 of Lecture Notes in Computer Science,
pages 307?318. Springer, APR.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In CIKM?07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowledge
management, pages 861?864, New York, NY, USA.
ACM.
Susan Windisch Brown, Dmitriy Dligach, and Martha
Palmer. 2011. Verbnet class assignment as a wsd task.
In Proceedings of the Ninth International Conference
on Computational Semantics, IWCS ?11, pages 85?94,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2001. Latent semantic kernels. In Carla Brodley and
Andrea Danyluk, editors, Proceedings of ICML-01,
18th International Conference on Machine Learning,
pages 66?73, Williams College, US. Morgan Kauf-
mann Publishers, San Francisco, US.
Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain semantic
role labeling. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 237?246, Uppsala, Sweden, July. Association
for Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured Lexical Similarity via Convolution
Kernels on Dependency Trees. In Proceedings of
EMNLP 2011.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL (Short Papers), pages 29?32. The Association for
Computer Linguistics.
J. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis. Philological
Society, Oxford. reprinted in Palmer, F. (ed. 1968) Se-
lected Papers of J. R. Firth, Longman, Harlow.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proc. of SIGIR ?88, New York, USA.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
Ana-Maria Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via framenet, verbnet and prop-
bank. In Proceedings of ACL, pages 929?936, Sydney,
Australia, July.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
G. Golub and W. Kahan. 1965. Calculating the singular
values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics: Se-
ries B, Numerical Analysis.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML?00.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of
CoNLL 2008, pages 183?187.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Tom Landauer and Sue Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis theory
271
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between prop-
bank and verbnet. In In Proceedings of the 7th Inter-
national Workshop on Computational Linguistics.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In HLT-NAACL,
pages 1020?1028.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. Isp:
Learning inferential selectional preferences. In Pro-
ceedings of HLT/NAACL 2007.
Daniele Pighin and Alessandro Moschitti. 2009a. Ef-
ficient linearization of tree kernel functions. In Pro-
ceedings of CoNLL?09.
Daniele Pighin and Alessandro Moschitti. 2009b. Re-
verse engineering of tree kernel feature spaces. In Pro-
ceedings of EMNLP, pages 111?120, Singapore, Au-
gust. Association for Computational Linguistics.
Daniele Pighin and Alessandro Moschitti. 2010. On
reverse feature engineering of syntactic tree kernels.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, CoNLL ?10,
pages 223?233, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. Journal of Machine Learning
Research, 5:101?141.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylyania.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89?96, Sapporo, Japan.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwells, Oxford.
Alexander S. Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In COLING,
pages 947?953.
Ben?at Zapirain, Eneko Agirre, Llu??s Ma`rquez, and Mi-
hai Surdeanu. 2010. Improving semantic role classi-
fication with selectional preferences. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 373?376,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
272
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 759?767,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Modeling Topic Dependencies in Hierarchical Text Categorization
Alessandro Moschitti and Qi Ju
University of Trento
38123 Povo (TN), Italy
{moschitti,qi}@disi.unitn.it
Richard Johansson
University of Gothenburg
SE-405 30 Gothenburg, Sweden
richard.johansson@gu.se
Abstract
In this paper, we encode topic dependencies
in hierarchical multi-label Text Categoriza-
tion (TC) by means of rerankers. We rep-
resent reranking hypotheses with several in-
novative kernels considering both the struc-
ture of the hierarchy and the probability of
nodes. Additionally, to better investigate the
role of category relationships, we consider two
interesting cases: (i) traditional schemes in
which node-fathers include all the documents
of their child-categories; and (ii) more gen-
eral schemes, in which children can include
documents not belonging to their fathers. The
extensive experimentation on Reuters Corpus
Volume 1 shows that our rerankers inject ef-
fective structural semantic dependencies in
multi-classifiers and significantly outperform
the state-of-the-art.
1 Introduction
Automated Text Categorization (TC) algorithms for
hierarchical taxonomies are typically based on flat
schemes, e.g., one-vs.-all, which do not take topic
relationships into account. This is due to two major
problems: (i) complexity in introducing them in the
learning algorithm and (ii) the small or no advan-
tage that they seem to provide (Rifkin and Klautau,
2004).
We speculate that the failure of using hierarchi-
cal approaches is caused by the inherent complexity
of modeling all possible topic dependencies rather
than the uselessness of such relationships. More pre-
cisely, although hierarchical multi-label classifiers
can exploit machine learning algorithms for struc-
tural output, e.g., (Tsochantaridis et al, 2005; Rie-
zler and Vasserman, 2010; Lavergne et al, 2010),
they often impose a number of simplifying restric-
tions on some category assignments. Typically, the
probability of a document d to belong to a subcate-
gory Ci of a category C is assumed to depend only
on d and C, but not on other subcategories of C,
or any other categories in the hierarchy. Indeed, the
introduction of these long-range dependencies lead
to computational intractability or more in general to
the problem of how to select an effective subset of
them. It is important to stress that (i) there is no
theory that can suggest which are the dependencies
to be included in the model and (ii) their exhaustive
explicit generation (i.e., the generation of all hierar-
chy subparts) is computationally infeasible. In this
perspective, kernel methods are a viable approach
to implicitly and easily explore feature spaces en-
coding dependencies. Unfortunately, structural ker-
nels, e.g., tree kernels, cannot be applied in struc-
tured output algorithms such as (Tsochantaridis et
al., 2005), again for the lack of a suitable theory.
In this paper, we propose to use the combination
of reranking with kernel methods as a way to han-
dle the computational and feature design issues. We
first use a basic hierarchical classifier to generate a
hypothesis set of limited size, and then apply rerank-
ing models. Since our rerankers are simple binary
classifiers of hypothesis pairs, they can encode com-
plex dependencies thanks to kernel methods. In par-
ticular, we used tree, sequence and linear kernels ap-
plied to structural and feature-vector representations
describing hierarchical dependencies.
Additionally, to better investigate the role of topi-
cal relationships, we consider two interesting cases:
(i) traditional categorization schemes in which node-
759
fathers include all the documents of their child-
categories; and (ii) more general schemes, in which
children can include documents not belonging to
their fathers. The intuition under the above setting
is that shared documents between categories create
semantic links between them. Thus, if we remove
common documents between father and children, we
reduce the dependencies that can be captured with
traditional bag-of-words representation.
We carried out experiments on two entire hierar-
chies TOPICS (103 nodes organized in 5 levels) and
INDUSTRIAL (365 nodes organized in 6 levels) of
the well-known Reuters Corpus Volume 1 (RCV1).
We first evaluate the accuracy as well as the ef-
ficiency of several reranking models. The results
show that all our rerankers consistently and signif-
icantly improve on the traditional approaches to TC
up to 10 absolute percent points. Very interestingly,
the combination of structural kernels with the lin-
ear kernel applied to vectors of category probabil-
ities further improves on reranking: such a vector
provides a more effective information than the joint
global probability of the reranking hypothesis.
In the rest of the paper, Section 2 describes the hy-
pothesis generation algorithm, Section 3 illustrates
our reranking approach based on tree kernels, Sec-
tion 4 reports on our experiments, Section 5 illus-
trates the related work and finally Section 6 derives
the conclusions.
2 Hierarchy classification hypotheses from
binary decisions
The idea of the paper is to build efficient models
for hierarchical classification using global depen-
dencies. For this purpose, we use reranking mod-
els, which encode global information. This neces-
sitates of a set of initial hypotheses, which are typ-
ically generated by local classifiers. In our study,
we used n one-vs.-all binary classifiers, associated
with the n different nodes of the hierarchy. In the
following sections, we describe a simple framework
for hypothesis generation.
2.1 Top k hypothesis generation
Given n categories, C1, . . . , Cn, we can define
p1Ci(d) and p
0
Ci(d) as the probabilities that the clas-
sifier i assigns the document d to Ci or not, respec-
tively. For example, phCi(d) can be computed from
M132 
M11 M12 M13 M14 
M143 M142 M141 
MCAT 
M131 
Figure 1: A subhierarchy of Reuters.
-M132 
M11 -M12 M13 M14 
 M143 -M142 -M141 
MCAT 
-M131 
Figure 2: A tree representing a category assignment hy-
pothesis for the subhierarchy in Fig. 1.
the SVM classification output (i.e., the example mar-
gin). Typically, a large margin corresponds to high
probability for d to be in the category whereas small
margin indicates low probability1. Let us indicate
with h = {h1, .., hn} ? {0, 1}n a classification hy-
pothesis, i.e., the set of n binary decisions for a doc-
ument d. If we assume independence between the
SVM scores, the most probable hypothesis on d is
h? = argmax
h?{0,1}n
n?
i=1
phii (d) =
(
argmax
h?{0,1}
phi (d)
)n
i=1
.
Given h?, the second best hypothesis can be ob-
tained by changing the label on the least probable
classification, i.e., associated with the index j =
argmin
i=1,..,n
ph?ii (d). By storing the probability of the
k ? 1 most probable configurations, the next k best
hypotheses can be efficiently generated.
3 Structural Kernels for Reranking
Hierarchical Classification
In this section we describe our hypothesis reranker.
The main idea is to represent the hypotheses as a
tree structure, naturally derived from the hierarchy
and then to use tree kernels to encode such a struc-
tural description in a learning algorithm. For this
purpose, we describe our hypothesis representation,
kernel methods and the kernel-based approach to
preference reranking.
3.1 Encoding hypotheses in a tree
Once hypotheses are generated, we need a represen-
tation from which the dependencies between the dif-
1We used the conversion of margin into probability provided
by LIBSVM.
760
M11 M13 M14 
 M143 
MCAT 
Figure 3: A compact representation of the hypothesis in
Fig. 2.
ferent nodes of the hierarchy can be learned. Since
we do not know in advance which are the important
dependencies and not even the scope of the interac-
tion between the different structure subparts, we rely
on automatic feature engineering via structural ker-
nels. For this paper, we consider tree-shaped hier-
archies so that tree kernels, e.g. (Collins and Duffy,
2002; Moschitti, 2006a), can be applied.
In more detail, we focus on the Reuters catego-
rization scheme. For example, Figure 1 shows a sub-
hierarchy of the Markets (MCAT) category and its
subcategories: Equity Markets (M11), Bond Mar-
kets (M12), Money Markets (M13) and Commod-
ity Markets (M14). These also have subcategories:
Interbank Markets (M131), Forex Markets (M132),
Soft Commodities (M141), Metals Trading (M142)
and Energy Markets (M143).
As the input of our reranker, we can simply use
a tree representing the hierarchy above, marking the
negative assignments of the current hypothesis in the
node labels with ?-?, e.g., -M142 means that the doc-
ument was not classified in Metals Trading. For ex-
ample, Figure 2 shows the representation of a classi-
fication hypothesis consisting in assigning the target
document to the categories MCAT, M11, M13, M14
and M143.
Another more compact representation is the hier-
archy tree from which all the nodes associated with
a negative classification decision are removed. As
only a small subset of nodes of the full hierarchy will
be positively classified the tree will be much smaller.
Figure 3 shows the compact representation of the hy-
pothesis in Fig. 2. The next sections describe how to
exploit these kinds of representations.
3.2 Structural Kernels
In kernel-based machines, both learning and classi-
fication algorithms only depend on the inner prod-
uct between instances. In several cases, this can be
efficiently and implicitly computed by kernel func-
tions by exploiting the following dual formulation:
?
i=1..l yi?i?(oi)?(o) + b = 0, where oi and o are
two objects, ? is a mapping from the objects to fea-
ture vectors ~xi and ?(oi)?(o) = K(oi, o) is a ker-
nel function implicitly defining such a mapping. In
case of structural kernels,K determines the shape of
the substructures describing the objects above. The
most general kind of kernels used in NLP are string
kernels, e.g. (Shawe-Taylor and Cristianini, 2004),
the Syntactic Tree Kernels (Collins and Duffy, 2002)
and the Partial Tree Kernels (Moschitti, 2006a).
3.2.1 String Kernels
The String Kernels (SK) that we consider count
the number of subsequences shared by two strings
of symbols, s1 and s2. Some symbols during the
matching process can be skipped. This modifies
the weight associated with the target substrings as
shown by the following SK equation:
SK(s1, s2) =
?
u???
?u(s1) ? ?u(s2) =
?
u???
?
~I1:u=s1[~I1]
?
~I2:u=s2[~I2]
?d(
~I1)+d(~I2)
where, ?? =
??
n=0 ?
n is the set of all strings, ~I1 and
~I2 are two sequences of indexes ~I = (i1, ..., i|u|),
with 1 ? i1 < ... < i|u| ? |s|, such that u = si1 ..si|u| ,
d(~I) = i|u| ? i1 + 1 (distance between the first and
last character) and ? ? [0, 1] is a decay factor.
It is worth noting that: (a) longer subsequences
receive lower weights; (b) some characters can be
omitted, i.e. gaps; (c) gaps determine a weight since
the exponent of ? is the number of characters and
gaps between the first and last character; and (c)
the complexity of the SK computation is O(mnp)
(Shawe-Taylor and Cristianini, 2004), where m and
n are the lengths of the two strings, respectively and
p is the length of the largest subsequence we want to
consider.
In our case, given a hypothesis represented as
a tree like in Figure 2, we can visit it and derive
a linearization of the tree. SK applied to such
a node sequence can derive useful dependencies
between category nodes. For example, using the
Breadth First Search on the compact representa-
tion, we get the sequence [MCAT, M11, M13,
M14, M143], which generates the subsequences,
[MCAT, M11], [MCAT, M11, M13, M14],
[M11, M13, M143], [M11, M13, M143]
and so on.
761
M11 -M12  M13 M14 
MCAT 
M11 -M12  M13 M14 
MCAT 
-M132 -M131 
-M132 -M131 
  M14 
 M143 -M142 -M141 
M11 -M12  M13 M14 
MCAT 
 M143 -M142 -M141   M13 
Figure 4: The tree fragments of the hypothesis in Fig. 2
generated by STK
M14 
-M143 -M142 -M141 -M132 
M13 
-M131 
M11 -M12  M13 M14 
MCAT 
M11 
  MCAT 
-M132 
 M13 
-M131 
M13 
MCAT 
-M131 
-M132 
  M13 M14 
-M142 -M141 M11 -M12 M13 
MCAT MCAT MCAT 
Figure 5: Some tree fragments of the hypothesis in Fig. 2
generated by PTK
3.2.2 Tree Kernels
Convolution Tree Kernels compute the number
of common substructures between two trees T1
and T2 without explicitly considering the whole
fragment space. For this purpose, let the set
F = {f1, f2, . . . , f|F|} be a tree fragment space and
?i(n) be an indicator function, equal to 1 if the
target fi is rooted at node n and equal to 0 oth-
erwise. A tree-kernel function over T1 and T2 is
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), NT1
and NT2 are the sets of the T1?s and T2?s nodes,
respectively and ?(n1, n2) =
?|F|
i=1 ?i(n1)?i(n2).
The latter is equal to the number of common frag-
ments rooted in the n1 and n2 nodes. The ? func-
tion determines the richness of the kernel space and
thus different tree kernels. Hereafter, we consider
the equation to evaluate STK and PTK.2
Syntactic Tree Kernels (STK) To compute STK,
it is enough to compute ?STK(n1, n2) as follows
(recalling that since it is a syntactic tree kernels, each
node can be associated with a production rule): (i)
if the productions at n1 and n2 are different then
?STK(n1, n2) = 0; (ii) if the productions at n1
and n2 are the same, and n1 and n2 have only
leaf children then ?STK(n1, n2) = ?; and (iii) if
the productions at n1 and n2 are the same, and n1
and n2 are not pre-terminals then ?STK(n1, n2) =
?
?l(n1)
j=1 (1 + ?STK(c
j
n1 , c
j
n2)), where l(n1) is the
2To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
number of children of n1 and c
j
n is the j-th child
of the node n. Note that, since the productions
are the same, l(n1) = l(n2) and the computational
complexity of STK is O(|NT1 ||NT2 |) but the aver-
age running time tends to be linear, i.e. O(|NT1 | +
|NT2 |), for natural language syntactic trees (Mos-
chitti, 2006a; Moschitti, 2006b).
Figure 4 shows the five fragments of the hypothe-
sis in Figure 2. Such fragments satisfy the constraint
that each of their nodes includes all or none of its
children. For example, [M13 [-M131 -M132]] is an
STF, which has two non-terminal symbols, -M131
and -M132, as leaves while [M13 [-M131]] is not an
STF.
The Partial Tree Kernel (PTK) The compu-
tation of PTK is carried out by the following
?PTK function: if the labels of n1 and n2 are dif-
ferent then ?PTK(n1, n2) = 0; else ?PTK(n1, n2) =
?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(~I1)+d(~I2)
l(~I1)?
j=1
?PTK(cn1(~I1j), cn2(~I2j))
)
where d(~I1) = ~I1l(~I1) ?
~I11 and d(~I2) = ~I2l(~I2) ?
~I21. This way, we penalize both larger trees and
child subsequences with gaps. PTK is more gen-
eral than STK as if we only consider the contribu-
tion of shared subsequences containing all children
of nodes, we implement STK. The computational
complexity of PTK isO(p?2|NT1 ||NT2 |) (Moschitti,
2006a), where p is the largest subsequence of chil-
dren that we want consider and ? is the maximal out-
degree observed in the two trees. However the aver-
age running time again tends to be linear for natural
language syntactic trees (Moschitti, 2006a).
Given a target T , PTK can generate any subset of
connected nodes of T , whose edges are in T . For
example, Fig. 5 shows the tree fragments from the
hypothesis of Fig. 2. Note that each fragment cap-
tures dependencies between different categories.
3.3 Preference reranker
When training a reranker model, the task of the ma-
chine learning algorithm is to learn to select the best
candidate from a given set of hypotheses. To use
SVMs for training a reranker, we applied Preference
Kernel Method (Shen et al, 2003). The reduction
method from ranking tasks to binary classification is
an active research area; see for instance (Balcan et
al., 2008) and (Ailon and Mohri, 2010).
762
Category
Child-free Child-full
Train Train1 Train2 TEST Train Train1 Train2 TEST
C152 837 370 467 438 837 370 467 438
GPOL 723 357 366 380 723 357 366 380
M11 604 309 205 311 604 309 205 311
.. .. .. .. .. .. .. .. ..
C31 313 163 150 179 531 274 257 284
E41 191 89 95 102 223 121 102 118
GCAT 345 177 168 173 3293 1687 1506 1600
.. .. .. .. .. .. .. .. ..
E31 11 4 7 6 32 21 11 19
M14 96 49 47 58 1175 594 581 604
G15 5 4 1 0 290 137 153 146
Total: 103 10,000 5,000 5,000 5,000 10,000 5,000 5,000 5,000
Table 1: Instance distributions of RCV1: the most populated categories are on the top, the medium sized ones follow
and the smallest ones are at the bottom. There are some difference between child-free and child-full setting since for
the former, from each node, we removed all the documents in its children.
In the Preference Kernel approach, the reranking
problem ? learning to pick the correct candidate h1
from a candidate set {h1, . . . , hk} ? is reduced to a
binary classification problem by creating pairs: pos-
itive training instances ?h1, h2?, . . . , ?h1, hk? and
negative instances ?h2, h1?, . . . , ?hk, h1?. This train-
ing set can then be used to train a binary classifier.
At classification time, pairs are not formed (since the
correct candidate is not known); instead, the stan-
dard one-versus-all binarization method is still ap-
plied.
The kernels are then engineered to implicitly
represent the differences between the objects in
the pairs. If we have a valid kernel K over the
candidate space T , we can construct a preference
kernel PK over the space of pairs T ?T as follows:
PK(x, y) =
PK(?x1, x2?, ?y1, y2?) = K(x1, y1)+
K(x2, y2)?K(x1, y2)?K(x2, y1),
(1)
where x, y ? T ? T . It is easy to show (Shen et al,
2003) that PK is also a valid Mercer?s kernel. This
makes it possible to use kernel methods to train the
reranker.
We explore innovative kernels K to be used in
Eq. 1:
KJ = p(x1) ? p(y1) + S, where p(?) is the global
joint probability of a target hypothesis and S is
a structural kernel, i.e., SK, STK and PTK.
KP = ~x1 ? ~y1 + S, where ~x1={p(x1, j)}j?x1 ,
~y1 = {p(y1, j)}j?y1 , p(t, n) is the classifica-
tion probability of the node (category) n in the
F1 BL BOL SK STK PTK
Micro-F1 0.769 0.771 0.786 0.790 0.790
Macro-F1 0.539 0.541 0.542 0.547 0.560
Table 2: Comparison of rerankers using different kernels,
child-full setting (KJ model).
F1 BL BOL SK STK PTK
Micro-F1 0.640 0.649 0.653 0.677 0.682
Macro-F1 0.408 0.417 0.431 0.447 0.447
Table 3: Comparison of rerankers using different kernels,
child-free setting (KJ model).
tree t ? T and S is again a structural kernel,
i.e., SK, STK and PTK.
For comparative purposes, we also use for S a lin-
ear kernel over the bag-of-labels (BOL). This is
supposed to capture non-structural dependencies be-
tween the category labels.
4 Experiments
The aim of the experiments is to demonstrate that
our reranking approach can introduce semantic de-
pendencies in the hierarchical classification model,
which can improve accuracy. For this purpose, we
show that several reranking models based on tree
kernels improve the classification based on the flat
one-vs.-all approach. Then, we analyze the effi-
ciency of our models, demonstrating their applica-
bility.
4.1 Setup
We used two full hierarchies, TOPICS and INDUS-
TRY of Reuters Corpus Volume 1 (RCV1)3 TC cor-
3trec.nist.gov/data/reuters/reuters.html
763
pus. For most experiments, we randomly selected
two subsets of 10k and 5k of documents for train-
ing and testing from the total 804,414 Reuters news
from TOPICS by still using all the 103 categories
organized in 5 levels (hereafter SAM). The distri-
bution of the data instances of some of the dif-
ferent categories in such samples can be observed
in Table 1. The training set is used for learning
the binary classifiers needed to build the multiclass-
classifier (MCC). To compare with previous work
we also considered the Lewis? split (Lewis et al,
2004), which includes 23,149 news for training and
781,265 for testing.
Additionally, we carried out some experiments on
INDUSTRY data from RCV1. This contains 352,361
news assigned to 365 categories, which are orga-
nized in 6 levels. The Lewis? split for INDUSTRY in-
cludes 9,644 news for training and 342,117 for test-
ing. We used the above datasets with two different
settings: the child-free setting, where we removed
all the document belonging to the child nodes from
the parent nodes, and the normal setting which we
refer to as child-full.
To implement the baseline model, we applied the
state-of-the-art method used by (Lewis et al, 2004)
for RCV1, i.e.,: SVMs with the default parameters
(trade-off and cost factor = 1), linear kernel, normal-
ized vectors, stemmed bag-of-words representation,
log(TF + 1) ? IDF weighting scheme and stop
list4. We used the LIBSVM5 implementation, which
provides a probabilistic outcome for the classifica-
tion function. The classifiers are combined using the
one-vs.-all approach, which is also state-of-the-art
as argued in (Rifkin and Klautau, 2004). Since the
task requires us to assign multiple labels, we simply
collect the decisions of the n classifiers: this consti-
tutes our MCC baseline.
Regarding the reranker, we divided the training
set in two chunks of data: Train1 and Train2. The
binary classifiers are trained on Train1 and tested on
Train2 (and vice versa) to generate the hypotheses
on Train2 (Train1). The union of the two sets con-
stitutes the training data for the reranker. We imple-
4We have just a small difference in the number of tokens,
i.e., 51,002 vs. 47,219 but this is both not critical and rarely
achievable because of the diverse stop lists or tokenizers.
5http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
0.626
0.636
0.646
0.656
0.666
0.676
2 7 12 17 22 27 32
Micr
o-F1
Training Data Size (thousands of instances)
BL (Child-free)
RR (Child-free)
FRR (Child-free)
Figure 6: Learning curves of the reranking models using
STK in terms of MicroAverage-F1, according to increas-
ing training set (child-free setting).
0.365
0.375
0.385
0.395
0.405
0.415
0.425
0.435
0.445
2 7 12 17 22 27 32
Mac
ro-F
1
Training Data Size (thousands of instances)
BL (Child-free)
RR (Child-free)
FRR (Child-free)
Figure 7: Learning curves of the reranking models using
STK in terms of MacroAverage-F1, according to increas-
ing training set (child-free setting).
mented two rerankers: RR, which use the represen-
tation of hypotheses described in Fig. 2; and FRR,
i.e., fast RR, which uses the compact representation
described in Fig. 3.
The rerankers are based on SVMs and the Prefer-
ence Kernel (PK) described in Sec. 1 built on top of
SK, STK or PTK (see Section 3.2.2). These are ap-
plied to the tree-structured hypotheses. We trained
the rerankers using SVM-light-TK6, which enables
the use of structural kernels in SVM-light (Joachims,
1999). This allows for applying kernels to pairs of
trees and combining them with vector-based kernels.
Again we use default parameters to facilitate replica-
bility and preserve generality. The rerankers always
use 8 best hypotheses.
All the performance values are provided by means
of Micro- and Macro-Average F1, evaluated on test
6disi.unitn.it/moschitti/Tree-Kernel.htm
764
Cat.
Child-free Child-full
BL KJ KP BL KJ KP
C152 0.671 0.700 0.771 0.671 0.729 0.745
GPOL 0.660 0.695 0.743 0.660 0.680 0.734
M11 0.851 0.891 0.901 0.851 0.886 0.898
.. .. .. .. .. .. ..
C31 0.225 0.311 0.446 0.356 0.421 0.526
E41 0.643 0.714 0.719 0.776 0.791 0.806
GCAT 0.896 0.908 0.917 0.908 0.916 0.926
.. .. .. .. .. .. ..
E31 0.444 0.600 0.600 0.667 0.765 0.688
M14 0.591 0.600 0.575 0.887 0.897 0.904
G15 0.250 0.222 0.250 0.823 0.806 0.826
103 cat.
Mi-F1 0.640 0.677 0.731 0.769 0.794 0.815
Ma-F1 0.408 0.447 0.507 0.539 0.567 0.590
Table 4: F1 of some binary classifiers along with the
Micro and Macro-Average F1 over all 103 categories
of RCV1, 8 hypotheses and 32k of training data for
rerankers using STK.
data over all categories (103 or 363). Additionally,
the F1 of some binary classifiers are reported.
4.2 Classification Accuracy
In the first experiments, we compared the different
kernels using the KJ combination (which exploits
the joint hypothesis probability, see Sec. 3.3) on
SAM. Tab. 2 shows that the baseline (state-of-the-
art flat model) is largely improved by all rerankers.
BOL cannot capture the same dependencies as the
structural kernels. In contrast, when we remove the
dependencies generated by shared documents be-
tween a node and its descendants (child-free setting)
BOL improves on BL. Very interestingly, TK and
PTK in this setting significantly improves on SK
suggesting that the hierarchical structure is more im-
portant than the sequential one.
To study how much data is needed for the
reranker, the figures 6 and 7 report the Micro and
Macro Average F1 of our rerankers over 103 cate-
gories, according to different sets of training data.
This time, KJ is applied to only STK. We note that
(i) a few thousands of training examples are enough
to deliver most of the RR improvement; and (ii) the
FRR produces similar results as standard RR. This is
very interesting since, as it will be shown in the next
section, the compact representation produces much
faster models.
Table 4 reports the F1 of some individual cate-
gories as well as global performance. In these exper-
iments we used STK in KJ and KP . We note that
0
50
100
150
200
250
300
350
400
450
2 12 22 32 42 52 62
Time 
(min)
Training Data Size (thousands of instances)
RR trainingTime
RR testTime
FRR trainingTime
FRR testTime
Figure 8: Training and test time of the rerankers trained
on data of increasing size.
KP highly improves on the baseline on child-free
setting by about 7.1 and 9.9 absolute percent points
in Micro-and Macro-F1, respectively. Also the im-
provement on child-full is meaningful, i.e., 4.6 per-
cent points. This is rather interesting as BOL (not
reported in the table) achieved a Micro-average of
80.4% and a Macro-average of 57.2% when used in
KP , i.e., up to 2 points below STK. This means that
the use of probability vectors and combination with
structural kernels is a very promising direction for
reranker design.
To definitely assess the benefit of our rerankers
we tested them on the Lewis? split of two different
datasets of RCV1, i.e., TOPIC and INDUSTRY. Ta-
ble 5 shows impressive results, e.g., for INDUSTRY,
the improvement is up to 5.2 percent points. We car-
ried out statistical significance tests, which certified
the significance at 99%. This was expected as the
size of the Lewis? test sets is in the order of several
hundreds thousands.
Finally, to better understand the potential of
reranking, Table 6 shows the oracle performance
with respect to the increasing number of hypothe-
ses. The outcome clearly demonstrates that there is
large margin of improvement for the rerankers.
4.3 Running Time
To study the applicability of our rerankers, we have
analyzed both the training and classification time.
Figure 8 shows the minutes required to train the dif-
ferent models as well as to classify the test set ac-
cording to data of increasing size.
It can be noted that the models using the compact
hypothesis representation are much faster than those
765
F1
Topic Industry
BL (Lewis) BL (Ours) KJ (BOL) KJ KP BL (Lewis) BL (Ours) KJ (BOL) KJ KP
Micro-F1 0.816 0.815 0.818 0.827 0.849 0.512 0.562 0.566 0.576 0.628
Macro-F1 0.567 0.566 0.571 0.590 0.615 0.263 0.289 0.243 0.314 0.341
Table 5: Comparison between rankers using STK or BOL (when indicated) with the KJ and KP schema. 32k
examples are used for training the rerankers with child-full setting.
k Micro-F1 Macro-F1
1 0.640 0.408
2 0.758 0.504
4 0.821 0.566
8 0.858 0.610
16 0.898 0.658
Table 6: Oracle performance according to the number of
hypotheses (child-free setting).
using the complete hierarchy as representation, i.e.,
up to five times in training and eight time in test-
ing. This is not surprising as, in the latter case,
each kernel evaluation requires to perform tree ker-
nel evaluation on trees of 103 nodes. When using
the compact representation the number of nodes is
upper-bounded by the maximum number of labels
per documents, i.e., 6, times the depth of the hierar-
chy, i.e., 5 (the positive classification on the leaves
is the worst case). Thus, the largest tree would con-
tain 30 nodes. However, we only have 1.82 labels
per document on average, therefore the trees have
an average size of only about 9 nodes.
5 Related Work
Tree and sequence kernels have been successfully
used in many NLP applications, e.g.: parse rerank-
ing and adaptation (Collins and Duffy, 2002; Shen
et al, 2003; Toutanova et al, 2004; Kudo et al,
2005; Titov and Henderson, 2006), chunking and
dependency parsing (Kudo and Matsumoto, 2003;
Daume? III and Marcu, 2004), named entity recog-
nition (Cumby and Roth, 2003), text categorization
(Cancedda et al, 2003; Gliozzo et al, 2005) and re-
lation extraction (Zelenko et al, 2002; Bunescu and
Mooney, 2005; Zhang et al, 2006).
To our knowledge, ours is the first work explor-
ing structural kernels for reranking hierarchical text
categorization hypotheses. Additionally, there is a
substantial lack of work exploring reranking for hi-
erarchical text categorization. The work mostly re-
lated to ours is (Rousu et al, 2006) as they directly
encoded global dependencies in a gradient descen-
dent learning approach. This kind of algorithm is
less efficient than ours so they could experiment
with only the CCAT subhierarchy of RCV1, which
only contains 34 nodes. Other relevant work such
as (McCallum et al, 1998) and (Dumais and Chen,
2000) uses a rather different datasets and a different
idea of dependencies based on feature distributions
over the linked categories. An interesting method is
SVM-struct (Tsochantaridis et al, 2005), which has
been applied to model dependencies expressed as
category label subsets of flat categorization schemes
but no solution has been attempted for hierarchical
settings. The approaches in (Finley and Joachims,
2007; Riezler and Vasserman, 2010; Lavergne et al,
2010) can surely be applied to model dependencies
in a tree, however, they need that feature templates
are specified in advance, thus the meaningful depen-
dencies must be already known. In contrast, kernel
methods allow for automatically generating all pos-
sible dependencies and reranking can efficiently en-
code them.
6 Conclusions
In this paper, we have described several models for
reranking the output of an MCC based on SVMs
and structural kernels, i.e., SK, STK and PTK.
We have proposed a simple and efficient algorithm
for hypothesis generation and their kernel-based
representations. The latter are exploited by SVMs
using preference kernels to automatically derive
features from the hypotheses. When using tree
kernels such features are tree fragments, which can
encode complex semantic dependencies between
categories. We tested our rerankers on the entire
well-known RCV1. The results show impressive
improvement on the state-of-the-art flat TC models,
i.e., 3.3 absolute percent points on the Lewis? split
(same setting) and up to 10 absolute points on
samples using child-free setting.
Acknowledgements This research is partially sup-
ported by the EC FP7/2007-2013 under the grants:
247758 (ETERNALS), 288024 (LIMOSINE) and 231126
(LIVINGKNOWLEDGE). Many thanks to the reviewers
for their valuable suggestions.
766
References
Nir Ailon and Mehryar Mohri. 2010. Preference-based
learning to rank. Machine Learning.
Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer,
Don Coppersmith, John Langford, and Gregory B.
Sorkin. 2008. Robust reductions from ranking to clas-
sification. Machine Learning, 72(1-2):139?153.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of ACL?02, pages 263?270.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Susan T. Dumais and Hao Chen. 2000. Hierarchical clas-
sification of web content. In Nicholas J. Belkin, Peter
Ingwersen, and Mun-Kew Leong, editors, Proceedings
of SIGIR-00, 23rd ACM International Conference on
Research and Development in Information Retrieval,
pages 256?263, Athens, GR. ACM Press, New York,
US.
T. Finley and T. Joachims. 2007. Parameter learning
for loopy markov random fields with structural support
vector machines. In ICML Workshop on Constrained
Optimization and Structured Output Spaces.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods ? Sup-
port Vector Learning, 13.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
T. Lavergne, O. Cappe?, and F. Yvon. 2010. Practical very
large scale CRFs. In Proc. of ACL, pages 504?513.
D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. Rcv1: A
new benchmark collection for text categorization re-
search. The Journal of Machine Learning Research,
(5):361?397.
Andrew McCallum, Ronald Rosenfeld, Tom M. Mitchell,
and Andrew Y. Ng. 1998. Improving text classifica-
tion by shrinkage in a hierarchy of classes. In ICML,
pages 359?367.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proccedings of
EACL?06.
S. Riezler and A. Vasserman. 2010. Incremental feature
selection and l1 regularization for relaxed maximum-
entropy modeling. In EMNLP.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101?
141, December.
Juho Rousu, Craig Saunders, Sandor Szedmak, and John
Shawe-Taylor. 2006. Kernel-based learning of hierar-
chical multilabel classification models. The Journal of
Machine Learning Research, (7):1601?1626.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89?96, Sapporo, Japan.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output vari-
ables. J. Machine Learning Reserach., 6:1453?1484,
December.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
767
Tutorial Abstracts of ACL 2012, page 2,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
State-of-the-Art Kernels for Natural Language Processing
Alessandro Moschitti
Department of Computer Science and Information Engineering
University of Trento
Via Sommarive 5, 38123 Povo (TN), Italy
moschitti@disi.unitn.it
Introduction
In recent years, machine learning (ML) has been
used more and more to solve complex tasks in dif-
ferent disciplines, ranging from Data Mining to In-
formation Retrieval or Natural Language Processing
(NLP). These tasks often require the processing of
structured input, e.g., the ability to extract salient
features from syntactic/semantic structures is criti-
cal to many NLP systems. Mapping such structured
data into explicit feature vectors for ML algorithms
requires large expertise, intuition and deep knowl-
edge about the target linguistic phenomena. Ker-
nel Methods (KM) are powerful ML tools (see e.g.,
(Shawe-Taylor and Cristianini, 2004)), which can al-
leviate the data representation problem. They substi-
tute feature-based similarities with similarity func-
tions, i.e., kernels, directly defined between train-
ing/test instances, e.g., syntactic trees. Hence fea-
ture vectors are not needed any longer. Additionally,
kernel engineering, i.e., the composition or adapta-
tion of several prototype kernels, facilitates the de-
sign of effective similarities required for new tasks,
e.g., (Moschitti, 2004; Moschitti, 2008).
Tutorial Content
The tutorial aims at addressing the problems above:
firstly, it will introduce essential and simplified the-
ory of Support Vector Machines and KM with the
only aim of motivating practical procedures and in-
terpreting the results. Secondly, it will simply de-
scribe the current best practices for designing ap-
plications based on effective kernels. For this pur-
pose, it will survey state-of-the-art kernels for di-
verse NLP applications, reconciling the different ap-
proaches with a uniform and global notation/theory.
Such survey will benefit from practical expertise ac-
quired from directly working on many natural lan-
guage applications, ranging from Text Categoriza-
tion to Syntactic/Semantic Parsing. Moreover, prac-
tical demonstrations using SVM-Light-TK toolkit
will nicely support the application-oriented perspec-
tive of the tutorial. The latter will lead NLP re-
searchers with heterogeneous background to the ac-
quisition of the KM know-how, which can be used
to design any target NLP application.
Finally, the tutorial will propose interesting new
best practices, e.g., some recent methods for large-
scale learning with structural kernels (Severyn
and Moschitti, 2011), structural lexical similarities
(Croce et al, 2011) and reverse kernel engineering
(Pighin and Moschitti, 2009).
References
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured Lexical Similarity via Convolution
Kernels on Dependency Trees. In Proc. of EMNLP.
Alessandro Moschitti. 2004. A Study on Convolution
Kernels for Shallow Semantic Parsing. In Proceedings
of ACL.
Alessandro Moschitti. 2008. Kernel Methods, Syntax
and Semantics for Relational Text Categorization. In
Proceedings of CIKM.
Daniele Pighin and Alessandro Moschitti. 2009. Effi-
cient Linearization of Tree Kernel Functions. In Pro-
ceedings of CoNLL.
Aliaksei Severyn and Alessandro Moschitti. 2011. Fast
Support Vector Machines for Structural Kernels. In
ECML.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univ. Press.
2
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498?1507,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Embedding Semantic Similarity in Tree Kernels for Domain Adaptation
of Relation Extraction
Barbara Plank?
Center for Language Technology
University of Copenhagen, Denmark
bplank@gmail.com
Alessandro Moschitti
QCRI - Qatar Foundation &
DISI - University of Trento, Italy
amoschitti@qf.org.qa
Abstract
Relation Extraction (RE) is the task of
extracting semantic relationships between
entities in text. Recent studies on rela-
tion extraction are mostly supervised. The
clear drawback of supervised methods is
the need of training data: labeled data is
expensive to obtain, and there is often a
mismatch between the training data and
the data the system will be applied to.
This is the problem of domain adapta-
tion. In this paper, we propose to combine
(i) term generalization approaches such as
word clustering and latent semantic anal-
ysis (LSA) and (ii) structured kernels to
improve the adaptability of relation ex-
tractors to new text genres/domains. The
empirical evaluation on ACE 2005 do-
mains shows that a suitable combination
of syntax and lexical generalization is very
promising for domain adaptation.
1 Introduction
Relation extraction is the task of extracting se-
mantic relationships between entities in text, e.g.
to detect an employment relationship between the
person Larry Page and the company Google in
the following text snippet: Google CEO Larry
Page holds a press announcement at its headquar-
ters in New York on May 21, 2012. Recent stud-
ies on relation extraction have shown that super-
vised approaches based on either feature or ker-
nel methods achieve state-of-the-art accuracy (Ze-
lenko et al, 2002; Culotta and Sorensen, 2004;
? The first author was affiliated with the Department of
Computer Science and Information Engineering of the Uni-
versity of Trento (Povo, Italy) during the design of the mod-
els, experiments and writing of the paper.
Zhang et al, 2005; Zhou et al, 2005; Zhang et
al., 2006; Bunescu, 2007; Nguyen et al, 2009;
Chan and Roth, 2010; Sun et al, 2011). How-
ever, the clear drawback of supervised methods is
the need of training data, which can slow down
the delivery of commercial applications in new
domains: labeled data is expensive to obtain, and
there is often a mismatch between the training data
and the data the system will be applied to. Ap-
proaches that can cope with domain changes are
essential. This is the problem of domain adapta-
tion (DA) or transfer learning (TL). Technically,
domain adaptation addresses the problem of learn-
ing when the assumption of independent and iden-
tically distributed (i.i.d.) samples is violated. Do-
main adaptation has been studied extensively dur-
ing the last couple of years for various NLP tasks,
e.g. two shared tasks have been organized on do-
main adaptation for dependency parsing (Nivre et
al., 2007; Petrov and McDonald, 2012). Results
were mixed, thus it is still a very active research
area.
However, to the best of our knowledge, there
is almost no work on adapting relation extraction
(RE) systems to new domains.1 There are some
prior studies on the related tasks of multi-task
transfer learning (Xu et al, 2008; Jiang, 2009)
and distant supervision (Mintz et al, 2009), which
are clearly related but different: the former is the
problem of how to transfer knowledge from old
to new relation types, while distant supervision
tries to learn new relations from unlabeled text
by exploiting weak-supervision in the form of a
knowledge resource (e.g. Freebase). We assume
the same relation types but a shift in the underlying
1Besides an unpublished manuscript of a student project,
but it is not clear what data was used. http://tinyurl.com/
bn2hdwk
1498
data distribution. Weak supervision is a promis-
ing approach to improve a relation extraction sys-
tem, especially to increase its coverage in terms of
types of relations covered. In this paper we ex-
amine the related issue of changes in the underly-
ing data distribution, while keeping the relations
fixed. Even a weakly supervised system is ex-
pected to perform well when applied to any kind of
text (other domain/genre), thus ideally, we believe
that combining domain adaptation with weak su-
pervision is the way to go in the future. This study
is a first step towards this.
We focus on unsupervised domain adaptation,
i.e. no labeled target data. Moreover, we consider
a particular domain adaptation setting: single-
system DA, i.e. learning a single system able to
cope with different but related domains. Most
studies on DA so far have focused on building
a specialized system for every specific target do-
main, e.g. Blitzer et al (2006). In contrast, the
goal here is to build a single system that can ro-
bustly handle several domains, which is in line
with the setup of the recent shared task on pars-
ing the web (Petrov and McDonald, 2012). Par-
ticipants were asked to build a single system that
can robustly parse all domains (reviews, weblogs,
answers, emails, newsgroups), rather than to build
several domain-specific systems. We consider this
as a shift in what was considered domain adapta-
tion in the past (adapt from source to a specific tar-
get) and what can be considered a somewhat dif-
ferent recent view of DA, that became widespread
since 2011/2012. The latter assumes that the tar-
get domain(s) is/are not really known in advance.
In this setup, the domain adaptation problem boils
down to finding a more robust system (S?gaard
and Johannsen, 2012), i.e. one wants to build a
system that can robustly handle any kind of data.
We propose to combine (i) term generalization
approaches and (ii) structured kernels to improve
the performance of a relation extractor on new
domains. Previous studies have shown that lexi-
cal and syntactic features are both very important
(Zhang et al, 2006). We combine structural fea-
tures with lexical information generalized by clus-
ters or similarity. Given the complexity of feature
engineering, we exploit kernel methods (Shawe-
Taylor and Cristianini, 2004). We encode word
clusters or similarity in tree kernels, which, in
turn, produce spaces of tree fragments. For ex-
ample, ?president?, ?vice-president? and ?Texas?,
?US?, are terms indicating an employment rela-
tion between a person and a location. Rather than
only matching the surface string of words, lexi-
cal similarity enables soft matches between similar
words in convolution tree kernels. In the empir-
ical evaluation on Automatic Content Extraction
(ACE) data, we evaluate the impact of convolu-
tion tree kernels embedding lexical semantic sim-
ilarities. The latter is derived in two ways with:
(a) Brown word clustering (Brown et al, 1992);
and (b) Latent Semantic Analysis (LSA). We first
show that our system aligns well with the state of
the art on the ACE 2004 benchmark. Then, we
test our RE system on the ACE 2005 data, which
exploits kernels, structures and similarities for do-
main adaptation. The results show that combining
the huge space of tree fragments generalized at the
lexical level provides an effective model for adapt-
ing RE systems to new domains.
2 Semantic Syntactic Tree Kernels
In kernel-based methods, both learning and classi-
fication only depend on the inner product between
instances. Kernel functions can be efficiently and
implicitly computed by exploiting the dual formu-
lation: ?i=1..l yi?i?(oi)?(o) + b = 0, where oi
and o are two objects, ? is a mapping from an ob-
ject to a feature vector ~xi and ?(oi)?(o) =K(oi, o)
is a kernel function implicitly defining such a map-
ping. In case of structural kernels, K determines
the shape of the substructures describing the ob-
jects. Commonly used kernels in NLP are string
kernels (Lodhi et al, 2002) and tree kernels (Mos-
chitti, 2006; Moschitti, 2008).
NP
PP
NP
E2
NNP
Texas
IN
from
NP
E1
NNP
governor
?
NP
PPNP
NP
PPNP
E1
NP
PPNP
E1
NNP
governor
E1
NNP
governor
. . .
NNP
Texas
Figure 1: Syntactic tree kernel (STK).
Syntactic tree kernels (Collins and Duffy, 2001)
compute the similarity between two trees T1
and T2 by counting common sub-trees (cf. Fig-
ure 1), without enumerating the whole fragment
space. However, if two trees have similar sub-
structures that employ different though related ter-
minal nodes, they will not be matched. This is
1499
clearly a limitation. For instance, the fragments
corresponding to governor from Texas and
head of Maryland are intuitively semanti-
cally related and should obtain a higher match
when compared to mother of them.
Semantic syntactic tree kernels (Bloehdorn
and Moschitti, 2007a; Bloehdorn and Moschitti,
2007b; Croce et al, 2011) provide one way to ad-
dress this problem by introducing similarity ? that
allows soft matches between words and, conse-
quently, between fragments containing them. Let
N1 and N2 be the set of nodes in T1 and T2, re-
spectively. Moreover, let Ii(n) be an indicator
variable that is 1 if subtree i is rooted at n and
0 otherwise. The syntactic semantic convolution
kernel TK? (Bloehdorn and Moschitti, 2007b)
over T1 and T2 is computed as TK?(T1, T2) =?
n1?N1,n2?N2 ??(n1, n2) where ??(n1, n2) =?
n1?N1
?
n2?N2
?
i Ii(n1)Ii(n2) is computed ef-
ficiently using the following recursive defini-
tion: i) If the nodes n1 and n2 are ei-
ther different or have different number of chil-
dren then ??(n1, n2) = 0; else ii) If
n1 and n2 are pre-terminals then ??(n1, n2)
= ??nc(n1)j=1 ??(ch(n1, j), ch(n2, j)), where ?
measures the similarity between the correspond-
ing children of n1 and n2; iii) If n1 and n2 have
identical children: ??(n1, n2) = ??nc(n1)j=1 (1 +
??(ch(n1, j)), ch(n2, j)); else ??(n1, n2) = 0.
TK? combines generalized lexical with structural
information: it allows matching tree fragments
that have the same syntactic structure but differ in
their terminals. After introducing related work, we
will discuss computational structures for RE and
their extension with semantic similarity.
3 Related Work
Semantic syntactic tree kernels have been previ-
ously used for question classification (Bloehdorn
and Moschitti, 2007a; Bloehdorn and Moschitti,
2007b; Croce et al, 2011). These kernels have
not yet been studied for either domain adaptation
or RE. Brown clusters were studied previously for
feature-based approaches to RE (Sun et al, 2011;
Chan and Roth, 2010), but they were not yet eval-
uated in kernels. Thus, we present a novel applica-
tion of semantic syntactic tree kernels and Brown
clusters for domain adaptation of tree-kernel based
relation extraction.
Regarding domain adaptation, several meth-
ods have been proposed, ranging from instance
weighting (Jiang and Zhai, 2007) to approaches
that change the feature representation (Daume? III,
2007) or try to exploit pivot features to find
a generalized shared representation between do-
mains (Blitzer et al, 2006). The easy-adapt ap-
proach presented in Daume? III (2007) assumes the
supervised adaptation setting and is thus not ap-
plicable here. Structural correspondence learn-
ing (Blitzer et al, 2006) exploits unlabeled data
from both source and target domain to find cor-
respondences among features from different do-
mains. These correspondences are then integrated
as new features in the labeled data of the source
domain. The key to SCL is to exploit pivot fea-
tures to automatically identify feature correspon-
dences, and as such is applicable to feature-based
approaches but not in our case since we do not as-
sume availability of target domain data. Instead,
we apply a similar idea where we exploit an en-
tire unlabeled corpus as pivot, and compare our
approach to instance weighting (Jiang and Zhai,
2007).
Instance weighting is a method for domain
adaptation in which instance-dependent weights
are assigned to the loss function that is mini-
mized during the training process. Let l(x, y, ?)
be some loss function. Then, as shown in Jiang
and Zhai (2007), the loss function can be weighted
by ?il(x, y, ?), such that ?i = Pt(xi)Ps(xi) , where Psand Pt are the source and target distributions, re-
spectively. Huang et al (2007) present an appli-
cation of instance weighting to support vector ma-
chines by minimizing the following re-weighted
function: min?,? 12 ||?||2 + C
?m
i=1 ?i?i. Finding
a good weight function is non-trivial (Jiang and
Zhai, 2007) and several approximations have been
evaluated in the past, e.g. S?gaard and Haulrich
(2011) use a bigram-based text classifier to dis-
criminate between domains. We will use a binary
classifier trained on RE instance representations.
4 Computational Structures for RE
A common way to represent a constituency-based
relation instance is the PET (path-enclosed-tree),
the smallest subtree including the two target enti-
ties (Zhang et al, 2006). This is basically the for-
mer structure PAF2 (predicate argument feature)
defined in Moschitti (2004) for the extraction of
predicate argument relations. The syntactic rep-
2It is the smallest subtree enclosing the predicate and one
of its argument node.
1500
resentation used by Zhang et al (2006) (we will
refer to it as PET Zhang) is the PET with enriched
entity information: e.g. E1-NAM-PER, including
entity type (PER, GPE, LOC, ORG) and mention
type (NAM, NOM, PRO, PRE: name, nominal,
pronominal or premodifier). An alternative ker-
nel that does not use syntactic information is the
Bag-of-Words (BOW) kernel, where a single root
node is added above the terminals. Note that in
this BOW kernel we actually mark target entities
with E1/E2. Therefore, our BOW kernel can be
considered an enriched BOW model. If we do not
mark target entities, performance drops consider-
ably, as discussed later.
As shown by Zhang et al (2006), includ-
ing gold-standard information on entity and men-
tion type substantially improves relation extrac-
tion performance. We will use this gold infor-
mation also in Section 6.1 to show that our sys-
tem aligns well to the state of the art on the ACE
2004 benchmark. However, in a realistic setting
this information is not available or noisy. In fact,
as we discuss later, excluding gold entity informa-
tion decreases system performance considerably.
In the case of porting a system to new domains
entity information will be unreliable or missing.
Therefore, in our domain adaptation experiments
on the ACE 2005 data (Section 6.3) we will not
rely on this gold information but rather train a sys-
tem using PET (target mentions only marked with
E1/E2 and no gold entity label).3
4.1 Syntactic Semantic Structures
Combining syntax with semantics has a clear ad-
vantage: it generalizes lexical information encap-
sulated in syntactic parse trees, while at the same
time syntax guides semantics in order to obtain an
effective semantic similarity. In fact, lexical infor-
mation is highly affected by data-sparseness, thus
tree kernels combined with semantic information
created from additional resources should provide
a way to obtain a more robust system.
We exploit this idea here for domain adaptation
(DA): if words are generalized by semantic simi-
larity LS, then in a hypothetical world changing
LS such that it reflects the target domain would
3In a setup where gold label info is included, the impact
of similarity-based methods is limited ? gold information
seems to predominate. We argue that whenever gold data is
not available, distributional semantics paired with kernels can
be useful to improve generalization and complement missing
gold info.
allow the system to perform better in the target
domain. The question remains how to establish a
link between the semantic similarity in the source
and target domain. We propose to use an entire
unlabeled corpus as pivot: this corpus must be
general enough to encapsulate the source and tar-
get domains of interest. The idea is to (i) learn
semantic similarity between words on the pivot
corpus and (ii) use tree kernels embedding such
a similarity to learn a RE system on the source,
which allows to generalize to the new target do-
main. This reasoning is related to Structural Cor-
respondence Learning (SCL) (Blitzer et al, 2006).
In SCL, a representation shared across domains is
learned by exploiting pivot features, where a set
of pivot features has to be selected (usually a few
thousands). In our case pivots are words that co-
occur with the target words in a large unlabeled
corpus and are thus implicitly represented in the
similarity matrix. Thus, in contrast to SCL, we do
not need to select a set of pivot features but rather
rely on the distributional hypothesis to infer a se-
mantic similarity from a large unlabeled corpus.
Then, this similarity is incorporated into the tree
kernel that provides the necessary restriction for
an effective semantic similarity calculation. One
peculiarity of our work is that we exploit a large
amount of general data, i.e. data gathered from the
web, which is a different but also more challeng-
ing scenario than the general unsupervised DA set-
ting where domain specific data is available. We
study two ways for term generalization in tree ker-
nels: Brown words clusters and Latent Semantic
Analysis (LSA), both briefly described next.
a) replace pos
NP
PP
NP
E2
1111100110
Seoul
10001110
from
NP
E1
1101100011
officials
b) replace word
..
NP
E2
NNP
1111100110
c) above pos
..
NP
E2
1111100110
NNP
Seoul
Figure 2: Integrating Brown cluster information
The Brown algorithm (Brown et al, 1992) is
a hierarchical agglomerative hard-clustering algo-
rithm. The path from the root of the tree down to
a leaf node is represented compactly as a bitstring.
By cutting the hierarchy at different levels one can
obtain different granularities of word clusters. We
1501
evaluate different ways to integrate cluster infor-
mation into tree kernels, some of which are illus-
trated in Figure 2.
For LSA, we compute term similarity functions
following the distributional hypothesis (Harris,
1964), i.e. the meaning of a word can be described
by the set of textual contexts in which it appears.
The original word-by-word context matrix M is
decomposed through Singular Value Decomposi-
tion (SVD) (Golub and Kahan, 1965), where M
is approximated by UlSlV Tl . This approxima-
tion supplies a way to project a generic term wi
into the l-dimensional space using W = UlS1/2l ,
where each row corresponds to the vectors ~wi.
Given two words w1 and w2, the term similarity
function ? is estimated as the cosine similarity be-
tween the corresponding projections ~w1, ~w2 and
used in the kernel as described in Section 2.
5 Experimental Setup
We treat relation extraction as a multi-class classi-
fication problem and use SVM-light-TK4 to train
the binary classifiers. The output of the classifiers
is combined using the one-vs-all approach. We
modified the SVM-light-TK package to include
the semantic tree kernels and instance weight-
ing. The entire software package is publicly avail-
able.5 For the SVMs, we use the same parameters
as Zhang et al (2006): ? = 0.4, c = 2.4 using the
Collins Kernel (Collins and Duffy, 2001). The pre-
cision/recall trade-off parameter for the none class
was found on held-out data: j = 0.2. Evalua-
tion metrics are standard micro average Precision,
Recall and balanced Fscore (F1). To compute sta-
tistical significance, we use the approximate ran-
domization test (Noreen, 1989).6 In all our exper-
iments, we model argument order of the relations
explicitly. Thus, for instance for the 7 coarse ACE
2004 relations, we build 14 coarse-grained classi-
fiers (two for each coarse ACE 2004 relation type
except for PER-SOC, which is symmetric, and one
classifier for the none relation).
Data We use two datasets. To compare our
model against the state of the art we use the ACE
2004 data. It contains 348 documents and 4,374
positive relation instances. To generate the train-
ing data, we follow prior studies and extract an
instance for every pair of mentions in the same
4http://disi.unitn.it/moschitti/Tree-Kernel.htm
5http://disi.unitn.it/ikernels/RelationExtraction
6http://www.nlpado.de/?sebastian/software/sigf.shtml
sentence, which are separated by no more than
three other mentions (Zhang et al, 2006; Sun et
al., 2011). After data preprocessing, we obtained
4,327 positive and 39,120 negative instances.
ACE 2005 docs sents ASL relations
nw+bn 298 5029 18.8 3562
bc 52 2267 16.3 1297
cts 34 2696 15.3 603
wl 114 1697 22.6 677
Table 1: Overview of the ACE 2005 data.
For the domain adaptation experiments we use
the ACE 2005 corpus. An overview of the data
is given in Table 1. Note that this data is dif-
ferent from ACE 2004: it covers different years
(ACE 2004: texts from 2001-2002; ACE 2005:
2003-2005). Moreover, the annotation guidelines
have changed (for example, ACE 2005 contains no
discourse relation, some relation (sub)types have
changed/moved, and care must be taken for differ-
ences in SGM markup, etc.).
More importantly, the ACE 2005 corpus cov-
ers additional domains: weblogs, telephone con-
versation, usenet and broadcast conversation. In
the experiments, we use news (the union of nw
and bn) as source domain, and weblogs (wl), tele-
phone conversations (cts) and broadcast conversa-
tion (bc) as target domains.7 We take half of bc
as only target development set, and leave the re-
maining data and domains for final testing (since
they are already small, cf. Table 1). To get a feel-
ing of how these domains differ, Figure 3 depicts
the distribution of relations in each domain and Ta-
ble 2 provides the most frequent out-of-vocabulary
words together with their percentage.
Lexical Similarity and Clustering We applied
LSA to ukWaC (Baroni et al, 2009), a 2 billion
word corpus constructed from the Web8 using the
s-space toolkit.9 Dimensionality reduction was
performed using SVD with 250 dimensions, fol-
lowing (Croce et al, 2011). The co-occurrence
matrix was transformed by tfidf. For the Brown
word clusters, we used Percy Liang?s implemen-
tation10 of the Brown clustering algorithm (Liang,
2005). We incorporate cluster information by us-
7We did not consider the usenet subpart, since it is among
the smaller domains and data-preprocessing was difficult.
8http://wacky.sslmit.unibo.it/
9http://code.google.com/p/airhead-research/
10https://github.com/percyliang/brown-cluster
1502
nw_bn bc cts wl
ARTGEN?AFFORG?AFFPART?WHOLEPER?SOCPHYS
Distribution of relations across domains (normalized)
Domain
Pro
por
tion
0.0
0.1
0.2
0.3
0.4
Figure 3: Distribution of relations in ACE 2005.
Dom Most frequent OOV words
bc
(24%)
insurance, unintelligible, malprac-
tice, ph, clip, colonel, crosstalk
cts
(34%)
uh, Yeah, um, eh, mhm, uh-huh, ?,
ah, mm, th, plo, topic, y, workplace
wl
(49%)
title, Starbucks, Well, blog, !!,
werkheiser, undefeated, poor, shit
Table 2: For each domain the percentage of target
domain words (types) that are unseen in the source
together with the most frequent OOV words.
ing the 10-bit cluster prefix (Sun et al, 2011; Chan
and Roth, 2010). For the domain adaptation exper-
iments, we use ukWaC corpus-induced clusters as
bridge between domains. We limited the vocabu-
lary to that in ACE 2005, which are approximately
16k words. Following previous work, we left case
intact in the corpus and induced 1,000 word clus-
ters from words appearing at least 100 times.11
DA baseline We compare our approach to in-
stance weighting (Jiang and Zhai, 2007). We mod-
ified SVM-light-TK such that it takes a parameter
vector ?i, .., ?m as input, where each ?i represents
the relative importance of example i with respect
to the target domain (Huang et al, 2007; Wid-
mer, 2008). To estimate the importance weights,
we train a binary classifier that distinguishes be-
tween source and target domain instances. We
consider the union of the three target domains as
target data. To train the classifier, the source in-
stances are marked as negative and the target in-
stances are marked as positive. Then, this classi-
11Clusters are available at http://disi.unitn.it/ikernels/
RelationExtraction
Prior Work: Type P R F1
Zhang (2006), tree only K,yes 74.1 62.4 67.7
Zhang (2006), linear K,yes 73.5 67.0 70.1
Zhang (2006), poly K,yes 76.1 68.4 72.1
Sun & Grishman (2011) F,yes 73.4 67.7 70.4
Jiang & Zhai (2007) F,no 73.4 70.2 71.3
Our re-implementation: Type P R F1
Tree only (PET Zhang) K,yes 70.7 62.5 66.3
Linear composite K,yes 71.3 66.6 68.9
Polynomial composite K,yes 72.6 67.7 70.1
Table 3: Comparison to previous work on the 7 re-
lations of ACE 2004. K: kernel-based; F: feature-
based; yes/no: models argument order explicitly.
fier is applied to the source data. To obtain the
weights ?i, we convert the SVM scores into pos-
terior probabilities by training a sigmoid using the
modified Platt algorithm (Lin et al, 2007).12
6 Results
6.1 Alignment to Prior Work
Although most prior studies performed 5-fold
cross-validation on ACE 2004, it is often not clear
whether the partitioning has been done on the in-
stance or on the document level. Moreover, it is
often not stated whether argument order is mod-
eled explicitly, making it difficult to compare sys-
tem performance. Citing Wang (2008), ?We feel
that there is a sense of increasing confusion down
this line of research?. To ease comparison for fu-
ture research we use the same 5-fold split on the
document level as Sun et al (2011)13 and make
our system publicly available (see Section 5).
Table 3 shows that our system (bottom) aligns
well with the state of the art. Our best sys-
tem (composite kernel with polynomial expan-
sion) reaches an F1 of 70.1, which aligns well to
the 70.4 of Sun et al (2011) that use the same data-
split. This is slightly behind that of Zhang (2006);
the reason might be threefold: i) different data par-
titioning; ii) different pre-processing; iii) they in-
corporate features from additional sources, i.e. a
phrase chunker, dependency parser and semantic
resources (Zhou et al, 2005) (we have on aver-
age 9 features/instance, they use 40). Since we
focus on evaluating the impact of semantic simi-
larity in tree kernels, we think our system is very
competitive. Removing gold entity and mention
12Other weightings/normalizations (like LDA) didn?t im-
prove the results; best was to take the posteriors and add c.
13http://cs.nyu.edu/?asun/pub/ACL11_CVFileList.txt
1503
information results in a significant F1 drop from
66.3% to 54.2%. However, in a realistic setting
we do not have gold entity info available, espe-
cially not in the case when we apply the system
to any kind of text. Thus, in the domain adapta-
tion setup we assume entity boundaries given but
not their label. Clearly, evaluating the approach on
predicted mentions, e.g. Giuliano et al (2007), is
another important dimension, however, out of the
scope of the current paper.
6.2 Tree Kernels with Brown Word Clusters
To evaluate the effectiveness of Brown word clus-
ters in tree kernels, we evaluated different instance
representations (cf. Figure 2) on the ACE 2005 de-
velopment set. Table 4 shows the results.
bc-dev P R F1
baseline 52.2 41.7 46.4
replace word 49.7 38.6 43.4
replace pos 56.3 41.9 48.0
replace pos only mentions 55.3 41.6 47.5
above word 54.5 42.2 47.6
above pos 55.8 41.1 47.3
Table 4: Brown clusters in tree kernels (cf. Fig 2).
To summarize, we found: i) it is generally a bad
idea to dismiss lexical information completely,
i.e. replacing or ignoring terminals harms perfor-
mance; ii) the best way to incorporate Brown clus-
ters is to replace the Pos tag with the cluster bit-
string; iii) marking all words is generally better
than only mentions; this is in contrast to Sun et
al. (2011) who found that in their feature-based
system it was better to add cluster information
to entity mentions only. As we will discuss, the
combination of syntax and semantics exploited in
this novel kernel avoids the necessity of restricting
cluster information to mentions only.
6.3 Semantic Tree Kernels for DA
To evaluate the effectiveness of the proposed ker-
nels across domains, we use the ACE 2005 data
as testbed. Following standard practices on ACE
2004, the newswire (nw) and broadcast news (bn)
data from ACE 2005 are considered training data
(labeled source domain). The test data consists
of three targets: broadcast conversation, telephone
conversation, weblogs. As we want to build a sin-
gle system that is able to handle heterogeneous
data, we do not assume that there is further unla-
beled domain-specific data, but we assume to have
a large unlabeled corpus (ukWaC) at our disposal
to improve the generalizability of our models.
Table 5 presents the results. In the first three
rows we see the performance of the baseline
models (PET, BOW and BOW without mark-
ing). In-domain (col 1): when evaluated on the
same domain the system was trained on (nw+bn,
5-fold cross-validation). Out-of-domain perfor-
mance (cols 2-4): the system evaluated on the
targets, namely broadcast conversation (bc), tele-
phone conversation (cts) and weblogs (wl). While
the system achieves a performance of 46.0 F1
within its own domain, the performance drops to
45.3, 43.4 and 34.0 F1 on the target domains, re-
spectively. The BOW kernel that disregards syn-
tax is often less effective (row 2). We see also
the effect of target entity marking: the BOW ker-
nel without entity marking performs substantially
worse (row 3). For the remaining experiments we
use the BOW kernel with entity marking.
Rows 4 and 5 of Table 5 show the effect of
using instance weighting for the PET baseline.
Two models are shown: they differ in whether
PET or BOW was used as instance representa-
tion for training the discriminative classifier. In-
stance weighting shows mixed results: it helps
slightly on the weblogs domain, but does not help
on broadcast conversation and telephone conversa-
tions. Interestingly, the two models used to obtain
the weights perform similarly, despite the fact that
their performance differs (F1: 70.5 BOW, 73.5
PET); it turns out that the correlation between the
weights is high (+0.82).
The next part (rows 6-9) shows the effect of en-
riching the syntactic structures with either Brown
word clusters or LSA. The Brown cluster ker-
nel applied to PET (P WC) improves performance
over the baseline over all target domains. The
same holds also for the lexical semantic kernel
based on LSA (P LSA), however, to only two out
of three domains. This suggests that the two ker-
nels capture different information and a combined
kernel might be effective. More importantly, the
table shows the effect of adding Brown clusters or
LSA semantics to the BOW kernel: it can actually
hurt performance, sometimes to a small but other
times to a considerably degree. For instance, WC
applied to PET achieves an F1 of 47.0 (baseline:
45.3) on the bc domain, while applied to BOW it
hurts performance significantly, i.e. it drops from
1504
nw+bn (in-dom.) bc cts wl
Baseline: P: R: F1: P: R: F1: P: R: F1: P: R: F1:
PET 50.6 42.1 46.0 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.0
BOW 55.1 37.3 44.5 57.2 37.1 45.0 57.5 31.8 41.0 41.1 27.2 32.7
BOW no marking 49.6 34.6 40.7 51.5 34.7 41.4 54.6 30.7 39.3 37.6 25.7 30.6
PET adapted: P: R: F: P: R: F: P: R: F: P: R: F:
IW1 (using PET) 51.4 44.1 47.4 49.1 41.1 44.7 50.8 37.5 43.1 35.5 33.9 34.7
IW2 (using BOW) 51.2 43.6 47.1 49.1 41.3 44.9 51.2 37.8 43.5 35.6 33.8 34.7
With Similarity: P: R: F1: P: R: F1: P: R: F1: P: R: F1:
P WC 55.4 44.6 49.4 54.3 41.4 47.0 55.9 37.1 44.6 40.0 32.7 36.0
B WC 47.9 36.4 41.4 49.5 35.2 41.2 53.3 33.2 40.9 31.7 24.1 27.4
P LSA 52.3 44.1 47.9 51.4 41.7 46.0 49.7 36.5 42.1 38.1 36.5 37.3
B LSA 53.7 37.8 44.4 55.1 33.8 41.9 54.9 32.3 40.7 39.2 28.6 33.0
P+P WC 55.0 46.5 50.4 54.4 43.4 48.3 54.1 38.1 44.7 38.4 34.5 36.3
P+P LSA 52.7 46.6 49.5 53.9 45.2 49.2 49.9 37.6 42.9 37.9 38.3 38.1
P+P WC+P LSA 55.1 45.9 50.1 55.3 43.1 48.5? 53.1 37.0 43.6 39.9 35.8 37.8?
Table 5: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005.
PET and BOW are abbreviated by P and B, respectively. If not specified BOW is marked.
45.0 to 41.2. This is also the case for LSA ap-
plied to the BOW kernel, which drops to 41.9. On
the cts domain this is less pronounced. Only on
the weblogs domain B LSA achieves a minor im-
provement (from 32.7 to 33.0). In general, dis-
tributional semantics constrained by syntax (i.e.
combined with PET) can be effectively exploited,
while if applied ?blindly? ? without the guide of
syntax (i.e. BOW) ? performance might drop, of-
ten considerably. We believe that the semantic in-
formation does not help the BOW kernel as there is
no syntactic information that constrains the appli-
cation of the noisy source, as opposed to the case
with the PET kernel.
As the two semantically enriched kernels,
PET LSA and PET WC, seem to capture different
information we use composite kernels (rows 10-
11): the baseline kernel (PET) summed with the
lexical semantic kernels. As we can see, results
improve further: for instance on the bc test set,
PET WC reaches an F1 of 47.0, while combined
with PET (PET+PET WC) this improves to 48.3.
Adding also PET LSA results in the best perfor-
mance and our final system (last row): the com-
posite kernel (PET+PET WC+PET LSA) reaches
an F1 of 48.5, 43.6 and 37.8 on the target domains,
respectively, i.e. with an absolute improvement of:
+3.2%, +0.2% and +3.8%, respectively. Two out
of three improvements are significant at p < 0.05
(indicated by ? in Table 5). Moreover, the system
also improved in its own domain (first column),
therefore having achieved robustness.
By performing an error analysis we found that,
for instance, the Brown clusters help to general-
ize locations and professions. For example, the
baseline incorrectly considered ?Dutch filmmaker?
in a PART-WHOLE relation, while our system
correctly predicted GEN-AFF(filmmaker,Dutch).
?Filmmaker? does not appear in the source, how-
ever ?Dutch citizen? does. Both ?citizen? and ?film-
maker? appear in the same cluster, thereby helping
the system to recover the correct relation.
bc cts wl
Relation: BL SYS BL SYS BL SYS
PART-WHOLE 37.8 43.1 59.3 52.3 30.5 36.3
ORG-AFF 60.7 62.9 35.5 42.3 41.0 42.0
PHYS 35.3 37.6 25.4 28.7 25.2 26.9
ART 20.8 37.9 34.5 43.5 26.5 40.3
GEN-AFF 30.1 33.0 16.8 18.6 21.6 28.1
PER-SOC 74.1 74.2 66.3 63.1 42.6 48.0
? average 45.3 48.5 43.4 43.6 34.0 37.8
Table 6: F1 per coarse relation type (ACE
2005). SYS is the final model, i.e. last row
(PET+PET WC+PET LSA) of Table 5.
Furthermore, Table 6 provides the performance
breakdown per relation for the baseline (BL) and
our best system (SYS). The table shows that our
system is able to improve F1 on all relations for
the broadcast and weblogs data. On most rela-
tions, this is also the case for the telephone (cts)
data, although the overall improvement is not sig-
nificant. Most errors were made on the PER-SOC
1505
relation, which constitutes the largest portion of
cts (cf. Figure 3). As shown in the same figure,
the relation distribution of the cts domain is also
rather different from the source. This conversation
data is a very hard domain, with a lot of disflu-
encies and spoken language patterns. We believe
it is more distant from the other domains, espe-
cially from the unlabeled collection, thus other ap-
proaches might be more appropriate, e.g. domain
identification (Dredze et al, 2010).
7 Conclusions and Future Work
We proposed syntactic tree kernels enriched by
lexical semantic similarity to tackle the portabil-
ity of a relation extractor to different domains.
The results of diverse kernels exploiting (i) Brown
clustering and (ii) LSA show that a suitable com-
bination of syntax and lexical generalization is
very promising for domain adaptation. The pro-
posed system is able to improve performance sig-
nificantly on two out of three target domains (up
to 8% relative improvement). We compared it to
instance weighting, which gave only modest or
no improvements. Brown clusters remained un-
explored for kernel-based approaches. We saw
that adding cluster information blindly might ac-
tually hurt performance. In contrast, adding lex-
ical information combined with syntax can help
to improve performance: the syntactic structure
enriched with lexical information provides a fea-
ture space where syntax constrains lexical similar-
ity obtained from unlabeled data. Thus, seman-
tic syntactic tree kernels appear to be a suitable
mechanism to adequately trade off the two kinds
of information. In future we plan to extend the
evaluation to predicted mentions, which necessar-
ily includes a careful evaluation of pre-processing
components, as well as evaluating the approach on
other semantic tasks.
Acknowledgments
We would like to thank Min Zhang for discus-
sions on his prior work as well as the anony-
mous reviewers for their valuable feedback. The
research described in this paper has been sup-
ported by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
the grant #288024: LIMOSINE ? Linguistically
Motivated Semantic aggregation engiNes.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, pages 209?226.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Corre-
spondence Learning. In Conference on Empirical
Methods in Natural Language Processing, Sydney,
Australia.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In ECIR, pages 307?318.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Exploiting Structure and Semantics for Expressive
Text Kernels. In Conference on Information Knowl-
edge and Management, Lisbon, Portugal.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-Based n-gram Models of Natural Language.
Computational Linguistics, 18:467?479.
Razvan C. Bunescu. 2007. Learning to extract rela-
tions from the web using minimal supervision. In
Proceedings of ACL.
Yee Seng Chan and Dan Roth. 2010. Exploiting
background knowledge for relation extraction. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
152?160, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion Kernels for Natural Language. In Proceedings
of Neural Information Processing Systems (NIPS
2001).
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Semantic convolution kernels over
dependency trees: smoothed partial tree kernel. In
CIKM, pages 2013?2016.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting on ACL, Barcelona,
Spain.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
ACL, pages 256?263, Prague, Czech Republic, June.
Mark Dredze, Tim Oates, and Christine Piatko. 2010.
We?re not in kansas anymore: Detecting domain
changes in streams. In Proceedings of EMNLP,
pages 585?595, Cambridge, MA.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2007. Relation extraction and the influence
of automatic named-entity recognition. ACM Trans.
Speech Lang. Process., 5(1):2:1?2:26, December.
1506
G. Golub and W. Kahan. 1965. Calculating the singu-
lar values and pseudo-inverse of a matrix. Journal of
the Society for Industrial and Applied Mathematics:
Series B, Numerical Analysis, 2(2):pp. 205?224.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics. Oxford University Press.
Jiayuan Huang, Arthur Gretton, Bernhard Scho?lkopf,
Alexander J. Smola, and Karsten M. Borgwardt.
2007. Correcting sample selection bias by unlabeled
data. In In NIPS. MIT Press.
Jing Jiang and Chengxiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In In ACL
2007, pages 264?271.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th IJCNLP, pages
1012?1020, Suntec, Singapore.
Percy Liang. 2005. Semi-Supervised Learning for
Natural Language. Master?s thesis, Massachusetts
Institute of Technology.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007. A note on platt?s probabilistic outputs for
support vector machines. Mach. Learn., 68(3):267?
276.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, pages 419?444.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of
ACL-IJCNLP, pages 1003?1011, Suntec, Singapore,
August.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Meeting of the ACL, Barcelona,
Spain.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th ECML, Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM, pages 253?262.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In Proceedings of EMNLP
?09, pages 1378?1387, Stroudsburg, PA, USA.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL, pages 915?932.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. Notes of
the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Anders S?gaard and Martin Haulrich. 2011.
Sentence-level instance-weighting for graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies, IWPT ?11, pages 43?47,
Stroudsburg, PA, USA.
Anders S?gaard and Anders Johannsen. 2012. Robust
learning in random subspaces: equipping NLP for
OOV effects. In Proceedings of Coling.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proceedings of ACL-HLT, pages
521?529, Portland, Oregon, USA.
Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In Pro-
ceedings of the 3rd International Joint Conference
on Natural Language Processing-IJCNLP.
Christian Widmer. 2008. Domain adaptation in
sequence analysis. Diplomarbeit, University of
Tu?bingen.
Feiyu Xu, Hans Uszkoreit, Hond Li, and Niko Felger.
2008. Adaptation of relation extraction rules to new
domains. In Proceedings of LREC?08, Marrakech,
Morocco.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations
between named entities from a large raw corpus us-
ing tree similarity-based clustering. In Proceedings
of IJCNLP?2005, pages 378?389, Jeju Island, South
Korea.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of COLING-ACL 2006, pages 825?
832.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
of ACL), pages 427?434, Ann Arbor, Michigan.
1507
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 714?718,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Semantic Textual Similarity with Structural Representations
Aliaksei Severyn(1) and Massimo Nicosia(1) and Alessandro Moschitti1,2
(1)DISI, University of Trento, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
(2)QCRI, Qatar Foundation, Doha, Qatar
amoschitti@qf.org.qa
Abstract
Measuring semantic textual similarity
(STS) is at the cornerstone of many NLP
applications. Different from the major-
ity of approaches, where a large number
of pairwise similarity features are used to
represent a text pair, our model features
the following: (i) it directly encodes input
texts into relational syntactic structures;
(ii) relies on tree kernels to handle feature
engineering automatically; (iii) combines
both structural and feature vector repre-
sentations in a single scoring model, i.e.,
in Support Vector Regression (SVR); and
(iv) delivers significant improvement over
the best STS systems.
1 Introduction
In STS the goal is to learn a scoring model that
given a pair of two short texts returns a similar-
ity score that correlates with human judgement.
Hence, the key aspect of having an accurate STS
framework is the design of features that can ade-
quately represent various aspects of the similarity
between texts, e.g., using lexical, syntactic and se-
mantic similarity metrics.
The majority of approaches treat input text pairs
as feature vectors where each feature is a score
corresponding to a certain type of similarity. This
approach is conceptually easy to implement and
the STS shared task at SemEval 2012 (Agirre et
al., 2012) (STS-2012) has shown that the best sys-
tems were built following this idea, i.e., a num-
ber of features encoding similarity of an input text
pair were combined in a single scoring model, e.g.,
SVR. Nevertheless, one limitation of using only
similarity features to represent a text pair is that of
low representation power.
The novelty of our approach is that we treat the
input text pairs as structural objects and rely on the
power of kernel learning to extract relevant struc-
tures. To link the documents in a pair we mark the
nodes in the related structures with a special rela-
tional tag. This way effective structural relational
patterns are implicitly encoded in the trees and
can be automatically learned by the kernel-based
machines. We combine our relational structural
model with the features from two best systems of
STS-2012. Finally, we use the approach of classi-
fier stacking to combine several structural models
into the feature vector representation.
The contribution of this paper is as follows: (i) it
provides a convincing evidence that adding struc-
tural features automatically extracted by structural
kernels yields a significant improvement in accu-
racy; (ii) we define a combination kernel that inte-
grates both structural and feature vector represen-
tations within a single scoring model, e.g., Sup-
port Vector Regression; (iii) we provide a sim-
ple way to construct relational structural models
that can be built using off-the-shelf NLP tools;
(iv) we experiment with four structural representa-
tions and show that constituency and dependency
trees represent the best source for learning struc-
tural relationships; and (v) using a classifier stack-
ing approach, structural models can be easily com-
bined and integrated into existing feature-based
STS models.
2 Structural Relational Similarity
The approach of relating pairs of input struc-
tures by learning predictable syntactic transforma-
tions has shown to deliver state-of-the-art results
in question answering, recognizing textual entail-
ment, and paraphrase detection, e.g. (Wang et al,
2007; Wang and Manning, 2010; Heilman and
Smith, 2010). Previous work relied on fairly com-
plex approaches, e.g. applying quasi-synchronous
grammar formalism and variations of tree edit dis-
tance alignments, to extract syntactic patterns re-
lating pairs of input structures. Our approach
is conceptually simpler, as it regards the prob-
lem within the kernel learning framework, where
we first encode salient syntactic/semantic proper-
714
ties of the input text pairs into tree structures and
rely on tree kernels to automatically generate rich
feature spaces. This work extends in several di-
rections our earlier work in question answering,
e.g., (Moschitti et al, 2007; Moschitti and Quar-
teroni, 2008), in textual entailment recognition,
e.g., (Moschitti and Zanzotto, 2007), and more in
general in relational text categorization (Moschitti,
2008; Severyn and Moschitti, 2012).
In this section we describe: (i) a kernel frame-
work to combine structural and vector models; (ii)
structural kernels to handle feature engineering;
and (iii) suitable structural representations for re-
lational learning.
2.1 Structural Kernel Learning
In supervised learning, given labeled data
{(xi, y i)}ni=1, the goal is to estimate a decision
function h(x) = y that maps input examples to
their targets. A conventional approach is to rep-
resent a pair of texts as a set of similarity fea-
tures {fi}, s.t. the predictions are computed as
h(x) = w ? x = ?iwifi, where w is the model
weight vector. Hence, the learning problem boils
down to estimating individual weights of each of
the similarity features fi. One downside of such
approach is that a great deal of similarity infor-
mation encoded in a given text pair is lost when
modeled by single real-valued scores.
A more versatile approach in terms of the input
representation relies on kernels. In a typical kernel
learning approach, e.g., SVM, the prediction func-
tion for a test input x takes on the following form
h(x) =
?
i ?iyiK(x,xi), where ?i are the model
parameters estimated from the training data, yi are
target variables, xi are support vectors, andK(?, ?)
is a kernel function.
To encode both structural representation and
similarity feature vectors of a given text pair in a
single model we define each document in a pair
to be composed of a tree and a vector: ?t, v?.
To compute a kernel between two text pairs xi
and xj we define the following all-vs-all kernel,
where all possible combinations of components,
x(1) and x(2), from each text pair are consid-
ered: K(xi,xj) = K(x(1)i ,x(1)j )+K(x(1)i ,x(2)j )+
K(x(2)i ,x
(1)
j ) + K(x
(2)
i ,x
(2)
j ). Each of the ker-
nel computations K can be broken down into
the following: K(x(1),x(2)) = KTK(t(1), t(2)) +
Kfvec(v(1), v(2)), where KTK computes a struc-
tural kernel and Kfvec is a kernel over feature vec-
tors, e.g., linear, polynomial or RBF, etc. Further
in the text we refer to structural tree kernel models
as TK and explicit feature vector representation as
fvec.
Having defined a way to jointly model text pairs
using structural TK representations along with the
similarity features fvec, we next briefly review
tree kernels and our relational structures.
2.2 Tree Kernels
We use tree structures as our base representation
since they provide sufficient flexibility in repre-
sentation and allow for easier feature extraction
than, for example, graph structures. Hence, we
rely on tree kernels to compute KTK(?, ?). Given
two trees it evaluates the number of substructures
(or fragments) they have in common, i.e., it is a
measure of their overlap. Different TK functions
are characterized by alternative fragment defini-
tions. In particular, we focus on the Syntactic Tree
kernel (STK) (Collins and Duffy, 2002) and a Par-
tial Tree Kernel (PTK) (Moschitti, 2006).
STK generates all possible substructures rooted in
each node of the tree with the constraint that pro-
duction rules can not be broken (i.e., any node in a
tree fragment must include either all or none of its
children).
PTK can be more effectively applied to both con-
stituency and dependency parse trees. It general-
izes STK as the fragments it generates can contain
any subset of nodes, i.e., PTK allows for breaking
the production rules and generating an extremely
rich feature space, which results in higher gener-
alization ability.
2.3 Structural representations
In this paper, we define simple-to-build relational
structures based on: (i) a shallow syntactic tree,
(ii) constituency, (iii) dependency and (iv) phrase-
dependency trees.
Shallow tree is a two-level syntactic hierarchy
built from word lemmas (leaves), part-of-speech
tags (preterminals) that are further organized into
chunks. It was shown to significantly outperform
feature vector baselines for modeling relationships
between question answer pairs (Severyn and Mos-
chitti, 2012).
Constituency tree. While shallow syntactic pars-
ing is very fast, here we consider using con-
stituency structures as a potentially richer source
of syntactic/semantic information.
Dependency tree. We propose to use depen-
dency relations between words to derive an alter-
native structural representation. In particular, de-
715
Figure 1: A phrase dependency-based structural representation of a text pair (s1, s2): A woman with
a knife is slicing a pepper (s1) vs. A women slicing green pepper (s2) with a high semantic similarity
(human judgement score 4.0 out of 5.0). Related tree fragments are linked with a REL tag.
pendency relations are used to link words in a way
that they are always at the leaf level. This reorder-
ing of the nodes helps to avoid the situation where
nodes with words tend to form long chains. This
is essential for PTK to extract meaningful frag-
ments. We also plug part-of-speech tags between
the word nodes and nodes carrying their grammat-
ical role.
Phrase-dependency tree. We explore a phrase-
dependency tree similar to the one defined in (Wu
et al, 2009). It represents an alternative struc-
ture derived from the dependency tree, where the
dependency relations between words belonging to
the same phrase (chunk) are collapsed in a unified
node. Different from (Wu et al, 2009), the col-
lapsed nodes are stored as a shallow subtree rooted
at the unified node. This node organization is par-
ticularly suitable for PTK that effectively runs a
sequence kernel on the tree fragments inside each
chunk subtree. Fig 1 gives an example of our vari-
ation of a phrase dependency tree.
As a final consideration, if a document contains
multiple sentences they are merged in a single tree
with a common root. To encode the structural
relationships between documents in a pair a spe-
cial REL tag is used to link the related structures.
We adopt a simple strategy to establish such links:
words from two documents that have a common
lemma get their parents (POS tags) and grandpar-
ents, non-terminals, marked with a REL tag.
3 Pairwise similarity features.
Along with the direct representation of input text
pairs as structural objects our framework is also
capable of encoding pairwise similarity feature
vectors (fvec), which we describe below.
Baseline features. (base) We adopt similar-
ity features from two best performing systems
of STS-2012, which were publicly released1:
namely, the Takelab2 system (S?aric? et al, 2012)
and the UKP Lab?s system3 (Bar et al, 2012).
Both systems represent input texts with similarity
features combining multiple text similarity mea-
sures of varying complexity.
UKP (U) provides metrics based on match-
ing of character, word n-grams and common
subsequences. It also includes features derived
from Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) and aggregation of word sim-
ilarity based on lexical-semantic resources, e.g.,
WordNet. In total it provides 18 features.
Takelab (T) includes n-gram matching of vary-
ing size, weighted word matching, length differ-
ence, WordNet similarity and vector space simi-
larity where pairs of input sentences are mapped
into Latent Semantic Analysis (LSA) space. The
features are computed over several sentence rep-
resentations where stop words are removed and/or
lemmas are used in place of raw tokens. The total
number of Takelab?s features is 21. The combined
system consists of 39 features.
Additional features. We also augment the U and
T feature sets, with an additional set of features (A)
which includes: a cosine similarity scores com-
puted over (i) n-grams of part-of-speech tags (up
to 4-grams), (ii) SuperSense tags (Ciaramita and
1Note that only a subset of the features used in the fi-
nal evaluation was released, which results in lower accuracy
when compared to the official rankings.
2http://takelab.fer.hr/sts/
3https://code.google.com/p/dkpro-similarity-
asl/wiki/SemEval2013
716
Altun, 2006), (iii) named entities, (iv) dependency
triplets, and (v) PTK syntactic similarity scores
computed between documents in a pair, where as
input representations we use raw dependency and
constituency trees. To alleviate the problem of do-
main adaptation, where datasets used for training
and testing are drawn from different sources, we
include additional features to represent the com-
bined text of a pair: (i) bags (B) of lemmas, de-
pendency triplets, production rules (from the con-
stituency parse tree) and a normalized length of
the entire pair; and (ii) a manually encoded cor-
pus type (M), where we use a binary feature with
a non-zero entry corresponding to a dataset type.
This helps the learning algorithm to learn implic-
itly the individual properties of each dataset.
Stacking. To integrate multiple TK representa-
tions into a single model we apply a classifier
stacking approach (Fast and Jensen, 2008). Each
of the learned TK models is used to generate pre-
dictions which are then plugged as features into
the final fvec representation, s.t. the final model
uses only explicit feature vector representation. To
obtain prediction scores, we apply 5-fold cross-
validation scheme, s.t. for each of the held-out
folds we obtain independent predictions.
4 Experiments
We present the results of our model tested on the
data from the Core STS task at SemEval 2012.
4.1 Setup
Data. To compare with the best systems of the
STS-2012 we followed the same setup used in
the final evaluation, where 3 datasets (MSRpar,
MSRvid and SMTeuroparl) are used for training
and 5 for testing (two ?surprise? datasets were
added: OnWN and SMTnews). We use the entire
training data to obtain a single model for making
predictions on each test set.
Software. To encode TK models along with the
similarity feature vectors into a single regression
scoring model, we use an SVR framework imple-
mented in SVM-Light-TK4. We use the follow-
ing parameter settings -t 5 -F 1 -W A -C
+, which specifies a combination of trees and fea-
ture vectors (-C +), STK over trees (-F 1) (-F
3 for PTK) computed in all-vs-all mode (-W A)
and polynomial kernel of degree 3 for the feature
vector (active by default).
4http://disi.unitn.it/moschitti/Tree-Kernel.htm
Metrics. We report the following metrics em-
ployed in the final evaluation: Pearson correlation
for individual test sets5 and Mean ? an average
score weighted by the test set size.
4.2 Results
Table 1 summarizes the results of combining TK
models with a strong feature vector model. We
test structures defined in Sec. 2.3 when using STK
and PTK. The results show that: (i) combining
all three features sets (U, T, A) provides a strong
baseline system that we attempt to further improve
with our relational structures; (ii) the generality of
PTK provides an advantage over STK for learn-
ing more versatile models; (iii) constituency and
dependency representations seem to perform bet-
ter than shallow and phrase-dependency trees; (iv)
using structures with no relational linking does not
work; (v) TK models provide a far superior source
of structural similarity than U + T + A that already
includes PTK similarity scores as features, and fi-
nally (vi) the domain adaptation problem can be
addressed by including corpus specific features,
which leads to a large improvement over the pre-
vious best system.
5 Conclusions and Future Work
We have presented an approach where text pairs
are directly treated as structural objects. This pro-
vides a much richer representation for the learning
algorithm to extract useful syntactic and shallow
semantic patterns. We have provided an exten-
sive experimental study of four different structural
representations, e.g. shallow, constituency, de-
pendency and phrase-dependency trees using STK
and PTK. The novelty of our approach is that it
goes beyond a simple combination of tree kernels
with feature vectors as: (i) it directly encodes input
text pairs into relationally linked structures; (ii) the
learned structural models are used to obtain pre-
diction scores thus making it easy to plug into ex-
isting feature-based models, e.g. via stacking; (iii)
to our knowledge, this work is the first to apply
structural kernels and combinations in a regres-
sion setting; and (iv) our model achieves the state
of the art in STS largely improving the best pre-
vious systems. Our structural learning approach
to STS is conceptually simple and does not re-
quire additional linguistic sources other than off-
the-shelf syntactic parsers. It is particularly suit-
able for NLP tasks where the input domain comes
5we also report the results for a concatenation of all five
test sets (ALL)
717
Experiment U T A S C D P STK PTK B M ALL Mean MSRp MSRv SMTe OnWN SMTn
fvec
model
? .7060 .6087 .6080 .8390 .2540 .6820 .4470
? .7589 .6863 .6814 .8637 .4950 .7091 .5395
? ? .8079 .7161 .7134 .8837 .5519 .7343 .5607
? ? ? .8187 .7137 .7157 .8833 .5131 .7355 .5809
TK
models
with STK
and PTK
? ? ? ? ? .8261 .6982 .7026 .8870 .4807 .7258 .5333
? ? ? ? ? .8326 .6970 .7020 .8925 .4826 .7190 .5253
? ? ? ? ? .8341 .7024 .7086 .8921 .4671 .7319 .5495
? ? ? ? ? .8211 .6693 .6994 .8903 .2980 .7035 .5603
? ? ? ? ? .8362 .7026 .6927 .8896 .5282 .7144 .5485
? ? ? ? ? .8458 .7047 .6935 .8953 .5080 .7101 .5834
? ? ? ? ? .8468 .6954 .6717 .8902 .4652 .7089 .6133
? ? ? ? ? .8326 .6693 .7108 .8879 .4922 .7215 .5156
REL tag ? ? ? ? .8218 .6899 .6644 .8726 .4846 .7228 .5684? ? ? ? .8250 .7000 .6806 .8822 .5171 .7145 .5769
domain
adaptation
? ? ? ? ? .8539 .7132 .6993 .9005 .4772 .7189 .6481
? ? ? ? ? .8529 .7249 .7080 .8984 .5142 .7263 .6700
? ? ? ? ? ? .8546 .7156 .6989 .8979 .4884 .7181 .6609
? ? ? ? ? ? .8810 .7416 .7210 .8971 .5912 .7328 .6778
UKP (best system of STS-2012) .8239 .6773 .6830 .8739 .5280 .6641 .4937
Table 1: Results on STS-2012. First set of experiments studies the combination of fvec models from
UKP (U), Takelab (T) and (A). Next we show results for four structural representations: shallow (S),
constituency (C), dependency (D) and phrase-dependency (P) trees with STK and PTK; next row set
demonstrates the necessity of relational linking for two best structures, i.e. C and D (empty circle denotes
a structures with no relational linking.); finally, domain adaptation via bags of features (B) of the entire
pair and (M) manually encoded dataset type show the state of the art results.
as pairs of objects, e.g., question answering, para-
phrasing and recognizing textual entailment.
6 Acknowledgements
This research is supported by the EU?s Seventh
Framework Program (FP7/2007-2013) under the
#288024 LIMOSINE project.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre. 2012. Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In *SEM.
Daniel Bar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In SemEval.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
EMNLP.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
ACL.
Andrew S. Fast and David Jensen. 2008. Why stacked
models perform effective collective classification.
In ICDM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In NAACL.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In ACL.
Alessandro Moschitti and Fabio Massimo Zanzotto.
2007. Fast and effective kernels for relational learn-
ing from texts. In ICML.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In SemEval.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In ACL.
Mengqiu Wang, Noah A. Smith, and Teruko Mitaura.
2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP.
718
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1252?1261,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Opinion Mining on YouTube
Aliaksei Severyn
1
, Alessandro Moschitti
3,1
,
Olga Uryupina
1
, Barbara Plank
2
, Katja Filippova
4
1
DISI - University of Trento,
2
CLT - University of Copenhagen,
3
Qatar Computing Research Institute,
4
Google Inc.
severyn@disi.unitn.it, amoschitti@qf.org.qa,
uryupina@gmail.com, bplank@cst.dk, katjaf@google.com
Abstract
This paper defines a systematic approach
to Opinion Mining (OM) on YouTube
comments by (i) modeling classifiers for
predicting the opinion polarity and the
type of comment and (ii) proposing ro-
bust shallow syntactic structures for im-
proving model adaptability. We rely on the
tree kernel technology to automatically ex-
tract and learn features with better gener-
alization power than bag-of-words. An ex-
tensive empirical evaluation on our manu-
ally annotated YouTube comments corpus
shows a high classification accuracy and
highlights the benefits of structural mod-
els in a cross-domain setting.
1 Introduction
Social media such as Twitter, Facebook or
YouTube contain rapidly changing information
generated by millions of users that can dramati-
cally affect the reputation of a person or an orga-
nization. This raises the importance of automatic
extraction of sentiments and opinions expressed in
social media.
YouTube is a unique environment, just like
Twitter, but probably even richer: multi-modal,
with a social graph, and discussions between peo-
ple sharing an interest. Hence, doing sentiment
research in such an environment is highly relevant
for the community. While the linguistic conven-
tions used on Twitter and YouTube indeed show
similarities (Baldwin et al, 2013), focusing on
YouTube allows to exploit context information,
possibly also multi-modal information, not avail-
able in isolated tweets, thus rendering it a valuable
resource for the future research.
Nevertheless, there is almost no work showing
effective OM on YouTube comments. To the best
of our knowledge, the only exception is given by
the classification system of YouTube comments
proposed by Siersdorfer et al (2010).
While previous state-of-the-art models for opin-
ion classification have been successfully applied
to traditional corpora (Pang and Lee, 2008),
YouTube comments pose additional challenges:
(i) polarity words can refer to either video or prod-
uct while expressing contrasting sentiments; (ii)
many comments are unrelated or contain spam;
and (iii) learning supervised models requires train-
ing data for each different YouTube domain, e.g.,
tablets, automobiles, etc. For example, consider a
typical comment on a YouTube review video about
a Motorola Xoom tablet:
this guy really puts a negative spin on
this , and I ?m not sure why , this seems
crazy fast , and I ?m not entirely sure
why his pinch to zoom his laggy all the
other xoom reviews
The comment contains a product name xoom and
some negative expressions, thus, a bag-of-words
model would derive a negative polarity for this
product. In contrast, the opinion towards the prod-
uct is neutral as the negative sentiment is ex-
pressed towards the video. Similarly, the follow-
ing comment:
iPad 2 is better. the superior apps just
destroy the xoom.
contains two positive and one negative word, yet
the sentiment towards the product is negative (the
negative word destroy refers to Xoom). Clearly,
the bag-of-words lacks the structural information
linking the sentiment with the target product.
In this paper, we carry out a systematic study on
OM targeting YouTube comments; its contribution
is three-fold: firstly, to solve the problems outlined
above, we define a classification schema, which
separates spam and not related comments from the
informative ones, which are, in turn, further cate-
gorized into video- or product-related comments
1252
(type classification). At the final stage, differ-
ent classifiers assign polarity (positive, negative or
neutral) to each type of a meaningful comment.
This allows us to filter out irrelevant comments,
providing accurate OM distinguishing comments
about the video and the target product.
The second contribution of the paper is the cre-
ation and annotation (by an expert coder) of a
comment corpus containing 35k manually labeled
comments for two product YouTube domains:
tablets and automobiles.
1
It is the first manu-
ally annotated corpus that enables researchers to
use supervised methods on YouTube for comment
classification and opinion analysis. The comments
from different product domains exhibit different
properties (cf. Sec. 5.2), which give the possibility
to study the domain adaptability of the supervised
models by training on one category and testing on
the other (and vice versa).
The third contribution of the paper is a novel
structural representation, based on shallow syn-
tactic trees enriched with conceptual information,
i.e., tags generalizing the specific topic of the
video, e.g., iPad, Kindle, Toyota Camry. Given the
complexity and the novelty of the task, we exploit
structural kernels to automatically engineer novel
features. In particular, we define an efficient tree
kernel derived from the Partial Tree Kernel, (Mos-
chitti, 2006a), suitable for encoding structural rep-
resentation of comments into Support Vector Ma-
chines (SVMs). Finally, our results show that our
models are adaptable, especially when the struc-
tural information is used. Structural models gen-
erally improve on both tasks ? polarity and type
classification ? yielding up to 30% of relative im-
provement, when little data is available. Hence,
the impractical task of annotating data for each
YouTube category can be mitigated by the use of
models that adapt better across domains.
2 Related work
Most prior work on more general OM has been
carried out on more standardized forms of text,
such as consumer reviews or newswire. The most
commonly used datasets include: the MPQA cor-
pus of news documents (Wilson et al, 2005), web
customer review data (Hu and Liu, 2004), Ama-
zon review data (Blitzer et al, 2007), the JDPA
1
The corpus and the annotation guidelines are pub-
licly available at: http://projects.disi.unitn.
it/iKernels/projects/sentube/
corpus of blogs (Kessler et al, 2010), etc. The
aforementioned corpora are, however, only par-
tially suitable for developing models on social
media, since the informal text poses additional
challenges for Information Extraction and Natu-
ral Language Processing. Similar to Twitter, most
YouTube comments are very short, the language
is informal with numerous accidental and deliber-
ate errors and grammatical inconsistencies, which
makes previous corpora less suitable to train mod-
els for OM on YouTube. A recent study focuses on
sentiment analysis for Twitter (Pak and Paroubek,
2010), however, their corpus was compiled auto-
matically by searching for emoticons expressing
positive and negative sentiment only.
Siersdorfer et al (2010) focus on exploiting user
ratings (counts of ?thumbs up/down? as flagged by
other users) of YouTube video comments to train
classifiers to predict the community acceptance of
new comments. Hence, their goal is different: pre-
dicting comment ratings, rather than predicting the
sentiment expressed in a YouTube comment or its
information content. Exploiting the information
from user ratings is a feature that we have not ex-
ploited thus far, but we believe that it is a valuable
feature to use in future work.
Most of the previous work on supervised senti-
ment analysis use feature vectors to encode doc-
uments. While a few successful attempts have
been made to use more involved linguistic anal-
ysis for opinion mining, such as dependency
trees with latent nodes (T?ackstr?om and McDonald,
2011) and syntactic parse trees with vectorized
nodes (Socher et al, 2011), recently, a comprehen-
sive study by Wang and Manning (2012) showed
that a simple model using bigrams and SVMs per-
forms on par with more complex models.
In contrast, we show that adding structural fea-
tures from syntactic trees is particularly useful for
the cross-domain setting. They help to build a sys-
tem that is more robust across domains. Therefore,
rather than trying to build a specialized system
for every new target domain, as it has been done
in most prior work on domain adaptation (Blitzer
et al, 2007; Daum?e, 2007), the domain adapta-
tion problem boils down to finding a more robust
system (S?gaard and Johannsen, 2012; Plank and
Moschitti, 2013). This is in line with recent ad-
vances in parsing the web (Petrov and McDonald,
2012), where participants where asked to build a
single system able to cope with different yet re-
1253
lated domains.
Our approach relies on robust syntactic struc-
tures to automatically generate patterns that adapt
better. These representations have been inspired
by the semantic models developed for Ques-
tion Answering (Moschitti, 2008; Severyn and
Moschitti, 2012; Severyn and Moschitti, 2013)
and Semantic Textual Similarity (Severyn et al,
2013). Moreover, we introduce additional tags,
e.g., video concepts, polarity and negation words,
to achieve better generalization across different
domains where the word distribution and vocab-
ulary changes.
3 Representations and models
Our approach to OM on YouTube relies on the
design of classifiers to predict comment type and
opinion polarity. Such classifiers are traditionally
based on bag-of-words and more advanced fea-
tures. In the next sections, we define a baseline
feature vector model and a novel structural model
based on kernel methods.
3.1 Feature Set
We enrich the traditional bag-of-word representa-
tion with features from a sentiment lexicon and
features quantifying the negation present in the
comment. Our model (FVEC) encodes each docu-
ment using the following feature groups:
- word n-grams: we compute unigrams and
bigrams over lower-cased word lemmas where
binary values are used to indicate the pres-
ence/absence of a given item.
- lexicon: a sentiment lexicon is a collection of
words associated with a positive or negative senti-
ment. We use two manually constructed sentiment
lexicons that are freely available: the MPQA Lex-
icon (Wilson et al, 2005) and the lexicon of Hu
and Liu (2004). For each of the lexicons, we use
the number of words found in the comment that
have positive and negative sentiment as a feature.
- negation: the count of negation words, e.g.,
{don?t, never, not, etc.}, found in a comment.
2
Our structural representation (defined next) en-
ables a more involved treatment of negation.
- video concept: cosine similarity between a com-
ment and the title/description of the video. Most
of the videos come with a title and a short descrip-
tion, which can be used to encode the topicality of
2
The list of negation words is adopted from
http://sentiment.christopherpotts.net/lingstruc.html
each comment by looking at their overlap.
3.2 Structural model
We go beyond traditional feature vectors by em-
ploying structural models (STRUCT), which en-
code each comment into a shallow syntactic tree.
These trees are input to tree kernel functions
for generating structural features. Our struc-
tures are specifically adapted to the noisy user-
generated texts and encode important aspects of
the comments, e.g., words from the sentiment lexi-
cons, product concepts and negation words, which
specifically targets the sentiment and comment
type classification tasks.
In particular, our shallow tree structure is a
two-level syntactic hierarchy built from word lem-
mas (leaves) and part-of-speech tags that are fur-
ther grouped into chunks (Fig. 1). As full syn-
tactic parsers such as constituency or dependency
tree parsers would significantly degrade in perfor-
mance on noisy texts, e.g., Twitter or YouTube
comments, we opted for shallow structures, which
rely on simpler and more robust components: a
part-of-speech tagger and a chunker. Moreover,
such taggers have been recently updated with
models (Ritter et al, 2011; Gimpel et al, 2011)
trained specifically to process noisy texts show-
ing significant reductions in the error rate on user-
generated texts, e.g., Twitter. Hence, we use the
CMU Twitter pos-tagger (Gimpel et al, 2011;
Owoputi et al, 2013) to obtain the part-of-speech
tags. Our second component ? chunker ? is taken
from (Ritter et al, 2011), which also comes with a
model trained on Twitter data
3
and shown to per-
form better on noisy data such as user comments.
To address the specifics of OM tasks on
YouTube comments, we enrich syntactic trees
with semantic tags to encode: (i) central con-
cepts of the video, (ii) sentiment-bearing words
expressing positive or negative sentiment and (iii)
negation words. To automatically identify con-
cept words of the video we use context words (to-
kens detected as nouns by the part-of-speech tag-
ger) from the video title and video description and
match them in the tree. For the matched words,
we enrich labels of their parent nodes (part-of-
speech and chunk) with the PRODUCT tag. Sim-
ilarly, the nodes associated with words found in
3
The chunker from (Ritter et al, 2011) relies on its own
POS tagger, however, in our structural representations we fa-
vor the POS tags from the CMU Twitter tagger and take only
the chunk tags from the chunker.
1254
Figure 1: Shallow tree representation of the example comment (labeled with product type and
negative sentiment): ?iPad 2 is better. the superior apps just destroy the xoom.? (lemmas are replaced
with words for readability) taken from the video ?Motorola Xoom Review?. We introduce additional tags
in the tree nodes to encode the central concept of the video (motorola xoom) and sentiment-bearing words
(better, superior, destroy) directly in the tree nodes. For the former we add a PRODUCT tag on the chunk
and part-of-speech nodes of the word xoom) and polarity tags (positive and negative) for the latter. Two
sentences are split into separate root nodes S.
the sentiment lexicon are enriched with a polar-
ity tag (either positive or negative), while nega-
tion words are labeled with the NEG tag. It should
be noted that vector-based (FVEC) model relies
only on feature counts whereas the proposed tree
encodes powerful contextual syntactic features in
terms of tree fragments. The latter are automati-
cally generated and learned by SVMs with expres-
sive tree kernels.
For example, the comment in Figure 1 shows
two positive and one negative word from the senti-
ment lexicon. This would strongly bias the FVEC
sentiment classifier to assign a positive label
to the comment. In contrast, the STRUCT model
relies on the fact that the negative word, destroy,
refers to the PRODUCT (xoom) since they form a
verbal phase (VP). In other words, the tree frag-
ment: [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP [PRODUCT-N
[xoom]]]] is a strong feature (induced
by tree kernels) to help the classifier to dis-
criminate such hard cases. Moreover, tree
kernels generate all possible subtrees, thus
producing generalized (back-off) features,
e.g., [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] or [S
[negative-VP [PRODUCT-NP]]]].
3.3 Learning
We perform OM on YouTube using supervised
methods, e.g., SVM. Our goal is to learn a model
to automatically detect the sentiment and type of
each comment. For this purpose, we build a multi-
class classifier using the one-vs-all scheme. A bi-
nary classifier is trained for each of the classes
and the predicted class is obtained by taking a
class from the classifier with a maximum predic-
tion score. Our back-end binary classifier is SVM-
light-TK
4
, which encodes structural kernels in the
SVM-light (Joachims, 2002) solver. We define a
novel and efficient tree kernel function, namely,
Shallow syntactic Tree Kernel (SHTK), which is
as expressive as the Partial Tree Kernel (PTK)
(Moschitti, 2006a) to handle feature engineering
over the structural representations of the STRUCT
model. A polynomial kernel of degree 3 is applied
to feature vectors (FVEC).
Combining structural and vector models. A
typical kernel machine, e.g., SVM, classifies a
test input x using the following prediction func-
tion: h(x) =
?
i
?
i
y
i
K(x,x
i
), where ?
i
are
the model parameters estimated from the training
data, y
i
are target variables, x
i
are support vec-
tors, and K(?, ?) is a kernel function. The latter
computes the similarity between two comments.
The STRUCT model treats each comment as a tu-
ple x = ?T ,v? composed of a shallow syntactic
tree T and a feature vector v . Hence, for each pair
of comments x
1
and x
2
, we define the following
comment similarity kernel:
K(x
1
,x
2
) = K
TK
(T
1
,T
2
) +K
v
(v
1
, v
2
), (1)
where K
TK
computes SHTK (defined next), and
K
v
is a kernel over feature vectors, e.g., linear,
polynomial, Gaussian, etc.
Shallow syntactic tree kernel. Following the
convolution kernel framework, we define the new
4
http://disi.unitn.it/moschitti/Tree-Kernel.htm
1255
SHTK function from Eq. 1 to compute the similar-
ity between tree structures. It counts the number of
common substructures between two trees T
1
and
T
2
without explicitly considering the whole frag-
ment space. The general equations for Convolu-
tion Tree Kernels is:
TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
), (2)
where N
T
1
and N
T
2
are the sets of the T
1
?s and
T
2
?s nodes, respectively and ?(n
1
, n
2
) is equal to
the number of common fragments rooted in the n
1
and n
2
nodes, according to several possible defini-
tion of the atomic fragments.
To improve the speed computation of TK, we
consider pairs of nodes (n
1
, n
2
) belonging to the
same tree level. Thus, given H , the height of the
STRUCT trees, where each level h contains nodes
of the same type, i.e., chunk, POS, and lexical
nodes, we define SHTK as the following
5
:
SHTK(T
1
, T
2
) =
H
?
h=1
?
n
1
?N
h
T
1
?
n
2
?N
h
T
2
?(n
1
, n
2
), (3)
where N
h
T
1
and N
h
T
2
are sets of nodes at height h.
The above equation can be applied with any ?
function. To have a more general and expressive
kernel, we use ? previously defined for PTK.
More formally: if n
1
and n
2
are leaves then
?(n
1
, n
2
) = ??(n
1
, n
2
); else ?(n
1
, n
2
) =
?
(
?
2
+
?
~
I
1
,
~
I
2
,|
~
I
1
|=|
~
I
2
|
?
d(
~
I
1
)+d(
~
I
2
)
|
~
I
1
|
?
j=1
?(c
n
1
(
~
I
1j
), c
n
2
(
~
I
2j
))
)
,
where ?, ? ? [0, 1] are decay factors; the large
sum is adopted from a definition of the sub-
sequence kernel (Shawe-Taylor and Cristianini,
2004) to generate children subsets with gaps,
which are then used in a recursive call to ?. Here,
c
n
1
(i) is the i
th
child of the node n
1
;
~
I
1
and
~
I
2
are
two sequences of indexes that enumerate subsets
of children with gaps, i.e.,
~
I = (i
1
, i
2
, .., |I|), with
1 ? i
1
< i
2
< .. < i
|I|
; and d(
~
I
1
) =
~
I
1l(
~
I
1
)
?
~
I
11
+ 1
and d(
~
I
2
) =
~
I
2l(
~
I
2
)
?
~
I
21
+ 1, which penalizes
subsequences with larger gaps.
It should be noted that: firstly, the use of a
subsequence kernel makes it possible to generate
child subsets of the two nodes, i.e., it allows for
gaps, which makes matching of syntactic patterns
5
To have a similarity score between 0 and 1, a normaliza-
tion in the kernel space, i.e.
SHTK(T
1
,T
2
)
?
SHTK(T
1
,T
1
)?SHTK(T
2
,T
2
)
is
applied.
less rigid. Secondly, the resulting SHTK is essen-
tially a special case of PTK (Moschitti, 2006a),
adapted to the shallow structural representation
STRUCT (see Sec. 3.2). When applied to STRUCT
trees, SHTK exactly computes the same feature
space as PTK, but in faster time (on average). In-
deed, SHTK required to be only applied to node
pairs from the same level (see Eq. 3), where the
node labels can match ? chunk, POS or lexicals.
This reduces the time for selecting the matching-
node pairs carried out in PTK (Moschitti, 2006a;
Moschitti, 2006b). The fragment space is obvi-
ously the same, as the node labels of different
levels in STRUCT are different and will not be
matched by PTK either.
Finally, given its recursive definition in Eq. 3
and the use of subsequence (with gaps), SHTK can
derive useful dependencies between its elements.
For example, it will generate the following subtree
fragments: [positive-NP [positive-A
N]], [S [negative-VP [negative-V
[destroy]] [PRODUCT-NP]]]] and so on.
4 YouTube comments corpus
To build a corpus of YouTube comments, we fo-
cus on a particular set of videos (technical reviews
and advertisings) featuring commercial products.
In particular, we chose two product categories:
automobiles (AUTO) and tablets (TABLETS). To
collect the videos, we compiled a list of prod-
ucts and queried the YouTube gData API
6
to re-
trieve the videos. We then manually excluded
irrelevant videos. For each video, we extracted
all available comments (limited to maximum 1k
comments per video) and manually annotated each
comment with its type and polarity. We distin-
guish between the following types:
product: discuss the topic product in general or
some features of the product;
video: discuss the video or some of its details;
spam: provide advertising and malicious links; and
off-topic: comments that have almost no content
(?lmao?) or content that is not related to the video
(?Thank you!?).
Regarding the polarity, we distinguish between
{positive, negative, neutral} sentiments with re-
spect to the product and the video. If the comment
contains several statements of different polarities,
it is annotated as both positive and negative: ?Love
the video but waiting for iPad 4?. In total we have
6
https://developers.google.com/youtube/v3/
1256
annotated 208 videos with around 35k comments
(128 videos TABLETS and 80 for AUTO).
To evaluate the quality of the produced labels,
we asked 5 annotators to label a sample set of one
hundred comments and measured the agreement.
The resulting annotator agreement ? value (Krip-
pendorf, 2004; Artstein and Poesio, 2008) scores
are 60.6 (AUTO), 72.1 (TABLETS) for the senti-
ment task and 64.1 (AUTO), 79.3 (TABLETS) for
the type classification task. For the rest of the
comments, we assigned the entire annotation task
to a single coder. Further details on the corpus can
be found in Uryupina et al (2014).
5 Experiments
This section reports: (i) experiments on individ-
ual subtasks of opinion and type classification; (ii)
the full task of predicting type and sentiment; (iii)
study on the adaptability of our system by learn-
ing on one domain and testing on the other; (iv)
learning curves that provide an indication on the
required amount and type of data and the scalabil-
ity to other domains.
5.1 Task description
Sentiment classification. We treat each com-
ment as expressing positive, negative or
neutral sentiment. Hence, the task is a three-
way classification.
Type classification. One of the challenging as-
pects of sentiment analysis of YouTube data is that
the comments may express the sentiment not only
towards the product shown in the video, but
also the video itself, i.e., users may post posi-
tive comments to the video while being generally
negative about the product and vice versa. Hence,
it is of crucial importance to distinguish between
these two types of comments. Additionally, many
comments are irrelevant for both the product and
the video (off-topic) or may even contain
spam. Given that the main goal of sentiment
analysis is to select sentiment-bearing comments
and identify their polarity, distinguishing between
off-topic and spam categories is not critical.
Thus, we merge the spam and off-topic into
a single uninformative category. Similar to
the opinion classification task, comment type clas-
sification is a multi-class classification with three
classes: video, product and uninform.
Full task. While the previously discussed sen-
timent and type identification tasks are useful to
Task class
AUTO TABLETS
TRAIN TEST TRAIN TEST
Sentiment
positive 2005 (36%) 807 (27%) 2393 (27%) 1872 (27%)
neutral 2649 (48%) 1413 (47%) 4683 (53%) 3617 (52%)
negative 878 (16%) 760 (26%) 1698 (19%) 1471 (21%)
total 5532 2980 8774 6960
Type
product 2733 (33%) 1761 (34%) 7180 (59%) 5731 (61%)
video 3008 (36%) 1369 (26%) 2088 (17%) 1674 (18%)
off-topic 2638 (31%) 2045 (39%) 2334 (19%) 1606 (17%)
spam 26 (>1%) 17 (>1%) 658 (5%) 361 (4%)
total 8405 5192 12260 9372
Full
product-pos. 1096 (13%) 517 (10%) 1648 (14%) 1278 (14%)
product-neu. 908 (11%) 729 (14%) 3681 (31%) 2844 (32%)
product-neg. 554 (7%) 370 (7%) 1404 (12%) 1209 (14%)
video-pos. 909 (11%) 290 (6%) 745 (6%) 594 (7%)
video-neu. 1741 (21%) 683 (14%) 1002 (9%) 773 (9%)
video-neg. 324 (4%) 390 (8%) 294 (2%) 262 (3%)
off-topic 2638 (32%) 2045 (41%) 2334 (20%) 1606 (18%)
spam 26 (>1%) 17 (>1%) 658 (6%) 361 (4%)
total 8196 5041 11766 8927
Table 1: Summary of YouTube comments data
used in the sentiment, type and full classification
tasks. The comments come from two product cate-
gories: AUTO and TABLETS. Numbers in paren-
thesis show proportion w.r.t. to the total number of
comments used in a task.
model and study in their own right, our end goal is:
given a stream of comments, to jointly predict both
the type and the sentiment of each comment. We
cast this problem as a single multi-class classifica-
tion task with seven classes: the Cartesian product
between {product, video} type labels and
{positive, neutral, negative} senti-
ment labels plus the uninformative category
(spam and off-topic). Considering a real-life ap-
plication, it is important not only to detect the po-
larity of the comment, but to also identify if it is
expressed towards the product or the video.
7
5.2 Data
We split all the videos 50% between training
set (TRAIN) and test set (TEST), where each
video contains all its comments. This ensures
that all comments from the same video appear
either in TRAIN or in TEST. Since the number
of comments per video varies, the resulting sizes
of each set are different (we use the larger split
for TRAIN). Table 1 shows the data distribution
across the task-specific classes ? sentiment and
type classification. For the sentiment task we ex-
clude off-topic and spam comments as well
as comments with ambiguous sentiment, i.e., an-
7
We exclude comments annotated as both video and
product. This enables the use of a simple flat multi-
classifiers with seven categories for the full task, instead of
a hierarchical multi-label classifiers (i.e., type classification
first and then opinion polarity). The number of comments as-
signed to both product and video is relatively small (8%
for TABLETS and 4% for AUTO).
1257
notated as both positive and negative.
For the sentiment task about 50% of the
comments have neutral polarity, while the
negative class is much less frequent. Inter-
estingly, the ratios between polarities expressed
in comments from AUTO and TABLETS are very
similar across both TRAIN and TEST. Conversely,
for the type task, we observe that comments from
AUTO are uniformly distributed among the three
classes, while for the TABLETS the majority of
comments are product related. It is likely due
to the nature of the TABLETS videos, that are
more geek-oriented, where users are more prone
to share their opinions and enter involved discus-
sions about a product. Additionally, videos from
the AUTO category (both commercials and user
reviews) are more visually captivating and, be-
ing generally oriented towards a larger audience,
generate more video-related comments. Regard-
ing the full setting, where the goal is to have
a joint prediction of the comment sentiment and
type, we observe that video-negative and
video-positive are the most scarce classes,
which makes them the most difficult to predict.
5.3 Results
We start off by presenting the results for the tradi-
tional in-domain setting, where both TRAIN and
TEST come from the same domain, e.g., AUTO or
TABLETS. Next, we show the learning curves to
analyze the behavior of FVEC and STRUCT mod-
els according to the training size. Finally, we per-
form a set of cross-domain experiments that de-
scribe the enhanced adaptability of the patterns
generated by the STRUCT model.
5.3.1 In-domain experiments
We compare FVEC and STRUCT models on three
tasks described in Sec. 5.1: sentiment, type and
full. Table 2 reports the per-class performance
and the overall accuracy of the multi-class clas-
sifier. Firstly, we note that the performance on
TABLETS is much higher than on AUTO across
all tasks. This can be explained by the follow-
ing: (i) TABLETS contains more training data and
(ii) videos from AUTO and TABLETS categories
draw different types of audiences ? well-informed
users and geeks expressing better-motivated opin-
ions about a product for the former vs. more gen-
eral audience for the latter. This results in the
different quality of comments with the AUTO be-
ing more challenging to analyze. Secondly, we
observe that the STRUCT model provides 1-3%
of absolute improvement in accuracy over FVEC
for every task. For individual categories the F1
scores are also improved by the STRUCT model
(except for the negative classes for AUTO, where
we see a small drop). We conjecture that sentiment
prediction for AUTO category is largely driven
by one-shot phrases and statements where it is
hard to improve upon the bag-of-words and senti-
ment lexicon features. In contrast, comments from
TABLETS category tend to be more elaborated
and well-argumented, thus, benefiting from the ex-
pressiveness of the structural representations.
Considering per-class performance, correctly
predicting negative sentiment is most difficult
for both AUTO and TABLETS, which is proba-
bly caused by the smaller proportion of the neg-
ative comments in the training set. For the type
task, video-related class is substantially more dif-
ficult than product-related for both categories. For
the full task, the class video-negative ac-
counts for the largest error. This is confirmed by
the results from the previous sentiment and type
tasks, where we saw that handling negative sen-
timent and detecting video-related comments are
most difficult.
5.3.2 Learning curves
The learning curves depict the behavior of FVEC
and STRUCT models as we increase the size of
the training set. Intuitively, the STRUCT model
relies on more general syntactic patterns and may
overcome the sparseness problems incurred by the
FVEC model when little training data is available.
Nevertheless, as we see in Figure 2, the learning
curves for sentiment and type classification tasks
across both product categories do not confirm this
intuition. The STRUCT model consistently outper-
forms the FVEC across all training sizes, but the
gap in the performance does not increase when we
move to smaller training sets. As we will see next,
this picture changes when we perform the cross-
domain study.
5.3.3 Cross-domain experiments
To understand the performance of our classifiers
on other YouTube domains, we perform a set of
cross-domain experiments by training on the data
from one product category and testing on the other.
Table 3 reports the accuracy for three tasks
when we use all comments (TRAIN + TEST) from
AUTO to predict on the TEST from TABLETS
1258
Task class
AUTO TABLETS
FVEC STRUCT FVEC STRUCT
P R F1 P R F1 P R F1 P R F1
Sent
positive 49.1 72.1 58.4 50.1 73.9 59.0 67.5 70.3 69.9 71.2 71.3 71.3
neutral 68.2 55.0 61.4 70.1 57.6 63.1 81.3 71.4 76.9 81.1 73.1 77.8
negative 42.0 36.9 39.6 41.3 35.8 38.8 48.3 60.0 54.8 50.2 62.6 56.5
Acc 54.7 55.7 68.6 70.5
Type
product 66.8 73.3 69.4 68.8 75.5 71.7 78.2 95.3 86.4 80.1 95.5 87.6
video 45.0 52.8 48.2 47.8 49.9 48.7 83.6 45.7 58.9 83.5 46.7 59.4
uninform 59.3 48.2 53.1 60.6 53.0 56.4 70.2 52.5 60.7 72.9 58.6 65.0
Acc 57.4 59.4 77.2 78.6
Full
product-pos 34.0 49.6 39.2 36.5 51.2 43.0 48.4 56.8 52.0 52.4 59.3 56.4
product-neu 43.4 31.1 36.1 41.4 36.1 38.4 68.0 67.5 68.1 59.7 83.4 70.0
product-neg 26.3 29.5 28.8 26.3 25.3 25.6 43.0 49.9 45.4 44.7 53.7 48.4
video-pos 23.2 47.1 31.9 26.1 54.5 35.5 69.1 60.0 64.7 64.9 68.8 66.4
video-neu 26.1 30.0 29.0 26.5 31.6 28.8 56.4 32.1 40.0 55.1 35.7 43.3
video-neg 21.9 3.7 6.0 17.7 2.3 4.8 39.0 17.5 23.9 39.5 6.1 11.5
uninform 56.5 52.4 54.9 60.0 53.3 56.3 60.0 65.5 62.2 63.3 68.4 66.9
Acc 40.0 41.5 57.6 60.3
Table 2: In-domain experiments on AUTO and TABLETS using two models: FVEC and STRUCT. The
results are reported for sentiment, type and full classification tasks. The metrics used are precision (P),
recall (R) and F1 for each individual class and the general accuracy of the multi-class classifier (Acc).
AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVECAccuracy
55
60
65
70
training size1k 2k 3k 4k 5k ALL
(a) Sentiment classification
AUTOSTRUCTAUTOFVECTABLETSSTRUCTTABLETSFVEC
Accurac
y
40
45
50
55
60
65
70
75
80
training size1k 2k 3k 4k 5k ALL
(b) Type classification
Figure 2: In-domain learning curves. ALL refers
to the entire TRAIN set for a given product cate-
gory, i.e., AUTO and TABLETS (see Table 1)
and in the opposite direction (TABLETS?AUTO).
When using AUTO as a source domain, STRUCT
model provides additional 1-3% of absolute im-
Source Target Task FVEC STRUCT
AUTO TABLETS
Sent 66.1 66.6
Type 59.9 64.1
?
Full 35.6 38.3
?
TABLETS AUTO
Sent 60.4 61.9
?
Type 54.2 55.6
?
Full 43.4 44.7
?
Table 3: Cross-domain experiment. Ac-
curacy using FVEC and STRUCT models
when trained/tested in both directions, i.e.
AUTO?TABLETS and TABLETS?AUTO.
?
de-
notes results statistically significant at 95% level
(via pairwise t-test).
provement, except for the sentiment task.
Similar to the in-domain experiments, we stud-
ied the effect of the source domain size on the tar-
get test performance. This is useful to assess the
adaptability of features exploited by the FVEC and
STRUCT models with the change in the number
of labeled examples available for training. Addi-
tionally, we considered a setting including a small
amount of training data from the target data (i.e.,
supervised domain adaptation).
For this purpose, we drew the learning curves of
the FVEC and STRUCT models applied to the sen-
timent and type tasks (Figure 3): AUTO is used
as the source domain to train models, which are
tested on TABLETS.
8
The plot shows that when
8
The results for the other direction (TABLETS?AUTO)
show similar behavior.
1259
STRUCTFVEC
Source +Target
Accurac
y
62
63
64
65
66
67
68
training size1k 2k 3k 4k 5k 8.5k(ALL) 100 500 1k
(a) Sentiment classification
STRUCTFVEC
Source +Target
Accurac
y
30
35
40
45
50
55
60
65
70
training size1k 2k 3k 4k 5k 8.5k(TRAIN) 13k(ALL) 100 500 1k
(b) Type classification
Figure 3: Learning curves for the cross-domain
setting (AUTO?TABLETS). Shaded area refers to
adding a small portion of comments from the same
domain as the target test data to the training.
little training data is available, the features gener-
ated by the STRUCT model exhibit better adapt-
ability (up to 10% of improvement over FVEC).
The bag-of-words model seems to be affected by
the data sparsity problem which becomes a crucial
issue when only a small training set is available.
This difference becomes smaller as we add data
from the same domain. This is an important ad-
vantage of our structural approach, since we can-
not realistically expect to obtain manual annota-
tions for 10k+ comments for each (of many thou-
sands) product domains present on YouTube.
5.4 Discussion
Our STRUCT model is more accurate since it is
able to induce structural patterns of sentiment.
Consider the following comment: optimus pad
is better. this xoom is just to bulky but optimus
pad offers better functionality. The FVEC bag-
of-words model misclassifies it to be positive,
since it contains two positive expressions (better,
better functionality) that outweigh a single nega-
tive expression (bulky). The structural model, in
contrast, is able to identify the product of interest
(xoom) and associate it with the negative expres-
sion through a structural feature and thus correctly
classify the comment as negative.
Some issues remain problematic even for the
structural model. The largest group of errors are
implicit sentiments. Thus, some comments do not
contain any explicit positive or negative opinions,
but provide detailed and well-argumented criti-
cism, for example, this phone is heavy. Such com-
ments might also include irony. To account for
these cases, a deep understanding of the product
domain is necessary.
6 Conclusions and Future Work
We carried out a systematic study on OM from
YouTube comments by training a set of su-
pervised multi-class classifiers distinguishing be-
tween video and product related opinions. We
use standard feature vectors augmented by shallow
syntactic trees enriched with additional conceptual
information.
This paper makes several contributions: (i) it
shows that effective OM can be carried out with
supervised models trained on high quality annota-
tions; (ii) it introduces a novel annotated corpus
of YouTube comments, which we make available
for the research community; (iii) it defines novel
structural models and kernels, which can improve
on feature vectors, e.g., up to 30% of relative im-
provement in type classification, when little data
is available, and demonstrates that the structural
model scales well to other domains.
In the future, we plan to work on a joint model
to classify all the comments of a given video, s.t. it
is possible to exploit latent dependencies between
entities and the sentiments of the comment thread.
Additionally, we plan to experiment with hierar-
chical multi-label classifiers for the full task (in
place of a flat multi-class learner).
Acknowledgments
The authors are supported by a Google Fac-
ulty Award 2011, the Google Europe Fellowship
Award 2013 and the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der the grant #288024: LIMOSINE.
1260
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources? In
IJCNLP.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Hal Daum?e, III. 2007. Frustratingly easy domain
adaptation. ACL.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA
sentiment corpus for the automotive domain. In
ICWSM-DWC.
Klaus Krippendorf, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
Alessandro Moschitti. 2006a. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ECML.
Alessandro Moschitti. 2006b. Making tree kernels
practical for natural language learning. In EACL,
pages 113?120.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In ACL.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In EMNLP.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual simi-
larity with structural representations. In ACL.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl,
and Jose San Pedro. 2010. How useful are your
comments?: Analyzing and predicting YouTube
comments and comment ratings. In WWW.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In EMNLP.
Anders S?gaard and Anders Johannsen. 2012. Ro-
bust learning in random subspaces: Equipping nlp
for oov effects. In COLING.
Oscar T?ackstr?om and Ryan McDonald. 2011. Semi-
supervised latent variable models for sentence-level
sentiment analysis. In ACL.
Olga Uryupina, Barbara Plank, Aliaksei Severyn,
Agata Rotondi, and Alessandro Moschitti. 2014.
SenTube: A corpus for sentiment analysis on
YouTube social media. In LREC.
Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP.
1261
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 53?58, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
iKernels-Core: Tree Kernel Learning for Textual Similarity
Aliaksei Severyn1 and Massimo Nicosia1 and Alessandro Moschitti1,2
1University of Trento, DISI, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
2Qatar Foundation, QCRI, Doha, Qatar
{amoschitti}@qf.org.qa
Abstract
This paper describes the participation of iKer-
nels system in the Semantic Textual Similar-
ity (STS) shared task at *SEM 2013. Different
from the majority of approaches, where a large
number of pairwise similarity features are
used to learn a regression model, our model
directly encodes the input texts into syntac-
tic/semantic structures. Our systems rely on
tree kernels to automatically extract a rich set
of syntactic patterns to learn a similarity score
correlated with human judgements. We ex-
periment with different structural representa-
tions derived from constituency and depen-
dency trees. While showing large improve-
ments over the top results from the previous
year task (STS-2012), our best system ranks
21st out of total 88 participated in the STS-
2013 task. Nevertheless, a slight refinement to
our model makes it rank 4th.
1 Introduction
Comparing textual data to establish the degree of se-
mantic similarity is of key importance in many Nat-
ural Language Processing (NLP) tasks ranging from
document categorization to textual entailment and
summarization. The key aspect of having an accu-
rate STS framework is the design of features that can
adequately represent various aspects of the similar-
ity between texts, e.g. using lexical, syntactic and
semantic similarity metrics.
The majority of approaches to semantic textual
similarity treat the input text pairs as feature vec-
tors where each feature is a score corresponding to a
certain type of similarity. This approach is concep-
tually easy to implement and STS-2012 (Agirre et
al., 2012) has shown that the best systems were built
following this idea, i.e. a number of features encod-
ing similarity of an input text pair were combined in
a single scoring model, such as Linear Regression
or Support Vector Regression (SVR). One potential
limitation of using only similarity features to repre-
sent a text pair is that of low representation power.
The novelty of our approach is that we encode the
input text pairs directly into structural objects, e.g.
trees, and rely on the power of kernel learning to ex-
tract relevant structures. This completely different
from (Croce et al, ), where tree kernels where used
to establish syntactic similarity and then plugged as
similarity features. To link the documents in a pair
we mark the nodes in the related structures with a
special relational tag. In this way effective struc-
tural relational patterns are implicitly encoded in the
trees and can be automatically learned by the kernel-
based machine learning methods. We build our sys-
tems on top of the features used by two best systems
from STS-2012 and combine them with the tree ker-
nel models within the Support Vector Regression to
derive a single scoring model. Since the test data
used for evaluation in STS-2013 (Agirre et al, 2013)
is different from the 2012 data provided for the sys-
tem development, domain adaptation represents an
additional challenge. To address this problem we
augment our feature vector representation with fea-
tures extracted from a text pair as a whole to capture
individual properties of each dataset. Additionally,
we experiment with a corpus type classifier and in-
clude its prediction score as additional features. Fi-
nally, we use stacking to combine several structural
models into the feature vector representation.
53
In the following sections we describe our ap-
proach to combine structural representations with
the pairwise similarity features in a single SVR
learning framework. We then report results on both
STS-2012 and 2013 tasks.
2 Structural Relational Similarity
In this section we first describe the kernel framework
to combine structural and vector models, then we
explain how to construct the tree models and briefly
describe tree kernels we use to automatically extract
the features.
2.1 Structural Kernel Learning
In supervised learning, given the labeled data
{(xi, y i)}ni=1, the goal is to estimate a decision func-
tion h(x) = y that maps input examples to the tar-
get variables. A conventional approach is to rep-
resent a pair of texts as a set of similarity features
{fi}, s.t. the predictions are computed as h(x) =
w ? x =
?
iwifi, wherew is the model weight vec-
tor. Hence, the learning problem boils down to es-
timating the individual weight of each of the sim-
ilarity feature fi. One downside of such approach
is that a great deal of similarity information carried
by a given text pair is lost when modeled by single
real-valued scores.
A more versatile approach in terms of the input
representation relies on kernels. In a typical ker-
nel machine, e.g. SVM, the prediction function for
a test input x takes on the following form h(x) =
?
i ?iyiK(x,xi), where ?i are the model parame-
ters estimated from the training data, yi - target vari-
ables, xi are support vectors, and K(?, ?) is a kernel
function.
To encode both structural representation and sim-
ilarity feature vectors of input text pairs xi in a sin-
gle model, we treat it as the following tuple: xi =
?xai ,x
b
i? = ?(t
a
i , v
a
i ), (t
b
i , v
b
i)?, where x
a
i x
b
i are the
first and the second document of xi, and t and v de-
note tree and vector representations respectively.
To compute a kernel between two text pairs xi
and xj we define the following all-vs-all kernel,
where all possible combinations of documents from
each pair are considered: K(xi,xj) = K(xai ,x
a
j ) +
K(xai ,x
b
j) + K(x
b
i ,x
a
j ) + K(x
b
i ,x
b
j). Each of the
kernel computations K between two documents xa
and xb can be broken down into the following:
K(xa,xb) = KTK(ta, tb) + Kfvec(va, vb), where
KTK computes a tree kernel and Kfvec is a kernel
over feature vectors, e.g. linear, polynomial or RBF,
etc. Further in the text we refer to structural tree
kernel models as TK and explicit feature vector rep-
resentation as fvec.
Having defined a way to jointly model text pairs
using structural TK representations along with the
similarity features fvec, we next briefly review tree
kernels and our relational structures derived from
constituency and dependency trees.
2.2 Tree Kernels
We use tree structures as our base representation
since they provide sufficient flexibility in represen-
tation and allow for easier feature extraction than,
for example, graph structures. We use a Partial Tree
Kernel (PTK) (Moschitti, 2006) to take care of auto-
matic feature extraction and compute KTK(?, ?).
PTK is a tree kernel function that can be ef-
fectively applied to both constituency and depen-
dency parse trees. It generalizes a subset tree ker-
nel (STK) (Collins and Duffy, 2002) that maps a
tree into the space of all possible tree fragments con-
strained by the rule that the sibling nodes from their
parents cannot be separated. Different from STK
where the nodes in the generated tree fragments are
constrained to include none or all of their direct chil-
dren, PTK fragments can contain any subset of the
features, i.e. PTK allows for breaking the production
rules. Consequently, PTK generalizes STK generat-
ing an extremely rich feature space, which results in
higher generalization ability.
2.3 Relational Structures
The idea of using relational structures to jointly
model text pairs was previously proposed in (Sev-
eryn and Moschitti, 2012), where shallow syntactic
structures derived from chunks and part-of-speech
tags were used to represent question/answer pairs.
In this paper, we define novel relational structures
based on: (i) constituency and (ii) dependency trees.
Constituency tree. Each document in a given text
pair is represented by its constituency parse tree.
If a document contains multiple sentences they are
merged in a single tree with a common root. To
encode the structural relationships between docu-
54
Figure 1: A dependency-based structural representation of a text pair. REL tag links related fragments.
ments in a pair a special REL tag is used to link
the related structures. We adopt a simple strategy
to establish such links: words from two documents
that have a common lemma get their parents (POS
tags) and grandparents, non-terminals, marked with
a REL tag.
Dependency tree. We propose to use dependency
relations between words to derive an alternative
structural representation. In particular, dependency
relations are used to link words in a way that words
are always at the leaf level. This reordering of the
nodes helps to avoid the situation where nodes with
words tend to form long chains. This is essential
for PTK to extract meaningful fragments. We also
plug part-of-speech tags between the word nodes
and nodes carrying their grammatical role. Again
a special REL tag is used to establish relations be-
tween tree fragments. Fig. 1 gives an example of
a dependency-based structure taken from STS-2013
headlines dataset.
3 Pairwise similarity features.
Along with the direct representation of input text
pairs as structural objects our framework also en-
codes feature vectors (base), which we describe
below.
3.1 Baseline features
We adopt similarity features from two best perform-
ing systems of STS-2012, which were publicly re-
leased: namely, the Takelab1 system (S?aric? et al,
2012) and the UKP Lab?s system2 (Bar et al, 2012).
Both systems represent input texts with similar-
1http://takelab.fer.hr/sts/
2https://code.google.com/p/dkpro-similarity-
asl/wiki/SemEval2013
ity features which combine multiple text similarity
measures of varying complexity.
UKP provides metrics based on matching of char-
acter, word n-grams and common subsequences. It
also includes features derived from Explicit Seman-
tic Analysis vector comparisons and aggregation of
word similarity based on lexical-semantic resources,
e.g. WordNet. In total it provides 18 features.
Takelab includes n-gram matching of varying size,
weighted word matching, length difference, Word-
Net similarity and vector space similarity where
pairs of input sentences are mapped into Latent Se-
mantic Analysis (LSA) space (Turney and Pantel,
2010). The features are computed over several sen-
tence representations where stop words are removed
and/or lemmas are used in place of raw tokens.
The total number of Takelab?s features is 21. Even
though some of the UKP and Takelab features over-
lap we include all of them in a combined system with
the total of 39 features.
3.2 iKernels features
Here we describe our additional features added to
the fvec representation. First, we note that word
frequencies used to compute weighted word match-
ings and the word-vector mappings to compute LSA
similarities required by Takelab features are pro-
vided only for the vocabulary extracted from 2012
data. Hence, we use both STS-2012 and 2013 data to
obtain the word counts and re-estimate LSA vector
representations. For the former we extract unigram
counts from Google Books Ngrams3, while for the
latter we use additional corpora as described below.
LSA similarity. To construct LSA word-vector
mappings we use the following three sources: (i)
3http://storage.googleapis.com/books/ngrams/books/datasetsv2.html
55
Aquaint4, which consists of more than 1 million
newswire documents, (ii) ukWaC (Baroni et al,
2009) - a 2 billion word corpus constructed from
the Web, and (iii) and a collection of documents
extracted from Wikipedia dump5. To extract LSA
topics we use GenSim6 software. We preprocess
the data by lowercasing, removing stopwords and
words with frequency lower than 5. Finally, we ap-
ply tf-idf weighting. For all representations we fix
the number of dimensions to 250. For all corpora
we use document-level representation, except for
Wikipedia we also experimented with a sentence-
level document representation, which typically pro-
vides a more restricted context for estimating word-
document distributions.
Brown Clusters. In addition to vector represen-
tations derived from LSA, we extract word-vector
mappings using Brown word clusters7 (Turian et al,
2010), where words are organized into a hierarchy
and each word is represented as a bit-string. We
encode each word by a feature vector where each
entry corresponds to a prefix extracted from its bit-
string. We use prefix lengths in the following range:
k = {4, 8, 12, 16, 20}. Finally, the document is rep-
resented as a feature vector composed by the indi-
vidual word vectors.
Term-overlap features. In addition to the word
overlap features computed by UKP and Takelab
systems we also compute a cosine similarity over
the following representations: (i) n-grams of part-
of-speech tags (up to 4-grams), (ii) SuperSense
tags (Ciaramita and Altun, 2006), (iii) named enti-
ties, and (iv) dependency triplets.
PTK similarity. We use PTK to provide a syn-
tactic similarity score between documents in a pair:
PTK(a, b) = PTK(a, b), where as input represen-
tations we use dependency and constituency trees.
Explicit Semantic Analysis (ESA) similarity.
ESA (Gabrilovich and Markovitch, 2007) represents
input documents as vectors of Wikipedia concepts.
To compute ESA features we use Lucene8 to in-
dex documents extracted from a Wikipedia dump.
Given a text pair we retrieve k top documents (i.e.
4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/
5http://dumps.wikimedia.org/
6http://radimrehurek.com/gensim/
7http://metaoptimize.com/projects/wordreprs/
8http://lucene.apache.org/
Wikipedia concepts) and compute the metric by
looking at the overlap of the concepts between the
documents: esak(a, b) =
|Wa
?
Wb|
k , where Wa is
the set of concepts retrieved for document a. We
compute esa features with k ? {10, 25, 50, 100}.
3.3 Corpus type features
Here we describe two complementary approaches
(corpus) in an attempt to alleviate the problem of
domain adaptation, where the datasets used for train-
ing and testing are drawn from different sources.
Pair representation. We treat each pair of texts as a
whole and extract the following sets of corpus fea-
tures: plain bag-of-words, dependency triplets, pro-
duction rules of the syntactic parse tree and a length
feature, i.e. a log-normalized length of the combined
text. Each feature set is normalized and added to the
fvec model.
Corpus classifier. We use the above set of features
to train a multi-class classifier to predict for each in-
stance its most likely corpus type. Our categories
correspond to five dataset types of STS-2012. Pre-
diction scores for each of the dataset categories are
then plugged as features into the final fvec repre-
sentation. Our multi-class classifier is a one-vs-all
binary SVM trained on the merged data from STS-
2012. We apply 5-fold cross-validation scheme, s.t.
for each of the held-out folds we obtain independent
predictions. The accuracy (averaged over 5-folds)
on the STS-2012 data is 92.0%.
3.4 Stacking
To integrate multiple TK models into a single model
we apply a classifier stacking approach (Fast and
Jensen, 2008). Each of the learned TK models is
used to generate predictions which are then plugged
as features into the final fvec representation, s.t.
the final model uses only explicit feature vector
representation. We apply a 5-fold cross-validation
scheme to obtain prediction scores in the same man-
ner as described above.
4 Experimental Evaluation
4.1 Experimental setup
To encode TK models along with the similarity fea-
ture vectors into a single regression scoring model,
56
base corpus TK
U T I B O M C D ALL Mean MSRp MSRv SMTe OnWN SMTn
? 0.7060 0.6087 0.6080 0.8390 0.2540 0.6820 0.4470
? 0.7589 0.6863 0.6814 0.8637 0.4950 0.7091 0.5395
? ? 0.8079 0.7161 0.7134 0.8837 0.5519 0.7343 0.5607
? ? ? 0.8187 0.7137 0.7157 0.8833 0.5131 0.7355 0.5809
? ? ? ? 0.8458 0.7047 0.6935 0.8953 0.5080 0.7101 0.5834
? ? ? ? 0.8468 0.6954 0.6717 0.8902 0.4652 0.7089 0.6133
? ? ? ? ? 0.8539 0.7132 0.6993 0.9005 0.4772 0.7189 0.6481
? ? ? ? ? 0.8529 0.7249 0.7080 0.8984 0.5142 0.7263 0.6700
Sys1 ? ? ? ? ? ? 0.8546 0.7156 0.6989 0.8979 0.4884 0.7181 0.6609
Sys3 ? ? ? ? ? ? 0.8810 0.7416 0.7210 0.8971 0.5912 0.7328 0.6778
Sys2 ? ? ? ? ? ? 0.8705 0.7339 0.7039 0.9012 0.5629 0.7376 0.6656
UKPbest 0.8239 0.6773 0.6830 0.8739 0.5280 0.6641 0.4937
Table 1: System configurations and results on STS-2012. Column set base lists 3 feature sets : UKP (U), Takelab
(T) and iKernels (I); corpus type features (corpus) include plain features (B), corpus classifier (O), and manually
encoded dataset category (M); TK contains constituency (C) and dependency-based (D) models. UKPbest is the best
system of STS-2012. First column shows configuration of our three system runs submitted to STS-2013.
we use an SVR framework implemented in SVM-
Light-TK9. We use the following parameter settings
-t 5 -F 3 -W A -C +, which specifies to use
a combination of trees and feature vectors (-C +),
PTK over trees (-F 3) computed in all-vs-all mode
(-W A) and using polynomial kernel of degree 3 for
the feature vector (active by default).
We report the following metrics employed in the
final evaluation: Pearson correlation for individual
test sets10 and Mean ? an average score weighted by
the test set size.
4.2 STS-2012
For STS-2013 task the entire data from STS-2012
was provided for the system development. To com-
pare with the best systems of the previous year we
followed the same setup, where 3 datasets (MSRp,
MSRv and SMTe) are used for training and 5 for test-
ing (two ?surprise? datasets were added: OnWN and
SMTn). We use the entire training data to obtain a
single model.
Table 1 summarizes the results using structural
models (TK), pairwise similarity (base) and corpus
type features (corpus). We first note, that com-
bining all three features sets (U, T and I) provides
a good match to the best system UKPbest. Next,
adding TK models results in a large improvement
beating the top results in STS-2012. Furthermore,
using corpus features results in even greater im-
9http://disi.unitn.it/moschitti/Tree-Kernel.htm
10for STS-2012 we also report the results for a concatenation
of all five test sets (ALL)
provement with the Mean = 0.7416 and Pearson
ALL = 0.8810.
4.3 STS-2013
Below we specify the configuration for each of the
submitted runs (also shown in Table 1) and report the
results on the STS-2013 test sets: headlines (head),
OnWN, FNWN, and SMT:
Sys1: combines base features (U, T and I), TK
models (C and D) and plain corpus type features (B).
We use STS-2012 data to train a single model.
Sys2: different from Sys1 where a single model
trained on the entire data is used to make predictions,
we adopt a different training/test setup to account for
the different nature of the data used for training and
testing. After performing manual analysis of the test
data we came up with the following strategy to split
the training data into two sets to learn two differ-
ent models: STMe and OnWN (model1) and MSRp,
SMTn and STMe (model2); model1 is then used to
get predictions for OnWN, FNWN, while model2 is
used for SMT and headlines.
Sys3: same as Sys1 + a corpus type classifier O as
described in Sec. 3.3.
Table 2 shows the resulting performance of our
systems and the best UMBC system published in the
final ranking. Sys2 appears the most accurate among
our systems, which ranked 21st out of 88. Compar-
ing to the best system across four datasets we ob-
serve that it performs reasonably well on the head-
lines dataset (it is 5th best), while completely fails
on the OnWN and FNWN test sets. After performing
57
error analysis, we found that TK models underper-
form on FNWN and OnWN sets, which appear un-
derrepresented in the training data from STS-2012.
We build a new system (Sys?2), which is based on
Sys2, by making two adjustments in the setup: (i)
we exclude SMTe from training to obtain predictions
on SMT and head and (ii) we remove all TK features
to train a model for FNWN and OnWN. This is mo-
tivated by the observation that text pairs from STS-
2012 yield a paraphrase model, since the texts are
syntactically very similar. Yet, two datasets from
STS-2013 FNWN, and OnWN contain text pairs
where documents exhibit completely different struc-
tures. This is misleading for our syntactic similarity
model learned on the STS-2012.
System head OnWN FNWN SMT Mean Rank
UMBC 0.7642 0.7529 0.5818 0.3804 0.6181 1
Sys2 0.7465 0.5572 0.3875 0.3409 0.5339 21
Sys1 0.7352 0.5432 0.3842 0.3180 0.5188 28
Sys3 0.7395 0.4228 0.3596 0.3294 0.4919 40
Sys?2 0.7538 0.6872 0.4478 0.3391 0.5732 4*
Table 2: Results on STS-2013.
5 Conclusions and Future Work
We have described our participation in STS-2013
task. Our approach treats text pairs as structural
objects which provides much richer representation
for the learning algorithm to extract useful patterns.
We experiment with structures derived from con-
stituency and dependency trees where related frag-
ments are linked with a special tag. Such struc-
tures are then used to learn tree kernel models which
can be efficiently combined with the a feature vector
representation in a single scoring model. Our ap-
proach ranks 1st with a large margin w.r.t. to the
best systems in STS-2012 task, while it is 21st ac-
cording to the final rankings of STS-2013. Never-
theless, a small change in the system setup makes
it rank 4th. Clearly, domain adaptation represents a
big challenge in STS-2013 task. We plan to address
this issue in our future work.
6 Acknowledgements
This research has been supported by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) under the #288024 LIMOSINE
project.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre. 2012. Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In First Joint Conference on
Lexical and Computational Semantics (*SEM).
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
EMNLP.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete Structures, and the Voted Perceptron. In ACL.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. Unitor: Combining semantic text similarity
functions through sv regression. In SemEval 2012.
Andrew S. Fast and David Jensen. 2008. Why stacked
models perform effective collective classification. In
ICDM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI.
A. Moschitti. 2006. Efficient convolution kernels for
dependency and constituent syntactic trees. In ECML.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In SIGIR.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141?188.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceedings
of the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012).
58
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 25?32
Manchester, August 2008
Encoding Tree Pair-based Graphs in Learning Algorithms:
the Textual Entailment Recognition Case
Alessandro Moschitti
DISI, University of Trento
Via Sommarive 14
38100 POVO (TN) - Italy
moschitti@dit.unitn.it
Fabio Massimo Zanzotto
DISP, University of Rome ?Tor Vergata?
Via del Politecnico 1
00133 Roma, Italy
zanzotto@info.uniroma2.it
Abstract
In this paper, we provide a statistical ma-
chine learning representation of textual en-
tailment via syntactic graphs constituted
by tree pairs. We show that the natural way
of representing the syntactic relations be-
tween text and hypothesis consists in the
huge feature space of all possible syntac-
tic tree fragment pairs, which can only be
managed using kernel methods. Experi-
ments with Support Vector Machines and
our new kernels for paired trees show the
validity of our interpretation.
1 Introduction
Recently, a lot of valuable work on the recogni-
tion of textual entailment (RTE) has been carried
out (Bar Haim et al, 2006). The aim is to detect
implications between sentences like:
T
1
? H
1
T
1
?Wanadoo bought KStones?
H
1
?Wanadoo owns KStones?
where T
1
and H
1
stand for text and hypothesis, re-
spectively.
Several models, ranging from the simple lexi-
cal similarity between T and H to advanced Logic
Form Representations, have been proposed (Cor-
ley and Mihalcea, 2005; Glickman and Dagan,
2004; de Salvo Braz et al, 2005; Bos and Mark-
ert, 2005). However, since a linguistic theory able
to analytically show how to computationally solve
the RTE problem has not been developed yet, to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
design accurate systems, we should rely upon the
application of machine learning. In this perspec-
tive, TE training examples have to be represented
in terms of statistical feature distributions. These
typically consist in word sequences (along with
their lexical similarity) and the syntactic structures
of both text and hypothesis (e.g. their parse trees).
The interesting aspect with respect to other natural
language problems is that, in TE, features useful
at describing an example are composed by pairs of
features from Text and Hypothesis.
For example, using a word representation, a text
and hypothesis pair, ?T,H?, can be represented
by the sequences of words of the two sentences,
i.e. ?t
1
, .., t
n
? and ?h
1
, .., h
m
?, respectively. If we
carry out a blind and complete statistical correla-
tion analysis of the two sequences, the entailment
property would be described by the set of subse-
quence pairs from T and H , i.e. the set R =
{?s
t
, s
h
? : s
t
= ?t
i
1
, .., t
i
l
?, s
h
= ?h
j
1
, .., h
j
r
?, l ?
n, r ? m}. The relation set R constitutes a
naive and complete representation of the example
?T,H? in the feature space {?v,w? : v,w ? V ?},
where V is the corpus vocabulary1 .
Although the above representation is correct and
complete from a statistically point of view, it suf-
fers from two practical drawbacks: (a) it is expo-
nential in V and (b) it is subject to high degree of
data sparseness which may prevent to carry out ef-
fective learning. The traditional solution for this
problem relates to consider the syntactic structure
of word sequences which provides their general-
ization.
The use of syntactic trees poses the problem
of representing structures in learning algorithms.
1
V
? is larger than the actual space, which is the one of
all possible subsequences with gaps, i.e. it only contains all
possible concatenations of words respecting their order.
25
For this purpose, kernel methods, and in partic-
ular tree kernels allow for representing trees in
terms of all possible subtrees (Collins and Duffy,
2002). Unfortunately, the representation in entail-
ment recognition problems requires the definition
of kernels over graphs constituted by tree pairs,
which are in general different from kernels applied
to single trees. In (Zanzotto and Moschitti, 2006),
this has been addressed by introducing semantic
links (placeholders) between text and hypothesis
parse trees and evaluating two distinct tree ker-
nels for the trees of texts and for those of hypothe-
ses. In order to make such disjoint kernel combi-
nation effective, all possible assignments between
the placeholders of the first and the second en-
tailment pair were generated causing a remarkable
slowdown.
In this paper, we describe the feature space of
all possible tree fragment pairs and we show that it
can be evaluated with a much simpler kernel than
the one used in previous work, both in terms of
design and computational complexity. Moreover,
the experiments on the RTE datasets show that our
proposed kernel provides higher accuracy than the
simple union of tree kernel spaces.
2 Fragments of Tree Pair-based Graphs
The previous section has pointed out that RTE can
be seen as a relational problem between word se-
quences of Text and Hypothesis. The syntactic
structures embedded in such sequences can be gen-
eralized by natural language grammars. Such gen-
eralization is very important since it is evident that
entailment cases depend on the syntactic structures
of Text and Hypothesis. More specifically, the set
R described in the previous section can be ex-
tended and generalized by considering syntactic
derivations2 that generate word sequences in the
training examples. This corresponds to the follow-
ing set of tree fragment pairs:
R
?
= {??
t
, ?
h
? : ?
t
? F(T ), ?
h
? F(H)}, (1)
where F(?) indicates the set of tree fragments of a
parse tree (i.e. the one of the text T or of the hy-
pothesis H). R? contains less sparse relations than
R. For instance, given T
1
and H
1
of the previous
section, we would have the following relational de-
scription:
2By cutting derivation at different depth, different degrees
of generalization can be obtained.
R
?
=
{
?
NP
NNP
,
NP
NNP
? , ?
S
NP VP
,
S
NP VP
? ,
?
S
NP
NNP
VP
VBP
bought
NP
NNP
,
S
NP
NNP
VP
VBP
owns
NP
NNP
? ,
?
VP
VBP
bought
NP
NNP
,
VP
VBP
owns
NP
NNP
? , ..
}
These features (relational pairs) generalize the
entailment property, e.g. the pair ?[VP [VBP bought] [NP]],
[VP [VBP own] [NP]]? generalizes many word sequences,
i.e. those external to the verbal phrases and inter-
nal to the NPs.
We can improve this space by adding semantic
links between the tree fragments. Such links
or placeholders have been firstly proposed in
(Zanzotto and Moschitti, 2006). A placeholder
assigned to a node of ?
t
and a node of ?
h
states
that such nodes dominate the same (or similar) in-
formation. In particular, placeholders are assigned
to nodes whose words t
i
in T are equal, similar, or
semantically dependent on words h
j
in H . Using
placeholders, we obtain a richer fragment pair
based representation that we call R?p, exemplified
hereafter:
{
?
S
NP
NNP X
VP
VBP
bought
NP
NNP Y
,
S
NP
NNP X
VP
VBP
owns
NP
NNP Y
?
, ?
S
NP VP
VBP
bought
NP
NNP Y
,
S
NP VP
VBP
owns
NP
NNP Y
?
, ?
S
NP VP
,
S
NP VP
? , ...
}
The placeholders (or variables) indicated with
X and Y specify that the NNPs labeled by
the same variables dominate similar or identical
words. Therefore, an automatic algorithm that
assigns placeholders to semantically similar con-
stituents is needed. Moreover, although R?p con-
tains more semantic and less sparse features than
26
both R? and R, its cardinality is still exponential in
the number of the words of T and H . This means
that standard machine learning algorithms cannot
be applied. In contrast, tree kernels (Collins and
Duffy, 2002) can be used to efficiently generate
the huge space of tree fragments but, to generate
the space of pairs of tree fragments, a new kernel
function has to be defined.
The next section provides a solution to both
problems. i.e. an algorithm for placeholders as-
signments and for the computation of paired tree
kernels which generates R? and R?p representa-
tions.
F
(
VP
V
book
NP
D
a
N
flight
)
=
{
VP
V NP
D
a
N
flight
,
VP
V NP
D N
,
NP
D
a
N
flight
,
NP
D
a
N ,
NP
D N
flight
,
NP
D N
,
N
flight
, . . .
}
Figure 1: A syntactic parse tree.
3 Kernels over Semantic Tree Pair-based
Graphs
The previous section has shown that placeholders
enrich a tree-based graph with relational informa-
tion, which, in turn, can be captured by means
of word semantic similarities sim
w
(w
t
, w
h
), e.g.
(Corley and Mihalcea, 2005; Glickman et al,
2005). More specifically, we use a two-step greedy
algorithm to anchor the content words (verbs,
nouns, adjectives, and adverbs) in the hypothesis
W
H
to words in the text W
T
.
In the first step, each word w
h
in W
H
is con-
nected to all words w
t
in W
T
that have the max-
imum similarity sim
w
(w
t
, w
h
) with it (more than
one w
t
can have the maximum similarity with w
h
).
As result, we have a set of anchors A ? W
T
?W
H
.
sim
w
(w
t
, w
h
) is computed by means of three tech-
niques:
1. Two words are maximally similar if they have
the same surface form w
t
= w
h
.
2. Otherwise, WordNet (Miller, 1995) similari-
ties (as in (Corley and Mihalcea, 2005)) and
different relation between words such as verb
entailment and derivational morphology are
applied.
3. The edit distance measure is finally used to
capture the similarity between words that are
missed by the previous analysis (for mis-
spelling errors or for the lack of derivational
forms in WordNet).
In the second step, we select the final anchor set
A
?
? A, such that ?w
t
(or w
h
) ?!?w
t
, w
h
? ? A
?
.
The selection is based on a simple greedy algo-
rithm that given two pairs ?w
t
, w
h
? and ?w?
t
, w
h
?
to be selected and a pair ?s
t
, s
h
? already selected,
considers word proximity (in terms of number of
words) between w
t
and s
t
and between w?
t
and s
t
;
the nearest word will be chosen.
Once the graph has been enriched with seman-
tic information we need to represent it in the learn-
ing algorithm; for this purpose, an interesting ap-
proach is based on kernel methods. Since the con-
sidered graphs are composed by only two trees, we
can carried out a simplified computation of a graph
kernel based on tree kernel pairs.
3.1 Tree Kernels
Tree Kernels (e.g. see NLP applications in (Giu-
glea and Moschitti, 2006; Zanzotto and Moschitti,
2006; Moschitti et al, 2007; Moschitti et al,
2006; Moschitti and Bejan, 2004)) represent trees
in terms of their substructures (fragments) which
are mapped into feature vector spaces, e.g. ?n.
The kernel function measures the similarity be-
tween two trees by counting the number of their
common fragments. For example, Figure 1 shows
some substructures for the parse tree of the sen-
tence "book a flight". The main advantage of
tree kernels is that, to compute the substructures
shared by two trees ?
1
and ?
2
, the whole fragment
space is not used. In the following, we report the
formal definition presented in (Collins and Duffy,
2002).
Given the set of fragments {f
1
, f
2
, ..} = F , the
indicator function I
i
(n) is equal 1 if the target f
i
is
rooted at node n and 0 otherwise. A tree kernel is
then defined as:
TK(?
1
, ?
2
) =
?
n
1
?N
?
1
?
n
2
?N
?
2
?(n
1
, n
2
) (2)
where N
?
1
and N
?
2
are the sets of the ?
1
?s and ?
2
?s
27
nodes, respectively and
?(n
1
, n
2
) =
|F|
?
i=1
I
i
(n
1
)I
i
(n
2
)
The latter is equal to the number of common frag-
ments rooted in the n
1
and n
2
nodes and ? can be
evaluated with the following algorithm:
1. if the productions at n
1
and n
2
are different
then ?(n
1
, n
2
) = 0;
2. if the productions at n
1
and n
2
are the
same, and n
1
and n
2
have only leaf children
(i.e. they are pre-terminals symbols) then
?(n
1
, n
2
) = 1;
3. if the productions at n
1
and n
2
are the same,
and n
1
and n
2
are not pre-terminals then
?(n
1
, n
2
) =
nc(n
1
)
?
j=1
(1 + ?(c
j
n
1
, c
j
n
2
)) (3)
where nc(n
1
) is the number of the children of
n
1
and cj
n
is the j-th child of the node n. Note
that since the productions are the same, nc(n
1
) =
nc(n
2
).
Additionally, we add the decay factor ? by mod-
ifying steps (2) and (3) as follows3:
2. ?(n
1
, n
2
) = ?,
3. ?(n
1
, n
2
) = ?
nc(n
1
)
?
j=1
(1 + ?(c
j
n
1
, c
j
n
2
)).
The computational complexity of Eq. 2 is
O(|N
?
1
| ? |N
?
2
|) although the average running
time tends to be linear (Moschitti, 2006).
3.2 Tree-based Graph Kernels
The above tree kernel function can be applied to
the parse trees of two texts or those of the two hy-
potheses to measure their similarity in terms of the
shared fragments. If we sum the contributions of
the two kernels (for texts and for hypotheses) as
proposed in (Zanzotto and Moschitti, 2006), we
just obtain the feature space of the union of the
fragments which is completely different from the
space of the tree fragments pairs, i.e. R? . Note
that the union space is not useful to describe which
3To have a similarity score between 0 and 1, we also ap-
ply the normalization in the kernel space, i.e. K?(?
1
, ?
2
) =
TK(?
1
,?
2
)
?
TK(?
1
,?
1
)?TK(?
2
,?
2
)
.
grammatical and lexical property is at the same
time held by T and H to trig the implication.
Therefore to generate the space of the frag-
ment pairs we need to define the kernel between
two pairs of entailment examples ?T
1
,H
1
? and
?T
2
,H
2
? as
K
p
(?T
1
,H
1
?, ?T
2
,H
2
?) =
=
?
n
1
?T
1
?
n
2
?T
2
?
n
3
?H
1
?
n
4
?H
2
?(n
1
, n
2
, n
3
, n
4
),
where ? evaluates the number of subtrees rooted
in n
1
and n
2
combined with those rooted in n
3
and
n
4
. More specifically, each fragment rooted into
the nodes of the two texts? trees is combined with
each fragment rooted in the two hypotheses? trees.
Now, since the number of subtrees rooted in the
texts is independent of the number of trees rooted
in the hypotheses,
?(n
1
, n
2
, n
3
, n
4
) = ?(n
1
, n
2
)?(n
3
, n
4
).
Therefore, we can rewrite K
p
as:
K
p
(?T
1
,H
1
?, ?T
2
,H
2
?) =
=
?
n
1
?T
1
?
n
2
?T
2
?
n
3
?H
1
?
n
4
?H
2
?(n
1
, n
2
)?(n
3
, n
4
) =
=
?
n
1
?T
1
?
n
2
?T
2
?(n
1
, n
2
)
?
n
3
?H
1
?
n
4
?H
2
?(n
3
, n
4
) =
= K
t
(T
1
, T
2
)?K
t
(H
1
,H
2
).
(4)
This result shows that the natural kernel to rep-
resent textual entailment sentences is the kernel
product, which corresponds to the set of all possi-
ble syntactic fragment pairs. Note that, such kernel
can be also used to evaluate the space of fragment
pairs for trees enriched with relational information,
i.e. by placeholders.
4 Approximated Graph Kernel
The feature space described in the previous sec-
tion correctly encodes the fragment pairs. How-
ever, such huge space may result inadequate also
for algorithms such as SVMs, which are in general
robust to many irrelevant features. An approxima-
tion of the fragment pair space is given by the ker-
nel described in (Zanzotto and Moschitti, 2006).
Hereafter we illustrate its main points.
First, tree kernels applied to two texts or two hy-
potheses match identical fragments. When place-
holders are added to trees, the labeled fragments
28
are matched only if the basic fragments and the
assigned placeholders match. This means that
we should use the same placeholders for all texts
and all hypotheses of the corpus. Moreover, they
should be assigned in a way that similar syntac-
tic structures and similar relational information be-
tween two entailment pairs can be matched, i.e.
same placeholders should be assigned to the po-
tentially similar fragments.
Second, the above task cannot be carried out at
pre-processing time, i.e. when placeholders are
assigned to trees. At the running time, instead,
we can look at the comparing trees and make a
more consistent decision on the type and order of
placeholders. Although, there may be several ap-
proaches to accomplish this task, we apply a basic
heuristic which is very intuitive:
Choose the placeholder assignment that maxi-
mizes the tree kernel function over all possible cor-
respondences
More formally, let A and A? be the placeholder sets
of ?T,H? and ?T ?,H ??, respectively, without loss
of generality, we consider |A| ? |A?| and we align
a subset of A to A?. The best alignment is the one
that maximizes the syntactic and lexical overlap-
ping of the two subtrees induced by the aligned set
of anchors. By calling C the set of all bijective
mappings from S ? A, with |S| = |A?|, to A?,
an element c ? C is a substitution function. We
define the best alignment c
max
the one determined
by
c
max
= argmax
c?C
(TK(t(T, c), t(T
?
, i))+
TK(t(H, c), t(H
?
, i)),
where (1) t(?, c) returns the syntactic tree enriched
with placeholders replaced by means of the sub-
stitution c, (2) i is the identity substitution and (3)
TK(?
1
, ?
2
) is a tree kernel function (e.g. the one
specified by Eq. 2) applied to the two trees ?
1
and
?
2
.
At the same time, the desired similarity value
to be used in the learning algorithm is given
by the kernel sum: TK(t(T, c
max
), t(T
?
, i)) +
TK(t(H, c
max
), t(H
?
, i)), i.e. by solving the fol-
lowing optimization problem:
K
s
(?T,H?, ?T
?
,H
?
?) =
max
c?C
(TK(t(T, c), t(T
?
, i))+
TK(t(H, c), t(H
?
, i)),
(5)
For example, let us compare the following two
pairs (T
1
,H
1
) and (T
2
,H
2
) in Fig. 2.
To assign the placeholders 1 , 2 and 3 of
(T
2
,H
2
) to those of (T
1
,H
1
), i.e. X and Y , we
need to maximize the similarity between the two
texts? trees and between the two hypotheses? trees.
It is straightforward to derive that X=1 and Y=3 al-
low more substructures (i.e. large part of the trees)
to be identical, e.g. [S [NP 1 X VP]] , [VP [VBP
NP 3 Y ]], [S [NP 1 X VP [VBP NP 3 Y ]]].
Finally, it should be noted that, (a)
K
s
(?T,H?, ?T
?
,H
?
?) is a symmetric function
since the set of derivation C are always computed
with respect to the pair that has the largest anchor
set and (b) it is not a valid kernel as the max
function does not in general produce valid kernels.
However, in (Haasdonk, 2005), it is shown that
when kernel functions are not positive semidef-
inite like in this case, SVMs still solve a data
separation problem in pseudo Euclidean spaces.
The drawback is that the solution may be only a
local optimum. Nevertheless, such solution can
still be valuable as the problem is modeled with a
very rich feature space.
Regarding the computational complexity, run-
ning the above kernel on a large training set may
result very expensive. To overcome this drawback,
in (Moschitti and Zanzotto, 2007), it has been de-
signed an algorithm to factorize the evaluation of
tree subparts with respect to the different substitu-
tion. The resulting speed-up makes the application
of such kernel feasible for datasets of ten of thou-
sands of instances.
5 Experiments
The aim of the experiments is to show that the
space of tree fragment pairs is the most effective
to represent Tree Pair-based Graphs for the design
of Textual Entailment classifiers.
5.1 Experimental Setup
To compare our model with previous work we
implemented the following kernels in SVM-light
(Joachims, 1999):
? K
s
(e
1
, e
2
) = K
t
(T
1
, T
2
) + K
t
(H
1
,H
2
),
where e
1
= ?T
1
,H
1
? and e
2
= ?T
2
,H
2
?
are two text and hypothesis pairs and K
t
is
the syntactic tree kernel (Collins and Duffy,
2002) presented in the previous section.
? K
p
(e
1
, e
2
) = K
t
(T
1
, T
2
) ? K
t
(H
1
,H
2
),
which (as shown in the previous sections) en-
29
T1
? H
1
S
NP X
NNP X
Wanadoo
VP
VBP
bought
NP Y
NNP Y
KStones
S
NP X
NNP X
Wanadoo
VP
VBP
owns
NP Y
NNP Y
KStones
T
2
? H
2
S
NP 1
NP 1
DT
the
NN 1
president
PP 2
IN
of
NP 2
NNP 2
Miramax
VP
VBP
bought
NP 3
DT
a
NN 3
castle
S
NP 1
NP 1
DT
the
NN 1
president
PP 2
IN
of
NP 2
NNP 2
Miramax
VP
VBP
own
NP 3
DT
a
NN 3
castle
Figure 2: The problem of finding the correct mapping between placeholders
codes the tree fragment pairs with and with-
out placeholders.
? K
max
(e
1
, e
2
) = max
c?C
(
K
t
(?
c
(T
1
), ?
c
(T
2
))+
K
t
(?
c
(H
1
), ?
c
(H
2
))
)
, where c is a possi-
ble placeholder assignment which connects
nodes from the first pair with those of the sec-
ond pair and ?
c
(?) transforms trees according
to c.
? K
pmx
(e
1
, e
2
) = max
c?C
(
K
t
(?
c
(T
1
), ?
c
(T
2
))?
K
t
(?
c
(H
1
), ?
c
(H
2
))
)
.
Note that K
max
is the kernel proposed in (Zanzotto
and Moschitti, 2006) and K
pmx
is a hybrid kernel
based on the maximum K
p
, which uses the space
of tree fragment pairs. For all the above kernels,
we set the default cost factor and trade-off param-
eters and we set ? to 0.4.
To experiment with entailment relations, we
used the data sets made available by the first (Da-
gan et al, 2005) and second (Bar Haim et al, 2006)
Recognizing Textual Entailment Challenge. These
corpora are divided in the development sets D1
and D2 and the test sets T1 and T2. D1 contains
567 examples whereas T1, D2 and T2 all have the
same size, i.e. 800 instances. Each example is an
ordered pair of texts for which the entailment rela-
tion has to be decided.
5.2 Evaluation and Discussion
Table 1 shows the results of the above kernels
on the split used for the RTE competitions. The
first column reports the kernel model. The second
and third columns illustrate the model accuracy for
RTE1 whereas column 4 and 5 show the accuracy
for RTE2. Moreover, ? P indicates the use of stan-
dard syntactic trees and P the use of trees enriched
with placeholders. We note that:
First, the space of tree fragment pairs, gener-
ated by K
p
improves the one generated by K
s
(i.e.
the simple union of the fragments of texts and hy-
potheses) of 4 (58.9% vs 54.9%) and 0.9 (53.5%
vs 52.6%) points on RTE1 and RTE2, respectively.
This suggests that the fragment pairs are more ef-
fective for encoding the syntactic rules describing
the entailment concept.
Second, on RTE1, the introduction of placehold-
ers does not improve K
p
or K
s
suggesting that for
their correct exploitation an extension of the space
of tree fragment pairs should be modeled.
Third, on RTE2, the impact of placeholders
seems more important but only K
max
and K
s
are able to fully exploit their semantic contribu-
tion. A possible explanation is that in order to
use the set of all possible assignments (required by
K
max
), we needed to prune the ?too large? syntac-
tic trees as also suggested in (Zanzotto and Mos-
chitti, 2006). This may have negatively biased the
statistical distribution of tree fragment pairs.
Finally, although we show that K
p
is better
30
Kernels RTE1 RTE2
? P P ? P P
K
s
54.9 50.0 52.6 59.5
K
p
58.9 55.5 53.5 56.0
K
max
- 58.25 - 61.0
K
pmx
- 50.0 - 56.8
Table 1: Accuracy of different kernel models using
(P) and not using (? P) placeholder information on
RTE1 and RTE2.
suited for RTE than the other kernels, its accuracy
is lower than the state-of-the-art in RTE. This is be-
cause the latter uses additional models like the lex-
ical similarity between text and hypothesis, which
greatly improve accuracy.
6 Conclusion
In this paper, we have provided a statistical ma-
chine learning representation of textual entailment
via syntactic graphs constituted by tree pairs. We
have analytically shown that the natural way of
representing the syntactic relations between text
and hypothesis in learning algorithms consists in
the huge feature space of all possible syntactic tree
fragment pairs, which can only be managed using
kernel methods.
Therefore, we used tree kernels, which allow for
representing trees in terms of all possible subtrees.
More specifically, we defined a new model for the
entailment recognition problems, which requires
the definition of kernels over graphs constituted by
tree pairs. These are in general different from ker-
nels applied to single trees. We also studied an-
other alternative solution which concerns the use
of semantic links (placeholders) between text and
hypothesis parse trees (to form relevant semantic
fragment pairs) and the evaluation of two distinct
tree kernels for the trees of texts and for those of
hypotheses. In order to make such disjoint kernel
combination effective, all possible assignments be-
tween the placeholders of the first and the second
entailment pair have to be generated (causing a re-
markable slowdown).
Our experiments on the RTE datasets show that
our proposed kernel may provide higher accuracy
than the simple union of tree kernel spaces with a
much simpler and faster algorithm. Future work
will be devoted to make the tree fragment pair
space more effective, e.g. by using smaller and ac-
curate tree representation for text and hypothesis.
Acknowledgments
We would like to thank the anonymous reviewers
for their professional and competent reviews and
for their invaluable suggestions.
Alessandro Moschitti would like to thank the Eu-
ropean Union project, LUNA (spoken Language
UNderstanding in multilinguAl communication
systems) contract n 33549 for supporting part of
his research.
References
Bar Haim, Roy, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The II PASCAL RTE challenge.
In PASCAL Challenges Workshop, Venice, Italy.
Bos, Johan and Katja Markert. 2005. Recognising
textual entailment with logical inference. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 628?635, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Collins, Michael and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of ACL02.
Corley, Courtney and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proc. of the
ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 13?18, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL RTE challenge. In PASCAL
Challenges Workshop, Southampton, U.K.
de Salvo Braz, R., R. Girju, V. Punyakanok, D. Roth,
and M. Sammons. 2005. An inference model for se-
mantic entailment in natural language. In Proceed-
ings of AAAI, pages 1678?1679.
Giuglea, Ana-Maria and Alessandro Moschitti. 2006.
Semantic role labeling via framenet, verbnet and
propbank. In Proceedings of Coling-ACL, Sydney,
Australia.
Glickman, Oren and Ido Dagan. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In Proceedings of the Workshop on
Learning Methods for Text Understanding and Min-
ing, Grenoble, France.
Glickman, Oren, Ido Dagan, and Moshe Koppel. 2005.
Web based probabilistic textual entailment. In Pro-
ceedings of the 1st Pascal Challenge Workshop,
Southampton, UK.
31
Haasdonk, Bernard. 2005. Feature space interpretation
of SVMs with indefinite kernels. IEEE Trans Pat-
tern Anal Mach Intell, 27(4):482?92, Apr.
Joachims, Thorsten. 1999. Making large-scale svm
learning practical. In Schlkopf, B., C. Burges, and
A. Smola, editors, Advances in Kernel Methods-
Support Vector Learning. MIT Press.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41, November.
Moschitti, Alessandro and Cosmin Adrian Bejan.
2004. A semantic kernel for predicate argument
classification. In CoNLL-2004, USA.
Moschitti, A. and F. Zanzotto. 2007. Fast and effective
kernels for relational learning from texts. In Ghahra-
mani, Zoubin, editor, Proceedings of the 24th An-
nual International Conference on Machine Learning
(ICML 2007).
Moschitti, Alessandro, Daniele Pighin, and Roberto
Basili. 2006. Semantic Role Labeling via Tree Ker-
nel Joint Inference. In Proceedings of CoNLL-X.
Moschitti, Alessandro, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings ACL, Prague,
Czech Republic.
Moschitti, Alessandro. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ECML?06.
Zanzotto, Fabio Massimo and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In Proceedings of the
21st Coling and 44th ACL, pages 401?408, Sydney,
Australia, July.
32
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 67?76,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Syntactic and Semantic Structure for Opinion Expression Detection
Richard Johansson and Alessandro Moschitti
DISI, University of Trento
Via Sommarive 14 Povo, 38123 Trento (TN), Italy
{johansson, moschitti}@disi.unitn.it
Abstract
We demonstrate that relational features
derived from dependency-syntactic and
semantic role structures are useful for the
task of detecting opinionated expressions
in natural-language text, significantly im-
proving over conventional models based
on sequence labeling with local features.
These features allow us to model the way
opinionated expressions interact in a sen-
tence over arbitrary distances.
While the relational features make the pre-
diction task more computationally expen-
sive, we show that it can be tackled effec-
tively by using a reranker. We evaluate
a number of machine learning approaches
for the reranker, and the best model re-
sults in a 10-point absolute improvement
in soft recall on the MPQA corpus, while
decreasing precision only slightly.
1 Introduction
The automatic detection and analysis of opinion-
ated text ? subjectivity analysis ? is potentially
useful for a number of natural language processing
tasks. Examples include retrieval systems answer-
ing queries about how a particular person feels
about a product or political question, and various
types of market analysis tools such as review min-
ing systems.
A primary task in subjectivity analysis is to
mark up the opinionated expressions, i.e. the
text snippets signaling the subjective content of
the text. This is necessary for further analysis,
such as the determination of opinion holder and
the polarity of the opinion. The MPQA corpus
(Wiebe et al, 2005), a widely used corpus anno-
tated with subjectivity information, defines two
types of subjective expressions: direct subjective
expressions (DSEs), which are explicit mentions
of opinion, and expressive subjective elements
(ESEs), which signal the attitude of the speaker
by the choice of words. DSEs are often verbs of
statement and categorization, where the opinion
and its holder tend to be direct semantic arguments
of the verb. ESEs, on the other hand, are less easy
to categorize syntactically; prototypical examples
would include value-expressing adjectives such
as beautiful, biased, etc. In addition to DSEs and
ESEs, the MPQA corpus also contains annotation
for non-subjective statements, which are referred
to as objective speech events (OSEs). Examples
(1) and (2) show two sentences from the MPQA
corpus where DSEs and ESEs have been manually
annotated.
(1) For instance, he [denounced]DSE as a [human
rights violation]ESE the banning and seizure of
satellite dishes in Iran.
(2) This [is viewed]DSE as the [main
impediment]ESE to the establishment of po-
litical order in the country .
The task of marking up these expressions has
usually been approached using straightforward
sequence labeling techniques using simple fea-
tures in a small contextual window (Choi et al,
2006; Breck et al, 2007). However, due to the
simplicity of the feature sets, this approach fails
to take into account the fact that the semantic
and pragmatic interpretation of sentences is not
only determined by words but also by syntactic
and shallow-semantic relations. Crucially, taking
grammatical relations into account allows us to
model how expressions interact in various ways
that influence their interpretation as subjective
or not. Consider, for instance, the word said in
examples (3) and (4) below, where the interpre-
tation as a DSE or an OSE is influenced by the
subjective content of the enclosed statement.
67
(3) ?We will identify the [culprits]ESE of these
clashes and [punish]ESE them,? he [said]DSE .
(4) On Monday, 80 Libyan soldiers disembarked
from an Antonov transport plane carrying military
equipment, an African diplomat [said]OSE .
In this paper, we demonstrate how syntactic
and semantic structural information can be used
to improve opinion detection. While this fea-
ture model makes it impossible to use the stan-
dard sequence labeling method, we show that with
a simple strategy based on reranking, incorporat-
ing structural features results in a significant im-
provement. We investigate two different reranking
strategies: the Preference Kernel approach (Shen
and Joshi, 2003) and an approach based on struc-
ture learning (Collins, 2002). In an evaluation
on the MPQA corpus, the best system we evalu-
ated, a structure learning-based reranker using the
Passive?Aggressive learning algorithm, achieved
a 10-point absolute improvement in soft recall,
and a 5-point improvement in F-measure, over the
baseline sequence labeler .
2 Motivation and Related Work
Most approaches to analysing the sentiment of
natural-language text have relied fundamentally
on purely lexical information (see (Pang et al,
2002; Yu and Hatzivassiloglou, 2003), inter alia)
or low-level grammatical information such as part-
of-speech tags and functional words (Wiebe et al,
1999). This is in line with the general consensus
in the information retrieval community that very
little can be gained by complex linguistic process-
ing for tasks such as text categorization and search
(Moschitti and Basili, 2004).
However, it has been suggested that subjectiv-
ity analysis is inherently more subtle than cate-
gorization and that structural linguistic informa-
tion should therefore be given more attention in
this context. For instance, Karlgren et al (2010)
argued from a Construction Grammar viewpoint
(Croft, 2005) that grammatical constructions not
only connect words, but can also be viewed as lex-
ical items in their own right. Starting from this
intuition, they showed that incorporating construc-
tion items into a bag-of-words feature representa-
tion resulted in improved results on a number of
coarse-grained opinion analysis tasks. These con-
structional features were domain-independent and
were manually extracted from dependency parse
trees. They found that the most prominent con-
structional feature for subjectivity analysis was the
Tense Shift construction.
While the position by Karlgren et al (2010)
? that constructional features signal opinion ?
originates from a particular theoretical framework
and may be controversial, syntactic and shallow-
semantic relations have repeatedly proven useful
for subtasks of subjectivity analysis that are in-
herently relational, above all for determining the
holder or topic of a given opinion. Works us-
ing syntactic features to extract topics and holders
of opinions are numerous (Bethard et al, 2005;
Kobayashi et al, 2007; Joshi and Penstein-Rose?,
2009; Wu et al, 2009). Semantic role analysis has
also proven useful: Kim and Hovy (2006) used
a FrameNet-based semantic role labeler to deter-
mine holder and topic of opinions. Similarly, Choi
et al (2006) successfully used a PropBank-based
semantic role labeler for opinion holder extrac-
tion, and Wiegand and Klakow (2010) recently ap-
plied tree kernel learning methods on a combina-
tion of syntactic and semantic role trees for the
same task. Ruppenhofer et al (2008) argued that
semantic role techniques are useful but not com-
pletely sufficient for holder and topic identifica-
tion, and that other linguistic phenomena must be
studied as well. One such linguistic pheonomenon
is the discourse structure, which has recently at-
tracted some attention in the opinion analysis com-
munity (Somasundaran et al, 2009).
3 Opinion Expression Detection Using
Syntactic and Semantic Structures
Previous systems for opinionated expression
markup have typically used simple feature sets
which have allowed the use of efficient off-the-
shelf sequence labeling methods based on Viterbi
search (Choi et al, 2006; Breck et al, 2007). This
is not possible in our case since we would like to
extract structural, relational features that involve
pairs of opinionated expressions and may apply
over an arbitrarily long distance in the sentence.
While it is possible that search algorithms for
exact or approximate inference can be construc-
tured for the arg max problem in this model, we
sidestepped this issue by using a reranking decom-
position of the problem: We first apply a standard
Viterbi-based sequence labeler using no structural
features and generate a small candidate set of size
k. Then, a second and more complex model picks
68
the top candidate from this set without having to
search the whole candidate space.
The advantages of a reranking approach com-
pared to more complex approaches requiring ad-
vanced search techniques are mainly simplicity
and efficiency: this approach is conceptually sim-
ple and fairly easy to implement provided that k-
best output can be generated efficiently, and fea-
tures can be arbitrarily complex ? we don?t have
to think about how the features affect the algorith-
mic complexity of the inference step. A common
objection to reranking is that the candidate set may
not be diverse enough to allow for much improve-
ment unless it is very large; the candidates may
be trivial variations that are all very similar to the
top-scoring candidate (Huang, 2008).
3.1 Syntactic and Semantic Structures
We used the syntactic?semantic parser by Johans-
son and Nugues (2008a) to annnotate the sen-
tences with dependency syntax (Mel?c?uk, 1988)
and shallow semantic structures in the PropBank
(Palmer et al, 2005) and NomBank (Meyers et
al., 2004) frameworks. Figure 1 shows an example
of the annotation: The sentence they called him a
liar, where called is a DSE and liar is an ESE, has
been annotated with dependency syntax (above the
text) and PropBank-based semantic role structure
(below the text). The predicate called, which is
an instance of the PropBank frame call.01, has
three semantic arguments: the Agent (A0), the
Theme (A1), and the Predicate (A2), which are re-
alized on the surface-syntactic level as a subject,
a direct object, and an object predicative comple-
ment, respectively.
]
ESE
They called
call.01
SBJ
OPRD
liarhim[ [a
A1A0 A2
]
DSE
NMODOBJ
Figure 1: Syntactic and shallow semantic struc-
ture.
3.2 Sequence Labeler
We implemented a standard sequence labeler fol-
lowing the approach of Collins (2002), while
training the model using the Passive?Aggressive
algorithm (Crammer et al, 2006) instead of the
perceptron. We encoded the opinionated expres-
sion brackets using the IOB2 encoding scheme
(Tjong Kim Sang and Veenstra, 1999). Figure 2
shows an example of a sentence with a DSE and
an ESE and how they are encoded in the IOB2 en-
coding.
This O
is O
viewed B-DSE
as O
the O
main B-ESE
impediment I-ESE
Figure 2: Sequence labeling example.
The sequence labeler used word, POS tag, and
lemma features in a window of size 3. In addi-
tion, we used prior polarity and intensity features
derived from the lexicon created by Wilson et al
(2005). In the example, viewed is listed as hav-
ing strong prior subjectivity but no polarity, and
impediment has strong prior subjectivity and neg-
ative polarity. Note that prior subjectivity does not
always imply subjectivity in a particular context;
this is why contextual features are essential for this
task.
This sequence labeler is used to generate the
candidate set for the reranker; the Viterbi algo-
rithm is easily modified to give k-best output. To
generate training data for the reranker, we carried
out a 5-fold cross-validation procedure: We split
the training set into 5 pieces, trained a sequence
labeler on pieces 1 to 4, applied it to piece 5 and
so on.
3.3 Reranker Features
The rerankers use two types of structural fea-
tures: syntactic features extracted from the depen-
dency tree, and semantic features extracted from
the predicate?argument (semantic role) graph.
The syntactic features are based on paths
through the dependency tree. This creates a small
complication for multiword opinionated expres-
sions; we select the shortest possible path in such
cases. For instance, in Example (1), the path will
be computed between denounced and violation,
and in Example (2) between viewed and impedi-
ment.
We used the following syntactic features:
69
SYNTACTIC PATH. Given a pair of opinion ex-
pressions, we use a feature representing the
labels of the two expressions and the path be-
tween them through the syntactic tree. For
instance, for the DSE called and the ESE liar
in Figure 1, we represent the syntactic config-
uration using the feature DSE:OPRD?:ESE,
meaning that the path from the DSE to the
ESE consists of a single link, where the de-
pendency edge label is OPRD (object predica-
tive complement).
LEXICALIZED PATH. Same as above,
but with lexical information attached:
DSE/called:OPRD?:ESE/liar.
DOMINANCE. In addition to the features based
on syntactic paths, we created a more generic
feature template describing dominance re-
lations between expressions. For instance,
from the graph in Figure 1, we extract the
feature DSE/called?ESE/liar, mean-
ing that a DSE with the word called domi-
nates an ESE with the word liar.
The semantic features were the following:
PREDICATE SENSE LABEL. For every predi-
cate found inside an opinion expression, we
add a feature consisting of the expression la-
bel and the predicate sense identifier. For in-
stance, the verb call which is also a DSE is
represented with the feature DSE/call.01.
PREDICATE AND ARGUMENT LABEL. For
every argument of a predicate inside an
opinion expression, we create a feature
representing the predicate?argument pair:
DSE/call.01:A0.
CONNECTING ARGUMENT LABEL. When a
predicate inside some opinion expression is
connected to some argument inside another
opinion expression, we use a feature con-
sisting of the two expression labels and the
argument label. For instance, the ESE liar
is connected to the DSE call via an A2 la-
bel, and we represent this using a feature
DSE:A2:ESE.
Apart from the syntactic and semantic features,
we also used the score output from the base se-
quence labeler as a feature. We normalized the
scores over the k candidates so that their exponen-
tials summed to 1.
3.4 Preference Kernel Approach
The first reranking strategy we investigated was
the Preference Kernel approach (Shen and Joshi,
2003). In this method, the reranking problem ?
learning to select the correct candidate h1 from a
candidate set {h1, . . . , hk} ? is reduced to a bi-
nary classification problem by creating pairs: pos-
itive training instances ?h1, h2?, . . . , ?h1, hk? and
negative instances ?h2, h1?, . . . , ?hk, h1?. This ap-
proach has the advantage that the abundant tools
for binary machine learning can be exploited.
It is also easy to show (Shen and Joshi, 2003)
that if we have a kernel K over the candidate space
T , we can construct a valid kernel PK over the
space of pairs T ? T as follows:
PK(h1, h2) = K(h
1
1
, h1
2
) + K(h2
1
, h2
2
)
? K(h1
1
, h2
2
) ? K(h2
1
, h1
2
),
where hi are the pairs of hypotheses ?h1i , h
2
i ? gen-
erated by the base model. This makes it possible
to use kernel methods to train the reranker. We
tried two types of kernels: linear kernels and tree
kernels.
3.4.1 Linear Kernel
We created feature vectors extracted from the can-
didate sequences using the features described in
Section 3.3. We then trained linear SVMs using
the LIBLINEAR software (Fan et al, 2008), using
L1 loss and L2 regularization.
3.4.2 Tree Kernel
Tree kernels have been successful for a number of
structure extraction tasks, such as relation extrac-
tion (Zhang et al, 2006; Nguyen et al, 2009) and
opinion holder extraction (Wiegand and Klakow,
2010). A tree kernel implicitly represents a large
space of fragments extracted from trees and could
thus reduce the need for manual feature design.
Since the paths that we extract manually (Sec-
tion 3.3) can be expressed as tree fragments, this
method could be an interesting alternative to the
manually extracted features used with the linear
kernel.
We therefore implemented a reranker using
the Partial Tree Kernel (Moschitti, 2006), and
we trained it using the SVMLight-TK software1,
which is a modification of SVMLight (Joachims,
1Available at http://dit.unitn.it/?moschitt
70
1999)2. It is still an open question how depen-
dency trees should be represented for use with
tree kernels (Suzuki et al, 2003; Nguyen et al,
2009); we used the representation shown in Fig-
ure 3. Note that we have concatenated the opinion
expression labels to the POS tag nodes. We did not
use any of the features from Section 3.3 except for
the base sequence labeler score.
TOP
ROOT
OBJSBJ
PRP
they him
OPRD
PRP
NMOD
DT
NN?ES
VBD?DS
called
a
liar
Figure 3: Representation of a dependency tree
with opinion expressions for tree kernels.
3.5 Structure Learning Approach
The Preference Kernel approach reduces the
reranking problem to a binary classification task
on pairs, after which a standard SVM optimizer is
used to train the reranker. A problem with this
method is that the optimization problem solved
by the SVM ? maximizing the classification ac-
curacy on a set of independent pairs ? is not di-
rectly related to the performance of the reranker.
Instead, the method employed by many rerankers
following Collins and Duffy (2002) directly learn
a scoring function that is trained to maximize per-
formance on the reranking task. We will refer to
this approach as the structure learning method.
While there are batch learning algorithms that
work in this setting (Tsochantaridis et al, 2005),
online learning methods have been more popular
for efficiency reasons. We investigated two online
learning algorithms: the popular structured per-
ceptron Collins and Duffy (2002) and the Passive?
Aggressive (PA) algorithm (Crammer et al, 2006).
To increase robustness, we averaged the weight
vectors seen during training as in the Voted Per-
ceptron (Freund and Schapire, 1999).
The difference between the two algorithms is
the way the weight vector is incremented in each
step. In the perceptron, for a given input x, we up-
date based on the difference between the correct
2http://svmlight.joachims.org
output y and the predicted output y?, where ? is
the feature representation function:
y? ? arg maxh w ? ?(x, h)
w ? w + ?(x, y) ? ?(x, y?)
In the PA algorithm, which is based on the the-
ory of large-margin learning, we instead find the
y? that violates the margin constraints maximally.
The update step length ? is computed based on the
margin; this update is bounded by a regularization
constant C:
y? ? arg maxh w ? ?(x, h) +
?
?(y, h)
? ? min
(
C,
w(?(x,y?)??(x,y))+
?
?(y,y?)
??(x,y?)??(x,y)?2
)
w ? w + ?(?(x, y) ? ?(x, y?))
The algorithm uses a cost function ?. We used
the function ?(y, y?) = 1 ? F (y, y?), where F is
the soft F-measure described in Section 4.1. With
this approach, the learning algorithm thus directly
optimizes the measure we are interested in, i.e. the
F-measure.
4 Experiments
We carried out the experiments on version 2 of the
MPQA corpus (Wiebe et al, 2005), which we split
into a test set (150 documents, 3,743 sentences)
and a training set (541 documents, 12,010 sen-
tences).
4.1 Evaluation Metrics
Since expression boundaries are hard to define ex-
actly in annotation guidelines (Wiebe et al, 2005),
we used soft precision and recall measures to score
the quality of the system output. To derive the soft
precision and recall, we first define the span cov-
erage c of a span s with respect to another span s?,
which measures how well s? is covered by s:
c(s, s?) =
|s ? s?|
|s?|
In this formula, the operator | ? | counts tokens, and
the intersection ? gives the set of tokens that two
spans have in common. Since our evaluation takes
span labels (DSE, ESE, OSE) into account, we set
c(s, s?) to zero if the labels associated with s and
s? are different.
Using the span coverage, we define the span set
coverage C of a set of spans S with respect to a
set S?:
C(S,S?) =
?
s
j
?S
?
s?
k
?S
?
c(sj, s?k)
71
We now define the soft precision P and recall R
of a proposed set of spans ?S with respect to a gold
standard set S as follows:
P (S, ?S) = C(S,
?
S)
|
?
S|
R(S, ?S) = C(
?
S,S)
|S|
Note that the operator | ? | counts spans in this for-
mula.
Conventionally, when measuring the quality of
a system for an information extraction task, a pre-
dicted entity is counted as correct if it exactly
matches the boundaries of a corresponding en-
tity in the gold standard; there is thus no reward
for close matches. However, since the boundaries
of the spans annotated in the MPQA corpus are
not strictly defined in the annotation guidelines
(Wiebe et al, 2005), measuring precision and re-
call using exact boundary scoring will result in fig-
ures that are too low to be indicative of the use-
fulness of the system. Therefore, most work us-
ing this corpus instead use overlap-based preci-
sion and recall measures, where a span is counted
as correctly detected if it overlaps with a span in
the gold standard (Choi et al, 2006; Breck et al,
2007). As pointed out by Breck et al (2007), this
is problematic since it will tend to reward long
spans ? for instance, a span covering the whole
sentence will always be counted as correct if the
gold standard contains any span for that sentence.
The precision and recall measures proposed
here correct the problem with overlap-based mea-
sures: If the system proposes a span covering the
whole sentence, the span coverage will be low and
result in a low soft precision. Note that our mea-
sures are bounded below by the exact measures
and above by the overlap-based measures.
4.2 Reranking Approaches
We compared the reranking architectures and the
machine learning methods described in Section 3.
In these experiments, we used a candidate set size
k of 8. Table 1 shows the results of the evaluations
using the precision and recall measures described
above. The baseline is the result of taking the top-
scoring output from the sequence labeler without
applying any reranking.
The results show that the rerankers using man-
ual feature extraction outperform the tree-kernel-
based reranker, which obtains a score just above
the baseline. It should be noted that the mas-
sive training time of kernel-based machine learn-
ing precluded a detailed tuning of parameters and
System P R F
Baseline 63.36 46.77 53.82
Pref-linear 64.60 50.17 56.48
Pref-TK 63.97 46.94 54.15
Struct-Perc 62.84 48.13 54.51
Struct-PA 63.50 51.79 57.04
Table 1: Evaluation of reranking architectures and
learning methods.
representation ? on the other hand, we did not need
to spend much time on parameter tuning and fea-
ture design for the other rerankers.
In addition, we note that the best performance
was obtained using the PA algorithm and the struc-
ture learning architecture. The PA algorithm is
a simple online learning method and still out-
performs the SVM used in the preference-kernel
reranker. This suggests that the structure learning
approach is superior for this task. It is possible
that a batch learning method such as SVMstruct
(Tsochantaridis et al, 2005) could improve the re-
sults even further.
4.3 Candidate Set Size
In any method based on reranking, it is important
to study the influence of the candidate set size on
the quality of the reranked output. In addition, an
interesting question is what the upper bound on
reranker performance is ? the oracle performance.
Table 2 shows the result of an experiment that in-
vestigates these questions. We used the reranker
based on the Passive?Aggressive method in this
experiment since this reranker gave the best results
in the previous experiment.
Reranked Oracle
k P R F P R F
1 63.36 46.77 53.82 63.36 46.77 53.82
2 63.70 48.17 54.86 72.66 55.18 62.72
4 63.57 49.78 55.84 79.12 62.24 69.68
8 63.50 51.79 57.04 83.72 68.14 75.13
16 63.00 52.94 57.54 86.92 72.79 79.23
32 62.15 54.50 58.07 89.18 76.76 82.51
64 61.02 55.67 58.22 91.08 80.19 85.28
128 60.22 56.45 58.27 92.63 83.00 87.55
256 59.87 57.22 58.51 94.01 85.27 89.43
Table 2: Oracle and reranker performance as a
function of candidate set size.
As is common in reranking tasks, the reranker
can exploit only a fraction of the potential im-
provement ? the reduction of the F-measure error
72
is between 10 and 15 percent of the oracle error
reduction for all candidate set sizes.
The most visible effect of the reranker is that
the recall is greatly improved. However, this does
not seem to have an adverse effect on the precision
until the candidate set size goes above 8 ? in fact,
the precision actually improves over the baseline
for small candidate set sizes. After the size goes
above 8, the recall (and the F-measure) still rises,
but at the cost of decreased precision.
4.4 Impact of Features
We studied the impact of syntactic and seman-
tic structural features on the performance of the
reranker. Table 3 shows the result of the inves-
tigation for syntactic features. Using all the syn-
tactic features (and no semantic features) gives an
F-measure roughly 4 points above the baseline, us-
ing the PA reranker with a k of 64. We then mea-
sured the F-measure obtained when each one of
the three syntactic features had been removed. It
is clear that the unlexicalized syntactic path is the
most important syntactic feature; the effect of the
two lexicalized features seems to be negligible.
System P R F
Baseline 63.36 46.77 53.82
All syntactic 62.45 53.19 57.45
No SYN PATH 64.40 48.69 55.46
No LEX PATH 62.62 53.19 57.52
No DOMINANCE 62.32 52.92 57.24
Table 3: Effect of syntactic features.
A similar result was obtained when studying the
semantic features (Table 4). Removing the CON-
NECTING ARGUMENT LABEL feature, which is
unlexicalized, has a greater effect than removing
the other two semantic features, which are lexical-
ized.
System P R F
Baseline 63.36 46.77 53.82
All semantic 61.26 53.85 57.31
No PREDICATE SL 61.28 53.81 57.30
No PRED+ARGLBL 60.96 53.61 57.05
No CONN ARGLBL 60.73 50.47 55.12
Table 4: Effect of semantic features.
Since our most effective structural features
combine a pair of opinion expression labels with
a tree fragment, it is interesting to study whether
the expression labels alone would be enough. If
this were the case, we could conclude that the
improvement is caused not by the structural fea-
tures, but just by learning which combinations
of labels are common in the training set, such
as that DSE+ESE would be more common than
OSE+ESE. We thus carried out an experiment
comparing a reranker using label pair features
against rerankers based on syntactic features only,
semantic features only, and the full feature set. Ta-
ble 5 shows the results. We see that the reranker
using label pairs indeed achieves a performance
well above the baseline. However, its performance
is below that of any reranker using structural fea-
tures. In addition, we see no improvement when
adding label pair features to the structural feature
set; this is to be expected since the label pair infor-
mation is subsumed by the structural features.
System P R F
Baseline 63.36 46.77 53.82
Label pairs 62.05 52.68 56.98
All syntactic 62.45 53.19 57.45
All semantic 61.26 53.85 57.31
Syn + sem 61.02 55.67 58.22
Syn + sem + pairs 61.61 54.78 57.99
Table 5: Structural features compared to label
pairs.
4.5 Comparison with Breck et al (2007)
Comparison of systems in opinion expression de-
tection is often nontrivial since evaluation settings
have differed widely. Since our problem setting
? marking up and labeling opinion expressions in
the MPQA corpus ? is most similar to that de-
scribed by Breck et al (2007), we carried out an
evaluation using the setting used in their experi-
ment.
For compatibility with their experimental setup,
this experiment differed from the ones described
in the previous sections in the following ways:
? The system did not need to distinguish DSEs
and ESEs and did not have to detect the
OSEs.
? The results were measured using the overlap-
based precision and recall, although this is
problematic as pointed out in Section 4.1.
73
? Instead of the training/test split we used in the
previous evaluations, the systems were evalu-
ated using a 10-fold cross-validation over the
same set of 400 documents as used in Breck?s
experiment.
Again, our reranker uses the PA method with a
k of 64. Table 6 shows the results.
System P R F
Breck et al (2007) 71.64 74.70 73.05
Baseline 80.85 64.38 71.68
Reranked 76.40 78.23 77.30
Table 6: Results using the Breck et al (2007) eval-
uation setting.
We see that the performance of our system is
clearly higher ? in both precision and recall ? than
that reported by Breck et al (2007). This shows
again that the structural features are effective for
the task of finding opinionated expressions.
We note that the performance of our base-
line sequence labeler is lower than theirs; this
is to be expected since they used a more com-
plex batch learning algorithm (conditional random
fields) while we used an online learner, and they
spent more effort on feature design. This indicates
that we should be able to achieve even higher per-
formance using a stronger base model.
5 Conclusion
We have shown that features derived from gram-
matical and semantic role structure can be used to
improve the detection of opinionated expressions
in subjectivity analysis. Most significantly, the re-
call is drastically increased (10 points) while the
precision decreases only slightly (3 points). This
result compares favorably with previously pub-
lished results, which have been biased towards
precision and scored low on recall.
The long-distance structural features gives us a
model that has predictive power as well as being of
theoretical interest: this model takes into account
the interactions between opinion expressions in a
sentence. While these structural features give us
a powerful model, they come at a computational
cost; prediction is more complex than in a stan-
dard sequence labeler based on purely local fea-
tures. However, we have shown that a prediction
strategy based on reranking suffices for this task.
We analyzed the impact of the syntactic and se-
mantic features and saw that the best model in-
cludes both types of features. The most effective
features we have found are purely structural, i.e.
based on tree fragments in a syntactic or seman-
tic tree. Features involving words did not seem to
have the same impact. We also showed that the im-
provement is not explainable by mere correlations
between opinion expression labels.
We investigated a number of implementation
strategies for the reranker and concluded that the
structural learning framework seemed to give the
best performance. We were not able to achieve
the same performance using tree kernels as with
manually extracted features. It is possible that this
could be improved with a better strategy for rep-
resenting dependency structure for tree kernels, or
if the tree kernels could be incorporated into the
structural learning framework.
The flexible architecture we have presented en-
ables interesting future research: (i) a straight-
forward improvement is the use of lexical simi-
larity to reduce data sparseness, e.g. (Basili et
al., 2005; Basili et al, 2006; Bloehdorn et al,
2006). However, the similarity between subjective
words, which have multiple senses against other
words may negatively impact the system accu-
racy. Therefore, the use of the syntactic/semantic
kernels, i.e. (Bloehdorn and Moschitti, 2007a;
Bloehdorn and Moschitti, 2007b), to syntactically
contextualize word similarities may improve the
reranker accuracy. (ii) The latter can be fur-
ther boosted by studying complex structural ker-
nels, e.g. (Moschitti, 2008; Nguyen et al, 2009;
Dinarelli et al, 2009). (iii) More specific pred-
icate argument structures such those proposed in
FrameNet, e.g. (Baker et al, 1998; Giuglea and
Moschitti, 2004; Giuglea and Moschitti, 2006; Jo-
hansson and Nugues, 2008b) may be useful to
characterize the opinion holder and the sentence
semantic context.
Finally, while the strategy based on reranking
resulted in a significant performance boost, it re-
mains to be seen whether a higher accuracy can
be achieved by developing a more sophisticated
inference algorithm based on dynamic program-
ming. However, while the development of such
an algorithm is an interesting problem, it will not
necessarily result in a more usable system ? when
using a reranker, it is easy to trade accuracy for
efficiency.
74
Acknowledgements
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement 231126: LivingKnowledge ?
Facts, Opinions and Bias in Time, and from Trust-
worthy Eternal Systems via Evolving Software,
Data and Knowledge (EternalS, project number
FP7 247758). In addition, we would like to thank
Eric Breck for clarifying his results and experi-
mental setup.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING/ACL-1998.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet seman-
tics via kernel-based learning. In Proceedings of
CoNLL-2005, pages 1?8, Ann Arbor, Michigan.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. In in Informatica, an in-
ternational journal of Computing and Informatics.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extract-
ing opinion propositions and opinion holders using
syntactic and lexical cues. In James G. Shanahan,
Yan Qu, and Janyce Wiebe, editors, Computing Atti-
tude and Affect in Text: Theory and Applications.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Proceedings of ECIR 2007, Rome,
Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In In Proceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of ICDM 06,
Hong Kong, 2006.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI-2007, Hyderabad, India.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of EMNLP 2006.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. InPro-
ceedings of ACL?02.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2002),
pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 2006(7):551?585.
William Croft. 2005. Radical and typological argu-
ments for radical construction grammar. In J.-O.
O?stman and M. Fried, editors, Construction Gram-
mars: Cognitive grounding and theoretical exten-
sions.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-ranking models based-on small
training data for spoken language understanding.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1076?1085, Singapore, August.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovering using FrameNet, VerbNet
and PropBank. In In Proceedings of the Workshop
on Ontology and Knowledge Discovering at ECML
2004, Pisa, Italy.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic role labeling via FrameNet, VerbNet and
PropBank. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 929?936, Sydney, Aus-
tralia, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, United
States.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods ?
Support Vector Learning, 13.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic?semantic analysis with
PropBank and NomBank. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Natural
Language Learning, pages 183?187, Manchester,
United Kingdom.
75
Richard Johansson and Pierre Nugues. 2008b. The
effect of syntactic representation on semantic role
labeling. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 393?400, Manchester, UK.
Mahesh Joshi and Carolyn Penstein-Rose?. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of ACL/IJCNLP 2009, Short Papers
Track.
Jussi Karlgren, Gunnar Eriksson, Magnus Sahlgren,
and Oscar Ta?ckstro?m. 2010. Between bags
and trees ? constructional patterns in text used for
attitude identification. In Proceedings of ECIR
2010, 32nd European Conference on Information
Retrieval, Milton Keynes, United Kingdom.
Soo-Min Kim and Eduard Hovy. 2006. Extract-
ing opinions, opinion holders, and topics expressed
in online news media text. In Proceedings of
ACL/COLING Workshop on Sentiment and Subjec-
tivity in Text.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-CoNLL-2007).
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University Press of New York,
Albany.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, United States.
Alessandro Moschitti and Roberto Basili. 2004. Com-
plex linguistic features for text classification: A
comprehensive study. In Proceedings of ECIR.
AlessandroMoschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proccedings
of EACL?06.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
Proceeding of CIKM ?08, NY, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In Proceedings of EMNLP.
Martha Palmer, DanGildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1):71?
105.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP.
Josef Ruppenhofer, Swapna Somasundaran, and Janyce
Wiebe. 2008. Finding the sources and targets of
subjective expressions. In Proceedings of LREC.
Libin Shen and Aravind Joshi. 2003. An SVM based
voting algorithmwith application to parse reranking.
In Proceedings of the CoNLL.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of EMNLP 2009: conference on Em-
pirical Methods in Natural Language Processing.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku
Maeda. 2003. Hierarchical directed acyclic graph
kernel: Methods for structured natural language
data. In Proceedings of the 41th Annual Meeting of
Association for Computational Linguistics (ACL).
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of EACL99,
pages 173?179, Bergen, Norway.
Iannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6(Sep):1453?1484.
Janyce Wiebe, Rebecca Bruce, and Thomas O?Hara.
1999. Development and use of a gold standard data
set for subjectivity classifications. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Michael Wiegand and Dietrich Klakow. 2010. Con-
volution kernels for opinion holder extraction. In
Proceedings of HLT-NAACL 2010. To appear.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLT/EMNLP 2005.
YuanbinWu, Qi Zhang, XuanjingHuang, and LideWu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of EMNLP.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003), pages 129?136, Sapporo, Japan.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring
Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
76
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 223?233,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
On Reverse Feature Engineering of Syntactic Tree Kernels
Daniele Pighin
FBK-irst, DISI, University of Trento
Via di Sommarive, 14
I-38123 Povo (TN) Italy
daniele.pighin@gmail.com
Alessandro Moschitti
DISI, University of Trento
Via di Sommarive, 14
I-38123 Povo (TN) Italy
moschitti@disi.unitn.it
Abstract
In this paper, we provide a theoretical
framework for feature selection in tree ker-
nel spaces based on gradient-vector com-
ponents of kernel-based machines. We
show that a huge number of features can
be discarded without a significant decrease
in accuracy. Our selection algorithm is as
accurate as and much more efficient than
those proposed in previous work. Com-
parative experiments on three interesting
and very diverse classification tasks, i.e.
Question Classification, Relation Extrac-
tion and Semantic Role Labeling, support
our theoretical findings and demonstrate
the algorithm performance.
1 Introduction
Kernel functions are very effective at modeling
diverse linguistic phenomena by implicitly rep-
resenting data in high dimensional spaces, e.g.
(Cumby and Roth, 2003; Culotta and Sorensen,
2004; Kudo et al, 2005; Moschitti et al, 2008).
However, the implicit nature of the kernel space
causes two major drawbacks: (1) high computa-
tional costs for learning and classification, and (2)
the impossibility to identify the most important
features. A solution to both problems is the ap-
plication of feature selection techniques.
In particular, the problem of feature selection
in Tree Kernel (TK) spaces has already been ad-
dressed by previous work in NLP, e.g. (Kudo
and Matsumoto, 2003; Suzuki and Isozaki, 2005).
However, these approaches lack a theoretical char-
acterization of the problem that could support and
justify the design of more effective algorithms.
In (Pighin and Moschitti, 2009a) and (Pighin
and Moschitti, 2009b) (P&M), we presented a
heuristic framework for feature selection in kernel
spaces that selects features based on the compo-
nents of the weight vector, ~w, optimized by Sup-
port Vector Machines (SVMs). This method ap-
pears to be very effective, as the model accuracy
does not significantly decrease even when a large
number of features are filtered out. Unfortunately,
we could not provide theoretical or intuitive moti-
vations to justify our proposed apporach.
In this paper, we present and empirically val-
idate a theory which aims at filling the above-
mentioned gaps. In particular we provide: (i) a
proof of the equation for the exact computation of
feature weights induced by TK functions (Collins
and Duffy, 2002); (ii) a theoretical characteriza-
tion of feature selection based on ?~w?. We show
that if feature selection does not sensibly reduces
?~w?, the margin associated with ~w does not sen-
sibly decrease as well. Consequently, the theoret-
ical upperbound to the probability error does not
sensibly increases; (iii) a proof that the convolu-
tive nature of TK allows for filtering out an expo-
nential number of features with a small ?~w? de-
crease. The combination of (ii) with (iii) suggests
that an extremely aggressive feature selection can
be applied. We describe a greedy algorithm that
exploits these results. Compared to the one pro-
posed in P&M, the new version of the algorithm
has only one parameter (instead of 3), it is more
efficient and can be more easily connected with the
amount of gradient norm that is lost after feature
selection.
In the remainder: Section 2 briefly reviews
SVMs and TK functions; Section 3 describes the
problem of selecting and projecting features from
very high onto lower dimensional spaces, and pro-
vides the theoretical foundation to our approach;
Section 4 presents a selection of related work; Sec-
tion 5 describes our approach to tree fragment se-
lection; Section 6 details the outcome of our ex-
periments; finally, in Section 7 we draw our con-
clusions.
223
2 Fragment Weights in TK Spaces
The critical step for feature selection in tree ker-
nel spaces is the computation of the weights of
features (tree fragments) in the kernel machines?
gradient. The basic parameters are the fragment
frequencies which are combined with a decay fac-
tor used to downscale the weight of large sub-
trees (Collins and Duffy, 2002). In this section, af-
ter introducing basic kernel concepts, we describe
a theorem that establishes the correct weight1 of
features in the STK space.
2.1 Kernel Based-Machines
Typically, a kernel machine is a linear classifier
whose decision function can be expressed as:
c(~x) = ~w ? ~x+ b =
?`
i=1
?iyi ~xi ? ~x+ b (1)
where ~x ? <N is a classifying example and
~w ? <N and b ? < are the separating hyper-
plane?s gradient and its bias, respectively. The
gradient is a linear combination of ` training
points ~xi ? <N multiplied by their labels
yi ? {?1,+1} and their weights ?i ? <+.
Different optimizers use different strategies to
learn the gradient. For instance, an SVM learns
to maximize the distance between positive and
negative examples, i.e. the margin ?. Applying
the so-called kernel trick, it is possible to replace
the scalar product with a kernel function defined
over pairs of objects, which can more efficiently
compute it:
c(o) =
?`
i=1
?iyik(oi, o) + b,
where k(oi, o) = ?(oi) ? ?(o), with the advantage
that we do not need to provide an explicit mapping
? : O ? <N of our example objects O in a vec-
tor space. In the next section, we show a kernel
directly working on syntactic trees.
2.2 Syntactic Tree Kernel (STK)
Tree Kernel (TK) functions are convolution ker-
nels (Haussler, 1999) defined over pairs of trees.
Different TKs are characterized by alternative
fragment definitions, e.g. (Collins and Duffy,
2002; Kashima and Koyanagi, 2002; Moschitti,
2006). We will focus on the syntactic tree kernel
described in (Collins and Duffy, 2002), which re-
lies on a fragment definition that does not allow to
1In P&M we provided an approximation of the real
weight.
break production rules (i.e. if any child of a node is
included in a fragment, then also all the other chil-
dren have to). As such, it is especially indicated
for tasks involving constituency parsed texts.
Tree kernels compute the number of common
substructures between two trees T1 and T2
without explicitly considering the whole feature
(fragment) space. Let F = {f1, f2, . . . , f|F|}
be the set of tree fragments, i.e. the explicit
representation for the components of the fragment
space, and ?i(n) be an indicator function2, equal
to 1 if the target fi is rooted at node n and equal
to 0 otherwise. A tree kernel function over T1 and
T2 is defined as
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), (2)
whereNT1 andNT2 are the sets of nodes in T1 and
T2, respectively and
?(n1, n2) =
|F|?
i=1
?i(n1)?i(n2). (3)
The ? function counts the number of common
subtrees rooted in n1 and n2 and weighs them
according to their size. It can be evaluated as
follows (Collins and Duffy, 2002):
1. if the productions at n1 and n2 are different,
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminal symbols) then ?(n1, n2) = ?;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) = ?
l(n1)?
j=1
(1 + ?(cjn1 , c
j
n2)), (4)
where l(n1) is the number of children of n1, cjn
is the j-th child of node n and ? is a decay factor
penalizing larger structures.
2.3 Tree Fragment Weights
Eq. 3 shows that ? counts the shared fragments
rooted in n1 and n2 in the form of scalar product,
as evaluated by Eq. 2. However, when ? is used in
? as in Eq. 4, it changes the weight of the product
?i(n1)?i(n2). As ? multiplies ? in each recur-
sion step, we may be induced to assume3 that the
2We will consider it as a weighting function.
3In (Collins and Duffy, 2002), there is a short note about
the correct value weight of lambda for each product compo-
nents (i.e. pairs of fragments). This is in line with the formu-
lation we provide.
224
weight of a fragment is ?d, where d is the depth of
the fragment. On the contrary, we show the actual
weight by providing the following:
Theorem 1. Let T and f be a tree and one of
its fragments, respectively, induced by STK. The
weight of f accounted by STK is ?
s(f)
2 , where
lf (n) is the number of children of n in f and
s(f) = |{n ? T : lf (n) > 0}| is the number
of nodes that have active productions in the frag-
ment, i.e. the size of the fragment.
In other words, the exponent of ? is the number
of fragment nodes that have at least one child (i.e.
active productions), divided by 2.
Proof. The thesis can be proven by induction on
the depth d of f . The base case is f of depth
1. Fragments of depth 1 are matched by step 2
of ?(n1, n2) computation, which assigns a value
? = ?i(n1)?i(n2) independently of the number of
children (where fi = f ). It follows that the weight
of f is ?i(n1) = ?i(n2) = ?1/2.
Suppose that the thesis is valid for depth d and
let us consider a fragment f of depth d+ 1, rooted
in r. Without loss of generality, we can assume
that f is in the set of the fragments rooted in n1
and n2, as evaluated by Eq. 4. It follows that
the production rules associated with n1 and n2 are
identical to the production rule in r. Let us con-
sider M = {i ? {1, .., l(n1)} : l(cir) > 0},
i.e. the set of child indices of r which have at
least a child. Thus, for j ? M , cir has a pro-
duction shared by cjn1 and c
j
n2 . Conversely, for
j /? M , there is no match and ?(cjn1 , c
j
n2) = 0.
Therefore, the product in Eq. 4 can be rewrit-
ten as ?
?
j?M ?(c
j
n1 , c
j
n2), where the term 1 in
(1 + ?(cjn1 , c
j
n2)) is not considered since it ac-
counts for those cases where there are no common
productions in the children, i.e. cjn1 6= c
j
n2?j ?
M .
We can now substitute ?(cjn1 , c
j
n2) with the
weight of the subtree tj of f rooted in cjr (and ex-
tended until its leaves), which is ?s(tj) by induc-
tive hypothesis (since tj has depth lower than d).
Thus, the weight of f is s(f) = ?
?
j?M ?
s(tj) =
?1+
?
j?M
s(tj), where
?
j?M s(tj) is the num-
ber of nodes in f ?s subtrees rooted in r?s chil-
dren and having at least one child; by adding
1, i.e. the root of f , we obtain s(f). Finally,
?s(f) = ?i(n1)?i(n2), which satisfies our thesis:
?i(n1) = ?i(n2) = ?
s(f)
2 .
2.4 Weights in Feature Vectors
In the light of this result, we can use the definition
of a TK function to project a tree t onto a linear
space by recognizing that t can be represented as a
vector ~xi = [x
(1)
i , . . . , x
(N)
i ] whose attributes are
the counts of the occurrences for each fragment,
weighed with respect to the decay factor ?.
For a normalized STK kernel, the value of the
j-th attribute of the example ~xi is therefore:
x(j)i =
ti,j?
s(fj)
2
?~xi?
=
ti,j?
s(fj)
2
??N
k=1 t
2
i,k?
s(fk)
(5)
where: ti,j is the number of occurrences of the
fragment fj , associated with the j-th dimension
of the feature space, in the tree ti. It follows that
the components of ~w (see Eq. 1) can be rewritten
as:
w(j) =
?`
i=1
?iyix
(j)
i =
?`
i=1
?iyiti,j?
s(fj)
2
??N
k=1 t
2
i,k?
s(fk)
(6)
3 Projecting Exponentially Large Spaces
In order to provide a theoretical background to our
feature selection technique and to develop effec-
tive algorithms, we want to relate our approach to
statistical learning and, in particular, support vec-
tor classification theory. Since we select features
with respect to their weight w(j), we can use the
following theorem that establishes a general bound
for margin-based classifiers.
Theorem 2. (Bartlett and Shawe-Taylor, 1998)
Let C = {~x ? ~w ? ~x : ?~w? ? 1, ?~x? ? R}
be the class of real-valued functions defined in a
ball of radius R in <N . Then there is a con-
stant k such that ?c ? C having a margin ?, i.e.
|~w ? ~x| ? ?,?~x ? X (training set), the error of c
is bounded by b/` +
?
k
`
(
R2
?2 log
2`+ log 1?
)
with
a probability 1 ? ?, where ` = |X | and b is the
number of examples with margin less than ?.
In other words, if X is separated with a margin
? by a linear classifier, then the error has a bound
depending on ?. Another conclusion is that a fea-
ture selection algorithm that wants to preserve the
accuracy of the original space should not affect the
margin.
Since we would like to exploit the availability
of the initial gradient ~w derived by the applica-
tion of SVMs, it makes sense to try to quantify the
percentage of ? reduction after feature selection,
which we indicate by ?. We found out that ? is
225
linked to the reduction of ||~w||, as illustrated by
the next lemma.
Lemma 1. Let X be a set of points in a vector
space and ~w be the gradient vector which sepa-
rates them with a margin ?. If the selection de-
creases ||~w|| of a ? rate, then the resulting hyper-
plane separates X by a margin larger than ?in =
? ? ?R||~w||.
Proof. Let ~w = ~win+ ~wout, where ~win and ~wout ?
<N are constituted by the components of ~w that
are selected in and out, respectively, and have zero
values in the remaining positions. By hypothesis
|~w ? ~x| ? ?; without loss of generality, we can
consider just the case ~w ? ~x ? ?, and write ~w ?
~x = ~win ? ~x + ~wout ? ~x ? ? ? ~win ? ~x ? ? ?
~wout ? ~x ? ? ? |~wout ? ~x| ? ? ? ||~wout|| ? ||~x||,
where the last inequality holds owing to Cauchy-
Schwarz inequality. The margin associated with
~win, i.e. ?in, is therefore ? ? ||~wout|| ? ||~x|| ?
? ? ||~wout||R = ? ? ?R||~w||.
Remark 1. The lemma suggests that, even in case
of very aggressive feature selection, if a small per-
centage ? of ||~w|| is lost, the margin reduction is
small. Consequently, through Theorem 2, we can
conclude that the accuracy of the model is by and
large preserved.
Remark 2. We prefer to show the lemma in the
more general form, but if we use normalized ~x and
classifiers with ||~w|| ? 1, then ?in = ??||~w||? >
? ? ?.
The last result that we present justifies our se-
lection approach as it demonstrates that most of
the gradient norm is concentrated in relatively few
features, with respect to the huge space induced
by tree kernels. The selection of these few fea-
tures allows us to preserve most of the norm and
the margin.
Lemma 2. Let ~w be a linear separator of a set of
points X , where each ~xi ? X is an explicit vector
representations of a tree ti in the space induced by
STK and let ? be the largest s(ti), i.e. the max-
imum tree size. Then, if we discard fragments of
size greater than ?, ||~wout|| ? ??2
?
(??)??(??)?
1??? .
Proof. By applying simple norm proper-
ties, ||~wout|| =
?
?
?
?`
i=1 ?iyi~xouti
?
?
? ?
?`
i=1
||?iyi~xouti || =
?`
i=1 ?i||~xouti ||. To evaluate
the latter, we first re-organize the summation in
Eq. 5 (with no normalization) such that ?~xi?2
=
??
k=1
?
j:s(fj)=k t
2
i,j?
s(fj). Since a fragment
fj can be at maximum rooted in ? nodes, then
ti,j ? ?. Therefore, by replacing the number of
trees of size k with the upperbound ?k, we have
?~xi? <
???
k=1 ?2?k?k =
???
k=1 ?2(??)k =
?
?2 1??
?
1?? , where we applied geometric series
summation. Now if we assume that our algorithm
selects out (i.e. discards) fragments with size
s(f) > ?, ?~xouti? <
?
?2 ?
????
1?? . It follows that
||~wout|| <
?`
i=1 ?i
?
?2 ?
????
1?? . In case of hard-
margin SVMs, we have
?`
i=1 ?i = 1/?
2. Thus,
||~wout|| < ??2
?
?????
1?? =
?
?2
?
(??)??(??)?
1??? .
Remark 3. The lemma shows that for an enough
large ? and ? < 1/?, ||~wout|| can be very small,
even though it includes an exponential number of
features, i.e. all the subtrees whose size ranges
from ? to ?. Therefore, according to Lemma 1 and
Theorem 2, we can discard an exponential number
of features with a limited loss in accuracy.
Remark 4. Regarding the proposed norm bound,
we observe that ?k is a rough overestimation of the
the real number of fragments having size k rooted
in the nodes of the target tree t. This suggests that
we don?t really need ? < 1/?. Moreover, in case
of soft-margin SVMs, we can bound ?i with the
value of the trade-off parameter C.
4 Previous Work
Initial work on feature selection for text, e.g.
(Yang and Pedersen, 1997), has shown that it may
improve the accuracy or, at least, improve effi-
ciency while preserving accuracy. Our context for
feature selection is different for several important
reasons: (i) we focus on structured features with
a syntactic nature, which show different behaviour
from lexical ones, e.g. they tend to be more sparse;
(ii) in the TK space, the a-priori weights are very
skewed, and large fragments receive exponentially
lower scores than small ones; (iii) there is high re-
dundancy and inter-dependency between such fea-
tures; (iv) we want to be able to observe the most
relevant features automatically generated by TKs;
and (v) the huge number of features makes it im-
possible to evaluate the weight of each feature in-
dividually.
Guyon and Elisseeff (2003) carries out a very
informative survey of feature selection techniques.
Non-filter approaches for SVMs and kernel ma-
chines are often concerned with polynomial and
226
Gaussian kernels, e.g. (Weston et al, 2001; Neu-
mann et al, 2005). In (Kudo and Matsumoto,
2003), an extension of the PrefixSpan algo-
rithm (Pei et al, 2001) is used to efficiently mine
the features in a low degree polynomial kernel
space. The authors discuss an approximation
of their method that allows them to handle high
degree polynomial kernels. Suzuki and Isozaki
(2005) present an embedded approach to feature
selection for convolution kernels based on ?2-
driven relevance assessment. With respect to their
work, the main differences in the approach that we
propose are that we want to exploit the SVM op-
timizer to select the most relevant features, and to
be able to observe the relevant fragments.
Regarding work that may directly benefit from
reverse kernel engineering is worthwhile mention-
ing: (Cancedda et al, 2003; Shen et al, 2003;
Daume? III and Marcu, 2004; Giuglea and Mos-
chitti, 2004; Toutanova et al, 2004; Kazama and
Torisawa, 2005; Titov and Henderson, 2006; Kate
and Mooney, 2006; Zhang et al, 2006; Bloehdorn
et al, 2006; Bloehdorn and Moschitti, 2007; Mos-
chitti and Zanzotto, 2007; Surdeanu et al, 2008;
Moschitti, 2008; Moschitti and Quarteroni, 2008;
Martins et al, 2009; Nguyen et al, 2009a)
5 Mining Fragments Efficiently
The high-level description of our feature selection
technique is as follows: we start by learning an
STK model and we greedily explore the support
vectors in search for the most relevant fragments.
We store them in an index, and then we decode (or
linearize) all the trees in the dataset, i.e. we repre-
sent them as vectors in a linear space where only a
very small subset of the fragments in the original
space are accounted for. These vectors are then
employed for learning and classification in the lin-
ear space.
To explore the fragment space defined by a set
of support vectors, we adopt the greedy strategy
described in Algorithm 5.1. Its arguments are a
model M , and the threshold factor L. The greedy
algorithm explores the fragment space in a small to
large fashion. The first step is the generation of the
all base fragments F encoded in each tree, i.e. the
smallest possible fragments according to the defi-
nition of the kernel function. For STK, such frag-
ments are all those consisting of a node and all its
direct children (i.e. production rules of the gram-
mar). We assess the cumulative relevance of each
Algorithm 5.1: GREEDY MODEL MINER(M,L)
B ? BASE FRAGS(model)
B ? REL(BEST(B))
? ? B/L
Dprev ? FILTER(B, ?)
UPDATE(Dprev)
while Dprev 6= ?
do
?
????????????????????
????????????????????
Dnext ? ?
? ? 1/ ? widthfactor ? /
Wprev ? Dprev
whileWprev 6= ?
do
?
???????????
???????????
Wnext ? ?
for each f ? Wprev
do
?
?????
?????
Ef ? EXPAND(f, ?)
F ? FILTER(Ef , ?)
if F 6= ?
then
{
Wnext ?Wnext ? {f}
Dnext ? Dnext ? F
UPDATE(F )
? ? ? + 1
Wprev ?Wnext
Dprev ? Dnext
return (result)
base fragment according to Eq. 6 and then use the
relevanceB of the heaviest fragment, i.e. the frag-
ment with the highest relevance in absolute value,
as a criterion to set our fragment mining threshold
? to B/L. We then apply the FILTER(?) operator
which discards all the fragments whose cumula-
tive score is less than ?. Then, the UPDATE(?) op-
erator stores the ramaining fragments in the index.
The exploration of the kernel space is carried
out via the process of fragment expansion, by
which each fragment retained at the previous step
is incrementally grown to span more levels of the
tree and to include more nodes at each level. These
two directions of growth are controlled by the
outer and the inner while loops, respectively. Frag-
ment expansion is realized by the EXPAND(f, n)
operator, that grows the fragment f by including
the children of n expandable nodes in the frag-
ment. Expandable nodes are nodes which are
leaves in f but that have children in the tree that
originated f .
After each expansion, the FILTER(?) operator is
invoked on the set of generated fragments. If the
filtered set is empty, i.e. no fragments more rele-
vant than ? have been found during the previous
iteration, then the loop is terminated.
Unlike previous attempts, this algorithm relies
on just one parameter, i.e. L. As it revolves around
the weight of the most relevant fragment, it oper-
ates according to the norm-preservation principle
described in the previous sections. In fact, if we
call N the number of fragments mined for a given
value of L, the norm after feature selection can be
227
bounded by BL
?
N ? ?win? ? B
?
N .
The choice of B, i.e. the highest relevance of
a base fragment, as an upper bound for fragment
relevance is motivated as follows. In Eq. 6, we can
identify a term Ti = ?iyi/?ti? that is the same for
all the fragments in the tree ti. For 0 < ? ? 1,
if fj is an expansion of fk, then from our defini-
tion of fragment expansion it follows that ?
s(fj)
2 <
?
s(fk)
2 . It can also be observed that ti,j ? ti,k. In-
deed, if ti,k is a subset of ti,j , then it will occur at
least as many times as its expansion ti,k, possibly
occurring as a seed fragment for different expan-
sions in other parts of the tree as well. Therefore,
if Ef is the set of expansions of f , for every two
fragments fi,j , fi,k coming from the same tree ti,
we can conclude that x(j)i < x
(k)
i ?fi,j ? Efi,k . In
other words, for each tree in the model, base frag-
ments are the most relevant, and we can assume
that the relevance of the heaviest fragment is an
upper bound for the relevance of any fragment 4.
6 Experiments
We ran a set of thorough experiments to sup-
port our claims with empirical evidence. We
show our results on three very different bench-
marks: Question Classification (QC) using TREC
10 data (Voorhees, 2001), Relation Extraction
(RE) based on the newswire and broadcast news
domain of the ACE 2004 English corpus (Dod-
dington et al, 2004) and Semantic Role Labeling
(SRL) on the CoNLL 2005 shared task data (Car-
reras and Ma`rquez, 2005). In the next sections we
elaborate on the setup and outcome of each set
of experiments. As a supervised learning frame-
work we used SVM-Light-TK5, which extends the
SVM-Light optimizer (Joachims, 2000) with sup-
port for tree kernel functions.
Unless differently stated, all the classifiers are
parametrized for optimal Precision and Recall on
a development set, obtained by selecting one ex-
ample in ten from the training set with the same
positive-to-negative example ratio. The results
that we show are obtained on the test sets by using
all the available data for training. For multi-class
scenarios, the classifiers are arranged in a one vs.
4In principle, the weight of some fragment encoded in the
model M may be greater than B. However, as an empirical
justification, we report that in all our experiments we have
never been able to observe such case. Thus, with a certain
probability, we can assume that the highest weight will be
obtained from the heaviest of the base fragments.
5
http://disi.unitn.it/?moschitt/Tree-Kernel.htm
all configuration, where each sentence is a positive
example for one of the classes, and negative for
the others. While binary classifiers are evaluated
in terms of F1 measure, for multi-class classifiers
we show the final accuracy.
The next paragraphs describe the datasets used
for the experiments.
Question Classification (QC) Given a question,
the task consists in selecting the most appropriate
expected answer type from a given set of possibil-
ities. We adopted the question taxonomy known
as coarse grained, which has been described
in (Zhang and Lee, 2003) and (Li and Roth, 2006),
consisting of six non overlapping classes: Abbre-
viations (ABBR), Descriptions (DESC, e.g. def-
initions or explanations), Entity (ENTY, e.g. an-
imal, body or color), Human (HUM, e.g. group
or individual), Location (LOC, e.g. cities or coun-
tries) and Numeric (NUM, e.g. amounts or dates).
The TREC 10 QA data set accounts for 6,000
questions. For each question, we generate the
full parse of the sentence and use it to train our
models. Automatic parses are obtained with the
Stanford parser6 (Klein and Manning, 2003), and
we actually have only 5,953 sentences in our data
set due to parsing issues. During preliminary ex-
periments, we observed an uneven distribution of
examples in the traditional training/test split (the
same used in P&M). Therefore, we used a ran-
dom selection to generate an unbiased split, with
5,468 sentences for training and 485 for testing.
The resulting data set is available for download
at http://danielepighin.net/cms/research/
QC_dataset.tgz.
Relation Extraction (RE) The corpus
consists of 348 documents, and contains
seven relation classes defined over pairs of
mentions: Physical, Person/Social, Employ-
ment/Membership/Subsidiary, Agent-Artifact,
PER/ORG Affiliation, GPE Affiliation, and
Discourse. There are 4,400 positive and 38,696
negative examples when the potential relations
are generated using all the entity/mention pairs in
the same sentence.
Documents are parsed using the Stanford
Parser, where the nodes of the entities are enriched
with information about the entity type. Overall,
we used the setting and data defined in (Nguyen et
al., 2009b).
6
http://nlp.stanford.edu/software/lex-parser.shtml
228
Semantic Role Labeling (SRL) SRL can be de-
composed into two tasks: boundary detection,
where the word sequences that are arguments of
a predicate word w are identified, and role clas-
sification, where each argument is assigned the
proper role. For these experiments we concen-
trated on this latter task and used exactly the same
setup as P&M. We considered all the argument
nodes of any of the six PropBank (Palmer et al,
2005) core roles7 (i.e. A0, . . . , A5) from all the
available training sections, i.e. 2 through 21, for a
total of 179,091 training instances. Similarly, we
collected 9,277 test instances from the annotations
of Section 23.
6.1 Model Comparison
To show the validity of Lemma 1 in practical sce-
narios, we compare the accuracy of our linearized
models against vanilla STK classifiers. We de-
signed two types of classifiers:
LIN, a linearized STK model, which uses the
weights estimated by the learner in the STK space
and linearized examples; in other words LIN uses
~wIN . It allows us to measure exactly the loss in
accuracy with respect to the reduction of ||~w||.
OPT, a linearized STK model that is re-
optimized in the linear space, i.e. for which we
retrained an SVM using the linearized training ex-
amples as input data. Since the LIN solution is
part of the candidate solutions from which OPT is
selected, we always expect higher accuracy from
it.
Additionally, we compare selection based on
gradient ~w (as detailed in Section 2.4) against to
?2 selection, which evaluates the relevance of fea-
tures, in a similar way to (Suzuki and Isozaki,
2005). The relevance of a fragment is calculated
as
?2 =
N(yN ?Mx)2
x(N ? x)M(N ?M)
,
where N is the number of support vectors, M is
the number of positive vectors (i.e. ?i > 0), and x
and y are the fractions ofN andM where the frag-
ment is instantiated, respectively. We specify the
selection models by means of Grad for the former
and Chi for the latter. For example, a model called
OPT/Grad is a re-trained model using the features
selected according the highest gradient weights,
while LIN/Chi would be a linearized tree kernel
model using ?2 for feature selection.
7We do not consider adjuncts because we preferred the
number of classes to be similar across the three benchmarks.
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 1  10  100  1000  10000
Number of fragments (log)
ABBRDESCENTYHUMLOCNUM
1?
?
Figure 1: Percentage of gradient Norm, i.e. 1? ?,
according to the number of selected fragments, for
different QC classifiers.
STK Linearized
LIN OPT
F1 ||~w|| Frags F1 ||~win|| F1
A 80.00 11.77 566 66.67 7.13 90.91
D 86.26 41.33 5161 81.87 25.10 83.72
E 76.86 51.71 5,702 73.03 31.06 75.56
H 84.92 43.61 5,232 80.47 26.20 77.08
L 81.69 38.73 1,732 78.87 24.27 82.89
N 92.31 37.65 1,015 85.07 24.53 87.07
Table 1: Per-class comparison between STK and
the LIN/Grad and OPT/Grad models on the QC
task. Each class is identified by its initial (e.g.
A=ABBR). For each class, we considered a value
of the threshold factor parameter L so as to retain
at least 60% of the gradient norm after feature se-
lection.
6.2 Results
The plots in Figure 1 show, for each class, the per-
centage of the gradient norm (i.e. 1 ? ?, see Sec-
tion 3) retained when including a different num-
ber of fragments. This graph empirically validates
Lemma 2 since it clearly demonstrates that after
1,000-10,000 features the percentage of the norm
reaches a plateau (around 60-65%). This means
that after such threshold, which interestingly gen-
eralizes across all classifiers, a huge number of
features is needed for a small increase of the norm.
We recall that the maximum reachable norm is
around 70% since we apriori filter out fragments
of frequency lower than three.
Table 1 shows the F1 of the binary question clas-
sifiers learned with STK, LIN/Grad and OPT/Grad
models. It also shows the norm of the gradi-
ent before, ||~w||, and after, ||~win||, feature selec-
229
 50
 55
 60
 65
 70
 75
 80
 85
 90
 0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7
F1 
(LO
C)
LIN/GradOPT/GradLIN/ChiOPT/ChiSTK
1? ?
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0.1  0.2  0.3  0.4  0.5  0.6
F1 
(DE
SC)
LIN/GradOPT/GradLIN/ChiOPT/ChiSTK
1? ?
Figure 2: F1-measure of LOC and DESC wrt dif-
ferent 1? ? values.
tion along with the number of selected fragments,
Frags. Instead of selecting an optimal number of
fragments on a validation set, we investigated the
60% value suggested by the previous plot. Thus,
for each category we selected the feature set reach-
ing approximately 60% of ||~w||. The table shows
that the accuracy of the OPT/Grad model is in
line with STK. In some cases, e.g. ABBR, the
projected model is more accurate, i.e. 90.91 vs.
80.00, whereas in others, e.g. HUM, STK per-
forms better, i.e. 84.92 vs. 77.08. It is interesting
to see how the empirical results clearly comple-
ment the theoretical findings of the previous sec-
tions. For example, the LOC classifier uses only
1,732 of the ? 1012 features encoded by the cor-
responding STK model, but since only 40% of the
norm of ~w is lost, classification accuracy is af-
fected only marginally.
As mentioned above, the selected number of
features is not optimal for every class. Fig-
ure 2 plots the accuracy of the LIN/Grad and
OPT/Grad for different numbers of fragments on
two classes 8. These show that the former, with
8The other classes, which show similar behaviour, are
omitted due to lack of space.
 20
 30
 40
 50
 60
 70
 80
 90
 100  1000  10000  100000
Mu
ltic
lass
 ac
cur
acy
Number of fragments (log)
OPT/GradOPT/ChiSTK
Figure 3: Multiclass accuracy obtained by includ-
ing a growing number of fragments.
more than 60% of the norm, approaches STK
whereas the latter requires less fragments. The
plots also show the comparison against the same
fragment mining algorithm and learning frame-
work when using ?2-based selection. This also
provides similar good results, as far as the reduc-
tion of ||~w|| is kept under control, i.e. as far as we
select the components of the gradient that mostly
affect its norm.
To concretely assess the benefits of our models
for QC, Figure 3 plots the accuracy of OPT/Grad
and OPT/Chi on the multiclass QC problem wrt
the number of fragments employed. The results
for the multi-class classifier are less biased by the
binary Precision/Recall classifiers thus they are
more stable and clearly show how, after selecting
the optimal number of fragments (1,000-10,000
i.e. 60-65% of the norm), the accuracy of the OPT
and CHI classifiers stabilize around levels of accu-
racy which are in line with STK.
STK OPT/Grad
F1 F1 Frags
QC 83.70 84.12 ?2k
RE 67.53 66.31 ?10k
SRL 87.56 88.17 ?300k
Table 2: Multiclass classification accuracy on
three benchmarks.
Finally, Table 2 shows the best results that we
achieved on the three multi-class classification
tasks, i.e. QC, RE9 and SRL, and compares them
against the STK 10. For all the tasks OPT/Grad
9For RE, we show lower accuracy than in (Nguyen et al,
2009b) since, to have a closer comparison with STK, we do
not combine structural features with manual designed fea-
tures.
10We should point out that this models are only partially
230
produces the best results for all the tests, even
though the difference with OPT/Chi is generally
not statistically significant. Out of three tasks,
OPT/Grad manages to slightly improve two of
them, i.e. QC (84.12 vs. 83.7) and SRL (88.17
vs. 87.56), while STK is more accurate on RE, i.e.
67.53 vs. 66.31.
6.3 Comparison with P&M
The results on SRL can be compared against
those that we presented in (Pighin and Moschitti,
2009a), where we measured an accuracy of 87.13
exactly on the same benchmark. As we can see in
Table 2, our model improves the classification ac-
curacy of about 1 point, i.e. 88.17. On the other
hand, such comparison is not really fair since the
algorithms rely on different parameter sets, and it
is almost impossible to find matching configura-
tions for the different versions of the algorithms
that would result in exactly the same number of
fragments. In a projected space with approxi-
mately 103 or 104 fragments, including a few hun-
dred more features can produce noticeably differ-
ent accuracy readings.
Generally speaking, the current model can
achieve comparable accuracy with P&M while
considering a smaller number of fragments. For
example, in (Pighin and Moschitti, 2009b) the
best model for the A1 binary classifier of the
SRL benchmark was obtained by including 50,000
fragments, achieving an F1 score of 89.04. With
the new algorithm, using approximately half the
fragments the accuracy of the linearized A1 clas-
sifier is 90.09. In P&M, the algorithm would only
consider expansions of a fragment f where at most
m nodes are expanded. Consequently, the set of
mined fragments may include some small struc-
tures which can be less relevant than larger ones.
Conversely, the new algorithm (see Alg. 5.1) may
include larger but more relevant structures, thus
accounting for a larger fraction of the gradient
norm with a smaller number of fragments.
Concerning efficiency, the complexity of both
mining algorithms is proportional to the number
of fragments that they generate. Therefore, we can
conclude that the new implementation is more effi-
cient by considering that we can achieve the same
accuracy with less fragments. As for the complex-
optimized, as we evaluated them by using the same threshold
factor parameter L for all the classes. Better performances
could be achieved by selecting an optimal value of L for in-
dividual classes when building the multi-class classifier.
ity of decoding, i.e. providing explicit vector rep-
resentations of the input trees, in P&M, we used
a very naive approach, i.e. the generation of all
the fragments encoded in the tree and then look up
each fragment in the index. This solution has ex-
ponential complexity with the number of nodes in
the tree. Conversely, the new implementation has
approximately linear complexity. The approach is
based on the idea of an FST-like index, that we
can query with a tree node. Every time the tree
matches one of the fragments, the index increases
the count of that fragment for the tree. The reduc-
tion in time complexity is made possible by en-
coding in the index the sequence of expansion op-
erations that produced each indexed fragment, and
by considering only those expansions at decoding
time.
7 Conclusions
Available feature selection frameworks for very
high dimensional kernel families, such as tree ker-
nels, suffer from the lack of a theory that could
justify the very aggressive selection strategies nec-
essary to cope with the exceptionally high dimen-
sional feature space.
In this paper, we have provided a theoretical
foundation in the context of margin classifiers by
(i) linking the reduction of the gradient norm to the
theoretical error bound and (ii) by proving that the
norm is mostly concentrated in a relatively small
number of features. The two properties suggest
that we can apply an extremely aggressive fea-
ture selection by keeping the same accuracy. We
described a very efficient algorithm to carry out
such strategy in the fragment space. Our experi-
ments empirically support our theoretical findings
on three very different NLP tasks.
Acknowledgements
We would like to thank Truc-Vien T. Nguyen for
providing us with the SVM learning and test files
of the Relation Extraction dataset. Many thanks to
the anonymous reviewers for their valuable sug-
gestions.
This research has been partially supported by the
EC project, EternalS: ?Trustworthy Eternal Sys-
tems via Evolving Software, Data and Knowl-
edge?, project number FP7 247758.
231
References
P. Bartlett and J. Shawe-Taylor, 1998. Advances in Kernel
Methods ? Support Vector Learning, chapter Generaliza-
tion Performance of Support Vector Machines and other
Pattern Classifiers. MIT Press.
Stephan Bloehdorn and Alessandro Moschitti. 2007. Struc-
ture and semantics for expressive text kernels. In In Pro-
ceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and
Alessandro Moschitti. 2006. Semantic kernels for text
classification based on topological measures of feature
similarity. In Proceedings of ICDM 06, Hong Kong, 2006.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL?05.
Michael Collins and Nigel Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over Discrete
Structures, and the Voted Perceptron. In Proceedings of
ACL?02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceedings of
ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for Re-
lational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing by
maximum entropy tagging and SVM reranking. In Pro-
ceedings of EMNLP?04.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Auto-
matic Content Extraction (ACE) Program?Tasks, Data,
and Evaluation. Proceedings of LREC 2004, pages 837?
840.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovering using FrameNet, VerbNet and
PropBank. In In Proceedings of the Workshop on On-
tology and Knowledge Discovering at ECML 2004, Pisa,
Italy.
Isabelle Guyon and Andre? Elisseeff. 2003. An introduc-
tion to variable and feature selection. Journal of Machine
Learning Research, 3:1157?1182.
David Haussler. 1999. Convolution kernels on discrete struc-
tures. Technical report, Dept. of Computer Science, Uni-
versity of California at Santa Cruz.
T. Joachims. 2000. Estimating the generalization perfor-
mance of a SVM efficiently. In Proceedings of ICML?00.
Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for
semi-structured data. In Proceedings of ICML?02.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proceedings of
the 21st ICCL and 44th Annual Meeting of the ACL, pages
913?920, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding up
training with tree kernels for node relation labeling. In
Proceedings of HLT-EMNLP?05.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL?03, pages
423?430.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-
based parse reranking with subtree features. In Proceed-
ings of ACL?05.
Xin Li and Dan Roth. 2006. Learning question classifiers:
the role of semantic information. Natural Language En-
gineering, 12(3):229?249.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro
M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2009. Nonex-
tensive information theoretic kernels on measures. J.
Mach. Learn. Res., 10:935?975.
Alessandro Moschitti and Silvia Quarteroni. 2008. Kernels
on linguistic structures for answer extraction. In Proceed-
ings of ACL-08: HLT, Short Papers, Columbus, Ohio.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from
texts. In Zoubin Ghahramani, editor, Proceedings of the
24th Annual International Conference on Machine Learn-
ing (ICML 2007).
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2008. Tree kernels for semantic role labeling. Compu-
tational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Pro-
ceedings of ECML?06, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Proceeding
of CIKM ?08, NY, USA.
Julia Neumann, Christoph Schnorr, and Gabriele Steidl.
2005. Combined SVM-Based Feature Selection and Clas-
sification. Machine Learning, 61(1-3):129?150.
Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe
Riccardi. 2009a. Convolution kernels on constituent, de-
pendency and sequential structures for relation extraction.
In Proceedings of EMNLP.
Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe
Riccardi. 2009b. Convolution kernels on constituent,
dependency and sequential structures for relation extrac-
tion. In EMNLP ?09: Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing,
pages 1378?1387, Morristown, NJ, USA. Association for
Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of semantic
roles. Comput. Linguist., 31(1):71?106.
J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal,
and M. C. Hsu. 2001. PrefixSpan Mining Sequential Pat-
terns Efficiently by Prefix Projected Pattern Growth. In
Proceedings of ICDE?01.
232
Daniele Pighin and Alessandro Moschitti. 2009a. Efficient
linearization of tree kernel functions. In Proceedings of
CoNLL?09.
Daniele Pighin and Alessandro Moschitti. 2009b. Reverse
engineering of tree kernel feature spaces. In Proceedings
of EMNLP, pages 111?120, Singapore, August. Associa-
tion for Computational Linguistics.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Us-
ing LTAG Based Features in Parse Reranking. In Proceed-
ings of EMNLP?06.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large online
QA collections. In Proceedings of ACL-08: HLT, Colum-
bus, Ohio.
Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree
Kernels with Statistical Feature Mining. In Proceedings
of NIPS?05.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings of
CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher Man-
ning. 2004. The Leaf Path Projection View of Parse
Trees: Exploring String Kernels for HPSG Parse Selec-
tion. In Proceedings of EMNLP 2004.
Ellen M. Voorhees. 2001. Overview of the trec 2001 ques-
tion answering track. In In Proceedings of the Tenth Text
REtrieval Conference (TREC, pages 42?51.
Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimil-
iano Pontil, Tomaso Poggio, and Vladimir Vapnik. 2001.
Feature Selection for SVMs. In Proceedings of NIPS?01.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In Dou-
glas H. Fisher, editor, Proceedings of ICML-97, 14th In-
ternational Conference on Machine Learning, pages 412?
420, Nashville, US. Morgan Kaufmann Publishers, San
Francisco, US.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of SI-
GIR?03, pages 26?32.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntac-
tic Features for Relation Extraction using a Convolution
tree kernel. In Proceedings of NAACL.
233
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 124?133,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
A Study on Dependency Tree Kernels
for Automatic Extraction of Protein-Protein Interaction
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ? and Alessandro Moschitti ?
? Department of Information Engineering and Computer Science, University of Trento, Italy
? Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy
{chowdhury,lavelli}@fbk.eu, moschitti@disi.unitn.it
Abstract
Kernel methods are considered the most ef-
fective techniques for various relation extrac-
tion (RE) tasks as they provide higher accu-
racy than other approaches. In this paper,
we introduce new dependency tree (DT) ker-
nels for RE by improving on previously pro-
posed dependency tree structures. These are
further enhanced to design more effective ap-
proaches that we call mildly extended depen-
dency tree (MEDT) kernels. The empirical re-
sults on the protein-protein interaction (PPI)
extraction task on the AIMed corpus show that
tree kernels based on our proposed DT struc-
tures achieve higher accuracy than previously
proposed DT and phrase structure tree (PST)
kernels.
1 Introduction
Relation extraction (RE) aims at identifying in-
stances of pre-defined relation types in text as for
example the extraction of protein-protein interaction
(PPI) from the following sentence:
?Native C8 also formed a heterodimer
with C5, and low concentrations of
polyionic ligands such as protamine and
suramin inhibited the interaction.?
After identification of the relevant named entities
(NE, in this case proteins) C8 and C5, the RE task
determines whether there is a PPI relationship be-
tween the entities above (which is true in the exam-
ple).
Kernel based approaches for RE have drawn a lot
of interest in recent years since they can exploit a
huge amount of features without an explicit repre-
sentation. Some of these approaches are structure
kernels (e.g. tree kernels), which carry out struc-
tural similarities between instances of relations, rep-
resented as phrase structures or dependency trees,
in terms of common substructures. Other kernels
simply use techniques such as bag-of-words, subse-
quences, etc. to map the syntactic and contextual
information to flat features, and later compute simi-
larity.
One variation of tree kernels is the dependency
tree (DT) kernel (Culotta and Sorensen, 2004;
Nguyen et al, 2009). A DT kernel (DTK) is a
tree kernel that is computed on a dependency tree
(or subtree). A dependency tree encodes grammati-
cal relations between words in a sentence where the
words are nodes, and dependency types (i.e. gram-
matical functions of children nodes with respect to
their parents) are edges. The main advantage of a
DT in comparison with phrase structure tree (PST)
is that the former allows for relating two words di-
rectly (and in more compact substructures than PST)
even if they are far apart in the corresponding sen-
tence according to their lexical word order.
Several kernel approaches exploit syntactic de-
pendencies among words for PPI extraction from
biomedical text in the form of dependency graphs or
dependency paths (e.g. Kim et al (2010) or Airola
et al (2008)). However, to the best of our knowl-
edge, there are only few works on the use of DT
kernels for this task. Therefore, exploring the po-
tential of DTKs applied to different structures is a
worthwhile research direction. A DTK, pioneered
by Culotta and Sorensen (2004), is typically applied
to the minimal or smallest common subtree that in-
cludes a target pair of entities. Such subtree reduces
124
Figure 1: Part of the DT for the sentence ?The binding
epitopes of BMP-2 for BMPR-IA was characterized using
BMP-2 mutant proteins?. The dotted area indicates the
minimal subtree.
unnecessary information by placing word(s) closer
to its dependent(s) inside the tree and emphasizes
local features of relations. Nevertheless, there are
cases where a minimal subtree might not contain im-
portant cue words or predicates. For example, con-
sider the following sentence where a PPI relation
holds between BMP-2 and BMPR-IA, but the mini-
mal subtree does not contain the cue word ?binding?
as shown in Figure 1:
The binding epitopes of BMP-2 for
BMPR-IA was characterized using BMP-
2 mutant proteins.
In this paper we investigate two assumptions. The
first is that a DTK based on a mild extension of
minimal subtrees would produce better results than
the DTK on minimal subtrees. The second is that
previously proposed DT structures can be further
improved by introducing simplified representation
of the entities as well as augmenting nodes in the
DT tree structure with relevant features. This paper
presents an evaluation of the above assumptions.
More specifically, the contributions of this paper
are the following:
? We propose the use of new DT structures,
which are improvement on the structures de-
fined in Nguyen et al (2009) with the most gen-
eral (in terms of substructures) DTK, i.e. Par-
tial Tree Kernel (PTK) (Moschitti, 2006).
? We firstly propose the use of the Unlexicalized
PTK (Severyn and Moschitti, 2010) with our
dependency structures, which significantly im-
proves PTK.
? We compare the performance of the proposed
DTKs on PPI with the one of PST kernels and
show that, on biomedical text, DT kernels per-
form better.
? Finally, we introduce a novel approach (called
mildly extended dependency tree (MEDT) ker-
nel1, which achieves the best performance
among various (both DT and PST) tree kernels.
The remainder of the paper is organized as fol-
lows. In Section 2, we introduce tree kernels and re-
lation extraction and we also review previous work.
Section 3 describes the unlexicalized PTK (uPTK).
Then, in Section 4, we define our proposed DT struc-
tures including MEDT. Section 5 describes the ex-
perimental results on the AIMed corpus (Bunescu et
al., 2005) and discusses their outcomes. Finally, we
conclude with a summary of our study as well as
plans for future work.
2 Background and Related Work
The main stream work for Relation Extraction uses
kernel methods. In particular, as the syntactic struc-
ture is very important to derive the relationships be-
tween entities in text, several tree kernels have been
designed and experimented. In this section, we in-
troduce such kernels, the problem of relation extrac-
tion and we also focus on the biomedical domain.
2.1 Tree Kernel types
The objective behind the use of tree kernels is
to compute the similarity between two instances
through counting similarities of their sub-structures.
Among the different proposed methods, two of the
most effective approaches are Subset Tree (SST)
kernel (Collins and Duffy, 2001) and Partial Tree
Kernel (PTK) (Moschitti, 2006).
The SST kernel generalizes the subtree ker-
nel (Vishwanathan and Smola, 2002), which consid-
ers all common subtrees in the tree representation of
two compared sentences. In other words, two sub-
trees are identical if the node labels and order of chil-
dren are identical for all nodes. The SST kernel re-
laxes the constraint that requires leaves to be always
included in the sub-structures. In SST, for a given
node, either none or all of its children have to be in-
cluded in the resulting subset tree. An extension of
1We defined new structures, which as it is well known it
corresponds to define a new kernel.
125
the SST kernel is the SST+bow (bag-of-words) ker-
nel (Zhang and Lee, 2003; Moschitti, 2006a), which
considers individual leaves as sub-structures as well.
The PT kernel (Moschitti, 2006) is more flexi-
ble than SST by virtually allowing any tree sub-
structure; the only constraint is that the order of child
nodes must be identical. Both SST and PT kernels
are convolution tree kernels2.
The PT kernel is the most complete in terms of
structures. However, the massive presence of child
node subsequences and single child nodes, which in
a DT often correspond to words, may cause overfit-
ting. Thus we propose the use of the unlexicalized
(i.e. PT kernel without leaves) tree kernel (uPTK)
(Severyn and Moschitti, 2010), in which structures
composed by only one lexical element, i.e. single
nodes, are removed from the feature space (see Sec-
tion 3).
2.2 Relation Extraction using Tree Kernels
A first version of dependency tree kernels (DTKs)
was proposed by Culotta and Sorensen (2004). In
their approach, they find the smallest common sub-
tree in the DT that includes a given pair of enti-
ties. Then, each node of the subtree is represented
as a feature vector. Finally, these vectors are used
to compute similarity. However, the tree kernel they
defined is not a convolution kernel, and hence it gen-
erates a much lower number of sub-structures result-
ing in lower performance.
For any two entities e1 and e2 in a DT, Nguyen
et al (2009) defined the following three dependency
structures to be exploited by convolution tree ker-
nels:
? Dependency Words (DW) tree: a DW tree is
the minimal subtree of a DT, which includes e1
and e2. An extra node is inserted as parent of
the corresponding NE, labeled with the NE cat-
egory. Only words are considered in this tree.
? Grammatical Relation (GR) tree: a GR tree
is similar to a DW tree except that words are
replaced by their grammatical functions, e.g.
prep, nsubj, etc.
2Convolution kernels aim to capture structural information
in term of sub-structures, providing a viable alternative to flat
features (Moschitti, 2004).
? Grammatical Relation and Words (GRW) tree:
a GRW tree is the minimal subtree that uses
both words and grammatical functions, where
the latter are inserted as parent nodes of the for-
mer.
Using PTK for the above dependency tree struc-
tures, the authors achieved an F-measure of 56.3 (for
DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE
2004 corpus3.
Moschitti (2004) proposed the so called path-
enclosed tree (PET)4 of a PST for Semantic Role
Labeling. This was later adapted by Zhang et al
(2005) for relation extraction. A PET is the smallest
common subtree of a PST, which includes the two
entities involved in a relation.
Zhou et al (2007) proposed the so called context-
sensitive tree kernel approach based on PST, which
expands PET to include necessary contextual in-
formation. The expansion is carried out by some
heuristics tuned on the target RE task.
Nguyen et al (2009) improved the PET represen-
tation by inserting extra nodes for denoting the NE
category of the entities inside the subtree. They also
used sequence kernels from tree paths, which pro-
vided higher accuracy.
2.3 Relation Extraction in the biomedical
domain
There are several benchmarks for the PPI task,
which adopt different PPI annotations. Conse-
quently the experimental results obtained by dif-
ferent approaches are often difficult to compare.
Pyysalo et al (2008) put together these corpora (in-
cluding the AIMed corpus used in this paper) in a
common format for comparative evaluation. Each
of these corpora is known as converted corpus of the
corresponding original corpus.
Several kernel-based RE approaches have been
reported to date for the PPI task. These are based on
various methods such as subsequence kernel (Lodhi
et al, 2002; Bunescu and Mooney, 2006), depen-
dency graph kernel (Bunescu and Mooney, 2005),
etc. Different work exploited dependency analy-
ses with different kernel approaches such as bag-of-
3http://projects.ldc.upenn.edu/ace/
4Also known as shortest path-enclosed tree or SPT (Zhou et
al., 2007).
126
words kernel (e.g. Miwa et al (2009)), graph based
kernel (e.g. Kim et al (2010)), etc. However, there
are only few researches that attempted the exploita-
tion of tree kernels on dependency tree structures.
S?tre et al (2007) used DT kernels on AIMed
corpus and achieved an F-score of 37.1. The re-
sults were far better when they combined the out-
put of the dependency parser with that of a Head-
driven Phrase Structure Grammar (HPSG) parser,
and applied tree kernel on it. Miwa et al (2009) also
proposed a hybrid kernel 5, which is a composition
of all-dependency-paths kernel (Airola et al, 2008),
bag-of-words kernel and SST kernel. They used
multiple parser inputs. Their system is the current
state-of-the-art for PPI extraction on several bench-
marks. Interestingly, they applied SST kernel on the
shortest dependency paths between pairs of proteins
and achieved a relatively high F-score of 55.1. How-
ever, the trees they constructed from the shortest de-
pendency paths are actually not dependency trees. In
a dependency tree, there is only one node for each
individual word whereas in their constructed trees
(please refer to Fig. 6 of Miwa et al (2009)), a word
(that belongs to the shortest path) has as many node
representations as the number of dependency rela-
tions with other words (those belonging to the short-
est path). Perhaps, this redundancy of information
might be the reason their approach achieved higher
result. In addition to work on PPI pair extraction,
there has been some approaches that exploited de-
pendency parse analyses along with kernel methods
for identifying sentences that might contain PPI pairs
(e.g. Erkan et al (2007)).
In this paper, we focus on finding the best repre-
sentation based on a single structure. We speculate
that this can be helpful to improve the state-of-the-
art using several combinations of structures and fea-
tures. As a first step, we decided to use uPTK, which
is more robust to overfitting as the description in the
next section unveil.
5The term ?hybrid kernel? is identical to ?combined kernel?.
It refers to those kernels that combine multiple types of kernels
(e.g., tree kernels, graph kernels, etc)
3 Unlexicalized Partial Tree Kernel
(uPTK)
The uPTK was firstly proposed in (Severyn and
Moschitti, 2010) and experimented with semantic
role labeling (SRL). The results showed no improve-
ment for such task but it is well known that in SRL
lexical information is essential (so in that case it
could have been inappropriate). The uPTK defini-
tion follows the general setting of tree kernels.
A tree kernel function over two trees, T1 and T2,
is defined as
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2),
where NT1 and NT2 are the sets of nodes in T1 and
T2, respectively, and
?(n1, n2) =
|F|?
i=1
?i(n1)?i(n2).
The ? function is equal to the number of common
fragments rooted in nodes n1 and n2 and thus de-
pends on the fragment type.
The algorithm for the uPTK computation straight-
forwardly follows from the definition of the ? func-
tion of PTK provided in (Moschitti, 2006). Given
two nodes n1 and n2 in the corresponding two trees
T1 and T2, ? is evaluated as follows:
1. if the node labels of n1 and n2 are different then
?(n1, n2) = 0;
2. else ?(n1, n2) = ?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(
~I1)+d(~I2)
l(~I1)?
j=1
?(cn1(~I1j), cn2(~I2j))
)
,
where:
1. ~I1 = ?h1, h2, h3, ..? and ~I2 = ?k1, k2, k3, ..?
are index sequences associated with the ordered
child sequences cn1 of n1 and cn2 of n2, respec-
tively;
2. ~I1j and ~I2j point to the j-th child in the corre-
sponding sequence;
3. l(?) returns the sequence length, i.e. the number
of children;
127
4. d(~I1) = ~I1l(~I1)?
~I11 + 1 and d(~I2) = ~I2l(~I2)?
~I21 + 1; and
5. ? and ? are two decay factors for the size of
the tree and for the length of the child subse-
quences with respect to the original sequence,
i.e. we account for gaps.
The uPTK can be obtained by removing ?2 from
the equation in step 2. An efficient algorithm for the
computation of PTK is given in (Moschitti, 2006).
This evaluates ? by summing the contribution of
tree structures coming from different types of se-
quences, e.g. those composed by p children such
as:
?(n1, n2) = ?
(
?2 +
?lm
p=1 ?p(cn1 , cn2)
)
, (1)
where ?p evaluates the number of common subtrees
rooted in subsequences of exactly p children (of n1
and n2) and lm = min{l(cn1), l(cn2)}. It is easy to
verify that we can use the recursive computation of
?p by simply removing ?2 from Eq. 1.
4 Proposed dependency structures and
MEDT kernel
Our objective is twofold: (a) the definition of im-
proved DT structures and (b) the design of new DT
kernels to include important words residing outside
of the shortest dependency tree, which are neglected
in current approaches. For achieving point (a), we
modify the DW, GR and GRW structures, previously
proposed by Nguyen et al (2009). The new pro-
posed structures are the following:
? Grammatical Relation and lemma (GRL) tree:
A GRL tree is similar to a GRW tree except
that words are replaced by their corresponding
lemmas.
? Grammatical Relation, PoS and lemma
(GRPL) tree: A GRPL tree is an extension of a
GRL tree, where the part-of-speech (PoS) tag
of each of the corresponding words is inserted
as a new node between its grammatical func-
tion and its lemma, i.e. the new node becomes
the parent node of the node containing the
lemma.
Figure 2: Part of the DT for the sentence ?Interaction
was identified between BMP-2 and BMPR-IA?. The dot-
ted area indicates the minimal subtree.
Figure 3: Part of the DT for the sentence ?Phe93 forms
extensive contacts with a peptide ligand in the crystal
structure of the EBP bound to an EMP1?. The dotted
area indicates the minimal subtree.
? Ordered GRL (OGRL) or ordered GRW
(OGRW) tree: in a GRW (or GRL) tree, the
node containing the grammatical function of
a word is inserted as the parent node of such
word. So, if the word has a parent node con-
taining its NE category, the newly inserted node
with grammatical function becomes the child
node of the node containing NE category, i.e.
the order of the nodes is the following ? ?NE
category ? grammatical relation ? word (or
lemma)?. However, in OGRW (or OGRL), this
ordering is modified as follows ? ?grammatical
relation? NE category? word (or lemma)?.
? Ordered GRPL (OGRPL) tree: this is similar
to the OGRL tree except for the order of the
nodes, which is the following ? ?grammatical
relation? NE category? PoS? lemma?.
? Simplified (S) tree: any tree structure would
become an S tree if it contains simplified repre-
sentations of the entity types, where all its parts
except the head word of a multi-word entity are
not considered in the minimal subtree.
The second objective is to extend DTKs to include
important cue words or predicates that are missing
128
in the minimal subtree. We do so by mildly expand-
ing the minimal subtree, i.e. we define the mildly
extended DT (MEDT) kernel. We propose three dif-
ferent expansion rules for three versions of MEDT
as follows:
? Expansion rule for MEDT-1 kernel: If the root
of the minimal subtree is not a modifier (e.g.
adjective) or a verb, then look for such node in
its children or in its parent (in the original DT
tree) to extend the subtree.
The following example shows a sentence where
this rule would be applicable:
The binding epitopes of BMP-2
for BMPR-IA was characterized us-
ing BMP-2 mutant proteins.
Here, the cue word is ?binding?, the root of the
minimal subtree is ?epitopes? and the target en-
tities are BMP-2 and BMPR-IA. However, as
shown in Figure 1, the minimal subtree does
not contain the cue word.
? Expansion rule for MEDT-2 kernel: If the root
of the minimal subtree is a verb and its subject
(or passive subject) in the original DT tree is
not included in the subtree, then include it.
Consider the following sentence:
Interaction was identified be-
tween BMP-2 and BMPR-IA.
Here, the cue word is ?Interaction?, the root
is ?identified? and the entities are BMP-2 and
BMPR-IA. The passive subject ?Interaction?
does not belong to the minimal subtree (see
Figure 2).
? Expansion rule for MEDT-3 kernel: If the root
of the minimal subtree is the head word of one
of the interacting entities, then add the parent
node (in the original DT tree) of the root node
as the new root of the subtree.
This is an example sentence where this rule is
applicable (see Figure 3):
Phe93 forms extensive contacts
with a peptide ligand in the crystal
structure of the EBP bound to an
EMP1.
5 Experiments and results
We carried out several experiments with different
dependency structures and tree kernels. Most im-
portantly, we tested tree kernels on PST and our im-
proved representations for DT.
5.1 Data and experimental setup
We used the AIMed corpus (Bunescu et al, 2005)
converted using the software provided by Pyysalo et
al. (2008). AIMed is the largest benchmark corpus
(in terms of number of sentences) for the PPI task.
It contains 1,955 sentences, in which are annotated
1,000 positive PPI and 4,834 negative pairs.
We use the Stanford parser6 for parsing the data.7
The SPECIALIST lexicon tool8 is used to normalize
words to avoid spelling variations and also to pro-
vide lemmas. For training and evaluating tree ker-
nels, we use the SVM-LIGHT-TK toolkit9 (Mos-
chitti, 2006; Joachims, 1999). We tuned the param-
eters ?, ? and c following the approach described by
Hsu et al (2003), and used biased hyperplane.10 All
the other parameters are left as their default values.
Our experiments are evaluated with 10-fold cross
validation using the same split of the AIMed corpus
used by Bunescu et al (2005).
5.2 Results and Discussion
The results of different tree kernels applied to dif-
ferent structures are shown in Tables 1 and 2. All
the tree structures are tested with four different tree
kernel types: SST, SST+bow, PTK and uPTK.
According to the empirical outcome, our new DT
structures perform better than the existing tree struc-
tures. The highest result (F: 46.26) is obtained by
applying uPTK to MEDT-3 (SOGRL). This is 6.68
higher than the best F-measure obtained by previous
DT structures proposed in Nguyen et al (2009), and
0.36 higher than the best F-measure obtained using
PST (PET).
6http://nlp.stanford.edu/software/lex-parser.shtml
7For some of the positive PPI pairs, the connecting depen-
dency tree could not be constructed due to parsing errors for
the corresponding sentences. Such pairs are considered as false
negative (FN) during precision and recall measurements.
8http://lexsrv3.nlm.nih.gov/SPECIALIST/index.html
9http://disi.unitn.it/moschitti/Tree-Kernel.htm
10Please refer to http://svmlight.joachims.org/ and
http://disi.unitn.it/moschitti/Tree-Kernel.htm for details
about parameters of the respective tools
129
DT DT DT DT DT DT DT DT DT
(GR) (SGR) (DW) (SDW) (GRW) (SGRW) (SGRL) (SGRPL) (OGRPL)
SST P: 55.29 P: 54.22 P: 31.87 P: 30.74 P: 52.76 P: 52.47 P: 56.09 P: 56.03 P: 57.85
R: 23.5 R: 24.4 R: 27.5 R: 27.3 R: 33.4 R: 30.8 R: 33.6 R: 33.0 R: 31.7
F: 32.98 F: 33.66 F: 29.52 F: 28.92 F: 40.9 F: 38.82 F: 42.03 F: 41.54 F: 40.96
SST P: 57.87 P: 54.91 P: 30.71 P: 29.98 P: 52.98 P: 51.06 P: 51.99 P: 56.8 P: 61.73
+ R: 21.7 R: 23.5 R: 26.9 R: 25.9 R: 32.0 R: 31.3 R: 31.4 R: 28.8 R: 29.2
bow F: 31.56 F: 32.91 F: 28.68 F: 27.79 F: 39.9 F: 38.81 F: 39.15 F: 38.22 F: 39.65
PT P: 60.0 P: 57.84 P: 40.44 P: 42.2 P: 53.35 P: 53.41 P: 51.29 P: 52.88 P: 53.55
R: 15.9 R: 16.6 R: 23.9 R: 26.5 R: 34.2 R: 36.0 R: 37.9 R: 33.0 R: 33.2
F: 25.14 F: 25.8 F: 30.04 F: 32.56 F: 41.68 F: 43.01 F: 43.59 F: 40.64 F: 40.99
uPT P: 58.77 P: 59.5 P: 29.21 P: 29.52 P: 51.86 P: 52.17 P: 52.1 P: 54.64 P: 56.43
R: 23.8 R: 26.0 R: 30.2 R: 31.5 R: 32.0 R: 33.7 R: 36.0 R: 31.2 R: 30.7
F: 33.88 F: 36.19 F: 29.7 F: 30.48 F: 39.58 F: 40.95 F: 42.58 F: 39.72 F: 39.77
Table 1: Performance of DT (GR), DT (DW) and DT (GRW) (proposed by (Nguyen et al, 2009)) and their modified
and improved versions on the converted AIMed corpus.
RE experiments carried out on newspaper text
corpora (such as ACE 2004) have indicated that ker-
nels based on PST obtain better results than kernels
based on DT. Interestingly, our experiments on a
biomedical text corpus indicate an opposite trend.
Intuitively, this might be due to the different na-
ture of the PPI task. PPI can be often identified by
spotting cue words such as interaction, binding, etc,
since the interacting entities (i.e. proteins) usually
have direct syntactic dependency relation on such
cue words. This might have allowed kernels based
on DT to be more accurate.
Although tree kernels applied on DT and PST
structures have produced high performance on cor-
pora of news text (Zhou et al, 2007; Nguyen et al,
2009), in case of biomedical text the results that we
obtained are relatively low. This may be due to the
fact that biomedical texts are different from newspa-
per texts: more variation in vocabulary, more com-
plex naming of (bio) entities, more diversity of the
valency of verbs and so on.
One important finding of our experiments is the
effectiveness of the mild extension of DT struc-
tures. MEDT-3 achieves the best result for all ker-
nels (SST, SST+bow, PTK and uPTK). However, the
other two versions of MEDT appear to be less effec-
tive.
In general, the empirical outcome suggests that
uPTK can better exploit our proposed DT structures
as well as PST. The superiority of uPTK on PTK
demonstrates that single lexical features (i.e. fea-
tures with flat structure) tend to overfit.
Finally, we have performed statistical tests to as-
sess the significance of our results. For each kernel
(i.e. SST, SST+bow, PTK, uPTK), the PPI predic-
tions using the best structure (i.e. MEDT-3 applied
to SOGRL) are compared against the predictions of
the other structures. The tests were performed using
the approximate randomization procedure (Noreen,
1989). We set the number of iterations to 1,000 and
the confidence level to 0.01. According to the tests,
for each kernel, our best structure produces signifi-
cantly better results.
5.3 Comparison with previous work
To the best of our knowledge, the only work on tree
kernel applied on dependency trees that we can di-
rectly compare to ours is reported by S?tre et al
(2007). Their DT kernel achieved an F-score of
37.1 on AIMed corpus which is lower than our best
results. As discussed earlier, Miwa et al (2009))
also used tree kernel on dependency analyses and
achieved a much higher result. However, the tree
structure they used contains multiple nodes for a sin-
gle word and this does not comply with the con-
straints usually applied to dependency tree structures
(refer to Section 2.3). It would be interesting to ex-
amine why such type of tree representation leads to
130
DT DT DT DT MEDT-1 MEDT-2 MEDT-3 PST
(SOGRPL) (OGRL) (SOGRW) (SOGRL) (SOGRL) (SOGRL) (SOGRPL) (PET)
SST P: 57.59 P: 54.38 P: 51.49 P: 54.08 P: 58.15 P: 54.46 P: 59.55 P: 52.72
R: 33.0 R: 33.5 R: 31.2 R: 33.8 R: 34.6 R: 33.6 R: 37.1 R: 35.9
F: 41.96 F: 41.46 F: 38.86 F: 41.6 F: 43.39 F: 41.56 F: 45.72 F: 42.71
SST P: 60.31 P: 53.22 P: 50.08 P: 53.26 P: 58.84 P: 52.87 P: 59.35 P: 52.88
+ R: 30.7 R: 33.1 R: 30.9 R: 32.7 R: 32.6 R: 32.2 R: 34.9 R: 37.7
bow F: 40.69 F: 40.82 F: 38.22 F: 40.52 F: 41.96 F: 40.02 F: 43.95 F: 44.02
PT P: 55.45 P: 49.78 P: 51.05 P: 51.61 P: 52.94 P: 50.89 P: 54.1 P: 58.39
R: 34.6 R: 34.6 R: 34.1 R: 36.9 R: 36.0 R: 37.0 R: 38.9 R: 36.9
F: 42.61 F: 40.82 F: 40.89 F: 43.03 F: 42.86 F: 42.85 F: 45.26 F: 45.22
uPT P: 56.2 P: 50.87 P: 50.0 P: 52.74 P: 55.0 P: 52.17 P: 56.85 P: 56.6
R: 32.2 R: 35.0 R: 33.0 R: 35.6 R: 34.1 R: 34.8 R: 39.0 R: 38.6
F: 40.94 F: 41.47 F: 39.76 F: 42.51 F: 42.1 F: 41.75 F: 46.26 F: 45.9
Table 2: Performance of the other improved versions of DT kernel structures (including MEDT kernels) as well as
PST (PET) kernel (Moschitti, 2004; Nguyen et al, 2009) on the converted AIMed corpus.
a better result.
In this work, we compare the performance of tree
kernels applied of DT with that of PST. Previously,
Tikk et al (2010) applied similar kernels on PST for
exactly the same task and data set. They reported
that SST and PTK (on PST) achieved F-scores of
26.2 and 34.6, respectively on the converted AIMed
corpus (refer to Table 2 in their paper). Such results
do not match our figures obtained with the same
kernels on PST. We obtain much higher results for
those kernels. It is difficult to understand the rea-
son for such differences between our and their re-
sults. A possible explanation could be related to pa-
rameter settings. Another source of uncertainty is
given by the tool for tree kernel computation, which
in their case is not mentioned. Moreover, their de-
scription of PT and SST (in Figure 1 of their paper)
appears to be imprecise: for example, in (partial or
complete) phrase structure trees, words can only ap-
pear as leaves but in their figure they appear as non-
terminal nodes.
The comparison with other kernel approaches (i.e.
not necessarily tree kernels on DT or PST) shows
that there are model achieving higher results (e.g.
Giuliano et al (2006), Kim et al (2010), Airola et
al. (2008), etc). State-of-the-art results on most of
the PPI data sets are obtained by the hybrid kernel
presented in Miwa et al (2009). As noted earlier,
our work focuses on the design of an effective DTK
for PPI that can be combined with others and that
can hopefully be used to design state-of-the-art hy-
brid kernels.
6 Conclusion
In this paper, we have proposed a study of PPI ex-
traction from specific biomedical data based on tree
kernels. We have modeled and experimented with
new kernels and DT structures, which can be ex-
ploited for RE tasks in other domains too.
More specifically, we applied four different tree
kernels on existing and newly proposed DT and PST
structures. We have introduced some extensions of
DT kernel structures which are linguistically moti-
vated. We call these as mildly extended DT kernels.
We have also shown that in PPI extraction lexical
information can lead to overfitting as uPTK outper-
forms PTK. In general, the empirical results show
that our DT structures perform better than the previ-
ously proposed PST and DT structures.
The ultimate objective of our work is to improve
tree kernels applied to DT and then combine them
with other types of kernels and data to produce more
accurate models.
Acknowledgments
This work was carried out in the context of the project
?eOnco - Pervasive knowledge and data management in
cancer care?. The authors would like to thank the anony-
mous reviewers for providing excellent feedback.
131
References
A Airola, S Pyysalo, J Bj?orne, T Pahikkala, F Gin-
ter, and T Salakoski. 2008. A graph kernel for
protein-protein interaction extraction. In Proceedings
of BioNLP 2008, pages 1?9, Columbus, USA.
R Bunescu and R Mooney. 2005. A shortest path depen-
dency kernel for relation extraction. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 724?731, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
R Bunescu and RJ Mooney. 2006. Subsequence ker-
nels for relation extraction. In Proceedings of the
19th Conference on Neural Information Processing
Systems, pages 171?178.
R Bunescu, R Ge, RJ Kate, EM Marcotte, RJ Mooney,
AK Ramani, and YW Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine (Special Issue on Summarization
and Information Extraction from Medical Documents),
33(2):139?155.
M Collins and N Duffy. 2001. Convolution kernels for
natural language. In Proceedings of Neural Informa-
tion Processing Systems (NIPS?2001).
A Culotta and J Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain.
G Erkan, A Ozgur, and DR Radev. 2007. Semi-
Supervised Classification for Extracting Protein Inter-
action Sentences using Dependency Parsing. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 228?237.
C Giuliano, A Lavelli, and L Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL?2006),
pages 401?408, Trento, Italy.
CW Hsu, CC Chang, and CJ Lin, 2003. A practical guide
to support vector classification. Department of Com-
puter Science and Information Engineering, National
Taiwan University, Taipei, Taiwan.
T Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in ker-
nel methods: support vector learning, pages 169?184.
MIT Press, Cambridge, MA, USA.
S Kim, J Yoon, J Yang, and S Park. 2010. Walk-weighted
subsequence kernels for protein-protein interaction ex-
traction. BMC Bioinformatics, 11(1).
H Lodhi, C Saunders, J Shawe-Taylor, N Cristianini, and
C Watkins. 2002. Text classification using string ker-
nels. Journal of Machine Learning Research, 2:419?
444, March.
M Miwa, R S?tre, Y Miyao, T Ohta, and J Tsujii. 2009.
Protein-protein interaction extraction by leveraging
multiple kernels and parsers. International Journal of
Medical Informatics, 78.
A Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, ACL ?04, Barcelona, Spain.
A Moschitti. 2006. Efficient convolution kernels for de-
pendency and constituent syntactic trees. In Johannes
Fu?rnkranz, Tobias Scheffer, and Myra Spiliopoulou,
editors, Machine Learning: ECML 2006, volume 4212
of Lecture Notes in Computer Science, pages 318?329.
Springer Berlin / Heidelberg.
A Moschitti. 2006a. Making Tree Kernels Practical for
Natural Language Learning. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics, Trento, Italy.
TT Nguyen, A Moschitti, and G Riccardi. 2009. Con-
volution kernels on constituent, dependency and se-
quential structures for relation extraction. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing (EMNLP?2009), pages
1378?1387, Singapore, August.
EW Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
S Pyysalo, A Airola, J Heimonen, J Bjo?rne, F Ginter,
and T Salakoski. 2008. Comparative analysis of five
protein-protein interaction corpora. BMC Bioinfor-
matics, 9(Suppl 3):S6.
R S?tre, K Sagae, and J Tsujii. 2007. Syntactic features
for protein-protein interaction extraction. In Proceed-
ings of the Second International Symposium on Lan-
guages in Biology and Medicine (LBM 2007), pages
6.1?6.14, Singapore.
A Severyn and A Moschitti. 2010. Fast cutting plane
training for structural kernels. In Proceedings of
ECML-PKDD.
D Tikk, P Thomas, P Palaga, J Hakenberg, and U Leser.
2010. A Comprehensive Benchmark of Kernel Meth-
ods to Extract Protein-Protein Interactions from Liter-
ature. PLoS Computational Biology, 6(7), July.
SVN Vishwanathan and AJ Smola. 2002. Fast kernels on
strings and trees. In Proceedings of Neural Informa-
tion Processing Systems (NIPS?2002), pages 569?576,
Vancouver, British Columbia, Canada.
D Zhang and WS Lee. 2003. Question classification us-
ing support vector machines. In Proceedings of the
132
26th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?03, pages 26?32, Toronto, Canada.
M Zhang, J Su, D Wang, G Zhou, and CL Tan. 2005.
Discovering relations between named entities from a
large raw corpus using tree similarity-based clustering.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 378?389. Springer Berlin / Heidelberg.
GD Zhou, M Zhang, DH Ji, and QM Zhu. 2007. Tree
kernel-based relation extraction with context-sensitive
structured parse tree information. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
728?736, June.
133
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1?40,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
CoNLL-2012 Shared Task:
Modeling Multilingual Unrestricted Coreference in OntoNotes
Sameer Pradhan
Raytheon BBN Technologies,
Cambridge, MA 02138
USA
pradhan@bbn.com
Alessandro Moschitti
University of Trento,
38123 Povo (TN)
Italy
moschitti@disi.unitn.it
Nianwen Xue
Brandeis University,
Waltham, MA 02453
USA
xuen@cs.brandeis.edu
Olga Uryupina
University of Trento,
38123 Povo (TN)
Italy
uryupina@gmail.com
Yuchen Zhang
Brandeis University,
Waltham, MA 02453
USA
yuchenz@brandeis.edu
Abstract
The CoNLL-2012 shared task involved pre-
dicting coreference in English, Chinese, and
Arabic, using the final version, v5.0, of the
OntoNotes corpus. It was a follow-on to the
English-only task organized in 2011. Un-
til the creation of the OntoNotes corpus, re-
sources in this sub-field of language process-
ing were limited to noun phrase coreference,
often on a restricted set of entities, such as
the ACE entities. OntoNotes provides a large-
scale corpus of general anaphoric coreference
not restricted to noun phrases or to a spec-
ified set of entity types, and covers multi-
ple languages. OntoNotes also provides ad-
ditional layers of integrated annotation, cap-
turing additional shallow semantic structure.
This paper describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task in-
cluding the format, pre-processing informa-
tion, evaluation criteria, and presents and dis-
cusses the results achieved by the participat-
ing systems. The task of coreference has
had a complex evaluation history. Potentially
many evaluation conditions, have, in the past,
made it difficult to judge the improvement in
new algorithms over previously reported re-
sults. Having a standard test set and stan-
dard evaluation parameters, all based on a re-
source that provides multiple integrated anno-
tation layers (syntactic parses, semantic roles,
word senses, named entities and coreference)
and in multiple languages could support joint
modeling and help ground and energize on-
going research in the task of entity and event
coreference.
1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Early work on corpus-based coreference resolu-
tion dates back to the mid-90s by McCarthy and
Lenhert (1995) where they experimented with deci-
sion trees and hand-written rules. Corpora to support
supervised learning of this task date back to the Mes-
sage Understanding Conferences (MUC) (Hirschman
and Chinchor, 1997; Chinchor, 2001; Chinchor and
Sundheim, 2003). The de facto standard datasets
for current coreference studies are the MUC and the
ACE1 (Doddington et al, 2004) corpora. These cor-
pora were tagged with coreferring entities in the
form of noun phrases in the text. The MUC corpora
cover all noun phrases in text but are relatively small
in size. The ACE corpora, on the other hand, cover
much more data, but the annotation is restricted to a
small subset of entities.
Automatic identification of coreferring entities
and events in text has been an uphill battle for sev-
eral decades, partly because it is a problem that re-
quires world knowledge to solve and word knowl-
edge is hard to define, and partly owing to the lack
of substantial annotated data. Aside from the fact
that resolving coreference in text is simply a very
hard problem, there have been other hindrances that
further contributed to the slow progress in this area:
(i) Smaller sized corpora such as MUC which cov-
ered coreference across all noun phrases. Cor-
pora such as ACE which are larger in size, but
cover a smaller set of entities; and
(ii) low consistency in existing corpora annotated
with coreference ? in terms of inter-annotator
agreement (ITA) (Hirschman et al, 1998) ?
owing to attempts at covering multiple coref-
erence phenomena that are not equally anno-
tatable with high agreement which likely less-
ened the reliability of statistical evidence in the
form of lexical coverage and semantic related-
ness that could be derived from the data and
1http://projects.ldc.upenn.edu/ace/data/
1
used by a classifier to generate better predic-
tive models. The importance of a well-defined
tagging scheme and consistent ITA has been
well recognized and studied in the past (Poe-
sio, 2004; Poesio and Artstein, 2005; Passon-
neau, 2004). There is a growing consensus that
in order to take language understanding appli-
cations such as question answering or distilla-
tion to the next level, we need more consistent
annotation for larger amounts of broad cover-
age data to train better automatic models for
entity and event detection.
(iii) Complex evaluation with multiple evaluation
metrics and multiple evaluation scenarios,
complicated with varying training and test
partitions, led to situations where many re-
searchers report results with only one or a few
of the available metrics and under a subset of
evaluation scenarios. This has made it hard to
gauge the improvement in algorithms over the
years (Stoyanov et al, 2009), or to determine
which particular areas require further attention.
Looking at various numbers reported in litera-
ture can greatly affect the perceived difficulty
of the task. It can seem to be a very hard prob-
lem (Soon et al, 2001) or one that is relatively
easy (Culotta et al, 2007).
(iv) the knowledge bottleneck which has been a
well-accepted ceiling that has kept the progress
in this task at bay.
These issues suggest that the following steps
might take the community in the right direction to-
wards improving the state of the art in coreference
resolution:
(i) Create a large corpus with high inter-
annotator agreement possibly by restricting
the coreference annotating to phenomena that
can be annotated with high consistency, and
covering an unrestricted set of entities and
events; and
(ii) Create a standard evaluation scenario with an
official evaluation setup, and possibly several
ablation settings to capture the range of perfor-
mance. This can then be used as a standard
benchmark by the research community.
(iii) Continue to improve learning algorithms that
better incorporate world knowledge and jointly
incorporate information from other layers of
syntactic and semantic annotation to improve
the state of the art.
One of the many goals of the OntoNotes
project2 (Hovy et al, 2006; Weischedel et al, 2011)
2http://www.bbn.com/nlp/ontonotes
was to explore whether it could fill this void and help
push the progress further ? not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-
ers entities and events not limited to noun phrases
or a subset of entity types. The coreference layer
in OntoNotes constitutes just one part of a multi-
layered, integrated annotation of shallow semantic
structures in text with high inter-annotator agree-
ment. This addresses the first issue.
In the language processing community, the field
of speech recognition probably has the longest his-
tory of shared evaluations held primary by NIST3
(Pallett, 2002). In the past decade machine trans-
lation has been a topic of shared evaluations also
by NIST4. There are many syntactic and semantic
processing tasks that are not quite amenable to such
continued evaluation efforts. The CoNLL shared
tasks over the past 15 years have filled that gap, help-
ing establish benchmarks and advance the state of
the art in various sub-fields within NLP. The impor-
tance of shared tasks is now in full display in the
domain of clinical NLP (Chapman et al, 2011) and
recently a coreference task was organized as part
of the i2b2 workshop (Uzuner et al, 2012). The
computational learning community is also witness-
ing a shift towards joint inference based evaluations,
with the two previous CoNLL tasks (Surdeanu et al,
2008; Hajic? et al, 2009) devoted to joint learning of
syntactic and semantic dependencies. A SemEval-
2010 coreference task (Recasens et al, 2010) was
the first attempt to address the second issue. It
included six different Indo-European languages ?
Catalan, Dutch, English, German, Italian, and Span-
ish. Among other corpora, a small subset (?120K)
of English portion of OntoNotes was used for this
purpose. However, the lack of a strong participa-
tion prevented the organizers from reaching any firm
conclusions. The CoNLL-2011 shared task was an-
other attempt to address the second issue. It was well
received, but the shared task was only limited to the
English portion of OntoNotes. In addition, the coref-
erence portion of OntoNotes did not have a concrete
baseline prior to the 2011 evaluation, thereby mak-
ing it challenging for participants to gauge the per-
formance of their algorithms in the absence of es-
tablished state of the art on this flavor of annotation.
The closest comparison was to the results reported
by Pradhan et al (2007b) on the newswire portion of
OntoNotes. Since the corpus also covers two other
languages from completely different language fami-
lies, Chinese and Arabic, it provided a great oppor-
tunity to have a follow-on task in 2012 covering all
3http://www.itl.nist.gov/iad/mig/publications/ASRhistory/index.html
4http://www.itl.nist.gov/iad/mig/tests/mt/
2
three languages. As we will see later, peculiarities
of each of these languages had to be considered in
creating the evaluation framework.
The first systematic learning-based study in coref-
erence resolution was conducted on the MUC cor-
pora, using a decision tree learner, by Soon et al
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have pushed the state
of the art in coreference resolution forward (Mor-
ton, 2000; Harabagiu et al, 2001; McCallum and
Wellner, 2004; Culotta et al, 2007; Denis and
Baldridge, 2007; Rahman and Ng, 2009; Haghighi
and Klein, 2010). Researchers have continued to
find novel ways of exploiting ontologies such as
WordNet. Various knowledge sources from shallow
semantics to encyclopedic knowledge have been ex-
ploited (Ponzetto and Strube, 2005; Ponzetto and
Strube, 2006; Versley, 2007; Ng, 2007). Given
that WordNet is a static ontology and as such has
limitation on coverage, more recently, there have
been successful attempts to utilize information from
much larger, collaboratively built resources such as
Wikipedia (Ponzetto and Strube, 2006). More re-
cently researchers have used graph based algorithms
(Cai et al, 2011a) rather than pair-wise classifica-
tions. For a detailed survey of the progress in this
field, we refer the reader to a recent article (Ng,
2010) and a tutorial (Ponzetto and Poesio, 2009)
dedicated to this subject. In spite of all the progress,
current techniques still rely primarily on surface
level features such as string match, proximity, and
edit distance; syntactic features such as apposition;
and shallow semantic features such as number, gen-
der, named entities, semantic class, Hobbs? distance,
etc. Further research to reduce the knowledge gap is
essential to take coreference resolution techniques to
the next level.
The rest of the paper is organized as follows: Sec-
tion 2 presents an overview of the OntoNotes cor-
pus. Section 3 describes the range of phenomena
annotated in OntoNotes, and language-specific is-
sues. Section 4 describes the shared task data and
the evaluation parameters, with Section 4.4.2 exam-
ining the performance of the state-of-the-art tools
on all/most intermediate layers of annotation. Sec-
tion 5 describes the participants in the task. Sec-
tion 6 briefly compares the approaches taken by var-
ious participating systems. Section 7 presents the
system results with some analysis. Section 8 com-
pares the performance of the systems on the a subset
of the Engish test set that corresponds with the test
set used for the CoNLL-2011 evaluation. Section 9
draws some conclusions.
2 The OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of mul-
tiple levels of the shallow semantic structure in text.
The English and Chinese language portion com-
prises roughly one million words per language of
newswire, magazine articles, broadcast news, broad-
cast conversations, web data and conversational
speech data. The English subcorpus also contains
an additional 200K words of the English translation
of the New Testament as Pivot Text. The Arabic por-
tion is smaller, comprising 300K words of newswire
articles. The hope is that this rich, integrated an-
notation covering many layers will allow for richer,
cross-layer models and enable significantly better
automatic semantic analysis. In addition to coref-
erence, this data is also tagged with syntactic trees,
propositions for most verb and some noun instances,
partial verb and noun word senses, and 18 named en-
tity types. Manual annotation of a large corpus with
multiple layers of syntax and semantic information
is a costly endeavor. Over the years in the devel-
opment of this corpus, there were various priorities
that came into play, and therefore not all the data in
the corpus could be annotated with all the different
layers of annotation. However, such multi-layer an-
notations, with complex, cross-layer dependencies,
demands a robust, efficient, scalable storage mech-
anism while providing efficient, convenient, inte-
grated access to the the underlying structure. To
this effect, it uses a relational database representa-
tion that captures both the inter- and intra-layer de-
pendencies and also provides an object-oriented API
for efficient, multi-tiered access to this data (Prad-
han et al, 2007a). This facilitates the extraction of
cross-layer features in integrated predictive models
that will make use of these annotations.
OntoNotes comprises the following layers of an-
notation:
? Syntax ? A layer of syntactic annotation for
English, Chinese and Arabic based on a revised
guidelines for the Penn Treebank (Marcus et
al., 1993; Babko-Malaya et al, 2006), the Chi-
nese Treebank (Xue et al, 2005) and the Arabic
Treebank (Maamouri and Bies, 2004).
? Propositions ? The proposition structure of
verbs based on revised guidelines for the En-
glish PropBank (Palmer et al, 2005; Babko-
Malaya et al, 2006), the Chinese PropBank
(Xue and Palmer, 2009) and the Arabic Prop-
Bank (Palmer et al, 2008; Zaghouani et al,
2010).
? Word Sense ? Coarse-grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize token
3
coverage. The word sense granularity is tai-
lored to achieve 90% inter-annotator agreement
as demonstrated by Palmer et al (2007). These
senses are defined in the sense inventory files.
In case of English and Arabic languages, the
sense-inventories (and frame files) are defined
separately for each part of speech that is real-
ized by the lemma in the text. For Chinese,
however the sense inventories (and frame files)
are defined per lemma ? independent of the
part of speech realized in the text. For the
English portion of OntoNotes, each individual
sense has been connected to multiple WordNet
senses. This provides users direct access to the
WordNet semantic structure. There is also a
mapping from the OntoNotes word senses to
PropBank frames and to VerbNet (Kipper et
al., 2000) and FrameNet (Fillmore et al, 2003).
Unfortunately, owing to lack of comparable re-
sources as comprehensive as WordNet in Chi-
nese or Arabic, neither language has any inter-
resource mappings available.
? Named Entities ? The corpus was tagged
with a set of 18 well-defined proper named en-
tity types that have been tested extensively for
inter-annotator agreement by Weischedel and
Burnstein (2005).
? Coreference ? This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a lim-
ited set of entity types (Pradhan et al, 2007b).
It considers all pronouns (PRP, PRP$), noun
phrases (NP) and heads of verb phrases (VP)
as potential mentions. Unlike English, Chinese
and Arabic have dropped subjects and objects
which were also considered during coreference
annotation5. We will take a look at this in detail
in the next section.
3 Coreference in OntoNotes
General anaphoric coreference that spans a rich
set of entities and events ? not restricted to a few
types, as has been characteristic of most coreference
data available until now ? has been tagged with a
high degree of consistency in the OntoNotes corpus.
Two different types of coreference are distinguished:
Identity (IDENT), and Appositive (APPOS). Identity
coreference (IDENT) is used for anaphoric corefer-
ence, meaning links between pronominal, nominal,
and named mentions of specific referents. It does not
include mentions of generic, underspecified, or ab-
stract entities. Appositives (APPOS) are treated sep-
arately because they function as attributions, as de-
scribed further below. Coreference is annotated for
all specific entities and events. There is no limit on
5As we will see later these are not used during the task.
the semantic types of NP entities that can be consid-
ered for coreference, and in particular, coreference
is not limited to ACE types. The guidelines are fairly
language independent. We will look at some salient
aspects of the coreference annotation in OntoNotes.
For more details, and examples, we refer the reader
to the release documentation. We will primarily use
English examples to describe various aspects of the
annotation and use Chinese and Arabic examples es-
pecially to illustrate phenomena not observed in En-
glish, or that have some language specific peculiari-
ties.
3.1 Noun Phrases
The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by automat-
ically extracting all of the NP mentions from parse
trees in the syntactic layer of OntoNotes annotation,
though the annotators can also add additional men-
tions when appropriate. In the following two exam-
ples (and later ones), the phrases in bold form the
links of an IDENT chain.
(1) She had a good suggestion and it was unani-
mously accepted by all.
(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst?s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are ?slightly above? fiscal 1989 sales of $
155 million.
Noun phrases (NPs) in Chinese can be complex
noun phrases or bare nouns (nouns that lack a de-
terminer such as ?the? or ?this?). Complex noun
phrases contain structures modifying the head noun,
as in the following examples:
(3) (??????????? ? (???
????? (???))).
((His last APEC (summit meeting)) as the
President)
(4) (?? ?? ? (?? ? ?? ?? ?? ?
(????)))
((The first (U.S. president)) who went to visit
Vietnam after its unification)
In these examples, the smallest phrase in paren-
theses is the bare noun. The longer phrase in paran-
theses includes modifying structures. All the expres-
sions in the parantheses, however, share the same
head noun, i.e., ???? (summit meeting)?, and
????? (U.S. president)? respectively. Nested
noun phrases, or nested NPs, are contained within
4
longer noun phrases. In the above example, ?sum-
mit meeting? and ?U.S. president? are nested NPs.
Wherever NPs are nested, the largest logical span is
used in coreference.
3.2 Verbs
Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with another
verb. The intent is to annotate the VP, but the single-
word verb head is marked for convenience. This
includes morphologically related nominalizations as
in (5) and noun phrases that refer to the same event,
even if they are lexically distinct from the verb as in
(6). In the following two examples, only the chains
related to the growth event are shown in bold. The
Arabic translation of the same example identifies
mentions using parantheses.
(5) The European economy grew rapidly over the
past years, this growth helped raising ....
H@?
	
J??@ ?C
	
g

??Q??. ?

G
.
?P?

B@ XA?

J

?B

@ ( A? 	? ) Y??
. . . ?
	
P? ?


	
? ??A? ( ??	J? @ @ 	Y? ) , ?J

	
?A?? @
(6) Japan?s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers? Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.
3.3 Pronouns
All pronouns and demonstratives are linked to
anything that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged in the coreference annotation.)
(7) Senate majority leader Bill Frist likes to tell
a story from his days as a pioneering heart
surgeon back in Tennessee. A lot of times,
Frist recalls, *you?d have a critical patient ly-
ing there waiting for a new heart, and *you?d
want to cut, but *you couldn?t start unless *you
knew that the replacement heart would make
*it to the operating room.
In Chinese, all the following pronouns ? ??
???, ?, ?? ????????????
?, ?, ?? (you, me, he, she, and so on), and
demonstrative pronouns ?????????,?
? (this, that, these, those) in singular, plural or pos-
sessive forms are linked to anything they refer to.
Pronouns from classical Chinese such as ? ?
(among which),? (he/she/it),? (he/she/it) are also
linked with other mentions to which they refer.
In Arabic, the following pronouns are corefer-
enced ? nominative personal pronouns (subject) and
demonstrative pronouns which are detached. Sub-
ject pronouns are often null in Arabic; overt subject
pronouns are rare, but do occur.
	?
	
K @ / ?

?
	
K @ / A?

J
	
K @ / 	?mProceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122?128,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
BART goes multilingual: The UniTN / Essex submission to the CoNLL-2012
Shared Task
Olga Uryupina? Alessandro Moschitti? Massimo Poesio??
?University of Trento
? University of Essex
uryupina@gmail.com, moschitti@disi.unitn.it, massimo.poesio@unitn.it
Abstract
This paper describes the UniTN/Essex sub-
mission to the CoNLL-2012 Shared Task on
the Multilingual Coreference Resolution. We
have extended our CoNLL-2011 submission,
based on BART, to cover two additional lan-
guages, Arabic and Chinese. This paper fo-
cuses on adapting BART to new languages,
discussing the problems we have encountered
and the solutions adopted. In particular, we
propose a novel entity-mention detection algo-
rithm that might help identify nominal men-
tions in an unknown language. We also dis-
cuss the impact of basic linguistic information
on the overall performance level of our coref-
erence resolution system.
1 Introduction
A number of high-performance coreference resolu-
tion (CR) systems have been created for English in
the past decades, implementing both rule-based and
statistical approaches. For other languages, how-
ever, the situation is far less optimistic. For Ro-
mance and German languages, several systems have
been developed and evaluated, in particular, at the
SemEval-2010 track 1 on Multilingual Coreference
Resolution (Recasens et al, 2010). For other lan-
guages, individual approaches have been proposed,
covering specific subparts of the task, most com-
monly pronominal anaphors (cf., for example, (Iida
and Poesio, 2011; Arregi et al, 2010) and many oth-
ers).
Two new languages, Arabic and Chinese, have
been proposed for the CoNLL-2012 shared task
(Pradhan et al, 2012). They present a challeng-
ing problem: the systems are required to pro-
vide entity mention detection (EMD) and design a
proper coreference resolver for both languages. At
UniTN/Essex, we have focused on these parts of the
task, relying on a modified version of our last-year
submission for English.
Most state-of-the-art full-scale coreference reso-
lution systems rely on hand-written rules for the
mention detection subtask.1 For English, such rules
may vary from corpus to corpus, reflecting specifics
of particular guidelines (e.g. whether nominal pre-
modifiers can be mentions, as in MUC, or not, as in
most other corpora). However, for each corpus, such
heuristics can be adjusted in a straightforward way.
Creating a robust rule-based EMD module for a new
language, on the contrary, is a challenging issue that
requires substantial linguistic knowledge.
In this paper, we advocate a novel approach, re-
casting parse-based EMD as a statistical problem.
We consider a node-filtering model that does not rely
on any linguistic expertise in a given language. In-
stead, we use tree kernels (Moschitti, 2008; Mos-
chitti, 2006) to induce a classifier for mention NP-
nodes automatically from the data.
Another issue to be solved when designing a
coreference resolution system for a new language
is a possible lack of relevant linguistic information.
Most state-of-the-art CR algorithms rely on rela-
tively advanced linguistic representations of men-
tions. This can be seen as a remarkable shift
1Statistical EMD approaches have been proved useful for
ACE-style coreference resolution, where mentions are basic
units belonging to a restricted set of semantic types.
122
from knowledge-lean approaches of the late nineties
(Harabagiu and Maiorano, 1999). In fact, modern
systems try to account for complex coreference links
by incorporating lexicographic and world knowl-
edge, for example, using WordNet (Harabagiu et al,
2001; Huang et al, 2009) or Wikipedia (Ponzetto
and Strube, 2006). For languages other than English,
however, even the most basic properties of mentions
can be intrinsically difficult to extract. For example,
Baran and Xue (2011) have shown that a complex al-
gorithm is needed to identify the number property
of Chinese nouns.
Both Arabic and Chinese have long linguistic tra-
ditions and therefore most grammar studies rely on
terminology that can be very confusing for an out-
sider. For example, several works on Arabic (Hoyt,
2008) mention that nouns can be made definite with
the suffix ?Al-?, but this is not a semantic, but syn-
tactic definiteness. Without any experience in Ara-
bic, one can hardly decide how such ?syntactic defi-
niteness? might affect coreference.
In the present study, we have used the informa-
tion provided by the CoNLL organizers to try and
extract at least some linguistic properties of men-
tions for Arabic and Chinese. We have run several
experiments, evaluating the impact of such very ba-
sic knowledge on the performance level of a coref-
erence resolution system.
The rest of the paper is organized as follows. In
the next section we briefly describe the general ar-
chitecture and the system for English, focusing on
the adjustments made after the last year competition.
Section 3 is devoted to new languages: we first dis-
cuss our EMD module and then describe the proce-
dures for extracting linguistic knowledge. Section 4
discusses the impact of our solutions to the perfor-
mance level of a coreference resolver. The official
evaluation results are presented in Section 5.
2 BART
Our CoNLL submission is based on BART (Versley
et al, 2008). BART is a modular toolkit for corefer-
ence resolution that supports state-of-the-art statisti-
cal approaches to the task and enables efficient fea-
ture engineering. BART has originally been created
and tested for English, but its flexible modular archi-
tecture ensures its portability to other languages and
domains.
The BART toolkit has five main components: pre-
processing pipeline, mention factory, feature extrac-
tion module, decoder and encoder. In addition, an
independent LanguagePlugin module handles all the
language specific information and is accessible from
any component.
The architecture is shown in Figure 1. Each mod-
ule can be accessed independently and thus adjusted
to leverage the system?s performance on a particular
language or domain.
The preprocessing pipeline converts an input doc-
ument into a set of linguistic layers, represented
as separate XML files. The mention factory uses
these layers to extract mentions and assign their
basic properties (number, gender etc). The fea-
ture extraction module describes pairs of mentions
{Mi,Mj}, i < j as a set of features. At the
moment we have around 45 different feature ex-
tractors, encoding surface similarity, morpholog-
ical, syntactic, semantic and discourse informa-
tion. Note that no language-specific information
is encoded in the extractors explicitly: a language-
independent representation, provided by the Lan-
guage Plugin, is used to compute feature val-
ues. For CoNLL-2012, we have created two addi-
tional features: lemmata-match (similar to string
match, but uses lemmata instead of tokens) and
number-agreement-du (similar to commonly
used number agreement features, but supports dual
number).
The encoder generates training examples through
a process of sample selection and learns a pairwise
classifier. Finally, the decoder generates testing ex-
amples through a (possibly distinct) process of sam-
ple selection, runs the classifier and partitions the
mentions into coreference chains.
2.1 Coreference resolution in English
The English track at CoNLL-2012 can be considered
an extension of the last year?s CoNLL task. New
data have been added to the corpus, including two
additional domains, but the annotation guidelines re-
main the same.
We have therefore mainly relied on the CoNLL-
2011 version of our system (Uryupina et al, 2011)
for the current submission, providing only minor ad-
justments. Thus, we have modified our preprocess-
123
POS Tagger
Merger
Mention Tagger
Parser
Preprocessing
Mention
Factory
Coreference
chains(entities)
Language Plugin
UnannotatedText
FeatureExtractor
LearnerMachine
Encoder/Decoder
Figure 1: BART architecture
ing pipeline to operate on the OntoNotes NE-types,
mapping them into MUC types required by BART.
This allows us to participate in the closed track, as
no external material is used any longer.
Since last year, we have continued with our exper-
iments on multi-objective optimization, proposed in
our CoNLL-2011 paper (Uryupina et al, 2011). We
have extended the scope of our work to cover differ-
ent machine learning algorithms and their parame-
ters (Saha et al, 2011). For CoNLL-2012, we have
re-tested all the solutions of our optimization exper-
iments, picking the one with the highest score on the
current development set.
Finally, our recent experiments on domain se-
lection (Uryupina and Poesio, 2012) suggest that,
at least for some subparts of OntoNotes, a sys-
tem might benefit from training a domain-specific
model. We have tested this hypothesis on the
CoNLL-2012 data and have consequently trained
domain-specific classifiers for the nw and bc do-
mains.
3 Coreference resolution in Arabic and
Chinese
We have addressed two main issues when develop-
ing our coreference resolvers for Arabic and Chi-
nese: mention detection and extraction of relevant
linguistic properties of our mentions.
3.1 Mention detection
Mention detection is rarely considered to be a sepa-
rate task. Only very few studies on coreference reso-
lution report on their EMD techniques. Existing cor-
pora of coreference follow different approaches to
mention annotation: this includes defining mention
boundaries (basic vs. maximal NPs), alignment pro-
cedures (strict vs. relaxed with manually annotated
minimal spans vs. relaxed with automatically ex-
tracted heads), the position on singleton and/or non-
referential mentions (annotated vs. not).
The CoNLL-2011/2012 guidelines take a very
strict view on mention boundaries: only the maxi-
mal spans are annotated and no approximate match-
ing is allowed. Moreover, the singleton mentions
(i.e. not participating in coreference relations) are
not marked. This makes the mention detection task
for OntoNotes extremely challenging, especially for
the two new languages: on the one hand, one has
to provide exact boundaries; on the other hand, it is
hard to learn such information explicitly, as not all
the candidate mentions are annotated.
Most CoNLL-2011 systems relied on hand-
written rules for the mention detection subtask. This
was mainly possible due to the existence of well-
studied and thoroughly documented head-detection
rules for English, available as a description for reim-
plementing (Collins, 1999) or as a downloadable
package. Consider the following example:
(1) ..((the rising price)NP2 of (food)NP3)NP1 ..
124
In this fragment, three nominal phrases can be iden-
tified, with the first one (?the rising price of food?)
spanning over the two others (?the rising price?) and
(?food?). According to the OntoNotes annotation
guidelines, the second noun phrase cannot be a men-
tion, because it is embedded in an upper NP and they
share the same head noun. The third noun phrase, on
the contrary, could be a mention?even though it?s
embedded in another NP, their heads are different.
Most CoNLL-2011 participants used as a backbone
a heuristic discarding embedded noun phrases.
For less-known languages, however, this heuris-
tic is only applicable as long as we can compute an
NP?s head reliably. Otherwise it?s hard to distinguish
between candidate mentions similar to NP1 and to
NP2 in the example above.
A set of more refined heuristics is typically ap-
plied to discard or add some specific types of men-
tions. For example, several studies (Bergsma and
Yarowsky, 2011) have addressed the issue of detect-
ing expletive pronouns in English. Again, in the ab-
sence of linguistic expertise, one can hardly engi-
neer such heuristics for a new language manually.
We have investigated the possibility of learn-
ing mention boundaries automatically from the
OntoNotes data. We recast the problem as an NP-
node filtering task: we analyze automatically com-
puted parse trees and consider all the NP-nodes to be
candidate instances to learn a classifier of correct vs.
incorrect mention nodes. Clearly, this approach can-
not account for mentions that do not correspond to
NP-nodes. However, as Table 1 shows, around 85-
89% of all the mentions, both for Arabic and Chi-
nese, are NP-nodes.
train development
NP-nodes % NP-nodes %
Arabic 24068 87.23 2916 87.91
Chinese 88523 85.96 12572 88.52
Table 1: NP-nodes in OntoNotes for Arabic and Chinese:
total numbers and percentage of mentions.
We use tree kernels (Moschitti, 2008; Moschitti,
2006) to induce a classifier that labels an NP node
and a part of the parse tree that surrounds it as
?mention. Two integer parameters control the se-
lection of the relevant part of the parse tree, allowing
for pruning the nodes that are far above or far below
the node of interest.
Our classifier is supposed to decide whether an
NP-node is a mention of a real-world object. Such
mentions, however, are annotated in OntoNotes as
positive instances only when they corefer with some
other mentions. The classifier works as a preproces-
sor for a CR system and therefore has no information
that would allow it to discriminate between single-
ton vs. non-singleton mentions. One can investigate
possibilities for joint EMD and CR to alleviate the
problem. We have adopted a simpler solution: we
tune a parameter (cost factor) that controls the pre-
cision/recall trade-off to bias the classifier strongly
towards recall.
We use a small subset (1-5%) of the training data
to train the EMD classifier. We tune the EMD pa-
rameters to optimize the overall performance: we
run the classifier to extract mentions for the whole
training and development sets, run the coreference
resolver and record the obtained result (CoNLL
score). The whole set of parameters to be tuned
comprise: the size of the training set for EMD, the
precision-recall trade-off, and two pruning thresh-
olds.
3.2 Extracting linguistic properties
All the features implemented in BART use some
kind of linguistic information from the mentions.
For example, the number-agreement feature
first extracts the number properties of individual
mentions. For a language supported by BART, such
properties are computed by the MentionFactory. For
a new language, they should be provided as a part of
the mention representation computed by some ex-
ternal preprocessing facilities. The only obligatory
mention property is its span? the sequence of rel-
evant token ids?all the properties discussed below
are optional.
The following properties have been extracted for
new languages directly from the CoNLL table:
? sentence id
? sequence of lemmata
? speaker (Chinese only)
Coordinations have been determined by analyz-
ing the sequence of PoS tags: any span containing
125
a coordinate conjunction is a coordination. They are
always considered plural and unspecified for gender,
their heads correspond to their entire spans.
For non-coordinate NPs, we extract the head
nouns using simple heuristics. In Arabic, the first
noun in a sequence is a head. In Chinese, the last
one is a head. If no head can be found through this
heuristic, we try the same method, but allow for pro-
nouns to be heads, and, as a default, consider the
whole span to be the head.
Depending on the PoS tag of the head noun, we
classify a mention as an NE, a pronoun or a nomi-
nal (default). For named entities, no further mention
properties have been extracted.
We have compiled lists of pronouns for both Ara-
bic and Chinese from the training and development
data. For Arabic, we use gold PoS tags to classify
pronouns into subtypes, person, number and gender.
For Chinese, no such information is available, so we
have consulted several grammar sketches and lists of
pronouns on the web. We do not encode clusivity2
and honorifics.3
For Arabic, we extract the gender and number
properties of nominals in the following way. First,
we have processed the gold PoS tags to create a list
of number and gender affixes. We compute the prop-
erties of our mentions by analyzing the affixes of
their heads. In a number of constructions, however,
the gender is not marked explicitly, so we have com-
piled a gender dictionary for Arabic lemmata on the
training and development data. If the gender can-
not be computed from affixes, we look it up in the
dictionary.
Finally, we have made an attempt at computing
the definiteness of nominal expressions. For Arabic,
we consider as definites all mentions with definite
head nouns (prefixed with ?Al?) and all the idafa
constructs with a definite modifier.4 We could not
compute definiteness for Chinese reliably.
2In some dialects of Chinese, a distinction is made between
the first person plural inclusive (?you and me?) and the first
person exclusive (?me and somebody else?) pronouns.
3In Chinese, different pronouns should be used address-
ing different persons, reflecting the relative social status of the
speaker and the listener.
4Idafa-constructs are syntactic structures, conveying, very
roughly speaking, genitive semantics, commonly used in Ara-
bic. Their accurate analysis requires some language-specific
processing.
4 Evaluating the impact of kernel-based
mention detection and basic linguistic
knowledge
To adopt our system to new languages, we have fo-
cused on two main issues: EMD and extraction of
linguistic properties. In this section we discuss the
impact of each factor on the overall performance.
Table 2 summarizes our evaluation experiments. All
the figures reported in this section are CoNLL scores
(averages of MUC, B3 and CEAFe) obtained on the
development data.
To evaluate the impact of our kernel-based EMD
(TKEMD), we compare its performance against two
baselines. The lower bound, ?allnp?, considers all
the NP-nodes in a parse tree to be candidate men-
tions. The upper bound, ?goldnp? only considers
gold NP-nodes to be candidate mentions. Note that
the upper bound does not include mentions that do
not correspond to NP-nodes at all (around 12% of
all the mentions in the development data, cf. Table 1
above).
We have created three versions of our corefer-
ence resolver, using different amounts of linguistic
knowledge. The baseline system (Table 2, first col-
umn) relies only on mention spans. The system it-
self is a reimplementation of Soon et al (2001), but,
clearly, only the string-matching feature can be com-
puted without specifying mention properties.
A more advanced version of the system (second
column) uses the same model and the same feature
set, but relies on mention properties, extracted as de-
scribed in Section 3.2 above. The final version (third
column) makes use of all the features implemented
in BART. We run a greedy feature selection algo-
rithm, starting from the string matching and adding
features one by one, until the performance stops in-
creasing.
For Chinese, our EMD approach has proved to be
useful, bringing around 1.5-2% improvement over
the ?allnp? baseline for all the versions of the coref-
erence resolver. The module for extracting mention
properties has only brought a moderate improve-
ment. This is not surprising, as we have not been
able to extract many relevant linguistic properties,
especially for nominals. We believe that an improve-
ment can be achieved on the Chinese data by incor-
porating more linguistic information.
126
baseline +linguistics +linguistics
+features
Arabic
allnp 45.47 46.15 46.32
TKEMD 46.98 47.44 49.07
goldnp 51.08 63.27 64.55
Chinese
allnp 50.72 51.04 51.40
TKEMD 53.10 53.33 53.53
goldnp 57.78 57.30 57.98
Table 2: Evaluating the impact of EMD and linguistic
knowledge: CoNLL F-score.
For Arabic, the linguistic properties could poten-
tially be very helpful: on gold NPs, our linguistically
rich system outperforms its knowledge-lean coun-
terpart by 13 percentage points. Unfortunately, this
improvement is mirrored only partially on the fully
automatically acquired mentions.
5 Official results
Table 3 shows the official results obtained by our
system at the CoNLL-2012 competition.
Metric Recall Precision F-score
English
MUC 61.00 60.78 60.89
BCUBED 63.59 68.48 65.95
CEAF (M) 52.44 52.44 52.44
CEAF (E) 41.42 41.64 41.53
BLANC 67.40 72.83 69.65
Arabic
MUC 41.33 41.66 41.49
BCUBED 65.77 69.23 67.46
CEAF (M) 50.82 50.82 50.82
CEAF (E) 42.43 42.13 42.28
BLANC 65.58 70.56 67.69
Chinese
MUC 45.62 63.13 52.97
BCUBED 59.17 80.78 68.31
CEAF (M) 52.40 52.40 52.40
CEAF (E) 48.47 34.52 40.32
BLANC 68.72 80.76 73.11
Table 3: BART performance at CoNLL-2012: official re-
sults on the test set.
6 Conclusion
In this paper we have discussed our experiments
on adapting BART to two new languages, Chinese
and Arabic, for the CoNLL-2012 Shared Task on
the Multilingual Coreference Resolution. Our team
has some previous experience with extending BART
to cover languages other than English, in particular,
Italian and German. For those languages, however,
most of our team members had at least an advanced
knowledge, allowing for more straightforward engi-
neering and error analysis. Both Arabic and Chi-
nese present a challenge: they require new mention
detection algorithms, as well as special language-
dependent techniques for extracting mention prop-
erties.
For Arabic, we have proposed several simple ad-
justments to extract basic morphological informa-
tion. As our experiments show, this can potentially
lead to a substantial improvement. The progress,
however, is hindered by the mention detection qual-
ity: even though our TKEMD module outperforms
the lower bound baseline, there is still a lot of
room for improvement, that can be achieved after
a language-aware error analysis.
For Chinese, the subtask of extracting relevant lin-
guistic information has turned out to be very chal-
lenging. We believe that, by elaborating on the
methods for assigning linguistic properties to nomi-
nal mentions and combining them with the TKEMD
module, one can boost the performance level of a
coreference resolver.
7 Acknowledgments
The research described in this paper has been par-
tially supported by the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der the grants #247758: ETERNALS ? Trustworthy
Eternal Systems via Evolving Software, Data and
Knowledge, and #288024: LIMOSINE ? Linguis-
tically Motivated Semantic aggregation engiNes.
127
References
Olatz Arregi, Klara Ceberio, Arantza D??az De Illar-
raza, Iakes Goenaga, Basilio Sierra, and Ana Zelaia.
2010. A first machine learning approach to pronom-
inal anaphora resolution in Basque. In Proceedings
of the 12th Ibero-American conference on Advances in
artificial intelligence, IBERAMIA?10, pages 234?243,
Berlin, Heidelberg. Springer-Verlag.
Elizabeth Baran and Nianwen Xue. 2011. Singular or
plural? Exploiting parallel corpora for Chinese num-
ber prediction. In Proceedings of the Machine Trans-
lation Summit XIII.
Shane Bergsma and David Yarowsky. 2011. NADA:
A robust system for non-referential pronoun detec-
tion. In Proceedings of the Discourse Anaphora and
Anaphor Resolution Colloquium, Faro, Portugal, Oc-
tober.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Sanda Harabagiu and Steven Maiorano. 1999.
Knowledge-lean coreference resolution and its rela-
tion to textual cohesion and coherence. In Proceed-
ings of the ACL Workshop On The Relation Of Dis-
course/Dialogue Structure And Reference.
Sanda Harabagiu, Ra?zvan Bunescu, and Steven Maio-
rano. 2001. Text and knowledge mining for coref-
erence resolution. In Proceedings of the 2nd Meeting
of the North American Chapter of the Association for
Computational Linguistics, pages 55?62.
Frederick Hoyt. 2008. The Arabic noun phrase. In
The Encyclopedia of Arabic Language and Linguis-
tics. Leiden:Brill.
Zhiheng Huang, Guangping Zeng, Weiqun Xu, and Asli
Celikyilmaz. 2009. Effectively exploiting WordNet
in semantic class classification for coreference resolu-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics, pages 804?813.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of European Conference on Machine
Learning, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of the International Conference on Informa-
tion and Knowledge Management, NY, USA.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 192?199.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M.Anto`nia Mart??, Mariona Taule?, Ve?ronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Sriparna Saha, Asif Ekbal, Olga Uryupina, and Massimo
Poesio. 2011. Single and multi-objective optimiza-
tion for feature selection in anaphora resolution. In
Proceedings of the International Joint Conference on
Natural Language Processing.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistic, 27(4):521?544.
Olga Uryupina and Massimo Poesio. 2012. Domain-
specific vs. uniform modeling for coreference resolu-
tion. In Proceedings of the Language Resources and
Evaluation Conference.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Mas-
simo Poesio. 2011. Multi-metric optimization for
coreference: The UniTN / IITP / Essex submission to
the 2011 CONLL shared task. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011).
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: a modular toolkit for coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies, pages 9?12.
128
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 75?83,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Learning Adaptable Patterns for Passage Reranking
Aliaksei Severyn(1) and Massimo Nicosia(1) and Alessandro Moschitti1,2
(1)DISI, University of Trento, 38123 Povo (TN), Italy
{severyn,m.nicosia,moschitti}@disi.unitn.it
(2)QCRI, Qatar Foundation, 5825 Doha, Qatar
amoschitti@qf.org.qa
Abstract
This paper proposes passage reranking
models that (i) do not require manual fea-
ture engineering and (ii) greatly preserve
accuracy, when changing application do-
main. Their main characteristic is the
use of relational semantic structures rep-
resenting questions and their answer pas-
sages. The relations are established us-
ing information from automatic classifiers,
i.e., question category (QC) and focus
classifiers (FC) and Named Entity Recog-
nizers (NER). This way (i) effective struc-
tural relational patterns can be automati-
cally learned with kernel machines; and
(ii) structures are more invariant w.r.t. dif-
ferent domains, thus fostering adaptability.
1 Introduction
A critical issue for implementing Question An-
swering (QA) systems is the need of designing
answer search and extraction modules specific to
the target application domain. These modules en-
code handcrafted rules based on syntactic patterns
that detect the relations between a question and its
candidate answers in text fragments. Such rules
are triggered when patterns in the question and the
passage are found. For example, given a ques-
tion1:
What is Mark Twain?s real name?
and a relevant passage, e.g., retrieved by a search
engine:
Samuel Langhorne Clemens, better
known as Mark Twain.
the QA engineers typically apply a syntactic parser
to obtain the parse trees of the above two sen-
tences, from which, they extract rules like:
1We use this question/answer pair from TREC QA as a
running example in the rest of the paper.
if the pattern ?What is NP2?s ADJ
name? is in the question and the pat-
tern ?NP1 better known as NP2?
is in the answer passage then associate
the passage with a high score2.
Machine learning has made easier the task of
QA engineering by enabling the automatic learn-
ing of answer extraction modules. However, new
features and training data have to be typically de-
veloped when porting a QA system from a domain
to another. This is even more critical considering
that effective features tend to be as much complex
and similar as traditional handcrafted rules.
To reduce the burden of manual feature engi-
neering for QA, we proposed structural models
based on kernel methods, (Moschitti et al, 2007;
Moschitti and Quarteroni, 2008; Moschitti, 2008)
with passages limited to one sentence. Their main
idea is to: (i) generate question and passage pairs,
where the text passages are retrieved by a search
engine; (ii) assuming those containing the correct
answer as positive instance pairs and all the oth-
ers as negative ones; (iii) represent such pairs with
two syntactic trees; and (ii) learn to rank answer
passages by means of structural kernels applied to
two trees. This enables the automatic engineering
of structural/lexical semantic patterns.
More recently, we showed that such models can
be learned for passages constituted by multiple
sentences on very large-scale (Severyn and Mos-
chitti, 2012). For this purpose, we designed a shal-
low syntactic representation of entire paragraphs
by also improving the pair representation using re-
lational tags.
In this paper, we firstly use our model in (Sev-
eryn and Moschitti, 2012) as the current baseline
and compare it with more advanced structures de-
rived from dependency trees.
2If the point-wise answer is needed rather than the entire
passage, the rule could end with: returns NP1
75
Search Engine
Kernel-based 
reranker
Reranked
answers
Candidate 
answers
Query
Evaluation
UIMA pipeline
NLP 
annotators
Focus and 
Question 
classifiers
syntactic/semantic 
graph
q/a similarity 
features
train/test 
data
Figure 1: Kernel-based Answer Passage Reranking system
Secondly, we enrich the semantic representa-
tion of QA pairs with the categorical informa-
tion provided by automatic classifiers, i.e., ques-
tion category (QC) and focus classifiers (FC) and
Named Entity Recognizers (NER). FC determines
the constituent of the question to be linked to the
named entities (NEs) of the answer passage. The
target NEs are selected based on their compatibil-
ity with the category of the question, e.g., an NE
of type PERSON is compatible with a category of
a question asking for a human (HUM).
Thirdly, we tested our models in a cross-domain
setting since we believe that: (i) the enriched rep-
resentation is supposed to increase the capability
of learning effective structural relational patterns
through kernel machines; and (ii) such structural
features are more invariant with respect to differ-
ent domains, fostering their adaptability.
Finally, the results show that our methods
greatly improve on IR baseline, e.g., BM25, by
40%, and on previous reranking models, up to
10%. In particular, differently from our previous
work such models can effectively use NERs and
the output of different automatic modules.
The rest of the paper is organized as follows,
Sec. 2 describes our kernel-based reranker, Sec. 3
illustrates our question/answer relational struc-
tures; Sec. 5 briefly describes the feature vectors,
and finally Sec. 6 reports the experimental results
on TREC and Answerbag data.
2 Learning to rank with kernels
2.1 QA system
Our QA system is based on a rather simple rerank-
ing framework as displayed in Figure 1: given a
query question a search engine retrieves a list of
candidate passages ranked by their relevancy. Var-
ious NLP components embedded in the pipeline as
UIMA3 annotators are then used to analyze each
question together with its candidate answers, e.g.,
part-of-speech tagging, chunking, named entity
recognition, constituency and dependency pars-
ing, etc. These annotations are then used to
produce structural models (described in Sec. 3),
which are further used by a question focus detector
and question type classifiers to establish relational
links for a given question/answer pair. The result-
ing tree pairs are then used to train a kernel-based
reranker, which outputs the model to refine the ini-
tial ordering of the retrieved answer passages.
2.2 Tree kernels
We use tree structures as our base representation
since they provide sufficient flexibility in repre-
sentation and allow for easier feature extraction
than, for example, graph structures. We rely on
the Partial Tree Kernel (PTK) (Moschitti, 2006) to
handle feature engineering over the structural rep-
resentations. The choice of PTK is motivated by
its ability to generate rich feature spaces over both
constituency and dependency parse trees. It gen-
eralizes a subset tree kernel (STK) (Collins and
Duffy, 2002) that maps a tree into the space of
all possible tree fragments constrained by the rule
that the sibling nodes from their parents cannot be
separated. Different from STK where the nodes
in the generated tree fragments are constrained to
include none or all of their direct children, PTK
fragments can contain any subset of the features,
i.e., PTK allows for breaking the production rules.
Consequently, PTK generalizes STK, thus gener-
ating an extremely rich feature space, which re-
sults in higher generalization ability.
2.3 Preference reranking with kernels
To enable the use of kernels for learning to
rank with SVMs, we use preference reranking
(Joachims, 2002), which reduces the task to bi-
nary classification. More specifically, the problem
of learning to pick the correct candidate hi from
a candidate set {h1, . . . , hk} is reduced to a bi-
nary classification problem by creating pairs: pos-
itive training instances ?h1, h2?, . . . , ?h1, hk? and
negative instances ?h2, h1?, . . . , ?hk, h1?. This set
can then be used to train a binary classifier. At
classification time the standard one-versus-all bi-
narization method is applied to form all possible
3http://uima.apache.org/
76
pairs of hypotheses. These are ranked according
to the number of classifier votes they receive: a
positive classification of ?hk, hi? gives a vote to
hk whereas a negative one votes for hi.
A vectorial representation of such pairs is the
difference between the vectors representing the
hypotheses in a pair. However, this assumes that
features are explicit and already available whereas
we aim at automatically generating implicit pat-
terns with kernel methods. Thus, for keeping im-
plicit the difference between such vectors we use
the following preference kernel:
PK(?h1, h2?, ?h?1, h?2?) = K(h1, h?1)+
K(h2, h?2)?K(h1, h?2)?K(h2, h?1),
(1)
where hi and h?i refer to two sets of hypothe-
ses associated with two rankings and K is a ker-
nel applied to pairs of hypotheses. We represent
the latter as pairs of question and answer passage
trees. More formally, given two hypotheses, hi =
?hi(q), hi(a)? and hi = ?h?i(q), h?i(a)?, whose
members are the question and answer passage
trees, we define K(hi, h?i) as TK(hi(q), h?i(q)) +
TK(hi(a), h?i(a)), where TK can be any tree ker-
nel function, e.g., STK or PTK.
To enable traditional feature vectors it is enough
to add the product (~xh1 ? ~xh2) ? (~xh?1 ? ~xh?2) tothe structural kernel PK , where ~xh is the feature
vector associated with the hypothesis h.
We opted for a simple kernel sum over a prod-
uct, since the latter rarely works in practice. Al-
though in (Moschitti, 2004) the kernel product has
been shown to provide some improvement when
applied to tree kernels over a subcategorization
frame structure, in general, it seems to work well
only when the tree structures are small and derived
rather accurately (Giordani and Moschitti, 2009;
Giordani and Moschitti, 2012).
3 Structural models of Q/A pairs
First, we briefly describe a shallow tree represen-
tation that we use as our baseline model and then
propose a new dependency-based representation.
3.1 Shallow tree structures
In a shallow syntactic representation first explored
for QA in (Severyn and Moschitti, 2012) each
question and its candidate answer are encoded into
a tree where part-of-speech tags are found at the
pre-terminal level and word lemmas at the leaf
level. To encode structural relationships for a
given q/a pair a special REL tag is used to link
the related structures. The authors adopt a sim-
ple strategy to establish such links: lemmas shared
between a question and and answer get their par-
ents (POS tags) and grandparents (chunk labels)
marked by a REL tag.
3.2 Dependency-based structures
Given the ability of PTK to generate a rich set
of structural features from a relatively flat shal-
low tree representation, we propose to use depen-
dency relations between words to derive an al-
ternative structural model. In particular, we use
a variation of the dependency tree, where depen-
dency relations are altered in such a way that the
words are always at the leaf level. This reorder-
ing of the nodes in the dependency tree, s.t. words
do not form long chains, which is typical in the
standard dependency tree representation, is essen-
tial for PTK to extract meaningful fragments. We
also add part-of-speech tags between the words
and the nodes encoding their grammatical roles
(provided by the original dependency parse tree).
Again a special REL tag is used in the same man-
ner as in the shallow representation to establish
structural links between a question and an answer.
Fig. 2 (top) gives an example of a dependency-
based structure for our example q/a pair.
4 Relational Linking
The use of a special tag to mark the related frag-
ments in the question and answer tree represen-
tations has been shown to yield more accurate
relational models (Severyn and Moschitti, 2012).
However, previous approach was based on a na??ve
hard matching between word lemmas.
Below we propose a novel strategy to estab-
lish relational links using named entities extracted
from the answer along with question focus and
category classifiers. In particular, we use a ques-
tion category to link the focus word of a question
with the named entities extracted from the candi-
date answer. For this purpose, we first introduce
our tree kernel-based models for building a ques-
tion focus and category classifiers.
4.1 Question focus detection
The question focus is typically a simple noun rep-
resenting the entity or property being sought by
the question (Prager, 2006). It can be used to
search for semantically compatible candidate an-
77
NER: Person NER: Personfocus
Figure 2: Dependency-based structure DEP (top) for the q/a pair. Q: What is Mark Twain?s real name? A: Samuel Langhorne
Clemens, better known as Mark Twain. Arrows indicate the tree fragments in the question and its answer passage linked by the
relational REL tag. Shallow tree structure CH (bottom) with a typed relation tag REL-FOCUS-HUM to link a question focus
word name with the named entities of type Person corresponding to the question category (HUM).
swers in document passages, thus greatly reduc-
ing the search space (Pinchak, 2006). While sev-
eral machine learning approaches based on man-
ual features and syntactic structures have been
recently explored, e.g. (Quarteroni et al, 2012;
Damljanovic et al, 2010; Bunescu and Huang,
2010), we opt for the latter approach where tree
kernels handle automatic feature engineering.
In particular, to detect the question focus word
we train a binary SVM classifier with tree ker-
nels applied to the constituency tree representa-
tion. For each given question we generate a set
of candidate trees where the parent (node with the
POS tag) of each candidate focus word is anno-
tated with a special FOCUS tag. Trees with the
correctly tagged focus word constitute a positive
example, while the others are negative examples.
To detect the focus for an unseen question we clas-
sify the trees obtained after tagging each candidate
focus word. The tree yielding the highest classifi-
cation score reveals the target focus word.
4.2 Question classification
Question classification is the task of assigning a
question to one of the pre-specified categories. We
use the coarse-grain classes described in (Li and
Roth, 2002): six non-overlapping classes: Abbre-
viations (ABBR), Descriptions (DESC, e.g. def-
initions or explanations), Entity (ENTY, e.g. an-
imal, body or color), Human (HUM, e.g. group
or individual), Location (LOC, e.g. cities or coun-
tries) and Numeric (NUM, e.g. amounts or dates).
These categories can be used to determine the Ex-
pected Answer Type for a given question and find
the appropriate entities found in the candidate an-
swers. Imposing such constraints on the potential
answer keys greatly reduces the search space.
Previous work in Question Classification re-
veals the power of syntactic/semantic tree repre-
sentations coupled with tree kernels to train the
state-of-the-art models (Bloehdorn and Moschitti,
2007). Hence, we opt for an SVM multi-classifier
using tree kernels to automatically extract the
question class. To build a multi-class classifier
we train a binary SVM for each of the classes and
apply a one-vs-all strategy to obtain the predicted
class. We use constituency trees as our input rep-
resentation.
4.3 Linking focus word with named entities
using question class
Question focus captures the target information
need posed by a question, but to make this piece
of information effective, the focus word needs to
be linked to the target candidate answer. The focus
word can be lexically matched with words present
in an answer, or the match can be established us-
ing semantic information. Clearly, the latter ap-
proach is more appealing since it helps to allevi-
ate the lexical gap problem which makes the na?ive
string matching of words between a question and
its answer less reliable.
Hence, we propose to exploit a question cate-
gory (automatically identified by a question type
classifier) along with named entities found in the
answer to establish relational links between the
tree structures of a given q/a pair. In particu-
lar, once the question focus and question category
78
Table 1: Question classes ? named entity types.
Question Category Named Entity types
HUM Person
LOC Location
NUM Date, Time, Money, Percentage
ENTY Organization, Person
are determined, we link the focus word wfocus in
the question, with all the named entities whose
type matches the question class. Table 1 provides
the correspondence between question classes and
named entity types. We perform tagging at the
chunk level and use two types of relational tags:
plain REL-FOCUS and a tag typed with a ques-
tion class, e.g., REL-FOCUS-HUM. Fig. 2 (bot-
tom) shows an example q/a pair where the typed
relational tag is used in the shallow syntactic tree
representation to link the chunk containing the
question focus name with the named entities of the
corresponding type Person (according to the map-
ping defined in Table 1), i.e. samuel langhorne
clemens and mark twain.
5 Feature vector representation
While the primary focus of our study is on the
structural representations and relations between
q/a pairs we also include basic features widely
used in QA:
Term-overlap features. A cosine similarity be-
tween a question and an answer: simCOS(q, a),
where the input vectors are composed of: (i) n-
grams (up to tri-grams) of word lemmas and part-
of-speech tags, and (ii) dependency triplets. For
the latter, we simply hash the string value of the
predicate defining the triple together with its argu-
ment, e.g. poss(name, twain).
PTK score. For the structural representations we
also define a similarity based on the PTK score:
simPTK(q, a) = PTK(q, a), where the input
trees can be both dependency trees and shallow
chunk trees. Note that this similarity is computed
between the members of a q/a pair, thus, it is very
different from the one defined in Eq. 1.
NER relatedness represents a match between a
question category and the related named entity
types extracted from the candidate answer. It
counts the proportion of named entities in the an-
swer that correspond to the question type returned
by the question classifier.
In our study feature vectors serve a complemen-
tary purpose, while the main focus is to study the
virtue of structural representations for reranking.
The effect of a more extensive number of pairwise
similarity features in QA has been studied else-
where, e.g., (Surdeanu et al, 2008).
6 Experiments
We report the results on two QA collections: fac-
toid open-domain QA corpus from TREC and a
community QA corpus Answerbag. Since we fo-
cus on passage reranking we do not carry out an-
swer extraction. The goal is to rank the passage
containing the right answer in the top position.
6.1 Corpora
TREC QA. In the TREC QA tasks, answer pas-
sages containing correct information nuggets, i.e.
answer keys, have to be extracted from a given text
corpus, typically a large corpus from newswire.
In our experiments, we opted for questions from
2002 and 2003 years, which totals to 824 ques-
tions. AQUAINT newswire corpus4 is used for
searching the supporting answers.
Answerbag is a community-driven QA collection
that contains a large portion of questions that have
?professionally researched? answers. Such an-
swers are provided by the website moderators and
allow for training high quality models. From the
original corpus containing 180k question/answer
pairs, we use 1k randomly sampled questions for
testing and 10k for training.
Question Focus. We use 3 datasets for train-
ing and evaluating the performance of our fo-
cus detector: SeCo-600 (Quarteroni et al, 2012),
Mooney GeoQuery (Damljanovic et al, 2010) and
the dataset from (Bunescu and Huang, 2010). The
SeCo dataset contains 600 questions from which
we discarded a subset of multi-focus questions
and non-interrogative queries. The Mooney Geo-
Query contains 250 question targeted at geograph-
ical information in the U.S. The first two datasets
are very domain specific, so we also carried out
experiments with the dataset from (Bunescu and
Huang, 2010), which contains the first 2000 ques-
tions from the answer type dataset from Li and
Roth annotated with focus words. We removed
questions with implicit and multiple focuses.
Question Classification. We used the UIUIC
dataset (Li and Roth, 2002)5 which contains 5952
4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/
5although the QC dataset from (Li and Roth, 2002) in-
cludes additional 50 fine grain classes we opted for using only
6 coarse classes that are sufficient to capture the coarse se-
mantic answer type of the candidate answer. This choice also
results in a more accurate multi-class classifier.
79
factoid questions from different sources (USC,
TREC 8, TREC 9, TREC 10). For training the
classifiers we excluded questions from TREC 8 to
ensure there is no overlap with the data used for
testing models trained on TREC QA.
6.2 Models and Metrics
Our models are built applying a kernel-based
reranker to the output of a search engine.
6.2.1 BM25
We use Terrier6 search engine, which provides
BM25 scoring model for indexing and retrieval.
For the TREC QA 2002 and 2003 task we index
AQUAINT corpus treating paragraphs as docu-
ments. The resulting index contains about 12 mil-
lion items. For the Answerbag we index the entire
collection of 180k answers. We retrieve a list of
top 50 candidate answers for each question.
6.2.2 Reranking models
To train our reranking models we used SVM-light-
TK7, which encodes structural kernels in SVM-
light (Joachims, 2002) solver. In particular, we
use PTK on the relational tree structures combined
with the polynomial kernel of degree 3 applied to
the feature vectors. Therefore, different represen-
tations lead to different models described below.
CH - our basic shallow chunk tree (Severyn and
Moschitti, 2012) used as a baseline structural
reranking model.
DEP - dependency tree augmented with POS tags
and reorganized relations suitable for PTK.
V - reranker model using similarity features de-
fined in Sec. 5.
DEP+V, CH+V - a combination of tree structures
and similarity feature vectors.
+FC+QC - relational linking of the question focus
word and named entities of the corresponding type
using Focus and Question classifiers.
+TFC+QC - a typed relational link refined a ques-
tion category.8
6.2.3 Metrics
We report the following metrics, most commonly
used in QA: Precision at rank 1 (P@1), i.e.,
6http://terrier.org/
7http://disi.unitn.it/moschitti/Tree-Kernel.htm
8? is used for showing the results of DEP, DEP+V and
CH+V structural representations that are significantly better
than the baseline model CH, while ? indicates improvement
of +QC+FC and +QC+TFC tagging applied to basic struc-
tural representations, e.g. CH+V and DEP+V.
Table 2: Structural representations on TREC QA.
MODELS MAP MRR P@1
BM25 0.22 28.02 18.17
V 0.22 28.40 18.54
STRUCTURAL REPRESENTATIONS
CH (S&M, 2012) 0.28 35.63 24.88
CH+V 0.30? 37.45? 27.91?
DEP 0.30? 37.87? 28.05?
DEP+V 0.30? 37.64? 28.05?
REFINED RELATIONAL TAG
CH+V+QC+FC 0.32? 39.48? 29.63?
CH+V+QC+TFC 0.32? 39.49? 30.00?
DEP+V+QC+FC 0.31? 37.49 28.56
DEP+V+QC+TFC 0.31? 38.05? 28.93?
the percentage of questions with a correct an-
swer at rank 1, Mean Reciprocal Rank (MRR),
and Mean Average Precision (MAP). The reported
metrics are averages after performing a 5-fold
cross-validation. We used a paired t-test at 95%
confidence to compare the performance of our
models to a baseline.
6.3 Passage Reranking Results
We first evaluate the impact of two different syn-
tactic representations using shallow and depen-
dency trees. Then, we evaluate the accuracy boost
when such structures are enriched with automati-
cally derived tags, e.g., question focus and ques-
tion category and NEs found in the answer pas-
sage.
6.3.1 Structural representations
Table 2 reveals that using V model results in a
small improvement over BM25 baseline. Indeed,
similarity scores that are most often based on
word-overlap measures even when computed over
various q/a representations are fairly redundant to
the search engine similarity score. Instead, using
the structural representations, CH and DEP, gives
a bigger boost in the performance. Interestingly,
having more features in the CH+V model results
in further improvement while DEP+V seems to re-
main insensitive to additional features provided by
the V model.
6.3.2 Semantically Enriched Structures
In the following set of experiments we explore an-
other strategy for linking structures for a given
q/a pair. We automatically detect the question
focus word and link it to the related named en-
tities in the answer, selected accordingly to the
question category identified by the question clas-
sifier (QC+FC). Further refining the relational link
80
Table 3: Accuracy (%) of focus classifiers.
DATASET ST STK STK+BOW PTK
MOONEY 73.0 81.9 81.5 80.5
SECO-600 90.0 94.5 94.5 90.0
BUNESCU 89.7 98.3 98.2 96.9
Table 4: Accuracy (%) of question classifiers.
DATASET STK+BOW PTK
LI & ROTH 86.1 82.2
TREC TEST 79.3 78.1
with the question category yields QC+TFC model.
First, we report the results of training our question
focus detector and question category classifier.
Focus classifier results. Table 3 displays the ac-
curacies obtained by the question focus detector
on 3 datasets using different kernels: the ST (sub-
tree kernel where fragments contain full subtrees
including leaves), STK, STK+bow (bag-of-words
feature vector is added) and PTK. As we can see,
using STK model yields the best accuracy and we
use it in our pipeline to automatically detect the
focus.
Question classifier results. Table 4 contains the
accuracies of the question classifier on the UIUIC
dataset and the TREC questions that we also use
for testing our reranker models. STK+bow per-
forms better than PTK, since here the input rep-
resentation is a plain constituency tree, for which
STK is particularly suited. Hence, we use this
model to predict the question category.
Ranking results. Table 2 (bottom) summarizes
the performance of the CH+V and DEP+V models
when coupled with QC+FC and QC+TFC strate-
gies to establish the links between the structures
in a given q/a pair. CH structural representation
with QC+FC yields an interesting improvement,
while further refining the relational tag by adding
a question category (QC+TFC) gives slightly bet-
ter results.
Integrating the refined relational tag into the
DEP based structures results more problematic,
since the dependency tree is less suitable for repre-
senting multi-word expressions, named entities in
our case. Hence, using the relational tag to mark
the nodes spanning such multi-word entities in the
dependency structure may result in less meaning-
ful features than in CH model, where words in a
phrase are naturally grouped under a chunk node.
A more detailed discussion on the merits of each
model is provided in the Sec. 6.5.
Table 5: Cross-domain experiment: training on Answerbag
and testing on TREC QA.
MODELS MAP MRR P@1
BM25 0.22 27.91 18.08
V 0.23 28.86 18.90
BASIC STRUCTURAL REPRESENTATIONS
CH (S&M, 2012) 0.24 30.25 20.42
CH+V 0.25? 31.31? 21.28?
DEP+V 0.26? 33.26? 22.21?
REFINED RELATIONAL TAG
CH+V+QC+TFC 0.27? 33.53? 22.81?
DEP+V+QC+TFC 0.29? 34.25? 23.45?
6.4 Learning cross-domain pairwise
structural relationships
To test the robustness of the syntactic patterns au-
tomatically learned by our structural models, we
conduct a cross-domain experiment, i.e. we train
a model on Answerbag data and test it on TREC. It
should be noted that unlike TREC data, where the
answers are simply passages containing the cor-
rect answer phrase, answers in Answerbag specif-
ically address a given question and are generated
by humans. Additionally, TREC QA contains only
factoid questions, while Answerbag is a commu-
nity QA corpus with a large portion of non-factoid
questions. Interestingly, the results demonstrate
the robustness of our syntactic relational model
which captures patterns shared across different do-
mains, e.g. TREC and Answerbag data.
Table 5 shows that: (i) models based on depen-
dency structures result in a better generalization
ability extracting more robust syntactic patterns;
and (ii) the strategy to link the question focus with
the related named entities in the answer provides
an interesting improvement over the basic struc-
tural representations.
6.5 Error Analysis
Consider our running example q/a pair from
Sec. 1. As the first candidate answer, the
search engine retrieves the following incorrect
passage: ?The autobiography of Mark Twain?,
Mark Twain. It is relatively short and mentions the
keywords {Mark, Twain} twice, which apparently
results in a high score for the BM25 model. In-
stead, the search engine ranks the correct answer at
position 34. After reranking using the basic CH+V
model the correct answer is promoted by 20 posi-
tions. While using the CH+V+QC+FC model the
correct answer advances to position 6. Below, we
provide the intuition behind the merits of QC+FC
and QC+TFC encoding question focus and ques-
81
tion category into the basic models.
The model learned by the reranker represents a
collection of q/a pairs from the training set (sup-
port vectors) which are matched against each can-
didate q/a pair. We isolated the following pair
from the model that has a high structural similarity
with our running example:
Q: What is Barbie?s full name?
A: The toy is called after Barbie Millicent
Roberts from Willows.
Despite differences in the surface forms of
the words, PTK extracts matching patterns,
e.g. [S NP [VP VBN] [PP IN] REL-NP],
which yields a high similarity score boosting the
rank of the correct candidate. However, we
note that at the same time an incorrect candi-
date answer, e.g. Mark Twain was accused of
racist language., exhibits similar patterns and also
gets a high rank. The basic structural repre-
sentation is not able to encode essential differ-
ences from the correct answer candidate. This
poses a certain limitation on the discriminative
power of CH and DEP representations. Intro-
ducing a focus tag changes the structural repre-
sentation of both q/a pairs, s.t. the correct q/a
pair preserves the pattern (after identifying word
name as focus and question category as HUM,
it is transformed to [S REL-FOCUS-NP [VP
VBN] [PP IN] REL-FOCUS-NP]), while it
is absent in the incorrect candidate. Thus, linking
the focus word with the related NEs in the answer
helps to discriminate between structurally similar
yet semantically different candidates.
Another step towards a more fine-grained struc-
tural representation is to specialize the relational
focus tag (QC+TFC model). We propose to aug-
ment the focus tag with the question category to
avoid matches with other structurally similar but
semantically different candidates. For example, a
q/a pair found in the list of support vectors:
Q: What is Mark Twain?s place of birth?
A: Mark Twain was raised in Hannibal Missouri.
would exhibit high structural similarity even when
relational focus is used (since the relational tag
does not incorporate the question class LOC), but
refining the focus tag with the question class elim-
inates such cases.
7 Related Work
Previous studies similar to ours carry out pas-
sage reranking by exploiting structural informa-
tion, e.g. using subject-verb-object relations (At-
tardi et al, 2001; Katz and Lin, 2003). Un-
fortunately, the large variability of natural lan-
guage makes such triples rather sparse thus dif-
ferent methods explore soft matching (i.e., lexical
similarity) based on answer types and named en-
tity types (Aktolga et al, 2011). Passage reranking
using classifiers of question and answer pairs were
proposed in (Radlinski and Joachims, 2006; Jeon
et al, 2005).
Regarding kernel methods, our work in (Mos-
chitti et al, 2007; Severyn and Moschitti, 2012)
was the first to exploit tree kernels for modeling
answer reranking. However, such method lacks
the use of important relational information be-
tween a question and a candidate answer, which
is essential to learn accurate relational patterns. In
contrast, this paper relies on shallow and depen-
dency trees encoding the output of question and
focus classifiers to connect focus word and NEs of
the answer passage. This provides more effective
relational information, which allows our model to
significantly improve on previous rerankers.
8 Conclusions
This paper shows a viable research direction in
the automatic QA engineering. One of its main
characteristics is the use of structural kernel tech-
nology to induce features from structural seman-
tic representations of question and answer pas-
sage pairs. The same technology is also used to
construct question and focus classifiers, which are
used to derive relational structures.
An interesting result of this paper is that to de-
sign an answer passage reranker for a new do-
main, we can use off-the-shelf syntactic parsers
and NERs along with little training data for the
QC and FC classifiers. This is due to the fact
that: (i) the kernel technology is able to automat-
ically extract effective structural patterns; and (ii)
the extracted patterns are rather robust, e.g., mod-
els learned on Answerbag improve accuracy on
TREC test data.
Acknowledgements
This research is partially supported by the EU?s 7th
Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and an Open Collaborative
Research (OCR) award from IBM Research.
82
References
Elif Aktolga, James Allan, and David A. Smith. 2011.
Passage reranking for question answering using syn-
tactic structures and answer types. In ECIR.
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, and Ro Tommasi. 2001.
Piqasso: Pisa question answering system. In TREC,
pages 599?607.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Combined syntactic and semantic kernels for text
classification. In ECIR.
Razvan Bunescu and Yunfeng Huang. 2010. Towards
a general model of answer typing: Question focus
identification. In CICLing.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
ACL.
Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Identification of the question
focus: Combining syntactic analysis and ontology-
based lookup through the user interaction. In LREC.
Alessandra Giordani and Alessandro Moschitti. 2009.
Syntactic structural kernels for natural language in-
terfaces to databases. In Proceedings of ECML
PKDD, ECML PKDD ?09. Springer-Verlag.
Alessandra Giordani and Alessandro Moschitti. 2012.
Translating questions to sql queries with generative
parsers discriminatively reranked. In Proceedings
of The 24rd International Conference on Computa-
tional Linguistics, India. Coling 2012.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD),
pages 133?142.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In COLING.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In ACL.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
335?342, Barcelona, Spain, July.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Christopher Pinchak. 2006. A probabilistic answer
type model. In In EACL.
John M. Prager. 2006. Open-domain question-
answering. Foundations and Trends in Information
Retrieval, 1(2):91?231.
Silvia Quarteroni, Vincenzo Guerrisi, and Pietro La
Torre. 2012. Evaluating multi-focus natural lan-
guage queries over data services. In LREC.
Filip Radlinski and Thorsten Joachims. 2006. Query
chains: Learning to rank from implicit feedback.
CoRR.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT.
83
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143?152,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Towards Robust Linguistic Analysis Using OntoNotes
Sameer Pradhan1, Alessandro Moschitti2,3, Nianwen Xue4, Hwee Tou Ng5
Anders Bjo?rkelund6, Olga Uryupina2, Yuchen Zhang4 and Zhi Zhong5
1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA
2 University of Trento, University of Trento, 38123 Povo (TN), Italy
3 QCRI, Qatar Foundation, 5825 Doha, Qatar
4 Brandeis University, Brandeis University, Waltham, MA 02453, USA
5 National University of Singapore, Singapore, 117417
6 University of Stuttgart, 70174 Stuttgart, Germany
Abstract
Large-scale linguistically annotated cor-
pora have played a crucial role in advanc-
ing the state of the art of key natural lan-
guage technologies such as syntactic, se-
mantic and discourse analyzers, and they
serve as training data as well as evaluation
benchmarks. Up till now, however, most
of the evaluation has been done on mono-
lithic corpora such as the Penn Treebank,
the Proposition Bank. As a result, it is still
unclear how the state-of-the-art analyzers
perform in general on data from a vari-
ety of genres or domains. The completion
of the OntoNotes corpus, a large-scale,
multi-genre, multilingual corpus manually
annotated with syntactic, semantic and
discourse information, makes it possible
to perform such an evaluation. This paper
presents an analysis of the performance of
publicly available, state-of-the-art tools on
all layers and languages in the OntoNotes
v5.0 corpus. This should set the bench-
mark for future development of various
NLP components in syntax and semantics,
and possibly encourage research towards
an integrated system that makes use of the
various layers jointly to improve overall
performance.
1 Introduction
Roughly a million words of text from the Wall
Street Journal newswire (WSJ), circa 1989, has
had a significant impact on research in the lan-
guage processing community ? especially those
in the area of syntax and (shallow) semantics, the
reason for this being the seminal impact of the
Penn Treebank project which first selected this text
for annotation. Taking advantage of a solid syn-
tactic foundation, later researchers who wanted to
annotate semantic phenomena on a relatively large
scale, also used it as the basis of their annota-
tion. For example the Proposition Bank (Palmer et
al., 2005), BBN Name Entity and Pronoun coref-
erence corpus (Weischedel and Brunstein, 2005),
the Penn Discourse Treebank (Prasad et al, 2008),
and many other annotation projects, all annotate
the same underlying body of text. It was also con-
verted to dependency structures and other syntac-
tic formalisms such as CCG (Hockenmaier and
Steedman, 2002) and LTAG (Shen et al, 2008),
thereby creating an even bigger impact through
these additional syntactic resources. The most re-
cent one of these efforts is the OntoNotes corpus
(Weischedel et al, 2011). However, unlike the
previous extensions of the Treebank, in addition
to using roughly a third of the same WSJ subcor-
pus, OntoNotes also added several other genres,
and covers two other languages ? Chinese and
Arabic: portions of the Chinese Treebank (Xue et
al., 2005) and the Arabic Treebank (Maamouri and
Bies, 2004) have been used to sample the genre of
text that they represent.
One of the current hurdles in language process-
ing is the problem of domain, or genre adaptation.
Although genre or domain are popular terms, their
definitions are still vague. In OntoNotes, ?genre?
means a type of source ? newswire (NW), broad-
cast news (BN), broadcast conversation (BC), mag-
azine (MZ), telephone conversation (TC), web data
(WB) or pivot text (PT). Changes in the entity and
event profiles across source types, and even in the
same source over a time duration, as explicitly ex-
pressed by surface lexical forms, usually account
for a lot of the decrease in performance of mod-
els trained on one source and tested on another,
usually because these are the salient cues that are
relied upon by statistical models.
Large-scale corpora annotated with multiple
layers of linguistic information exist in various
languages, but they typically consist of a single
source or collection. The Brown corpus, which
consists of multiple genres, have been usually used
to investigate issues of genres of sensitivity, but it
is relatively small and does not include any infor-
1A portion of the English data in the OntoNotes corpus
is a selected set of sentences that were annotated for parse
and word sense information. These sentences are present in a
document of their own, and so the documents for parse layers
for English are inflated by about 3655 documents and for the
word sense are inflated by about 8797 documents.
143
Language Parse Proposition Sense Name Coreference
Documents Words Documents Verb Prop. Noun Prop. Documents Verb Sense Noun Sense Documents Words Documents Words
English 7,9671 2.6M 6,124 300K 18K 12K 173K 120K 3,637 2.0M 2,384(3493) 1.7M
Chinese 2002 1.0M 1861 148K 7K 1573 83K 1K 1,911 988K 1,729(2,280) 950K
Arabic 599 402K 599 30K - 310 4.3K 8.7K 446 298K 447(447) 300K
Table 1: Coverage for each layer in the OntoNotes v5.0 corpus, by number of documents, words, and
some other attributes. The numbers in parenthesis are the total number of parts in the documents.
mal genres such as web data. Very seldom has it
been the case that the exact same phenomena have
been annotated on a broad cross-section of the
same language before OntoNotes. The OntoNotes
corpus thus provides an opportunity for studying
the genre effect on different syntactic, semantic
and discourse analyzers.
Parts of the OntoNotes Corpus have been used
for various shared tasks organized by the language
processing community. The word sense layer was
the subject of prediction in two SemEval-2007
tasks, and the coreference layer was the subject
of prediction in the SemEval-20102 (Recasens et
al., 2010), CoNLL-2011 and 2012 shared tasks
(Pradhan et al, 2011; Pradhan et al, 2012). The
CoNLL-2012 shared task provided predicted in-
formation to the participants, however, that did not
include a few layers such as the named entities
for Chinese and Arabic, propositions for Arabic,
and for better comparison of the English data with
the CoNLL-2011 task, a smaller OntoNotes v4.0
portion of the English parse and propositions was
used for training.
This paper is a first attempt at presenting a co-
herent high-level picture of the performance of
various publicly available state-of-the-art tools on
all the layers of OntoNotes in all three languages,
so as to pave the way for further explorations in
the area of syntax and semantics processing.
The possible avenues for exploratory studies
on various fronts are enormous. However, given
space considerations, in this paper, we will re-
strict our presentation of the performance on all
layers of annotation in the data by using a strat-
ified cross-section of the corpus for training, de-
velopment, and testing. The paper is organized
as follows: Section 2 gives an overview of the
OntoNotes corpus. Section 3 explains the param-
eters of the evaluation and the various underlying
assumptions. Section 4 presents the experimental
results and discussion, and Section 5 concludes the
paper.
2 OntoNotes Corpus
The OntoNotes project has created a large-scale
corpus of accurate and integrated annotation of
2A small portion 125K words in English was used for this
evaluation.
multiple layers of syntactic, semantic and dis-
course information in text. The English lan-
guage portion comprises roughly 1.7M words and
Chinese language portion comprises roughly 1M
words of newswire, magazine articles, broadcast
news, broadcast conversations, web data and con-
versational speech data3. The Arabic portion is
smaller, comprising 300K words of newswire ar-
ticles. This rich, integrated annotation covering
many layers aims at facilitating the development
of richer, cross-layer models and enabling bet-
ter automatic semantic analysis. The corpus is
tagged with syntactic trees, propositions for most
verb and some noun instances, partial verb and
noun word senses, coreference, and named enti-
ties. Table 1 gives an overview of the number of
documents that have been annotated in the entire
OntoNotes corpus.
2.1 Layers of Annotation
This section provides a very concise overview of
the various layers of annotations in OntoNotes.
For a more detailed description, the reader is re-
ferred to (Weischedel et al, 2011) and the docu-
mentation accompanying the v5.04 release.
2.1.1 Syntax
This represents the layer of syntactic annotation
based on revised guidelines for the Penn Tree-
bank (Marcus et al, 1993; Babko-Malaya et al,
2006), the Chinese Treebank (Xue et al, 2005)
and the Arabic Treebank (Maamouri and Bies,
2004). There were two updates made to the parse
trees as part of the OntoNotes project: i) the in-
troduction of NML phrases, in the English portion,
to mark nominal sub-constituents of flat NPs that
do not follow the default right-branching structure,
and ii) re-tokenization of hyphenated tokens into
multiple tokens in English and Chinese. The Ara-
bic Treebank on the other hand was also signifi-
cantly revised in an effort to increase consistency.
2.1.2 Word Sense
Coarse-grained word senses are tagged for the
most frequent polysemous verbs and nouns, in or-
3These numbers are for the portion that has all layers of
annotations. The word count for each layer is mentioned in
Table 1
4For all the layers of data used in this study, the
OntoNotes v4.99 pre-release that was used for the CoNLL-
2012 shared task is identical to the v5.0 release.
144
der to maximize token coverage. The word sense
granularity is tailored to achieve very high inter-
annotator agreement as demonstrated by Palmer et
al. (2007). These senses are defined in the sense
inventory files. In the case of English and Arabic
languages, the sense-inventories (and frame files)
are defined separately for each part of speech that
is realized by the lemma in the text. For Chinese,
however the sense inventories (and frame files) are
defined per lemma ? independent of the part of
speech realized in the text.
2.1.3 Proposition
The propositions in OntoNotes are PropBank-style
semantic roles for English, Chinese and Arabic.
Most English verbs and few nouns were anno-
tated using the revised guidelines for the English
PropBank (Babko-Malaya et al, 2006) as part of
the OntoNotes effort. Some enhancements were
made to the English PropBank and Treebank to
make them synchronize better with each other:
one of the outcomes of this effort was that two
types of LINKs that represent pragmatic coref-
erence (LINK-PCR) and selectional preferences
(LINK-SLC) were added to the original PropBank
(Palmer et al, 2005). More details can be found in
the addendum to the PropBank guidelines5 in the
OntoNotes v5.0 release. A part of speech agnostic
Chinese PropBank (Xue and Palmer, 2009) guide-
lines were used to annotate most frequent lem-
mas in Chinese. Many verbs and some nouns and
adjectives were annotated using the revised Ara-
bic PropBank guidelines (Palmer et al, 2008; Za-
ghouani et al, 2010).
2.1.4 Named Entities
The corpus was tagged with a set of 18 well-
defined proper named entity types that have been
tested extensively for inter-annotator agreement
by Weischedel and Burnstein (2005).
2.1.5 Coreference
This layer captures general anaphoric corefer-
ence that covers entities and events not limited
to noun phrases or a limited set of entity types
(Pradhan et al, 2007). It considers all pronouns
(PRP, PRP$), noun phrases (NP) and heads of verb
phrases (VP) as potential mentions. Unlike En-
glish, Chinese and Arabic have dropped subjects
and objects which were also considered during
coreference annotation6. The mentions formed by
these dropped pronouns total roughly about 11%
for both Chinese and Arabic. Coreference is the
only document-level phenomenon in OntoNotes.
Some of the documents in the corpus ? especially
the ones in the broadcast conversation, web data,
5doc/propbank/english-propbank.pdf
6As we will see later these are not used during the task.
and telephone conversation genre ? are very long
which prohibited efficient annotation in their en-
tirety. These are split into smaller parts, and each
part is considered a separate document for the sake
of coreference evaluation.
3 Evaluation Setting
Given the scope of the corpus and the multitude of
settings one can run evaluations, we had to restrict
this study to a relatively focused subset. There has
already been evidence of models trained on WSJ
doing poorly on non-WSJ data on parses (Gildea,
2001; McClosky et al, 2006), semantic role label-
ing (Carreras and Ma`rquez, 2005; Pradhan et al,
2008), word sense (Escudero et al, 2000; ?), and
named entities. The phenomenon of coreference is
somewhat of an outlier. The winning system in the
CoNLL-2011 shared task was one that was com-
pletely rule-based and not directly trained on the
OntoNotes corpus. Given this overwhelming evi-
dence, we decided not to focus on potentially com-
plex cross-genre evaluations. Instead, we decided
on evaluating the performance on each layer of an-
notation using an appropriately selected, stratified
training, development and test set, so as to facili-
tate future studies.
3.1 Training, Development and Test
Partitions
In this section we will have a brief discussion
on the logic behind the partitioning of the data
into training, development and test sets. Before
we do that, it would help to know that given the
range and peculiarities of the layers of annota-
tion and presence of various resource and techni-
cal constraints, not all the documents in the cor-
pus are annotated with all the layers of informa-
tion, and token-centric phenomena (such as word
sense and propositions of predicates) were not an-
notated with 100% coverage. Most of the propo-
sition annotation in English and Arabic is for the
verb predicates, with a few nouns annotated in
English and some adjectives in Arabic. In Chi-
nese, the selection is part of speech agnostic, and is
based on the lemmas that can be considered predi-
cates. Some documents in the corpora are actually
snippets from larger documents, and have been an-
notated for a combination of parse, propositions,
word sense and names, but not coreference. If one
considers each layer independently, then an ideal
partitioning scheme would create a separate parti-
tion for each layer such that it maximizes the num-
ber of examples that can be extracted for that layer
from the corpus. The upside is that one would
get as much data there is to train and estimate the
performance of each layer across the entire cor-
pus. The downside is that this might cover vari-
145
ous cross sections of the documents in the corpus,
and would not provide a clean picture when look-
ing at the collective performance for all the lay-
ers. The documents that are annotated with coref-
erence correspond to the intersection of all anno-
tations. These are the documents that have also
been annotated with all the other layers of infor-
mation. The amount of data we can get together
in such a test set is big enough to be represen-
tative. Therefore, we decided that it would be
ideal to choose a portion of these documents as
the test collection for all layers. An additional ad-
vantage is that it is the exact same test set used
in the CoNLL-2012 shared task, and so in a way
is already a standard. On the training and devel-
opment side however, one can still imagine using
all possible information for training models for a
particular layer, and that is what we decided to
do. The training and development data is gener-
ated by providing all documents with all available
layers of annotation for input, however, the test
set is generated by providing as input to the algo-
rithm the set of documents in the corpus that have
been annotated for coreference. This algorithm
tries to reuse previously established partitions for
English, i.e., the WSJ portion. Unfortunately, in
the case of Chinese and Arabic, either the histor-
ical partitions were not in the selection used for
OntoNotes, or were partially overlapping with the
ones created using this scheme, and/or had a very
small portion of OntoNotes covered in the test set.
Therefore, we decided to create a fresh partition
for the Chinese and Arabic data. Note, however,
that the these test sets also match the ones used
in the CoNLL-2012 evaluation. The algorithm for
selecting the training, development and test parti-
tions is described on the CoNLL-2012 shared task
webpage, along with the list of training, develop-
ment, and test document IDs7.
3.2 Assumptions
Next we had to decide on a set of assumptions
to use while designing the experiments to mea-
sure the automatic prediction accuracy for each of
the layers. Since some of these decisions affect
more than one layer of annotation, we will de-
scribe these in this section instead of in the section
where we discuss the experiment with a particular
layer of annotation.
7http://conll.cemantix.org/2012/download/ids/
For each language there are two sub-directories ? ?all?
contains more general lists which include documents
that had at least one of the layers of annotation, and
?coref? contains the lists that include documents that
have coreference annotation. The former were used to
generate training, development, test sets for layers other
than coreference, and the latter was used to generate
training/development/test sets for the coreference layer
used in the CoNLL-2012 shared task.
Word Segmentation The three languages that
we are evaluating are from quite different lan-
guage families. Arabic has a complex morphol-
ogy, English has limited morphology, whereas
Chinese has very little morphology. English word
segmentation amounts to rule-based tokenization,
and is close to perfect. In the case of Chinese and
Arabic, although the tokenization/segmentation is
not as good as English, the accuracies are in the
high 90s. Given this we decided to use gold,
Treebank segmentation for all languages. In the
case of Chinese, the words themselves are lem-
mas, whereas in English they can be predicted
with very high accuracy. For Arabic, by default
written text is unvocalised, and lemmatization is a
complex process which we considered out of the
scope of this study, so we decided to use correct,
gold standard lemmas, along with the correct vo-
calized version of the tokens.
Traces and Function Tags Treebank traces
have hardly played a role in the mainstream parser
and semantic role labeling evaluation. Function
tags also have received similar treatment in the
parsing community, and though they are impor-
tant, there is also a significant information overlap
between them and the proposition structure pro-
vided by the PropBank layer. Whereas in English,
most traces represent syntactic phenomena such
as movement and raising, in Chinese and Arabic,
they can also represent dropped subjects/objects.
These subset of traces directly affect the corefer-
ence layer, since, unlike English, traces in Chinese
and Arabic (*pro* and * respectively) are legit-
imate targets of mentions and are considered for
coreference annotation in OntoNotes. Recovering
traces in text is a hard problem, and the most re-
cently reported numbers in literature for Chinese
are around a F-score of 50 (Yang and Xue, 2010;
Cai et al, 2011). For Arabic there have not been
much studies on recovering these. A study by
Gabbard (2010) shows that these can be recovered
with an F-score of 55 with automatic parses and
roughly 65 using gold parses. Considering the low
level of prediction accuracy of these tokens, and
their relative low frequency, we decided to con-
sider predicting traces in trees out of the scope of
this study. In other words, we removed the man-
ually identified traces and function tags from the
Treebanks across all three languages, in all the
three ? training, development and test partitions.
This meant removing any and all dependent an-
notation in layers such as PropBank and Coref-
erence. In the case of PropBank these are the
argument bearing traces, whereas in coreference
these are the mentions formed by these elided sub-
jects/objects.
146
Disfluencies One thing that needs to be dealt
with in conversational data is the presence of dis-
fluencies (restarts, etc.). In the English parses of
the OntoNotes, disfluencies are marked using a
special EDITED8 phrase tag ? as was the case for
the Switchboard Treebank. Computing the accu-
racy of identifying disfluencies is also out of the
scope of this study. Given the frequency of dis-
fluencies and the performance with which one can
identify them automatically,9 a probable process-
ing pipeline would filter them out before parsing.
We decided to remove them using oracle infor-
mation available in the English Treebank, and the
coreference chains were remapped to trees with-
out disfluencies. Owing to various technical con-
straints, we decided to retain the disfluencies in the
Chinese data.
Spoken Genre Given the scope of this study, we
make another significant assumption. For the spo-
ken genres ? BC, BN and TC ? we use the manual
transcriptions rather than the output of a speech
recognizer, as would be the case in real world. The
performance on various layers for these genres
would therefore be artificially inflated, and should
be taken into account while analyzing results. Not
many studies have previously reported on syntac-
tic and semantic analysis for spoken genre. Favre
et al (2010) report the performance on the English
subset of an earlier version of OntoNotes.
Discourse The corpus contains information on
the speaker for broadcast communication, conver-
sation, telephone conversation and writer for the
web data. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be
automatically deduced, but is also not within the
scope of our study. Therefore, we decided to pro-
vide gold, instead of predicted, data both during
training and testing. Table 2 lists the status of the
layers.
4 Experiments
In this section, we will report on the experiments
carried out using all available data in the train-
ing set for training models for a particular layer,
and using the CoNLL-2012 test set as the test set.
8There is another phrase type ? EMBED in the telephone
conversation genre which is similar to the EDITED phrase
type, and sometimes identifies insertions, but sometimes con-
tains logical continuation of phrases by different speakers, so
we decided not to remove that from the data.
9A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 precision
and 67 recall.
10The predicted part of speech for Arabic are a mapped
down version of the richer gold version present in the Tree-
bank
Layer English Chinese Arabic
Segmentation ? ? ?
Lemma ? ? ?
Parse ? ? ?10
Proposition ? ? ?
Predicate Frame ? ? ?
Word Sense ? ? ?
Name Entities ? ? ?
Coreference ? ? ?
Speaker ? ? ?
Number ? ? ?
Gender ? ? ?
Table 2: Status of layers used during prediction
of other layers. A ??? indicates gold annotation,
a ??? indicates predicted, a ??? indicates an ab-
sence of the predicted layer, and a ??? indicates
that the layer is not applicable to the language.
The predicted annotation layers input to down-
stream models were automatically annotated by
using NLP processors learned with n-cross fold
validation on the training data. This way, the n
chunks of training data are annotated avoiding de-
pendencies with the data used for training the NLP
processors.
4.1 Syntax
Predicted parse trees for English were produced
using the Charniak parser11 (Charniak and John-
son, 2005). Some additional tag types used in
the OntoNotes trees were added to the parser?s
tagset, including the nominal (NML) tag, and the
rules used to determine head words were extended
correspondingly. Chinese and Arabic parses were
generated using the Berkeley parser (Petrov and
Klein, 2007). In the case of Arabic, the pars-
ing community uses a mapping from rich Arabic
part of speech tags to Penn-style part of speech
tags. We used the mapping that is included with
the Arabic Treebank. The predicted parses for
the training portion of the data were generated us-
ing 10-fold (5-folds for Arabic) cross-validation.
For testing, we used a model trained on the entire
training portion. Table 3 shows the precision, re-
call and F1-scores of the re-trained parsers on the
CoNLL-2012 test along with the part of speech ac-
curacies (POS) using the standard evalb scorer.
The performance on the PT genre for English is
the highest among other English genres. This is
possibly because of the professional, clean trans-
lations of the underlying text, and are mostly
shorter sentences. The MZ genre and the NW both
of which contain well edited text, share similar
scores. There is a few points gap between these
and the other genres. As for Chinese, the per-
formance on MZ is the highest followed by BN.
Surprisingly, the WB genre has a similar score and
the others are close behind except for TC. As ex-
pected, the Arabic parser performance is the low-
11http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz
147
All Sentences
N POS P R F
English BC 2,211 97.33 86.36 86.11 86.23
BN 1,357 97.32 87.61 87.03 87.32
MZ 780 96.58 89.90 89.49 89.70
NW 2,327 97.15 87.68 87.25 87.47
TC 1,366 96.11 85.09 84.13 84.60
WB 1,787 96.03 85.46 85.26 85.36
PT 1,869 98.77 95.29 94.66 94.98
Overall 11,697 97.09 88.08 87.65 87.87
Chinese BC 885 94.79 80.17 79.35 79.76
BN 929 93.85 83.49 80.13 81.78
MZ 451 97.06 88.48 83.85 86.10
NW 481 94.07 82.26 77.28 79.69
TC 968 92.22 71.90 69.19 70.52
WB 758 92.37 82.57 78.92 80.70
Overall 4,472 94.12 82.23 78.93 80.55
Arabic NW 1,003 94.12 74.71 75.67 75.19
Table 3: Parser performance on the CoNLL-2012
test set.
est among the three languages.
4.2 Word Sense
We used the IMS12 (It Makes Sense) (Zhong and
Ng, 2010) word sense tagger. IMS was trained on
all the word sense data that is present in the train-
ing portion of the OntoNotes corpus using cross-
validated predictions on the input layers similar
to the proposition tagger. During testing, for En-
glish and Arabic, IMS must first use the auto-
matic POS information to identify the nouns and
verbs in the test data, and then assign senses to
the automatically identified nouns and verbs. In
the case of Arabic, IMS uses gold lemmas. Since
automatic POS tagging is not perfect, IMS does
not always output a sense to all word tokens that
need to be sense tagged due to wrongly predicted
POS tags. As such, recall is not the same as pre-
cision on the English and Arabic test data. For
Chinese the measure of performance is just the
accuracy since the senses are defined per lemma
rather than per part of speech. Since we provide
gold word segmentation, IMS attempts to sense
tag all correctly segmented Chinese words, so re-
call and precision are the same and so is the F1-
score. Table 4 shows the performance of this clas-
sifier aggregated over both the verbs and nouns
in the CoNLL-2012 test set and an overall score
split by nouns and verbs for English and Ara-
bic. For both nouns and verbs in English, the
F1-score is over 80%. The performance on En-
glish nouns is slightly higher than English verbs.
Comparing to the other two languages, the perfor-
mance on Arabic is relatively lower, especially the
performance on Arabic verbs, whose F1-score is
less than 70%. For English, genres PT and TC,
and for Chinese genres TC and WB, no gold stan-
dard senses were available, and so their accuracies
could not be computed. Previously, Zhong et al
(2008) reported the word sense performance on
the Wall Street Journal portion of an earlier ver-
12http://www.comp.nus.edu.sg/?nlp/sw/IMS v0.9.2.1.tar.gz
Performance
P R F A
English BC 81.2 81.3 81.2 -
BN 82.0 81.5 81.7 -
MZ 79.1 78.8 79.0 -
NW 85.7 85.7 85.7 -
WB 77.5 77.6 77.5 -
Overall 82.5 82.5 82.5 -
Nouns 83.4 83.1 83.2 -
Verbs 81.8 81.9 81.8 -
Chinese BC - - - 80.5
BN - - - 85.4
MZ - - - 82.4
NW - - - 89.1
Overall - - - 84.3
Arabic NW 75.9 75.2 75.6 -
Nouns 79.2 77.7 78.4 -
Verbs 68.8 69.5 69.1 -
Table 4: Word sense performance on the CoNLL-
2012 test set.
sion of OntoNotes, but the results are not directly
comparable.
4.3 Proposition
The revised PropBank has introduced two new
links ? LINK-SLC and LINK-PCR. Since the com-
munity is not used to the new PropBank represen-
tation which (i) relies heavily on the trace struc-
ture in the Treebank and (ii) we decided to ex-
clude, we unfold the LINKs back to their original
representation as in the PropBank 1.0 release. We
used ASSERT15 (Pradhan et al, 2005) to predict
the propositional structure for English. We made
a small modification to ASSERT, and replaced
the TinySVM classifier with a CRF16 to speed
up training the model on all the data. The Chi-
nese propositional structure was predicted with the
Chinese semantic role labeler described in (Xue,
2008), retrained on the OntoNotes v5.0 data. The
Arabic propositional structure was predicted us-
ing the system described in Diab et al (2008).
(Diab et al, 2008) Table 5 shows the detailed per-
14The Frame ID column indicates the F-score for English
and Arabic, and accuracy for Chinese for the same reasons as
word sense.
15http://cemantix.org/assert.html
16http://leon.bottou.org/projects/sgd
Frame Total Total % Perfect Argument ID + Class
ID Sent. Prop. Prop. P R F
English BC 93.2 1994 5806 52.89 80.76 69.69 74.82
BN 92.7 1218 4166 54.78 80.22 69.36 74.40
MZ 90.8 740 2655 50.77 79.13 67.78 73.02
NW 92.8 2122 6930 46.45 79.80 66.80 72.72
TC 91.8 837 1718 49.94 79.85 72.35 75.91
WB 90.7 1139 2751 42.86 80.51 69.06 74.35
PT 96.6 1208 2849 67.53 89.35 84.43 86.82
Overall 92.8 9,261 26,882 51.66 81.30 70.53 75.53
Chinese BC 87.7 885 2,323 31.34 53.92 68.60 60.38
BN 93.3 929 4,419 35.44 64.34 66.05 65.18
MZ 92.3 451 2,620 31.68 65.04 65.40 65.22
NW 96.6 481 2,210 27.33 69.28 55.74 61.78
TC 82.2 968 1,622 32.74 48.70 59.12 53.41
WB 87.8 758 1,761 35.21 62.35 68.87 65.45
Overall 90.9 4,472 14,955 32.62 61.26 64.48 62.83
Arabic NW 85.6 1,003 2337 24.18 52.99 45.03 48.68
Table 5: Proposition and frameset disambiguation
performance14 in the CoNLL-2012 test set.
148
formance numbers17. The CoNLL-2005 scorer18
was used to compute the scores. At first glance,
the performance on the English newswire genre is
much lower than what has been reported for WSJ
Section 23. This could be attributed to several fac-
tors: i) the newswire in OntoNotes not only con-
tains WSJ data, but also Xinhua news, and some
other newswire evaluation data, ii) The WSJ train-
ing and test portions in OntoNotes are a subset of
the standard ones that have been used to report
performance earlier; iii) the PropBank guidelines
were significantly revised during the OntoNotes
project in order to synchronize well with the Tree-
bank, and finally iv) it includes propositions for
be verbs missing from the original PropBank. It
looks like the newly added Pivot Text data (com-
prised of the New Testament) shows very good
performance. The Chinese and Arabic19 accuracy
is much worse. In addition to automatically pre-
dicting the arguments, we also trained the IMS
system to tag PropBank frameset IDs.
Language Genre Entity Performance
Count P R F
English BC 1671 80.17 77.20 78.66
BN 2180 88.95 85.69 87.29
MZ 1161 82.74 82.17 82.45
NW 4679 86.79 84.25 85.50
TC 362 74.09 61.60 67.27
WB 1133 77.72 68.05 72.56
Overall 11186 84.04 80.86 82.42
Chinese BC 667 72.49 58.47 64.73
BN 3158 82.17 71.50 76.46
NW 1453 86.11 76.39 80.96
MZ 1043 65.16 56.66 60.62
TC 200 48.00 60.00 53.33
WB 886 80.60 51.13 62.57
Overall 7407 78.20 66.45 71.85
Arabic NW 2550 74.53 62.55 68.02
Table 6: Performance of the named entity recog-
nizer on the CoNLL-2012 test set.
4.4 Named Entities
We retrained the Stanford named entity recog-
nizer20 (Finkel et al, 2005) on the OntoNotes data.
Table 6 shows the performance details for all the
languages across all 18 name types broken down
by genre. In English, BN has the highest perfor-
mance followed by the NW genre. There is a sig-
nificant drop from those and the TC and WB genre.
Somewhat similar trend is observed in the Chi-
nese data, with Arabic having the lowest scores.
Since the Pivot Text portion (PT) of OntoNotes
was not tagged with names, we could not com-
pute the accuracy for that cross-section of the data.
Previously Finkel and Manning (2009) performed
17The number of sentences in this table are a subset of the
ones in the table showing parser performance, since these are
the sentences for which at least one predicate has been tagged
with its arguments
18http://www.lsi.upc.es/?srlconll/srl-eval.pl
19The system could not not use the morphology features in
Diab et al (2008).
20http://nlp.stanford.edu/software/CRF-NER.shtml
a joint estimation of named entity and parsing.
However, it was on an earlier version of the En-
glish portion of OntoNotes using a different cross-
section for training and testing and therefore is not
directly comparable.
4.5 Coreference
The task is to automatically identify mentions of
entities and events in text and to link the corefer-
ring mentions together to form entity/event chains.
The coreference decisions are made using auto-
matically predicted information on other structural
and semantic layers including the parses, seman-
tic roles, word senses, and named entities that
were produced in the earlier sections. Each docu-
ment part from the documents that were split into
multiple parts during coreference annotation were
treated as separate document.
We used the number and gender predictions
generated by Bergsma and Lin (2006). Unfortu-
nately neither Arabic, nor Chinese have compara-
ble data available. Chinese, in particular, does not
have number or gender inflections for nouns, but
(Baran and Xue, 2011) look at a way to infer such
information.
We trained the Bjo?rkelund and Farkas (2012)
coreference system21 which uses a combination of
two pair-wise resolvers, the first is an incremen-
tal chain-based resolution algorithm (Bjo?rkelund
and Farkas, 2012), and the second is a best-first
resolver (Ng and Cardie, 2002). The two resolvers
are combined by stacking, i.e., the output of the
first resolver is used as features in the second one.
The system uses a large feature set tailored for
each language which, in addition to classic coref-
erence features, includes both lexical and syntactic
information.
Recently, it was discovered that there is pos-
sibly a bug in the official scorer used for the
CoNLL 2011/2012 and the SemEval 2010 corefer-
ence tasks. This relates to the mis-implementation
of the method proposed by (Cai and Strube, 2010)
for scoring predicted mentions. This issue has also
been recently reported in Recasens et al, (2013).
As of this writing, the BCUBED metric has been
fixed, and the correctness of the CEAFm, CEAFe
and BLANC metrics is being verified. We will
be updating the CoNLL shared task webpages22
with more detailed information and also release
the patched scripts as soon as they are available.
We will also re-generate the scores for previous
shared tasks, and the coreference layer in this pa-
per and make them available along with the mod-
els and system outputs for other layers. Table
7 shows the performance of the system on the
21http://www.ims.uni-stuttgart.de/?anders/coref.html
22http://conll.cemantix.org
149
CoNLL-2012 test set, broken down by genre. The
same metrics that were used for the CoNLL-2012
shared task are computed, with the CONLL col-
umn being the official CONLL measure.
Language Genre MD MUC BCUBED CEAFm CEAFe BLANC CONLL
PREDICTED MENTIONS
English BC 73.43 63.92 61.98 54.82 42.68 73.04 56.19
BN 73.49 63.92 65.85 58.93 48.14 72.74 59.30
MZ 71.86 64.94 71.38 64.03 50.68 78.87 62.33
NW 68.54 60.20 65.11 57.54 45.10 73.72 56.80
PT 86.95 79.09 68.33 65.52 50.83 77.74 66.08
TC 80.81 76.78 71.35 65.41 45.44 82.45 64.52
WB 74.43 66.86 61.43 54.76 42.05 73.54 56.78
Overall 75.38 67.58 65.78 59.20 45.87 75.8 59.74
Chinese BC 68.02 59.6 59.44 53.12 40.77 73.63 53.27
BN 68.57 61.34 67.83 60.90 48.10 77.39 59.09
MZ 55.55 48.89 58.83 55.63 46.04 74.25 51.25
NW 89.19 80.71 73.64 76.30 70.89 82.56 75.08
TC 77.72 73.59 71.65 64.30 48.52 83.14 64.59
WB 72.61 65.79 62.32 56.71 43.67 77.45 57.26
Overall 66.37 58.61 66.56 59.01 48.19 76.07 57.79
Arabic NW 60.55 47.82 61.16 53.42 44.30 69.63 51.09
GOLD MENTIONS
English BC 85.63 76.09 68.70 61.73 49.87 76.24 64.89
BN 82.11 73.56 71.52 63.67 52.29 75.70 65.79
MZ 85.65 77.73 78.82 72.75 60.09 83.88 72.21
NW 80.68 73.52 73.08 65.63 51.96 81.06 66.19
PT 93.20 85.72 73.25 70.76 58.81 79.78 72.59
TC 90.68 86.83 78.94 73.87 56.26 85.82 74.01
WB 88.12 80.61 69.86 63.45 51.13 76.48 67.20
Overall 86.16 78.7 72.67 66.32 53.23 79.22 68.2
Chinese BC 84.88 76.34 69.89 62.02 49.29 76.89 65.17
BN 80.97 74.89 76.88 68.91 55.56 81.94 69.11
MZ 78.85 73.06 70.15 61.68 46.86 78.78 63.36
NW 93.23 86.54 86.70 80.60 76.60 85.75 83.28
TC 92.91 88.31 84.51 79.49 63.87 90.04 78.90
WB 85.87 77.61 69.24 60.71 47.47 77.67 64.77
Overall 83.47 76.85 76.30 68.30 56.61 81.56 69.92
Arabic NW 76.43 60.81 67.29 59.50 49.32 74.61 59.14
Table 7: Performance of the coreference system
on the CoNLL-2012 test set.
The varying results across genres mostly meet
our expectations. In English, the system does best
on TC and the PT genres. The text in the TC set
often involve long chains where the speakers re-
fer to themselves which, given speaker informa-
tion, is fairly easy to resolve. The PT section
includes many references to god (e.g. god and
the lord) which the lexicalized resolver is quite
good at picking up during training. The more dif-
ficult genres consist of texts where references to
many entities are interleaved in the discourse and
is as such harder to resolve correctly. For Chi-
nese the numbers on the TC genre are also quite
good, and the explanation above also holds here
? many mentions refer to either of the speak-
ers. For Chinese the NW section displays by far
the highest scores, however, and the reason for
this is not clear to us. Not surprisingly, restricting
the set of mentions only to gold mentions gives
a large boost across all genres and all languages.
This shows that mention detection (MD) and sin-
gleton detection (which is not part of the annota-
tion) remain a big source of errors for the coref-
erence resolver. For these experiments we used
a combination of training and development data
for training ? following the CoNLL-2012 shared
task specification. Leaving out the development
set has a very negligible effect on the CoNLL-
score for all the languages (English: 0.14; Chi-
nese 0.06; Arabic: 0.40 F-score respectively). The
effect on Arabic is the most (0.40 F-score) most
likely because of its much smaller size. To gauge
the performance improvement between 2011 and
2012 shared tasks, we performed a clean com-
parison of over the best performing system and
an earlier version of this system (Bjo?rkelund and
Nugues, 2011) on the CoNLL 2011 test set us-
ing the CoNLL 2011 train and development set
for training. The current system has a CoNLL
score of 60.09 (64.92+69.84+45.513 )23 as opposed tothe 54.53 reported in bjo?rkelund (Bjo?rkelund and
Nugues, 2011), and the 57.79 reported for the best
performing system of CoNLL-2011. One caveat
is that these score comparison are done using the
earlier version (v4) of the CoNLL scorer. Nev-
ertheless, it is encouraging to see that within a
short span of a year, there has been significant
improvement in system performance ? partially
owing to cross-pollination of research generated
through the shared tasks.
5 Conclusion
In this paper we reported work on finding a rea-
sonable training, development and test split for
the various layers of annotation in the OntoNotes
v5.0 corpus, which consists of multiple genres in
three typologically very different languages. We
also presented the performance of publicly avail-
able, state-of-the-art algorithms on all the different
layers of the corpus for the different languages.
The trained models as well as their output will
be made publicly available24 to serve as bench-
marks for language processing community. Train-
ing so many different NLP components is very
time-consuming, thus, we hope the work reported
here has lifted the burden of having to create rea-
sonable baselines for researchers who wish to use
this corpus to evaluate their systems. We created
just one data split in training, development and test
set, covering a collection of genres for each layer
of annotation in each language in order to keep the
workload manageable However, the results do not
discriminate the performance on individual gen-
res: we believe such a setup is still a more realistic
gauge for the performance of the state-of-the-art
NLP components than a monolithic corpus such
as the Wall Street Journal section of the Penn Tree-
bank. It can be used as a starting point for devel-
oping the next generation of NLP components that
are more robust and perform well on a multitude
of genres for a variety of different languages.
23(MUC + BCUBED + CEAFe)/3
24http://cemantix.org
150
6 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022for sponsoring the creation of the OntoNotes
corpus. This work was partially supported
by grants R01LM10090 and U54LM008748
from the National Library Of Medicine, and
R01GM090187 from the National Institutes ofGeneral Medical Sciences. We are indebted toSlav Petrov for helping us to retrain his syntactic
parser for Arabic. Alessandro Moschitti and
Olga Uryupina have been partially funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under the grant
number 288024 (LIMOSINE). The content
is solely the responsibility of the authors and
does not necessarily represent the official views
of the National Institutes of Health. NianwenXue and Yuchen Zhang are supported in part
by the DAPRA via contract HR0011-11-C-0145
entitled ?Linguistic Resources for Multilingual
Processing.?
References
Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Libin
Shen. 2006. Issues in synchronizing the English treebank
and propbank. In Workshop on Frontiers in Linguistically
Annotated Corpora 2006, July.
Elizabeth Baran and Nianwen Xue. 2011. Singular or plural?
exploiting parallel corpora for Chinese number prediction.
In Proceedings of Machine Translation Summit XIII, Xia-
men, China.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping path-
based pronoun resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computa-
tional Linguistics, pages 33?40, Sydney, Australia, July.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-driven
multilingual coreference resolution using resolver stack-
ing. In Joint Conference on EMNLP and CoNLL - Shared
Task, pages 49?55, Jeju Island, Korea, July. Association
for Computational Linguistics.
Anders Bjo?rkelund and Pierre Nugues. 2011. Exploring lex-
icalized features for coreference resolution. In Proceed-
ings of the Fifteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 45?50, Port-
land, Oregon, USA, June. Association for Computational
Linguistics.
Jie Cai and Michael Strube. 2010. Evaluation metrics for
end-to-end coreference resolution systems. In Proceed-
ings of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, SIGDIAL ?10, pages
28?36.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty elements. In
Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 212?216, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL), Ann Arbor, MI,
June.
Eugene Charniak and Mark Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings of the
Second Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Ann Arbor, MI,
June.
Mona Diab, Alessandro Moschitti, and Daniele Pighin. 2008.
Semantic role labeling systems for Arabic using kernel
methods. In Proceedings of ACL-08: HLT, pages 798?
806, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Gerard Escudero, Lluis Marquez, and German Rigau. 2000.
An empirical study of the domain dependence of super-
vised word disambiguation systems. In 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages 172?
180, Hong Kong, China, October. Association for Com-
putational Linguistics.
Benoit Favre, Bernd Bohnet, and D. Hakkani-Tur. 2010.
Evaluation of semantic role labeling and dependency
parsing of automatic speech recognition output. In
Proceedings of 2010 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), page
5342?5345.
Jenny Rose Finkel and Christopher D. Manning. 2009. Joint
parsing and named entity recognition. In Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 326?334, Boulder,
Colorado, June. Association for Computational Linguis-
tics.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into in-
formation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, page 363?370.
Ryan Gabbard. 2010. Null Element Restoration. Ph.D. the-
sis, University of Pennsylvania.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In 2001 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2002. Acquir-
ing compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of the Third LREC Conference, page
1974?1981.
Mohamed Maamouri and Ann Bies. 2004. Developing an
Arabic treebank: Methods, guidelines, procedures, and
tools. In Ali Farghaly and Karine Megerdoomian, edi-
tors, COLING 2004 Computational Approaches to Arabic
Script-based Languages, pages 2?9, Geneva, Switzerland,
August 28th. COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn treebank. Computational Linguis-
tics, 19(2):313?330, June.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceedings
of the Human Language Technology Conference/North
American Chapter of the Association for Computational
Linguistics (HLT/NAACL), New York City, NY, June.
151
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the Association for Computational Linguistics
(ACL-02), pages 104?111.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum.
2007. Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Journal of
Natural Language Engineering, 13(2).
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona Diab,
Mohammed Maamouri, Aous Mansouri, and Wajdi Za-
ghouani. 2008. A pilot Arabic propbank. In Proceedings
of the International Conference on Language Resources
and Evaluation (LREC), Marrakech, Morocco, May 28-
30.
Slav Petrov and Dan Klein. 2007. Improved inferencing for
unlexicalized parsing. In Proc of HLT-NAACL.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005. Support
vector learning for semantic argument classification. Ma-
chine Learning, 60(1):11?39.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel, Jes-
sica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Indentifying entities and events in
OntoNotes. In Proceedings of the IEEE International
Conference on Semantic Computing (ICSC), September
17-19.
Sameer Pradhan, Wayne Ward, and James H. Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics Special Issue on Semantic Role Labeling,
34(2).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha
Palmer, Ralph Weischedel, and Nianwen Xue. 2011.
CoNLL-2011 shared task: Modeling unrestricted corefer-
ence in OntoNotes. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learning:
Shared Task, pages 1?27, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga
Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 shared
task: Modeling multilingual unrestricted coreference in
OntoNotes. In Joint Conference on EMNLP and CoNLL -
Shared Task, pages 1?40, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki,
Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008.
The Penn discourse treebank 2.0. In Proceedings of the
Sixth International Conference on Language Resources
and Evaluation (LREC?08), Marrakech, Morocco, May.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena, M. Anto`nia
Mart??, Mariona Taule?, Ve?ronique Hoste, Massimo Poesio,
and Yannick Versley. 2010. Semeval-2010 task 1: Coref-
erence resolution in multiple languages. In Proceedings of
the 5th International Workshop on Semantic Evaluation,
pages 1?8, Uppsala, Sweden, July.
Marta Recasens, Marie-Catherine de Marneffe, and Christo-
pher Potts. 2013. The life and death of discourse enti-
ties: Identifying singleton mentions. In Proceedings of
the 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pages 627?633, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind K. Joshi. 2008.
LTAG-spinal and the treebank. Language Resources and
Evaluation, 42(1):1?19, March.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog no.:
LDC2005T33. BBN Technologies.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Robert Belvin, Sameer Pradhan, Lance Ramshaw,
and Nianwen Xue. 2011. OntoNotes: A large train-
ing corpus for enhanced processing. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language Ex-
ploitation. Springer.
Nianwen Xue and Martha Palmer. 2009. Adding semantic
roles to the Chinese Treebank. Natural Language Engi-
neering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer.
2005. The Penn Chinese TreeBank: phrase structure an-
notation of a large corpus. Natural Language Engineer-
ing, 11(2):207?238.
Nianwen Xue. 2008. Labeling Chinese predicates with se-
mantic roles. Computational Linguistics, 34(2):225?255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese treebank.
In Proceedings of the 23rd International Conference on
Computational Linguistics (COLING), Beijing, China.
Wajdi Zaghouani, Mona Diab, Aous Mansouri, Sameer Prad-
han, and Martha Palmer. 2010. The revised Arabic prop-
bank. In Proceedings of the Fourth Linguistic Annotation
Workshop, pages 222?226, Uppsala, Sweden, July.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-
coverage word sense disambiguation system for free text.
In Proceedings of the ACL 2010 System Demonstrations,
pages 78?83, Uppsala, Sweden.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An empiri-
cal study. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1002?
1010.
152
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 39?48,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Learning to Rank Answer Candidates
for Automatic Resolution of Crossword Puzzles
Gianni Barlacchi
University of Trento
38123 Povo (TN), Italy
gianni.barlacchi@gmail.com
Massimo Nicosia and Alessandro Moschitti
Qatar Computing Research Institute
5825 Doha, Qatar
m.nicosia@gmail.com, amoschitti@qf.org.qa
Abstract
In this paper, we study the impact of rela-
tional and syntactic representations for an
interesting and challenging task: the au-
tomatic resolution of crossword puzzles.
Automatic solvers are typically based on
two answer retrieval modules: (i) a web
search engine, e.g., Google, Bing, etc. and
(ii) a database (DB) system for access-
ing previously resolved crossword puz-
zles. We show that learning to rank models
based on relational syntactic structures de-
fined between the clues and the answer can
improve both modules above. In particu-
lar, our approach accesses the DB using
a search engine and reranks its output by
modeling paraphrasing. This improves on
the MRR of previous system up to 53% in
ranking answer candidates and greatly im-
pacts on the resolution accuracy of cross-
word puzzles up to 15%.
1 Introduction
Crossword puzzles (CPs) are probably the most
popular language games played around the world.
It is very challenging for human intelligence as it
requires high level of general knowledge, logical
thinking, intuition and the ability to deal with am-
biguities and puns. CPs normally have the form
of a square or rectangular grid of white and black
shaded squares. The white squares on the border
of the grid or adjacent to the black ones are associ-
ated with clues. The goal of the game is to fill the
sequences of white squares with words answering
the clues.
There have been many attempts to build auto-
matic CP solving systems, which have also par-
ticipated in competitions such as The American
Crossword Puzzle Tournament (ACPT). This is
the oldest and largest CP tournament for cross-
word experts held in the United States. The goal
of such systems is to outperform human players
in solving crosswords more accurately and in less
time.
Automatic CP solvers have been mainly tar-
geted by the artificial intelligence (AI) community,
who has mostly focused on AI techniques for fill-
ing the puzzle grid, given a set of answer candi-
dates for each clue. The basic idea is to optimize
the overall probability of correctly filling the entire
grid by exploiting the likelihood of each candidate
answer, fulfilling at the same time the grid con-
straints. After several failures in approaching the
human expert performance, it has become clear
that designing more accurate solvers would not
have provided a winning system. In contrast, the
Precision and Recall of the answer candidates are
obviously a key factor: a very high value for both
of them would enable the solver to quickly find the
correct solution.
This basically suggests that, similarly to the
Jeopardy! challenge case (Ferrucci et al., 2010b),
the solution relies on Question Answering (QA)
research. However, although some CP clues are
rather similar to standard questions, as for ex-
ample, in the clue/answer pair:

What keeps a
camera rolling?: dolly

, some specific differences
hold: (i) clues can be in interrogative form or not,
e.g.,

Capital of USA: Washington

; (ii) they can
contain riddles or be deliberately ambiguous and
misleading (e.g.,

It?s green at first: orange

);
(iii) the exact length of the answer keyword is
known in advance; and (vi) the confidence in the
answers is an extremely important input for the CP
solver.
In this paper, we study methods for improving
the quality of automatic extraction of answer can-
didate lists for automatic CP resolution. For this
purpose, we designed learning to rank models for
reordering the answers produced with two differ-
ent techniques typically used in CP systems: (i)
searching the Web with clue representations, e.g.,
39
exploiting Bing search engine
1
; and (ii) querying
the DB of previously resolved CP clues, e.g., using
standard SQL techniques.
We rerank the text snippets returned by Bing by
means of SVM preference ranking (Herbrich et al.,
2000) for improving the first technique. One in-
teresting contribution is that our model exploits a
syntactic representation of clues to improve Web
search. More in detail, we use structural kernels
(e.g., see (Moschitti, 2006; Moschitti, 2008)) in
SVMs applied to our syntactic representation of
pairs, formed by clues with their candidate snip-
pets. Regarding the DB approach, we provide a
completely novel solution by substituting it and
the SQL function with a search engine for retriev-
ing clues similar to the target one. Then, we rerank
the retrieved clues by applying SVMs and struc-
tural kernels to the syntactic representation of clue
pairs. This way, SVMs learn to choose the best
candidate among similar clues that are available in
the DB. The syntactic representation captures clue
paraphrasing properties.
In order to carry out our study, we created two
different corpora, one for each task: (i) a snip-
pets reranking dataset and (ii) a clue similarity
dataset. The first includes 21,000 clues, each asso-
ciated with 150 candidate snippets whereas the lat-
ter comprises 794,190 clues. These datasets con-
stitute interesting resources that we made available
to the research community
2
.
We compare our methods with one of the best
systems for automatic CP resolution, WebCrow
(Ernandes et al., 2005). Such system does use
the two approaches mentioned before. Regarding
snippet reranking, our structural models improve
on the basic approach of WebCrow based on Bing
by more than 4 absolute percent points in MRR,
for a relative improvement of 23%. Concerning
the similar clues retrieval, our methods improve
on the one used by WebCrow, based on DBs, by
25% absolute, i.e., about 53% of error reduction
whereas the answer accuracy at first position im-
proves up to 70%.
Given such promising results, we used our clue
reranking method in WebCrow, and obtained an
average improvement of 15% in resolving com-
plete CPs. This demonstrates that advanced QA
methods such as those based on syntactic struc-
tures and learning to rank methods can help to win
1
https://www.bing.com/
2
http://projects.disi.unitn.it/
iKernels/projects/webcrow/
the CP resolution challenge.
In the reminder of this paper, Sec. 2 introduces
the automatic CP resolution task in the context
of the related work, Sec. 3 introduces WebCrow,
Sec. 4 illustrates our models for snippets rerank-
ing and similar clue retrieval using kernel meth-
ods, syntactic structures, and traditional feature
vectors, Sec. 5 describes our experiments, and fi-
nally, Sec. 6 derives the conclusions.
2 Related Work
Proverb (Littman et al., 2002) was the first sys-
tem for the automatic resolution of CPs. It in-
cludes several modules for generating lists of can-
didate answers. These lists are merged and used to
solve a Probabilistic-Constraint Satisfaction Prob-
lem. Proverb relies on a very large crossword
database as well as several expert modules, each of
them mainly based on domain-specific databases
(e.g., movies, writers and geography). In addition,
it employs generic-word list generators and clue-
specific modules to find solutions for particular
kinds of clues like

Tel (4): aviv

. Proverb?s
modules use many knowledge sources: databases
of clues, encyclopedias and Web documents. Dur-
ing the 1998 ACPT, Proverb placed 109th out of
251 contestants.
WebCrow (Ernandes et al., 2005) is based on
Proverb. It incorporates additional knowledge
sources, provides a solver for the Italian language
and improves the clues retrieval model from DB.
In particular, it enables partial matching to re-
trieve clues that do not perfectly overlap with the
query. WebCrow carries out basic linguistic anal-
ysis such as Part-Of-Speech tagging and lemma-
tization. It takes advantage of semantic relations
contained in WordNet, dictionaries and gazetteers.
Its Web module is constituted by a search en-
gine, which can retrieve text snippets or docu-
ments related to the clue. Answer candidates
and their confidence scores are generated from
this content. WebCrow uses a WA* algorithm
(Pohl, 1970) for Probabilistic-Constraint Satisfac-
tion Problems, adapted for CP resolution. The
solver fills the grid entries for which no solution
was found by the previous modules. It tries com-
binations of letters that satisfy the crossword con-
straints, where the letters are derived from words
found in dictionaries or in the generated candidate
lists. WebCrow participated in international com-
petitions with good results.
40
Figure 1: Overview of WebCrow?s architecture.
Dr. Fill (Ginsberg, 2011) targets the crossword
filling task with a Weighted-Constraint Satisfac-
tion Problem. Constraint violations are weighted
and can be tolerated. It heavily relies on huge
databases of clues. It was placed 92nd out of more
than 600 opponents in the 2013 ACPT.
Specifically for QA using syntactic structures,
a referring work for our research is the IBM Wat-
son system (Ferrucci et al., 2010a). This is an ad-
vanced QA pipeline based on deep linguistic pro-
cessing and semantic resources. It demonstrated
that automatic methods can be more accurate than
human experts in answering complex questions.
More traditional studies on passage reranking,
exploiting structural information, were carried out
in (Katz and Lin, 2003), whereas other meth-
ods explored soft matching (i.e., lexical similarity)
based on answer and named entity types (Aktolga
et al., 2011). (Radlinski and Joachims, 2006; Jeon
et al., 2005) applied question and answer classi-
fiers for passage reranking. In this context, sev-
eral approaches focused on reranking the answers
to definition/description questions, e.g., (Shen and
Lapata, 2007; Moschitti et al., 2007; Surdeanu et
al., 2008; Severyn and Moschitti, 2012; Severyn
et al., 2013b).
3 WebCrow Architecture
Our research focuses on the generation of accurate
answer candidate lists, which, when used in a CP
resolution systems, can improve the overall solu-
tion accuracy. Therefore, the quality of our mod-
ules can be assessed by testing them within such
systems. For this purpose, we selected WebCrow
as it is rather modular, accurate and it was kindly
made available by the authors. Its architecture is
illustrated in Figure 1.
The solving process is divided in two phases:
in the first phase, the coordinator module forwards
the clues of an input CP to a set of modules for
the generation of several candidate answer lists.
Each module returns a list of possible solutions
for each clue. Such individual clue lists are then
merged by a specific Merger component, which
uses list confidence values and the probabilities of
correctness of each candidate in the lists. Eventu-
ally, a single list of candidate-probability pairs is
generated for each input clue. During the second
phase WebCrow fills the crossword grid by solving
a constraint-satisfaction problem. WebCrow se-
lects a single answer from each candidate merged
list, trying to satisfy the imposed constraints. The
goal of this phase is to find an admissible solution
maximizing the number of correct inserted words.
In this paper, we focus on two essential modules
of WebCrow: the Web and the DB modules, de-
scribed in the next sections.
3.1 WebSearch Module (WSM)
WSM carries out four different tasks: (i) the re-
trieval of useful text snippets (TS) and web docu-
ments, (ii) the extraction of the answer candidates
from such text, (iii) the scoring/filtering of the can-
didates, and (iv) the estimation of the list confi-
dence. The retrieval of TS is performed by the
Bing search engine by simply providing it the clue
through its APIs. Then, the latter again are used
to access the retrieved TS. The word list gener-
ator extracts possible candidate answers from TS
or Web documents by picking the terms (also mul-
tiwords) of the correct length. The generated lists
are merged and sorted using the candidate confi-
dence computed by two filters: the statistical filter
and the morphological filter. The score associated
with each candidate word w is given by the fol-
lowing heuristic formula:
p(w,C) = k(score
sf
(w,C)? score
mf
(w,C)),
where (i) C is the target clue, (ii) k is a
constant tuned on a validation set such that
?
n
i=0
p(w
n
, C) = 1, (iii) score
sf
(w,C) is com-
puted using statistical information extracted from
the text, e.g., the classical TF?IDF, and (iv)
score
mf
(w,C) is computed using morphological
features of w.
41
S
REL-NP REL-NP VP NP
REL-NNP REL-POS TO VB CC RB TO VB NN
hamlet 's to be or not to be addressee
S
NP VP REL-NP REL-NP VP VP NP VP PP NP ADVP PP NP PP NP
DT RBS JJ NN VBZ REL-NNP REL-POS TO VB CC RB TO VB DT NN VBZVBN IN DT NN RB TO PRP TO DT NN
the most obvious example be hamlet 's to be or not to be the monologue be address by a character either to himself to the audience
Figure 2: Shallow syntactic trees of clue (upper) and snippet (lower) and their relational links.
3.2 Database module (CWDB)
The knowledge about previous CPs is essential for
solving new ones. Indeed, clues often repeat in
different CPs, thus the availability of a large DB
of clue-answer pairs allows for easily finding the
answers to previously used clues. In order to ex-
ploit the database of clue-answer pairs, WebCrow
uses three different modules:
CWDB-EXACT, which simply checks for an
exact matching between the target clue and those
in the DB. The score of the match is computed
using the number of occurrences of the matched
clue.
CWDB-PARTIAL, which employs MySQL?s
partial matching function, query expansion and
positional term distances to compute clue-
similarity scores, along with the Full-Text search
functions.
CWDB-DICTIO, which simply returns the full
list of words of correct length, ranked by their
number of occurrences in the initial list.
We improve WSM and CWDB by applying
learning-to-rank algorithms based on SVMs and
tree kernels applied to structural representations.
We describe our models in detail in the next sec-
tion.
4 Learning to rank with kernels
The basic architecture of our reranking framework
is relatively simple: it uses a standard preference
kernel reranking approach (e.g., see (Shen and
Joshi, 2005; Moschitti et al., 2006)). The struc-
tural kernel reranking framework is a specializa-
tion of the one we proposed in (Severyn and Mos-
chitti, 2012; Severyn et al., 2013b; Severyn et al.,
2013a). However, to tackle the novelty of the task,
especially for clue DB retrieval, we modeled inno-
vative kernels. In the following, we first describe
the general framework and then we instantiate it
for the two reranking tasks studied in this paper.
4.1 Kernel framework
The framework takes a textual query and retrieves
a list of related text candidates using a search en-
gine (applied to the Web or a DB), according to
some similarity criteria. Then, the query and can-
didates are processed by an NLP pipeline. The
pipeline is based on the UIMA framework (Fer-
rucci and Lally, 2004) and contains many text
analysis components. The latter used for our spe-
cific tasks are: the tokenizer
3
, sentence detector
1
,
lemmatizer
1
, part-of-speech (POS) tagger
1
, chun-
ker
4
and stopword marker
5
.
The annotations produced by such processors
are used by additional components to produce
structural models representing clues and TS. The
structure component converts the text fragments
into trees. We use both trees and feature vectors
to represent pairs of clues and TS, which are em-
ployed to train kernel-based rerankers for reorder-
ing the candidate lists provided by a search engine.
Since the syntactic parsing accuracy can impact
the quality of our structure and thus the accuracy
of our learning to rank algorithms, we preferred
to use shallow syntactic trees over full syntactic
representations. In the next section, we first de-
scribe the structures we used in our kernels, then
the tree kernels used as building blocks for our
models. Finally, we show the reranking models
for both tasks, TS and clue reranking.
3
http://nlp.stanford.edu/software/
corenlp.shtml
4
http://cogcomp.cs.illinois.edu/page/
software_view/13
5
Based on a standard stoplist.
42
Rank Clue Answer
1 Kind of support for a computer user tech
2 Kind of computer connection wifi
3 Computer connection port
4 Comb users bees
5 Traveling bag grip
Table 1: Clue ranking for the query: Kind of con-
nection for traveling computer users (wifi)
4.2 Relational shallow tree representation
The structures we adopt are similar to those de-
fined in (Severyn et al., 2013b). They are essen-
tially shallow syntactic trees built from POS tags
grouped into chunks. Each clue and its answer
candidate (either a TS or clue) are encoded into
a tree having word lemmas at the leaves and POS
tags as pre-terminals. The higher tree level orga-
nizes POS tags into chunks. For example, the up-
per tree of Figure 2, shows a shallow tree for the
clue: Hamlet?s ?To be, or not to be? addressee,
whereas the lower tree represents a retrieved TS
containing the answer, himself : The most obvious
example is Hamlet?s ?To be or not to be ... the
monologue is addressed by a character either to
himself or to the audience.
Additionally, we use a special REL tag to link
the clue/snippet trees above such that structural re-
lations will be captured by tree fragments. The
links are established as follows: words from a
clue and a snippet sharing a lemma get their par-
ents (POS tags) and grandparents, i.e., chunk la-
bels, marked by a prepending REL tag. We build
such structural representations for both snippet
and similar clue reranking tasks.
4.3 Tree kernels
We briefly report the different types of kernels
(see, e.g., (Moschitti, 2006) for more details).
Syntactic Tree Kernel (STK), also known as a
subset tree kernel (Collins and Duffy, 2002), maps
objects in the space of all possible tree fragments
constrained by the rule that the sibling nodes from
their parents cannot be separated. In other words,
substructures are composed by atomic building
blocks corresponding to nodes along with all of
their direct children. These, in case of a syntac-
tic parse tree, are complete production rules of the
associated parser grammar.
STK
b
extends STK by allowing leaf nodes to be
part of the feature space. Leaf in syntactic trees are
words, from this the subscript b (bag-of-words).
Subtree Kernel (SbtK) is one of the simplest tree
kernels as it only generates complete subtrees, i.e.,
tree fragments that, given any arbitrary starting
node, necessarily include all its descendants.
Partial Tree Kernel (PTK) (Moschitti, 2006) can
be effectively applied to both constituency and de-
pendency parse trees. It generates all possible
connected tree fragments, e.g., sibling nodes can
also be separated and be part of different tree frag-
ments. In other words, a fragment is any possible
tree path, from whose nodes other tree paths can
depart. Thus, it can generate a very rich feature
space resulting in higher generalization ability.
4.4 Snippet reranking
The task of snippet reranking consists in reorder-
ing the list of snippets retrieved from the search
engine such that those containing the correct an-
swer can be pushed at the top of the list. For this
purpose, we transform the target clue in a search
query and retrieve candidate text snippets. In our
training set, these candidate text snippets are con-
sidered as positive examples if they contain the an-
swer to the target clue.
We rerank snippets using preference reranking
approach (see, e.g., (Shen and Joshi, 2005)). This
means that two snippets are compared to derive
which one is the best, i.e., which snippet contains
the answer with higher probability. Since we aim
at using kernel methods, we apply the following
preference kernel:
P
K
(?s
1
, s
2
?, ?s
?
1
, s
?
2
?) = K(s
1
, s
?
1
)+
+K(s
2
, s
?
2
)?K(s
1
, s
?
2
)?K(s
2
, s
?
1
),
where s
r
and s
?
r
refer to two sets of candidates
associated with two rankings and K is a kernel
applied to pairs of candidates. We represent the
latter as pairs of clue and snippet trees. More for-
mally, given two candidates, s
i
= ?s
i
(c), s
i
(s)?
and s
?
i
= ?s
?
i
(c), s
?
i
(s)?, whose members are the
clue and snippet trees, we define
K(s
i
, s
?
i
) = TK(s
i
(c), s
?
i
(c))+TK(s
i
(s), s
?
i
(s)),
where TK can be any tree kernel function, e.g.,
STK or PTK. Finally, it should be noted that, to
add traditional feature vectors to the reranker, it is
enough to add the product (~x
s
1
?~x
s
2
) ?(~x
s
?
1
?~x
s
?
2
)
to the structural kernel P
K
, where ~x
s
is the feature
vector associated with the snippet s.
43
S
REL-NP PP REL-NP PP REL-NP
REL-NNP IN REL-NN IN VBG REL-NN NNS
kind of connection for travel computer user
S
REL-NP PP REL-NP
REL-NNP IN REL-NN REL-NN
kind of computer connection
Figure 3: Two similar clues leading to the same answer.
4.5 Similar clue reranking
WebCrow creates answer lists by retrieving clues
from the DB of previously solved crosswords. It
simply uses the classical SQL operator and full-
text search. We instead verified the hypothesis
that a search engine could achieve a better re-
sult. Thus we opted for indexing the DB clues
and their answers with the open source search en-
gine Lucene (McCandless et al., 2010), using the
state-of-the-art BM25 retrieval model. This alone
significantly improved the quality of the retrieved
clue list, which could be further refined by apply-
ing reranking. The latter consists in (i) retrieving
a list of similar clues using a search engine and
(ii) moving those more similar, which more prob-
ably contain the same answer to the clue query,
at the top. For example, Table 1 shows the first
five clues, retrieved for a query built from the clue:
Kind of connection for traveling computer users.
The search engine retrieves the wrong clue, Kind
of connection for traveling computer users, at the
top since it overlaps more with the query.
To solve these kinds of problems by also en-
hancing the generalization power of our reranking
algorithm, we use a structural representation
similar to the one for TS that we illustrated in
the previous section. The main difference with
the previous models is that the reranking pair is
only constituted by clues. For example, Fig. 3
shows the representation of the pairs constituted
by the query clue and the correct clue ranked
in the second position (see Table 1). The rela-
tional arrows suggest a syntactic transformation
from connection for
*
computer to
computer connection, which can be used
by the reranker to prefer the correct clue to
the wrong one. Note that such transformation
corresponds to the pair of tree fragments: [S
[REL-NP[REL-NN]][PP][NP[VBG][REL-NN]]]
? [S [REL-NP[REL-NN][REL-NN]]], where the
node pairs, ?REL-NN,REL-NN? define the arguments
of the syntactic transformation. Such fragments
can be generated by PTK, which can thus be used
for learning clue paraphrasing.
To build the reranking training set, we used
the training clues for querying the search engine,
which draws candidates from the indexed clues.
We stress the fact that this set of clues is disjoint
from the clues in the training and test sets. Thus,
identical clues are not present across sets. At clas-
sification time, the new clue is used as a search
query. Similar candidate clues are retrieved and
used to form pairs.
4.6 Feature Vectors
In addition to structural representations, we also
used features for capturing the degrees of similar-
ity between clues within a pair.
DKPro Similarity. We used similarity features
from a top performing system in the Semantic
Textual Similarity (STS) task, namely DKPro
from the UKP Lab (B?ar et al., 2013). These
features were effective in predicting the degree
of similarity between two sentences. DKPro in-
cludes the following syntactic similarity metrics,
operating on string sequences, and more advanced
semantic similarities:
? Longest common substring measure (Gusfield,
1997). It determines the length of the longest
substring shared by two text segments.
? Longest common subsequence measure (Allison
and Dix, 1986). It extends the notion of substrings
to word subsequences by applying word insertions
or deletions to the original input text pairs.
? Running-Karp-Rabin Greedy String Tiling
(Wise, 1996). It provides a similarity between two
sentences by counting the number of shuffles in
their subparts.
? Resnik similarity (Resnik, 1995). The WordNet
hypernymy hierarchy is used to compute a mea-
sure of semantic relatedness between concepts
expressed in the text. The aggregation algorithm
by Mihalcea et al. (Mihalcea et al., 2006) is
applied to extend the measure from words to
sentences.
? Explicit Semantic Analysis (ESA) similarity
44
(Gabrilovich and Markovitch, 2007). It represents
documents as weighted vectors of concepts
learned from Wikipedia, WordNet and Wik-
tionary.
? Lexical Substitution (Biemann, 2013). A super-
vised word sense disambiguation system is used
to substitute a wide selection of high-frequency
English nouns with generalizations. Resnik
and ESA features are then computed on the
transformed text.
New features. Hereafter, we describe new fea-
tures that we designed for CP reranking tasks.
? Feasible Candidate. It is a binary feature sig-
naling the presence or absence of words with the
same length of the clue answer (only used for snip-
pet reranking).
? Term overlap features. They compute the co-
sine similarity of text pairs encoded into sets of n-
grams extracted from different text features: sur-
face forms of words, lemmas and POS-tags. They
are computed keeping and removing stop-words.
They complement DKPro features.
? Kernel similarities. These are computed using
(i) string kernels applied to sentences, or PTK ap-
plied to structural representations with and with-
out embedded relational information (REL). This
similarity is computed between the members of a
?clue, snippet? or a ?clue, clue? pair.
5 Experiments
Our experiments aim at demonstrating the effec-
tiveness of our models on two different tasks: (i)
Snippet Reranking and (ii) Similar Clue Retrieval
(SCR). Additionally, we measured the impact of
our best model for SCR in the WebCrow system
by comparing with it. Our referring database of
clues is composed by 1,158,202 clues, which be-
long to eight different crossword editors (down-
loaded from the Web
6
). We use the latter to create
one dataset for snippet reranking and one dataset
for clues retrieval.
5.1 Experimental Setup
To train our models, we adopted SVM-light-TK
7
,
which enables the use of structural kernels (Mos-
chitti, 2006) in SVM-light (Joachims, 2002), with
default parameters. We applied a polynomial ker-
nel of degree 3 to the explicit feature vectors,
6
http://www.crosswordgiant.com
7
http://disi.unitn.it/moschitti/
Tree-Kernel.htm
Model MAP MRR AvgRec REC@1 REC@5
Bing 16.00 18.09 69.00 12.50 24.80
V 18.00 19.88 76.00 14.20 26.10
SbtK 17.00 19.6 75.00 13.80 26.40
STK 18.00 20.44 76.00 15.10 27.00
STK
b
18.00 20.68 76.00 15.30 27.40
PTK 19.00 21.65 77.00 16.10 28.70
V+SbtK 20.00 22.39 80.00 17.20 29.10
V+STK 19.00 20.82 78.00 14.90 27.90
STK
b
19.00 21.20 79.00 15.60 28.40
V+PTK 19.00 21.68 79.00 16.00 29.40
V+DK 18.00 20.48 77.00 14.60 26.80
V+DK+SbtK 20.00 22.29 80.00 16.90 28.70
V+DK+STK 19.00 21.47 79.00 15.50 28.30
V+DK+STK
b
19.00 21.58 79.00 15.4 28.60
V+DK+PTK 20.00 22.24 80.00 16.80 29.30
Table 2: Snippet reranking
Model MAP MRR AvgRec REC@1 REC@5
MB25 69.00 73.78 80.00 62.11 81.23
WebCrow - 53.22 58.00 39.60 62.85
SbtK 52.00 54.72 69.00 36.50 64.05
STK 63.00 68.21 77.00 54.57 76.11
STK
b
63.00 67.68 77.00 53.85 75.63
PTK 65.00 70.12 78.00 57.39 77.65
V+SbtK 68.00 73.26 80.00 60.95 81.28
V+STK 71.00 76.01 82.00 64.58 83.95
V+STK
b
70.00 75.68 82.00 63.95 83.77
V+PTK 71.00 76.67 82.00 65.67 84.07
V+DK 71.00 76.76 81.00 65.55 84.29
V+DK+SbtK 72.00 76.91 82.00 65.87 84.51
V+DK+STK 73.00 78.37 84.00 67.83 85.87
V+DK+STK
b
73.00 78.29 84.00 67.71 85.77
V+DK+PTK 73.00 78.13 83.00 67.39 85.75
Table 3: Reranking of similar clues.
as we believe feature combinations can be valu-
able. To measure the impact of the rerankers as
well as the baselines, we used well known met-
rics for assessing the accuracy of QA and re-
trieval systems, i.e.: Recall at rank 1 (R@1 and
5), Mean Reciprocal Rank (MRR), Mean Average
Precision (MAP), the average Recall (AvgRec).
R@k is the percentage of questions with a cor-
rect answer ranked at the first position. MRR is
computed as follows: MRR =
1
|Q|
?
|Q|
q=1
1
rank(q)
,
where rank(q) is the position of the first correct an-
swer in the candidate list. For a set of queries Q,
MAP is the mean over the average precision scores
for each query:
1
Q
?
Q
q=1
AveP (q). AvgRec and
all the measures are evaluated on the first 10 re-
trieved snippets/clues. For training and testing the
reranker, only the first 10 snippets/clues retrieved
by the search engine are used.
5.2 Snippet Reranking
The retrieval from the Web is affected by a sig-
nificant query processing delay, which prevents us
45
to use entire documents. Thus, we only consid-
ered the text from Bing snippets. Moreover, since
our reranking approach does not include the treat-
ment of special clues such as anagrams or linguis-
tic games, e.g., fill-in-the blank clues, we have ex-
cluded them by our dataset. We crawled the lat-
ter from the Web. We converted each clue into a
query and downloaded the first 10 snippets as re-
sult of a Bing query. In order to reduce noise from
the data, we created a black list containing URLs
that must not be considered in the download phase,
e.g., crossword websites. The training set is com-
posed by 20,000 clues while the test set comprises
1,000 clues.
We implemented and compared many models
for reranking the correct snippets higher, i.e., con-
taining the answer to the clue. The compared sys-
tems are listed on the first column of Table 2,
where: V is the approach using the vector only
constituted by the new feature set (see Sec. 4.6);
DK is the model using the features made available
by DKPro; the systems ending in TK are described
in Sec. 4.3; and the plus operator indicates models
obtained by summing the related kernels.
Depending on the target measure they suggest
slightly different findings. Hereafter, we comment
on MRR as it is the most interesting from a rank-
ing viewpoint. We note that: (i) Bing is improved
by the reranker based on the new feature vector by
2 absolute points; (ii) DK+V improves on V by
just half point; (iii) PTK provides the highest re-
sult among individual systems; (iv) combinations
improve on the individual systems; and (v) over-
all, our reranking improves on the ranking of para-
graphs of Bing by 4 points in MRR and 5 points
in accuracy on the first candidate (REC@1), cor-
responding to about 20% and 50% of relative im-
provement and error reduction, respectively.
5.3 Similar clue retrieval
We compiled a crossword database of 794,190
unique pairs of clue-answer. Using the clues con-
tained in this set, we created three different sets:
training and test sets and the database of clues.
The database of clues can be indexed for retriev-
ing similar clues. It contains 700,000 unique clue-
answer pairs. The training set contains 39,504
clues whose answer may be found in database. Us-
ing the same approach, we created a test set con-
taining 5,060 clues that (i) are not in the training
set and (ii) have at least an answer in the database.
Model MRR REC@1 REC@5 REC@10
WebCrow 41.00 33.00 51.00 58.00
Our Model 46.00 39.00 56.00 59.00
Table 4: Performance on the word list candidates
averaged over the clues of 10 entire CPs
Model %Correct words %Correct letters
WebCrow 34.45 49.72
Our Model 39.69 54.30
Table 5: Performance given in terms of correct
words and letters averaged on the 10 CPs
We experimented with all models, as in the
previous section, trained for the similar clue re-
trieval task. However, since WebCrow includes a
database module, in Tab. 3, we have an extra row
indicating its accuracy. We note that: (i) BM25
shows a very accurate MRR, 73.78%. It largely
improves on WebCrow by about 20.5 absolute per-
cent points, demonstrating the superiority of an IR
approach over DB methods. (ii) All TK types do
not improve alone on BM25, this happens since
they do not exploit the initial rank provided by
BM25. (iii) All the feature vector and TK combi-
nations achieve high MRR, up to 4.5 absolute per-
cent points of improvement over BM25 and thus
25 points more than WebCrow, corresponding to
53% of error reduction. Finally, (iv) the relative
improvement on REC@1 is up to 71% (28.23%
absolute). This high result is promising in the light
of improving WebCrow for the end task of solving
complete CPs.
5.4 Impact on WebCrow
In these experiments, we used our reranking
model of similar clues (more specifically, the
V+DK+STK model) using 10 complete CPs (for
a total of 760 clues) from the New York Times
and Washington Post. This way, we could mea-
sure the impact of our model on the complete task
carried out by WebCrow. More specifically, we
give our reranked list of answers to WebCrow in
place of the list it would have extracted with the
CWDB module. It should be noted that to evalu-
ate the impact of our list, we disabled WebCrow
access to other lists, e.g., dictionaries. This means
that the absolute resolution accuracy of WebCrow
using our and its own lists can be higher (see (Er-
nandes et al., 2008) for more details).
46
The first result that we derive is the accuracy
of the answer list produced from the new data,
i.e., constituted by the 10 entire CPs. The results
are reported in Tab. 4. We note that the improve-
ment of our model is lower than before as a non-
negligible percentage of clues are not solved us-
ing the clue DB. However, when we compute the
accuracy in solving the complete CPs, the impact
is still remarkable as reported by Tab. 5. Indeed,
the results show that when the lists reordered by
our reranker are used by WebCrow, the latter im-
proves by more than 5 absolute percent points in
both word and character accuracy.
6 Conclusions
In this paper, we improve automatic CP resolution
by modeling two innovative reranking tasks for:
(i) CP answer list derived from Web search and
(ii) CP clue retrieval from clue DBs.
Our rankers are based on SVMs and structural
kernels, where the latter are applied to robust shal-
low syntactic structures. Our model applied to
clue reranking is very interesting as it allows us
to learn clue paraphrasing by exploiting relational
syntactic structures representing pairs of clues.
For our study, we created two different corpora
for Snippet Reranking Dataset and Clue Similarity
Dataset on which we tested our methods. The lat-
ter improve on the lists generated by WebCrow by
25 absolute percent points in MRR (about 53% of
relative improvement). When such improved lists
are used in WebCrow, its resolution accuracy in-
creases by 15%, demonstrating that there is a large
room for improvement in automatic CP resolution.
In the future, we would like to add more seman-
tic information to our rerankers and include an an-
swer extraction component in the pipeline.
Acknowledgments
We are deeply in debt with Marco Gori and
Marco Ernandes for making available WebCrow,
for helping us with their system and for the useful
technical discussion regarding research directions.
This research has been partially supported by the
EC?s Seventh Framework Programme (FP7/2007-
2013) under the grants #288024: LIMOSINE
? Linguistically Motivated Semantic aggregation
engiNes. Many thanks to the anonymous review-
ers for their valuable work.
References
Elif Aktolga, James Allan, and David A. Smith. 2011.
Passage reranking for question answering using syn-
tactic structures and answer types. In ECIR.
L Allison and T I Dix. 1986. A bit-string longest-
common-subsequence algorithm. Inf. Process. Lett.,
23(6):305?310, December.
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013.
Dkpro similarity: An open source framework for
text similarity. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (System Demonstrations) (ACL 2013),
pages 121?126, Stroudsburg, PA, USA, August. As-
sociation for Computational Linguistics.
Chris Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Lang. Resour. Eval., 47(1):97?122, March.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ?02, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Marco Ernandes, Giovanni Angelini, and Marco Gori.
2005. Webcrow: A web-based system for crossword
solving. In In Proc. of AAAI 05, pages 1412?1417.
Menlo Park, Calif., AAAI Press.
Marco Ernandes, Giovanni Angelini, and Marco Gori.
2008. A web-based agent challenges human experts
on crosswords. AI Magazine, 29(1).
David Ferrucci and Adam Lally. 2004. Uima: An
architectural approach to unstructured information
processing in the corporate research environment.
Nat. Lang. Eng., 10(3-4):327?348, September.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010a.
Building watson: An overview of the deepqa
project. AI Magazine, 31(3).
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010b. Building watson: An overview of the
deepqa project. AI Magazine, 31(3):59?79.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI?07, pages 1606?1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
47
Matthew L. Ginsberg. 2011. Dr.fill: Crosswords and
an implemented solver for singly weighted csps. J.
Artif. Int. Res., 42(1):851?886, September.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational
Biology. Cambridge University Press, New York,
NY, USA.
R Herbrich, T Graepel, and K Obermayer. 2000. Large
margin rank boundaries for ordinal regression. In
A.J. Smola, P.L. Bartlett, B. Sch?olkopf, and D. Schu-
urmans, editors, Advances in Large Margin Classi-
fiers, pages 115?132, Cambridge, MA. MIT Press.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?02,
pages 133?142, New York, NY, USA. ACM.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
Michael L. Littman, Greg A. Keim, and Noam Shazeer.
2002. A probabilistic approach to solving crossword
puzzles. Artificial Intelligence, 134(12):23 ? 55.
Michael McCandless, Erik Hatcher, and Otis Gospod-
netic. 2010. Lucene in Action, Second Edition:
Covers Apache Lucene 3.0. Manning Publications
Co., Greenwich, CT, USA.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st National Conference on Artificial Intelli-
gence - Volume 1, AAAI?06, pages 775?780. AAAI
Press.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for ques-
tion/answer classification. In ACL.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(34):193 ? 204.
Filip Radlinski and Thorsten Joachims. 2006. Query
chains: Learning to rank from implicit feedback.
CoRR.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th International Joint Confer-
ence on Artificial Intelligence - Volume 1, IJCAI?95,
pages 448?453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th in-
ternational ACM SIGIR conference on Research and
development in information retrieval (SIGIR), pages
741?750. ACM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from clas-
sifiers for passage reranking. In CIKM, pages 969?
978.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 75?83, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Libin Shen and Aravind K. Joshi. 2005. Ranking
and reranking with perceptron. Machine Learning,
60(1-3):73?96.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In EMNLP-CoNLL.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT.
Michael J. Wise. 1996. Yap3: Improved detection
of similarities in computer program and other texts.
In Proceedings of the Twenty-seventh SIGCSE Tech-
nical Symposium on Computer Science Education,
SIGCSE ?96, pages 130?134, New York, NY, USA.
ACM.
48
