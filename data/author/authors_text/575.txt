C
oling
2008:
E
ducationalN
aturalL
anguage
P
rocessing
?
Tutorialnotes
M
anchester,A
ugust2008
Educational Natural Language ProcessingTutorial at COLING?08
Iryna Gurevych, Delphine Bernhard
Educational Natural Language Processing
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  2/206
Iryna Gurevych Delphine Bernhard
Presenters
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  3/206
Technische Universit?t Darmstadt
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  4/206
Quality ineLearning
Ambient learning// University 2020
Serious Games
E-Didactics
Ubiquitous Knowledge Processing
Semantics-based know-ledge acquis.
CRE?E-Learning?
Graduate School Research Unit
Research Unit
Center of Research Excellence eLearning 2.0
1
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  5/206
UKP Lab Research Topics
Text Mining
Natural Language Processing / SemanticsSemantic Information Management
Web 2
.0 \ Se
rvices
eLearn
ing 2.0
User-g
enerat
ed Dis
course
Darmstadt Knowledge Processing Repository WikiMining
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  6/206
Profession 3
Profession 1
Profession 2
Profession ...Query
Profession ...
Profession ...
Profession ...
Semantic Information Retrieval (SIR)
AQUA
Sentiment Analysis in User Generated Discourse (SentAL)
Internet der Dienste (THESEUS) ?
Semantic Question Answer-ing for eLearning 2.0 (QA-EL)
Self-Improving WikisWikis 2.0
Wiki-Mining NLP
Wiki
Darmstadt Knowledge Processing Repository
Data export
      Project specific analysis 
Semantic analysis
Syntactic analysis
Morphological analysis
Linguistic preprocessing
Data import
Research Projects and eLearning
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  7/206
Introduction: eLearning and NLPAutomatic generation of exercisesAssessment of learner generated discourse  Reading and writing assistanceTutoring systemsWeb 2.0 and computer supported collaborative learningExample e-NLP application: electronic career guidance
Outline
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  8/206
Educational Natural Language Processing
eLearning NLP
Computer-assisted learning / instruction Analysis and use of language by machines
e-NLP
2
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  9/206
Field of research exploring the use of NLP techniques in educational contexts
Definition
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  10/206
Web 2.0 & eLearning 2.0
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  11/206
? Creation of large repositories with user generated discourse and user generated metadata
? Using repositories to create structured knowledge bases to improve NLP
? Repositories need advanced information management and NLP to be efficiently accessed
Some Observations
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  12/206
Content creationSemantic knowledge Wikis,Blogs,...
NLP eLearning2.0
Intuitive access
Feedback Loop: NLP & eLearning 2.0
3
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  13/206
Introduction: eLearning and NLPAutomatic generation of exercisesAssessment of learner generated discourse  Reading and writing assistanceTutoring systemsWeb 2.0 and computer supported collaborative learningExample e-NLP application: electronic career guidance
Outline
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  14/206
Computer-based Testing
? Definition: All forms of assessment delivered with the help of computers? Also called: ? Computer Assisted/Aided Assessment (CAA)? Adequate question types for CAA (McKenna & Bull, 1999):? Multiple choice questions (MCQs)? True/False questions? Matching questions? Ranking questions? Sequencing questions? etc.
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  15/206
Question Types
? Objective test items? constrained answer, to be selected among a set of alternatives? short answer (word or phrase) in response to a question ? objective and impartial scoring? Examples:? Fill-in-the-blanks questions? Multiple-choice questions? Matching questions
? Subjective test items? original answer
? variable length
? biased scoring? Examples:? Short-answer essays? Extended-response essays
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  16/206
Role of Test Items in Learning
? Summative assessment? "Assessment of learning"? Measuring student achievement
? Formative assessment? "Assessment for learning"? Active learning: encourage learners to practice and apply newly acquired knowledge by answering test items
4
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  17/206
NLP for CAA
? Generation of questions and exercises?Writing test questions, especially objective test items, is an extremely difficult and time consuming task for teachers? Use of NLP to automatically generate objective test items, esp. for language learning? Assessment and evaluation of answers to subjective test items? Use of NLP to automatically:? Diagnose errors in short-answer essays? Grade essays
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  18/206
Automatic Generation of Test Items? Source data? Corpora: texts should be chosen according to? the learner model (level, mastered vocabulary)? the instructor model (target language, word category)? Lexical semantic resources, e.g. WordNet? Tools? Tokeniser and sentence splitter? Lemmatiser? Conjugation and declension tools? POS tagger? Parser and chunker
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  19/206
Multiple-Choice Questions (MCQ)
? Choose the correct answer among a set of possible answers? Example (Mitkov et al, 2006)Who was voted the best international footballer for 2004?(a) Henry(b) Beckham(c) Ronaldinho(d) Ronaldo
? Usually 3 to 5 alternative answers
StemKey
Distractors /Distracters
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  20/206
Distractors
? Distractors (also distracters) are the incorrect answers presented as a choice in a multiple-choice test? Generation of "good" distractors (McKenna & Bull, 1999; Duvall)? Ensure that there is only one correct response for single response MCQ? The key should not always occur at the same position in the list of answers? Distractors should be grammatically parallel with each other and approximately equal in length? Distractors should be plausible and attractive? However, distractors should not be too close to the correct answer and risk confusing students
5
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  21/206
Automatic Generation of MCQs1. Selection of the key   
? Unknown words that appear in a reading (Heilman & Eskenazi, 2007)
? Domain-specific terms:
? Automatically extracted (Mitkov et al, 2006)
? Present in a thesaurus , e.g. UMLS (Karamanis et al, 2006)2. Generation of the stem   
? Constrained patterns (Heilman & Eskenazi, 2007):Which set of words are most related in meaning to "reject"?? Transformation of source clauses to stems, using transformation and agreement rules (Mitkov et al, 2006):Transitive verbs require objects ? Which kind of verbs require objects?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  22/206
Automatic Generation of MCQs3. Generation of the distractors   ?WordNet concepts which are semantically close to the key , e.g. hypernyms and co-hyponyms (Mitkov et al, 2006; Karamanis et al, 2006)Stem: "Which part of speech serves as the most central element in a clause?"Key: "verb", Distractors: "noun", "adjective", "preposition"? Thesaurus-based and distributional similarity measures (Mitkov et al, 2006)? Other NPs with the same head as the key, retrieved from a corpus (Mitkov et al, 2006)Key: "verb", Distractors: "modal verbs", "phrasal verbs", "active verbs"
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  23/206
Fill-in-the-Blank Questions (FIB)? Also called cloze test? Technique which dates from 1953 (Wilson Taylor)? Consists of a portion of text with certain words removed ? The student is asked to "fill in the blanks"? Objective cloze items = multiple-choice cloze items, i.e. students are given a list of words to use in a cloze? Subjective cloze items = students can choose the words? Challenges:? Phrase the question so that only one correct answer is possible? Spelling errors in objective cloze items
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  24/206
Fill-in-the-Blank Examples
? Blank = preposition (Source: http://www.purl.org/net/WERTI)
? Blank = verb to be conjugated (Source: http://www.nonstopenglish.com/exercise.asp?exid=915)
6
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  25/206
Fill-in-the-Blank Question Generation
1. Selection of an input corpus2. POS tagging 3. Selection of the blanks in the input corpus4. Where needed, provide some information about the word in the blank, e.g. verb lemma when the test targets verb conjugation (Aldabe et al, 2006)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  26/206
Selection of the Blanks
? Every "n-th" (e.g. fifth or eighth) word in the text (Coniam, 1997)
? Words in specified frequency ranges, e.g. only high frequency or low frequency words (Coniam,1997)
? Words belonging to a given grammatical category (Coniam, 1997; Aldabe et al, 2006)
? Open-class words, given their POS, and possibly targeted word sense (Liu et al, 2005; Brown et al, 2005)
? Using machine learning, based on a pool of input questions used as training data (Hoshino & Nakawaga, 2005)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  27/206
Objective Multiple-choice Cloze Items
http://www.wordlearner.com
Combination of a cloze item with multiple-choice answers
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  28/206
Generation of the Distractors
? Randomly chosen in the text from which the question was generated (Hoshino & Nakagawa, 2005)? Same POS (Coniam, 1997)? Similar frequency range (Coniam, 1997)? For grammar questions, use a declension or a conjugation tool to generate different forms of the key, e.g. change case, number, person, mode, tense, etc. (Aldabe et al, 2006, Chen et al, 2006)? Common student errors in the given context (Lee & Seneff, 2007)? Collocations: frequent co-occurrence with either the left or the right context (Lee & Seneff, 2007)? Open class words: semantic similarity based on distributional similarity (Smith et al, 2008) or a thesaurus (Sumita et al, 2005)
7
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  29/206
The Frequency Heuristic(Coniam, 1997)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  30/206
Verification of the Distractors
? Basic verifications:? there must be enough distractors? there must be no duplicated distractors (Aldabe et al, 2006)? Collocations: choose distractors that do not collocate with important words in the target sentence (Liu et al, 2005; Smith et al, 2008)? Use of the web: if the sentence/phrase containing the distractor is frequent on the web, then the distractor should be rejected (Sumita et al, 2005)The child's misery would move even the most  ____ heart.(a) torpid hits("the most torpid heart") = 4(b) invidious hits("the most invidious heart") = 0(c) stolid hits("the most stolid heart") = 6(d) obdurate hits("the most obdurate heart") = 1 240 Good distractorsbecause infrequent
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  31/206
Student Project in the e-NLP Course (Gurevych & Bernhard)? Based on "Automatic generation of cloze items for prepositions" (Lee & Seneff, 2007)? Example:If you don't have anything planned for this evening, let's go __ a movie.(a) to  (b) of   (c) on   (d) null? Tasks:? INPUT: sentence + key, OUTPUT: list of three distractors? The three distractors must each be generated taking a different approach? baseline: word frequencies? collocations? "creative" method:? Conclusion: a motivating and interesting project for students 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  32/206
Matching Test Items
? Task: match items on the left column with response items on the right column? Kinds of elements matched:?Word ? Synonym? Definition ? term?Word ? antonym? Hypernym ? hyponym? Historical event ? date? etc.?Matching test items assess a learner's understanding of relationships
8
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  33/206
Matching Test Items
http://www.thefreedictionary.com
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  34/206
Matching Test Items for Vocabulary Assessment (Brown et al, 2005)
Glosses for specific word senses in WordNet
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  35/206
Error Detection Questions
? Aim: detect and possibly correct errors, which can be marked or not? Example (Chen et al, 2006)Although maple trees are among the most colorful varieties        (A)in the fall, they lose its leaves sooner than oak trees.     (B)      (C) (D)?Wrong statements are produced by the distractor generator
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  36/206
Evaluation of Generated Questions
? Student evaluation ? Difficulty and response time? Comparison with results obtained for manually generated tests (Heilman & Eskenazi, 2007)? Instructor evaluation? Usability: "all distractors result in an inappropriate sentence" (Liu et al, 2005; Lee & Seneff, 2007)? Post-editing: count how many test items are accepted, rejected or revised by instructors during post-editing (Aldabe et al, 2006; Mitkov et al, 2006)
9
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  37/206
Pre-requisites for Student Evaluation
? External assessment? Evaluate the linguistic and / or factual knowledge of the students before they take the test , e.g. Nelson-Denny Reading Test, the Raven's Matrices Test, the Lexical Knowledge Battery (Brown et al, 2005)? Self-assessment? Have the students assess whether they know the key or not (Heilman & Eskenazi, 2007)"Do you know the word 'w'?"
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  38/206
Item Analysis? Investigate the quality of the test items (Zurawski, 1998)? Quantitative item analysis:? Facility / Difficulty index (p): number of test takers who answered the item correctly divided by the total number of students who answered the item? Discrimination index (D): "does the test item differentiate those who did well on the exam overall from those who did not?" ? Divide the students in two groups: high-scoring and low-scoring (above and below the median)? Compute the item difficulty separately for both groups: pupper and plower? Discrimination index D = pupper - plower
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  39/206
Item Analysis? ExampleThe child's misery would move even the most  ____ heart.(a) torpid chosen by 7  students(b) invidious chosen by 1  students(c) stolid chosen by 3  students(d) obdurate chosen by 15  students#Students: 26? Difficulty index: 15 / 26 = 0.58 ? neither too difficult nor too simple (recommended score: 0.5)? Discrimination index? 9 out of 12 students in the high group found the correct answer? 6 out of 14 students in the low group found the correct answer? D = 9/12 ? 6/14 = 0.75 ? 0.43 = 0.32 ? The test item is a quite good discriminator 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  40/206
Item Analysis
? Item distractor analysis: examine the percentage of students who select each incorrect alternative, to determine if the distractors are functioning well
Well-designed item
Possibly miskeyed
Candidate for removal
Candidate for revision
10
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  41/206
Efficiency of the Automatic Generation of Test Items
? Even though automatically generated test items have to be post-edited, this is still a lot faster than writing new test items from scratch.
?Mitkov et al (2006) report the following figures:? an average of 1 minute and 40 seconds was needed to post-edit a test item in order to produce a worthy item? an average of 6 minutes was needed to manually produce a test item
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  42/206
Summary
? The generation of questions and exercises is actually semi-automatic: the system's output has to be verified and modified by an instructor? However, NLP-based systems considerably reduce the time spent by instructors to write test items, even if they have to manually correct the generated test items? A great variety of NLP technologies and resources have been successfully used so far:? POS tagging and parsing?WSD? Term extraction? ...
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  43/206
Introduction: eLearning and NLPAutomatic generation of exercisesAssessment of learner generated discourse  Reading and writing assistanceTutoring systemsWeb 2.0 and computer supported collaborative learningExample e-NLP application: electronic career guidance
Outline
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  44/206
? Types of learner generated discourse:?Emerging in institutional settings, e.g. solutions to exercises?Emerging in informal settings, e.g. discussions in forums? Language forms: written or spoken? Relevant NLP technologies:?Automatic essay grading?Detecting meaning errors?Plagiarism detection?Quality assessment 
Assessment of Learner Generated Discourse  
11
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  45/206
? Feedback to the student about her level of knowledge
? Feedback to the instructor about the progress of students? learning
? Incentive to study certain things, to study them in certain ways, to master certain skills
? Formal data to determine the grade and/or making a pass/fail decision
Importance of Institutional eAssessment
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  46/206
? Advantages over traditional multiple-choice assessments (Bennett & Ward, 1993)
?Major obstacle is the large cost and effort required for scoring
? Automatic systems:? Reduce these costs? Facilitate extended feedback to students
Importance of Free-Text Assessments
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  47/206
? Proposed in the context of language learning, but applicable  to different topics
?We will focus on essay grading
Learning Exercise Spektrum Model(Bailey & Meurers 2008)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  48/206
? A major part of formal education? Secondary students are taught structured essay formats to improve their writing skills? Often used by universities in selecting applicants, e.g. admission essays? Used to judge the mastery and comprehension of material? Students are asked to explain, comment on, or assess a topic of study
What is an Essay?
12
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  49/206
? Descriptive prompt ? ?Imagine that you have a pen pal from another country. Write a descriptive essay explaining how your school looks and sounds, and how your school makes you feel.? ? Persuasive prompt ? ?Some people think the school year should be lengthened at the expense of vacations. What is your opinion? Give specific reasons to support your opinion.?Source: Y. Attali and J. Burstein. Automated essay scoring with e-rater v.2. The Journal of Technology, Learning, and Assessment, 4(3), February 2006.
Essay Prompts
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  50/206
Source: Marti A. Hearst, The Debate on Automated Essay Grading, IEEE Intelligent Systems, IEEE Educational Activities Department, 2000, 15, 22-37.
Research Development in Writing Evaluation
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  51/206
? Intelligent Essay Assessor (Landauer, Foltz & Laham, 1998)? Based on a statistical technique for summarizing the relations between words in a document, i.e. every word is a ?mini-feature?? Intellimetric (Elliot, 2001)? Based on hundreds of undisclosed features? Project Essay Grade (PEG, Page, 1994)? Based on dozens of mostly undisclosed features? E-Rater (Burstein et al, 1998)? The 1st version used more than 60 features? E-rater 2.0 uses a small set of features
Most Prominent Systems
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  52/206
? Humans evaluate various intrinsic variables of interest ? essay score:? Content adequacy? Structure? Argumentation? Diction? Fluency? Correct language use
?Machines use approximations or possible correlates of intrinsic variables ? scoring model
How Do Humans and Machines Rate Essays?
13
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  53/206
How is a Scoring Model Created?
? Analyze a few hundred essays: ?Written on a specific prompt? Pre-scored by as many human raters as possible
? Identify most useful approximations (classification features) out of those available to the system
? Employ a statistical modeling procedure to combine the features and produce a machine-generated score
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  54/206
Validating the Meaning of Scores (Yang et al 2002)? Relationship between human and machine scores of the same prompt:? Compare the machine-human and human-human agreement (Burstein et al, 1998; Elliot, 2001; Landauer et al, 2001)? Estimate a true score as the one assigned by multiple raters (Page, 1966)? Relationship between test scores and other similar measures:? Compare automatic scores with multiple-choice test results and teacher judgments (Powers et al, 2002)? Understanding the scoring process, i.e. relative importance of different writing dimensions:? Most commonly used features in scoring models (Burstein et al, 1998)? The most important component is content (Landauer et al, 2001)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  55/206
Skepticism and Criticism (Page and Petersen, 1995)? Three general objectives:? Humanistic ? never understand or appreciate an essay as a human? Use automatic scoring as a second rater? Defensive ? playful or hostile students produce "bad faith" essays? a study by Powers et al (2001), a lot of data needed? Construct ? computer-measured variables is not what is really important for an essay? an improved ability to additionally provide diagnostic feedback
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  56/206
Features Used by e-Rater 2.0
?Measures of:? Grammar, usage, typos? Style? Organization & development? Lexical complexity? Prompt-specific vocabulary usage
? Implemented in different writing analysis tools
? Based on an NLP foundation that provides instructional feedback to students in the web-based Criterion system
14
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  57/206
Writing Analysis Tools: Correctness
? Identify five main types of grammar, usage and mechanics errors:? Agreement and verb formation errors, wrong word use, missing punctuation, typographical errors
? Corpus-based approach:? Train the system on a large corpus of edited text? Extract and count bigrams of words and POS? Search for bigrams in essay that occur much less often (Chodorow & Leacock, 2000)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  58/206
Writing Analysis Tools: Aspects of Style? The writer may wish to revise:? The use of passive sentences? Very long or very short sentences? Overly repetitious words (Burstein & Wolska, 2003)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  59/206
Writing Analysis Tools: Organization & Development? Discourse elements present or absent in the essay (Burstein, Marcu and Knight, 2003)? A linear representation of text as a sequence of:? Introductory material? A thesis statement? Main ideas? Supporting ideas? A conclusion? Train a system on a large corpus of human annotated essays to identify "good" sequences? Mandatory parts, > 3 main ideas, ?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  60/206
Essay Annotated with Discourse Elements
Source: Y. Attali and J. Burstein. Automated essay scoring with e-rater v.2. The Journal of Technology, Learning, and Assessment, 4(3), February 2006.
15
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  61/206
Writing Analysis Tools: Lexical Complexity? Related to word-specific characteristics
? A measure of vocabulary-level, based on Breland, Jones and Jenkins (1994) Standardized Frequency Index across the words in an essay
? The  average word length in characters in an essay
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  62/206
Writing Analysis Tools: Prompt-Specific Vocabulary Usage? Intuition: good essays resemble each other in their word choice, as will poor essays (within the same prompt)
? Idea: compare an essay to a sample of essays from each score category (usually 1-6)? Each essay and a set of training essays from each score category is converted to a vector? Some function words are removed? Each vector element is a weight based on a word frequency function? Six cosine correlations are computed between the essay and each score category to determine the similarity
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  63/206
Scoring in e-Rater 2.0
? Input: all features of all writing analysis tools? Grammar, usage, mechanics, style (4 features)? Organization & development (2 features)? Lexical complexity (2 features)? Prompt-specific vocabulary usage (2 features)
? Straightforward:? Apply a linear transformation on feature values to achieve a desired scale? A weighted average of the standardized feature values
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  64/206
Future Directions
? Better standardization of scoring - a single scoring model for all prompts of a program or assessment? Better understanding and control over the automated scores? Cover more aspects of writing quality, devise new features? Prefer features providing useful instructional feedback? Detection of anomalous and bad-faith essays? Characterize different types of anomalies? Detect off-topic essays (Higgins, Burstein and Attali, 2006)
16
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  65/206
?Plagiarism is representing the words or ideas of someone else as your own. Examples include, but are not limited to, failing to properly cite direct quotes and failing to give credit for someone else's ideas?. University of Miami Honor Council, Honor Code?Plagiarize: To practice plagiarism upon; to take and use as one's own the thoughts, writings, or inventions of another. (With the thing, rarely the person, as object.)? Oxford English Dictionary Online
Plagiarism
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  66/206
? Clearly define plagiarism to the students and use explicit examples? Educate the students about the honor code and the ramifications if it is violated? Create assignments that make plagiarism difficult? Make sure the students are familiar with online resources? Have the students submit evidence of the research process as well as the paper? Avoid repeat assignments and paper topics? Inform the students you are Internet savvy and you know about the paper mills (visit the sites with the students to evaluate the quality of the work)? Inform the students that you use plagiarism detection software
                   From ?Plagiarism in the 21st century? Carrie Leslie. Lunch & Learn. 2004. Otto G. Richter Library
How to Avoid it?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  67/206
? "Copy" work:? From another student (intra-corpal)? From a source outside the corpus of submissions (extra-corpal)? Self-plagiarism? The Internet makes it easier than ever:? Download a term paper? Fail to give proper credit to the source of an idea? Copy extensive passages without attribution? Inserting someone else?s phrases or sentences (minimally paraphrased) into your own prose and forget to supply a set of quotation marks
Main Ways of Plagiarism
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  68/206
? Replacing odd or unusual words ? Changing formatting ? Adding filler words or phrases ? Changing headings ? Rephrasing sentences ? Removing or re-ordering sections ? Changing spelling (usually from American English to British English, if the document is plagiari[s|z]ed from the Web) ? Producing consistency by find-and-replace (as an example, if some papers refer to the World Wide Web, some to the WWW, some to the Web, a student may perform a global find-and-replace to ensure consistency within the plagiarised document) ? In programming, changing variable names and comments The use of electronic tools to support plagiarism detection: http://www.comp.leeds.ac.uk/hannah/CandIT/plagiarism.html
Types of Techniques Used to Conceal Copying
17
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  69/206
(1) Word-for-word plagiarism: direct copying of phrases or passages from a published text without quotation or acknowledgement.(2) Paraphrasing plagiarism: when words or syntax are changed (rewritten), but the source text can still be recognised.(3) Plagiarism of secondary sources: when original sources are referenced or quoted, but obtained from a secondary source text without looking up the original.(4) Plagiarism of the form of a source: the structure of an argument in a source is copied (verbatim or rewritten)(5) Plagiarism of ideas: the reuse of an original thought from a source text without dependence on the words or form of the source(6) Plagiarism of authorship: the direct case of putting your own name to someone else?s work
Based on Martin (1994) and Clough (2003)
Forms of Plagiarism
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  70/206
? Use of advanced or technical vocabulary beyond that expected of the writer? A large improvement in writing style compared to previous submitted work? Inconsistencies within the written text itself, e.g. changes in vocabulary, style or quality? Incoherent text where the flow is not consistent or smooth, which may signal that a passage has been cut-and-pasted from an existing electronic source? A large degree of similarity between the content of two or more submitted texts. This may include similarity of style as well as content? Shared spelling mistakes or errors between texts? Dangling references, e.g. a reference appears in the text, but not in the bibliography? Use of inconsistent referencing in the bibliography suggesting cut-and-paste
Based on Clough (2003)
Typical Plagiarism Indicators
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  71/206
?Most popular plagiarism detection scheme:? Finding the overlap of matching subsequences and substrings (consecutive tokens) of length ? n (where n is derived empirically)? The longer n becomes, the more unlikely it is that the same sequence of n tokens (words or characters) will appear in the same order in independently written texts? A similarity function is used to capture the degree of overlap between the two texts represented by the sets of n-grams and a chosen threshold above which texts are deemed plagiarised? Problem: larger N-grams are rare, difficult to define thresholds
String Matching Algorithms
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  72/206
? Figures taken from 769 texts in the METER corpus:
Uniqueness of N-grams (from Clough 2003)
18
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  73/206
? Greedy String Tiling (or GST: see, e.g. (Wise,1993)), an algorithm which computes a 1:1 mapping between the tokens in a text pair in such a way that as much of one text as possible is covered with maximal non-overlapping substrings (called tiles) from the other. ? This algorithm computes the longest common substrings (greater than length n) between two texts without having to define an n-gram size a priori. ? Figure 1 represents a tiling of two sentences after running GST (tiles are highlighted) with a minimum match length of 1 word.
Longest Common SubstringsComputed between Two Sentences
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206
? The output of GST algorithm is a set of maximal matches between the text pair: [for two years], [driver who], [into the], [a], [queen], [was] and [banned]. ? Different quantitative measures to detect plagiarism, e.g.:? the minimum and maximum tile length? the average tile length? the dispersion of tile lengths? a similarity score based on tile length (similar to that for n-gram containment). ? The challenge is to capture these tiling patterns such that derived and non-derived texts are distinguishable.
Longest Common SubstringsComputed between Two Sentences
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206
Example of Tiling for Derived and Non-Derived Text (from Clough 2003)
? It has been empirically found that: ? derived texts (top) share longer matching substrings? both the tiling for a derived and non-derived text pair are in most cases apparently different
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  76/206
? Combining evidence from various sources, e.g. ? use a Na?ve Bayes probabilistic classifier to combine evidence from several measures of similarity taken from a GST tiling and make a decision: derived or not-derived
? Supervised learning: training data required (texts which have already been classified as plagiarised or not)
? Unsupervised learning: can also be helpful in grouping together texts which exhibit similar characteristics (e.g. clustering)
Machine Learning in Plagiarism Detection
19
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  77/206
Preserving longer matching n-grams and tile lengths to make the approach resistant to simple edits
? Allow small gaps to represent token deletion 
? Detect simple word substitution (using WordNet) 
? The insertion of certain words such as domain-specific terminology and function words (e.g. conjunctions)
? Simple reordering of tokens (e.g. transposition)
Relaxing the Approach
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  78/206
? Existing work involves minimal natural language processing (NLP)? Areas of NLP that could aid plagiarism detection, particularly in identifying texts which exhibit similarity in semantics, structure or discourse, but differ in lexical overlap and syntax? NLP methods include: ? morphological analysis, part-of-speech tagging, anaphora resolution, parsing (syntactic and semantic), co-reference resolution, word sense disambiguation, and discourse processing? Future work:? several similarity scores based on lexical overlap, syntax, semantics, discourse and other structural features
NLP in Plagiarism Detection
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  79/206
Online Internet Plagiarism Services
? Plagiarism.org www.plagiarism.org? The largest online plagiarism service available? IntegriGuard www.integrigaurd.com
? EVE2 www.canexus.com/eve/abouteve.shtml
? None of the services details their implementation details
? All of them are commercial, but plagiarism.org allows free trial
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  80/206
? Automatic scoring ? Essays (e-Rater, Burstein and Chodorow, 1999)? Longer texts (AutoTutor, Wiemer-Hastings et al, 1999)
? Automatic diagnosis, i.e. content assessment (CAM) on learner data? Language learning (Bailey and Meurers, 2008)? Error detection in C-rater (Leacock, 2004)? 85% accuracy 
Assessing Short Textual Answers
20
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  81/206
?Measures student understanding with little regard to writing skills
? Example question (4th grade math question used in the National Assessment for Educational Progress (NAEP)):
C-Rater (Chodorow 2004)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  82/206
Technology of c-Rater? Content expert develops a scoring guide? Gold standard responses? Recognizing the equivalence of the response to the correct answers? Essentially paraphrase recognition? Analysis in terms of: ? predicate argument structure? resolving the referent of any pronouns in the response ? regularizing over morphological variation? matching on synonyms or similar words? resolving the spelling of unrecognized words? Mapping canonical representations to those of the gold standard responses? Rule-based? 11th grade reading comprehension items? Exact agreement with human scorers 84%
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  83/206
? Analysis of responses to short-answer comprehension tests? 1-3 sentences in length? Error codes:? Necessary concepts left out of learner response? Response with extraneous, incorrect concepts? An incorrect blend/substitution (correct concept missing, incorrect one present)? Multiple incorrect concepts? Human disagreement in 12%, eliminated from the evaluation data
Detecting Meaning Errors (Bailey and Meuerers, 2008) 
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  84/206
? Input:? Learner?s response, one+ target responses, question, source reading passage? String-based analysis filter? Linguistic analysis: annotation, alignment, diagnosis
Technology of CAM 
21
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  85/206
? Alignment maps new concepts from  learner's response to those in target? Token level (abstraction from string to lemma, semantic type (e.g. date, location)? Chunk level? Relation level? Diagnosis analyzes if the learner's response contains content errors? Evaluation? Hand-written rules 81% on the development data, 63% on the test data? Machine learning (TiMBL), 88% accuracy on the test data for binary semantic error detection task? Viable results
Technology of CAM 
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  86/206
? Non-native speech scoring (Bernstein 1999; Zechner and Bejar, 2006, Zechner et al, 2007)? SET-10 (Bernstein 1999) focuses on the lower entropy language aspects? Tasks such as ?reading? or ?repetition?? Highly predictable word sequences? TOEFL Practice Online Speaking test (Zechner et al, 2007)? Focus on spontaneous, high-entropy responses? Test with Heterogeneous Tasks (THT) (Zechner and Xi, 2008)? Ranges from reading speech to opinion giving? Assess communicative competence
Automatically Scoring Speech
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  87/206
? Dimensions of assessement:? Comprehensibility, accuracy, clarity, coherence, appropriateness
? Evident through:? Speaker?s pronunciation, fluency, use of grammar and vocabulary, development of ideas, sensitivity to communicative context
Test with Heterogeneous Tasks
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  88/206
1. Reading aloud2. Picture description (medium-entropy)? Describe a picture in detail? Rated on the combined impact of delivery, use of structures, vocabulary, content relevance and fullness (3-point scale)3. Open-end short-answer questions4. Constrained short-answer questions5. Respond to a voice mail6. Opinion task (high-entropy)? State an opinion on an issue and support its with reasons, examples, arguments, etc.? Rated on the combined impact of fluency, pronunciation, intonation and stress, grammar, vocabulary, content relevance, and cohesion and ides progression (5-point scale)
THT Task Types
22
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  89/206
? Adapt a non-native English speech recognizer (trained on TOEFL Practice Online data) to transcribed THT task responses? Compute a set of relevant speech features based on the recognition output? Build a scoring model using a subset of features to predict human scores
Technology of SpeechRater
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  90/206
? Human agreement (kappa): around 0.50 (Picture) and 0.72 (Opinion)? Opinion task ? multiple regression employing Equal, Expert, or Optimal Weights; picture task ? CART 5.0 (classificaiton trees)
Evaluation
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  91/206
Introduction: eLearning and NLPAutomatic generation of exercisesAssessment of learner generated discourse  Reading and writing assistanceTutoring systemsWeb 2.0 and computer supported collaborative learningExample e-NLP application: electronic career guidance
Outline
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  92/206
Readability
? "Readability is what makes some texts easier to read than others" (DuBay, 2004)? A text's readability can be estimated with readability formulas, which provide an objective prediction of text difficulty? Aims: ? match reading materials with the abilities of the readers? support authors in writing clearly understandable texts 
23
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  93/206
Traditional Readability Measures
Formula Date Features Example values1948
Fog index 1952
SMOG grading 1969 - # words with more than 3 syllables
Flesch index - average # syllables / word- average sentence length - 30 = "very difficult"- 70 = "easy"- # words with more than 2 syllables- average sentence length - 5 = comic books- 10 = newspapers                    - 0 to 6 =  low-literate- 19+ = post-graduate
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  94/206
Readability Statistics
? Computed using the style command
Rotk?ppchen
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  95/206
Statistical Models for Reading Difficulty
? Based on statistical models representing norms, specific populations and individuals (Brown & Eskenazi, 2004)? Different models are created for each level of reading difficulty? Features:? Lexical features: word unigrams (Collins-Thompson & Callan, 2005; Heilman et al, 2008)? Grammatical features: frequency of specific grammatical constructions (Heilman et al, 2007)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  96/206
Document Retrieval for Reading Practice? Reading proficiency is a widespread problem? Only 29% of high school seniors in public schools across the USA were proficient in reading according to a 2005 NCES study (Miltsakaki & Troutt, 2008)? Low reading proficiency may have dramatic consequences (DuBay, 2004):? The strongest risk factor for injury in a traffic accident is the improper use of child safety seats? 79 to 94% of car seats are used improperly? Installation instructions are too difficult to read for 80% adult readers in the US? Use readability measures to identify suitable and authentic documents, given a reader profile / reading grade
24
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  97/206
Vygotsky's Zone of Proximal Development?Materials for assisted reading should be harder than the reader's tested reading level, but within the zone of proximal development
?Materials for unassisted reading , e.g. medicine inserts, instructions, should be as easy as possible
http://www.education.vic.gov.au/
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  98/206
Read-X (Miltsakaki & Troutt, 2008)
? http://net-read.blogspot.com/
Keywords
Texts
ReadingLevel
Yahoo! Internet search
Text extraction
Readability analysis
Text classification
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  99/206
REAP search (Heilman et al, 2008)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  100/206
Text Simplification
? The readability of a text can be improved by transforming it into a simpler text? Characteristics of manually simplified texts (Petersen & Ostendorf, 2007) :? shorter sentences? fewer and shorter phrases? fewer adjectives, adverbs and coordinating conjunctions? nouns are less often replaced with pronounsOriginal text: Congress gave Yosemite the money to repair damage from the 1997 flood.Abridged text: Congress gave the money after the 1997 flood
25
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  101/206
Automatic Text Simplification? Related techniques: summarisation and sentence compression? Syntactic simplification:? Removal or replacement of difficult syntactic structures, using hand-built transformational rules applied to dependency and parse trees (Carroll et al, 1999; Inui et al, 2003)? Lexical simplification:? Goal: replace difficult words with simpler ones (Carroll et al, 1999; Lal & R?ger, 2002)? Difficult words are identified using the number of syllables and/or frequency counts in a corpus? Choose the simplest synonym for difficult words in WordNet
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  102/206
Vocabulary Assistance for Reading
? Overall goal: support vocabulary acquisition during reading for:? children, who learn to read (Aist, 2001)? foreign language learners, who read texts in a foreign language? Problem: a word's context may not provide enough information about its meaning? Aim: augment documents with dynamically generated annotations about (problematic) words
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  103/206
Selection of Target Words
? All words are annotated? Annotate selected words? Manually selected target words? Automatically selected target words? (Aist, 2001):? Words with few senses in WordNet (to avoid WSD)? Not a trivially easy word: three or more letters long, not in a stop list of function words, not a number? Not a proper noun? Socially acceptable , e.g. no secondary slang meanings? (Mihalcea & Csomai, 2007): keyword extraction methods
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  104/206
Resources for Vocabulary Assistance
?WordNet (Aist, 2001):? Extraction of comparison words for a target word: antonym, hypernym, synonym? Generation of factoids:? eggshell can be a kind of natural covering? Problems: ? some of the automatically generated factoids are too obscure or do not match the sense of the word used in the original text? some of the comparison words may be harder to understand than the target word? hypernyms do not always capture the key elements of the meaning of a word
26
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  105/206
Resources for vocabulary assistance
? Collaborative and online resources, e.g. Wikipedia, Wiktionary
http://lingro.com/
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  106/206
Wikipedia and Wiktionary as Lexical-Semantic Resources
+
This image is licensed under the GFDL. It is based on Bild:Foerderturm-Kamen.jpg.? Structure Mining? Content Mining? Usage Mining
= Lexical semantic resources
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  107/206
Wikipedia Article PageFirst paragraph? First paragraph? Definition / Gloss
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  108/206
Wikipedia  ? Redirect Pages
? Synonyms? Pope Benedict XVI? Joseph Ratzinger? Joseph Cardinal Ratzinger? Spelling variations? Benedict the Sixteenth? Benedict the 16th? Benedict 16th? Benedict 16? Benedict XVI? Benedict xvi? Misspellings? Josef Ratzinger (instead of Joseph)? Abbreviations? PB16
27
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  109/206
Wikipedia ? Categories
? Articles? Hierarchy
Engines Energy conversion
Piston engines
Aircraft piston engine
Piston Engine ConfigurationsAutomobile engines
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  110/206
JWPL ? Wikipedia API
? Freely available for research purposes?http://www.ukp.tu-darmstadt.de/software/
CategoryGraph
Page
CategoryWikipedia
ParsedPage SectionParagraph
Link
Table
...MetaData
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  111/206
Wiktionary as Lexical-Semantic Resource? Language? Etymology? Pronunciation? Part-of-speech? Word senses? Synonyms? Derived Terms? Translations? Abbreviations, Antonyms, Categories, Collocations, Examples, Glosses, Hypernyms, Hyponyms, Morphology, Quotations, Related terms, Troponyms 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  112/206
JWKTL ? Wiktionary API
LanguageWiktionaryWord
PoSWiktionary
Sense SynonymsTranslations
Etymology
Pronunciation
...?? Freely available for research purposes?http://www.ukp.tu-darmstadt.de/software/
28
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  113/206
Wikify! (Mihalcea & Csomai, 2007)
? Aim: link keywords (important concepts) in a document to the corresponding Wikipedia page? Keywords extraction? Ranking: tf.idf, ?2 independence test, keyphraseness?Word Sense Disambiguation to identify the target Wikipedia page:? Lesk algorithm: measure of contextual overlap between the Wikipedia page of the ambiguous word / phrase and the context where the ambiguous word / phrase occurs? Machine Learning classifier
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  114/206
Spelling Error Detection and Correction
? Aim: identify and correct spelling errors? Types of spelling errors:? Non-word spelling errorsoccured instead of occurredater instead of after, later, alter, water, ate?Word conflation or splitting? ofthe, understandhme? sp ent, th ebook? Malapropisms: real-word spelling errors in open-class wordsdiary ? dairythere ? their ? they're
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  115/206
Research Problems (Kukich, 1992)
? Non-word error detection? From the early 1970s to the early 1980s? Focus on efficient pattern-matching and string comparison techniques? Isolated-word error correction? Started in the early 1960s? Context-dependent word correction? Started in the early 1980s? Use of statistical language modelsTextbook overviews: (Jurafsky & Martin, 2008; Manning, Raghavan and Sch?tze, 2008)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  116/206
Non-word Error Detection
? n-gram analysis: ? n-gram = n-letter sub-sequences of words or strings? examine each letter n-gram in an input string? find the n-gram in a table of n-gram statistics compiled from a corpus of text? highly infrequent n-grams indicate probable misspellings? especially useful for optical character recognition devices? Dictionary lookup:? check if an input string appears in a dictionary of acceptable words? techniques: hash tables, tries, finite-state automata, Aho-Corasick algorithm, ternary search trees
29
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  117/206
Isolated Word Error Correction1) Detection of errors in single words, out of context2) Generation of candidate corrections
? Distance/Proximity metric between the correct word and the erroneous word
? Minimum edit distance: minimum number of editing operations (i.e., insertions, deletions, and substitutions) needed to transform one string into another
"=" Match; "o" Substitution; "+" Insertion; "-" Deletion3) Ranking of candidate corrections based on the distance/proximity metric or occurrence counts
Distance = 4
(c) www.levenshtein.net
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  118/206
Isolated Word Error Correction
Problem: even humans do not achieve 100% accuracy levels, given isolated misspelled strings (Kukich, 1992):
? vver ? over, ever, very?
? wekk ? week, well, weak?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  119/206
Context-dependent Error Correction
? Also called context-sensitive spelling correction? Aim: correct real-word spelling errors, which cannot be identified by dictionary lookup? Between 25% and 40% of spelling errors are valid English words (Kukich, 1992)? Use the context to help detect and correct spelling errors? Based on language models
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  120/206
Spelling Correction for Foreign Language Learners (Heift & Rimrott, 2007)? 80% of the mispellings produced by non-native writers of German are due to insufficient command of the foreign language:Metz for Fleisch (from Metzger)tanzed for tanzte (from danced)? These errors are difficult to correct for generic spell checkers ? need for rules that are geared towards common L2 errors? Importance of feedback: learners are more likely to correct a mistake if the feedback contains explicit information on the error and correction suggestions
30
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  121/206
Grammar Checking
? Tasks:? Grammatical error detection: identify sentences which are grammatically ill-formed? Grammatical error correction: correct grammatically ill-formed sentences?Methods:? Rule-based checking: use of manually written rules? Syntax-based checking: use the output of a parser? Statistics-based: use statistical information about n-gram frequencies? The methods usually focus on a specific part-of-speech
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  122/206
Grammatical Error Types
? According to (Nicholls, 1999):? Insertion of an unnecessary word: *affect to their emotions? Deletion of a word: *opportunity of job?Word or phrase that needs replacing: *every jobs?Word use in the wrong form: *knowledges? Grammatical difficulties for ESL learners:? Prepositions: *arrive to the town, *most of people, *He is fond this book (Chodorow et al, 2007)? Verb forms: I can't *skiing well, I don't want *have a baby (Lee & Seneff, 2008)? Articles
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  123/206
Rule-based Grammar Checking
? Analyse errors in a corpus and write rules to identify and correct these errors, based on POS information? Rule patterns should not occur in correct sentences? Examples:? Language Tool (Naber, 2003)? Open Source language checker? Rules are defined in XML configuration files and include feedback messages? GRANSKA (Eeg-Olofsson & Knutsson, 2003)? Rules expressed in a specific rule language 
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  124/206
Syntax-based Grammar Checking
? Template-matching on parse trees (Lee & Seneff, 2008)? Automatic introduction of verb form errors in a corpus? Parsing of the corpus? Identification of templates in the "disturbed" parse trees
31
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  125/206
Statistics-based Grammar Checking
? Detection of unfrequent sequences of words and/or POS tags:? POS bigrams (Atwell, 1987)? POS tags and function words n-grams (Chodorow & Leacock, 2000)?Machine learning:? Maximum entropy model trained with contextual features and rule-based filters (Chodorow et al, 2007)? Machine learning model based on automatically labelled sequential patterns (Sun et al, 2007)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  126/206
The Tip of the Tongue Problem
Writers may want to look for words that express a given concept and are appropriate in a given contextProblem: in order to access words in a traditional dictionary, you have to know the word you are looking for
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  127/206
Dictionary Lookup (Ferret & Zock, 2006)
? Tip of the tongue problem: ? domesticated animal, producing milk suitable for making cheese? NOT (cow, buffalo, sheep)?? goat? The mental lexicon is a huge network of interconnected words and concepts? The network is entered through the first word that comes to mind and the target word is retrieved thanks to connecting links
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  128/206
Internal Representation
32
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  129/206
Wikipedia Graph
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  130/206
Introduction: eLearning and NLPAutomatic generation of exercisesAssessment of learner generated discourse  Reading and writing assistanceTutoring systemsWeb 2.0 and computer supported collaborative learningExample e-NLP application: electronic career guidance
Outline
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  131/206
? Developed during last 25 years, typically the domains of e.g. mathematics, science and technology? Goal: the ability to engage learners in rich natural language dialogue? Significant learning gains beyond classroom environments:? Learning gains from computer tutors by approximately .3 to 1.0 grade unit (Corbett et al 1999)? Learning gains from human tutors by .4 to 2.3 grade units, though ? modest domain knowledge? no training in pedagogy? rare use of sophisticated tutoring strategies
Intelligent Tutoring Systems with Conversational Dialogue
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  132/206
? System presents problems and questions to learners
? Learner types in / utters answers in natural language
? Lengthy multi-turn dialogues as complete solutions / answers evolve
Interaction with Intelligent Tutoring Systems
33
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  133/206
? CIRCSIM (Evens and Michael 2006)? BEETLE (Zinn et al 2002) ? Geometry Explanation Tutor (Aleven et al 2003) ? Why2/Atlas (VanLehn et al 2002) ? students explain physical systems? ITSpoke (Litman et al 2006) ? builds upon Why2, spoken language based? SCOT (Pon-Barry et al 2006) ? ProPL (Lane and VanLehn 2005) ? AutoTutor (Graesser et al 2003) ? students answer deep questions about computer technology? a core set of foundational requirements for mixed-initiative natural language interaction in tutorial dialogue
Research on ITS
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  134/206
? Speech acts in tutorial dialogue (Marineau et al 2000)
? Dialogue acts' correlation with learning (Forbes-Riley et al 2005, Core et al 2003, Ros? et al 2003, Katz et al 2003)
? Student uncertainty in dialogue (Liscombe et al 2005, Forbes-Riley and Litman 2005)
? Comparing text-based and spoken dialogue (Litman et al 2006)
Corpus-Based Studies
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  135/206
Cognitive and Affective States in Learning? ITS as platforms to investigate the impact of tutorial interactions on affective and motivational outcomes (e.g. self-efficacy) along with cognitive measures (i.e. learning gains)
? Goal: identifying tutorial strategies that balance the tradeoff between cognitive and affective learning outcomes
?Widespread methodology: investigate human-human tutorial dialogues (e.g. Boyer et al 2008)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  136/206
? By dialogue initiative:? System initiative? Mixed-initiative
? By interaction modality:? Text-based? Speech-based
ITS Interaction Style
34
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  137/206
? Tutoring Research Group at the University of Memphis (e.g. Graesser et al, 1999)
? Intended for college students who take an introductory course in computer literacy ? Fundamentals of computer hardware, operating system and the Internet
? Goals:? To comprehend student contributions ? To simulate dialogue moves of normal (unskilled) or sophisticated tutors
AutoTutor
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  138/206
Screenshot of AutoTutor(Graesser et al, 2001)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  139/206
?Major problem is printed at the top of the screen?Major questions are generated from a curriculum script:? Questions invite lengthy explanations and deep reasoning?Why, how and what-if questions? Deep reasoning rather than short snippets of shallow knowledge? 10 to 30 turns for a single question from a curriculum script? Learner?s contributions are typed in
Interface Description
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  140/206
Example Tutorial Dialogue(AutoTutor: Graesser et al, 2001)
35
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  141/206
? The answer is not graded (good / bad / score)
?Multi-turn conversation to extract more information from the student
? Students learn by constructing explanations and elaborations of the material (e.g. Chi et al, 1994)
Information Delivery versus Knowledge Construction
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  142/206
System Architecture1. Animated agent?Tree-dimensional2. Curriculum script? Important concepts, questions, cases, and problems3. Speech act classifier?Segmenting, parsing student?s response, rule-based utterance classification4. Latent semantic analysis (LSA)?Evaluating the quality of students? contributions5. Dialogue move generator?Can include question answering, repeating the question, encouraging 6. Dialogue Advancer Network?Uses speech act and LSA to select next dialogue move and discourse marker7. Question answering tool
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  143/206
? Dialogue moves:? E.g. open-ended pumps, e.g. What else?? Tutors have a set of expectations about what to include into the answer? Expectation-1? Expectation-2? AutoTutor decides what expectation to handle next and selects a dialogue move? Hints (indirect)? Prompts (in-between)? Assertions (direct)? Exit the cycle when the student articulated the expected answer
How to Engage the Student in Conversation?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  144/206
?Match students utterances to expectations
? Statistical, corpus-based measure of representing knowledge? Latent Semantic Analysis (LSA)
?max function considering the current utterance and all combinations with previous learner?s utterances
? An expectation is considered covered if it exceeds some threshold value
How to Evaluate the Quality of the Answer?
36
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  145/206
? Use LSA in conjunction with various criteria
? Use next expectation with the highest score below threshold (zone of proximal development)
? Use next expectation with the highest LSA overlap with the previous covered expectation (coherence)
? Further constraints to advance the agenda in an optimal way
How to Select the Next Expectation to Cover?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  146/206
? Three channels of feedback:? Backchannel ? acknowledge the learner?s input, based on important nouns, e.g. uh-huh? Pedagogical feedback on the learner?s previous turn, based on LSA scores? Negative, e.g. not really? Neutral negative, e.g. okay? Neutral positive, e.g. okay? Positive, e.g. right? Corrective feedback ? repair bugs and misconceptions? Need to be explicitly anticipated
How to Give Feedback to a Student?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  147/206
? Dialogue advancer network (DAN), mixed-initiative dialogue
? Formally an augmented state transition network? Selection of dialogue move on turn N+1 is sensitive a large set of parameters computed from dialogue history
? Student: What does X mean?Tutor: answer by giving definition from a glossary
? Student: gives an assertionTutor: evaluate the quality and give short evaluative feedback
Dialogue Management
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  148/206
? Pump? Hint? Splice? Prompt? Prompt response? Elaboration? Summary? Five forms of immediate short-feedback
Types of Dialogue Moves
37
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  149/206
? Organizes the content of topics covered in the dialogue
? Each topic is associated with:? A set of expectations? A set of hints and prompts for each expectation? A set of anticipated bugs/misconceptions and their corrections? (optinally) pictures or animations
Curriculum Script
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  150/206
? Create an LSA space? Identify a corpus of documents on the domain knowledge
? Lesson planner? Create a curriculum script with deep reasoning questions and problems
? Compute LSA vectors on the content of curriculum scripts
? Prepare glossary of important terms and their definitions
Authoring Tools
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  151/206
1. Glossary of terms and definitions  (metacognition)2. LSA space for conceptual physics (comprehension)3. Curriculum script with deep reasoning questions and associated answers (production)
? Most labour-intensive
Domain Adaptation
 Levels:
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  152/206
Why2 (http://www.pitt.edu/~vanlehn/why2000.html)? Chi et al found that having students explain physical systems qualitatively positively correlated with learning outcomes
? Explanations can be done on formal and graphical languages, but also in natural languages
?Why2 targets to coach students explain physical systems in natural language
? Idea: ask the student to type in an explanation for a simple physical situation
38
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  153/206
Example dialogue
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  154/206
? Student's utterance is analyzed to detect any misconceptions
? If a misconception is detected, a knowledge construction dialogue is initiated (KCD)
?Misconceptions are anticipated by collecting and analyzing a corpus of explanations from students
Dialogue Management
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  155/206
? A speech-enabled version of Why2-Atlas tutoring system
?Workflow:? The student?s essay is parsed? A set of dialogue topics concerning misconceptions or incomplete explanations is extracted ? ITSpoke than engages student in a dialogue that covers these topics? Therefore, the student revises the essay? End the tutoring problem? Cause another round of dialogue/essay revision
ITSpoke (Intelligent Tutoring SPOKEn dialogue system)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  156/206
? Back-end is Why2-Atlas system (VanLehn et al 2002)
39
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  157/206? Back-end is Why2-Atlas system (VanLehn et al 2002) 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  158/206? Back-end is Why2-Atlas system (VanLehn et al 2002)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  159/206
? Sphinx speech recognizer (Huang et al, 1993)? Trained with example user utterances? Domain adaptation by human-computer typed corpus ? Language model enhancement by human-human spoken language corpus? Festival speech synthesizer (Black and Taylor, 1997)? Sentence-level syntactic and semantic analysis modules (Ros?, 2000)? Discourse and domain level processors (Makatchev et al, 2002)
System Architecture
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  160/206
ITSpoke Annotated Dialogue Excerpt
40
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  161/206
Benefits of Spoken Interaction
? Benefits of human-human tutoring through spoken interacton (Lemke, 1990; Chi et al 1994)
? Spontaneous self-explanantion occurs more frequently in spoken tutoring (Hausmann and Chi, 2002)
? Speech contains prosodic and acoustic information to predict emotional states (Ang et al, 2002; Batliner et al, 2000) ? Connection between learning and emotion (Coles, 1999)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  162/206
Introduction: eLearning and NLPAutomatic generation of exercisesAssessment of learner generated discourse  Reading and writing assistanceTutoring systemsWeb 2.0 and computer supported collaborative learningExample e-NLP application: electronic career guidance
Outline
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  163/206
Characteristics of Web 2.0
? Collective intelligence? Huge amount of data? Fast growing
? Noise? Duplicates? Content of different quality
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  164/206
eLearning 2.0
?Main characteristics:?Worldwide learning community? Educational material produced both by students and teachers? Tools:?Wikis? Blogs? Podcasts?Widgets? ...
41
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  165/206
"CALL 2.0"
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  166/206
Widgets for CALL
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  167/206
Use of Web 2.0 Resources
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  168/206
Community-rule-based Grammar Checking? A new paradigm? http://community.languagetool.org
42
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  169/206
Motivation: Information overload in E-Learning
QA-ELQuestion Answering for E-Learning
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  170/206
QA-ELQuestion Answering for E-Learning
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  171/206
Social Q&A Sites
? Solution to the problem of automatically answering learners' questions: use repositories of already answered questions (Bernhard & Gurevych, 2008)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  172/206
What is actually the Quality of Web 2.0 Resources??Wikipedia:? Open edit policy, yet high quality articles (Giles, 2005)? 42 entries tested by experts? average science entry in Wikipedia contained around four inaccuracies? average science entry in Encyclopaedia Britannica contained around three inaccuracies? Automatic assessment of the quality of these ressources:? Social Q&A sites (Jeon et al, 2006; Agichtein et al, 2008)?Wikipedia (Druck et al, 2008)? Forums (Weimer et al, 2007; Weimer & Gurevych, 2007)
43
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  173/206
?Web 2.0 leads to massive amounts of data? Users need content of good quality? Current approach? Users label the data for quality? Labels are used for filtering? Problems:? Happens rarely? New item problem? Premature negative consent (Lampe and Resnick, 2004)
Quality Assessment of User Generated Discourse
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  174/206
Markus Weimer and Iryna Gurevych. 2007. Predicting the Perceived Quality of Web Forum Posts. RANLP, Borovetz, Bulgaria.
Goal: Develop a system to automatically assess the perceived quality of forum posts
Case Study
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  175/206
? Essay scoring? Established in systems like e-Rater (Attali and Burstein, 2006)? Very specialized approach: It is known what a ?good? essay is? Input on which features to use? Automatically assessing review helpfulness (Kim et al, 2006)? Goal: predict the helpfulness of product reviews on Amazon.com? Also very specialized:? The rating task is clearly defined: helpful / not helpful for buying decision? Dominant feature is metadata-dependent: star rating of the product
Related Work on Quality Assessment
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  176/206
? Adapt to the quality standards of a user community
? Be independent of metadata-based features
? Apply the system to forums from different domains
Requirements
44
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  177/206
Approach in Weimer and Gurevych (2007)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  178/206
? Surface? Length in tokens? Question Frequency? Exclamation Frequency? Capital WORD Frequency? Lexical? Spelling Error Frequency? Swear Word Frequency? Syntactic? Part of speech distribution
? Form Specific? IsHTML? IsMail? Quote Fraction? URL Count? Path Count? Similarity? Cosine between the post unigram and the forum unigram
Classification Features
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  179/206
? Provided by Nabble.com? Preprocessing of the data:? Removal Non-English posts? Removal of posts with a rating of exactly 3 stars? Binarization of the data into good/bad posts? Three data sets:? ALL: All the posts? SOFT: Posts from the software category at Nabble.com? MISC: Posts from the other categories? Data available upon request
Data
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  180/206
Descriptive Statistics
45
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  181/206
? Stratified tenfold cross validation with different feature sets? Evaluation measure: mean average precision? Features were extracted using Apache UIMA? Classifier:? LibSVM? Gauss Kernel? Parameters C = 10,  ? = 0.1? No model selection was performed? Baseline: Majority class classifier
Experiments: Setup
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  182/206
Results
77,5 74,1 69,2 74,1
46,5
64,7
53,5
89,1 85,1 82,6 71,8 62,0 61,8 61,872,0 66,0 66,7 66,0 66,0 71,3 66,0
0,0
22,5
45,0
67,5
90,0
All Forum Specific Syntactic Lexical Similarity Surface Baseline
ALLSOFTMISC
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  183/206
true good true bad sumpred. good 1517 456 1973pred. bad 312 1133 1445sum 1829 1589 3418
true good true bad sumpred. good 490 72 562pred. bad 95 875 970sum 585 947 1532
true good true bad sumpred. good 1231 516 1747pred. bad 13 126 139sum 1244 642 1886
ALL
SOFT
MISC
Error Analysis: Confusion Matrix
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  184/206
? Automatically generated mails? Can be filtered out in preprocessing? Non-textual content? May be used as a feature, e.g. code examples in a software developer?s forum? Very short posts? Might be improved through metadata about the user or thread information? Opinion based ratings? Ratings based on domain knowledge? Probably form the upper bound for our approach
Error Analysis: Typical Errors
46
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  185/206
> Thank You for the fast response, but I?m not> sure if I understand you right. INTERRUPTs can> be interrupted (by other interrupts or signals) and> SIGNALS not.Yup. And I responded faster than my brain couldshift gears and got my INTERRUPT and SIGNAL crossed.> All my questions still remain!Believe J"org addressed everything in full. That thecompiler simply can?t know that other routines haveleft zero reg alone and the compiler expects tofind zero there.As for SREG, no telling what another routine wasdoing with the status bits so it too has to be savedand restored before any of its contents possibly getmodified. CISC CPUs do this for you when stackingthe IRQ, and on RTI.
Human rating: -System rating: +
Ratings Based on Domain Knowledge
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  186/206
> But you would impose US law even in a country> where smoking weed is legalGiven that most of our users and most significant press coverage is American, yes. That is why I drew the line there.Yes, I know it isn?t perfect. But it?s better than anything else I?ve seen. Human rating:  -System rating: +
Opinion Based Ratings
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  187/206
? Quality assessment is machine learnable? The system performs best with forum specific features (~90%)? Even without forum specific features, the system gives satisfactory result (~82%)? Further experiments needed on:? different data sets ? types of user-generated discourse? New classification features:? structure of the forum? lexical semantic features
Conclusions
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  188/206
Introduction: eLearning and NLPAutomatic generation of exercisesAssessment of learner generated discourse  Reading and writing assistanceTutoring systemsWeb 2.0 and computer supported collaborative learningExample e-NLP application: electronic career guidance
Outline
47
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  189/206
 
The SIR project:Semantic Information Retrieval for Electronic Career Guidance
funded by the German Research Foundation
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  190/206
Information Retrieval
Descriptions of professions
Documents
1. ...2. ..3. ? Ranked List of Professions 
Essay about professional interests
Query
Electronic Career Guidance
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  191/206
Profession 3
Profession 1
Profession 2
Profession ...Essay
Profession ...
Profession ...
Profession ...
Semantic Relatedness
I like baking cakes...
...pastries......confectioner... ...food processing industry
Vocabulary Mismatch Problem
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  192/206
? Semantic relatedness (SR) as measure for document relevance 
Lexical-Semantic KnowledgeSemantic Relatedness Measure
Information Retrieval System
Semantic IR Models
48
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  193/206
Lexical Semantic Knowledge
? GermaNet: German lexical-semantic wordnet ? Nouns, verbs, adjectives? 27,824 noun synsets, 8,810 verb synsets, 5,141 adjective synsets? 60,646 words in synsets?Wikipedia ? Free online collaboratively constructed encyclopedia? Articles, links, categories (Zesch, Gurevych &M?hlh?user, 2007)?Wiktionary? Free online collaboratively constructed dictionary?Words, categories, semantic relations? http://www.ukp.tu-darmstadt.de/software/WikipediaAPI
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  194/206
Semantic Relatedness Measures
? Path length (PL)? Pseudo glosses based (Gurevych, 2005)? Information content based? Resnik (1995)? Jiang & Conrath (1997)? Lin (1998)? Explicit semantic analysis (Gabrilovich & Markovitch, 2007)
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  195/206
Experiments in Information Retrieval A ?Andererseits arbeite ich besonders gerne am Computer, kann programmieren in C, Python und VB und k?nnte mir deshalb auch vorstellen in der Software-Entwicklung zu arbeiten.?
? Topics - 30 essays of human subjects about professional interests? Queries:- Nouns, Verbs, Adjectives- Nouns- Keywords (set of 41 keywords)Profession 3
Profession 1
Profession 2
Profession ...Query
Profession ...
Profession ...
Profession ...
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  196/206
? Provided by the German Federal Labour Office? Descriptions of 4,000 professions and 1,800 vocational trainings? Prepared by professionals? Evaluation on 529 descriptions of vocational trainings? Using parts which describe profession itself, but not training or administrative details
Document Collection
49
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  197/206
? 41 keywords in 3 categories? Ranked list of professions for each topic ? Automatically extracted from knowledge base? Used for creating relevance judgments 
"Gold Standard"
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  198/206
41 Keywords educate, use/program computer, office, outside, animals/plants, ...
Essay Profes-sion 1 Profes-sion 2 Profes-sion 3Human Annotation
Scoring
Profes-sion 1Profes-sion 2 Profes-sion 31. 2. 3. irrelevantrelevant
Relevance Judgments
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  199/206
? Standard IR measures using relevance judgements? Precision ? recall diagrams? Mean average precision
? Rank correlation with knowledge-based ranked list? Spearman?s Rank Correlation Coefficient
? Parameters:? Pre-processing configurations? Semantic relatedness measures? Lexical-semantic knowledge sources
Evaluation
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  200/206
Pre-processing Configurations & Measures, Precision-Recall
50
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  201/206
Pre-processing Configurations & MeasuresSpearman?s Rank Correlation
00,1
0,20,3
0,40,5
0,6
N, V, A Nouns Keywords
EBEB+SYNEB+HypoLINESA-WordESA-Text
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  202/206
ESA-Text tf.idf with Different Lexical-Semantic Resources
Nouns,Verbs,Adjectives Nouns Keywords0
0.050.1
0.150.2
0.250.3
0.350.4
0.450.5
0.550.6
0.65 Mean Average Precision
WikipediaGermaNet HyperGermaNet RadialWiktionary
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  203/206
? Opportunity for NLP and e-NLP?? Remove knowledge acquisition bottleneck? New forms of eLearning
? Excellent playground for NLP?? eLearning 2.0 discourse types almost not studied
? Can we actually learn from BioNLP?
Some Thoughts on eLearning 2.0?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  204/206
? Establish an international community? ACL-associated meeting series (e.g. ACL-BEA Workshop 2008)? Related Tutorials? Resources:? Bibliography? Research groups? Projects? Annotated corpora? Tools
How to Promote e-NLP?
51
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  205/206
?A lot more research is done on:?Computer-Assisted Language Learning? Intelligent Tutoring Systems? Information search for eLearning?Educational blogging?Annotations and social tagging?Analyzing collaborative learning processes automatically? Learner?s corpora and resources? eLearning standards, e.g. SCORM
What the tutorial has not covered?
17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  206/206
Thank you! http://www.ukp.tu-darmstadt.de/
52
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728?736,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Combining Lexical Semantic Resources
with Question & Answer Archives
for Translation-Based Answer Finding
Delphine Bernhard and Iryna Gurevych
Ubiquitous Knowledge Processing (UKP) Lab
Computer Science Department
Technische Universita?t Darmstadt, Hochschulstra?e 10
D-64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de/
Abstract
Monolingual translation probabilities have
recently been introduced in retrieval mod-
els to solve the lexical gap problem.
They can be obtained by training statisti-
cal translation models on parallel mono-
lingual corpora, such as question-answer
pairs, where answers act as the ?source?
language and questions as the ?target?
language. In this paper, we propose
to use as a parallel training dataset the
definitions and glosses provided for the
same term by different lexical semantic re-
sources. We compare monolingual trans-
lation models built from lexical semantic
resources with two other kinds of datasets:
manually-tagged question reformulations
and question-answer pairs. We also show
that the monolingual translation probabil-
ities obtained (i) are comparable to tradi-
tional semantic relatedness measures and
(ii) significantly improve the results over
the query likelihood and the vector-space
model for answer finding.
1 Introduction
The lexical gap (or lexical chasm) often observed
between queries and documents or questions and
answers is a pervasive problem both in Informa-
tion Retrieval (IR) and Question Answering (QA).
This problem arises from alternative ways of con-
veying the same information, due to synonymy
or paraphrasing, and is especially severe for re-
trieval over shorter documents, such as sentence
retrieval or question retrieval in Question & An-
swer archives. Several solutions to this problem
have been proposed including query expansion
(Riezler et al, 2007; Fang, 2008), query refor-
mulation or paraphrasing (Hermjakob et al, 2002;
Tomuro, 2003; Zukerman and Raskutti, 2002)
and semantic information retrieval (Mu?ller et al,
2007).
Berger and Lafferty (1999) have formulated a
further solution to the lexical gap problem con-
sisting in integrating monolingual statistical trans-
lation models in the retrieval process. Monolin-
gual translation models encode statistical word as-
sociations which are trained on parallel monolin-
gual corpora. The major drawback of this ap-
proach lies in the limited availability of truly par-
allel monolingual corpora. In practice, training
data for translation-based retrieval often consist in
question-answer pairs, usually extracted from the
evaluation corpus itself (Riezler et al, 2007; Xue
et al, 2008; Lee et al, 2008). While collection-
specific translation models effectively encode sta-
tistical word associations for the target document
collection, it also introduces a bias in the evalua-
tion and makes it difficult to assess the quality of
the translation model per se, independently from a
specific task and document collection.
In this paper, we propose new kinds of
datasets for training domain-independent mono-
lingual translation models. We use the defini-
tions and glosses provided for the same term
by different lexical semantic resources to auto-
matically train the translation models. This ap-
proach has been very recently made possible by
the emergence of new kinds of lexical seman-
tic and encyclopedic resources such as Wikipedia
and Wiktionary. These resources are freely avail-
able, up-to-date and have a broad coverage and
good quality. Thanks to the combination of sev-
eral resources, it is possible to obtain monolin-
gual parallel corpora which are large enough to
train domain-independent translation models. In
addition, we collected question-answer pairs and
manually-tagged question reformulations from a
social Q&A site. We use these datasets to build
further translation models.
Translation-based retrieval models have been
728
widely used in practice by the IR and QA commu-
nity. However, the quality of the semantic infor-
mation encoded in the translation tables has never
been assessed intrinsically. To do so, we com-
pare translation probabilities with concept vector
based semantic relatedness measures with respect
to human relatedness rankings for reference word
pairs. This study provides empirical evidence for
the high quality of the semantic information en-
coded in statistical word translation tables. We
then use the translation models in an answer find-
ing task based on a new question-answer dataset
which is totally independent from the resources
used for training the translation models. This ex-
trinsic evaluation shows that our translation mod-
els significantly improve the results over the query
likelihood and the vector-space model.
The remainder of the paper is organised as fol-
lows. Section 2 discusses related work on seman-
tic relatedness and statistical translation models
for retrieval. Section 3 presents the monolingual
parallel datasets we used for obtaining monolin-
gual translation probabilities. Semantic related-
ness experiments are detailed in Section 4. Section
5 presents answer finding experiments. Finally, we
conclude in Section 6.
2 Related Work
2.1 Statistical Translation Models for
Retrieval
Statistical translation models for retrieval have
first been introduced by Berger and Lafferty
(1999). These models attempt to address syn-
onymy and polysemy problems by encoding sta-
tistical word associations trained on monolingual
parallel corpora. This method offers several ad-
vantages. First, it bases upon a sound mathe-
matical formulation of the retrieval model. Sec-
ond, it is not as computationally expensive as
other semantic retrieval models, since it only re-
lies on a word translation table which can easily
be computed before retrieval. The main draw-
back lies in the availability of suitable training data
for the translation probabilities. Berger and Laf-
ferty (1999) initially built synthetic training data
consisting of queries automatically generated from
documents. Berger et al (2000) proposed to train
translation models on question-answer pairs taken
from Usenet FAQs and call-center dialogues, with
answers corresponding to the ?source? language
and questions to the ?target? language.
Subsequent work in this area often used simi-
lar kinds of training data such as question-answer
pairs from Yahoo! Answers (Lee et al, 2008) or
from the Wondir site (Xue et al, 2008). Lee et
al. (2008) tried to further improve translation mod-
els based on question-answer pairs by selecting the
most important terms to build compact translation
models.
Other kinds of training data have also been pro-
posed. Jeon et al (2005) automatically clustered
semantically similar questions based on their an-
swers. Murdock and Croft (2005) created a first
parallel corpus of synonym pairs extracted from
WordNet, and an additional parallel corpus of En-
glish words translating to the same Arabic term in
a parallel English-Arabic corpus.
Similar work has also been performed in the
area of query expansion using training data con-
sisting of FAQ pages (Riezler et al, 2007) or
queries and clicked snippets from query logs (Rie-
zler et al, 2008).
All in all, translation models have been shown
to significantly improve the retrieval results
over traditional baselines for document retrieval
(Berger and Lafferty, 1999), question retrieval in
Question & Answer archives (Jeon et al, 2005;
Lee et al, 2008; Xue et al, 2008) and for sentence
retrieval (Murdock and Croft, 2005).
Many of the approaches previously described
have used parallel data extracted from the retrieval
corpus itself. The translation models obtained are
therefore domain and collection-specific, which
introduces a bias in the evaluation and makes
it difficult to assess to what extent the transla-
tion model may be re-used for other tasks and
document collections. We henceforth propose a
new approach for building monolingual transla-
tion models relying on domain-independent lexi-
cal semantic resources. Moreover, we extensively
compare the results obtained by these models with
models obtained from a different type of dataset,
namely Question & Answer archives.
2.2 Semantic Relatedness
The rationale behind translation-based retrieval
models is that monolingual translation probabil-
ities encode some form of semantic knowledge.
The semantic similarity and relatedness of words
has traditionally been assessed through corpus-
based and knowledge-based measures. Corpus-
based measures include Hyperspace Analogue to
729
Language (HAL) (Lund and Burgess, 1996) and
Latent Semantic Analysis (LSA) (Landauer et al,
1998). Knowledge-based measures rely on lexical
semantic resources such as WordNet and comprise
path length based measures (Rada et al, 1989)
and concept vector based measures (Qiu and Frei,
1993). These measures have recently also been ap-
plied to new collaboratively constructed resources
such as Wikipedia (Zesch et al, 2007) and Wik-
tionary (Zesch et al, 2008), with good results.
While classical measures of semantic related-
ness have been extensively studied and compared,
based on comparisons with human relatedness
judgements or word-choice problems, there is no
comparable intrinsic study of the relatedness mea-
sures obtained through word translation probabil-
ities. In this study, we use the correlation with
human rankings for reference word pairs to inves-
tigate how word translation probabilities compare
with traditional semantic relatedness measures. To
our knowledge, this is the first time that word-to-
word translation probabilities are used for ranking
word-pairs with respect to their semantic related-
ness.
3 Parallel Datasets
In order to obtain parallel training data for the
translation models, we collected three different
datasets: manually-tagged question reformula-
tions and question-answer pairs from the WikiAn-
swers social Q&A site (Section 3.1), and glosses
from WordNet, Wiktionary, Wikipedia and Simple
Wikipedia (Section 3.2).
3.1 Social Q&A Sites
Social Q&A sites, such as Yahoo! Answers and
AnswerBag, provide portals where users can ask
their own questions as well as answer questions
from other users.
For our experiments we collected a dataset of
questions and answers, as well as question refor-
mulations, from the WikiAnswers1 (WA) web site.
WikiAnswers is a social Q&A site similar to Ya-
hoo! Answers and AnswerBag. The main orig-
inality of WikiAnswers is that users might manu-
ally tag question reformulations in order to prevent
the duplication of answers to questions asking the
same thing in a different way. When a user enters
a question that is not already part of the question
repository, the web site displays a list of already
1http://wiki.answers.com/
existing questions similar to the one just asked by
the user. The user may then freely select the ques-
tion which paraphrases her question, if available.
The question reformulations thus labelled by the
users are stored in order to retrieve the same an-
swer when a given question reformulation is asked
again.
We collected question-answer pairs and ques-
tion reformulations from the WikiAnswers site.
The resulting dataset contains 480,190 questions
with answers.2 We use this dataset in order to train
two different translation models:
Question-Answer Pairs (WAQA) In this set-
ting, question-answer pairs are considered as a
parallel corpus. Two different forms of combi-
nations are possible: (Q,A), where questions act
as source and answers as target, and (A,Q), where
answers act as source and questions as target. Re-
cent work by Xue et al (2008) has shown that the
best results are obtained by pooling the question-
answer pairs {(q, a)1, ..., (q, a)n} and the answer-
question pairs {(a, q)1, ..., (a, q)n} for training,
so that we obtain the following parallel corpus:
{(q, a)1, ..., (q, a)n}?{(a, q)1, ..., (a, q)n}. Over-
all, this corpus contains 1,227,362 parallel pairs
and will be referred to as WAQA (WikiAnswers
Question-Answers) in the rest of the paper.
Question Reformulations (WAQ) In this set-
ting, question and question reformulation pairs
are considered as a parallel corpus, e.g. ?How
long do polar bears live?? and ?What is
the polar bear lifespan??. For a given
user question q1, we retrieve its stored re-
formulations from the WikiAnswers dataset;
q11, q12, .... The original question and reformu-
lations are subsequently combined and pooled to
obtain a parallel corpus of question reformula-
tion pairs: {(q1, q11), (q1, q12), ..., (qn, qnm)} ?
{(q11, q1), (q12, q1), ..., (qnm, qn)}. This corpus
contains 4,379,620 parallel pairs and will be re-
ferred to as WAQ (WikiAnswers Questions) in the
rest of the paper.
3.2 Lexical Semantic Resources
Glosses and definitions for the same lexeme in dif-
ferent lexical semantic and encyclopedic resources
can actually be considered as near-paraphrases,
since they define the same terms and hence have
2A question may have more than one answer.
730
gem moon
WAQ WAQA LSR ALLPool WAQ WAQA LSR ALLPool
gem explorer gem gem moon moon moon moon
95 ford diamonds xlt land earth lunar land
xlt gem gemstone 95 foot lunar sun earth
module xlt diamond explorer armstrong apollo earth landed
stones demand natural gemstone set landed tides armstrong
expedition lists facets diamonds actually neil moons neil
ring dash rare natural neil 1969 phase apollo
gemstone center synthetic diamond landed armstrong crescent set
modual play ruby ford apollo space astronomical foot
crystal lights usage ruby walked surface occurs actually
Table 1: Sample top translations for different training data. ALL corresponds to WAQ+WAQA+LSR.
the same meaning, as shown by the following ex-
ample for the lexeme ?moon?:
? Wordnet (sense 1): the natural satellite of the
Earth.
? English Wiktionary: The Moon, the satellite
of planet Earth.
? English Wikipedia: The Moon (Latin: Luna)
is Earth?s only natural satellite and the fifth
largest natural satellite in the Solar System.
We use glosses and definitions contained in the
following resources to build a parallel corpus:
? WordNet (Fellbaum, 1998). We use a freely
available API for WordNet (JWNL3) to ac-
cess WordNet 3.0.
? English Wiktionary. We use the Wiktionary
dump from January 11, 2009.
? English and Simple English Wikipedia. We
use the Wikipedia dump from February
6, 2007 and the Simple Wikipedia dump
from July 24, 2008. The Simple English
Wikipedia is an English Wikipedia targeted
at non-native speakers of English which uses
simpler words than the English Wikipedia.
Wikipedia and Simple Wikipedia articles do
not directly correspond to glosses such as
those found in dictionaries, we therefore con-
sidered the first paragraph in articles as a sur-
rogate for glosses.
Given a list of 86,584 seed lexemes extracted
from WordNet, we collected the glosses for each
lexeme from the four English resources described
3http://sourceforge.net/projects/
jwordnet/
above. We then built pairs of glosses by consid-
ering each possible pair of resource. Given that a
lexeme might have different senses, and hence dif-
ferent glosses, it is possible to extract several gloss
pairs for one and the same lexeme and one and the
same pair of resources. It is therefore necessary to
perform word sense alignment. As we do not need
perfect training data, but rather large amounts of
training data, we used a very simple method con-
sisting in eliminating gloss pairs which did not at
least have one lemma in common (excluding stop
words and the seed lexeme itself).
The final pooled parallel corpus contains
307,136 pairs and is henceforth much smaller
than the previous datasets extracted from WikiAn-
swers. This corpus will be referred to as LSR.
3.3 Translation Model Training
We used the GIZA++ SMT Toolkit4 (Och and
Ney, 2003) in order to obtain word-to-word
translation probabilities from the parallel datasets
described above. As is common practice in
translation-based retrieval, we utilised the IBM
translation model 1. The only pre-processing steps
performed for all parallel datasets were tokenisa-
tion and stop word removal.5
3.4 Comparison of Word-to-Word
Translations
Table 1 gives some examples of word-to-word
translations obtained for the different parallel cor-
pora used (the column ALLPool will be described
in the next section). As evidenced by this table,
4http://code.google.com/p/giza-pp/
5For stop word removal we used the list avail-
able at: http://truereader.com/manuals/onix/
stopwords1.html.
731
the different kinds of data encode different types
of information, including semantic relatedness and
similarity, as well as morphological relatedness.
As could be expected, the quality of the ?trans-
lations? is variable and heavily dependent on the
training data: the WAQ and WAQA models reveal
the users? interests, while the LSR model encodes
lexicographic and encyclopedic knowledge. For
instance, ?gem? is an acronym for ?generic elec-
tronic module?, which is found in Ford vehicles.
Since many question-answer pairs in WA are re-
lated to cars, this very particular use of ?gem? is
predominant in the WAQ and WAQA translation
tables.
3.5 Combination of the Datasets
In order to investigate the role played by differ-
ent kinds of training data, we combined the sev-
eral translation models, using the two methods de-
scribed by Xue et al (2008). The first method con-
sists in a linear combination of the word-to-word
translation probabilities after training:
PLin(wi|wj) = ?PWAQ(wi|wj)
+ ?PWAQA(wi|wj)
+ ?PLSR(wi|wj) (1)
where ? + ? + ? = 1. This approach will be
labelled with the Lin subscript.
The second method consists in pooling the
training datasets, i.e. concatenating the parallel
corpora, before training. This approach will be
labelled with the Pool subscript. Examples for
word-to-word translations obtained with this type
of combination can be found in the last column for
each word in Table 1. The ALLPool setting corre-
sponds to the pooling of all three parallel datasets:
WAQ+WAQA+LSR.
4 Semantic Relatedness Experiments
The aim of this first experiment is to perform an
intrinsic evaluation of the word translation proba-
bilities obtained by comparing them to traditional
semantic relatedness measures on the task of rank-
ing word pairs. Human judgements of semantic re-
latedness can be used to evaluate how well seman-
tic relatedness measures reflect human rankings by
correlating their ranking results with Spearman?s
rank correlation coefficient. Several evaluation
datasets are available for English, but we restrict
our study to the larger dataset created by Finkel-
stein et al (2002) due to the low coverage of many
pairs in the word-to-word translation tables. This
dataset comprises two subsets, which have been
annotated by different annotators: Fin1?153, con-
taining 153 word pairs, and Fin2?200, containing
200 word pairs.
Word-to-word translation probabilities are com-
pared with a concept vector based measure relying
on Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007), since this approach has been
shown to yield very good results (Zesch et al,
2008). The method consists in representing words
as a concept vector, where concepts correspond to
WordNet synsets, Wikipedia article titles or Wik-
tionary entry names. Concept vectors for each
word are derived from the textual representation
available for each concept, i.e. glosses in Word-
Net, the full article or the first paragraph of the
article in Wikipedia or the full contents of a Wik-
tionary entry. We refer the reader to (Gabrilovich
and Markovitch, 2007; Zesch et al, 2008) for tech-
nical details on how the concept vectors are built
and used to obtain semantic relatedness values.
Table 2 lists Spearman?s rank correlation coeffi-
cients obtained for concept vector based measures
and translation probabilities. In order to ensure
a fair evaluation, we limit the comparison to the
word pairs which are contained in all resources
and translation tables.
Dataset Fin1-153 Fin2-200
Word pairs used 46 42
Concept vectors
WordNet .26 .46
Wikipedia .27 .03
WikipediaFirst .30 .38
Wiktionary .39 .58
Translation probabilities
WAQ .43 .65
WAQA .54 .37
LSR .51 .29
ALLPool .52 .57
Table 2: Spearman?s rank correlation coefficients
on the Fin1-153 and Fin2-200 datasets. Best val-
ues for each dataset are in bold format. For
WikipediaFirst, the concept vectors are based on
the first paragraph of each article.
The first observation is that the coverage over
the two evaluation datasets is rather small: only 46
pairs have been evaluated for the Fin1-153 dataset
and 42 for the Fin2-200 dataset. This is mainly
732
due to the natural absence of many word pairs in
the translation tables. Indeed, translation proba-
bilities can only be obtained from observed paral-
lel pairs in the training data. Concept vector based
measures are more flexible in that respect since the
relatedness value is based on a common represen-
tation in a concept vector space. It is therefore
possible to measure relatedness for a far greater
number of word pairs, as long as they share some
concept vector dimensions. The second observa-
tion is that, on the restricted subset of word pairs
considered, the results obtained by word-to-word
translation probabilities are most of the time better
than those of concept vector measures. However,
the differences are not statistically significant.6
5 Answer Finding Experiments
5.1 Retrieval based on Translation Models
The second experiment aims at providing an ex-
trinsic evaluation of the translation probabilities
by employing them in an answer finding task.
In order to perform retrieval, we use a rank-
ing function similar to the one proposed by Xue
et al (2008), which builds upon previous work
on translation-based retrieval models and tries to
overcome some of their flaws:
P (q|D) =
?
w?q
P (w|D) (2)
P (w|D) = (1? ?)Pmx(w|D) + ?P (w|C) (3)
Pmx(w|D) = (1? ?)Pml(w|D) +
?
?
t?D
P (w|t)Pml(t|D) (4)
where q is the query, D the document, ? the
smoothing parameter for the document collection
C and P (w|t) is the probability of translating a
document term t to the query term w.
The only difference to the original model by
Xue et al (2008) is that we use Jelinek-Mercer
smoothing for equation 3 instead of Dirichlet
Smoothing, as it has been done by Jeon et al
(2005). In all our experiments, ? was set to 0.8
and ? to 0.5.
5.2 The Microsoft Research QA Corpus
We performed an extrinsic evaluation of mono-
lingual word translation probabilities by integrat-
ing them in the retrieval model previously de-
scribed for an answer finding task. To this aim,
6Fisher-Z transformation, two-tailed test with ?=.05.
we used the questions and answers contained in
the Microsoft Research Question Answering Cor-
pus.7 This corpus comprises approximately 1.4K
questions collected from 10-13 year old school-
children, who were asked ?If you could talk to an
encyclopedia, what would you ask it??. The an-
swers to the questions have been manually identi-
fied in the full text of Encarta 98 and annotated
with the following relevance judgements: exact
answer (1), off topic (3), on topic - off target (4),
partial answer (5). In order to use this dataset for
an answer finding task, we consider the annotated
answers as the documents to be retrieved and use
the questions as the set of test queries.
This corpus is particularly well suited to con-
duct experiments targeted at the lexical gap prob-
lem: only 28% of the question-answer pairs corre-
spond to a strong match (two or more query terms
in the same answer sentence), while about a half
(52%) are a weak match (only one query term
matched in the answer sentence) and 16 % are in-
direct answers which do not explicitly contain the
answer but provide enough information for deduc-
ing it. Moreover, the Microsoft QA corpus is not
limited to a specific topic and entirely indepen-
dent from the datasets used to build our translation
models.
The original corpus contained some inconsis-
tencies due to duplicated data and non-labelled
entries. After cleaning, we obtained a corpus of
1,364 questions and 9,780 answers. Table 3 gives
one example of a question with different answers
and relevance judgements.
We report the retrieval performance in terms
of Mean Average Precision (MAP) and Mean R-
Precision (R-prec), MAP being our primary evalu-
ation metric. We consider the following relevance
categories, corresponding to increasing levels of
tolerance for inexact or partial answers:
? MAP1, R-Prec1: exact answer (1)
? MAP1,5, R-Prec1,5: exact answer (1) or par-
tial answer (5)
? MAP1,4,5, R-Prec1,4,5: exact answer (1) or
partial answer (5) or on topic - off target (4)
Similarly to the training data for translation
models, the only pre-processing steps performed
7http://research.microsoft.
com/en-us/downloads/
88c0021c-328a-4148-a158-a42d7331c6cf/
default.aspx
733
Question Why is the sun bright?
Exact answer Star, large celestial body composed of gravitationally contained hot gases
emitting electromagnetic radiation, especially light, as a result of nuclear
reactions inside the star. The sun is a star.
Partial answer Solar Energy, radiant energy produced in the sun as a result of nuclear fu-
sion reactions (see Nuclear Energy; Sun).
On topic - off target The sun has a magnitude of -26.7, inasmuch as it is about 10 billion times
as bright as Sirius in the earth?s sky.
Table 3: Example relevance judgements in the Microsoft QA corpus.
Model MAP1 R-Prec1 MAP1,5 R-Prec1,5 MAP1,4,5 R-Prec1,4,5
QLM 0.2679 0.1941 0.3179 0.2963 0.3215 0.3057
Lucene 0.2705 0.2002 0.3167 0.2956 0.3192 0.3030
WAQ 0.3002 0.2149* 0.3557 0.3269 0.3583 0.3375
WAQA 0.3000 0.2211 0.3640 0.3328 0.3664 0.3405
LSR 0.3046 0.2171* 0.3666 0.3327 0.3723 0.3464
WAQ+WAQAPool 0.3062 0.2259 0.3685 0.3339 0.3716 0.3454
WAQ+LSRPool 0.3117 0.2224 0.3736 0.3399 0.3766 0.3487
WAQA+LSRPool 0.3135 0.2267 0.3818 0.3444 0.3840 0.3515
WAQ+WAQA+LSRPool 0.3152 0.2286 0.3832 0.3495 0.3848 0.3569
WAQ+WAQA+LSRLin 0.3215 0.2343 0.3921 0.3536 0.3967 0.3673
Table 4: Answer retrieval results. The WAQ+WAQA+LSRLin results have been obtained with ?=0.2
?=0.2 and ?=0.6 (the parameter values have been determined empirically based on MAP and R-Prec).
The performance gaps between the translation-based models and the baseline models are statistically
significant, except for those marked with a ?*? (two-tailed paired t-test, p < 0.05).
for this corpus were tokenisation and stop word
removal. Due to the small size of the answer
corpus, we built an open vocabulary background
collection model to deal with out of vocabulary
words by smoothing the unigram probabilities
with Good-Turing discounting, using the SRILM
toolkit8 (Stolcke, 2002).
5.3 Results
As baselines, we consider the query-likelihood
model (QLM), corresponding to equation 4 with
? = 0, and Lucene.9
The results reported in Table 4 show that models
incorporating monolingual translation probabili-
ties perform consistently better than both baseline
systems especially when they are used in combi-
nation. It is however difficult to provide a ranking
of the different types of training data based on the
retrieval results: it seems that LSR is slightly more
performant than WAQ and WAQA, both alone and
8http://www.speech.sri.com/projects/
srilm/
9http://lucene.apache.org
in combination, but the improvement is minor. It
is worth noticing that while the LSR training data
are comparatively smaller than WAQ and WAQA,
they however yield comparable results. The linear
combination of datasets (WAQ+WAQA+LSRLin)
yields statistically significant performance im-
provement when compared to the models without
combinations (except when compared to WAQA
for R-Prec1, p>0.05), which shows that the differ-
ent datasets and resources used are complemen-
tary and each contribute to the overall result.
Three answer retrieval examples are given in
Figure 1. They provide further evidence for
the results obtained. The correct answer to the
first question ?Who invented Halloween?? is
retrieved by the WAQ+WAQA+LSRLin model,
but not by the QLM. This is a case of a weak
match with only ?Halloween? as matching term.
The WAQ+WAQA+LSRLin model is however able
to establish the connection between the ques-
tion term ?invented? and the answer term ?orig-
inated?. Questions 2 and 3 show that transla-
tion probabilities can also replace word normali-
734
QLM top answer WAQ+WAQA+LSRLin top answer
Question 1: Who invented Halloween?
Halloween occurs on October 31 and is observed
in the U.S. and other countries with masquerad-
ing, bonfires, and games.
The observances connected with Halloween are
thought to have originated among the ancient
Druids, who believed that on that evening,
Saman, the lord of the dead, called forth hosts
of evil spirits.
Question 2: Can mosquito bites spread AIDS?
Another species, the Asian tiger mosquito, has
caused health experts concern since it was first
detected in the United States in 1985. Proba-
bly arriving in shipments of used tire casings,
this fierce biter can spread a type of encephalitis,
dengue fever, and other diseases.
Studies have shown no evidence of HIV trans-
mission through insects ? even in areas where
there are many cases of AIDS and large popu-
lations of insects such as mosquitoes.
Question 3: How do the mountains form into a shape?
In 1985, scientists vaporized graphite to produce
a stable form of carbon molecule consisting of
60 carbon atoms in a roughly spherical shape,
looking like a soccer ball.
Geologists believe that most mountains are
formed by movements in the earth?s crust.
Figure 1: Top answer retrieved by QLM and WAQ+WAQA+LSRLin. Lexical overlaps between question
and answer are in bold, morphological relations are in italics.
sation techniques such as stemming and lemmati-
sation, since the answers do not contain the ques-
tion terms ?mosquito? (for question 2) and ?form?
(for question 3), but only their inflected forms
?mosquitoes? and ?formed?.
6 Conclusion and Future Work
We have presented three datasets for training sta-
tistical word translation models for use in answer
finding: question-answer pairs, manually-tagged
question reformulations and glosses for the same
term extracted from several lexical semantic re-
sources. It is the first time that the two latter types
of datasets have been used for this task. We have
also provided the first intrinsic evaluation of word
translation probabilities with respect to human re-
latedness rankings for reference word pairs. This
evaluation has shown that, despite the simplicity
of the method, monolingual translation models are
comparable to concept vector semantic relatedness
measures for this task. Moreover, models based on
translation probabilities yield significant improve-
ment over baseline approaches for answer finding,
especially when different types of training data are
combined. The experiments bear strong evidence
that several datasets encode different and comple-
mentary types of knowledge, which are all use-
ful for retrieval. In order to integrate semantics
in retrieval, it is therefore advisable to combine
both knowledge specific to the task at hand, e.g.
question-answer pairs, and external knowledge, as
contained in lexical semantic resources.
In the future, we would like to further evalu-
ate the models presented in this paper for different
tasks, such as question paraphrase retrieval, and
larger datasets. We also plan to improve ques-
tion analysis by automatically identifying question
topic and question focus.
Acknowledgments We thank Konstantina
Garoufi, Nada Mimouni, Christof Mu?ller and
Torsten Zesch for contributions to this work.
We also thank Mark-Christoph Mu?ller and the
anonymous reviewers for insightful comments.
We are grateful to Bill Dolan for making us
aware of the Microsoft Research QA Corpus.
This work has been supported by the German
Research Foundation (DFG) under the grant No.
GU 798/3-1, and by the Volkswagen Foundation
as part of the Lichtenberg-Professorship Program
under the grant No. I/82806.
References
Adam Berger and John Lafferty. 1999. Information
Retrieval as Statistical Translation. In Proceedings
of the 22nd Annual International Conference on Re-
735
search and Development in Information Retrieval
(SIGIR ?99), pages 222?229.
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the Lexical
Chasm: Statistical Approaches to Answer-Finding.
In Proceedings of the 23rd Annual International
Conference on Research and Development in Infor-
mation Retrieval (SIGIR ?00), pages 192?199.
Hui Fang. 2008. A Re-examination of Query Expan-
sion Using Lexical Resources. In Proceedings of
ACL-08: HLT, pages 139?147, Columbus, Ohio.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing Search in Context: the
Concept Revisited. ACM Transactions on Informa-
tion Systems (TOIS), 20(1):116?131, January.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI), pages 1606?1611.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural Language Based Reformu-
lation Resource and Wide Exploitation for Question
Answering. In Proceedings of the Eleventh Text Re-
trieval Conference (TREC 2002).
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005. Finding Similar Questions in Large Question
and Answer Archives. In Proceedings of the 14th
ACM International Conference on Information and
Knowledge Management (CIKM ?05), pages 84?90.
Thomas K. Landauer, Darrell Laham, and Peter Foltz.
1998. Learning Human-like Knowledge by Singu-
lar Value Decomposition: A Progress Report. Ad-
vances in Neural Information Processing Systems,
10:45?51.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and
Hae-Chang Rim. 2008. Bridging Lexical Gaps be-
tween Queries and Questions on Large Online Q&A
Collections with Compact Translation Models. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
410?418, Honolulu, Hawaii.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments & Computers, 28(2):203?208.
Christof Mu?ller, Iryna Gurevych, and Max Mu?hlha?user.
2007. Integrating Semantic Knowledge into Text
Similarity and Information Retrieval. In Proceed-
ings of the First IEEE International Conference on
Semantic Computing (ICSC), pages 257?264.
Vanessa Murdock and W. Bruce Croft. 2005. A Trans-
lation Model for Sentence Retrieval. In Proceedings
of the Conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP?05), pages 684?691.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1):19?51.
Yonggang Qiu and Hans-Peter Frei. 1993. Concept
Based Query Expansion. In Proceedings of the 16th
Annual International Conference on Research and
Development in Information Retrieval (SIGIR ?93),
pages 160?169.
Roy Rada, Hafedh Mili, Ellen Bicknell, and Maria
Blettner. 1989. Development and Application of
a Metric on Semantic Nets. IEEE Transactions on
Systems, Man and Cybernetics, 19(1):17?30.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical Machine Translation for Query Ex-
pansion in Answer Retrieval. In Proceedings
of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL? 07), pages
464?471.
Stefan Riezler, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Im-
proved Query Expansion. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING 2008), pages 737?744.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904.
Noriko Tomuro. 2003. Interrogative Reformulation
Patterns and Acquisition of Question Paraphrases.
In Proceedings of the International Workshop on
Paraphrasing, pages 33?40.
Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft.
2008. Retrieval Models for Question and Answer
Archives. In Proceedings of the 31st Annual Inter-
national Conference on Research and Development
in Information Retrieval (SIGIR ?08), pages 475?
482.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007. Analyzing and Accessing Wikipedia as a Lex-
ical Semantic Resource. In Data Structures for Lin-
guistic Resources and Applications, pages 197?205.
Gunter Narr, Tu?bingen.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic
Relatedness. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence (AAAI
2008), pages 861?867.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lex-
ical Query Paraphrasing for Document Retrieval.
In Proceedings of the 19th International Confer-
ence on Computational linguistics, pages 1177?
1183, Taipei, Taiwan.
736
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 44?52,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Answering Learners? Questions by Retrieving Question Paraphrases
from Social Q&A Sites
Delphine Bernhard and Iryna Gurevych
Ubiquitous Knowledge Processing Lab
Computer Science Department
Technische Universita?t Darmstadt, Hochschulstra?e 10
D-64289 Darmstadt, Germany
{delphine|gurevych}@tk.informatik.tu-darmstadt.de
Abstract
Information overload is a well-known prob-
lem which can be particularly detrimental to
learners. In this paper, we propose a method
to support learners in the information seek-
ing process which consists in answering their
questions by retrieving question paraphrases
and their corresponding answers from social
Q&A sites. Given the novelty of this kind of
data, it is crucial to get a better understand-
ing of how questions in social Q&A sites can
be automatically analysed and retrieved. We
discuss and evaluate several pre-processing
strategies and question similarity metrics, us-
ing a new question paraphrase corpus col-
lected from the WikiAnswers Q&A site. The
results show that viable performance levels of
more than 80% accuracy can be obtained for
the task of question paraphrase retrieval.
1 Introduction
Question asking is an important component of effi-
cient learning. However, instructors are often over-
whelmed with students? questions and are therefore
unable to provide timely answers (Feng et al, 2006).
Information seeking is also rendered difficult by the
sheer amount of learning material available, espe-
cially online. The use of advanced information re-
trieval and natural language processing techniques
to answer learners? questions and reduce the diffi-
culty of information seeking is henceforth particu-
larly promising. Question Answering (QA) systems
seem well suited for this task since they aim at gen-
erating precise answers to natural language ques-
tions instead of merely returning documents con-
taining answers. However, QA systems have to be
adapted to meet learners? needs. Indeed, learners
do not merely ask concrete or factoid questions, but
rather open-ended, explanatory or methodological
questions which cannot be answered by a single sen-
tence (Baram-Tsabari et al, 2006). Despite a recent
trend to render the tasks more complex at large scale
QA evaluation campaigns such as TREC or CLEF,
current QA systems are still ill-suited to meet these
requirements.
A first alternative to full-fledged QA consists in
making use of already available question and answer
pairs extracted from archived discussions. For in-
stance, Feng et al (2006) describe an intelligent dis-
cussion bot for answering student questions in fo-
rums which relies on answers retrieved from an an-
notated corpus of discussions. This renders the task
of QA easier since answers do not have to be gener-
ated from heterogeneous documents by the system.
The scope of such a discussion bot is however inher-
ently limited since it relies on manually annotated
data, taken from forums within a specific domain.
We propose a different solution which consists in
tapping into the wisdom of crowds to answer learn-
ers? questions. This approach provides the com-
pelling advantage that it utilises the wealth of al-
ready answered questions available in online social
Q&A sites. The task of Question Answering can
then be boiled down to the problem of finding ques-
tion paraphrases in a database of answered ques-
tions. Question paraphrases are questions which
have identical meanings and expect the same answer
while presenting alternate wordings. Several meth-
ods have already been proposed to identify question
44
paraphrases mostly in FAQs (Tomuro and Lytinen,
2004) or search engine logs (Zhao et al, 2007).
In this paper, we focus on the problem of question
paraphrase identification in social Q&A sites within
a realistic information seeking scenario: given a user
question, we want to retrieve the best matching ques-
tion paraphrase from a database of previously an-
swered questions in order to display the correspond-
ing answer. The use of social Q&A sites for ed-
ucational applications brings about new challenges
linked to the variable quality of social media content.
As opposed to questions in FAQs, which are subject
to editorial control, questions in social Q&A sites
are often ill-formed or contain spelling errors. It is
therefore crucial to get a better understanding of how
they can be automatically analysed and retrieved. In
this work, we focus on several pre-processing strate-
gies and question similarity measures applied to the
task of identifying question paraphrases in a social
Q&A site. We chose WikiAnswers which has been
ranked by comScore as the first fastest growing do-
main of the top 1,500 in the U.S. in 2007.
The remainder of the paper is organised as fol-
lows. Section 2 first discusses related work on
paraphrase identification and question paraphrasing.
Section 3 then presents question and answer repos-
itories with special emphasis on social Q&A sites.
Our methods to identify question paraphrases are de-
tailed in section 4. Finally, we present and analyse
the experimental results obtained in section 5 and
conclude in section 6.
2 Related Work
The identification of question paraphrases in ques-
tion and answer repositories is related to research
focusing on sentence paraphrase identification (sec-
tion 2.1) and query paraphrasing (section 2.2). The
specific features of question paraphrasing have also
already been investigated (section 2.3).
2.1 Sentence Paraphrase Identification
Paraphrases are alternative ways to convey the same
information (Barzilay and McKeown, 2001). Para-
phrases can be found at different levels of lin-
guistic structure: words, phrases and whole sen-
tences. While word and phrasal paraphrases can
be assimilated to the well-studied notion of syn-
onymy, sentence level paraphrasing is more difficult
to grasp and cannot be equated with word-for-word
or phrase-by-phrase substitution since it might en-
tail changes in the structure of the sentence (Barzi-
lay and Lee, 2003). In practice, sentence para-
phrases are identified using various string and se-
mantic similarity measures which aim at captur-
ing the semantic equivalence of the sentences being
compared. String similarity metrics, when applied
to sentences, consist in comparing the words con-
tained in the sentences. There exist many different
string similarity measures: word overlap (Tomuro
and Lytinen, 2004), longest common subsequence
(Islam and Inkpen, 2007), Levenshtein edit distance
(Dolan et al, 2004), word n-gram overlap (Barzilay
and Lee, 2003) etc. Semantic similarity measures
are obtained by first computing the semantic simi-
larity of the words contained in the sentences being
compared. Mihalcea et al (2006) use both corpus-
based and knowledge-based measures of the seman-
tic similarity between words. Both string similarity
and semantic similarity might be combined: for in-
stance, Islam and Inkpen (2007) combine semantic
similarity with longest common subsequence string
similarity, while Li et al (2006) make additional use
of word order similarity.
2.2 Query Paraphrasing
In Information Retrieval, research on paraphrasing
is dedicated to query paraphrasing which consists in
identifying semantically similar queries. The over-
all objective is to discover frequently asked ques-
tions and popular topics (Wen et al, 2002) or sug-
gest related queries to users (Sahami and Heilman,
2006). Traditional string similarity metrics are usu-
ally deemed inefficient for such short text snip-
pets and alternative similarity metrics have therefore
been proposed. For instance, Wen et al (2002) rely
on user click logs, based on the idea that queries and
questions which result in identical document clicks
are bound to be similar.
2.3 Question Paraphrasing
Following previous research in this domain, we de-
fine question paraphrases as questions which have
all the following properties: (a) they have identi-
cal meanings, (b) they have the same answers, and
(c) they present alternate wordings. Question para-
45
phrases differ from sentence paraphrases by the ad-
ditional condition (b). This definition encompasses
the following questions, taken from the WikiAn-
swers web site: How many ounces are there in a
pound?, What?s the number of ounces per pound?,
How many oz. in a lb.?
Question paraphrases share some properties both
with declarative sentence paraphrases and query
paraphrases. On the one hand, questions are com-
plete sentences which differ from declarative sen-
tences by their specific word order and the presence
of question words and a question focus. On the other
hand, questions are usually associated with answers,
which makes them similar to queries associated with
documents. Accordingly, research on the identifi-
cation of question paraphrases in Q&A repositories
builds upon both sentence and query paraphrasing.
Zhao et al (2007) propose to utilise user click
logs from the Encarta web site to identify question
paraphrases. Jeon et al (2005) employ a related
method, in that they identify similar answers in the
Naver Question and Answer database to retrieve se-
mantically similar questions, while Jijkoun and de
Rijke (2005) include the answer in the retrieval pro-
cess to return a ranked list of QA pairs in response
to a user?s question. Lytinen and Tomuro (2002)
suggest yet another feature to identify question para-
phrases, namely question type similarity, which con-
sists in determining a question?s category in order to
match questions only if they belong to the same cat-
egory.
Our focus is on question paraphrase identification
in social Q&A sites. Previous research was mostly
based on question paraphrase identification in FAQs
(Lytinen and Tomuro, 2002; Tomuro and Lytinen,
2004; Jijkoun and de Rijke, 2005). In FAQs, ques-
tions and answers are edited by expert information
suppliers, which guarantees stricter conformance to
conventional writing rules. In social Q&A sites,
questions and answers are written by users and may
hence be error-prone. Question paraphrase identi-
fication in social Q&A sites has been little investi-
gated. To our knowledge, only Jeon et al (2005)
have used data from a Q&A site, namely the Korean
Naver portal, to find semantically similar questions.
Our work is related to the latter since it employs a
similar dataset, yet in English and from a different
social Q&A site.
3 Question and Answer Repositories
3.1 Properties of Q&A Repositories
Question and answer repositories have existed for a
long time on the Internet. Their form has evolved
from Frequently Asked Questions (FAQs) to Ask-
an-expert services (Baram-Tsabari et al, 2006) and,
even more recently, social Q&A sites. The latest,
which include web sites such as Yahoo! Answers
and AnswerBag, provide portals where users can
ask their own questions as well as answer ques-
tions from other users. Social Q&A sites are in-
creasingly popular. For instance, in December 2006
Yahoo! Answers was the second-most visited edu-
cation/reference site on the Internet after Wikipedia
according to the Hitwise company (Prescott, 2006).
Even more strikingly, the Q&A portal Naver is the
leader of Internet search in South Korea, well ahead
of Google (Sang-Hun, 2007).
Several factors might explain the success of social
Q&A sites:
? they provide answers to questions which are
difficult to answer with a traditionalWeb search
or using static reference sites like Wikipedia,
for instance opinions or advice about a specific
family situation or a relationship problem;
? questions can be asked anonymously;
? users do not have to browse a list of documents
but rather obtain a complete answer;
? the answers are almost instantaneous and nu-
merous, due to the large number of users.
Social Q&A sites record the questions and their
answers online, and thus constitute a formidable
repository of collective intelligence, including an-
swers to complex questions. Moreover, they make
it possible for learners to reach other people world-
wide. The relevance of social Q&A sites for learning
has been little investigated. To our knowledge, there
has been only one study which has shown that Ko-
rean users of the Naver Question and Answer plat-
form consider that social Q&A sites can satisfacto-
rily and reliably support learning (Lee, 2006).
3.2 WikiAnswers
For our experiments we collected a dataset of ques-
tions and their paraphrases from the WikiAnswers
46
web site. WikiAnswers1 is a social Q&A site similar
to Yahoo! Answers and AnswerBag. As of Febru-
ary 2008, it contained 1,807,600 questions, sorted in
2,404 categories (Answers Corporation, 2008).
Compared with its competitors, the main origi-
nality of WikiAnswers is that it relies on the wiki
technology used in Wikipedia, which means that an-
swers can be edited and improved over time by all
contributors. Moreover, the Answers Corporation,
which owns the WikiAnswers site, explicitly tar-
gets educational uses and even provides an educator
toolkit.2 Another interesting property of WikiAn-
swers is that users might manually tag question re-
formulations in order to prevent the duplication of
questions asking the same thing in a different way.
When a user enters a question which is not already
part of the question repository, the web site dis-
plays a list of questions already existing on the site
and similar to the one just asked by the user. The
user may then freely select the question which para-
phrases her question, if available, or choose to view
one of the proposed alternatives without labelling it
as a paraphrase. The user-labelled question refor-
mulations are stored in order to retrieve the same
answer when the question rephrasing is asked again.
The wiki principle holds for the stored reformula-
tions too, since they can subsequently be edited by
other users if they consider that they correspond to
another existing question or actually ask an entirely
new question. It should be noted that contributors
get not reward in terms of trust points for providing
or editing alternate wordings for questions.
We use the wealth of question paraphrases avail-
able on the WikiAnswers website as the so called
user generated gold standard in our question para-
phrasing experiments. User generated gold stan-
dards have been increasingly used in recent years
for research evaluation purposes, since they can be
easily created from user annotated content. For
instance, Mihalcea and Csomai (2007) use manu-
ally annotated keywords (links to other articles) in
Wikipedia articles to evaluate their automatic key-
word extraction and word sense disambiguation al-
gorithms. Similarly, quality assessments provided
by users in social media have been used as gold
1http://wiki.answers.com/
2http://educator.answers.com/
standards for the automatic assessment of post qual-
ity in forum discussions (Weimer et al, 2007). It
should however be kept in mind that user generated
gold standards are not perfect, as already noticed by
(Mihalcea and Csomai, 2007), and thus constitute a
trade-off solution.
For the experiments described hereafter, we ran-
domly extracted a collection of 1,000 questions
along with their paraphrases (totalling 7,434 ques-
tion paraphrases) from 100 randomly selected FAQ
files in the Education category of the WikiAnswers
web site. In what follows, the corpus of 1,000 ques-
tions is called the target questions collection, while
the 7,434 question paraphrases constitute the input
questions collection. The objective of the task is to
retrieve the corresponding target question for each
input question. The target question selected is the
one which maximises the question similarity value
(see section 4.2).
4 Method
In order to rate the similarity of input and target
questions, we have first pre-processed both the in-
put and target questions and then experimented with
several question similarity measures.
4.1 Pre-processing
We employ the following steps in pre-processing the
questions:
Stop words elimination however, we keep ques-
tion words such as how, why, what, etc. since these
make it possible to implicitly identify the question
type (Lytinen and Tomuro, 2002; Jijkoun and de Ri-
jke, 2005)
Stemming using the Porter Stemmer3
Lemmatisation using the TreeTagger4
Spelling correction using a statistical system
based on language modelling (Norvig, 2007).5
3http://snowball.tartarus.org/
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
5We used a Java implementation of the system, jSpell-
Correct available at http://developer.gauner.org/
jspellcorrect/, trained with the default English training
data, to which we appended the myspell English dictionaries.
47
Stop words were eliminated in all the experi-
mental settings, while stemming and lemmatisation
were optionally performed to evaluate the effects
of these pre-processing steps on the identification
of question paraphrases. We added spelling correc-
tion to the conventional pre-processing steps, since
we target paraphrasing of questions which often
contain spelling errors, such as When was indoor
pluming invented? or What is the largest countery
in the western Hemipher? Other related endeav-
ours at retrieving question paraphrases have identi-
fied spelling mistakes in questions as a significant
source of errors in the retrieval process, but have not
attempted to solve this problem (Jijkoun and de Ri-
jke, 2005; Zhao et al, 2007).
4.2 Question Similarity Measures
We have experimented with several kinds of ques-
tion similarity measures, belonging to two different
families of measures: string similarity measures and
vector space measures.
4.3 String Similarity Measures
Basic string similarity measures compare the words
contained in the questions without taking word fre-
quency into account.
Matching coefficient The matching coefficient of
two questions q1 and q2 represented by the set of
distinct words Q1 and Q2 they contain is computed
as follows (Manning and Schu?tze, 1999):
matching coefficient =| Q1 ? Q2 |
Overlap coefficient The overlap coefficient is
computed according to the following formula (Man-
ning and Schu?tze, 1999):
overlap coefficient = | Q1 ? Q2 |min(| Q1 |, | Q2 |)
Normalised Edit Distance The edit distance of
two questions is the number of words that need to be
substituted, inserted, or deleted, to transform q1 into
q2. In order to be able to compare the edit distance
with the other metrics, we have used the follow-
ing formula (Wen et al, 2002) which normalises the
minimum edit distance by the length of the longest
question and transforms it into a similarity metric:
normalised edit distance = 1? edit dist(q1, q2)max(| q1 |, | q2 |)
Word Ngram Overlap This metric compares the
word n-grams in both questions:
ngram overlap = 1N
N
?
n=1
| Gn(q1) ? Gn(q2) |
min(| Gn(q1) |, | Gn(q2) |)
where Gn(q) is the set of n-grams of length n in
question q and N usually equals 4 (Barzilay and
Lee, 2003; Cordeiro et al, 2007).
4.4 Vector Space Based Measures
Vector space measures represent questions as real-
valued vectors by taking word frequency into ac-
count.
Term Vector Similarity Questions are repre-
sented as term vectors V1 and V2. The feature val-
ues of the vectors are the tf.idf scores of the corre-
sponding terms:
tf.idf = (1 + log(tf)) ? log N + 1df
where tf is equal to the frequency of the term in
the question, N is the number of target questions
and df is the number of target questions in which
the term occurs, computed by considering the in-
put question as part of the target questions collection
(Lytinen and Tomuro, 2002).
The similarity of an input question vector and a
target question vector is determined by the cosine
coefficient:
cosine coefficient = V1 ? V2| V1 | ? | V2 |
Lucene?s Extended Boolean Model The prob-
lem of question paraphrase identification can be
cast as an Information Retrieval problem, since in
real-world applications the user posts a question
and the system returns the best matching questions
from its database. We have therefore tested the re-
sults obtained using an Information Retrieval sys-
tem, namely Lucene6, which combines the Vector
Space Model and the Boolean model. Lucene has
already been successfully used by Jijkoun and de Ri-
jke (2005) to retrieve answers from FAQ web pages
by combining several fields: question text, answer
text and the whole FAQ page. The target questions
are indexed as documents and retrieved by trans-
forming the input questions into queries.
6http://lucene.apache.org/java/docs/
48
T
-SW
T
-SW
+SC
S
-SW
S
-SW
+SC
L
-SW
L
-SW
Preprocessing
50
60
70
80
90
100
Ac
cu
ra
cy
T
-SW
T
-SW
+SC
S
-SW
S
-SW
+SC
L
-SW
L
-SW
Preprocessing
0.5
0.6
0.7
0.8
0.9
1.0
M
RR
Matching coefficient
Overlap coefficient
Normalised edit distance
Ngram Overlap
Term vector similarity
Lucene
Figure 1: Accuracy (%) and Mean Reciprocal Rank obtained for different question similarity measures and pre-
processing strategies: tokens (T), stemming (S), lemmatisation (L), stop words removal (-SW), spelling correction
(+SC).
5 Evaluation and Experimental Results
5.1 Evaluation Measures
We use the following evaluation measures for evalu-
ating the results:
Mean Reciprocal Rank For a question, the recip-
rocal rank RR is 1r where r is the rank of the correct
target question, or zero if the target question was not
found. The Mean Reciprocal Rank (MRR) is the
mean of the reciprocal ranks over all the input ques-
tions.
Accuracy We define accuracy as Success@1,
which is the percentage of input questions for which
the correct target question has been retrieved at rank
1.
5.2 Experimental Results
Figure 1 displays the accuracy and the mean recip-
rocal ranks obtained with the different question sim-
ilarity measures and pre-processing strategies. As
could be expected, vector space based similarity
measures are consistently more accurate than sim-
ple string similarity measures. Moreover, both the
accuracy and the MRR are rather high for vector
space metrics (accuracy around 80-85% and MRR
around 0.85-0.9), which shows that good results can
be obtained with these retrieval mechanisms. Addi-
tional pre-processing, i.e. stemming, lemmatisation
and spelling correction, does not ameliorate the to-
kens minus stop words (T -SW) baseline.
5.3 Detailed Error Analysis
Stemming and lemmatisation Morphological
pre-processing brings about mitigated improve-
ments over the tokens-only baseline. On the one
hand, it improves paraphrase retrieval for ques-
tions containing morphological variants of the same
words such asWhat are analogies for mitochondria?
and What is an analogy for mitochondrion? On the
other hand, it also leads to false positives, such has
How was calculus started?, stemmed as How was
calculus start? and lemmatised as How be calculus
start?, which is mapped by Lucene to the question
How could you start your MA English studies?
instead of Who developed calculus?. The negative
effect of stemming has already been identified by
(Jijkoun and de Rijke, 2005) and our results are
consistent with this previous finding.
Spelling correction We expected that spelling
correction would have a positive impact on the re-
sults. There are indeed cases when spelling correc-
tion helps. For instance, given the question How do
you become an anestesiologist?, it is impossible to
retrieve the target question How many years of med-
ical school do you need to be an anesthesiolgist?
without spelling correction since anesthesiologist is
ill-spelled both in the paraphrase and the target ques-
tion.
49
Lemma + Stop words + Spelling correction
Lemma + Stop words
Stem + Stop words + Spelling correction
Stem + Stop words
Token + Stop words + Spelling correction
Token + Stop words
(a)
Lucene
Term Vector similarity
Word Ngram overlap
Overlap coefficient
Matching coefficient
Edit distance
(b)
Figure 2: Comparison of the different pre-processing strategies 2(a) and methods 2(b) for 50 input questions. For the
pre-processing comparison, the Lucene retrieval method has been used, while the methods have been compared using
baseline pre-processing (tokens minus stop words). A filled square indicates that the target question has been retrieved
at rank 1, while a blank square indicates that the target question has not been retrieved at rank 1.
There are however cases when spelling correction
induces worse results, since it is accurate in only ap-
proximately 70% of the cases (Norvig, 2007). A
major source of errors lies in named entities and ab-
breviations, which are recognised as spelling errors
when they are not part of the training lexicon. For
instance, the question What are the GRE score re-
quired to get into top100 US universities? (where
GRE stands for Graduate Record Examination) is
badly corrected as What are the are score required
to get into top100 US universities?.
Spelling correction also induces an unexpected
side effect, when the spelling error does not affect
the question?s focus. For instance, consider the fol-
lowing question, with a spelling error: What events
occured in 1919?, which gets correctly mapped to
the target question What important events happened
in 1919? by Lucene; however, after spelling correc-
tion (What events occurred in 1919?), it has a big-
ger overlap with an entirely different question: What
events occurred in colonial South Carolina 1674-
1775?.
The latter example also points at another limita-
tion of the evaluated methods, which do not identify
semantically similar words, such as occurred and
happened.
Errors in the gold standard Some errors can ac-
tually be traced back to inaccuracies in the gold stan-
dard: some question pairs which have been flagged
as paraphrases by the WikiAnswers contributors are
actually distantly related. For instance, the questions
When was the first painting made? and Where did
leanardo da vinci live? are marked as reformula-
tions of the question What is the secret about mona
lisa? Though these questions all share a common
broad topic, they cannot be considered as relevant
paraphrases.
We can deduce several possible improvements
from what precedes. First, named entities and ab-
breviations play an important role in questions and
should therefore be identified and treated differently
from other kinds of tokens. This could be achieved
by using a named entity recognition component
during pre-processing and then assigning a higher
weight to named entities in the retrieval process.
This should also improve the results of spelling cor-
rection since named entities and abbreviations could
be excluded from the correction. Second, seman-
tic errors could be dealt with by using a semantic
similarity metric similar to those used in declarative
sentence paraphrase identification (Li et al, 2006;
Mihalcea et al, 2006; Islam and Inkpen, 2007).
5.4 Comparison and Combination of the
Methods
In a second part of the experiment, we investigated
whether the evaluated methods display independent
50
error patterns, as suggested by our detailed results
analysis. Figure 2 confirms that the pre-processing
techniques as well as the methods employed result
in dissimilar error patterns. We therefore combined
several methods and pre-processing techniques in
order to verify if we could improve accuracy.
We obtained the best results by performing a ma-
jority vote combination of the following methods
and pre-processing strategies: Lucene, Term Vector
Similarity with stemming and Ngram Overlap with
spelling correction. The combination yielded an ac-
curacy of 88.3%, that is 0.9% over the best Lucene
results with an accuracy of 87.4%.
6 Conclusion and Outlook
In this paper, we have shown that it is feasible to an-
swer learners? questions by retrieving question para-
phrases from social Q&A sites. As a first step to-
wards this objective, we investigated several ques-
tion similarity metrics and pre-processing strategies,
using WikiAnswers as input data and user generated
gold standard. The approach is however not limited
to this dataset and can be easily applied to retrieve
question paraphrases from other social Q&A sites.
We also performed an extended failure analysis
which provided useful insights on how results could
be further improved by performing named entity
analysis and using semantic similarity metrics.
Another important challenge in using social Q&A
sites for educational purposes lies in the quality of
the answers retrieved from such sites. Previous re-
search on the identification of high quality content in
social Q&A sites has defined answer quality in terms
of correctness, well-formedness, readability, objec-
tivity, relevance, utility and interestingness (Jeon et
al., 2006; Agichtein et al, 2008). It is obvious that
all these elements play an important role in the ac-
ceptance of the answers by learners. We therefore
plan to integrate quality measures in the retrieval
process and to perform evaluations in a real educa-
tional setting.
Acknowledgments
This work was supported by the Emmy Noether Pro-
gramme of the German Research Foundation (DFG)
under grant No. GU 798/3-1.
References
Eugene Agichtein, Carlos Castillo, Debora Donato, Aris-
tides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In WSDM ?08:
Proceedings of the international conference on Web
search and web data mining, pages 183?194.
Answers Corporation. 2008. WikiAnswers Jour-
nalist Quick Guide. [Online; visited March
4, 2008]. http://site.wikianswers.com/
resources/WikiAnswers_1-pager.pdf.
Ayelet Baram-Tsabari, Ricky J. Sethi, Lynn Bry, and
Anat Yarden. 2006. Using questions sent to an Ask-A-
Scientist site to identify children?s interests in science.
Science Education, 90(6):1050?1072.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT
2003, pages 16?23. Association for Computational
Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In ACL
?01: Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics, pages 50?
57. Association for Computational Linguistics.
Joa?o Cordeiro, Gae?l Dias, and Pavel Brazdil. 2007.
Learning Paraphrases from WNS Corpora. In David
Wilson and Geoff Sutcliffe, editors, Proceedings of
the Twentieth International Florida Artificial Intelli-
gence Research Society Conference (FLAIRS), pages
193?198, Key West, Florida, USA, May 7-9. AAAI
Press.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In COL-
ING ?04: Proceedings of the 20th international con-
ference on Computational Linguistics, pages 350?356.
Association for Computational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.
2006. An Intelligent Discussion-Bot for Answering
Student Queries in Threaded Discussions. In Proceed-
ings of the 11th international conference on Intelligent
user interfaces (IUI?06), pages 171?177.
Aminul Islam and Diana Inkpen. 2007. Semantic Sim-
ilarity of Short Texts. In Proceedings of the Interna-
tional Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2007), Borovets, Bulgaria,
September.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and answer
archives. In CIKM ?05: Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 84?90.
51
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answers with non-textual features. In SIGIR ?06: Pro-
ceedings of the 29th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 228?235.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In CIKM ?05: Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, pages 76?83.
Yu Sun Lee. 2006. Toward a New Knowledge Shar-
ing Community: Collective Intelligence and Learn-
ing through Web-Portal-Based Question-Answer Ser-
vices. Masters of arts in communication, culture &
technology, Faculty of the Graduate School of Arts
and Sciences of Georgetown University, May. [On-
line; visited February 15, 2008], http://hdl.
handle.net/1961/3701.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2006. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 18(8):1138?1150.
Steven L. Lytinen and Noriko Tomuro. 2002. The Use
of Question Types to Match Questions in FAQFinder.
In Proceedings of the 2002 AAAI Spring Symposium
on Mining Answers from Texts and Knowledge Bases,
pages 46?53.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
RadaMihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In CIKM
?07: Proceedings of the sixteenth ACM conference on
information and knowledge management, pages 233?
242.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of AAAI
2006, Boston, July.
Peter Norvig. 2007. How to Write a Spelling Correc-
tor. [Online; visited February 22, 2008]. http:
//norvig.com/spell-correct.html.
Lee Ann Prescott. 2006. Yahoo! Answers Cap-
tures 96% of Q and A Market Share. Hit-
wise Intelligence [Online; visited February
26, 2008]. http://weblogs.hitwise.
com/leeann-prescott/2006/12/yahoo_
answers_captures_96_of_q.html.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In WWW ?06: Proceedings of
the 15th international conference on World Wide Web,
pages 377?386.
Choe Sang-Hun. 2007. To outdo Google,
Naver taps into Korea?s collective wis-
dom. International Herald Tribune, July 4.
http://www.iht.com/articles/2007/
07/04/technology/naver.php.
Noriko Tomuro and Steven Lytinen. 2004. Retrieval
Models and Q&A Learning with FAQ Files. In
Mark T. Maybury, editor, New Directions in Question
Answering, pages 183?194. AAAI Press.
Markus Weimer, Iryna Gurevych, and Max Mu?hlha?user.
2007. Automatically Assessing the Post Quality in
Online Discussions on Software. In Proceedings of the
Demo and Poster Sessions of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
pages 125?128, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query clustering using user logs. ACM Trans.
Inf. Syst., 20(1):59?81.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learn-
ing Question Paraphrases for QA from Encarta Logs.
In Proceedings of the 20th International Joint Confer-
ence on Artificial Intelligence, pages 1795?1801, Hy-
derabad, India, January 6-12.
52
Multilingual Term Extraction from Domain-specific Corpora
Using Morphological Structure
Delphine Bernhard
TIMC-IMAG
Institut de l?Inge?nierie et de l?Information de Sante?
Faculte? de Me?decine
F-38706 LA TRONCHE cedex
Delphine.Bernhard@imag.fr
Abstract
Morphologically complex terms com-
posed from Greek or Latin elements are
frequent in scientific and technical texts.
Word forming units are thus relevant cues
for the identification of terms in domain-
specific texts. This article describes a
method for the automatic extraction of
terms relying on the detection of classi-
cal prefixes and word-initial combining
forms. Word-forming units are identi-
fied using a regular expression. The sys-
tem then extracts terms by selecting words
which either begin or coalesce with these
elements. Next, terms are grouped in fam-
ilies which are displayed as a weighted list
in HTML format.
1 Introduction
Many methods for the automatic extraction of
terms make use of patterns describing the structure
of terms. This approach is especially helpful for
multi-word terms. Depending on the method, pat-
terns rely on morpho-syntactic properties (Daille,
1996; Ibekwe-SanJuan, 1998), the co-occurrence
of terms and connectors (Enguehard, 1992; Ba-
roni and Bernardini, 2004) or the alternation of
informative and non-informative words (Vergne,
2005). These patterns use words as basic units
and thus apply to multi-word terms. Methods for
the acquisition of single-word terms generally de-
pend on frequency-related information. For in-
stance, the frequency of occurrence of a word in
a domain-specific corpus can be compared with
its frequency of occurrence in a reference corpus
(Rayson and Garside, 2000; Baroni and Bernar-
dini, 2004). Technical words usually have a high
relative frequency difference between the domain-
specific corpus and the reference corpus.
In this paper, we present a pattern-based tech-
nique to extract single-word terms. In technical
and scientific domains like medicine many terms
are derivatives or neoclassical compounds (Cot-
tez, 1984). There are several types of classical
word-forming units: prefixes (extra-, anti-), ini-
tial combining forms (hydro-, pharmaco-), suf-
fixes (-ism) and final combining forms (-graphy,
-logy). Interestingly, these units are rather con-
stant in many European languages (Namer, 2005).
Consequently, instead of relying on a subword dic-
tionary to analyse compounds like (Schulz et al,
2002), our method makes use of these regularities
to automatically extract prefixes and initial com-
bining forms from corpora. The system then iden-
tifies terms by selecting words which either begin
or coalesce with these units. Moreover, forming
elements are used to group terms in morphological
and hence semantic families. The different stages
of the process are detailed in section 2. Section 3
describes the results of experiments performed on
four corpora, in English and in French.
2 Description of the method
2.1 Extraction of words
The system takes as input a corpus of texts. Para-
graphs written in another language than the target
language are filtered out. Texts are then tokenised
and words are converted to lowercase. Besides,
words containing digits or other non-word charac-
ters are eliminated. However, hyphenated words
are kept since hyphens mark morpheme bound-
aries. This preliminary step produces a word fre-
quency list for the corpus.
171
2.2 Acquisition of combining forms
Prefixes and initial combining forms are auto-
matically acquired using the following regular
expression: ([aio]-)?(\w{3,}[aio])-. This regu-
lar expression represents character strings whose
length is higher or equal to 4, ending with a,
i or o and immediately followed by a hyphen.
The first part of the regular expression accounts
for words where several prefixes or combining
forms follow one another (as for instance in
the French word ?he?pato-gastro-ente?rologues?).
This regular expression applies to English but
also to other languages like French or German:
see for instance ?chimio-radiothe?rapie? in French,
?chemo-radiotherapy? in English or ?Chemo-
radiotherapie? in German.
2.3 Identification of terms
Terms are identified using the following pattern
describing their morphological structure: E+W
where E is a prefix or combining form and W is a
word whose length is higher than 3; the ?+? charac-
ter represents the possible succession of several E
elements at the beginning of a term. Prefixes and
combining forms may be separated by a hyphen.
When this pattern applies to one of the words in
the corpus, two terms are recognised, one with a
E+W structure and the other with a W structure.
For instance, given the word ?ferrobasalts?, the
system identifies the terms ?ferrobasalts? (E+W)
and ?basalts? (W).
2.4 Conflation of terms
Term variants are grouped in order to ease the
analysis of results. The method for terms confla-
tion can be decomposed in two stages:
1. Terms containing the same word W belong to
the same family, represented by the word W.
For instance, both ?chemotherapy? and ?ra-
diotherapy? contain the word ?therapy?: they
belong to the same family of terms, repre-
sented by the word ?therapy?.
2. Two families are merged if they are rep-
resented by words sharing the same ini-
tial substring (with a minimum initial sub-
string length of 4) and if the same prefix
or combining form occurs in one term of
each family. Consider for instance the fam-
ilies F1= [oncology, psycho-oncology, radio-
oncology, neuro-oncology, psychooncology,
neurooncology] and F2 = [oncologist, neuro-
oncologist]. The terms representing F1 (?on-
cology?) and F2 (?oncologist?) share an ini-
tial substring of length 7. Moreover the
terms ?neuro-oncology? from F1 and ?neuro-
oncologist? from F2 contain the combining
form ?neuro?. Families F1 and F2 are there-
fore united.
When terms have been conflated, we select the
most frequent term as a family?s representative.
2.5 Data visualisation
The results obtained are displayed as a weighted
list in HTML format. Such lists, also named ?heat
maps? or ?tag clouds? when they describe tags1
usually represent the terms and topics which ap-
pear most frequently on websites or RSS feeds
(Wikipedia, 2006). They can also be used to rep-
resent any kind of word list (Ve?ronis, 2005). Dif-
ferent colours and font sizes are used depending
on the word?s frequency of occurrence. We have
adapted this method to visualise the list of ex-
tracted terms. Since several hundred terms may
be extracted, only the terms representing a fam-
ily are displayed on the weighted list. Weight is
given by the cumulated frequency of all the terms
belonging to the family (see Figure 1).
Figure 1: Term cloud example (Corpus: BC en)
Further information (terms and frequencies) is
displayed thanks to tooltips (see Figure 2), us-
ing the JavaScript overLIB libray ( http://www.
bosrup.com/web/overlib).
1See for example TagCloud: http://www.
tagcloud.com
172
Figure 2: Detailed term family displayed as a
tooltip (Corpus: V fr)
3 Experiments and results
3.1 Corpora
The system has been experimented on 4 corpora
covering the domains of volcanology (V) and
breast cancer (BC), in English (en) and in French
(fr). The corpora have been automatically built
from the web, using the methodology described
in (Baroni and Bernardini, 2004), via the Ya-
hoo! Search Web Services ( http://developer.
yahoo.net/search/). The size of the corpora ob-
tained are given in Table 1. This table also gives
the number of key words, i.e., single-word terms
extracted by comparing the frequency of occur-
rence of words in both corpora for each language
(Rayson and Garside, 2000). Only terms with a
log-likelihood of 3.8 or higher (p<0.05) have been
kept in the key words list. Table 2 gives a nu-
merical overview of the results obtained by our
method.
Corpus Tokens Word forms Key words
BC fr 1,451,809 46,834 13,700
BC en 7,044,146 88,726 17,602
V fr 1,777,030 59,909 13,673
V en 2,929,591 48,257 19,641
Table 1: Size of the corpora
3.2 Prefixes and initial combining forms
As shown by Table 2, the number of prefixes and
initial combining forms identified is proportion-
ally less for the volcanology corpora both in En-
glish and in French. Medical corpora seem to
be more adapted to the method since the num-
Corpus Word-forming
elements
Terms Term
families
BC fr 334 4,248 911
BC en 382 5,444 1,338
V fr 182 1,842 583
V en 188 1,648 564
Table 2: Number of word-forming elements, terms
and term families identified for each corpus
ber of terms extracted is higher. The prefixes
and combining forms identified are also highly
dependent on the corpus domain. For instance,
amongst the most frequent combining forms ex-
tracted for the BC corpora, we find ?radio? and
?chemo? (?chimio? in French) and for the V cor-
pora, ?strato? and ?volcano?.
3.3 Terms
The overlap percentage between the list of terms
and the list of key words ranges from 38.65%
(V fr) to 56.92% (V en) of the total amount of
terms extracted. If we compare both the list of key
words and the list of terms extracted for the BC en
corpus with the Unified Medical Language Sys-
tem Metathesaurus (http://www.nlm.nih.gov/
research/umls/) we notice that some highly spe-
cific terms like ?disease?, ?blood? or ?x-ray? are
not identified by our method, while they occur
in the key words list. These are usually mor-
phologically simple terms, also used in everyday
language. Conversely, terms with low frequency
like ?adenoacanthoma?, ?chondroma? or ?mam-
motomy? are correctly identified by the pattern-
based approach but are missing in the key words
list. Both methods are therefore complementary.
In some cases, stop-words are extracted. This
is a side effect of the pattern used to retrieve
terms. Remember that terms are words which co-
alesce with combining forms, possibly with hy-
phenation. In English hyphens are sometimes mis-
takenly used instead of the dash to mark com-
ment clauses. Consider for instance the follow-
ing sentence: ?As this magma-which drives one
of the worlds largest volcanic systems-rises, it
pushes up the Earths crust beneath the Yellow-
stone Plateau.?. Here ?magma? is identified as
a combining form since it ends with ?a? and is
directly followed by a hyphen. Consequently,
?which? is wrongly identified as a term.
173
3.4 Term families
Several types of term variants are grouped by the
term conflation algorithm: (a) graphical and ortho-
graphical variants like ?tumour? (British variant)
and ?tumor? (American variant); (b) inflectional
variants like ?tumor? and ?tumors?; (c) deriva-
tional variants like ?tumor? and ?tumoral?.
Two types of conflation errors may however oc-
cur: over-conflation, i.e., the conflation of terms
which do not belong to the same morphologi-
cal family and under-conflation, i.e. the absence
of conflation for morphologically related terms.
Some cases of over-conflation are obvious, such
as the grouping of ?significant? with ?cant?. In
some other cases it is more difficult to tell. This
especially applies to the conflation of terms com-
posed of word final combining forms like ?-gram?
or ?-graph?. Under-conflation occurs when no
combining form is shared between terms belong-
ing to families represented by graphically similar
terms. For instance, the following term families
are extracted from the French volcanology corpus
(V fr): F1= [basalte, me?tabasalte, me?ta-basalte],
F2= [basaltes, ferro-basaltes, pale?obasaltes] and
F3= [basaltique, ande?sitico-basaltique]. These
families are not conflated, even though they ob-
viously belong to the same morphological family.
4 Conclusion
We have presented a method for the automatic ac-
quisition of terms from domain-specific texts us-
ing morphological structure. The method also
groups terms in morphological families. Fami-
lies are displayed as a weighted list, thus giving
an instant overview of the main topics in the cor-
pus under study. Results obtained from the first
experiments confirm the usefulness of a morpho-
logical pattern based approach for the extraction
of terms from domain-specific corpora and espe-
cially medical texts. The method for the identifi-
cation of compound words could be improved by
an automatic approach to morphological segmen-
tation as done by (Creutz and Lagus, 2004). Term
clustering could be ameliorated as well by investi-
gating the usefulness of stemming to avoid under-
conflation.
References
Marco Baroni and Silvia Bernardini. 2004. Boot-
CaT: Bootstrapping Corpora and Terms from the
Web. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation
(LREC), pages 1313?1316.
Henri Cottez. 1984. Dictionnaire des structures du vo-
cabulaire savant. E?le?ments et mode`les de formation.
Le Robert, Paris, 3rd edition.
Mathias Creutz and Krista Lagus. 2004. Induc-
tion of a Simple Morphology for Highly-Inflecting
Languages. In Proceedings of the 7th Meeting of
the ACL Special Interest Group in Computational
Phonology (SIGPHON), pages 43?51.
Be?atrice Daille. 1996. Study and Implementation of
Combined Techniques for Automatic Extraction of
Terminology. In Judith Klavans and Philip Resnik,
editors, The Balancing Act: Combining Symbolic
and Statistical Approaches to Language, pages 49?
66. The MIT Press, Cambridge, Massachusetts.
Chantal Enguehard. 1992. ANA, Apprentissage Na-
turel Automatique d?un Re?seau Se?mantique. Ph.D.
thesis, Universite? de Technologie de Compie`gne.
Fidelia Ibekwe-SanJuan. 1998. Terminological vari-
ation, a means of identifying research topics from
texts. In Proceedings of the Joint International Con-
ference on Computational Linguistics (COLING-
ACL?98), pages 564?570.
Fiammetta Namer. 2005. Morphose?mantique pour
l?appariement de termes dans le vocabulaire me?dical
: approche multilingue. In Actes de TALN 2005,
pages 63?72.
Paul Rayson and Roger Garside. 2000. Comparing
Corpora using Frequency Profiling. In Proceedings
of the ACL Workshop on Comparing Corpora, pages
1?6.
Stefan Schulz, Martin Honeck, and Udo Hahn. 2002.
Biomedical Text Retrieval in Languages with a
Complex Morphology. In Proceedings of the ACL
Workshop on Natural Language Processing in the
Biomedical Domain, pages 61?68.
Jacques Vergne. 2005. Une me?thode inde?pendante
des langues pour indexer les documents de l?internet
par extraction de termes de structure contro?le?e. In
Actes de la Confe?rence Internationale sur le Docu-
ment E?lectronique (CIDE 8), pages 155?168.
Jean Ve?ronis. 2005. Nuage de mots d?aujourd?hui.
http://aixtal.blogspot.com/2005/07/
lexique-nuage-de-mots-daujourdhui.
html. [Online; accessed 31-January-2006].
Wikipedia. 2006. RSS (file format) ?
Wikipedia, The Free Encyclopedia. http:
//en.wikipedia.org/w/index.php?title=
RSS_(file_format)&oldid=37472136. [On-
line; accessed 31-January-2006].
174
Coling 2008: Educational Natural Language Processing ? Tutorial notes
Manchester, August 2008
References
Generation of Exercises
? Computer-based Testing and Question Generation ?
Duvall, K. Improving Your Test Questions. [Online; visited May 26, 2008]. Center for Teaching Excellence,
University of Illinois at Urbana-Champaign. http://www.oir.uiuc.edu/dme/exams/ITQ.html.
McKenna, C. and Bull, J. (1999). Designing effective objective test questions: an introductory workshop.
[Online; visited May 26, 2008]. CAA Centre, Loughborough University, http://caacentre.lboro.ac.uk/
dldocs/otghdout.pdf.
? Multiple-choice Questions ?
Brown, J. C., Frishkoff, G. A., and Eskenazi, M. (2005). Automatic question generation for vocabulary
assessment. In HLT ?05: Proceedings of the conference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 819?826, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Heilman, M. and Eskenazi, M. (2007). Application of Automatic Thesaurus Extraction for Computer
Generation of Vocabulary Questions. In Proceedings of Speech and Language Technology in Education
(SLaTE2007), pages 65?68.
Karamanis, N., Ha, L. A., and Mitkov, R. (2006). Generating Multiple-Choice Test Items from Medical
Text: A Pilot Study. In Proceedings of the Fourth International Natural Language Generation Conference,
pages 111?113, Sydney, Australia. Association for Computational Linguistics.
Mitkov, R., Ha, L. A., and Karamanis, N. (2006). A computer-aided environment for generating multiple-
choice test items. Natural Language Engineering, 12(2):177?194.
? Fill-in-the-blank Questions ?
Aldabe, I., de Lacalle, M. L., Maritxalar, M., Martinez, E., and Uria, L. (2006). ArikIturri: An Automatic
Question Generator Based on Corpora and NLP Techniques. In Ikeda, M., Ashley, K. D., and Chan, T.-W.,
editors, Intelligent Tutoring Systems, volume 4053 of Lecture Notes in Computer Science, pages 584?594.
Springer.
Coniam, D. (1997). A Preliminary Inquiry Into Using Corpus Word Frequency Data in the Automatic
Generation of English Language Cloze Tests. CALICO Journal, 14:15?33.
? Multiple-choice Cloze Questions ?
Chen, C.-Y., Liou, H.-C., and Chang, J. S. (2006). FAST: an automatic generation system for grammar
tests. In Proceedings of the COLING/ACL Interactive presentation sessions, pages 1?4, Morristown, NJ,
USA. Association for Computational Linguistics.
Hoshino, A. and Hiroshi, N. (2005). A Real-Time Multiple-Choice Question Generation For Language
Testing: A Preliminary Study. In Proceedings of the Second Workshop on Building Educational Applications
Using NLP, pages 17?20, Ann Arbor, Michigan. Association for Computational Linguistics.
Lee, J. and Seneff, S. (2007). Automatic Generation of Cloze Items for Prepositions. In Proceedings of
INTERSPEECH 2007, pages 2173?2176, Antwerp, Belgium.
Liu, C.-L., Wang, C.-H., Gao, Z.-M., and Huang, S.-M. (2005). Applications of Lexical Information for
Algorithmically Composing Multiple-Choice Cloze Items. In Proceedings of the Second Workshop on Build-
ing Educational Applications Using NLP, pages 1?8, Ann Arbor, Michigan. Association for Computational
Linguistics.
Smith, S., Sommers, S., and Kilgarriff, A. (2008). Learning words right with the Sketch Engine and
WebBootCat: Automatic cloze generation from corpora and the web. In Proceedings of the Conference of
English Teaching and Learning in R.O.C.
1
53
Sumita, E., Sugaya, F., and Yamamoto, S. (2005). Measuring Non-native Speakers? Proficiency of En-
glish by Using a Test with Automatically-Generated Fill-in-the-Blank Questions. In Proceedings of the
Second Workshop on Building Educational Applications Using NLP, pages 61?68, Ann Arbor, Michigan.
Association for Computational Linguistics.
? Matching Test Items ?
Brown, J. C., Frishkoff, G. A., and Eskenazi, M. (2005). Automatic question generation for vocabulary
assessment. In HLT ?05: Proceedings of the conference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 819?826, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
? Error Correction Questions ?
Chen, C.-Y., Liou, H.-C., and Chang, J. S. (2006). FAST: an automatic generation system for grammar
tests. In Proceedings of the COLING/ACL Interactive presentation sessions, pages 1?4, Morristown, NJ,
USA. Association for Computational Linguistics.
? Item Analysis ?
Zurawski, R. M. (1998). Making the Most of Exams: Procedures for Item Analysis. The National Teaching
& Learning FORUM, 7(6):1?4.
Assessment of Learner-Generated Discourse
? Essay Scoring ?
Attali, Y. and Burstein, J. (2006). Automated Essay Scoring With e-rater
R
? V.2. Journal of Technology,
Learning and Assessment, 4(3).
Breland, H. M., Jones, R. J., and Jenkins, L. (1994). The College Board vocabulary study. Technical
report, College Board Report No. 94?4, New York: College Entrance Examination Board.
Burstein, J., Kukich, K., Wolff, S., Lu, C., Chodorow, M., Braden-Harder, L., and Harris, M. D. (1998).
Automated scoring using a hybrid feature identification technique. In Proceedings of the 17th international
conference on Computational linguistics, pages 206?210, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Burstein, J., Marcu, D., and Knight, K. (2003). Finding the WRITE Stuff: Automatic Identification of
Discourse Structure in Student Essays. IEEE Intelligent Systems, 18(1):32?39.
Burstein, J. and Wolska, M. (2003). Toward evaluation of writing style: finding overly repetitive word use in
student essays. In EACL ?03: Proceedings of the tenth conference on European chapter of the Association for
Computational Linguistics, pages 35?42, Morristown, NJ, USA. Association for Computational Linguistics.
Elliot, S. M. (2001). IntelliMetric: from here to validity. In Paper presented at the annual meeting of the
American Educational Research Association, Seattle, WA.
Hearst, M. A. (2000). The Debate on Automated Essay Grading. IEEE Intelligent Systems, 15(5):22?37.
Higgins, D., Burstein, J., and Attali, Y. (2006). Identifying off-topic student essays without topic-specific
training data. Natural Language Engineering, 12(2):145?159.
Landauer, T. K., Laham, D., and Foltz, P. (1998). Learning Human-like Knowledge by Singular Value
Decomposition: A Progress Report. Advances in Neural Information Processing Systems, 10:45?51.
Page, E. B. (1966). The imminence of grading essays by computer. Phi Delta Kappan, 47:238?243.
Page, E. B. (1994). Computer Grading of Student Prose, Using Modern Concepts and Software. Journal
of Experimental Education, 62:127?142.
Yang, Y., Buckendahl, C. W., Juszkiewicz, P. J., and Bhola, D. S. (2002). A Review of Strategies for
Validating Computer-Automated Scoring. Applied Measurement in Education, 15(4):391?412.
2
54
? Plagiarism ?
Clough, P. (2000). Plagiarism in Natural and Programming Languages: an Overview of Current Tools and
Technologies. Technical report, Internal Report CS-00-05, University of Sheffield.
Clough, P. (2003). Old and new challenges in automatic plagiarism detection. Technical report, National
UK Plagiarism Advisory Service.
Martin, B. (1994). Plagiarism: a misplaced emphasis. Journal of Information Ethics, 3(2):36?47.
? Short Answer Assessment ?
Bailey, S. and Meurers, D. (2008). Diagnosing Meaning Errors in Short Answers to Reading Comprehension
Questions. In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational
Applications, pages 107?115, Columbus, Ohio. Association for Computational Linguistics.
Leacock, C. (2004). Scoring free-responses automatically: A case study of a large-scale assessment. Exam-
ens, 1(3).
Leacock, C. and Chodorow, M. (2003). c-rater: Scoring of short-answer questions. Computers and the
Humanities, 37:389?405.
Sandene, B., Horkay, N., Bennett, R. E., Allen, N., Braswell, J., Kaplan, B., , and Oranje, A. (2005). Online
Assessment in Mathematics and Writing: Reports From the NAEP Technology-Based Assessment Project,
Research and Development Series. Technical report, National Assessment of Educational Progresss.
? Speech Assessment ?
Bernstein, J. (1999). PhonePass testing: Structure and construct. Technical report, Ordinate Corporation.
Bernstein, J., DeJong, J., Pisoni, D., and Townshend, B. (2000). Two experiments on automatic scoring of
spoken language proficiency. In Proceedings of InSTIL2000.
Zechner, K. and Bejar, I. I. (2006). Towards automatic scoring of non-native spontaneous speech. In
Proceedings of the main conference on Human Language Technology Conference of the North American
Chapter of the Association of Computational Linguistics, pages 216?223.
Zechner, K., Higgins, D., and Xi, X. (2007). SpeechRaterTM: A Construct-Driven Approach to Scoring
Spontaneous Non-Native Speech. In roceedings of the 2007 Workshop of the International Speech Com-
munication Association (ISCA) Special Interest Group on Speech and Language Technology in Education
(SLaTE2007).
Zechner, K. and Xi, X. (2008). Towards Automatic Scoring of a Test of Spoken Language with Het-
erogeneous Task Types. In Proceedings of the Third Workshop on Innovative Use of NLP for Building
Educational Applications, pages 98?106, Columbus, Ohio. Association for Computational Linguistics.
Reading and Writing Assistance
? Text Readability ?
Brown, J. and Eskenazi, M. (2004). Retrieval of Authentic Documents for Reader-Specific Lexical Practice.
In Proceedings of the InSTIL/ICALL 2004 Symposium on Computer Assisted Learning, Venice, Italy.
Collins-Thompson, K. and Callan, J. (2005). Predicting reading difficulty with statistical language models.
Journal of the American Society for Information Science and Technology, 56(13):1448?1462.
DuBay, W. H. (2004). The Principles of Readability. Costa Mesa, California. Impact Information.
? Document Retrieval for Reading Practice ?
Heilman, M., Zhao, L., Pino, J., and Eskenazi, M. (2008). Retrieval of Reading Materials for Vocabulary
and Reading Practice. In Proceedings of the Third Workshop on Innovative Use of NLP for Building
Educational Applications, pages 80?88, Columbus, Ohio. Association for Computational Linguistics.
Miltsakaki, E. and Troutt, A. (2008). Real Time Web Text Classification and Analysis of Reading Difficulty.
In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,
pages 89?97, Columbus, Ohio. Association for Computational Linguistics.
3
55
? Text Simplification ?
Carroll, J., Minnen, G., Pearce, D., Canning, Y., Devlin, S., and Tait, J. (1999). Simplifying Text for
Language-Impaired Readers. In Proceedings of the Ninth Conference of the European Chapter of the
Association for Computational Linguistics, pages 269?270.
Inui, K., Fujita, A., Takahashi, T., Iida, R., and Iwakura, T. (2003). Text simplification for reading
assistance: a project note. In Proceedings of the second international workshop on Paraphrasing, pages
9?16, Morristown, NJ, USA. Association for Computational Linguistics.
Lal, P. and Ru?ger, S. (2002). Extract-based Summarization with Simplification. In Proceedings of the
Workshop on Text Summarization at DUC 2002.
Petersen, S. E. and Ostendorf, M. (2007). Text Simplification for Language Learners: A Corpus Analysis.
In Proceedings of Speech and Language Technology in Education (SLaTE2007), pages 69?72.
? Vocabulary Assistance ?
Aist, G. (2001). Towards automatic glossarization: automatically constructing and administering vocabu-
lary assistance factoids and multiple-choice assessment. International Journal of Artificial Intelligence in
Education, 12:212 ? 231.
Csomai, A. and Mihalcea, R. (2007). Linking Educational Materials to Encyclopedic Knowledge. In Pro-
ceedings of the International Conference on Artificial Intelligence in Education (AIED 2007), Los Angeles,
CA.
Mihalcea, R. and Csomai, A. (2007). Wikify!: linking documents to encyclopedic knowledge. In CIKM ?07:
Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,
pages 233?242, New York, NY, USA. ACM.
Zesch, T., Gurevych, I., and Mu?hlha?user, M. (2007). Analyzing and Accessing Wikipedia as a Lexical
Semantic Resource. In Rehm, G., Witt, A., and Lemnitzer, L., editors, Data Structures for Linguistic
Resources and Applications, pages 197?205. Gunter Narr, Tu?bingen.
Zesch, T., Mu?ller, C., and Gurevych, I. (2008). Extracting Lexical Semantic Knowledge from Wikipedia
and Wiktionary. In Proceedings of LREC?08.
? Spell Checking ?
Heift, T. and Rimrott, A. (2008). Learner Responses to Corrective Feedback for Spelling Errors in CALL.
System, 36:196?213.
Jurafsky, D. and Martin, J. H. (2008). Speech and Language Processing. Prentice Hall. 2nd edition.
Kukich, K. (1992). Techniques for automatically correcting words in text. ACM Computing Surveys,
24(4):377?439.
Manning, C. D., Raghavan, P., and Schu?tze, H. (2008). Introduction to Information Retrieval. Cambridge
University Press.
? Grammar Checking ?
Atwell, E. S. (1987). How to detect grammatical errors in a text without parsing it. In Proceedings of
the third conference of the European chapter of the Association for Computational Linguistics, pages 38?45,
Morristown, NJ, USA. Association for Computational Linguistics.
Chodorow, M. and Leacock, C. (2000). An Unsupervised Method for Detecting Grammatical Errors.
In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational
Linguistics, pages 140?147.
Chodorow, M., Tetreault, J., and Han, N.-R. (2007). Detection of Grammatical Errors Involving Prepo-
sitions. In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 25?30, Prague, Czech
Republic. Association for Computational Linguistics.
Eeg-Olofsson, J. and Knutsson, O. (2003). Automatic grammar checking for second language learners - the
use of prepositions. In Proceedings of NoDaLiDa 2003.
4
56
Lee, J. and Seneff, S. (2006). Automatic Grammar Correction for Second-Language Learners. In Proceedings
of INTERSPEECH 2006, pages 1978?1981.
Lee, J. and Seneff, S. (2008). Correcting Misuse of Verb Forms. In Proceedings of ACL-HLT-08, pages
174?182.
Naber, D. (2003). A Rule-Based Style and Grammar Checker. Master?s thesis, Technische Fakulta?t,
Universita?t Bielefeld.
Nicholls, D. (1999). The Cambridge learner corpus - error coding and analysis. In Summer Workshop on
Learner Corpora, Tokyo, Japan.
Sun, G., Liu, X., Cong, G., Zhou, M., Xiong, Z., Lee, J., and Lin, C.-Y. (2007). Detecting Erroneous
Sentences using Automatically Mined Sequential Patterns. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Wagner, J., Foster, J., and van Genabith, J. (2007). A Comparative Evaluation of Deep and Shallow
Approaches to the Automatic Detection of Common Grammatical Errors. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Language Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 112?121.
? Dictionary Lookup ?
Ferret, O. and Zock, M. (2006). Enhancing electronic dictionaries with an index based on associations. In
ACL ?06: Proceedings of the 21st International Conference on Computational Linguistics and the 44th an-
nual meeting of the ACL, pages 281?288, Morristown, NJ, USA. Association for Computational Linguistics.
Tutoring Systems
Aleven, V., Koedinger, K. R., and Popescu, O. (2003). A Tutorial Dialog System to Support Self-
Explanation: Evaluation and Open Questions. In Proceedings of the 11th International Conference on
Artificial Intelligence in Education (AIED 2003), pages 39?46.
Boyer, K., Phillips, R., Wallis, M., Vouk, M., and Lester, J. (2008). Learner Characteristics and Feedback
in Tutorial Dialogue. In Proceedings of the Third Workshop on Innovative Use of NLP for Building
Educational Applications, pages 53?61, Columbus, Ohio. Association for Computational Linguistics.
Chi, M. T. H., Leeuw, N. D., Chiu, M.-H., and Lavancher, C. (1994). Eliciting self-explanations improves
understanding. Cognitive Science, 18(3):439?477.
Evens, M. and Michael, J. (2005). One-on-One Tutoring by Humans and Computers. Lawrence Erlbaum
Associates.
Forbes-Riley, K. and Litman, D. J. (2005). Using bigrams to identify relationships between student certain-
ness states and tutor responses in a spoken dialogue corpus. In Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue, pages 87?96., Lisbon, Portugal.
Forbes-Riley, K. M., Litman, D., Huettner, A., and Ward, A. (2005). Dialogue-Learning Correlations in
Spoken Dialogue Tutoring. In Proceedings of the 12th International Conference on Artificial Intelligence
in Education (AIED), pages 225?232, Amsterdam, The Netherlands.
Graesser, A., Wiemer-Hastings, K., Wiemer-Hastings, P., R, R. K., and the Tutoring Research Group U.o.M.
(1999). AutoTutor: A simulation of a human tutor. Cognitive Systems Research, 1(1):35?51.
Graesser, A. C., Lu, S., Jackson, G. T., Mitchell, H. H., Ventura, M., Olney, A., and Louwerse, M. M.
(2004). AutoTutor: A tutor with dialogue in natural language. Behavior Research Methods, Instruments,
& Computers, 36(2):180?192.
Graesser, A. C., Moreno, K. N., Marineau, J. C., Adcock, A. B., Olney, A. M., and Person, N. K. (2003).
AutoTutor Improves Deep Learning of Computer Literacy: Is It the Dialog or the Talking Head? In
Proceedings of the 11th Conference on Artificial Intelligence in Education (AIED 2003), pages 47?54.
Graesser, A. C., VanLehn, K., Rose?, C. P., Jordan, P. W., and Harter, D. (2001). Intelligent Tutoring
Systems with Conversational Dialogue. AI Magazine, 22(4):39?52.
5
57
Hausmann, R. G. and Chi, M. T. (2002). Can a Computer Interface Support Self-explaining? Cognitive
Technology Journal, 7(1):4?15.
Lane, H. and VanLehn, K. (2005). Teaching the tacit knowledge of programming to novices with natural
language tutoring. Computer Science Education, Special issue on doctoral research in CS Education,
15(3):183?201.
Litman, D. J., Rose?, C. P., Forbes-Riley, K., VanLehn, K., Bhembe, D., and Silliman, S. (2006). Spoken
Versus Typed Human and Computer Dialogue Tutoring. International Journal of Artificial Intelligence in
Education, 16(2):145?170.
Litman, D. J. and Silliman, S. (2004). ITSPOKE: An Intelligent Tutoring Spoken Dialogue System. In
Susan Dumais, D. M. and Roukos, S., editors, HLT-NAACL 2004: Demonstration Papers, pages 5?8,
Boston, Massachusetts, USA. Association for Computational Linguistics.
Person, N. K., Graesser, A. C., Bautista, L., Mathews, E., and the Tutoring Research Group (2001).
Evaluating Student Learning Gains in Two Versions of AutoTutor. In Proceedings of Artificial Intelligence
in Education: AI-ED in the wired and wireless future, pages 286?293.
Pon-Barry, H., Schultz, K., Bratt, E. O., Clark, B., and Peters, S. (2006). Responding to Student Uncer-
tainty in Spoken Tutorial Dialogue Systems. International Journal of Artificial Intelligence in Education,
16(2):171?194.
VanLehn, K., Jordan, P. W., Rose?, C. P., Bhembe, D., Bo?ttner, M., Gaydos, A., Makatchev, M., Pap-
puswamy, U., Ringenberg, M. A., Roque, A., Siler, S., and Srivastava, R. (2002). The Architecture of
Why2-Atlas: A Coach for Qualitative Physics Essay Writing. In ITS ?02: Proceedings of the 6th Interna-
tional Conference on Intelligent Tutoring Systems, pages 158?167, London, UK. Springer-Verlag.
Zinn, C., Moore, J. D., and Core, M. G. (2002). A 3-Tier Planning Architecture for Managing Tutorial
Dialogue. In ITS ?02: Proceedings of the 6th International Conference on Intelligent Tutoring Systems,
pages 574?584, London, UK. Springer-Verlag.
Web 2.0 and Computer Supported Collaborative Learning
Bernhard, D. and Gurevych, I. (2008). Answering Learners? Questions by Retrieving Question Paraphrases
from Social Q&A Sites. In Proceedings of the 3rd Workshop on Innovative Use of NLP for Building
Educational Applications, ACL 2008, pages 44?52, Columbus, Ohio, USA.
? Quality of User-Generated Content ?
Agichtein, E., Castillo, C., Donato, D., Gionis, A., and Mishne, G. (2008). Finding high-quality content
in social media. In WSDM ?08: Proceedings of the international conference on Web search and web data
mining, pages 183?194, New York, NY, USA. ACM.
Druck, G., Miklau, G., and McCallum, A. (2008). Learning to Predict the Quality of Contributions to
Wikipedia. In Proceedings of the ?Wikipedia and Artificial Intelligence: An Evolving Synergy? Workshop at
AAAI-08.
Giles, J. (2005). Internet encyclopaedias go head to head. Nature, 438:900?901.
Jeon, J., Croft, W. B., Lee, J. H., and Park, S. (2006). A framework to predict the quality of answers with
non-textual features. In SIGIR ?06: Proceedings of the 29th annual international ACM SIGIR conference
on Research and development in information retrieval, pages 228?235, New York, NY, USA. ACM.
Kim, S.-M., Pantel, P., Chklovski, T., and Pennacchiotti, M. (2006). Automatically Assessing Review
Helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,
pages 423?430, Sydney, Australia. Association for Computational Linguistics.
Weimer, M. and Gurevych, I. (2007). Predicting the Perceived Quality of Web Forum Posts. In Proceedings
of the Conference on Recent Advances in Natural Language Processing (RANLP), pages 643?648.
Weimer, M., Gurevych, I., and Mu?hlha?user, M. (2007). Automatically Assessing the Post Quality in Online
Discussions on Software. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics, Companion Volume, Proceedings of the Demo and Poster Sessions, pages 125?128, Prague,
Czech Republic. Association for Computational Linguistics.
6
58
Electronic Career Guidance
Gurevych, I., Mu?ller, C., and Zesch, T. (2007). What to be? - Electronic Career Guidance Based on
Semantic Relatedness. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics, pages 1032?1039, Prague, Czech Republic. Association for Computational Linguistics.
7
59
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353?1361,
Beijing, August 2010
A Monolingual Tree-based Translation Model for Sentence Simplification?
Zhemin Zhu1
Department of Computer Science
Technische Universita?t Darmstadt
Delphine Bernhard2
LIMSI-CNRS
Iryna Gurevych1
Department of Computer Science
Technische Universita?t Darmstadt
1http://www.ukp.tu-darmstadt.de 2delphine.bernhard@limsi.fr
Abstract
In this paper, we consider sentence sim-
plification as a special form of translation
with the complex sentence as the source
and the simple sentence as the target.
We propose a Tree-based Simplification
Model (TSM), which, to our knowledge,
is the first statistical simplification model
covering splitting, dropping, reordering
and substitution integrally. We also de-
scribe an efficient method to train our
model with a large-scale parallel dataset
obtained from the Wikipedia and Simple
Wikipedia. The evaluation shows that our
model achieves better readability scores
than a set of baseline systems.
1 Introduction
Sentence simplification transforms long and dif-
ficult sentences into shorter and more readable
ones. This helps humans read texts more easily
and faster. Reading assistance is thus an impor-
tant application of sentence simplification, espe-
cially for people with reading disabilities (Carroll
et al, 1999; Inui et al, 2003), low-literacy read-
ers (Watanabe et al, 2009), or non-native speakers
(Siddharthan, 2002).
Not only human readers but also NLP ap-
plications can benefit from sentence simplifica-
tion. The original motivation for sentence sim-
plification is using it as a preprocessor to facili-
tate parsing or translation tasks (Chandrasekar et
al., 1996). Complex sentences are considered as
stumbling blocks for such systems. More recently,
sentence simplification has also been shown help-
ful for summarization (Knight and Marcu, 2000),
? This work has been supported by the Emmy Noether
Program of the German Research Foundation (DFG) under
the grant No. GU 798/3-1, and by the Volkswagen Founda-
tion as part of the Lichtenberg-Professorship Program under
the grant No. I/82806.
sentence fusion (Filippova and Strube, 2008b), se-
mantic role labeling (Vickrey and Koller, 2008),
question generation (Heilman and Smith, 2009),
paraphrase generation (Zhao et al, 2009) and
biomedical information extraction (Jonnalagadda
and Gonzalez, 2009).
At sentence level, reading difficulty stems ei-
ther from lexical or syntactic complexity. Sen-
tence simplification can therefore be classified
into two types: lexical simplification and syntac-
tic simplification (Carroll et al, 1999). These two
types of simplification can be further implemented
by a set of simplification operations. Splitting,
dropping, reordering, and substitution are widely
accepted as important simplification operations.
The splitting operation splits a long sentence into
several shorter sentences to decrease the complex-
ity of the long sentence. The dropping operation
further removes unimportant parts of a sentence to
make it more concise. The reordering operation
interchanges the order of the split sentences (Sid-
dharthan, 2006) or parts in a sentence (Watanabe
et al, 2009). Finally, the substitution operation re-
places difficult phrases or words with their simpler
synonyms.
In most cases, different simplification opera-
tions happen simultaneously. It is therefore nec-
essary to consider the simplification process as
a combination of different operations and treat
them as a whole. However, most of the ex-
isting models only consider one of these opera-
tions. Siddharthan (2006) and Petersen and Osten-
dorf (2007) focus on sentence splitting, while sen-
tence compression systems (Filippova and Strube,
2008a) mainly use the dropping operation. As far
as lexical simplification is concerned, word sub-
stitution is usually done by selecting simpler syn-
onyms from Wordnet based on word frequency
(Carroll et al, 1999).
In this paper, we propose a sentence simplifica-
tion model by tree transformation which is based
1353
on techniques from statistical machine translation
(SMT) (Yamada and Knight, 2001; Yamada and
Knight, 2002; Graehl et al, 2008). Our model in-
tegrally covers splitting, dropping, reordering and
phrase/word substitution. The parameters of our
model can be efficiently learned from complex-
simple parallel datasets. The transformation from
a complex sentence to a simple sentence is con-
ducted by applying a sequence of simplification
operations. An expectation maximization (EM)
algorithm is used to iteratively train our model.
We also propose a method based on monolingual
word mapping which speeds up the training pro-
cess significantly. Finally, a decoder is designed to
generate the simplified sentences using a greedy
strategy and integrates language models.
In order to train our model, we further com-
pile a large-scale complex-simple parallel dataset
(PWKP) from Simple English Wikipedia1 and En-
glish Wikipedia2, as such datasets are rare.
We organize the remainder of the paper as fol-
lows: Section 2 describes the PWKP dataset. Sec-
tion 3 presents our TSM model. Sections 4 and 5
are devoted to training and decoding, respectively.
Section 6 details the evaluation. The conclusions
follow in the final section.
2 Wikipedia Dataset: PWKP
We collected a paired dataset from the English
Wikipedia and Simple English Wikipedia. The
targeted audience of Simple Wikipedia includes
?children and adults who are learning English lan-
guage?. The authors are requested to ?use easy
words and short sentences? to compose articles.
We processed the dataset as follows:
Article Pairing 65,133 articles from Simple
Wikipedia3 and Wikipedia4 were paired by fol-
lowing the ?language link? using the dump files
in Wikimedia.5 Administration articles were fur-
ther removed.
Plain Text Extraction We use JWPL (Zesch et
al., 2008) to extract plain texts from Wikipedia ar-
ticles by removing specific Wiki tags.
Pre-processing including sentence boundary
detection and tokenization with the Stanford
1http://simple.wikipedia.org
2http://en.wikipedia.org
3As of Aug 17th, 2009
4As of Aug 22nd, 2009
5http://download.wikimedia.org
Parser package (Klein and Manning, 2003),
and lemmatization with the TreeTagger (Schmid,
1994).
Monolingual Sentence Alignment As we need
a parallel dataset algned at the sentence level,
we further applied monolingual sentence align-
ment on the article pairs. In order to achieve
the best sentence alignment on our dataset, we
tested three similarity measures: (i) sentence-level
TF*IDF (Nelken and Shieber, 2006), (ii) word
overlap (Barzilay and Elhadad, 2003) and (iii)
word-based maximum edit distance (MED) (Lev-
enshtein, 1966) with costs of insertion, deletion
and substitution set to 1. To evaluate their perfor-
mance we manually annotated 120 sentence pairs
from the article pairs. Tab. 1 reports the precision
and recall of these three measures. We manually
adjusted the similarity threshold to obtain a recall
value as close as possible to 55.8% which was pre-
viously adopted by Nelken and Shieber (2006).
Similarity Precision Recall
TF*IDF 91.3% 55.4%
Word Overlap 50.5% 55.1%
MED 13.9% 54.7%
Table 1: Monolingual Sentence Alignment
The results in Tab. 1 show that sentence-level
TF*IDF clearly outperforms the other two mea-
sures, which is consistent with the results reported
by Nelken and Shieber (2006). We henceforth
chose sentence-level TF*IDF to align our dataset.
As shown in Tab. 2, PWKP contains more
than 108k sentence pairs. The sentences from
Wikipedia and Simple Wikipedia are considered
as ?complex? and ?simple? respectively. Both the
average sentence length and average token length
in Simple Wikipedia are shorter than those in
Wikipedia, which is in compliance with the pur-
pose of Simple Wikipedia.
Avg. Sen. Len Avg. Tok. Len #Sen.Pairs
complex simple complex simple -
25.01 20.87 5.06 4.89 108,016
Table 2: Statistics for the PWKP dataset
In order to account for sentence splitting, we al-
low 1 to n sentence alignment to map one complex
sentence to several simple sentences. We first per-
form 1 to 1 mapping with sentence-level TF*IDF
and then combine the pairs with the same complex
sentence and adjacent simple sentences.
3 The Simplification Model: TSM
We apply the following simplification operations
to the parse tree of a complex sentence: splitting,
1354
dropping, reordering and substitution. In this sec-
tion, we use a running example to illustrate this
process. c is the complex sentence to be simpli-
fied in our example. Fig. 1 shows the parse tree of
c (we skip the POS level).
c: August was the sixth month in the ancient Ro-
man calendar which started in 735BC.
NP VP
S
August was
NPinsixththe
SBAR
NP
NP PP
WHNP S
VP
started PP
in 735BC
ancient calendar whichthe Roman
month
Figure 1: Parse Tree of c
3.1 Splitting
The first operation is sentence splitting, which we
further decompose into two subtasks: (i) segmen-
tation, which decides where and whether to split
a sentence and (ii) completion, which makes the
new split sentences complete.
First, we decide where we can split a sentence.
In our model, the splitting point is judged by the
syntactic constituent of the split boundary word
in the complex sentence. The decision whether a
sentence should be split is based on the length of
the complex sentence. The features used in the
segmentation step are shown in Tab. 3.
Word Constituent iLength isSplit Prob.
?which? SBAR 1 true 0.0016
?which? SBAR 1 false 0.9984
?which? SBAR 2 true 0.0835
?which? SBAR 2 false 0.9165
Table 3: Segmentation Feature Table (SFT)
Actually, we do not use the direct constituent of
a word in the parse tree. In our example, the direct
constituent of the word ?which? is ?WHNP?. In-
stead, we use Alg. 1 to calculate the constituent
of a word. Alg. 1 returns ?SBAR? as the ad-
justed constituent for ?which?. Moreover, di-
rectly using the length of the complex sentence
is affected by the data sparseness problem. In-
stead, we use iLength as the feature which is
calculated as iLength = ceiling( comLengthavgSimLength),
where comLength is the length of the complex
sentence and avgSimLength is the average length
of simple sentences in the training dataset. The
?Prob.? column shows the probabilities obtained
after training on our dataset.
Algorithm 1 adjustConstituent(word, tree)
constituent? word.father;
father ? constituent.father;
while father 6= NULL AND constituent is the most
left child of father do
constituent? father;
father ? father.father;
end while
return constituent;
In our model, one complex sentence can be split
into two or more sentences. Since many splitting
operations are possible, we need to select the most
likely one. The probability of a segmentation op-
eration is calculated as:
P (seg|c) =
?
w:c
SFT (w|c) (1)
where w is a word in the complex sentence c and
SFT (w|c) is the probability of the word w in the
Segmentation Feature Table (SFT); Fig. 2 shows
a possible segmentation result of our example.
NP VP
S
August was
NPinsixththe
SBAR
NP
NP PP
WHNP S
VP
started PP
in 735BC
ancient calendar
which
the Roman
month
Figure 2: Segmentation
The second step is completion. In this step,
we try to make the split sentences complete and
grammatical. In our example, to make the second
sentence ?which started in 735BC? complete and
grammatical we should first drop the border word
?which? and then copy the dependent NP ?the
ancient Roman calendar? to the left of ?started?
to obtain the complete sentence ?the ancient Ro-
man calendar started in 735BC?. In our model,
whether the border word should be dropped or
retained depends on two features of the border
word: the direct constituent of the word and the
word itself, as shown in Tab. 4.
Const. Word isDropped Prob.
WHNP which True 1.0
WHNP which False Prob.Min
Table 4: Border Drop Feature Table (BDFT)
In order to copy the necessary parts to complete
the new sentences, we must decide which parts
should be copied and where to put these parts in
the new sentences. In our model, this is judged
by two features: the dependency relation and the
constituent. We use the Stanford Parser for pars-
ing the dependencies. In our example, the de-
1355
pendency relation between ?calendar? in the com-
plex sentence and the verb ?started? in the second
split sentence is ?gov nsubj?.6 The direct con-
stituent of ?started? is ?VP? and the word ?calen-
dar? should be put on the ?left? of ?started?, see
Tab. 5.
Dep. Const. isCopied Pos. Prob.
gov nsubj VP(VBD) True left 0.9000
gov nsubj VP(VBD) True right 0.0994
gov nsubj VP(VBD) False - 0.0006
Table 5: Copy Feature Table (CFT)
For dependent NPs, we copy the whole NP
phrase rather than only the head noun.7 In our
example, we copy the whole NP phrase ?the an-
cient Roman calendar? to the new position rather
than only the word ?calendar?. The probability of
a completion operation can be calculated as
P (com|seg) =
Y
bw:s
BDFT (bw|s)
Y
w:s
Y
dep:w
CFT (dep).
where s are the split sentences, bw is a border
word in s, w is a word in s, dep is a dependency
of w which is out of the scope of s. Fig. 3 shows
the most likely result of the completion operation
for our example.
NP VP
pt1
August was
NPinsixththe
NP
NP PPpt2
VP
started PP
in 735BC
ancient calendarthe RomanNP
ancient calendarthe Roman
month
Figure 3: Completion
3.2 Dropping and Reordering
We first apply dropping and then reordering to
each non-terminal node in the parse tree from top
to bottom. We use the same features for both drop-
ping and reordering: the node?s direct constituent
and its children?s constituents pattern, see Tab. 6
and Tab. 7.
Constituent Children Drop Prob.
NP DT JJ NNP NN 1101 7.66E-4
NP DT JJ NNP NN 0001 1.26E-7
Table 6: Dropping Feature Table (DFT)
6With Stanford Parser, ?which? is a referent of ?calender?
and the nsubj of ?started?. ?calender? thus can be considered
to be the nsubj of ?started? with ?started? as the governor.
7The copied NP phrase can be further simplified in the
following steps.
Constituent Children Reorder Prob.
NP DT JJ NN 012 0.8303
NP DT JJ NN 210 0.0039
Table 7: Reordering Feature Table (RFT)
The bits ?1? and ?0? in the ?Drop? column indi-
cate whether the corresponding constituent is re-
tained or dropped. The number in the ?Reorder?
column represents the new order for the children.
The probabilities of the dropping and reordering
operations can be calculated as Equ. 2 and Equ. 3.
P (dp|node) = DFT (node) (2)
P (ro|node) = RFT (node) (3)
In our example, one of the possible results is
dropping the NNP ?Roman?, as shown in Fig. 4.
NP VP
pt1
August was
NPinsixththe
NP
NP PPpt2
VP
started PP
in 735BC
ancient calendartheNP
ancient calendarthe
month
Figure 4: Dropping & Reordering
3.3 Substitution
3.3.1 Word Substitution
Word substitution only happens on the termi-
nal nodes of the parse tree. In our model, the
conditioning features include the original word
and the substitution. The substitution for a word
can be another word or a multi-word expression
(see Tab. 8). The probability of a word substitu-
tion operation can be calculated as P (sub|w) =
SubFT (Substitution|Origin).
Origin Substitution Prob.
ancient ancient 0.963
ancient old 0.0183
ancient than transport 1.83E-102
old ancient 0.005
Table 8: Substitution Feature Table (SubFT)
3.3.2 Phrase Substitution
Phrase substitution happens on the non-
terminal nodes and uses the same conditioning
features as word substitution. The ?Origin? con-
sists of the leaves of the subtree rooted at the
node. When we apply phrase substitution on a
non-terminal node, then any simplification opera-
tion (including dropping, reordering and substitu-
tion) cannot happen on its descendants any more
1356
because when a node has been replaced then its
descendants are no longer existing. Therefore, for
each non-terminal node we must decide whether a
substitution should take place at this node or at its
descendants. We perform substitution for a non-
terminal node if the following constraint is met:
Max(SubFT (?|node)) ?
Y
ch:node
Max(SubFT (?|ch)).
where ch is a child of the node. ??? can
be any substitution in the SubFT. The proba-
bility of the phrase substitution is calculated as
P (sub|node) = SubFT (Substitution|Origin).
Fig. 5 shows one of the possible substitution re-
sults for our example where ?ancient? is replaced
by ?old?.
NP VP
pt1
August was
NPinsixththe
NP
NP PPpt2
VP
started PP
in 735BC
old calendartheNP
old calendarthe
month
Figure 5: Substitution
As a result of all the simplification operations,
we obtain the following two sentences: s1 =
Str(pt1)=?August was the sixth month in the old
calendar.? and s2 = Str(pt2)=?The old calendar
started in 735BC.?
3.4 The Probabilistic Model
Our model can be formalized as a direct transla-
tion model from complex to simple P (s|c) multi-
plied by a language model P (s) as shown in Equ.
4.
s = argmax
s
P (s|c)P (s) (4)
We combine the parts described in the previous
sections to get the direct translation model:
P (s|c) =
?
?:Str(?(c))=s
(P (seg|c)P (com|seg)
(5)
?
node
P (dp|node)P (ro|node)P (sub|node)
?
w
(sub|w)).
where ? is a sequence of simplification operations
and Str(?(c)) corresponds to the leaves of a sim-
plified tree. There can be many sequences of op-
erations that result in the same simplified sentence
and we sum up all of their probabilities.
4 Training
In this section, we describe how we train the prob-
abilities in the tables. Following the work of
Yamada and Knight (2001), we train our model
by maximizing P (s|c) over the training corpus
with the EM algorithm described in Alg. 2, us-
ing a constructed graph structure. We develop the
Training Tree (Fig. 6) to calculate P (s|c). P (s|c)
is equal to the inside probability of the root in the
Training Tree. Alg. 3 and Alg. 4 are used to cal-
culate the inside and outside probabilities. We re-
fer readers to Yamada and Knight (2001) for more
details.
Algorithm 2 EM Training (dataset)
Initialize all probability tables using the uniform distribu-
tion;
for several iterations do
reset al cnt = 0;
for each sentence pair < c, s > in dataset do
tt = buildTrainingTree(< c, s >);
calcInsideProb(tt);
calcOutsideProb(tt);
update cnt for each conditioning feature in each
node of tt: cnt = cnt + node.insideProb ?
node.outsideProb/root.insideProb;
end for
updateProbability();
end for
root
sp
sp_res1 sp_res2
dp
ro
mp
mp_res1 mp_res2
sub
mp
mp_res
subsub
dp
ro
mp_res
root
sp
sp_res sp_res
dp
ro
ro_res ro_res
sub
ro_res
subsub
dp
ro
ro_res
sub_res
sub_res sub_res
Figure 6: Training Tree (Left) and Decoding Tree
(Right)
We illustrate the construction of the training
tree with our running example. There are two
kinds of nodes in the training tree: data nodes in
rectangles and operation nodes in circles. Data
nodes contain data and operation nodes execute
operations. The training is a supervised learning
1357
process with the parse tree of c as input and the
two strings s1 and s2 as the desired output. root
stores the parse tree of c and also s1 and s2. sp,
ro, mp and sub are splitting, reordering, mapping
and substitution operations. sp res and mp res
store the results of sp and mp. In our example,
sp splits the parse tree into two parse trees pt1
and pt2 (Fig. 3). sp res1 contains pt1 and s1.
sp res2 contains pt2 and s2. Then dp, ro and mp
are iteratively applied to each non-terminal node
at each level of pt1 and pt2 from top to down.
This process continues until the terminal nodes
are reached or is stopped by a sub node. The func-
tion of mp operation is similar to the word map-
ping operation in the string-based machine trans-
lation. It maps substrings in the complex sentence
which are dominated by the children of the current
node to proper substrings in the simple sentences.
Speeding Up The example above is only one
of the possible paths. We try all of the promis-
ing paths in training. Promising paths are the
paths which are likely to succeed in transform-
ing the parse tree of c into s1 and s2. We select
the promising candidates using monolingual word
mapping as shown in Fig. 7. In this example,
only the word ?which? can be a promising can-
didate for splitting. We can select the promising
candidates for the dropping, reordering and map-
ping operations similarly. With this improvement,
we can train on the PWKP dataset within 1 hour
excluding the parsing time taken by the Stanford
Parser.
We initialize the probabilities with the uniform
distribution. The binary features, such as SFT and
BDFT, are assigned the initial value of 0.5. For
DFT and RFT, the initial probability is 1N! , where
N is the number of the children. CFT is initial-
ized as 0.25. SubFT is initialized as 1.0 for any
substitution at the first iteration. After each itera-
tion, the updateProbability function recalculates
these probabilities based on the cnt for each fea-
ture.
Algorithm 3 calcInsideProb (TrainingTree tt)
for each node from level = N to root of tt do
if node is a sub node then
node.insideProb = P (sub|node);
else if node is a mp OR sp node then
node.insideProb =Qchild child.insideProb;else
node.insideProb =Pchild child.insideProb;end if
end for
Algorithm 4 calcOutsideProb (TrainingTree tt)
for each node from root to level = N of tt do
if node is the root then
node.outsideProb = 1.0;
else if node is a sp res OR mp res node then
{COMMENT: father are the fathers of the current
node, sibling are the children of father excluding
the current node}
node.outsideProb =
P
father
father.outsideProb ?Qsibling sibling.insideProb;else if node is a mp node then
node.outsideProb = father.outsideProb ? 1.0;
else if node is a sp, ro, dp or sub node then
node.outsideProb = father.outsideProb ?
P (sp or ro or dp or sub|node);
end if
end for
August was the sixth in the ancient Roman calendar statedwhich in 735BC
August was the sixth in the old Roman calendar stated in 735BCThe old calendar.
.
.
Complex sentence
Simple sentences
month
month
Figure 7: Monolingual Word Mapping
5 Decoding
For decoding, we construct the decoding tree
(Fig. 6) similarly to the construction of the train-
ing tree. The decoding tree does not have mp op-
erations and there can be more than one sub nodes
attached to a single ro res. The root contains the
parse tree of the complex sentence. Due to space
limitations, we cannot provide all the details of the
decoder.
We calculate the inside probability and out-
side probability for each node in the decoding
tree. When we simplify a complex sentence, we
start from the root and greedily select the branch
with the highest outside probability. For the sub-
stitution operation, we also integrate a trigram
language model to make the generated sentences
more fluent. We train the language model with
SRILM (Stolcke, 2002). All the articles from the
Simple Wikipedia are used as the training corpus,
amounting to about 54 MB.
6 Evaluation
Our evaluation dataset consists of 100 complex
sentences and 131 parallel simple sentences from
PWKP. They have not been used for training.
Four baseline systems are compared in our eval-
uation. The first is Moses which is a state of
the art SMT system widely used as a baseline in
MT community. Obviously, the purpose of Moses
is cross-lingual translation rather than monolin-
1358
gual simplification. The goal of our comparison
is therefore to assess how well a standard SMT
system may perform simplification when fed with
a proper training dataset. We train Moses with the
same part of PWKP as our model. The second
baseline system is a sentence compression sys-
tem (Filippova and Strube, 2008a) whose demo
system is available online.8 As the compression
system can only perform dropping, we further ex-
tend it to our third and fourth baseline systems,
in order to make a reasonable comparison. In our
third baseline system, we substitute the words in
the output of the compression system with their
simpler synonyms. This is done by looking up
the synonyms in Wordnet and selecting the most
frequent synonym for replacement. The word fre-
quency is counted using the articles from Simple
Wikipedia. The fourth system performs sentence
splitting on the output of the third system. This
is simply done by splitting the sentences at ?and?,
?or?, ?but?, ?which?, ?who? and ?that?, and dis-
carding the border words. In total, there are 5
systems in our evaluation: Moses, the MT sys-
tem; C, the compression system; CS, the com-
pression+substitution system; CSS, the compres-
sion+substitution+split system; TSM, our model.
We also provide evaluation measures for the sen-
tences in the evaluation dataset: CW: complex
sentences from Normal Wikipedia and SW: par-
allel simple sentences from Simple Wikipedia.
6.1 Basic Statistics and Examples
The first three columns in Tab. 9 present the ba-
sic statistics for the evaluation sentences and the
output of the five systems. tokenLen is the aver-
age length of tokens which may roughly reflect the
lexical difficulty. TSM achieves an average token
length which is the same as the Simple Wikipedia
(SW). senLen is the average number of tokens in
one sentence, which may roughly reflect the syn-
tactic complexity. Both TSM and CSS produce
shorter sentences than SW. Moses is very close to
CW. #sen gives the number of sentences. Moses,
C and CS cannot split sentences and thus produce
about the same number of sentences as available
in CW.
Here are two example results obtained with our
TSM system.
Example 1. CW: ?Genetic engineering has ex-
panded the genes available to breeders to utilize
in creating desired germlines for new crops.? SW:
8http://212.126.215.106/compression/
?New plants were created with genetic engineer-
ing.? TSM: ?Engineering has expanded the genes
available to breeders to use in making germlines
for new crops.?
Example 2. CW: ?An umbrella term is a word that
provides a superset or grouping of related con-
cepts, also called a hypernym.? SW: ?An umbrella
term is a word that provides a superset or group-
ing of related concepts.? TSM: ?An umbrella term
is a word. A word provides a superset of related
concepts, called a hypernym.?
In the first example, both substitution and drop-
ping happen. TSM replaces ?utilize? and ?cre-
ating? with ?use? and ?making?. ?Genetic? is
dropped. In the second example, the complex sen-
tence is split and ?also? is dropped.
6.2 Translation Assessment
In this part of the evaluation, we use traditional
measures used for evaluating MT systems. Tab. 9
shows the BLEU and NIST scores. We use
?mteval-v11b.pl?9 as the evaluation tool. CW
and SW are used respectively as source and ref-
erence sentences. TSM obtains a very high BLEU
score (0.38) but not as high as Moses (0.55).
However, the original complex sentences (CW)
from Normal Wikipedia get a rather high BLEU
(0.50), when compared to the simple sentences.
We also find that most of the sentences generated
by Moses are exactly the same as those in CW:
this shows that Moses only performs few modi-
fications to the original complex sentences. This
is confirmed by MT evaluation measures: if we
set CW as both source and reference, the BLEU
score obtained by Moses is 0.78. TSM gets 0.55
in the same setting which is significantly smaller
than Moses and demonstrates that TSM is able to
generate simplifications with a greater amount of
variation from the original sentence. As shown in
the ?#Same? column of Tab. 9, 25 sentences gen-
erated by Moses are exactly identical to the com-
plex sentences, while the number for TSM is 2
which is closer to SW. It is however not clear how
well BLEU and NIST discriminate simplification
systems. As discussed in Jurafsky and Martin
(2008), ?BLEU does poorly at comparing systems
with radically different architectures and is most
appropriate when evaluating incremental changes
with similar architectures.? In our case, TSM and
CSS can be considered as having similar architec-
tures as both of them can do splitting, dropping
9http://www.statmt.org/moses/
1359
TokLen SenLen #Sen BLEU NIST #Same Flesch Lix(Grade) OOV% PPL
CW 4.95 27.81 100 0.50 6.89 100 49.1 53.0 (10) 52.9 384
SW 4.76 17.86 131 1.00 10.98 3 60.4 (PE) 44.1 (8) 50.7 179
Moses 4.81 26.08 100 0.55 7.47 25 54.8 48.1 (9) 52.0 363
C 4.98 18.02 103 0.28 5.37 1 56.2 45.9 (8) 51.7 481
CS 4.90 18.11 103 0.19 4.51 0 59.1 45.1 (8) 49.5 616
CSS 4.98 10.20 182 0.18 4.42 0 65.5 (PE) 38.3 (6) 53.4 581
TSM 4.76 13.57 180 0.38 6.21 2 67.4 (PE) 36.7 (5) 50.8 353
Table 9: Evaluation
and substitution. But Moses mostly cannot split
and drop. We may conclude that TSM and Moses
have different architectures and BLEU or NIST is
not suitable for comparing them. Here is an exam-
ple to illustrate this: (CW): ?Almost as soon as he
leaves, Annius and the guard Publius arrive to es-
cort Vitellia to Titus, who has now chosen her as
his empress.? (SW): ?Almost as soon as he leaves,
Annius and the guard Publius arrive to take Vitel-
lia to Titus, who has now chosen her as his em-
press.? (Moses): The same as (SW). (TSM): ?An-
nius and the guard Publius arrive to take Vitellia
to Titus. Titus has now chosen her as his empress.?
In this example, Moses generates an exactly iden-
tical sentence to SW, thus the BLUE and NIST
scores of Moses is the highest. TSM simplifies
the complex sentence by dropping, splitting and
substitution, which results in two sentences that
are quite different from the SW sentence and thus
gets lower BLUE and NIST scores. Nevertheless,
the sentences generated by TSM seem better than
Moses in terms of simplification.
6.3 Readability Assessment
Intuitively, readability scores should be suitable
metrics for simplification systems. We use the
Linux ?style? command to calculate the Flesch
and Lix readability scores. The results are pre-
sented in Tab. 9. ?PE? in the Flesch column stands
for ?Plain English? and the ?Grade? in Lix repre-
sents the school year. TSM achieves significantly
better scores than Moses which has the best BLEU
score. This implies that good monolingual trans-
lation is not necessarily good simplification. OOV
is the percentage of words that are not in the Ba-
sic English BE850 list.10 TSM is ranked as the
second best system for this criterion.
The perplexity (PPL) is a score of text proba-
bility measured by a language model and normal-
ized by the number of words in the text (Equ. 6).
10http://simple.wikipedia.org/wiki/
Wikipedia:Basic_English_alphabetical_
wordlist
PPL can be used to measure how tight the lan-
guage model fits the text. Language models con-
stitute an important feature for assessing readabil-
ity (Schwarm and Ostendorf, 2005). We train a
trigram LM using the simple sentences in PWKP
and calculate the PPL with SRILM. TSM gets the
best PPL score. From this table, we can conclude
that TSM achieves better overall readability than
the baseline systems.
PPL(text) = P (w1w2...wN )?
1
N (6)
There are still some important issues to be con-
sidered in future. Based on our observations, the
current model performs well for word substitution
and segmentation. But the completion of the new
sentences is still problematic. For example, we
copy the dependent NP to the new sentences. This
may break the coherence between sentences. A
better solution would be to use a pronoun to re-
place the NP. Sometimes, excessive droppings oc-
cur, e.g., ?older? and ?twin? are dropped in ?She
has an older brother and a twin brother...?. This
results in a problematic sentence: ?She has an
brother and a brother...?. There are also some er-
rors which stem from the dependency parser. In
Example 2, ?An umbrella term? should be a de-
pendency of ?called?. But the parser returns ?su-
perset? as the dependency. In the future, we will
investigate more sophisticated features and rules
to enhance TSM.
7 Conclusions
In this paper, we presented a novel large-scale par-
allel dataset PWKP for sentence simplification.
We proposed TSM, a tree-based translation model
for sentence simplification which covers splitting,
dropping, reordering and word/phrase substitution
integrally for the first time. We also described an
efficient training method with speeding up tech-
niques for TSM. The evaluation shows that TSM
can achieve better overall readability scores than
a set of baseline systems.
1360
References
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Process-
ing, pages 25?32.
Carroll, John, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999.
Simplifying text for language-impaired readers. In
Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL?99), pages 269?270.
Chandrasekar, R., Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simpli-
fication. In Proceedings of the Sixteenth Inter-
national Conference on Computational Linguistics
(COLING?96), pages 1041?1044.
Filippova, Katja and Michael Strube. 2008a. Depen-
dency tree based sentence compression. In Inter-
national Natural Language Generation Conference
(INLG?08), pages 25?32.
Filippova, Katja and Michael Strube. 2008b. Sen-
tence fusion via dependency graph compression. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 177?185.
Graehl, Jonathan, Kevin Knight, and Jonathan May.
2008. Training tree transducers. In Computational
Linguistics, volume 34, pages 391?427. MIT Press.
Heilman, M. and N. A. Smith. 2009. Question gener-
ation via overgenerating transformations and rank-
ing. Technical Report CMU-LTI-09-013, Language
Technologies Institute, Carnegie Mellon University.
Inui, Kentaro, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplifi-
cation for reading assistance: A project note. In
Proceedings of the 2nd International Workshop on
Paraphrasing: Paraphrase Acquisition and Appli-
cations (IWP), pages 9?16.
Jonnalagadda, Siddhartha and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of the 3rd
International Symposium on Languages in Biology
and Medicine.
Jurafsky, Daniel and James H. Martin. 2008. Speech
and Language Processing (2nd Edition). Prentice
Hall, 2 edition.
Klein, Dan and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NISP?02), pages 3?10.
Knight, Kevin and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In AAAI, pages 703?710.
Levenshtein. 1966. Binary code capable of correct-
ing deletions, insertions and reversals. In Soviet
Physics, pages 707?710.
Nelken, Rani and Stuart M. Shieber. 2006. To-
wards robust context-sensitive sentence alignment
for monolingual corpora. In Proceedings of 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 161?168.
Petersen, Sarah E. and Mari Ostendorf. 2007. Text
simplification for language learners: a corpus anal-
ysis. In Proc. of Workshop on Speech and Language
Technology for Education, pages 69?72.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49.
Schwarm, Sarah E. and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In ACL?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 523?530.
Siddharthan, Advaith. 2002. An architecture for a
text simplification system. In Proceedings of the
Language Engineering Conference (LEC?02), pages
64?71.
Siddharthan, Advaith. 2006. Syntactic simplifica-
tion and text cohesion. In Research on Language
& Computation, volume 4, pages 77?109. Springer
Netherlands, June.
Stolcke, Andreas. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. pages 901?904.
Vickrey, David and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL-08: HLT, pages 344?352, June.
Watanabe, Willian Massami, Arnaldo Candido Junior,
Vin??cius Rodriguez Uze?da, Renata Pontin de Mat-
tos Fortes, Thiago Alexandre Salgueiro Pardo, and
Sandra Maria Alu??sio. 2009. Facilita: reading as-
sistance for low-literacy readers. In SIGDOC ?09:
Proceedings of the 27th ACM international confer-
ence on Design of communication, pages 29?36.
ACM.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL?01: Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, pages 523?530.
Yamada, Kenji and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In ACL?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 303?310.
Zesch, Torsten, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings
of the Sixth International Language Resources and
Evaluation (LREC?08), pages 1646?1652.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li.
2009. Application-driven statistical paraphrase gen-
eration. In Proceedings of ACL-IJCNLP, pages
834?842, Suntec, Singapore, August.
1361
Coling 2010: Poster Volume, pages 54?62,
Beijing, August 2010
Query Expansion based on Pseudo Relevance Feedback
from Definition Clusters
Delphine Bernhard
LIMSI-CNRS
Delphine.Bernhard@limsi.fr
Abstract
Query expansion consists in extending
user queries with related terms in order
to solve the lexical gap problem in Infor-
mation Retrieval and Question Answer-
ing. The main difficulty lies in identi-
fying relevant expansion terms in order
to prevent query drift. We propose to
use definition clusters built from a com-
bination of English lexical resources for
query expansion. We apply the technique
of pseudo relevance feedback to obtain
expansion terms from definition clusters.
We show that this expansion method out-
performs both local feedback, based on
the document collection, and expansion
with WordNet synonyms, for the task of
document retrieval in Question Answer-
ing.
1 Introduction
Question Answering (QA) systems aim at pro-
viding precise answers to user questions. Most
QA systems integrate a document retrieval com-
ponent, which is in charge of retrieving the most
relevant documents or passages for a given user
question. Since document retrieval is performed
in early stages of QA, it is of the uttermost im-
portance that all relevant documents be retrieved,
to limit the loss of relevant answers for further
processing. However, document retrieval systems
have to solve the lexical gap problem, which arises
from alternative ways of conveying the same piece
of information in questions and answers. One of
the solutions proposed to deal with this issue is
query expansion (QE), which consists in extend-
ing user queries with related terms.
This paper describes a new method for us-
ing lexical-semantic resources in query expansion
with a focus on QA applications. While some
research has been devoted to using explicit se-
mantic relationships for QE, such as synonymy
or hypernymy, with rather disappointing results
(Voorhees, 1994), we focus on the usefulness of
textual and unstructured dictionary definitions for
question expansion. Definitions extracted from
seven English lexical resources are first grouped
to obtain definition clusters, which capture redun-
dancies and sense mappings across resources. Ex-
pansion terms are extracted from these definition
clusters using pseudo relevance feedback: we first
retrieve the definition clusters which are most re-
lated to the user query, and then extract the most
relevant terms from these definition clusters to ex-
pand the query.
The contributions of this work are as fol-
lows: (i) we build definition clusters across seven
different lexical resources for English, (ii) we
thoroughly compare different question expansion
methods using local and global feedback, and (iii)
we address both the lexical gap and question am-
biguity problems by integrating expansion and
disambiguation in one and the same step.
In the next section, we describe related work.
In Section 3, we describe our method for acquir-
ing definition clusters from seven English lexical
resources. In Section 4, we detail query expan-
sion methods. We present experimental results in
Section 5 and conclude in Section 6.
2 Related Work
Query expansion attempts to solve the vocabu-
lary mismatch problem by adding new semanti-
cally related terms to the query. The goal is to
increase recall by retrieving more relevant doc-
uments. Two types of query expansion methods
are usually distinguished (Manning et al, 2008):
global techniques, which do not take the results
obtained for the original query into account, and
54
local techniques, which expand the query based
on an analysis of the documents returned. Local
methods are also known as relevance feedback.
A first type of global QE methods relies on
external hand-crafted lexical-semantic resources
such as WordNet. While expansion based on ex-
ternal resources is deemed more efficient than ex-
pansion relying on relevance feedback, it also has
to tackle problems of semantic ambiguity, which
explains why local analysis has been shown to
be generally more effective than global analysis
(Xu and Croft, 1996). However, recent work by
Fang (2008) has demonstrated that global expan-
sion based on WordNet and co-occurrence based
resources can lead to performance improvement
in an axiomatic model of information retrieval.
Corpus-derived co-occurrence relationships are
also exploited for query expansion. Qiu and Frei
(1993) build a corpus-based similarity thesaurus
using the method described in Schu?tze (1998) and
expand a query with terms which are similar to the
query concept based on the similarity thesaurus.
Song and Bruza (2003) construct vector represen-
tations for terms from the target document collec-
tion using the Hyperspace Analogue to Language
(HAL) model (Lund and Burgess, 1996). The
representations for all the terms in the query are
then combined by a restricted form of vector ad-
dition. Finally, expansion terms are derived from
this combined vector by information flow.
Quasi-parallel monolingual corpora have been
recently employed for query expansion, using sta-
tistical machine translation techniques. Expan-
sion terms are acquired by training a transla-
tion model on question-answer pairs (Riezler et
al., 2007) or query-snippets pairs (Riezler et al,
2008) and by extracting paraphrases from bilin-
gual phrase tables (Riezler et al, 2007).
The main difficulty of QE methods lies in se-
lecting the most relevant expansion terms, espe-
cially when the query contains ambiguous words.
Moreover, even if the original query is not am-
biguous, it might become so after expansion. Re-
cent attempts at integrating word sense disam-
biguation (WSD) in IR within the CLEF Robust
WSD track1 have led to mixed results which show
1http://ixa2.si.ehu.es/clirwsd/
that in most cases WSD does not improve perfor-
mance of monolingual and cross-lingual IR sys-
tems (Agirre et al, 2009). For query expansion
based on translation models, ambiguity problems
are solved by a language model trained on queries
(Riezler et al, 2008), in order to select the most
likely expansion terms in the context of a given
query.
In this article, we propose to integrate disam-
biguation and expansion in one and the same
step by retrieving expansion terms from defini-
tion clusters acquired by combining several En-
glish lexical resources.
3 Acquisition of Definition Clusters
Dictionary definitions constitute a formidable re-
source for Natural Language Processing. In con-
trast to explicit structural and semantic relations
between word senses such as synonymy or hy-
pernymy, definitions are readily available, even
for less-resourced languages. Moreover, they can
be used for a wide variety of tasks, ranging from
word sense disambiguation (Lesk, 1986), to pro-
ducing multiple-choice questions for educational
applications (Kulkarni et al, 2007) or synonym
discovery (Wang and Hirst, 2009). However, all
resources differ in coverage and word sense gran-
ularity, which may lead to several shortcomings
when using a single resource. For instance, the
sense inventory in WordNet has been shown to
be too fine-grained for efficient word sense dis-
ambiguation (Navigli, 2006; Snow et al, 2007).
Moreover, gloss and definition-based measures of
semantic relatedness which rely on the overlap be-
tween the definition of a target word and its dis-
tributional context (Lesk, 1986) or the definition
of another concept (Banerjee and Pedersen, 2003)
yield low results when the definitions provided are
short and do not overlap sufficiently.
As a consequence, we propose combining lex-
ical resources to alleviate the coverage and gran-
ularity problems. To this aim, we automatically
build cross-resource sense clusters. The goal of
our approach is to capture redundancy in several
resources, while improving coverage over the use
of a single resource.
55
3.1 Resources
In order to build definition clusters, we used the
following seven English resources:
WordNet We used WordNet 3.0, which con-
tains 117,659 synset definitions.2
GCIDE The GCIDE is the GNU version of the
Collaborative International Dictionary of English,
derived from Webster?s 1913 Revised Unabridged
Dictionary. We used a recent XML version of this
resource,3 from which we extracted 196,266 defi-
nitions.
English Wiktionary and Simple English Wik-
tionary Wiktionary is a collaborative online
dictionary, which is also available in a simpler
English version targeted at children or non-native
speakers. We used the English Wiktionary dump
dated August 16, 2009 and the Simple English
Wiktionary dump dated December 9, 2009. The
English Wiktionary comprises 245,078 defini-
tions, while the Simple English Wiktionary totals
11,535 definitions.
English Wikipedia and Simple English
Wikipedia Wikipedia is a collaborative online
encyclopedia. As Wiktionary, it provides a
Simple English version. We used the Medi-
awiki API to extract 152,923 definitions from
the English Wikipedia4 and 53,993 definitions
from the Simple English Wikipedia. Since full
Wikipedia articles can be very long in comparison
to the other resources, we only retrieved the first
sentence of each page to constitute the definition
database, following (Kazama and Torisawa,
2007).
OmegaWiki OmegaWiki is a collaborative
multilingual dictionary based on a relational
database. We used the SQL database dated De-
cember 17, 2009,5 comprising 29,179 definitions.
2Statistics obtained from http://wordnet.
princeton.edu/wordnet/man/wnstats.7WN.
html
3Retrieved from http://rali.iro.umontreal.
ca/GCIDE/
4As we mainly aimed at capturing the redundancy across
resources, we only extracted definitions for the Wikipedia
terms which were also found in the GCIDE, Omegawiki,
Wiktionary or Simple English Wikipedia.
5Retrieved from http://omegawiki.org/
3.2 Definition Clustering
In order to cluster definitions, we first build a
definition graph: each node in the graph corre-
sponds to a definition in one of our input resources
and two definition nodes are linked if they de-
fine the same term and their definitions are similar
enough. Links are weighted by the cosine similar-
ity of the definition nodes. To compute the cosine
similarity, we stem the definition words with the
Porter Stemmer and remove stop words. More-
over, we weigh words with their tf.idf value in the
definitions. Document frequency (df ) counts are
derived from the definitions contained in all our
resources.
Definition clusters are identified with a com-
munity detection algorithm applied to the defini-
tion graph. Communities correspond to groups of
nodes with dense interconnections: in our case,
we aim at retrieving groups of related definitions.
We used the algorithm proposed by Blondel et al
(2008), based on modularity optimisation.6 The
modularity function measures the quality of a di-
vision of a graph into communities (Newman and
Girvan, 2004).
In order to increase the precision of clustering,
we remove edges from the graph whose cosine
value is lower than a given threshold.
3.3 Evaluation of Definition Clusters
We built a gold-standard by manually grouping
the definitions contained in our source resources
for 20 terms from the Basic English Word List,7
totalling 726 definitions, grouped in 321 classes.
We evaluated the definition clusters in terms of
clustering purity (Manning et al, 2008), which is
a classical evaluation measure to measure cluster-
ing quality. Purity is defined as follows:
purity(?, C) = 1N
?
k
max
j
|?k ? cj | (1)
where N is the number of clustered definitions,
? = {?1, ?2, . . . , ?K} is the set of definition
6We used its Python implementation by Thomas
Aynaud, available at http://perso.crans.org/
aynaud/communities/community.py [Visited on
October 26, 2009].
7http://en.wiktionary.org/wiki/
Appendix:Basic_English_word_list
56
Resource Definition
WordNet an arc of colored light in the sky caused by refraction of the sun?s rays by
rain
Gcide A bow or arch exhibiting, in concentric bands, the several colors of the
spectrum, and formed in the part of the hemisphere opposite to the sun by
the refraction and reflection of the sun?s rays in drops of falling rain.
Simple Wikipedia A rainbow is an arc of color in the sky that you can see when the sun shines
through falling rain.
Simple Wiktionary The arch of colours in the sky you can see in the rain when the sun is at
your back.
Table 1: Excerpt from a definition cluster.
clusters obtained, wk is the set of definitions in
cluster k, C = {c1, c2, . . . , cJ} is the set of def-
inition families expected and cj is the set of defi-
nitions in family j.
We also report the amount of clusters obtained
for each cosine threshold value. The evaluation
results are detailed in Table 2.
Cosine threshold Purity # Clusters
0.0 0.363 73
0.1 0.464 135
0.2 0.644 234
0.3 0.848 384
0.4 0.923 458
0.5 0.957 515
Table 2: Evaluation results for definition cluster-
ing.
Overall, the results which account for the best
compromise between purity and cluster count are
obtained for a threshold of 0.3: for this threshold,
we obtain 384 clusters, which is closest to the ex-
pected value of 321 classes. The purity obtained
for this cosine threshold is very close to the val-
ues obtained by Kulkarni et al (2007), who clus-
tered definitions extracted from only two source
dictionaries and report a purity of 0.88 for their
best results. In total we obtain 307,570 definition
clusters. Table 1 displays an excerpt from one of
the definition clusters obtained.
4 Query Expansion Methods
In this section, we describe the methods used for
performing query expansion. We first describe
two simple baseline methods, one based on local
feedback, the other based on WordNet. Then, we
detail our method relying on the definition clusters
previously described.
4.1 Query Expansion based on Local
Feedback
In order to perform local feedback based on the
document collection, we used the pseudo rel-
evance feedback methods implemented in the
Terrier information retrieval platform (Ounis et
al., 2007): Bo1 (Bose-Einstein 1), Bo2 (Bose-
Einstein 2) and KL (Kullback-Leibler). These
methods extract informative terms from the top-
ranked documents retrieved using the original
query and use them for query expansion.
4.2 Query Expansion based on WordNet
Synonyms
As a second baseline for query expansion, we
expand the query terms with their synonyms ex-
tracted from WordNet. For each query term t,
we retrieve its WordNet synsets and keep the cor-
responding synset members as expansion terms.8
We weigh the expansion terms in each synset by
the frequency score provided in WordNet, which
indicates how often the query term t occurs with
the corresponding sense. In the rest of the paper,
this method is referred to as WN-synonyms.
The expansion terms obtained using WN-
synonyms are further reweighted using Rocchio?s
beta formula which computes the weight qtw of
8We use NLTK (http://www.nltk.org/) to access
WordNet.
57
query term t as follows (Rocchio, 1971; Macdon-
ald et al, 2005):
qtw = qtfqtfmax
+ ? w(t)wmax(t)
(2)
where qtf is the frequency of term t in the query,
qtfmax is the maximum query term frequency
among the query terms, w(t) is the expansion
weight of t, detailed in Equation 3, and wmax(t)
is the maximum w(t) of the expansion terms. In
all our experiments, ? is set to 0.4, which is the
default value used in Terrier.
Given this formula, if an original query term oc-
curs among the expansion terms, its weight in the
expanded query increases. For expansion terms
which do not occur in the original query, qtf = 0.
This formula has been proposed in the setting
of pseudo relevance feedback, where expansion
terms are chosen based on the top documents re-
trieved for the original query. However, in our
WN-synonyms setting, one and the same expan-
sion term might be obtained from different origi-
nal query terms with different weights. It is there-
fore necessary to obtain a global similarity weight
for one expansion term with respect to the whole
query. Following Qiu and Frei (1993), we define
w(t) as:
w(t) =
?
ti?q qtfi ? s(t, ti)?
ti?q qtfi
(3)
where q is the original query and s(t, ti) is the
similarity between expansion term t and query
term ti, i.e., the frequency score in WordNet.
For final expansion, we keep the top T terms
with the highest expansion weight.
4.3 Query Expansion Based on Definition
Clusters
In order to use definition clusters (DC) for query
expansion, we first use Terrier to index the clus-
ters which obtained the best overall results in our
evaluation of definition clustering, corresponding
to a cosine threshold of 0.3.9 For each cluster, we
index both the definitions and the list of terms they
define, which makes it possible to include syn-
onyms or Wikipedia redirects in the index.
9We used the 2.2.1 version of Terrier, downloadable from
http://terrier.org/
For a given question, we retrieve the top D def-
inition clusters: the retrieval of definition clusters
is based on all the question terms, and thus en-
ables indirect contextual word sense disambigua-
tion. Then, we extract expansion terms from these
clusters using pseudo relevance feedback (PRF)
as implemented in Terrier. The top T most in-
formative terms are retrieved from the top D def-
inition clusters retrieved and used for expansion.
The expansion terms are weighted using the KL
(Kullback-Leibler) term weighting model in Ter-
rier. We chose this particular weighting model, as
it yielded the best results for local feedback (see
Table 3).
We name this method DC-PRF.
5 Experiments
In this section, we describe the experimental re-
sults obtained for the query expansion methods
presented in the previous section. We used the Mi-
crosoft Research Question-Answering Corpus10
(MSRQA) as our evaluation dataset.
5.1 Microsoft Research Question-Answering
Corpus (MSRQA)
MSRQA provides a fully annotated set of ques-
tions and answers retrieved from the Encarta 98
encyclopedia. The Encarta corpus contains
32,715 articles, ranging from very short (3 tokens)
to very long (59,798 tokens). QA systems usu-
ally split documents into smaller passages. We
have therefore segmented the Encarta articles into
smaller parts representing subsections of the orig-
inal article, using a regular expression for iden-
tifying section headers in the text. As a result,
the dataset comprises 61,604 documents, with a
maximum of 2,730 tokens. The relevance judge-
ments provided comprise the document id as well
as the sentences (usually one) containing the an-
swer. We processed these sentence level relevance
judgements to obtain judgements for documents:
a document is considered as relevant if it contains
an exact answer sentence. Overall, we obtained
relevance judgements for 1,098 questions.
10Downloadable from http://research.
microsoft.com/en-us/downloads/
88c0021c-328a-4148-a158-a42d7331c6cf/
58
All questions Easy questions Medium questions Hard questions
Expansion MAP MRR MAP MRR MAP MRR MAP MRR
none 0.2257 0.2681 0.2561 0.3125 0.1720 0.1965 0.1306 0.1392
Terrier-Bo1 0.2268 0.2674 0.2625 0.3157 0.1642 0.1903 0.1222 0.1240
Terrier-Bo2 0.2234 0.2602 0.2581 0.3077 0.1660 0.1872 0.1126 0.1146
Terrier-KL 0.2274 0.2684 0.2635 0.3167 0.1644 0.1915 0.1220 0.1236
WN-synonyms 0.2260 0.2687 0.2536 0.3098 0.1785 0.2055 0.1254 0.1260
DC-PRF 0.2428 0.2929 0.2690 0.3361 0.2004 0.2294 0.1385 0.1472
+7.6% +9.2% +5.0% +7.5% +16.5% +16.7% +6.0% +5.7%
DC-PRF 0.2361 0.2796 0.2625 0.3184 0.1928 0.2213 0.1389 0.1484+ Terrier KL
Table 3: Experimental results. The performance gaps between the DC-PRF and the baseline retrieval
models without expansion (none), Terrier-KL and WN-synonyms are statistically significant (two-tailed
paired t-test, p < 0.05), except for hard questions and for the MAP comparison with Terrier-KL for
easy questions. We also report the improvement percentage.
Based on the annotations available in the
MSRQA dataset, we further distinguish three
question types:
? easy questions, which have at least one an-
swer with a strong match (two or more query
terms in the answer).
? medium questions, which have no strong
match answer, but at least an answer with a
weak match (one query term in the answer).
? hard questions, which have neither a strong
nor a weak match answer, but only answers
which contain no query terms, and at the
best synonyms and derivational morpholog-
ical variants for query terms.
Overall, the evaluation dataset comprises 651
easy questions, 397 medium questions and 64
hard questions (some of these questions have no
exact answer).
5.2 Results
As our baseline, we use the BB2 (Bose-Einstein
model for randomness) retrieval model in Terrier.
We varied the values for the parameters T (num-
ber of expansion terms) and D (number of ex-
pansion documents) and used the settings yield-
ing the best evaluation results. For the PRF meth-
ods implemented in Terrier, the default settings
(T=10, D=3) worked best; for DC-PRF, we used
T=20 and D=40. Finally, for WN-synonyms we
used T=10. We also combined both DC-PRF
and Terrier-KL by first applying DC-PRF expan-
sion and then using local Terrier-KL feedback on
the retrieved documents (DC-PRF + Terrier KL).
Prior to retrieval, all questions are tokenised and
part-of-speech tagged using Xerox?s Incremental
Parser XIP (A??t-Mokhtar et al, 2002). Moreover,
we retrieve 100 documents for each question and
stem the Encarta document collection. The results
shown in Table 3 are evaluated in terms of Mean-
Average Precision (MAP) and Mean Reciprocal
Rank (MRR). Table 4 provides examples of the
top 5 expansion terms obtained for each expan-
sion method.
The DC-PRF expansion method performs best
overall, as well as for easy and medium question
types. For medium questions, DC-PRF leads to
an increase of 16.5% in MAP and 16.7% in MRR,
with respect to the ?none? baseline. Local feed-
back methods, such as Terrier-KL, only bring mi-
nor improvements for easy questions, but lead to
slightly lower results for medium and hard ques-
tions. This might be due to the small size of the
document collection, which therefore lacks redun-
dancy. The simple baseline expansion method
based on WordNet leads to very slight improve-
ments for medium questions over the setting with-
out expansion. The combination of DC-PRF and
Terrier-KL leads to lower results than using only
59
Terrier-KL WN-synonyms DC-PRF
12: Are there UFOs?
sight ? unidentifi ? report ?
object ? fly
flying ? unidentified ? object
? UFO ? saucer
unidentified ? ufo ? flying ?
ufology ? objects
104: What is the most deadly insect in the world?
speci ? plant ? feed ? anim ?
liv
cosmos ? creation ? existence
? macrocosm ? universe
nightshade ? belladonna ?
mortal ? death ? lethal
107: When was the little ice age
drift ? glacial ? ago ? sheet ?
million
small ? slight ? historic ?
period ? water
floe ? period ? glacial ? cold ?
interglacial
449: How does a TV screen get a picture from the air waves?
light ? beam ? televi ?
electron ? signal
moving ? ridge ? image ? icon
? ikon
television ? movie ? image ?
motion ? door
810: Do aliens really exist?
sedition ? act ? govern ?
deport ? see
live ? subsist ? survive ?
alienate ? extraterrestrial
alien ? extraterrestrial ?
monsters ? dreamworks ?
animation
Table 4: Expansion examples. The expansion terms produced by Terrier-KL are actually stemmed, as
they are retrieved from a stemmed index.
DC-PRF, except for hard questions, for which the
combination brings a very slight improvement.
The expansion samples provided in Table 4 ex-
emplify the query drift problem of local feed-
back methods (Terrier-KL): for question 810, ex-
pansion terms focus on the ?foreigner? sense of
alien rather than on the ?extraterrestrial? sense.
The WN-synonyms method suffers from the prob-
lem of weighting synonyms, and mainly focuses
on synonyms for the most frequent term of the
question, e.g. ?world? in question 104. Inter-
estingly, the DC-PRF method accounts for neol-
ogisms, such as ?ufology? which can be found
in new collaboratively constructed resources such
as Wikipedia or Wiktionary, but not in WordNet.
This is made possible by the combination of di-
versified resources. It is also able to provide en-
cyclopedic knowledge, such as ?dreamworks? and
?animation? in question 810, referring to the fea-
ture film ?Monsters vs. Aliens?.
The DC-PRF method also has some limitations.
Even though the expansion terms ?dreamworks?
and ?animation? correspond to the intended mean-
ing of the word ?alien? in question 810, they nev-
ertheless might introduce some noise in the re-
trieval. Some other cases exemplify slight drifts in
meaning from the query: in question 104, the ex-
pansion terms ?nightshade? and ?belladonna? re-
fer to poisonous plants and not insects; ?deadly
nightshade? is actually the other name of the ?bel-
ladonna?. Similarly, in question 449, the ex-
pansion term ?door? is obtained, in relation to
the word ?screen? in the question (as in ?screen
door?). This might be due to the fact that the terms
defined by the definition clusters are indexed as
well, leading to a high likelihood of retrieving
syntagmatically related terms for multiword ex-
pressions. In future work, it might be relevant
to experiment with different indexing schemes for
definition clusters, e.g. indexing only the defini-
tions, or adding the defined terms to the index only
if they are not present in the definitions.
6 Conclusions and Future Work
In this paper, we presented a novel method for
query expansion based on pseudo relevance feed-
back from definition clusters. The definition clus-
ters are built across seven different English lexical
resources, in order to capture redundancy while
improving coverage over the use of a single re-
source. The expansions provided by feedback
from definition clusters lead to a significant im-
60
provement of the retrieval results over a retrieval
setting without expansion.
In the future, we would like to further amelio-
rate definition clustering and incorporate other re-
sources, e.g. resources for specialised domains.
Moreover, we have shown that query expansion
based on definition clusters is most useful when
applied to medium difficulty questions. We there-
fore consider integrating automatic prediction of
query difficulty to select the best retrieval method.
Finally, we would like to evaluate the method pre-
sented in this paper for larger datasets.
Acknowledgments
This work has been partially financed by OSEO
under the Qu?ro program.
References
Agirre, Eneko, Giorgio M. Di Nunzio, Thomas Mandl,
and Arantxa Otegi. 2009. CLEF 2009 Ad Hoc
Track Overview: Robust - WSD Task. In Working
Notes for the CLEF 2009 Workshop, Corfu, Greece.
A??t-Mokhtar, Salah, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: in-
cremental deep parsing. Natural Language Engi-
neering, 8(2-3):121?144.
Banerjee, Satanjeev and Ted Pedersen. 2003. Ex-
tended Gloss Overlaps as a Measure of Semantic
Relatedness. In Proceedings of the Eighteenth In-
ternational Joint Conference on Artificial Intelli-
gence, pages 805?810.
Blondel, Vincent D., Jean-Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Journal
of Statistical Mechanics: Theory and Experiment,
2008(10):P10008+, October.
Fang, Hui. 2008. A Re-examination of Query Ex-
pansion Using Lexical Resources. In Proceedings
of ACL-08: HLT, pages 139?147, Columbus, Ohio,
June.
Kazama, Jun?ichi and Kentaro Torisawa. 2007.
Exploiting Wikipedia as External Knowledge for
Named Entity Recognition. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 698?707.
Kulkarni, Anagha, Jamie Callan, and Maxine Eske-
nazi. 2007. Dictionary Definitions: The Likes
and the Unlikes. In Proceedings of Speech and
Language Technology in Education (SLaTE2007),
pages 73?76.
Lesk, Michael. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to
tell a pine cone from an ice cream cone. In SIG-
DOC ?86: Proceedings of the 5th annual interna-
tional conference on Systems documentation, pages
24?26.
Lund, Kevin and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments & Computers, 28(2):203?208.
Macdonald, Craig, Ben He, Vassilis Plachouras, and
Iadh Ounis. 2005. University of Glasgow at
TREC 2005: Experiments in Terabyte and Enter-
prise Tracks with Terrier. In Proceedings of the 14th
Text REtrieval Conference (TREC 2005), Gaithers-
burg, MD, USA.
Manning, Christopher D., Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Navigli, Roberto. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In ACL-44: Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 105?112.
Newman, M. E. J. and M. Girvan. 2004. Finding and
evaluating community structure in networks. Phys-
ical review E, 69.
Ounis, Iadh, Christina Lioma, Craig Macdonald, and
Vassilis Plachouras. 2007. Research Directions in
Terrier: a Search Engine for Advanced Retrieval
on the Web. Novatica/UPGRADE Special Issue on
Web Information Access, Ricardo Baeza-Yates et al
(Eds), Invited Paper.
Qiu, Yonggang and Hans-Peter Frei. 1993. Concept
based query expansion. In SIGIR ?93: Proceedings
of the 16th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 160?169.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical Machine Translation for Query Ex-
pansion in Answer Retrieval. In Proceedings of
the 45th Annual Meeting of the Association of
Computational Linguistics, pages 464?471, Prague,
Czech Republic, June.
Riezler, Stefan, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Im-
proved Query Expansion. In Proceedings of the
61
22nd International Conference on Computational
Linguistics (Coling 2008), pages 737?744, Manch-
ester, UK, August.
Rocchio, J., 1971. The SMART Retrieval System,
chapter Relevance Feedback in Information Re-
trieval, pages 313?323.
Schu?tze, Hinrich. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1):97?
123.
Snow, Rion, Sushant Prakash, Daniel Jurafsky, and
Andrew Y. Ng. 2007. Learning to Merge Word
Senses. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1005?
1014, Prague, Czech Republic, June.
Song, Dawei and Peter D. Bruza. 2003. Towards con-
text sensitive information inference. Journal of the
American Society for Information Science and Tech-
nology (JASIST), 54(4):321?334.
Voorhees, Ellen M. 1994. Query expansion using
lexical-semantic relations. In SIGIR ?94: Proceed-
ings of the 17th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 61?69.
Wang, Tong and Graeme Hirst. 2009. Extracting Syn-
onyms from Dictionary Definitions. In Proceedings
of RANLP 2009.
Xu, Jinxi and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In SIGIR
?96: Proceedings of the 19th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 4?11.
62
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 487?492,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ANNLOR: A Na??ve Notation-system for Lexical Outputs Ranking
Anne-Laure Ligozat
LIMSI-CNRS/ENSIIE
rue John von Neumann
91400 Orsay, France
annlor@limsi.fr
Cyril Grouin
LIMSI-CNRS
rue John von Neumann
91400 Orsay, France
cyril.grouin@limsi.fr
Anne Garcia-Fernandez
CEA-LIST
NANO INNOV, Bt. 861
91191 Gif-sur-Yvette cedex, France
anne.garcia-fernandez@cea.fr
Delphine Bernhard
LiLPa, Universite? de Strasbourg
22 rue Rene? Descartes, BP 80010
67084 Strasbourg cedex, France
dbernhard@unistra.fr
Abstract
This paper presents the systems we developed
while participating in the first task (English
Lexical Simplification) of SemEval 2012. Our
first system relies on n-grams frequencies
computed from the Simple English Wikipedia
version, ranking each substitution term by de-
creasing frequency of use. We experimented
with several other systems, based on term fre-
quencies, or taking into account the context in
which each substitution term occurs. On the
evaluation corpus, we achieved a 0.465 score
with the first system.
1 Introduction
In this paper, we present the methods we used while
participating to the Lexical Simplification task at Se-
mEval 2012 (Specia et al, 2012). We experimented
with several methods:
? using word frequencies or other statistical fig-
ures from the BNC corpus, Google Books
NGrams, the Simple English Wikipedia, and
results from the Bing search engine (with/with-
out lemmatization);
? using association measures for a word and its
context based on language models (with/with-
out inflection);
? making a combination of previous methods
with SVMRank.
Depending on the results obtained on the training
corpus, we chose the methods that seemed to best fit
the data.
2 Task description
2.1 Presentation
The Lexical Simplification task aimed at determin-
ing the degree of simplicity of words. The inputs
given were a short text, in which a target word was
chosen, and several substitutes for the target word
that fit the context.
An example of a short text follows; the target
word is ?outdoor?, and other words of this text will
be considered as the context of this target word.
< i n s t a n c e i d =? 270 ?>
<c o n t e x t>With t h e growing demand f o r
t h e s e f i n e g a r de n f u r n i s h i n g s ,
t h e y found i t n e c e s s a r y t o d e d i c a t e
a p o r t i o n o f t h e i r b u s i n e s s t o
<head>o u t d o o r< / head> l i v i n g and
p a t i o f u r n i s h i n g s .< / c o n t e x t>
< / i n s t a n c e>
The substitutes given for this target word were
the following: ?alfresco;outside;open-air;outdoor;?.
The objective was to order these words by descend-
ing simplicity.
2.2 Corpora
Two corpora were provided: the trial corpus with
development examples, and the test corpus for eval-
uation.
In the trial corpus, a gold standard was also given.
For the previous example, it stated that the substi-
tutes had to be in the following order: ?outdoor
open-air outside, alfresco?, ?outdoor? being consid-
ered as the simplest substitute, and ?outside? and
?alfresco? being considered as the less simple ones.
487
Three baselines have been given by the organiz-
ers: the first one is a simple randomization of the
substitute list, the second one keeps the substitute
list as it is, and the third one (called ?simple fre-
quency?) relies on the use of the Google Web 1T
corpus.
3 Preprocessing
3.1 Corpus constitution
In order to use machine-learning based approaches,
we produced two sub-corpora respectively for the
training and evaluation stages from the trial corpus.
The training sub-corpus is used to develop and tune
the systems we produced while the evaluation sub-
corpus is used to evaluate the results of these sys-
tems.
For each set from the SemEval trial corpus, if the
set is composed of at least eight lexical elements be-
longing to the same morpho-syntactic category (e.g.,
a set with at least eight instances of ?bright? as an
adjective), we extracted three instances from this
set for the evaluation sub-corpus, the remaining in-
stances being part of the training sub-corpus. If the
set is composed of less than eight instances, all in-
stances are used in the training sub-corpus. We also
kept two complete sets of lexical elements for the
evaluation sub-corpus in order to test the robustness
of our methods on new lexical elements that have not
been studied yet. This distribution allows us to bene-
fit from a repartition between training and evaluation
sub-corpora where the instances ratio is of 66/33%.
3.2 Corpus cleaning
While studying the trial corpus, we noticed that the
texts were not always in plain text, and in particular
contained HTML entities. As some of our methods
used the context of target words, we decided to cre-
ate a cleaner version of the corpora. For the dash and
quote HTML entities (&#8211; &#8220; etc.), we
replaced each entity by its refering symbol. When
replacing the apostrophe HTML entity (&apos;), we
decided to link the abbreviated token with the previ-
ous one because all n-grams methods worked better
with abbreviated terms of one token-length (don?t)
than two token-length (do n?t) (see section 5).
3.3 Inflection
In some sentences, the target words are inflected, but
the substitutes are given in their lemmatized forms.
For example, one of the texts was the following :
<c o n t e x t>In f a c t , d u r i n g a t l e a s t s i x
d i s t i n c t p e r i o d s i n Army h i s t o r y
s i n c e World War I , l a c k o f t r u s t and
c o n f i d e n c e i n s e n i o r l e a d e r s c au se d
t h e so?c a l l e d b e s t and
<head>b r i g h t e s t< / head> t o l e a v e t h e
Army i n d r o v e s .< / c o n t e x t>
For this text and target word, the proposed sub-
stitutes were ?capable; most able; motivated; in-
telligent; bright; clever; sharp; promising?, and if
we want to test the simplicity of the words in con-
text, for example with a 2-words left context, we
will obtain unlikely phrases such as ?best and capa-
ble? (which should be ?best and most capable?). We
thus used several resources to get inflected forms of
words: we used the Lingua::EN::Conjugate and Lin-
gua::EN::Inflect Perl modules, which give inflected
forms of verbs and plural forms of nouns, as well as
the English dictionary of inflected forms DELA,1 to
validate the Perl modules outputs if necessary, and
get comparatives and superlatives of adjectives, and
a list of irregular English verbs, also to validate the
Perl modules outputs.
4 Simple English Wikipedia based system
Our best system, called ANNLOR-simple, is based
on Simple English Wikipedia frequencies. As the
challenge focused on substitutions performed by
non-native English speakers, we tried to use linguis-
tic resources that best fit this kind of data. In this
way, we made the hypothesis that training our sys-
tem on documents written by or written for non-
native English speakers would be useful.
The use of the Simple English version from
Wikipedia seems to be a good solution as it is tar-
geted at people who do not have English as their
mother tongue. Our hypothesis seems to be correct
due to the results we obtained. Morevover, the Sim-
ple English Wikipedia has been used previously in
work on automatic text simplification, e.g. (Zhu et
al., 2010).
1http://infolingu.univ-mlv.fr/
DonneesLinguistiques/Dictionnaires/
telechargement.html
488
First, we produced a plain text version of the Sim-
ple English Wikipedia. We downloaded the dump
dated February 27, 2012 and extracted the textual
contents using the wikipedia2text tool.2 The
final plaintext file contains approximately 10 million
words.
We extracted word n-grams (n ranging from 1 to
3) and their frequencies from this corpus thanks to
the Text-NSP Perl module 3 and its count.pl pro-
gram, which produces the list of n-grams of a docu-
ment, with their frequencies. Table 1 gives the num-
ber of n-grams produced.
Table 1: Number of distinct n-grams extracted from the
Simple English Wikipedia
n #n-grams
1 301,718
2 2,517,394
3 6,680,906
1 to 3 9,500,018
Some of these n-grams are invalid, and result
from problems when extracting plain text from
Wikipedia, such as ?27|ufc 1?, which corresponds
to wiki syntax. As we would not find these n-grams
in our substitution lists, we did not try to clean the
n-gram data.
Then, we ranked the possible substitutes of a lex-
ical item according to these frequencies, in descend-
ing order. For example, for the substitution list (in-
telligent, bright, clever, smart), the respective fre-
quencies in the Simple English Wikipedia are (206,
475, 141, 201), and the substitutes will be ranked in
descending frequencies: (bright, intelligent, smart,
clever).
Several tests were conducted, with varying pa-
rameters. We used the plain text version of the Sim-
ple English Wikipedia, but also tried to lemmatize it,
since substitutes are lemmatized. We used the Tree-
Tagger 4 (Schmid, 1994) and applied it on the whole
2See http://www.polishmywriting.com/
download/wikipedia2text\_rsm\_mods.tgz
and http://blog.afterthedeadline.com/
2009/12/04/generating-a-plain-text-corpus
-from-wikipedia
3http://search.cpan.org/?tpederse/
Text-NSP-1.25/lib/Text/NSP.pm
4http://www.ims.uni-stuttgart.de/
corpus, before counting n-grams. Moreover, since
bigrams and trigrams increase a lot, the size of n-
gram data, we evaluated their influence on results.
These tests are summed up in table 2.
Table 2: Results obtained with the Simple English
Wikipedia based system, on the trial and test corpora
reference lemmas score on score on
n-grams trial corpus test corpus
1-grams only no 0.333 ?
1 and 2-grams no 0.371 ?
1 to 3-grams no 0.381 0.465
1 to 3-grams yes 0.380 0.462
Simple Frequency
0.398 0.471
baseline
WLV-SHEF-SimpLex
(best system ? 0.496
@SemEval2012)
With unigrams only, 158 substitutes of the trial
corpus are absent of the reference dataset, 105 when
adding bigrams, and 91 when adding trigrams. Most
of the missing n-grams (when using 1 to 3-grams)
indeed seem to be very uncommon, such as ?undo-
mesticated? or ?telling untruths?.
The small difference between the lemmatized and
inflected versions of Wikipedia is due to two rea-
sons: some substitutes are found in the lemmatized
version because substitutes are given in the lemma-
tized form (for example ?abnormal growth? is only
present in its plural form ?abnormal growths? in
the inflected Wikipedia); and some other substitutes
are missing in the lemmatized version, mostly be-
cause of errors from the TreeTagger (for example
?be scared of? becomes ?be scare of?).
We kept the system that obtained the best scores
on the trial corpus, that is with 1 to 3-grams and non-
lemmatized n-grams, with a score of 0.381. This
system obtained a score of 0.465 on the evalua-
tion corpus, thus ranking second ex-aequo at the Se-
mEval evaluation.
projekte/corplex/TreeTagger/
489
5 Other frequency-based methods
We tried several other reference corpora, always
with the idea that the more frequent a word is, the
simpler it is. We used the BNC corpus,5 as well
as the Google Books NGrams.6 These NGrams
were calculated on the books digitized by Google,
and contain for each encountered n-gram, its num-
ber of occurrences for a given year. As the Google
Books NGrams are quite voluminous, we selected a
random year (2008), and kept only alphabetical n-
grams with potential hyphens, and used n-grams for
n ranging from 1 to 4. The dataset used contains
477,543,736 n-grams.
We also used the Microsoft Web N-gram Service
(more details on this service are given in the fol-
lowing section) to rank substitutes in descending or-
der. The results of these methods on the trial corpus
are given in table 3. The result of the simple fre-
quency baseline is also given: this baseline is also
frequency-based, but words are ranked according to
the number of hits found when querying the Google
Web 1T corpus with each substitute.
Table 3: Results obtained with frequency-based methods,
on the trial corpus
reference corpus score
BNC 0.347
Google Books NGrams 0.367
Microsoft NGrams 0.383
Simple Frequency baseline 0.398
This table shows that all frequency-based meth-
ods have lower scores than the Simple Frequency
baseline, although the score obtained with the Mi-
crosoft NGrams is quite close to the baseline. The
results from Microsoft Ngrams and the Simple En-
glish are very close. We decided to submit the Sim-
ple English Wikipedia-based system because it was
more different from the simple frequency baseline.
6 Contextual methods
We also wanted to use contextual information, since,
according to the contexts of the target word, dif-
ferent substitutes can be used, or ranked differ-
5http://www.natcorp.ox.ac.uk/
6http://books.google.com/ngrams/datasets
ently. In the following two examples, the same word
?film? is targetted, and the same substitutes are pro-
posed ?film;picture;movie;?; yet, in the gold stan-
dard, ?film? is placed before ?movie? in instance 19,
and after it in instance 15.
< i n s t a n c e i d =? 15 ?>
<c o n t e x t>Film Music L i t e r a t u r e
C y b e r p l a c e ? I n c l u d e s
<head>f i l m< / head> r e v i e w s , message
b o a r d s , c h a t room , and images
from v a r i o u s f i l m s .< / c o n t e x t>
< / i n s t a n c e>
( . . . )
< i n s t a n c e i d =? 19 ?>
<c o n t e x t>A f i n e s c o r e by George Fen ton
( THE CRUCIBLE ) and b e a u t i f u l
p h o t o g r a h y by Roger P r a t t add
g r e a t l y t o t h e e f f e c t i v e n e s s o f t h e
<head>f i l m< / head> .< / c o n t e x t>
< / i n s t a n c e>
Ranking substitutes thus depends on the context
of the target word. We implemented two systems
taking the context of target words into account.
6.1 Language model probabilities
The other system submitted (called ANNLOR-
lmbing) relies on language models, which was the
method used by the organizers in their Simple Fre-
quency baseline. While the organizers used Google
n-grams to rank terms to be substituted by decreas-
ing frequency of use, we used Microsoft Web n-
grams in the same way. Nevertheless, we also added
the contexts of each term to be substituted.
We used the Microsoft Web N-gram Service7 to
obtain joint probability for text units, and more
precisely its Python library.8 We used the bing-
body/apr10/ ) N-Gram model.
We considered a text unit composed of the lexi-
cal item and a contextual window of 4 words to the
left and 4 words to the right (words being separated
by spaces). For example, in the following sentence,
we tested ?He brings an incredibly rich and diverse
background that?, and the same unit with the tar-
get word replaced by substitutes, for example ?He
brings an incredibly lush and diverse background
that?.
7http://research.microsoft.com/en-us/
collaboration/focus/cs/web-ngram.aspx
8http://web-ngram.research.microsoft.
com/info/MicrosoftNgram-1.02.zip
490
< i n s t a n c e i d =? 118 ?>
<c o n t e x t>He b r i n g s an i n c r e d i b l y
<head> r i c h< / head> and d i v e r s e
background t h a t i n c l u d e s e v e r y t h i n g
from e x e c u t i v e c o a c h i n g , l e a r n i n g
&amp ; deve lopmen t and management
c o n s u l t i n g , t o s e n i o r o p e r a t i o n s
r o l e s , mixed wi th a m a s t e r s i n
o r g a n i z a t i o n a l
deve lopmen t .< / c o n t e x t>
< / i n s t a n c e>
We performed several tests, with different N-
Gram models, and different context sizes. Some of
these results for the trial corpus are given in table 4.
Table 4: Results obtained with Microsoft Web N-gram
Service, on the trial corpus
Size of left context Size of right context Score
0 3 0.362
3 0 0.358
2 2 0.365
3 3 0.358
4 4 0.370
For the evaluation, this system was our second
run, with the parameters that obtained the best scores
on the training corpus (contexts of 4 words to the
left and to the right). This method obtained a 0.370
score on the trial corpus and a 0.396 score on the test
corpus.9
7 Combination of methods
As each method seemed to have its own benefits, we
tried to combine them using SVMRank 10(Joachims,
2006). The output of each system is converted into
a feature file. For example, the output of the Simple
English Wikipedia based system begins with:
1 bright 475 1
1 intelligent 206 2
1 smart 201 3
1 clever 141 4
2 light 3241 1
2 clear 707 2
9This result is different from the official one, because an
incorrect file was submitted at the time.
10http://www.cs.cornell.edu/people/tj/
svm_light/svm_rank.html
2 bright 475 3
2 luminous 14 4
2 well-lit 0 5
The first column represent the instance id, the sec-
ond one the considered substitute, the third one the
feature (in this case, the frequency of the substitute
in the Simple English Wikipedia), and the last one,
the substitute rank according to this method. Then,
we combined these files to include all features (after
basic query-wise feature scaling). For example, the
training file begins with:
1 qid:1 2:-0.00461061395325929
3:0.0345010535723618
#intelligent
2 qid:1 2:-0.00485010755325339
3:-0.0213467053270483 #clever
3 qid:1 2:-0.00462903653787422
3:0.092640777900771 #smart
4 qid:1 2:-0.00361947890097599
3:0.0489145618699556 #bright
1 qid:4 2:-0.00461061395325929
3:0.0345010535723618
#intelligent
The first column gives the gold standard rank for
the substitute (in training phase), the second one the
instance id, and then feature ids and values for each
substitute. Default parameters were used.
We used the division of the trial corpus into a
training corpus and a development corpus. Table 5
gives some examples of scores obtained by combin-
ing two methods. The scores are not exactly those
presented earlier, since they correspond to a part of
the trial corpus only. Even though some improve-
ment can be obtained by this combination, it was
quite small, and so we did not use it for the evalua-
tion.
Table 5: Results obtained with combination of methods
with SVMRank, on the trial corpus
Simple English Microsoft
SVM
Wikipedia NGrams
0.352 0.352 0.354
491
8 Conclusion
In this paper, we present several systems developed
for the English Lexical Simplification task of Se-
mEval 2012. The best results are obtained using fre-
quencies from the Simple English Wikipedia. We
found the task quite hard to solve, since none of
our experiments significantly outperforms the Sim-
ple Frequency baseline. On the trial corpus, our
system based upon the Simple English Wikipedia
achieved a score of 0.381 (below the 0.399 base-
line score); on the test corpus, we achieved a score
of 0.465 with the Simple English Wikipedia system
while the baseline achieved a score of 0.471 score.
All our systems using contextual information did not
achieve high scores.
References
Thorsten Joachims. 2006. Training Linear SVMs in Lin-
ear Time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proc. of the Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplification.
In Proc. of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), Montre?al, Canada.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A Monolingual Tree-based Translation Model
for Sentence Simplification. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING 2010), pages 1353?1361.
492
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 47?56,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Syntactic Sentence Simplification for French
Laetitia Brouwers
Aspirante FNRS
CENTAL, IL&C
UCLouvain
Belgium
Delphine Bernhard
LiLPa
Universit?e de Strasbourg
France
Anne-Laure Ligozat
LIMSI-CNRS
ENSIIE
France
Thomas Franc?ois
CENTAL, IL&C
UCLouvain
Belgium
Abstract
This paper presents a method for the syn-
tactic simplification of French texts. Syn-
tactic simplification aims at making texts
easier to understand by simplifying com-
plex syntactic structures that hinder read-
ing. Our approach is based on the study
of two parallel corpora (encyclopaedia ar-
ticles and tales). It aims to identify the lin-
guistic phenomena involved in the manual
simplification of French texts and organ-
ise them within a typology. We then pro-
pose a syntactic simplification system that
relies on this typology to generate simpli-
fied sentences. The module starts by gen-
erating all possible variants before select-
ing the best subset. The evaluation shows
that about 80% of the simplified sentences
produced by our system are accurate.
1 Introduction
In most of our daily activities, the ability to read
quickly and effectively is an undeniable asset,
even often a prerequisite (Willms, 2003). How-
ever, a sizeable part of the population is not able
to deal adequately with the texts they face. For
instance, Richard et al. (1993) reported that, in 92
applications for an unemployment allowance filled
by people with a low level of education, about half
of the required information was missing (some of
which was crucial for the processing of the appli-
cation), mainly because of comprehension issues.
These comprehension issues are often related
to the complexity of texts, particularly at the lex-
ical and syntactic levels. These two factors are
known to be important causes of reading difficul-
ties (Chall and Dale, 1995), especially for young
children, learners of a foreign language or people
with language impairments or intellectual disabil-
ities.
In this context, automatic text simplification
(ATS) appears as a means to help various peo-
ple access more easily the contents of the written
documents. ATS is an application domain of Nat-
ural Language Processing (NLP) aiming at mak-
ing texts more accessible for readers, while en-
suring the integrity of their contents and structure.
Among the investigations in this regard are those
of Caroll et al. (1999), Inui et al. (2003) and, more
recently, of Rello et al. (2013), who developed
tools to produce more accessible texts for people
with language disabilities such as aphasia, deaf-
ness or dyslexia. In the FIRST project, Barbu et
al. (2013) and Evans and Or?asan (2013) imple-
mented a simplification system for patients with
autism, who may also struggle to understand diffi-
cult texts.
However, reading assistance is not only in-
tended for readers with disabilities, but also for
those who learn a new language (as first or sec-
ond language). De Belder and Moens (2010) fo-
cused on ATS for native English schoolchildren,
while Siddharthan (2006), Petersen and Ostendorf
(2007) and Medero and Ostendorf (2011) focused
on learners of a second language. Williams and
Reiter (2008), Aluisio et al. (2008) and Gasperin
et al. (2009) addressed ATS for illiterate adults.
Most of these studies are dealing with the En-
glish language, with the exception of some work
in Japanese (Inui et al., 2003), Spanish (Saggion
et al., 2011; Bott et al., 2012), Portuguese (Alu??sio
et al., 2008) and French (Seretan, 2012).
ATS was also used as a preprocessing step to
increase the effectiveness of subsequent NLP op-
erations on texts. Chandrasekar et al. (1996)
first considered that long and complex sentences
were an obstacle for automatic parsing or machine
translation and they showed that a prior simplifi-
cation may result in a better automatic analysis
of sentences. More recently, Heilman and Smith
(2010) showed that adding ATS in the context
47
of automatic question generation yields better re-
sults. Similarly, Lin and Wilbur (2007) and Jon-
nalagadda et al. (2009) optimized information ex-
traction from biomedical texts using ATS as a pre-
processing step.
In these studies, the simplifications carried out
are generally based on a set of manually defined
transformation rules. However, ATS may also
be solved with methods from machine transla-
tion and machine learning. This lead some re-
searchers (Zhu et al., 2010; Specia, 2010; Wood-
send and Lapata, 2011) to train statistical models
from comparable corpora of original and simpli-
fied texts. The data used in these studies are of-
ten based on the English Wikipedia (for original
texts) and the Simple English Wikipedia, a simpli-
fied version for children and non-native speakers
that currently comprises more than 100,000 arti-
cles. Similar resources exist for French, such as
Vikidia and Wikimini, but texts are far less nu-
merous in these as in their English counterpart.
Moreover, the original and simplified versions of
an article are not strictly parallel, which further
complicates machine learning. This is why, so far,
there was no attempt to adapt this machine learn-
ing methodology to French. The only previous
work on French, to our knowledge, is that of Sere-
tan (2012), which analysed a corpus of newspa-
pers to semi-automatically detect complex struc-
tures that has to be simplified. However, her sys-
tem of rules has not been implemented and evalu-
ated.
In this paper, we aim to further investigate the
issue of syntactic simplification for French. We
assume a midway point between the two main ten-
dencies in the field. We use parallel corpora sim-
ilar to those used in machine learning approaches
and analyse it to manually define a set of simpli-
fication rules. We have also implemented the syn-
tactic part of our typology through a simplification
system. It is based on the technique of overgen-
eration, which consists in generating all possible
simplified variants of a sentence, and then on the
selection of the best subset of variants for a given
text with the optimization technique known as in-
teger linear programming (ILP). ILP allows us to
specify a set of constraints that regulate the selec-
tion of the output by the syntactic simplification
system. This method has already been applied to
ATS in English by Belder and Moens (2010) and
Woodsend and Lapata (2011).
To conclude, the contributions of this paper are:
(1) a first corpus-based study of simplification pro-
cesses in French that relies on a corpus of parallel
sentences, (2) the organization of this study?s re-
sults in what might be the first typology of sim-
plification for French based on a corpus analysis
of original and simplified texts; (3) two new crite-
ria to select the best subset of simplified sentences
among the set of variants, namely the spelling list
of Catach (1985) and the use of keywords, and
finally (4) a syntactic simplification system for
French, a language with little resources as regards
text simplification.
In the next sections, we first present the cor-
pora building process (Section 2.1) and describe
a general typology of simplification derived from
our corpora (Section 2.2). Then, we present the
system based on the syntactic part of the typol-
ogy, which operates in two steps: overgeneration
of all possible simplified sentences (Section 2.3.1)
and selection of the best subset of candidates us-
ing readability criteria (Section 2.3.2) and ILP.
Finally, we evaluate the quality of the syntacti-
cally simplified sentences as regards grammatical-
ity, before performing some error analysis (Sec-
tion 3).
2 Methodology
2.1 Corpus Description
We based our typology of simplification rules on
the analysis of two corpora. More specifically,
since our aim is to identify and classify the var-
ious strategies used to transform a complex sen-
tence into a more simple one, the corpora had to
include parallel sentences. The reason why we
analysed two corpora is to determine whether dif-
ferent genres of texts lead to different simplifica-
tion strategies. In this study, we focused on the
analysis of informative and narrative texts. The in-
formative corpus comprises encyclopaedia articles
from Wikipedia
1
and Vikidia
2
. For the narrative
texts, we used three classic tales by Perrault, Mau-
passant and Daudet and their simplified versions
for learners of French as a foreign language.
To collect the first of our parallel corpora, we
used the MediaWiki API to retrieve Wikipedia
and Vikidia articles with the same title. The
1
http://fr.wikipedia.org
2
This site is intended for young people from eight to thirteen years and gathers more
accessible articles than Wikipedia, both in terms of language and content. It is available at
the address http://fr.vikidia.org
48
WikiExtractor
3
was then applied to the articles
to discard the wiki syntax and only keep the raw
texts. This corpus comprises 13,638 texts (7,460
from Vikidia and only 6,178 from Wikipedia,
since some Vikidia articles had no counterpart in
Wikipedia).
These articles were subsequently processed to
identify parallel sentences (Wikipedia sentence
with a simplified equivalent in Vikidia). The align-
ment has been made partly manually and partly
automatically with the monolingual alignment al-
gorithm described in Nelken and Shieber (2006),
which relies on a cosine similarity between sen-
tence vectors weighted with the tf-idf. This pro-
gram outputs alignments between sentences, along
with a confidence score. Among these files,
twenty articles or excerpts from Wikipedia were
selected along with their equivalent in Vikidia.
This amounts to 72 sentences for the former and
80 sentences for the latter.
The second corpus is composed of 16 narra-
tive texts, and more specifically tales, by Perrault,
Maupassant, and Daudet. We used tales since
their simplified version was closer to the origi-
nal than those of longer novels, which made the
sentence alignment simpler. The simplified ver-
sions of these tales were found in two collections
intended to learners of French as a foreign lan-
guage (FFL): ?Hachette - Lire en franc?ais facile?
and ?De Boeck - Lire et s?entrainer?. Their level
of difficulty ranges from A1 (Daudet) to B1 (Mau-
passant) on the CEFR scale (Council of Europe,
2001), with Perrault being A2. The texts were dig-
itized by OCR processing and manually aligned,
by two annotators, with an adjudication phase for
the disagreement cases. In this corpus, we anal-
ysed 83 original sentences and their correspond-
ing 98 simplified versions, which gives us a size
roughly similar to the Wikipedia-Vikidia corpus.
The two corpora created are relevant for a man-
ual analysis, as done in the next section, but they
are too small for automatic processing. We plan
to implement a method to align automatically the
narrative texts in the near future and thus be able
to collect a larger corpus.
2.2 Simplification Typology
The observations carried out on these two cor-
pora have made it possible to establish a typol-
ogy organised according to three main linguistic
3
http://medialab.di.unipi.it/wiki/Wikipedia\_Extractor
levels of transformation: lexical, discursive and
syntactic, which can be further divided into sub-
categories. It is worth mentioning that in previ-
ous work, simplification is commonly regarded as
pertaining to two categories of phenomena: lexi-
cal and syntactic (Carroll et al., 1999; Inui et al.,
2003; De Belder and Moens, 2010). Little atten-
tion has been paid to discourse in the area of auto-
matic simplification (Siddharthan, 2006).
The typology is summarized in Table 1. As
regards the lexicon, the phenomena we observed
involve four types of substitution. First, dif-
ficult terms can be replaced by a synonym or
an hypernym perceived as simpler. Second,
some anaphoric expressions, considered simpler
or more explicit, are preferred to their counter-
parts in the original texts. For example, in our
three tales, simplified nominal anaphora are regu-
larly used instead of pronominal anaphora. Third,
rather than using synonymy, the authors of the
simplified texts sometimes replace difficult words
with a definition or an explanatory paraphrase. Fi-
nally, in the particular case where the original texts
contain concepts in a foreign language, these non-
French terms are translated.
At the discourse level, the authors of simple
texts pay particular attention to the organization
of the information which has to be clear and con-
cise. To this end, clauses may be interchanged
to ensure a better presentation of the information.
In addition, information of secondary importance
can be removed while explanations or examples
are added for clarity. These two phenomena can
appear to be contradictory (deletion and addition),
but they actually operate in a common goal: make
the main information more comprehensible. Par-
ticular attention is also placed on the coherence
and cohesion of the text: Authors tend to explain
the pronouns and explicit the relations between
sentences. The last observed strategy is that im-
personal structures are often personalized.
Finally, at the syntactic level, five types of
changes are observed: tense modification, dele-
tion, modification, splitting and grouping. The last
two types can be considered together since they
are two opposite phenomena.
? First, the tenses used in the simplified ver-
sions are more common and less literary than
those used in the original texts. Thus, the
present and present perfect are preferred to
the simple past, imperfect and past perfect.
49
Lexicon Discourse Syntax
Translation Reorganisation Tense
Anaphoric synonyms Addition Modification
Definition and paraphrase Deletion Grouping
Synonym or hypernym Coherence and cohesion Deletion
Personalisation Splitting
Table 1: Typology of simplifications
? Secondary or redundant information, that is
generally considered removable at the syn-
tactic level, is not included in the simplified
texts. Adverbial clauses, some adverbs and
adjectives and subordinate clauses, among
others, are omitted.
? When some complex structures are not
deleted, then they are often moved or modi-
fied for better clarity. Such structures include
negative sentences, impersonal structures, in-
direct speech and subordinate clauses.
? The authors sometimes choose to divide long
sentences or conversely merge several sen-
tences into one. The grouping of elements is
much less frequent than the division of sen-
tences. To split a sentence, the authors gen-
erally transform a secondary clause?be it rel-
ative, coordinate, subordinate, participial or
adjectival?into an independent clause.
This classification can be compared with that of
Medero et al. (2011) who propose three categories
? division, deletion and extension ? or that of Zhu
et al. (2010), which includes division, deletion, re-
organization, and substitution.
Among those transformations, some are hardly
implementable. This is the case when a change
requires the use of semantics. For example, noun
modifiers may sometimes be removed, but in other
cases, they are necessary. However, there are of-
ten neither typographical nor grammatical marked
differences between the two cases.
Another issue is that other syntactic changes
should be accompanied by lexical transforma-
tions, which are difficult to generalize. For exam-
ple, transforming a negative sentence into its af-
firmative equivalent requires to find a verb whose
affirmative form includes the meaning of the neg-
ative construction to replace.
There are also changes that are very particular
and require a manual rather than an automatic pro-
cessing of the text, in the sense that each case is
different (even if part of a more global rule). In
addition, they usually involve discourse or lexical
information and not just syntactic one.
Finally, the syntactic changes impacting other
parts of the text or concerning elements that de-
pend on another structure require more compre-
hensive changes to the text. Therefore, they are
also difficult to handle automatically. Thus, to
change the tense of a verb in a sentence, we must
ensure that the sequence of tenses agree in the en-
tire text.
2.3 The Sentence Simplification System
We used this typology to implement a system of
syntactic simplification for French sentences. The
simplification is performed as a two-step process.
First, for each sentence of the text, we generate the
set of all possible simplifications (overgeneration
step), and then, we select the best subset of sim-
plified sentences using several criteria.
2.3.1 Generation of the Simplified Sentences
The sentence overgeneration module is based on a
set of rules (19 rules), which rely both on morpho-
syntactic features of words and on syntactic rela-
tionships within sentences. To obtain this infor-
mation, the texts from our corpus are analyzed
by MELT
4
(Denis and Sagot, 2009) and Bon-
sai
5
(Candito et al., 2010) during a preprocessing
phase. As a result, texts are represented as syn-
tax trees that include the information necessary to
apply our simplification rules. After preprocess-
ing, the set of simplification rules is applied recur-
sively, one sentence at a time, until there is no fur-
ther structure to simplify. All simplified sentences
produced by a given rule are saved and gathered in
a set of variants.
The rules for syntactic simplification included
in our program are of three kinds: deletion rules
(12 rules), modification rules (3 rules) and split-
ting rules (4 rules). With regards to our typology, it
can be noted that two types of rules have not been
implemented: aggregation rules and tense simpli-
fication rules. The merging strategies (in which
several sentences are aggregated into one) were
4
https://gforge.inria.fr/projects/lingwb
5
http://alpage.inria.fr/statgram/frdep/fr_stat_dep_
parsing.html
50
not observed consistently in the corpus. Moreover,
aggregation rules could have come into conflict
with the deletion rules, since they have opposite
goals. Concerning tense aspects, some of them
are indeed more likely to be used than others in
Vikidia. However, this strategy has not been im-
plemented, since it implies global changes to the
text. For instance, when a simple past is replaced
by a present form, we must also adapt the verbs in
the surrounding context in accordance with tense
agreement. This requires to consider the whole
text, or at least the paragraph that contains the
modified verbal form, and be able to automatically
model tense agreement. Otherwise, we may alter
the coherence of the text and decrease its readabil-
ity.
This leaves us with 19 simplification rules.
6
To
apply them, the candidate structures for simplifi-
cation first need to be detected using regular ex-
pressions, via Tregex
7
(Levy and Andrew, 2006)
that allows the retrieval of elements and relation-
ships in a parse tree. In a second step, syntactic
trees in which a structure requires simplification
are modified according a set of operations imple-
mented through Tsurgeon.
The operations to perform depend on the type
of rules:
1. For the deletion cases, simply deleting all
the elements involved is sufficient (via the
delete operation in Tsurgeon). The ele-
ments affected by the deletion rules are ad-
verbial clauses, clauses between brackets,
some of the subordinate clauses, clauses be-
tween commas or introduced by words such
as ?comme? (as), ?voire? (even), ?soit? (ei-
ther), or similar terms, some adverbs and
agent prepositional phrases.
2. For the modification rules, several opera-
tions need to be combined: some terms are
dropped (via Tsurgeon delete), others
are moved (operation Tsurgeon move)
and specific labels are added to the text to sig-
nal a possible later processing. These labels
are useful for rules implying a modification
of tense or mode aspects for a verb. In such
cases, tags are added around the verb to indi-
cate that it needs to be modified. The mod-
ification is performed later, using the conju-
6
These 19 rules are available at http://cental.fltr.ucl.ac.be/team/
lbrouwers/rules.pdf
7
http://nlp.stanford.edu/software/tregex.shtml
gation system Verbiste.
8
For instance, to
change a passive into an active structure, not
only the voice must be changed, but some-
times also the person, so that the verb agrees
well with the agent that has become the new
subject. As regards modification rules, three
changes were implemented: moving adver-
bial clauses at the beginning of the sentence,
transforming passive structures into active
forms, and transforming a cleft to a non-cleft.
3. For the splitting rules, we followed a two-
step process. The subordinate clause is first
deleted, while the main clause is saved as a
new sentence. Resuming from the original
sentence, the main clause is, in turn, removed
to keep only the subordinate clause, which
must then be transformed into an independent
clause. In general, the verbal form of the sub-
ordinate clause needs to be altered in order to
operate as a main verb. Moreover, the pro-
noun governing the subordinated clause must
be substituted with its antecedent and the sub-
ject must be added when missing. In the case
of a relative clause, the relative pronoun thus
needs to be substituted by its antecedent, but
it is also important to consider the function
of the pronoun to find out where to insert this
antecedent. Our splitting rules apply when a
sentence includes either relative or participle
clauses, or clauses introduced by a colon or a
coordinating conjunction.
All these simplification rules are applied recur-
sively to a sentence until all possible alternatives
have been generated. Therefore, it is common to
have more than one simplified variant for a given
sentence. In this case, the next step consists in se-
lecting the most suitable variant to substitute the
original one. The selection process is described in
the next section.
2.3.2 Selection of the Best Simplifications
Given a set of candidate simplified sentences for a
text, our goal is to select the best subset of simpli-
fied sentences, that is to say the subset that max-
imizes some measure of readability. More pre-
cisely, text readability is measured through differ-
ent criteria, which are optimized with an Integer
Linear Programming (ILP) approach (Gillick and
Favre, 2009). These criteria are rather simple in
8
This software is available at the address http://sarrazip.com/dev/
verbiste.html under GNU general public license and was developed by Pierre Sar-
razin.
51
this approach. They are used to ensure that not
only the syntactic difficulty, but also the lexical
complexity decrease, since syntactic transforma-
tions may cause lexical or discursive alterations in
the text.
We considered four criteria to select the most
suitable sentences among the simplified set: sen-
tence length (in words) (h
w
), mean word length
(in characters) in the sentence (h
s
), familiarity of
the vocabulary (h
a
), and presence of some key-
words (h
c
). While the first two criteria are pretty
obvious as regards implementation, we measured
word familiarity based on Catach?s list (1985).
9
It
contains about 3,000 of the most frequent words
in French, whose spelling should be taught in pri-
ority to schoolchildren. The keywords were in this
study simply defined as any term occurring more
than once in the text.
These four criteria were combined using integer
linear programming as follows:
10
Maximize : h
w
+ h
s
+ h
a
+ h
c
Where : h
w
= wps?
?
i
s
i
?
?
i
l
w
i
s
i
h
s
= cpw?
?
i
l
w
i
s
i
?
?
i
l
c
i
s
i
h
a
= aps?
?
i
s
i
?
?
i
l
a
i
s
i
h
c
=
?
j
w
j
c
j
Subject to:
?
i?g
k
s
i
= 1 ?g
k
s
i
occ
ij
? c
j
?i, j
?
i
s
i
occ
ij
? c
j
?j
(1)
The above variables are defined as follows:
? wps: desired (mean) number of words per sentence
? cpw: desired (mean) number of characters per word
? aps: desired (mean) number of words absent from
Catach?s list for a sentence
? s
i
: binary variable indicating whether the sentence i
should be kept or not, with i varying from 1 to the total
number of simplified sentences
? c
j
: binary variable indicating whether keyword j is in
the simplification or not, with j varying from 1 to the
total number of keywords
? l
w
i
: length of sentence i in words
? l
c
i
: number of characters in sentence i
? l
a
i
: number of words absent from Catach?s list in sen-
tence i
? w
j
: number of occurrences of keyword j
? g
k
: set of simplified sentences obtained from the same
original sentence k
? occ
ij
: binary variable indicating the presence of term j
in sentence i
9
This list is available at the site http://www.ia93.ac-creteil.fr/spip/
spip.php?article2900.
10
We used an ILP module based on glpk that is available at the address http://
www.gnu.org/software/glpk/
wps, cpw and aps are constant parameters
whose values have been set respectively to 10, 5
and 2 for this study. 5 for cpw corresponds to the
value computed on the Vikidia corpus, while for
wps and aps, lower values than observed were
used to force simplification (respectively 10 in-
stead of 17 and 2 instead of 31).
However, these parameters may vary depending
on the context of use and the target population, as
they determine the level of difficulty of the simpli-
fied sentences obtained.
The constraints specify that (i) for each original
sentence, at most one simplification set should be
chosen, (ii) selecting a sentence means selecting
all the terms it contains and (iii) selecting a key-
word is only possible if it is present in at least one
selected sentence.
We illustrate this process with the Wikipedia ar-
ticle entitled Abel. This article contains 25 sen-
tences, from which 67 simplified sentences have
been generated. For the original sentence (1a) for
example, 5 variants were generated and simplifi-
cation (2) was selected by ILP.
(1a) Original sentence
11
: Ca??n, l?a??n?e, cultive la
terre et Abel ( ?etymologie : de l?h?ebreu

souffle

,

vapeur

,

existence pr?ecaire

) garde le trou-
peau.
(1b) Possible simplifications :
Simplification 1 : Ca??n, l?a??n?e, cultive la terre et
Abel garde le troupeau.
Simplification 2 : Ca??n, l?a??n?e, cultive la terre.
Abel garde le troupeau.
Simplification 3 : Ca??n, l?a??n?e, cultive la terre.
Simplification 4 : Abel garde le troupeau.
(...)
(1c) Selected simplification (2) : Ca??n, l?a??n?e,
cultive la terre. Abel garde le troupeau.
3 Evaluation
Syntactic simplification involves substantial
changes within the sentence both in terms of
contents and form. It is therefore important to
check that the application of a rule does not cause
errors that would make the sentences produced
unintelligible or ungrammatical. A manual
evaluation of our system?s efficiency to generate
correct simplified sentences was carried out on
our two corpora. In each of them, we selected a
set of texts that had not been previously used for
11
Ca??n, the eldest brother, farms the land and Abel (etymology : from Hebrew

breath

,

steam

,

fragile existence

) looks after the flock.
52
Sentence length Word length Word familiarity Keywords
Expected values 10 5 2 /
Original 19 6.1 11 5
Simplification 1 11 4.3 5 5
Simplification 2 5 4.6 2 5
Simplification 3 6 4.5 3 3
Simplification 4 4 4.7 2 2
Simplification 5 9 6.3 5 5
Simplification 6 12 7.3 8 2
Table 2: Values of the criteria in IPL for example (1).
the typological analysis, that is to say 9 articles
from Wikipedia (202 sentences) and two tales
from Perrault (176 sentences). In this evaluation,
all simplified sentences are considered, not only
those selected by ILP. The results are displayed in
Table 3 and discussed in Section 3.1. Two types
of errors can be detected: those resulting from
morpho-syntactic preprocessing, and particularly
the syntactic parser, and the simplification errors
per se, that we discuss in larger details in Section
3.2.
3.1 Quantitative Evaluation
Out of the 202 sentences selected in the informa-
tive corpus for evaluation, 113 (56%) have under-
gone one or more simplifications, which gives us
333 simplified variants. Our manual error analy-
sis revealed that 71 sentences (21%) contain some
errors, among which we can distinguish those due
to the preprocessing from those actually due to the
simplification system itself. It is worth mention-
ing that the first category amounts to 89% of the
errors, while the simplification rule are only re-
sponsible for 11% of those. We further refined the
analysis of the system?s errors distinguishing syn-
tactic from semantic errors.
The scores obtained on the narrative corpus are
slightly less good: out of the 369 simplified vari-
ants produced from the 154 original sentences, 77
(20.9%) contain errors. This value is very simi-
lar to the percentage for the informative corpus.
However, only 50.7% of these errors are due to the
preprocessing, while the remaining 49.3% come
from our rules. It means that our rules yield about
10.3% incorrect simplified variants compared to
2.7% for the informative corpus. Nevertheless,
these errors are caused mostly by 2 or 3 rules:
the deletion of subordinate clauses, of infinitives
or of clauses coordinated with a colon. This loss
in efficiency can be partly explained by the greater
presence of indirect speech in the tales that include
more non-removable subordinate clauses, difficult
to distinguish from removable clauses.
Globally, our results appear to be in line with
those of similar systems developed for English.
12
Yet, few studies have a methodology and evalu-
ation close enough to ours to allow comparison
of the results. Siddharthan (2006) assessed his
system output using three judges who found that
about 80% of the simplified sentences were gram-
matical, while 87% preserved the original mean-
ing. These results are very similar to our find-
ings that mixed the syntactic and discourse dimen-
sions. Drndarevi?c et al. (2013) also presented the
output of their system to human judges who esti-
mated that 60% of the sentences were grammatical
and that 70% preserved the initial meaning. These
scores appear lower than ours, but Drndarevi?c et
al. also used lexical rules, which means that their
error rate includes both grammatical and lexical
errors.
3.2 Error Analysis
As regards syntax, the structure of a sentence can
be modified so that it becomes grammatically in-
correct. Three simplification rules are concerned.
Deletion rules may cause this kind of problem, be-
cause they involve removing a part of the sentence,
considered as secondary. However, sometimes the
deleted element is essential, as in the case of the
removal of the referent of a pronoun. This type of
problem arises both with the deletion of a subor-
dinate clause or that of an infinitive clause. Dele-
tion rules are also subject to a different kind of
errors. During the reconstruction of the sentence
resulting from the subordinate clause, some con-
stituents, such as the subject, may not be properly
identified and will be misplaced in the new sen-
tence.
At the semantic level, the information conveyed
by the original sentence may be modified or even
removed. When an agent or an infinitive clause
12
We do not discuss French here, since no simplification system were found for French,
as explained previously.
53
Wikipedia-Vikidia corpus
nb. sent. % correct % preproc. errors % simplification errors
333 262 (78.7 %) 63 (18.9%)
8 (2.4 %)
syntax: semantics:
6 (1.8%) 2 (0.6%)
Narrative corpus
nb. sent. % correct % preproc. errors % simplification errors
369 292 (79.1 %) 39 (10.6%)
38 (10.3 %)
syntax: semantics:
20 (5.4%) 18 (4.9%)
Table 3: Performance of the simplification system on both corpora
are suppressed, the meaning of the sentence may
be disrupted or some of the content lost. For in-
stance, in the following sentence ? extracted from
the Wikipedia article abb?e (abbot) ? the infinitive
clause explaining the term is dropped:
(2a) C?est aussi depuis le XVIIIe si`ecle
le terme en usage pour d?esigner un clerc
s?eculier ayant au moins rec?u la ton-
sure.
13
(2b) C?est aussi depuis le XVIIIe si`ecle
le terme en usage.
To fix the errors identified above, our rules should
be refined and developed, with the addition of
better tools for sentence regeneration as well as
some exclusion criteria for the incorrect sentences
within the ILP module, as discussed in the next
section.
4 Perspectives and Conclusions
This article describes an automatic syntactic sim-
plification system for French intended for children
and language learners. It is based on a set of rules
defined after a corpus study, which also led to the
development of a typology of simplifications in
French. It would be easy to extend our typology to
other target users based on other appropriate cor-
pora, such as people with language disorders.
Our approach also uses the technique of over-
generation, which makes it possible to retain the
best set of simplifications based on readability cri-
teria. Note that among those employed, some had
not been considered previously and produce inter-
esting results. Finally, we showed that the perfor-
mance of our system is good (about 80 % of the
generated sentences are correct) and in line with
previous studies.
13
It is also the term in use since the 18th to refer to a secu-
lar cleric who, at least, received the tonsure.
The evaluation showed that the rules imple-
mented are more suitable for expository texts,
probably because they are more explicit, as style
there is of a minor importance. In addition, the
system set up was first tested on and therefore
adapted to Wikipedia. It was only subsequently
applied to narratives, that revealed new challenges,
especially concerning the deletion rules. The in-
formation provided in secondary clauses or com-
plements indeed seems most essential to under-
standing the story, especially when it comes to di-
rect or indirect speech. In order to comprehend
the differences in terms of efficiency and rules to
be applied between genres, it would be necessary
to extend our study to other texts collected in the
corpora.
We envision multiple perspectives to improve
our system. First, syntactic simplification could be
supplemented by lexical simplification, as is done
in some studies for English (Woodsend and Lap-
ata, 2011). Moreover, our error analysis has high-
lighted the need to add or repeat words when a
sentence is split. It would therefore be useful to
use a tool that manages references in order to im-
prove the quality of simplified text. In addition,
the sentence selection module could include addi-
tional selection criteria, based on the work done in
readability of French (Franc?ois and Fairon, 2012).
A final perspective of improvement would be to
make the rule system adapt to the target audience
and the genre of the texts. This would require as-
sessing the relevance of various transformations
and selection criteria of the best simplifications.
This perspective would also require assessing the
effectiveness of the rules by means of comprehen-
sion tests both on the original and simplified sen-
tences, which we plan to do.
54
References
S. Alu??sio, L. Specia, T. Pardo, E. Maziero, and
R. Fortes. 2008. Towards brazilian portuguese auto-
matic text simplification systems. In Proceedings of
the eighth ACM symposium on Document engineer-
ing, pages 240?248.
E. Barbu, P. de Las Lagunillas, M. Mart?n-Valdivia, and
L. Urena-L?opez. 2013. Open book: a tool for help-
ing asd users? semantic comprehension. NLP4ITA
2013, pages 11?19.
S. Bott, L. Rello, B. Drndarevic, and H. Saggion. 2012.
Can Spanish Be Simpler? LexSiS: Lexical Simpli-
fication for Spanish. In Proceedings of COLING
2012, pages 357?374.
M. Candito, B. Crabb?e, and P. Denis. 2010. Statisti-
cal french dependency parsing: treebank conversion
and first results. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 1840?1847.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying Text for
Language-Impaired Readers. In Proceedings of
EACL, pages 269?270.
N. Catach. 1985. Les listes orthographiques de base
du franc?ais. Nathan, Paris.
J.S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brook-
line Books, Cambridge.
R. Chandrasekar, C. Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of the 16th conference on Computa-
tional linguistics, pages 1041?1044.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Press Syndicate of the University
of Cambridge.
J. De Belder and M.-F. Moens. 2010. Text Simplifica-
tion for Children. In Proceedings of the Workshop
on Accessible Search Systems.
P. Denis and B. Sagot. 2009. Coupling an annotated
corpus and a morphosyntactic lexicon for state-of-
the-art pos tagging with less human effort. In Pro-
ceedings of PACLIC.
B. Drndarevi?c, S.
?
Stajner, S. Bott, S. Bautista, and
H. Saggion. 2013. Automatic text simplification in
spanish: a comparative evaluation of complement-
ing modules. In Computational Linguistics and In-
telligent Text Processing, pages 488?500.
R. Evans and C. Or?asan. 2013. Annotating signs of
syntactic complexity to support sentence simplifica-
tion. In Text, Speech, and Dialogue, pages 92?104.
T. Franc?ois and C. Fairon. 2012. An ?AI readability?
formula for French as a foreign language. In Pro-
ceedings of EMNLP 2012, pages 466?477.
C. Gasperin, E. Maziero, L. Specia, T. Pardo, and
S. Aluisio. 2009. Natural language processing
for social inclusion: a text simplification architec-
ture for different literacy levels. Proceedings of
SEMISH-XXXVI Semin?ario Integrado de Software e
Hardware, pages 387?401.
D. Gillick and B. Favre. 2009. A scalable global model
for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18.
M. Heilman and N. A. Smith. 2010. Extracting Simpli-
fied Statements for Factual Question Generation. In
Proceedings of the 3rd Workshop on Question Gen-
eration.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the second interna-
tional workshop on Paraphrasing, pages 9?16.
S. Jonnalagadda, L. Tari, J. Hakenberg, C. Baral, and
G. Gonzalez. 2009. Towards Effective Sentence
Simplification for Automatic Processing of Biomed-
ical Text. In Proceedings of NAACL-HLT 2009.
R. Levy and G. Andrew. 2006. Tregex and tsurgeon:
tools for querying and manipulating tree data struc-
tures. In Proceedings of LREC, pages 2231?2234.
L. Lin and W. J. Wilbur. 2007. Syntactic sentence
compression in the biomedical domain: facilitat-
ing access to related articles. Information Retrieval,
10(4):393?414, October.
J. Medero and M. Ostendorf. 2011. Identifying Tar-
gets for Syntactic Simplification. In Proceedings of
the SLaTE 2011 workshop.
R. Nelken and S.M. Shieber. 2006. Towards robust
context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of EACL, pages 161?
168.
S. E. Petersen and M. Ostendorf. 2007. Text Simpli-
fication for Language Learners: A Corpus Analysis.
In Proceedings of SLaTE2007, pages 69?72.
L. Rello, C. Bayarri, A. G`orriz, R. Baeza-Yates,
S. Gupta, G. Kanvinde, H. Saggion, S. Bott, R. Car-
lini, and V. Topac. 2013. Dyswebxia 2.0!: more
accessible text for people with dyslexia. In Proceed-
ings of the 10th International Cross-Disciplinary
Conference on Web Accessibility, page 25.
J.F. Richard, J. Barcenilla, B. Brie, E. Charmet,
E. Clement, and P. Reynard. 1993. Le traite-
ment de documents administratifs par des popula-
tions de bas niveau de formation. Le Travail Hu-
main, 56(4):345?367.
H. Saggion, E. Mart??nez, E. Etayo, A. Anula, and
L. Bourg. 2011. Text simplification in simplext.
making text more accessible. Procesamiento del
lenguaje natural, 47:341?342.
V. Seretan. 2012. Acquisition of syntactic simplifica-
tion rules for french. In LREC, pages 4019?4026.
A. Siddharthan. 2006. Syntactic Simplification and
Text Cohesion. Research on Language & Computa-
tion, 4(1):77?109, jun.
L. Specia. 2010. Translating from Complex to Sim-
plified Sentences. In Proceedings of the 9th Inter-
national Conference on Computational Processing
of the Portuguese Language (Propor-2010)., pages
30?39.
S. Williams and E. Reiter. 2008. Generating basic
skills reports for low-skilled readers. Natural Lan-
guage Engineering, 14(4):495?525.
J.D. Willms. 2003. Literacy proficiency of youth:
Evidence of converging socioeconomic gradients.
International Journal of Educational Research,
55
39(3):247?252.
K. Woodsend and M. Lapata. 2011. Learning to
Simplify Sentences with Quasi-Synchronous Gram-
mar and Integer Programming. In Proceedings of
EMNLP, pages 409?420.
Z. Zhu, D. Bernhard, and I. Gurevych. 2010. A Mono-
lingual Tree-based Translation Model for Sentence
Simplification. In Proceedings of COLING 2010,
pages 1353?1361.
56
