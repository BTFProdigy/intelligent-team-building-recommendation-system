Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 103?108,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual semantic parsing with a pipeline of linear classifiers
Oscar Ta?ckstro?m
Swedish Institute of Computer Science
SE-16429, Kista, Sweden
oscar@sics.se
Abstract
I describe a fast multilingual parser for seman-
tic dependencies. The parser is implemented
as a pipeline of linear classifiers trained with
support vector machines. I use only first or-
der features, and no pair-wise feature combi-
nations in order to reduce training and pre-
diction times. Hyper-parameters are carefully
tuned for each language and sub-problem.
The system is evaluated on seven different
languages: Catalan, Chinese, Czech, English,
German, Japanese and Spanish. An analysis
of learning rates and of the reliance on syn-
tactic parsing quality shows that only modest
improvements could be expected for most lan-
guages given more training data; Better syn-
tactic parsing quality, on the other hand, could
greatly improve the results. Individual tun-
ing of hyper-parameters is crucial for obtain-
ing good semantic parsing quality.
1 Introduction
This paper presents my submission for the seman-
tic parsing track of the CoNLL 2009 shared task on
syntactic and semantic dependencies in multiple lan-
guages (Hajic? et al, 2009). The submitted parser is
simpler than the submission in which I participated
at the CoNLL 2008 shared task on joint learning of
syntactic and semantic dependencies (Surdeanu et
al., 2008), in which we used a more complex com-
mittee based approach to both syntax and semantics
(Samuelsson et al, 2008). Results are on par with
our previous system, while the parser is orders of
magnitude faster both at training and prediction time
and is able to process natural language text in Cata-
lan, Chinese, Czech, English, German, Japanese and
Spanish. The parser depends on the input to be anno-
tated with part-of-speech tags and syntactic depen-
dencies.
2 Semantic parser
The semantic parser is implemented as a pipeline of
linear classifiers and a greedy constraint satisfaction
post-processing step. The implementation is very
similar to the best performing subsystem of the com-
mittee based system in Samuelsson et al (2008).
Parsing consists of four steps: predicate sense
disambiguation, argument identification, argument
classification and predicate frame constraint satis-
faction. The first three steps are implemented us-
ing linear classifiers, along with heuristic filtering
techniques. Classifiers are trained using the sup-
port vector machine implementation provided by the
LIBLINEAR software (Fan et al, 2008). MALLET
is used as a framework for the system (McCallum,
2002).
For each classifier, the c-parameter of the SVM
is optimised by a one dimensional grid search using
threefold cross validation on the training set. For
the identification step, the c-parameter is optimised
with respect to F1-score of the positive class, while
for sense disambiguation and argument labelling the
optimisation is with respect to accuracy. The regions
to search were identified by initial runs on the devel-
opment data. Optimising these parameters for each
classification problem individually proved to be cru-
cial for obtaining good results.
2.1 Predicate sense disambiguation
Since disambiguation of predicate sense is a multi-
class problem, I train the classifiers using the method
of Crammer and Singer (2002), using the implemen-
tation provided by LIBLINEAR. Sense labels do not
generalise over predicate lemmas, so one classifier
is trained for each lemma occurring in the training
data. Rare predicates are given the most common
sense of the predicate. Predicates occurring less than
103
7 times in the training data were heuristically deter-
mined to be considered rare. Predicates with unseen
lemmas are labelled with the most common sense
tag in the training data.
2.1.1 Feature templates
The following feature templates are used for predi-
cate sense disambiguation:
PREDICATEWORD
PREDICATE[POS/FEATS]
PREDICATEWINDOWBAGLEMMAS
PREDICATEWINDOWPOSITION[POS/FEATS]
GOVERNORRELATION
GOVERNOR[WORD/LEMMA]
GOVERNOR[POS/FEATS]
DEPENDENTRELATION
DEPENDENT[WORD/LEMMA]
DEPENDENT[POS/FEATS]
DEPENDENTSUBCAT.
The *WINDOW feature templates extract features
from the two preceding and the two following tokens
around the predicate, with respect to the linear order-
ing of the tokens. The *FEATS templates are based
on information in the PFEATS input column for the
languages where this information is provided.
2.2 Argument identification and labelling
In line with most previous pipelined systems, iden-
tification and labelling of arguments are performed
as two separate steps. The classifiers in the identi-
fication step are trained with the standard L2-loss
SVM formulation, while the classifiers in the la-
belling step are trained using the method of Cram-
mer and Singer.
In order to reduce the number of candidate argu-
ments in the identification step, I apply the filter-
ing technique of Xue and Palmer (2004), trivially
adopted to the dependency syntax formalism. Fur-
ther, a filtering heuristic is applied in which argu-
ment candidates with rare predicate / argument part-
of-speech combinations are removed; rare meaning
that the argument candidate is actually an argument
in less than 0.05% of the occurrences of the pair.
These heuristics greatly reduce the number of in-
stances in the argument identification step and im-
prove performance by reducing noise from the train-
ing data.
Separate classifiers are trained for verbal pred-
icates and for nominal predicates, both in order
to save computational resources and because the
frame structures do not generalise between verbal
and nominal predicates. For Czech, in order to re-
duce training time I split the argument identification
problem into three sub-problems: verbs, nouns and
others, based on the part-of-speech of the predicate.
In hindsight, after solving a file encoding related bug
which affected the separability of the Czech data
set, a split into verbal and nominal predicates would
have sufficed. Unfortunately I was not able to rerun
the Czech experiments on time.
2.2.1 Feature templates
The following feature templates are used both for
argument identification and argument labelling:
PREDICATELEMMASENSE
PREDICATE[POS/FEATS]
POSITION
ARGUMENT[POS/FEATS]
ARGUMENT[WORD/LEMMA]
ARGUMENTWINDOWPOSITIONLEMMA
ARGUMENTWINDOWPOSITION[POS/FEATS]
LEFTSIBLINGWORD
LEFTSIBLING[POS/FEATS]
RIGHTSIBLINGWORD
RIGHTSIBLING[POS/FEATS]
LEFTDEPENDENTWORD
RIGHTDEPENDENT[POS/FEATS]
RELATIONPATH
TRIGRAMRELATIONPATH
GOVERNORRELATION
GOVERNORLEMMA
GOVERNOR[POS/FEATS]
Most of these features, introduced by Gildea and Ju-
rafsky (2002), belong to the folklore by now. The
TRIGRAMRELATIONPATH is a ?soft? version of the
RELATIONPATH template, which treats the relation
path as a bag of triplets of directional labelled depen-
dency relations. Initial experiments suggested that
this feature slightly improves performance, by over-
coming local syntactic parse errors and data sparse-
ness in the case of small training sets.
2.2.2 Predicate frame constraints
Following Johansson and Nugues (2008) I impose
the CORE ARGUMENT CONSISTENCY and CON-
104
TINUATION CONSISTENCY constraints on the gen-
erated semantic frames. In the cited work, these
constraints are used to filter the candidate frames
for a re-ranker. I instead perform a greedy search
in which only the core argument with the highest
score is kept when the former constraint is violated.
The latter constraint is enforced by simply dropping
any continuation argument lacking its correspond-
ing core argument. Initial experiments on the de-
velopment data indicates that these simple heuristics
slightly improves semantic parsing quality measured
with labelled F1-score. It is possible that the im-
provement could be greater by using L2-regularised
logistic regression scores instead of the SVM scores,
since the latter can not be interpreted as probabili-
ties. However, logistic regression performed consis-
tently worse than the SVM formulation of Crammer
and Singer in the argument labelling step.
2.2.3 Handling of multi-function arguments
In Czech and Japanese an argument can have multi-
ple relations to the same predicate, i.e. the seman-
tic structure needs sometimes be represented by a
multi-graph. I chose the simplest possible solution
and treat these structures as ordinary graphs with
complex labels. This solution is motivated by the
fact that the palette of multi-function arguments is
small, and that the multiple functions mostly are
highly interdependent, such as in the ACT|PAT com-
plex which is the most common in Czech.
3 Results
The semantic parser was evaluated on in-domain
data for Catalan, Chinese, Czech, English, German,
Japanese and Spanish, and on out-of-domain data
for Czech, English and German. The respective
data sets are described in Taule? et al (2008), Palmer
and Xue (2009), Hajic? et al (2006), Surdeanu et al
(2008), Burchardt et al (2006) and Kawahara et al
(2002).
My official submission scores are given in table
1, together with post submission labelled and un-
labelled F1-scores. The official submissions were
affected by bugs related to file encoding and hyper-
parameter search. After resolving these bugs, I ob-
tained an improvement of mean F1-score of almost
10 absolute points compared to the official scores.
Lab F1 Lab F1 Unlab F1
Catalan 57.11 67.14 93.31
Chinese 63.41 74.14 82.57
Czech 71.05 78.29 89.20
English 67.64 78.93 88.70
German 53.42 62.98 89.64
Japanese 54.74 61.44 66.01
Spanish 61.51 69.93 93.54
Mean 61.27 70.41 86.14
Czech? 71.59 78.77 87.13
English? 59.82 68.96 86.23
German? 50.43 47.81 79.52
Mean? 60.61 65.18 84.29
Table 1: Semantic labelled and unlabelled F1-scores for
each language and domain. Left column: official labelled
F1-score. Middle column: post submission labelled F1-
score. Right column: post submission unlabelled F1-
score. ? indicates out-of-domain test data.
Clearly, there is a large difference in performance
for the different languages and domains. As could
be expected the parser performs much better for the
languages for which a large training set is provided.
However, as discussed in the next section, simply
adding more training data does not seem to solve the
problem.
Comparing unlabelled F1-scores with labelled
F1-scores, it seems that argument identification and
labelling errors contribute almost equally to the total
errors for Chinese, Czech and English. For Catalan,
Spanish and German argument identification scores
are high, while labelling scores are in the lower
range. Japanese stands out with exceptionally low
identification scores. Given that the quality of the
predicted syntactic parsing was higher for Japanese
than for any other language, the bottleneck when
performing semantic parsing seems to be the limited
expressivity of the Japanese syntactic dependency
annotation scheme.
Interestingly, for Czech, the result on the out-of-
domain data set is better than the result on the in-
domain data set, even though the unlabelled result
is slightly worse. For English, on the other hand
the performance drop is in the order of 10 absolute
labelled F1 points, while the drop in unlabelled F1-
score is comparably small. The result on German
out-of-domain data seems to be an outlier, with post-
submission results even worse than the official sub-
105
10% 25% 50% 75% 100%
Catalan 54.86 60.52 65.22 66.35 67.14
Chinese 72.93 73.40 73.77 74.08 74.14
Czech 75.42 76.90 77.69 78.00 78.29
English 75.75 77.56 78.37 78.71 78.93
German 47.77 54.74 58.94 61.02 62.98
Japanese 59.82 60.34 60.99 61.37 61.44
Spanish 58.80 64.32 68.35 69.34 69.93
Mean 63.62 66.83 69.05 69.84 70.41
Czech? 76.51 77.48 78.41 78.59 78.77
English? 66.04 67.54 68.37 69.00 68.96
German? 41.65 45.94 46.24 47.45 47.81
Mean? 61.40 63.65 64.34 65.01 65.18
Table 2: Semantic labelled F1-scores w.r.t. training set
size. ? indicates out-of-domain test data.
mission results. I suspect that this is due to a bug.
3.1 Learning rates
In order to assess the effect of training set size on
semantic parsing quality, I performed a learning rate
experiment, in which the proportion of the training
set used for training was varied in steps between
10% and 100% of the full training set size.
Learning rates with respect to labelled F1-scores
are given in table 2. The improvement in scores are
modest for Chinese, Czech, English and Japanese,
while Catalan, German and Spanish stand out by
vast improvements with additional training data.
However, the improvement when going from 75% to
100% of the training data is only modest for all lan-
guages. With the exception for English, for which
the parser achieves the highest score, the relative
labelled F1-scores follow the relative sizes of the
training sets.
Looking at learning rates with respect to unla-
belled F1-scores, given in table 3, it is evident that
adding more training data only has a minor effect on
the identification of arguments.
From table 4, one can see that predicate sense dis-
ambiguation is the sub-task that benefits most from
additional training data. This is not surprising, since
the senses does not generalise, and hence we cannot
hope to correctly label the senses of unseen predi-
cates; the only way to improve results with the cur-
rent formalism seems to be by adding more training
data.
The limited power of a pipeline of local classi-
10% 25% 50% 75% 100%
Catalan 93.12 93.18 93.28 93.35 93.31
Chinese 82.37 82.45 82.54 82.55 82.57
Czech 89.03 89.12 89.17 89.21 89.20
English 87.96 88.38 88.52 88.67 88.70
German 88.23 89.02 89.63 89.53 89.64
Japanese 65.64 65.75 65.88 66.02 66.01
Spanish 93.52 93.49 93.52 93.53 93.54
Mean 85.70 85.91 86.08 86.12 86.14
Czech? 86.76 87.02 87.16 87.08 87.13
English? 85.67 86.14 86.22 86.20 86.23
German? 77.35 78.31 79.09 79.10 79.52
Mean? 83.26 83.82 84.16 84.13 84.29
Table 3: Semantic unlabelled F1-scores w.r.t. training set
size. ? indicates out-of-domain test data.
10% 25% 50% 75% 100%
Catalan 30.61 40.29 53.83 55.83 58.95
Chinese 94.06 94.37 94.71 95.10 95.26
Czech 83.24 84.75 85.78 86.21 86.60
English 92.18 93.68 94.83 95.35 95.60
German 34.91 47.27 58.18 62.18 66.55
Japanese 99.07 99.07 99.07 99.07 99.07
Spanish 38.53 50.22 59.59 62.01 66.26
Mean 67.51 72.81 78.00 79.39 81.18
Czech? 89.05 89.88 91.06 91.38 91.56
English? 83.64 84.27 84.83 85.70 85.94
German? 33.64 43.36 42.59 44.44 45.22
Mean? 68.78 72.51 72.83 73.84 74.24
Table 4: Predicate sense disambiguation F1-scores w.r.t.
training set size. ? indicates out-of-domain test data.
fiers shows itself in the exact match scores, given
in table 5. This problem is clearly not remedied by
additional training data.
3.2 Dependence on syntactic parsing quality
Since I only participated in the semantic parsing
task, the results reported above rely on the provided
predicted syntactic dependency parsing. In order to
investigate the effect of parsing quality on the cur-
rent system, I performed the same learning curve
experiments with gold standard parse information.
These results, shown in tables 6 and 7, give an upper
bound on the possible improvement of the current
system by means of improved parsing quality, given
that the same syntactic annotation formalism is used.
Labelled F1-scores are greatly improved for all
languages except for Japanese, when using gold
106
10% 25% 50% 75% 100%
Catalan 6.77 9.08 11.39 11.17 12.24
Chinese 17.02 17.33 17.61 17.76 17.68
Czech 9.33 9.59 9.97 9.95 10.11
English 12.01 12.76 12.96 13.13 13.17
German 76.95 78.50 78.95 79.20 79.50
Japanese 1.20 1.40 1.80 1.60 1.60
Spanish 8.23 10.20 12.93 13.39 13.16
Mean 18.79 19.84 20.80 20.89 21.07
Czech? 2.53 2.79 2.79 2.87 2.87
English? 19.06 19.53 19.76 20.00 20.00
German? 15.98 19.24 17.82 19.94 20.08
Mean? 12.52 13.85 13.46 14.27 14.32
Table 5: Percentage of exactly matched predicate-
argument frames w.r.t. training set size. ? indicates out-
of-domain test data.
10% 25% 50% 75% 100%
Catalan 62.65 72.50 75.39 77.03 78.86
Chinese 82.59 83.23 83.90 83.94 84.03
Czech 79.15 80.62 81.46 81.91 82.24
English 79.84 81.74 82.65 83.01 83.25
German 52.15 60.66 65.12 65.71 68.36
Japanese 60.85 61.76 62.55 62.85 63.23
Spanish 66.40 72.47 75.70 77.73 78.38
Mean 69.09 73.28 75.25 76.03 76.91
Czech? 78.64 80.07 80.77 81.01 81.20
English? 73.05 74.18 74.99 75.28 75.81
German? 52.06 52.77 54.72 56.22 56.35
Mean? 67.92 69.01 70.16 70.84 71.12
Table 6: Semantic labelled F1-scores w.r.t. training set
size, using gold standard syntactic and part-of-speech tag
annotation. ? indicates out-of-domain test data.
standard syntactic and part-of-speech annotations.
For Catalan, Chinese and Spanish the improvement
is in the order of 10 absolute points. For Japanese
the improvement is a meagre 2 absolute points. This
is not surprising given that the quality of the pro-
vided syntactic parsing was already very high for
Japanese, as discussed previously.
Results with respect to unlabelled F1-scores fol-
low the same pattern as for labelled F1-scores.
Again, with Japanese the semantic parsing does not
benefit much from better syntactic parsing quality.
For Catalan and Spanish on the other hand, the iden-
tification of arguments is almost perfect with gold
standard syntax. The poor labelling quality for these
languages can thus not be attributed to the syntactic
10% 25% 50% 75% 100%
Catalan 99.94 99.98 99.99 99.99 99.99
Chinese 92.55 92.67 92.72 92.63 92.62
Czech 91.21 91.27 91.30 91.30 91.31
English 92.34 92.61 92.85 92.89 92.95
German 93.46 93.59 94.08 93.85 94.14
Japanese 66.98 67.20 67.58 67.62 67.74
Spanish 99.99 99.99 100.00 100.00 100.00
Mean 90.92 91.04 91.22 91.18 91.25
Czech? 89.00 89.22 89.34 89.38 89.36
English? 92.71 92.56 92.91 93.06 93.04
German? 90.54 90.23 90.77 90.86 90.99
Mean? 90.75 90.67 91.01 91.10 91.13
Table 7: Semantic unlabelled F1-scores w.r.t. training set
size, using gold standard syntactic and part-of-speech tag
annotation. ? indicates out-of-domain test data.
parse quality.
3.3 Computational requirements
Training and prediction times on a 2.3 GHz quad-
core AMD OpteronTMsystem are given in table 8.
Since only linear classifiers and no pair-wise feature
combinations are used, training and prediction times
are quite modest. Verbal and nominal predicates are
trained in parallel, no additional parallelisation is
employed. Most of the training time is spent on op-
timising the c parameter of the SVM. Training times
are roughly ten times as long as compared to training
times with no hyper-parameter optimisation. Czech
stands out as much more computationally demand-
ing, especially in the sense disambiguation training
step. The reason is the vast number of predicates in
Czech compared to the other languages. The ma-
jority of the time in this step is, however, spent on
writing the SVM training problems to disk.
Memory requirements range between approxi-
mately 1 Gigabytes for the smallest data sets and
6 Gigabytes for the largest data set. Memory us-
age could be lowered substantially by using a more
compact feature dictionary. Currently every feature
template / value pair is represented as a string, which
is wasteful since many feature templates share the
same values.
4 Conclusions
I have presented an effective multilingual pipelined
semantic parser, using linear classifiers and a simple
107
Sense ArgId ArgLab Tot Pred
Catalan 7m 11m 33m 51m 13s
Chinese 7m 13m 22m 42m 15s
Czech 10h 1h 1.5h 12.5h 34.5m
English 16m 14m 28m 58m 14.5s
German 4m 2m 5m 13m 3.5s
Japanese 1s 1m 4m 5m 4s
Spanish 10m 16m 40m 1.1h 13s
Table 8: Training times for each language and sub-
problem and approximate prediction times. Columns:
training times for sense disambiguation (Sense), ar-
gument identification (ArgId), argument labelling (Ar-
gLab), total training time (Tot), and total prediction time
(Pred). Training times are measured w.r.t. to the union
of the official training and development data sets. Predic-
tion times are measured w.r.t. to the official evaluation
data sets.
greedy constraint satisfaction heuristic. While the
semantic parsing results in these experiments fail to
reach the best results given by other experiments, the
parser quickly delivers quite accurate semantic pars-
ing of Catalan, Chinese, Czech, English, German,
Japanese and Spanish.
Optimising the hyper-parameters of each of the
individual classifiers is essential for obtaining good
results with this simple architecture. Syntactic pars-
ing quality has a large impact on the quality of the
semantic parsing; a problem that is not remedied by
adding additional training data.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Koby Crammer and Yoram Singer. 2002. On the learn-
ability and design of output codes for multiclass prob-
lems. Machine Learning, 47(2):201?233, May.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague
Dependency Treebank 2.0. CD-ROM, Cat. No.
LDC2006T01, ISBN 1-58563-370-4, Linguistic Data
Consortium, Philadelphia, Pennsylvania, USA.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008, Manchester,
UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Yvonne Samuelsson, Oscar Ta?ckstro?m, Sumithra
Velupillai, Johan Eklund, Mark Fishel, and Markus
Saers. 2008. Mixing and blending syntactic and
semantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 248?252, Manchester,
England, August. Coling 2008 Organizing Committee.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), Manchester, Great Britain.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 88?94, Barcelona, Spain, July. Association for
Computational Linguistics.
108
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477?487,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure
Oscar Ta?ckstro?m?
SICS / Uppsala University
Kista / Uppsala, Sweden
oscar@sics.se
Ryan McDonald
Google
New York, NY
ryanmcd@google.com
Jakob Uszkoreit
Google
Mountain View, CA
uszkoreit@google.com
Abstract
It has been established that incorporating word
cluster features derived from large unlabeled
corpora can significantly improve prediction of
linguistic structure. While previous work has
focused primarily on English, we extend these
results to other languages along two dimen-
sions. First, we show that these results hold
true for a number of languages across families.
Second, and more interestingly, we provide an
algorithm for inducing cross-lingual clusters
and we show that features derived from these
clusters significantly improve the accuracy of
cross-lingual structure prediction. Specifically,
we show that by augmenting direct-transfer sys-
tems with cross-lingual cluster features, the rel-
ative error of delexicalized dependency parsers,
trained on English treebanks and transferred
to foreign languages, can be reduced by up to
13%. When applying the same method to di-
rect transfer of named-entity recognizers, we
observe relative improvements of up to 26%.
1 Introduction
The ability to predict the linguistic structure of sen-
tences or documents is central to the field of nat-
ural language processing (NLP). Structures such as
named-entity tag sequences (Bikel et al, 1999) or sen-
timent relations (Pang and Lee, 2008) are inherently
useful in data mining, information retrieval and other
user-facing technologies. More fundamental struc-
tures such as part-of-speech tag sequences (Ratna-
parkhi, 1996) or syntactic parse trees (Collins, 1997;
Ku?bler et al, 2009), on the other hand, comprise the
core linguistic analysis for many important down-
stream tasks such as machine translation (Chiang,
?The majority of this work was performed while the author
was an intern at Google, New York, NY.
2005; Collins et al, 2005). Currently, supervised
data-driven methods dominate the literature on lin-
guistic structure prediction (Smith, 2011). Regret-
tably, the majority of studies on these methods have
focused on evaluations specific to English, since it is
the language with the most annotated resources. No-
table exceptions include the CoNLL shared tasks
(Tjong Kim Sang, 2002; Tjong Kim Sang and
De Meulder, 2003; Buchholz and Marsi, 2006; Nivre
et al, 2007) and subsequent studies on this data, as
well as a number of focused studies on one or two
specific languages, as discussed by Bender (2011).
While annotated resources for parsing and several
other tasks are available in a number of languages, we
cannot expect to have access to labeled resources for
all tasks in all languages. This fact has given rise to
a large body of research on unsupervised (Klein and
Manning, 2004), semi-supervised (Koo et al, 2008)
and transfer (Hwa et al, 2005) systems for prediction
of linguistic structure. These methods all attempt to
benefit from the plethora of unlabeled monolingual
and/or cross-lingual data that has become available
in the digital age. Unsupervised methods are ap-
pealing in that they are often inherently language
independent. This is borne out by the many recent
studies on unsupervised parsing that include evalu-
ations covering a number of languages (Cohen and
Smith, 2009; Gillenwater et al, 2010; Naseem et al,
2010; Spitkovsky et al, 2011). However, the perfor-
mance for most languages is still well below that of
supervised systems and recent work has established
that the performance is also below simple methods
of linguistic transfer (McDonald et al, 2011).
In this study we focus on semi-supervised and
linguistic-transfer methods for multilingual structure
prediction. In particular, we pursue two lines of re-
search around the use of word cluster features in
discriminative models for structure prediction:
477
1. Monolingual word cluster features induced from
large corpora of text for semi-supervised learn-
ing (SSL) of linguistic structure. Previous stud-
ies on this approach have typically focused only
on a small set of languages and tasks (Freitag,
2004; Miller et al, 2004; Koo et al, 2008;
Turian et al, 2010; Faruqui and Pado?, 2010; Haf-
fari et al, 2011; Tratz and Hovy, 2011). Here
we show that this method is robust across 13 lan-
guages for dependency parsing and 4 languages
for named-entity recognition (NER). This is the
first study with such a broad view on this subject,
in terms of language diversity.
2. Cross-lingual word cluster features for transfer-
ring linguistic structure from English to other
languages. We develop an algorithm that gener-
ates cross-lingual word clusters; that is clusters
of words that are consistent across languages.
This is achieved by means of a probabilistic
model over large amounts of monolingual data
in two languages, coupled with parallel data
through which cross-lingual word-cluster con-
straints are enforced. We show that by augment-
ing the delexicalized direct transfer system of
McDonald et al (2011) with cross-lingual clus-
ter features, we are able to reduce its error by
up to 13% relative. Further, we show that by ap-
plying the same method to direct-transfer NER,
we achieve a relative error reduction of 26%.
By incorporating cross-lingual cluster features in a
linguistic transfer system, we are for the first time
combining SSL and cross-lingual transfer.
2 Monolingual Word Cluster Features
Word cluster features have been shown to be use-
ful in various tasks in natural language processing,
including syntactic dependency parsing (Koo et al,
2008; Haffari et al, 2011; Tratz and Hovy, 2011),
syntactic chunking (Turian et al, 2010), and NER
(Freitag, 2004; Miller et al, 2004; Turian et al, 2010;
Faruqui and Pado?, 2010). Intuitively, the reason for
the effectiveness of cluster features lie in their abil-
ity to aggregate local distributional information from
large unlabeled corpora, which aid in conquering data
sparsity in supervised training regimes as well as in
mitigating cross-domain generalization issues.
In line with much previous work on word clusters
for tasks such as dependency parsing and NER, for
which local syntactic and semantic constraints are
of importance, we induce word clusters by means of
a probabilistic class-based language model (Brown
et al, 1992; Clark, 2003). However, rather than the
more commonly used model of Brown et al (1992),
we use the predictive class bigram model introduced
by Uszkoreit and Brants (2008). The two models
are very similar, but whereas the former takes class-
to-class transitions into account, the latter directly
models word-to-class transitions. By ignoring class-
to-class transitions, an approximate maximum likeli-
hood clustering can be found efficiently with the dis-
tributed exchange algorithm (Uszkoreit and Brants,
2008). This is a useful property, as we later develop
an algorithm for inducing cross-lingual word clusters
that calls this monolingual algorithm as a subroutine.
More formally, let C : V 7? 1, . . . ,K be a (hard)
clustering function that maps each word type from the
vocabulary, V , to one ofK cluster identities. With the
model of Uszkoreit and Brants (2008), the likelihood
of a sequence of word tokens, w = ?wi?
m
i=1, with
wi ? V ? {S}, where S is a designated start-of-
segment symbol, factors as
L(w; C) =
m?
i=1
p(wi|C(wi))p(C(wi)|wi?1) . (1)
Compare this to the model of Brown et al (1992):
L?(w; C) =
m?
i=1
p(wi|C(wi))p(C(wi)|C(wi?1)) .
While the use of class-to-class transitions can lead
to more compact models, which is often useful for
conquering data sparsity, when clustering large data
sets we can get reliable statistics directly on the word-
to-class transitions (Uszkoreit and Brants, 2008).
In addition to the clustering model that we make
use of in this study, a number of additional word
clustering and embedding variants have been pro-
posed. For example, Turian et al (2010) assessed
the effectiveness of the word embedding techniques
of Collobert and Weston (2008) and Mnih and Hin-
ton (2007) along with the word clustering technique
of Brown et al (1992) for syntactic chunking and
NER. Recently, Dhillon et al (2011) proposed a word
478
Single words S0c{p}, N0c{p}, N1c{p}, N2c{p}
Word pairs S0c{p}N0c{p}, S0pcN0p, S0pN0pc,
S0wN0c, S0cN0w, N0cN1c, N1cN2c
Word triples N0cN1cN2c, S0cN0cN1c, S0hcS0cN0c,
S0cS0lcN0c, S0cS0rcN0c, S0cN0cN0lc
Distance S0cd, N0cd, S0cN0cd
Valency S0cvl, S0cvr , N0cS0vl
Unigrams S0hc, S0lc, S0rc, N0lc
Third-order S0h2c, S0l2c, S0r2c, N0l2c
Label set S0cS0ll, S0cS0rl, N0cN0ll, N0cN0rl
Table 1: Additional cluster-based parser features. Si and
Ni: the ith tokens in the stack and buffer. p: the part-of-
speech tag, c: the cluster. v: the valence of the left (l) or
right (r) set of children. l: the label of the token under
consideration. d: distance between the words on the top of
the stack and buffer. Sih, Sir and Sil: the head, right-most
modifier and left-most modifier of the token at the top of
the stack. Gx{y} expands to Gxy and Gx.
embedding method based on canonical correlation
analysis that provides state-of-the art results for word-
based SSL for English NER. As an alternative to clus-
tering words, Lin and Wu (2009) proposed a phrase
clustering approach that obtained the state-of-the-art
result for English NER.
3 Monolingual Cluster Experiments
Before moving on to the multilingual setting, we
conduct a set of monolingual experiments where we
evaluate the use of the monolingual word clusters
just described as features for dependency parsing and
NER. In the parsing experiments, we study the fol-
lowing thirteen languages:1 Danish (DA), German
(DE), Greek (EL), English (EN), Spanish (ES), French
(FR), Italian (IT), Korean (KO), Dutch (NL), Portugese
(PT), Russian (RU), Swedish (SV) and Chinese (ZH)
? representing the Chinese, Germanic, Hellenic, Ro-
mance, Slavic, Altaic and Korean genera. In the NER
experiments, we study three Germanic languages:
German (DE), English (EN) and Dutch (NL); and one
Romance language: Spanish (ES).
Details of the labeled and unlabeled data sets used
are given in Appendix A. For all experiments we
fixed the number of clusters to 256 as this performed
well on held-out data. Furthermore, we only clus-
tered the 1 million most frequent word types in each
language for both efficiency and sparsity reasons. For
1The particular choice of languages was made purely based
on data availability and institution licensing.
Word & bias w?1,0,1, w?1:0, w0:1, w?1:1, b
Pre-/suffix w:1,:2,:3,:4,:5?1,0,1 , w
?5:,?4:,?3:,?2:,?1:
?1,0,1
Orthography Hyp?1,0,1, Cap?1,0,1, Cap?1:0,
Cap0:1, Cap?1:1
PoS p?1,0,1, p?1:0, p0:1, p?1:1, p?2:1, p?1:2
Cluster c?1,0,1, c?1:0, c0:1, c?1:1, c?2:1, c?1:2
Transition ?/p?1,0,1,?/c?1,0,1,?/Cap?1,0,1,?/b
Table 2: NER features. Hyp: Word contains hyphen. Cap:
First letter is capitalized. ?/f - Transition from previous
to current label conjoined with feature f . w:j : j-character
prefix of w. w?j:: j-character suffix of w. fi: Feature f
at relative position i. fi,j : Union of features at positions i
and j. fi:j : Conjoined feature sequence between relative
positions i and j (inclusive). b: Bias.
languages in which our unlabeled data did not have
at least 1 million types, we considered all types.
3.1 Cluster Augmented Feature Models
All of the parsing experiments reported in this study
are based on the transition-based dependency parsing
paradigm (Nivre, 2008). For all languages and set-
tings, we use an arc-eager decoding strategy, with a
beam of eight hypotheses, and perform ten epochs of
the averaged structured perceptron algorithm (Zhang
and Clark, 2008). We extend the state-of-the-art fea-
ture model recently introduced by Zhang and Nivre
(2011) by adding an additional word cluster based
feature template for each word based template. Ad-
ditionally, we add templates where one or more part-
of-speech feature is replaced with the corresponding
cluster feature. The resulting set of additional fea-
ture templates are shown in Table 1. The expanded
feature model includes all of the feature templates de-
fined by Zhang and Nivre (2011), which we also use
as the baseline model, whereas Table 1 only shows
our new templates due to space limitations.
For all NER experiments, we use a sequential first-
order conditional random field (CRF) with a unit
variance Normal prior, trained with L-BFGS until
-convergence ( = 0.0001, typically obtained after
less than 400 iterations). The feature model used
for the NER tagger is shown in Table 2. These are
similar to the features used by Turian et al (2010),
with the main difference that we do not use any long
range features and that we add templates that conjoin
adjacent clusters and adjacent tags as well as tem-
plates that conjoin label transitions with tags, clusters
and capitalization features.
479
DA DE EL EN ES FR IT KO NL PT RU SV ZH AVG
NO CLUSTERS 84.3 88.9 76.1 90.3 82.8 85.7 81.4 82.0 77.2 86.9 83.5 84.7 74.9 83.0
CLUSTERS 85.8 89.5 77.3 90.7 83.6 85.7 82.2 83.6 77.8 87.6 86.0 86.5 75.5 84.0
Table 3: Supervised parsing results measured with labeled attachment score (LAS) on the test set. All results are
statistically significant at p < 0.05, except FR and NL.
DE EN ES NL AVG
NO CLUSTERS 65.4 89.2 75.0 75.7 76.3
CLUSTERS 74.8 91.8 81.1 84.2 83.0
? DEVELOPMENT SET ? TEST SET
NO CLUSTERS 69.1 83.5 78.9 79.6 77.8
CLUSTERS 74.4 87.8 82.0 85.7 82.5
Table 4: Supervised NER results measured with F1-score
on the CoNLL 2002/2003 development and test sets.
3.2 Results
The results of the parsing experiments, measured
with labeled accuracy score (LAS) on all sentence
lengths, excluding punctuation, are shown in Table 3.
The baselines are all comparable to the state-of-the-
art. On average, the addition of word cluster features
yields a 6% relative reduction in error and upwards
of 15% (for RU). All languages improve except FR,
which sees neither an increase nor a decrease in LAS.
We observe an average absolute increase in LAS
of approximately 1%, which is inline with previous
observations (Koo et al, 2008). It is perhaps not
surprising that RU sees a large gain as it is a highly
inflected language, making observations of lexical
features far more sparse. Some languages, e.g., FR,
NL, and ZH see much smaller gains. One likely cul-
prit is a divergence between the tokenization schemes
used in the treebank and in our unlabeled data, which
for Indo-European languages is closely related to the
Penn Treebank tokenization. For example, the NL
treebank contains many multi-word tokens that are
typically broken apart by our automatic tokenizer.
The NER results, in terms of F1 measure, are listed
in Table 4. Introducing word cluster features for
NER reduces relative errors on the test set by 21%
(39% on the development set) on average. Broken
down per language, reductions on the test set vary
from substantial for NL (30%) and EN (26%), down
to more modest for DE (17%) and ES (12%). The
addition of cluster features most markedly improve
recognition of the PER category, with an average error
reduction on the test set of 44%, while the reductions
for ORG (19%), LOC (17%) and MISC (10%) are more
modest, but still significant. Although our results
are below the best reported results for EN and DE
(Lin and Wu, 2009; Faruqui and Pado?, 2010), the
relative improvements of adding word clusters are
inline with previous results on NER for EN (Miller
et al, 2004; Turian et al, 2010), who report error
reductions of approximately 25% from adding word
cluster features. Slightly higher reductions where
achieved for DE by Faruqui and Pado? (2010), who
report a reduction of 22%. Note that we did not tune
hyper-parameters of the supervised learning methods
and of the clustering method, such as the number
of clusters (Turian et al, 2010; Faruqui and Pado?,
2010), and that we did not apply any heuristic for data
cleaning such as that used by Turian et al (2010).
4 Cross-lingual Word Cluster Features
All results of the previous section rely on the avail-
ability of large quantities of language specific anno-
tations for each task. Cross-lingual transfer methods
and unsupervised methods have recently been shown
to hold promise as a way to at least partially sidestep
the demand for labeled data. Unsupervised methods
attempt to infer linguistic structure without using any
annotated data (Klein and Manning, 2004) or possi-
bly by using a set of linguistically motivated rules
(Naseem et al, 2010) or a linguistically informed
model structure (Berg-Kirkpatrick and Klein, 2010).
The aim of transfer methods is instead to use knowl-
edge induced from labeled resources in one or more
source languages to construct systems for target lan-
guages in which no or few such resources are avail-
able (Hwa et al, 2005). Currently, the performance
of even the most simple direct transfer systems far
exceeds that of unsupervised systems (Cohen et al,
2011; McDonald et al, 2011; S?gaard, 2011).
480
Figure 1: Cross-lingual word cluster features for parsing. Top-left: Cross-lingual (EN-ES) word clustering model.
Top-right: Samples of some of the induced cross-lingual word clusters. Bottom-left: Delexicalized cluster-augmented
source (EN) treebank for training transfer parser. Bottom-right: Parsing of target (ES) sentence using the transfer parser.
4.1 Direct Transfer of Discriminative Models
Our starting point is the delexicalized direct transfer
method proposed by McDonald et al (2011) based on
work by Zeman and Resnik (2008). This method was
shown to outperform a number of state-of-the-art un-
supervised and transfer-based baselines. The method
is simple; for a given training set, the learner ignores
all lexical identities and only observes features over
other characteristics, e.g., part-of-speech tags, ortho-
graphic features, direction of syntactic attachment,
etc. The learner builds a model from an annotated
source language data set, after which the model is
used to directly make target language predictions.
There are three basic assumptions that drive this ap-
proach. First, that high-level tasks, such as syntactic
parsing, can be learned reliably using coarse-grained
statistics, such as part-of-speech tags, in place of
fine-grained statistics such as lexical word identities.
Second, that the parameters of features over coarse-
grained statistics are in some sense language inde-
pendent, e.g., a feature that indicates that adjectives
modify their closest noun is useful in all languages.
Third, that these coarse-grained statistics are robustly
available across languages. The approach proposed
by McDonald et al (2011) relies on these three as-
sumptions. Specifically, by replacing fine-grained
language specific part-of-speech tags with universal
part-of-speech tags, generated with the method de-
scribed by Das and Petrov (2011), a universal parser
is achieved that can be applied to any language for
which universal part-of-speech tags are available.
Below, we extend this approach to universal pars-
ing by adding cross-lingual word cluster features. A
cross-lingual word clustering is a clustering of words
in two languages, in which the clusters correspond to
some meaningful cross-lingual property. For exam-
ple, prepositions from both languages should be in
the same cluster, proper names from both languages
in another cluster and so on. By adding features de-
fined over these clusters, we can, to some degree,
481
re-lexicalize the delexicalized models, while main-
taining the ?universality? of the features. This ap-
proach is outlined in Figure 1. Assuming that we
have an algorithm for generating cross-lingual word
clusters (see Section 4.2), we can augment the delex-
icalized parsing algorithm to use these word cluster
features at training and testing time.
In order to further motivate the proposed approach,
consider the accuracy of the supervised English
parser. A parser with lexical, part-of-speech and
cluster features achieves 90.7% LAS (see Table 3). If
we remove all lexical and cluster features, the same
parser achieves 83.1%. However, if we add back just
the cluster features, the accuracy jumps back up to
89.5%, which is only 1.2% below the full system.
Thus, if we can accurately learn cross-lingual clus-
ters, there is hope of regaining some of the accuracy
lost due to the delexicalization process.
4.2 Inducing Cross-lingual Word Clusters
Our first method for inducing cross-lingual clusters
has two stages. First, it clusters a source language
(S) as in the monolingual case, and then projects
these clusters to a target language (T), using word
alignments. Given two aligned word sequences
wS =
?
wSi
?mS
i=1 and w
T =
?
wTi
?mT
j=1, let A
T |S be a
set of scored alignments from the source language to
the target language, where (wTj , w
S
aj , sj,aj ) ? A
T |S
is an alignment from the aj th source word to the jth
target word, with score sj,aj ? ?.
2 We use the short-
hand j ? AT |S to denote those target words wTj that
are aligned to some source word wSaj . Provided a
clustering CS , we assign the target word t ? VT to
the cluster with which it is most often aligned:
CT (t) = argmax
k
?
j?AT |S
s.t. wTj =t
sj,aj
[
CS(wSaj ) = k
]
, (2)
where [?] is the indicator function. We refer to the
cross-lingual clusters induced in this way as PRO-
JECTED CLUSTERS.
This simple projection approach has two potential
drawbacks. First, it only provides a clustering of
those target language words that occur in the word
2In our case, the alignment score corresponds to the condi-
tional alignment probability p(wTj |w
S
aj ). All -alignments are
ignored and we use ? = 0.95 throughout.
aligned data, which is typically smaller than our
monolingual data sets. Second, the mapped cluster-
ing may not necessarily correspond to an acceptable
target language clustering in terms of monolingual
likelihood. In order to tackle these issues, we pro-
pose the following more complex model. First, to
find clusterings that are good according to both the
source and target language, and to make use of more
unlabeled data, we model word sequences in each lan-
guage by the monolingual language model with like-
lihood function defined by equation (1). Denote these
likelihood functions respectively by LS(wS ; CS) and
LT (wT ; CT ), where we have overloaded notation so
that the word sequences denoted by wS and wT in-
clude much more plentiful non-aligned data when
taken as an argument of the monolingual likelihood
functions. Second, we couple the clusterings defined
by these individual models, by introducing additional
factors based on word alignments, as proposed by
Och (1999):
LT |S(wT ;AT |S , CT , CS) =
?
j?AT |S
p(wTj |C
T (wTj ))p(C
T (wTj )|C
S(wSaj )) .
and the symmetric LS|T (wS ;AS|T , CS , CT ). Note
that the simple projection defined by equation (2)
correspond to a hard assignment variant of this prob-
abilistic formulation when the source clustering is
fixed. Combining all four factors results in the joint
monolingual and cross-lingual objective function
LS,T (wS ,wT ;AT |S ,AS|T , CS , CT ) =
LS(. . .) ? LT (. . .) ? LT |S(. . .) ? LS|T (. . .) . (3)
The intuition of this approach is that the clusterings
CS and CT are forced to jointly explain the source
and target data, treating the word alignments as a
form of soft constraints. We approximately optimize
(3) with the alternating procedure in Algorithm 1, in
which we iteratively maximize LS and LT , keeping
the other factors fixed. In this way we can generate
cross-lingual clusterings using all the monolingual
data while forcing the clusterings to obey the word
alignment constraints. We refer to the clusters in-
duced with this method as X-LINGUAL CLUSTERS.
In practice we found that each unconstrained
monolingual run of the exchange algorithm (lines
482
Algorithm 1 Cross-lingual clustering.
Randomly initialize source/target clusterings CS and CT .
for i = 1 . . . N do
1. Find C?S ? argmaxCS L
S(wS ; CS). (?)
2. Project C?S to CT using equation (2).
- keep cluster of non-projected words in CT fixed.
3. Find C?T ? argmaxCT L
T (wT ; CT ). (?)
4. Project C?T to CS using equation (2).
- keep cluster of non-projected words in CS fixed.
end for
? Optimized via the exchange algorithm keeping the cluster
of projected words fixed and only clustering additional words
not in the projection.
1 and 3) moves the clustering too far from those that
obey the word alignment constraints, which causes
the procedure to fail to converge. However, we found
that fixing the clustering of the words that are as-
signed clusters in the projection stages (lines 2 and
4) and only clustering the remaining words works
well in practice. Furthermore, we found that iterating
the procedure has little effect on performance and set
N = 1 for all subsequent experiments.
5 Cross-lingual Experiments
In our first set of experiments on using cross-lingual
cluster features, we evaluate direct transfer of our
EN parser, trained on Stanford style dependencies
(De Marneffe et al, 2006), to the the ten non-EN
Indo-European languages listed in Section 3. We ex-
clude KO and ZH as initial experiments proved direct
transfer a poor technique when transferring parsers
between such diverse languages. We study the impact
of using cross-lingual cluster features by comparing
the strong delexicalized baseline model of McDon-
ald et al (2011), which only has features derived
from universal part-of-speech tags, projected from
English with the method of Das and Petrov (2011), to
the same model when adding features derived from
cross-lingual clusters. In both cases the feature mod-
els are the same as those used in Section 3.1, except
that they are delexicalized by removing all lexical
word-identity features. We evaluate both the PRO-
JECTED CLUSTERS and the X-LINGUAL CLUSTERS.
For these experiments we train the perceptron
for only five epochs in order to prevent over-fitting,
which is an acute problem due to the divergence be-
tween the training and testing data sets in this setting.
Furthermore, in accordance to standard practices we
only evaluate unlabeled attachment score (UAS) due
to the fact that each treebank uses a different ? possi-
bly non-overlapping ? label set.
In our second set of experiments, we evaluate di-
rect transfer of a NER system trained on EN to DE,
ES and NL. We use the same feature models as in
the monolingual case, with the exception that we use
universal part-of-speech tags for all languages and
we remove the capitalization feature when transfer-
ring from EN to DE. Capitalization is both a prevalent
and highly predictive feature of named-entities in EN,
while in DE, capitalization is even more prevalent, but
has very low predictive power. Interestingly, while
delexicalization has shown to be important for di-
rect transfer of dependency-parsers (McDonald et al,
2011), we noticed in preliminary experiments that
it substantially degrades performance for NER. We
hypothesize that this is because word features are pre-
dictive of common proper names and that these are
often translated directly across languages, at least in
the case of newswire text. As for the transfer parser,
when training the source NER model, we regularize
the model more heavily by setting ? = 0.1.
Appendix A contains the details of the training,
testing, unlabeled and parallel/aligned data sets.
5.1 Results
Table 5 lists the results of the transfer experiments
for dependency parsing. The baseline results are
comparable to those in McDonald et al (2011) and
thus also significantly outperform the results of re-
cent unsupervised approaches (Berg-Kirkpatrick and
Klein, 2010; Naseem et al, 2010). Importantly, cross-
lingual cluster features are helpful across the board
and give a relative error reduction ranging from 3%
for DA to 13% for PT, with an average reduction of
6%, in terms of unlabeled attachment score (UAS).
This shows the utility of cross-lingual cluster fea-
tures for syntactic transfer. However, X-LINGUAL
CLUSTERS provides roughly the same performance
as PROJECTED CLUSTERS suggesting that even sim-
ple methods of cross-lingual clustering are sufficient
for direct transfer dependency parsing.
We would like to stress that these results are likely
to be under-estimating the parsers? actual ability to
predict Stanford-style dependencies in the target lan-
guages. This is because the target language anno-
tations that we use for evaluation differ from the
483
DA DE EL ES FR IT NL PT RU SV AVG
NO CLUSTERS 36.7 48.9 59.5 60.2 70.0 64.6 52.8 66.8 29.7 55.4 54.5
PROJECTED CLUSTERS 38.9 50.3 61.1 62.6 71.6 68.6 54.5 70.7 32.9 57.0 56.8
X-LINGUAL CLUSTERS 38.7 50.7 63.0 62.9 72.1 68.8 54.3 71.0 34.4 56.9 57.3
? ALL DEPENDENCY RELATIONS ? ONLY SUBJECT/OBJECT RELATIONS
NO CLUSTERS 44.6 56.7 67.2 60.7 77.4 64.6 59.5 53.3 29.3 57.3 57.1
PROJECTED CLUSTERS 49.8 57.1 72.2 65.9 80.4 70.5 67.0 62.6 34.6 65.0 62.5
X-LINGUAL CLUSTERS 49.2 59.0 72.5 65.9 80.9 72.7 65.7 62.5 37.2 64.4 63.0
Table 5: Direct transfer dependency parsing from English. Results measured by unlabeled attachment score (UAS).
ONLY SUBJECT/OBJECT RELATIONS ? UAS measured only over words marked as subject/object in the evaluation data.
Stanford dependency annotation. Some of these dif-
ferences are warranted in that certain target language
phenomena are better captured by the native annota-
tion. However, differences such as choice of lexical
versus functional head are more arbitrary.
To highlight this point we run two additional ex-
periments. First, we had linguists, who were also
fluent speakers of German, re-annotate the DE test set
so that unlabeled arcs are consistent with Stanford-
style dependencies. Using this data, NO CLUSTERS
obtains 60.0% UAS, PROJECTED CLUSTERS 63.6%
and X-LINGUAL CLUSTERS 64.4%. When compared
to the scores on the original data set (48.9%, 50.3%
and 50.7%, respectively) we can see that not only is
the baseline system doing much better, but that the
improvements from cross-lingual clustering are much
more pronounced. Next, we investigated the accuracy
of subject and object dependencies, as these are often
annotated in similar ways across treebanks, typically
modifying the main verb of the sentence. The bottom
half of Table 5 gives the scores when restricted to
such dependencies in the gold data. We measure the
percentage of modifiers in subject and object depen-
dencies that modify the correct word. Indeed, here
we see the difference in performance become clearer,
with the cross-lingual cluster model reducing errors
by 14% relative to the non-cross-lingual model and
upwards of 22% relative for IT.
We now turn to the results of the transfer experi-
ments for NER, listed in Table 6. While the perfor-
mance of the transfer systems is very poor when no
word clusters are used, adding cross-lingual word
clusters give substantial improvements across all lan-
guages. The simple PROJECTED CLUSTERS work
well, but the X-LINGUAL CLUSTERS provide even
larger improvements. On average the latter reduce
DE ES NL AVG
NO CLUSTERS 25.4 49.5 49.9 41.6
PROJECTED CLUSTERS 39.1 62.1 61.8 54.4
X-LINGUAL CLUSTERS 43.1 62.8 64.7 56.9
? DEVELOPMENT SET ? TEST SET
NO CLUSTERS 23.5 45.6 48.4 39.1
PROJECTED CLUSTERS 35.2 59.1 56.4 50.2
X-LINGUAL CLUSTERS 40.4 59.3 58.4 52.7
Table 6: Direct transfer NER results (from English) mea-
sured with average F1-score on the CoNLL 2002/2003
development and test sets.
errors on the test set by 22% in terms of F1 and up
to 26% for ES. We also measure how well the di-
rect transfer NER systems are able to detect entity
boundaries (ignoring the entity categories). Here, on
average, the best clusters provide a 24% relative error
reduction on the test set (75.8 vs. 68.1 F1).
To our knowledge there are no comparable results
on transfer learning of NER systems. Based on the
results of this first attempt at this scenario, we believe
that transfer learning by multilingual word clusters
could be developed into a practical way to construct
NER systems for resource poor languages.
6 Conclusion
In the first part of this study, we showed that word
clusters induced from a simple class-based language
model can be used to significantly improve on state-
of-the-art supervised dependency parsing and NER
for a wide range of languages and even across lan-
guage families. Although the improvements vary
between languages, the addition of word cluster fea-
tures never has a negative impact on performance.
484
This result has important practical consequences as
it allows practitioners to simply plug in word clus-
ter features into their current feature models. Given
previous work on word clusters for various linguistic
structure prediction tasks, these results are not too
surprising. However, to our knowledge this is the first
study to apply the same type of word cluster features
across languages and tasks.
In the second part, we provided two simple meth-
ods for inducing cross-lingual word clusters. The first
method works by projecting word clusters, induced
from monolingual data, from a source language to
a target language directly via word alignments. The
second method, on the other hand, makes use of
monolingual data in both the source and the target
language, together with word alignments that act as
constraints on the joint clustering. We then showed
that by using these cross-lingual word clusters, we
can significantly improve on direct transfer of dis-
criminative models for both parsing and NER. As
in the monolingual case, both types of cross-lingual
word cluster features yield improvements across the
board, with the more complex method providing a
significantly larger improvement for NER. Although
the performance of transfer systems is still substan-
tially below that of supervised systems, this research
provides one step towards bridging this gap. Further,
we believe that it opens up an avenue for future work
on multilingual clustering methods, cross-lingual fea-
ture projection and domain adaptation for direct trans-
fer of linguistic structure.
Acknowledgments
We thank John DeNero for help with creating the
word alignments; Reut Tsarfaty and Joakim Nivre for
rewarding discussions on evaluation; Slav Petrov and
Kuzman Ganchev for discussions on cross-lingual
clustering; and the anonymous reviewers, along with
Joakim Nivre, for valuable comments that helped
improve the paper. The first author is grateful for the
financial support of the Swedish National Graduate
School of Language Technology (GSLT).
A Data Sets
In the parsing experiments, we use the following data
sets. For DA, DE, EL, ES, IT, NL, PT and SV, we
use the predefined training and evaluation data sets
from the CoNLL 2006/2007 data sets (Buchholz and
Marsi, 2006; Nivre et al, 2007). For EN we use
sections 02-21, 22, and 23 of the Penn WSJ Tree-
bank (Marcus et al, 1993) for training, development
and evaluation. For FR we used the French Treebank
(Abeille? and Barrier, 2004) with splits defined in Can-
dito et al (2010). For KO we use the Sejong Korean
Treebank (Han et al, 2002) randomly splitting the
data into 80% training, 10% development and 10%
evaluation. For RU we use the SynTagRus Treebank
(Boguslavsky et al, 2000; Apresjan et al, 2006) ran-
domly splitting the data into 80% training, 10% devel-
opment and 10% evaluation. For ZH we use the Penn
Chinese Treebank v6 (Xue et al, 2005) using the
proposed data splits from the documentation. Both
EN and ZH were converted to dependencies using
v1.6.8 of the Stanford Converter (De Marneffe et al,
2006). FR was converted using the procedure defined
in Candito et al (2010). RU and KO are native depen-
dency treebanks. For the CoNLL data sets we use
the part-of-speech tags provided with the data. For
all other data sets, we train a part-of-speech tagger
on the training data in order to tag the development
and evaluation data.
For the NER experiments we use the training, de-
velopment and evaluation data sets from the CoNLL
2002/2003 shared tasks (Tjong Kim Sang, 2002;
Tjong Kim Sang and De Meulder, 2003) for all
four languages (DE, EN, ES and NL). The data set
for each language consists of newswire text anno-
tated with four entity categories: Location (LOC),
Miscellaneous (MISC), Organization (ORG) and Per-
son (PER). We use the part-of-speech tags supplied
with the data, except for ES where we instead use
universal part-of-speech tags (Petrov et al, 2011).
Unlabeled data for training the monolingual cluster
models was extracted from one year of newswire ar-
ticles from multiple sources from a news aggregation
website. This consists of 0.8 billion (DA) to 121.6 bil-
lion (EN) tokens per language. All word alignments
for the cross-lingual clusterings were produced with
the dual decomposition aligner described by DeNero
and Macherey (2011) using 10.5 million (DA) to 12.1
million (FR) sentences of aligned web data.
485
References
Anne Abeille? and Nicolas Barrier. 2004. Enriching a
french treebank. In Proceedings of LREC.
Juri Apresjan, Igor Boguslavsky, Boris Iomdin, Leonid
Iomdin, Andrei Sannikov, and Victor Sizov. 2006. A
syntactically and semantically tagged corpus of russian:
State of the art and prospects. In Proceedings of LREC.
Emily M. Bender. 2011. On achieving and evaluating
language-independence in NLP. Linguistic Issues in
Language Technology, 6(3):1?26.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of ACL.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34(1):211?231.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grigoriev,
Leonid Kreidlin, and Nadezhda Frid. 2000. Depen-
dency treebank for Russian: Concept, tools, types of
information. In Proceedings of COLING.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Marie Candito, Beno??t Crabbe?, and Pascal Denis. 2010.
Statistical french dependency parsing: treebank conver-
sion and first results. In Proceedings of LREC.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induction.
In Proceedings of EACL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine trans-
lation. In Proceedings of ACL.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceed-
ings of LREC.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Paramveer Dhillon, Dean Foster, and Lyle Dean. 2011.
Multi-view learning of word embeddings via cca. In
Proceedings of NIPS.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and evaluating a german named entity recognizer with
semantic generalization. In Proceedings of KONVENS.
Dayne Freitag. 2004. Trained named entity recogni-
tion using distributional clusters. In Proceedings of
EMNLP.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity in
dependency grammar induction. In Proceedings of
ACL.
Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar.
2011. An ensemble model that combines syntactic
and semantic clustering for discriminative dependency
parsing. In Proceedings of ACL.
Chung-hye Han, Na-Rare Han, Eon-Suk Ko, and Martha
Palmer. 2002. Development and evaluation of a korean
treebank and its application to nlp. In Proceedings of
LREC.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of dependency
and constituency. In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-HLT.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre. 2009.
Dependency parsing. Morgan & Claypool Publishers.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP, pages 1030?1038.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
486
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative
training. In Proceedings of HLT-NAACL.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of ICML.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Franz Josef Och. 1999. An efficient method for determin-
ing bilingual word classes. In Proceedings of EACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Publishers Inc.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A
universal part-of-speech tagset. In ArXiv:1104.2086.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Morgan & Claypool Publishers.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of EMNLP.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL.
Erik F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named entity
recognition. In Proceedings of CoNLL.
Stephen Tratz and Eduard Hovy. 2011. A fast, effec-
tive, non-projective, semantically-enriched parser. In
Proceedings of EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.
2005. The penn chinese treebank: Phrase structure
annotation of a large corpus. Natural Language Engi-
neering, 11(02):207?238.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In IJC-
NLP Workshop: NLP for Less Privileged Languages.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of EMNLP.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
487
Proceedings of NAACL-HLT 2013, pages 1061?1071,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Target Language Adaptation of Discriminative Transfer Parsers
Oscar T?ckstr?m?
SICS | Uppsala University
Sweden
oscar@sics.se
Ryan McDonald
Google
New York
ryanmcd@google.com
Joakim Nivre?
Uppsala University
Sweden
joakim.nivre@lingfil.uu.se
Abstract
We study multi-source transfer parsing for
resource-poor target languages; specifically
methods for target language adaptation of
delexicalized discriminative graph-based de-
pendency parsers. We first show how recent
insights on selective parameter sharing, based
on typological and language-family features,
can be applied to a discriminative parser by
carefully decomposing its model features. We
then show how the parser can be relexicalized
and adapted using unlabeled target language
data and a learning method that can incorporate
diverse knowledge sources through ambiguous
labelings. In the latter scenario, we exploit
two sources of knowledge: arc marginals de-
rived from the base parser in a self-training
algorithm, and arc predictions from multiple
transfer parsers in an ensemble-training algo-
rithm. Our final model outperforms the state of
the art in multi-source transfer parsing on 15
out of 16 evaluated languages.
1 Introduction
Many languages still lack access to core NLP tools,
such as part-of-speech taggers and syntactic parsers.
This is largely due to the reliance on fully supervised
learning methods, which require large quantities of
manually annotated training data. Recently, meth-
ods for cross-lingual transfer have appeared as a
promising avenue for overcoming this hurdle for both
part-of-speech tagging (Yarowsky et al, 2001; Das
and Petrov, 2011) and syntactic dependency parsing
(Hwa et al, 2005; Zeman and Resnik, 2008; Ganchev
et al, 2009; McDonald et al, 2011; Naseem et al,
?Work primarily carried out while at Google, NY.
2012). While these methods do not yet compete with
fully supervised approaches, they can drastically out-
perform both unsupervised methods (Klein and Man-
ning, 2004) and weakly supervised methods (Naseem
et al, 2010; Berg-Kirkpatrick and Klein, 2010).
A promising approach to cross-lingual transfer
of syntactic dependency parsers is to use multiple
source languages and to tie model parameters across
related languages. This idea was first explored for
weakly supervised learning (Cohen and Smith, 2009;
Snyder et al, 2009; Berg-Kirkpatrick and Klein,
2010) and recently by Naseem et al (2012) for multi-
source cross-lingual transfer. In particular, Naseem
et al showed that by selectively sharing parameters
based on typological features of each language, sub-
stantial improvements can be achieved, compared
to using a single set of parameters for all languages.
However, these methods all employ generative mod-
els with strong independence assumptions and weak
feature representations, which upper bounds their ac-
curacy far below that of feature-rich discriminative
parsers (McDonald et al, 2005; Nivre, 2008).
In this paper, we improve upon the state of the art
in cross-lingual transfer of dependency parsers from
multiple source languages by adapting feature-rich
discriminatively trained parsers to a specific target
language. First, in ?4 we show how selective sharing
of model parameters based on typological traits can
be incorporated into a delexicalized discriminative
graph-based parsing model. This requires a careful
decomposition of features into language-generic and
language-specific sets in order to tie specific target
language parameters to their relevant source language
counterparts. The resulting parser outperforms the
method of Naseem et al (2012) on 12 out of 16 eval-
uated languages. Second, in ?5 we introduce a train-
1061
ing method that can incorporate diverse knowledge
sources through ambiguously predicted labelings of
unlabeled target language data. This permits effective
relexicalization and target language adaptation of the
transfer parser. Here, we experiment with two differ-
ent knowledge sources: arc sets, which are filtered by
marginal probabilities from the cross-lingual transfer
parser, are used in an ambiguity-aware self-training
algorithm (?5.2); these arc sets are then combined
with the predictions of a different transfer parser in an
ambiguity-aware ensemble-training algorithm (?5.3).
The resulting parser provides significant improve-
ments over a strong baseline parser and achieves a
13% relative error reduction on average with respect
to the best model of Naseem et al (2012), outper-
forming it on 15 out of the 16 evaluated languages.
2 Multi-Source Delexicalized Transfer
The methods proposed in this paper fall into the delex-
icalized transfer approach to multilingual syntactic
parsing (Zeman and Resnik, 2008; McDonald et al,
2011; Cohen et al, 2011; S?gaard, 2011). In contrast
to annotation projection approaches (Yarowsky et al,
2001; Hwa et al, 2005; Ganchev et al, 2009; Spreyer
and Kuhn, 2009), delexicalized transfer methods do
not rely on any bitext. Instead, a parser is trained
on annotations in a source language, relying solely
on features that are available in both the source
and the target language, such as ?universal? part-of-
speech tags (Zeman and Resnik, 2008; Naseem et al,
2010; Petrov et al, 2012), cross-lingual word clusters
(T?ckstr?m et al, 2012) or type-level features derived
from bilingual dictionaries (Durrett et al, 2012).1
This parser is then directly used to parse the target
language. For languages with similar typology, this
method can be quite accurate, especially when com-
pared to purely unsupervised methods. For instance,
a parser trained on English with only part-of-speech
features can correctly parse the Greek sentence in Fig-
ure 1, even without knowledge of the lexical items
since the sequence of part-of-speech tags determines
the syntactic structure almost unambiguously.
Learning with multiple languages has been shown
to benefit unsupervised learning (Cohen and Smith,
1Note that T?ckstr?m et al (2012) and Durrett et al (2012)
do require bitext or a bilingual dictionary. The same holds for
most cross-lingual representations, e.g., Klementiev et al (2012).
? ???? ????? ???? ????? ?? ?????? .
(The) (John) (gave) (to-the) (Maria) (the) (book) .
DET NOUN VERB ADP NOUN DET NOUN P
DET NSUBJ
ROOT
PREP POBJ
DOBJ
DET
PUNC
Figure 1: A Greek sentence which is correctly parsed by a
delexicalized English parser, provided that part-of-speech
tags are available in both the source and target language.
2009; Snyder et al, 2009; Berg-Kirkpatrick and
Klein, 2010). Annotations in multiple languages
can be combined in delexicalized transfer as well, as
long as the parser features are available across the in-
volved languages. This idea was explored by McDon-
ald et al (2011), who showed that target language
accuracy can be improved by simply concatenating
delexicalized treebanks in multiple languages. In
similar work, Cohen et al (2011) proposed a mixture
model in which the parameters of a generative target
language parser is expressed as a linear interpola-
tion of source language parameters, whereas S?gaard
(2011) showed that target side language models can
be used to selectively subsample training sentences
to improve accuracy. Recently, inspired by the phylo-
genetic prior of Berg-Kirkpatrick and Klein (2010),
S?gaard and Wulff (2012) proposed ? among other
ideas ? a typologically informed weighting heuristic
for linearly interpolating source language parameters.
However, this weighting did not provide significant
improvements over uniform weighting.
The aforementioned approaches work well for
transfer between similar languages. However, their
assumptions cease to hold for typologically divergent
languages; a target language can rarely be described
as a linear combination of data or model parameters
from a set of source languages, as languages tend
to share varied typological traits; this critical insight
is discussed further in ?4. To account for this issue,
Naseem et al (2012) recently introduced a novel gen-
erative model of dependency parsing, in which the
generative process is factored into separate steps for
the selection of dependents and their ordering. The
parameters used in the selection step are all language
independent, capturing only head-dependent attach-
ment preferences. In the ordering step, however, pa-
rameters are selectively shared between subsets of
1062
Feature Description
81A Order of Subject, Object and Verb
85A Order of Adposition and Noun
86A Order of Genitive and Noun
87A Order of Adjective and Noun
88A Order of Demonstrative and Noun
89A Order of Numeral and Noun
Table 1: Typological features from WALS (Dryer and
Haspelmath, 2011), proposed for selective sharing by
Naseem et al (2012). Feature 89A has the same value for
all studied languages, while 88A differs only for Basque.
These features are therefore subsequently excluded.
source languages based on typological features of
the languages extracted from WALS ? the World
Atlas of Language Structures (Dryer and Haspelmath,
2011) ? as shown in Table 1. In the transfer scenario,
where no supervision is available in the target lan-
guage, this parser achieves the hitherto best published
results across a number of languages; in particular
for target languages with a word order divergent from
the source languages.
However, the generative model of Naseem et al is
quite impoverished. In the fully supervised setting,
it obtains substantially lower accuracies compared
to a standard arc-factored graph-based parser (Mc-
Donald et al, 2005). Averaged across 16 languages,2
the generative model trained with full supervision on
the target language obtains an accuracy of 67.1%. A
comparable lexicalized discriminative arc-factored
model (McDonald et al, 2005) obtains 84.1%. Even
when delexicalized, this model reaches 78.9%. This
gap in supervised accuracy holds for all 16 languages.
Thus, while selective sharing is a powerful device for
transferring parsers across languages, the underly-
ing generative model used by Naseem et al (2012)
restricts its potential performance.
3 Basic Models and Experimental Setup
Inspired by the superiority of discriminative graph-
based parsing in the supervised scenario, we inves-
tigate whether the insights of Naseem et al (2012)
on selective parameter sharing can be incorporated
into such models in the transfer scenario. We first re-
view the basic graph-based parser framework and the
2Based on results in Naseem et al (2012), excluding English.
experimental setup that we will use throughout. We
then delve into details on how to incorporate selec-
tive sharing in this model in ?4. In ?5, we show how
learning with ambiguous labelings in this parser can
be used for further target language adaptation, both
through self-training and through ensemble-training.
3.1 Discriminative Graph-Based Parser
Let x denote an input sentence and let y ? Y(x)
denote a dependency tree, where Y(x) is the set of
well-formed dependency trees spanning x. Hence-
forth, we restrictY(x) to projective dependency trees,
but all our methods are equally applicable in the non-
projective case. Provided a vector of model parame-
ters ?, the probability of a dependency tree y ? Y(x),
conditioned on a sentence x, has the following form:
p?(y | x) =
exp
{
?>?(x, y)
}
?
y??Y(x) exp {?
>?(x, y?)}
.
Without loss of generality, we restrict ourselves to
first-order models, where the feature function ?(x, y)
factors over individual arcs (h,m) in y, such that
?(x, y) =
?
(h,m)?y
?(x, h,m) ,
where h ? [0, |x|] and m ? [1, |x|] are the indices
of the head word and the dependent word of the
arc; h = 0 represents a dummy ROOT token. The
model parameters are estimated by maximizing the
log-likelihood of the training dataD = {(xi, yi)}ni=1,
L(?;D) =
n?
i=1
log p?(yi | xi) .
We use the standard gradient-based L-BFGS algo-
rithm (Liu and Nocedal, 1989) to maximize the log-
likelihood. Eisner?s algorithm (Eisner, 1996) is used
for inference of the Viterbi parse and arc-marginals.
3.2 Data Sets and Experimental Setup
To facilitate comparison with the state of the art, we
use the same treebanks and experimental setup as
Naseem et al (2012). Notably, we use the map-
ping proposed by Naseem et al (2010) to map from
fine-grained treebank specific part-of-speech tags to
coarse-grained ?universal? tags, rather than the more
recent mapping proposed by Petrov et al (2012). For
1063
l[l]? h.p
[l]? m.p
[l]? h.p? m.p
d? w.81A? 1[h.p = VERB ? m.p = NOUN]
d? w.81A? 1[h.p = VERB ? m.p = PRON]
d? w.85A? 1[h.p = ADP ? m.p = NOUN]
d? w.85A? 1[h.p = ADP ? m.p = PRON]
d? w.86A? 1[h.p = NOUN ? m.p = NOUN]
d? w.87A? 1[h.p = NOUN ? m.p = ADJ]
d? l; [d? l]? h.p; [d? l]? m.p
[d? l]? h.p? m.p
[d? l]? h.p? h+1.p? m?1.p? m.p
[d? l]? h?1.p? h.p? m?1.p? m.p
[d? l]? h.p? h+1.p? m.p? m+1.p
[d? l]? h?1.p? h.p? m.p? m+1.p
h.p? between.p? m.p
Delexicalized MSTParser Selectively sharedBare
Figure 2: Arc-factored feature templates for graph-based parsing. Direction: d ? {LEFT, RIGHT}; dependency length:
l ? {1, 2, 3, 4, 5+}; part of speech of head / dependent / words between head and dependent: h.p / m.p / between.p ?
{NOUN, VERB, ADJ, ADV, PRON, DET, ADP, NUM, CONJ, PRT, PUNC, X}; token to the left / right of z: z?1 / z+1; WALS
features: w.X for X = 81A, 85A, 86A, 87A (see Table 1). [?] denotes an optional template, e.g., [d? l] ? h.p? m.p
expands to templates d? l? h.p? m.p and h.p? m.p, so that the template also falls back on its undirectional variant.
each target language evaluated, the treebanks of the
remaining languages are used as labeled training data,
while the target language treebank is used for testing
only (in ?5 a different portion of the target language
treebank is additionally used as unlabeled training
data). We refer the reader to Naseem et al (2012) for
detailed information on the different treebanks. Due
to divergent treebank annotation guidelines, which
makes fine-grained evaluation difficult, all results
are evaluated in terms of unlabeled attachment score
(UAS). In line with Naseem et al (2012), we use gold
part-of-speech tags and evaluate only on sentences
of length 50 or less excluding punctuation.
3.3 Baseline Models
We compare our models to two multi-source base-
line models. The first baseline, NBG, is the gener-
ative model with selective parameter sharing from
Naseem et al (2012).3 This model is trained without
target language data, but we investigate the use of
such data in ?5.4. The second baseline, Delex, is a
delexicalized projective version of the well-known
graph-based MSTParser (McDonald et al, 2005).
The feature templates used by this model are shown
to the left in Figure 2. Note that there is no selective
sharing in this model.
The second and third columns of Table 2 show the
unlabeled attachment scores of the baseline models
for each target language. We see that Delex performs
well on target languages that are related to the major-
ity of the source languages. However, for languages
3Model ?D-,To? in Table 2 from Naseem et al (2012).
that diverge from the Indo-European majority family,
the selective sharing model, NBG, achieves substan-
tially higher accuracies.
4 Feature-Based Selective Sharing
The results for the baseline models are not surpris-
ing considering the feature templates used by Delex.
There are two fundamental issues with these fea-
tures when used for direct transfer. First, all but
one template include the arc direction. Second,
some features are sensitive to local word order; e.g.,
[d? l]? h.p? h+1.p? m?1.p? m.p, which models
direction as well as word order in the local contexts
of the head and the dependent. Such features do not
transfer well across typologically different languages.
In order to verify that these issues are the cause of
the poor performance of the Delex model, we remove
all directional features and all features that model
local word order from Delex. The feature templates
of the resulting Bare model are shown in the center
of Figure 2. These features only model selectional
preferences and dependency length, analogously to
the selection component of NBG. The performance
of Bare is shown in the fourth column of Table 2.
The removal of most of the features results in a per-
formance drop on average. However, for languages
outside of the Indo-European family, Bare is often
more accurate, especially for Basque, Hungarian and
Japanese, which supports our hypothesis.
4.1 Sharing Based on Typological Features
After removing all directional features, we now care-
fully reintroduce them. Inspired by Naseem et al
1064
Graph-Based Models
Lang. NBG Delex Bare Share Similar Family
ar 57.2 43.3 43.1 52.7 52.7 52.7
bg 67.6 64.5 56.1 65.4 62.4 65.4
ca 71.9 72.0 58.1 66.1 80.2 77.6
cs 43.9 40.5 43.1 42.5 45.3 43.5
de 54.0 57.0 49.3 55.2 58.1 59.2
el 61.9 63.2 57.7 62.9 59.9 63.2
es 62.3 66.9 52.6 59.3 69.0 67.1
eu 39.7 29.5 43.3 46.8 46.8 46.8
hu 56.9 56.2 60.5 64.5 64.5 64.5
it 68.0 70.8 55.7 63.5 74.6 72.5
ja 62.3 38.9 50.6 57.1 64.6 65.9
nl 56.2 57.9 51.6 55.0 51.8 56.8
pt 76.2 77.5 63.0 72.7 78.4 78.4
sv 52.0 61.4 55.9 58.8 48.8 63.5
tr 59.1 37.4 36.0 41.7 59.5 59.4
zh 59.9 45.1 47.9 54.8 54.8 54.8
avg 59.3 55.1 51.5 57.4 60.7 62.0
Table 2: Unlabeled attachment scores of the multi-source
transfer models. Boldface numbers indicate the best result
per language. Underlined numbers indicate languages
whose group is not represented in the training data (these
default to Share under Similarity and Family). NBG is the
?D-,To? model in Table 2 from Naseem et al (2012).
(2012), we make use of the typological features from
WALS (Dryer and Haspelmath, 2011), listed in Ta-
ble 1, to selectively share directional parameters be-
tween languages. As a natural first attempt at sharing
parameters, one might consider forming the cross-
product of all features of Delex with all WALS prop-
erties, similarly to a common domain adaptation tech-
nique (Daum? III, 2007; Finkel and Manning, 2009).
However, this approach has two issues. First, it re-
sults in a huge number of features, making the model
prone to overfitting. Second, and more critically, it
ties together languages via features for which they
are not typologically similar. Consider English and
French, which are both prepositional and thus have
the same value for WALS property 85A. These lan-
guages will end up sharing a parameter for the feature
[d? l]?h.p = NOUN?m.p = ADJ?w.85A; yet they
have the exact opposite direction of attachment pref-
erence when it comes to nouns and adjectives. This
problem applies to any method for parameter mixing
that treats all the parameters as equal.
Like Naseem et al (2012), we instead share pa-
rameters more selectively. Our strategy is to use the
relevant part-of-speech tags of the head and depen-
dent to select which parameters to share, based on
very basic linguistic knowledge. The resulting fea-
tures are shown to the right in Figure 2. For example,
there is a shared directional feature that models the or-
der of Subject, Object and Verb by conjoining WALS
feature 81A with the arc direction and an indicator
feature that fires only if the head is a verb and the de-
pendent is a noun. These features would not be very
useful by themselves, so we combine them with the
Bare features. The accuracy of the resulting Share
model is shown in column five of Table 2. Although
this model still performs worse than NBG, it is an
improvement over the Delex baseline and actually
outperforms the former on 5 out of the 16 languages.
4.2 Sharing Based on Language Groups
While Share models selectional preferences and arc
directions for a subset of dependency relations, it
does not capture the rich local word order informa-
tion captured by Delex. We now consider two ways of
selectively including such information based on lan-
guage similarity. While more complex sharing could
be explored (Berg-Kirkpatrick and Klein, 2010), we
use a flat structure and consider two simple groupings
of the source and target languages.
First, the Similar model consists of the features
used by Share together with the features from Delex
in Figure 2. The latter are conjoined with an indicator
feature that fires only when the source and target
languages share values for all the WALS features in
Table 1. This is accomplished by adding the template
f? [w.81A? w.85A? w.86A? w.87A? w.88A]
for each template f in Delex. This groups: 1) Cata-
lan, Italian, Portuguese and Spanish; 2) Bulgarian,
Czech and English; 3) Dutch, German and Greek;
and 4) Japanese and Turkish. The remaining lan-
guages do not share all WALS properties with at
least one source language and thus revert to Share,
since they cannot exploit these grouped features.
Second, instead of grouping languages according
to WALS, the Family model is based on a simple
subdivision into Indo-European languages (Bulgar-
ian, Catalan, Czech, Greek, English, Spanish, Italian,
1065
Dutch, Portuguese, Swedish) and Altaic languages
(Japanese, Turkish). This is accomplished with in-
dicator features analogous to those used in Similar.
The remaining languages are again treated as isolates
and revert to Similar.
The results for these models are given in the last
two columns of Table 2. We see that by adding these
rich features back into the fold, but having them fire
only for languages in the same group, we can sig-
nificantly increase the performance ? from 57.4%
to 62.0% on average when considering Family. If
we consider our original Delex baseline, we see an
absolute improvement of 6.9% on average and a rela-
tive error reduction of 15%. Particular gains are seen
for non-Indo-European languages; e.g., Japanese in-
creases from 38.9% to 65.9%. Furthermore, Family
achieves a 7% relative error reduction over the NBG
baseline and outperforms it on 12 of the 16 languages.
This shows that a discriminative graph-based parser
can achieve higher accuracies compared to generative
models when the features are carefully constructed.
5 Target Language Adaptation
While some higher-level linguistic properties of the
target language have been incorporated through se-
lective sharing, so far no features specific to the target
language have been employed. Cohen et al (2011)
and Naseem et al (2012) have shown that using
expectation-maximization (EM) to this end can in
some cases bring substantial accuracy gains. For dis-
criminative models, self-training has been shown to
be quite effective for adapting monolingual parsers to
new domains (McClosky et al, 2006), as well as for
relexicalizing delexicalized parsers using unlabeled
target language data (Zeman and Resnik, 2008). Sim-
ilarly T?ckstr?m (2012) used self-training to adapt a
multi-source direct transfer named-entity recognizer
(T?ckstr?m et al, 2012) to different target languages,
?relexicalizing? the model with word cluster features.
However, as discussed in ?5.2, standard self-training
is not optimal for target language adaptation.
5.1 Ambiguity-Aware Training
In this section, we propose a related training method:
ambiguity-aware training. In this setting a discrimi-
native probabilistic model is induced from automat-
ically inferred ambiguous labelings over unlabeled
target language data, in place of gold-standard depen-
dency trees. The ambiguous labelings can combine
multiple sources of evidence to guide the estimation
or simply encode the underlying uncertainty from the
base parser. This uncertainty is marginalized out dur-
ing training. The structure of the output space, e.g.,
projectivity and single-headedness constraints, along
with regularities in the feature space, can together
guide the estimation, similar to what occurs with the
expectation-maximization algorithm.
Core to this method is the idea of an ambiguous
labeling y?(x) ? Y(x), which encodes a set of pos-
sible dependency trees for an input sentence x. In
subsequent sections we describe how to define such
labelings. Critically, y?(x) should be large enough to
capture the correct labeling, but on the other hand
small enough to provide concrete guidance for model
estimation. Ideally, y?(x) will capture heterogenous
knowledge that can aid the parser in target language
adaptation. In a first-order arc-factored model, we
define y?(x) in terms of a collection of ambiguous
arc setsA(x) = {A(x,m)}|x|m=1, whereA(x,m) de-
notes the set of ambiguously specified heads for the
mth token in x. Then, y?(x) is defined as the set of
all projective dependency trees spanning x that can
be assembled from the arcs in A(x).
Methods for learning with ambiguous labelings
have previously been proposed in the context of
multi-class classification (Jin and Ghahramani, 2002),
sequence-labeling (Dredze et al, 2009), log-linear
LFG parsing (Riezler et al, 2002), as well as for
discriminative reranking of generative constituency
parsers (Charniak and Johnson, 2005). In contrast to
Dredze et al, who allow for weights to be assigned
to partial labels, we assume that the ambiguous arcs
are weighted uniformly. For target language adapta-
tion, these weights would typically be derived from
unreliable sources and we do not want to train the
model to simply mimic their beliefs. Furthermore,
with this assumption, learning is simply achieved
by maximizing the marginal log-likelihood of the
ambiguous training set D? = {(xi, y?(xi)}ni=1,
L(?; D?) =
n?
i=1
log
?
?
?
?
y?y?(xi)
p?(y | xi)
?
?
?
? ? ???22 .
In maximizing the marginal log-likelihood, the model
is free to distribute probability mass among the trees
1066
in the ambiguous labeling to its liking, as long as the
marginal log-likelihood improves. The same objec-
tive function is used by Riezler et al (2002) and Char-
niak and Johnson (2005). A key difference is that in
these works, the ambiguity is constrained through a
supervised signal, while we use ambiguity as a way
to achieve self-training, using the base-parser itself,
or some other potentially noisy knowledge source as
the sole constraints. Note that we have introduced
an `2-regularizer, weighted by ?. This is important
as we are now training lexicalized target language
models which can easily overfit. In all experiments,
we optimize parameters with L-BFGS. Note also that
the marginal likelihood is non-concave, so that we
are only guaranteed to find a local maximum.
5.2 Ambiguity-Aware Self-Training
In standard self-training ? hereafter referred to as
Viterbi self-training ? a base parser is used to la-
bel each unlabeled sentence with its most probable
parse tree to create a self-labeled data set, which is
subsequently used to train a supervised parser. There
are two reasons why this simple approach may work.
First, if the base parser?s errors are not too systematic
and if the self-training model is not too expressive,
self-training can reduce the variance on the new do-
main. Second, self-training allows for features in the
new domain with low support ? or no support in the
case of lexicalized features ? in the base parser to
be ?filled in? by exploiting correlations in the feature
representation. However, a potential pitfall of this
approach is that the self-trained parser is encouraged
to blindly mimic the base parser, which leads to error
reinforcement. This may be particularly problematic
when relexicalizing a transfer parser, since the lexical
features provide the parser with increased power and
thereby an increased risk of overfitting to the noise.
To overcome this potential problem, we propose an
ambiguity-aware self-training (AAST) method that is
able to take the noise of the base parser into account.
We use the arc-marginals of the base parser to
construct the ambiguous labeling y?(x) for a sentence
x. For each token m ? [1, |x|], we first sort the set of
arcs in which m is the dependent, {(h,m)}|x|h=0, by
the marginal probabilities of the arcs:
p?(h,m | x) =
?
{y?Y(x) | (h,m)?y}
p?(y | x)
We next construct the ambiguous arc set A(x,m) by
adding arcs (h,m) in order of decreasing probability,
until their cumulative probability exceeds ?, i.e. until
?
(h,m)?A(x,m)
p?(h,m | x) ? ? .
Lower values of ? result in more aggressive pruning,
with ? = 0 corresponding to including no arc and
? = 1 corresponding to including all arcs. We always
add the highest scoring tree y? to y?(x) to ensure that
it contains at least one complete projective tree.
Figure 3 outlines an example of how (and why)
AAST works. In the Greek example, the genitive
phrase ? pi??????? ?????? (the stay of vessels) is
incorrectly analyzed as a flat noun phrase. This is not
surprising given that the base parser simply observes
this phrase as DET NOUN NOUN. However, looking
at the arc marginals we can see that the correct anal-
ysis is available during AAST, although the actual
marginal probabilities are quite misleading. Further-
more, the genitive noun ?????? also appears in other
less ambiguous contexts, where the base parser cor-
rectly predicts it to modify a noun and not a verb.
This allows the training process to add weight to the
corresponding lexical feature pairing ?????? with a
noun head and away from the feature pairing it with
a verb. The resulting parser correctly predicts the
genitive construction.
5.3 Ambiguity-Aware Ensemble-Training
While ambiguous labelings can be used as a means
to improve self-training, any information that can
be expressed as hard arc-factored constraints can be
incorporated, including linguistic expert knowledge
and annotation projected via bitext. Here we explore
another natural source of information: the predic-
tions of other transfer parsers. It is well known that
combining several diverse predictions in an ensem-
ble often leads to improved predictions. However, in
most ensemble methods there is typically no learning
involved once the base learners have been trained
(Sagae and Lavie, 2006). An exception is the method
of Sagae and Tsujii (2007), who combine the outputs
of many parsers on unlabeled data to train a parser
for a new domain. However, in that work the learner
is not exposed to the underlying ambiguity of the
base parsers; it is only given the Viterbi parse of the
combination system as the gold standard. In contrast,
1067
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
0.55
0.44
0.62
0.36
0.10
0.87
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
? pi??????? ?????? ?pi????pi???? ???? ?? ????
DET NOUN NOUN VERB ADV DET NOUN
Figure 3: An example of ambiguity-aware self-training
(AAST) on a sentence from the Greek self-training data.
The sentence roughly translates to The stay of vessels
is permitted only for the day. Top: Arcs from the base
model?s Viterbi parse are shown above the sentence. When
only the part-of-speech tags are observed, the parser tends
to treat everything to the left of the verb as a head-final
noun phrase. The dashed arcs below the sentence are
the arcs for the true genitive construction stay of vessels.
These arcs and the corresponding incorrect arcs in the
Viterbi parse are marked with their marginal probabilities.
Middle: The ambiguous labeling y?(x), which is used
as supervision in AAST. Additional non-Viterbi arcs are
present in y?(x); for clarity, these are not shown. When
learning with AAST, probability mass will be pushed to-
wards any tree consistent with y?(x). Marginal probabili-
ties are ignored at this stage, so that all arcs in y?(x) are
treated as equals. Bottom: The Viterbi parse of the AAST
model, which has selected the correct arcs from y?(x).
we propose an ambiguity-aware ensemble-training
(AAET) method that treats the union of the ensemble
predictions for a sentence x as an ambiguous labeling
y?(x). An additional advantage of this approach is
that the ensemble is compiled into a single model
and therefore does not require multiple models to be
stored and used at runtime.
It is straightforward to construct y?(x) from multi-
ple parsers. Let Ak(x,m) be the set of arcs for the
mth token in x according to the kth parser in the en-
semble. When arc-marginals are used to construct the
ambiguity set, |Ak(x,m)| ? 1, but when the Viterbi-
parse is used, Ak(x,m) is a singleton. We next form
A(x,m) =
?
kAk(x,m) as the ensemble arc ambi-
guity set from which y?(x) is assembled. In this study,
we combine the arc sets of two base parsers: first, the
arc-marginal ambiguity set of the base parser (?5.2);
and second, the Viterbi arc set from the NBG parser
of Naseem et al (2012) in Table 2.4 Thus, the lat-
ter will have singleton arc ambiguity sets, but when
combined with the arc-marginal ambiguity sets of
our base parser, the result will encode uncertainty
derived from both parsers.
5.4 Adaptation Experiments
We now study the different approaches to target lan-
guage adaptation empirically. As in Naseem et al
(2012), we use the CoNLL training sets, stripped of
all dependency information, as the unlabeled target
language data in our experiments. We use the Family
model as the base parser, which is used to label the
unlabeled target data with the Viterbi parses as well
as with the ambiguous labelings. The final model
is then trained on this data using standard lexical-
ized features (McDonald et al, 2005). Since labeled
training data is unavailable in the target language,
we cannot tune any hyper-parameters and simply set
?= 1 and ?= 0.95 throughout. Although the latter
may suggest that y?(x) contains a high degree of am-
biguity, in reality, the marginal distributions of the
base model have low entropy and after filtering with
? = 0.95, the average number of potential heads per
dependent ranges from 1.4 to 3.2, depending on the
target language.
The ambiguity-aware training methods, that is
ambiguity-aware self-training (AAST) and ambiguity-
aware ensemble-training (AAET), are compared to
three baseline systems. First, NBG+EM is the gen-
erative model of Naseem et al (2012) trained with
expectation-maximization on additional unlabeled
target language text. Second, Family is the best dis-
criminative model from the previous section. Third,
Viterbi is the basic Viterbi self-training model. The
results of each of these models are shown in Table 3.
There are a number of things that can be observed.
First, Viterbi self-training helps slightly on average,
but the gains are not consistent and there are even
drops in accuracy for some languages. Second, AAST
outperforms the Viterbi variant on all languages and
4We do not have access to the marginals of NBG.
1068
Target Adaptation
Lang. NBG+EM Family Viterbi AAST AAET
ar 59.3 52.7 52.6 53.5 58.7
bg 67.0 65.4 66.4 67.9 73.0
ca 71.7 77.6 78.0 79.9 76.1
cs 44.3 43.5 43.6 44.4 48.3
de 54.1 59.2 59.7 62.5 61.5
el 67.9 63.2 64.5 65.5 69.6
es 62.0 67.1 68.2 68.5 66.9
eu 47.8 46.8 47.5 48.6 49.4
hu 58.6 64.5 64.6 65.6 67.5
it 65.6 72.5 71.6 72.4 73.4
ja 64.1 65.9 65.7 68.8 72.0
nl 56.6 56.8 57.9 58.1 60.2
pt 75.8 78.4 79.9 80.7 79.9
sv 61.7 63.5 63.4 65.5 65.5
tr 59.4 59.4 59.5 64.1 64.2
zh 51.0 54.8 54.8 57.9 60.7
avg 60.4 62.0 62.4 64.0 65.4
Table 3: Target language adaptation using unlabeled tar-
get data. AAST: ambiguity-aware self-training. AAET:
ambiguity-aware ensemble-training. Boldface numbers
indicate the best result per language. Underlined numbers
indicate the best result, excluding AAET. NBG+EM is the
?D+,To? model from Naseem et al (2012).
nearly always improves on the base parser, although
it sees a slight drop for Italian. AAST improves the
accuracy over the base model by 2% absolute on av-
erage and by as much as 5% absolute for Turkish.
Comparing this model to the NBG+EM baseline, we
observe an improvement by 3.6% absolute, outper-
forming it on 14 of the 16 languages. Furthermore,
ambiguity-aware self-training appears to help more
than expectation-maximization for generative (unlex-
icalized) models. Naseem et al observed an increase
from 59.3% to 60.4% on average by adding unlabeled
target language data and the gains were not consistent
across languages. AAST, on the other hand, achieves
consistent gains, rising from 62.0% to 64.0% on av-
erage. Third, as shown in the rightmost column of
Table 3, ambiguity-aware ensemble-training is indeed
a successful strategy; AAET outperforms the previ-
ous best self-trained model on 13 and NB&G+EM
on 15 out of 16 languages. The relative error reduc-
tion with respect to the base Family model is 9% on
average, while the average reduction with respect to
NBG+EM is 13%.
Before concluding, two additional points are worth
making. First, further gains may potentially be
achievable with feature-rich discriminative models.
While the best generative transfer model of Naseem
et al (2012) approaches its upper-bounding super-
vised accuracy (60.4% vs. 67.1%), our relaxed self-
training model is still far below its supervised coun-
terpart (64.0% vs. 84.1%). One promising statistic
along these lines is that the oracle accuracy for the
ambiguous labelings of AAST is 75.7%, averaged
across languages, which suggests that other training
algorithms, priors or constraints could improve the
accuracy substantially. Second, relexicalization is a
key component of self-training. If we use delexical-
ized features during self-training, we only observe a
small average improvement from 62.0% to 62.1%.
6 Conclusions
We contributed to the understanding of multi-source
syntactic transfer in several complementary ways.
First, we showed how selective parameter sharing,
based on typological features and language family
membership, can be incorporated in a discriminative
graph-based model of dependency parsing. We then
showed how ambiguous labelings can be used to in-
tegrate heterogenous knowledge sources in parser
training. Two instantiations of this framework were
explored. First, an ambiguity-aware self-training
method that can be used to effectively relexicalize
and adapt a delexicalized transfer parser using unla-
beled target language data. Second, an ambiguity-
aware ensemble-training method, in which predic-
tions from different parsers can be incorporated and
further adapted. On average, our best model provides
a relative error reduction of 13% over the state-of-
the-art model of Naseem et al (2012), outperforming
it on 15 out of 16 evaluated languages.
Acknowledgments We thank Alexander Rush for
help with the hypergraph framework used for inference.
Tahira Naseem kindly provided us with her data sets and
the predictions of her systems. This work benefited from
many discussions with Yoav Goldberg and members of the
Google parsing team. We finally thank the three anony-
mous reviewers for their valuable feedback. The work of
the first author was partly funded by the Swedish National
Graduate School of Language Technology (GSLT).
1069
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative reranking.
In Proceedings of ACL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Hal Daum? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of ACL.
Mark Dredze, Partha Pratim Talukdar, and Koby Cram-
mer. 2009. Sequence learning from data with multiple
labels. In Proceedings of the ECML/PKDD Workshop
on Learning from Multi-Label Data.
Matthew S. Dryer and Martin Haspelmath, editors. 2011.
The World Atlas of Language Structures Online. Mu-
nich: Max Planck Digital Library. http://wals.info/.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntac-
tic transfer using a bilingual lexicon. In Proceedings of
EMNLP-CoNLL.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: an exploration. In Proceedings of
COLING.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical Bayesian domain adaptation. In Proceed-
ings of HLT-NAACL.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL-IJCNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Rong Jin and Zoubin Ghahramani. 2002. Learning with
multiple labels. In Proceedings of NIPS.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of dependency
and constituency. In Proceedings of ACL.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012. Inducing crosslingual distributed representations
of words. In Proceedings of COLING.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adaptation.
In Proceedings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
EMNLP.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of ACL.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, III, and Mark Johnson.
2002. Parsing the Wall Street Journal using a lexical-
functional grammar and discriminative estimation tech-
niques. In Proceedings of ACL.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of EMNLP-CoNLL.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2009. Adding more languages im-
proves unsupervised multilingual part-of-speech tag-
ging: A Bayesian non-parametric approach. In Pro-
ceedings of NAACL.
Anders S?gaard and Julie Wulff. 2012. An empirical
study of non-lexical extensions to delexicalized transfer.
In Proceedings of COLING.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Pro-
ceedings of ACL.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using incomplete
and noisy training data. In Proceedings of CONLL.
Oscar T?ckstr?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Oscar T?ckstr?m. 2012. Nudging the envelope of direct
transfer methods for multilingual named entity recog-
nition. In Proceedings of the NAACL-HLT 2012 Work-
shop on Inducing Linguistic Structure (WILS 2012).
1070
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In IJC-
NLP Workshop: NLP for Less Privileged Languages.
1071
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 569?574,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-supervised latent variable models for sentence-level sentiment analysis
Oscar Ta?ckstro?m
SICS, Kista / Uppsala University, Uppsala
oscar@sics.se
Ryan McDonald
Google, Inc., New York
ryanmcd@google.com
Abstract
We derive two variants of a semi-supervised
model for fine-grained sentiment analysis.
Both models leverage abundant natural super-
vision in the form of review ratings, as well as
a small amount of manually crafted sentence
labels, to learn sentence-level sentiment clas-
sifiers. The proposed model is a fusion of a
fully supervised structured conditional model
and its partially supervised counterpart. This
allows for highly efficient estimation and infer-
ence algorithms with rich feature definitions.
We describe the two variants as well as their
component models and verify experimentally
that both variants give significantly improved
results for sentence-level sentiment analysis
compared to all baselines.
1 Sentence-level sentiment analysis
In this paper, we demonstrate how combining
coarse-grained and fine-grained supervision bene-
fits sentence-level sentiment analysis ? an important
task in the field of opinion classification and retrieval
(Pang and Lee, 2008). Typical supervised learning ap-
proaches to sentence-level sentiment analysis rely on
sentence-level supervision. While such fine-grained
supervision rarely exist naturally, and thus requires
labor intensive manual annotation effort (Wiebe et
al., 2005), coarse-grained supervision is naturally
abundant in the form of online review ratings. This
coarse-grained supervision is, of course, less infor-
mative compared to fine-grained supervision, how-
ever, by combining a small amount of sentence-level
supervision with a large amount of document-level
supervision, we are able to substantially improve on
the sentence-level classification task. Our work com-
bines two strands of research: models for sentiment
analysis that take document structure into account;
and models that use latent variables to learn unob-
served phenomena from that which can be observed.
Exploiting document structure for sentiment anal-
ysis has attracted research attention since the early
work of Pang and Lee (2004), who performed min-
imal cuts in a sentence graph to select subjective
sentences. McDonald et al (2007) later showed that
jointly learning fine-grained (sentence) and coarse-
grained (document) sentiment improves predictions
at both levels. More recently, Yessenalina et al
(2010) described how sentence-level latent variables
can be used to improve document-level prediction
and Nakagawa et al (2010) used latent variables over
syntactic dependency trees to improve sentence-level
prediction, using only labeled sentences for training.
In a similar vein, Sauper et al (2010) integrated gen-
erative content structure models with discriminative
models for multi-aspect sentiment summarization
and ranking. These approaches all rely on the avail-
ability of fine-grained annotations, but Ta?ckstro?m
and McDonald (2011) showed that latent variables
can be used to learn fine-grained sentiment using only
coarse-grained supervision. While this model was
shown to beat a set of natural baselines with quite a
wide margin, it has its shortcomings. Most notably,
due to the loose constraints provided by the coarse
supervision, it tends to only predict the two dominant
fine-grained sentiment categories well for each docu-
ment sentiment category, so that almost all sentences
in positive documents are deemed positive or neutral,
and vice versa for negative documents. As a way of
overcoming these shortcomings, we propose to fuse
a coarsely supervised model with a fully supervised
model.
Below, we describe two ways of achieving such
a combined model in the framework of structured
conditional latent variable models. Contrary to (gen-
erative) topic models (Mei et al, 2007; Titov and
569
a) yd
? ? ? ysi?1 y
s
i y
s
i+1 ? ? ?
? ? ? si?1 si si+1 ? ? ?
b) yd
? ? ? ysi?1 y
s
i y
s
i+1 ? ? ?
? ? ? si?1 si si+1 ? ? ?
Figure 1: a) Factor graph of the fully observed graphical model. b) Factor graph of the corresponding latent variable
model. During training, shaded nodes are observed, while non-shaded nodes are unobserved. The input sentences si are
always observed. Note that there are no factors connecting the document node, yd, with the input nodes, s, so that the
sentence-level variables, ys, in effect form a bottleneck between the document sentiment and the input sentences.
McDonald, 2008; Lin and He, 2009), structured con-
ditional models can handle rich and overlapping fea-
tures and allow for exact inference and simple gradi-
ent based estimation. The former models are largely
orthogonal to the one we propose in this work and
combining their merits might be fruitful. As shown
by Sauper et al (2010), it is possible to fuse gener-
ative document structure models and task specific
structured conditional models. While we do model
document structure in terms of sentiment transitions,
we do not model topical structure. An interesting
avenue for future work would be to extend the model
of Sauper et al (2010) to take coarse-grained task-
specific supervision into account, while modeling
fine-grained task-specific aspects with latent vari-
ables.
Note also that the proposed approach is orthogonal
to semi-supervised and unsupervised induction of
context independent (prior polarity) lexicons (Turney,
2002; Kim and Hovy, 2004; Esuli and Sebastiani,
2009; Rao and Ravichandran, 2009; Velikovich et al,
2010). The output of such models could readily be
incorporated as features in the proposed model.
1.1 Preliminaries
Let d be a document consisting of n sentences, s =
(si)ni=1, with a document?sentence-sequence pair de-
noted d = (d, s). Let yd = (yd,ys) denote random
variables1 ? the document level sentiment, yd, and the
sequence of sentence level sentiment, ys = (ysi )
n
i=1.
1We are abusing notation throughout by using the same sym-
bols to refer to random variables and their particular assignments.
In what follows, we assume that we have access to
two training sets: a small set of fully labeled in-
stances, DF = {(dj ,ydj )}
mf
j=1, and a large set of
coarsely labeled instances DC = {(dj , ydj )}
mf+mc
j=mf+1
.
Furthermore, we assume that yd and all ysi take val-
ues in {POS, NEG, NEU}.
We focus on structured conditional models in the
exponential family, with the standard parametrization
p?(y
d,ys|s) = exp
{
??(yd,ys, s), ?? ?A?(s)
}
,
where ? ? <n is a parameter vector, ?(?) ? <n is a
vector valued feature function that factors according
to the graph structure outlined in Figure 1, and A?
is the log-partition function. This class of models is
known as conditional random fields (CRFs) (Lafferty
et al, 2001), when all variables are observed, and as
hidden conditional random fields (HCRFs) (Quattoni
et al, 2007), when only a subset of the variables are
observed.
1.2 The fully supervised fine-to-coarse model
McDonald et al (2007) introduced a fully super-
vised model in which predictions of coarse-grained
(document) and fine-grained (sentence) sentiment are
learned and inferred jointly. They showed that learn-
ing both levels jointly improved performance at both
levels, compared to learning each level individually,
as well as to using a cascaded model in which the
predictions at one level are used as input to the other.
Figure 1a outlines the factor graph of the corre-
570
sponding conditional random field.2 The parameters,
?F , of this model can be estimated from the set of
fully labeled data, DF , by maximizing the joint con-
ditional likelihood function
LF (?F ) =
mf?
j=1
log p?F (y
d
j ,y
s
j |sj)?
??F ?
2
2?2F
,
where ?2F is the variance of the Normal(0, ?
2
F ) prior.
Note that LF is a concave function and consequently
its unique maximum can be found by gradient based
optimization techniques.
1.3 Latent variables for coarse supervision
Recently, Ta?ckstro?m and McDonald (2011) showed
that fine-grained sentiment can be learned from
coarse-grained supervision alone. Specifically, they
used a HCRF model with the same structure as that
in Figure 1a, but with sentence labels treated as la-
tent variables. The factor graph corresponding to this
model is outlined in Figure 1b.
The fully supervised model might benefit from fac-
tors that directly connect the document variable, yd,
with the inputs s. However, as argued by Ta?ckstro?m
and McDonald (2011), when only document-level
supervision is available, the document variable, yd,
should be independent of the input, s, conditioned
on the latent variables, ys. This prohibits the model
from bypassing the latent variables, which is crucial,
since we seek to improve the sentence-level predic-
tions, rather than the document-level predictions.
The parameters, ?C , of this model can be esti-
mated from the set of coarsely labeled data, DC , by
maximizing the marginalized conditional likelihood
function
LC(?C) =
mf+mc?
j=mf+1
log
?
ys
p?C (y
d
j ,y
s|sj)?
??C?
2
2?2C
,
where the marginalization is over all possible se-
quences of latent sentence label assignments ys.
Due to the introduction of latent variables, the
marginal likelihood function is non-concave and thus
there are no guarantees of global optimality, how-
ever, we can still use a gradient based optimization
technique to find a local maximum.
2Figure 1a differs slightly from the model employed by Mc-
Donald et al (2007), where they had factors connecting the
document label yd with each input si as well.
2 Combining coarse and full supervision
The fully supervised and the partially supervised
models both have their merits. The former requires
an expensive and laborious process of manual an-
notation, while the latter can be used with readily
available document labels, such as review star rat-
ings. The latter, however, has its shortcomings in
that the coarse-grained sentiment signal is less infor-
mative compared to a fine-grained signal. Thus, in
order to get the best of both worlds, we would like to
combine the merits of both of these models.
2.1 A cascaded model
A straightforward way of fusing the two models is
by means of a cascaded model in which the predic-
tions of the partially supervised model, trained by
maximizing LC(?C) are used to derive additional
features for the fully supervised model, trained by
maximizing LF (?F ).
Although more complex representations are pos-
sible, we generate meta-features for each sentence
based solely on operations on the estimated distribu-
tions, p?C (y
d, ysi |s). Specifically, we encode the fol-
lowing probability distributions as discrete features
by uniform bucketing, with bucket width 0.1: the
joint distribution, p?C (y
d, ysi |s); the marginal docu-
ment distribution, p?C (y
d|s); and the marginal sen-
tence distribution, p?C (y
s
i |s). We also encode the
argmax of these distributions, as well as the pair-
wise combinations of the derived features.
The upshot of this cascaded approach is that it is
very simple to implement and efficient to train. The
downside is that only the partially supervised model
influences the fully supervised model; there is no
reciprocal influence between the models. Given the
non-concavity of LC(?C), such influence could be
beneficial.
2.2 Interpolating likelihood functions
A more flexible way of fusing the two models is to
interpolate their likelihood functions, thereby allow-
ing for both coarse and joint supervision of the same
model. Such a combination can be achieved by con-
straining the parameters so that ?I = ?F = ?C and
taking the mean of the likelihood functions LF and
LC , appropriately weighted by a hyper-parameter ?.
571
The result is the interpolated likelihood function
LI(?I) = ?LF (?I) + (1? ?)LC(?I) .
A simple, yet efficient, way of optimizing this ob-
jective function is to use stochastic gradient ascent
with learning rate ?. At each step we select a fully
labeled instance, (dj ,ydj ) ? DF , with probability ?
and a coarsely labeled instance, (dj , ydj ) ? DC , with
probability (1? ?). We then update the parameters,
?I , according to the gradients ?LF and ?LC , respec-
tively. In principle we could use different learning
rates ?F and ?C as well as different prior variances
?2F and ?
2
C , but in what follows we set them equal.
Since we are interpolating conditional models, we
need at least partial observations of each instance.
Methods for blending discriminative and generative
models (Lasserre et al, 2006; Suzuki et al, 2007;
Agarwal and Daume?, 2009; Sauper et al, 2010),
would enable incorporation of completely unlabeled
data as well. It is straightforward to extend the pro-
posed model along these lines, however, in practice
coarsely labeled sentiment data is so abundant on
the web (e.g., rated consumer reviews) that incorpo-
rating completely unlabeled data seems superfluous.
Furthermore, using conditional models with shared
parameters throughout allows for rich overlapping
features, while maintaining simple and efficient in-
ference and estimation.
3 Experiments
For the following experiments, we used the same data
set and a comparable experimental setup to that of
Ta?ckstro?m and McDonald (2011).3 We compare the
two proposed hybrid models (Cascaded and Interpo-
lated) to the fully supervised model of McDonald et
al. (2007) (FineToCoarse) as well as to the soft vari-
ant of the coarsely supervised model of Ta?ckstro?m
and McDonald (2011) (Coarse).
The learning rate was fixed to ? = 0.001, while
we tuned the prior variances, ?2, and the number of
epochs for each model. When sampling according to
? during optimization of LI(?I), we cycle through
DF and DC deterministically, but shuffle these sets
between epochs. Due to time constraints, we fixed the
interpolation factor to ? = 0.1, but tuning this could
3The annotated test data can be downloaded from
http://www.sics.se/people/oscar/datasets.
potentially improve the results of the interpolated
model. For the same reason we allowed a maximum
of 30 epochs, for all models, while Ta?ckstro?m and
McDonald (2011) report a maximum of 75 epochs.
To assess the impact of fully labeled versus
coarsely labeled data, we took stratified samples with-
out replacement, of sizes 60, 120, and 240 reviews,
from the fully labeled folds and of sizes 15,000 and
143,580 reviews from the coarsely labeled data. On
average each review consists of ten sentences. We
performed 5-fold stratified cross-validation over the
labeled data, while using stratified samples for the
coarsely labeled data. Statistical significance was as-
sessed by a hierachical bootstrap of 95% confidence
intervals, using the technique described by Davison
and Hinkley (1997).
3.1 Results and analysis
Table 1 lists sentence-level accuracy along with 95%
confidence interval for all tested models. We first
note that the interpolated model dominates all other
models in terms of accuracy. While the cascaded
model requires both large amounts of fully labeled
and coarsely labeled data, the interpolated model
is able to take advantage of both types of data on
its own and jointly. Still, by comparing the fully
supervised and the coarsely supervised models, the
superior impact of fully labeled over coarsely labeled
data is evident. As can be seen in Figure 2, when
all data is used, the cascaded model outperforms the
interpolated model for some recall values, and vice
versa, while both models dominate the supervised
approach for the full range of recall values.
As discussed earlier, and confirmed by Table 2,
the coarse-grained model only performs well on the
predominant sentence-level categories for each docu-
ment category. The supervised model handles nega-
tive and neutral sentences well, but performs poorly
on positive sentences even in positive documents.
The interpolated model, while still better at capturing
the predominant category, does a better job overall.
These results are with a maximum of 30 training
iterations. Preliminary experiments with a maximum
of 75 iterations indicate that all models gain from
more iterations; this seems to be especially true for
the supervised model and for the cascaded model
with less amount of course-grained data.
572
|DC | = 15,000 |DC | = 143,580
|DF | = 60 |DF | = 120 |DF | = 240 |DF | = 60 |DF | = 120 |DF | = 240
FineToCoarse 49.3 (-1.3, 1.4) 53.4 (-1.8, 1.7) 54.6 (-3.6, 3.8) 49.3 (-1.3, 1.4) 53.4 (-1.8, 1.7) 54.6 (-3.6, 3.8)
Coarse 49.6 (-1.5, 1.8) 49.6 (-1.5, 1.8) 49.6 (-1.5, 1.8) 53.5 (-1.2, 1.4) 53.5 (-1.2, 1.4) 53.5 (-1.2, 1.4)
Cascaded 39.7 (-6.8, 5.7) 45.4 (-3.1, 2.9) 42.6 (-6.5, 6.5) 55.6 (-2.9, 2.7) 55.0 (-3.2, 3.4) 56.8 (-3.8, 3.6)
Interpolated 54.3 (-1.4, 1.4) 55.0 (-1.7, 1.6) 57.5 (-4.1, 5.2) 56.0 (-2.4, 2.1) 54.5 (-2.9, 2.8) 59.1 (-2.8, 3.4)
Table 1: Sentence level results for varying numbers of fully labeled (DF ) and coarsely labeled (DC) reviews. Bold:
significantly better than the FineToCoarse model according to a hierarchical bootstrapped confidence interval, p < 0.05.
0 10 20 30 40 50 60 70 80 90 100
Recall
0
10
20
30
40
50
60
70
80
90
100
Pr
ec
is
io
n
POS sentences
FineToCoarse
Cascaded
Interpolated
0 10 20 30 40 50 60 70 80 90 100
Recall
0
10
20
30
40
50
60
70
80
90
100
Pr
ec
is
io
n
NEG sentences
FineToCoarse
Cascaded
Interpolated
Figure 2: Interpolated POS / NEG sentence-level precision-recall curves with |DC | = 143,580 and |DF | = 240.
POS docs. NEG docs. NEU docs.
FineToCoarse 35 / 11 / 59 33 / 76 / 42 29 / 63 / 55
Coarse 70 / 14 / 43 11 / 71 / 34 43 / 47 / 53
Cascaded 43 / 17 / 61 0 / 75 / 49 10 / 64 / 50
Interpolated 73 / 16 / 51 42 / 72 / 48 54 / 52 / 57
Table 2: POS / NEG / NEU sentence-level F1-scores per
document category (|DC | = 143,580 and |DF | = 240).
4 Conclusions
Learning fine-grained classification tasks in a fully su-
pervised manner does not scale well due to the lack of
naturally occurring supervision. We instead proposed
to combine coarse-grained supervision, which is natu-
rally abundant but less informative, with fine-grained
supervision, which is scarce but more informative.
To this end, we introduced two simple, yet effective,
methods of combining fully labeled and coarsely
labeled data for sentence-level sentiment analysis.
First, a cascaded approach where a coarsely super-
vised model is used to generate features for a fully
supervised model. Second, an interpolated model
that directly optimizes a combination of joint and
marginal likelihood functions. Both proposed mod-
els are structured conditional models that allow for
rich overlapping features, while maintaining highly
efficient exact inference and robust estimation prop-
erties. Empirically, the interpolated model is superior
to the other investigated models, but with sufficient
amounts of coarsely labeled and fully labeled data,
the cascaded approach is competitive.
Acknowledgments
The first author acknowledges the support of the
Swedish National Graduate School of Language
Technology (GSLT). The authors would also like to
thank Fernando Pereira and Bob Carpenter for early
discussions on using HCRFs in sentiment analysis.
573
References
Arvind Agarwal and Hal Daume?. 2009. Exponential
family hybrid semi-supervised learning. In Proceed-
ings of the International Jont conference on Artifical
Intelligence (IJCAI).
Anthony C. Davison and David V. Hinkley. 1997. Boot-
strap Methods and Their Applications. Cambridge Se-
ries in Statistical and Probabilistic Mathematics. Cam-
bridge University Press, Cambridge, UK.
Andrea Esuli and Fabrizio Sebastiani. 2009. SentiWord-
Net: A publicly available lexical resource for opinion
mining. In Proceedings of the Language Resource and
Evaluation Conference (LREC).
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning (ICML).
Julia A. Lasserre, Christopher M. Bishop, and Thomas P.
Minka. 2006. Principled hybrids of generative and
discriminative models. In Proceedings of the IEEE
Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR).
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In Proceeding of the Con-
ference on Information and Knowledge Management
(CIKM).
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the Annual Conference of the Association for Computa-
tional Linguistics (ACL).
Q. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the International
Conference on World Wide Web (WWW).
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency Tree-based Sentiment Classification
using CRFs with Hidden Variables. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the Associ-
ation for Computational Linguistics (ACL).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Publishers.
Ariadna Quattoni, Sybor Wang, Louis-Philippe Morency,
Michael Collins, and Trevor Darrell. 2007. Hidden
conditional random fields. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the European Chapter of the Association for Compu-
tational Linguistics (EACL).
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text analy-
sis applications. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structured output learning based on
a hybrid generative and discriminative approach. In
Porceedings of the Conference on Emipirical Methods
in Natural Language Processing (EMNLP).
Oscar Ta?ckstro?m and Ryan McDonald. 2011. Discov-
ering fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the Euro-
pean Conference on Information Retrieval (ECIR).
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of the Annual World Wide Web Conference (WWW).
Peter Turney. 2002. Thumbs up or thumbs down? Senti-
ment orientation applied to unsupervised classification
of reviews. In Proceedings of the Annual Conference of
the Association for Computational Linguistics (ACL).
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of
web-derived polarity lexicons. In Proceedings of the
North American Chapter of the Association for Compu-
tational Linguistics (NAACL).
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. In Language Resources and Evaluation
(LREC).
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level senti-
ment classification. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP).
574
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
Transactions of the Association for Computational Linguistics, 1 (2013) 1?12. Action Editor: Sharon Goldwater.
Submitted 11/2012; Revised 1/2013; Published 3/2013. c?2013 Association for Computational Linguistics.
Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging
Oscar Ta?ckstro?m?? Dipanjan Das? Slav Petrov? Ryan McDonald? Joakim Nivre??
 Swedish Institute of Computer Science
?Department of Linguistics and Philology, Uppsala University
?Google Research, New York
oscar@sics.se
{dipanjand|slav|ryanmcd}@google.com
joakim.nivre@lingfil.uu.se
Abstract
We consider the construction of part-of-speech
taggers for resource-poor languages. Recently,
manually constructed tag dictionaries from
Wiktionary and dictionaries projected via bitext
have been used as type constraints to overcome
the scarcity of annotated data in this setting.
In this paper, we show that additional token
constraints can be projected from a resource-
rich source language to a resource-poor target
language via word-aligned bitext. We present
several models to this end; in particular a par-
tially observed conditional random field model,
where coupled token and type constraints pro-
vide a partial signal for training. Averaged
across eight previously studied Indo-European
languages, our model achieves a 25% relative
error reduction over the prior state of the art.
We further present successful results on seven
additional languages from different families,
empirically demonstrating the applicability of
coupled token and type constraints across a
diverse set of languages.
1 Introduction
Supervised part-of-speech (POS) taggers are avail-
able for more than twenty languages and achieve ac-
curacies of around 95% on in-domain data (Petrov et
al., 2012). Thanks to their efficiency and robustness,
supervised taggers are routinely employed in many
natural language processing applications, such as syn-
tactic and semantic parsing, named-entity recognition
and machine translation. Unfortunately, the resources
required to train supervised taggers are expensive to
create and unlikely to exist for the majority of written
?Work primarily carried out while at Google Research.
languages. The necessity of building NLP tools for
these resource-poor languages has been part of the
motivation for research on unsupervised learning of
POS taggers (Christodoulopoulos et al, 2010).
In this paper, we instead take a weakly supervised
approach towards this problem. Recently, learning
POS taggers with type-level tag dictionary constraints
has gained popularity. Tag dictionaries, noisily pro-
jected via word-aligned bitext, have bridged the gap
between purely unsupervised and fully supervised
taggers, resulting in an average accuracy of over 83%
on a benchmark of eight Indo-European languages
(Das and Petrov, 2011). Li et al (2012) further im-
proved upon this result by employing Wiktionary1 as
a tag dictionary source, resulting in the hitherto best
published result of almost 85% on the same setup.
Although the aforementioned weakly supervised
approaches have resulted in significant improvements
over fully unsupervised approaches, they have not
exploited the benefits of token-level cross-lingual
projection methods, which are possible with word-
aligned bitext between a target language of interest
and a resource-rich source language, such as English.
This is the setting we consider in this paper (?2).
While prior work has successfully considered both
token- and type-level projection across word-aligned
bitext for estimating the model parameters of genera-
tive tagging models (Yarowsky and Ngai, 2001; Xi
and Hwa, 2005, inter alia), a key observation under-
lying the present work is that token- and type-level
information offer different and complementary sig-
nals. On the one hand, high confidence token-level
projections offer precise constraints on a tag in a
particular context. On the other hand, manually cre-
1http://www.wiktionary.org/.
1
ated type-level dictionaries can have broad coverage
and do not suffer from word-alignment errors; they
can therefore be used to filter systematic as well as
random noise in token-level projections.
In order to reap these potential benefits, we pro-
pose a partially observed conditional random field
(CRF) model (Lafferty et al, 2001) that couples to-
ken and type constraints in order to guide learning
(?3). In essence, the model is given the freedom to
push probability mass towards hypotheses consistent
with both types of information. This approach is flex-
ible: we can use either noisy projected or manually
constructed dictionaries to generate type constraints;
furthermore, we can incorporate arbitrary features
over the input. In addition to standard (contextual)
lexical features and transition features, we observe
that adding features from a monolingual word cluster-
ing (Uszkoreit and Brants, 2008) can significantly im-
prove accuracy. While most of these features can also
be used in a generative feature-based hidden Markov
model (HMM) (Berg-Kirkpatrick et al, 2010), we
achieve the best accuracy with a globally normalized
discriminative CRF model.
To evaluate our approach, we present extensive
results on standard publicly available datasets for 15
languages: the eight Indo-European languages pre-
viously studied in this context by Das and Petrov
(2011) and Li et al (2012), and seven additional lan-
guages from different families, for which no compa-
rable study exists. In ?4 we compare various features,
constraints and model types. Our best model uses
type constraints derived from Wiktionary, together
with token constraints derived from high-confidence
word alignments. When averaged across the eight
languages studied by Das and Petrov (2011) and Li
et al (2012), we achieve an accuracy of 88.8%. This
is a 25% relative error reduction over the previous
state of the art. Averaged across all 15 languages,
our model obtains an accuracy of 84.5% compared to
78.5% obtained by a strong generative baseline. Fi-
nally, we provide an in depth analysis of the relative
contributions of the two types of constraints in ?5.
2 Coupling Token and Type Constraints
Type-level information has been amply used in
weakly supervised POS induction, either via pure
manually crafted tag dictionaries (Smith and Eisner,
2005; Ravi and Knight, 2009; Garrette and Baldridge,
2012), noisily projected tag dictionaries (Das and
Petrov, 2011) or through crowdsourced lexica, such
as Wiktionary (Li et al, 2012). At the other end
of the spectrum, there have been efforts that project
token-level information across word-aligned bitext
(Yarowsky and Ngai, 2001; Xi and Hwa, 2005). How-
ever, systems that combine both sources of informa-
tion in a single model have yet to be fully explored.
The following three subsections outline our overall
approach for coupling these two types of information
to build robust POS taggers that do not require any
direct supervision in the target language.
2.1 Token Constraints
For the majority of resource-poor languages, there
is at least some bitext with a resource-rich source
language; for simplicity, we choose English as our
source language in all experiments. It is then nat-
ural to consider using a supervised part-of-speech
tagger to predict part-of-speech tags for the English
side of the bitext. These predicted tags can subse-
quently be projected to the target side via automatic
word alignments. This approach was pioneered by
Yarowsky and Ngai (2001), who used the resulting
partial target annotation to estimate the parameters
of an HMM. However, due to the automatic nature
of the word alignments and the POS tags, there will
be significant noise in the projected tags. To conquer
this noise, they used very aggressive smoothing tech-
niques when training the HMM. Fossum and Abney
(2005) used similar token-level projections, but in-
stead combined projections from multiple source lan-
guages to filter out random projection noise as well
as the systematic noise arising from different source
language annotations and syntactic divergences.
2.2 Type Constraints
It is well known that given a tag dictionary, even if
it is incomplete, it is possible to learn accurate POS
taggers (Smith and Eisner, 2005; Goldberg et al,
2008; Ravi and Knight, 2009; Naseem et al, 2009).
While widely differing in the specific model struc-
ture and learning objective, all of these approaches
achieve excellent results. Unfortunately, they rely
on tag dictionaries extracted directly from the un-
derlying treebank data. Such dictionaries provide in
depth coverage of the test domain and also list all
2
	
 
     
   

 
   	
 
  	  	  

	

	
 



	



 

	    


Figure 1: Lattice representation of the inference search space Y(x) for an authentic sentence in Swedish (?The farming
products must be pure and must not contain any additives?), after pruning with Wiktionary type constraints. The
correct parts of speech are listed underneath each word. Bold nodes show projected token constraints y?. Underlined
text indicates incorrect tags. The coupled constraints lattice Y?(x, y?) consists of the bold nodes together with nodes for
words that are lacking token constraints; in this case, the coupled constraints lattice thus defines exactly one valid path.
inflected forms ? both of which are difficult to obtain
and unrealistic to expect for resource-poor languages.
In contrast, Das and Petrov (2011) automatically
create type-level tag dictionaries by aggregating over
projected token-level information extracted from bi-
text. To handle the noise in these automatic dictionar-
ies, they use label propagation on a similarity graph
to smooth (and also expand) the label distributions.
While their approach produces good results and is
applicable to resource-poor languages, it requires a
complex multi-stage training procedure including the
construction of a large distributional similarity graph.
Recently, Li et al (2012) presented a simple and
viable alternative: crowdsourced dictionaries from
Wiktionary. While noisy and sparse in nature, Wik-
tionary dictionaries are available for 170 languages.2
Furthermore, their quality and coverage is growing
continuously (Li et al, 2012). By incorporating type
constraints from Wiktionary into the feature-based
HMM of Berg-Kirkpatrick et al (2010), Li et al were
able to obtain the best published results in this setting,
surpassing the results of Das and Petrov (2011) on
eight Indo-European languages.
2.3 Coupled Constraints
Rather than relying exclusively on either token or
type constraints, we propose to complement the one
with the other during training. For each sentence in
our training set, a partially constrained lattice of tag
sequences is constructed as follows:
2http://meta.wikimedia.org/wiki/
Wiktionary ? October 2012.
1. For each token whose type is not in the tag dic-
tionary, we allow the entire tag set.
2. For each token whose type is in the tag dictio-
nary, we prune all tags not licensed by the dictio-
nary and mark the token as dictionary-pruned.
3. For each token that has a tag projected via a
high-confidence bidirectional word alignment:
if the projected tag is still present in the lattice,
then we prune every tag but the projected tag for
that token; if the projected tag is not present in
the lattice, which can only happen for dictionary-
pruned tokens, then we ignore the projected tag.
Figure 1 provides a running example. The lattice
shows tags permitted after constraining the words
to tags licensed by the dictionary (up until Step 2
from above). There is only a single token ?Jordbruk-
sprodukterna? (?the farming products?) not in the
dictionary; in this case the lattice permits the full
set of tags. With token-level projections (Step 3;
nodes with bold border in Figure 1), the lattice can
be further pruned. In most cases, the projected tag
is both correct and is in the dictionary-pruned lattice.
We thus successfully disambiguate such tokens and
shrink the search space substantially.
There are two cases we highlight in order to show
where our model can break. First, for the token
?Jordbruksprodukterna?, the erroneously projected
tag ADJ will eliminate all other tags from the lattice,
including the correct tag NOUN. Second, the token
?na?gra? (?any?) has a single dictionary entry PRON
and is missing the correct tag DET. In the case where
3
DET is the projected tag, we will not add it to the
lattice and simply ignore it. This is because we hy-
pothesize that the tag dictionary can be trusted more
than the tags projected via noisy word alignments. As
we will see in ?4, taking the union of tags performs
worse, which supports this hypothesis.
For generative models, such as HMMs (?3.1), we
need to define only one lattice. For our best gen-
erative model this is the coupled token- and type-
constrained lattice.3 At prediction time, in both the
discriminative and the generative cases, we find the
most likely label sequence using Viterbi decoding.
For discriminative models, such as CRFs (?3.2),
we need to define two lattices: one that the model
moves probability mass towards and another one
defining the overall search space (or partition func-
tion). In traditional supervised learning without a
dictionary, the former is a trivial lattice containing
the gold standard tag sequence and the latter is the
set of all possible tag sequences spanning the tokens.
With our best model, we will move mass towards the
coupled token- and type-constrained lattice, such that
the model can freely distribute mass across all paths
consistent with these constraints. The lattice defining
the partition function will be the full set of possible
tag sequences when no dictionary is used; when a
dictionary is used it will consist of all dictionary-
pruned tag sequences (sans Step 3 above; the full set
of possibilities shown in Figure 1 for our running
example).
Figures 2 and 3 provide statistics regarding the
supervision coverage and remaining ambiguity. Fig-
ure 2 shows that more than two thirds of all tokens in
our training data are in Wiktionary. However, there is
considerable variation between languages: Spanish
has the highest coverage with over 90%, while Turk-
ish, an agglutinative language with a vast number
of word forms, has less than 50% coverage. Fig-
ure 3 shows that there is substantial uncertainty left
after pruning with Wiktionary, since tokens are rarely
fully disambiguated: 1.3 tags per token are allowed
on average for types in Wiktionary.
Figure 2 further shows that high-confidence align-
ments are available for about half of the tokens for
most languages (Japanese is a notable exception with
3Other training methods exist as well, for example, con-
trastive estimation (Smith and Eisner, 2005).
0
25
50
75
100
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Pe
rc
en
t o
f to
ke
ns
 c
ov
er
ed
Token
coverage Wiktionary Projected Projected+Filtered
Figure 2: Wiktionary and projection dictionary coverage.
Shown is the percentage of tokens in the target side of the
bitext that are covered by Wiktionary, that have a projected
tag, and that have a projected tag after intersecting the two.
0.0
0.5
1.0
1.5
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Nu
mb
er 
of 
tag
s p
er 
tok
en
Figure 3: Average number of licensed tags per token on
the target side of the bitext, for types in Wiktionary.
less than 30% of the tokens covered). Intersecting the
Wiktionary tags and the projected tags (Step 2 and 3
above) filters out some of the potentially erroneous
tags, but preserves the majority of the projected tags;
the remaining, presumably more accurate projected
tags cover almost half of all tokens, greatly reducing
the search space that the learner needs to explore.
3 Models with Coupled Constraints
We now formally present how we couple token and
type constraints and how we use these coupled con-
straints to train probabilistic tagging models. Let
x = (x1x2 . . . x|x|) ? X denote a sentence, where
each token xi ? V is an instance of a word type from
the vocabulary V and let y = (y1y2 . . . y|x|) ? Y de-
note a tag sequence, where yi ? T is the tag assigned
to token xi and T denotes the set of all possible part-
of-speech tags. We denote the lattice of all admissible
tag sequences for the sentence x by Y(x). This is the
4
inference search space in which the tagger operates.
As we shall see, it is crucial to constrain the size of
this lattice in order to simplify learning when only
incomplete supervision is available.
A tag dictionary maps a word type xj ? V to
a set of admissible tags T (xj) ? T . For word
types not in the dictionary we allow the full set of
tags T (while possible, in this paper we do not at-
tempt to distinguish closed-class versus open-class
words). When provided with a tag dictionary, the
lattice of admissible tag sequences for a sentence x
is Y(x) = T (x1) ? T (x2) ? . . . ? T (x|x|). When
no tag dictionary is available, we simply have the full
lattice Y(x) = T |x|.
Let y? = (y?1y?2 . . . y?|x|) be the projected tags for
the sentence x. Note that {y?i} = ? for tokens without
a projected tag. Next, we define a piecewise operator
_ that couples y? and Y(x) with respect to every
sentence index, which results in a token- and type-
constrained lattice. The operator behaves as follows,
coherent with the high level description in ?2.3:
T? (xi, y?i) = y?i _ T (xi) =
{
{y?i} if y?i ? T (xi)
T (xi) otherwise .
We denote the token- and type-constrained lattice as
Y?(x, y?) = T? (x1, y?1)?T? (x2, y?2)?. . .?T? (x|x|, y?|x|).
Note that when token-level projections are not used,
the dictionary-pruned lattice and the lattice with cou-
pled constraints are identical, that is Y?(x, y?) = Y(x).
3.1 HMMs with Coupled Constraints
A first-order hidden Markov model (HMM) specifies
the joint distribution of a sentence x ? X and a
tag-sequence y ? Y(x) as:
p?(x, y) =
|x|?
i=1
p?(xi | yi)? ?? ?
emission
p?(yi | yi?1)? ?? ?
transition
.
We follow the recent trend of using a log-linear
parametrization of the emission and the transition
distributions, instead of a multinomial parametriza-
tion (Chen, 2003). This allows model parameters ?
to be shared across categorical events, which has
been shown to give superior performance (Berg-
Kirkpatrick et al, 2010). The categorical emission
and transition events are represented by feature vec-
tors ?(xi, yi) and ?(yi, yi?1). Each element of the
parameter vector ? corresponds to a particular fea-
ture; the component log-linear distributions are:
p?(xi | yi) =
exp
(
?>?(xi, yi)
)
?
x?i?V exp (?
>?(x?i, yi))
,
and
p?(yi | yi?1) =
exp
(
?>?(yi, yi?1)
)
?
y?i?T exp (?
>?(y?i, yi?1))
.
In maximum-likelihood estimation of the parameters,
we seek to maximize the likelihood of the observed
parts of the data. For this we need the joint marginal
distribution p?(x, Y?(x, y?)) of a sentence x, and its
coupled constraints lattice Y?(x, y?), which is obtained
by marginalizing over all consistent outputs:
p?(x, Y?(x, y?)) =
?
y?Y?(x,y?)
p?(x, y) .
If there are no projections and no tag dictionary, then
Y?(x, y?) = T |x|, and thus p?(x, Y?(x, y?)) = p?(x),
which reduces to fully unsupervised learning. The
`2-regularized marginal joint log-likelihood of the
constrained training data D = {(x(i), y?(i))}ni=1 is:
L(?;D) =
n?
i=1
log p?(x(i), Y?(x(i), y?(i)))?? ???22 .
(1)
We follow Berg-Kirkpatrick et al (2010) and take a
direct gradient approach for optimizing Eq. 1 with
L-BFGS (Liu and Nocedal, 1989). We set ? = 1 and
run 100 iterations of L-BFGS. One could also em-
ploy the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977) to optimize this objective, al-
though the relative merits of EM versus direct gradi-
ent training for these models is still a topic of debate
(Berg-Kirkpatrick et al, 2010; Li et al, 2012).4 Note
that since the marginal likelihood is non-concave, we
are only guaranteed to find a local maximum of Eq. 1.
After estimating the model parameters ?, the tag-
sequence y? ? Y(x) for a sentence x ? X is pre-
dicted by choosing the one with maximal joint prob-
ability:
y? ? arg max
y?Y(x)
p?(x, y) .
4We trained the HMM with EM as well, but achieved better
results with direct gradient training and hence omit those results.
5
3.2 CRFs with Coupled Constraints
Whereas an HMM models the joint probability of
the input x ? X and output y ? Y(x), using locally
normalized component distributions, a conditional
random field (CRF) instead models the probability of
the output conditioned on the input as a globally nor-
malized log-linear distribution (Lafferty et al, 2001):
p?(y | x) =
exp
(
?>?(x, y)
)
?
y??Y(x) exp (?>?(x, y?))
,
where ? is a parameter vector. As for the HMM,
Y(x) is not necessarily the full space of possible
tag-sequences; specifically, for us, it is the dictionary-
pruned lattice without the token constraints.
With a first-order Markov assumption, the feature
function factors as:
?(x, y) =
|x|?
i=1
?(x, yi, yi?1) .
This model is more powerful than the HMM in that
it can use richer feature definitions, such as joint in-
put/transition features and features over a wider input
context. We model a marginal conditional probabil-
ity, given by the total probability of all tag sequences
consistent with the lattice Y?(x, y?):
p?(Y?(x, y?) | x) =
?
y?Y?(x,y?)
p?(y | x) .
The parameters of this constrained CRF are estimated
by maximizing the `2-regularized marginal condi-
tional log-likelihood of the constrained data (Riezler
et al, 2002):
L(?;D) =
n?
i=1
log p?(Y?(x(i), y?(i)) | x(i))? ????22 .
(2)
As with Eq. 1, we maximize Eq. 2 with 100 itera-
tions of L-BFGS and set ? = 1. In contrast to the
HMM, after estimating the model parameters ?, the
tag-sequence y? ? Y(x) for a sentence x ? X is
chosen as the sequence with the maximal conditional
probability:
y? ? arg max
y?Y(x)
p?(y | x) .
4 Empirical Study
We now present a detailed empirical study of the mod-
els proposed in the previous sections. In addition to
comparing with the state of the art in Das and Petrov
(2011) and Li et al (2012), we present models with
several combinations of token and type constraints,
additional features incorporating word clusters. Both
generative and discriminative models are explored.
4.1 Experimental Setup
Before delving into the experimental details, we
present our setup and datasets.
Languages. We evaluate on eight target languages
used in previous work (Das and Petrov, 2011; Li et
al., 2012) and on seven additional languages (see Ta-
ble 1). While the former eight languages all belong to
the Indo-European family, we broaden the coverage
to language families more distant from the source
language (for example, Chinese, Japanese and Turk-
ish). We use the treebanks from the CoNLL shared
tasks on dependency parsing (Buchholz and Marsi,
2006; Nivre et al, 2007) for evaluation.5 The two-
letter abbreviations from the ISO 639-1 standard are
used when referring to these languages in tables and
figures.
Tagset. In all cases, we map the language-specific
POS tags to universal POS tags using the mapping
of Petrov et al (2012).6 Since we use indirect super-
vision via projected tags or Wiktionary, the model
states induced by all models correspond directly to
POS tags, enabling us to compute tagging accuracy
without a greedy 1-to-1 or many-to-1 mapping.
Bitext. For all experiments, we use English as the
source language. Depending on availability, there
are between 1M and 5M parallel sentences for each
language. The majority of the parallel data is gath-
ered automatically from the web using the method
of Uszkoreit et al (2010). We further include data
from Europarl (Koehn, 2005) and from the UN par-
allel corpus (UN, 2006), for languages covered by
these corpora. The English side of the bitext is
POS tagged with a standard supervised CRF tagger,
trained on the Penn Treebank (Marcus et al, 1993),
with tags mapped to universal tags. The parallel sen-
5For French we use the treebank of Abeille? et al (2003).
6We use version 1.03 of the mappings available at http:
//code.google.com/p/universal-pos-tags/.
6
tences are word aligned with the aligner of DeNero
and Macherey (2011). Intersected high-confidence
alignments (confidence >0.95) are extracted and ag-
gregated into projected type-level dictionaries. For
purely practical reasons, the training data with token-
level projections is created by randomly sampling
target-side sentences with a total of 500K tokens.
Wiktionary. We use a snapshot of the Wiktionary
word definitions, and follow the heuristics of Li et
al. (2012) for creating the Wiktionary dictionary by
mapping the Wiktionary tags to universal POS tags.7
Features. For all models, we use only an identity
feature for tag-pair transitions. We use five features
that couple the current tag and the observed word
(analogous to the emission in an HMM): word iden-
tity, suffixes of up to length 3, and three indicator
features that fire when the word starts with a capital
letter, contains a hyphen or contains a digit. These are
the same features as those used by Das and Petrov
(2011). Finally, for some models we add a word
cluster feature that couples the current tag and the
word cluster identity of the word. These (monolin-
gual) word clusters are induced with the exchange
algorithm (Uszkoreit and Brants, 2008). We set the
number of clusters to 256 across all languages, as this
has previously been shown to produce robust results
for similar tasks (Turian et al, 2010; Ta?ckstro?m et
al., 2012). The clusters for each language are learned
on a large monolingual newswire corpus.
4.2 Models with Type Constraints
To examine the sole effect of type constraints, we
experiment with the HMM, drawing constraints from
three different dictionaries. Table 1 compares the per-
formance of our models with the best results of Das
and Petrov (2011, D&P) and Li et al (2012, LG&T).
As in previous work, training is done exclusively on
the training portion of each treebank, stripped of any
manual linguistic annotation.
We first use all of our parallel data to generate
projected tag dictionaries: the English POS tags are
projected across word alignments and aggregated to
tag distributions for each word type. As in Das and
Petrov (2011), the distributions are then filtered with
a threshold of 0.2 to remove noisy tags and to create
7The definitions were downloaded on August 31, 2012 from
http://toolserver.org/?enwikt/definitions/.
This snapshot is more recent than that used by Li et al
Prior work HMM with type constraints
Lang. D&P LG&T YHMMproj. YHMMwik. YHMMunion YHMMunion +C
bg ? ? 84.2 68.1 87.2 87.9
cs ? ? 75.4 70.2 75.4 79.2
da 83.2 83.3 87.7 82.0 78.4 89.5
de 82.8 85.8 86.6 85.1 80.0 88.3
el 82.5 79.2 83.3 83.8 86.0 83.2
es 84.2 86.4 83.9 83.7 88.3 87.3
fr ? ? 88.4 75.7 75.6 86.6
it 86.8 86.5 89.0 85.4 89.9 90.6
ja ? ? 45.2 76.9 74.4 73.7
nl 79.5 86.3 81.7 79.1 83.8 82.7
pt 87.9 84.5 86.7 79.0 83.8 90.4
sl ? ? 78.7 64.8 82.8 83.4
sv 80.5 86.1 80.6 85.9 85.9 86.7
tr ? ? 66.2 44.1 65.1 65.7
zh ? ? 59.2 73.9 63.2 73.0
avg (8) 83.4 84.8 84.9 83.0 84.5 87.3
avg ? ? 78.5 75.9 80.0 83.2
Table 1: Tagging accuracies for type-constrained HMM
models. D&P is the ?With LP? model in Table 2 of
Das and Petrov (2011), while LG&T is the ?SHMM-ME?
model in Table 2 of Li et al (2012). YHMMproj. , YHMMwik. and
YHMMunion are HMMs trained solely with type constraints
derived from the projected dictionary, Wiktionary and
the union of these dictionaries, respectively. YHMMunion +C is
equivalent to YHMMunion with additional cluster features. All
models are trained on the treebank of each language,
stripped of gold labels. Results are averaged over the
8 languages from Das and Petrov (2011), denoted avg (8),
as well as over the full set of 15 languages, denoted avg.
an unweighted tag dictionary. We call this model
YHMMproj. ; its average accuracy of 84.9% on the eight
languages is higher than the 83.4% of D&P and on
par with LG&T (84.8%).8 Our next model (YHMMwik. )
simply draws type constraints from Wiktionary. It
slightly underperforms LG&T (83.0%), presumably
because they used a second-order HMM. As a simple
extension to these two models, we take the union
of the projected dictionary and Wiktionary to con-
strain an HMM, which we name YHMMunion . This model
performs a little worse on the eight Indo-European
languages (84.5), but gives an improvement over the
projected dictionary when evaluated across all 15
languages (80.0% vs. 78.5%).
8Our model corresponds to the weaker, ?No LP? projection
of Das and Petrov (2011). We found that label propagation was
only beneficial when small amounts of bitext were available.
7
Token constraints HMM with coupled constraints CRF with coupled constraints
Lang. YHMMunion +C+L y?HMM+C+L y?CRF+C+L Y?HMMproj. +C+L Y?HMMwik. +C+L Y?HMMunion +C+L Y?CRFproj. +C+L Y?CRFwik. +C+L Y?CRFunion+C+L
bg 87.7 77.9 84.1 84.5 83.9 86.7 86.0 87.8 85.4
cs 78.3 65.4 74.9 74.8 81.1 76.9 74.7 80.3** 75.0
da 87.3 80.9 85.1 87.2 85.6 88.1 85.5 88.2* 86.0
de 87.7 81.4 83.3 85.0 89.3 86.7 84.4 90.5** 85.5
el 85.9 81.1 77.8 80.1 87.0 83.9 79.6 89.5** 79.7
es 89.1** 84.1 85.5 83.7 85.9 88.0 85.7 87.1 86.0
fr 88.4** 83.5 84.7 85.9 86.4 87.4 84.9 87.2 85.6
it 89.6 85.2 88.5 88.7 87.6 89.8 88.3 89.3 89.4
ja 72.8 47.6 54.2 43.2 76.1 70.5 44.9 81.0** 68.0
nl 83.1 78.4 82.4 82.3 84.2 83.2 83.1 85.9** 83.2
pt 89.1 84.7 87.0 86.6 88.7 88.0 87.9 91.0** 88.3
sl 82.4 69.8 78.2 78.5 81.8 80.1 79.7 82.3 80.0
sv 86.1 80.1 84.2 82.3 87.9 86.9 84.4 88.9** 85.5
tr 62.4 58.1 64.5 64.6 61.8 64.8 65.0 64.1** 65.2
zh 72.6 52.7 39.5 56.0 74.1 73.3 59.7 74.4** 73.4
avg (8) 87.2 82.0 84.2 84.5 87.0 86.8 84.9 88.8 85.4
avg 82.8 74.1 76.9 77.6 82.8 82.3 78.2 84.5 81.1
Table 2: Tagging accuracies for models with token constraints and coupled token and type constraints. All models use
cluster features (. . . +C) and are trained on large training sets each containing 500k tokens with (partial) token-level
projections (. . . +L). The best type-constrained model, trained on the larger datasets, YHMMunion +C+L, is included for
comparison. The remaining columns correspond to HMM and CRF models trained only with token constraints (y? . . .)
and with coupled token and type constraints (Y? . . .). The latter are trained using the projected dictionary (?proj.),
Wiktionary (?wik.) and the union of these dictionaries (?union), respectively. The search spaces of the models trained with
coupled constraints (Y? . . .) are each pruned with the respective tag dictionary used to derive the coupled constraints.
The observed difference between Y?CRFwik. +C+L and YHMMunion +C+L is statistically significant at p < 0.01 (**) and p < 0.015(*) according to a paired bootstrap test (Efron and Tibshirani, 1993). Significance was not assessed for avg or avg (8).
We next add monolingual cluster features to
the model with the union dictionary. This model,
YHMMunion +C, significantly outperforms all other type-
constrained models, demonstrating the utility of
word-cluster features.9 For further exploration, we
train the same model on the datasets containing 500K
tokens sampled from the target side of the parallel
data (YHMMunion +C+L); this is done to explore the effects
of large data during training. We find that training
on these datasets result in an average accuracy of
87.2% which is comparable to the 87.3% reported
for YHMMunion +C in Table 1. This shows that the different
source domain and amount of training data does not
influence the performance of the HMM significantly.
Finally, we train CRF models where we treat type
constraints as a partially observed lattice and use the
full unpruned lattice for computing the partition func-
9These are monolingual clusters. Bilingual clusters as intro-
duced in Ta?ckstro?m et al (2012) might bring additional benefits.
tion (?3.2). Due to space considerations, the results
of these experiments are not shown in table 1. We ob-
serve similar trends in these results, but on average,
accuracies are much lower compared to the type-
constrained HMM models; the CRF model with the
union dictionary along with cluster features achieves
an average accuracy of 79.3% when trained on same
data. This result is not unsurprising. First, the CRF?s
search space is fully unconstrained. Second, the dic-
tionary only provides a weak set of observation con-
straints, which do not provide sufficient information
to successfully train a discriminative model. How-
ever, as we will observe next, coupling the dictionary
constraints with token-level information solves this
problem.
4.3 Models with Token and Type Constraints
We now proceed to add token-level information,
focusing in particular on coupled token and type
8
constraints. Since it is not possible to generate
projected token constraints for our monolingual
treebanks, we train all models in this subsection
on the 500K-tokens datasets sampled from the bi-
text. As a baseline, we first train HMM and CRF
models that use only projected token constraints
(y?HMM+C+L and y?CRF+C+L). As shown in Table 2,
these models underperform the best type-level model
(YHMMunion +C+L),10 which confirms that projected to-
ken constraints are not reliable on their own. This
is in line with similar projection models previously
examined by Das and Petrov (2011).
We then study models with coupled token and type
constraints. These models use the same three dictio-
naries as used in ?4.2, but additionally couple the
derived type constraints with projected token con-
straints; see the caption of Table 2 for a list of these
models. Note that since we only allow projected tags
that are licensed by the dictionary (Step 3 of the trans-
fer, ?2.3), the actual token constraints used in these
models vary with the different dictionaries.
From Table 2, we see that coupled constraints are
superior to token constraints, when used both with
the HMM and the CRF. However, for the HMM, cou-
pled constraints do not provide any benefit over type
constraints alone, in particular when the projected
dictionary or the union dictionary is used to derive the
coupled constraints (Y?HMMproj. +C+L and Y?HMMunion +C+L).
We hypothesize that this is because these dictionar-
ies (in particular the former) have the same bias as
the token-level tag projections, so that the dictionary
is unable to correct the systematic errors in the pro-
jections (see ?2.1). Since the token constraints are
stronger than the type constraints in the coupled mod-
els, this bias may have a substantial impact. With
the Wiktionary dictionary, the difference between the
type-constrained and the coupled-constrained HMM
is negligible: YHMMunion +C+L and Y?HMMwik. +C+L both av-
erage at an accuracy of 82.8%.
The CRF model, on the other hand, is able to take
advantage of the complementary information in the
coupled constraints, provided that the dictionary is
able to filter out the systematic token-level errors.
With a dictionary derived from Wiktionary and pro-
jected token-level constraints, Y?CRFwik. +C+L performs
10To make the comparison fair vis-a-vis potential divergences
in training domains, we compare to the best type-constrained
model trained on the same 500K tokens training sets.
0 1 2 3
0
25
50
75
100
0 1 10 100 0 1 10 100 0 1 10 100 0 1 10 100
Number of token?level projections
Ta
gg
ing
 ac
cur
ac
y
Number of tags listed in Wiktionary
Figure 4: Relative influence of token and type constraints
on tagging accuracy in the Y?CRFwik. +C+L model. Word typesare categorized according to a) their number of Wiktionary
tags (0,1,2 or 3+ tags, with 0 representing no Wiktionary
entry; top-axis) and b) the number of times they are token-
constrained in the training set (divided into buckets of
0, 1-9, 10-99 and 100+ occurrences; x-axis). The boxes
summarize the accuracy distributions across languages
for each word type category as defined by a) and b). The
horizontal line in each box marks the median accuracy,
the top and bottom mark the first and third quantile, re-
spectively, while the whiskers mark the minimum and
maximum values of the accuracy distribution.
better than all the remaining models, with an average
accuracy of 88.8% across the eight Indo-European
languages available to D&P and LG&T. Averaged
over all 15 languages, its accuracy is 84.5%.
5 Further Analysis
In this section we provide a detailed analysis of the
impact of token versus type constraints and we study
the pruning and filtering mistakes resulting from in-
complete Wiktionary entries in detail. This analysis
is based on the training portion of each treebank.
5.1 Influence of Token and Type Constraints
The empirical success of the model trained with cou-
pled token and type constraints confirms that these
constraints indeed provide complementary signals.
Figure 4 provides a more detailed view of the rela-
tive benefits of each type of constraint. We observe
several interesting trends.
First, word types that occur with more token con-
straints during training are generally tagged more
accurately, regardless of whether these types occur
9
90.0
92.5
95.0
97.5
100.0
0 50 100 150 200 250
Number of corrected Wiktionary entries
Pr
un
ing
 ac
cur
ac
y
Figure 5: Average pruning accuracy (line) across lan-
guages (dots) as a function of the number of hypotheti-
cally corrected Wiktionary entries for the k most frequent
word types. For example, position 100 on the x-axis cor-
responds to manually correcting the entries for the 100
most frequent types, while position 0 corresponds to ex-
perimental conditions.
in Wiktionary. The most common scenario is for a
word type to have exactly one tag in Wiktionary and
to occur with this projected tag over 100 times in
the training set (facet 1, rightmost box). These com-
mon word types are typically tagged very accurately
across all languages.
Second, the word types that are ambiguous accord-
ing to Wiktionary (facets 2 and 3) are predominantly
frequent ones. The accuracy is typically lower for
these words compared to the unambiguous words.
However, as the number of projected token con-
straints is increased from zero to 100+ observations,
the ambiguous words are effectively disambiguated
by the token constraints. This shows the advantage
of intersecting token and type constraints.
Finally, projection generally helps for words that
are not in Wiktionary, although the accuracy for these
words never reach the accuracy of the words with
only one tag in Wiktionary. Interestingly, words that
occur with a projected tag constraint less than 100
times are tagged more accurately for types not in the
dictionary compared to ambiguous word types with
the same number of projected constraints. A possible
explanation for this is that the ambiguous words are
inherently more difficult to predict and that most of
the words that are not in Wiktionary are less common
words that tend to also be less ambiguous.
zh
tr
sv
sl
pt
nl
jait
fr
es
el
de
da
cs
bg
avg
0 25 50 75 100
Proportion of pruning errors
PRON
NOUN
DET
ADP
PRT
ADV
NUM
CONJ
ADJ
VERB
X
.
Figure 6: Prevalence of pruning mistakes per POS tag,
when pruning the inference search space with Wiktionary.
5.2 Wiktionary Pruning Mistakes
The error analysis by Li et al (2012) showed that the
tags licensed by Wiktionary are often valid. When
using Wiktionary to prune the search space of our
constrained models and to filter token-level projec-
tions, it is also important that correct tags are not
mistakenly pruned because they are missing from
Wiktionary. While the accuracy of filtering is more
difficult to study, due to the lack of a gold standard
tagging of the bitext, Figure 5 (position 0 on the x-
axis) shows that search space pruning errors are not
a major issue for most languages; on average the
pruning accuracy is almost 95%. However, for some
languages such as Chinese and Czech the correct tag
is pruned from the search space for nearly 10% of all
tokens. When using Wiktionary as a pruner, the upper
bound on accuracy for these languages is therefore
only around 90%. However, Figure 5 also shows that
with some manual effort we might be able to remedy
many of these errors. For example, by adding miss-
ing valid tags to the 250 most common word types in
the worst language, the minimum pruning accuracy
would rise above 95% from below 90%. If the same
was to be done for all of the studied languages, the
mean pruning accuracy would reach over 97%.
Figure 6 breaks down pruning errors resulting from
incorrect or incomplete Wiktionary entries across
the correct POS tags. From this we observe that,
for many languages, the pruning errors are highly
skewed towards specific tags. For example, for Czech
over 80% of the pruning errors are caused by mistak-
enly pruned pronouns.
10
6 Conclusions
We considered the problem of constructing multilin-
gual POS taggers for resource-poor languages. To
this end, we explored a number of different models
that combine token constraints with type constraints
from different sources. The best results were ob-
tained with a partially observed CRF model that ef-
fectively integrates these complementary constraints.
In an extensive empirical study, we showed that this
approach substantially improves on the state of the
art in this context. Our best model significantly out-
performed the second-best model on 10 out of 15
evaluated languages, when trained on identical data
sets, with an insignificant difference on 3 languages.
Compared to the prior state of the art (Li et al, 2012),
we observed a relative reduction in error by 25%,
averaged over the eight languages common to our
studies.
Acknowledgments
We thank Alexander Rush for help with the hyper-
graph framework that was used to implement our
models and Klaus Macherey for help with the bi-
text extraction. This work benefited from many dis-
cussions with Yoav Goldberg, Keith Hall, Kuzman
Ganchev and Hao Zhang. We also thank the editor
and the three anonymous reviewers for their valuable
feedback. The first author is grateful for the financial
support from the Swedish National Graduate School
of Language Technology (GSLT).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a Treebank for French. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, John
DeNero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL-HLT.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Stanley F Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.
1977. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Brad Efron and Robert J. Tibshirani. 1993. An Introduc-
tion to the Bootstrap. Chapman & Hall, New York, NY,
USA.
Victoria Fossum and Steven Abney. 2005. Automatically
inducing a part-of-speech tagger by projecting from
multiple source languages across aligned corpora. In
Proceedings of IJCNLP.
Dan Garrette and Jason Baldridge. 2012. Type-supervised
hidden markov models for part-of-speech tagging with
incomplete tag dictionaries. In Proceedings of EMNLP-
CoNLL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of ACL-HLT.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2).
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. JAIR, 36.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
11
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, III, and Mark Johnson. 2002.
Parsing the wall street journal using a lexical-functional
grammar and discriminative estimation techniques. In
Proceedings of ACL.
Noah Smith and Jason Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data. In
Proceedings of ACL.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-English languages.
In Proceedings of HLT-EMNLP.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
12
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 248?252
Manchester, August 2008
Mixing and Blending Syntactic and Semantic Dependencies
Yvonne Samuelsson
Dept. of Linguistics
Stockholm University
yvonne.samuelsson@ling.su.se
Johan Eklund
SSLIS
University College of Bor?as
johan.eklund@hb.se
Oscar T
?
ackstr
?
om
Dept. of Linguistics and Philology
SICS / Uppsala University
oscar@sics.se
Mark Fi
?
sel
Dept. of Computer Science
University of Tartu
fishel@ut.ee
Sumithra Velupillai
Dept. of Computer and Systems Sciences
Stockholm University / KTH
sumithra@dsv.su.se
Markus Saers
Dept. of Linguistics and Philology
Uppsala University
markus.saers@lingfil.uu.se
Abstract
Our system for the CoNLL 2008 shared
task uses a set of individual parsers, a set of
stand-alone semantic role labellers, and a
joint system for parsing and semantic role
labelling, all blended together. The system
achieved a macro averaged labelled F
1
-
score of 79.79 (WSJ 80.92, Brown 70.49)
for the overall task. The labelled attach-
ment score for syntactic dependencies was
86.63 (WSJ 87.36, Brown 80.77) and the
labelled F
1
-score for semantic dependen-
cies was 72.94 (WSJ 74.47, Brown 60.18).
1 Introduction
This paper presents a system for the CoNLL 2008
shared task on joint learning of syntactic and se-
mantic dependencies (Surdeanu et al, 2008), com-
bining a two-step pipelined approach with a joint
approach.
In the pipelined system, eight different syntac-
tic parses were blended, yielding the input for two
variants of a semantic role labelling (SRL) system.
Furthermore, one of the syntactic parses was used
with an early version of the SRL system, to pro-
vide predicate predictions for a joint syntactic and
semantic parser. For the final submission, all nine
syntactic parses and all three semantic parses were
blended.
The system is outlined in Figure 1; the dashed
arrow indicates the potential for using the predi-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Process
Parse + SRL
Process
8 MaltParsers
Parser Blender
Process
2 Pipelined SRLs
SRL Blender
Joint Parser/SRL
Possible
Iteration
Figure 1: Overview of the submitted system.
cate prediction to improve the joint syntactic and
semantic system.
2 Dependency Parsing
The initial parsing system was created using Malt-
Parser (Nivre et al, 2007) by blending eight dif-
ferent parsers. To further advance the syntactic ac-
curacy, we added the syntactic structure predicted
by a joint system for syntactic and semantic depen-
dencies (see Section 3.4) in the blending process.
2.1 Parsers
The MaltParser is a dependency parser genera-
tor, with three parsing algorithms: Nivre?s arc
standard, Nivre?s arc eager (see Nivre (2004)
for a comparison between the two Nivre algo-
rithms), and Covington?s (Covington, 2001). Both
of Nivre?s algorithms assume projectivity, but
the MaltParser supports pseudo-projective parsing
(Nilsson et al, 2007), for projectivization and de-
projectivization.
248
WSJ Brown
Best single parse 85.22% 78.37%
LAS weights 87.00% 80.60%
Learned weights 87.36% 80.77%
Table 1: Labelled attachment score on the two test
sets of the best single parse, blended with weights
set to PoS labelled attachment score (LAS) and
blended with learned weights.
Four parsing algorithms (the two Nivre al-
gorithms, and Covington?s projective and non-
projective version) were used, creating eight
parsers by varying the parsing direction, left-to-
right and right-to-left. The latter was achieved by
reversing the word order in a pre-processing step
and then restoring it in post-processing. For the fi-
nal system, feature models and training parameters
were adapted from Hall et al (2007).
2.2 Blender
The single parses were blended following the pro-
cedure of Hall et al (2007). The parses of each
sentence were combined into a weighted directed
graph. The Chu-Liu-Edmonds algorithm (Chu and
Liu, 1965; Edmonds, 1967) was then used to find
the maximum spanning tree (MST) of the graph,
which was considered the final parse of the sen-
tence. The weight of each graph edge was calcu-
lated as the sum of the weights of the correspond-
ing edges in each single parse tree.
We used a simple iterative weight updating algo-
rithm to learn the individual weights of each single
parser output and part-of-speech (PoS) using the
development set. To construct an initial MST, the
labelled attachment score was used. Each single
weight, corresponding to an edge of the hypoth-
esis tree, was then iteratively updated by slightly
increasing or decreasing the weight, depending on
whether it belonged to a correct or incorrect edge
as compared to the reference tree.
2.3 Results
The results are summarized in Table 1; the parse
with LAS weights and the best single parse
(Nivre?s arc eager algorithm with left-to-right pars-
ing direction) are also included for comparison.
3 Semantic Role Labelling
The SRL system is a pipeline with three chained
stages: predicate identification, argument identifi-
cation, and argument classification. Predicate and
argument identification are treated as binary clas-
sification problems. In a simple post-processing
predicate classification step, a predicted predicate
is assigned the most frequent sense from the train-
ing data. Argument classification is treated as a
multi-class learning problem, where the classes
correspond to the argument types.
3.1 Learning and Parameter Optimization
For learning and prediction we used the freely
available support vector machine (SVM) imple-
mentation LIBSVM (version 2.86) (Chang and
Lin, 2001). The choice of cost and kernel parame-
ter values will often significantly influence the per-
formance of the SVM classifier. We therefore im-
plemented a parameter optimizer based on the DI-
RECT optimization algorithm (Gablonsky, 2001).
It iteratively divides the search space into smaller
hyperrectangles, sampling the objective function
in the centroid of each hyperrectangle, and select-
ing those hyperrectangles that are potentially opti-
mal for further processing. The search space con-
sisted of the SVM parameters to optimize and the
objective function was the cross-validation accu-
racy reported by LIBSVM.
Tests performed during training for predicate
identification showed that the use of runtime opti-
mization of the SVM parameters for nonlinear ker-
nels yielded a higher average F
1
-score effective-
ness. Surprisingly, the best nonlinear kernels were
always outperformed by the linear kernel with de-
fault settings, which indicates that the data is ap-
proximately linearly separable.
3.2 Filtering and Data Set Splitting
To decrease the number of instances during train-
ing, all predicate and argument candidates with
PoS-tags that occur very infrequently in the
training set were filtered out. Some PoS-tags
were filtered out for all three stages, e.g. non-
alphanumerics, HYPH, SYM, and LS. This ap-
proach was effective, e.g. removing more than half
of the total number of instances for predicate pre-
diction.
To speed up the SVM training and allow for
parallelization, each data set was split into several
bins. However, there is a trade-off between speed
and accuracy. Performance consistently deterio-
rated when splitting into smaller bins. The final
system contained two variants, one with more bins
based on a combination of PoS-tags and lemma
frequency information, and one with fewer bins
249
based only on PoS-tag information. The three
learning tasks used different splits. In general, the
argument identification step was the most difficult
and therefore required a larger number of bins.
3.3 Features
We implemented a large number of features (over
50)
1
for the SRL system. Many of them can be
found in the literature, starting from Gildea and
Jurafsky (2002) and onward. All features, except
bag-of-words, take nominal values, which are bi-
narized for the vectors used as input to the SVM
classifier. Low-frequency feature values (except
for Voice, Initial Letter, Number of Words, Rela-
tive Position, and the Distance features), below a
threshold of 20 occurrences, were given a default
value.
We distinguish between single node and node
pair features. The following single node features
were used for all three learning tasks and for both
the predicate and argument node:
2
? Lemma, PoS, and Dependency relation (DepRel) for
the node itself, the parent, and the left and right sibling
? Initial Letter (upper-case/lower-case), Number of
Words, and Voice (based on simple heuristics, only for
the predicate node during argument classification)
? PoS Sequence and PoS bag-of-words (BoW) for the
node itself with children and for the parent with chil-
dren
? Lemma and PoS for the first and last child of the node
? Sequence and BoW of Lemma and PoS for content
words
? Sequence and BoW of PoS for the immediate children?s
content words
? Sequence and BoW of PoS for the parent?s content
words and for the parent?s immediate children
? Sequence and BoW of DepRels for the node itself, for
the immediate children, and for the parent?s immediate
children
All extractors of node pair features, where the pair
consists of the predicate and the argument node,
can be used both for argument identification and
argument classification. We used the following
node pair features:
? Relative Position (the argument is before/after the pred-
icate), Distance in Words, Middle Distance in DepRels
? PoS Full Path, PoS Middle Path, PoS Short Path
1
Some features were discarded for the final system based
on Information Gain, calculated using Weka (Witten and
Frank, 2005).
2
For all features using lemma or PoS the (predicted) split
value is used.
The full path feature contains the PoS-tag of the ar-
gument node, all dependency relations between the
argument node and the predicate node and finally
the PoS-tag of the predicate node. The middle path
goes to the lowest common ancestor for argument
and predicate (this is also the distance calculated
by Middle Distance in DepRels) and the short path
only contains the dependency relation of the argu-
ment and predicate nodes.
3.4 Joint Syntactic and Semantic Parsing
When considering one predicate at a time, SRL be-
comes a regular labelling problem. Given a pre-
dicted predicate, joint learning of syntactic and se-
mantic dependencies can be carried out by simulta-
neously assigning an argument label and a depen-
dency relation. This is possible because we know
a priori where to attach the argument, since there
is only one predicate candidate
3
. The MaltParser
system for English described in Hall et al (2007)
was used as a baseline, and then optimized for this
new task, focusing on feature selection.
A large feature model was constructed, and
backward selection was carried out until no fur-
ther gain could be observed. The feature model of
MaltParser consists of a number of feature types,
each describing a starting point, a path through the
structure so far, and a column of the node arrived
at. The number of feature types was reduced from
37 to 35 based on the labelled F
1
-score.
As parsing is done at the same time as argu-
ment labelling, different syntactic structures risk
being assigned to the same sentence, depending
on which predicate is currently processed. This
means that several, possibly different, parses have
to be combined into one. In this experiment, the
head and the dependency label were concatenated,
and the most frequent one was used. In case of
a tie, the first one to appear was used. The like-
lihood of the chosen labelling was also used as a
confidence measure for the syntactic blender.
3.5 Blending and Post-Processing
Combining the output from several different sys-
tems has been shown to be beneficial (Koomen
et al, 2005). For the final submission, we com-
bined the output of two variants of the pipelined
SRL system, each using different data splits, with
3
The version of the joint system used in the submission
was based on an early predicate prediction. More accurate
predicates would give a major improvement for the results.
250
Test set Pred PoS Labelled F
1
Unlabelled F
1
WSJ All 82.90 90.90
NN* 81.12 86.39
VB* 85.52 96.49
Brown All 67.48 85.49
NN* 58.34 75.35
VB* 73.24 91.97
Table 2: Semantic predicate results on the test sets.
the SRL output of the joint system. A simple uni-
form weight majority vote heuristic was used, with
no combinatorial constraints on the selected argu-
ments. For each sentence, all predicates that were
identified by a majority of the systems were se-
lected. Then, for each selected predicate, its ar-
guments were picked by majority vote (ignoring
the systems not voting for the predicate). The best
single SRL system achieved a labelled F
1
-score
of 71.34 on the WSJ test set and 57.73 on the
Brown test set, compared to 74.47 and 60.18 for
the blended system.
As a final step, we filtered out all verbal and
nominal predicates not in PropBank or NomBank,
respectively, based on the predicted PoS-tag and
lemma. Each lexicon was expanded with lemmas
from the training set, due to predicted lemma er-
rors in the training data. This turned out to be a
successful strategy for the individual systems, but
slightly detrimental for the blended system.
3.6 Results
Semantic predicate results for WSJ and Brown can
be found in Table 2. Table 4 shows the results for
identification and classification of arguments.
4 Analysis and Conclusions
In general, the mixed and blended system performs
well on all tasks, rendering a sixth place in the
CoNLL 2008 shared task. The overall scores for
the submitted system can be seen in Table 3.
4.1 Parsing
For the blended parsing system, the labelled at-
tachment score drops from 87.36 for the WSJ test
set to 80.77 for the Brown test set, while the unla-
belled attachment score only drops from 89.88 to
86.28. This shows that the system is robust with
regards to the overall syntactic structure, even if
picking the correct label is more difficult for the
out-of-domain text.
The parser has difficulties finding the right head
for punctuation and symbols. Apart from errors re-
WSJ + Brown WSJ Brown
Syn + Sem 79.79 80.92 70.49
Syn 86.63 87.36 80.77
Sem 72.94 74.47 60.18
Table 3: Syntactic and semantic scores on the test
sets for the submitted system. The scores, from top
to bottom, are labelled macro F
1
, labelled attach-
ment score and labelled F
1
.
garding punctuation, most errors occur for IN and
TO. A majority of these problems are related to as-
signing the correct dependency. This is not surpris-
ing, since these are categories that focus on form
rather than function.
There is no significant difference in score for left
and right dependencies, presumably because of the
bi-directional parsing. However, the system over-
predicts dependencies to the root. This is mainly
due to the way MaltParser handles tokens not be-
ing attached anywhere during parsing. These to-
kens are by default assigned to the root.
4.2 SRL
Similarly to the parsing results, the blended SRL
system is less robust with respect to labelled F
1
-
score, dropping from 74.47 on the WSJ test set to
60.18 on the Brown test set. The corresponding
drop in unlabelled F
1
-score is from 82.90 to 75.49.
The simple method of picking the most com-
mon sense from the training data works quite well,
but the difference in domain makes it more diffi-
cult to find the correct sense for the Brown corpus.
In the future, a predicate classification module is
needed. For the WSJ corpus, assigning the most
common predicate sense works better with nomi-
nal than with verbal predicates, while verbal pred-
icates are handled better for the Brown corpus.
In general, verbal predicate-argument structures
are handled better than nominal ones, for both
test sets. This is not surprising, since nominal
predicate-argument structures tend to vary more in
their composition.
Since we do not use global constraints for the
argument labelling (looking at the whole argument
structure for each predicate), the system can out-
put the same argument label for a predicate several
times. For the WSJ test set, for instance, the ra-
tio of repeated argument labels is 5.4% in the sys-
tem output, compared to 0.3% in the gold standard.
However, since there are no confidence scores for
predictions it is difficult to handle this in the cur-
rent system.
251
PPOSS(pred) + ARG WSJ F
1
Brown F
1
NN* + A0 61.42 38.99
NN* + A1 67.07 53.10
NN* + A2 57.02 26.19
NN* + A3 63.08 (16.67)
NN* + AM-ADV 4.65 (-)
NN* + AM-EXT 44.78 (40.00)
NN* + AM-LOC 49.45 (-)
NN* + AM-MNR 53.51 21.82
NN* + AM-NEG 79.37 (46.15)
NN* + AM-TMP 67.23 (25.00)
VB* + A0 81.72 73.58
VB* + A1 81.77 67.99
VB* + A2 60.91 50.67
VB* + A3 61.49 (14.28)
VB* + A4 77.84 (40.00)
VB* + AM-ADV 47.49 30.33
VB* + AM-CAU 55.12 (35.29)
VB* + AM-DIR 41.86 37.14
VB* + AM-DIS 71.91 37.04
VB* + AM-EXT 60.38 (-)
VB* + AM-LOC 55.69 37.50
VB* + AM-MNR 49.54 36.25
VB* + AM-MOD 94.85 82.42
VB* + AM-NEG 93.45 77.08
VB* + AM-PNC 50.00 (62.50)
VB* + AM-TMP 69.59 49.07
VB* + C-A1 70.76 55.32
VB* + R-A0 83.68 70.83
VB* + R-A1 68.87 51.43
VB* + R-AM-LOC 38.46 (25.00)
VB* + R-AM-TMP 56.82 (58.82)
Table 4: Semantic argument results on the two
test sets, showing arguments with more than 20
instances in the gold test set (fewer instances for
Brown are given in parentheses).
Acknowledgements
This project was carried out within the course Ma-
chine Learning 2, organized by GSLT (Swedish
National Graduate School of Language Tech-
nology), with additional support from NGSLT
(Nordic Graduate School of Language Technol-
ogy). We thank our supervisors Joakim Nivre,
Bj?orn Gamb?ack and Pierre Nugues for advice and
support. Computations were performed on the
BalticGrid and UPPMAX (projects p2005008 and
p2005028) resources. We thank Tore Sundqvist at
UPPMAX for technical assistance.
References
Chang, Chih-Chung and Chih-Jen Lin, 2001. LIBSVM:
A library for support vector machines.
Chu, Y. J. and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
39th Annual Association for Computing Machinery
Southeast Conference, Athens, Georgia.
Edmonds, Jack. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233?240.
Gablonsky, J?org M. 2001. Modifications of the DI-
RECT algorithm. Ph.D. thesis, North Carolina State
University, Raleigh, North Carolina.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Hall, Johan, Jens Nilsson, Joakim Nivre, G?uls?en
Eryi?git, Be?ata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, Prague, Czech Republic.
Koomen, Peter, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), Ann Arbor,
Michigan.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, Prague, Czech Republic.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
2(13):95?135.
Nivre, Joakim. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the ACL?04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, Barcelona, Spain.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), Manchester, Great
Britain.
Witten, Ian H. and Eibe Frank. 2005. Data mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, Amsterdam, 2nd edition.
252
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 84?91,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Uncertainty Detection as Approximate Max-Margin Sequence Labelling
Oscar Ta?ckstro?m
SICS / Uppsala University
Kista / Uppsala, Sweden
oscar@sics.se
Gunnar Eriksson
SICS
Kista, Sweden
guer@sics.se
Sumithra Velupillai
DSV, Stockholm University
Kista, Sweden
sumithra@dsv.su.se
Hercules Dalianis
DSV, Stockholm University
Kista, Sweden
hercules@dsv.su.se
Martin Hassel
DSV, Stockholm University
Kista, Sweden
xmartin@dsv.su.se
Jussi Karlgren
SICS
Kista, Sweden
jussi@sics.se
Abstract
This paper reports experiments for the
CoNLL-2010 shared task on learning to
detect hedges and their scope in natu-
ral language text. We have addressed
the experimental tasks as supervised lin-
ear maximum margin prediction prob-
lems. For sentence level hedge detection
in the biological domain we use an L1-
regularised binary support vector machine,
while for sentence level weasel detection
in the Wikipedia domain, we use an L2-
regularised approach. We model the in-
sentence uncertainty cue and scope de-
tection task as an L2-regularised approxi-
mate maximum margin sequence labelling
problem, using the BIO-encoding. In ad-
dition to surface level features, we use a
variety of linguistic features based on a
functional dependency analysis. A greedy
forward selection strategy is used in ex-
ploring the large set of potential features.
Our official results for Task 1 for the bio-
logical domain are 85.2 F1-score, for the
Wikipedia set 55.4 F1-score. For Task 2,
our official results are 2.1 for the entire
task with a score of 62.5 for cue detec-
tion. After resolving errors and final bugs,
our final results are for Task 1, biologi-
cal: 86.0, Wikipedia: 58.2; Task 2, scopes:
39.6 and cues: 78.5.
1 Introduction
This paper reports experiments to detect uncer-
tainty in text. The experiments are part of the two
shared tasks given by CoNLL-2010 (Farkas et al,
2010). The first task is to identify uncertain sen-
tences; the second task is to detect the cue phrase
which makes the sentence uncertain and to mark
its scope or span in the sentence.
Uncertainty as a target category needs to be ad-
dressed with some care. Sentences, utterances,
statements are not uncertain ? their producer, the
speaker or author, is. Statements may explicitly
indicate this uncertainty, employing several differ-
ent linguistic and textual mechanisms to encode
the speaker?s attitude with respect to the verac-
ity of an utterance. The absence of such markers
does not necessarily indicate certainty ? the oppo-
sition between certain and uncertain is not clearly
demarkable, but more of a dimensional measure.
Uncertainty on the part of the speaker may be dif-
ficult to differentiate from a certain assessment of
an uncertain situation, It is unclear whether this
specimen is an X or a Y vs. The difference between
X and Y is unclear.
In this task, the basis for identifying uncertainty
in utterances is almost entirely lexical. Hedges,
the main target of this experiment, are an estab-
lished category in lexical grammar analyses - see
e.g. Quirk et al (1985), for examples of English
language constructions. Most languages use vari-
ous verbal markers or modifiers for indicating the
speaker?s beliefs in what is being said, most proto-
typically using conditional or optative verb forms,
Six Parisiens seraient morts, or auxiliaries, This
mushroom may be edible, but aspectual markers
may also be recruited for this purpose, more indi-
rectly, I?m hoping you will help vs. I hope you will
help; Do you want to see me now vs. Did you want
to see me now. Besides verbs, there are classes
of terms that through their presence, typically in
an adverbial role, in an utterance make explicit
its tentativeness: possibly, perhaps... and more
complex constructions with some reservation, es-
pecially such that explicitly mention the speaker
and the speaker?s beliefs or doubts, I suspect that
X.
Weasels, the other target of this experiment,
on the other hand, do not indicate uncertainty.
84
Weasels are employed when speakers attempt to
convince the listener of something they most likely
are certain of themselves, by anchoring the truth-
fulness of the utterance to some outside fact or au-
thority (Most linguists believe in the existence of
an autonomous linguistic processing component),
but where the authority in question is so unspecific
as not to be verifiable when scrutinised.
We address both CoNLL-2010 shared tasks
(Farkas et al, 2010). The first, detecting uncer-
tain information on a sentence level, we solve by
using an L1-regularised support vector machine
with hinge loss for the biological domain, and
an L2-regularised maximum margin model for the
Wikipedia domain. The second task, resolution of
in-sentence scopes of hedge cues, we approach as
an approximate L2-regularized maximum margin
structured prediction problem. Our official results
for Task 1 for the biological domain are 85.2 F1-
score, for the Wikipedia set 55.4 F1-score. For
Task 2, our official results were 2.1 for the entire
task with a score of 62.5 for cue detection. After
resolving errors and unfortunate bugs, our final re-
sults are for Task 1, biological: 86.0, Wikipedia:
58.2; Task 2: 39.6 and 78.5 for cues.
2 Detecting Sentence Level Uncertainty
On the sentence level, word- and lemma-based
features have been shown to be useful for uncer-
tainty detection (see e.g. Light et al (2004), Med-
lock and Briscoe (2007), Medlock (2008), and
Szarvas (2008)). Medlock (2008) and Szarvas
(2008) employ probabilistic, weakly supervised
methods, where in the former, a stemmed single
term and bigram representation achieved best re-
sults (0.82 BEP), and in the latter, a more complex
n-gram feature selection procedure was applied
using a Maximum Entropy classifier, achieving
best results when adding reliable keywords from
an external hedge keyword dictionary (0.85 BEP,
85.08 F1-score on biomedical articles). More lin-
guistically motivated features are used by Kil-
icoglu and Bergler (2008), such as negated ?un-
hedging? verbs and nouns and that preceded by
epistemic verbs and nouns. On the fruit-fly dataset
(Medlock and Briscoe, 2007) they achieve 0.85
BEP, and on the BMC dataset (Szarvas, 2008) they
achieve 0.82 BEP. Light et al (2004) also found
that most of the uncertain sentences appeared to-
wards the end of the abstract, indicating that the
position of an uncertain sentence might be a use-
ful feature.
Ganter and Strube (2009) consider weasel tags
in Wikipedia articles as hedge cues, and achieve
results of 0.70 BEP using word- and distance
based features on a test set automatically derived
from Wikipedia, and 0.69 BEP on a manually an-
notated test set using syntactic patterns as fea-
tures. These results suggest that syntactic features
are useful for identifying weasels that ought to be
tagged. However, evaluation is performed on bal-
anced test sets, which gives a higher baseline.
2.1 Learning and Optimization Framework
A guiding principle in our approach to this shared
task has been to focus on highly computationally
efficient models, both in terms of training and pre-
diction times. Although kernel based non-linear
separators may sometimes obtain better predic-
tion performance, compared to linear models, the
speed penalty at prediction time is often substan-
tial, since the number of support patterns often
grows linearly with the size of the training set. We
therefore restrict ourselves to linear models, but
allow for a restricted family of explicit non-linear
mappings by feature combinations.
For sentence level hedge detection in the bio-
logical domain, we employ an L1-regularised sup-
port vector machine with hinge loss, as provided
by the library implemented by Fan et al (2008),
while for weasel detection in the Wikipedia do-
main, we instead use the L2-regularised maximum
margin model described in more detail in section
3.1. In both cases, we approximately optimise the
F1-measure by weighting each class by the inverse
of its proportion in the training data.
The reason for using L1-regularisation in the bi-
ological domain is that the annotation is heavily
biased towards a rather small number of lexical
cues, making most of the potential surface features
irrelevant. The Wikipedia weasel annotation, on
the other hand, is much more noisy and less de-
termined by specific lexical markers. Regularising
with respect to the L1-norm is known to give pref-
erence to sparse models and for the special case
of logistic regression, Ng (2004) proved that the
sample complexity grows only logarithmically in
the number of irrelevant features, instead of lin-
early as when regularising with respect to the L2-
norm. Our preliminary experiments indicated that
L1-regularisation is superior to L2-regularisation
in the biological domain, while slightly inferior in
85
the Wikipedia domain.
2.2 Feature Definitions
The asymmetric relationship between certain and
uncertain sentences becomes evident when one
tries to learn this distinction based on surface level
cues. While the UNCERTAIN category is to a large
extent explicitly anchored in lexical markers, the
CERTAIN category is more or less defined implic-
itly as the complement of the UNCERTAIN cate-
gory. To handle this situation, we use a bias fea-
ture to model the weight of the CERTAIN category,
while explicit features are used to model the UN-
CERTAIN category.
The following list describes the feature tem-
plates explored for sentence level uncertainty de-
tection. Some features are based on a linguistic
analysis by the Connexor Functional Dependency
(FDG) parser (Tapanainen and Ja?rvinen, 1997).
SENLEN Preliminary experiments indicated that taking sen-
tence length into account is beneficial. We incorporate
this by using three different bias terms, according to the
length (in tokens) of the sentences. This feature takes
the following values: S < 18 ? M ? 32 < L.
DOCPT Document part, e.g., TITLE, ABSTRACT and BODY
TEXT, allowing for different models for different docu-
ment parts.
TOKEN, LEMMA Tokens in most cases equals words, but
may in some special cases also be multiword units, e.g.
of course, as defined by the FDG tokenisation. Lemmas
are base forms of words, with some special features
introduced for numeric tokens, e.g., year, short number,
and long number.
QUANT Syntactic function of a noun phrase with a quanti-
fier head (at least some of the isoforms are conserved
between mouse and humans), or a modifying quantifier
(Recently, many investigators have been interested in
the study on eosinophil biology).
HEAD, DEPREL Functional dependency head of the token,
and the type of dependency relation between the head
and the token, respectively.
SYN Phrase-level and clause-level syntactic functions of a
word.
MORPH Part-of-speech and morphological traits of a word.
Each feature template defines a set of features
when applied to data. The TOKEN, LEMMA,
QUANT, HEAD, DEPREL templates yield single-
ton sets of features for each token, while the SYN
and MORPH templates extends to sets consisting
of several features for each token. A sentence is
represented as the union of all active token level
features and the SENLEN and DOCPT, if active.
In addition to the linear combination of concrete
features, we allow combined features by the Carte-
sian product of the feature set extensions of two or
more feature templates.
2.3 Feature Template Selection
Although regularised maximum margin models
often cope well even in the presence of irrelevant
features, it is a good idea to search the large set of
potential features for an optimal subset.
In order to make this search feasible we make
two simplifications. First, we do not explore the
full set of individual features, but instead the set of
feature templates, as defined above. Second, we
perform a greedy search in which we iteratively
add the feature template that gives the largest per-
formance improvement, when added to the cur-
rent optimal set of templates. The performance of
a feature set for sentence level detection is mea-
sured as the mean F1-score, with respect to the
UNCERTAIN class, minus one standard deviation
? the mean and standard deviation are computed
by three fold cross-validation on the training set.
We subtract one standard deviation from the mean
in order to promote stable solutions over unstable
ones.
Of course, these simplifications do not come for
free. The solution of the optimisation problem
might be quite unstable with respect to the optimal
hyper-parameters of the learning algorithm, which
in turn may depend on the feature set used. This
risk could be reduced by conducting a more thor-
ough parameter search for each candidate feature
set, however, this was simply too time consuming
for the present work. A further risk of using for-
ward selection is that feature interactions are ig-
nored. This issue is handled better with backward
elimination, but that is also more time consuming.
The full set of explored feature templates is too
large to be listed here; instead we list the features
selected in each iteration of the search, together
with their corresponding scores, in Table 1.
3 Detecting In-sentence Uncertainty
When it comes to the automatic identification of
hedge cues and their linguistic scopes, Morante
and Daelemans (2009) and O?zgu?r and Radev
(2009) report experiments on the BioScope cor-
pus (Vincze et al, 2008), achieving best results
(10-fold cross evaluation) on the identification of
hedge cues of 71.59 F-score (using IGTree with
current, preceding and subsequent word and cur-
86
Task Template set Dev F1 Test F1
Bio
SENLEN - -
? LEMMA 88.9 (.25) 78.79
? LEMMABI 90.3 (.19) 85.86
? LEMMA?QUANT 90.3 (.07) 85.97
Wiki
SENLEN - -
? TOKEN?DOCPT 59.0 (.76) 60.12
? TOKENBI?SENLEN 59.9 (.09) 58.26
Table 1: Top feature templates for sentence level
hedge and weasel detection.
rent lemma as features) and 82.82 F-score (using a
Support Vector Machine classifier and a complex
feature set including keyword and dependency re-
lation information), respectively. On the task of
automatic scope resolution, best results are re-
ported as 59.66 (F-score) and 61.13 (accuracy),
respectively, on the full paper subset. O?zgu?r and
Radev (2009) use a rule-based method for this sub-
task, while Morante and Daelemans (2009) use
three different classifiers as input to a CRF-based
meta-learner, with a complex set of features, in-
cluding hedge cue information, current and sur-
rounding token information, distance information
and location information.
3.1 Learning and Optimisation Framework
In recent years, a wide range of different ap-
proaches to general structured prediction prob-
lems, of which sequence labelling is a special
case, have been suggested. Among others, Con-
ditional Random Fields (Lafferty et al, 2001),
Max-Margin Markov Networks (Taskar et al,
2003), and Structured Support Vector Machines
(Tsochantaridis et al, 2005). A drawback of
these approaches is that they are all quite com-
putationally demanding. As an alternative, we
propose a much more computationally lenient ap-
proach based on the regularised margin-rescaling
formulation of Taskar et al (2003), which we in-
stead optimise by stochastic subgradient descent
as suggested by Ratliff et al (2007). In addi-
tion we only perform approximate decoding, us-
ing beam search, which allows arbitrary complex
joint feature maps to be employed, without sacri-
ficing speed.
3.1.1 Technical Details
Let X denote the pattern set and let Y denote the
set of structured labels. Let A denote the set of
atomic labels and let each label y ? Y consist of
an indexed sequence of atomic labels yi ? A. De-
note by Yx ? Y the set of possible label assign-
ments to pattern x ? X and by yx ? Yx its cor-
rect label. In the specific case of BIO-sequence
labelling, A = {BEGIN, INSIDE, OUTSIDE} and
Yx = A|x|, where |x| is the length of the sequence
x ? X .
A structured classification problem amounts
to learning a mapping from patterns to labels,
f : X 7? Y , such that the expected loss
EX?Y [?(yx, f(x))] is minimised. The prediction
loss, ? : Y ? Y 7? <+, measures the loss of
predicting label y = f(x) when the correct la-
bel is yx, with ?(yx, yx) = 0. Here we assume
the Hamming loss, ?H(y, y?) = ?|y|i=1 ?(yi, y?i),
where ?(yi, y?i) = 1 if yi 6= y?i and 0 otherwise.
The idea of the margin-rescaling approach is to
let the structured margin between the correct label
yx and a hypothesis y ? Yx scale linearly with the
prediction loss ?(yx, y) (Taskar et al, 2003). The
structured margin is defined in terms of a score
function S : X ? Y 7? <, in our case the linear
score function S(x, y) = wT?(x, y), where w ?
<m is a vector of parameters and? : X?Y 7? <m
is a joint feature function. The learning problem
then amounts to finding parameters w such that
S(x, yx) ? S(x, y) + ?(yx, y) for all y ? Yx \
{yx} over the training data D. In other words, we
want the score of the correct label to be higher than
the score plus the loss, of all other labels, for each
instance. In order to balance margin maximisation
and margin violation, we add theL2-regularisation
term ?w?2.
By making use of the loss augmented decoding
function
f?(x, yx) = argmax
y?Yx
[S(x, y) + ?(yx, y)] , (1)
we get the following regularised risk functional:
Q?,D(w) =
|D|?
i=1
S?(x(i), yx(i)) + ?2 ?w?
2, (2)
where
S?(x, yx) = maxy?Yx [S(x, y) + ?(yx, y)]?S(x, yx)
(3)
We optimise (2) by stochastic approximate subgra-
dient descent with step size sequence [?0/?t]?t=1
(Ratliff et al, 2007). The initial step size ?0
and the regularisation factor ? are data depen-
dent hyper-parameters, which we tune by cross-
validation.
87
This framework is highly efficient both at learn-
ing and prediction time. Training cues and scopes
on the biological data, takes about a minute, while
prediction times are in the order of seconds, using
a Java based implementation on a standard laptop;
the absolute majority of that time is spent on read-
ing and extracting features from an inefficient in-
ternal JSON-based format.
3.1.2 Hashed Feature Functions
Joint feature functions enable encoding of depen-
dencies between labels and relations between pat-
tern and label. Most feature templates are de-
fined based on input only, while some are de-
fined with respect to output features as well. Let
?(x, y1:i?1, i) ? <m denote the joint feature func-
tion corresponding to the application of all active
feature templates to pattern x ? X and partially
decoded label y1:i?1 ? Ai?1 when decoding at
position i. The feature mapping used in scoring
candidate label yi ? A is then computed as the
Cartesian product ?(x, y, i) = ?(x, y1:i?1, i) ?
?(yi), where ?(yi) ? <m is a unique unitary fea-
ture vector representation of label yi. The feature
representation for a complete sequence x and its
associated label y is then computed as
?(x, y) =
|x|?
i=1
?(x, y, i)
When employing joint feature functions and com-
bined features, the number of unique features may
grow very large. This is a problem when the
amount of internal memory is limited. Feature
hashing, as described by Weinberger et al (2009),
is a simple trick to circumvent this problem. As-
sume that we have an original feature function
? : X ? Y 7? <m, where m might be arbitrar-
ily large. Let h : N+ 7? [1, n] be a hash function
and let h?1(i) ? [1,m] be the set of integers such
that j ? h?1(i) iff h(j) = i. We now use this
hash function to map the index of each feature in
?(x, y) to its corresponding index in ?(x, y), as
?i(x, y) =?j?h?1(i) ?j(x, y). The features in ?
are thus unions of multisets of features in ?. Given
a hash function with good collision properties, we
can expect that the subset of features mapped to
any index in?(x, y) is small and composed of ele-
ments drawn at random from ?(x, y). Weinberger
et al (2009) contains proofs of bounds on these
distributions. Furthermore, by using a k-valued
hash function h : Nk 7? [1, n], the Cartesian prod-
uct of k feature sets can be computed much more
efficiently, compared to using a dictionary.
3.2 Position Based Feature Definitions
For in-sentence cue and scope prediction we make
use of the same token level feature templates as
for sentence level detection. An additional level
of expressivity is added in that each token level
template is associated with a token position. A
template is addressed either relative to the token
currently being decoded, or by the dependency arc
of a token, which in turn is addressed by a relative
position. The addressing can be either to a single
position, or a range of positions. Feature templates
may further be defined with respect to features of
the input pattern, the token level labels predicted
so far, or with respect to combinations of input
and label features. Joint features, just as complex
feature combinations, are created by forming the
Cartesian product of an input feature set and a la-
bel feature set.
The feature templates are instantiated by pre-
fixing the template name to each member of the
feature set. To exemplify, the single position tem-
plate TOKENi, given that the token currently be-
ing decoded at position i is suggests, is instanti-
ated as the singleton set {TOKENi = suggests}.
The range template TOKENi,i+1, given that the
current token is suggests and the next token is
that, is instantiated as the set {TOKENi,i+1 =
suggests, TOKENi,i+1 = that}; i.e. each member
of the set is prefixed by the range template name.
In addition to the token level templates used for
sentence level prediction, the following templates
were explored:
LABEL Label predicted so far at the addressed position(s).
HEAD.X An arbitrary feature, X, addressed by follow-
ing the dependency arc(s) from the addressed posi-
tion(s). For example, HEAD.LEMMAi corresponds to
the lemma found by looking at the dependency head of
the current token.
CUE, CUESCOPE Whether the token(s) addressed is re-
spectively, a cue marker, or within the syntactic scope
of the current cue, following the definition of scope
provided by Vincze et al (2008).
3.3 Feature Template Selection
Just as with sentence level detection, we used a
greedy forward selection strategy when searching
for the optimal subset of feature templates. The
cue and scope detection subtasks were optimised
separately.
88
The scoring measures used in the search for
cue and scope detection features differ. In order
to match the official scoring measure for cue de-
tection, we optimise the F1-score of labels cor-
responding to cue tags, i.e. we treat the BEGIN
and INSIDE cue tags as an equivalence class. The
official scoring measure for scope prediction, on
the other hand, corresponds to the exact match
of scope boundaries. Unfortunately using exact
match performance turned out to be not very well
suited for use in greedy forward selection. This
is because before a sufficient per token accuracy
has been reached, and even when it has, the ex-
act match score may fluctuate wildly. Therefore,
as a substitute, we instead guide the search by to-
ken level accuracy. This discrepancy between the
search criterion and the official scoring metric is
unfortunate.
Again, when taking into account position ad-
dressing, joint features and combined features, the
complete set of explored templates is too large to
fit in the current experiment. The selected features
together with their corresponding scores are found
in Table 2.
Task Template set Dev F1 Test F1
Cue
TOKENi 74.0 (1.5) -
? TOKENi?1 81.0 (.30) 68.78
? MORPHi 83.6 (.10) 74.06
? LEMMAi ? LEMMAi+1 85.6 (.20) 78.41
? SYNi 86.5 (.41) 78.28
? LEMMAi?1 ? LEMMAi 86.7 (.42) 78.52
Scope
CueScopei 66.9 (.92) -
? LABELi?2,i?1 79.5 (.67) 34.80
? LEMMAi 82.4 (1.1) 33.18
? MORPHi 83.1 (.35) 35.70
? CUEi?2,i?1 83.4 (.13) 40.14
? CUEi,i+1,i+2 83.6 (.11) 41.15
? LEMMAi?1 84.1 (.16) 40.04
? MORPHi 84.4 (.33) 40.04
? TOKENi+1 84.5 (.09) 39.64
Table 2: Top feature templates for in-sentence de-
tection of hedge cues and scopes.
4 Discussion
Our final F1-score results for the corrected system
are, in Task 1 for the biological domain 85.97, for
the Wikipedia domain 58.25; for Task 2, our re-
sults are 39.64 for the entire task with a score of
78.52 for cue detection.
Any gold standard-based shared experiment un-
avoidably invites discussion on the reliability of
the gold standard. It is easy to find borderline ex-
amples in the evaluation corpus, e.g. sentences
that may just as well be labeled ?certain? rather
than ?uncertain?. This gives an indication of the
true complexity of assessing the hidden variable of
uncertainty and coercing it to a binary judgment
rather than a dimensional one. It is unlikely that
everyone will agree on a binary judgment every
time.
To improve experimental results and the gen-
eralisability of the results for the task of detect-
ing uncertain information on a sentence level, we
would need to break reliance on the purely lexical
cues. For instance, we now have identified possi-
ble and putative as markers for uncertainty, but in
many instances they are not (Finally, we wish to
ensure that others can use and evaluate the GREC
as simply as possible). This would be avoidable
through either a deeper analysis of the sentence
to note that possible in this case does not modify
anything of substance in the sentence, or alterna-
tively through a multi-word term preprocessor to
identify as simply as possible as an analysis unit.
In the Wikipedia experiment, where the objec-
tive is to identify weasel phrases, the judicious en-
coding of quantifiers such as ?some of the most
well-known researchers say that X? would be
likely to identify the sought-for sentences when
the quantified NP is in subject position. In our
experiment we find that our dependency analysis
did not distinguish between the various syntactic
roles of quantified NPs. As a result, we marked
several sentences with a quantifier as a ?weasel?
sentence, even where the quantified NP was in a
non-subject role ? leading to overly many weasel
sentences. An example is given in Table 3.
If certainty can be identified separately, not as
absence of overt uncertainty, identifying uncer-
tainty can potentially be aided through the iden-
tification of explicit certainty together with nega-
tion, as found by Kilicoglu and Bergler (2008). In
keeping with their results, we found negations in a
sizeable proportion of the annotated training mate-
rial. Currently we capture negation as a lexical cue
in immediate bigrams, but with longer range nega-
tions, we will miss some clear cases: Table 3 gives
two examples. To avoid these misses, we will both
need to identify overt expressions of certainty and
to identify and track the scope of negation ? the
first challenge is unexplored but would not seem
to be overly complex; the second is a well-known
89
and established challenge for NLP systems in gen-
eral.
In the task of detecting in-sentence uncertainty
? identification of hedge cues and their scopes ?
we find that an evaluation method based on ex-
act match of a token sequence is overly unforgiv-
ing. There are many cases where the marginal to-
kens of a sequence are less than central or irrele-
vant for the understanding of the hedge cue and its
scope: moving the boundary by one position over
an uninteresting token may completely invalidate
an otherwise arguably correct analysis. A token-
by-token scoring would be a more functional eval-
uation criterion, or perhaps a fuzzy match, allow-
ing for a certain amount of erroneous characters.
For our experiments, this has posed some chal-
lenges. While we model the in-sentence un-
certainty detection as a sequence labelling prob-
lem in the BIO-representation (BEGIN, INSIDE,
OUTSIDE), the provided corpus uses an XML-
representation. Moreover, the official scoring tool
requires that the predictions are well formed XML,
necessitating a conversion from XML to BIO prior
to training and from BIO to XML after prediction.
Consistent tokenisation is important, but the syn-
tactic analysis components used by us distorted the
original tokenisation and restoring the exact same
token sequence proved problematic.
Conversion from BIO to XML is straightforward
for cues, while some care must be taken when an-
notating scopes, since erroneous scope predictions
may result in malformed XML. When adding the
scope annotation, we use a stack based algorithm.
For each sentence, we simultaneously traverse the
scope-sequence corresponding to each cue, left to
right, token by token. The stack is used to en-
sure that scopes are either separated or nested and
an additional restriction ensures that scopes may
never start or end inside a cue. In case the al-
gorithm fails to place a scope according to these
restrictions, we fall back and let the scope cover
the whole sentence. Several of the more frequent
errors in our analyses are scoping errors, many
likely to do with the fallback solution. Our analy-
sis quite frequently fails also to assign the subject
of a sentence to the scope of a hedging verb. Ta-
ble 3 shows one example each of these errors ?
overextended scope and missing subject.
Unfortunately, the tokenisation output by our
analysis components is not always consistent with
the tokenisation assumed by the BioScope annota-
tion. A post-processing step was therefore added
in which each, possibly complex, token in the pre-
dicted BIO-sequence is heuristically mapped to its
corresponding position in the XML structure. This
post-processing is not perfect and scopes and cues
at non-word token boundaries, such as parenthe-
ses, are quite often misplaced with respect to the
BioScope annotation. Table 3 gives one example
which is scored ?erroneous? since the token ?(63)?
is in scope, where the ?correct? solution has it out-
side the scope. These errors are not important to
address, but are quite frequent in our results ? ap-
proximately 80 errors are of this type.
To achieve more general and effective methods
to detect uncertainty in an argument, we should
note that uncertainty is signalled in a text through
many mechanisms, and that the purely lexical and
explicit signal found through the present experi-
ments in hedge identification is effective and use-
ful, but will not catch everything we might want to
find. Lexical approaches are also domain depen-
dent. For instance, Szarvas (2008) and Morante
and Daelemans (2009) report loss in performance,
when applying the same methods developed on bi-
ological data, on clinical text. Using the systems
developed for scientific text elsewhere poses a mi-
gration challenge. It would be desirable both to
automatically learn a hedging lexicon from a gen-
eral seed set and to have features on a higher level
of abstraction.
Our main result is that casting this task as a se-
quence labelling problem affords us the possibility
to combine linguistic analyses with a highly effi-
cient implementation of a max-margin prediction
algorithm. Our framework processes the data sets
in minutes for training and seconds for prediction
on a standard personal computer.
5 Acknowledgements
The authors would like to thank Joakim Nivre
for feedback in earlier stages of this work. This
work was funded by The Swedish National Grad-
uate School of Language Technology and by the
Swedish Research Council.
References
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learn-
ing Research, 9:1871?1874.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
90
Neg + certain However, how IFN-? and IL-4 inhibit IL-17 production is not yet known.
Neg + certain The mechanism by which Tregs preserve peripheral tolerance is still not entirely clear.
?some?: not weasel Tourist folks usually visit this peaceful paradise to enjoy some leisurenonsubj .
?some?: weasel Somesubj suggest that the origin of music likely stems from naturally occurring sounds and rhythms.
Prediction dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR? because
dRas85DV12 can activate endogenous PI3K signaling [16]</xcope>.
Gold standard dRas85DV12 <xcope .1><cue .1>may</cue> be more potent than dEGFR?</xcope> because
dRas85DV12 can activate endogenous PI3K signaling [16].
Prediction However, the precise molecular mechanisms of Stat3-mediated expression of ROR?t
<xcope .1>are still <cue .1>unclear</cue></xcope>.
Gold standard However, <xcope .1>the precise molecular mechanisms of Stat3-mediated expression of ROR?t
are still <cue .1>unclear</cue></xcope>.
Prediction Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit ROR?t
activity on its target genes, at least in par,t through direct interaction with ROR?t (63)</xcope>.
Gold standard Interestingly, Foxp3 <xcope .1><cue .1>may</cue> inhibit RORt
activity on its target genes, at least in par,t through direct interaction with RORt</xcope> (63).
Table 3: Examples of erroneous analyses.
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their Scope
in Natural Language Text. In Proceedings of the 14th
Conference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 1?12, Uppsala,
Sweden, July. Association for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding hedges
by chasing weasels: hedge detection using Wikipedia tags
and shallow linguistic features. In ACL-IJCNLP ?09: Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Halil Kilicoglu and Sabine Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a linguis-
tically motivated perspective. BMC Bioinformatics, 9.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data. In
Proc. 18th Int. Conf. on Machine Learning. Morgan Kauf-
mann Publishers.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and state-
ments in between. In Lynette Hirschman and James
Pustejovsky, editors, HLT-NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, Ontologies
and Databases, Boston, USA. ACL.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature. In
Proceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Informatics,
41(4):636?654.
Roser Morante and Walter Daelemans. 2009. Learning the
scope of hedge cues in biomedical texts. In BioNLP ?09:
Proceedings of Workshop on BioNLP, Morristown, NJ,
USA. ACL.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regulariza-
tion, and rotational invariance. In ICML ?04: Proceedings
of the 21st International Conference on Machine learning,
page 78, New York, NY, USA. ACM.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detecting
speculations and their scopes in scientific text. In Pro-
ceedings of 2009 Conference on Empirical Methods in
Natural Language Processing, Singapore. ACL.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and
Jan Svartvik. 1985. A comprehensive grammar of the
English language. Longman.
Nathan D. Ratliff, Andrew J. Bagnell, and Martin A. Zinke-
vich. 2007. (Online) subgradient methods for structured
prediction. In Eleventh International Conference on Arti-
ficial Intelligence and Statistics (AIStats).
Gyo?rgy Szarvas. 2008. Hedge classification in biomedical
texts with a weakly supervised selection of keywords. In
Proceedings of ACL-08: HLT, Columbus, Ohio. ACL.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Conference
on Applied Natural Language Processing.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard Scho?lkopf, editors,
NIPS. MIT Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Jour-
nal of Machine Learning Research, 6:1453?1484.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas, Gyo?rgy
Mo?ra, and Ja?nos Csirik. 2008. The BioScope corpus:
biomedical texts annotated for uncertainty, negation and
their scopes. BMC Bioinformatics, 9(S-11).
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In ICML ?09: Proceedings
of the 26th Annual International Conference on Machine
Learning, New York, NY, USA. ACM.
91
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 55?63,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Nudging the Envelope of Direct Transfer Methods
for Multilingual Named Entity Recognition
Oscar Ta?ckstro?m
SICS / Uppsala University
Sweden
oscar@sics.se
Abstract
In this paper, we study direct transfer meth-
ods for multilingual named entity recognition.
Specifically, we extend the method recently
proposed by Ta?ckstro?m et al (2012), which is
based on cross-lingual word cluster features.
First, we show that by using multiple source
languages, combined with self-training for tar-
get language adaptation, we can achieve sig-
nificant improvements compared to using only
single source direct transfer. Second, we in-
vestigate how the direct transfer system fares
against a supervised target language system
and conclude that between 8,000 and 16,000
word tokens need to be annotated in each tar-
get language to match the best direct transfer
system. Finally, we show that we can signif-
icantly improve target language performance,
even after annotating up to 64,000 tokens in
the target language, by simply concatenating
source and target language annotations.
1 Introduction
Recognition of named entities in natural language
text is an important subtask of information extrac-
tion and thus bears importance for modern text min-
ing and information retrieval applications. The need
to identify named entities such as persons, loca-
tions, organizations and places, arises both in ap-
plications where the entities are first class objects of
interest, such as in Wikification of documents (Rati-
nov et al, 2011), and in applications where knowl-
edge of named entities is helpful in boosting perfor-
mance, e.g., machine translation (Babych and Hart-
ley, 2003) and question answering (Leidner et al,
2003). The advent of massive machine readable fac-
tual databases, such as Freebase1 and the proposed
1http://www.freebase.com
Wikidata2, will likely push the need for automatic
extraction tools further. While these databases store
information about entity types and the relationships
between those types, the named entity recognition
(NER) task concerns finding occurrences of named
entities in context. This view originated with the Mes-
sage Understanding Conferences (MUC) (Grishman
and Sundheim, 1996).
As with the majority of tasks in contemporary nat-
ural language processing, most approaches to NER
have been based on supervised machine learning.
However, although resources for a handful of lan-
guages have been created, through initiatives such
as MUC, the Multilingual Entity Task (Merchant
et al, 1996) and the CoNLL shared tasks (Tjong
Kim Sang, 2002; Tjong Kim Sang and De Meul-
der, 2003), coverage is still very limited in terms of
both domains and languages. With fine-grained en-
tity taxonomies such as that proposed by Sekine and
Nobata (2004), who define over two hundred cate-
gories, we can expect an increase in the amount of
annotated data required for acceptable performance,
as well as an increased annotation cost for each entity
occurrence. Although semi-supervised approaches
have been shown to reduce the need for manual an-
notation (Freitag, 2004; Miller et al, 2004; Ando
and Zhang, 2005; Suzuki and Isozaki, 2008; Lin and
Wu, 2009; Turian et al, 2010; Dhillon et al, 2011;
Ta?ckstro?m et al, 2012), these methods still require a
substantial amount of manual annotation for each tar-
get language. Manually creating a sufficient amount
of annotated resources for all entity types in all lan-
guages thus seems like an Herculean task.
In this study, we turn to direct transfer methods
(McDonald et al, 2011; Ta?ckstro?m et al, 2012) as
2http://meta.wikimedia.org/wiki/Wikidata
55
a way to combat the need for annotated resources
in all languages. These methods allow one to train
a system for a target language, using only annota-
tions in some source language, as long as all source
language features also have support in the target lan-
guages. Specifically, we extend the direct transfer
method proposed by Ta?ckstro?m et al (2012) in two
ways. First, in ?3, we use multiple source languages
for training. We then propose a self-training algo-
rithm, which allows for the inclusion of additional
target language specific features, in ?4. By com-
bining these extensions, we achieve significant error
reductions on all tested languages. Finally, in ?5,
we assess the viability of the different direct transfer
systems compared to a supervised system trained on
target language annotations, and conclude that direct
transfer methods may be useful even in this scenario.
2 Direct Transfer for Cross-lingual NER
Rather than starting from scratch when creating sys-
tems that predict linguistic structure in one language,
we should be able to take advantage of any cor-
responding annotations that are available in other
languages. This idea is at the heart of both direct
transfer methods (McDonald et al, 2011; Ta?ckstro?m
et al, 2012) and of annotation projection methods
(Yarowsky et al, 2001; Diab and Resnik, 2002; Hwa
et al, 2005). While the aim of the latter is to transfer
annotations across languages, direct transfer meth-
ods instead aim to transfer systems, trained on some
source language, directly to other languages. In this
paper, we focus on direct transfer methods, however,
we briefly discuss the relationship between these ap-
proaches in ?6.
Considering the substantial differences between
languages at the grammatical and lexical level, the
prospect of directly applying a system trained on
one language to another language may seem bleak.
However, McDonald et al (2011) showed that a lan-
guage independent dependency parser can indeed be
created by training on a delexicalized treebank and
by only incorporating features defined on universal
part-of-speech tags (Das and Petrov, 2011).
Recently, Ta?ckstro?m et al (2012) developed an al-
gorithm for inducing cross-lingual word clusters and
proposed to use these clusters to enrich the feature
space of direct transfer systems. The richer set of
cross-lingual features was shown to substantially im-
prove on direct transfer of both dependency parsing
and NER from English to other languages.
Cross-lingual word clusters are clusterings of
words in two (or more) languages, such that the clus-
ters are adequate in each language and at the same
time consistent across languages. For cross-lingual
word clusters to be useful in direct transfer of lin-
guistic structure, the clusters should capture cross-
lingual properties on both the semantic and syntac-
tic level. Ta?ckstro?m et al (2012) showed that this
is, at least to some degree, achievable by coupling
monolingual class-based language models, via word
alignments. The basic building block is the follow-
ing simple monolingual class-based language model
(Saul and Pereira, 1997; Uszkoreit and Brants, 2008):
L(w; C) =
m?
i=1
p(wi|C(wi))p(C(wi)|wi?1) ,
where L(w; C) is the likelihood of a sequence of
words, w, and C is a (hard) clustering function, which
maps words to cluster identities. These monolingual
models are coupled through word alignments, which
constrains the clusterings to be consistent across lan-
guages, and optimized by approximately maximizing
the joint likelihood across languages. Just as monolin-
gual word clusters are broadly applicable as features
in monolingual models for linguistic structure predic-
tion (Turian et al, 2010), the resulting cross-lingual
word clusters can be used as features in various cross-
lingual direct transfer models. We believe that the
extensions that we propose are likely to be useful for
other tasks as well, e.g., direct transfer dependency
parsing, in this paper, we focus solely on discrimina-
tive direct transfer models for NER.
3 Multi-source Direct Transfer
Learning from multiple languages have been shown
to be of benefit both in unsupervised learning of syn-
tax and part-of-speech (Snyder et al, 2009; Berg-
Kirkpatrick and Klein, 2010) and in transfer learning
of dependency syntax (Cohen et al, 2011; McDonald
et al, 2011). Here we perform a set of experiments
where we investigate the potential of multi-source
transfer for NER, in German (DE), English (EN),
Spanish (ES) and Dutch (NL), using cross-lingual
word clusters. For all experiments, we use the same
56
Source DE ES NL
EN 39.7 62.0 63.7
EN + DE ? 61.8 65.5
EN + ES 39.3 ? 65.6
EN + NL 41.0 62.5 ?
ALL 41.0 63.6 66.4
? DEVELOPMENT SET ? TEST SET
EN 37.8 59.1 57.2
EN + DE ? 59.4 57.9
EN + ES 35.9 ? 59.1
EN + NL 38.1 59.7 ?
ALL 36.4 61.9 59.9
Table 1: Results of multi-source direct transfer, measured
with F1-score on the CoNLL 2002/2003 development and
test sets. ALL: all languages except the target language
are used as source languages.
256 cross-lingual word clusters and the same feature
templates as Ta?ckstro?m et al (2012), with the ex-
ception that the transition factors are not conditioned
on the input.3 The features used are similar to those
used by Turian et al (2010), but include cross-lingual
rather than monolingual word clusters. We remove
the capitalization features when transferring to Ger-
man, but keep them in all other cases, even when Ger-
man is included in the set of source languages. We
use the training, development and test data sets pro-
vided by the CoNLL 2002/2003 shared tasks (Tjong
Kim Sang, 2002; Tjong Kim Sang and De Meul-
der, 2003). The multi-source training sets are cre-
ated by concatenating each of the source languages?
training sets. In order to have equivalent label sets
across languages, we use the IO (inside/outside) en-
coding, rather than the BIO (begin/inside/outside) en-
coding, since the latter is available only for Spanish
and Dutch. The models are trained using CRFSuite
0.12 (Okazaki, 2007), by running stochastic gradient
descent for a maximum of 100 iterations.
Table 1 shows the result of using different source
languages for different target languages. We see that
multi-source transfer is somewhat helpful in general,
but that the results are sensitive to the combination
of source and target languages. On average, using all
source languages only give a relative error reduction
of about 3% on the test set. However, results for
3This is due to limitations in the sequence labeling software
used and gives slightly lower results, across the board, than those
reported by Ta?ckstro?m et al (2012).
DE ES NL AVG
NATIVE CLUSTERS 71.2 80.7 82.5 78.1
X-LING CLUSTERS 68.9 78.8 80.9 76.2
NATIVE & X-LING CLUST. 72.5 81.2 83.6 79.1
? DEVELOPMENT SET ? TEST SET
NATIVE CLUSTERS 72.2 81.0 83.0 78.7
X-LING CLUSTERS 71.0 80.2 80.7 77.3
NATIVE & X-LING CLUST. 73.5 81.8 83.7 79.7
Table 2: The impact of different word clusters in the
supervised monolingual setting. Results are measured
with F1-score on the CoNLL 2002/2003 development
and test sets. NATIVE/X-LING CLUSTERS: The cross-
lingual/monolingual clusters from Ta?ckstro?m et al (2012).
Spanish and Dutch are more promising, with relative
reductions of 7% and 6%, respectively, when using
all source languages. Using all available source lan-
guages gives the best results for both Spanish and
Dutch, but slightly worse results for German. When
transferring to Dutch, using more source languages
consistently help, while Spanish and German are
more sensitive to the choice of source languages.
Based on the characteristics of these languages, this
is not too surprising: while Dutch and German has
the most similar vocabularies, Dutch uses similar cap-
italization rules to English and Spanish. Dutch should
thus benefit from all the other languages, while Span-
ish may not bring much to the table for German and
vice versa, given their lexical differences. Knowl-
edge of such relationships between the languages,
could potentially be used to give different weights to
different source languages in the training objective,
as was shown effective by Cohen et al (2011) in the
context of direct transfer of generative dependency
parsing models. Although better results could be
achieved by cherry-picking language combinations,
since we do not have any general principled way of
choosing/weighting source languages in discrimina-
tive models, we include all source languages with
equal weight in all subsequent experiments where
multiple source languages are used.
4 Domain Adaptation via Self-Training
Thus far, we have not made use of any information
specific to the target language, except when inducing
the cross-lingual word clusters. However, as shown
in Table 2, which lists the results of experiments on
57
Algorithm 1 Self-Training for Domain Adaptation
Dls: Labeled source domain data
Dlt: Labeled target domain data (possibly empty)
Dut : Unlabeled target domain data
?: Dominance threshold
T : Number of iterations
procedure SELFTRAIN(Dls,Dlt,Dut , ?, T )
?0 ? LEARN(Dls ? Dlt) . Train supervised model
for i? 1 to T do
P i ? PREDICT(Dut , ?i?1) . Predict w/ curr. mod.
F i ? FILTER(P i, ?) . Filter p?i?1(y?|x) ? ?
Si ? SAMPLE(F i) . Pick ? p?i?1(y|x). (?)
?i ? LEARN(Dls ? Dlt ? Si) . Retrain
end for
return ?T . Return adapted model
end procedure
? If LEARN(?) supports instance weighting, we could weight
each instance (x,y?) ? F i by p?i?1(y?|x) in the training
objective, rather than performing sampling according to the
same distribution.
supervised target language models trained with differ-
ent cluster features,4 these clusters are not optimally
adapted to the target language, compared to the mono-
lingual native clusters that are induced solely on the
target language, without any cross-lingual constraints.
This is to be expected, as the probabilistic model used
to learn the cross-lingual clusters strikes a balance
between two language specific models. On the other
hand, this suggests an opportunity for adapting to tar-
get language specific features through self-training.
In fact, since the direct transfer models are trained
using cross-lingual features, the target language can
be viewed as simply representing a different domain
from the source language.
Self-training has previously been shown to be a
simple and effective way to perform domain adapta-
tion for syntactic parsers and other tasks (McClosky
et al, 2006; Chen et al, 2011). The idea of self-
training for domain adaptation is to first train a su-
pervised predictor on labeled instances from a source
domain. This predictor is then used to label instances
from some unlabeled target domain. Those instances
for which the predictor is confident are added to the
source training set, and the process is repeated until
some stopping criterion is met. Recently, Daume?
et al (2010) and Chen et al (2011) proposed more
4For these experiments, the same settings were used as in the
multi-source transfer experiments in ?3, with the difference that
only target language training data was used.
complex domain adaptation techniques, based on co-
training. In this work, however, we stick with the sim-
ple single-view self-training approach just outlined.
In the self-training for domain adaptation method, de-
scribed by Chen et al (2011), the top-k instances for
which the predictor is most confident are added to the
training set in each iteration. We instead propose to
weight the target instances selected for self-training
in each iteration proportional to the confidence of the
classifier trained in the previous iteration.
In short, let x ? Dut be an unlabeled target lan-
guage input sequence (in our case a sentence) and
y? ? Yt(x) its top-ranked label sequence (in our
case an IO sequence). In the first iteration, a predictor
is trained on the labeled source language data, Dls. In
each subsequent iteration the sequences are scored
according to the probabilities assigned by the pre-
dictor trained in the previous iteration, p?i?1(y?|x).
When constructing the training set for the next it-
eration, we first filter out all instances for which
the top-ranked label sequence is not ?-dominating.
That is, we filter out all instances x ? Dtu such that
p?i?1(y
?|x) < ?, for some user-specified ?. In this
work, we set ? = 0.5, since this guarantees that the
output associated with each instance that is kept is
assigned the majority of the probability mass. This is
important, as we only consider the most likely output
y? for each input x, so that sampling low-confidence
instances will result in a highly biased sample. After
filtering, we sample from the remaining instances,
i.e. from the set of instances x ? Dtu such that
p?i?1(y
?|x) ? ?, adding each instance (x,y?) to
the training set with probability p?i?1(y?|x). This
procedure is repeated for T iterations as outlined
in Algorithm 1. By using instance weighting rather
than a top-k list, we remove the need to heuristically
set the number of instances to be selected for self-
training in each iteration. Further, although we have
not verified this empirically, we hypothesize that us-
ing instance weighting is more robust than picking
only the most confident instances, as it maintains di-
versity in the training set in the face of uncertainty.
Note also that when we have access to target language
test data during training, we can perform transduc-
tive learning by including the test set in the pool of
unlabeled data. This gives the model the opportunity
to adapt to the characteristics of the test domain.
Our use of self-training for exploiting features na-
58
DE ES NL AVG
SINGLE 39.7 62.0 63.7 55.2
MULTI 41.0 63.6 66.4 57.0
SINGLE + SELF 42.6 65.7 64.0 57.4
SINGLE + SELF/NATIVE 44.5 66.5 65.9 59.0
MULTI + SELF 48.4 64.7 68.1 60.4
MULTI + SELF/NATIVE 49.5 66.5 69.7 61.9
? DEVELOPMENT SET ? TEST SET
SINGLE 37.8 59.1 57.2 51.4
MULTI 36.4 61.9 59.9 52.8
SINGLE + SELF 41.3 61.0 57.8 53.3
SINGLE + SELF/NATIVE 43.0 62.5 58.9 54.8
MULTI + SELF 45.3 62.3 61.9 56.5
MULTI + SELF/NATIVE 47.2 64.8 63.1 58.4
Table 3: Results of different extensions to direct trans-
fer as measured with F1-score on the CoNLL 2002/2003
development and test sets. SINGLE: single-source trans-
fer, MULTI: multi-source transfer, SELF: self-training
with only cross-lingual word clusters, SELF/NATIVE: self-
training with cross-lingual and native word clusters.
tive to the target language resembles the way McDon-
ald et al (2011) re-lexicalize a delexicalized direct
transfer parser. Both methods allow the model to
move weights from shared parameters to more pre-
dictive target language specific parameters. However,
rather than using the direct transfer parser?s own pre-
dictions through self-training, these authors project
head-modifier relations to the target language through
loss-augmented learning (Hall et al, 2011). The boot-
strapping methods for language independent NER of
Cucerzan and Yarowsky (1999) have a similar effect.
Our self-training approach is largely orthogonal to
these approaches. We therefore believe that combin-
ing these methods could be fruitful.
4.1 Experiments
In these experiments we combine direct transfer with
self-training using unlabeled target data. This is the
transductive setting, as we include the test data (with
labels removed, of course) in the unlabeled target
data. We investigate the effect of adding self-training
(SELF) to the single-source and multi-source transfer
settings of ?3, where only cross-lingual features are
used (SINGLE and MULTI, respectively). We further
study the effect of including native monolingual word
cluster features in addition to the cross-lingual fea-
tures (SELF/NATVE). The experimental settings and
datasets used are the same as those described in ?3.
We performed self-training for T = 5 iterations for
all languages, as preliminary experiments indicated
that the procedure converges to a stable solution af-
ter this number of iterations. CRFSuite was used to
compute all the required probabilities for the filtering
and sampling steps.
The results of these experiments are shown in Ta-
ble 3. By itself, self-training without target specific
features result in an average relative error reduction
of less than 4%, compared to the baseline direct
transfer system. This is only slightly better than
the improvement achieved with multi-source transfer.
However, when adding target specific features, self-
training works better, with a 7% reduction. Combin-
ing multi-source transfer with self-training, without
target specific features, performs even better with
a 10% reduction. Finally, combining multi-source
transfer and self-training with target specific features,
gives the best result across all three languages, with
an average relative error reduction of more than 14%.
The results for German are particularly interest-
ing, in that they highlight a rather surprising general
trend. The relative improvement achieved by com-
bining multi-source transfer and self training with na-
tive clusters is almost twice as large as that achieved
when using only self-training with native clusters,
despite the fact that multi-source transfer is not very
effective on its own ? in the case of German, multi-
source transfer actually hurts results when used in
isolation. One explanation for this behavior could be
that the regularization imposed by the use of multi-
ple source languages is beneficial to self-training, in
that it generates better confidence estimates. Another,
perhaps more speculative, explanation could be that
each source language shares different characteristics
with the target language. Even though the predictions
on the target language are not much better on aver-
age in this case, as long as a large enough subset of
the confident predictions are better than with single-
source transfer, these predictions can be exploited
during self-training.
In addition to using self-training with native word
cluster features, we also experimented with creating
target language specific versions of the cross-lingual
features by means of the feature duplication trick
(Daume?, 2007). However, preliminary experiments
suggested that this is not an effective strategy in the
59
0
10
20
30
40
50
60
70
80
0 125 250 500 1k 2k 4k 8k 16k 32k 64k 128k
F 1
Number of annotated target tokens
supervised
single
multi
single + self/native
multi + self/native
Figure 1: Learning curves for German.
0
10
20
30
40
50
60
70
80
90
0 125 250 500 1k 2k 4k 8k 16k 32k 64k 128k
F 1
Number of annotated target tokens
supervised
single
multi
single + self/native
multi + self/native
Figure 2: Learning curves for Spanish.
cross-lingual direct transfer scenario. It thus seems
likely that the significant improvements that we ob-
serve are at least in part explained by the fact that
the native features are distinct from the cross-lingual
features and not mere duplicates.
5 Direct Transfer vs. Supervised Learning
Finally, we look at the relative performance of the dif-
ferent direct transfer methods and a target language
specific supervised system trained with native and
cross-lingual word cluster features. For these experi-
ments we use the same settings as for the experiments
in ?3 and ?4.1.
Figures 1?3 show the learning curves for the su-
pervised system, as more and more target language
annotations, selected by picking sentences at random
from the full training set, are added to the training
set, compared to the same system when combined
with different direct transfer methods. From these
curves, we can see that the purely supervised model
0
10
20
30
40
50
60
70
80
90
0 125 250 500 1k 2k 4k 8k 16k 32k 64k 128k
F 1
Number of annotated target tokens
supervised
single
multi
single + self/native
multi + self/native
Figure 3: Learning curves for Dutch.
requires between 8,000 and 16,000 annotated word
tokens (roughly corresponding to between 430 and
860 sentences) in each target language to match the
best direct transfer system. The learning curves also
show that adding source language data improves per-
formance with as many as 64,000 annotated target
language tokens.
Although we believe that the results on combin-
ing source and target data are interesting, in practice
the marginal cost of annotation is typically quite low
compared to the initial cost. Therefore, the cost of
going from 125 to 64,000 annotated tokens is likely
not too high, so that the benefit of cross-lingual trans-
fer is small on the margin in this scenario. However,
we believe that direct transfer methods can reduce
the initial cost as well, especially when a larger label
set is used, since a larger label set implies a larger
cognitive load throughout annotation, but especially
in the initial phase of the annotation.
Another aspect, which we were unable to investi-
gate is the relative performance of these methods on
domains other than news text. It is well known that
the performance of supervised NER systems drop sig-
nificantly when applied to data outside of the training
domain (Nothman et al, 2008). Although the direct
transfer systems in these experiments are also trained
on news data, we suspect that the advantage of these
methods will be more pronounced when applied to
other domains, since the supervised target system
runs a higher risk of overfitting to the characteristics
of the target language training domain compared to
the direct transfer system, which has already to some
degree overfitted to the source language.
60
6 Discussion
We have focused on direct transfer methods that ex-
ploit cross-lingual word clusters, which are induced
with the help of word alignments. A more com-
mon use of word alignments for cross-lingual linguis-
tic structure prediction is for projecting annotations
across languages (Yarowsky et al, 2001; Diab and
Resnik, 2002; Hwa et al, 2005).
Apart from the algorithmic differences between
these approaches, there are more fundamental differ-
ences in terms of the assumptions they make. An-
notation projection relies on the construction of a
mapping from structures in the source language to
structures in the target language, Ys 7? Y ?t. Based
on the direct correspondence assumption (Diab and
Resnik, 2002; Hwa et al, 2005), word alignments are
assumed to be a good basis for this mapping. When
projecting annotations, no consideration is taken to
the source language input space, Xs, nor to the target
language input space, Xt, except implicitly in the
construction of the word alignments. The learning al-
gorithm is thus free to use any parameters when train-
ing on instances from Xt ? Y ?t, but can at the same
time not exploit any additional information that may
be present in Xs ? Ys about Xt ? Yt. Furthermore,
word alignments are noisy and often only provide
partial information about the target side annotations.
Direct transfer, on the other hand, makes a stronger
assumption, as it relies on a mapping from the joint
space of source inputs and output structures to the
target language, Xs ? Ys 7? X ?t ? Y ?t. Actually,
the assumption is even stronger, since in order to
achieve low error on the target language with a dis-
criminative model, we must further assume that the
conditional distribution P (Y ?t|X ?t ) does not diverge
too much from P (Yt|Xt) in regions where P (Xt)
is large. This suggests that direct transfer might be
preferable when source and target languages are suffi-
ciently similar so that a good mapping can be found.
These differences suggest that it may be fruitful
to combine direct transfer with annotation projec-
tion. For example, direct transfer could be used
to first map Xs ? Ys 7? X ?t ? Y ?t, while annota-
tion projection could be used to derive constraints
on the target output space by means of a mapping
Ys 7? Y ??t . These constraints could perhaps be ex-
ploited in self-training, e.g., through posterior reg-
ularization (Ganchev et al, 2010), or be used for
co-training (Blum and Mitchell, 1998).
7 Conclusions
We investigated several open questions regarding the
use of cross-lingual word clusters for direct transfer
named entity recognition. First, we looked at the sce-
nario where no annotated resources are available in
the target language. We showed that multi-source di-
rect transfer and self-training with additional features,
exclusive to the target language, both bring benefits
in this setting, but that combining these methods
provide an even larger advantage. We then exam-
ined the rate with which a supervised system, trained
with cross-lingual and native word cluster features,
approaches the performance of the direct transfer
system. We found that on average between 8,000
and 16,000 word tokens need to be annotated in each
target language to match our best direct transfer sys-
tem. We also found that combining native and cross-
lingual word clusters leads to improved results across
the board. Finally, we showed that direct transfer
methods can aid even in the supervised target lan-
guage scenario. By simply mixing annotated source
language data with target language data, we can sig-
nificantly reduce the annotation burden required to
reach a given level of performance in the target lan-
guage, even with up to 64,000 tokens annotated in the
target language. We hypothesize that more elaborate
domain adaptation techniques, such as that proposed
by Chen et al (2011), can lead to further improve-
ments in these scenarios.
Our use of cross-lingual word clusters is orthog-
onal to several other approaches discussed in this
paper. We therefore suggest that such clusters could
be of general use in multilingual learning of lin-
guistic structure, in the same way that monolingual
word clusters have been shown to be a robust way to
bring improvements in many monolingual applica-
tions (Turian et al, 2010; Ta?ckstro?m et al, 2012).
Acknowledgments
This work benefited from discussions with Ryan Mc-
Donald and from comments by Joakim Nivre and
three anonymous reviewers. The author is grateful
for the financial support of the Swedish National
Graduate School of Language Technology (GSLT).
61
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for text
chunking. In Proceedings of ACL.
Bogdan Babych and Anthony Hartley. 2003. Improving
machine translation quality with automatic named en-
tity recognition. In Proceedings of the EAMT workshop
on Improving MT through other Language Technology
Tools: Resources and Tools for Building MT.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of ACL.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of
COLT, COLT? 98, New York, NY, USA. ACM.
Minmin Chen, John Blitzer, and Kilian Q. Weinberger.
2011. Co-training for domain adaptation. In Proceed-
ings of NIPS.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Silviu Cucerzan and David Yarowsky. 1999. Language
independent named entity recognition combining mor-
phological and contextual evidence. In Proceedings of
EMNLP-Very Large Corpora.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Hal Daume?, III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain adap-
tation. In Proceedings of the 2010 Workshop on Do-
main Adaptation for Natural Language Processing.
Hal Daume?, III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of ACL.
Paramveer Dhillon, Dean Foster, and Lyle Dean. 2011.
Multi-view learning of word embeddings via cca. In
Proceedings of NIPS.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
Dayne Freitag. 2004. Trained named entity recogni-
tion using distributional clusters. In Proceedings of
EMNLP.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine Learn-
ing Research.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: a brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics - Volume 1.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
Proceedings of EMNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Jochen L. Leidner, Gail Sinclair, and Bonnie Webber.
2003. Grounding spatial named entities for information
extraction and question answering. In Proceedings of
HLT-NAACL-GEOREF.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP, pages 1030?1038.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of NAACL-HLT.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Roberta Merchant, Mary Ellen Okurowski, and Nancy
Chinchor. 1996. The multilingual entity task (met)
overview. In Proceedings of a workshop on held at
Vienna, Virginia: May 6-8, 1996.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative
training. In Proceedings of HLT-NAACL.
Joel Nothman, James R Curran, and Tara Murphy. 2008.
Transforming wikipedia into named entity training data.
In Proceedings of the Australasian Language Technol-
ogy Association Workshop 2008, pages 124?132, Ho-
bart, Australia, December.
Naoaki Okazaki. 2007. Crfsuite: a fast implementation
of conditional random fields (crfs).
Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to wikipedia. In Proceedings of ACL-HLT.
Lawrence Saul and Fernando Pereira. 1997. Aggregate
and mixed-order markov models for statistical language
processing. In Proceedings of EMNLP, pages 81?89.
Satoshi Sekine and Chikashi Nobata. 2004. Definition,
dictionaries and tagger for extended named entity hier-
archy. In Proceedings of LREC.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2009. Adding more languages im-
proves unsupervised multilingual part-of-speech tag-
ging: A bayesian non-parametric approach. In Pro-
ceedings of NAACL.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL-HLT.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
62
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL.
Erik F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named entity
recognition. In Proceedings of CoNLL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT.
63
