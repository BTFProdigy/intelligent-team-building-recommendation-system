Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 937?944
Manchester, August 2008
Source Language Markers in EUROPARL Translations 
Hans van Halteren 
Radboud University Nijmegen 
Department of Linguistics / CLST 
P.O. Box 9103, NL-6500HD Nijmegen  
The Netherlands 
hvh@let.ru.nl 
 
 
Abstract 
This paper shows that it is very often 
possible to identify the source language 
of medium-length speeches in the EU-
ROPARL corpus on the basis of fre-
quency counts of word n-grams (87.2%-
96.7% accuracy depending on classifica-
tion method). The paper also examines in 
detail which positive markers are most 
powerful and identifies a number of lin-
guistic aspects as well as culture- and 
domain-related ones.1 
1 Introduction 
The EUROPARL Corpus (Koehn, 2005) is one 
of the most important resources for translation 
research. It is used extensively, mostly but cer-
tainly not exclusively in statistical machine trans-
lation. In much of that research, the relation be-
tween a source language SL and target language 
TL is investigated on the basis of aligned sen-
tences of SL and TL without considering whether 
SL is indeed the actual source language. In this 
paper we question the lack of attention for the 
true SL, because we expect significant differ-
ences between texts written originally in TL, 
texts translated from SL to TL and texts trans-
lated from another language into TL, even if all 
three types have been produced by native speak-
ers of TL.2 At least with respect to the differ-
ences between original and translated texts, our 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
2
 In principle, the EU translation service always lets transla-
tors translate into their mother tongue. Given that we will 
here be working only with major EU languages, it is 
unlikely that the principle was violated for our selected 
texts.  
expectations are supported by the literature (for 
an overview, see e.g. Baroni and Bernardini, 
2006). 
We embarked on a data-driven investigation, 
using several text classification techniques to 
determine if the differences are salient enough to 
distinguish between different source languages 
for various texts and target languages. In this 
paper, we first describe the data and methods we 
used (Sections 2 and 3). Then we present the 
classification results (Section 4), after which we 
give a description of some types of markers we 
were able to identify (Section 5). Finally, in Sec-
tion 6, we present our conclusions and plans for 
future research.  
2 Experimental Data 
The experimental data was taken from the EU-
ROPARL Corpus, but had to be preprocessed to 
some degree to be suitable for our experiments. 
First of all, the annotation of the corpus includes 
a LANGUAGE attribute, indicating the original 
language of the text. However, it is often absent 
in one or more versions of the text. If we exam-
ine the speeches of at least 100 words in the 
whole corpus, we find that 11% of them lack the 
attribute in all available versions and for 5% the 
attribute has different values in different ver-
sions. We linked the attribute across the various 
versions and excluded all speeches showing in-
consistent values.  
We decided to focus on the six most common 
languages in the corpus: English (EN), German 
(DE), French (FR), Dutch (NL), Spanish (ES) 
and Italian (IT). For each of these languages as a 
source language, we aimed for 1000 speeches 
(henceforth: texts) which were present as original 
and as translations into each of the other five 
languages. Furthermore, we decided to focus on 
the medium range as for length, avoiding very 
short and very long texts, as these were likely to 
937
be different in nature.3  Our aim of 1000 texts 
from each source language led us to settle on 
texts between 380 and 2500 words. 
Finally, as we wanted the classification sys-
tems to focus on language use rather than con-
tents, we transformed the tokens in the texts, 
separately for each target language. Only tokens 
that occurred in at least 10% of the texts re-
mained intact. Other tokens were mapped to a 
marker <X>. 
3 Methods 
We classified all texts in our experimental set 
with several text classification methods (see Sec-
tions 3.1 to 3.3), using 10-fold cross-validation, 
each time with 80% of the texts used to train 
models for each source language, 10% of the 
texts used to tune parameters and to base thresh-
olds on and 10% of the texts to test classification 
accuracy. For all methods, the same train/ 
tune/test splits were used and some dozens of 
promising parameter settings were tried, after 
which the best one for each specific run was se-
lected on the basis of tune set results. 
Each method was provided with the same text 
features, viz. n-gram counts. From each selected 
text (in its mapped version), we extracted counts 
for all uni-, bi- and trigrams of tokens occurring 
in at least 10% of the texts (i.e. those not trans-
formed to <X>) and allowing <X> markers to in-
tervene between any two tokens in the n-gram. 
For example, the sequence "I join with Roy 
Perry in" would give rise to the n-gram 
<3>_I_<X>_with_<X>_<X>_in, with the <3> in-
dicating that there are three real tokens in the n-
gram, and possibly any number of intervening 
<X> markers.  
3.1 Marker-Based Classification 
Our first classification method attempts to clas-
sify texts on the basis of individual markers 
which are by themselves a strong indication that 
the text originated in a certain source language. 
All n-grams which occurred more often in the 
training data with a specific source language SL 
than with all other source languages taken to-
gether were deemed to be markers for SL.  
When classifying test data, each marker for an 
SL observed in the test text leads to an increase 
of the score for that SL for that text. The exact 
                                                 
3
 E.g., short texts tend to be interruptions and statements 
such as the opening of the session, where long texts include 
presentations of written reports.  
calculation of the increase depends on some pa-
rameter settings. It always involves both the 
marker's precision (how often its presence indeed 
coincides with the specific SL) and its recall 
(how many of the specific SL texts contain the 
marker), but with a weighting favouring preci-
sion over recall by a factor of 10 to 100. Preci-
sion and recall can be based on either raw or 
smoothed counts. The calculation may also in-
volve the frequency of the marker in the test text. 
In this way, the frequency of a marker is not 
taken into account for determining whether it is a 
marker and what value it is given, but may be 
taken into account when classifying test texts. 
3.2 Linguistic Profiling 
The second classification system was Linguistic 
Profiling, which was previously shown to be use-
ful for language verification (van Halteren and 
Oostdijk, 2004), a task which is similar to the 
current task.  Roughly speaking, it classifies on 
the basis of noticeable over- and underuse of 
specific n-grams. As the marker-based classifica-
tion only used overuse and the necessary degree 
of overuse is lower for Linguistic Profiling, the 
latter pays attention to many more features and 
should be able to attain a better classification 
rate. However, if we want to determine which 
features are most powerful, interpreting the 
workings of Linguistic Profiling will be more 
difficult than for the marker-based approach.  
All n-gram counts were normalized to counts 
per 1000 words. Furthermore, in order to reduce 
the number of counts, so that the system could 
cope with the resulting vectors, we included only 
n-grams which occurred in at least 10 texts. This 
led to vectors with about 90,000 counts for each 
target language. 
3.3 Support Vector Methods 
Finally, we employed Support Vector Machines, 
viz. LIBSVM (Chang and Lin, 2001). We of-
fered the same vectors we used for Linguistic 
Profiling to both standard Support Vector Classi-
fication (SVC; RBF kernel, various settings for C 
and ?) and Support Vector Regression (?-SVR; 
RBF kernel, various settings for C, ? and ?). 
The Support Vector methods use all available 
information rather than focusing on over- and 
underuse. They should therefore attain the best 
classification results. However, extracting infor-
mation about salient features from the results will 
be virtually impossible. 
 
 
938
 MB LP SVC SVR 
     
SL vs TL: TL 84.1 91.5 92.6 95.2 
SL vs TL: SL 75.3 88.4 87.6 91.6 
Combination 87.2 94.7 90.1 96.9 
     
SL vs SL: SL 74.2 85.8 85.4 89.7 
Combination 81.1 91.8 85.4 94.3 
Table 1: Average accuracies for two-way SL vs 
SL choices for various classification methods.  
The top part represents cases where one of the 
SL is the TL, the bottom where neither SL is TL.  
3.4 Score Comparability 
For all methods except SVC, the ranges of test 
text scores vary greatly as train sets and parame-
ter settings are changed. As we wanted to com-
bine scores from classifications based on models 
for various SL, however, we needed the scores to 
be comparable. In order to make them so, we 
compared the score for each test text with the 
scores for all tune texts.4 All lower scoring posi-
tive examples provided a increase of the final 
(comparable) score and all higher scoring nega-
tive examples a decrease. For fine tuning, the 
relative position of the test text's score between 
the next higher and next lower scoring tune texts 
was also taken into consideration.5  
4 Classification Results 
We first used our classification techniques to 
choose between every possible pair of source 
languages for each text (Section 4.1). Then we 
combined the various two-way decisions into a 
single six-way decision (Section 4.2). 
4.1 Two-Way Decisions 
For each choice between two possible source 
languages, there are two classification models 
(one for each of the SL) that can make the choice 
individually, but we can also combine the two 
opinions by choosing the SL whose classification 
model claims the text with a higher score.6  
The quality of these decisions, for those cases 
where  the  actual  SL  is  present  in  the  pair,  is  
 
                                                 
4
 We deliberately chose a non-parametric technique here. 
However, preliminary additional experiments show that a 
parametric alternative may actually provide slightly better 
results. 
5
 This technique did not work for SVC, as SVC only scores 
a text with 1 or -1. However, for SVC the technique was 
also not needed as the scores were already comparable. We 
did add a very small random number to each score to re-
solve ties where necessary. 
6
 Remember that we made all classification scores compara-
ble. 
 
 
  MB LP SVC SVR 
ES   64.7 82.9 81.1 87.4 
DE   62.0 81.6 80.8 87.5 
EN   60.4 80.6 79.3 86.8 
FR   58.7 77.0 77.7 85.7 
NL   58.4 76.7 75.2 83.1 
IT   52.8 73.6 60.7 81.5 
ALL TL   87.2 90.6 91.5 96.7 
Table 2: Average accuracies for six-way SL 
choices for various classification methods. The 
rows with a TL indication represent the results 
when the text is only available in that TL. The 
bottom row shows the combined result, using all 
six versions of the text. 
 
shown in Table 1. We distinguish two types of 
choices. The top of the table shows the results for 
the cases in which one of the two SL is the target 
language, with SL equal to TL indicating that the 
text is an original TL text rather than a transla-
tion. For the cases represented in the table, i.e. 
cases where one of the two SL is the actual one, 
the task measured here is in fact translation rec-
ognition. As the table shows, the accuracy for 
this task is higher when modeling original TL 
text than when modeling translations, meaning 
that it is easier to spot (violations against) regu-
larities in the target language than it is to spot 
regularities in the translations from a specific 
source language. Combining the two models 
yields even better classification, except for SVC, 
where combination cannot be used properly be-
cause SVC only produces scores of 1 and -1. The 
bottom of the table shows the results for the 
cases in which both SL are different from the 
target language. Here the choices appear to be 
more difficult on average, but the classification 
quality is still impressive. 
4.2 Six-Way Decisions 
Once we had classification scores for the choice 
between all fifteen possible pairs of source 
languages (actually the combination scores), we 
could use these to choose a single SL from 
among the set of six possible SL. For the current 
paper, we did this by simply adding all 
classification scores in favour of each specific SL 
and then choosing the SL with the highest total. 
The addition can either be done over all two-way 
choices referring to a specific target language, or 
over all choices for all six possible target 
languages. 
The quality of these decisions is shown in Ta-
ble  2.  For  the  individual  target languages,  the 
939
Table 3: Confusion table for SVR classification 
of each of the 6000 selected speeches on the 
basis of all six language versions of each speech. 
Rows are actual languages; columns are 
languages assigned by the classifier. 
 
accuracies are not all that high, but this was to be 
expected.  Interestingly,  the relative difficulty of 
the choice for the various TL is largely inde-
pendent of the classification method used, with 
texts in Spanish being most easily classifiable 
and texts in Italian least easily. The relative per-
formance of the classification methods is again 
as expected, except that SVC is slightly behind 
Linguistic Profiling for the individual languages. 
For the best performing classification method, 
Support Vector Regression, we also show the 
confusion table for SL assignment (Table 3). In 
general, there are more confusions within the 
Germanic and Romance language families than 
between them. However, English seems to take 
an intermediate position between the two fami-
lies. On the other hand, the English model ap-
pears to be somewhat greedier than the others, 
which confuses the analysis. Also, given that the 
LANGUAGE attribute does not seem to be as-
signed perfectly, it would be advisable to exam-
ine the misassigned texts, so as to check if they 
are indeed misassigned or merely mislabeled. 
5 A Look at Source Language Mark-
ers 
Now we have shown that word n-grams provide 
a solid basis for source language identification, 
we proceed to an examination of the n-grams that 
are the most useful in this identification. Unfor-
tunately, as stated above, Linguistic Profiling and 
the Support Vector methods are not very amena-
ble to extracting information about the most sali-
ent features. Therefore, for now, we will have to 
fall   back  on   the   marker-based   classification 
where it is trivial to identify the source language 
markers which characterize their source lan-
guages most strongly. Admittedly, the marker-
based  classification had  the lowest performance 
in the classification task, but still it is good 
enough for an examination to make sense.  
Table 4: Numbers of source language markers 
for various source languages SL in translation to 
English.  
 
To optimize understandability for all readers, 
we focus on English as the target language in this 
examination.  We distinguish between  two kinds  
of marker strength. Obviously, there is the 
strength in the SL classification described above. 
For a marker to be strong in this sense, it has to 
occur more often with the source language in 
question than with all other source languages 
taken together (Section 5.1). If, however, we en-
vision an application where we know that a spe-
cific SL is being translated to a specific TL and 
we want to give feedback to the translator that 
the translation contains strong influences from 
the SL, then strength can also be taken to be the 
degree to which the marker occurs more in texts 
translated from SL than in original TL texts (Sec-
tion 5.2). For both kinds of marker strength, we 
can identify specific types of markers, related 
either to linguistic or culture- and domain-related 
aspects of texts and their translations (Section 5.3)  
5.1 Statistics for SL vs All Others 
We took all n-grams from the English versions of 
the texts in our experimental data and counted 
the number of texts they were contained in for 
each of the six source languages. We then calcu-
lated their strength for each source language SL 
by dividing the observed number of SL texts, 
plus one, by the number of non-SL texts, again 
plus one. The classification described in Section 
3.1 in principle used all markers with strength 
greater than one.  
If the full set of texts were used as training 
material, we would find the numbers of markers 
as shown in Table 4. The majority of n-grams 
occurs only once (Column 2) and necessarily 
with a single specific SL so that they can be 
taken to be markers. Note, however, that these 
markers did not play a role in the classification in 
the ten-fold cross-validation, since they were 
always either only in the training set, only in the 
tune set or only in the test set. The third column 
represents n-grams which also occur only with  
 
      
EN DE FR NL ES IT 
EN 980 1 12 2 1 4 
DE 16 961 6 9 3 5 
FR 15 3 969 2 7 4 
NL 16 10 12 956 4 2 
ES 14 5 13 2 960 6 
IT 8 2 9 0 8 973 
 
Occurring 
only once; 
With SL 
Occurring 
more often; 
Only with SL 
Occurring 
also with 
non-SL 
DE 137256 6691 9459 
FR 122386 4916 7102 
NL 120071 5594 7653 
ES 119899 5740 8495 
IT 129251 5900 8538 
940
Table 5: Strongest n-grams occurring only with a 
specific SL  
 
one specific SL. These markers are in principle 
the most useful because of their precision. How-
ever, their number of occurrences turns out to be 
generally very low,  with a maximum of 23 and a  
mean of 2.1, so that their recall is low to very 
low. Finally, there are the n-grams which do oc-
cur  with  various  source  languages,   but  most 
often with one specific SL. They are represented 
by the last column.  
Probably more insightful than the general sta-
tistics are the actual markers themselves, here 
represented by the strongest ones for each source 
language,  still  with  English  as  the  target  lan- 
Table 6: Strongest  n-grams occurring also with 
other languages   
 
guage. The lists we find are always headed by n-
grams which occur only with a specific SL (Ta-
ble 5).  Then follow the n-grams  which can also 
be found with other languages. We show the 
strongest ones of this type separately as Table 6. 
As already stated above, most of the stronger 
markers occur in only few SL texts. The big ex-
ception seems to be Dutch, which has several 
markers (notably <3>_._<X>_to_say with com-
peting languages and <3>_like_to_<X>_off 
exclusively) which show up quite often. Only 
<2>_framework_conditions for German and the 
certain_number cluster for French occur over 
20 times too, but they are not exclusive. We also 
see various clusters, where longer strings are rep-
resented by several n-grams, such as "a certain 
number of"  and  "with  no regard for"  for 
SL n-gram texts  
DE <3>_is_,_though 10 
 
<2>_Commission_here <3>_,_again_and 
<3>_are_right_<X>_you <3>_If_,_though 
<3>_me_say_this <3>_the_Commission_here 
<3>_the_<X>_Council_Presidency 
7 
 
<3>_._What_that <2>_What_that 
<3>_reason_we_must <3>_must_at_last 
<2>_more_able <3>_go_without_saying 
<3>_not_,_though <2>_need_<X>_- 
<2>_taken_here <3>_believe_that_here 
<3>_is_needed_here 
<3>_gentlemen_,_<X>_<X>_is  
6 
FR <3>_common_to_the <3>_quality_of_her 6 
 
<3>_with_no_regard <3>_no_regard_for 
<2>_no_regard <3>_conditions_of_<X>_of 
<2>_into_<X>_this <2>_on_<X>_services 
<2>_particular_about <2>_various_policies 
<3>_all_those_, <3>_of_cooperation_is 
<3>_the_United_<X>_<X>_Commissioner 
<3>_,_<X>_society_, 
5 
 
NL <3>_like_to_<X>_off 23 
 
<3>_to_<X>_off_with 16 
 
<3>_On_a_final 14 
 
<2>_Commissioner_whether 
<3>_the_Commissioner_whether 
10 
 
<3>_and_such_like <2>_such_like 8 
 
<3>_past_<X>_of_years <3>_,_it_<X>_as 
<3>_are_in_order <3>_too_<X>_for_words 
<3>_to_<X>_off_by 
7 
 
ES <3>_going_to_support 11 
 
<3>_amendments_presented_by <3>_end_here_, 
<3>_Community_system_for 
<3>_the_people_responsible 
8 
 
<3>_citizens_._And <3>_going_to_debate 7 
 
<3>_the_Community_<X>_sector 
<2>_Community_<X>_sector 
<2>_than_<X>_<X>_<X>_million 
<3>_million_<X>_year_. 
<3>_President_,_without <3>_Let_us_see 
<3>_adopt_measures_to  
<3>_move_<X>_with_the 
<3>_And_the_Commission 
<3>_people_responsible_for 
6 
IT <3>_least_in_that 9 
 
<2>_task_before <3>_or_,_<X>_still 7 
 
<3>_change_the_current <3>_is_the_Europe 
<3>_feel_that_Parliament <3>_task_before_us 
<3>_of_my_Commission <2>_with_<X>_<X>_' 
<3>_the_<X>_available_. <2>_:_<X>_which 
<3>_security_and_peace  
6 
 
SL n-gram SL 
texts 
other source 
language 
texts 
DE <3>_means_is_that 11 1 IT 
 
<2>_framework_conditions 22 2 EN, 1 FR 
 
<3>_in_future_be 14 1 EN, 1 IT 
 
<3>_,_that_being 13 2 NL 
 
<2>_action_here 8 1 FR 
 
<3>_So_let_me 8 1 FR 
 
<3>_to_at_last 8 1 FR 
FR <3>_why_I_shall 8 1 NL 
 
<2>_certain_number 25 1 DE, 2 IT, 
2 NL 
 
<3>_certain_number_of 25 1 DE, 2 IT, 
2 NL 
 
<3>_a_certain_number 24 1 DE, 2 IT, 
2 NL 
 
<3>_thank_our_rapporteur 7 1 NL 
 
<3>_We_now_know 6 1 NL 
 
<3>_provide_itself_with 6 1 ES 
NL <3>_._<X>_to_say 61 4 EN, 2 DE, 
1 FR 
 
<3>_that_is_concerned 10 1 DE 
 
<3>_we_as_Parliament 9 1 IT 
 
<3>_group_,_it 9 1 ES 
 
<3>_is_every_reason 9 1 ES 
 
<3>_deal_of_support 8 1 DE 
 
<3>_think_that_that 12 1 ES, 1 FR 
ES <3>_._<X>_this_context 9 1 IT 
 
<3>_I_<X>_this_to 9 1 EN 
 
<2>_people_responsible 8 1 NL 
 
<3>_going_to_deal 8 1 NL 
 
<3>_President_,_<X>_allow 8 1 IT 
 
<3>_legislation_in_force 8 1 FR 
 
<3>_report_,_since 8 1 FR 
IT <2>_the_now 10 1 FR 
 
<3>_other_,_there 10 1 DE 
 
<3>_the_individual_States 10 1 DE 
 
<3>_,_<X>_<X>_,_ladies 10 1 DE 
 
<2>_my_Commission 8 1 FR 
 
<3>_due_regard_for 12 1 EN, 1 NL 
 
<2>_quite_aware 16 3 FR 
941
Table 7: Strongest individual tokens marking 
translations into English 
 
French and "the people responsible for" 
for Spanish. 
5.2 Markers for Translation vs Original 
If we want to call a translator's attention to a 
translation which deviates from the general lan-
guage use in the target language, the marker 
identifying the deviation need not be exclusive to 
a single source language. Such markers are much 
more common than source language specific 
markers. They also include large numbers of sin-
gle tokens (i.e. unigrams; Table 7), something we 
did not find in Section 5.1.  
Most remarkable are the top two, ladies and 
gentlemen. Apparently, speakers from all over 
Europe address the whole house when opening 
their speech, but those speaking English only 
address the President (i.e. the chairperson). The 
rest are mostly words providing discourse func-
tions, either by themselves, such as therefore 
and though, or in larger combinations, such as 
hand in "on the one hand" and opinion in "in 
my opinion". There are only a few words with 
actual content, such as guarantee, freedom and 
Office, although the latter may also well be part 
of a parliament term. 
As for the longer n-grams, we will not present 
the full list of strongest markers, but instead a 
filtered selection (Table 8). The reason for this is 
that there is quite a lot of repetition in the full 
list. The strongest eleven, and fifteen more of the 
strongest fifty, are (parts of) combinations of 
vocatives, such as "Commissioner, ladies and 
gentlemen", which we already addressed above. 
As  vocative   use  in  the   European  Parliament 
Table 8: Selection from the strongest n-grams 
marking translations into English 
 
could well be a separate study in itself, and is 
probably not something we need to bother trans-
lators with, we leave out all vocatives.   
5.3 Types of Markers 
The markers shown in the previous sections are 
of a rather varied nature. Some of them have lin-
guistic explanations, but there are also quite a 
few which are more culture- and domain-related. 
The best example in the latter category is the 
already mentioned use of vocatives. Although 
there are clear links to at least one source lan-
guage, English, this is not something that is 
caused by translation. Another example of seem-
ingly typical parliamentary behaviour are the 
phrases "like to finish off" and "On a fi-
nal note", responsible for the three strongest 
Dutch markers in Table 5. The Dutch (or Flem-
ish) parliamentarians announce in some way that 
they are nearing the end of their speech. How-
ever, if we examine the original Dutch text, we 
observe a much more varied phrasing. We find 
the literal counterpart ("ik wil afsluiten"), 
but also "ter afsluiting" ("to close") and 
"dan nog iets" ("then another thing"). Ap-
parently, one or more of the Dutch to English 
translators have developed their own favourite 
phrases to cover this general situation. 
Another domain-specific type of marker can 
be found in content words (here mostly com-
pounds) referring to parliamentary matters. Here 
we turn to German for some examples. In Table 
token EN DE FR NL ES IT 
<1>_ladies 11 574 362 197 273 378 
<1>_gentlemen 14 584 383 205 304 388 
<1>_And 69 154 160 164 307 128 
<1>_above 58 151 119 115 145 198 
<1>_guarantee 53 97 110 87 164 151 
<1>_favour 75 157 197 173 164 122 
<1>_everyone 55 144 141 126 57 93 
<1>_namely 54 158 94 184 54 60 
<1>_opinion 140 164 244 272 250 311 
<1>_mention 65 109 120 86 127 121 
<1>_although 95 122 110 116 233 219 
<1>_therefore 296 396 488 477 580 525 
<1>_regard 174 212 292 254 408 286 
<1>_shall 108 178 240 144 166 169 
<1>_though 80 215 131 116 82 117 
<1>_everything 69 152 126 98 97 98 
<1>_various 107 168 188 154 186 175 
<1>_hand 103 160 183 178 145 171 
<1>_freedom 72 118 119 88 105 157 
<1>_Office 73 158 110 77 142 102 
token EN DE ES FR IT NL 
<3>_of_the_Group 0 27 41 19 16 34 
<2>_end_by 0 2 42 18 25 7 
<3>_,_by_means 0 13 40 21 11 8 
<3>_s_<X>_(_<X>_<X>_) 0 35 13 8 11 21 
<3>_)_and_European 0 29 14 8 11 21 
<3>_,_for_we 0 25 0 7 26 20 
<3>_at_last_, 0 9 3 5 53 5 
<3>_countries_,_which 0 13 16 24 10 9 
<2>_we_therefore 0 10 19 8 22 10 
<3>_the_Europe_of 0 5 12 18 30 3 
<3>_order_to_guarantee 0 9 26 14 6 11 
<3>_and_above_all 1 35 29 26 28 13 
<3>_out_that_, 0 11 22 12 12 8 
<2>_therefore_believe 1 13 67 21 19 8 
<3>_think_that_this 1 14 22 29 15 46 
<3>_think_that_, 0 9 15 14 10 11 
<3>_like_to_<X>_this 0 5 15 4 7 27 
<3>_of_third_countries 0 4 17 15 12 9 
<2>_here_too 0 27 1 4 8 15 
<3>_of_all_like 0 4 12 14 2 22 
<3>_,_though_, 3 128 4 17 14 53 
<3>_too_,_we 0 21 0 6 11 15 
<3>_I_shall_not 0 3 10 21 7 12 
<3>_State_or_Government 0 15 5 10 16 6 
942
5 we find <3>_the_<X>_Council_Presidency. 
One might think that only Germans are interested 
in who runs the council at any given time, but in 
fact this is an idiosyncratic alternative translation 
as "Council Presidency" is generally just 
called Presidency. Another example is 
<2>_framework_conditions in Table 6, with 22 
German SL occurrences, 2 English and 1 French. 
When examining the source text, we find Rah-
menbedingungen, elsewhere translated (probably 
better) as "basic conditions". From these ex-
amples it would seem that some work may still 
be needed on harmonizing terminology.7 A lack 
of (knowledge of) central terms leads in one case 
to a translation with some unfortunate connota-
tions and in another case to an acceptable but 
deviant translation. In both cases the use of a 
single term would most certainly also improve 
information retrieval on the parliament proceed-
ings.  
Related to the domain, but much more culture 
specific is the variation in the way the speakers 
organize their argumentation. The example of 
speakers of Dutch announcing the last part of 
their speech has already been mentioned. An-
other thing speakers of Dutch seem to do is that 
they exaggerate their viewpoint, both positively 
and negatively. The positive exaggeration is visi-
ble in the word natuurlijk (naturally, obvi-
ously, ?), which is found in almost a third 
(326) of the originally Dutch speeches. One of 
the translations chosen for this word is "need-
less to say", thus giving rise to the extremely 
strong marker for Dutch in Table 6. The negative 
exaggeration is present in cases where a situation 
is called insane: waanzinnig, "te gek voor 
woorden" ("too crazy for words") or "te gek 
om los te lopen" ("too crazy to walk 
around freely"). A possible translation tem-
plate here is "too crazy/absurd/ridiculous 
for words", explaining another marker for 
Dutch in Table 5. A further obviously discourse-
related marker is therefore. As Table 7 shows, 
it is found from 30% (DE) to 100% (ES) more in 
translations than in original English text. This 
may mean that speakers of the other languages 
place more causal relations in their arguments, 
but it can also be that therefore just happens to 
be a favourite translation option among the trans-
lators. Again, this could be a research topic by 
                                                 
7
 Since EUROPARL only contains speeches some years in 
the past, the situation may well have been remedied in the 
meantime. 
itself and a thorough investigation is beyond the 
scope of this paper.  
Obviously, each source language has some 
words or phrases in its vocabulary which make 
themselves felt in the translations. Apart from the 
ones already mentioned, the strongest example 
here is the French marker "a certain number 
of". The original turns out to be "un certain 
nombre", elsewhere more English-like translated 
as "some of".  
There must also be instances of influences 
from languages' syntax, but these are much 
harder to find. It is possible that the overuse of 
shall, especially for French, is linked to verbs 
which are morphologically marked for future 
tense. Also the overuse of And at the beginning of 
sentences may be linked to splitting source lan-
guage long sentences into two English sentences 
or merely to more extensive use of a coordinat-
ing connective in the other languages. For both 
these words, there are also far too many occur-
rences to examine at this time. 
6 Comparison to Other Work 
The most similar investigation we are aware of is 
that by Baroni and Bernardini (2006). They 
worked on a corpus of Italian geopolitical journal 
articles and used SVMs to distinguish translated 
and original Italian text on the basis of mostly n-
gram features representing both types of text. 
They did not attempt to identify the source lan-
guage. Their task corresponds to the translation 
recognition task presented in the top half of Ta-
ble 1 and their method is comparable to the com-
bination of the two models for original texts and 
translations. They report an accuracy of 86.7%. 
If we examine the texts with TL equal to Italian, 
we find combination scores of 85.4% (MB), 
86.0% (SVC), 93.2% (LP) and 96.3% (SVR). 
Although their work is different in the choice of 
domain (geopolitical journal articles) and they do 
not distinguish translated texts as to source lan-
guage, the results for SVM classifiers are compa-
rable.   
They also mention that in earlier research 
(Baroni and Bernardini, 2003), they found that 
"bigrams most characteristic of translated text 
are sequences of function words", for both the 
Italian corpus already mentioned and a corpus of 
EU reports written in and translated into English. 
The 2003 paper itself, however, reports that "a 
more thorough investigation of the EU data ? 
failed to reveal systematic differences between 
translated and original documents". We must 
943
therefore conclude that their observation must 
refer to the Italian texts. Still, our tables do show 
quite a few function word bigrams, but of course 
we have contributed to any such predominance 
by blanking out most content words. 
Borin and Pr?tz (2001) examined translations 
from Swedish into English, using news articles. 
They examine over- and underuse of POS n-
grams. They manage to explain some of their 
observations, but not the overuse in the transla-
tions of adverbs, infinitives, pronouns and sen-
tence-initial prepositions. We have not examined 
POS classes, but only specific words. However, 
we do see various adverbs in prominent positions, 
especially in Table 7, which indeed shows over-
use. The sentence-initial prepositions might also 
be partly explained by (often lexicalized) adver-
bial prepositional phrases. 
7 Conclusions 
We have shown that classification on the basis of 
word n-grams markers is able to identify the 
source language of medium-length European 
Parliament speeches. Depending on the classifi-
cation method used, the actual source language 
can be identified for 87.2% to 96.7% of the texts 
when all six target language versions of the text 
can be accessed. If only a single version is avail-
able, classification is considerably worse and 
only Support Vector Regression consistently 
shows relatively high scores, with accuracies of 
81.5% for the Italian rendering to 87.4% for the 
Spanish one.  
We also examined the strongest markers. We 
found that they are rather varied in nature and 
represent a wide range of information sources. 
Vocabulary, discourse structure and probably 
syntax of the source language all contribute. 
Contrasts between source and target languages 
can be seen to have an influence too, both purely 
linguistically and through the behaviour of the 
translators. But also the behaviour patterns of the 
parliamentarians of the various countries have a 
clear influence. Some of these influences are 
harmless or even attractive. Others should be 
followed up on, e.g. it would be good to attempt 
a harmonization of terminology throughout the 
various translation services, so that information 
retrieval on the parliamentary proceedings can be 
improved. 
As for further research, it is vital to first inves-
tigate how exactly the European Parliament pro-
ceedings have been translated in the past, are 
being translated in the present and will be trans-
lated in the future. It may well be that some of 
the effects we are finding are limited to individ-
ual translators, or caused by their use of (ma-
chine) translation tools. Once it is clear that we 
are measuring what we think we are measuring, 
namely general trends in speaker and translator 
behaviour, we need to automate the retrieval of 
target language and source language phrases 
(maybe using statistical machine translation 
methodology), and possibly also to address se-
mantically related clusters. Only then can we 
really investigate if observations like "Dutch 
speakers exaggerate more often" are valid or are 
just false impressions from looking at the data 
through too small a window. 
Once all this is in place, we will have the 
means for a whole range of activities, e.g. to 
study parliamentary behaviour, to study the 
translation process, to determine if source lan-
guage should be taken more into account in EU-
ROPARL translation models and potentially 
even to give useful advise to the EU translation 
services.  
References 
Marco Baroni and Silvia Bernardini. 2003. A Prelimi-
nary Analysis of Collocational Differences in 
Monolingual Comparable Corpora. Proc. Corpus 
Linguistics 2003, Lancaster, UK. 
Marco Baroni and Silvia Bernardini.2006. A New 
Approach to the Study of Translationese: Machine-
Learning the Difference between Original and 
Translated Text. Literary and Linguistic Comput-
ing, 21(3): 259-274. 
Lars Borin and Klas Pr?tz. 2001. Through a glass 
darkly: Part of speech distribution in original and 
translated text. Computational linguistics in the 
Netherlands 2000. Edited by Walter Daelemans, 
Khalil Sima'an, Jorn Veenstra, Jakub Zavrel. Am-
sterdam: Rodopi. 2001. 30-44 
Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a 
library for support vector machines, 2001. Avail-
able at http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Hans van Halteren and Nelleke Oostdijk. 2004. Lin-
guistic Profiling of Texts for the Purpose of Lan-
guage Verification. COLING 2004, Geneva: 966-
972.  
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. MT Summit X, 
Phuket, Thailand: 79-86. 
944
Linguistic profiling of texts for the purpose of language verification 
Hans VAN HALTEREN 
Dept. of Language and Speech, Univ.of 
Nijmegen, The Netherlands 
P.O. Box 9103, 6500 HD Nijmegen 
hvh@let.kun.nl 
Nelleke OOSTDIJK 
Dept. of Language and Speech, Univ. of 
Nijmegen, The Netherlands 
P.O. Box 9103, 6500 HD Nijmegen 
n.oostdijk@let.kun.nl 
 
Abstract 
In order to control the quality of internet-based 
language corpora, we developed a method to 
verify automatically that texts are of (near-) 
native quality. For the LOCNESS and ICLE 
corpora, the method is rather successful in 
separating native and non-native learner texts. 
The Equal Error Rate is about 10%. However, 
for other domains, such as internet texts, 
separate classifiers have to be trained on the 
basis of suitable seed corpora. 
1 Introduction 
Research in linguistics and language engineering 
thrives on the availability of data. Traditionally, 
corpora would be compiled with a specific purpose 
in mind. Such corpora characteristically were well-
balanced collections of data. In the form of 
metadata, record was kept of the design criteria, 
sampling procedures, etc. Thus the researcher 
would have a fair idea of where his data originated 
from. Over past decades, data collection has been 
boosted by technological developments. More and 
more and increasingly large collections of data 
have been and are being compiled. It is tempting to 
think that the problem of data sparseness has been 
solved ? at least for raw data or data without any 
annotation other than can be provided fully 
automatically ? especially now that large amounts 
of data can be accessed through the internet. 
However, with data coming to us from all over the 
world, originating from all sorts of sources, we 
now possibly have a new problem on our hands: 
often the origins of the found data remain obscure. 
It is not always clear what exactly the 
implications for our research are of employing data 
whose origin we do not know. Is it legal to use 
these data, ethical, appropriate, ?? In this paper 
we will focus on the last point: the appropriateness 
of the data in the light of a specific application or 
research goal. More in particular, we will 
investigate to what extent we can devise a 
procedure that will enable us to identify texts 
produced by native speakers of the language (and 
thus by default those produced by non-native 
speakers). The present study is motivated by the 
fact that for many uses the (near-)nativeness of the 
data is a critical factor in the development of 
adequate resources and applications. Thus, for 
example, a style checker or some other writing 
assistant tool which has been based on erroneous 
materials or at least materials deviant from the 
language targeted, will not always respond 
appropriately. 
1.1 Assessing (near-)nativeness 
In the general absence of metadata which attest 
that texts have been produced by native speakers, 
there is one obvious approach that one may 
consider in order to assess the (near-)nativeness of 
texts of unknown origin and that is to exploit their 
specific linguistic characteristics.  
Previous studies investigating language variation 
(eg Biber, 1995, 1998; Biber et al, 1998; Conrad 
and Biber, 2001; Granger, 1998, Granger et al, 
2002) have shown that language use in different 
genres and by different (groups of) speakers 
displays characteristic use of specific linguistic 
features (lexical, morphological, syntactic, 
semantic, discoursal). These studies are all based 
on data of known origin. In the present study, we 
take a somewhat different approach as we aim to 
profile texts of unknown origin and identify native 
vs non-native language use, a task for which we 
coined the term language verification.  
1.2 Non-native language use 
Texts produced by non-native speakers will 
generally pass superficial inspection, i.e. they are 
deemed to be texts in the target language and will 
be treated as such. However, on closer inspection 
there is a wide range of features in the language 
use of non-natives which may have a disruptive 
effect on for instance derived language models. It 
is important to realize that non-native use is the 
complex result of different processes and 
conditions. First of all, there is the level of 
achievement. A non-native user gradually 
developes language skills in the target language. 
As he/she masters certain lexical items or morpho-
syntactic structures and feels confident in using 
them, certain items and structures are bound to be 
overused. At the same time, other items and 
structures remain underused as the user avoids 
them since he is not familiar with them or does not 
(yet) feel confident enough to employ them. 
Moreover, even for speakers who have attained a 
relatively high degree of proficiency, the influence 
of the native language remains. This may lead to 
transfer effects and interference (the effects of 
which are found, for example, in the use of false 
friends and word order deviations).  
 
In the present paper, we report the results 
obtained in some experiments that were carried out 
and which aimed to assess whether texts are of 
(British English) native or non-native origin using 
the method of linguistic profiling. The structure of 
the paper is as follows: In section 2, we describe 
the method of linguistic profiling. Next, in section 
3, its application in establishing the nativeness of 
texts is described, while in section 4 it is 
investigated whether the approach holds up when 
we shift from one domain to another. Finally, 
section 5 presents the conclusions. 
2 Linguistic profiling 
In linguistic profiling, the occurrences in a text 
are counted of a large number of linguistic 
features, either individual items or combinations of 
items. These counts are then normalized for text 
length and it is determined to what extent 
(calculated on the basis of the number of standard 
deviations) they differ from the mean observed in a 
profile reference corpus. For each text, the 
deviation scores are combined into a profile vector, 
on which a variety of distance measures can be 
used to position the text relative to any group of 
other texts. 
2.1 Language verification 
Linguistic profiling makes it possible to identify 
(groups of) texts which are similar, at least similar 
in terms of the profiled features (cf. van Halteren, 
2004). We have found that the recognition process 
can be vastly improved by not only providing 
positive examples (in the present case native texts) 
but also negative examples (here the non-native 
texts). So we expect that, given a seed corpus 
containing both native and non-native texts, 
linguistic profiling should be able to distinguish 
between these two types of texts. 
2.2 Features 
As previous research has shown (see e.g Biber 
1995), there are a great many linguistic features 
that contribute to marked structural differences 
between texts. These features mark ?basic 
grammatical, discourse, and communicative 
functions? (Biber, 1995: 104). They comprise 
features referring to vocabulary, lexical patterning, 
syntax, semantics, pragmatics, information content 
or item distribution through a text. Here we restrict 
ourselves to lexical features. 
Sufficiently frequent tokens, i.e. those that were 
observed to occur with a certain frequency in some 
language reference corpus, are used as features by 
themselves. In the present case these are ?tems that 
occur at least five times in the written texts from 
the BNC Sampler (BNC, 2002). For less frequent 
tokens, we determine a token pattern consisting of 
the sequence of character types. For example, the 
token Uefa-cup is represented by the pattern 
?#L#6+/CL-L?, where the first ?L? indicates low 
frequency, 6+ the size bracket, and the sequence 
?CL-L? a capital letter followed by one or more 
lower case letters followed by a hyphen and again 
one or more lower case letters. For lower case 
words, the final three letters of the word are also 
included in the pattern. For example, the token 
altercation is represented by the pattern 
?#L#6+/L/ion?. These patterns were originally 
designed for English and Dutch and will probably 
have to be extended for use with other languages. 
Furthermore, for this specific task, we wanted to 
avoid recognizing text topics rather than 
nativeness, and decided to mask content words. 
Any high frequency word classified primarily as 
noun, verb or adjective (see below), which had a 
high document bias (cf. van Halteren, 2003) was 
replaced by the marker #HC# followed by the 
same type of pattern we use for low frequency 
words, but always without the final three letters. 
This occludes topical words like brain or injury, 
while leaving more functional words like case or 
times intact. 
In addition to the form of the token, we also use 
the syntactic potential of the token as a feature. We 
apply the first few modules of a morphosyntactic 
tagger (in this case the tagger described by van 
Halteren, 2000) to the text, which determine which 
word class tags could apply to each token. For 
known words, the tags are taken from a lexicon; 
for unknown words, they are estimated on the basis 
of the word patterns described above. The most 
likely tags (with a maximum of three) are 
combined into a single feature. Thus still is 
associated with the feature ?RR-JJ-NN1? and 
forms with the feature ?NN2-VVZ?. Note that the 
most likely tags are determined exclusively on the 
basis of the current token; the context in which the 
token occurs is not taken into account. The 
modules of the tagger which are normally used to 
obtain a context dependent disambiguation are not 
applied. 
On top of the individual token and tag features 
we use all possible bi- and trigrams. For example, 
the token combination an attractive option is 
associated with the complex feature ?wcw= 
#HF#an#HC#JJ#HC#6+/L?. Since the number of 
features quickly grows too big to allow for 
efficient processing, we filter the set of features. 
This done by requiring that a feature occur in a set 
minimum number of texts in the profile reference 
corpus (in the present case a feature must occur in 
at least two texts). A feature which is filtered out 
contributes to a rest category feature. Thus, the 
complex feature above would contribute to 
?wcw=<OTHER>?. 
The lexical features currently also include 
features that relate to utterance length. For each 
utterance two such features are determined, viz. the 
exact length (e.g. ?len=15?) and the length bracket 
(e.g. ?len=10-19?). 
2.3 Classification 
When offered a list of positive and negative texts 
for training, and a list of test texts, the system first 
constructs a featurewise average of the profile 
vectors of all positive texts. It then determines a 
raw score for all text samples in the list. Rather 
than using the normal distance measure, we opted 
for a non-symmetric measure which is a weighted 
combination of two factors: a) the difference 
between text score and average profile score for 
each feature and b) the text score by itself. This 
makes it possible to assign more importance to 
features whose count deviates significantly from 
the norm. The following distance formula is used: 
?T = (? |Ti?Ai| D  |Ti| S) 1/(D+S) 
In this formula, Ti and Ai are the values for the ith 
feature for the text sample profile and the positive 
average profile respectively, and D and S are the 
weighting factors that can be used to assign more 
or less importance to the two factors described. 
The distance measure is then transformed into a 
score by the formula 
ScoreT = (? |Ti|(D+S)) 1/(D+S)   ?  ?T 
The score will grow with the similarity between 
text sample profile and positive average profile. 
The first component serves as a correction factor 
for the length of the text sample profile vector. 
The order of magnitude of the score values 
varies with the setting of D and S, and with the text 
collection. In order to bring the values into a range 
which is suitable for subsequent calculations, we 
express them as the number of standard deviations 
they differ from the mean of the scores of the 
negative example texts. 
3 Language verification 
In order to test the feasibility of language 
verification by way of linguistic profiling, we need 
data which is guaranteed to be written by native 
and non-native speakers respectively. Moreover, 
the texts (native and non-native) should be as 
similar as possible with respect to the genre they 
represent. For the present study, therefore, we 
opted for the student essays in the Louvain Corpus 
of Native English Essays (LOCNESS) and the 
International Corpus of Learner English (ICLE; 
Granger et al, 2002). 
3.1 LOCNESS and ICLE 
ICLE is a collection of mostly argumentative 
essays written by advanced EFL students from 
various mother-tongue backgrounds. The essays 
each are some 500-1000 words long (unabridged) 
and although they ?cover a variety of topics, the 
content is similar in so far as the topics are all non-
technical and argumentative (rather than narrative, 
for instance)? (cf. Granger, 1998:10). The size of 
the national sub-corpora is approx. 200,000 words 
per corpus. With the data metadata are available as 
they have been collected via a learner profile 
questionnaire. 
The LOCNESS in various respects is 
comparable to ICLE. It is a 300,000-word corpus 
mainly of essays written by English and American 
university students. A small part of the corpus 
(60,000 odd words) is constituted by British 
English A-level essays. Topics include transport, 
the parliamentary system, fox hunting, boxing, the 
National Lottery, and genetic engineering. 
3.2 Training and test texts 
In order to be able to control for language 
variation between British and American English, 
we opted for only the British part of LOCNESS. 
Because this totalled only some 155,000 words, we 
decided to hold out about one third as test material 
and use the other two thirds for training. In order to 
have as little overlap as possible in essay type and 
topic between training and test material, we used 
sub-corpora 2, 3 and 8 of the A-level essays and 
sub-corpus 3 of the university student essays for 
testing. 
For the ICLE texts, we chose to use each tenth 
text for training purposes. The remaining texts 
were used for testing.  
3.3 General results 
In the first step of training, we selected the 
features to be profiled. We used all features which 
occurred in more than one training text, i.e. about 
470K features. In the second step, we selected the 
system parameters D and S for two classification 
models: similarity to the native texts (D=1.0, 
S=0.0) and similarity to the non-native texts 
(D=1.2, S=0.2). The selection was based on the 
quality of classifying half of the training texts with 
the system having been trained on the other half. 
The verification results for the test set of A-level 
texts are shown in Figure 1. The further the texts 
are plotted to the right, the more similar their 
profile is to the mean profile for the A-level 
training texts. The further the texts are plotted 
towards the top, the more similar their profile is to 
the mean profile for the ICLE training texts.  
Most of the texts form a central cluster in the 
bottom right quadrant. A small gap separates them 
from a group of five near outliers, while there are 
two far outliers. We decided to use the limits of the 
central cluster as our classification separator, 
accepting that 10% of the LOCNESS texts would 
be rejected. We added the separation line to the 
plot. In order to create a reference frame linking 
this figure to the following ones, we add a second 
line, along the core of the cluster of the LOCNESS 
texts. Even though the core of the clusters in the 
successive figures may shift, this line remains 
constant, as does the plotting area.  
 
 
 
Figure 1. Text classification of the LOCNESS 
test texts in terms of similarity to native texts 
(horizontal axis) and similarity to non-native 
text (vertical axis). The separation line (top 
right to bottom left) divides the plot area in a 
native part (bottom right) and a non-native 
part (top left). The second line (top left to 
bottom right) is a reference line which 
allows comparison between this Figure and 
Figures 2-4. 
Figure 2. Text classification of the ICLE test 
texts 
Figure 2 shows the results for the ICLE test 
texts. 89% of the texts are rejected. The 
verification results differ per nationality. A more 
detailed examination of such variation, however, is 
beyond the scope of the present paper. 
The two dimensions, the degree of similarity to 
native texts and the degree of similarity to non-
native texts, are strongly (negatively) correlated. 
Still, there are also clear differences, so that both 
dimensions contribute substantially to the quality 
of the separation. 
3.4 Distinguishing features 
When examining some of the features that 
emerge from studies reported in the literature as 
salient in describing different language varieties, 
we find that none of these dominates the 
classification. Table 1 shows the influence of each 
feature in terms of its contribution (expressed as 
millionths of the total influence, so e.g. 3173 
corresponds to 0.3% of the total influence) to the 
decision to classify a text as native or non-native. 
The second and third column show the influence of 
the words (or word combinations) by themselves, 
which is extremely low. However, when 
examining all patterns containing these words, the 
fourth and fifth columns, their usefulness becomes 
visible.  
Previous studies into the use of intensifying 
adverbs have shown an overuse of the token very. 
Thus it is a likely candidate to be considered as a 
marker of non-native language use. The second 
column in the Table confirms this, but the 
contribution is a mere 0.001%. The picture 
changes when we consider all patterns in which 
very occurs, it appears that there is indeed a 
difference in use of the token by natives and non-
natives. However, there are as many patterns that 
point to nativeness as there are that point to non-
nativeness. Furthermore, the patterns provide a 
sizeable contribution in the classification either 
way. 
 
Word(s) Sep 
? 
ICLE 
Sep 
? 
LOC
Patterns 
? 
ICLE 
Patterns 
? 
LOCNESS
if 
   If  
   if 
 
13 
 
 
4
3931 4529
because 4 - 3230 2925
very 10 - 2860 3173
however - 1 686 644
therefore - 10 953 734
for instance 4 - 30 32
thus 2 - 411 287
yet 4 - 606 349
Table 1. Relative contribution to the overall 
classification of allegedly salient features 
Although the expected features (or rather 
features related to expected word or word 
combinations) have a visible contribution, their 
influence is still only a small part of the total 
influence. In fact, all features have only very little 
influence. The most influential single feature is 
ccc=#HF#AT--#HF#NN1--#HF#CC?RRx13, one 
of the representations of the, followed by a single 
common noun, followed by and, a pattern unlikely 
to be spotted by humans. It contributes 0.06% of 
the influence classifying texts as non-native. Only 
137 features in total contribute more than 0.01% 
either way. Classification by linguistic profiling is 
a matter of myriads of small hints rather than a few 
pieces of strong evidence. This is probably also 
what makes it robust against high text variability 
and sometimes small text sizes.  
4 Domain Shifts 
Now that we have seen that language 
verification is viable within the restricted domain 
of student essays, we may examine whether it 
survives the shift to a new domain. We tested this 
on two corpora: the FLOB corpus and (small) 
internet corpus that was especially collected for 
this purpose. 
4.1 FLOB 
The Freiburg LOB Corpus, informally known as 
FLOB (Hundt et al, 1998) is a modern counterpart 
to the much used Lancaster-Oslo/Bergen Corpus 
(LOB; Johansson, 1978) It is a one-million word 
corpus of written (educated) Modern British 
English. The composition of FLOB is essentially 
the same as that of LOB: it comprises 500 samples 
of 2,000 words each. In all, 15 text categories (A-
R) are distinguished. These fall into four main 
classes: newspaper text (A-C), miscellaneous 
informative prose (D-H), learned and scientific 
English (J), and fiction (K-R). 
 
 
Figure 3. Text classification of the FLOB 
learned and scientific texts (category J) 
 
Figure 4. Text classification of the FLOB non-
fiction texts (categories A-J) 
Of these texts, the learned and scientific class (J) 
is closest to the ICLE and LOCNESS texts, and we 
should expect that the FLOB texts of this category 
are all accepted. This is indeed the case, as can be 
seen in Figure 3, which shows the classification of 
these texts. Only 1 text is rejected (1.25%). This 
seems to confirm that we are indeed recognizing 
something like ?(near-)native English?.  
 
As soon as we shift the domain of the texts, 
however, the native texts are no longer 
distinguished as clearly. The larger the domain 
shift, the more texts are rejected. Within the non-
fiction portion of FLOB, the system rejects 2.3% 
of the newspaper texts (categories A-C) and 8.7% 
of the miscellaneous and informative prose texts 
(D-H). This leads to an overall reject rate of 5.6% 
for the non-fiction texts (Figure 4), which is still 
reasonably acceptable. When shifting to fiction 
texts (K-R), the reject rate jumps to 39.2% (Figure 
5), indicating that a new classifier would have to 
be trained for a proper handling of fiction texts. 
 
Figure 5. Text classification of the FLOB 
fiction texts (categories K-R) 
4.2 Capital-Born 
Since our original goal was the filtering of 
internet texts, we compiled a small corpus of such 
texts. We chose texts which were present as 
HTML. These, we expected, were likely to be 
rather abundant, while they would have been 
subjected to a relatively low degree of editing. 
Thus they would constitute likely candidates for 
filtering. In order to be able to decide whether the 
texts were native-written or not, we searched 
autobiographical material, as indicated by the 
phrase I was born in CITY, with CITY replaced by 
a name of a capital city. The initial set of 
documents appeared to be of a reasonable size. 
However, after filtering out webpages by multiple 
authors (e.g. guest books), fictional 
autobiographies (e.g. a joke page about Al Gore), 
texts judged likely to be edited possibly with the 
help of a native speaker (e.g. a page advertising 
Russian brides), misclassified city names (e.g. 
authors from Paris, Texas should not be assumed 
to be French) and texts outside the desired length 
of 500-1500 words, we ended up with a mere 20 
native British English texts and 18 non-native 
texts. We nicknamed the corpus ?Capital-Born 
corpus?. 
When classifying these texts with the A-level 
versus ICLE classifier, we see that they cluster 
tightly, outside the area plotted so far, and showing 
no useful separation of native and non-native texts. 
This implies that if we want a filter for such texts, 
we have to train a new classifier. 
 
Figure 6. Text classification of internet texts 
(for a description see section 4.2) 
We did train such a new classifier, using only the 
odd-numbered Capital-Born texts and classified 
the even-numbered ones, using the same 
parameters D and S as above. We repeated the 
process with the two sets switching roles. Figure 6 
shows a superposition of the classifications in the 
two experiments. The native texts appear as plus 
signs (+), the non-native texts as minus signs (?). 
Note that we adjusted the separation and support 
lines in order to bring them in line with the data. 
Only a rough separation is visible, with 2 out of 20 
native texts misclassified and 6 out of 18 non-
native texts. Still, given the extremely small size of 
the training sets and the variety of non-native 
nationalities, these results are rather promising. It 
appears that even internet texts can be filtered for 
nativeness, as long as a restricted, and more 
sizeable, seed corpus can be constructed. 
5 Conclusion 
The results show that language verification is 
indeed possible, as long as we accept that near-
native texts produced by non-natives will not be 
filtered out.  
Furthermore, whenever a verification filter is 
needed, it will be necessary to create a new filter, 
based on a seed corpus which contains both native 
and non-native texts as similar as possible in type 
to the texts which are to be filtered. 
There are now two avenues open for future 
research. First of all, we would like to explore the 
classification procedure linguistically: a) examine 
the distinguishing features in more detail and 
compare our findings with those in the literature, 
and b) examine the correlation of the nativeness 
score of the various texts to extra-linguistic text 
variables such as mother tongue and learner level. 
Secondly, once more insight is gained into the 
linguistic workings of the procedure, the 
classification process can be refined. At this point, 
we would also like to examine the effects of 
domain shift in more detail, and attempt to 
estimate a minimum size for seed corpora for use 
in filtering internet material. 
6 Acknowledgements 
Thanks are due to Sylviane Granger and Sylvie 
De Cock (Centre for English Corpus Linguistics, 
Universit? Catholique de Louvain, Belgium) for 
making the LOCNESS and ICLE data available to 
us. 
References  
Douglas Biber. 1995. Dimensions of register 
variation. A cross-linguistic comparison. 
Cambridge: Cambridge University Press. 
 
Douglas Biber 1998. Variation across Speech and 
Writing. Cambridge: Cambridge University 
Press. 
Douglas Biber, Susan Conrad and Randi Reppen. 
1998. Corpus Linguistics: Investigating 
language structure and use. Cambridge: 
Cambridge University Press. 
BNC. 2002. The BNC sampler. Web page: 
www.natcorp.ox.ac.uk/getting/sampler.html 
Susan Conrad and Douglas Biber (eds.) 2001. 
Variation in English: Multi-dimensional 
studies. Harlow, England: Longman. 
Alan Davies. 2003. The Native Speaker: Myth and 
Reality. Clevedon: Multimingual Matters Ltd. 
Sylviane Granger (ed.) 1998. Learner English on 
Computer. London and New York: Longman. 
Sylviane Granger. 1998. The computer learner 
corpus. In Sylviane Granger (ed.): 3-18. 
Sylviane Granger, Joseph Hung, and Stephanie 
Petch-Tyson (eds.) 2002. Computer Learner 
Corpora, Second Language Acquisition and 
Foreign Language Teaching. Amsterdam: 
Benjamins. 
Sylviane Granger, E. Dagneaux, and Fanny 
Meunier (eds.) 2002. International Corpus of 
Learner English. Louvain: UCL Presses 
Universitaires de Louvain. 
Hans van Halteren. 2000. The detection of 
inconsistencies in manually tagged text. Proc. 
Workshop on Linguistically Interpreted 
Corpora (LINC2000). 48-55. 
Hans van Halteren. 2003. New feature sets for 
summarization by sentence extraction. IEEE 
Intelligent Systems, July/August 2003: 34-42. 
Hans van Halteren. 2004. Linguistic profiling for 
author recognition and verification. Proc. ACL 
2004. 
Marianne Hundt, Andrea Sandt and Rainer 
Siemund. 1998. Flobman. Manual of 
Information to accompany the Freiburg-LOB 
Corpus of British English (?FLOB?). Freiburg: 
Englisches Seminar. 
Stig Johansson with Geoffrey Leech and Helen 
Goodluck. 1978. Manual of Information to 
Accompany the Lancaster-Oslo/Bergen Corpus 
of British English, for Use with Digital 
Computers. Oslo: Dept. of English, University of 
Oslo. 
M. van der Laaken, R. Lankamp, and Michael 
Sharwood Smith. 1997. Writing Better English. 
Bussum: Coutinho. 
LOCNESS. 
http://juppiter.fltr.ucl.ac.be/FLTR/GERM/ETAN
/CECL/Cecl-Projects/Icle/LOCNESS.htm 
Improving Accuracy in Word Class 
Tagging through the Combination of 
Machine Learning Systems 
Hans  van  Halteren* 
TOSCA/Language & Speech, 
University of Nijmegen 
Walter Daelemans~ 
CNTS/Language Technology Group, 
University of Antwerp 
Jakub Zavrel t 
Textkernel BV, 
University of Antwerp 
We examine how differences in language models, learned by different data-driven systems per- 
forming the same NLP task, can be exploited to yield a higher accuracy than the best individual 
system. We do this by means of experiments involving the task of morphosyntactic word class 
tagging, on the basis of three different tagged corpora. Four well-known tagger generators (hidden 
Markov model, memory-based, transformation rules, and maximum entropy) are trained on the 
same corpus data. After comparison, their outputs are combined using several voting strategies 
and second-stage classifiers. All combination taggers outperform their best component. The re- 
duction in error rate varies with the material in question, but can be as high as 24.3% with the 
LOB corpus. 
1. Introduction 
In all natural anguage processing (NLP) systems, we find one or more language 
models that are used to predict, classify, or interpret language-related observations. 
Because most real-world NLP tasks require something that approaches full language 
understanding in order to be perfect, but automatic systems only have access to limited 
(and often superficial) information, as well as limited resources for reasoning with that 
information, such language models tend to make errors when the system is tested on 
new material. The engineering task in NLP is to design systems that make as few errors 
as possible with as little effort as possible. Common ways to reduce the error rate are to 
devise better epresentations of the problem, to spend more time on encoding language 
knowledge (in the case of hand-crafted systems), or to find more training data (in the 
case of data-driven systems). However, given limited resources, these options are not 
always available. 
Rather than devising a new representation forour task, in this paper, we combine 
different systems employing known representations. The observation that suggests 
this approach is that systems that are designed ifferently, either because they use a 
different formalism or because they contain different knowledge, will typically produce 
different errors. We hope to make use of this fact and reduce the number of errors with 
* P.O. Box 9103, 6500 HD Nijmegen, The Netherlands. E-mail: hvh@let.kun.nl. 
t Universiteitsplein 1, 2610 Wilrijk, Belgium. E-mail: zavrel@textkerneLnl. 
:~ Universiteitsplein 1, 2610 Wilrijk, Belgium. E-mail: daelem@uia.ua.ac.be. 
Q 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 2 
very little additional effort by exploiting the disagreement between different language 
models. Although the approach is applicable to any type of language model, we focus 
on the case of statistical disambiguators that are trained on annotated corpora. The 
examples of the task that are present in the corpus and its annotation are fed into a 
learning algorithm, which induces amodel of the desired input-output mapping in the 
form of a classifier. We use a number of different learning algorithms simultaneously 
on the same training corpus. Each type of learning method brings its own "inductive 
bias" to the task and will produce a classifier with slightly different characteristics, so 
that different methods will tend to produce different errors. 
We investigate two ways of exploiting these differences. First, we make use of 
the gang effect. Simply by using more than one classifier, and voting between their 
outputs, we expect to eliminate the quirks, and hence errors, that are due to the 
bias of one particular learner. However, there is also a way to make better use of 
the differences: we can create an arbiter effect. We can train a second-level c assifier 
to select its output on the basis of the patterns of co-occurrence of the outputs of 
the various classifiers. In this way, we not only counter the bias of each component, 
but actually exploit it in the identification of the correct output. This method even 
admits the possibility of correcting collective rrors. The hypothesis that both types 
of approaches can yield a more accurate model from the same training data than the 
most accurate component of the combination, and that given enough training data the 
arbiter type of method will be able to outperform the gang type. 1 
In the machine learning literature there has been much interest recently in the the- 
oretical aspects of classifier combination, both of the gang effect ype and of the arbiter 
type (see Section 2). In general, it has been shown that, when the errors are uncorre- 
lated to a sufficient degree, the resulting combined classifier will often perform better 
than any of the individual systems. In this paper we wish to take a more empirical 
approach and examine whether these methods result in substantial ccuracy improve- 
ments in a situation typical for statistical NLP, namely, learning morphosyntactic word 
class tagging (also known as part-of-speech or POS tagging) from an annotated corpus 
of several hundred thousand words. 
Morphosyntactic word class tagging entails the classification (tagging) of each 
token of a natural anguage text in terms of an element of a finite palette (tagset) of 
word class descriptors (tags). The reasons for this choice of task are several. First of 
all, tagging is a widely researched and well-understood task (see van Halteren \[1999\]). 
Second, current performance l vels on this task still leave room for improvement: 
"state-of-the-art" performance for data-driven automatic word class taggers on the 
usual type of material (e.g., tagging English text with single tags from a low-detail 
tagset) is at 96-97% correctly tagged words, but accuracy levels for specific classes 
of ambiguous words are much lower. Finally, a number of rather different methods 
that automatically generate a fully functional tagging system from annotated text are 
available off-the-shelf. First experiments (van Halteren, Zavrel, and Daelemans 1998; 
Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with 
the error rate of the best combiner being 19.1% lower than that of the best individual 
tagger (van Halteren, Zavrel, and Daelemans 1998). However, these experiments were 
restricted to a single language, a single tagset and, more importantly, a limited amount 
of training data for the combiners. This led us to perform further, more extensive, 
1 In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter 
half of the hypothesis unequivocally. As we judged this to be due to insufficient training data for 
proper training of the second-level classifiers, we greatly increase the amount of training data in the 
present work through the use of cross-validation. 
200 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
tagging experiments before moving on to other tasks. Since then the method has also 
been applied to other NLP tasks with good results (see Section 6). 
In the remaining sections, we first introduce classifier combination on the basis of 
previous work in the machine learning literature and present he combination meth- 
ods we use in our experiments (Section 2). Then we explain our experimental setup 
(Section 3), also describing the corpora (3.1) and tagger generators (3.2) used in the 
experiments. In Section 4, we go on to report the overall results of the experiments, 
starting with a comparison between the component taggers (and hence between the 
underlying tagger generators) and continuing with a comparison of the combination 
methods. The results are examined in more detail in Section 5, where we discuss uch 
aspects as accuracy on specific words or tags, the influence of inconsistent training 
data, training set size, the contribution of individual component taggers, and tagset 
granularity. In Section 6, we discuss the results in the light of related work, after 
which we conclude (Section 7) with a summary of the most important observations 
and interesting directions for future research. 
2. Combination Methods 
In recent years there has been an explosion of research in machine learning on finding 
ways to improve the accuracy of supervised classifier learning methods. An important 
finding is that a set of classifiers whose individual decisions are combined in some 
way (an ensemble) can be more accurate than any of its component classifiers if the 
errors of the individual classifiers are sufficiently uncorrelated (see Dietterich \[1997\], 
Chan, Stolfo, and Wolpert \[1999\] for overviews). There are several ways in which an 
ensemble can be created, both in the selection of the individual classifiers and in the 
way they are combined. 
One way to create multiple classifiers is to use subsamples of the training exam- 
ples. In bagging, the training set for each individual classifier is created by randomly 
drawing training examples with replacement from the initial training set (Breiman 
1996a). In boosting, the errors made by a classifier learned from a training set are 
used to construct a new training set in which the misclassified examples get more 
weight. By sequentially performing this operation, an ensemble is constructed (e.g., 
ADABOOST, \[Freund and Schapire 1996\]). This class of methods is also called arcing 
(for adaptive resampling and combining). In general, boosting obtains better results 
than bagging, except when the data is noisy (Dietterich 1997). Another way to cre- 
ate multiple classifiers is to train classifiers on different sources of information about 
the task by giving them access to different subsets of the available input features 
(Cherkauer 1996). Still other ways are to represent the output classes as bit strings 
where each bit is predicted by a different component classifier (error correcting output 
coding \[Dietterich and Bakiri 1995\]) or to develop learning-method-specific methods 
for ensuring (random) variation in the way the different classifiers of an ensemble are 
constructed (Dietterich 1997). 
In this paper we take a multistrategy approach, in which an ensemble is con- 
structed by classifiers resulting from training different learning methods on the same 
data (see also Alpaydin \[1998\]). 
Methods to combine the outputs of component classifiers in an ensemble include 
simple voting where each component classifier gets an equal vote, and weighted 
voting, in which each component classifier's vote is weighted by its accuracy (see, for 
example, Golding and Roth \[1999\]). More sophisticated weighting methods have been 
designed as well. Ali and Pazzani (1996) apply the Naive Bayes' algorithm to learn 
weights for classifiers. Voting methods lead to the gang effect discussed earlier. The 
201 
Computational Linguistics Volume 27, Number 2 
Let Ti be the component taggers, Si(tok) the most probable tag for a token tok as suggested by Ti, and let 
the quality of tagger Ti be measured by 
? the precision of Ti for tag tag: Prec(Ti, tag) 
? the recall of T i for tag tag: Rec(Ti, tag) 
? the overall precision of Ti: Prec(Ti) 
Then the vote V(tag, tok) for tagging token tok with tag tag is given by: 
? Majority: 
? TotPrecision: 
? TagPrecision: 
? Precision-Recall: 
~_IF  Si(tok)= tag THEN 1 ELSE 0 
i 
~_ IF  Si(tok)= tag THEN Prec(Ti) ELSE 0 
i 
~__IF Si(tok ) = tag THEN Prec(Ti, tag) ELSE 0 
i 
~_.IF Si(tok ) =tag THEN Prec(Ti, tag) ELSE 1-Rec(Ti, tag) 
i 
Figure 1 
Simple algorithms for voting between component taggers. 
most interesting approach to combination is stacking in which a classifier is trained 
to predict the correct output class when given as input the outputs of the ensemble 
classifiers, and possibly additional information (Wolpert 1992; Breiman 1996b; Ting and 
Witten 1997a, 1997b). Stacking can lead to an arbiter effect. In this paper we compare 
voting and stacking approaches on the tagging problem. 
In the remainder of this section, we describe the combination methods we use 
in our experiments. We start with variations based on weighted voting. Then we go 
on to several types of stacked classifiers, which model the disagreement situations 
observed in the training data in more detail. The input to the second-stage classifier 
can be limited to the first-level outputs or can contain additional information from the 
original input pattern. We will consider a number of different second-level learners. 
Apart from using three well-known machine learning methods, memory-based learn- 
ing, maximum entropy, and decision trees, we also introduce a new method, based on 
grouped voting. 
2.1 Simple Voting 
The most straightforward method to combine the results of multiple taggers is to do 
an n-way vote. Each tagger is allowed to vote for the tag of its choice, and the tag with 
the highest number of votes is selected. 2 The question is how large a vote we allow 
each tagger (Figure 1). The most democratic option is to give each tagger one vote 
(Majority). This does not require any tuning of the voting mechanism on training data. 
However, the component taggers can be distinguished by several figures of merit, 
and it appears more useful to give more weight to taggers that have proved their 
quality. For this purpose we use precision and recall, two well-known measures, which 
2 In all our experiments, any ties are resolved by a random selection from among the winning tags. 
202 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
can be applied to the evaluation of tagger output as well. For any tag X, precision 
measures which percentage of the tokens tagged X by the tagger are also tagged X 
in the benchmark. Recall measures which percentage of the tokens tagged X in the 
benchmark are also tagged X by the tagger. When abstracting away from individual 
tags, precision and recall are equal (at least if the tagger produces exactly one tag per 
token) and measure how many tokens are tagged correctly; in this case we also use 
the more generic term accuracy. We will call the voting method where each tagger is 
weighted by its general quality TotPrecision, i.e., each tagger votes its overall precision. 
To allow for more detailed interactions, each tagger is weighted by the quality in 
relation to the current situation, i.e., each tagger votes its precision on the tag it suggests 
(TagPrecision). This way, taggers that are accurate for a particular type of ambiguity 
can act as specialized experts. The information about each tagger's quality is derived 
from a cross-validation f its results on the combiner training set. The precise setup 
for deriving the training data is described in more detail below, in Section 3. 
We have access to even more information on how well the taggers perform. We 
not only know whether we should believe what they propose (precision) but know as 
well how often they fail to recognize the correct ag (1 - recall). This information can 
be used by forcing each tagger to add to the vote for tags suggested by the opposition 
too, by an amount equal to 1 minus the recall on the opposing tag (Precision-Recall). 
As an example, suppose that the MXPOST tagger suggests DT and the HMM tagger 
TnT suggests CS (two possible tags in the LOB tagset for the token that). If MXPOST 
has a precision on DT of 0.9658 and a recall on CS of 0.8927, and TnT has a precision on 
CS of 0.9044 and a recall on DT of 0.9767, then DT receives a 0.9658 + 0.0233 = 0.9991 
vote and CS a 0.9044 + 0.1073 = 1.0117 vote. 
Note that simple voting combiners can never return a tag that was not suggested 
by a (weighted) majority of the component taggers. As a result, they are restricted 
to the combination of taggers that all use the same tagset. This is not the case for 
all the following (arbiter type) combination methods, a fact which we have recently 
exploited in bootstrapping a word class tagger for a new corpus from existing taggers 
with completely different tagsets (Zavrel and Daelemans 2000). 
2.2 Stacked Probabilistic Voting 
One of the best methods for tagger combination i (van Halteren, Zavrel, and Daele- 
mans 1998) is the TagPair method. It looks at all situations where one tagger suggests 
tag 1 and the other tag 2 and estimates the probability that in this situation the tag 
should actually be tag x. Although it is presented as a variant of voting in that paper, 
it is in fact also a stacked classifier, because it does not necessarily select one of the 
tags suggested by the component taggers. Taking the same example as in the voting 
section above, if tagger MXPOST suggests DT and tagger TnT suggests CS, we find 
that the probabilities for the appropriate tag are: 
CS 
CS22 
DT 
QL 
WPR 
subordinating conjunction 0.4623 
second half of a two-token subordinating conjunction, e.g., so that 0.0171 
determiner 0.4966 
quantifier 0.0103 
wh-pronoun 0.0137 
When combining the taggers, every tagger pair is taken in turn and allowed to 
vote (with a weight equal to the probability P(tag x I tag1, tag2) as described above) 
for each possible tag (Figure 2). If a tag pair tagl-tag 2 has never been observed in the 
training data, we fall back on information on the individual taggers, i.e., P(tag x I tag1) 
203 
Computational Linguistics Volume 27, Number 2 
Let Ti be the component taggers and Si(tok) the most probable tag for a token tok as suggested by Ti. Then 
the vote V(tag, tok) for tagging token tok with tag tag is given by: 
V(tag, tok) = ~_~ Vi,j(tag , tok) 
i,jliKj 
where Vial(tag, tok) is given by 
IF frequency(Si(tokx) = Si(tok), Sj(tokx) = Sj(tok) ) > 0 
THEN Vial(tag, tok) = P(tag l Si(tok~) = Si(tok), Sj(tokx) = Sj(tok) ) 
Vi,j(tag, tok) = ~P(tag \] Si(tokx) = Si(tok) ) + ~P(tag I Sj(tokx) = Sj(tok) ) ELSE 
Figure 2 
The TagPair algorithm for voting between component taggers. 
If the case to be classified corresponds to the feature-value pair set 
Fcase = {0Cl = V l}  . . . . .  {fn = Vn}} 
then estimate the probability of each class Cx for Fcase as a weighted sum over all possible subsets Fsu b of 
Fcase: 
~(Cx) ~_, Wr~.bP(CK I Esub) 
Fsub C Fcase 
with the weight WG, b for an Fsub containing n elements equal to n---L-r' where Wnorm is a normalizing Wnor m i 
constant so that ~cx P(Cx) = 1. 
Figure 3 
The Weighted Probability Distribution Voting classification algorithm, as used in the 
combination experiments. 
and P(tag x I tag2). Note that with this method (and all of the following), a tag suggested 
by a minority (or even none) of the taggers actually has a chance to win, although in 
practice the chance to beat a majority is still very slight. 
Seeing the success of TagPair in the earlier experiments, we decided to try to 
generalize this stacked probabilistic voting approach to combinations larger than pairs. 
Among other things, this would let us include word and context features here as well. 
The method that was eventually developed we have called Weighted Probability 
Distribution Voting (henceforth WPDV). 
A WPDV classification model  is not l imited to pairs of features (such as the pairs 
of tagger outputs for TagPair), but can use the probabil ity distributions for all feature 
combinations observed in the training data (Figure 3). During voting, we do not use a 
fallback strategy (as TagPair does) but use weights to prevent he lower-order combi- 
nations from excessively influencing the final results when a higher-order combination 
(i.e., more exact information) is present. The original system, as used for this paper, 
weights a combination of order n with a factor n!, a number  based on the observation 
that a combination of order m contains m combinations of order (m - 1) that have to 
be competed with. Its only parameter  is a threshold for the number  of times a combi- 
nation must  be observed in the training data in order to be used, which helps prevent 
a combinatorial explosion when there are too many atomic features. 3 
3 In our experiments, his parameter is always set to 5. WPDV has since evolved, using more parameters 
and more involved weighting schemes, and also been tested on tasks other than tagger combination 
(van Halteren 2000a, 2000b). 
204 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
? Tags uggested by the base taggers, used by all systems: 
TagTBL = JJ TagMBT = VBN TagMXP = VBD TagHMM = JJ 
? The focus token, used by stacked classifiers at level Tags+Word: 
Word = restored 
? Full form tags uggested by the base tagger for the previous and next oken, used by stacked 
classifiers at level Tags+Context, except for WPDV: 
PrevTBL = JJ P revMBT = NN PrevMXP = NN PrevHMM = J\] 
NextTBL  = NN NextMBT = NN NextMXP = NN NextHMM = NN 
? Compressed form of the context tags, used by WPDV(Tags+Context), because the system was 
unable to cope with the large number of features: 
Prev ~- JJ + NN + NN + JJ Next  = NN + NN + NN + NN 
? Target feature, used by all systems: 
Tag = VBD 
Figure 4 
Features used by the combination systems. Examples are taken from the LOB material. 
In contrast o voting, stacking classifiers allows the combination of the outputs of 
component systems with additional information about the decision's context. We in- 
vestigated several versions of this approach. In the basic version (Tags), each training 
case for the second-level learner consists of the tags suggested by the component tag- 
gers and the correct ag (Figure 4). In the more advanced versions, we add information 
about the word in question (Tags+Word) and the tags suggested by all taggers for the 
previous and the next position (Tags+Context). These types of extended second-level 
features can be exploited by WPDV, as well as by a wide selection of other machine 
learning algorithms. 
2.3 Memory-based Combination 
Our first choice from these other algorithms is a memory-based second-level learner, 
implemented in TiMBL (Daelemans et al 1999), a package developed at Tilburg Uni- 
versity and Antwerp University. 4 
Memory-based learning is a learning method that is based on storing all examples 
of a task in memory and then classifying new examples by similarity-based reasoning 
from these stored examples. Each example is represented by a fixed-length vector of 
feature values, called a case. If the case to be classified has been observed before, 
that is, if it is found among the stored cases (in the case base), the most frequent 
corresponding output is used. If the case is not found in the case base, k nearest 
neighbors are determined with some similarity metric, and the output is based on the 
observed outputs for those neighbors. Both the value of k and the similarity metric 
used can be selected by parameters of the system. For the Tags version, the similarity 
metric used is Overlap (a count of the number of matching feature values between a
test and a training item) and k is kept at 1. For the other two versions (Tags+Word and 
Tags+Context), a value of k = 3 is used, and each overlapping feature is weighted by 
its Information Gain (Daelemans, Van den Bosch, and Weijters 1997). The Information 
4 TiMBL is available from http://ilk.kub.nl/. 
205 
Computational Linguistics Volume 27, Number 2 
Gain of a feature is defined as the difference between the entropy of the a priori class 
distribution and the conditional entropy of the classes given the value of the feature. ~
2.4 Maximum Entropy Combination 
The second machine learning method, maximum entropy modeling, implemented in
the Maccent system (Dehaspe 1997), does the classification task by selecting the most 
probable class given a maximum entropy model. 6This type of model represents ex- 
amples of the task (Cases) as sets of binary indicator features, for the task at hand 
conjunctions of a particular tag and a particular set of feature values. The model has 
the form of an exponential model: 
1 e Y~i ~i~(ca~,~g) pA(tag l Case) -- Za(Case) 
where i indexes all the binary features, fi is a binary indicator function for feature i, 
ZA is a normalizing constant, and )~i is a weight for feature i. The model is trained by 
iteratively adding binary features with the largest gain in the probability of the train- 
ing data, and estimating the weights using a numerical optimization method called 
improved iterafive scaling. The model is constrained by the observed istribution of 
the features in the training data and has the property of having the maximum en- 
tropy of all models that fit the constraints, i.e., all distributions that are not directly 
constrained by the data are left as uniform as possible. 7 
The maximum entropy combiner takes the same information as the memory-based 
learner as input, but internally translates all multivalued features to binary indicator 
functions. The improved iterative scaling algorithm is then applied, with a maximum 
of one hundred iterations. This algorithm is the same as the one used in the MXPOST 
tagger described in Section 3.2.3, but without the beam search used in the tagging 
application. 
2.5 Decision Tree Combination 
The third machine learning method we used is c5.0 (Quinlan 1993), an example of 
top-down induction of decision trees. 8 A decision tree is constructed by recursively 
partitioning the training set, selecting, at each step, the feature that most reduces the 
uncertainty about the class in each partition, and using it as a split, c5.0 uses Gain 
Ratio as an estimate of the utility of splitting on a feature. Gain Ratio corresponds to 
the Information Gain measure of a feature, as described above, except hat the measure 
is normalized for the number of values of the feature, by dividing by the entropy of the 
feature's values. After the decision tree is constructed, it is pruned to avoid overfitting, 
using a method described in detail in Quinlan (1993). A classification for a test case 
is made by traversing the tree until either a leaf node is found or all further branches 
do not match the test case, and returning the most frequent class at the last node. The 
case representation uses exactly the same features as the memory-based learner. 
3. Experimental Setup 
In order to test the potential of system combination, we obviously need systems to 
combine, i.e., a number of different aggers. As we are primarily interested in the 
5 This is also sometimes referred to as mutual information i the computational linguistics literature. 
6 Maccent isavailable from http://www.cs.kuleuven.ac.be/~ldh. 
7 For a more detailed iscussion, see Berger, Della Pietra, and Della Pietra (1996) and Ratnaparkhi (1996). 
8 c5.0 is commercially available from http://www.rulequest.com/. Its predecessor, c4.5, can be 
downloaded from http://www.cse.unsw.edu.au/~quinlan/. 
206 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
combination of classifiers trained on the same data sets, we are in fact looking for 
data sets (in this case, tagged corpora) and systems that can automatically gener- 
ate a tagger on the basis of those data sets. For the current experiments, we have 
selected three tagged corpora and four tagger generators. Before giving a detailed 
description of each of these, we first describe how the ingredients are used in the 
experiments. 
Each corpus is used in the same way to test tagger and combiner performance. 
First of all, it is split into a 90% training set and a 10% test set. We can evaluate 
the base taggers by using the whole training set to train the tagger generators and 
the test set to test the resulting tagger. For the combiners, a more complex strategy 
must be followed, since combiner training must be done on material unseen by the 
base taggers involved. Rather than setting apart a fixed combiner training set, we use 
a ninefold training strategy? The 90% trai1~ing set is split into nine equal parts. Each 
part is tagged with component taggers that have been trained on the other eight parts. 
All results are then concatenated for use in combiner training, so that, in contrast o 
our earlier work, all of the training set is effectively available for the training of the 
combiner. Finally, the resulting combiners are tested on the test set. Since the test set 
is identical for all methods, we can compute the statistical significance of the results 
using McNemar's chi-squared test (Dietterich 1998). 
As we will see, the increase in combiner training set size (90% of the corpus versus 
the fixed 10% tune set in the earlier experiments) indeed results in better performance. 
On the other hand, the increased amount of data also increases time and space require- 
ments for some systems to such a degree that we had to exclude them from (some 
parts of) the experiments. 
The data in the training set is the only information used in tagger and combiner 
construction: all components of all taggers and combiners (lexicon, context statistics, 
etc.) are entirely data driven, and no manual adjustments are made. If any tagger or 
combiner construction method is parametrized, we use default settings where avail- 
able. If there is no default, we choose intuitively appropriate values without prelimi- 
nary testing. In these cases, we report such parameter settings in the introduction to 
the system. 
3.1 Data 
In the current experiments we make use of three corpora. The first is the LOB corpus 
(Johansson 1986), which we used in the earlier experiments as well (van Halteren, 
Zavrel, and Daelemans 1998) and which has proved to be a good testing ground. We 
then switch to Wall Street Journal material (WSJ), tagged with the Penn Treebank II 
tagset (Marcus, Santorini, and Marcinkiewicz 1993). Like LOB, it consists of approx- 
imately 1M words, but unlike LOB, it is American English. Furthermore, it is of a 
different structure (only newspaper text) and tagged with a rather different agset. 
The experiments with WSJ will also let us compare our results with those reported by 
Brill and Wu (1998), which show a much less pronounced accuracy increase than ours 
with LOB. The final corpus is the slightly smaller (750K words) Eindhoven corpus (Uit 
den Boogaart 1975) tagged with the Wotan tagset (Berghmans 1994). This will let us 
examine the tagging of a language other than English (namely, Dutch). Furthermore, 
the Wotan tagset is a very detailed one, so that the error rate of the individual taggers 
9 Compare this to the "tune" set in van Halteren, Zavrel, and Daelemans (1998). This consisted of 114K 
tokens, but, because of a 92.5% agreement over all four taggers, ityielded less than 9K tokens of useful 
training material to resolve disagreements. This was suspected tobe the main reason for the relative 
lack of performance by the more sophisticated combiners. 
207 
Computational Linguistics Volume 27, Number 2 
tends to be higher. Moreover,  we can more easily use projections of the tagset and 
thus s tudy the effects of levels of granularity. 
3.1.1 LOB. The first data set we use for our exper iments consists of the tagged 
Lancaster-Oslo/Bergen corpus (LOB \[ Johansson 1986\]). The corpus comprises about 
one mil l ion words  of British Engl ish text, d iv ided over 500 samples of 2,000 words  
f rom 15 text types. 
The tagging of the LOB corpus, wh ich  was manual ly  checked and corrected, is 
general ly accepted to be quite accurate. Here we use a slight adaptat ion of the tagset. 
The changes are main ly  cosmetic, e.g., nonalphabet ic  haracters uch as "$" in tag 
names have been replaced. However ,  there has also been some retokenization: genitive 
markers have been split off and the negative marker  n't has been reattached. 
An  example sentence tagged with the result ing tagset is: 
The ATI singular or plural article 
Lord NPT singular titular noun 
Major NPT singular titular noun 
extended VBD past tense of verb 
an AT singular article 
invitation NN singular common oun 
to IN preposition 
all ABN pre-quantifier 
the ATI singular or plural article 
parliamentary JJ adjective 
candidates NNS plural common oun 
SPER period 
The tagset consists of 170 different tags ( including ditto tags), and has an average 
ambigui ty  of 2.82 tags per  word form over the corpus. 1? An  impression of the difficulty 
of the tagging task can be gained f rom the two baseline measurements  in Table 2 (in 
Section 4.1 below), representing a completely random choice f rom the potential  tags 
for each token (Random) and selection of the lexically most  l ikely tag (LexProb)J 1 
The training/test  separat ion of the corpus is done at utterance boundar ies  (each 1st 
to 9th utterance is training and each 10th is test) and leads to a 1,046K token training 
set and a 115K token test set. A round 2.14% of the test set are tokens unseen in the 
training set and a further 0.37% are known tokens but  with unseen tags. 12 
3.1.2 WSJ. The second data set consists of 1M words  of Wall Street Journal material. 
It differs f rom LOB in that it is Amer ican Engl ish and, more importantly,  in that it is 
completely made up of newspaper  text. The material  is tagged with the Penn Treebank 
tagset (Marcus, Santorini, and Marcinkiewicz 1993), wh ich  is much smaller than the 
LOB one. It consists of only 48 tags. 13 There is no attempt to annotate compound 
words,  so there are no ditto tags. 
10 Ditto tags are used for the components ofmultitoken units, e.g. if as well as is taken to be a coordinating 
conjunction, it is tagged "as_CC-1 well_CC-2 as_CC-3", using three related but different ditto tags. 
11 These numbers are calculated on the basis of a lexicon derived from the whole corpus. An actual 
tagger will have to deal with unknown words in the test set, which will tend to increase the ambiguity 
and decrease Random and LexProb. Note that all actual taggers and combiners in this paper do have 
to cope with unknown words as their lexicons are based purely on their training sets. 
12 Because of the way in which the tagger generators treat their input, we do count okens as different 
even though they are the same underlying token, but differ in capitalization of one or more characters. 
13 In the material we have available, quotes are represented slightly differently, so that there are only 45 
different ags. In addition, the corpus contains a limited number of instances of 38 "indeterminate" 
tags, e.g., JJ\]VBD indicates a choice between adjective and past participle which cannot be decided or 
about which the annotator was unsure. 
208 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
An example sentence is: 
By IN preposition/subordinating conjunction 
10 CD cardinal number 
a.m. RB adverb 
Tokyo NNP singular proper noun 
time NN singular common oun 
comma 
the E)T determiner 
index NN singular common oun 
was VBD past tense verb 
up RB adverb 
435.11 CD cardinal number 
points NNS plural common oun 
comma 
to  ,,to,, 
34903.80 CD cardinal number 
as IN preposition/subordinating conjunction 
investors NNS plural common oun 
hailed VBD past tense verb 
New NNP singular proper noun 
York NNP singular proper noun 
's POS possessive ending 
overnight JJ adjective 
rally NN singular common oun 
sentence-final punctuation 
Mostly because of the less detailed tagset, the average ambiguity of the tags is 
lower than LOB's, at 2.34 tags per token in the corpus. This means that the tagging 
task should be an easier one than that for LOB. This is supported by the values for 
Random and LexProb in Table 2. On the other hand, the less detailed tagset alo means 
that the taggers have less detailed information to base their decisions on. Another 
factor that influences the quality of automatic tagging is the consistency of the tagging 
over the corpus. The WSJ material has not been checked as extensively as the LOB 
corpus and is expected to have a much lower consistency level (see Section 5.3 below 
for a closer examination). 
The training/test separation of the corpus is again done at utterance boundaries 
and leads to a 1,160K token training set and a 129K token test set. Around 1.86% of 
the test set are unseen tokens and a further 0.44% are known tokens with previously 
unseen tags. 
3.1.3 Eindhoven. The final two data sets are both based on the Eindhoven corpus 
(Uit den Boogaart 1975). This is slightly smaller than LOB and WSJ. The written part, 
which we use in our experiments, consists of about 750K words, in samples ranging 
from 53 to 451 words. In variety, it lies between LOB and WSJ, containing 150K words 
each of samples from Dutch newspapers (subcorpus CDB), weeklies (OBL), magazines 
(GBL), popular scientific writings (PWE), and novels (RNO). 
The tagging of the corpus, as used here, was created in 1994 as part of a mas- 
ter's thesis project (Berghmans 1994). It employs the Wotan tagset for Dutch, newly 
designed uring the project. It is based on the classification used in the most popular 
descriptive grammar of Dutch, the Algemene Nederlandse Spraakkunst (ANS \[Geerts et 
al. 1984\]). The actual distinctions encoded in the tagset were selected on the basis of 
their importance to the potential users, as estimated from a number of in-depth inter- 
views with interested parties in the Netherlands. The Wotan tagset is not only very 
large (233 base tags, leading to 341 tags when counting each ditto tag separately), but 
furthermore contains distinctions that are very difficult for automatic taggers, such as 
verb transitivity, syntactic use of adjectives, and the recognition of multitoken units. 
It has an average ambiguity of 7.46 tags per token in the corpus. For our experiments, 
209 
Computational Linguistics Volume 27, Number 2 
we also designed a simplif ication of the tagset, dubbed WotanLite, which no longer 
contains the most  problematic distinctions. WotanLite has 129 tags (with a complement  
of ditto tags leading to a total of 173) and an average ambigui ty  of 3.46 tags per token. 
An  example of Wotan tagging is given below (only under l ined parts remain in 
WotanLite): 14 
Mr. (Master, t i t le )  N(eigen,ev, neut):l/2 
Rijpstra N(eigen,ev, neut):2/2 
heeft (has) V(hulp,ott,3,ev) 
de (the) Art(bep,zijd-of-mv, neut) 
Commissarispost N(soort,ev, neut) 
(post of Commissioner) 
in (in) Prep(voor) 
Friesland N(eigen,ev, neut) 
geambieerd (aspired to) V(trans,verl-dw, onverv) 
en (and) Conj(neven) 
hij (he) Pron(per,3,ev, nom) 
moet (should) V(hulp,ott,3,ev) 
dus (therefore) Adv(gew, aanw) 
alle (a l l )  Pron(onbep,neut,attr) 
kans (opportunity) N_~(soort,ev, n ut) 
hebben (have) V__((trans,inf) 
er (there) Adv(pron,er) 
het (the) Art(bep,onzijd,neut) 
beste (best) Adj (zelfst,over tr,verv-neut) 
van (of) Adv(deel-adv) 
te (to) Prep(voor-inf) 
maken (make) V(trans,inf) 
Punc(punt) 
first part of singular neutral case proper noun 
second part of singular neutral case proper 
noun 
3rd person singular present tense auxiliary verb 
neutral case non-neuter or plural definite article 
singular neutral case common oun 
adposition used as preposition 
singular neutral case proper noun 
base form of past participle of transitive verb 
coordinating conjunction 
3rd person singular nominative personal 
pronoun 
3rd person singular present tense auxiliary verb 
demonstrative non-pronominal adverb 
attributively used neutral case indefinite 
pronoun 
singular neutral case common oun 
infinitive of transitive verb 
pronominal dverb "er" 
neutral case neuter definite article 
nominally used inflected superlative 
form of adjective 
particle adverb 
infinitival "te" 
infinitive of transitive verb 
period 
The annotat ion of the corpus was realized by a semiautomat ic  upgrade  of the 
tagging inherited f rom an earlier project. The result ing consistency has never been 
exhaust ively measured for either the Wotan or the original tagging. 
The training/test  separat ion of the corpus is done at sample boundar ies  (each 1st 
to 9th sample is training and each 10th is test). This is a much stricter separat ion than 
appl ied for LOB and WSJ, as for those two corpora our test utterances are related to 
the training ones by  being in the same samples. Part ly as a result of this, but  also very  
much because of word  compound ing  in Dutch,  we  see a much higher percentage of 
new tokens--6.24% tokens unseen in the training set. A further 1.45% known tokens 
have new tags for Wotan, and 0.45% for WotanLite. The training set consists of 640K 
tokens and the test set of 72K tokens. 
3.2 Tagger Generators 
The second ingredient for our exper iments is a set of four tagger generator  systems, 
selected on the basis of var iety and availability, is Each of the systems represents a
14 The example sentence could be rendered in English as Master Rijpstra has aspired to the post of 
Commissioner in Friesland and he should therefore be given every opportunity to make the most of it. 
15 The systems have to differ as much as possible in their learning strategies and biases, as otherwise 
there will be insufficient differences of opinion for the combiners to make use of. This was shown 
clearly in early experiments in 1992, where only n-gram taggers were used, and which produced only a 
very limited improvement in accuracy (van Ha|teren 1996). 
210 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
Table 1 
The features available to the four taggers in our study. Except for MXPOST, all systems use 
different models (and hence features) for known (k) and unknown (u) words. However, Brill's 
transformation-based learning system (TBL) applies its two models in sequence when faced 
with unknown words, thus giving the unknown-word tagger access to the features used by 
the known-word model as well. The first five columns in the table show features of the focus 
word: capitalization (C), hyphen (H), or digit (D) present, and number of suffix (S) or prefix 
(P) letters of the word. Brill's TBL system (for unknown words) also takes into account 
whether the addition or deletion of a suffix results in a known lexicon entry (indicated by an 
L). The next three columns represent access to the actual word (W) and any range of words to 
the left (Wleft) or  right (Wright). The last three columns how access to tag information for the 
word itself (T) and any range of words left (Tleft) or right (Tright). Note that the expressive 
power of a method is not purely determined by the features it has access to, but also by its 
algorithm, and what combinations of the available features this allows it to consider. 
Features 
System C D N S P W Wtedt Wright T Tteft Zright 
TBL (k) x 1-2 1-2 x 1-3 1-3 
TBL (u) x x x 4,L 4,L 1-2 1-2 1-3 1-3 
MBT (k) x x 1-2 1-2 
MBT (u) x x x 3 1 1 
MXP (all) x x x 4 4 x 1-2 1-2 1-2 
TNT (k) x x x 1-2 
TNT (u) x 10 1-2 
popular type of learning method, each uses slightly different features of the text (see 
Table 1), and each has a completely different representation for its language model. 
All publicly available systems are used with the default settings that are suggested in 
their documentation. 
3.2.1 Error-driven Transformation-based Learning. This learning method finds a set 
of rules that transforms the corpus from a baseline annotation so as to minimize the 
number of errors (we will refer to this system as TBL below). A tagger generator using 
this learning method is described in Brill (1992, 1994). The implementation that we 
use is Eric Brill's publicly available set of C programs and Perl scripts. 16 
When training, this system starts with a baseline corpus annotation A0. In A0, 
each known word is tagged with its most likely tag in the training set, and each 
unknown word is tagged as a noun (or proper noun if capitalized). The system then 
searches through a space of transformation rules (defined by rule templates) in order 
to reduce the discrepancy between its current annotation and the provided correct 
one. There are separate templates for known words (mainly based on local word 
and tag context), and for unknown words (based on suffix, prefix, and other lexical 
information). The exact features used by this tagger are shown in Table 1. The learner 
for the unknown words is trained and applied first. Based on its output, the rules for 
context disambiguation are learned. In each learning step, all instantiations of the rule 
templates that are present in the corpus are generated and receive a score. The rule that 
corrects the highest number of errors at step n is selected and applied to the corpus to 
yield an annotation A,, which is then used as the basis for step n + 1. The process tops 
when no rule reaches a score above a predefined threshold. In our experiments this 
has usually yielded several hundreds of rules. Of the four systems, TBL has access to 
16 Brill's system can be downloaded from 
ftp : // ftp.cs.jhu.edu /pub /brill /Programs / R ULE_B ASED_TA GG ER_V.1.14.tar.Z. 
211 
Computational Linguistics Volume 27, Number 2 
the most features: contextual information (the words and tags in a window spanning 
three positions before and after the focus word) as well as lexical information (the 
existence of words formed by the addition or deletion of a suffix or prefix). However, 
the conjunctions of these features are not all available in order to keep the search space 
manageable. Even with this restriction, the search is computationally very costly. The 
most important rule templates are of the form 
i f  context = x change tag i to  tagj 
where context is some condition on the tags of the neighbouring words. Hence learning 
speed is roughly cubic in the tagset size. 17 
When tagging, the system again starts with a baseline annotation for the new 
text, and then applies all rules that were derived during training, in the sequence in 
which they were derived. This means that application of the rules is fully deterministic. 
Corpus statistics have been at the basis of selecting the rule sequence, but the resulting 
tagger does not explicitly use a probabilistic model. 
3.2.2 Memory-Based Learning. Another learning method that does not explicitly ma- 
nipulate probabilities i  machine-based learning. However, rather than extracting a
concise set of rules, memory-based learning focuses on storing all examples of a task 
in memory in an efficient way (see Section 2.3). New examples are then classified by 
similarity-based reasoning from these stored examples. A tagger using this learning 
method, MBT, was proposed by Daelemans et al (1996). TM
During the training phase, the training corpus is transformed into two case bases, 
one which is to be used for known words and one for unknown words. The cases are 
stored in an IGTree (a heuristically indexed version of a case memory \[Daelemans, 
Van den Bosch, and Weijters 1997\]), and during tagging, new cases are classified by 
matching cases with those in memory going from the most important feature to the 
least important. The order of feature relevance is determined by Information Gain. 
For known words, the system used here has access to information about he focus 
word and its potential tags, the disambiguated tags in the two preceding positions, 
and the undisambiguated tags in the two following positions. For unknown words, 
only one preceding and following position, three suffix letters and information about 
capitalization and presence of a hyphen or a digit are used as features. The case base 
for unknown words is constructed from only those words in the training set that occur 
five times or less. 
3.2.3 Maximum Entropy Modeling. Tagging can also be done using maximum en- 
tropy modeling (see Section 2.4): a maximum entropy tagger, called MXPOST, was 
developed by Ratnaparkhi (1996) (we will refer to this tagger as MXP below). 19 This 
system uses a number of word and context features rather similar to system MBT, and 
trains a maximum entropy model using the improved iterative scaling algorithm for 
one hundred iterations. The final model has a weighting parameter for each feature 
value that is relevant o the estimation of the probability P(tag I features), and com- 
bines the evidence from diverse features in an explicit probability model. In contrast 
to the other taggers, both known and unknown words are processed by the same 
17 Because ofthe computational complexity, we have had to exclude the system from the experiments 
with the very large Wotan tagset. 
18 An on-line version of the tagger isavailable at http://ilk.kub.nl/. 
19 Ratnaparkhi's Java implementation of this system isfreely available for noncommercial research 
purposes at ftp://ftp.cis.upenn.edu/pub/adwait/jmx/. 
212 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
model. Another striking difference is that this tagger does not have a separate storage 
mechanism for lexical information about the focus word (i.e., the possible tags). The 
word is merely another feature in the probability model. As a result, no generaliza- 
tions over groups of words with the same set of potential tags are possible. In the 
tagging phase, a beam search is used to find the highest probability tag sequence for 
the whole sentence. 
3.2.4 Hidden Markov Models. In a Hidden Markov Model, the tagging task is viewed 
as finding the maximum probability sequence of states in a stochastic finite-state ma- 
chine. The transitions between states emit the words of a sentence with a probability 
P(w \[ St), the states St themselves model tags or sequences of tags. The transitions are 
controlled by Markovian state transition probabilities P(Stl \] Sti_l ). Because a sentence 
could have been generated by a number of different state sequences, the states are 
considered to be "Hidden." Although methods for unsupervised training of HMM's 
do exist, training is usually done in a supervised way by estimation of the above prob- 
abilities from relative frequencies in the training data. The HMM approach to tagging 
is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993). 
In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im- 
plementation of HMM's, which turned out to have the worst accuracy of the four 
competing methods. In the present work, we have replaced this by the TnT system 
(we will refer to this tagger as HMM below). 2? TnT is a trigram tagger (Brants 2000), 
which means that it considers the previous two tags as features for deciding on the 
current ag. Moreover, it considers the capitalization of the previous word as well in 
its state representation. The lexical probabilities depend on the identity of the current 
word for known words and on a suffix tree smoothed with successive abstraction 
(Samuelsson 1996) for guessing the tags of unknown words. As we will see below, it 
shows a surprisingly higher accuracy than our previous HMM implementation. When 
we compare it with the other taggers used in this paper, we see that a trigram HMM 
tagger uses a very limited set of features (Table 1). on  the other hand, it is able to 
access ome information about the rest of the sentence indirectly, through its use of 
the Viterbi algorithm. 
4. Overall Results 
The first set of results from our experiments i  the measurement of overall accuracy 
for the base taggers. In addition, we can observe the agreement between the systems, 
from which we can estimate how much gain we can possibly expect from combination. 
The application of the various combination systems, finally, shows us how much of 
the projected gain is actually realized. 
4.1 Base Tagger Quality 
An additional benefit of training four popular tagging systems under controlled con- 
ditions on several corpora is an experimental comparison of their accuracy. Table 2 
lists the accuracies as measured on the test set. 21 We see that TBL achieves the lowest 
accuracy on all data sets. MBT is always better than TBL, but is outperformed by both 
MXP and HMM. On two data sets (LOB and Wotan) the Hidden Markov Model sys- 
tem (TnT) is better than the maximum entropy system (MXPOST). On the other two 
20 The TnT system can be obtained from its author through ttp://www.coli.uni-sb.de/~thorsten/tnt/. 
21 In this and several following tables, the best performance is indicated with bold type. 
213 
Computational Linguistics Volume 27, Number 2 
Table 2 
Baseline and individual tagger test set accuracy for each of our four data sets. The bottom four 
rows show the accuracies of the four tagging systems on the various data sets. In addition, we 
list two baselines: the selection of a completely random tag from among the potential tags for 
the token (Random) and the selection of the lexically most likely tag (LexProb). 
LOB WSJ Wotan WotanLite 
Baseline 
Random 61.46 63.91 42.99 54.36 
LexProb 93.22 94.57 89.48 93.40 
Single Tagger 
TBL 96.37 96.28 -* 94.63 
MBT 97.06 96.41 89.78 94.92 
MXP 97.52 96.88 91.72 95.56 
HMM 97.55 96.63 92.06 95.26 
*The training of TBL on the large Wotan tagset was 
aborted after several weeks of training failed to pro- 
duce any useful results. 
Table 3 
Pairwise agreement between the base taggers. For each base tagger pair and data set, we list 
the percentage of tokens in the test set on which the two taggers elect he same tag. 
Tagger Pair 
MXP MXP MXP HMM HMM MBT 
Data Set HMM MBT TBL MBT TBL TBL 
LOB 97.56 96.70 96.27 97.27 96.96 96.78 
WSJ 97.41 96.85 96.90 97.18 97.39 97.21 
Wotan 93.02 90.81 - 92.06 - - 
WotanLite 95.74 95.12 95.00 95.48 95.36 95.52 
(WSJ and WotanLite) MXPOST is the better system. In all cases, except the difference 
between MXP and HMM on LOB, the differences are statistically significant (p K 0.05, 
McNemar 's  chi-squared test). 
We can also see from these results that WSJ, although it is about the same size as 
LOB, and has a smaller tagset, has a higher difficulty level than LOB. We suspect hat 
an important reason for this is the inconsistency in the WSJ annotation (cf. Ratnaparkhi 
1996). We examine this effect in more detail below. The Eindhoven corpus, both with 
Wotan and WotanLite tagsets, is yet more difficult, but here the difficulty lies mainly 
in the complexity of the tagset and the large percentage of unknown words in the 
test sets. We see that the reduction in the complexity of the tagset from Wotan to 
WotanLite leads to an enormous improvement  in accuracy. This granularity effect is 
also examined in more detail below. 
4.2 Base Tagger Agreement 
On the basis of the output of the single taggers we can also examine the feasibility 
of combination, as combination is dependent on different systems producing different 
errors. As expected, a large part of the errors are indeed uncorrelated: the agreement 
between the systems (Table 3) is at about the same level as their agreement with 
the benchmark tagging. A more detailed view of intertagger agreement is shown 
in Table 4, which lists the (groups of) patterns of (dis)agreement for the four data 
sets. 
214 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
Table 4 
The presence of various tagger (dis)agreeement patterns for the four data sets. In addition to 
the percentage of the test sets for which the pattern is observed (%), we list the cumulative 
percentage (%Cum). 
LOB WSJ Wotan WotanLite 
Pattern % %Cum % %Cure % %Cure % %Cum 
All taggers 93.93 93.93 93.80 93.80 85.68 85.68 90.50 90.50 
agree and 
are correct. 
A majority is 3.30 97.23 2.64 96.44 6.54 92.22 4.73 95.23 
correct. 
Correct tag is 1.08 98.31 1.07 97.51 0.82 93.04 1.59 96.82 
present but is 
tied. 
A minority is 0.91 99.22 1.12 98.63 2.62 95.66 1.42 98.24 
correct. 
The taggers 0.21 99.43 0.26 98.89 1.53 97.19 0.46 98.70 
vary, but 
are all wrong. 
All taggers 0.57 100.00 1.11 100.00 2.81 100.00 1.30 100.00 
agree but 
are wrong. 
It is interest ing to see that a l though the genera l  accuracy for WSJ is lower  than 
for LOB, the inter tagger  agreement  for WSJ is on average higher.  It wou ld  seem that 
the less consistent  tagg ing for WSJ makes  it easier for all systems to fall into the same 
traps. This becomes even clearer when we examine the pat terns  of agreement  and  
see, for example ,  that  the number  of tokens where  all taggers  agree on a wrong tag is 
pract ica l ly  doub led .  
The agreement  pat tern  d is t r ibut ion  enables us to determine  levels of combinat ion  
quality. Table 5 lists both  the accuracies of several  ideal  combiners  (%) and  the error  
reduct ion  in re lat ion to the best  base tagger  for the data set in quest ion (/~Err). 22 
For example ,  on LOB, "Al l  ties correct" p roduces  1,941 errors (cor respond ing  to an 
accuracy of 98.31%), wh ich  is 31.3% less than HMM's  2,824 errors. A min imal  level  of 
combinat ion  ach ievement  is that a major i ty  or better  wi l l  lead to the correct tag and  
that ties are hand led  appropr ia te ly  about  50% of the t ime for the (2-2) pat tern  and 
25% for the (1-1-1-1)  pat tern  (or 33.3% for the (1-1-1) pat tern  for Wotan).  In more  
opt imist ic  scenarios,  a combiner  is able to select the correct tag in all t ied cases, or  
even in cases where  a two-  or three- tagger  major i ty  must  be overcome.  A l though the 
poss ib i l i ty  of overcoming  a major i ty  is present  w i th  the arbi ter  type  combiners ,  the 
s i tuat ion is rather  improbab le .  As a result,  we  ought  to be more  than sat isf ied if any  
combiners  approach  the level  cor respond ing  to the pro jected combiner  wh ich  resolves 
all ties correctly. 23 
22 We express the error reduction in the form of a percentage, i.e., a relative measure, instead of by an 
absolute value, because we feel this is the more informative of the two. After all, there is a vast 
difference between an accuracy improvement of 0.5% from 50% to 50.5% (a /KEr r of 1%) and one of 
0.5% from 99% to 99.5% (a /KErr of 50%). 
23 The bottom rows of Table 5 might be viewed in the light of potential future extremely intelligent 
combination systems. For the moment, however, it is better to view them as containing recall values for 
n-best versions of the combination taggers, e.g., an n-best combination tagger for LOB, which simply 
provides all tags suggested by its four components, will have a recall score of 99.22%. 
215 
Computational Linguistics Volume 27, Number 2 
Table 5 
Projected accuracies for increasingly successful levels of combination achievement. For each 
level we list the accuracy (%) and the percentage of errors made by the best individual tagger 
that can be corrected by combination (AEFr). 
LOB WSJ Wotan WotanLite 
Pattern % AEr r % AEr r % AEr r % AEr r 
Best Single Tagger HMM MXP HMM MXP 
97.55 - 96.88 - 92.06 - 95.56 - 
Ties randomly 97.77 9.0 96.97 2.8 92.49 5.5 96.01 10.1 
correct. 
All ties correct. 98.31 31.3 97.50 19.9 93.04 12.4 96.82 28.3 
Minority vs. two-tagger 98.48 48.5 97.67 25.4 95.66 45.3 97.09 34.3 
correct. 
Minority vs three-tagger 99.22 68.4 98.63 56.0 - - 98.24 60.3 
correct. 
Table 6 
Accuracies of the combination systems on all four corpora. For each system we list its 
accuracy (%) and the percentage of errors made by the best individual tagger that is corrected 
by the combination system (A~,). 
LOB WSJ Wotan WotanLite 
% AErr % AErr % AErr % AErr 
Best Single Tagger HMM MXP HMM MXP 
97.55 - 96.88 - 92.06 - 95.56 
Voting 
Majority 97.76 9.0 96.98 3.1 92.51 5.7 96.01 10.1 
TotPrecision 97.95 16.2 97.07 6.1 92.58 6.5 96.14 12.9 
TagPrecision 97.82 11.2 96.99 3.4 92.51 5.7 95.98 9.5 
Precision-Recall 97.94 16.1 97.05 5.6 92.50 5.6 96.22 14.8 
TagPair 97.98 17.8 97.11 7.2 92.72 8.4 96.28 16.2 
Stacked Classifiers 
WPDV(Tags) 98.06 20.8 97.15 8.7 92.86 10.1 96.33 17.2 
WPDV(Tags+Word) 98.07 21.4 97.17 9.3 92.85 10.0 96.34 17.5 
WPDV(Tags+Context) 98.14 24.3 97.23 11.3 93.03 12.2 96.42 19.3 
MBL(Tags) 98.05 20.5 97.14 8.5 92.72 8.4 96.30 16.7 
MBL(Tags+Word) 98.02 19.2 97.12 7.6 92.45 5.0 96.30 16.6 
MBL(Tags+Context) 98.10 22.6 97.11 7.2 92.75 8.7 96.31 16.8 
DecTrees(Tags) 98.01 18.9 97.14 8.3 92.63 7.2 96.31 16.8 
DecTrees(Tags+Word) -* . . . . . . .  
DecTrees(Tags+Context) 98.03 19.7 97.12 7.7 - - 96.26 15.7 
Maccent(Tags) 98.03 19.6 97.10 7.1 92.76 8.9 96.29 16.4 
Maccent(Tags+Word) 98.02 19.3 97.09 6.6 92.63 7.2 96.27 16.0 
Maccent(Tags+Context) 98.12 23.5 97.10 7.0 93.25 15.0 96.37 18.2 
c5.0 was not able to cope with the large amount of data involved in all Tags+Word 
experiments and the Tags+Context experiment with Wotan. 
4.3 Results of Combinat ion  
In Table 6 the results of our exper iments  wi th  the var ious combinat ion  methods  are 
shown.  Aga in  we list both  the accuracies of the combiners  (%) and  the error reduct ion  
in relat ion to the best base tagger (AEr~). For example,  on LOB, TagPair produces 
2,321 errors (corresponding to an accuracy of 97.98%), which  is 17.8% less than HMM's  
2,824 errors. 
216 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
Although the combiners generally fall short of the "All ties correct" level (cf. 
Table 5), even the most trivial voting system (Majority), significantly outperforms the 
best individual tagger on all data sets. Within the simple voting systems, it appears 
that use of more detailed voting weights does not necessarily lead to better esults. 
TagPrecision is clearly inferior to TotPrecision. On closer examination, this could have 
been expected. Looking at the actual tag precision values (see Table 9 below), we 
see that the precision is generally more dependent on the tag than on the tagger, so 
that TagPrecision always tends to select the easier tag. In other words, it uses less 
specific rather than more specific information. Precision-Recall is meant o correct his 
behavior by the involvement of recall values. As intended, Precision-Recall generally 
has a higher accuracy than TagPrecision, but does not always improve on TotPrecision. 
Our previously unconfirmed hypothesis, that arbiter-type combiners would be able 
to outperform the gang-type ones, is now confirmed. With the exception of several of 
the Tags+Word versions and the Tags+Context version for WSJ, the more sophisticated 
modeling systems have a significantly better accuracy than the simple voting systems 
on all four data sets. TagPair, being somewhere between simple voting and stacking, 
also falls in the middle where accuracy is concerned. In general, it can at most be 
said to stay close to the real stacking systems, except for the cleanest data set, LOB, 
where it is clearly being outperformed. This is a fundamental change from our earlier 
experiments, where TagPair was significantly better than MBL and Decision Trees. Our 
explanation at the time, that the stacked systems uffered from a lack of training data, 
appears to be correct. A closer investigation below shows at which amount of training 
data the crossover point in quality occurs (for LOB). 
Another unresolved issue from the earlier experiments is the effect of making word 
or context information available to the stacked classifiers. With LOB and a single 114K 
tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees 
degraded significantly when adding context, and MBL degraded when adding the 
word .  24 With the increased amount of training material, addition of the context gener- 
ally leads to better esults. For MBL, there is a degradation only for the WSJ data, and 
of a much less pronounced nature. With the other data sets there is an improvement, 
significantly so for LOB. For Decision Trees, there is also a limited degradation for WSJ 
and WotanLite, and a slight improvement for LOB. The other two systems appear to be 
able to use the context more effectively. WPDV shows a relatively constant significant 
improvement over all data sets. Maccent shows more variation, with a comparable 
improvement on LOB and WotanLite, a very slight degradation on WSJ, and a spec- 
tacular improvement on Wotan, where it even yields an accuracy higher than the "All 
ties correct" level. 25 Addition of the word is still generally counterproductive. Only 
WPDV sometimes manages to translate the extra information i to an improvement in 
accuracy, and even then a very small one. It would seem that vastly larger amounts 
of training data are necessary if the word information is to become useful. 
5. Combination in Detail 
The observations about the overall accuracies, although the most important, are not 
the only interesting ones. We can also examine the results of the experiments above 
in more detail, evaluating the results of combination for specific words and tags, and 
24 Just as in the current experiments, the Decision Tree system could not cope with the amount  of data 
when the word was added. 
25 We have no clear explanation for this exceptional behavior, but conjecture that Maccent is able to make 
optimal use of the tagging differences caused by the high error rate of all four taggers. 
217 
Computational Linguistics Volume 27, Number 2 
Table 7 
Error rates for the most confusing words. For each word, we list the total number of instances 
in the test set (n), the number of tags associated with the word (tags), and then, for each base 
tagger and WPDV(Tags+Context), the rank in the error list (rank), the absolute number of 
errors (err), and the percentage of instances that is mistagged (%). 
MXP HMM MBT TBL WPDV(T+C) 
Word n/tags ra"k:err % rank:err % rank:err % rank:err % rank:err % 
as 719/17 1:102 14.19 1:130 18.08 3:120 16.69 1:167 23.23 1:82 11.40 
that 1,108/6 2:98 8.84 2:105 9.48 1:130 11.73 2:134 12.09 2:80 7.22 
to 2,645/9 3:81 2.76 3:59 2.23 2:122 4.61 3:131 4.27 3:40 1.51 
more 224/4 4:52 23.21 4:42 18.75 4:46 20.54 5:53 23.76 5:30 13.39 
so 247/10 6:32 12.96 6:40 16.19 6:40 16.19 4:63 25.51 4:31 12.55 
in 2,102/14 11:22 1.05 7:35 1.67 5:43 2.46 6:48 2.28 6:25 1.19 
about 177/3 5:37 20.90 5:41 23.16 7:30 16.95 17:23 12.99 7:22 12.43 
much 117/2 7:30 25.64 l?:27 23.08 s:27 23.08 9:35 29.91 9:20 17.09 
her 373/3 lS:13 3.49 21:10 2.68 17:18 4.83 7:39 10.46 25:7 1.88 
trying to discover why such disappointing results are found for WSJ. Furthermore, we 
can run additional experiments, to determine the effects of the size of the training set, 
the number of base tagger components involved, and the granularity of the tagset. 
5.1 Specific Words 
The overall accuracy of the various tagging systems gives a good impression of relative 
performance, but it is also useful to have a more detailed look at the tagging results. 
Most importantly for this paper, the details give a better feel for the differences between 
the base taggers and for how well a combiner can exploit these differences. More 
generally, users of taggers or tagged corpora are rarely interested in the whole corpus. 
They focus rather on specific words or word classes, for which the accuracy of tagging 
may differ greatly from the overall accuracy. 
We start our detailed examination with the words that are most often mistagged. 
We use the LOB corpus for this evaluation, as it is the cleanest data set and hence 
the best example. For each base tagger, and for WPDV(Tags+Context), we list the top 
seven mistagged words, in terms of absolute numbers of errors, in Table 7. Although 
the base taggers have been shown (in Section 4.2) to produce different errors, we see 
that they do tend to make errors on the same words, as the five top-sevens together 
contain only nine words. 
A high number of errors for a word is due to a combination of tagging difficulty 
and frequency. Examples of primarily difficult words are much and more. Even though 
they have relatively low frequencies, they are ranked high on the error lists. Words 
whose high error rate stems from their difficulty can be recognized by their high 
error percentage scores. Examples of words whose high error rate stems from their 
frequency are to and in. The error percentages show that these two words are actually 
tagged surprisingly well, as to is usually quoted as a tough case and for in the taggers 
have to choose between 14 possible tags. The first place on the list is taken by as, which 
has both a high frequency and a high difficulty level (it is also the most ambiguous 
word with 17 possible tags in LOB). 
Table 7 shows yet again that there are clear differences between the base taggers, 
providing the opportunity for effective combination. For all but one word, in, the 
combiner manages to improve on the best tagger for that specific word. If we compare 
to the overall best tagger, HMM, the improvements are sometimes spectacular. This is 
218 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
Table 8 
Confusion rates for the tag pairs most often confused. For each pair (tagger, correct), we first 
take the two possible confusion directions eparately and list the corresponding error list 
ranks (rank) and absolute number of errors (err) for the four base taggers and for 
WPDV(Tags+Context). Then we list the same information for the pair as a whole, i~e., for the 
two directions together. 
MXP HMM MBT TBL WPDV(T+C) 
Tagger Correct rank err rank err rank err rank err rank err 
VBN VBD 6 92 1 154 1 205 1 236 3 102 
VBD VBN 3 118 3 117 3 152 3 149 4 100 
pair 210 271 357 385 202 
JJ NN 2 132 2 150 2 168 2 205 2 109 
NN JJ 1 153 6 75 4 148 4 148 1 110 
pair 285 225 316 353 219 
IN CS 4 105 4 93 5 122 s 97 5 79 
CS IN 10 55 7 70 10 64 6 122 8 48 
pair 160 163 186 219 127 
NN VB 5 98 5 78 6 116 5 132 6 59 
VB NN 25 28 14 45 12 60 7 100 15 35 
pair 126 123 176 232 94 
IN RP 7 59 10 61 7 99 12 83 7 50 
RP 1N 24 30 18 38 27 34 21 42 18 30 
pair 89 99 133 125 80 
of course especia l ly  the case where  HMM has par t icu lar  diff icult ies w i th  a word ,  e.g., 
about with  a 46.3% reduct ion  in error rate, but  in other  cases as well ,  e.g., to with  a 
32.2% reduct ion,  wh ich  is stil l wel l  above  the overal l  error  rate reduct ion  of 24.3%. 
5.2 Specific Tags 
We can also abstract  away f rom the words  and  s imply  look at common word  class 
confusions,  e.g., a token that shou ld  be tagged VBD (past tense verb) is actua l ly  tagged 
VBN (past part ic ip le  verb). Table 8 shows  the tag confus ions  that are present  in the 
top seven confus ion list of at least  one of the systems (again the four  base taggers  
and  WPDV(Tags+Context)  used  on LOB). The number  on the r ight  in each system 
co lumn is the number  of t imes the error was  made and the number  on the left is the 
pos i t ion  in the confus ion list. The rows marked  wi th  tag va lues  show the ind iv idua l  
errors. 26 In add i t ion ,  the "pa i r "  rows show the combined  va lue  of the two inverse 
errors preced ing  it. 27 
As w i th  the word  errors above,  we  see substant ia l  di f ferences between the base 
taggers.  Unl ike the s i tuat ion wi th  words ,  there are now a number  of cases where  
base taggers  per fo rm better  than the combiner.  Partly, this is because the base tagger  
is outvoted  to such a degree that its qua l i ty  cannot  be mainta ined ,  e.g., NN ---, JJ. 
Fur thermore ,  it is p robab ly  unfa i r  to look at on ly  one hal f  of a pair. Any  a t tempt  to 
decrease the number  of errors of type X --~ Y wi l l  tend to increase the number  of errors 
of type  Y --* X. The balance between the two is best  shown in the "pa i r "  rows,  and 
26 The tags are: CS = subordinat ing conjunction, IN = preposit ion, JJ = adjective, NN = singular 
common noun,  RP = adverbial particle, VB = base form of verb, VBD = past tense of verb, VBN = 
past participle. 
27 RP --~ IN is not  actually in any top seven, but  has been added to complete the last pair  of inverse 
errors. 
219 
Computational Linguistics Volume 27, Number 2 
Table 9 
Precision and recall for tags involved in the tag pairs most often confused. For each tag, we 
list the percentage of tokens in the test set that are tagged with that tag (%test), followed by 
the precision (Prec) and recall (Rec) values for each of the systems. 
MXP HMM MBT TBL WPDV(T+C) 
Tag %test Prec/Rec Prec/Rec Prec/Rec Prec/Rec Prec/Rec 
CS 1.48 
IN 10.57 
JJ 5.58 
NN 13.11 
RP 0.79 
VB 2.77 
VBD 2.17 
VBN 2.30 
92.69/90.69 
97.58/98.95 
94.52/94.55 
96.68/97.85 
95.74/91.82 
98.04/95.55 
94.20/95.22 
94.07/93.29 
90.14/91.10 
97.83/98.59 
94.07/95.61 
97.91/97.24 
94.78/92.27 
97.95/95.99 
94.23/93.06 
90.93/93.37 
89.46/89.05 
97.14/98.17 
92.79/94.38 
96.59/97.22 
95.26/88.84 
96.79/94.55 
92.48/90.29 
89.59/90.54 
84.85/91.51 93.11193.38 
97.33/97.62 98.37199.03 
90.66/94.06 95.64196.00 
96.00/96.31 97.66/98.25 
93.05/90.28 95.95/94.14 
95.09/93.36 98.13197.06 
91.74/87.40 95.26/95.14 
87.09/90.99 94.25194.50 
Table 10 
A comparison of benchmark consistency on a small sample of WSJ and LOB. We list the 
reasons for differences between WPDV(Tags+Context) output and the benchmark tagging, 
both in terms of absolute numbers and percentages of the whole test set. 
WSJ LOB 
tokens % tokens % 
Tagger wrong, benchmark right 250 1.97 200 1.75 
Benchmark wrong, tagger ight 90 0.71 11 0.10 
Both wrong 7 0.06 1 0.01 
Benchmark left ambiguous, tagger ight 2 0.02 - - 
here the combiner is again performing excellently, in all cases improving on the best 
base tagger for the pair. 
For an additional point of view, we show the precision and recall values of the 
systems on the same tags in Table 9, as well as the percentage of the test set that 
should be tagged with each specific tag. The differences between the taggers are again 
present, and in all but two cases the combiner produces the best score for both preci- 
sion and recall. Furthermore, as precision and recall form yet another balanced pair, 
that is, as improvements in recall tend to decrease precision and vice versa, the re- 
maining two cases (NN and VBD), can be considered to be handled quite adequately 
as well. 
5.3 Effects of Inconsistency 
Seeing the rather bad overall performance of the combiners on WSJ, we feel the need 
to identify a property of the WSJ material that can explain this relative lack of success. 
A prime candidate for this property is the allegedly very low degree of consistency 
of the WSJ material. We can investigate the effects of the low consistency by way of 
comparison with the LOB data set, which is known to be very consistent. 
We have taken one-tenth of the test sets of both WSJ and LOB and manually 
examined each token where the WPDV(Tags+Context) tagging differs from the bench- 
mark tagging. The first indication that consistency is a major factor in performance is 
found in the basic correctness information, given in Table 10. For WSJ, there is a much 
higher percentage where the difference in tagging is due to an erroneous tag in the 
benchmark. This does not mean, however, that the tagger should be given a higher 
accuracy score, as it may well be that the part of the benchmark where tagger and 
220 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
benchmark do agree contains a similar percentage of benchmark errors. It does imply, 
though, that the WSJ tagging contains many more errors than the LOB tagging, which 
is likely to be detrimental to the derivation of automatic taggers. 
The cases where the tagger is found to be wrong provide interesting information 
as well. Our examination shows that 109 of the 250 erroneous tags occur in situations 
that are handled rather inconsistently in the corpus. 
In some of these situations we only have to look at the word itself. The most 
numerous type of problematic word (21 errors) is the proper noun ending in s. It 
appears to be unclear whether such a word should be tagged NNP or NNPS. When 
taking the words leading to errors in our 1% test set and examining them in the 
training data, we see a near even split for practically every word. The most  frequent 
ones are Securities (146 NNP vs. 160 NNPS) and Airlines (72 NNP vs. 83 NNPS). There 
are only two very unbalanced cases: Times (78 NNP vs. 6 NNPS) and Savings (76 NNP 
vs. 21 NNPS). A similar situation occurs, although less frequently, for common nouns, 
for example, headquarters gets 67 NN and 21 NNS tags. 
In other cases, difficult words are handled inconsistently in specific contexts. Ex- 
amples here are about in cases such as about 20 (405 IN vs. 385 RB) or about $20 (243 
IN vs. 227 RB), ago in cases such as years ago (152 IN vs. 410 RB) and more in more than 
(558 JJR vs. 197 RBR). 
Finally, there are more general word class confusions, such as adjective/particle 
or noun/adject ive in noun premodify ing positions. Here it is much harder to provide 
numerical examples, as the problematic situation must  first be recognized. We therefore 
limit ourselves to a few sample phrases. The first is stock-index, which leads to several 
errors in combinations like stock-index futures or stock-index arbitrage. In the training set, 
stock-index in premodify ing position is tagged JJ 64 times and NN 69 times. The second 
phrase chief executive officer has three words so that we have four choices of tagging: 
JJ-JJ-NN is chosen 90 times, J J -NN-NN 63 times, NN-J J -NN 33 times, and NN-NN-NN 
30 times. 
Admittedly, all of these are problematic ases and many other cases are han- 
dled quite consistently. However,  the inconsistently handled cases do account for 44% 
of the errors found for our best tagging system. Under the circumstances, we feel 
quite justified in assuming that inconsistency is the main cause of the low accuracy 
scores. 28 
5.4 Size of the Training Set 
The most important result that has undergone a change between van Halteren, Zavrel, 
and Daelemans (1998) and our current experiments i the relative accuracy of TagPair 
and stacked systems uch as MBL. Where TagPair used to be significantly better than 
MBL, the roles are now well reversed. It appears that our hypothesis at the time, that 
the stacked systems were plagued by a lack of training data, is correct, since they 
can now hold their own. In order to see at which point TagPair is overtaken, we 
have trained several systems on increasing amounts of training data from LOB. 29 Each 
increment is one of the 10% training corpus parts described above. The results are 
shown in Figure 5. 
28 Another property that might contribute to the relatively low scores for the WSJ material is the use of a 
very small tagset. This makes annotation easier for human annotators, but it provides much less 
information to the automatic taggers and combiners. It may well be that the remaining information is 
insufficient for the systems to discover useful disambiguation patterns in. Although we cannot measure 
this effect for WSJ, because of the many differences with the LOB data set, we feel that it has much less 
influence than the inconsistency of the WSJ material. 
29 Only combination uses a variable number of parts. The base taggers are always trained on the full 90%. 
221 
Computational Linguistics Volume 27, Number 2 
98.160 
98.140 
98,120 
98.100 
98.080 f ~ f J  
4 /,-J 
99.040 - "~/  
99.ooo 
97.940 
97,920 
97,900 
116K 231K 351K 468K 583K 697K 814K 931K 1045K 
1 2 3 4 5 6 7 8 9 
Size of Training Set 
Figure 5 
The accuracy of combiner methods on LOB as a function of the number of tokens of training 
material. 
TagPair is only best when a single part is used (as in the earlier experiments). 
After that it is overtaken and quickly left behind, as it is increasingly unable to use 
the additional training data to its advantage. 
The three systems using only base tagger outputs have comparable accuracy 
growth curves, although the initial growth is much higher for WPDV. The curves 
for WPDV and Maccent appear to be leveling out towards the right end of the graph. 
For MBL, this is much less clear. However, it would seem that the accuracy level at 
1M words is a good approximation of the eventual ceiling. 
The advantage of the use of context information becomes clear at 500K words. 
Here the tags-only systems tart to level out, but WPDV(Tags+Context) keeps show- 
ing a constant growth. Even at 1M words, there is no indication that the accuracy is 
approaching a ceiling. The model seems to be getting increasingly accurate in correct- 
ing very specific contexts of mistagging. 
5.5 Interaction of Components 
Another way in which the amount of input data can be varied is by taking subsets 
of the set of component taggers. The relation between the accuracy of combinations 
for LOB (using WPDV(Tags+Context)) and that of the individual taggers is shown 
in Table 11. The first three columns show the combination, the accuracy, and the 
improvement in relation to the best component. The other four columns show the 
further improvement gained when adding yet another component. 
The most important observation is that every combination outperforms the com- 
bination of any strict subset of its components. The difference is always significant, ex- 
cept in the cases MXP+HMM+MBT+TBL vs. MXP+HMM+MBT and HMM+MBT+TBL 
vs. HMM+MBT. 
We can also recognize the quality of the best component as a major factor in the 
quality of the combination results. HMM and MXP always add more gain than MBT, 
which always adds more gain than TBL. Another major factor is the difference in 
222 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
Table 11 
WPDV(Tags+Context) accuracy measurements forvarious component tagger combinations. 
For each combination, we list the tagging accuracy (Test), the error reduction expressed as a 
percentage of the error count for the best component base tagger (AErr(best)) and any 
subsequent error reductions when adding further components (Gain). 
Gain Gain Gain Gain 
Combination Test AErr(best) +TBL +MBT +MXP +HMM 
TBL 96.37 - - 29.1 40.2 38.9 
MBT 97.06 - 12.5 - 28.4 26.0 
MBT+TBL 97.43 12.5 (MBT) - - 20.6 17.2 
MXP 97.52 - 12.3 15.0 - 16.2 
HMM 97.55 - 9.5 11.3 15.3 - 
HMM+TBL 97.78 9.5 (HMM) - 4.0 11.8 - 
HMM+MBT 97.82 11.3 (HMM) 2.0 - 13.7 - 
MXP+TBL 97.83 12.3 (MXP) - 6.0 - 9.9 
HMM+MBT+TBL 97.87 13.1 (HMM) - - 12.9 - 
MXP+MBT 97.89 15.0 (MXP) 3.0 - - 10.8 
MXP+HMM 97.92 15.3 (HMM) 5.7 9.6 - - 
MXP+MBT+TBL 97.96 17.6 (MXP) - - - 9.1 
MXP+HMM+TBL 98.04 20.1 (HMM) - 5.2 - - 
MXP+HMM+MBT 98.12 23.4 (HMM) 1.1 - - - 
MXP+HMM+MBT+TBL 98.14 24.3 (HMM) . . . .  
language model. MXP, although having a lower accuracy by itself than HMM, yet 
leads to better combination results, again witnessed by the Gain columns. In some 
cases, MXP is even able to outperform pairs of components in combination: both 
MXP+MBT and MXP+HMM are better than HMM+MBT+TBL. 
5.6 Effects of Granularity 
The final influence on combination that we measure is that of the granularity of the 
tagset, which can be examined with the highly structured Wotan tagset. Part of the 
examination has already taken place above, as we have added the WotanLite tagset, a 
less granular projection of Wotan. As we have seen, the WotanLite taggers undeniably 
have a much higher accuracy than the Wotan ones. However, this is hardly surprising, 
as they have a much easier task to perform. In order to make a fair comparison, we 
now measure them at their performance of the same task, namely, the prediction of 
WotanLite tags. We do this by projecting the output of the Wotan taggers (i.e., the base 
taggers, WPDV(Tags), and WPDV(Tags+Context)) o WotanLite tags. Additionally, we 
measure all taggers at the main word class level, i.e., after the removal of all attributes 
and ditto tag markers. 
All results are listed in Table 12. The three major horizontal blocks each represent 
a level at which the correctness of the final output is measured. Within the lower two 
blocks, the three rows represent the type of tags used by the base taggers. The rows 
for Wotan and WotanLite represent the actual taggers, as described above. The row for 
BestLite does not represent a real tagger, but rather a virtual tagger that corresponds 
to the best tagger from among Wotan (with its output projected to WotanLite format) 
and WotanLite. This choice for the best granularity is taken once for each system 
as a whole, not per individual token. This leads to BestLite being always equal to 
WotanLite for TBL and MBT, and to projected Wotan for MXP and HMM. 
The three major vertical blocks represent combination strategies: no combination, 
combination using only the tags, and combination using tags and direct context. The 
two combination blocks are divided into three columns, representing the tag level 
223 
Computational Linguistics Volume 27, Number 2 
Table 12 
Accuracy for base taggers and different levels combiners, as measured at various levels of 
granularity. The rows are divided into blocks, each listing accuracies for a different comparison 
granularity. Within a block, the individual rows list which base taggers are used as ingredients 
in the combination. The columns contain, from left to right, the accuracies for the base taggers, 
the combination accuracies when using only tags (WPDV(Tags)) at three different levels of 
combination granularity (Full, Lite, and Main) and the combination accuracies when adding 
context (WPDV(Tags+Context)), atthe same three levels of combination granularity. 
Base Taggers WPDV(Tags) WPDV(Tags+Context) 
TBL MBT MXP HMM Full Lite Main Full Lite Main 
Measured as Wotan Tags 
Wotan 89.78 91.72 92.06 I 92.83 - - I 93.03 - 
Measured as WotanLite Tags 
Wotan - 94.56 95.71 95.98 96.50 96.49 - 96.53 96.54 - 
WotanLite 94.63 94.92 95.56 95.26 - 96.32 - - 96.42 - 
BestLite 94.63 94.92 95.71 95.98 - 96.58 - - 96.64 - 
. m  
Measured as Main Word Class Tags 
Wotan 
WotanLite 
BestLite 
- 96.55 97.23 97.54 
96.37 96.76 97.12 96.96 
96.37 96.76 97.23 97.54 
97.88 97.87 97.85 
- 97.69 97.71 
- 97.91 97.90 
97.88 97.89 97.91 
- 97.76 97.77 
- 97.94 97.93 
at which combination is performed, for example, for the Lite column the output of 
the base taggers is projected to WotanLite tags, which are then used as input for the 
combiner. 
We hypothesized beforehand that, in general, the more information a system can 
use, the better its results are. Unfortunately, even for the base taggers, reality is not that 
simple. For both MXP and HMM, the Wotan tagger indeed yields a better WotanLite 
tagging than the WotanLite tagger itself, thus supporting the hypothesis. On the other 
hand, the results for MBT do not confirm this, as here the WotanLite tagger is more ac- 
curate. However, we have already seen that MBT has severe problems in dealing with 
the complex Wotan data. Furthermore, the lowered accuracy of the MBL combiners 
when provided with words (see Section 4.3) also indicate that memory-based learning 
sometimes has problems in coping with a surplus of information. This means that we 
have to adjust our hypothesis: more information is better, but only up to the point 
where the wealth of information overwhelms the machine learning system. Where this 
point is found obviously differs for each system. 
For the combiners, the situation is rather inconclusive. In some cases, especially 
for WPDV(Tags), combining at a higher granularity (i.e., using more information) pro- 
duces better results. In others, combining at a lower granularity works better. In all 
cases, the difference in scores between the columns is extremely small and hardly 
supports any conclusions either way. What is obviously much more important for the 
combiners is the quality of the information they can work with. Here, higher granular- 
ity on the part of the ingredients is preferable, as combiners based on Wotan taggers 
perform better than those based on WotanLite taggers, 3?and ingredient performance 
seems to be even more useful, as BestLite yields yet better results in all cases. 
30 However, this comparison is not perfect, as the combination fWotan tags does not include TBL. On 
the one hand, this means the combination has less information to go on and we should hence be even 
more impressed with the better performance. Onthe other hand, TBL is the lowest scoring base tagger, 
so maybe the better performance is due to not having to cope with a flawed ingredient. 
224 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
Table 13 
A comparison of our results for WSJ with those by Brill and Wu (1998). 
Brill and Wu Our Experiments 
Training/Test Split 80/20 Training/Test Split 90/10 
Unigram 93.26 LexProb 94.57 
Trigram 96.36 TnT 96.63 
- MBT 96.41 
Transformation 96.61 Transformation 96.28 
Maximum Entropy 96.83 Maximum Entropy 96.88 
Transformation-based combination 97.16 
Error rate reduction 10.4% 
WPDV(Tags+Context) 97.23 
Error rate reduction 11.3% 
6. Related Research 
Combination of ensembles of classifiers, although well-established in the machine 
learning literature, has only recently been applied as a method for increasing accuracy 
in natural language processing tasks. There has of course always been a lot of research 
on the combination of different methods (e.g., knowledge-based and statistical) in hy- 
brid systems, or on the combination of different information sources. Some of that 
work even explicitly uses voting and could therefore also be counted as an ensemble 
approach. For example, Rigau, Atserias, and Agirre (1997) combine different heuris- 
tics for word sense disambiguation by voting, and Agirre et al (1998) do the same 
for spelling correction evaluation heuristics. The difference between single classifiers 
learning to combine information sources, i.e., their input features (see Roth \[1998\] for a 
general framework), and the combination ofensembles ofclassifiers trained on subsets 
of those features is not always very clear anyway. 
For part-of-speech tagging, a significant increase in accuracy through combining 
the output of different aggers was first demonstrated in van Halteren, Zavrel, and 
Daelemans (1998) and Brill and Wu (1998). In both approaches, different agger gen- 
erators were applied to the same training data and their predictions combined using 
different combination methods, including stacking. Yet the latter paper reported much 
lower accuracy improvement figures. As we now apply the methods of van Halteren, 
Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison. An 
exact comparison is still impossible, as we have not used the exact same data prepara- 
tion and taggers, but we can put roughly corresponding fi ures ide by side (Table 13). 
As for base taggers, the first two differences are easily explained: Unigram has to deal 
with unknown words, while LexProb does not, and TnT is a more advanced trigram 
system. The slight difference for Maximum Entropy might be explained by the dif- 
ference in training/test plit. What is more puzzling is the substantial difference for 
the transformation-based tagger. Possible explanations are that Brill and Wu used a 
much better parametrization f this system or that they used a different version of the 
WSJ material. Be that as it may, the final results are comparable and it is clear that 
the lower numbers in relation to LOB are caused by the choice of test material (WSJ) 
rather than by the methods used. 
In Tufi~ (1999), a single tagger generator is trained on different corpora repre- 
senting different language registers. For the combination, a method called credibility 
profiles worked best. In such a profile, for each component tagger, information is 
kept about its overall accuracy, its accuracy for each tag, etc. In another ecent study, 
Marquez et al (1999) investigate several types of ensemble construction i a decision 
tree learning framework for tagging specific classes of ambiguous words (as opposed 
225 
Computational Linguistics Volume 27, Number 2 
to tagging all words). The construction of ensembles was based on bagging, selection 
of different subsets of features (e.g., context and lexical features) in decision tree con- 
struction, and selection of different splitting criteria in decision tree construction. In 
all experiments, imple voting was used to combine component tagger decisions. All 
combination approaches resulted in a better accuracy (an error reduction between 8% 
and 12% on average compared to the basic decision tree trained on the same data). But 
as these error reductions refer to only part of the tagging task (18 ambiguity classes), 
they are hard to compare with our own results. 
In Abney, Schapire, and Singer (1999), ADABOOST variants are used for tagging 
WSJ material. Component classifiers here are based on different information sources 
(subsets of features), e.g., capitalization of current word, and the triple "string, cap- 
italization, and tag" of the word to the left of the current word are the basis for 
the training of some of their component classifiers. Resulting accuracy is comparable 
to, but not better than, that of the maximum entropy tagger. Their approach is also 
demonstrated for prepositional phrase attachment, again with results comparable to 
but not better than state-of-the-art single classifier systems. High accuracy on the same 
task is claimed by Alegre, Sopena, and Lloberas (1999) for combining ensembles of 
neural networks. ADABOOST has also been applied to text filtering (Schapire, Singer, 
and Singhal 1998) and text categorization (Schapire and Singer 1998). 
In Chen, Bangalore, and Vijay-Shanker (1999), classifier combination is used to 
overcome the sparse data problem when using more contextual information in super- 
tagging, an approach in which parsing is reduced to tagging with a complex tagset 
(consisting of partial parse trees associated with lexical items). When using pairwise 
voting on models trained using different contextual information, an error reduction 
of 5% is achieved over the best component model. Parsing is also the task to which 
Henderson and Brill (1999) apply combination methods with reductions of up to 30% 
precision error and 6% recall error compared to the best previously published results 
of single statistical parsers. 
This recent research shows that the combination approach is potentially useful for 
many NLP tasks apart from tagging. 
7. Conclusion 
Our experiments have shown that, at least for the word class tagging task, combina- 
tion of several different systems enables us to raise the performance ceiling that can be 
observed when using data-driven systems. For all tested data sets, combination pro- 
vides a significant improvement over the accuracy of the best component tagger. The 
amount of improvement varies from 11.3% error reduction for WSJ to 24.3% for LOB. 
The data set that is used appears to be the primary factor in the variation, especially 
the data set's consistency. 
As for the type of combiner, all stacked systems using only the set of proposed 
tags as features reach about the same performance. They are clearly better than sim- 
ple voting systems, at least as long as there is sufficient raining data. In the absence 
of sufficient data, one has to fall back to less sophisticated combination strategies. 
Addition of word information does not lead to improved accuracy, at least with the 
current raining set size. However, it might still be possible to get a positive effect by 
restricting the word information to the most frequent and ambiguous words only. Ad- 
dition of context information does lead to improvements for most systems. WPDV 
and Maccent make the best use of the extra information, with WPDV having an 
edge for less consistent data (WSJ) and Maccent for material with a high error rate 
(Wotan). 
226 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
Although the results reported in this paper are very positive, many directions for 
research remain to be explored in this area. In particular, we have high expectations for 
the following two directions. First, there is reason to believe that better esults can be 
obtained by using the probability distributions generated by the component systems, 
rather than just their best guesses (see, for example, Ting and Witten \[1997a\]). Second, 
in the present paper we have used disagreement between a fixed set of component 
classifiers. However, there exist a number of dimensions of disagreement (inductive 
bias, feature set, data partitions, and target category encoding) that might fruitfully 
be searched to yield large ensembles of modular components that are evolved to 
cooperate for optimal accuracy. 
Another open question is whether and, if so, when, combination is a worthwile 
technique in actual NLP applications. After all, the natural language text at hand has to 
be processed by each of the base systems, and then by the combiner. Now none of these 
is especially bothersome atrun-time (most of the computational difficulties being expe- 
rienced during training), but when combining N systems, the time needed to process 
the text can be expected to be at least a factor N+ 1 more than when using a single sys- 
tem. Whether this is worth the improvement that is achieved, which is as yet expressed 
in percents rather than in factors, will depend very much on the amount of text that has 
to be processed and the use that is made of the results. There are a few clear-cut cases, 
such as a corpus annotation project where the CPU time for tagging is negligible in 
relation to the time needed for manual correction afterwards (i.e., do use combination), 
or information retrieval on very large text collections where the accuracy improvement 
does not have enough impact o justify the enormous amount of extra CPU time (i.e., 
do not use combination). However, most of the time, the choice between combining or 
not combining will have to be based on evidence from carefully designed pilot experi- 
ments, for which this paper can only hope to provide suggestions and encouragement. 
Acknowledgments 
The authors would like to thank the 
creators of the tagger generators and 
classification systems used here for making 
their systems available, and Thorsten 
Brants, Guy De Pauw, Erik Tjong Kim Sang, 
Inge de M6nnink, the other members of the 
CNTS, ILK, and TOSCA research groups, 
and the anonymous reviewers for 
comments and discussion. 
This research was done while the second 
and third authors were at Tilburg 
University. Their research was done in the 
context of the Induction of Linguistic 
Knowledge (ILK) research program, 
supported partially by the Netherlands 
Organization for Scientific Research (NWO). 
References 
Abney, S., R. E. Schapire, and Y. Singer. 
1999. Boosting applied to tagging and PP 
attachment. In Proceedings ofthe 1999 Joint 
SIGDAT Conference on Empirical Methods in 
Natural Language Processing and Very Large 
Corpora, pages 38-45. 
Agirre, E., K. Gojenola, K. Sarasola, and A. 
Voutilainen. 1998. Towards a single 
proposal in spelling correction. In 
COLING-ACL "98, pages 22-28. 
Alegre, M., J. Sopena, and A. Lloberas. 1999. 
PP-attachment: A committee machine 
approach. In Proceedings ofthe 1999 Joint 
SIGDAT Conference on Empirical Methods in 
Natural Language Processing and Very Large 
Corpora, pages 231-238. 
Ali, K. M., and M. J. Pazzani. 1996. Error 
reduction through learning multiple 
descriptions. Machine Learning, 
24(3):173-202. 
Alpaydin, E. 1998. Techniques for 
combining multiple learners. In E. 
Alpaydin, editor, Proceedings ofEngineering 
of Intelligent Systems, pages 6-12. 
Berger, A., S. Della Pietra, and V. Della 
Pietra. 1996. A maximum entropy 
approach to natural language processing. 
Computational Linguistics, 22(1):39-71. 
Berghmans, J. 1994. Wotan, een 
automatische grammatikale tagger voor 
het Nederlands. Master's thesis, Dept. of 
Language and Speech, University of 
Nijmegen. 
Brants, T. 2000. TnT--A statistical 
part-of-speech tagger. In Proceedings ofthe 
Sixth Applied Natural Language Processing 
227 
Computational Linguistics Volume 27, Number 2 
Conference, (ANLP-2000), pages 224-231, 
Seattle, WA. 
Breiman, L. 1996a. Bagging predictors. 
Machine Learning, 24(2):123-140. 
Breiman, L. 1996b. Stacked regressions. 
Machine Learning, 24(3):49-64. 
Brill, E. 1992. A simple rule-based 
part-of-speech tagger. In Proceedings ofthe 
Third ACL Conference on Applied NLP, 
pages 152-155, Trento, Italy. 
Brill, E. 1994. Some advances in 
transformation-based part-of-speech 
tagging. In Proceedings ofthe Twelfth 
National Conference on Artificial Intelligence 
(AAAI '94). 
Brill, E. and Jun Wu. 1998. Classifier 
combination for improved lexical 
disarnbiguation. I  COLING-ACL '98: 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics, pages 191-195, Montreal, 
Quebec, Canada. 
Chan, P. K., S. J. Stolfo, and D. Wolpert. 
1999. Guest editors' introduction. Special 
Issue on Integrating Multiple Learned 
Models for Improving and Scaling Machine 
Learning Algorithms. Machine Learning, 
36(1-2):5-7. 
Charniak, E. 1993. Statistical Language 
Learning. MIT Press, Cambridge, MA. 
Chen, J., S. Bangalore, and K. Vijay-Shanker. 
1999. New models for improving supertag 
disambiguation. I  Proceedings ofthe 1999 
Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and 
Very Large Corpora, pages 188-195. 
Cherkauer, K. J. 1996. Human expert-level 
performance on a scientific image analysis 
task by a system using combined artificial 
neural networks. In P. Chan, editor, 
Working Notes of the AAAI Workshop on 
Integrating Multiple Learned Models, 
pages 15-21. 
Church, K. W. 1988. A stochastic parts 
program and noun phrase parser for 
unrestricted text. In Proceedings ofthe 
Second Conference on Applied Natural 
Language Processing. 
Daelemans, W., A. Van den Bosch, and A. 
Weijters. 1997. IGTree: Using trees for 
compression and classification i lazy 
learning algorithms. Artificial Intelligence 
Review, 11:407-423. 
Daelemans, W., J. Zavrel, P. Berck, and S. 
Gillis. 1996. MBT: A memory-based part 
of speech tagger generator. In E. Ejerhed 
and I. Dagan, editors, Proceedings ofthe 
Fourth Workshop on Very Large Corpora, 
pages 14-27. ACL SIGDAT. 
Daelemans, W., J. Zavrel, K. Van der Sloot, 
and A. Van den Bosch. 1999. TiMBL: 
Tilburg memory based learner, version 
2.0, reference manual. Technical Report 
ILK-9901, ILK, Tilburg University. 
Dehaspe, L. 1997. Maximum entropy 
modeling with clausal constraints. In
Inductive Logic Programming: Proceedings of
the 7th International Workshop (ILP-97), 
Lecture Notes in Artificial Intelligence, 1297, 
pages 109-124. Springer Verlag. 
DeRose, S. 1988. Grammatical category 
disambiguation by statistical 
optimization. Computational Linguistics, 
14:31-39. 
Dietterich, T. G. 1997. Machine learning 
research: Four current directions. AI 
Magazine, 18(4):97-136. 
Dietterich, T. G. 1998. Approximate 
statistical tests for comparing supervised 
classification learning algorithms. Neural 
Computation, 10(7):1895-1924. 
Dietterich, T. G. and G. Bakiri. 1995. Solving 
multiclass learning problems via 
error-correcting output codes. Journal of 
Artificial Intelligence Research, 2:263-286. 
Freund, Y. and R. E. Schapire. 1996. 
Experiments with a new boosting 
algorithm. In L. Saitta, editor, Proceedings 
of the 13th International Conference on 
Machine Learning, ICML '96, pages 148-156, 
San Francisco, CA. Morgan Kaufmann. 
Geerts, G., W. Haeseryn, J. de Rooij, and M. 
van der Toorn. 1984. Algemene Nederlandse 
Spraakkunst. Wolters-Noordhoff, 
Groningen and Wolters, Leuven. 
Golding, A. R. and D. Roth. 1999. A 
winnow-based approach to 
context-sensitive spelling correction. 
Machine Learning, 34(1-3):107-130. 
Henderson, J. and E. Brill. 1999. Exploiting 
diversity in natural anguage processing: 
Combining parsers. In Proceedings ofthe 
1999 Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and 
Very Large Corpora, pages 187-194. 
Johansson, S. 1986. The Tagged LOB Corpus: 
User's Manual. Norwegian Computing 
Centre for the Humanities, Bergen, 
Norway. 
Marcus, M., B. Santorini, and M. A. 
Marcinkiewicz. 1993. Building a large 
annotated corpus of English: The Penn 
Treebank. Computational Linguistics, 
19(2):313-330. 
Marquez, L., H. Rodrfguez, J. Carmona, and 
J. Montolio. 1999. Improving POS tagging 
using machine-learning techniques. In
Proceedings ofthe 1999 Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora, 
pages 53-62. 
228 
van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems 
Quinlan, J. R. 1993. c4.5: Programs for 
Machine Learning. Morgan Kaufmann, San 
Mateo, CA. 
Ratnaparkhi, A. 1996. A maximum entropy 
model for part-of-speech tagging. In 
Proceedings ofthe Conference on Empirical 
Methods in Natural Language Processing, 
pages 133-142, University of 
Pennsylvania. 
Rigau, G., J. Atserias, and E. Agirre. 1997. 
Combining unsupervised lexical 
knowledge methods for word sense 
disambiguation. In Proceedings of
ACL/EACL "97, pages 48-55. 
Roth, D. 1998. Learning to resolve natural 
language ambiguities: A unified 
approach. In Proceedings ofAAAI '98, 
pages 806-813. 
Samuelsson, Christer. 1996. Handling sparse 
data by successive abstraction. In
Proceedings ofthe 16th International 
Conference on Computational Linguistics, 
COLING-96, Copenhagen, Denmark. 
Schapire, R. E. and Y. Singer. 1998. 
BoosTexter: A system for multiclass 
multi-label text categorization. Technical 
Report, AT&T Labs. To appear in Machine 
Learning. 
Schapire, R. E., Y. Singer, and A. Singhal. 
1998. Boosting and Rocchio applied to 
text filtering. In Proceedings ofthe 21st 
Annual International Conference on Research 
and Development in Information Retrieval, 
(SIGIR "98), pages 215-223. 
Ting, K. M. and I. H. Witten. 1997a. Stacked 
generalization: When does it work? In 
International Joint Conference on Artificial 
Intelligence, Japan, pages 866-871. 
Ting, K. M. and I. H. Witten. 1997b. 
Stacking bagged and dagged models. In 
International Conference on Machine 
Learning, Tennessee, pages 367-375. 
Tufi~, D. 1999. Tiered tagging and combined 
language models classifiers. In Proceedings 
Workshop on Text, Speech, and Dialogue. 
Uit den Boogaart, P. C. 1975. 
Woordfrequenties in geschreven  gesproken 
Nederlands. Utrecht, Oosthoek, Scheltema 
& Holkema. 
van Halteren, H. 1996. Comparison of 
tagging strategies, a prelude to 
democratic tagging. In S. Hockey and 
N. Ide, editors, Research in Humanities 
Computing 4. Selected Papers from the 
ALLC/ACH Conference, Christ Church, 
Oxford, April 1992, pages 207-215, Oxford, 
England. Clarendon Press. 
van Halteren, H., editor. 1999. Syntactic 
Wordclass Tagging. Kluwer Academic 
Publishers, Dordrecht, The Netherlands. 
van Halteren, H. 2000a. A default first order 
weight determination procedure for 
WPDV models. In Proceedings of
CoNLL-2000 and LLL-2000, pages 119-122, 
Lisbon, Portugal. 
van Halteren, H. 2000b. Chunking with 
WPDV models. In Proceedings of
CoNLL-2000 and LLL-2000, pages 154-156, 
Lisbon, Portugal. 
van Halteren, H., J. Zavrel, and W. 
Daelemans. 1998. Improving data-driven 
wordclass tagging by system 
combination. In COLING-ACL "98: 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics, pages 491--497, Montreal, 
Quebec, Canada. 
Wolpert, D. H. 1992. Stacked generalization. 
Neural Networks, 5:241-259. 
Zavrel, J. and W. Daelemans. 2000. 
Bootstrapping a tagged corpus through 
combination of existing heterogeneous 
taggers. In Proceedings ofthe Second 
International Conference on Language 
Resources and Evaluation (LREC 2000), 
pages 17-20. 
229 

Linguistic Profiling for Author Recognition and Verification 
Hans van Halteren 
Language and Speech, Univ. of Nijmegen  
P.O. Box 9103  
NL-6500 HD, Nijmegen, The Netherlands 
hvh@let.kun.nl 
 
Abstract 
A new technique is introduced, linguistic 
profiling, in which large numbers of 
counts of linguistic features are used as a 
text profile, which can then be compared 
to average profiles for groups of texts. 
The technique proves to be quite effective 
for authorship verification and recogni-
tion. The best parameter settings yield a 
False Accept Rate of 8.1% at a False Re-
ject Rate equal to zero for the verification 
task on a test corpus of student essays, 
and a 99.4% 2-way recognition accuracy 
on the same corpus. 
1 Introduction 
There are several situations in language research 
or language engineering where we are in need of 
a specific type of extra-linguistic information 
about a text (document) and we would like to 
determine this information on the basis of lin-
guistic properties of the text. Examples are the 
determination of the language variety or genre of 
a text, or a classification for document routing or 
information retrieval. For each of these applica-
tions, techniques have been developed focusing 
on specific aspects of the text, often based on 
frequency counts of functions words in linguis-
tics and of content words in language engineer-
ing. 
 
In the technique we are introducing in this paper, 
linguistic profiling, we make no a priori choice 
for a specific type of word (or more complex fea-
ture) to be counted. Instead, all possible features 
are included and it is determined by the statistics 
for the texts under consideration, and the distinc-
tion to be made, how much weight, if any, each 
feature is to receive. Furthermore, the frequency 
counts are not used as absolute values, but rather 
as deviations from a norm, which is again deter-
mined by the situation at hand. Our hypothesis is 
that this technique can bring a useful contribution 
to all tasks where it is necessary to distinguish 
one group of texts from another.  In this paper the 
technique is tested for one specific type of group, 
namely the group of texts written by the same 
author. 
2 Tasks and Application Scenarios 
Traditionally, work on the attribution of a text to 
an author is done in one of two environments. 
The first is that of literary and/or historical re-
search where attribution is sought for a work of 
unknown origin (e.g. Mosteller & Wallace, 1984; 
Holmes, 1998). As secondary information gener-
ally identifies potential authors, the task is au-
thorship recognition: selection of one author from 
a set of known authors. Then there is forensic 
linguistics, where it needs to be determined if a 
suspect did or did not write a specific, probably 
incriminating, text (e.g. Broeders, 2001; Chaski, 
2001). Here the task is authorship verification: 
confirming or denying authorship by a single 
known author. We would like to focus on a third 
environment, viz. that of the handling of large 
numbers of student essays. 
 
For some university courses, students have to 
write one or more essays every week and submit 
them for grading. Authorship recognition is 
needed in the case the sloppy student, who for-
gets to include his name in the essay. If we could 
link such an essay to the correct student our-
selves, this would prevent delays in handling the 
essay. Authorship verification is needed in the 
case of the fraudulous student, who has decided 
that copying is much less work than writing an 
essay himself, which is only easy to spot if the 
original is also submitted by the original author. 
 
In both scenarios, the test material will be siz-
able, possibly around a thousand words, and at 
least several hundred. Training material can be 
sufficiently available as well, as long as text col-
lection for each student is started early enough. 
Many other authorship verification scenarios do 
not have the luxury of such long stretches of test 
text. For now, however, we prefer to test the ba-
sic viability of linguistic profiling on such longer 
stretches. Afterwards, further experiments can 
show how long the test texts need to be to reach 
an acceptable recognition/verification quality. 
2.1 Quality Measures 
For recognition, quality is best expressed as the 
percentage of correct choices when choosing be-
tween N authors, where N generally depends on 
the attribution problem at hand. We will use the 
percentage of correct choices between two au-
thors, in order to be able to compare with previ-
ous work. For verification, quality is usually 
expressed in terms of erroneous decisions. When 
the system is asked to verify authorship for the 
actual author of a text and decides that the text 
was not written by that author, we speak of a 
False Reject. The False Reject Rate (FRR) is the 
percentage of cases in which this happens, the 
percentage being taken from the cases which 
should be accepted. Similarly, the False Accept 
Rate (FAR) is the percentage of cases where 
somebody who has not written the test text is ac-
cepted as having written the text. With increasing 
threshold settings, FAR will go down, while FRR 
goes up. The behaviour of a system can be shown 
by one of several types of FAR/FRR curve, such 
as the Receiver Operating Characteristic (ROC). 
Alternatively, if a single number is preferred, a 
popular measure is the Equal Error Rate (EER), 
viz. the threshold value where FAR is equal to 
FRR. However, the EER may be misleading, 
since it does not take into account the conse-
quences of the two types of errors. Given the ex-
ample application, plagiarism detection, we do 
not want to reject, i.e. accuse someone of plagia-
rism, unless we are sure. So we would like to 
measure the quality of the system with the False 
Accept Rate at the threshold at which the False 
Reject Rate becomes zero. 
2.2 The Test Corpus 
Before using linguistic profiling for any real task, 
we should test the technique on a benchmark 
corpus. The first component of the Dutch Au-
thorship Benchmark Corpus (ABC-NL1) appears 
to be almost ideal for this purpose. It contains 
widely divergent written texts produced by first-
year and fourth-year students of Dutch at the 
University of Nijmegen. The ABC-NL1 consists 
of 72 Dutch texts by 8 authors, controlled for age 
and educational level of the authors, and for reg-
ister, genre and topic of the texts. It is assumed 
that the authors? language skills were advanced, 
but their writing styles were as yet at only weakly 
developed and hence very similar, unlike those in 
literary attribution problems.  
 
Each author was asked to write nine texts of 
about a page and a half. In the end, it turned out 
that some authors were more productive than 
others, and that the text lengths varied from 628 
to 1342 words. The authors did not know that the 
texts were to be used for authorship attribution 
studies, but instead assumed that their writing 
skill was measured. The topics for the nine texts 
were fixed, so that each author produced three 
argumentative non-fiction texts, on the television 
program Big Brother, the unification of Europe 
and smoking, three descriptive non-fiction texts, 
about soccer, the (then) upcoming new millen-
nium and the most recent book they read, and 
three fiction texts, namely a fairy tale about Little 
Red Riding Hood, a murder story at the univer-
sity and a chivalry romance. 
 
The ABC-NL1 corpus is not only well-suited 
because of its contents. It has also been used in 
previously published studies into authorship at-
tribution. A ?traditional? authorship attribution 
method, i.e. using the overall relative frequencies 
of the fifty most frequent function words and a 
Principal Components Analysis (PCA) on the 
correlation matrix of the corresponding 50-
dimensional vectors, fails completely (Baayen et 
al., 2002). The use of Linear Discriminant Analy-
sis (LDA) on overall frequency vectors for the 50 
most frequent words achieves around 60% cor-
rect attributions when choosing between two au-
thors, which can be increased  to around 80%  by 
the application of cross-sample entropy weight-
ing (Baayen et al, 2002). Weighted Probability 
Distribution Voting (WPDV) modeling on the 
basis of a very large number of features achieves 
97.8% correct attributions (van Halteren et al, To 
Appear). Although designed to produce a hard 
recognition task, the latter result show that very 
high recognition quality is feasible. Still, this ap-
pears to be a good test corpus to examine the ef-
fectiveness of a new technique.  
3 Linguistic Profiling 
In linguistic profiling, the occurrences in a text 
are counted of a large number of linguistic fea-
tures, either individual items or combinations of 
items. These counts are then normalized for text 
length and it is determined how much (i.e. how 
many standard deviations) they differ from the 
mean observed in a profile reference corpus. For 
the authorship task, the profile reference corpus 
consists of the collection of all attributed and 
non-attributed texts, i.e. the entire ABC-NL1 
corpus. For each text, the deviation scores are 
combined into a profile vector, on which a vari-
ety of distance measures can be used to position 
the text in relation to any group of other texts.   
3.1 Features 
Many types of linguistic features can be profiled, 
such as features referring to vocabulary, lexical 
patterns, syntax, semantics, pragmatics, informa-
tion content or item distribution through a text.  
However, we decided to restrict the current ex-
periments to a few simpler types of features to 
demonstrate the overall techniques and method-
ology for profiling before including every possi-
ble type of feature. In this paper, we first show 
the results for lexical features and continue with 
syntactic features, since these are the easiest ones 
to extract automatically for these texts. Other 
features will be the subject of further research. 
3.2 Authorship Score Calculation 
In the problem at hand, the system has to decide 
if an unattributed text is written by a specific  
author, on the basis of attributed texts by that and 
other authors.  We test our system?s ability to 
make this distinction by means of a 9-fold cross-
validation experiment. In each set of runs of the 
system, the training data consists of attributed 
texts for eight of the nine essay topics. The test 
data consists of the unattributed texts for the 
ninth essay topic. This means that for all runs, the 
test data is not included in the training data and is 
about a different topic than what is present in the 
training material. During each run within a set, 
the system only receives information about 
whether each training text is written by one spe-
cific author. All other texts are only marked as 
?not by this author?.  
3.3 Raw Score 
The system first builds a profile to represent text 
written by the author in question. This is simply 
the featurewise average of the profile vectors of 
all text samples marked as being written by the 
author in question. The system then determines a 
raw score for all text samples in the list. Rather 
than using the normal distance measure, we opted 
for a non-symmetric measure which is a 
weighted combination of two factors: a) the dif-
ference between sample score and author score 
for each feature and b) the sample score by itself. 
This makes it possible to assign more importance 
to features whose count deviates significantly 
from the norm. The following distance formula is 
used: 
?T = (? |Ti?Ai| D  |Ti| S) 1/(D+S) 
In this formula, Ti and Ai are the values for the ith 
feature for the text sample profile and the author 
profile respectively, and D and S are the weight-
ing factors that can be used to assign more or less 
importance to the two factors described. We will 
see below how the effectiveness of the measure 
varies with their setting. The distance measure is 
then transformed into a score by the formula 
ScoreT = (? |Ti|(D+S)) 1/(D+S)   ?  ?T 
In this way, the score will grow with the similar-
ity between text sample profile and author pro-
file. Also, the first component serves as a 
correction factor for the length of the text sample 
profile vector.  
3.4 Normalization and Renormalization 
The order of magnitude of the score values varies 
with the setting of D and S. Furthermore, the val-
ues can fluctuate significantly with the sample 
collection. To bring the values into a range which 
is suitable for subsequent calculations, we ex-
press them as the number of standard deviations 
they differ from the mean of the scores of the text 
samples marked as not being written by the au-
thor in question.  
 
In the experiments described in this paper, a 
rather special condition holds. In all tests, we 
know that the eight test samples are comparable 
in that they address the same topic, and that the 
author to be verified produced exactly one of the 
eight test samples. Under these circumstances, 
we should expect one sample to score higher than 
the others in each run, and we can profit from 
this knowledge by performing a renormalization, 
viz. to the number of standard deviations the 
score differs from the mean of the scores of the 
unattributed samples. However, this renormaliza-
tion only makes sense in the situation that we 
have a fixed set of authors who each produced 
one text for each topic. This is in fact yet a dif-
ferent task than those mentioned above, say au-
thorship sorting. Therefore, we will report on the 
results with renormalization, but only as addi-
tional information. The main description of the 
results will focus on the normalized scores. 
4 Profiling with Lexical Features 
The most straightforward features that can be 
used are simply combinations of tokens in the 
text.  
4.1 Lexical features 
Sufficiently frequent tokens, i.e. those that were 
observed at least a certain amount of times (in 
this case 5) in some language reference corpus 
(in this case the Eindhoven corpus; uit den 
Boogaart, 1975) are used as features by them-
selves. For less frequent tokens we determine a 
token pattern consisting of the sequence of char-
acter types, e.g., the token ?Uefa-cup? is repre-
sented by the pattern ?#L#6+/CL-L?, where the 
first ?L? indicates low frequency, 6+ the size 
bracket, and the sequence ?CL-L? a capital letter 
followed by one or more lower case letters fol-
lowed by a hyphen and again one or more lower 
case letters. For lower case words, the final three 
letters of the word are included too, e.g. ?waar-
maken? leads to ?#L#6+/L/ken?. These patterns 
have been originally designed for English and 
Dutch and will probably have to be extended 
when other languages are being handled. 
 
In addition to the form of the token, we also use 
the potential syntactic usage of the token as a 
feature. We apply the first few modules of a 
morphosyntactic tagger (in this case Wotan-Lite; 
Van Halteren et al, 2001) to the text, which de-
termine which word class tags could apply to 
each token. For known words, the tags are taken 
from a lexicon; for unknown words, they are es-
timated on the basis of the word patterns de-
scribed above. The three (if present) most likely 
tags are combined into a feature, e.g. ?niet? leads 
to ?#H#Adv(stell,onverv)-N(ev,neut)? and 
?waarmaken? to ?#L#V(inf)-N(mv,neut)-
V(verldw, onverv)?. Note that the most likely 
tags are determined on the basis of the token it-
self and that the context is not consulted. The 
modules of the tagger which do context depend-
ent disambiguation are not applied. 
 
Op top of the individual token and tag features 
we use all possible bi- and trigrams which can be 
built with them, e.g. the token combination ?kon 
niet waarmaken? leads to features such as 
?wcw=#H#kon#H#Adv(stell,onverv)-N(ev,neut) 
#L#6+/L/ken?. Since the number of features 
quickly grows too high for efficient processing, 
we filter the set of features by demanding that a 
feature occurs in a set minimum number of texts 
in the profile reference corpus (in this case two). 
A feature which is filtered out instead contributes 
to a rest category feature, e.g. the feature above 
would contribute to ?wcw=<OTHER>?. For the 
current corpus, this filtering leads to a feature set 
of about 100K features. 
 
The lexical features currently also include fea-
tures for utterance length. Each utterance leads to 
two such features, viz. the exact length (e.g. 
?len=15?) and the length bracket (e.g. ?len=10-
19?).  
4.2 Results with lexical features 
A very rough first reconnaissance of settings for 
D and S suggested that the best results could be 
achieved with D between 0.1 and 2.4 and S be-
tween 0.0 and 1.0. Further examination of this 
area leads to FAR FRR=0  scores ranging down to 
around 15%. Figure 1 shows the scores at various 
settings for D and S. The z-axis is inverted (i.e. 1 
- FAR FRR=0  is used) to show better scores as 
peaks rather than troughs.  
 
The most promising area is the ridge along the 
trough at D=0.0, S=0.0. A closer investigation of 
this area shows that the best settings are D=0.575 
and S=0.15. The FAR FRR=0  score here is 14.9%, 
i.e. there is a threshold setting such that if all 
texts by the authors themselves are accepted, 
only 14.9% of texts by other authors are falsely 
accepted.  
 
The very low value for S is surprising. It indi-
cates that it is undesirable to give too much atten-
tion to features which deviate much in the sample 
being measured; still, in the area in question, the 
score does peak at a positive S value, indicating 
that some such weighting does have effect. Suc-
cessful low scores for S can also be seen in the 
hill leading around D=1.0, S=0.3, which peaks at 
an FAR FRR=0  score of around 17 percent. From 
the shape of the surface it would seem that an 
investigation of the area across the S=0.0 divide 
might still be worthwhile, which is in contradic-
tion with the initial finding that negative values 
produce no useful results. 
5 Beyond Lexical Features 
As stated above, once the basic viability of the 
technique was confirmed, more types of features 
would be added. As yet, this is limited to syntac-
tic features. We will first describe the system 
quality using only syntactic features, and then 
describe the results when using lexical and syn-
tactic features in combination. 
5.1 Syntactic Features 
We used the Amazon parser to derive syntactic 
constituent analyses of each utterance (Coppen,  
2003). We did not use the full rewrites, but rather 
constituent N-grams. The N-grams used were: 
 
? left hand side label, examining constituent 
occurrence 
? left hand side label plus one label from the 
right hand side, examining dominance 
? left hand side plus label two labels from 
the right hand side, in their actual order, 
examining dominance and linear prece-
dence 
For each label, two representations are used. The 
first is only the syntactic constituent label, the 
second is the constituent label plus the head 
word. This is done for each part of the N-grams 
independently, leading to 2, 4 and 8 features re-
spectively for the three types of N-gram. Fur-
thermore, each feature is used once by itself, 
once with an additional marking for the depth of 
the rewrite in the analysis tree, once with an addi-
tional marking for the length of the rewrite, and 
once with both these markings. This means an-
other multiplication factor of four for a total of 8, 
16 and 32 features respectively. After filtering for 
minimum number of observations, again at least 
an observation in two different texts, there are 
about 900K active syntactic features, nine times 
as many as for the lexical features. 
 
Investigation of the results for various settings 
has not been as exhaustive as for the lexical fea-
tures. The best settings so far, D=1.3, S=1.4, 
yield an FAR FRR=0  of 24.8%, much worse than 
the 14.9% seen for lexical features.  
5.2 Combining Lexical and Syntactic Fea-
tures 
From the FAR FRR=0  score, it would seem that 
syntactic features are not worth pursuing any fur-
Figure 1: The variation of FAR (or rather 1-FAR) 
as a function of D and S, with D ranging from 0.1 
to 2.4 and S from 0.0 to 1.0.  
ther, since they perform much worse than lexical 
ones. However, they might still be useful if we 
combine their scores with those for the lexical 
features. For now, rather than calculating new 
combined profiles, we just added the scores from 
the two individual systems. The combination of 
the best two individual systems leads to an FAR 
FRR=0  of 10.3%,  a solid improvement over lexical 
features  by themselves. However, the best indi-
vidual systems are not necessarily the best com-
biners. The best combination systems produce 
FAR FRR=0  measurements down to 8.1%, with 
settings in different parts of the parameter space. 
 
It should be observed that the improvement 
gained by combination is linked to the chosen 
quality measure.  If we examine the ROC-curves 
for several types of systems (plotting the FAR 
against the FRR; Figure 2), we see that the com-
bination curves as a whole do not differ much 
from the lexical feature curve. In fact, the EER 
for the ?best? combination system is worse than 
that for the best lexical feature system. This 
means that we should be very much aware of the 
relative importance of FAR and FRR in any spe-
cific application when determining the ?optimal? 
features and parameters.  
6 Parameter Settings 
A weak point in the system so far is that there is 
no automatic parameter selection. The best re-
sults reported above are the ones at optimal set-
tings. One would hope that optimal settings on 
training/tuning data will remain good settings for 
new data. Further experiments on other data will 
have to shed more light on this. Another choice 
which cannot yet be made automatically is that of 
a threshold. So far, the presentation in this paper 
has been based on a single threshold for all au-
thor/text combinations. That there is an enormous 
potential for improvement can be shown by as-
suming a few more informed methods of thresh-
old selection. 
 
The first method uses the fact that, in our ex-
periments, there are always one true and seven 
false authors. This means we can choose the 
threshold at some point below the highest of the 
eight scores. We can hold on to the single thresh-
old strategy  if we first renormalize, as  described 
 
 
 
 
 
 
 
 
in Section 3.4, and then choose a single value to 
threshold the renormalized values against. The 
second method assumes that we will be able to 
find an optimal threshold for each individual run 
of the system. The maximum effect of this can be 
estimated with an oracle providing the optimal 
threshold. Basically, since the oracle threshold 
will be at the score for the text by the author, we 
Figure 2: ROC (FAR plotted against FRR) for a 
varying threshold at good settings of D and S for 
different types of features. The top pane shows the 
whole range (0 to 1)  for FAR and FRR. The bottom 
pane shows the area from 0.0 to 0.2. 
are examining how many texts by other authors 
score better than the text by the actual author.  
 
Table 1 compares the results for the best settings 
for these two new scenarios with the results pre-
sented above. Renormalizing already greatly im-
proves the results. Interestingly, in this scenario, 
the syntactic features outperform the lexical ones, 
something which certainly merits closer investi-
gation after the parameter spaces have been 
charted more extensively. The full potential of 
profiling becomes clear in the Oracle threshold 
scenario, which shows extremely good scores. 
Still, this potential will yet have to be realized by 
finding the right automatic threshold determina-
tion mechanism.  
7 Comparison to Previous Authorship 
Attribution Work 
Above, we focused on the authorship verification 
task, since it is the harder problem, given that the 
potential group of authors is unknown. However, 
as mentioned in Section 2, previous work with 
this data has focused on the authorship recogni-
tion problem, to be exact on selecting the correct 
author out of two potential authors. We repeat the 
previously published results in Table 2, together 
with linguistic profiling scores, both for the 2-
way and for the 8-way selection problem. 
 
To do attribution with linguistic profiling, we 
calculated the author scores for each author from 
the set for a given text, and then selected the au-
thor with the highest score. The results are shown 
in Table 2, using lexical or syntactic features or 
both, and with and without renormalization. The 
Oracle scenario is not applicable as we are com-
paring rather than thresholding. 
 
In each case, the best results are not just found at 
a single parameter setting, but rather over a larger 
area in the parameter space. This means that the 
choice of optimal parameters will be more robust 
with regard to changes in authors and text types. 
We also observe that the optimal settings for rec-
ognition are very different from those for verifi-
cation. A more detailed examination of the 
results is necessary to draw conclusions about 
these differences, which is again not possible 
until the parameter spaces have been charted 
more exhaustively. 
 
 Lexical 
Features 
Syntactic 
Features 
Com-
bina-
tion 
Single 
threshold 
14.9% 24.8% 8.1% 
Single  
threshold after 
renormalization 
9.3% 6.0% 2.4% 
Oracle thresh-
old per run 
0.8% 1.6% 0.2% 
 
Table 1: Best FAR FRR=0 scores for verification with 
various feature types and threshold selection mecha-
nisms. 
 2-way 
errors 
/504 
2-way 
percent 
correct 
8-way 
errors 
/72 
8-way 
percent 
correct 
50 func-
tion words, 
PCA 
 ? 50%   
followed 
by LDA 
 ? 60%   
LDA with 
cross-
sample 
entropy 
weighting  
 ? 80%   
all tokens, 
WPDV 
modeling 
 97.8%   
Lexical 6 98.8% 5 93% 
Syntactic 14 98.2% 10 86% 
Combined 3 99.4% 2 97% 
Lexical 
(renorm.) 
1 99.8% 1 99% 
Syntactic 
(renorm.) 
4 99.2% 3 96% 
Combined 
(renorm.) 
0 100.0% 0 100% 
 
Table 2: Authorship recognition quality for various 
methods. 
All results with normalized scores are already 
better than the previously published results. 
When applying renormalization, which might be 
claimed to be justified in this particular author-
ship attribution problem, the combination system 
reaches the incredible level of making no mis-
takes at all.  
8 Conclusion 
Linguistic profiling has certainly shown its worth 
for authorship recognition and verification. At 
the best settings found so far, a profiling system 
using combination of lexical and syntactic fea-
tures is able select the correct author for 97% of 
the texts in the test corpus. It is also able to per-
form the verification task in such a way that it 
rejects no texts that should be accepted, while 
accepting only 8.1% of the texts that should be 
rejected. Using additional knowledge about the 
test corpus can improve this to 100% and 2.4%. 
 
The next step in the investigation of linguistic 
profiling for this task should be a more exhaus-
tive charting of the parameter space, and espe-
cially the search for an automatic parameter 
selection procedure. Another avenue of future 
research is the inclusion of even more types of 
features. Here, however, it would be useful to 
define an even harder verification task, as the 
current system scores already very high and fur-
ther improvements might be hard to measure. 
With the current corpus, the task might be made 
harder by limiting the size of the test texts.  
 
Other corpora might also serve to provide more 
obstinate data, although it must be said that the 
current test corpus was already designed specifi-
cally for this purpose. Use of further corpora will 
also help with parameter space charting, as they 
will show the similarities and/or differences in 
behaviour between data sets. Finally, with the 
right types of corpora, the worth of the technique 
for actual application scenarios could be investi-
gated.  
 
So there are several possible routes to further 
improvement. Still, the current quality of the sys-
tem is already such that the system could be ap-
plied as is. Certainly for authorship recognition 
and verification, as we hope to show by our par-
ticipation in Patrick Juola?s Ad-hoc Authorship 
Attribution Contest (to be presented at 
ALLC/ACH 2004), for language verification (cf. 
van Halteren and Oostdijk, 2004), and possibly 
also for other text classification tasks, such as 
language or language variety recognition, genre 
recognition, or document classification for IR 
purposes. 
References  
Harald Baayen, Hans van Halteren, Anneke Neijt, and 
Fiona Tweedie. 2002. An Experiment in Author-
ship Attribution. Proc. JADT 2002, pp. 69-75. 
Ton Broeders. 2001. Forensic Speech and Audio 
Analysis, Forensic Linguistics 1998-2001 ? A Re-
view. Proc. 13th Interpol Forensic Science Sympo-
sium, Lyon, France. 
C. Chaski. 2001. Empirical Evaluations of Language-
Based Author Identification Techniques. Forensic 
Linguistics 8(1): 1-65. 
Peter Arno Coppen. 2003. Rejuvenating the Amazon 
parser. Poster presentation CLIN2003, Antwerp, 
Dec. 19, 2003. 
David Holmes. 1998. Authorship attribution. Literary 
and Linguistic Computing 13(3):111-117. 
F. Mosteller, and D.L. Wallace. 1984. Applied Bayes-
ian and Classical Inference in the Case of the Fed-
eralist Papers (2nd edition). Springer Verlag, New 
York. 
P. C. Uit den Boogaart. 1975. Woordfrequenties in 
geschreven en gesproken Nederlands. Oosthoek, 
Scheltema & Holkema, Utrecht.  
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 2001. Improving accuracy in word class tag-
ging through the combination of machine learning 
systems. Computational Linguistics 27(2):199-230.  
Hans van Halteren and Nelleke Oostdijk, 2004. Lin-
guistic Profiling of Texts for the Purpose of Lan-
guage Verification. Proc. COLING 2004. 
Hans van Halteren, Marco Haverkort, Harald Baayen, 
Anneke Neijt, and Fiona Tweedie. To appear. New 
Machine Learning Methods Demonstrate the Exis-
tence of a Human Stylome. Journal of Quantitative 
Linguistics. 
 
In: Proceedings o/CoNLL-2000 and LLL-2000, pages 119-122, Lisbon, Portugal, 2000. 
A Default First Order Family Weight Determination Procedure 
for WPDV Models 
Hans  van  Ha l te ren  
Dept. of Language and Speech, University of Ni jmegen 
P.O. Box 9103, 6500 HD Nijmegen, The Netherlands 
hvh@let, kun. nl 
Abst rac t  
Weighted Probability Distribution Voting 
(WPDV) is a newly designed machine learning 
algorithm, for which research is currently 
aimed at the determination of good weighting 
schemes. This paper describes a simple yet 
effective weight determination procedure, which 
leads to models that can produce competitive 
results for a number of NLP classification 
tasks. 
1 The  WPDV a lgor i thm 
Weighted Probability Distribution Voting 
(WPDV) is a supervised learning approach to 
classification. A case which is to be classified is 
represented as a feature-value pair set: 
Fcase -- {{fl : Vl}, . . . ,  { fn  :Vn}} 
An estimation of the probabilities of the various 
classes for the case in question is then based on 
the classes observed with similar feature-value 
pair sets in the training data. To be exact, the 
probability of class C for Fcase is estimated as 
a weighted sum over all possible subsets Fsub of 
Fcase: 
w /req(CJF  b) 
P(C) = N(C) /req(F  b) 
FsubCFcase 
with the frequencies (freq) measured on the 
training data, and N(C) a normalizing factor 
such that ~/5(C)  = 1. 
In principle, the weight factors WF,~,~ can be 
assigned per individual subset. For the time 
being, however, they are assigned for groups of 
subsets. First of all, it is possible to restrict 
the subsets that are taken into account in the 
model, using the size of the subset (e.g. Fsub 
contains at most 4 elements) and/or its fre- 
quency (e.g. Fsub occurs at least twice in the 
training material). Subsets which do not fulfil 
the chosen criteria are not used. For the sub- 
sets that are used, weight factors are not as- 
signed per individual subset either, but rather 
per "family", where a family consists of those 
subsets which contain the same combination of 
feature types (i.e. the same f/). 
The two components of a WPDV model, dis- 
tributions and weights, are determined sepa- 
rately. In this paper, I will use the term training 
set for the data on which the distributions are 
based and tuning set for the data on the basis of 
which the weights are selected. Whether these 
two sets should be disjunct or can coincide is 
one of the subjects under investigation. 
2 Fami ly  we ights  
The various family weighting schemes can be 
classified according to the type of use they make 
of the tuning data. Here, I use a very rough 
classification, into weighting scheme orders. 
With 0 th order  weights,  no information 
whatsoever is used about the data in tuning 
set. Examples of such rudimentary weighting 
schemes are the use of a weight of k! for all sub- 
sets containing k elements, as has been used e.g. 
for wordclass tagger combination (van Halteren 
et al, To appear), or even a uniform weight for 
all subsets. 
With 1 st order  weights,  information is used 
about the individual feature types, i.e. 
WF,~b = IT WIt 
{il(f i :vi}eF, ub} 
First order weights ignore any possible inter- 
action between two or more feature types, but 
119 
have the clear advantage of corresponding to a 
reasonably low number of weights, viz. as many 
as there are feature types. 
With n th order  weights,  interaction pat- 
terns are determined of up to n feature types 
and the family weights are adjusted to compen- 
sate for the interaction. When n is equal to 
the total number of feature types, this corre- 
sponds to weight determination per individual 
family, n th order weighting generally requires 
much larger numbers of weights, which can be 
expected to lead to much slower tuning proce- 
dures. In this paper, therefore, I focus on first 
order weighting. 
3 F i r s t  o rder  we ight  determinat ion  
As argumented in an earlier paper (van Hal- 
teren, 2000a), a theory-based feature weight de- 
termination would have to take into account 
each feature's decisiveness and reliability. How- 
ever, clear definitions of these qualities, and 
hence also means to measure them, are as yet 
sorely lacking. As a result, a more pragmatic 
approach will have to be taken. Reliability is ig- 
nored altogether at the moment, 1 and decisive- 
ness replaced by an entropy-related measure. 
3.1 Initial weights 
The weight given to each feature type fi should 
preferably increase with the amount of informa- 
tion it contributes to the classification process. 
A measure related to this is Information Gain, 
which represents the difference between the en- 
tropy of the choice with and without knowledge 
of the presence of a feature (cf. Quinlan (1986)). 
As do Daelemans et al (2000), I opt for a fac- 
tor proportional to the feature type's Gain Ra- 
tio, a normalising derivative of the Information 
Gain value. The weight factors W/~ are set to 
an optimal multiplication constant C times the 
measured Gain Ratio for fi- C is determined by 
calculating the accuracies for various values of 
C on the tuning set 2 and selecting the C which 
yields the highest accuracy. 
lit may still be present, though, in the form of the 
abovementioned frequency threshold for features. 
2If the tuning set coincides with the training set, all 
parts of the tuning procedure are done in leave-one-out 
mode: in the WPDV implementation, it is possible to 
(virtually) remove the information about each individual 
instance from the model when that specific instance has 
to be classified. 
3.2 Hi l l -c l imbing 
Since the initial weight determination is based 
on pragmatic rather than theoretical consider- 
ations, it is unlikely that the resulting weights 
are already the optimal ones. For this reason, 
an attempt is made to locate even better weight 
vectors in the n-dimensional weight space. The 
navigation mechanism used in this search is hill- 
climbing. This means that systematic variations 
of the currently best vector are investigated. If
the best variation is better than the currently 
best vector, that variation is taken as the best 
vector and the process is repeated. This repeti- 
tion continues until no better vector is found. 
In the experiments described here, the varia- 
tion consists of multiplication or division of each 
individual W/i by a variable V (i.e. 2n new vec- 
tors are tested each time), which is increased if a 
better vector is found, and otherwise decreased. 
The process is halted as soon as V falls below 
some pre-determined threshold. 
Hill-climbing, as most other optimaliza- 
tion techniques, is vulnerable to overtraining. 
To lessen this vulnerability, the WPDV hill- 
climbing implementation splits its tuning mate- 
rial into several (normally five) parts. A switch 
to a new weight vector is only taken if the ac- 
curacy increases on the tuning set as a whole 
and does not decrease on more than one part, 
i.e. some losses are accepted but only if they 
are localized. 
4 Qua l i ty  o f  the  f i rs t  o rder  we ights  
In order to determine the quality of the WPDV 
system, using first order weights as described 
above, I run a series of experiments, using tasks 
introduced by Daelemans et al (1999): 3
The Par t -o f - speech  tagg ing  task (POS) is 
to determine a wordclass tag on the basis of dis- 
ambiguated tags of two preceding tokens and 
undisambiguated tags for the focus and two fol- 
lowing tokens. 4 5 features with 170-480 values; 
169 classes; 837Kcase training; 2xl05Kcase test. 
The Grapheme- to -phoneme convers ion  
with stress  task (GS) is to determine the pro- 
nunciation of an English grapheme, including 
aI only give a rough description of the tasks here. For 
the exact details, I refer the reader to Daelemans et al 
(1999). 
4For a overall WPDV approach to wordclass tagging, 
see van Halteren (2000b). 
120 
Table h Accuracies for the POS task (with the 
training set ah tested in leave-one-out mode) 
Table 2: Accuracies for the GS task (with the 
training set ah tested in leave-one-out mode) 
Weighting scheme Test set 
ah i 
Comparison 
Naive Bayes 
TiMBL (k=l) 
Maccent (freq=2;iter=150) 
Maccent (freq=l;iter=300) 
WPDV 0 th order weights 
1 
kl 
96.41 
97.83 
98.07 
98.13 
97.66 97.71 
96.86 96.92 
WPDV initial 18t order 
tune = ah (10GR) 98.14 98.16 
tune = i (12GR) 98.14 98.17 
tune = j (llGR) 98.14 98.16 
WPDV with hill-climbing 
tune = ah (30 steps) 98.17 98.21 
tune = i (20 steps) 98.15 98.20 
tune = j (20 steps) 98.15 98.18 
96.24 
97.79 
98.03 
98.10 
97.63 
96.86 
98.12 
98.12 
98.13 
98.15 
98.12 
98.16 
Weighting scheme 
Comparison 
Naive Bayes 
TiMBL (k----l) 
Maccent (freq=2;iter=150) 
Maccent (freq=l;iter=300) 
Test set 
ah i j 
50.05 49.98 
92.25 92.02 
79.41 79.36 
80.43 80.35 
WPDV 0 th order weights 
1 90.99 90.49 90.25 
k! 92.77 92.05 91.89 
WPDV initial 1 st order 
tune = ah (30GR) 93.27 92.74 92.52 
tune = i (25GR) 93.24 92.76 92.54 
tune = j (25GR) 93.24 92.76 92.54 
WPDV with hill-climbing 
tune = ah (34 steps) 93.29 92.77 92.53 
tune = i (28 steps) 93.25 92.79 92.53 
tune = j (12 steps) 93.24 92.76 92.54 
presence of stress, on the basis of the focus 
grapheme, three preceding and three following 
graphemes. 7 features with 42 values each; 159 
classes; 540Kcase training; 2x68Kcase test. 
The PP  a t tachment  ask (PP) is preposi- 
tional phrase attachment to either a preceding 
verb or a preceding noun, on the basis of the 
verb, the noun, the preposition in question and 
the head noun of the prepositional complement. 
4 features with 3474, 4612, 68 and 5780 values; 
2 classes; 19Kcase training; 2x2Kcase test. 
The NP  chunk ing  task (NP) is the deter- 
ruination of the position of the focus token in a 
base NP chunk (at beginning of chunk, in chunk, 
or not in chunk), on the basis of the words and 
tags for two preceding tokens, the focus and 
one following token, and also the predictions by 
three newfirst stage classifiers for the task. 5 11 
features with 3 (first stage classifiers), 90 (tags) 
and 20K (words) values; 3 classes; 201Kcase 
training; 2x25Kcase test. 6 
For each of the tasks, sections a to h of the data 
set are used as the training set and sections i
5For a WPDV approach to a more general chunking 
task, see my contribution to the CoNLL shared task, 
elsewhere in these proceedings. 
~The number of feature combinations for the NP task 
is so large that the WPDV model has to be limited. For 
the current experiments, I have opted for a maximum 
size for fsub of four features and a threshold frequency 
of two observations in the training set. 
and j as (two separate) test sets. All three are 
also used as tuning sets. This allows a compari- 
son between tuning on the training set itself and 
on a held-out uning set. For comparison with 
some other well-known machine learning algo- 
rithms, I complement the WPDV experiments 
with accuracy measurements forthree other sys- 
tems: 1) A system using a Na ive  Bayes  prob- 
ability estimation; 2) T iMBL ,  using memory 
based learning and probability estimation based 
on the nearest neighbours (Daelemans et al, 
2000), 7 for which I use the parameters which 
yielded the best results according to Daelemans 
et al (1999); and 3) Maccent ,  a maximum en- 
tropy based system, s for which I use both the 
default parameters, viz. a frequency threshold 
of 2 for features to be used and 150 iterations 
of improved iterative scaling, and a more am- 
bitious parameter setting, viz. a threshold of 1 
and 300 iterations. 
The results for various WPDV weights, and 
the other machine learning techniques are listed 
in Tables 1 to 4. 9 Except for one case (PP with 
tune on j and test on i), the first order weight 
WPDV results are all higher than those for the 
7http:// i lk.kub.nl/. 
Shttp://w~.cs.kuleuven.ac.be/~ldh. 
9The accuracy isshown in itMics wheneverthetuning 
set is equM to the test set, i.e. when there is anunf~r 
advantage. 
121 
Table 3: Accuracies for the PP  task (with the 
training set ah tested in leave-one-out mode) 
Table 4: Accuracies for the NP task (with the 
training set ah tested in leave-one-out mode) 
Weighting scheme Test set 
ah i 
Comparison 
Naive Bayes 
TiMBL (k=l) 
Maccent (freq=2;iter=150) 
Maccent (freq=l;iter=300) 
WPDV 0 ~h order weights 
1 
k~ 
82.68 82.64 
83.43 81.97 
81.00 80.25 
79.41 79.79 
80.83 82.26 81.46 
80.76 82.30 81.30 
WPDV initial 18t order 
tune = ah (21GR) 82.89 83.64 82.38 
tune = i (15GR) 82.82 83.81 82.55 
tune = j (llGR) 82.60 83.26 82.76 
WPDV with hill-climbing 
tune --- ah (19 steps) 83.10 83.72 82.68 
tune = i (18 steps) 82.95 84.06 82.80 
tune = j (16 steps) 82.65 83.10 82.93 
Weighting scheme i Test set 
ah i j 
Comparison 
Naive Bayes 
TiMBL (k=3) 
Maccent (freq=2;iter=150) 
Maccent (freq=l;iter=300) 
WPDV 0 th order weights 
1 
k~ 
96.52 96.49 
98.34 98.22 
97.89 97.75 
97.66 97.45 
97.56 97.77 97.69 
97.74 97.97 97.87 
WPDV initial I st order 
tune = ah (380GR) 98.19 98.38 98.26 
tune = i (60GR) 98.14 98.39 98.17 
tune = j (360GR) 98.19 98.38 98.27 
WPDV with hill-climbing 
tune = ah (50 steps) 98.36 98.54 98.44 
tune = i (34 steps) 98.25 98.57 98.33 
tune = j (12 steps) 98.19 98.38 98.27 
comparison systems. 1? 0 th order weights gener- 
ally do not reach this level of accuracy. 
Hill-climbing with the tuning set equal to the 
training set produces the best results overall. 
It always leads to an improvement over initial 
weights of the accuracies on both test sets, al- 
though sometimes very small (GS). Equally im- 
portant, the improvement on the test sets is 
comparable to that on the tuning/training set. 
This is certainly not the case for hill-climbing 
with the tuning set equal to the other test set, 
which generally does not reach the same level of 
accuracy and may even be detrimental (climb- 
ing on PPj) .  
Strangely enough, hill-climbing with the tun- 
ing set equal to the test set itself sometimes does 
not even yield the best quality for that test set 
(POS with test set i and especially NP with j). 
This shows that the weight-+accuracy function 
does have local maxima~ and the increased risk 
for smaller data sets to run into a sub-optimal 
one is high enough that it happens in at least 
two of the eight test set climbs. 
1?The accuracies for TiMBL are lower than those 
found by Daelemans et ai. (1999): POSi 97.95, POSj 
97.90, GS~ 93.75, GSj 93.58, PP~ 83.64, PPj 82.51, NP~ 
98.38 and NPj 98.25. This is due to the use of eight part 
training sets instead of nine. The extreme differences for 
the GS task show how much this task depends on indi- 
vidual observations rather than on generalizations, which 
probably also explains why Naive Bayes and Maximum 
Extropy (Maccent) handle this task so badly. 
In summary, hill-climbing should preferably 
be done with the tuning set equal to the training 
set. This is not surprising, as the leave-one- 
out mechanism allows the training set to behave 
as held-out data, while containing eight times 
more cases than a test set turned tuning set. 
The disadvantage is a much more time-intensive 
hill-climbing procedure, but when developing an 
actual production model, the weights only have 
to be determined once and the results appear to 
be worth it most of the time. 
References  
w. Daelemans, A. Van den Bosch, and J. Zavrel. 
1999. Forgetting exceptions is harmful in lan- 
guage learning. Machine Learning, Special issue 
on Natural Language Learning, 34:11-41. 
W. Daelemans, J. Zavrel, K. Van der Sloot, and 
A. Van den Bosch. 2000. TiMBL: Tilburg Mem- 
ory Based Learner, version 3.0, reference manual. 
Tech. Report ILK-00-01, ILK, Tilburg University. 
H. van Halteren. 2000a. Weighted Probability Dis- 
tribution Voting, an introduction. In Computa- 
tional linguistics in the Netherlands, 1999. 
H. van Halteren. 2000b. The detection of in- 
consistency in manually tagged text. In Proc. 
LINC2000. 
H. van Halteren, J. Zavrel, and W. Daelemans. 
To appear. Improving accuracy in NLP through 
combination of machine learning systems. Com- 
putational Linguistics. 
J.R. Quinlan. 1986. Induction of Decision Trees. 
Machine Learning, 1:81-206. 
122 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 154-156, Lisbon, Portugal, 2000. 
Chunking with WPDV Models 
Hans  van  Ha l te ren  
Dept. of Language and Speech, Univ. of Nijmegen 
P.O. Box 9103, 6500 HD Nijmegen 
The Netherlands 
hvh@let ,  kun .  n l  
1 In t roduct ion  
In this paper I describe the application of the 
WPDV algorithm to the CoNLL-2000 shared 
task, the identification ofbase chunks in English 
text (Tjong Kim Sang and Buchholz, 2000). For 
this task, I use a three-stage architecture: I 
first run five different base chunkers, then com- 
bine them and finally try to correct some recur- 
ring errors. Except for one base chunker, which 
uses the memory-based machine learning sys- 
tern TiMBL, 1 all modules are based on WPDV 
models (van Halteren, 2000a). 
2 Arch i tec ture  components  
The first stage of the chunking architecture con- 
sists of five different base chunkers: 
1) As a baseline, I use a stacked TiMBL 
model. For the first level, following Daelemans 
et al (1999), I use as features all words and 
tags in a window ranging from five tokens to 
the left to three tokens to the right. For the 
second level (cf. Tjong Kim Sang (2000)), I use 
a smaller window, four left and two right, but 
add the IOB suggestions made by the first level 
for one token left and right (but not the focus). 
2) The basic WPDV model uses as features 
the words in a window ranging from one left to 
one right, the tags in a window ranging from 
three left to three right, and the IOB sugges- 
tions for the previous two tokens? 
3) In the reverse WPDV model, the direction 
of chunking is reversed, i.e. it chunks from the 
end of each utterance towards the beginning. 
4) In the R&M WPDV model, Ramshaw and 
Marcus's type of IOB-tags are used, i.e. starts 
of chunks are tagged with a B-tag only if the 
1Cf. ht tp : / / i l k .  kub. nl/ .  
2For unseen data, i.e. while being applied, the IOB 
suggestions used are of course those suggested by the 
model itself, not the true ones. 
preceding chunk is of the same type, and with 
an I-tag otherwise. 
5) In the LOB WPDV model, the Penn word- 
class tags (as produced by the Brill tagger) 
are replaced by the output of a WPDV tagger 
trained on 90% of the LOB corpus (van Hal- 
teren, 2000b). 
For all WPDV models, the number of fea- 
tures is too high to be handled comfortably by 
the current WPDV implementation. For this 
reason, I use a maximum feature subset size of 
four and a threshold frequency of two. 3 
The second stage consists of a combinat ion of 
the outputs of the five base chunkers, using an- 
other WPDV model. Each chunker contributes 
a feature containing the IOB suggestions for the 
previous, current and next token. In addition, 
there is a feature for the word and a feature 
combining the (Penn-style) wordclass tags of 
the previous, current and next token. For the 
combination model, I use no feature restrictions, 
and the default hill-climbing procedure. 
In the final stage, I apply correct ive mea- 
sures to systematic errors which are observed 
in the output of leave-one-out experiments on 
the training data. For now, I focus on the most 
frequent phrase type, the NP, and especially on 
one weak point: determination f the start po- 
sition of NPs. I use separate WPDV models for 
each of the following cases: 
1) Shou ld  a token  now marked  I -NP  start a 
~Cf. van Halteren (2000a). Also, the difference be- 
tween training and running (correct IOB-tags vs model 
suggestions) leads to a low expected generalization qual- 
ity of hill-climbing. I therefore stop climbing after a 
single effective step, but using an alternative climbing 
procedure, in which not only the single best multiplica- 
tions/division is applied per step, but which during ev- 
ery step applies all multiplications/divisions that yielded 
improvements while the opposite operation did not. 
154 
Phrase 
type 
ADJP 
ADVP 
CONJP 
INTJ  
LST 
NP 
PP 
PRT 
SBAR 
VP 
Number in 
test set 
438 
866 
9 
2 
5 
12422 
4811 
106 
535 
4658 
TiMBL WPDV 
basic reverse R&M LOB 
64.99 71.14 76.18 70.52 69.83 74.55 
75.03 78.96 79.83 78.16 78.50 80.09 
36.36 45.45 18.18 20.69 58.82 42.11 
66.67 66.67 66.67 66.67 0.00 66.67 
0.00 0.00 0.00 0.00 0.00 0.00 
91.85 92.65 92.56 92.00 92.35 93.72 
95.66 96.53 96.85 96.06 96.65 97.09 
63.10 73.63 68.60 74.07 73.45 74.31 
76.50 82.27 85.54 84.18 84.77 85.41 
92.11 92.80 92.84 92.37 91.45 93.61 
NO OtN NOt A'7 O1 T ?) N1 ON 
Combination Corrective 
measures 
74.52 
79.86 
42.11 
66.67 
0.00 
93.84 
97.10 
74.31 
85.41 
93.65 
O~ OE Qq q'~ 
Table 1: FZ=i measurements for all systems (as described in the text). In addition we list the 
number of occurrences of each phrase type in the test set. 
new NP? 4 Features used: the wordclass tag se- 
quence within the NP up to the current oken, 
the wordclass equence within the NP from the 
current token, and the current, previous and 
next word within the NP. 
2) Should a token now marked B-NP con- 
tinue a preceding NP? Features used: type and 
structure (in terms of wordclass tags) of the cur- 
rent and the preceding two chunks, and the final 
word of the current and the preceding chunk. 
3) Should (part of) a chunk now preceding 
an NP be part of the NP? Features used: type 
and structure (in wordclass tags) of the current, 
preceding and next chunk (the latter being the 
NP), and the final word of the current and next 
chunk. 
For all three models, the number of different 
features i large. Normally, this would force the 
use of feature restrictions. The training sets are 
very small, however, so that the need for feature 
restrictions disappears and the full model can 
be used. On the other hand, the limited size 
of the training sets has as a disadvantage that 
hill-climbing becomes practically useless. For 
this reason, I do not use hill-climbing but simply 
take the initial first order weight factors. 
Each token is subjected to the appropriate 
model, or, if not in any of the listed situations, 
left untouched. To remove (some) resulting in- 
consistencies, I let an AWK script then change 
the IOB-tag of all comma's and coordinators 
that now end an NP into O. 
4This cannot already be the first token of an NP, 
as I-tags following a different ype of chunk are always 
immediately transformed to B-tags. 
3 Resul ts  
The Ff~=l scores for all systems are listed in Ta- 
ble 1. They vary greatly per phrase type, partly 
because of the relative difficulty of the tasks but 
also because of the variation in the number of 
relevant raining and test cases: the most fre- 
quent phrase types (NP, PP and VP) also show 
the best results. Note that three of the phrase 
types (CONJP, INTJ and LST) are too infre- 
quent o yield statistically sensible information. 
The TiMBL results are worse than the ones 
reported by Buchholz et al (1999), 5 but the lat- 
ter were based on training on WSJ sections 00- 
19 and testing on 20-24. When comparing with 
the NP scores of Daelemans et al (1999), we see 
a comparable accuracy (actually slightly higher 
because of the second level classification). 
The WPDV accuracies are almost all much 
higher. For NP, the basic and reverse model 
produce accuracies which can compete with 
the highest published non-combination accura- 
cies so far. Interestingly, the reverse mode l  
yields the best overall score. This can be ex- 
plained by the observation that many choices, 
e.g. PP/PRT and especially ADJP/part of NP, 
are based mostly on the right context, about 
which more information becomes available when 
the text is handled from right to left. The 
R&M-type IOB-tags are generally less useful 
than the standard ones, but still show excep- 
tional quality for some phrase types, e.g. PRT. 
The results for the LOB model are disappoint- 
ing, given the overall quality of the tagger used 
~FADJP----66.7, FADVP----77.9 FNp=92.3, Fpp=96.8, 
Fvp----91.8 
155 
test data precision (97.82% on the held-out 10% of LOB). I hypoth- 
esize this to be due to: a) differences in text 
type between LOB and WSJ, b) partial incom- 
patibility between the LOB tags and the WSJ 
chunks and c) insufficiency of chunker training 
set size for the more varied LOB tags. 
Combination, as in other tasks (e.g. van Hal- 
teren et al (To appear)), leads to an impressive 
accuracy increase, especially for the three most 
frequent phrase types, where there is a suffi- 
cient number of cases to train the combination 
model on. There are only two phrase types, 
ADVP and SBAR, where a base chunker (re- 
verse WPDV) manages to outperform the com- 
bination. In both cases the four normal direc- 
tion base chunkers outvote the better-informed 
reverse chunker, probably because the combina- 
tion system has insufficient training material to 
recognize the higher information value of the re- 
verse model (for these two phrase types). Even 
though the results are already quite good, I ex- 
pect that even more effective combination is 
possible, with an increase in training set size 
and the inclusion of more base chunkers, espe- 
cially ones which differ substantially from the 
current, still rather homogeneous, set. 
The corrective measures yield further im- 
provement, although less impressive. Unsur- 
prisingly, the increase is found mostly for the 
NP. The next most affected phrase type is the 
ADJP, which can often be joined with or re- 
moved from the NP. There is an increase in re- 
call for ADJP (71.23% to 71.46%), but a de- 
crease in precision (78.20% to 77.86%), leav- 
ing the FZ=I value practically unchanged. For 
ADVP, there is a loss of accuracy, most likely 
caused by the one-shot correction procedure. 
This loss will probably disappear when a proce- 
dure is used which is iterative and also targets 
other phrase types than the NP. For VP, on the 
other hand, there is an accuracy increase, prob- 
ably due to a corrected inclusion/exclusion f 
participles into/from NPs. The overall scores 
show an increase, especially due to the per-type 
increases for the very frequent NP and VP. 
All scores for the chunking system as a whole, 
including precision and recall percentages, are 
listed in Table 2. For all phrase types, the 
system yields substantially better results than 
any previously published. I attribute the im- 
provements primarily to the combination archi- 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
77.86% 
80.52% 
40.00% 
100.00% 
O.00% 
93.55% 
96.43% 
72.32% 
87.77% 
93.36% 
all 93.13% 93.51% 
recall Ff~=l 
71.46% 74.52 
79.21% 79.86 
44.44% 42.11 
50.00% 66.67 
0.00% 0.00 
94.13% 93.84 
97.78% 97.10 
76.42% 74.31 
83.18% 85.41 
93.95% 93.65 
93.32 
Table 2: Final results per chunk type, i.e. af- 
ter applying corrective measures to base chun- 
ker combination. 
tecture, with a smaller but yet valuable contri- 
bution by the corrective measures. The choice 
for WPDV proves a good one, as the WPDV 
algorithm is able to cope well with all the mod- 
eling tasks in the system. Whether it is the best 
choice can only be determined by future experi- 
ments, using other machine learning techniques 
in the same architecture. 
Re ferences  
Sabine Buchholz, Jorn Veenstra, and Walter Daele- 
mans. 1999. Cascaded grammatical relation as- 
signment. In Proceedings of EMNLP/VLC-99. 
Association for Computational Linguistics. 
W. Daelemans, S. Buchholz and J. Veenstra. 1999. 
Memory-based shallow parsing. In Proceedings of
CoNLL, Bergen, Norway. 
H. van Halteren. 2000a. A default first order family 
weight determination procedure for WPDV mod- 
els. In Proceedings of the CoNLL-2000. Associa- 
tion for Computational Linguistics. 
H. van Halteren. 2000b. The detection of inconsis- 
tency in manually tagged text. In Proceedings of
LINC2000. 
H. van Halteren, J. Zavrel, and W. Daelemans. To 
appear. Improving accuracy in wordclass tagging 
through combination ofmachine l arning systems. 
Computational Linguistics. 
E. F. Tjong Kim Sang. 2000. Noun phrase recogni- 
tion by system combination. In Proceedings o\] the 
ANLP-NAACL 2000. Seattle, Washington, USA. 
Morgan Kaufman Publishers. 
E. F. Tjong Kim Sang and S. Buchholz. 2000. Intro- 
duction to the CoNLL-2000 shared task: Chunk- 
ing. In Proceedings ofthe CoNLL-2000. Associa- 
tion for Computational Linguistics. 
156 
Teaching NLP/CL through Games: the Case of Parsing 
 
Hans van Halteren  
Department of Language and Speech 
University of Nijmegen 
P.O.Box 9103, NL-6500 HD, Nijmegen, The Netherlands 
hvh@let.kun.nl 
 
 
Abstract 
This paper advocates the use of games 
in teaching NLP/CL in cases where 
computational experiments are 
impossible because the students lack 
the necessary skills. To show the 
viability of this approach, three games 
are described which together teach 
students about the parsing process. 
The paper also shows how the specific 
game formats and rules can be tuned 
to the teaching goals and situations, 
thus opening the way to the creation 
of further teaching games. 
1 Introduction 
Experience is the best teacher, as proverbial 
wisdom tells us. This should bode well for 
teaching NLP/CL, where the implementation is 
normally as important as the theory, and 
students can get hands-on experience with most 
of the field. They can use existing NLP/CL 
systems, seeing what inputs can be dealt with, 
what outputs are produced, and what the effects 
of different parameters settings are. They can 
even (re)implement parts of systems, or whole 
systems of their own. So, at a first glance 
NLP/CL appears to be the ideal field for 
teaching by means of experiments. However, 
there are also circumstances which make 
personal experimentation impossible. One 
obvious limitation is shortage of time. 
Experiments, especially ones in which a system 
is thoroughly examined, tend to take up a 
prohibitive amount of time for most course 
schedules. This can be worked around by 
focusing the experiments on the more important 
and/or more widely instructive aspects. Harder 
to work around is the situation where 
experiments are impossible because the students 
lack the necessary knowledge. Students cannot 
program a system if they have insufficient 
programming skills; they cannot alter 
computational grammars if they have 
insufficient linguistic skills. This problem 
typically occurs in introductory courses, e.g. for 
general linguistics students, where the necessary 
knowledge is acquired only later (or never at 
all). If we want to keep the added value of 
personal experience, we will have to cast the 
intended experiencing into a different form. 
In this paper I propose the use of games, 
another time-honored learning method, even for 
linguistics (cf. Grammatical Game in Rhyme (A 
Lady, 1802), teaching parts of speech, or, more 
recently, WFF ?N PROOF games like Queries ?n 
Theories (Allen et al, 1970), teaching formal 
grammars). I will not go into computer games or 
simulations (e.g. VISPER; Nouza et al, 1997), 
assuming these to be sufficiently known, but 
will focus on three major types of ?unplugged? 
games instead: card games, board games and 
roleplaying games. In the following sections, I 
give an example of each type. Together, these 
examples form an introduction to parsing and 
parsers, showing what kind of grammatical units 
are used (card game), how sentences can be 
broken down into these units by following a 
recursive transition network (RTN; board game) 
and how a parser can decide which route to take 
within the RTN (roleplaying game). 
In all three games I use the descriptive model 
underlying the TOSCA/ICE parser (cf. Oostdijk, 
2000), which in turn took its inspiration from the 
widely used descriptive grammars of Quirk et al 
(1972, 1985). This is a constituent structure 
model where each constituent is labelled with a 
syntactic category (signifying what type of 
constituent it is) and a syntactic function 
(signifying what the constituent?s role is in the 
                       July 2002, pp. 1-9.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
immediately dominating constituent).1 
Furthermore, all utterances and analyses in the 
games are taken from actual syntactically 
analysed corpus material, to be exact from a 
single 20,000 word sample taken from a crime 
novel (Allingham, 1965). This choice of text 
material and descriptive model is not just made 
out of convenience. I feel that it is important that 
the students work with ?real? examples, and not 
with especially constructed sentences and/or toy 
grammars. 
2 
                                                     
A Card Game on Syntactic 
Building Blocks 
The introduction to parsing starts with a card 
game about syntactic constituents and their 
interrelations. After all, if we want the students 
to understand what a parser does, they will first 
have to learn about the building blocks that are 
used in syntactic analysis. Even if they have 
already taken a syntax course, it will be 
necessary to familiarize them with the specific 
grammatical units used in our own ?parser? (i.e. 
those from the TOSCA/ICE model). As the 
main goal is familiarization with the 
terminology, we do not want to spend too much 
time on this. Also, this may be the students? first 
encounter with syntactic analysis, so they should 
be able to focus on the sentences and not be 
distracted by game rules. These two demands 
lead us to create a short (half hour) card game 
with simple rummy-like rules, Ling Rummy. 
The Ling Rummy deck consists of 54 cards,2 
each of which (see Figure 1) depicts a syntactic 
function (e.g. CO = object complement), a 
terminal syntactic category (e.g. ART = article), 
a non-terminal syntactic category (e.g. NP = 
noun phrase), and an utterance. The goal of the 
game is to form combinations of three cards, a 
constituent in the utterance shown on the first 
card having a category shown on the second and 
the function shown on the third card (e.g. in 
Figure 1 ?absolutely quiet? is an adjective 
phrase (AJP) functioning as an object 
complement (CO)). This means that the 
elements on the same card need not be related, 
but that elements needed for combinations must 
actually be spread out carefully over different 
cards, so it remains possible to form all of the 
less frequent combinations with cards from one 
single deck.  
Figure 1: Three game cards from Ling Rummy 
with a scoring combination 
1 A list of the categories and functions used in this 
paper can be found in Appendix A. 
2 Card decks are typically printed in sheets of 54 or 
55 cards. 54 is also a good number for do-it-yourself 
construction, as 54 cards can be printed as 6 sheets 
(A4 or letter)  with 9 cards each. For Ling Rummy, a 
single 54-card deck suffices. 
At the start of the game, the players are dealt 
nine cards. Then follow three rounds of play. In 
each round a student first draws a new card, 
either from the draw pile or from earlier 
discards, then scores one combination and 
finally discards one card. Forming a 
combination brings a number of points, which is 
determined by multiplying the point scores for 
the function and the category (e.g. CO:4 x AJP:3 
= 12). If no combination can be formed, three 
non-combining cards must be ?scored? and ten 
points are deducted from the player?s score.  
The limitation to three rounds of play helps 
keep up the pace of the game, but also puts 
pressure on the players to focus on all 
combinations. During the first round, there are 
generally several options and the highest scoring 
one can be selected. During the second round, 
the players have to find a balance between 
getting a high score in this round and avoiding a 
point deduction during the last round. During 
the last round, finally, the players usually have 
to look for that vital combination-completing 
card in the discards or else hope for a lucky 
draw.   
After reading a short description of the 
function and category indications (somewhat 
more extensive than that in Appendix A), the 
students learn all they need to learn by playing 
the game, typically in groups of three or four. 
They have to analyse all the utterances in their 
hand and in the available discards in order to 
form the best-scoring combinations, and they 
have to check the combinations played by their 
opponents.3 If there is a disagreement among the 
players, they can refer to an accompanying 
booklet containing the analyses as made during 
the original annotation of the corpus. If the 
students have a problem with the analysis found 
in the booklet, they will have to call on the 
teacher for arbitration and/or more explanation. 
The students are likely to give special 
attention to the less frequent functions and 
categories because of their higher scores. More 
frequent combinations do not really need all that 
much attention, but are guaranteed to get some 
anyway when the student wants to avoid the 
point deduction in the last round and needs to 
prepare a sure combination. As all functions and 
categories, as well as most combinations, have 
to be present in the single deck, their frequencies 
deviate from those in real text. As a result, the 
students will not get the right feeling for those 
frequencies by playing this game. However, 
some indication is given by the difference in 
scores. Furthermore, the students do not play 
this game long enough to develop erroneous 
intuitions about frequencies and the actual 
frequencies get sufficient attention in the board 
game which follows. 
3 
                                                     
A Board Game on Syntactic 
Analysis 
The next step in getting to know parsers is the 
actual complete analysis of whole utterances in 
the way that a parser is supposed to do it. This 
necessarily takes some more time, say one to 
two hours. Also, much more information needs 
to be presented at the same time, which is not 
possible in a card game format but acceptable in 
a board game. The rules can be a bit more 
complicated as well, but not much, as the focus 
has to remain on syntax. However, in this 
particular case, a rule mechanism is needed to 
force the players to pay attention to each others? 
analysis activities and not only to their own. 
This interaction can be achieved by having 
                                                     
3 In the standard game, a scoring player points out the 
combination and the others merely check if they 
agree whether the combination is correct. It is also 
possible to have the other players try to find a 
combination in the three cards themselves, possibly 
for a (partial) score.  However, this would lead to a 
much slower game. 
players control elements that other players need 
in their analysis activities. 
There are at least two natural models for this 
type of control. The first is a kind of trading 
game where grammar rewrites are pictured as 
trading a constituent for its immediate 
constituents, with transaction costs (partially) 
dependent on the likelihood of the rewrite. The 
players might be able to have monopolies in 
certain rewrites and would have to be paid by 
other players wishing to do those rewrites. The 
exact rules can be adopted from one of the many 
money, stock or property trading games in 
existence. The second option is a kind of travel 
game. The analysis process is then pictured as a 
journey along a network, e.g. a recursive 
transition network (RTN; Woods, 1970). Again, 
players can control parts of the analysis process, 
by owning sections of the network, which are 
needed by other players during their journey. 
Again, there is sufficient inspiration for exact 
rules, now to be found in one of the many 
railroad games in existence. I have chosen the 
second option, the railroad-type game. The main 
reason is that a trading game tends to lead to too 
much interaction, typically when players keep 
spending way too much time on getting better 
deals. This takes attention away from the actual 
focus of the game, the analysis, far more than is 
desired in our setting. Furthermore, the RTN 
representation is a much more attractive 
visualization of the parsing process, which is of 
course very important for a board game. The 
main disadvantage of the chosen option is that 
RTN?s are hardly ever used in this specific form 
any more. However, their link to context free 
grammars should be readily understandable for 
most students. Also, similar networks are still in 
use, e.g. in the form of finite state machines. 
The details of the resulting game, called the 
RTN Game, are inspired on the railroad game  
Box Cars (Erickson and Erickson, 1974, later 
republished as Rail Baron, Erickson et al, 
1977), in which players move their train markers 
between cities in the United States and can buy 
historical railroads like the Southern Pacific. In 
the RTN Game, the map of the United States is 
replaced by a number of subboards, depicting 
networks for Adjective Phrase (Figure 2), 
Adverb Phrase, Noun Phrase, Prepositional 
Phrase, Sentence (Figure 3)4 and Utterance, and 
4 The Sentence subboard is different from the others 
players move their pawns through the networks 
in accordance with the analysis of specific 
sentences. They can buy network arcs on the 
board and get paid if anyone uses their arcs. As 
for Ling Rummy, the optimum number of 
players is four, but the game will work well with 
three or five players. 
The main activity during the game, then, is 
moving along the board, which corresponds to 
analysing specific utterances. The utterances are 
again taken from the abovementioned corpus 
sample. However, since the game board should 
not become too cluttered, the RTN has to be 
limited in complexity, and the most infrequent 
constructions are left out. The remaining RTN 
covers about half of the utterances, several 
hundred of which are provided on game cards. A 
few simple examples can be seen on the Ling 
Rummy cards in Figure 1. However, there are 
also more involved utterances, one of the 
longest being All that had been proved so_far 
was that thought could be transferred from one 
mind to another sometimes, and that the process 
could be mechanically assisted, at_least 
as_far_as reception was concerned.5 The 
correct analyses for all utterances (i.e. the 
analyses selected by the linguist who annotated 
the sample) are provided in an accompanying 
booklet which can be consulted if problems 
arise. 
                                                                               
                                                     
in that it has three exit paths, one for intransitive 
sentence patterns (John sleeps), one for transitive 
sentence patterns (John sees Peter) and one for 
intensive sentence patterns (John is ill). These three 
cannot be spread out over several boards because 
there are arcs for coordinated  structures which jump 
back from the separate parts to the common part. 
At the start of the game, the players each get 
three utterance cards from which they can select 
one to analyse. The analysis consists of moving 
a pawn (at a die-roll determined speed) along 
the nodes of the RTN in accordance to the 
structure of the utterance. Whenever the pawn 
encounters an arc marked with a recursion sign 
(@), there is a jump to another network. The 
current position of the pawn is marked on the 
larger @ next to the arc and the pawn is then 
placed at the start of the corresponding network 
subboard. After the recursion is finished, the 
pawn returns to the marked position. When 
moving along an arc, the players have to pay for 
the use of that arc, e.g. in the AJP network 
(Figure 2) a premodifying AVP costs 20 (and 
leads to a detour along the AVP network). The 
cost for each arc is determined by its frequency 
of use in the corpus sample; higher cost 
corresponds to lower frequency. After 
completion of the analysis of an utterance, the 
player receives about one and a half times the 
total cost of that utterance, so that player capital 
grows throughout the game. Also, after 
receiving payment, a player is allowed to buy an 
arc (which has to be paid to the ?bank? and 
always costs 20), and from that moment on 
receives the payment from anyone using that 
arc. Immediately after buying a new arc, the 
player again draws three utterance cards and 
selects one of them. The game ends after a fixed 
amount of time, the winner being the player with 
the highest amount of money.  
Figure 2: The Adjective Phrase subboard in The 
RTN Game, showing network nodes and arcs 
marked with a) a syntactic function, b) a 
syntactic category , c) an @ if the category is 
non-terminal and hence needs recursion, d) the 
cost for the arc, e) spaces to mark possession of 
the arc and f) a space to mark the current 
analysis position when recursing 
In the RTN Game, the players? choices 
consist of buying the right arcs and selecting the 
right utterances to analyse. Both types of 
5 Compound words have been connected with 
underscores, e.g. as_far_as, and have to be treated as 
if they are single words. 
choices force the desired involvement with other 
players? activities. If another player?s current 
utterance route contains a high-value arc, buying 
that arc will bring an instant return, and it is 
therefore useful to (partly) analyse the other 
players? utterances. If no short-term gain is 
identified, an arc has to be selected which has 
good long term prospects. Thinking about these 
prospects brings insight into grammatical 
probabilities, as the costs of the arcs depend on 
the frequencies of occurrence. For utterance 
selection, the aspects to be considered are the 
ownership of the needed arcs and the time it will 
take to traverse the network, i.e. how long it 
takes before something new can be bought. 
Again, analysis skills and probabilistic 
reasoning are honed. Even more than in Ling 
Rummy, there may be disagreements about 
analyses, which can either be resolved by 
referring to the accompanying booklet with 
?gold standard? analyses or by discussion with 
the teacher. 
Throughout the game, the students 
experience what a parser does and in which 
terms it ?sees? the analysis process. They do not 
yet experience what the parser cannot do, as all 
the utterances in the game can be parsed with 
the RTN on the board. However, this experience 
can now be provided with a few simple 
questions, such as ?Which utterances in text X 
cannot be parsed with this RTN?? or ?How 
would the RTN have to be extended to parse 
utterance Y??. Their experience with the 
existing RTN should form a sufficient basis for 
a discussion of such subjects. 
 
 
 
 
 
 
 
Figure 3: See separate figure at end of file 
 
 
 
 
 
 
 
 
 
Figure 3: The Sentence subboard in The RTN Game (cf. Figure 2 and Footnote 4).   
A Role-Playing Game on Parsing 
Algorithms  
4 
When playing the RTN game, the students have 
total information about the sentence, as well as 
their linguistic and world knowledge, and should 
therefore be able to choose the right path 
through the network immediately, even though 
the utterance may be globally or locally 
ambiguous. They may or may not realize that a 
parser has more limited knowledge and hence 
more trouble picking a route. This realization 
can again be induced with a few direct 
questions, such as ?In utterance X, how can the 
parser know whether to pick arc Y or Z at point 
P??. However, if there is sufficient time, it may 
be more useful to let the students each take the 
role of a parser component and get a wider 
experience. This can be done in a roleplaying 
game called Analyses and Ambiguities (A&A).  
In A&A, each player plays a component of a 
parser, either one of the constituent-based 
components that are also present in the RTN 
Game or a lower-level component like a 
to . 
E  
a  
in t 
a t 
h f 
th t 
lo -
m  
c  
u  
in  
d  
v  
a  
tr f 
r f 
backtracking. By experiencing the process at 
this level, the students learn how the individual 
components have to do their work.  
You are the AJP (adjective phrase) component of the
parser. You will be called upon to give information about
the presence and extent of AJP?s in an input utterance. 
 
You know that an AJP is composed of (in this order) 
1. zero or more premodifiers, each realized by an
AVP (P=20%) 
2. a head, realized by an adjective 
3. zero or more postmodifiers, each realized by
either a PP (P=5%), an S (P=5%) or an AVP
(P=1%) 
 
However, you cannot see the input itself. If you need to
know if any potential constituents of an AJP exist at
specific positions in the input, you will have to ask your
fellow phrase structure components about the existence
of PP?s, S?s or AVP?s, or the lexical-morphological
component about what kind of word is present at a
certain position in the input. 
 
Apart from the knowledge above, and the 
communication channels to the other components, you 
have access to processing power (your brain) and your 
own bit of memory (paper, blackboard or whiteboard). 
In addition, the controlling intelligence of the 
parser is played by all the players together. They 
decide as a group how to use the component 
knowledge to perform the overall parsing task. 
If necessary, they can create a central 
administration area (e.g. on a blackboard) to 
control the process as a whole. If there are 
students without roles, the group might assign 
one of them to take on the role of a new 
component, such as a separate recursion 
administrator or an analysis tree builder. 
 The group can experiment with various 
strategies like top-down or bottom-up parsing, 
look-ahead, parallel parsing, shared forests and 
probabilistic ordering. At the start, they should 
be allowed to come up with these strategies 
themselves, but it is likely that some hints from 
the teacher will be needed at some point. 
Alternatively, different teams can be instructed 
to investigate different strategies, e.g. top-down 
versus bottom-up, and given time to develop a 
system using the given strategies. After each 
team has finished, they can demonstrate their 
resulting system to the whole group and the 
relative merit of the systems can be discussed. 
Figure 4: Instructions for the AJP player in 
A&A 
The natural group size for the use of A&A is 
the number of components in the system, i.e. 
eight if playing the six RTN?s plus a tokenizer 
and a lexical-morphological analyzer. With 
more students there is a choice between splitting 
the component parts into subparts, or having the 
additional students only take part in the group 
discussion. However, care must be taken that all 
students are actively involved in the game, and 
experience shows that attention tends to wander 
if the group is larger than ten to fifteen 
students.6  
                                                     
6 A&A itself has not been used with students as yet, 
but the same teaching technique is used at the 
computer science department, where it is well-
appreciated by the students (Wupper, 1999). Here, 
the technique is known as technodrama, indicating 
parallels with the psychodrama used in psychology. 
More information, including  video?s of students in 
action, although only in Dutch, can be found at  
www.cs.kun.nl/ita/onderwijs/onderwijsvormen/ 
technodrama/uitleg.html.  kenizer or a lexical-morphological analyzer
ach component has only limited knowledge
bout the world and a limited access to the
put. The AJP component, e.g., will know tha
n AJP may start with premodifying AVP?s, bu
as to call on the AVP component to find out i
ere are actually AVP?s present at the curren
cation. Also, it has to call on the lexical
orphological component to find out if the
urrent word is an adjective and can hence be
sed as a adjective phrase head. After gaining
formation about the accessibility of the
ifferent arcs, it may have to choose between
arious competing routes. Finally, it has to keep
n administration in order to be able to keep
ack of its various instantiations in case o
ecursion, and possibly for purposes o
5 
                                                     
Conclusion 
In any course where the students are to be taught 
about parsing by personal experience, the three 
games described in this paper can be used 
directly as a teaching aid.7 For lower level 
courses, Ling Rummy and The RTN Game can 
be completed with a few simple extra 
assignments to give a good impression of what a 
parser does, what goes on inside and how an 
underlying grammar should be constructed, all 
within two to three hours. For more 
computationally minded students, the 
algorithmic complexities of the parsing process 
can be learned through A&A, probably taking 
another hour or two. In both cases, neither 
previous skills nor access to computing facilities 
are necessary. 
It should be noted that all three games are as 
yet at the playtesting stage. We plan to use Ling 
Rummy and The RTN Game for the first time in 
an actual classroom setting during a first-year 
linguistics course later this year. The most 
important lesson I hope to learn then is how 
university students react to being asked to play 
?games?. I expect the majority to react well, but 
some might well scoff at such ?childish? 
activities. For these students, the presentation 
may have to be altered. A minimal alteration is a 
mere change in terminology. The word games 
can be weakened, e.g. into game-like activities, 
or avoided altogether, leading to terms like 
simulation or technodrama (cf. Footnote 6). A 
step further would be to remove all game 
elements, like scoring. The same game boards 
and cards can also be used for straightforward 
simulations and/or exercises. However, I expect 
the removal of the game elements to have a 
detrimental effect on the average student?s 
involvement.  
If the games are received well, the road is 
open to further teaching games. The description 
of the games in this paper is therefore meant as 
more than just an introduction to these specific 
games. It is also intended as a demonstration of 
7 The games are currently completely in English 
(subject matter, game materials and instructions). A 
version of Ling Rummy for Dutch grammatical 
analysis is under consideration. All games are freely 
available for teaching purposes, the only condition 
being that evaluative feedback is provided. Contact 
the author if you are interested.  
how to translate your teaching goals into games, 
and how conditions on the teaching goals and 
situation should influence the game format and 
rules. The game should after all not become an 
end in itself, but should clearly be a vehicle for 
teaching the appropriate lessons. In some cases 
it may be impossible to create a game that is 
both playable and teaches the desired lessons, 
but it is my contention that games can certainly 
be developed for many more aspects of 
NLP/CL. 
 
References 
A Lady. 1802. Grammatical Game in Rhyme. 
Cheapside, London: Saml. Conder. 
Allen, L.E., P. Kugel and J.K. Ross. 1970. 
Queries ?n Theories. Fairfield, IA: WFF ?N 
PROOF Learning Games Associates. 
Allingham, M. 1965. The Mind Readers. 
London: Chatto & Windus. 
Erickson, T.F. Jr. and R.S. Erickson. 1974. 
Box Cars. Atlanta, Georgia: Erickson. 
Erickson, T.F. Jr., R.S. Erickson, R. 
Hamblen and M. Uhl. 1977. Rail Baron. 
Baltimore, Maryland: The Avalon Hill Game 
Co. 
Nouza, J., N. Holada and D. Hajek. 1997. An 
Educational and Experimental Workbench for 
Visual Processing of Speech Data. In Proc. 
Eurospeech?97, Rhodes, Greece, September 
1997, pages 661-664. 
Oostdijk, N. 2000. English descriptive 
linguistics at a cross-roads. English Studies, Vol. 
81,2:127-141.  
Quirk, R., S. Greenbaum, G. Leech and J. 
Svartvik. 1972. A Grammar of Contemporary 
English. London: Longman. 
Quirk, R., S. Greenbaum, G. Leech and J. 
Svartvik. 1985. A Comprehensive Grammar of 
the English Language. London: Longman. 
Woods, W.A. 1970. Transition network 
grammars for natural language analysis. CACM 
13(10): 591-606. 
Wupper, H. 1999. Anatomy of Computer 
Systems - Experiences with a new introductory 
informatics course. Report CSI-R9914. 
Nijmegen: CSI. 
 Appendix A 
In the examples and figures in this paper, 
syntactic categories and functions are often 
indicated by their abbreviated name. This 
appendix contains a list of these abbreviations 
and the corresponding full names. 
 
The various categories are: 
 
Adj Adjective 
Adv Adverb 
AJP Adjective Phrase 
Art Article 
Aux Auxiliary verb 
AVP Adverb Phrase 
Con Connective (adverb) 
Conj Conjunction 
Lv Lexical verb 
N Noun 
NP Noun Phrase 
PP Prepositional Phrase 
S Sentence 
To Infinitival to 
 
 The various functions are: 
 
A Adverbial 
AV Auxiliary verb 
CO Object complement 
COOR Coordinator 
CS Subject complement 
HD Head 
MV Main verb 
OD Direct object 
OI Indirect object 
PC Prepositional  
complement 
POM Postmodifier 
PREM Premodifier 
PRTCL Particle 
SU Subject 
SUB Subordinator 
 

Examining the consensus between human summaries: initial
experiments with factoid analysis
Hans van Halteren
Department of Language and Speech
University of Nijmegen, The Netherlands
Simone Teufel
Computer Laboratory
Cambridge University, UK
Abstract
We present a new approach to summary evaluation
which combines two novel aspects, namely (a) con-
tent comparison between gold standard summary
and system summary via factoids, a pseudo-semantic
representation based on atomic information units
which can be robustly marked in text, and (b) use
of a gold standard consensus summary, in our case
based on 50 individual summaries of one text. Even
though future work on more than one source text is
imperative, our experiments indicate that (1) rank-
ing with regard to a single gold standard summary is
insufficient as rankings based on any two randomly
chosen summaries are very dissimilar (correlations
average ? = 0.20), (2) a stable consensus summary
can only be expected if a larger number of sum-
maries are collected (in the range of at least 30-40
summaries), and (3) similarity measurement using
unigrams shows a similarly low ranking correlation
when compared with factoid-based ranking.
1 Introduction
It is an understatement to say that measuring the
quality of summaries is hard. In fact, there is unan-
imous consensus in the summarisation community
that evaluation of summaries is a monstrously diffi-
cult task. In the past years, there has been quite a
lot of summarisation work that has effectively aimed
at finding viable evaluation strategies (Spa?rck Jones,
1999; Jing et al, 1998; Donaway et al, 2000). Large-
scale conferences like SUMMAC (Mani et al, 1999)
and DUC (2002) have unfortunately shown weak re-
sults in that current evaluation measures could not
distinguish between automatic summaries ? though
they are effective enough to distinguish them from
human-written summaries.
In principle, the best way to evaluate a summary
is to try to perform the task for which the sum-
mary was meant in the first place, and measure the
quality of the summary on the basis of degree of
success in executing the task. However, such extrin-
sic evaluations are so time-consuming to set up that
they cannot be used for the day-to-day evaluation
needed during system development. So in practice,
a method for intrinsic evaluation is needed, where
the properties of the summary itself are examined,
independent of its application.
We think one of the reasons for the difficulty of an
intrinsic evaluation is that summarisation has to call
upon at least two hard subtasks: selection of infor-
mation and production of new text. Both tasks are
known from various NLP fields (e.g. information re-
trieval and information extraction for selection; gen-
eration and machine translation (MT) for produc-
tion) to be not only hard to execute, but also hard to
evaluate. This is caused for a large part by the fact
that in both cases there is no single ?best? result, but
rather various ?good? results. It is hence no won-
der that the evaluation of summarisation, combining
these two, is even harder. The general approach for
intrinsic evaluations, then (Mani, 2001), is to sepa-
rate the evaluation of the form of the text (quality)
and its information content (informativeness).
In this paper, we will focus on the latter, the in-
trinsic evaluation of informativeness, and we will ad-
dress two aspects: the (in)sufficiency of the single
human summary to measure against, and the infor-
mation unit on which similarity measures are based.
1.1 Gold standards
In various NLP fields, such as POS tagging, systems
are tested by way of comparison against a ?gold stan-
dard?, a manually produced result which is supposed
to be the ?correct?, ?true? or ?best? result. This
presupposes, however, that there is a single ?best?
result. In summarisation there appears to be no ?one
truth?, as is evidenced by a low agreement between
humans in producing gold standard summaries by
sentence selection (Rath et al, 1961; Jing et al,
1998; Zechner, 1996), and low overlap measures be-
tween humans when gold standards summaries are
created by reformulation in the summarisers? own
words (e.g. the average overlap for the 542 single
document summary pairs in DUC-02 was only about
47%).
But even though the non-existence of any one gold
standard is generally acknowledged in the summari-
sation community, actual practice nevertheless ig-
nores this. Comparisons against a single gold stan-
dard are widely used, due to the expense of compil-
ing summary gold standards and the lack of compos-
ite measures for comparison to more than one gold
standard.
In a related field, information retrieval (IR), the
problem of subjectivity of relevance judgements is
circumvented by extensive sampling: many differ-
ent queries are collected to level out the difference
humans have in suggesting queries and in select-
ing relevant documents. While relevance judgements
between humans remain different, Voorhees (2000)
shows that the relative rankings of systems are nev-
ertheless stable across annotators, which means that
meaningful IR measures have been found despite the
inherent subjectivity of relevance judgements.
Similarly, in MT, the recent Bleu measure also
uses the idea that one gold standard is not enough.
In an experiment, Papineni et al (2001) based an
evaluation on a collection of four reference trans-
lations of 40 general news stories and showed the
evaluation to be comparable to human judgement.
Lin and Hovy (2002) examine the use of a multi-
ple gold standard for summarisation evaluation, and
conclude ?we need more than one model summary
although we cannot estimate how many model sum-
maries are required to achieve reliable automated
summary evaluation?. We explore the differences
and similarities between various human summaries
in order to create a basis for such an estimate, and as
a side-effect, also re-examine the degree of difference
between the use of a single summary gold standard
and the use of a compound gold standard.
1.2 Similarity measures
The second aspect we examine is the similarity
measure to be used for gold standard comparison.
In principle, the comparison can be done via co-
selection of extracted sentences (Rath et al, 1961;
Jing et al, 1998; Zechner, 1996), by string-based sur-
face measures (Lin and Hovy, 2002; Saggion et al,
2002), or by subjective judgements of the amount
of information overlap (DUC, 2002). The rationale
for using information overlap judgement as the main
evaluation metric for DUC is the wish to measure
the meaning of sentences rather than use surface-
based similarity such as co-selection (which does not
even take identical information expressed in different
sentences into account) and string-based measures.
In the DUC competitions, assessors judge the infor-
mational overlap between ?model units? ( elemen-
tary discourse units (EDUs), i.e. clause-like units,
taken from the gold standard summary) and ?peer
units? (sentences taken from the participating sum-
maries) on the basis of the question: ?How much
of the information in a model unit is contained in a
peer unit: all of it, most, some, any, or none.? This
overlap judgement is done for each system-produced
summary, and weighted recall measures report how
much gold standard information is present in the
summaries.
However, Lin and Hovy (2002) report low agree-
ment for two tasks: producing the human summaries
(around 40%), and assigning information overlap be-
tween them. In those cases where annotators had
to judge a pair consisting of a gold standard sen-
tence and a system sentence more than once (be-
cause different systems returned the same sentence),
they agreed with their own prior judgement in only
82% of the cases. This relatively low intra-annotator
agreement points to the fact that the overlap judge-
ment remains a subjective task where judges will
disagree. Lin and Hovy show the instability of the
evaluation, expressed in system rankings.
We propose a gold standard comparison based
on factoids, a pseudo-semantic representation of the
text, which measures information rather than string
similarity, like DUC, but which is more objective
than DUC-style information overlap judgement.
2 Data and factoid annotation
Our goal is to compare the information content of
different summaries of the same text. In this ini-
tial investigation we decided to focus on a single
text. The text used for the experiment is a BBC
report on the killing of the Dutch politician Pim
Fortuyn. It is about 600 words long, and contains
a mix of factual information and personal reactions.
Our guidelines asked the human subjects to write
generic summaries of roughly 100 words. We asked
them to formulate the summary in their own words,
so that we can also see which different textual forms
are produced for the same information.
Knowledge about the variability of expression is
important both for evaluation and system building,
and particularly so in in multi-document summarisa-
tion, where redundant information is likely to occur
in different textual forms.
We used two types of human summarisers. The
largest group consisted of Dutch students of English
and of Business Communications (with English as
a chosen second language). Of the 60 summaries
we received, we had to remove 20. Summaries were
removed if it was obvious from the summary that
the student had insufficient skill in English or if the
word count was too high (above 130 words). A sec-
ond group consisted of 10 researchers, who are either
native or near-native English speakers. With this
group there were no problems with language, for-
mat or length, and we could use all 10 summaries.
Our total number of summaries was thus 50.
2.1 The factoid as atomic information units
We use atomic semantic units called factoids to rep-
resent the meaning of a sentence. For instance, we
represent the sentence
The police have arrested a white Dutch man.
by the union of the following factoids:
FP20 A suspect was arrested
FP21 The police did the arresting
FP24 The suspect is white
FP25 The suspect is Dutch
FP26 The suspect is male
Note that in this case, factoids correspond to ex-
pressions in a FOPL-style semantics, which are com-
positionally interpreted. However, we define atom-
icity as a concept which depends on the set of sum-
maries we work with. If a certain set of potential
factoids always occurs together, this set of factoids
is treated as one factoid, because differentiation of
this set would not help us in distinguishing the sum-
maries. If we had found, e.g., that there is no sum-
mary that mentions only one of FP25 and FP26,
those factoids would be combined into one new fac-
toid ?FP27 The suspect is a Dutch man?.
Our definition of atomicity means that the
?amount? of information associated with one factoid
can vary from a single word to an entire sentence.
An example for a large chunk of information that
occurred atomically in our texts was the fact that
the victim wanted to become PM (FV71), a factoid
which covers an entire sentence. On the other hand,
a single word may contain several factoids. The word
?gunman? leads to two factoids: ?FP24 The perpe-
trator is male? and ?FA20 A gun was used in the
attack?.
The advantage of our functional, summary-set-
dependent definition of atomicity is that the defi-
nition of what counts as a factoid is more objec-
tive than if factoids had to be invented by intuition,
which is hard. One possible disadvantage of our def-
inition of atomicity (which is dependent on a given
set of summaries) is that the set of factoids used
may have to be adjusted if further summaries are
added to the collection. In practice, for a fixed set
of summaries for experiments, this is less of an issue.
We decompose meanings into separate (composi-
tionally interpreted) factoids, if there are mentions
in our texts which imply information overlap. If
one summary contains ?was murdered? and another
?was shot dead?, we can identify the factoids
FA10 There was an attack
FA40 The victim died
FA20 A gun was used
The first summary contains only the first two fac-
toids, whereas the second contains all three. That
way, the semantic similarity between related words
can be expressed.
2.2 Compositionality, generalisation and
factuality
The guidelines for manual annotation of summaries
with factoids stated that only factoids which are
explicitly expressed in the text should be marked.
When we identified factoids in our actual summary
collection, most factoids turned out to be indepen-
dent of each other, i.e. the union of the factoids can
be compositionally interpreted. However, there are
relations between factoids which are not as straight-
forward. For instance, in the case of ?FA21 Mul-
tiple shots were fired? and ?FA22 Six shots were
fired?, FA22 implies FA21; any attempt to express
the relationship between the factoids in a composi-
tional way would result in awkward factoids. We
accept that there are factoids which are most natu-
rally expressed as generalisations of other factoids,
and record for each factoid a list of factoids that are
more general than it is, so that we can include these
related factoids as well. In one view of our data, if a
summary states FA22, FA21 is automatically added.
In addition to generality, there are two further
complicated phenomena we had to deal with. The
first one is real inference, rather than generalisation,
as in the following cases:
FL52 The scene of the murder had tight security
checks
FL51 The scene of the murder was difficult to
get into
FL50 It is unclear how the perpetrator got to
the victim
FL52 implies (in the sense of real inference) FL51,
which in turn implies FL50. We again record infer-
ence relations and automatically compute the transi-
tive closure of all inferences, but we do not currently
formally distinguish them from the simpler general-
isation relations.
The second phenomenon is the description of peo-
ple?s opinions. In our source document, quotations
of the reactions of several politicians were given. In
the summaries, our subjects often generalised these
reactions and produced statements such as
Dutch as well as international politicians have expressed
their grief and disbelief.
As more than one entity can be reported as saying
the same thing, straightforward factoid union is not
powerful enough to accurately represent the attri-
bution of opinions, as our notation does not contain
variables for discourse referents and quoted state-
ments. We therefore revert to a separate set of fac-
toids, which are multiplied-out factoids that com-
bine the statement (what is being said) together with
a description of who said it. Elements of the descrip-
tion can be interpreted in a compositional manner.
For instance, the above sentence is expressed in
our notation as
OG10 Grief was expressed
OG60 Dutch persons or organizations expressed
grief
OG62 International persons or organizations
expressed grief
OG40 Politicians expressed grief
OS10 Disbelief was expressed
OS60 Dutch persons or organizations expressed
disbelief
OS62 International persons or organizations
expressed disbelief
OS40 Politicians expressed disbelief
Another problem with attribution of opinions is
that there is not always a clear distinction between
fact and opinion. For instance, the following sen-
tence is presented as opinion in the original ?Geral-
dine Coughlan in the Hague says it would have been
difficult to gain access to the media park.? Never-
theless, our summarisers often decided to represent
such opinions as facts, ie. as ?The media park was
difficult to gain entry to.? ? in fact, in our data,
every summary containing this factoid presents it
as fact. For now, we have taken the pragmatic ap-
proach that the classification of factoids into factual
and opinion factoids is determined by the actual rep-
resentation of the information in the summaries (cf.
FL51 above, where the first letter ?F? stands for
factual, the first letter ?O? for opinion).
The factoid approach can capture much finer
shades of meaning differentiations than DUC-style
information overlap does ? in an example from Lin
and Hovy (2002), an assessor judged some content
overlap between ?Thousands of people are feared
dead? and ?3,000 and perhaps ... 5,000 people have
been killed.? In our factoid representation, a dis-
tinction between ?killed? and ?feared dead? would
be made, and different numbers of people mentioned
would have been differentiated.
2.3 Factoid annotation
The authors have independently marked the pres-
ence of factoids in all summaries in the collection.
Factoid annotation of a 100 word summary takes
roughly half an hour. Even with only short guide-
lines, the agreement on which factoids are present
in a summary appears to be high. The recall of an
individual annotator with regard to the consensus
annotation is about 96%, and precision about 97%.
This means that we can work with the current fac-
toid presence table with reasonable confidence.
Whereas single summaries contain between 32 and
55 factoids, the collection as a whole contains 256
different factoids. Figure 1 shows the growth of the
number of factoids with the size of the collection (1
to 40 summaries). We assume that the curve is Zip-
fian. This observation implies that larger numbers
of summaries are necessary if we are looking for a
definitive factoid list of a document.
Figure 1: Average number of factoids in collections
of size 1?40
The maximum number of possible factoids is not
bounded by the number of factoids occurring in the
document itself. As we explained above, factoids
come into existence because they are observed in the
collection of summaries, and summaries sometimes
contain factoids which are not actually present in the
document. Examples of such factoids are ?FP31 The
suspect has made no statement?, which is true but
not stated in the source text, and ?FP23 The suspect
was arrested on the scene?, which is not even true.
The reasons for such ?creative? factoids vary from
the expression of the summarisers? personal knowl-
edge or opinion to misinterpretation of the source
text. In total we find 87 such factoids, 51 factual
ones and 36 incorrect generalisations of attribution.
Of the remaining 169 ?correct? factoids, most
(125) are factual. Within these factoids, we find
74 generalisation links. The rest of the factoids con-
cern opinions and their attribution. There are 18
descriptions of opinion, with 11 generalisation links,
and 26 descriptions of attribution, with 16 general-
isation links. For all types, we see that most facts
are being represented at differing levels of generali-
sation. Some of the generalisation links are part of
3- or 4-link hierarchies, e.g. ?FV40 Victim outspo-
ken about/campaigning on immigration issues? (26
mentions) to ?FV41 Victim was anti- immigration?
(23) to ?FV42 Victim wanted to close borders to im-
migration? (9), or ?FV50 Victim outspoken about
race/religion issues? (17 mentions) to ?FV51 Vic-
tim outspoken about Islam/Muslims? (16) to ?FV52
Victim made negative remarks about Islam? (14) to
?FV53 Victim called Islam a backward religion? (9).
It is not surprising that more specific factoids are
less frequent than their generalisations, but we ex-
pect interesting correlations between a factoid?s im-
portance and the degree and shape of the decline
of its generalisation hierarchy, especially where fac-
toids about the attribution of opinion are concerned.
This is an issue for further research.
3 Human summaries as benchmark
for evaluation
If we plan to use human summaries as a refer-
ence point for the evaluation of machine-made sum-
maries, we are assuming that there is some consensus
between the human summarisers as to which infor-
mation is important enough to include in a summary.
Whether such consensus actually exists is uncertain.
In very broad terms, we can distinguish four possible
scenarios:
1. There is a good consensus between all human
summarisers. A large percentage of the factoids
present in the summaries is in fact present in a
large percentage of the summaries. We can de-
termine whether this is so by measuring factoid
overlap.
2. There is no such overall consensus between all
summarisers, but there are subsets of summaris-
ers between whom consensus exists. Each of
these subsets has summarised from a particular
point of view, even though a generic summary
was requested, and the point of view has led
to group consensus. We can determine whether
this is so by doing a cluster analysis on the fac-
toid presence vectors. We should find clusters
if and only if group consensus exists.
3. There is no such thing as overall consensus, but
there is a difference in perceived importance be-
tween the various factoids. We can determine
whether this is the case by examining how often
each factoid is used in the summaries. Factoids
that are more important ought to be included
more often. In that case, it is still possible to
create a consensus-like reference summary for
any desired summary size.
4. There is no difference in perceived importance
of the various factoids at all. Inclusion of fac-
toids in summaries appears to be random.
3.1 Factoid frequency and consensus
We will start by examining whether an importance
hierarchy exists, as this can help us decide between
scenario 1, 3 or 4. If still necessary, we can check for
group consensus later.
If we count how often each factoid is used, it
quickly becomes clear that we do not have to worry
about worst-case scenario 4. There are clear differ-
ences in the frequency of use of the factoids. On
the other hand, scenario 1 does not appear to be
very likely either. There is full consensus on the in-
clusion of only a meager 3 factoids, which can be
summarised in 3 words:
Fortuyn was murdered.
If we accept some disagreement, and take the fac-
toids which occur in at least 90% of the summaries,
this increases the consensus summary to 5 factoids
and 6 words:
Fortuyn, a politician, was shot dead.
Setting our aims ever lower, 75% of the summaries
include 6 further factoids and the summary goes up
to 20 words:
Pim Fortuyn, a Dutch right-wing politician, was shot
dead before the election. A suspect was arrested. Fortuyn
had received threats.
A 50% threshold yields 8 more factoids and the
47-word summary:
Pim Fortuyn, a Dutch right-wing politician, was shot
dead at a radio station in Hilversum. Fortuyn was cam-
paigning on immigration issues and was expected to do
well in the election. He had received threats. There were
shocked reactions. Political campaigning was halted. The
police arrested a man.
If we want to arrive at a 100-word summary (ac-
tually 104), we need to include 26 more factoids, and
we need to allow all factoids which occur in at least
30% of the summaries:
Pim Fortuyn was shot six times and died shortly after-
wards. He was attacked when leaving a radio station in
the (well-secured) media park in Hilversum. The Dutch
far-right politician was campaigning on an anti- immi-
gration ticket and was outspoken about Islam. He was
expected to do well in the upcoming election, getting at
least 15% of the votes. Fortuyn had received threats. He
expected an attack and used bodyguards. Dutch and in-
ternational politicians were shocked and condemned the
attack. The Dutch government called a halt to political
campaigning. The gunman was chased. The police later
arrested a white Dutch man. The motive is unknown.
We conclude that the extreme scenarios, full con-
sensus and full absence of consensus, can be rejected
for this text. This leaves the question whether the
partial consensus takes the form of clusters of con-
senting summarisers.
3.2 Summariser clusters
In order to determine whether the summarisers can
be assigned to groups within which a large amount of
consensus can be found, we turn to statistical tech-
niques. We first form 256-dimensional binary vectors
recording the presence of each of the factoids in each
?2 ?1 0 1 2
?
2
?
1
0
1
2
R001
R002
R003
R004
R005
R006
R007
R008
R009
R010
S001
S002S003
S004
S005
S007
S009
S010
S011S012
S013
S014
S015
S016
S017
S018
S019S023S027
S028S030
S031
S032
S033
S034
S036
S038
S041
S042
S045
S046
S048
S049
S051
S053
S054
S055
S056
S057 S061
Cons
Figure 2: Classical multi-dimensional scaling of dis-
tances between factoid vectors into two dimensions
summariser?s summary. We also added a vector for
the 104-word consensus summary above (?Cons?).
We then calculate the distances between the vari-
ous vectors and use these as input for classical multi-
dimensional scaling. The result of scaling into two
dimensions is shown in Figure 2.
Only a few small clusters appear to emerge. Al-
though we certainly cannot conclude that there are
no clusters, we would have expected more clearly de-
limited groups of summarisers, i.e. different points
of view, if scenario 2 described the actual situation.
For now we will assume that, for this document, sce-
nario 3 is the most likely.
3.3 The consensus summary as an
evaluation tool
Two of the main demands on a gold standard generic
summary for evaluation are: a) that it contains the
information deemed most important in the docu-
ment and b) that two gold standard summaries con-
structed along the same lines lead to the same, or
at least very similar, ranking of a set of summaries
which are evaluated.
If we decide to use a single human summary as
a gold standard, we in fact assume that this hu-
man?s choice of important material is acceptable for
all other summary users, which it the wrong assump-
tion, as the lack of consensus between the various
human summaries shows. We propose that the use
of a reference summary which is based on the factoid
importance hierarchy described above, as it uses a
less subjective indication of the relative importance
of the information units in the text across a popu-
lation of summary writers. The reference summary
would then take the form of a consensus summary,
in our case the 100-word compound summary on the
basis of factoids over the 30% threshold.
The construction of the consensus summary would
indicate that demand a) will be catered for, but we
still have to check demand b). We can do this by
computing rankings based on the F-measure for in-
cluded factoids, and measuring the correlation coef-
ficient ? between them.
As we do not have a large number of automatic
summaries of our text available, we use our 50 hu-
man summaries as data, pretending that they are
summaries we wish to rank (evaluate).
If we compare the rankings on the basis of sin-
gle human summaries as gold standard, it turns out
that the ranking correlation ? between two ?gold?
standards is indeed very low at an average of 0.20
(variation between -0.51 and 0.85). For the consen-
sus summary, we can compare rankings for various
numbers of base summaries. After all, the consensus
summary should improve with the number of con-
tributing base summaries and ought to approach an
ideal consensus summary, which would be demon-
strated by a stabilizing derived ranking.
We investigate if this assumption is correct by cre-
ating pairs of samples of N=5 to 200 base summaries,
drawn (in a way similar to bootstrapping) from our
original sample of 50. For each pair of samples, we
automatically create a pair of consensus summaries
and then determine how well these two agree in their
ranking. Figure 3 shows how ? increases with N
(based on 1000 trials per N). At N=5 and 10, ? has
a still clearly unacceptable average 0.40 or 0.53. The
average reaches 0.80 at 45, 0.90 at 95 and 0.95 at a
staggering 180 base summaries.
We must note, however, that we have to be care-
ful with these measurements, since 40 of our 50
starting summaries were made by less experienced
non-natives. In fact, if we bootstrap pairs of N=10
base summary samples (100 trials) on just the 10
higher-quality summaries (created by natives and
near-natives), we get an average ? of 0.74. The
same experiment on 10 different summaries from
the other 40 (100 trials for choosing the 10, and for
each 100 trials to estimate average ?) yields average
??s ranging from 0.55 to 0.63. So clearly the differ-
ence in experience has its effect. Even so, even the
?better? summaries lead to a ranking correlation of
?=0.74 at N=10, which still is much lower than we
would like to see. We estimate that with this type of
summaries an acceptably stable ranking (? around
0.90) would be reached somewhere between 30 and
40 summaries.
3.4 Using unigrams instead of factoids
Apart from the need for human summaries, the
factoid-based comparisons have another problem,
5 15 25 35 45 55 65 75 85 95 110 125 140 155 170 185 200
?0
.2
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Figure 3: Correlation coefficient ? between rankings
for 50 summaries on the basis of two consensus sum-
maries, each based on a size N base summary collec-
tion, for N between 5 and 200
viz. the need for human interpretation when map-
ping summaries to factoid lists. The question is
whether simpler measures might not be equally in-
formative. We investigate this using unigram over-
lap, following Papineni et al (2001) in their sug-
gestion that unigrams best represent contents, while
longer n-grams best represent fluency.
Again, we reuse our 50 summaries as summaries to
be evaluated. For each of these summaries, we cal-
culate the F-measure for the included factoids with
regard to the consensus summary shown above. In
a similar fashion, we build a consensus unigram list,
containing the 103 unigrams that occur in at least
11 summaries, and calculate the F-measure for un-
igrams. The two measures are plotted against each
other in Figure 4.
Some correlation is present (r = 0.48 and Spear-
man?s ranking correlation ? = 0.45), but there are
clearly profound differences. If we look at the rank-
ings produced from these two F-measures, S054, on
position 16 on the basis of factoids, drops to posi-
tion 37 on the basis of unigrams. S046, on the other
hand, climbs from 42nd to 4th place when consid-
ered by unigrams instead of factoids. Apart from
these extreme cases, these are also clear differences
in the top-5 for the two measurements: S030, S028,
R001, S003 and S023 are the top-5 when measuring
with factoids, whereas S032, R002, S030, S046 and
S028 are the top-5 when measuring with unigrams.
It would seem that unigrams, though they are much
cheaper, are not a viable substitute for factoids.
0.40 0.45 0.50 0.55 0.60 0.65 0.70
0.
40
0.
45
0.
50
0.
55
0.
60
F(factoids)
F(
un
ig
ra
m
s)
R001
R002
R003
R004
R005
R006
R007
R008
R009
R010 S001
S002
S003
S004
S005
S007 S009
S010
S011
S012
S013
S014
S015
S016
S017
S018
S019
S023
S027
S028
S030
S031
S032
S033
S034
S036
S038
S041 S042
S045
S046
S048
S049
S051
S053
S054
S055
S056
S057
S061
Figure 4: F-measures of summarisers with regard to
consensus data: factoid-based versus unigram-based
4 Discussion and future work
From our experiences so far, it seems that both our
innovations, viz. using multiple summaries and mea-
suring with factoids, appear to be worth pursuing
further. We summarise the results for our test text
in the following:
? We observe a very wide selection of factoids in
the summaries, only few of which are included
by all summarisers.
? The number of factoids found if new summaries
are considered does not tail off.
? There is a clear importance hierarchy of fac-
toids which allows us to compile a consensus
summary.
? If single summaries are used as gold standard,
the correlation between rankings based on two
such gold standard summaries is low.
? We could not find any large clusters of highly
correlated summarisers in our data.
? Stability with respect to the consensus sum-
mary can only be expected if a larger number
of summaries are collected (in the range of at
least 30-40 summaries).
? A unigram-based measurement shows only low
correlation with the factoid-based measure-
ment.
The information that is gained through multi-
ple summaries with factoid-similarity is insufficiently
approximated with the currently used substitutes,
as the observations above show. However, what we
have described here must clearly be seen as an initial
experiment, and there is yet much to be done.
First of all, the notation of the factoid (currently
flat atoms) needs to be made more expressive, e.g.
by the addition of variables for discourse referents
and events, which will make factoids more similar
to FOPL expressions, and/or by the use of a typing
mechanism to indicate the various forms of infer-
ence/implication.
We also need to identify a good weighting scheme
to be used in measuring similarity of factoid vec-
tors. The weighting should correct for the variation
between factoids in information content, for their
different position along an inference chain, and pos-
sibly for their position in the summary. It should
also be able to express some notion of importance
of the factoids, e.g. as measured by the number of
summaries containing the factoid.
Something else to investigate is the presence and
distribution of factoids, types of factoids and rela-
tions between factoids in summaries and summary
collections. We have the strong feeling that some
of our observations were tightly linked to the type
of text we used. We would like to build a balanced
corpus of texts, of various subject areas and lengths,
and their summaries, at several different lengths and
possibly even multi-document, so that we can study
this factor. An open question is how many sum-
maries we should try to get for each of the texts in
the corpus. It is unlikely we will be able to collect
50 summaries for each new text. Furthermore, the
texts of the corpus should also be summarised by as
many machine summarisers as possible, so that we
can test ranking these on the basis of factoids, in a
realistic framework.
A final line of investigation is searching for ways to
reduce the cost of factoid analysis. The first reason
why this analysis is currently expensive is the need
for large summary bases for consensus summaries.
There is yet hope that this can be circumvented by
using larger numbers of texts, as is the case in IR
and in MT, where discrepancies prove to average out
when large enough datasets are used. Papineni et al,
e.g., were able to show that the ranking with their
Bleu measure of the five evaluated translators (two
human and three machine) remained stable if only
a single reference translation was used, suggesting
that ?we may use a big corpus with a single reference
translation, provided that the translations are not all
from the same translator?. Possibly a similar aver-
aging effect will occur in the evaluation of summari-
sation so that smaller summary bases can be used.
The second reason is the need for human annotation
of factoids. Although simple unigram-based meth-
ods prove insufficient, we will hopefully be able to
come a long way in automating factoid identification
on the basis of existing NLP techniques, combined
with information gained about factoids in research
as described in the previous paragraph. All in all,
the use of consensus summaries and factoid analy-
sis, even though expensive to set up for the moment,
provides a promising alternative which could well
bring us closer to a solution to several problems in
summarisation evaluation.
References
Donaway, Robert L., Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced by
summarization evaluation measures. In Proceedings
of the ANLP/NAACL 2000 Workshop on Automatic
Summarization.
DUC. 2002. Document Understanding Conference
(DUC). Electronic proceedings, http://www-nlpir.
nist.gov/projects/duc/pubs.html.
Jing, Hongyan, Regina Barzilay, Kathleen R. McKe-
own, and Michael Elhadad. 1998. Summarization
Evaluation Methods: Experiments and Analysis. In
Dragomir R. Radev and Eduard H. Hovy, eds., Work-
ing Notes of the AAAI Spring Symposium on Intelli-
gent Text Summarization, 60?68.
Lin, Chin-Yew, and Eduard Hovy. 2002. Manual and
automatic evaluation of summaries. In DUC 2002.
Mani, Inderjeet. 2001. Automatic Summarization. John
Benjamins.
Mani, Inderjeet, Therese Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 1999.
The TIPSTER Summac Text Summarization Evalu-
ation. In Proceedings of EACL-99 , 77?85.
Papineni, K, S. Roukos, T Ward, and W-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL-02 , 311?318.
Rath, G.J, A. Resnick, and T. R. Savage. 1961. The
Formation of Abstracts by the Selection of Sentences.
American Documentation 12(2): 139?143.
Saggion, Horacio, Dragomir Radev, Simone Teufel, Wai
Lam, and Stephanie M. Strassel. 2002. Developing
Infrastructure for the Evaluation of Single and Multi-
document Summarization Systems in a Cross-lingual
Environment. In Proceedings of LREC 2002 , 747?754.
Spa?rck Jones, Karen. 1999. Automatic Summaris-
ing: Factors and Directions. In Inderjeet Mani and
Mark T. Maybury, eds., Advances in Automatic Text
Summarization, 1?12. Cambridge, MA: MIT Press.
Voorhees, Ellen. 2000. Variations in relevance judge-
ments and the measurement of retrieval effectiveness.
Information Processing and Management 36: 697?
716.
Zechner, Klaus. 1996. Fast Generation of Abstracts from
General Domain Text Corpora by Extracting Relevant
Sentences. In Proceedings of COLING-96 , 986?989.
Evaluating information content by factoid analysis: human
annotation and stability
Simone Teufel
Computer Laboratory
Cambridge University, UK
Hans van Halteren
Language and Speech
University of Nijmegen, The Netherlands
Abstract
We present a new approach to intrinsic sum-
mary evaluation, based on initial experiments
in van Halteren and Teufel (2003), which com-
bines two novel aspects: comparison of infor-
mation content (rather than string similarity)
in gold standard and system summary, mea-
sured in shared atomic information units which
we call factoids, and comparison to more than
one gold standard summary (in our data: 20
and 50 summaries respectively). In this paper,
we show that factoid annotation is highly re-
producible, introduce a weighted factoid score,
estimate how many summaries are required for
stable system rankings, and show that the fac-
toid scores cannot be sufficiently approximated
by unigrams and the DUC information overlap
measure.
1 Introduction
Many researchers in summarisation believe that
the best way to evaluate a summary is extrin-
sic evaluation (Spa?rck Jones, 1999): to measure
the quality of the summary on the basis of de-
gree of success in executing a specific task with
that summary. The summary evaluation per-
formed in SUMMAC (Mani et al, 1999) fol-
lowed that strategy. However, extrinsic eval-
uations are time-consuming to set up and can
thus not be used for the day-to-day evaluation
needed during system development. So in prac-
tice, a method for intrinsic evaluation is needed,
where the properties of the summary itself are
examined, independently of its application.
Intrinsic evaluation of summary quality is un-
deniably hard, as there are two subtasks of sum-
marisation which need to be evaluated, infor-
mation selection and text production ? in fact
these two subtasks are often separated in evalu-
ation (Mani, 2001). If we restrict our attention
to information selection, systems are tested by
way of comparison against a ?gold standard?, a
manually produced result which is supposed to
be the ?correct?, ?true? or ?best? result.
In summarisation there appears to be no ?one
truth?, but rather various ?good? results. Hu-
man subjectivity in what counts as the most im-
portant information is high. This is evidenced
by low agreement on sentence selection tasks
(Rath et al, 1961; Jing et al, 1998), and low
word overlap measures in the task of creating
summaries by reformulation in the summaris-
ers? own words (e.g. word overlap of the 542
single document summary pairs in DUC-02 av-
eraged only 47%).
But even though the non-existence of any one
gold standard is generally acknowledged in the
summarisation community, actual practice nev-
ertheless ignores this, mostly due to the expense
of compiling summary gold standards and the
lack of composite measures for comparison to
more than one gold standard.
Other fields such as information retrieval (IR)
also have to deal with human variability con-
cerning the question of what ?relevant to a
query? means. This problem is circumvented
by extensive sampling: many different queries
are collected to level out the differences in
query formulation and relevance judgements.
Voorhees (2000) shows that the relative rank-
ings of IR systems are stable across annota-
tors even though relevance judgements differ
significantly between humans. Similarly, in MT,
the recent BLEU metric (Papineni et al, 2001)
also uses the idea that one gold standard is
not enough. Their ngram-based metric derived
from four reference translations of 40 general
news stories shows high correlation with human
judgement.
Lin and Hovy (2002) examine the use of
ngram-based multiple gold standards for sum-
marisation evaluation, and conclude ?we need
more than one model summary although we
cannot estimate how many model summaries
are required to achieve reliable automated sum-
mary evaluation?. In this paper, we explore the
differences and similarities between various hu-
man summaries in order to create a basis for
such an estimate and examine the degree of dif-
ference between the use of a single summary
gold standard and the use of a consensus gold
standard for two sample texts, based on 20 and
50 summaries respectively.
The second aspect we examine is the similar-
ity measure which compares system and gold
standard summaries. In principle, the com-
parison can be done via co-selection of ex-
tracted sentences (Rath et al, 1961; Jing et al,
1998), by string-based surface measures (Lin
and Hovy, 2002), or by subjective judgements
of the amount of information overlap (DUC,
2002). String-based metrics are superior to sen-
tence co-selection, as co-selection cannot take
similar or even identical information into ac-
count if it does not occur in the sentences which
were chosen. The choice of information overlap
judgements as the main metric in DUC reflects
the intuition that human judgements of shared
?meaning? of two texts should in principle be
superior to surface-based similarity.
DUC assessors judge the informational over-
lap between ?model units? (elementary dis-
course units (EDUs), i.e. clause-like units,
taken from the gold standard summary) and
?peer units? (sentences taken from the partici-
pating summaries) on the basis of the question:
?How much of the information in a model unit is
contained in a peer unit: 100%, 80%, 60%, 40%,
20%, 0%?? Weighted recall measures report
how much gold standard information is present
in the summaries.
However, information overlap judgement is
not something humans seem to be good at, ei-
ther. Lin and Hovy (2002) show the instabil-
ity of the evaluation, expressed in system rank-
ings. They also examined those cases where
annotators incidentially had to judge a given
model?peer pair more than once (because differ-
ent systems returned the same ?peer? sentence).
In those cases, assessors agreed with their own
prior judgement in only 82% of the cases.
We propose a novel gold standard comparison
based on factoids. Identifying factoids in text is
a more objective task than judging information
overlap a` la DUC. Our annotation experiments
show high human agreement on the factoid an-
notation task. We believe this is due to the way
how factoids are defined, and due to our pre-
cise guidelines. The factoid measure also allows
quantification of the specific elements of infor-
mation overlap, rather than just giving a quan-
titative judgement expressed in percentages.
In an example from Lin and Hovy (2002), a
DUC assessor judged some content overlap be-
tween ?Thousands of people are feared dead?
and ?3,000 and perhaps ... 5,000 people have
been killed.? In our factoid representation, a
distinction between ?killed? and ?feared dead?
would be made, and different numbers of peo-
ple mentioned would have been differentiated.
Thus, the factoid approach can capture much
finer shades of meaning differentiation than
DUC-style information overlap does. Futher-
more, it can provide feedback to system builders
on the exact information their systems fail to
include or include superfluously.
We describe factoid analysis in section 2, a
method for comparison of the information con-
tent of different summaries of the same texts,
and describe our method for measuring agree-
ment and present results in section 3. We then
investigate the distribution of factoids across
the summaries in our data sets in section 4,
and define a weighted factoid score in section
5. In that section, we also perform stability ex-
periments, to test whether rankings of system
summaries remain stable if fewer than all sum-
maries which we have available are used, and
compare weighted factoid scores to other sum-
mary evaluation metrics.
2 Data and factoid annotation
We use two texts: a 600-word BBC report on
the killing of the Dutch politician Pim Fortuyn
(as used in van Halteren and Teufel (2003)),
which contains a mix of factual information and
personal reactions, and a 573-word article on
the Iraqi invasion of Kuwait (used in DUC-2002,
LA080290-0233).
For these two texts, we collected human writ-
ten generic summaries of roughly 100 words.
Our guidelines asked the human subjects to for-
mulate the summary in their own words, in or-
der to elicit different linguistic expressions for
the same information. Knowledge about the
variability of expression is important both for
evaluation and system building.
The Fortuyn text was summarised by 40
Dutch students1, and 10 NLP researchers (na-
tive or near-native English speakers), resulting
in a total of 50 summaries. For the Kuwait text,
1Another 20 summaries of the same source were re-
moved due to insufficient English or excessive length.
we used the 6 DUC-provided summaries, 17
ELSNET-02 student participants (7 summaries
removed), and summaries by 4 additional re-
searchers, resulting in a total of 20 summaries.
We use atomic semantic units called factoids
to represent the meaning of a sentence. For
instance, we represent the sentence ?The police
have arrested a white Dutch man? by the union of
the following factoids:
FP20 A suspect was arrested
FP21 The police did the arresting
FP24 The suspect is white
FP25 The suspect is Dutch
FP26 The suspect is male
Factoids are defined empirically based on the
data in the set of summaries we work with. Fac-
toid definition starts with the comparison of the
information contained in two summaries, and
gets refined (factoids get added or split) as in-
crementally other summaries are considered. If
two pieces of information occur together in all
summaries ? and within the same sentence ?
they are treated as one factoid, because dif-
ferentiation into more than one factoid would
not help us in distinguishing the summaries. In
our data, there must have been at least one
summary that contained either only FP25 or
only FP26 ? otherwise those factoids would have
been combined into a single factoid ?FP27 The
suspect is a Dutch man?. Factoids are labelled
with descriptions in natural language; initially,
these are close in wording to the factoid?s oc-
currence in the first summaries, though the an-
notator tries to identify and treat equally para-
phrases of the factoid information when they
occur in other summaries.
Our definition of atomicity implies that the
?amount? of information associated with one
factoid can vary from a single word to an en-
tire sentence. An example for a large chunk
of information that occurred atomically in our
texts was the fact that Fortuyn wanted to be-
come PM (FV71), a factoid which covers an en-
tire sentence. On the other hand, a single word
may break down into more than one factoids.
If (together with various statements in other
summaries) one summary contains ?was killed?
and another ?was shot dead?, we identify the
factoids
FA10 There was an attack
FA40 The victim died
FA20 A gun was used
The first summary contains only the first two
factoids, whereas the second contains all three.
That way, the semantic similarity between re-
lated sentences can be expressed.
When we identified factoids in our summary
collections, most factoids turned out to be in-
dependent of each other. But when dealing
with naturally occuring documents many dif-
ficult cases appear, e.g. ambiguous expressions,
slight differences in numbers and meaning, and
inference.
Another difficult phenomenon is attribution.
In both source texts, quotations of the reactions
of several politicians and officials are given, and
the subjects often generalised these reactions
and produced statements such as ?Dutch as well
as international politicians have expressed their grief
and disbelief.? Due to coordination of speak-
ers (in the subject) and coordination of reac-
tions (in the direct object), it is hard to ac-
curately represent the attribution of opinions.
We therefore introduce combinatorical factoids,
such as ? OG40 Politicians expressed grief? and
?OS62 International persons/organizations ex-
pressed disbelief? which can be combined with
similar factoids to express the above sentence.
We wrote guidelines (10 pages long) which
describe how to derive factoids from texts. The
guidelines cover questions such as: how to cre-
ate generalising factoids when numerical val-
ues vary (summaries might talk about ?200?,
?about 200? or ?almost 200 Kuwaitis were
killed?), how to create factoids dealing with at-
tribution of opinion, and how to deal with coor-
dination of NPs in subject position, cataphors
and other syntactic constructions. We believe
that written guidelines should contain all the
rules by which this process is done; this is the
only way that other annotators, who do not
have access to all the discussions the original an-
notators had, can replicate the annotation with
a high agreement. We therefore consider the
guidelines as one of the most valuable outcomes
of this exercise, and we will make them and our
annotated material generally available.
The advantage of our empirical, summary-
set-dependent definition of factoid atomicity is
that the annotation is more objective than if
factoids had to be invented by intuition of se-
mantic constructions from scratch. One possi-
ble disadvantage of our definition of atomicity
is that the set of factoids used may have to be
adjusted if new summaries are judged, as a re-
quired factoid might be missing, or an existing
one might require splitting. Using a large num-
ber of gold-standard summaries for the defini-
tion of factoids decreases the likelihood of this
happening.
3 Agreement
In our previous work, a ?definitive? list of fac-
toids was given (created by one author), and
we were interested in whether annotators could
consistently mark the text with the factoids con-
tained in this list. In the new annotation cycle
reported on here, we study the process of factoid
lists creation, which is more time-consuming.
We will discuss agreement in factoid annotation
first, as it is a more straightforward concept,
even though procedurally, factoids are first de-
fined (cf. section 3.2) and then annotated (cf.
section 3.1).
3.1 Agreement of factoid annotation
Assuming that we already have the right list of
factoids available, factoid annotation of a 100
word summary takes roughly 10 minutes, and
measuring agreement on the decision of assign-
ing factoids to sentences is relatively straight-
forward. We calculate agreement in terms of
Kappa, where the set of items to be classified are
all factoid?summary combinations (e.g. in the
case of Phase 1, N=153 factoids times 20 sen-
tences = 2920), and where there are two cate-
gories, either ?factoid is present in summary (1)?
or ?factoid is not present in summary (0)?. P(E),
probability of error, is calculated on the basis
of the distribution of the categories, whereas
P(A), probability of agreement, is calculated
as the average of observed to possible pairwise
agreements per item. Kappa is calculated as
k = P (A)?P (E)1?P (E) ; results for our two texts are
given in Figure 1.
We measure agreement at two stages in
the process: entirely independent annotation
(Phase 1), and corrected annotation (Phase 2).
In Phase 2, annotators see an automatically
generated list of discrepancies with the other
annotator, so that slips of attention can be cor-
rected. Crucially, Phase 2 was conducted with-
out any discussion. After Phase 2 measurement,
discussion on the open points took place and a
consensus was reached (which is used for the
experiments in the rest of the paper).
Figure 1 includes results for the Fortuyn text
as we have factoid?summary annotations by
both annotators for both texts. The Kappa fig-
ures indicate high agreement, even in Phase 1
(K=.87 and K=.86); in Phase 2, Kappas are as
high as .89 and .95. Note that there is a dif-
ference between the annotation of the Fortuyn
and the Kuwait text: in the Fortuyn case, there
was no discussion or disclosure of any kind in
Phase 1; one author created the factoids, and
both used this list to annotate. The agreement
of K=.86 was thus measured on entirely inde-
pendent annotations, with no prior communica-
tion whatsoever. In the case of the Kuwait text,
the prior step of finding a consensus factoid list
had already taken place, including some discus-
sion.
Fortuyn text
K N k n P(A) P(E)
Phase 1 .86 14178 2 2 .970 .787
Phase 2 .95 14178 2 2 .989 .779
Kuwait text
K N k n P(A) P(E)
Phase 1 .87 3060 2 2 .956 .670
Phase 2 .89 2940 2 2 .962 .663
Figure 1: Agreement of factoid annotation.
3.2 Agreement of factoid definition.
We realised during our previous work, where
only one author created the factoids, that the
task of defining factoids is a complicated pro-
cess and that we should measure agreement on
this task too (using the Kuwait text). Thus,
we do not have this information for the Fortuyn
text.
But how should the measurement of agree-
ment on factoid creation proceed? It is diffi-
cult to find a fair measure of agreement over set
operations like factoid splitting, particularly as
the sets can contain a different set of summaries
marked for each factoid. For instance, consider
the following two sentences: (1) M01-004 Sad-
dam Hussein said ... that they will leave the
country when the situation stabilizes. and (2)
M06-004 Iraq claims it ... would withdraw soon.
One annotator created a factoid ?(P30) Sad-
dam H/Iraq will leave the country soon/when
situation stabilises? whereas the other anno-
tator split this into two factoids (F9.21 and
F9.22). Note that the annotators use their own,
independently chosen factoid names.
Our procedure for annotation measurement is
as follows. We create a list of identity and sub-
sumption relations between factoids by the two
annotators. In the example above, P30 would
be listed as subsuming F9.21 and F9.22. It is
time-consuming but necessary to create such
a list, as we want to measure agreement only
amongst those factoids which are semantically
related. We use a program which maximises
shared factoids between two summary sentences
A1 A2 A1 A2
P30 ? F9.21 ? a 1 1 P30 ? F9.22 ? a 1 0
P30 ? F9.21 ? b 0 0 P30 ? F9.22 ? b 0 0
P30 ? F9.21 ? c 1 0 P30 ? F9.22 ? c 1 1
P30 ? F9.21 ? d 0 0 P30 ? F9.22 ? d 0 0
P30 ? F9.21 ? e 1 0 P30 ? F9.22 ? e 1 1
Figure 2: Items for kappa calculation.
to suggest such identities and subsumption re-
lations.
We then calculate Kappa at Phases 1 and 2.
It is not trivial to define what an ?item? in
the Kappa calculation should be. Possibly
the use of Krippendorff?s alpha will provide
a better approach (cf. Nenkova and Passon-
neau (2004)), but for now we measure using
the better-known kappa, in the following way:
For each equivalence between factoids A and
C, create items { A ? C ? s | s ? S } (where
S is the set of all summaries). For each fac-
toid A subsumed by a set B of factoids, create
items { A ? b ? s | b ? B, s ? S}. For exam-
ple, given 5 summaries a, b, c, d, e, Annotator
A1 assigns P30 to summaries a, c and e. An-
notator A2 (who has split P30 into F9.21 and
F9.22), assigns a to F9.21 and c and e to F9.22.
This creates the 10 items for Kappa calculation
given in Figure 2.
Results for our data set are given in Figure 3.
For Phase 1 of factoid definition, K=.7 indicates
relatively good agreement (but lower than for
the task of factoid annotation). Many of the
disagreements can be reduced to slips of atten-
tion, as the increased Kappa of .81 for Phase 2
shows.
Overall, we can observe that this high agree-
ment for both tasks points to the fact that the
task can be robustly performed in naturally oc-
curring text, without any copy-editing. Still,
from our observations, it seems that the task
of factoid annotation is easier than the task of
factoid definition.
Kuwait text
K N k n P(A) P(E)
Phase 1 .70 3560 2 2 .91 .69
Phase 2 .81 3240 2 2 .94 .67
Figure 3: Agreement of factoid definition.
One of us then used the Kuwait consensus
agreement to annotate the 16 machine sum-
maries for that text which were created by dif-
ferent participants in DUC-2002, an annotation
which could be done rather quickly. However, a
small number of missing factoids were detected,
for instance the (incorrect) factoid that Saudi
Arabia was invaded, that the invasion happened
on a Monday night, and that Kuwait City is
Kuwait?s only sizable town. Overall, the set of
factoids available was considered adequate for
the annotation of these new texts.
0 10 20 30 40 50
0
50
10
0
15
0
20
0
25
0
Number of summaries
Av
er
ag
e 
nu
m
be
r o
f f
ac
to
id
s
Figure 4: Average size of factoid inventory as a
function of number of underlying summaries.
4 Growth of the factoid inventory
The more summaries we include in the analy-
sis, the more factoids we identify. This growth
of the factoid set stems from two factors. Dif-
ferent summarisers select different information
and hence completely new factoids are intro-
duced to account for information not yet seen
in previous summaries. This factor also implies
that the factoid inventory can never be complete
as summarisers sometimes include information
which is not actually in the original text. The
second factor comes from splitting: when a new
summary is examined, it often becomes neces-
sary to split a single factoid into multiple fac-
toids because only a certain part of it is included
in the new summary. After the very first sum-
mary, each factoid is a full sentence, and these
are gradually subdivided.
In order to determine how many factoids exist
in a given set of N summaries, we simulate ear-
lier stages of the factoid set by automatically re?
merging those factoids which never occur apart
within the given set of summaries.
Figure 4 shows the average number of factoids
over 100 drawings of N different summaries from
the whole set, which grows from 1.0 to about 4.5
for the Kuwait text (long curve) and about 4.1
for the Fortuyn text (short curve). The Kuwait
curve shows a steeper incline, possibly due to
the fact that the sentences in the Kuwait text
are longer. Given the overall growth for the
total number of factoids and the number of fac-
toids per sentence, it would seem that the split-
ting factors and the new information factor are
equally productive.
Neither curve in Figure 4 shows signs that it
might be approaching an assymptote. This con-
firms our earlier conclusion (van Halteren and
Teufel, 2003) that many more summaries than
10 or 20 are needed for a full factoid inventory.2
5 Weighted factoid scores and
stability
The main reason to do factoid analysis is to
measure the quality of summaries, including
machine summaries. In our previous work, we
do this with a consensus summary. We are now
investigating different weighting factors for the
importance of factoids. Previously, the weight-
ing factors we suggested were information con-
tent, position in the summaries and frequency.
We investigated the latter two.
Each factoid we find in a summary to be eval-
uated contributes to the score of the summary,
by an amount which reflects the perceived value
of the factoid, what we will call the ?weighted
factoid score (WFS)?. The main component in
this value is frequency, i.e., the number of model
summaries in which the factoid is observed.
When frequency weighting is used by itself,
each factoid occurrence is worth one.3 We could
also assume that more important factoids are
placed earlier in a summary, and that the fre-
quency weight is adjusted on the basis of po-
sition. Experimentation is not complete, but
the adjustments appear to influence the rank-
ing only slightly. The results we present here
are those using pure frequency weights.
We noted in our earlier paper that a good
quality measure should demonstrate at least the
following properties: a) it should reward inclu-
sion in a summary of the information deemed
2It should be noted that the estimation in Figure 4
improves upon the original estimation in that paper, as
the determination of number of factoids for that figure
did not consider the splitting factor, but just counted
the number of factoids as taken from the inventory at its
highest granularity.
3This is similar to the relative utility measure intro-
duced by Radev and Tam (2003), which however oper-
ates on sentences rather than factoids. It also corre-
sponds to the pyramid measure proposed by Nenkova
and Passonneau (2004), which also considers an estima-
tion of the maximum value reachable. Here, we use no
such maximum estimation as our comparisons will all be
relative.
1 3 5 7 9 12 15 18 21 24 27 30 33 36 39 42 45 48
?0
.2
0.0
0.2
0.4
0.6
0.8
1.0
Number of summaries (N) that score is based on
Ra
nk
ing
 co
rre
lat
ion
 be
tw
ee
n t
wo
 sa
mp
lin
gs
 (a
llo
win
g r
ep
ea
ts)
 of
 N
 su
mm
ari
es
Figure 5: Correlation (Spearman?s ?) between
summary rankings on the basis of two different
sets of N summaries, for N between 1 and 50.
most important in the document and b) mea-
sures based on two factoid analyses constructed
along the same lines should lead to the same,
or at least very similar, ranking of a set of sum-
maries which are evaluated. Since our measure
rewards inclusion of factoids which are men-
tioned often and early, demand a) ought to be
satisfied by construction.
For demand b), some experimentation is in
order. For various numbers of summaries N,
we take two samples of N summaries from the
whole set (allowing repeats so that we can use N
larger than the number of available summaries;
a statistical method called ?bootstrap?). For
each sample in a pair, we use the weighted fac-
toid score with regard to that sample of N sum-
maries to rank the summaries, and then deter-
mine the ranking correlation (Spearman?s ?) be-
tween the two rankings. The summaries that we
rank here are the 20 human summaries of the
Kuwait text, plus 16 machine summaries sub-
mitted for DUC-2002.
Figure 5 shows how the ranking correlation
increases with N for the Kuwait text. Its mean
value surpasses 0.8 at N=11 and 0.9 at N=19.
At N=50, it is 0.98. What this means for the
scores of individual summaries is shown in Fig-
ure 6, which contains a box plot for the scores
for each summary as observed in the 200 draw-
ings for N=50. The high ranking correlation
and the reasonable stability of the scores shows
that our measure fulfills demand b), at least at
a high enough N. What could be worrying is
the fact that the machine summaries (right of
the dotted line) do not seem to be performing
significantly worse than the human ones (left
Submitted summaries (Human | Machine)
Sc
ore
 ba
se
d o
n 5
0 s
um
ma
rie
s, 
wit
h f
req
ue
nc
y w
eig
hti
ng
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 6: Variation in summary scores in eval-
uations based on 200 different sets of 50 model
summaries.
Submitted summaries (Human | Machine)
Sc
ore
 ba
se
d o
n 1
0 s
um
ma
rie
s, 
wit
h f
req
ue
nc
y w
eig
hti
ng
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 7: Variation in summary scores in eval-
uations based on 200 different sets of 10 model
summaries.
of the line). However, an examination of the
better scoring machine summaries show that in
this particular case, their information content
is indeed good. The very low human scores ap-
pear to be cases of especially short summaries
(including one DUC summariser) and/or sum-
maries with a deviating angle on the story.
It has been suggested in DUC circles that a
lower N should suffice. That even a value as
high as 10 is insufficient is already indicated by
the ranking correlation of only 0.76. It becomes
even clearer with Figure 7, which mirrors Figure
6 but uses N=10. The scores for the summaries
vary wildly, which means that ranking is almost
random.
Of course, the suggestion might be made that
the system ranking will most likely also be sta-
bilised by scoring summaries for more texts,
even with such a low (or even lower) N per text.
However, in that case, the measure only yields
information at the macro level: it merely gives
an ordering between systems. A factoid-based
measure with a high N also yields feedback on a
micro level: it can show system builders which
vital information they are missing and which
superfluous information they are including. We
expect this feedback only to be reliable at the
same order of N at which single-text-based scor-
ing starts to stabilise, i.e. around 20 to 30.
As the average ranking correlation between
two weighted factoid score rankings based on
20 summaries is 0.91, we could assume that
the ranking based on our full set of 20 differ-
ent summaries should be an accurate ranking.
If we compare it to the DUC information over-
lap rankings for this text, we find that the indi-
vidual rankings for D086, D108 and D110 have
correlations with our ranking of 0.50, 0.64 and
0.79. When we average over the three, this goes
up to 0.83.
In van Halteren and Teufel (2003), we com-
pared a consensus summary based on the top-
scoring factoids with unigram scores. For the 50
Fortuyn summaries, we calculate the F-measure
for the included factoids with regard to the con-
sensus summary. In a similar fashion, we build
a consensus unigram list, containing the 103
unigrams that occur in at least 11 summaries,
and calculate the F-measure for unigrams. The
correlation between those two scores was low
(Spearman?s ? = 0.45). We concluded from
this experiment that unigrams, though much
cheaper, are not a viable substitute for factoids.
6 Discussion and future work
We have presented a new information-based
summarization metric called weighted factoid
score, which uses multiple summaries as gold
standard and which measures information over-
lap, not string overlap. It can be reliably and
objectively annotated in arbitrary text, which is
shown by our high values for human agreement.
We summarise our results as follows: Factoids
can be defined with high agreement by indepen-
dently operating annotators in naturally occur-
ring text (K=.70) and independently annotated
with even higher agreement (K=.86 and .87).
Therefore, we consider the definition of factoids
intuitive and reproducible.
The number of factoids found if new sum-
maries are considered does not tail off, but
weighting of factoids by frequency and/or lo-
cation in the summary allows for a stable sum-
mary metric. We expect this can be improved
further by including an information content
weighting factor.
If single summaries are used as gold standard
(as many other summarization evaluations do),
the correlation between rankings based on two
such gold standard summaries can be expected
to be low; in our two experiments, the correla-
tions were ?=0.20 and 0.48 on average. Accord-
ing to our estimations, stability with respect
to the factoid scores can only be expected if
a larger number of summaries are collected (in
the range of 20?30 summaries).
System rankings based on the factoid score
shows only low correlation with rankings based
on a) DUC-based information overlap, and
b) unigrams, a measurement based on shared
words between gold standard summaries and
system summary. As far as b) is concerned,
this is expected, as factoid comparison abstracts
over wording and captures linguistic variation
of the same meaning. However, the ROUGE
measure currently in development is considering
various n-grams and Wordnet-based paraphras-
ing options (Lin, personal communication). We
expect that this measure has the potential for
better ranking correlation with factoid ranking,
and we are currently investigating this.
We also plan to expand our data sets to more
texts, in order to investigate the presence and
distribution of factoids, types of factoids and re-
lations between factoids in summaries and sum-
mary collections. Currently, we have two large
factoid-annotated data sets with 20 and 50 sum-
maries, and a workable procedure to annotate
factoids, including guidelines which were used
to achieve good agreement. We now plan to
elicit the help of new annotators to increase our
data pool.
Another pressing line of investigation is re-
ducing the cost of factoid analysis. The first rea-
son why this analysis is currently expensive is
the need for large summary bases for consensus
summaries. Possibly this can be circumvented
by using larger numbers of different texts, as is
the case in IR and in MT, where discrepancies
prove to average out when large enough datasets
are used. The second reason is the need for
human annotation of factoids. Although sim-
ple word-based methods prove insufficient, we
expect that existing and emerging NLP tech-
niques based on deeper processing might help
with automatic factoid identification.
All in all, the use of factoid analysis and
weighted factoid score, even though initially ex-
pensive to set up, provides a promising alterna-
tive which could well bring us closer to a solu-
tion to several problems in summarisation eval-
uation.
References
DUC. 2002. Document Understanding Con-
ference (DUC). Electronic proceedings,
http://www-nlpir.nist.gov/projects/duc/
pubs.html.
Jing, H., R. Barzilay, K. R. McKeown, and M. El-
hadad. 1998. Summarization Evaluation Meth-
ods: Experiments and Analysis. In Working Notes
of the AAAI Spring Symposium on Intelligent
Text Summarization, 60?68.
Lin, C., and E. Hovy. 2002. Manual and automatic
evaluation of summaries. In DUC 2002.
Mani, I. 2001. Automatic Summarization. John Ben-
jamins.
Mani, I., T. Firmin, D. House, G. Klein, B. Sund-
heim, and L. Hirschman. 1999. The TIPSTER
Summac Text Summarization Evaluation. In Pro-
ceedings of EACL-99 , 77?85.
Nenkova, A., and R. Passonneau. 2004. Evaluating
Content Selection in Summarization: the Pyra-
mid Method. In Proceedings of NAACL/HLT-
2003 .
Papineni, K, S. Roukos, T Ward, and W-J. Zhu.
2001. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of ACL-02 ,
311?318.
Radev, D., and D. Tam. 2003. Summarization eval-
uation using relative utility. In Proceedings of the
Twelfth International Conference on Information
and Knowledge Management , 508?511.
Rath, G.J, A. Resnick, and T. R. Savage. 1961. The
Formation of Abstracts by the Selection of Sen-
tences. American Documentation 12(2): 139?143.
Spa?rck Jones, K. 1999. Automatic Summarising:
Factors and Directions. In I. Mani and M. May-
bury, eds., Advances in Automatic Text Summa-
rization, 1?12. Cambridge, MA: MIT Press.
van Halteren, H., and S. Teufel. 2003. Examining
the consensus between human summaries: initial
experiments with factoid analysis. In Proceedings
of the HLT workshop on Automatic Summariza-
tion.
Voorhees, E. 2000. Variations in relevance judge-
ments and the measurement of retrieval effective-
ness. Information Processing and Management
36: 697?716.
