Broadening the Scope of the EAGLES/ISLE Lexical 
Standardization Initiative 
 
Nicoletta CALZOLARI 
Istituto di Linguistica Computazionale, CNR  
Area della Ricerca, Via Moruzzi 1 
Pisa, Italy, 56100 
glottolo@ilc.cnr.it 
 
Alessandro LENCI 
Dipartimento di Linguistica, Universit? di Pisa 
Via S. Maria 36 
Pisa, Italy, 56100 
alessandro.lenci@ilc.cnr.it 
 
Francesca BERTAGNA 
Dipartimento di Linguistica, Universit? di Pisa 
Via S. Maria 36 
Pisa, Italy, 56100 
francesca.bertagna@ilc.cnr.it 
Antonio ZAMPOLLI 
Istituto di Linguistica Computazionale, CNR  
Area della Ricerca, Via Moruzzi 1 
Pisa, Italy, 56100 
pisa@ilc.cnr.it 
Abstract  
ISLE is a continuation of the long standing EAGLES initiative and it is supported by EC and NSF under the 
Human Language Technology (HLT) programme. Its objective is to develop widely agreed and urgently 
demanded standards and guidelines for infrastructural language resources, tools, and HLT products. 
EAGLES itself is a well-known trademark and point of reference for HLT projects and products and its 
previous results have already become de facto widely adopted standards. Multilingual computational 
lexicons, natural interaction and multimodality, and evaluation are the three areas targeted by ISLE. In the 
first section of the paper we describe the overall goals and methodology of EAGLES/ISLE, in the second 
section we focus on the work of the Computational Lexicon Working Group, introducing its work strategy 
and the preliminary guidelines of a standard framework for multilingual computational lexicons, based on a 
general schema for the ?Multilingual ISLE Lexical Entry? (MILE). 
 
1 Introducing EAGLES/ISLE 
ISLE (International Standards for Language 
Engineering) is a continuation of the long 
standing European EAGLES (Expert Advisory 
Group for Language Engineering Standards) 
initiative (Calzolari et al, 1996), carried out 
through a number of subsequent projects 
funded by the European Commission (EC) 
since 1993. ISLE is an initiative under the 
Human Language Technology (HLT) 
programme within the EU-US International 
Research Co-operation with the aim to develop 
and promote widely agreed and urgently 
demanded HLT standards, common guidelines 
and best practice recommendations for 
infrastructural language resources (Zampolli, 
1998), (Calzolari, 1998), tools that exploit 
them, and language engineering products.  
Object of EAGLES/ISLE are large-scale 
language resources (such as text corpora, 
computational lexicons, speech corpora, 
multimodal resources), means of manipulating 
such knowledge via computational linguistic 
formalisms, mark-up languages and various 
software tools and means of assessing and 
evaluating  resources, tools and products 
(EAGLES EWG final report, 1996). 
EAGLES was set up to determine which 
aspects of our field are open to short-term de 
facto  standardisation and to encourage the 
development of such standards for the benefit 
of consumers and producers of language 
technology, through bringing together 
representatives of major collaborative 
European R&D projects, and of HLT industry, 
in relevant areas. In this respect, more than 150 
leading industrial and academic players in the 
HLT field have actively participated in the 
definition of this initiative and have lent 
invaluable support to its execution.  
Successful standards are those which respond 
to commonly perceived needs or aid in 
overcoming common problems. In terms of 
offering workable, compromise solutions, they 
must be based on some solid platform of 
accepted facts and acceptable practices.  
The current ISLE project1 targets the three 
areas of : 
-multilingual computational lexicons2, 
-natural interaction and multimodality 
(NIMM)3,  
-evaluation of HLT systems4.  
For multilingual computational lexicons, ISLE 
goals are: i) extending EAGLES work on 
lexical semantics, necessary to establish inter-
language links; ii) designing and proposing 
standards for multilingual lexicons; iii) 
developing a prototype tool to implement 
lexicon guidelines and standards; iv) creating 
exemplary EAGLES-conformant sample 
lexicons and tagging exemplary corpora for 
validation purposes; v) developing 
standardised evaluation procedures for 
lexicons.  
For NIMM, ISLE work is targeted to develop 
guidelines for: i) the creation of NIMM data 
resources; ii) interpretative annotation of 
NIMM data, including spoken dialogue in 
NIMM contexts; iii) metadata descriptions for 
large NIMM resources; iv) annotation of 
discourse phenomena.  
For evaluation, ISLE is working on: i) quality 
models for machine translation systems; ii) 
maintenance of previous guidelines - in an ISO 
based framework (ISO 9126, ISO 14598). 
Three Working Groups, and their sub-groups, 
carry out the work, according to the EAGLES 
                                                 
1 Coordinated by A. Zampolli for EU and M. Palmer for 
US, see 
http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE_Home_Pag
e.htm. 
2 EU chair: N. Calzolari; US chairs: M. Palmer and R. 
Grishman.  
3 EU chair: N. O. Bernsen; US chair: M. Liberman. 
4 EU chair: M. King; US chair: E. Hovy. 
methodology, with experts from both the EU 
and US, acting as a catalyst in order to pool 
concrete results coming from major 
international/national/industrial projects. 
Relevant common practices or upcoming 
standards are being used where appropriate as 
input to EAGLES/ISLE work. Numerous 
theories, approaches, and systems are being 
taken into account as any recommendation for 
harmonisation must take into account the needs 
and nature of the different major contemporary 
approaches.  
Results are widely disseminated, after due 
validation in collaboration with EU and US 
HLT R&D projects, National projects, and 
industry.  
In the following we concentrate on the 
Computational Lexicon Working Group 
(CLWG), trying to describe its specific 
methodology and its goal of establishing a 
general and consensual standardized 
environment for the development and 
integration of multilingual resources. The 
general vision adheres to the idea of enhancing 
the sharing and reusability of multilingual 
lexical resources, by promoting the definition 
of a common parlance for the community of 
multilingual HLT and computational lexicon 
developers. The CLWG pursues this goal by 
proposing a general schema for the encoding of 
multilingual lexical information, the MILE 
(Multilingual ISLE Lexical Entry). This has to 
be intended as a meta-entry, acting as a 
common representational layer for multilingual 
lexical resources.  
We describe the preliminary proposals of 
guidelines for the MILE, highlighting some 
methodological principles applied in previous 
EAGLES. 
2 The Computational Lexicon 
Working Group 
Existing EAGLES results in the Lexicon and 
Corpus areas are currently adopted by an 
impressive number of European - and recently 
also National ? projects and has became the 
?de-facto standard? for LR in Europe. This is a 
very good measure of the impact ? and of the 
need ? of such a standardisation initiative in 
the HLT sector. To mention just a few key 
examples:  
- the LE PAROLE/SIMPLE resources 
(morphological/syntactic/semantic  lexicons 
and corpora for 12 EU languages (Zampolli, 
1997) (Ruimy et al, 1998) (Lenci et al, 
1999) (Bel et al, 2000) rely on EAGLES 
results (Sanfilippo et al, 1996) (Sanfilippo 
et al, 1999), and are now being enlarged to 
real-size lexicons through many National 
Projects, thus building a really large 
infrastructural platform of harmonised 
lexicons in Europe, sharing the same model;   
- the ELRA Validation Manuals for Lexicons 
(Underwood and Navarreta, 1997) and 
Corpora (Burnard et al, 1997) are based on 
EAGLES guidelines;  
- morpho-syntactic encoding of lexicons and 
tagging of corpora in a very large number of 
EU, international and national projects ? and 
for more than 20 languages ? is conformant 
to EAGLES recommendations (Monachini 
and Calzolari, 1996) (Monachini and 
Calzolari, 1999) (Leech and Wilson, 1996).  
Standards must emerge from state -of-the-art 
developments. The process of standardisation, 
although by its own nature not intrinsically 
innovative, must ? and actually does ? proceed 
shoulder to shoulder with the most advanced 
research. Since ISLE involves many bodies 
active in EU-US NLP and speech projects, 
close collaboration with these projects is 
assured and, significantly, free manpower has 
been contributed by the projects, as a sign of 
both their commitment and of the crucial 
importance they place on reusability issues. 
Lexical semantics has always represented a 
sort of wild frontier in the investigation of 
natural language. In fact, the number of open 
issues in lexical semantics both on the 
representational, architectural and content level 
might induce an actually unjustified negative 
attitude towards the possibility of designing 
standards in this difficult territory. Rather to 
the contrary, standardisation must be conceived 
as enucleating and singling out the areas in the 
open field of lexical semantics, that already 
present themselves with a clear and high 
degree of stability, although this is often 
hidden behind a number of formal differences 
or representational variants, that prevent the 
possibility of exploiting and enhancing the 
aspects of commonality and the already 
consolidated achievements.  
With no intent of imposing any constraints on 
investigation and experimentation, the ISLE 
CLWG rather aims at selecting mature areas 
and results in computational lexical semantics 
and in multilingual lexicons, which can also be 
regarded as stabilised achievements, thus to be 
used as the basis for future research. Therefore, 
consolidation of a standards proposal must be 
viewed, by necessity, as a slow process 
comprising, after the phase of putting forward 
proposals, a cyclical phase involving ISLE 
external groups and projects with: i) careful 
evaluation and testing of recommendations in 
concrete applications; ii) application, if 
appropriate, to a large number of European 
languages; iii) feedback on and readjustment of 
the proposals until a stable platform is reached; 
dissemination and promotion of consensual 
recommendations. 
The process of standard definition undertaken 
by CLWG represents an essential interface 
between advanced research in the field of 
multilingual lexical semantics, and the 
practical task of developing resources for HLT 
systems and applications. It is through this 
interface that the crucial trade-off between 
research practice and applicative needs will 
actually be achieved. 
In what follows we briefly describe the two-
step strategy adopted in the journey towards 
standards design: a first activity of survey of 
existing multilingual resources both in the 
European and American research and industrial 
scenarios. A second ongoing phase aiming at 
individuating hot areas on the domains of 
multilingual lexical resources, which call ? and 
de facto  can access to ? a process of 
standardisation. 
2.1 Preliminary Step: the Survey Phase 
Following the well established EAGLES 
methodology, the first priority was to do a 
wide-range survey of bilingual/multilingual (or 
semantic monolingual) lexicons, so as to reach 
a fair level of coverage of existing lexical 
resources. With respect to this target, one of 
the first objectives is to discover and list the 
(maximal) set of (granular) basic notions 
needed to describe the multilingual level. The 
Survey of existing lexicons (Calzolari, 
Grishman and Palmer, 2001) has been 
accompanied by the analysis of the 
requirements of a few multilingual 
applications, and by the parallel analysis of 
typical cross-lingually complex phenomena5. 
The main issue is how to state in the most 
proper way the translation correspondences 
among entries in the multilingual lexicon. The 
passage from source language (SL) to target 
language (TL) makes it necessary to express 
very complex and articulated transfer 
conditions , which have to take into account as 
difficult and pervasive phenomena as argument 
switching, multi-word expressions, 
collocational patterns, etc.  
The function of an entry in a multilingual 
lexicon is to supply enough information to 
allow the system to identify a distinct sense of 
a word or phrase in SL, in many different 
contexts, and reliably associate each context 
with the most appropriate translation. The first 
step is to determine, of all the information that 
can be associated with SL lexical entries, what 
is the most relevant to a particular task. We 
decided to focus the work of survey and 
subsequent recommendations around two 
major broad categories of application: Machine 
Translation and Cross-Language Information 
Retrieval. They have partially 
different/complementary needs, and can be 
considered to represent the requirements of 
other application types. It is necessary in fact 
to ensure that any guidelines meet the 
requirements of industrial applications and that 
they are implementable. 
In the Survey, some Korean and Japanese 
examples were present in the case study 
dedicated to relevant cross-linguistic 
phenomena, (e. g. sense distinctions according 
to variation in syntactic frames/semantic type/ 
                                                 
5 Contributors are: Atkins, Bel, Bertagna, Bouillon, 
Calzolari, Dorr, Fellbaum, Grishman, Habash, Lange, 
Lehmann, Lenci, McCormick, McNaught, Ogonowski, 
Palmer, Pentheroudakis, Richardson, Thurmair, 
Vanderwende, Villegas, Vossen, Zampolli.   
domain information, differences in predicate 
argument structure, argument incorporation, 
conflation, head switching etc). 
2.2 Towards the Recommendation Phase: 
designing the MILE Architecture  
Since the architecture of the PAROLE-
SIMPLE lexicons has been selected to provide 
the necessary bootstrapping basis for the 
stepwise refinement cycle leading to MILE, we 
briefly provide here some information about 
these resources. The design of the SIMPLE 
lexicons (Bel et al, 2000) complies with the 
EAGLES Lexicon/Semantics Working Group 
guidelines (Sanfilippo et al, 1999), and the set 
of recommended semantic notions. 
The SIMPLE lexicons are built as a new layer 
connected to the PAROLE syntactic layer, and 
encode structured ?semantic types? and 
semantic (subcategorization) frames. They 
cover 12 languages (Catalan, Danish, Dutch, 
English, Finnish, French, German, Greek, 
Italian, Portuguese, Spanish, Swedish). The 
common model is designed to facilitate future 
cross-language linking: they share the same 
core ontology and the same set of semantic 
templates.  
The ?conceptual core? of the lexicons consists 
of the basic structured set of ?semantic types? 
(the SIMPLE ontology) and the basic set of 
notions to be encoded for each Semantic Unit 
(SemU): domain information, lexicographic 
gloss, argument structure, selectional 
restrictions/preferences on the arguments, 
event type, links of the arguments to the 
syntactic subcategorization frames as 
represented in the PAROLE lexicons, ?qualia? 
structure, following the Generative Lexicon 
(Pustejovsky, 1995), semantic relations, etc.. 
SIMPLE and PAROLE lexicons are layered 
resources, with links between the 
morphological and syntactic layers expressed 
in PAROLE and the semantic information 
present in SIMPLE. 
In its general design, also MILE is envisaged 
as a highly modular and layered architecture as 
described in Calzolari et al (2001). Modularity 
concerns the ?horizontal? MILE organization, 
in which independent and yet linked modules 
target different dimensions of lexical entries. 
On the other hand, at the ?vertical? level, a 
layered organization is necessary to allow for 
different degrees of granularity of lexical 
descriptions, so that both ?shallow? and ?deep? 
representations of lexical items can be 
captured. This feature is particularly crucial in 
order to stay open to the different styles and 
approaches to the lexicon adopted by existing 
multilingual systems. 
At the top level, MILE includes two main 
modules, mono-MILE, providing monolingual 
lexical representations, and multi-MILE, where 
multilingual correspondences are defined. With 
this design choice the ISLE-CLWG intends 
also to address the particularly complex and 
yet crucial issue of multilingual resource 
development through the integration of 
monolingual computational lexicons. As in the 
reference model, PAROLE/SIMPLE, Mono-
MILE is organized into independent modules, 
respectively providing morphological, 
syntactic and semantic descriptions. The latter 
surely represents the core and the most 
challenging part of the ISLE-CLWG activities, 
together with the two other crucial topics of 
collocations and multi-word expressions, 
which have often remained outside 
astandardization initiatives, and nevertheless 
have a crucial role at the multilingual level. 
This bias is motivated by the necessity of 
providing an answer to the most urgent needs 
and desiderata of next generation HLT, as also 
expressed by the industrial partners 
participating to the project. With respect to the 
issue of the representation of multi-word 
expressions in computational lexicons, the 
ISLE-CLWG is actively cooperating with the 
NSF sponsored XMELLT project (Calzolari et 
al.,  2002). 
Multi-MILE specifies a formal environment 
for the characterization of multilingual 
correspondences between lexical items. In 
particular, source and target lexical entries can 
be linked by exploiting (possibly combined) 
aspects of their monolingual descriptions. 
Moreover, in multi-MILE both syntactic and 
semantic lexical representations can also be 
enriched, so as to achieve the granularity of 
lexical description required to establish proper 
multilingual correspondences, and which is 
possibly lacking in the original monolingual 
lexicons. 
According to the ISLE approach, monolingual 
lexicons can thus be regarded as pivot lexical 
repositories, on top of which various language-
to-language multilingual modules can be 
defined, where lexical correspondences are 
established by partly exploiting and partly 
enriching the monolingual descriptions. This 
architecture guarantees the independence of 
monolingual descriptions while allowing for  
the maximum degree of flexibility and 
consistency in reusing existing monolingual 
resources to build new bilingual lexicons.  
The MILE architecture is intended to provide 
the common representational environment 
needed to implement such an approach to 
multilingual resource development, with the 
goal of maximizing the reuse, integration and 
extension of existing monolingual 
computational lexicons. 
In the process of specifying the various 
components of MILE, the ISLE-CLWG has 
adopted a two-track strategy:  
1) identifying the lexical dimensions and 
the various types of information which 
are relevant to establish multilingual 
correspondences; 
2) idefining a suitable formal data model 
to encode this information as well as 
the operations required at the 
multilingual level. 
To tackle point 1) the survey of the available 
computational lexicons (see section 2.1) has 
been complemented with a more 
lexicographic-based effort, to identify the types 
of information used in bilingual dictionaries to 
establish translation equivalents. To this 
purpose, the CLWG has organized two ?task 
forces? with the responsibility respectively of 
creating a sample of lexical entries and 
investigating the use of the so-called sense 
indicators in traditional bilingual dictionaries. 
The work on sense indicators has been carried 
out mainly by S. Atkins and P. Bouillon: sense 
indicators are the  ?clues? given by the 
lexicographer to the bilingual dictionary users 
in order to guide them to the most appropriate 
choice of equivalence in the foreign language. 
The source word with its syntactic category, 
the target words and the sense indicators were 
automatically extracted from an English-
French dictionary and then the sense indicators 
have been classified on the basis of lexical 
relevant facts (cf. Atkins et al, 2002). 
The aim of these activities has been twofold: 
on one hand, we wanted to be able to highlight 
the various types of information useful to 
determine the transfer conditions; on the other, 
we had to explore and evaluate the full 
expressive potentialities provided by the 
reference computational model (i.e. the 
PAROLE-SIMPLE architecture). 
3.2 The MILE Data Structure and 
Lexicographic Environment 
 
The CLWG is setting up a lexicographic 
environment consisting of the following four 
main components: i) the MILE Entry Skeleton, 
ii)  the MILE Lexical Data Categories, iii) the 
MILE Shared Lexical Objects, iv) the ISLE 
Lexicographic Station. 
The MILE Entry Skeleton, formalized as an 
XML DTD, is an Entity Relationship model 
that will define the general constraints for the 
construction of multilingual entries, as well as 
the grammar to build the whole array of lexical 
elements needed for a given lexical 
description. 
The MILE Lexical Data Categories will 
provide the lexical objects (syntactic and 
semantic features, semantic relations, syntactic 
constructions, predicates and arguments etc..) 
that are the basic components of MILE-
conformant lexical entries. Lexical Data 
Categories will be organized in a hierarchy and 
will be defined using RDF schema (Brickley 
and Guha, 2000) to formalize their properties 
and make their ?semantics? explicit. 
The MILE Shared Lexical Objects will 
instantiate the MILE Lexical Data Categories, 
to be used to build in an easy and 
straightforward way lexical entries. These will 
include main syntactic constructions, basic 
operations and conditions to establish 
multilingual links, macro-semantic objects, 
such as lexical conceptual templates acting as 
general constraints for the encoding of 
semantic units. 
For instance, at the multilingual level it is 
possible to identify a first set of basic 
operations that are at the basis of multilingual 
transfer tests and actions. This would include: 
i) adding to a monolingual lexical entry a new 
syntactic position (required for a given 
translation correspondence); ii) adding to a 
monolingual semantic description a new 
semantic feature (required for a given 
transla tion correspondence); iii) constraining 
the source-target correspondence to apply only 
if an existing syntactic position is realized by a 
certain type of phrase, etc. 
Lexical  objects will be identified by an URI 
and will act as a common resources for lexical 
representation, to be in turn described by RDF 
metadata. The defined lexical objects will be 
used by the lexicon (or applications) 
developers  to build and target lexical data at a 
higher level of abstraction. Thus, they have to 
be seen as a step in the direction of simplifying 
and improving the usability of the MILE 
recommendations. 
The ISLE Lexicographic Station is a 
development platform used to automatically 
generate a prototype tool starting from the 
MILE DTD. The aim of this prototype tool is 
to i) exemplify the MILE entry ii)  make 
extensive use of already existing monolingual 
resources, and iii) eventually test the guidelines 
in a real scenario. This situation led us to 
define a lexicographic station development 
platform that guarantees the portability of the 
final prototype to the final specifications as 
well as to existing monolingual resources 
which will serve as the basic data for MILE  
(for a detailed description, cf. Villegas and Bel, 
2002). 
Both at monolingual and multilingual level 
(but with particular emphasis on the latter), 
ISLE intends to start up the incremental 
definition of a more Object-Oriented layer for 
lexical description and to foster the vision of 
open and distributed lexicons, with elements 
possibly residing in different sites of the web.  
3 Enlargement to Asian Languages  
An enlargement of the group to involve also 
Asian languages is going on and 
representatives of Chinese, Japanese, Korean, 
and Thai languages have contributed to ISLE 
work and participated in some ISLE 
workshops.  
The cooperation between Asia and Europe has 
to be pursued also through new common 
initiatives, as the expression of interest for the 
creation of an Open Distributed Lexical 
Infrastructure that has been submitted to the 
European Commission for the 6th Framework 
Programme for Research.  
This expression of interest is supported by 
many non-EU participants, as the newly 
formed Asian Federation of Natural Language 
Processing Associations (AFNLPA), the 
Department of Computer Science of the 
University of Tokyo, the Korean KAIST and 
KORTERM, the Taiwanese Institute of 
Linguistics of the Academia Sinica. 
The Open Distributed Lexical Infrastructure, a 
natural development of the ISLE model, can be 
seen as a new paradigm of distributed lexicon 
creation and maintenance and it would be a 
step of great importance for the fulfilment of 
the vision of the Semantic Web (Berners-Lee, 
1998). The creation of such infrastructure has 
to be consensual and in this regard needs the 
collaboration of a group of languages as large 
as possible (for example the AFNLPA brings 
into the initiative many Asian languages, such 
as Chinese, Hindi, Indonesian, Japanese, 
Korean, Malay, Tamil, Thai and Urdu). A 
prerequisite in order to reach interoperability is 
the existence of best practices and standards 
that have been consensually agreed on or have 
been submitted to the international community 
as de-facto  standards. 
 
4 Conclusions 
In this paper we presented overall goals and 
methodological principles of the 
standardization activity of EAGLES/ISLE. In 
particular, we describe the work of the 
Computational Lexicon Working Group and its 
effort towards recommendations, focussing on 
the MILE, the multilingual lexical meta-entry 
proposed as the standard representational 
format for multilingual computational lexical 
resources. Lexical representation is articulated 
over different information layers, each 
factoring out different, but possibly inter-
related, linguistic facets of information, 
relevant in order to establish multilingual 
lexical links. We also pointed out the necessity 
to involve a broader group of languages in 
order to ensure the achievement of a real 
consensual standard. 
Acknowledgements 
We thank all the members of the ISLE CLWG 
for their active participation and contribution to 
this enterprise. 
References 
Atkins S., Bel N., Bertagna F., Bouillon P., 
Calzolari N., Fellbaum C., Grishman R., 
Lenci A., MacLeod C., Palmer M., Thurmair 
R., Villegas M., Zampolli A. (2002) From 
Resources to Applications. Designing The 
Multilingual ISLE Lexical Entry. In 
Proceedings of LREC 2002, Las Palmas, 
Canary Islands, Spain. 
Bel N., Busa, F., Calzolari, N., Gola, E., Lenci, 
A., Monachini, M., Ogonowski, A., Peters, 
I., Peters, W., Ruimy, N., Villegas, M., 
Zampolli A. (2000) Simple: A General 
Framework for the Development of 
Multilingual Lexicons. In: LREC 
Proceedings. Athens. 
Berners-Lee T. (1998) Semantic Web road 
map. Personal note. Available at: 
http://www.w3.org/DesignIssues/Semantic.ht
ml. 
Burnard L., Baker P., McEnery A., Wilson A. 
(1997) An analytic framework for the 
validation of language corpora. Report of 
the Elra Corpus Validation Group.  
Calzolari N (1998)  An Overview of Written 
Language Resources in Europe: a few 
Reflections, Facts, and a Vision. In: Rubio, 
A., Gallardo, N., Castro, R., Tejada A. (eds.) 
Proceedings of the First International 
Conference on Language Resources and 
Evaluation. Granada 217-224. 
Calzolari N., Grishman R., Palmer M. (eds.) 
(2001)  Survey of major approaches towards  
Bilingual/Multilingual Lexicons. ISLE 
Deliverable D2.1-D3.1. Pisa  
Calzolari N., Fillmore C.J., Grishman R., Ide 
N., Lenci A., MacLeod C., Zampolli A. 
(2002)  Towards Best Practice for Multiword 
Expressions in Computational Lexicons. In 
Proceedings of LREC 2002, Las Palmas, 
Canary Islands, Spain. 
Calzolari N., Lenci A., Zampolli A., Bel N., 
Villegas M., Thurmair G. (2001)  The ISLE 
in the Ocean. Transatlantic Standards for 
Multilingual Lexicons (with an eye to 
Machine Translation). In Proceedings of 
Machine Translation Summit VIII, Santiago 
de Compostela, Spain. 
Calzolari N., Mc Naught J., Zampolli A (1996)  
Eagles Final Report: Eagles Editors? 
Introduction. Pisa.  
Eagles (1996)  Evaluation of Natural Language 
Processing Systems. Final Report. CST, 
Copenhagen. Also at http://issco-
www.unige.ch/projects/ewg96/ewg96.html. 
Brickley D., Guha R. (2000) Resource 
Description Framework (RDF) Schema 
Specification 1.0, W3C Candidate 
Recommendation. Available online at 
http://www.w3.org/TR/rdf-schema. 
Leech G., Wilson A. (1996) Recommendations 
for the morphosyntactic annotation of 
corpora.  Lancaster. 
Lenci A., Busa F., Ruimy N., Gola E., 
Monachini M., Calzolari N., Zampolli A. 
(1999)  Linguistic Specifications. Simple 
Deliverable D2.1. ILC and University of 
Pisa. 
Monachini M., Calzolari N. (1996)  Synopsis 
and comparison of morphosyntactic 
phenomena encoded in lexicons and corpora. 
A common proposal and applications to 
European languages. ILC-CNR, Pisa. 
Monachini M., Calzolari N. (1999)  
Standardization in the Lexicon. In: H. van 
Halteren (ed.): Syntactic Wordclass Tagging. 
Kluwer, Dordrecht 149-173. 
Pustejovsky J. (1995)  The Generative Lexicon. 
Cambridge, MA, MIT Press. 
Ruimy N., Corazzari O., Gola E., Spanu A., 
Calzolari N., Zampolli A. (1998) The 
European LE-Parole Project: The Italian 
Syntactic Lexicon. In: Proceedings of the 
First International Conference on Language 
resources and Evaluation. Granada 241-248. 
Sanfilippo A. et al (1996) Eagles 
Subcategorization Standards. See 
http://www.icl.pi.cnr.it/EAGLES96/syntax/s
yntax.html 
Sanfilippo A. et al (1999) Eagles 
Recommendations on Semantic Encoding.. 
See http://www.ilc.pi.cnr.it/EAGLES96/rep2  
Underwood N., Navarretta C. (1997) A Draft 
Manual for the Validation of Lexica. Final 
ELRA Report. Copenhagen. 
Villegas M., Bel N. (2002) From DTDs to 
relational dBs. An automatic generation of a 
lexicographical station out off ISLE 
guidelines. In Proceedings of LREC 2002, 
Las Palmas, Canary Islands, Spain. 
Zampolli A. (1997) The PAROLE project in 
the general context of the European actions 
for Language Resources. In: 
Marcinkeviciene, R., Volz, N. (eds.): Telri 
Proceedings of the Second European 
Seminar: Language Applications for a 
Multilingual Europe. IDS/VDU, 
Manheim/Kaunas. 
Zampolli A. (1998) Introduction of the 
General Chairman. In: Rubio, A., Gallardo, 
N. Castro, R., Tejada A. (eds.): Proceedings 
of the First International Conference on 
Language Resources and Evaluation. 
Granada, Spain. 
 
Grammar and Lexicon in the Robust Parsing of Italian 
Towards a Non-Na?ve Interplay 
 
Roberto  
BARTOLINI 
Istituto di Linguistica 
Computazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
Alessandro  
LENCI 
Universit? di Pisa 
Via Santa Maria 36 
56100 PISA (Italy) 
Simonetta  
MONTEMAGNI 
Istituto di Linguistica Com-
putazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
Vito  
PIRRELLI 
Istituto di Linguistica 
Computazionale CNR 
Area della Ricerca 
Via Moruzzi 1 
56100 PISA (Italy) 
 
{roberto.bartolini, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.it 
 
Abstract 
In the paper we report a qualitative evalua-
tion of the performance of a dependency 
analyser of Italian that runs in both a non-
lexicalised and a lexicalised mode. Results 
shed light on the contribution of types of 
lexical information to parsing.    
Introduction 
It is widely assumed that rich computational 
lexicons form a fundamental component of reli-
able parsing architectures and that lexical infor-
mation can only have beneficial effects on 
parsing. Since the beginning of work on broad-
coverage parsing  (Jensen 1988a, 1988b), the 
key issue has been how to make effective use of 
lexical information. In this paper we put these 
assumptions to the test by addressing the follow-
ing questions: to what extent should a lexicon be 
trusted for parsing? What is the neat contribution 
of lexical information to overall parse success? 
We present here the results of a preliminary 
evaluation of the interplay between lexical and 
grammatical information in parsing Italian using 
a robust parsing system based on an incremental 
approach to shallow syntactic analysis. The sys-
tem can run in both a non-lexicalised and a lexi-
calised mode. Careful analysis of the results 
shows that contribution of lexical information to 
parse success is more selective than commonly 
assumed,  thus raising the parallel issues of how 
to promote a more effective integration between 
parsers and lexicons and how to develop better 
lexicons for parsing.  
1 Syntactic parsing lexicons 
Syntactic lexical information generally feeds 
parsing systems distilled in subcategorization 
frames. Subcategorization is a formal specifica-
tion of a predicate phrasal context in terms of the 
type of arguments syntactically selected by the 
predicate entry (e.g. the verb hit selects for a 
subject NP and an object NP). Lexical frames 
commonly include: i.) number of selected argu-
ments, ii.) syntactic categories of their possible 
realization (NP, PP, etc.), iii.) lexical constraints 
on the argument realization (e.g. the preposition 
heading a PP complement), and iv.) the argu-
ment functional role. Other types of syntactic in-
formation that are also found in syntactic 
lexicons are: argument optionality, verb control, 
auxiliary selection, order constraints, etc. On the 
other hand, collocation-based lexical informa-
tion is only rarely provided by computational 
lexicons, a gap often lamented in robust parsing 
system development. 
A number of syntactic computational lexi-
cons are nowadays available to the NLP com-
munity. Important examples are LDOCE 
(Procter 1987), ComLex (Grishman et al 1994), 
PAROLE (Ruimy et al 1998). These lexicons 
are basically hand-crafted by expert lexicogra-
phers, and their natural purpose is to provide 
general purpose, domain-independent syntactic 
information, covering the most frequent entries 
and frames. On the other hand, parsing systems 
often complement general lexicons with corpus-
driven, automatically harvested syntactic infor-
mation (Federici et al 1998b, Briscoe 2001, 
Korhonen 2002). Automatic acquisition of sub-
categorization frames allows systems to access 
highly context dependent constructions, to fill in 
possible lexical gaps and eventually rely on fre-
quency information to tune the relative impact of 
specific frames (Carroll et al 1998). 
Lexicon coverage is usually regarded as the 
main parameter affecting use of lexical informa-
tion for parsing. However, the real comparative 
impact of the type (rather than the mere quan-
tity) of lexical information has been seldom dis-
cussed. Our results show that the contribution of 
various lexical information types to parse suc-
cess is not uniform. The experiment focuses on a 
particular subset of the information available in 
syntactic lexicons - the representation of PP 
complements in lexical frames - tested on the 
task of PP-attachment. The reason for this 
choice is that this piece of information occupies 
a central and dominant position in existing lexi-
cons. For instance in the Italian PAROLE lexi-
con, more than one third of verb frames contain 
positions realized by a PP, and this percentage 
raises up to the near totality noun-headed 
frames. 
2 Robust Parsing of Italian 
The general architecture of the Italian parsing 
system used for testing adheres to the following 
principles: 1) modular approach to parsing, 2) 
underspecified output (whenever required), 3) 
cautious use of lexical information, generally re-
sorted to in order to refine and/or further specify 
analyses already produced on the basis of 
grammatical information. These principles un-
derlie other typical robust parsing architectures 
(Chanod 2001, Briscoe and Carroll 2002). 
The system consists of i.) CHUNK-IT 
(Federici et al 1998a), a battery of finite state 
automata for non-recursive text segmentation 
(chunking), and ii.) IDEAL (Lenci et al 2001), a 
dependency-based analyser of the full range of 
intra-sentential functional relations (e.g. subject, 
object, modifier, complement, etc.). CHUNK-IT 
requires a minimum of lexical knowledge: 
lemma, part of speech and morpho-syntactic fea-
tures. IDEAL includes in turn two main compo-
nents: (i.) a Core Dependency Grammar of 
Italian; (ii.) a syntactic lexicon of ~26,400 sub-
categorization frames for nouns, verbs and ad-
jectives derived from the Italian PAROLE 
syntactic lexicon (Ruimy et al 1998). The 
IDEAL Core Grammar is formed by ~100 rules 
(implemented as finite state automata) covering 
major syntactic phenomena,1 and organized into 
structurally-based rules and lexically-based 
rules. IDEAL adopts a slightly simplified ver-
sion of the FAME annotation scheme (Lenci et 
al. 2000), where functional relations are head-
based and hierarchically organised to make pro-
vision for underspecified representations of 
highly ambiguous functional analyses. This fea-
ture allows IDEAL to tackle cases where lexical 
information is incomplete, or where functional 
relations cannot be disambiguated conclusively 
(e.g. in the case of the argument vs. adjunct dis-
tinction). A ?confidence score? is associated 
with some of the identified dependency relations 
to determine a plausibility ranking among dif-
ferent possible analyses. 
In IDEAL, lexico-syntactic information inter-
venes only after possibly underspecified de-
pendency relations have been identified on the 
basis of structural information only. At this sec-
ond stage, the lexicon is accessed to provide ex-
tra conditions on parsing, so that the first stage 
parse can be non-monotonically altered in vari-
ous ways (see section 3.3). This strategy mini-
mises the impact of lexical gaps (whether at the 
level of lemma or of the associated subcategori-
zation frames) on the system performance (in 
particular on its coverage). 
3 The Experiment 
3.1 The Test Corpus (TC) 
The test corpus contains a selection of sentences 
extracted from the balanced partition of the Ital-
ian Syntactic Semantic Treebank (ISST, Mon-
temagni et al 2000), including articles from 
 
1 Adjectival and adverbial modification; negation; (non-
extraposed) sentence arguments (subject, object, indirect 
object); causative and modal constructions; predicative 
constructions; PP complementation and modification; em-
bedded finite and non-finite clauses; control of infinitival 
subjects; relative clauses (main cases); participial construc-
tions; adjectival coordination; noun-noun coordination 
(main cases); PP-PP coordination (main cases); cliticiza-
tion. 
contemporary Italian newspapers and periodicals 
covering a high variety of topics (politics, econ-
omy, culture, science, health, sport, leisure, etc.). 
TC consists of 23,919 word tokens, correspond-
ing to 721 sentences (with a mean sentence 
length of 33.18 words, including punctuation to-
kens). The mean number of grammatical rela-
tions per sentence is 18. 
3.2 The Baseline Parser (BP) 
The baseline parser is a non-lexicalised version 
of IDEAL including structurally-based rules 
only. The mean number of grammatical relations 
per sentence detected by BP in TC is 15. 
The output of the baseline parser is shallow in 
different respects. First, it contains underspeci-
fied analyses, resorted to whenever available 
structural information does not allow for a more 
specific syntactic interpretation: e.g. at this level, 
no distinction is made between arguments and 
modifiers, which are all generically tagged as 
?complements?. Concerning attachment, the sys-
tem tries all structurally-compatible attachment 
hypotheses and ranks them according to a confi-
dence score. Strong preference is given to 
rightmost attachments: e.g. a prepositional com-
plement is attached with the highest confidence 
score (50) to the closest, or rightmost, available 
lexical head. In the evaluation reported in section 
4, we consider top-ranked dependents only, i.e. 
those enforcing rightmost attachment. Moreover, 
in matching the relations yielded by the parser 
with the ISST relations in TC we make allowance 
for one level of subsumption, i.e. a BP relation can 
be one level higher than its ISST counterpart in 
the hierarchy of dependency relations. Finally, the 
BP output is partial with respect to those depend-
encies (e.g. a that-clause or a direct object) that 
would be very difficult to identify with a suffi-
cient degree of confidence through structurally-
based rules only.  
3.3 The Lexically-Augmented Parser (LAP) 
The lexically-augmented version of IDEAL in-
cludes both structurally-based and lexically-
based rules (using the PAROLE lexicon). In this 
lexically-augmented configuration, IDEAL first 
tries to identify as many dependencies as possi-
ble with structural information. Lexically-based 
rules intervene later to refine and/or complete 
structurally-based analyses. Those structurally-
based hypotheses that find support in the lexicon 
are assigned the highest score (60). The contri-
bution of lexically-based rules is non-monotonic:
old relations can eventually be downgraded, as 
they happen to score, in the newly ranked list of 
possible relations, lower than their lexically-
based alternatives. Furthermore, specification of 
a former underspecified relation is always ac-
companied by a re-ranking of the relations iden-
tified for a given sentence; from this re-ranking, 
restructuring (e.g. reattachment of complements) 
of the final output may follow. 
LAP output thus includes: 
a) fully specified dependency relations: e.g. an 
underspecified dependency relation such as 
?complement? (COMP), identified by a struc-
turally-based rule, is rewritten, when lexi-
cally-supported, as ?indirect object? (OBJI) 
and assigned a higher confidence value; 
b) new dependency relations: this is the case, 
for instance, of that-clauses, direct objects 
and other relation types whose identification 
is taken to be too difficult and noisy without 
support of lexical evidence; 
c) underspecified dependency relations, for 
those cases that find no lexical support. 
The mean number of grammatical relations per 
sentence detected by LAP in TC is 16. In the 
evaluation of section 4, we consider top-ranked 
dependents only (confidence score  50), corre-
sponding to either lexically-supported dependency 
relations or ? in their absence ? to rightmost at-
tachments. Again, in matching the relations 
yielded by the parser with the ISST relations in 
TC we make allowance for one level of subsump-
tion. 
4 Analysis of Results 
The parsing outputs of BP and LAP were com-
pared and projected against ISST annotation to 
assess the contribution of lexical information to 
parse success. In this paper, we focus on the 
evaluation of how and to which extent lexico-
syntactic information contributes to identifica-
tion of the proper attachment of prepositional 
complements. For an assessment of the role and 
impact of lexical information in the analysis of 
dependency pairs headed by specific words, the 
interested reader is referred to Bartolini et al
(2002). 
4.1 Quantitative Evaluation 
Table 1 summarises the results obtained by the 
two different parsing configurations (BP and 
LAP) on the task of attaching prepositional 
complements (PC). Prepositional complements 
are classified with respect to the governing head: 
PC_VNA refers to all prepositional comple-
ments governed by V(erbal), N(ominal) or 
A(djectival) heads. PC_V is the subset with a 
V(erbal) head and PC_N the subset with a 
N(ominal) head. For each PC class, precision, 
recall and f score figures are given for the differ-
ent parsing configurations. Precision is defined 
as the ratio of correctly identified dependency 
relations over all relations found by the parser 
(prec = correctly identified relations / total num-
ber of identified relations); recall refers to the ra-
tio of correctly identified dependency relations 
over all relations in ISST (recall = correctly 
identified relations / ISST relations). Finally, the 
overall performance of the parsing systems is 
described in terms of the f score, computed as 
follows: 2 prec recall / prec + recall. 
 
BP LAP ISST 
Prec recall F score Prec recall f score 
PC_VNA 3458 75,53 57,40 65,23 74,82 61,02 67,22
PC_V 1532 75,43 45,50 56,76 74,23 49,50 61,22
PC_N 1835 73,53 80,82 77,00 72,76 81,36 76,82
Table 1. Prepositional complement attachment in BP and LAP 
 
Table 2. Lexicalised attachments 
 
To focus on the role of the lexicon in either con-
firming or revising structure-based dependen-
cies, lexically-supported attachments are singled 
out for evaluation in Table 2. Their cumulative 
frequency counts are reported in the first three 
columns of Table 2 (?Lexicalised attachments?), 
together with their distribution per head catego-
ries. Lexicalised attachments include both those 
structure-based attachments that happen to be 
confirmed lexically (?Confirmed attachments?), 
and restructured attachments, i.e. when a prepo-
sitional complement previously attached to the 
closest available head to its left is eventually re-
assigned as the dependent of a farther head, on 
the basis of lexicon look-up (?Restructured at-
tachments?). Table 2 thus shows the impact of 
lexical information on the task of PP attachment. 
In most cases, 89% of the total of lexicalised at-
tachments, LAP basically confirms dependency 
relations already assigned at the previous stage. 
Newly discovered attachments, which are de-
tected thanks to lexicon look-up and re-ranking,  
amount to only 11% of all lexicalised attach-
ments, less than 3% of all PP attachments 
yielded by LAP.  
4.3 Discussion 
4.3.1 Recall and precision on noun and verb 
heads 
Let us consider the output of BP first. The strik-
ing difference in the recall of noun-headed vs 
verb-headed prepositional attachments (on com-
parable levels of precision, rows 2 and 3 of Ta-
ble 1) prompts the suggestion that the typical 
context of use of a noun is more easily described 
in terms of local, order-contingent criteria (e.g. 
rightmost attachment) than a verb context is. We 
can give at least three reasons for that. First, 
frame bearing nouns tend to select fewer argu-
 Lexicalised atts Confirmed atts Restructured atts 
total OK prec Total OK prec total OK prec 
PP_VNA 919 819 89,12 816 771 94,49 103 65 63,11
PP_V 289 244 84,43 201 194 96,52 88 61 69,32
PP_N 629 575 91,41 614 577 93,97 15 4 26,67
ments than verbs do. In our lexicon, 1693 verb-
headed frames out of 6924 have more than one 
non subject argument (24.4%), while there being 
only 1950 noun-headed frames out of 15399 
with more than one argument (12.6%). In TC, of 
2300 head verb tokens, 328 exhibit more than 
one non subject argument (14%). Rightmost at-
tachment trivially penalises such argument 
chains, where some arguments happen to be 
overtly realised in context one or more steps re-
moved from their heads. The second reason is 
sensitive to language variation: verb arguments 
tend to be dislocated more easily than noun ar-
guments, as dislocation heavily depends on sen-
tence-level (hence main verb-level) phenomena 
such as shift of topic or emphasis. In Italian, 
topic-driven argument dislocation in preverbal 
position is comparatively frequent and repre-
sents a problem for the baseline parser, which 
works on a head-first assumption. Thirdly, verbs 
are typically modified by a wider set of syntactic 
satellites than nouns are, such as temporal and 
circumstantial modifiers (Dik 1989). For exam-
ple, deverbal nouns do not inherit the possible 
temporal modifiers of their verb base (I run the 
marathon in three hours, but *the run of the 
marathon in three hours). Modifiers of this sort 
tend to be distributed in the sentence much more 
freely than ordinary arguments.  
4.3.2 Impact of the lexicon on recall 
Of the three above mentioned factors, only the 
first one has an obvious lexical character. We 
can provide a rough estimate of the impact of 
lexical information on the performance of LAP. 
The lexicon filter contributes a 9% increase of 
recall on verb complements (4% over 45.5%), 
by correctly reattaching to the verbal head those 
arguments (61) that were wrongly attached to 
their immediately preceding constituent by BP. 
This leads to an overall 49.5% recall. All re-
maining false negatives (about 48%) are i) either 
verb modifiers or ii) proper verb arguments ly-
ing out of the reach of structure-based criteria, 
due to syntactic phenomena such as complement 
dislocation, complex coordination, parenthetic 
constructions and ellipsis. We shall return to a 
more detailed analysis of false negatives in sec-
tion 4.3.4. In the case of noun complements, use 
of lexical information produces a negligible in-
crease of recall: 0.6% ( 0.5% over 80.8%). This 
is not surprising, as our test corpus contains very 
few cases of noun-headed argument chains, 
fewer than we could expect if the probability of 
their occurrence reflected the (uniform) type dis-
tribution of noun frames in the lexicon. The vast 
majority of noun-headed false negatives, as we 
shall see in more detail in a moment, is repre-
sented by modifiers. 
4.3.3 Impact of the lexicon on precision 
Reattachment is enforced by LAP when the 
preposition introducing a candidate complement 
in context is found in the lexical frame of its 
head. Table 2 shows that ~37% of the 103 re-
structured attachments proposed by the lexicon 
are wrong. Even more interestingly, there is a 
strong asymmetry between nouns and verbs. 
With verb heads, precision of lexically-driven 
reattachments is fairly high (~70%), nonetheless 
lower than precision of rightmost attachment 
(~75%). In the case of noun heads, the number 
of lexically reattached dependencies is instead 
extremely low. The percentage of mistakes is  
high, with precision dropping to 26.6%. 
The difference in the total number of restruc-
tured attachment may be again due to the richer 
complementation patterns exhibited by verbs in 
the lexicon. However, while in the case of verbs 
lexical information produces a significant im-
provement on restructured attachment precision, 
this contribution drops considerably for nouns. 
The main reason for this situation is that nouns 
tend to select semantically vacuous prepositions 
such as of much more often than verbs do. In our 
lexicon, out of 4157 frames headed by a noun, 
4015 contain the preposition di as an argument 
introducer (96.6%). Di is in fact an extremely 
polysemous preposition, heading, among others, 
also possessive phrases and other kinds of modi-
fiers. This trivially increases the number of cases 
of attachment ambiguity and eventually the pos-
sibility of getting false positives. Conversely, as 
shown by the number of confirmed attachments 
in Table 2, the role of lexical information in fur-
ther specifying an attachment with no restructur-
ing is almost uniform across nouns and verbs. 
4.3.4 False negatives  
The vast majority of undetected verb comple-
ments (80.6%) are modifiers of various kind. 
The remaining set of false negatives consists of 
48 complements (7.7%), 30 indirect objects 
(4.8%) and 43 oblique arguments (6.9%). Most 
such complements are by-phrases in passive 
constructions which are not as such very diffi-
cult to detect but just happen to fall out of the 
current coverage of LAP. More interestingly, 2/3 
of the remaining false negatives elude LAP be-
cause they are overtly realised far away from 
their verb head, often to its left. Most of these 
constructions involve argument dislocation and 
ellipsis. We can thus preliminarily conclude that 
argument dislocation and ellipsis accounts for 
about 14% of false negatives (7% over 50%). 
Finally, the number of false negatives due to at-
tachment ambiguity is almost negligible in the 
case of verbal heads. 
On the other hand, the impact of undetected 
modifiers of a verbal head on attachment recall 
is considerable. The most striking feature of this 
large subset is the comparative sparseness of 
modifiers introduced by di (of): 31 out of 504 
(6.2%). At a closer scrutiny, the majority of 
these di-phrases are either phraseological adver-
bial modifiers (di recente ?of late?, del resto ?be-
sides? etc.) or quasi-arguments headed by 
participle forms. Notably, 227 undetected modi-
fiers (45% of the total) are selected by semanti-
cally heavy and complex (possibly 
discontinuous) prepositions (davanti a ?in front 
of?, in mezzo a ?amid?, verso ?towards?, intorno 
a ?around?, contro ?against?, da ... a ?from ... to? 
etc.). As to the remaining 241 undetected modi-
fiers (48%), they are introduced by ?light? 
prepositions such as a ?to?, in ?in? and da ?from?. 
Although this 48% contains a number of diffi-
cult attachments, one can identify subsets of 
fairly reliable modifiers by focusing on the noun 
head introduced by the preposition, which usu-
ally gives a strong indication of the nature of the 
modifier, especially in the case of measure, tem-
poral and locative expressions.  
4.3.5 False positives 
Table 2 shows a prominent asymmetry in the 
precision of confirmed and restructured attach-
ments. Wrong restructured attachments are 
mainly due to a misleading match between the 
preposition introducing a PC and that introduc-
ing a slot in the lexical frame of its candidate 
head (~85%). This typically occurs with ?light? 
prepositions (e.g. di, a, etc.). Most notably, in a 
relevant subset of these mistakes, the verb or 
noun head belongs to an idiomatic multi-word 
expression. In the case of confirmed attach-
ments, about one third of false positives (~5%) 
involve multi-word expressions, in particular 
compound terms such as presidente del consig-
lio ?prime minister?, where the rightmost ele-
ment of the compound is wrongly selected as the 
head of the immediately following PP. In both 
restructured and confirmed attachments, the re-
maining cases (on average ~4%) are due to 
complex syntactic structures (e.g. appositive 
constructions, complex coordination, ellipsis 
etc.) which are outside the coverage of the cur-
rent grammar.  
Conclusion 
Larger lexicons are not necessarily better for 
parsing. The issue of the interplay of lexicon and 
grammar, although fairly well understood at the 
level of linguistic theory, still remains to be fully 
investigated at the level of parsing. In this paper, 
we tried to scratch the surface of the problem 
through a careful analysis of the performance of 
an incremental dependency analyser of Italian, 
which can run in both a non-lexicalised and a 
lexicalised mode.  
The contribution of lexical information to 
parse success is unevenly distributed over both 
part of speech categories and frame types. For 
reasons abundantly illustrated in section 4, the 
frames of noun heads are not quite as useful as 
those of verb heads, especially when available 
information is only syntactic. Moreover, while 
information on verb transitivity or clause em-
bedding is crucial to filter out noisy attachments, 
information on the preposition introducing the 
oblique complement or the indirect object of a 
verb can be misleading, and should thus be used 
for parsing with greater care. The main reason is 
that failure to register in the lexicon all possible 
prepositions actually found in real texts may 
cause undesired over-filtering of genuine 
arguments (false negatives). In many cases, 
argument prepositions are actually selected by 
the lexical head of the subcategorised argument, 
rather than by its subcategorising verb. Simi-
larly, while information about argument option-
ality vs obligatoriness is seldom confirmed in 
real language use, statistical preferences on the 
order of argument realisation can be very useful. 
Most current lexicons say very little about 
temporal and circumstantial modifiers, but much 
more can be said about them that is useful to 
parsing. First, some prepositions only occur to 
introduce verb modifiers. These semantically 
heavy prepositions, often consisting of more 
than one lexical item, play a fundamental role in 
the organization of written texts, and certainly 
deserve a special place in a parsing-oriented 
lexicon. Availability of this type of lexical in-
formation could pave the way to the develop-
ment of specialised ?mini-parsers? of those 
satellite modifiers whose structural position in 
the sentence is subject to considerable variation. 
These mini-parsers could benefit from informa-
tion about semantically-based classes of nouns, 
such as locations, measure terms, or temporal 
expressions, which should also contain indica-
tion of the preposition they are typically intro-
duced by. Clearly, this move requires 
abandoning the prejudice that lexical informa-
tion should only flow from the head to its 
dependents. Finally, availability of large 
repertoires of multi word units (both complex 
prepositions and compound terms) appears to 
have a large impact on improving parse preci-
sion.  
There is no doubt that harvesting such a wide 
range of lexical information in the quantity 
needed for accurate parsing will require exten-
sive recourse to bootstrapping methods of lexi-
cal knowledge acquisition from real texts.     
References  
Bartolini R., Lenci A., Montemagni S, Pirrelli V. 
(2002) The Lexicon-Grammar Balance in Robust 
Parsing of Italian, in Proceedings of the 3rd Inter-
national Conference on Language Resources and 
Evaluation, Las Palmas, Gran Canaria. 
Briscoe, E.J. (2001) From dictionary to corpus to 
self-organizing dictionary: learning valency asso-
ciations in the face of variation and change, in 
Proceedings of Corpus Linguistics 2001, Lancaster 
University, pp. 79-89. 
Briscoe T., Carroll J., (2002) Robust Accurate Statis-
tical Annotation of General Text, in Proceedings of 
the 3rd International Conference on Language Re-
sources and Evaluation, Las Palmas, Gran Canaria. 
Carroll, J., Minnen G., Briscoe E.J. (1998) Can sub-
categorisation probabilities help a statistical 
parser?, in Proceedings of the 6th ACL/SIGDAT 
Workshop on Very Large Corpora, Montreal, Can-
ada. 118-126. 
Chanod J.P. (2001) Robust Parsing and Beyond, in 
J.C. Junqua and G. van Noord (eds.) Robustness in 
Language and Speech Technology, Dordrecht, 
Kluwer, pp. 187-204. 
Federici, S., Montemagni, S., Pirrelli, V. (1998a) 
Chunking Italian: Linguistic and Task-oriented 
Evaluation, in Proceedings of the LREC Workshop 
on ?Evaluation of Parsing Systems?, Granada, 
Spain. 
Federici, S., Montemagni, S., Pirrelli, V., Calzolari, 
N. (1998b) Analogy-based Extraction of Lexical 
Knowledge from Corpora: the SPARKLE Experi-
ence, in Proceedings of the 1st International Con-
ference on Language resources and Evaluation, 
Granada, Spain. 
Grishman, R., Macleod C., Meyers A. (1994) 
COMLEX Syntax: Building a Computational Lexi-
con, in Proceedings of Coling 1994, Kyoto. 
Jensen K. (1988a) Issues in Parsing, in A. Blaser 
(ed.), Natural Language at the Computer, Springer 
Verlag, Berlin, pp. 65-83. 
Jensen K. (1988b) Why computational grammarians 
can be skeptical about existing linguistic theories,
in Proceedings of COLING-88, pp. 448-449. 
Lenci, A., Bartolini, R., Calzolari, N., Cartier, E. 
(2001) Document Analysis, MLIS-5015 MUSI, De-
liverable D3.1,. 
Lenci, A., Montemagni, S., Pirrelli, V., Soria, C. 
(2000) Where opposites meet. A Syntactic Meta-
scheme for Corpus Annotation and Parsing 
Evaluation, in Proceedings of the 2nd International 
Conference on Language Resources and Evalua-
tion, Athens, Greece. 
Montemagni S., Barsotti F., Battista M., Calzolari N., 
Corazzari O., Zampolli A., Fanciulli F., Massetani 
M., Raffaelli R., Basili R., Pazienza M.T., Saracino 
D., Zanzotto F., Mana N., Pianesi F., Delmonte R. 
(2000) The Italian Syntactic-Semantic Treebank: 
Architecture, Annotation, Tools and Evaluation, in 
Proceedings of the COLING Workshop on ?Lin-
guistically Interpreted Corpora (LINC-2000)?, 
Luxembourg, 6 August 2000, pp. 18-27. 
Procter, P. (1987) Longman Dictionary of Contempo-
rary English, Longman, London. 
Ruimy, N., Corazzari, O., Gola, E., Spanu, A., Cal-
zolari, N., Zampolli, A. (1998) The European LE-
PAROLE Project: The Italian Syntactic Lexicon, in 
Proceedings of the 1st International Conference on 
Language resources and Evaluation, Granada, 
Spain, 1998. 
 
RDF Instantiation of ISLE/MILE Lexical Entries
Nancy Ide
Department of Computer
Science
Vassar College
Poughkeepsie, New York
USA 12604-0520
ide@cs.vassar.edu
Alessandro Lenci
Universit? di Pisa
Dipartimento di Linguistica
Via Santa Maria 36
56100 PISA
Italy
lenci@ilc.cnr.it
Nicoletta Calzolari
Istituto di Linguistica
Computazionale, CNR
Area della Ricerca
Via Moruzzi 1 ? 56100 PISA
Italy
glottolo@ilc.cnr.it
Abstract
In this paper we describe the overall
model for MILE lexical entries and
provide an instantiation of the model in
RDF/OWL. This work has been done
with an eye toward the goal of creating a
web-based registry of lexical data
categories and enabling the description of
lexical information by establishing
relations among them, and/or using pre-
defined objects that may reside at various
locations on the web. It is also assumed
that using OWL specifications to enhance
specifications of the ontology of lexical
objects will eventually enable the
exploitation of inferencing engines to
retrieve and possibly create lexical
information on the fly, as suited to
particular contexts. As such, the model
and RDF instantiation provided here are
in line with the goals of ISO TC37 SC4,
and should be fully mappable to the
proposed pivot.
1 Introduction
The eventual vision for computational lexicons is
to enable universal access to sophisticated
linguistic information, which in turn will serve as a
central component for content-based information
management on the web. This demands, first of all,
some standardized means to represent complex
lexical information while retaining the flexibility
required to accommodate diverse approaches to
lexicon organization and use. To this end, the
ISLE
1
 ( International Standards for Language
Engineering) Computational Lexicons Working
Group (CLWG) has designed MILE (Multilingual
ISLE Lexical Entry), a general schema for the
encoding of multilingual lexical information
intended as a meta-entry that can serve as a
standardized representational layer for multilingual
lexical resources. MILE consists of an incremental
definition of object-oriented layers for lexical
description that will enable open and distributed
lexicons, with elements possibly residing in
different sites of the web. The defined lexical
objects are intended for use by lexicon and
application developers to build and target lexical
data at high level of abstraction.
The Resource Definition Framework (RDF) and
the Ontology Web Language (OWL) recently
developed by the World Wide Web Consortium
(W3C) build upon the XML web infrastructure to
enable the creation of a Semantic Web, wherein
web objects can be classified according to their
properties, and the semantics of their relations
(links) to other web objects can be precisely
defined. This in turn will enable powerful
inferencing capabilities that can adapt language
processing applications to particular contexts.
The MILE lexical entry is an ideal structure for
rendering via RDF/OWL. It consists of a hierarchy
of lexical objects that are built up in a layered
fashion by combining atomic data categories via
clearly defined relations. The overall architecture is
modular and layered, as described in Atkins et al
                                                       
1
 ISLE Web Site URL:
lingue.ilc.pi.cnr.it/EAGLES96/isle/ISLE_Home_Page.htm
(2002) and Calzolari et al (2003). On the
horizontal dimension, independent, linked modules
target different dimensions of lexical entries. On
the vertical dimension, the layered organization
allows for varying degrees of granularity in lexical
descriptions, allowing both ?shallow? and ?deep?
lexical representations. RDF?s class hierarchy
mechanism, together with its capacity to specify
named relations among objects in the various
classes, provide a web-based means to represent
this architecture.
2
 Furthermore, because RDF
allows for instantiating objects in any defined class
and subsequently referring to them as the target of
appropriate relations, lexical objects at any level of
specificity can be pre-defined. This provides an
important mechanism for standardization of lexical
elements, since these elements may be pre-defined,
organized in class hierarchies with inherited
properties, and used ?off-the-shelf? as needed.
In this paper we describe the overall model for
MILE lexical entries and provide an instantiation
of the model in RDF/OWL. This work has been
done with an eye toward the goal of creating a
web-based registry of lexical data categories and
enabling the description of lexical information by
establishing relations among them, and/or using
pre-defined objects that may reside at various
locations on the web. It is also assumed that using
OWL specifications to enhance specifications of
the ontology of lexical objects will eventually
enable the exploitation of inferencing engines to
retrieve and possibly create lexical information on
the fly, as suited to particular contexts. As such, the
model and RDF instantiation provided here are in
line with the goals of ISO TC37 SC4, and should
be fully mappable to the proposed pivot.
3
2 The MILE Lexical Model
The MILE Lexical Model (MLM) consists of two
primary components: a mono-lingual component
and a multi-lingual component. The mono-lingual
                                                       
2
 It should be noted that this architecture is analogous to
other data models, including ER diagrams and various
knowledge representation schemes.
3
 We have in fact produced a version of the prototype
ISLE lexical entry in an XML format instantiating the
proposed ISO pivot format (Ide and Romary,
Vassar/LORIA internal document).
component comprises three layers: morphological,
syntactic, and semantic. The overall architecture is
shown in Figure 1.
Within each of the MLM layers, two types of
objects are defined:
1. MILE Lexical Classes (MLC): the main
building blocks of lexical entries. They
formalize the basic lexical notions for each
layer defined in the ISLE project (Calzolari et
al. 2003). The MLM defines each class by
specifying its attributes and the relations
among them. Classes represent notions like
syntactic feature, syntactic phrase, predicate,
semantic relation, synset, etc. Instances of
MLCs are the MILE Data Categories (MDC).
So for instance, NP and VP are data category
instances of the class <Phrase>, and SUBJ and
OBJ are data category instances of the class
<Function>. Each MDC is identified by a URI.
MDC can be either user- defined or reside in a
shared repository.
2. lexical operations: special lexical entities
which allow users to state conditions and
perform complex operations over lexical
entries. They will for instance allow
lexicographers to establish multilingual
conditions, link the slots within two different
syntactic frames, link semantic arguments with
syntactic slots, etc.
The MLM is described with Entity-Relationship
(E-R) diagrams defining the entities of the lexical
model and the way they can be combined to design
an actual lexical entry. As such, the MLM does not
correspond to a specific lexical entry, but is rather
an entry schema corresponding to a lexical meta-
entry. This means that different possible lexical
entries can be designed as instances of the schema
provided by the MLM. Instance entries might
therefore differ for the type of information they
include (e.g. morphological, syntactic, semantic,
monolingual or multilingual, etc.), and for the
depth of lexical description.
Figure 2 depicts the MLM classes and relations for
the syntactic layer (SynU for ?syntactic unit?). Full
definitions for the MLM can be found in the ISLE
document (Calzolari et al 2003).
Figure 1. Overall MILE architecture
MLC:SynU 
 
 
id: xs:anyURY 
comment: xs:string 
example: xs:string 
MLC:SyntacticFrame  
hasSyntacticFrame  
1..* 
MLC:FrameSet  
Composition 
hasFrameSet  
* 
* 
composedBy  
MLC:SemU 
correspondsTo  
* 
CorrespSynUSemU  
Figure 2. Lexical classes and their relations for the syntactic layer (SynU)
morphological 
layer 
syntactic layer 
semantic layer 
linking conditions 
mono-Mile 
multi-MILE 
 
 
 
 
 
multilingual 
correspondence 
conditions 
mono-Mile 
3 RDF instantiation
We have created an RDF schema for the
syntactic layer of the ISLE/MILE lexical entry
and instantiated one entry in several alternative
forms to explore its potential as a representation
for lexical data that can be integrated into the
Semantic Web. The following describes the
various components.
3.1.1 RDF schema for ISLE lexical entries
An RDF schema defines classes of objects and
their relations to other objects. It does not in
itself comprise an instance of these objects, but
simply specifies the properties and constraints
applicable to objects that conform to it.
The RDF schema for the syntactic layer of ISLE
lexical entries can be accessed at
http://www.cs.vassar.edu/~ide/rdf/isle-schema-
v.6. The classes and relations (properties)
defined in the schema correspond to the ER
diagrams in Calzolari et al (2003). The schema
indicates that there is class of objects called
Entry; a property declaration indicates that the
relation hasSynU holds between Entry objects
and SynU  objects. Note that classes can be
defined to be subclasses of other classes, in
which case properties associated with the parent
class are inherited. In the ISLE schema, for
example, the objects Self and SlotRealization
are defined to be sub-classes of PhraseElement,
and the hasPhrase property holds between any
object of type PhraseElement (including its
sub-classes) and objects of type Phrase.
The ISLE RDF schema and entries have been
validated using the ICS-FORTH Validating RDF
Parser (VRP v2.1), which analyzes the syntax of
a given RDF/ XML file according to the RDF
Model and Syntax Specification
4
 and checks
whether the statements contained in both RDF
schemas and resource descriptions satisfy the
semantic constraints derived by the RDF Schema
Specification.
5
                                                       
4
 http://www.w3.org/TR/rdf-syntax-grammar/
5
 http://www.w3.org/TR/rdf-schema/
4  ISLE Lexical Entries and the Data
Category Registry
Appendix A contains three versions of the SynU
description for ?eat?, instantiated as RDF
objects. The first is a ?full? version in which all
of the information is specified, including atomic
values (strings) at the leaves of the tree structure.
The second two versions, rather than specifying
all information explicitly, rely on the existence
of a Data Category Registry (DCR) in which
pre-defined lexical objects are instantiated and
may be included in the entry by a direct
reference.
The potential to develop a Data Category
Registry in which lexical objects are instantiated
in RDF is one of the most important for the
creation of multi-lingual, reusable lexicons. It
allows for the following:
1. specification of a universally accessible,
standard set of morphological, syntactic, and
semantic information that can serve as a
reference for lexicons creators;
2. a fully modular specification of lexical
entities that enables use of all or parts of the
lexical information in the repository as
desired or appropriate, to build more
complex lexical information modules;
3. a template for data category description that
lexicon creators can use to create their own
data categories at any level of granularity;
4. means to reuse lexical specifications in
entries sharing common properties, thereby
eliminating redundancy as well as providing
direct means to identify lexical entries or
sub-entries with shared properties;
5. a universally accessible set of lexical
information categories that may be used in
applications or resources other than lexicons.
Note that the existence of a repository of lexical
objects, instantiated and specified at different
levels of complexity, does not imply that these
objects must be used by lexicon creators. Rather,
it provides a set of ?off the shelf? lexical objects
which either may be used as is, or which provide
a departure point for the definition of new or
modified categories.
The examples in Appendix A provide a general
idea of how a repository of RDF-instantiated
lexical objects can be used. Sample repositories
at three different levels of granularity,
corresponding to the examples in Appendix A,
are given in Appendix B:
1. a repository of enumerated classes for
lexical objects at the lowest level of
granularity; this comprises a definition of
sets of possible values for various lexical
objects. Any object of this type must be
instantiated with one of the listed values.
2. a repository of phrase classes which
instantiate common phrase types, e.g., NP,
VP, etc.
3. a repository of constructions containing
instantiations of common syntactic
constructions (e.g., for verbs which are both
transitive and intransitive, as shown in the
example).
The example entries demonstrate three different
possibilities for the use of information in the
repositories:
1. Entry 1 uses only the enumerated classes in
the LDCR for SynFeatureName and
SynFeatureValue. Note that in this case, the
LDCR only provides a closed list of possible
values, from which the assigned value in the
entry must be chosen.
2. Entry 2 refers to instances of phrase objects
in the LDCR rather than including them in
the entry; this enables referring to a complex
phrase (Vauxhave in the example) rather
than including it directly in the entry, and
provides the potential to reuse the same
instance by reference in the same or other
entries (this is done with N P  in the
example).
3. Entry 3 takes advantage of construction
instances in the LDCR, thus eliminating the
full specification in the entry and, again,
allowing for reuse in other entries.
5 Summary
This exercise is intended to exemplify how RDF
may be used to instantiate lexical objects at
various levels of granularity, which can be used
and reused to create lexical entries within a
single lexicon as well as across lexicons. By
relying on the developing standardized
technologies underlying the Semantic Web, we
ensure universal accessibility and commonality.
Ultimately, lexical objects defined in this way
can be used not only for lexicons, but also in
language processing and other applications.
This example serves primarily as a proof of
concept that may be refined and modified as we
consider in more depth the exact RDF
representation that would best serve the needs of
lexicon creation. However, the potential of
exploiting the developments in the Semantic
Web world for lexicon development should be
clear. More importantly, by situating our work in
the context of W3 standards, we are in step with
ISO TC37/SC4 vision of a Linguistic Annotation
Framework that includes a Data Category
Registry of the type we describe here.
References
Atkins, S., Bel, N., Bertagna, F., Bouillon, P.,
Calzolari, N., Fellbaum, C., Grishman, R., Lenci,
A., MacLeod, C., Palmer, M., Thurmair, G.,
Villegas, M., Zampolli A., 2002. ?From
Resources to Applications. Designing the
Multilingual ISLE Lexical Entry?, Proceedings
of LREC 2002, Las Palmas, Canary Islands,
Spain: 687-693.
Calzolari, N., Bertagna, F., Lenci, A.,
Monachini, M., 2003. Standards and best
Practice for Multilingual Computational
Lexicons and MILE (Multilingual ISLE Lexical
Entry), ISLE Computational Lexicon Working
Group deliverables D2.2 ? D3.2, Pisa.
Appendix A: Sample Entries
ENTRY 1 : Full entry
Highlighted lines refer to objects whose values are constrained in DCR definitions (Appendix B).
<?xml version="1.0"?>
<!-- Sample ISLE lexical Entry for EAT (transitive), SynU only
     Abbreviated syntax version using no pre-defined objects
     2002/10/23 Author: Nancy Ide -->
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:mlc="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#"
         xmlns="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Entry rdf:ID="eat1">
   <!-- The SynU for eat1 -->
   <hasSynu rdf:parseType="Resource">
      <SynU rdf:ID="eat1-SynU">
         <example>John ate the cake</example>
         <hasSyntacticFrame>
            <SyntacticFrame rdf:ID="eat1SynFrame">
               <hasSelf>
                  <Self rdf:ID="eat1Self">
                     <headedBy>
                        <Phrase rdf:ID="Vauxhave">
                           <hasSynFeature>
                              <SynFeature>
                                 <hasSynFeatureName rdf:value="aux"/>
                                 <hasSynFeatureValue rdf:value="have"/>
              </SynFeature></hasSynFeature></Phrase></headedBy></Self></hasSelf>
               <hasConstruction>
                  <Construction rdf:ID="eat1Const">
                     <slot>
                        <SlotRealization rdf:ID="NPsubj">
                            <hasFunction rdf:value="Subj"/>
                            <filledBy rdf:value="NP"/>
                     </SlotRealization></slot>
                     <slot>
                        <SlotRealization rdf:ID="NPobj">
                             <hasFunction rdf:value="Obj"/>
                             <filledBy rdf:value="NP"/>
              </SlotRealization></slot></Construction></hasConstruction>
                <hasFrequency rdf:value="8788" mlc:corpus="PAROLE"/>
</SyntacticFrame></hasSyntacticFrame></SynU></hasSynu></Entry></rdf:RDF>
ENTRY 2 : Using DCR categories for PHRASE
The highlighted lines refer to pre-instantiated lexical objects. A portion of the LDCR for Phrases is given
in Appendix C. The URL reference is to the actual web address where the object is instantiated.
<?xml version="1.0"?>
<!--
     Sample ISLE lexical Entry for EAT (transitive), SynU only
     Abbreviated syntax version using no pre-defined objects
     2002/10/23 Author: Nancy Ide -->
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:mlc="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#"
         xmlns="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Entry rdf:ID="eat1">
   <!-- The SynU for eat1 -->
   <hasSynu rdf:parseType="Resource">
      <SynU rdf:ID="eat1-SynU">
         <example>John ate the cake</example>
         <hasSyntacticFrame>
            <SyntacticFrame rdf:ID="eat1SynFrame">
               <hasSelf>
                  <Self rdf:ID="eat1Self">
                    <headedBy rdf:resource=
                    "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#Vauxhave"/>
               </Self></hasSelf>
               <hasConstruction>
                  <Construction rdf:ID="eat1Const">
                     <slot>
                       <SlotRealization rdf:ID="NPsubj">
                         <hasFunction rdf:value="Subj"/>
                         <filledBy rdf:resource=
                         "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#NP"/>
                     </SlotRealization></slot>
                     <slot>
                       <SlotRealization rdf:ID="NPobj">
                         <hasFunction rdf:value="Obj"/>
                         <filledBy rdf:resource=
                         "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#NP"/>
                </SlotRealization></slot></Construction></hasConstruction>
                <hasFrequency rdf:value="8788" mlc:corpus="PAROLE"/>
</SyntacticFrame></hasSyntacticFrame></SynU></hasSynu></Entry></rdf:RDF>
ENTRY 3 : Using DCR categories for CONSTRUCTION
The highlighted lines refer to a pre-instantiated Construction object. A portion of the DCR for
Constructions is given in Appendix B. The URL reference is to the actual web address where the object is
instantiated.
<?xml version="1.0"?>
<!-- Sample ISLE lexical Entry for EAT (transitive)
     Abbreviated syntax version using pre-defined construction
     2002/10/23 Author: Nancy Ide -->
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:mlc="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#"
         xmlns="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Entry rdf:ID="eat1">
   <!-- The SynU for eat1 -->
   <hasSynu rdf:parseType="Resource">
      <SynU rdf:ID="eat1-SynU">
         <example>John ate the cake</example>
         <hasSyntacticFrame>
            <SyntacticFrame rdf:ID="eat1SynFrame">
               <hasSelf>
                  <Self rdf:ID="eat1Self">
                     <headedBy rdf:resource=
                    "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#Vauxhave"/>
                  </Self></hasSelf>
               <hasConstruction rdf:resource=
          "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Constructions#TransIntrans"/>
               <hasFrequency rdf:value="8788" mlc:corpus="PAROLE"/>
            </SyntacticFrame></hasSyntacticFrame></SynU></hasSynu></Entry></rdf:RDF>
Appendix B: DCR definitions
Sample DCR entries specifying enumerated values for SynFeatureName, etc. The specification uses the
Ontology Web Language (OWL) to list valid values for objects of the defined class.
<!-- Enumerated classes for ISLE lexical entries v0.1 2002/10/23 Author: Nancy Ide  -->
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:owl ="http://www.w3.org/2002/07/owl#
         xmlns:isle ="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<rdfs:Class rdf:about=
            "http://www.cs.vassar.edu/~ide/rdf/isle-enumerated-classes#FunctionType">
<owl:oneOf>
   <rdf:Seq>
      <rdf:li>Subj</rdf:li>
      <rdf:li>Obj</rdf:li>
      <rdf:li>Comp</rdf:li>
      <rdf:li>Arg</rdf:li>
      <rdf:li>Iobj</rdf:li>
</rdf:Seq></owl:oneOf></rdfs:Class>
<rdfs:Class rdf:about=
     "http://www.cs.vassar.edu/~ide/rdf/isle-enumerated-classes#SynFeatureName">
<owl:oneOf>
   <rdf:Seq>
      <rdf:li>tense</rdf:li>
      <rdf:li>gender</rdf:li>
      <rdf:li>control</rdf:li>
      <rdf:li>person</rdf:li>
      <rdf:li>aux</rdf:li>
</rdf:Seq></owl:oneOf> </rdfs:Class>
<rdfs:Class rdf:about=
     "http://www.cs.vassar.edu/~ide/rdf/isle-enumerated-classes#SynFeatureValue">
<owl:oneOf>
   <rdf:Seq>
      <rdf:li>have</rdf:li>
      <rdf:li>be</rdf:li>
      <rdf:li>subject_control</rdf:li>
      <rdf:li>object_control</rdf:li>
      <rdf:li>masculine</rdf:li>
      <rdf:li>feminine</rdf:li>
</rdf:Seq></owl:oneOf></rdfs:Class></rdf:RDF>
Sample LDCR entry for two Phrase objects
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:mlc="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Phrase rdf:ID="NP" rdfs:label="NP"/>
<Phrase rdf:ID="Vauxhave">
   <hasSynFeature>
     <SynFeature>
        <hasSynFeatureName rdf:value="aux"/>
        <hasSynFeatureValue rdf:value="have"/>
      </SynFeature></hasSynFeature></Phrase></rdf:RDF>
Sample LDCR entry for a Construction object
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
          xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
          xmlns="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Construction rdf:ID="TransIntrans">
    <slot>
       <SlotRealization rdf:ID="NPsubj">
          <hasFunction rdf:value="Subj"/>
          <filledBy rdf:resource=
                   "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#NP"/>
    </SlotRealization></slot>
    <slot>
       <SlotRealization rdf:ID="NPobj">
          <hasFunction rdf:value="Obj"/>
          <filledBy rdf:resource=
                  "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#NP"/>
</SlotRealization></slot></Construction></rdf:RDF>
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 72?81,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Climbing the path to grammar: a maximum entropy model of 
subject/object learning 
 
 
Felice Dell?Orletta Alessandro Lenci Simonetta Montemagni Vito Pirrelli 
Dept. of Computer Science Dept. of Linguistics ILC-CNR ILC-CNR 
University of Pisa University of Pis a Area della Ricerca Area della Ricerca 
Largo Pontecorvo 3 
56100 Pisa (Italy) 
Via Santa Maria 36 
56100 Pisa (Italy) 
Via Moruzzi 1 
56100 Pisa (Italy) 
Via Moruzzi 1 
56100 Pisa (Italy) 
 
{felice.dellorletta, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.it 
 
 
 
 
Abstract 
In this paper, we discuss an applic ation of 
Maximum Entropy to modeling the acqui-
sition of subject and object processing in 
Italian. The model is able to learn from 
corpus data a set of experimentally and 
theoretically well-motivated linguistic 
constraints, as well as their relative sali-
ence in Italian grammar development and 
processing. The model is also shown to 
acquire robust syntactic generalizations 
by relying on the evidence provided by a 
small number of high token frequency 
verbs only. These results are consistent 
with current research focusing on the role 
of high frequency verbs in allowing chil-
dren to converge on the most salient con-
straints in the grammar. 
1 Introduction 
Current research in language learning supports the 
view that developing grammatical competence in-
volve mastering and integrating multiple, parallel, 
probabilistic constraints defined over different 
types of linguistic (and non linguistic) information 
(Seidenberg and MacDonald 1999, MacWhinney 
2004). This is particularly clear when we focus on 
the core of grammatical deve lopment, namely the 
ability to properly identify syntactic relations. Psy-
cholinguistic evidence shows that children learn to 
identify sentence subjects and direct objects by 
combining various types of probabilistic cues, such 
as word order, noun animacy, definiteness, agree-
ment, etc. The relative prominence of each of these 
cues during the development of a child?s syntactic 
competence can considerably vary cross-
linguistically, mirroring their relative salience in 
the adult grammar system (cf. Bates et al 1984). 
If grammatical constraints are inherently prob-
abilistic (Manning 2003), the path through which 
the child acquires adult grammar competence can 
be viewed as the process of building a stochastic 
model out of the linguistic input. Consistently with 
?usage-based? approaches to language acquisition 
(cf. Tomasello, 2000) grammatical constraints 
would thus emerge from language use thanks to the 
child?s ability to keep track of statistical regulari-
ties in linguistic cues. In turn, this raises the issue 
of how children are able to exploit the statistical 
distribution of cues in the linguistic input. Various 
types of cross-linguistic evidence converge on the 
hypothesis that children are actually able to take 
great advantage of the highly skewed distribution 
of naturalistic language data. Goldberg et al 
(2004), Matthews et al (2003), Ninio (1999) 
among the others argue that verbs with high token 
frequency in the input have a facilitatory effect in 
allowing children to derive robust syntactic gener-
alizations even from surprisingly minimal input. 
According to this model, syntactic learning is 
driven by a small pool of verbs occurring with the 
highest token frequency: they approximately corre-
spond to so-called ?light verbs? such as English 
go, give , want etc. These verbs would act as ?cata-
72
lysts? in allowing children to converge on the most 
salient grammar constraints of the language they 
are acquiring. 
In computational linguistics, Maximum Entropy 
models have proven to be robust statistical learning 
algorithms that perform well in a number of proc-
essing tasks (cf. Ratnaparkhi 1998). In this paper, 
we discuss successful application of a Maximum 
Entropy (ME) model to the processing of Italian 
syntactic relations. We believe that this discussion 
is of general interest for two basic reasons. First, 
the model is able to learn, from corpus data, a set 
of experimentally and theoretically well-motivated 
linguistic constraints, as well as their relative sali-
ence in the processing of Italian. This suggests that 
it is possible for a child to bootstrap and use this 
type of knowledge on the basis of a specific distri-
bution of real language data, a conclusion that 
bears on the question of the role and type of innate 
inductive biases. Secondly, the model is also 
shown to acquire robust syntactic generalizations 
by relying on the evidence provided by a small 
number of high token frequency verbs only. With 
some qualifications, this evidence sheds light on 
the interaction between highly skewed language 
data distributions and language maturation. Robust 
grammar generalizations emerge on the basis of 
exposure to early, statist ically stable and lexically 
underspecified evidence, thus providing a reliable 
backbone to children?s syntactic development and 
later lexical organization.  
In the following section we first broach the 
general problem of parsing subjects and objects in 
Italian. Section 3 describes an ME model of the 
problem. Section 4 and 5 are devoted to a detailed 
empirical analysis of the interaction of different 
feature configurations and of the interplay between 
verb token frequency and relevant generalizations. 
Conclusions are drawn in the final discussion. 
2 Subjects and Objects in Italian 
Children that learn how to process subjects and 
objects in Italian are confronted with a twofold 
challenge: i) the relatively free order of Italian sen-
tence constituents and ii) the possible absence of 
an overt subject. The existence of a preferred Sub-
ject Verb Object (SVO) order in Italian main 
clauses does not rule out all other possible permu-
tations of these units: in fact, they are all attested, 
albeit with considerable differences in distribution 
and degree of markedness (Bartolini et al 2004).1 
Moreover, because of pro-drop, an Italian Verb 
Noun (VN) sequence can either be interpreted as a 
VO construction with subject omission (e.g. ha 
dichiarato guerra ?(he) declared war?) or as an 
instance of postverbal subject (VS, e.g. ha di-
chiarato Giovanni ?John declared?). Symmetri-
cally, an NV sequence is potentially ambiguous 
between SV and OV: compare il bambino ha man-
giato  ?the child ate? with il gelato ha mangiato ?the 
ice-cream, (he) ate?. 
These grammatical facts are in keeping with 
what we know about Italian children?s parsing 
strategies. Bates et al (1984) show that while, in 
English, word order is by and large the most effec-
tive cue for subject-object identification (hence-
forth SOI) both in syntactic processing and during 
the child?s syntactic development, the same cue 
plays second fiddle in Italian. Bates and colleagues 
bring empirical evidence supporting the hypothesis 
that Italian children show extreme reliance on NV 
agreement and, secondly, on noun animacy, rather 
than word order. They conclude that the following 
syntactic constraints dominance hierarchy is opera-
tive in Italian: agreement > animacy > word order. 
The fact that animacy can reliably be resorted 
to in Italian SOI receives indirect confirmation 
from corpus data. We looked at the distribution of 
animate subjects and objects in the Italian Syntac-
tic Semantic Treebank (ISST, Montemagni et al, 
2003), a 300,000 tokens syntactically annotated 
corpus, including articles from contemporary Ita l-
ian newspapers and periodicals covering a broad 
variety of topics. Subjects and objects in ISST 
were automatically annotated for animacy using 
the SIMPLE Italian computational lexicon (Lenci 
et al 2000) as a background semantic resource. 
The annotation was then checked manually. Cor-
pus analysis highlights a strong asymmetry in the 
distribution of animate nouns in subject and object 
roles: over 56.6% of ISST subjects are animate 
(out of a total number of 12,646), while only the 
11.1% of objects are animate (out of a total number 
of 5,559). Such an overwhelming preference for 
inanimate ob jects in adult language data makes 
animacy play a very important role in SOI, both as 
a key developmental factor in the bootstrapping of 
the syntax-semantics mapping and as a reliable 
                                                               
1 In the present paper we restrict ourselves to the case of de-
clarative main clauses. 
73
processing cue, consistently with psycholinguistic 
data. 
On the other hand, the distribution of word or-
der configurations in the same corpus shows an-
other interesting asymmetry. NV sequences receive 
an SV interpretation in 95.6% of the cases, and an 
object interpretation in the remaining 4.4% (most 
of which are clitic and relative pronouns, whose 
preverbal pos ition is grammatically constrained). 
The situation is quite different when we turn to VN 
sequences, where verb-object pairs represent 
73.4% of the cases, with verb-subject pairs repre-
senting the remaining 26.6%. We infer that ? at 
least in standard written Italian ? VS is a much 
more consistently used construction than OV, and 
that the role of word order in Italian parsing is not 
a marginal one across the board, but rather relative 
to VN contexts only. In NV constructions there is a 
strong preference for a subject interpretation, and 
this suggests a more dynamic dominance hierarchy 
of Italian syntactic constraints than the one pro-
vided above. 
As for agreement, it represents conclusive evi-
dence for SOI only when a nominal constituent and 
a verb do not agree in number and/or person (as in 
leggono il libro ?(they) read the book?). On the 
contrary, when noun and verb share the same per-
son and number the impact of agreement on SOI is 
neutralised, as in il bambino legge il libro ?the 
child reads the book? or in ha dichiarato il presi-
dente ?the president declared?. Although this ambi-
guity arises in specific contexts (i.e. when the verb 
is used in the third person singular or plural and the 
subject/object candidate agrees with it), it is inter-
esting to note that in ISST: third person verb forms 
cover 95.6% of all finite verb forms; and, more 
interestingly for our present concerns, 87.9% of all 
VN and NV pairs involving a third person verb 
form contains an agreeing noun. From this we con-
clude that the contribution of agreement to our 
problem is fairly limited, as lack  of agreement 
shows up only in a limited number of contexts. 
All in all, corpus data lend support to the idea 
that in Italian SOI is governed by a complex inter-
play of probabilistic constraints of a different na-
ture (morpho-syntactic, semantic, word order etc.). 
Moreover, distributional asymmetries in language 
data seem to provide a fairly reliable statistical ba-
sis upon which relevant probabilistic constraints 
can be bootstrapped and combined consistently. In 
the following section we shall present a ME model 
of how constraints and their interaction can be 
bootstrapped from language data. 
3 A Maximum Entropy model of SOI 
The Maximum Entropy (ME) framework offers a 
mathematically  sound way to build a probabilistic 
model for SOI, which combines different linguistic 
cues. Given a linguistic context c and an outcome 
a?A that depends on c, in the ME framework the 
conditional probability distribution p(a|c) is esti-
mated on the basis of the assumption that no a pri-
ori constraints must be met other than those related 
to a set of features f j(a,c) of c, whose distribution is 
derived from the training data. It can be proven 
that the probability distribution p satisfying the 
above assumption is the one with the highest en-
tropy, is unique and has the following exponential 
form (Berger et al 1996): 
(1) ?
=
=
k
j
cajf
jcZ
cap
1
),(
)(
1)|( a  
where Z(c) is a normalization factor, f j(a,c) are the 
values of k features of the pair (a,c) and correspond 
to the linguistic cues of c that are relevant to pre-
dict the outcome a. Features are extracted from the 
training data and define the constraints that the 
probabilistic model p must satisfy. The parameters 
of the distribution a1, ?, ak correspond to weights 
associated with the features, and determine the 
relevance of each feature in the overall model. In 
the experiments reported below feature weights 
have been estimated with the Generative Iterative 
Scaling (GIS) algorithm implemented in the AMIS 
software (Miyao and Tsujii 2002). 
We model SOI as the task of predicting the cor-
rect syntactic function f  ? {subject, object} of a 
noun occurring in a given syntactic context s. This 
is equivalent to build the conditional probability 
distribution p(f |s) of having a syntactic function f  
in a syntactic context s . Adopting the ME ap-
proach, the distribution p can be rewritten in the 
parametric form of (1), with features correspond-
ing to the linguistic contextual cues relevant to 
SOI. The context s  is a pair <vs , ns>, where vs is 
the verbal head and ns its nominal dependent in s. 
This notion of s departs from more traditional 
ways of describing an SOI context as a triple of 
one verb and two nouns in a certain syntactic con-
figuration (e.g, SOV or VOS, etc.). In fact, we as-
sume that SOI can be stated in terms of the more 
74
local task of establishing the grammatical function 
of a noun n observed in a verb-noun pair. This 
simplifying assumption is consistent with the claim 
in MacWhinney et al (1984) that SVO word order 
is actually derivative from SV and VO local pat-
terns and downplays the role of the transitive com-
plex construction in sentence processing. Evidence 
in favour of this hypothesis also comes from cor-
pus data: in ISST, there are 4,072 complete sub-
ject-verb-object-configurations, a small number if 
compared to the 11,584 verb tokens appearing with 
either a subject or an object only. Due to the com-
parative sparseness of canonical SVO constructions 
in Italian, it seems more reasonable to assume that 
children should pay a great deal of attention to 
both SV and VO units as cues in sentence percep-
tion (Matthews et al 2004). Reconstruction of the 
whole lexical SVO pattern can accordingly be seen 
as the end point of an acquisition process whereby 
smaller units are re-analyzed as being part of more 
comprehensive constructions. This hypothesis is 
more in line with a distributed view of canonical 
constructions as derivative of more basic local po-
sitional patterns, working together to yield more 
complex and abstract constructions. Last but not 
least, assuming verb-noun pairs as the relevant 
context for SOI allows us to simultaneously model 
the interaction of word order variation with pro-
drop in Italian. 
4 Feature selection 
The most important part of any ME model is the 
selection of the context features whose weights are 
to be estimated from data distributions. Our feature 
selection strategy is grounded on the main assump-
tion that features should correspond to linguisti-
cally and psycholinguistically well-motivated 
contextual cues. This allows us to evaluate the 
probabilistic model also with respect to its ability 
to replicate psycholinguistic experimental results 
and to be consistent with linguistic generalizations. 
Features are binary functions fki,f  (f ,s), which 
test whether a certain cue ki for the function f  oc-
curs in the context s . For our ME model of SOI, 
we have selected the following types of features: 
Word order tests the position of the noun wrt the 
verb, for instance: 
(2)
?
?
? ==
otherwise
postposnounif
subjf subjpost 0
.1
),(,
ss  
Animacy  tests whether the noun in s  is animate or 
inanimate (cf. ?.2). The centrality of this cue in 
Italian is widely supported by psycholinguistic 
evidence. Another source of converging evidence 
comes from functional and typological linguistic 
research. For instance, Aissen (2003) argues for 
the universal value of the following hierarchy rep-
resenting the relative markedness of the associa-
tions between grammatical functions and animacy 
degrees (with each item in these scale been less 
marked than the elements to its right): 
Animacy Markedness Hierarchy 
Subj/Human > Subj/Animate > Subj/Inanimate 
Obj/Inanimate > Obj/Animate > Obj/Human 
Markedness hierarchies have also been interpreted 
as probabilistic constraints estimated form corpus 
data (Bresnan et al 2001, ?vrelid 2004). In our 
ME model we have used a reduced version of the 
animacy markedness hierarchy in which human 
and animate nouns have been both subsumed under 
the general class animate. 
Definiteness tests the degree of ?referentiality? of 
the noun in a context pair s . Like for animacy, 
definiteness has been claimed to be associated with 
grammatical functions, giving rise to the following 
universal markedness hierarchy Aissen (2003): 
Definiteness Markedness Hierarchy 
Subj/Pro > Subj/Name > Subj/Def > Subj/Indef 
Obj/Indef > Obj/Def > Obj/Name > Obj/Pro 
According to this hierarchy, subjects with a low 
degree of definiteness are more marked than sub-
jects with a high degree of definiteness (for objects 
the reverse pattern holds). Given the importance 
assigned to the definiteness markedness hierarchy 
in current linguistic research, we have included the 
definiteness cue in the ME model. It is worth re-
marking that, unlike animacy, in psycholinguistic 
experiments definiteness has not been assigned any 
effective role in SOI. This makes testing this cue in 
a computational model even more interesting, as a 
way to evaluate its effective contribution to Italian 
SOI. In our experiments, we have used a ?com-
pact? version of the definiteness scale: the defi-
niteness cue tests whether the noun in the context 
75
pair i) is a name or a pronoun ii) has a definite arti-
cle iii), has an indefinite article or iv) is a ?bare? 
noun (i.e. with no article). It is worth saying that 
?bare? nouns are usually placed at the bottom end 
of the definiteness scale. 
The three types of features above only refer to 
nominal cues in the context pairs. Nevertheless, 
specific lexical properties of the verb can also be 
resorted to in SOI. The probability for ns to be sub-
ject or object may also depend on the specific lexi-
cal preferences of vs. To take this lexical factor 
into account, we add a set of lexical cues to the 
three general feature types above. Lexical cues test 
animacy with respect to a specific verb vk: 
(3) 
?
?
?
?
?
=?==
otherwise
animnvvif
subjf ksubjkvanim
0
1
),(,,
sss  
Lexical features provide evidence of the propensity 
of a given verb to have an animate (inanimate) 
subject or object. In fact, the verb argument struc-
ture and thematic properties may well influence the 
possible distribution of animate (inanimate) sub-
jects and objects, thus overriding more general 
tendencies. By including lexical cues, we are thus 
able to test the interplay of lexical constraints with 
general grammatical ones. 
Note that in our ME model we have not in-
cluded agreement as a feature, in spite of its 
prominent role in Italian. The fact that agreement 
is often inconclusive for SOI (?.2) suggests that 
children must also acquire the ability to deal with 
the interplay of various concurrent constraints, 
none of which is singularly sufficient for the task 
completion this type of competence. It is exactly 
this area of syntactic competence that we wanted to 
explore with the experiments reported below (cf. 
MacWhinney et al 1984, who similarly abstract 
from the dominant role of case in German SOI). 
5 Testing feature configurations for SOI 
The ME model for Italian SOI has been trained on 
18,205 verb-subject/object pairs extracted from 
ISST. The training set was obtained by extracting 
all verb-subject and verb-object dependencies 
headed by an active verb occurring in a finite ver-
bal construction and by excluding all cases where 
the position of the nominal constituent was gram-
matically constrained (e.g. clitic objects, relative 
clauses). Two different feature configurations have 
been used for training: 
-  non-lexical feature configuration (NLC), in-
cluding only general features acting as global 
constraints: namely word order, noun animacy 
and noun definiteness; 
- lexical feature configuration (LC), including 
word order, noun animacy and definiteness, 
and information about the verb head.  
The test corpus consists of 645 verb-noun pairs 
extracted from contexts where agreement happens 
to be neutralized. Of them, 446 contained a subject 
(either pre- or post-verbal) and 199 contained an 
object (either pre- or post-verbal). The two feature 
configurations were evaluated by calculating the 
percentage of correctly assigned relations over the 
total number of test pairs (accuracy). As our model 
always assigns one syntactic relation to each test 
pair, accuracy equals both standard precision and 
recall. Finally, we have assumed a baseline score 
of 69%, corresponding to the result yielded by a 
dumb model assigning to each test pair the most 
frequent relation in the training corpus, i.e. subject. 
5.1 Non-lexical feature configuration 
Our first experiment was carried out with NLC. 
The accuracy on the test corpus is 91.5%; most 
errors (i.e. 96.4%) relate to the postverbal position, 
with 44 mistaken subjects (42 inanimate) and 9 
mistaken objects (all animate). The score was con-
firmed by a 10-fold cross-validation on the whole 
training set (89.3% accuracy). 
A further way to evaluate the goodness of the 
model is by inspecting the weights associated with 
feature values (Table 1). 
 Subj Obj 
Preverbal 1,34E+00 2,10E-02 
Postverbal 5,21E-01 1,47E+00 
Anim 1,28E+00 3,34E-01 
Inanim 8,60E-01 1,21E+00 
PronName  1,22E+00 5,75E-01 
DefArt 1,05E+00 1,00E+00 
IndefArt 8,33E-01 1,16E+00 
NoArticle 9,46E-01 1,07E+00 
Table 1 ? Feature value weights in NLC  
The grey cells in Table 1 highlight the preference 
of each feature value for either subject or object 
identif ication: e.g. preverbal subjects are strongly 
preferred over preverbal objects; animate subjects 
76
are preferred over animate objects, etc. Interest-
ingly, if we rank the Anim and Inanim values for 
subjects and objects, we can observe tha t they dis-
tribute consistently with the Animacy Markedness 
Hierarchy reported in ?.4: Subj /Anim > 
Subj/Inanim and Obj/Inanim > Obj/Anim. Simi-
larly, by ranking the values of the definiteness fea-
tures in the Subj column by decreasing weight 
values we obtain the following ordering: Pron-
Name > DefArt > IndefArt > NoArt, which nicely 
fits in with the Definiteness Markedness Hierarchy 
in ?.4. The so-called ?markedness reversal? is ob-
served if we focus on the values for the same fea-
tures in the Obj column: the PronName feature 
represents the most marked option, followed by 
DefArt. The only exception is represented by the 
relative ordering of IndefArt and NoArt which 
however show very close values. 
Evaluating feature salience 
In order to evaluate the most reliable cues in Italian 
SOI, we have analysed the model predictions for 
different bundles of feature values. For each of the 
16 different bundles (b) attested in the data, we 
have estimated p(subj|b) and p(obj|b): 
b p(subj|b) p(obj|b) 
Pre Anim IndefArt 0,994 0,006
Pre Anim DefArt 0,996 0,004
Pre Anim NoArt 0,995 0,005
Pre Anim PronName 0,998 0,002
Pre Inanim IndefArt 0,970 0,030
Pre Inanim DefArt 0,979 0,021
Pre Inanim NoArt 0,976 0,024
Pre Inanim PronName 0,990 0,010
Post Anim IndefArt 0,495 0,505
Post Anim DefArt 0,589 0,411
Post Anim NoArt 0,546 0,454
Post Anim PronName  0,743 0,257
Post Inanim IndefArt 0,153 0,847
Post Inanim DefArt 0,209 0,791
Post Inanim NoArt 0,182 0,818
Post Inanim PronName 0,348 0,652
Table 2 ? Subj/obj probabilities by different bundles 
The model shows a neat preference for subject 
when the noun is preverbal. Instead, when the noun 
is postverbal, function assignment is de facto de-
cided by the noun animacy. Conversely, definite-
ness features have a much more secondary role: 
they can re-enforce (or weaken) the preference ex-
pressed by animacy, but they do not have the 
strength to determine SOI. 
The relative salience of the different constraints 
acting on SOI can also be inferred by comparing 
the weights associated with individual feature val-
ues. For instance, Goldwater and Johnson (2003) 
show that ME can be successfully applied to learn 
constraint rankings in Optimality Theory, by as-
suming the parameter weights a1, ?, ak as the 
ranking values of the constraints. The following 
table lists the 16 general constraints of the model 
by increasing weight values: 
 
Feature Weight 
Preverbal_Obj 2,10E-02
Anim_Obj 3,34E-01
Postverbal_Subj 5,21E-01
ProName_Obj 5,75E-01
IndefArt_Subj 8,33E-01
Inanim_Subj 8,60E-01
NoArticle_Subj 9,46E-01
ArtDef_Obj 1,00E+00
DefArt_Subj 1,05E+00
NoArticle_Obj 1,07E+00
IndefArt_Obj 1,16E+00
Inanim_Obj 1,21E+00
PronName_Subj 1,22E+00
Anim_Subj 1,28E+00
Preverbal_Subj 1,34E+00
Postverbal_Obj 1,47E+00
Table 3 ? Constraint weights ranking 
The rankings in Table 3 can be used to derive the 
relative salience of each constraint. Lower ranked 
constraints correspond to more marked syntactic 
configurations that are then disfavoured in SOI. 
Notice that the two animacy constraints Anim_Obj 
and Anim_Subj are respectively placed near the 
bottom and the top end of the scale. Notwithstand-
ing the low position of Postverbal_Subj, animacy 
is thus able to override the word order constraint 
and to produce a strong tendency to identify ani-
mate nouns as subjects, even when they appear in 
postverbal position (cf. Table 2 above). The con-
straint ranking thus confirms the interplay between 
animacy and word order in Italian, with the former 
playing a decisive role in assigning the syntactic 
function of postverbal nouns. On the other hand, 
77
the constraints involving noun definiteness occupy 
a more intermediate position in the general rank-
ing, with very close values. This is again consistent 
with the less decisive role of this feature type in 
SOI, as shown above. 
5.2 Lexical feature configuration 
In this experiment the general features reported in 
Table 1 have been integrated with 4,316 verb-
specific features as the ones exemplified below for 
the verb dire ?say?: 
dire_animSog 1.228213e+00 
dire_noanimSog 7.028484e-01 
dire_animOgg 3.645964e-01 
dire_noanimOgg 1.321887e+00 
whose associated weights show the strong prefer-
ence of this verb to take animate subjects as op-
posed to inanimate ones as well as a preference for 
inanimate objects with respect to animate ones. 
The results achieved with LC on the test corpus 
show a significant improvement with respect to 
those obtained with NLC: the accuracy is now 
95.5%, with a  4% improvement, confirmed by a 
10-fold cross-validation (94.9%). Also in this case, 
most of the errors relate to the pos tverbal position 
(i.e. 27 out of 29), partitioned into 26 mistaken 
subjects and 1 mistaken object. Lexical features 
have been resorted to to solve most of the NLC 
errors (i.e. 34 out of 55). It is interesting to note 
however that lexical features can also be mislead-
ing. The LC results include 8 new errors, suggest-
ing that lexical features do not always provide 
conclusive evidence: in fact, in 185 cases out of 
645 test VN pairs (i.e. 28.7% of the cases) general 
features are preferred over lexical ones. It is also 
worth mentioning that the ranking of general ani-
macy and definiteness features in LC actually fits 
in with the respective markedness hierarchies even 
with a better approximation than the one produced 
by NLC.  Finally, the relative prominence of the 
different global features confirms the trend in Ta-
ble 2, with word order being predominant in pre-
verbal pos ition and animacy playing a major role 
with postverbal nouns. 
Both feature configurations of the ME model 
thus appear to comply with linguistic and psycho-
linguistic generalizations on SOI. On the linguistic 
side, the constraints learnt by the model are consis-
tent with universal markedness hierarchies for 
grammatical relations. Secondly, the prominence 
of the various constraints in the model fits in well 
with psycholinguistic data. Consistently with the 
results in Bates et al (1984), the model confirms 
the great impact of noun animacy in Italian, al-
though in this case its key role seems to be more 
directly limited to the postverbal position. Con-
versely, the preverbal position is by itself a very 
strong cue for subject interpretation. 
6 High frequency verbs and SOI  
Frequency is known to play a major influence in 
language learning. In morphology, for example, 
highly frequent lexical items tend to be shorter 
forms, more readily accessible in the mental lexi-
con, independently stored as whole items (Stem-
berger and MacWhinney 1986) and fairly resistant 
to morphological overgeneralization through time, 
thus establishing a correlation between irregular 
inflected forms and frequency. Frequency has also 
been assigned a key role in the acquisition of syn-
tactic constructions. In fact, Goldberg (1998) and 
Ninio (1999) have independently argued for the 
existence of a causal relation between early expo-
sure to highly frequent light verbs and acquisition 
of abstract syntax-semantics mappings (construc-
tions). Light verbs such as want, put and go tend to 
be very frequent, because they are applicable in a 
wider range of contexts and are learned and used at 
an early language maturation stage The main idea 
is that children?s early use of these high frequency 
verbs is conducive to the acquisition of abstract 
constructional properties generalizing over particu-
lar instances. 
Goldberg et al (2004) motivate this hypothesis 
by observing that light verbs have high input fre-
quency in the child?s developmental environment 
and, at the same time, exhibit a low degree of se-
mantic specialization. Hence, she argues, it takes a 
little abstraction step for a child to jump from ac-
tual instances of use of light verbs to the syntax-
semantics association of their underlying construc-
tion. On the other hand, Ninio (1999) grounds the 
facilitatory role of highly frequent verbs on their 
being ?pathbreaking? prototypes of the construc-
tion they instantiate, since they are the best models 
of the relevant combinatorial and semantic proper-
ties of their construction in a relatively undiluted 
fashion. However, in the case of light verb con-
structions, the correlation between high frequency 
78
and construction prototypicality and extension is 
tenuous. In fact, it is difficult to argue that frequent 
light verbs such as see, want or do exhibit a high 
degree of both semantic and constructional trans i-
tivity (Goldberg et al 2004). This is reminiscent of 
the morphological behaviour of very frequent word 
forms in inflectional languages, as most of these 
forms are highly fused and show a general ten-
dency towards irregular inflection and low mor-
phological prototypicality. Furthermore, it is 
difficult to reconcile the ?pathbreaking? view with 
the observation that frequently observed linguistic 
units are memorized in full, as unanalyzed wholes. 
6.1 Testing the role of frequency 
To address these open issues and put the alleged 
?pathbreaking? role of light verbs to the challeng-
ing test of a probabilistic model, we carried out a 
second battery of experiments to learn the general, 
non-lexical constraints from two training corpora 
of roughly equivalent size where overall type and 
token verb frequencies were controlled for. Both 
corpora are a subset of the original training set: 
1. skewed frequency corpus (SF) ? it includes 
5,261 context pairs, obtained by selecting 15 verbs 
occurring more than 100 times in ISST (figures in 
parentheses give their token frequency): essere 
?be? (2406), avere ?have? (708), fare  ?do, make? 
(527), dire ?say, tell? (275), dare ?give? (173), ve-
dere ?see? (134), andare ?go? (126), sembrare 
?seem? (124), cercare ?try? (122), mettere ?put? 
(122), portare ?take? (121), trovare ?find? (112), 
volere ?want? (105), lasciare ?leave? (105), riuscire 
?manage? (101). It is worth noticing that this set 
includes typical ?pathbreaking? verbs; 
2. balanced frequency corpus (BF) ? this corpus 
includes 5,373 context pairs selected in such a way 
to ensure that every verb type in the original train-
ing set is attested in BF and occurs at most 6 times. 
For verbs occurring with a higher frequency, the 
pairs to be included in BF have been randomly se-
lected. 
Thus SF and BF represent two opposite training 
situations: SF contains few types with very high 
token frequencies, while BF contains a high num-
ber of verb types (i.e. 1457), with very low and 
uniform token frequency. These training sets re-
semble the structure of linguistic input used by 
Goldberg et al (2004) for their experiments. In 
that case, one group of subjects was exposed to 
linguistic inputs in which some verbs occurred 
with a much higher frequency than the others; a 
second group of subjects was instead exposed to 
linguistic stimuli in which every verb occurred 
with roughly equal frequency. Therefore, by train-
ing our ME model on SF and BF we are able  to 
evaluate the effective role of high token frequency 
verbs in driving syntactic learning.  
The ME model with the general features only 
(i.e. NLC) was first trained on SF, and then tested 
on the 645-pair corpus in ?.5, showing a 90% ac-
curacy. The same ME model was then trained on 
BF, and then tested on the 645-pair corpus, scoring 
a 87% accuracy. The ME model trained on the 
skewed frequency data thus outperforms the model 
trained on BF in a statistically significant way (?2 = 
4.97; a=0.05; p-value = 0.025). 
By using a training set formed only by the verbs 
with the highest token frequency, the model has 
thus been able to acquire robust syntactic con-
straints for SOI. Once these constraints have been 
applied to unseen events, the model has achieved a 
performance comparable to the one of the general 
models in ?.5. This is somehow even more signif i-
cant if we consider that the training set was now 
formed by less than one-third of the pairs on which 
the models in ?.5 were trained. Data quantity aside, 
the most relevant fact is that it is the way verb fre-
quencies are distributed to determine the learning 
path, with a significant positive effect produced by 
high token frequency verbs. In the model trained 
on SF, feature ranking is also governed by mark-
edness relations, and the relative prominence of the 
various constraints is utterly similar to the one dis-
cussed in ?.5. In other terms, the results of this ex-
periment prove that frequent verbs are actually 
able to act as ?catalysts? of the syntactic acquis i-
tion process. It is possible for children to converge 
on the correct generalizations governing SOI in 
Italian, just by relying on the linguistic evidence 
provided by the most frequent verbs. 
This view suggests a way out of the apparent 
paradox of the ?pathbreaking? hypothesis: highly 
frequent verbs can be assumed to provide stable 
and consistent multiple probabilistic cues for the 
assignment of subject/object relations. The exis-
tence of pos itional patterns that occur with high 
token frequency may well provide a deeply en-
trenched and highly salient set of distributional 
cues that act as probabilistic constraints on con-
structional generalizations. We hypothesize that 
similar constructions of other less frequent verbs 
79
are processed, for lack of more specific overriding 
information, in the light of these constraints. Since 
processing is the result of a ?conspiracy? of dis-
tributed constraints, ?pathbreaking? prototypes 
need not be real construction exemplars but highly 
schematic patterns. We proved that highly frequent 
local positional patterns offer the right sort of con-
straint conspiracy. 
7 General discussion 
It appears that the distributional evidence of high 
frequency light verbs may well provide a solid 
cognitive anchor for sweeping perceptual generali-
zations on the syntax-semantics mapping. These 
generalizations are local, in that they involve pos i-
tional NV and VN pairs only, and are perceptual as 
they address the issue of identifying appropriate 
syntactic relations by relying on perceptual fea-
tures of linguistic contexts, such as position, ani-
macy, etc. On the basis of these findings, one can 
reasonably argue that complex lexical construc-
tions (in the sense of Goldberg 1998) are built 
upon these local patterns, by combining them in 
those contexts where the presence of a particular 
verb licenses such a combination.  
The two feature configurations discussed in ?.5 
(i.e. NLC and LC) can thus be viewed as two suc-
cessive steps along the path that leads towards the 
emergence of complex, lexically-driven construc-
tions. This can actually be modeled as the incre-
mental process of adding more and more lexical 
constraints to early lexicon-free generalizations 
(based on word order, animacy, definiteness etc.). 
As a result of such additional constraints, the pres-
ence of an intransitive verb may completely rule 
out the object interpretation of a VN pattern, flying 
in the face of a general bias towards viewing VN as 
a transitive pattern. This picture is compatible with 
the well-known observation that constructions are 
used rather conservatively by children at early 
stages of language maturation (Tomasello 2000). 
In fact, if early generalizations are mainly percep-
tual and local, we do not expect them to be used in 
production, at least until the child reaches a stage 
where they are combined into bigger lexically-
driven constructions. 
ME has proven to be a sound computational 
learning framework to simulate the interplay of 
complex probabilistic constraints in language. Our 
experiments confirm linguistic generalizations and 
psycholinguistic data for subjects and objects in 
Italian, while raising new interesting issues at the 
same time. This is the case of the role of definite-
ness in SOI. In fact, the model features neatly re-
produce the definiteness markedness hierarchy, but 
definiteness does not appear to be really influential 
for subject and object processing. Various hy-
potheses are compatible with such results, inclu d-
ing that definiteness is not a cue on which speakers 
rely for SOI in Italian. Another more interesting 
possibility is that definiteness constraints may in-
deed play a decisive role when the learner is asked 
to assign subject and object relations in the context 
of a more complex construction than a simple NV 
pair. Suppose that both nouns of a noun-noun-verb 
triple are amenable to a subject interpretation, but 
that one of them is a more likely subject than the 
other due to its being part of a definite noun 
phrase. Then, it is reasonable to expect that the 
model would select the definite noun phrase as the 
subject in the triple and opt for an object interpre-
tation of the other candidate noun phrase.  
As part of our future work, we plan to train the 
ME model on a more realistic corpus of parental 
input to Italian children, available in the CHILDES 
database (MacWhinney, 2000: http://childes.psy. 
cmu.edu/data/Romance/Italian). In fact, there is 
converging evidence that the use of particular con-
structions in parental speech is largely dominated 
by the use of each construction with one specific, 
highly frequent verb (e.g. go for the intransitive 
construction). The same trends noted in mother?s 
speech to children are mirrored in children?s early 
speech (Goldberg et al, 2004). Quochi (in prepara-
tion) reports a similar distributional pattern for the 
caused motion and intransitive motion verbs in two 
Italian CHILDES corpora (named ?Italian-
Antelmi? and ?Italian-Calambrone?). If these find-
ings are confirmed, the high accuracy of our ME 
model trained on the skewed frequency corpus 
(SF) allows us to expect an equally high accuracy 
when training the model on evidence from Italian 
parental speech.  
This brings us to another related point: lack of 
correction/supervision in parental input. Since our 
ME model heavily relies on previously classified 
noun-verb pairs, we can legitimately wonder how 
easily it can be extended to simulate child language 
learning in an unsupervised mode. In fact, it should 
be appreciated that, in our experiments, compar-
tively little rests on supervised classification. Iden-
80
tification of the contextually-relevant subject is, for 
lack of explicit morphosyntactic clues such as 
agreement and diathesis, simply a matter of guess-
ing the more likely agent of the action expressed 
by the verb on the basis of semantic and pragmatic 
features such as animacy, definiteness and noun 
position to the verb. Mutatis mutandis, the same 
holds for object identification. It is then highly 
likely that salient evidence for the correct sub-
ject/object classific ation comes to the child from 
direct observation of the situation described by a 
sentence. It is such systematic coupling of linguis-
tic evidence from the sentence with perceptual evi-
dence of the situation described by the sentence 
that can assist the child in developing interface 
notions such as subject, object and the like.  
References 
Aissen J., 2003. Differential object marking: iconicity 
vs. economy. Natural Language and Linguistic The-
ory, 21: 435-483. 
Bartolini R., Lenci A., Montemagni S., Pirrelli V., 2004. 
Hybrid constraints for robust parsing: First experi-
ments and evaluation. LREC2004: 859-862. 
Bates E., MacWhinney B., Caselli C., Devescovi A., 
Natale F., Venza V., 1984. A crosslinguistic study of 
the development of sentence interpretation strategies. 
Child Development, 55: 341-354. 
Berger A., Della Pietra S., Della Pietra V., 1996. A 
maximum entropy approach to natural language 
processing. Computational Linguistics 22(1): 39-71 
Bresnan J., Dingare D., Manning C. D., 2001. Soft con-
straints mirror hard constraints: voice and person in 
English and Lummi. Proceedings of the LFG01 Con-
ference , Hong Kong: 13-32. 
Goldberg A. E., 1998. The emergence of the semantics 
of argument structure constructions. In B. MacWhin-
ney (e d.), The Emergence of Language . Lawrence 
Erlbaum Associates, Hillsdale, N. J.: 197-212. 
Goldberg A. E., Casenhiser D., Sethuraman N., 2004. 
Learning argument structure generalizations, Cogni-
tive Linguistics. 
Goldwater S., Johnson M. 2003. Learning OT Con-
straint Rankings Using a Maximum Entropy Model. 
In Spenader J., Eriksson A., Dahl ?. (eds.), Proceed-
ings of the Stockholm Workshop on Variation within 
Optimality Theory. April 26-27, 2003, Stockholm 
University: 111-120. 
Lenci A. et al, 2000. SIMPLE: A Ge neral Framework 
for the Development of Multilingual Lexicons. Inter-
national Journal of Lexicography, 13 (4): 249-263. 
Manning C. D., 2003. Probabilistic syntax. In R. Bod, J. 
Hay, S. Jannedy (eds), Probabilistic Linguistics,  
MIT Press, Cambridge MA: 289-341. 
MacWhinney, B., 2000. The CHILDES project: Tools 
for analyzing talk. Third Edition. Mahwah, NJ: La w-
rence Erlbaum Associates 
MacWhinney B., Bates E., Kliegl R., 1984. Cue validity 
and sentence interpretation in English, German, and 
Italian. Journal of Verbal Learning and Verbal Be-
havior, 23: 127-150. 
MacWhinney B., 2004. A unified model of language 
acquisition. In J. Kroll & A. De Groot (eds.), Hand-
book of bilingualism: Psycholinguistic approaches, 
Oxford University Press, Oxford. 
Matthews D., Lieven E., Theakston A., Tomasello M., 
in press, The role of frequency in the acquisition of 
English word order, Cognitive Development. 
Miyao Y., Tsujii J., 2002. Maximum entropy estimation 
for feature forests. Proc. HLT2002. 
Montemagni S. et al 2003. Building the Italian syntac-
tic-semantic treebank. In Abeill? A. (ed.) Treebanks. 
Building and Using Parsed Corpora , Kluwer, 
Dordrecht: 189-210. 
Ninio, A. 1999. Pathbreaking verbs in syntactic devel-
opment and the question of prototypical transitivity. 
Journal of Child Language, 26: 619- 653. 
?vrelid L., 2004. Disambiguation of syntactic functions 
in Norwegian: modeling variation in word order in-
terpretations conditioned by animacy and definite-
ness. Proceedings of the 20th Scandinavian 
Conference of Linguistics, Helsinki. 
Quochi, V., (in preparation). A constructional analysis 
of parental speech: The role of frequency and predic-
tion in language acquisition, evidence from Italian. 
Ratnaparkhi A., 1998. Maximum Entropy Models for 
Natural Language Ambiguity Resolution. Ph.D. Dis-
sertation, University of Pennsylvania. 
Seidenberg M. S., MacDonald M. C. 1999. A probabil-
istic constraints approach to language acquisition and 
processing. Cognitive Science  23(4): 569-588. 
Stemberger, J., MacWhinney, B. 1986. Frequency and 
the lexical storage of regularly inflected forms. 
Memory and Cognition, 14:17-26. 
Tomasello M., 2000. Do young children have adult syn-
tactic competence? Cognition , 74: 209-253. 
81
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 21?28,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Probing the space of grammatical variation: 
induction of cross-lingual grammatical constraints from treebanks 
 
 
Felice Dell?Orletta 
Universit? di Pisa, Dipartimento di 
Informatica - Largo B. Pontecorvo 3 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
felice.dellorletta@ilc.cnr.it 
Alessandro Lenci 
Universit? di Pisa, Dipartimento di 
Linguistica - Via Santa Maria 36 
56100 Pisa, Italy 
 
alessandro.lenci@ilc.cnr.it 
Simonetta Montemagni 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
simonetta.montemagni@ilc.cnr.it 
Vito Pirrelli 
ILC-CNR - via G. Moruzzi 1 
56100 Pisa, Italy 
vito.pirrelli@ilc.cnr.it 
 
  
Abstract 
The paper reports on a detailed 
quantitative analysis of distributional 
language data of both Italian and Czech, 
highlighting the relative contribution of a 
number of distributed grammatical 
factors to sentence-based identification of 
subjects and direct objects. The work 
uses a Maximum Entropy model of 
stochastic resolution of conflicting 
grammatical constraints and is 
demonstrably capable of putting 
explanatory theoretical accounts to the 
test of usage-based empirical verification. 
1 Introduction 
The paper illustrates the application of a 
Maximum Entropy (henceforth MaxEnt) model 
(Ratnaparkhi 1998) to the processing of subjects 
and direct objects in Italian and Czech. The 
model makes use of richly annotated Treebanks 
to determine the types of linguistic factors 
involved in the task and weigh up their relative 
salience. In doing so,  we set ourselves a two-
fold goal. On the one hand, we intend to discuss 
the use of Treebanks to discover typologically 
relevant and linguistically motivated factors and 
assess the relative contribution of the latter to 
cross-linguistic parsing issues. On the other 
hand, we are interested in testing the empirical 
plausibility of constraint-resolution models of 
language processing (see infra) when confronted 
with real language data.  
Current research in natural language learning 
and processing supports the view that 
grammatical competence consists in mastering 
and integrating multiple, parallel constraints 
(Seidenberg and MacDonald 1999, MacWhinney 
2004). Moreover, there is growing consensus on 
two major properties of grammatical constraints: 
i.) they are probabilistic ?soft constraints? 
(Bresnan et al 2001), and ii.) they have an 
inherently functional nature, involving different 
types of linguistic (and non linguistic) 
information (syntactic, semantic, etc.). These 
features emerge particularly clearly in dealing 
with one of the core aspects of grammar 
learning: the ability to identify syntactic relations 
in text. Psycholinguistic evidence shows that 
speakers learn to identify sentence subjects and 
direct objects by combining various types of 
probabilistic, functional cues, such as word 
order, noun animacy, definiteness, agreement, 
etc. An important observation is that the relative 
prominence of each such cue can considerably 
vary cross-linguistically. Bates et al (1984), for 
example, argue that while, in English, word order 
is the most effective cue for Subject-Object 
Identification (henceforth SOI) both in syntactic 
processing and during the child?s syntactic 
development, the same cue plays second fiddle in 
relatively free phrase-order languages such as 
Italian or German. 
If grammatical constraints are inherently 
probabilistic (Manning 2003), the path through 
which adult grammar competence is acquired can 
be viewed as the process of building a stochastic 
model out of the linguistic input. In 
computational linguistics, MaxEnt models have 
21
proven to be robust statistical learning algorithms 
that perform well in a number of processing 
tasks. Being supervised learning models, they  
require richly annotated data as training input. 
Before we turn to the use of Treebanks for 
training a MaxEnt model for SOI, we first 
analyse the range of linguistic factors that are 
taken to play a significant role in the task.   
2 Subjects and objects in Czech and 
Italian 
Grammatical relations - such as subject (S) and 
direct object (O) - are variously encoded in 
languages, the two most widespread strategies 
being: i) structural encoding through word order, 
and ii) morpho-syntactic marking. In turn, 
morpho-syntactic marking can apply either on 
the noun head only, in the form of case 
inflections, or on both the noun and the verb, in 
the form of agreement marking (Croft 2003). 
Besides formal coding, the distribution of 
subjects and object is also governed by semantic 
and pragmatic factors, such as noun animacy, 
definiteness, topicality, etc. As a result, there 
exists a variety of linguistic clues jointly co-
operating in making a particular noun phrase the 
subject or direct object of a sentence. Crucially 
for our present purposes, cross-linguistic 
variation does not only concern the particular 
strategy used to encode S and O, but also the 
relative strength that each factor plays in a given 
language. For instance, while English word order 
is by and large the dominant clue to identify S 
and O, in other languages the presence of a rich 
morphological system allows word order to have 
a much looser connection with the coding of 
grammatical relations, thus playing a secondary 
role in their identification. Moreover, there are 
languages where semantic and pragmatic 
constraints such as animacy and/or definiteness 
play a predominant role in the processing of 
grammatical relations. A large spectrum of 
variations exists, ranging from languages where 
S must have a higher degree of animacy and/or 
definiteness relative to O, to languages where 
this constraint only takes the form of a softer 
statistical preference (cf. Bresnan et al 2001). 
The goal of this paper is to explore the area of 
this complex space of grammar variation through 
careful assessment of the distribution of S and O 
tokens in Italian and Czech. For our present 
analysis, we have used a MaxEnt statistical 
model trained on data extracted from two 
syntactically annotated corpora: the Prague 
Dependency Treebank (PDT, Bohmova et al 
2003) for Czech, and the Italian Syntactic 
Semantic Treebank (ISST, Montemagni et al 
2003) for Italian. These corpora have been 
chosen not only because they are the largest 
syntactically annotated resources for the two 
languages, but also because of their high degree 
of comparability, since they both adopt a 
dependency-based annotation scheme. 
Czech and Italian provide an interesting 
vantage point for the cross-lingual analysis of 
grammatical variation. They are both Indo-
European languages, but they do not belong to 
the same family: Czech is a West Slavonic 
language, while Italian is a Romance language. 
For our present concerns, they appear to share 
two crucial features: i) the free order of 
grammatical relations with respect to the verb; ii) 
the possible absence of an overt subject. 
Nevertheless, they also greatly differ due to: the 
virtual non-existence of case marking in Italian 
(with the only marginal exception of personal 
pronouns), and the degree of phrase-order 
freedom in the two languages. Empirical 
evidence supporting the latter claim is provided 
in Table 1, which reports data extracted from 
PDT and ISST. Notice that although in both 
languages S and O can occur either pre-verbally 
or post-verbally, Czech and Italian greatly differ 
in their propensity to depart from the (unmarked) 
SVO order. While in Italian preverbal O is 
highly infrequent (1.90%), in Czech more than 
30% of O tokens occur before the verb. The 
situation is similar but somewhat more balanced 
in the case of S, which occurs post-verbally in 
22.21% of the Italian cases, and  in  40% of 
Czech ones. For sure, one can argue that, in 
spoken Italian, the number of pre-verbal objects 
is actually higher, because of the greater number 
of left dislocations and topicalizations occurring 
in informal speech. However reasonable, the 
observation does not explain away the 
distributional differences in the two corpora, 
since both PDT and ISST contain written 
language only. We thus suggest that there is clear 
empirical evidence in favour of a systematic, 
higher phrase-order freedom in Czech, arguably 
related to the well-known correlation of Czech 
constituent placement with sentence information 
structure, with the element carrying new 
information showing a tendency to occur 
sentence-finally (Stone 1990). For our present 
concerns, however, aspects of information 
structure, albeit central in Czech grammar, were 
not taken into  account, as they  happen not to  be 
22
 
   Czech Italian 
    Subj Obj Subj Obj 
Pre 59.82% 30.27% 77.79% 1.90% 
Post 40.18% 69.73% 22.21% 98.10% Pos 
All 100.00% 100.00% 100.00% 100.00% 
Agr 98.50% 56.54% 97.73% 58.33% 
NoAgr 1.50% 43.46% 2.27% 41.67% Agr 
All 100.00% 100.00% 100.00% 100.00% 
Anim 34.10% 15.42% 50.18% 10.67% 
NoAnim 65.90% 84.58% 49.82% 89.33% Anim 
All 100.00% 100.00% 100.00% 100.00% 
Table 1 ?Distribution of Czech and Italian S and O wrt word order, 
agreement and noun animacy 
 Czech 
 Subj Obj 
Nominative 53.83% 0.65% 
Accusative 0.15% 28.30% 
Dative 0.16% 9.54% 
Genitive 0.22% 2.03% 
Instrumental 0.01% 3.40% 
Ambiguous 45.63% 56.08% 
All 100.00% 100.00% 
Table 2 - Distribution of Czech S and O 
wrt case 
marked-up in the Italian corpus.  
According to the data reported in Table 1, 
Czech and Italian show similar correlation 
patterns between animacy and grammatical 
relations. S and O in ISST were automatically 
annotated for animacy using the SIMPLE Italian 
computational lexicon (Lenci et al 2000) as a 
background semantic resource. The annotation 
was then checked manually. Czech S and O were 
annotated for animacy using Czech WordNet 
(Pala and Smrz 2004); it is worth remarking that 
in Czech animacy annotation was done only 
automatically, without any manual revision. 
Italian shows a prominent asymmetry in the 
distribution of animate nouns in subject and 
object roles: over 50% of ISST subjects are 
animate, while only 10% of the objects are 
animate. Such a trend is also confirmed in Czech 
? although to a lesser extent - with 34.10% of 
animate subjects vs. 15.42% of objects.1 Such an 
overwhelming preference for animate subjects in 
corpus data suggests that animacy may play a 
very important role for S and O identification in 
both languages. 
Corpus data also provide interesting evidence 
concerning the actual role of morpho-syntactic 
constraints in the distribution of grammatical 
relations. Prima facie, agreement and case are 
the strongest and most directly accessible clues 
for S/O processing, as they are marked both 
overtly and locally. This is also confirmed by 
psycholinguistic evidence, showing that subjects 
tend to rely on these clues to identify S/O. 
However, it should be observed that agreement 
can be relied upon conclusively in S/O 
processing only when a nominal constituent and 
                                               
1 In fact, the considerable difference in animacy distribution 
between the two languages might only be an artefact of the 
way we annotated Czech nouns semantically, on the basis of 
their context-free classification in the Czech WordNet. 
a verb do not agree in number and/or person (as 
in leggono il libro ?(they) read the book?). 
Conversely, when N and V share the same 
person and number, no conclusion can be drawn, 
as trivially shown by a sentence like il bambino 
legge il libro ?the child reads the book?. In ISST, 
more than 58% of O tokens agree with their 
governing V, thus being formally 
indistinguishable from S on the basis of 
agreement features. PDT also exhibits a similar 
ratio, with 56% of O tokens agreeing with their 
verb head. Analogous considerations apply to 
case marking, whose perceptual reliability is 
undermined by morphological syncretism,  
whereby different  cases are realized through the 
same marker. Czech data reveal the massive 
extent of this phenomenon and its impact on SOI. 
As reported in Table 2, more than 56% of O 
tokens extracted from PDT are formally 
indistinguishable from S in case ending. 
Similarly, 45% of S tokens are formally 
indistinguishable from O uses on the same 
ground. All in all, this means that in 50% of the 
cases a Czech noun can not be understood as the 
S/O of a sentence by relying on overt case 
marking only. 
To sum up, corpus data lend support to the 
idea that in both Italian and in Czech SOI is 
governed by a complex interplay of probabilistic 
constraints of a different nature (morpho-
syntactic, semantic, word order, etc.) as the latter 
are neither singly necessary nor jointly sufficient 
to attack the processing task at hand. It is 
tempting to hypothesize that the joint distribution 
of these data can provide a statistically reliable 
basis upon which relevant probabilistic 
constraints are bootstrapped and combined 
consistently. This should be possible due to i) the 
different degrees of clue salience in the two 
languages and ii) the functional need to minimize 
23
processing ambiguity in ordinary communicative 
exchanges. With reference to the latter point, for 
example, we may surmise that a speaker will be 
more inclined to violate one constraint on S/O 
distribution (e.g. word order) when another clue 
is available (e.g. animacy) that strongly supports 
the intended interpretation only. The following 
section illustrates how a MaxEnt model can be 
used to model these intuitions by bootstrapping 
constraints and their interaction from language 
data. 
3 Maximum Entropy modelling 
The MaxEnt framework offers a mathematically 
sound way to build a probabilistic model for SOI, 
which combines different linguistic cues. Given 
a linguistic context c and an outcome a?A that 
depends on c, in the MaxEnt framework the 
conditional probability distribution p(a|c) is 
estimated on the basis of the assumption that no 
a priori constraints must be met other than those 
related to a set of features fj(a,c) of c, whose 
distribution is derived from the training data. It 
can be proven that the probability distribution p 
satisfying the above assumption is the one with 
the highest entropy, is unique and has the 
following exponential form (Berger et al 1996): 
(1) ?
=
=
k
j
caf
j
j
cZcap 1
),(
)(
1)|( a  
where Z(c) is a normalization factor, fj(a,c) are 
the values of k features of the pair (a,c) and 
correspond to the linguistic cues of c that are 
relevant to predict the outcome a. Features are 
extracted from the training data and define the 
constraints that the probabilistic model p must 
satisfy. The parameters of the distribution ?1, ?, 
?k correspond to weights associated with the 
features, and determine the relevance of each 
feature in the overall model. In the experiments 
reported below feature weights have been 
estimated with the Generative Iterative Scaling 
(GIS) algorithm implemented in the AMIS 
software (Miyao and Tsujii 2002). 
We model SOI as the task of predicting the 
correct syntactic function ? ? {subject, object} 
of a noun occurring in a given syntactic context 
?. This is equivalent to building the conditional 
probability distribution p(?|?) of having a 
syntactic function ? in a syntactic context ?. 
Adopting the MaxEnt approach, the distribution 
p can be rewritten in the parametric form of (1), 
with features corresponding to the linguistic 
contextual cues relevant to SOI. The context ? is 
a pair <v?, n?>, where v? is the verbal head and n? 
its nominal dependent in ?. This notion of ? 
departs from more traditional ways of describing 
an SOI context as a triple of one verb and two 
nouns in a certain syntactic configuration (e.g, 
SOV or VOS, etc.). In fact, we assume that SOI 
can be stated in terms of the more local task of 
establishing the grammatical function of a noun 
n observed in a verb-noun pair. This simplifying 
assumption is consistent with the claim in 
MacWhinney et al (1984) that SVO word order 
is actually derivative from SV and VO local 
patterns and downplays the role of the transitive 
complex construction in sentence processing. 
Evidence in favour of this hypothesis also comes 
from corpus data: for instance, in ISST complete 
subject-verb-object configurations represent only 
26% of the cases, a small percentage if compared 
to the 74% of verb tokens appearing with either a 
subject or an object only; a similar situation can 
be observed in PDT where complete subject-
verb-object configurations occur in only 20% of 
the cases. Due to the comparative sparseness of 
canonical SVO constructions in Czech and 
Italian, it seems more reasonable to assume that 
children should pay a great deal of attention to 
both SV and VO units as cues in sentence 
perception (Matthews et al in press). 
Reconstruction of the whole lexical SVO pattern 
can accordingly be seen as the end point of an 
acquisition process whereby smaller units are re-
analyzed as being part of more comprehensive 
constructions. This hypothesis is more in line 
with a distributed view of canonical 
constructions as derivative of more basic local 
positional patterns, working together to yield 
more complex and abstract constructions. Last 
but not least, assuming verb-noun pairs as the 
relevant context for SOI allows us to 
simultaneously model the interaction of word 
order variation with pro-drop. 
4 Feature selection 
The most important part of any MaxEnt model is 
the selection of the context features whose 
weights are to be estimated from data 
distributions. Our feature selection strategy is 
grounded on the main assumption that features 
should correspond to theoretically and 
typologically well-motivated contextual cues. 
This allows us to evaluate the probabilistic 
model also with respect to its consistency with 
current linguistic generalizations. In turn, the 
model can be used as a probe into the 
correspondence between theoretically motivated 
24
generalizations and usage-based empirical 
evidence.  
Features are binary functions fki,? (?,?), which 
test whether a certain cue ki for the feature ? 
occurs in the context ?. For our MaxEnt model, 
we have selected different features types that test 
morpho-syntactic, syntactic, and semantic key 
dimensions in determining the distribution of S 
and O. 
 
Morpho-syntactic features. These include N-V 
agreement, for Italian and Czech, and case, only 
for Czech. The combined use of such features 
allow us not only to test the impact of morpho-
syntactic information on SOI, but also to analyze 
patterns of cross-lingual variation stemming 
from language specific morphological 
differences, e.g. lack of case marking in Italian. 
 
Word order. This feature essentially test the 
position of the noun wrt the verb, for instance: 
(2)
??
? == otherwise
postposnounifsubjf subjpost 0
.1),(, ss  
 
Animacy. This is the main semantic feature, 
which tests whether the noun in ? is animate or 
inanimate (cf. section 2). The centrality of this 
cue for grammatical relation assignment is 
widely supported by typological evidence (cf. 
Aissen 2003, Croft 2003). The Animacy 
Markedness Hierarchy - representing the relative 
markedness of the associations between 
grammatical functions and animacy degrees ? is 
actually assigned the role of a functional 
universal principle in grammar. The hierarchy is 
reported below, with each item in these scales 
been less marked than the elements to its right: 
 
Animacy Markedness Hierarchy 
Subj/Human > Subj/Animate > Subj/Inanimate 
Obj/Inanimate > Obj/Animate > Obj/Human 
 
Markedness hierarchies have also been 
interpreted as probabilistic constraints estimated 
from corpus data (Bresnan et al 2001). In our 
MaxEnt model we have used a reduced version 
of the animacy markedness hierarchy in which 
human and animate nouns have been both 
subsumed under the general class animate. 
 
Definiteness tests the degree of ?referentiality? of 
the noun in a context pair ?. Like for animacy, 
definiteness has been claimed to be associated 
with grammatical functions, giving rise to the 
following universal markedness hierarchy Aissen 
(2003): 
 
Definiteness Markedness Hierarchy 
Subj/Pro > Subj/Name > Subj/Def > Subj/Indef 
Obj/Indef > Obj/Def > Obj/Name > Obj/Pro 
 
According to this hierarchy, subjects with a low 
degree of definiteness are more marked than 
subjects with a high degree of definiteness (for 
objects the reverse pattern holds). Given the 
importance assigned to the definiteness 
markedness hierarchy in current linguistic 
research, we have included the definiteness cue 
in the MaxEnt model. In our experiments, for 
Italian we have used a compact version of the 
definiteness scale: the definiteness cue tests 
whether the noun in the context pair i) is a name 
or a pronoun ii) has a definite article iii), has an 
indefinite article or iv) is a bare noun (i.e. with 
no article). It is worth saying that bare nouns are 
usually placed at the bottom end of the 
definiteness scale. Since in Czech there is no 
article, we only make a distinction between 
proper names and common nouns. 
5 Testing the model 
The Italian MaxEnt model was trained on 14,643 
verb-subject/object pairs extracted from ISST. 
For Czech, we used a training corpus of 37,947 
verb-subject/object pairs extracted from PDT. In 
both cases, the training set was obtained by 
extracting all verb-subject and verb-object 
dependencies headed by an active verb, with the 
exclusion of all cases where the position of the 
nominal constituent was grammatically 
determined (e.g. clitic objects, relative clauses). 
It is interesting to note that in both training sets 
the proportion of subjects and objects relations is 
nearly the same: 63.06%-65.93% verb-subject 
pairs and 36.94%-34.07% verb-object pairs for 
Italian and Czech respectively. 
The test corpus consists of a set of verb-noun 
pairs randomly extracted from the reference 
Treebanks: 1,000 pairs for Italian and 1,373 for 
Czech. For Italian, 559 pairs contained a subject 
and 441 contained an object; for Czech, 905 
pairs contained a subject and 468 an object. 
Evaluation was carried out by calculating the 
percentage of  correctly  assigned  relations  over 
the total number of test pairs (accuracy). As our 
model always assigns one syntactic relation to 
each test pair, accuracy equals both standard 
precision and recall. 
25
  Czech Italian 
  Subj Obj Subj Obj 
Preverb 1.99% 19.40% 0.00% 6.90% 
Postverb 71.14% 7.46% 71.55% 21.55% 
Anim 0.50% 3.98% 6.90% 21.55% 
Inanim 72.64% 22.89% 64.66% 6.90% 
Nomin 0.00% 1.00% 
Genitive 0.50% 0.00% 
Dative 1.99% 0.00% 
Accus 0.00% 0.00% 
Instrum 0.00% 0.00% 
Ambig 70.65% 25.87% 
Na 
Agr 70.15% 25.87% 61.21% 12.07% 
NoAgr 2.99% 0.50% 7.76% 1.72% 
NAAgr 0.00% 0.50% 2.59% 14.66% 
Table 3 ? Types of errors for Czech and Italian 
 
 Czech Italian 
 Subj Obj Subj Obj 
Preverb 1.24E+00 5.40E-01 1.31E+00 2.11E-02 
Postverb 8.77E-01 1.17E+00 5.39E-01 1.38E+00 
Anim 1.16E+00 6.63E-01 1.28E+00 3.17E-01 
Inanim 1.03E+00 9.63E-01 8.16E-01 1.23E+00 
PronName 1.13E+00 7.72E-01 1.13E+00 8.05E-01 
DefArt 1.01E+00 1.02E+00 
IndefArt 6.82E-01 1.26E+00 
NoArticle 
1.05E+00 9.31E-01 
9.91E-01 1.02E+00 
Nomin 1.23E+00 2.22E-02 
Genitive 2.94E-01 1.51E+00 
Dative 2.85E-02 1.49E+00 
Accus 8.06E-03 1.39E+00 
Instrum 3.80E-03 1.39E+00 
Na 
Agr 1.18E+00 6.67E-01 1.28E+00 4.67E-01 
NoAgr 7.71E-02 1.50E+00 1.52E-01 1.58E+00 
NAAgr 3.75E-01 1.53E+00 2.61E-01 1.84E+00 
Table 4 - Feature value weights in NLC for Czech and 
Italian
We have assumed a baseline score of 56% for 
Italian and of 66% for Czech, corresponding to 
the result yielded by a naive model   assigning   
to  each   test   pair  the   most frequent relation 
in the training corpus, i.e. subject. Experiments 
were carried out with the general features 
illustrated in section 4: verb agreement, case (for 
Czech only), word order, noun animacy and 
noun definiteness. 
Accuracy on the test corpus is 88.4% for 
Italian and 85.4% for Czech. A detailed error 
analysis for the two languages is reported in 
Table 3, showing that in both languages subject 
identification appears to be particularly 
problematic. In Czech, it appears that the 
prototypically mistaken subjects are post-verbal 
(71.14%), inanimate (72.64%), ambiguously 
case-marked (70.65%) and agreeing with the 
verb (70.15%), where reported percentages refer 
to the whole error set. Likewise, Italian mistaken 
subjects can be described thus: they typically 
occur in post-verbal position (71.55%), are 
mostly inanimate (64.66%) and agree with the 
verb (61.21%). Interestingly, in both languages, 
the highest number of errors occurs when a) N 
has the least prototypical syntactic and semantic 
properties for O or S (relative to word order and 
noun animacy) and b) morpho-syntactic features 
such as agreement and case are neutralised. This 
shows that MaxEnt is able to home in on the core 
linguistic properties that govern the distribution 
of S and O in Italian and Czech, while remaining 
uncertain in the face of somewhat peripheral and 
occasional cases. 
A further way to evaluate the goodness of fit 
of our model is by inspecting the weights 
associated with feature values for the two 
languages. They are reported in Table 4, where 
grey cells highlight the preference of each 
feature value for either subject or object 
identification. In both languages agreement with 
the verb strongly relates to the subject relation. 
For Czech, nominative case is strongly 
associated with subjects while the other cases 
with objects. Moreover, in both languages 
preverbal subjects are strongly preferred over 
preverbal objects; animate subjects are preferred 
over animate objects; pronouns and proper 
names are typically subjects.  
Let us now try to relate these feature values to 
the Markedness Hierarchies reported in section 
4. Interestingly enough, if we rank the Italian 
Anim and Inanim values for subjects and objects, 
we observe that they distribute consistently with 
the Animacy Markedness Hierarchy: Subj/Anim 
> Subj/Inanim and Obj/Inanim > Obj/Anim. This 
is confirmed by the Czech results. Similarly, by 
ranking the Italian values for the definiteness 
features in the Subj column by decreasing weight 
values we obtain the following ordering: 
PronName > DefArt > IndefArt > NoArt, which 
nicely fits in with the Definiteness Markedness 
Hierarchy in section 4. The so-called 
?markedness reversal? is replicated with a good 
degree of approximation, if we focus on the 
values for the same features in the Obj column: 
the PronName feature represents the most 
marked option, followed by IndefArt, DefArt and 
NoArt (the latter two showing the same feature 
value). The exception here is represented by the 
relative ordering of IndefArt and DefArt which 
however show very close values. The same 
26
seems to hold for Czech, where the feature 
ordering for Subj is PronName > 
DefArt/IndefArt/NoArt and the reverse is 
observed for Obj.  
5.1 Evaluating comparative feature salience 
The relative salience of the different constraints 
acting on SOI can be inferred by comparing the 
weights associated with individual feature 
values. For instance, Goldwater and Johnson 
(2003) show that MaxEnt can successfully be 
applied to learn constraint rankings in Optimality 
Theory, by assuming the parameter weights <?1, 
?, ?k> as the ranking values of the constraints.  
Table 5 illustrates the constraint ranking for 
the two languages, ordered by decreasing weight 
values for both S and O. Note that, although not 
all constraints are applicable in both languages, 
the weights associated with applicable 
constraints exhibit the same relative salience in 
Czech and Italian. This seems to suggest the 
existence of a rather dominant (if not universal) 
salience scale of S and O processing constraints, 
in spite of the considerable difference in the 
marking strategies adopted by the two languages. 
As the relative weight of each constraint 
crucially depends on its overall interaction with 
other constraints on a given processing task, 
absolute weight values can considerably vary 
from language to language, with a resulting 
impact on the distribution of S and O 
constructions. For example, the possibility of 
overtly and unambiguously marking a direct 
object with case inflection makes wider room for 
preverbal use of objects in Czech. Conversely, 
lack of case marking in Italian considerably 
limits the preverbal distribution of direct objects.   
This evidence, however, appears to be an 
epiphenomenon of the interaction of fairly stable 
and invariant preferences, reflecting common 
functional tendencies in language processing. As 
shown in Table 5, if constraint ranking largely 
confirms the interplay between animacy and 
word order in Italian, Czech does not contradict 
it but rather re-modulate it somewhat, due to the 
?perturbation? factors introduced by its richer 
battery of case markers. 
6 Conclusions 
Probabilistic language models, machine language 
learning algorithms and linguistic theorizing all 
appear to support a view of language processing 
as a process of dynamic, on-line resolution of 
conflicting grammatical constraints. We begin to 
gain considerable insights into the complex 
process of bootstrapping nature and behaviour of 
these constraints upon observing their actual 
distribution in perceptually salient contexts. In 
our view of things, this trend outlines a 
promising framework providing fresh support to 
usage-based models of language acquisition 
through mathematical and computational 
simulations. Moreover, it allows scholars to 
investigate patterns of cross-linguistic 
typological variation that crucially depend on the 
appropriate setting of model parameters. Finally, 
it promises to solve, on a principled basis, 
traditional performance-oriented cruces of 
grammar theorizing such as degrees of human 
acceptability of ill-formed grammatical 
constructions (Hayes 2000) and the inherently 
graded compositionality of linguistic 
constructions such as morpheme-based words 
and word-based phrases (Bybee 2002, Hay and 
Baayen 2005).  
We argue that the current availability of 
comparable, richly annotated corpora and of 
mathematical tools and models for corpus 
exploration make time ripe for probing the space 
of grammatical variation, both intra- and inter-
linguistically, on unprecedented levels of 
sophistication and granularity. All in all, we 
anticipate that such a convergence is likely to 
have a twofold impact: it is bound to shed light 
on the integration of performance and 
competence factors in language study; it will 
make mathematical models of language 
increasingly able to accommodate richer and 
richer language evidence, thus putting 
explanatory theoretical accounts to the test of a 
usage-based empirical verification. 
In the near future, we intend to pursue two 
parallel lines of development. First we would 
like to increase the context-sensitiveness of our 
processing task by integrating binary 
grammatical constraints into the broader context 
of multiply conflicting grammar relations. This 
way, we will be in a position to capture the 
constraint that a (transitive) verb has at most one 
subject and one object, thus avoiding multiple 
assignment of subject (object) relations in the 
same context. Suppose, for example, that both 
nouns in a noun-noun-verb triple are amenable to 
a subject interpretation, but that one of them is a 
more likely subject than the other. Then, it is 
reasonable to expect the model to process the 
less likely subject candidate as the object of the 
verb in the triple. Another promising line of 
development is based on the observation that the 
27
order in which verb arguments appear in context 
is also lexically governed: in Italian, for 
example, report verbs show a strong tendency to 
select subjects post-verbally. Dell?Orletta et al 
(2005) report a substantial improvement on the 
model performance on Italian SOI when lexical 
information is taken into account, as a lexicalized 
MaxEnt model appears to integrate general 
constructional and semantic biases with 
lexically-specific preferences. In a cross-lingual 
perspective, comparable evidence of lexical 
constraints on word order would allow us to 
discover language-wide invariants in the lexicon-
grammar interplay.   
References 
Bates E., MacWhinney B., Caselli C., Devescovi A., 
Natale F., Venza V. 1984. A crosslinguistic study 
of the development of sentence interpretation 
strategies. Child Development, 55: 341-354. 
Bohmova A., Hajic J., Hajicova E., Hladka B. 2003. 
The Prague Dependency Treebank: Three-Level 
Annotation Scenario, in A. Abeille (ed.) 
Treebanks: Building and Using Syntactically 
Annotated Corpora, Kluwer Academic Publishers, 
pp. 103-128. 
Bybee J. 2002. Sequentiality as the basis of 
constituent structure. in T. Giv?n and B. Malle 
(eds.) The Evolution of Language out of Pre-
Language, Amsterdam: John Benjamins. 107-132. 
Croft W. 2003. Typology and Universals. Second 
Edition, Cambridge University Press, Cambridge. 
Bresnan J., Dingare D., Manning C. D. 2001. Soft 
constraints mirror hard constraints: voice and 
person in English and Lummi. Proceedings of the 
LFG01 Conference, Hong Kong: 13-32. 
Dell?Orletta F., Lenci A., Montemagni S., Pirrelli V. 
2005. Climbing the path to grammar: a maximum 
entropy model of subject/object learning. 
Proceedings of the ACL-2005 Workshop 
?Psychocomputational Models of Human 
Language Acquisition?, University of Michigan, 
Ann Arbour (USA), 29-30 June 2005. 
Hay J., Baayen R.H. 2005. Shifting paradigms: 
gradient structure in morphology, Trends in 
Cognitive Sciences, 9(7): 342-348. 
Hayes B. 2000. Gradient Well-Formedness in 
Optimality Theory, in Joost Dekkers, Frank van 
der Leeuw and Jeroen van de Weijer (eds.) 
Optimality Theory: Phonology, Syntax, and 
Acquisition, Oxford University Press, pp. 88-120. 
Lenci A. et al 2000. SIMPLE: A General Framework 
for the Development of Multilingual Lexicons. 
International Journal of Lexicography, 13 (4): 
249-263. 
MacWhinney B. 2004. A unified model of language 
acquisition. In J. Kroll & A. De Groot (eds.), 
Handbook of bilingualism: Psycholinguistic 
approaches, Oxford University Press, Oxford. 
Manning C. D. 2003. Probabilistic syntax. In R. Bod, 
J. Hay, S. Jannedy (eds), Probabilistic Linguistics,  
MIT Press, Cambridge MA: 289-341. 
Miyao Y., Tsujii J. 2002. Maximum entropy 
estimation for feature forests. Proc. HLT2002. 
Montemagni S. et al 2003. Building the Italian 
syntactic-semantic treebank. In Abeill? A. (ed.) 
Treebanks. Building and Using Parsed Corpora, 
Kluwer, Dordrecht: 189-210. 
Ratnaparkhi A. 1998. Maximum Entropy Models for 
Natural Language Ambiguity Resolution. Ph.D. 
Dissertation, University of Pennsylvania. 
   
Constraints for S  Constraints for O 
Feature Italian Czech  Feature Italian Czech 
Preverbal 1.31E+00 1.24E+00  Genitive na 1.51E+00 
Nomin na 1.23E+00  NoAgr 1.58E+00 1.50E+00 
Agr 1.28E+00 1.18E+00  Dative na 1.49E+00 
Anim 1.28E+00 1.16E+00  Accus na 1.39E+00 
Inanim 8.16E-01 1.03E+00  Instrum na 1.39E+00 
Postverbal 5.39E-01 8.77E-01  Postverbal 1.38E+00 1.17E+00 
Genitive na 2.94E-01  Inanim 1.23E+00 9.63E-01 
NoAgr 1.52E-01 7.71E-02  Agr 4.67E-01 6.67E-01 
Dative na 2.85E-02  Anim 3.17E-01 6.63E-01 
Accus na 8.06E-03  Preverbal 2.11E-02 5.40E-01 
Instrum na 3.80E-03  Nomin na 2.22E-02 
Table 5 ? Ranked constraints for S and O in Czech and Italian 
28
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 49?56,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
ISA meets Lara:
An incremental word space model
for cognitively plausible simulations of semantic learning
Marco Baroni
CIMeC (University of Trento)
C.so Bettini 31
38068 Rovereto, Italy
marco.baroni@unitn.it
Alessandro Lenci
Department of Linguistics
University of Pisa
Via Santa Maria 36
56126 Pisa, Italy
alessandro.lenci@ilc.cnr.it
Luca Onnis
Department of Psychology
Cornell University
Ithaca, NY 14853
lo35@cornell.edu
Abstract
We introduce Incremental Semantic Analy-
sis, a fully incremental word space model,
and we test it on longitudinal child-directed
speech data. On this task, ISA outperforms
the related Random Indexing algorithm, as
well as a SVD-based technique. In addi-
tion, the model has interesting properties
that might also be characteristic of the se-
mantic space of children.
1 Introduction
Word space models induce a semantic space from
raw textual input by keeping track of patterns of
co-occurrence of words with other words through a
vectorial representation. Proponents of word space
models such as HAL (Burgess and Lund, 1997) and
LSA (Landauer and Dumais, 1997) have argued that
such models can capture a variety of facts about hu-
man semantic learning, processing, and representa-
tion. As such, word space methods are not only
increasingly useful as engineering applications, but
they are also potentially promising for modeling
cognitive processes of lexical semantics.
However, to the extent that current word space
models are largely non-incremental, they can hardly
accommodate how young children develop a seman-
tic space by moving from virtually no knowledge
of the language to reach an adult-like state. The
family of models based on singular value decom-
position (SVD) and similar dimensionality reduc-
tion techniques (e.g., LSA) first construct a full co-
occurrence matrix based on statistics extracted from
the whole input corpus, and then build a model at
once via matrix algebra operations. Admittedly,
this is hardly a plausible simulation of how chil-
dren learn word meanings incrementally by being
exposed to short sentences containing a relatively
small number of different words. The lack of incre-
mentality of several models appears conspicuous es-
pecially given their explicit claim to solve old theo-
retical issues about the acquisition of language (e.g.,
(Landauer and Dumais, 1997)). Other extant models
display some degree if incrementality. For instance,
HAL and Random Indexing (Karlgren and Sahlgren,
2001) can generate well-formed vector representa-
tions at intermediate stages of learning. However,
they lack incrementality when they make use of stop
word lists or weigthing techniques that are based on
whole corpus statistics. For instance, consistently
with the HAL approach, Li et al (2000) first build
a word co-occurrence matrix, and then compute the
variance of each column to reduce the vector dimen-
sions by discarding those with the least contextual
diversity.
Farkas and Li (2000) and Li et al (2004) pro-
pose an incremental version of HAL by using a a re-
current neural network trained with Hebbian learn-
ing. The networks incrementally build distributional
vectors that are then used to induce word semantic
clusters with a Self-Organizing Map.Farkas and Li
(2000) does not contain any evaluation of the struc-
ture of the semantic categories emerged in the SOM.
A more precise evaluation is instead performed by
Li et al (2004), revealing the model?s ability to sim-
ulate interesting aspects of early vocabulary dynam-
ics. However, this is achieved by using hybrid word
49
representations, in which the distributional vectors
are enriched with semantic features derived from
WordNet.
Borovsky and Elman (2006) also model word
learning in a fairly incremental fashion, by using the
hidden layer vectors of a Simple Recurrent Network
as word representations. The network is probed at
different training epochs and its internal represen-
tations are evaluated against a gold standard ontol-
ogy of semantic categories to monitor the progress in
word learning. Borovsky and Elman (2006)?s claim
that their model simulates relevant aspects of child
word learning should probably be moderated by the
fact that they used a simplified set of artificial sen-
tences as training corpus. From their simulations it
is thus difficult to evaluate whether the model would
scale up to large naturalistic samples of language.
In this paper, we introduce Incremental Semantic
Indexing (ISA), a model that strives to be more de-
velopmentally plausible by achieving full incremen-
tality. We test the model and some of its less incre-
mental rivals on Lara, a longitudinal corpus of child-
directed speech based on samples of child-adult lin-
guistic interactions collected regularly from 1 to 3
years of age of a single English child. ISA achieves
the best performance on these data, and it learns
a semantic space that has interesting properties for
our understanding of how children learn and struc-
ture word meaning. Thus, the desirability of incre-
mentality increases as the model promises to cap-
ture specific developmental trajectories in semantic
learning.
The plan of the paper is as follows. First, we
introduce ISA together with its main predecessor,
Random Indexing. Then, we present the learning
experiments in which several versions of ISA and
other models are trained to induce and organize lexi-
cal semantic information from child-directed speech
transcripts. Lastly, we discuss further work in devel-
opmental computational modeling using word space
models.
2 Models
2.1 Random Indexing
Since the model we are proposing can be seen as
a fully incremental variation on Random Indexing
(RI), we start by introducing the basic features of
RI (Karlgren and Sahlgren, 2001). Initially, each
context word is assigned an arbitrary vector repre-
sentation of fixed dimensionality d made of a small
number of randomly distributed +1 and -1, with all
other dimensions assigned a 0 value (d is typically
much smaller than the dimensionality of the full co-
occurrence matrix). This vector representation is
called signature. The context-dependent represen-
tation for a given target word is then obtained by
adding the signatures of the words it co-occurs with
to its history vector. Multiplying the history by a
small constant called impact typically improves RI
performance. Thus, at each encounter of target word
t with a context word c, the history of t is updated as
follows:
ht += i? sc (1)
where i is the impact constant, ht is the history vec-
tor of t and sc is the signature vector of c. In this
way, the history of a word keeps track of the con-
texts in which it occurred. Similarity among words
is then measured by comparing their history vectors,
e.g., measuring their cosine.
RI is an extremely efficient technique, since it di-
rectly builds and updates a matrix of reduced di-
mensionality (typically, a few thousands elements),
instead of constructing a full high-dimensional co-
occurrence matrix and then reducing it through SVD
or similar procedures. The model is incremental
to the extent that at each stage of corpus process-
ing the vector representations are well-formed and
could be used to compute similarity among words.
However, RI misses the ?second order? effects that
are claimed to account, at least in part, for the ef-
fectiveness of SVD-based techniques (Manning and
Schu?tze, 1999, 15.4). Thus, for example, since dif-
ferent random signatures are assigned to the words
cat, dog and train, the model does not capture the
fact that the first two words, but not the third, should
count as similar contexts. Moreover, RI is not fully
incremental in several respects. First, on each en-
counter of two words, the same fixed random sig-
nature of one of them is added to the history of the
other, i.e., the way in which a word affects another
does not evolve with the changes in the model?s
knowledge about the words. Second, RI makes use
of filtering and weighting procedures that rely on
50
global statistics, i.e., statistics based on whole cor-
pus counts. These procedures include: a) treating
the most frequent words as stop words; b) cutting
off the lowest frequency words as potential contexts;
and c) using mutual information or entropy mea-
sures to weight the effect of a word on the other).
In addition, although procedures b) and c) may have
some psychological grounding, procedure a) would
implausibly entail that to build semantic represen-
tations the child actively filters out high frequency
words as noise from her linguistic experience. Thus,
as it stands RI has some noticeable limitations as a
developmental model.
2.2 Incremental Semantic Analysis
Incremental Semantic Analysis (ISA) differs from
RI in two main respects. First and most importantly,
when a word encounters another word, the history
vector of the former is updated with a weighted sum
of the signature and the history of the latter. This
corresponds to the idea that a target word is affected
not only by its context words, but also by the se-
mantic information encoded by that their distribu-
tional histories. In this way, ISA can capture SVD-
like second order effects: cat and dog might work
like similar contexts because they are likely to have
similar histories. More generally, this idea relies on
two intuitively plausible assumptions about contex-
tual effects in word learning, i.e., that the informa-
tion carried by a context word will change as our
knowledge about the word increases, and that know-
ing about the history of co-occurrence of a context
word is an important part of the information being
contributed by the word to the targets it affects.
Second, ISA does not rely on global statistics for
filtering and weighting purposes. Instead, it uses a
weighting scheme that changes as a function of the
frequency of the context word at each update. This
makes the model fully incremental and (together
with the previous innovation) sensitive not only to
the overall frequency of words in the corpus, but to
the order in which they appear.
More explicitly, at each encounter of a target word
t with a context word c, the history vector of t is
updated as follows:
ht += i? (mchc + (1?mc)sc) (2)
The constant i is the impact rate, as in the RI for-
mula (1) above. The valuemc determines how much
the history of a word will influence the history of an-
other word. The intuition here is that frequent words
tend to co-occur with a lot of other words by chance.
Thus, the more frequently a word is seen, the less
informative its history will be, since it will reflect
uninteresting co-occurrences with all sorts of words.
ISA implements this by reducing the influence that
the history of a context word c has on the target word
t as a function of the token frequency of c (notice
that the model still keeps track of the encounter with
c, by adding its signature to the history of t; it is just
the history of c that is weighted down). More pre-
cisely, the m weight associated with a context word
c decreases as follows:
mc =
1
exp
(
Count(c)
km
)
where km is a parameter determining how fast the
decay will be.
3 Experimental setting
3.1 The Lara corpus
The input for our experiments is provided by the
Child-Directed-Speech (CDS) section of the Lara
corpus (Rowland et al, 2005), a longitudinal cor-
pus of natural conversation transcripts of a single
child, Lara, between the ages of 1;9 and 3;3. Lara
was the firstborn monolingual English daughter of
two White university graduates and was born and
brought up in Nottinghamshire, England. The cor-
pus consists of transcripts from 122 separate record-
ing sessions in which the child interacted with adult
caretakers in spontaneous conversations. The total
recording time of the corpus is of about 120 hours,
representing one of the densest longitudinal corpora
available. The adult CDS section we used contains
about 400K tokens and about 6K types.
We are aware that the use of a single-child corpus
may have a negative impact on the generalizations
on semantic development that we can draw from the
experiments. On the other hand, this choice has the
important advantage of providing a fairly homoge-
neous data environment for our computational sim-
ulations. In fact, we can abstract from the intrin-
sic variability characterizing any multi-child corpus,
51
and stemming from differences in the conversation
settings, in the adults? grammar and lexicon, etc.
Moreover, whereas we can take our experiments to
constitute a (very rough) simulation of how a par-
ticular child acquires semantic representations from
her specific linguistic input, it is not clear what simu-
lations based on an ?averages? of different linguistic
experiences would represent.
The corpus was part-of-speech-tagged and lem-
matized using the CLAN toolkit (MacWhinney,
2000). The automated output was subsequently
checked and disambiguated manually, resulting in
very accurate annotation. In our experiments, we
use lemma-POS pairs as input to the word space
models (e.g., go-v rather than going, goes, etc.)
Thus, we make the unrealistic assumptions that the
learner already solved the problem of syntactic cate-
gorization and figured out the inflectional morphol-
ogy of her language. While a multi-level bootstrap-
ping process in which the morphosyntactic and lex-
ical properties of words are learned in parallel is
probably cognitively more likely, it seems reason-
able at the current stage of experimentation to fix
morphosyntax and focus on semantic learning.
3.2 Model training
We experimented with three word space models:
ISA, RI (our implementations in both cases) and the
SVD-based technique implemented by the Infomap
package.1
Parameter settings may considerably impact the
performance of word space models (Sahlgren,
2006). In a stage of preliminary investigations (not
reported here, and involving also other corpora) we
identified a relatively small range of values for each
parameter of each model that produced promising
results, and we focused on it in the subsequent, more
systematic exploration of the parameter space.
For all models, we used a context window of five
words to the left and five words to the right of the
target. For both RI and ISA, we set signature initial-
ization parameters (determining the random assign-
ment of 0s, +1s and -1s to signature vectors) similar
to those described by Karlgren and Sahlgren (2001).
For RI and SVD, we used two stop word filtering
lists (removing all function words, and removing the
1http://infomap-nlp.sourceforge.net/
top 30 most frequent words), as well as simulations
with no stop word filtering. For RI and ISA, we used
signature and history vectors of 1,800 and 2,400 di-
mensions (the first value, again, inspired by Karl-
gren and Sahlgren?s work). Preliminary experiments
with 300 and 900 dimensions produced poor results,
especially with RI. For SVD, we used 300 dimen-
sions only. This was in part due to technical lim-
itations of the implementation, but 300 dimensions
is also a fairly typical choice for SVD-based mod-
els such as LSA, and a value reported to produce
excellent results in the literature. More importantly,
in unrelated experiments SVD with 300 dimensions
and function word filtering achieved state-of-the-art
performance (accuracy above 90%) in the by now
standard TOEFL synonym detection task (Landauer
and Dumais, 1997).
After preliminary experiments showed that both
models (especially ISA) benefited from a very low
impact rate, the impact parameter i of RI and ISA
was set to 0.003 and 0.009. Finally, km (the ISA pa-
rameter determining the steepness of decay of the
influence of history as the token frequency of the
context word increases) was set to 20 and 100 (recall
that a higher km correspond to a less steep decay).
The parameter settings we explored were system-
atically crossed in a series of experiments. More-
over, for RI and ISA, given that different random
initializations will lead to (slightly) different results,
each experiment was repeated 10 times.
Below, we will report results for the best perform-
ing models of each type: ISA with 1,800 dimen-
sions, i set to 0.003 and km set to 100; RI with 2,400
dimensions, i set to 0.003 and no stop words; SVD
with 300-dimensional vectors and function words
removed. However, it must be stressed that 6 out
of the 8 ISA models we experimented with outper-
formed the best RI model (and they all outperformed
the best SVD model) in the Noun AP task discussed
in section 4.1. This suggests that the results we re-
port are not overly dependent on specific parameter
choices.
3.3 Evaluation method
The test set was composed of 100 nouns and 70
verbs (henceforth, Ns and Vs), selected from the
most frequent words in Lara?s CDS section (word
frequency ranges from 684 to 33 for Ns, and from
52
3501 to 89 for Vs). This asymmetry in the test
set mirrors the different number of V and N types
that occur in the input (2828 Ns vs. 944 Vs). As
a further constraint, we verified that all the words
in the test set alo appeared among the child?s pro-
ductions in the corpus. The test words were un-
ambiguously assigned to semantic categories pre-
viously used to model early lexical development
and represent plausible early semantic groupings.
Semantic categories for nouns and verbs were de-
rived by combining two methods. For nouns, we
used the ontologies from the Macarthur-Bates Com-
municative Development Inventories (CDI).2 All
the Ns in the test set alo appear in the Tod-
dler?s List in CDI. The noun semantic categories are
the following (in parenthesis, we report the num-
ber of words per class and an example): ANI-
MALS REAL OR TOY (19; dog), BODY PARTS (16;
nose), CLOTHING (5; hat), FOOD AND DRINK (13;
pizza), FURNITURE AND ROOMS (8; table), OUT-
SIDE THINGS AND PLACES TO GO (10; house),
PEOPLE (10; baby), SMALL HOUSEHOLD ITEMS
(13; bottle), TOYS (6; pen). Since categories for
verbs were underspecified in the CDI, we used
12 broad verb semantic categories for event types,
partly extending those in Borovsky and Elman
(2006): ACTION (11; play), ACTION BODY (6;
eat), ACTION FORCE (5; pull), ASPECTUAL (6;
start), CHANGE (12; open), COMMUNICATION (4;
talk), MOTION (5; run), PERCEPTION (6; hear),
PSYCH (7; remember), SPACE (3; stand), TRANS-
FER (6; buy).
It is worth emphasizing that this experimental set-
ting is much more challenging than those that are
usually adopted by state-of-the-art computational
simulations of word learning, as the ones reported
above. For instance, the number of words in our
test set is larger than the one in Borovsky and Elman
(2006), and so is the number of semantic categories,
both for Ns and for Vs. Conversely, the Lara corpus
is much smaller than the data-sets normally used to
train word space models. For instance, the best re-
sults reported by Li et al (2000) are obtained with
an input corpus which is 10 times bigger than ours.
As an evaluation measure of the model perfor-
mance in the word learning task, we adopted Aver-
2http://www.sci.sdsu.edu/cdi/
age Precision (AP), recently used by Borovsky and
Elman (2006). AP evaluates how close all members
of a certain category are to each other in the seman-
tic space built by the model.
To calculate AP, for each wi in the test set we first
extracted the corresponding distributional vector vi
produced by the model. Vectors were used to cal-
culate the pair-wise cosine between each test word,
as a measure of their distance in the semantic space.
Then, for each target word wi, we built the list ri of
the other test words ranked by their decreasing co-
sine values with respect to wi. The ranking ri was
used to calculate AP (wi), the Word Average Preci-
sion for wi, with the following formula:
AP (wi) =
1
|Cwi |
?
wj?Cwi
nwj (Cwi)
nwj
where Cwi is the semantic category assigned to wi,
nwj is the set of words appearing in ri up to the rank
occupied bywj , and nwj (Cwi) is the subset of words
in nwj that belong to category Cwi .
AP (wi) calculates the proportion of words that
belong to the same category of wi at each rank in
ri, and then divides this proportion by the number
of words that appear in the category. AP ranges
from 0 to 1: AP (wi) = 1 would correspond to the
ideal case in which all the closest words to wi in ri
belonged to the same category as wi; conversely, if
all the words belonging to categories other than Cwi
were closer to wi than the words in Cwi , AP (wi)
would approach 0. We also defined the Class AP
for a certain semantic category by simply averaging
over the Word AP (wi) for each word in that cate-
gory:
AP (Ci) =
?j=|Ci|
j=1 AP (wj)
|Ci|
We adopted AP as a measure of the purity and co-
hesiveness of the semantic representations produced
by the model. Words and categories for which the
model is able to converge on well-formed represen-
tations should therefore have higher AP values. If
we define Recall as the number of words in nwj be-
longing to Cwi divided by the total number of words
in Cwi , then all the AP scores reported in our exper-
iments correspond to 100% Recall, since the neigh-
bourhood we used to compute AP (wi) always in-
cluded all the words in Cwi . This represents a very
53
Nouns
Tokens ISA RI SVD
100k 0.321 0.317 0.243
200k 0.343 0.337 0.284
300k 0.374 0.367 0.292
400k 0.400 0.393 0.306
Verbs
100k 0.242 0.247 0.183
200k 0.260 0.266 0.205
300k 0.261 0.266 0.218
400k 0.270 0.272 0.224
Table 1: Word AP scores for Nouns (top) and Verbs
(bottom). For ISA and RI, scores are averaged
across 10 iterations
stringent evaluation condition for our models, far be-
yond what is commonly used in the evaluation of
classification and clustering algorithms.
4 Experiments and results
4.1 Word learning
Since we intended to monitor the incremental path
of word learning given increasing amounts of lin-
guistic input, AP scores were computed at four
?training checkpoints? established at 100K, 200K,
300K and 400K word tokens (the final point corre-
sponding to the whole corpus).3 Scores were calcu-
lated independently for Ns and Vs. In Table 1, we
report the AP scores obtained by the best perform-
ing models of each type , as described in section 3.2.
The reported AP values refer to Word AP averaged
respectively over the number of Ns and Vs in the test
set. Moreover, for ISA and RI we report mean AP
values across 10 repetitions of the experiment.
For Ns, both ISA and RI outperformed SVD at all
learning stages. Moreover, ISA also performed sig-
nificantly better than RI in the full-size input condi-
tion (400k checkpoint), as well as at the 300k check-
point (Welch t-test; df = 17, p < .05).
One of the most striking results of these experi-
ments was the strongN-V asymmetry in theWord AP
scores, with the Vs performing significantly worse
than the Ns. For Vs, RI appeared to have a small
advantage over ISA, although it was never signifi-
cant at any stage. The asymmetry is suggestive of
the widely attested N-V asymmetry in child word
3The checkpoint results for SVD were obtained by training
different models on increasing samples from the corpus, given
the non-incremental nature of this method.
learning. A consensus has gathered in the early
word learning literature that children from several
languages acquire Ns earlier and more rapidly than
Vs (Gentner, 1982). An influential account explains
this noun-bias as a product of language-external fac-
tors such as the different complexity of the world
referents for Ns and Vs. Recently, Christiansen and
Monaghan (2006) found that distributional informa-
tion in English CDS was more reliable for identi-
fying Ns than Vs. This suggests that the category-
bias may also be partly driven by how good cer-
tain language-internal cues for Ns and Vs are in a
given language. Likewise, distributional cues to se-
mantics may be stronger for English Ns than for
Vs. The noun-bias shown by ISA (and by the other
models) could be taken to complement the results
of Christiansen and Monaghan in showing that En-
glish Ns are more easily discriminable than Vs on
distributionally-grounded semantic terms.
4.2 Category learning
In Table 2, we have reported the Class AP scores
achieved by ISA, RI and SVD (best models) under
the full-corpus training regime for the nine nominal
semantic categories. Although even in this case ISA
and RI generally perform better than SVD (with the
only exceptions of FURNITURE AND ROOMS
and SMALL HOUSEHOLD ITEMS), results
show a more complex and articulated sit-
uation. With BODY PARTS, PEOPLE, and
SMALL HOUSEHOLD ITEMS, ISA significantly
outperforms its best rival RI (Welch t-test; p < .05).
For the other classes, the differences among the two
models are not significant, except for CLOTHING
in which RI performs significantly better than ISA.
For verb semantic classes (whose analytical data are
not reported here for lack of space), no significant
differences exist among the three models.
Some of the lower scores in Table 2 can be ex-
plained either by the small number of class mem-
bers (e.g. TOYS has only 6 items), or by the class
highly heterogeneous composition (e.g. in OUT-
SIDE THINGS AND PLACES TO GO we find nouns
like garden, flower and zoo). The case of PEOPLE,
for which the performance of all the three models
is far below their average Class AP score (ISA =
0.35; RI = 0.35; SVD = 0.27), is instead much more
surprising. In fact, PEOPLE is one of the classes
54
Semantic class ISA RI SVD
ANIMALS REAL OR TOY 0.616 0.619 0.438
BODY PARTS 0.671 0.640 0.406
CLOTHING 0.301 0.349 0.328
FOOD AND DRINK 0.382 0.387 0.336
FURNITURE AND ROOMS 0.213 0.207 0.242
OUTSIDE THINGS PLACES 0.199 0.208 0.198
PEOPLE 0.221 0.213 0.201
SMALL HOUSEHOLD ITEMS 0.208 0.199 0.244
TOYS 0.362 0.368 0.111
Table 2: Class AP scores for Nouns. For ISA and
RI, scores are averaged across 10 iterations
with the highest degree of internal coherence, be-
ing composed only of nouns unambiguously denot-
ing human beings, such as girl, man, grandma, etc.
The token frequency of the members in this class is
also fairly high, ranging between 684 and 55 occur-
rences. Last but not least, in unrelated experiments
we found that a SVD model trained on the British
National Corpus with the same parameters as those
used with Lara was able to achieve very good per-
formances with human denoting nouns, similar to
the members of our PEOPLE class.
These facts have prompted us to better investi-
gate the reasons why with Lara none of the three
models was able to converge on a satisfactory rep-
resentation for the nouns belonging to the PEO-
PLE class. We zoomed in on this semantic class
by carrying out another experiment with ISA. This
model underwent 8 cycles of evaluation, in each of
which the 10 words originally assigned to PEOPLE
have been reclassified into one of the other nom-
inal classes. For each cycle, AP scores were re-
computed for the 10 test words. The results are re-
ported in Figure 1 (where AP refers to the average
Word AP achieved by the 10 words originally be-
longing to the class PEOPLE). The highest score is
reached when the PEOPLE nouns are re-labeled as
ANIMALS REAL OR TOY (we obtained similar re-
sults in a parallel experiment with SVD). This sug-
gests that the low score for the class PEOPLE in the
original experiment was due to ISA mistaking peo-
ple names for animals. What prima facie appeared
as an error could actually turn out to be an interesting
feature of the semantic space acquired by the model.
The experiments show that ISA (as well as the other
models) groups together animals and people Ns, as
Figure 1: AP scores for Ns in PEOPLE reclassified
in the other classes
it has formed a general and more underspecified se-
mantic category that we might refer to as ANIMATE.
This hypothesis is also supported by qualitative ev-
idence. A detailed inspection of the CDS in the
Lara corpus reveals that the animal nouns in the
test set are mostly used by adults to refer either to
toy-animals with which Lara plays or to characters
in stories. In the transcripts, both types of entities
display a very human-like behavior (i.e., they talk,
play, etc.), as it happens to animal characters in most
children?s stories. Therefore, the difference between
model performance and the gold standard ontology
can well be taken as an interesting clue to a genuine
peculiarity in children?s semantic space with respect
to adult-like categorization. Starting from an input
in which animal and human nouns are used in sim-
ilar contexts, ISA builds a semantic space in which
these nouns belong to a common underspecified cat-
egory, much like the world of a child in which cats
and mice behave and feel like human beings.
5 Conclusion
Our main experiments show that ISA significantly
outperforms state-of-the-art word space models in
a learning task carried out under fairly challenging
training and testing conditions. Both the incremen-
tal nature and the particular shape of the semantic
representations built by ISA make it a (relatively)
realistic computational model to simulate the emer-
55
gence of a semantic space in early childhood.
Of course, many issues remain open. First of all,
although the Lara corpus presents many attractive
characteristics, it still contains data pertaining to a
single child, whose linguistic experience may be un-
usual. The evaluation of the model should be ex-
tended to more CDS corpora. It will be especially
interesting to run experiments in languages such as
as Korean (Choi and Gopnik, 1995), where no noun-
bias is attested. There, we would predict that the dis-
tributional information to semantics be less skewed
in favor of nouns. All CDS corpora we are aware of
are rather small, compared to the amount of linguis-
tic input a child hears. Thus, we also plan to test the
model on ?artificially enlarged? corpora, composed
of CDS from more than one child, plus other texts
that might be plausible sources of early linguistic in-
put, such as children?s stories.
In addition, the target of the model?s evaluation
should not be to produce as high a performance as
possible, but rather to produce performance match-
ing that of human learners.4 In this respect, the
output of the model should be compared to what is
known about human semantic knowledge at various
stages, either by looking at experimental results in
the acquisition literature or, more directly, by com-
paring the output of the model to what we can in-
fer about the semantic generalizations made by the
child from her/his linguistic production recorded in
the corpus.
Finally, further studies should explore how the
space constructed by ISA depends on the order in
which sentences are presented to it. This could shed
some light on the issue of how different experien-
tial paths might lead to different semantic general-
izations.
While these and many other experiments must be
run to help clarifying the properties and effective-
ness of ISA, we believe that the data presented here
constitute a very promising beginning for this new
line of research.
References
Borovsky, A. and J. Elman. 2006. Language input and
semantic categories: a relation between cognition and
4We thank an anonymous reviewer for this note
early word learning. Journal of Child Language, 33:
759-790.
Burgess, C. and K. Lund. 1997. Modelling parsing
constraints with high-dimensional context space. Lan-
guage and Cognitive Processes, 12: 1-34.
Choi, S. and A. Gopnik, A. 1995. Early acquisition of
verbs in Korean: a cross-linguistic study. Journal of
Child Language 22: 497-529.
Christiansen, M.H. and P. Monaghan. 2006. Dis-
covering verbs through multiple-cue integration. In
K. Hirsh-Pasek and R.M. Golinkoff (eds.), Action
meets word: How children learn verbs. OUP, Oxford.
Farkas, I. and P. Li. 2001. A self-organizing neural net-
work model of the acquisition of word meaning. Pro-
ceedings of the 4th International Conference on Cog-
nitive Modeling.
Gentner, D. 1982. Why nouns are learned before verbs:
Linguistic relativity versus natural partitioning. In
S.A. Kuczaj (ed.), Language development, vol. 2: Lan-
guage, thought and culture. Erlbaum, Hillsdale, NJ.
Karlgren, J. and M. Sahlgren. 2001. From words to un-
derstanding. In Uesaka, Y., P. Kanerva and H. Asoh
(eds.), Foundations of real-world intelligence, CSLI,
Stanford: 294-308,
Landauer, T.K. and S.T. Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis theory
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104(2): 211-240.
Li, P., C. Burgess and K. Lund. 2000. The acquisition of
word meaning through global lexical co-occurrences.
Proceedings of the 31st Child Language Research Fo-
rum: 167-178.
Li, P., I. Farkas and B. MacWhinney. 2004. Early lexical
acquisition in a self-organizing neural network. Neu-
ral Networks, 17(8-9): 1345-1362.
Manning Ch. and H. Schu?tze. 1999. Foundations of sta-
tistical natural language processing The MIT Press,
Cambridge, MASS.
MacWhinney, B. 2000. The CHILDES project: Tools for
analyzing talk (3d edition). Erlbaum, Mahwah, NJ.
Rowland, C., J. Pine, E. Lieven and A. Theakston.
2005. The incidence of error in young children?s wh-
questions. Journal of Speech, Language and Hearing
Research, 48(2): 384-404.
Sahlgren, M. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, Depart-
ment of Linguistics, Stockholm University.
56
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 1?8,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
One distributional memory, many semantic spaces
Marco Baroni
University of Trento
Trento, Italy
marco.baroni@unitn.it
Alessandro Lenci
University of Pisa
Pisa, Italy
alessandro.lenci@ilc.cnr.it
Abstract
We propose an approach to corpus-based
semantics, inspired by cognitive science,
in which different semantic tasks are tack-
led using the same underlying reposi-
tory of distributional information, col-
lected once and for all from the source
corpus. Task-specific semantic spaces are
then built on demand from the repository.
A straightforward implementation of our
proposal achieves state-of-the-art perfor-
mance on a number of unrelated tasks.
1 Introduction
Corpus-derived distributional semantic spaces
have proved valuable in tackling a variety of tasks,
ranging from concept categorization to relation ex-
traction to many others (Sahlgren, 2006; Turney,
2006; Pado? and Lapata, 2007). The typical ap-
proach in the field has been a ?local? one, in which
each semantic task (or set of closely related tasks)
is treated as a separate problem, that requires its
own corpus-derived model and algorithms. Its
successes notwithstanding, the ?one task ? one
model? approach has also some drawbacks.
From a cognitive angle, corpus-based models
hold promise as simulations of how humans ac-
quire and use conceptual and linguistic informa-
tion from their environment (Landauer and Du-
mais, 1997). However, the common view in cog-
nitive (neuro)science is that humans resort to a
multipurpose semantic memory, i.e., a database
of interconnected concepts and properties (Rogers
and McClelland, 2004), adapting the information
stored there to the task at hand. From an engineer-
ing perspective, going back to the corpus to train a
different model for each application is inefficient
and it runs the risk of overfitting the model to a
specific task, while losing sight of its adaptivity ? a
highly desirable feature for any intelligent system.
Think, by contrast, of WordNet, a single network
of semantic information that has been adapted to
all sorts of tasks, many of them certainly not en-
visaged by the resource creators.
In this paper, we explore a different approach
to corpus-based semantics. Our model consists
of a distributional semantic memory ? a graph of
weighted links between concepts - built once and
for all from our source corpus. Starting from the
tuples that can be extracted from this graph, we
derive multiple semantic spaces to solve a wide
range of tasks that exemplify various strands of
corpus-based semantic research: measuring se-
mantic similarity between concepts, concept cate-
gorization, selectional preferences, analogy of re-
lations between concept pairs, finding pairs that
instantiate a target relation and spotting an alterna-
tion in verb argument structure. Given a graph like
the one in Figure 1 below, adaptation to all these
tasks (and many others) can be reduced to two ba-
sic operations: 1) building semantic spaces, as co-
occurrence matrices defined by choosing different
units of the graph as row and column elements;
2) measuring similarity in the resulting matrix ei-
ther between specific rows or between a row and
an average of rows whose elements share a certain
property.
After reviewing some of the most closely re-
lated work (Section 2), we introduce our approach
(Section 3) and, in Section 4, we proceed to test
it in various tasks, showing that its performance is
always comparable to that of task-specific meth-
ods. Section 5 draws the current conclusions and
discusses future directions.
2 Related work
Turney (2008) recently advocated the need for a
uniform approach to corpus-based semantic tasks.
Turney recasts a number of semantic challenges in
terms of relational or analogical similarity. Thus,
if an algorithm is able to tackle the latter, it can
1
also be used to address the former. Turney tests his
system in a variety of tasks, obtaining good results
across the board. His approach amounts to pick-
ing a task (analogy recognition) and reinterpreting
other tasks as its particular instances. Conversely,
we assume that each task may keep its speci-
ficity, and unification is achieved by designing a
sufficiently general distributional structure, from
which semantic spaces can be generated on de-
mand. Currently, the only task we share with Tur-
ney is finding SAT analogies, where his method
outperforms ours by a large margin (cf. Section
4.2.1). However, Turney uses a corpus that is
25 times larger than ours, and introduces nega-
tive training examples, whereas we dependency-
parse our corpus ? thus, performance is not di-
rectly comparable. Besides the fact that our ap-
proach does not require labeled training data like
Turney?s one, it provides, we believe, a more intu-
itive measure of taxonomic similarity (taxonomic
neighbours are concepts that share similar con-
texts, rather than concepts that co-occur with pat-
terns indicating a taxonomic relation), and it is
better suited to model productive semantic phe-
nomena, such as the selectional preferences of
verbs with respect to unseen arguments (eating
topinambur vs. eating ideas). Such tasks will re-
quire an extension of the current framework of
Turney (2008) beyond evidence from the direct co-
occurrence of target word pairs.
While our unified framework is, as far as we
know, novel, the specific ways in which we tackle
the different tasks are standard. Concept similar-
ity is often measured by vectors of co-occurrence
with context words that are typed with dependency
information (Lin, 1998; Curran and Moens, 2002).
Our approach to selectional preference is nearly
identical to the one of Pado? et al (2007). We
solve SAT analogies with a simplified version of
the method of Turney (2006). Detecting whether
a pair expresses a target relation by looking at
shared connector patterns with model pairs is a
common strategy in relation extraction (Pantel and
Pennacchiotti, 2008). Finally, our method to de-
tect verb slot similarity is analogous to the ?slot
overlap? of Joanis et al (2008) and others. Since
we aim at a unified approach, the lack of origi-
nality of our task-specific methods should be re-
garded as a positive fact: our general framework
can naturally reproduce, locally, well-tried ad-hoc
solutions.
3 Distributional semantic memory
Many different, apparently unrelated, semantic
tasks resort to the same underlying information,
a ?distributional semantic memory? consisting of
weighted concept+link+concept tuples extracted
from the corpus. The concepts in the tuples are
typically content words. The link contains corpus-
derived information about how the two words are
connected in context: it could be for example a
dependency path or a shallow lexico-syntactic pat-
tern. Finally, the weight typically derives from co-
occurrence counts for the elements in a tuple, re-
scaled via entropy, mutual information or similar
measures. The way in which the tuples are iden-
tified and weighted when populating the memory
is, of course, of fundamental importance to the
quality of the resulting models. However, once
the memory has been populated, it can be used to
tackle many different tasks, without ever having to
go back to the source corpus.
Our approach can be compared with the typical
organization of databases, in which multiple alter-
native ?views? can be obtained from the same un-
derlying data structure, to answer different infor-
mation needs. The data structure is virtually inde-
pendent from the way in which it is accessed. Sim-
ilarly, the structure of our repository only obeys
to the distributional constraints extracted from the
corpus, and it is independent from the ways it will
be ?queried? to address a specific semantic task.
Different tasks can simply be defined by how we
split the tuples from the repository into row and
column elements of a matrix whose cells are filled
by the corresponding weights. Each of these de-
rived matrices represents a particular view of dis-
tributional memory: we will discuss some of these
views, and the tasks they are appropriate for, in
Section 4.
Concretely, we used here the web-derived, 2-
billion word ukWaC corpus,1 dependency-parsed
with MINIPAR.2 Focusing for now on modeling
noun-to-noun and noun-to-verb connections, we
selected the 20,000 most frequent nouns and 5,000
most frequent verbs as target concepts (minus stop
lists of very frequent items). We selected as tar-
get links the top 30 most frequent direct verb-
noun dependency paths (e.g., kill+obj+victim),
the top 30 preposition-mediated noun-to-noun or
1http://wacky.sslmit.unibo.it
2http://www.cs.ualberta.ca/?lindek/
minipar.htm
2
die
victim
subj_in
1335.2
teacher
subj_tr
109.4
soldier
subj_in
4547.5
policeman
subj_in
68.6
school
in
2.5
kill
       subj_tr
22.4
obj
915.4     obj
9.9
subj_tr
1306.9
obj
8948.3
subj_tr
38.2
obj
538.1
at
7020.1
with
28.9
in
11894.4
handbook
with
3.2
use
10.1
gun
with
105.9
use
41.0
in
2.8
at
10.3
in
2.5
with
30.5
use
7.4
Figure 1: A fragment of distributional memory
verb-to-noun paths (e.g., soldier+with+gun) and
the top 50 transitive-verb-mediated noun-to-noun
paths (e.g., soldier+use+gun). We extracted all
tuples in which a target link connected two target
concepts. We computed the weight (strength of
association) for all the tuples extracted in this way
using the local MI measure (Evert, 2005), that is
theoretically justified, easy to compute for triples
and robust against overestimation of rare events.
Tuples with local MI ? 0 were discarded. For
each preserved tuple c1+ l+c2, we added a same-
weight c1 + l?1 + c2 tuple. In graph-theoretical
terms (treating concepts as nodes and labeling the
weighted edges with links), this means that, for
each edge directed from c1 to c2, there is an edge
from c2 to c1 with the same weight and inverse
label, and that such inverse edges constitute the
full set of links directed from c2 to c1. The re-
sulting database (DM, for Distributional Memory)
contains about 69 million tuples. Figure 1 de-
picts a fragment of DM represented as a graph (as-
sume, for what we just said, that for each edge
from x to y there is a same-weight edge from y
to x with inverse label: e.g., the obj link from
kill to victim stands for the tuples kill+obj+victim
and victim+obj?1+kill, both with weight 915.4;
subj in identifies the subjects of intransitive con-
structions, as in The victim died; subj tr refers to
the subjects of transitive sentences, as in The po-
liceman killed the victim).
We also trained 3 closely comparable models
that use the same source corpus, the same tar-
get concepts (in one case, also the same target
links) and local MI as weighting method, with the
same filtering threshold. The myPlain model im-
plements a classic ?flat? co-occurrence approach
(Sahlgren, 2006) in which we keep track of verb-
to-noun co-occurrence within a window that can
include, maximally, one intervening noun, and
noun-to-noun co-occurrence with no more than
2 intervening nouns. The myHAL model uses
the same co-occurrence window, but, like HAL
(Lund and Burgess, 1996), treats left and right co-
occurrences as distinct features. Finally, myDV
uses the same dependency-based target links of
DM as filters. Like in the DV model of Pado?
and Lapata (2007), only pairs connected by target
links are preserved, but the links themselves are
not part of the model. Since none of these alter-
native models stores information about the links,
they are only appropriate for the concept similar-
ity tasks, where links are not necessary.
4 Semantic views and experiments
We now look at three views of the DM
graph: concept-by-link+concept (CxLC),
concept+concept-by-link (CCxL), and
concept+link-by-concept (CLxC). Each view
will be tested on one or more semantic tasks and
compared with alternative models. There is a
fourth possible view, links-by-concept+concept
(LxCC), that is not explored here, but would lead
to meaningful semantic tasks (finding links that
express similar semantic relations).
4.1 The CxLC semantic space
Much work in computational linguistics and re-
lated fields relies on measuring similarity among
words/concepts in terms of their patterns of co-
occurrence with other words/concepts (Sahlgren,
2006). For this purpose, we arrange the informa-
tion from the graph in a matrix where the concepts
(nodes) of interest are rows, and the nodes they
are connected to by outgoing edges are columns,
typed with the corresponding edge label. We re-
fer to this view as the concept-by-link+concept
3
(CxLC) semantic space. From the graph in Fig-
ure 1, we can for example construct the matrix
in Table 1 (here and below, showing only some
rows and columns of interest). By comparing the
row vectors of such matrix using standard geo-
metrical techniques (e.g., measuring the normal-
ized cosine distance), we can find out about con-
cepts that tend to share similar properties, i.e., are
taxonomically similar (synonyms, antonyms, co-
hyponyms), e.g., soldiers and policemen, that both
kill, are killed and use guns.
subj in?1subj tr?1 obj?1 with use
die kill kill gun gun
teacher 109.4 0.0 9.9 0.0 0.0
victim 1335.2 22.4 915.4 0.0 0.0
soldier 4547.5 1306.9 8948.3 105.9 41.0
policeman 68.6 38.2 538.1 30.5 7.4
Table 1: A fragment of the CxLC space
We use the CxLC space in three taxonomic sim-
ilarity tasks: modeling semantic similarity judg-
ments, noun categorization and verb selectional
restrictions.
4.1.1 Human similarity ratings
We use the dataset of Rubenstein and Goode-
nough (1965), consisting of 65 noun pairs rated
by 51 subjects on a 0-4 similarity scale (e.g. car-
automobile 3.9, cord-smile 0.0). The average rat-
ing for each pair is taken as an estimate of the
perceived similarity between the two words. Fol-
lowing Pado? and Lapata (2007), we use Pearson?s
r to evaluate how the distances (cosines) in the
CxLC space between the nouns in each pair cor-
relate with the ratings. Percentage correlations for
DM, our other models and the best absolute re-
sult obtained by Pado? and Lapata (DV+), as well
as their best cosine-based performance (cosDV+),
are reported in Table 2.
model r model r
myDV 70 DV+ 62
DM 64 myHAL 61
myPlain 63 cosDV+ 47
Table 2: Correlation with similarity ratings
DM is the second-best model, outperformed
only by DV when the latter is trained on compara-
ble data (myDV in Table 2). Notice that, here and
below, we did not try any parameter tuning (e.g.,
using a similarity measure different than cosine,
feature selection, etc.) to improve the performance
of DM.
4.1.2 Noun categorization
We use the concrete noun dataset of the ESSLLI
2008 Distributional Semantics shared task,3 in-
cluding 44 concrete nouns to be clustered into cog-
nitively justified categories of increasing general-
ity: 6-way (birds, ground animals, fruits, greens,
tools and vehicles), 3-way (animals, plants and
artifacts) and 2-way (natural and artificial enti-
ties). Following the task guidelines, we clustered
the target row vectors in the CxLX matrix with
CLUTO,4 using its default settings, and evalu-
ated the resulting clusters in terms of cluster-size-
weighted averages of purity and entropy (see the
CLUTO documentation). An ideal solution would
have 100% purity and 0% entropy. Table 3 pro-
vides percentage results for our models as well as
for the ESSLLI systems that reported all the rel-
evant performance measures, indexed by first au-
thor. Models are ranked by a global score given by
summing the 3 purity values and subtracting the 3
entropies.
model 6-way 3-way 2-way global
P E P E P E
Katrenko 89 13 100 0 80 59 197
Peirsman+ 82 23 84 34 86 55 140
DM 77 24 79 38 59 97 56
myDV 80 28 75 51 61 95 42
myHAL 75 27 68 51 68 89 44
Peirsman? 73 28 71 54 61 96 27
myPlain 70 31 68 60 59 97 9
Shaoul 41 77 52 84 55 93 -106
Table 3: Concrete noun categorization
DM outperforms our models trained on com-
parable resources. Katrenko?s system queries
Google for patterns that cue the category of a con-
cept, and thus its performance should rather be
seen as an upper bound for distributional models.
Peirsman and colleagues report results based on
different parameter settings: DM?s performance
? not tuned to the task ? is worse than their top
model, but better than their worse.
4.1.3 Selectional restrictions
In this task we test the ability of the CxLC space to
predict verbal selectional restrictions. We use the
CxLC matrix to compare a concept to a ?proto-
type? constructed by averaging a set of other con-
cepts, that in this case represent typical fillers of
3http://wordspace.collocations.de/
doku.php/esslli:start
4http://glaros.dtc.umn.edu/gkhome/
cluto/cluto/overview
4
a verbal slot ? for example, by averaging the vec-
tors of the nouns that are, according to the underly-
ing graph, objects of killing, we can build a vector
for the typical ?killee?, and model selectional re-
strictions by measuring the similarity of other con-
cepts (including concepts that have not been seen
as objects of killing in the corpus) to this proto-
type. Note that the DM graph is used both to find
the concepts to enter in the prototype (the set of
nouns that are connected to a verb by the relevant
edge) and to compute similarity. Thus, the method
is fully unsupervised.
We test on the two datasets of human judgments
about the plausibility of nouns as arguments (ei-
ther subjects or objects) of verbs used in Pado? et
al. (2007), one (McRae) consisting of 100 noun-
verb pairs rated by 36 subjects, the second (Pado?)
with 211 pairs rated by 20 subjects. For each verb
in these datasets, we built its prototypical sub-
ject/object argument vector by summing the nor-
malized vectors of the 50 nouns with the highest
weight on the appropriate dependency link to the
verb (e.g., the top 50 nouns connected to kill by an
obj link). The cosine distance of a noun to a proto-
type is taken as the model ?plausibility judgment?
about the noun occurring as the relevant verb ar-
gument. Since we are interested in generalization,
if the target noun is in the prototype set we sub-
tract its vector from the prototype before calculat-
ing the cosine. For our comparison models, there
is no way to determine which nouns would form
the prototype, and thus we train them using the
same top noun lists we employ for DM. Following
Pado? and colleagues, performance is measured by
the Spearman ? correlation coefficient between the
average human ratings and the model predictions.
Table 4 reports percentage coverage and correla-
tions for our models as well as those in Pado? et
al. (2007) (ParCos is the best among their purely
corpus-based systems).
model McRae Pado?
coverage ? coverage ?
Pado? 56 41 97 51
DM 96 28 98 50
ParCos 91 21 98 48
myDV 96 21 98 39
myHAL 96 12 98 29
myPlain 96 12 98 27
Resnik 94 3 98 24
Table 4: Correlation with verb-argument plausibil-
ity judgments
DM does very well on this task: its performance
on the Pado? dataset is comparable to that of the
Pado? system, that relies on FrameNet. DM has
nearly identical performance to the latter on the
Pado? dataset. On the McRae data, DM has a lower
correlation, but much higher coverage. Since we
are using a larger corpus than Pado? et al (2007),
who train on the BNC, a fairer comparison might
be the one with our alternative models, that are all
outperformed by DM by a large margin.
4.2 The CCxL semantic space
Another view of the DM graph is exemplified in
Table 5, where concept pairs are represented in
terms of the edge labels (links) connecting them.
Importantly, this matrix contains the same infor-
mation that was used to build the CxLC space
of Table 1, with a different arrangement of what
goes in the rows and in the columns, but the same
weights in the cells ? compare, for example, the
soldier+gun-by-with cell in Table 5 to the soldier-
by-with+gun cell in Table 1.
in at with use
teacher school 11894.47020.1 28.9 0.0
teacher handbook 2.5 0.0 3.2 10.1
soldier gun 2.8 10.3 105.9 41.0
Table 5: A fragment of the CCxL space
We use this space to measure ?relational? sim-
ilarity (Turney, 2006) of concept pairs, e.g., find-
ing that the relation between teachers and hand-
books is more similar to the one between soldiers
and guns, than to the one between teachers and
schools. We also extend relational similarity to
prototypes. Given some example pairs instantiat-
ing a relation, we can harvest new pairs linked by
the same relation by computing the average CCxL
vector of the examples, and finding the nearest
neighbours to this average. In the case at hand,
the link profile of pairs such as soldier+gun and
teacher+handbook could be used to build an ?in-
strument relation? prototype.
We test the CCxL semantic space on recogniz-
ing SAT analogies (relational similarity between
pairs) and semantic relation classification (rela-
tional similarity to prototypes).
4.2.1 Recognizing SAT analogies
We used the set of 374 multiple-choice ques-
tions from the SAT college entrance exam. Each
question includes one target pair, usually called
5
the stem (ostrich-bird) , and 5 other pairs (lion-
cat, goose-flock, ewe-sheep, cub-bear, primate-
monkey). The task is to choose the pair most anal-
ogous to the stem. Each SAT pair can be rep-
resented by the corresponding row vector in the
CCxL matrix, and we select the pair with the high-
est cosine to the stem. In Table 6 we report our
results, together with the state-of-the-art from the
ACL wiki5 and the scores of Turney (2008) (Pair-
Class) and from Amac? Herdag?delen?s PairSpace
system, that was trained on ukWaC. The Attr cells
summarize the performance of the 6 models on the
wiki table that are based on ?attributional similar-
ity? only (Turney, 2006). For the other systems,
see the references on the wiki. Since our coverage
is very low (44% of the stems), in order to make a
meaningful comparison with the other models, we
calculated a corrected score (DM?). Having full
access to the results of the ukWaC-trained, simi-
larly performing PairSpace system, we calculated
the adjusted score by assuming that the DM-to-
PairSpace error ratio (estimated on the items we
cover) is constant on the whole dataset, and thus
the DM hit count on the unseen items is approx-
imated by multiplying the PairSpace hit count on
the same items by the error ratio (DM+ is DM?s
accuracy on the covered test items only).
model % correct model % correct
LRA 56.1 KnowBest 43.0
PERT 53.3 DM? 42.3
PairClass 52.1 LSA 42.0
VSM 47.1 AttrMax 35.0
DM+ 45.3 AttrAvg 31.0
PairSpace 44.9 AttrMin 27.3
k-means 44.0 Random 20.0
Table 6: Accuracy with SAT analogies
DM does not excel in this task, but its corrected
performance is well above chance and that of all
the attributional models, and comparable to that of
a WordNet-based system (KnowBest) and a sys-
tem that uses manually crafted information about
analogy domains (LSA). All systems with perfor-
mance above DM+ (and k-means) use corpora that
are orders of magnitude larger than ukWaC.
4.2.2 Classifying semantic relations
We also tested the CCxL space on the 7
semantic relations between nominals adopted
in Task 4 of SEMEVAL 2007 (Girju et
5http://www.aclweb.org/aclwiki/index.
php?title=SAT_Analogy_Questions
al., 2007): Cause-Effect, Instrument-Agency,
Product-Producer, Origin-Entity, Theme-Tool,
Part-Whole, Content-Container. For each rela-
tion, the dataset includes 140 training examples
and about 80 test cases. Each example consists
of a small context retrieved from the Web, con-
taining word pairs connected by a certain pattern
(e..g., ?* contains *?). The retrieved contexts were
manually classified by the SEMEVAL organizers
as positive (e.g., wrist-arm) or negative (e.g., ef-
fectiveness-magnesium) instances of a certain re-
lation (e.g., Part-Whole). About 50% training and
test cases are positive instances. For each rela-
tion, we built ?hit? and ?miss? prototype vectors,
by averaging across the vectors of the positive and
negative training pairs attested in our CCxL model
(we use only the word pairs, not the surround-
ing contexts). A test pair is classified as a hit
for a certain relation if it is closer to the hit pro-
totype vector for that relation than to the corre-
sponding miss prototype. We used the SEMEVAL
2007 evaluation method, i.e., precision, recall, F-
measure and accuracy, macroaveraged over all re-
lations, as reported in Table 7. The DM+ scores
ignore the 32% pairs not in our CCxL space; the
DM? scores assume random performance on such
pairs. These scores give the range within which
our performance will lie once we introduce tech-
niques to deal with unseen pairs. We also report
results of the SEMEVAL systems that did not use
the organizer-provided WordNet sense labels nor
information about the query used to retrieve the
examples, as well as performance of several trivial
classifiers, also from the SEMEVAL task descrip-
tion.
model precision recall F accuracy
UCD-FC 66.1 66.7 64.8 66.0
UCB 62.7 63.0 62.7 65.4
ILK 60.5 69.5 63.8 63.5
DM+ 60.3 62.6 61.1 63.3
UMELB-B 61.5 55.7 57.8 62.7
SemeEval avg 59.2 58.7 58.0 61.1
DM? 56.7 58.2 57.1 59.0
UTH 56.1 57.1 55.9 58.8
majority 81.3 42.9 30.8 57.0
probmatch 48.5 48.5 48.5 51.7
UC3M 48.2 40.3 43.1 49.9
alltrue 48.5 100.0 64.8 48.5
Table 7: SEMEVAL relation classification
The DM accuracy is higher than the three SE-
MEVAL baselines (majority, probmatch and all-
true), DM+ is above the average performance of
6
the comparable SEMEVAL models. Differently
from DM, the models that outperform it use fea-
tures extracted from the training contexts and/or
specific additional resources: an annotated com-
pound database for UCD-FC, machine learning
algorithms to train the relation classifiers (ILK,
UCD-FC), Web counts (UCB), etc. The less than
optimal performance by DM is thus counterbal-
anced by its higher ?parsimony? and generality.
4.3 The CLxC semantic space
A third view of the information in the DM graph
is the concept+link-by-concept (CLxC) semantic
space exemplified by the matrix in Table 8.
teacher victim soldier policeman
kill subj tr 0.0 22.4 1306.9 38.2
kill obj 9.9 915.4 8948.3 538.1
die subj in 109.4 1335.2 4547.5 68.6
Table 8: A fragment of the CLxC space
This view captures patterns of similarity be-
tween (surface approximations to) argument slots
of predicative words. We can thus use the CLxC
space to extract generalizations about the inner
structure of lexico-semantic representations of the
sort formal semanticists have traditionally being
interested in. In the example, the patterns of
co-occurrence suggest that objects of killing are
rather similar to subjects of dying, hinting at the
classic cause(subj,die(obj)) analysis of killing by
Dowty (1977) and many others. Again, no new in-
formation has been introduced ? the matrix in Ta-
ble 8 is yet another re-organization of the data in
our graph (compare, for example, the die+subj in-
by-teacher cell of this matrix with the teacher-by-
subj in+die cell in Table 1).
4.3.1 The causative/inchoative alternation
Syntactic alterations (Levin, 1993) represent
a key aspect of the complex constraints that
shape the syntax-semantics interface. One of
the most important cases of alternation is the
causative/inchoative, in which the object argu-
ment (e.g., John broke the vase) can also be re-
alized as an intransitive subject (e.g., The vase
broke). Verbs differ with respect to the possi-
ble syntactic alternations they can participate in,
and this variation is strongly dependent on their
semantic properties (e.g. semantic roles, event
type, etc.). For instance, while break can undergo
the causative/inchoative alternation, mince cannot:
cf. John minced the meat and *The meat minced.
We test our CLxC semantic space on the
discrimination between transitive verbs un-
dergoing the causative-inchoative alterna-
tions and non-alternating ones. We took
232 causative/inchoative verbs and 170 non-
alternating transitive verbs from Levin (1993).
For each verb vi, we extracted from the CLxC
matrix the row vectors corresponding to its tran-
sitive subject (vi + subj tr), intransitive subject
(vi + subj in), and direct object (vi + obj) slots.
Given the definition of the causative/inchoative
alternation, we predict that with alternating verbs
vi + subj in should be similar to vi + obj
(the things that are broken also break), while
this should not hold for non-alternating verbs
(mincees are very different from mincers).
Our model is completely successful in detect-
ing the distinction. The cosine similarity between
transitive subject and object slots is fairly low for
both classes, as one would expect (medians of 0.16
for alternating verbs and 0.11 for non-alternating
verbs). On the other hand, while for the non-
alternating verbs the median cosine similarity be-
tween the intransitive subject and object slots is
a similarly low 0.09, for the alternating verbs the
median similarity between these slots jump up
to 0.31. Paired t-tests confirm that the per-verb
difference between transitive subject vs. object
cosines and intransitive subject vs. object cosines
is highly statistically significant for the alternating
verbs, but not for the non-alternating ones.
5 Conclusion
We proposed an approach to semantic tasks where
statistics are collected only once from the source
corpus and stored as a set of weighted con-
cept+link+concept tuples (naturally represented
as a graph). Different semantic spaces are con-
structed on demand from this underlying ?distri-
butional memory?, to tackle different tasks with-
out going back to the corpus. We have shown that
a straightforward implementation of this approach
leads to excellent performance in various taxo-
nomic similarity tasks, and to performance that,
while not outstanding, is at least reasonable on re-
lational similarity. We also obtained good results
in a task (detecting the causative/inchoative alter-
nation) that goes beyond classic NLP applications
and more in the direction of theoretical semantics.
The most pressing issue we plan to address is
how to improve performance in the relational sim-
7
ilarity tasks. Fortunately, some shortcomings of
our current model are obvious and easy to fix.
The low coverage is in part due to the fact that
our set of target concepts does not contain, by de-
sign, some words present in the task sets. More-
over, while our framework does not allow ad-hoc
optimization of corpus-collection methods for dif-
ferent tasks, the way in which the information in
the memory graph is adapted to tasks should of
course go beyond the nearly baseline approaches
we adopted here. In particular, we need to de-
velop a backoff strategy for unseen pairs in the
relational similarity tasks, that, following Turney
(2006), could be based on constructing surrogate
pairs of taxonomically similar words found in the
CxLC space.
Other tasks should also be explored. Here, we
viewed our distributional memory in line with how
cognitive scientists look at the semantic memory
of healthy adults, i.e., as an essentially stable long
term knowledge repository. However, much in-
teresting semantic action takes place when under-
lying knowledge is adapted to context. We plan
to explore how contextual effects can be modeled
in our framework, focusing in particular on how
composition affects word meaning (Erk and Pado?,
2008). Similarity could be measured directly on
the underlying graph, by relying on graph-based
similarity algorithms ? an elegant approach that
would lead us to an even more unitary view of
what distributional semantic memory is and what
it does. Alternatively, DM could be represented as
a three-mode tensor in the framework of Turney
(2007), enabling smoothing operations analogous
to singular value decomposition.
Acknowledgments
We thank Ken McRae and Peter Turney for pro-
viding data-sets, Amac? Herdag?delen for access to
his results, Katrin Erk for making us look at DM as
a graph, and the reviewers for helpful comments.
References
J. Curran and M. Moens. 2002. Improvements in auto-
matic thesaurus extraction. Proceedings of the ACL
Workshop on Unsupervised Lexical Acquisition, 59?
66.
D. Dowty. 1977. Word meaning and Montague Gram-
mar. Kluwer, Dordrecht.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. Proceedings of
EMNLP 2008.
S. Evert. 2005. The statistics of word cooccurrences.
Ph.D. dissertation, Stuttgart University, Stuttgart.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney and Y. Deniz. 2007. SemEval-2007 task 04:
Classification of semantic relations between nomi-
nals. Proceedings of SemEval-2007, 13?18.
E. Joanis, S. Stevenson and D. James. 2008. A gen-
eral feature space for automatic verb classification.
Natural Language Engineering, 14(3): 337?367.
T.K. Landauer and S.T. Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2): 211?
240.
B. Levin. 1993. English Verb Classes and Alterna-
tions. A Preliminary Investigation. Chicago, Uni-
versity of Chicago Press.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. Proceedings of ACL 1998, 768?774.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behaviour Research Methods, 28: 203?
208.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2): 161?199.
S. Pado?, S. Pado? and K. Erk. 2007. Flexible, corpus-
based modelling of human plausibility judgements.
Proceedings EMNLP 2007, 400?409.
P. Pantel and M. Pennacchiotti. 2008. Automatically
harvesting and ontologizing semantic relations. In
P. Buitelaar and Ph. Cimiano (eds.), Ontology learn-
ing and population. IOS Press, Amsterdam.
T. Rogers and J. McClelland. 2004. Semantic cog-
nition: A parallel distributed processing approach.
The MIT Press, Cambridge.
H. Rubenstein and J.B. Goodenough. 1965. ?Contex-
tual correlates of synonymy?. Communications of
the ACM, 8(10):627-633.
M. Sahlgren. 2006. The Word-space model. Ph.D. dis-
sertation, Stockholm University, Stockholm.
P. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3): 379?416.
P. Turney. 2007. Empirical evaluation of four ten-
sor decomposition algorithms. IIT Technical Report
ERB-1152, National Research Council of Canada,
Ottawa.
P. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms and associations. Proceedings
of COLING 2008, 905?912.
8
International Standards for Multilingual Resource Sharing: The
ISLE Computational Lexicon Working Group
Nicoletta Calzolari, Alessandro Lenci, Antonio Zampolli
Istituto di Linguistica Computazionale, CNR, Pisa
Consorzio Pisa Ricerche
Universit? di Pisa, Dipartimento di Linguistica
[glottolo,lenci,eagles]@ilc.pi.cnr.it
Abstract
The ISLE project is a continuation
of the long standing EAGLES
initiative, carried out under the Human
Language Technology (HLT)
programme in collaboration between
American and European groups in the
framework of the EU-US International
Research Co-operation, supported by
NSF and EC. We concentrate in this
paper on the current position of the
ISLE Computational Lexicon Working
Group. We provide a short description
of the EU SIMPLE lexicons built on
the basis of previous EAGLES
recommendations. We then point at a
few basic methodological principles
applied in previous EAGLES phases,
and describe a few principles to be
followed in the definition of a
Multilingual ISLE Lexical Entry
(MILE).
1 Introduction: the EAGLES
initiative
1.1. What is EAGLES/ISLE?
The ISLE project is a continuation of the
long standing EAGLES initiative (Calzolari et
al., 1996), carried out through a number of
subsequent projects funded by the European
Commission (EC) since 1993. EAGLES stands
for Expert Advisory Group for Language
Engineering Standards and was launched within
EC Directorate General XIII's Linguistic
Research and Engineering (LRE) programme,
continued under the Language Engineering (LE)
programme, and now under the Human
Language Technology (HLT) programme as
ISLE, since January 2000. ISLE stands for
International Standards for Language
Engineering, and is carried out in collaboration
between American and European groups in the
framework of the EU-US International Research
Co-operation, supported by NSF and EC. ISLE
was built on joint preparatory EU-US work of
the previous 2 years towards setting up a
transatlantic standards oriented initiative for
HLT.
The objective of the project is to support
HLT R&D international and national projects,
and HLT industry by developing, disseminating
and promoting widely agreed and urgently
demanded HLT standards and guidelines for
infrastructural language resources (see Zampolli,
1998, and Calzolari, 1998), tools that exploit
them and LE products. The aim of
EAGLES/ISLE is thus to accelerate the
provision of standards, common guidelines, best
practice recommendations for:
? very large-scale language resources (such as
text corpora, computational lexicons, speech
corpora (Gibbon et al, 1997), multimodal
resources);
? means of manipulating such knowledge, via
computational linguistic formalisms, mark-
up languages and various software tools;
? means of assessing and evaluating
resources, tools and products (EAGLES,
1996).
The basic idea behind EAGLES work is for
the group to act as a catalyst in order to pool
concrete results coming from current major
International/ National/industrial projects.
Relevant common practices or upcoming
standards are being used where appropriate as
input to EAGLES/ISLE work, particularly in the
areas of computational lexicons, text, speech,
and multimodal annotation, and evaluation.
Numerous theories, approaches, and systems are
being taken into account, where appropriate, as
any recommendation for harmonisation must
take into account the needs and nature of the
different major contemporary approaches.
EAGLES is also drawing strong inspiration
from the results of major projects whose results
have contributed to advancing our understanding
of harmonisation issues.
1.2 A quick Overview of the ISLE
Work
The current ISLE project (see
http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE_H
ome_Page.htm) targets the three areas of
multilingual computational lexicons, natural
interaction and multimodality (NIMM), and
evaluation of HLT systems. These areas were
chosen not only for their relevance to the HLT
call but also for their long-term significance.
? For multilingual computational lexicons,
ISLE is working to: extend EAGLES work
on lexical semantics, necessary to establish
inter-language links; design and propose
standards for multilingual lexicons; develop
a prototype tool to implement lexicon
guidelines and standards; create exemplary
EAGLES-conformant sample lexicons and
tag exemplary corpora for validation
purposes; and develop standardised
evaluation procedures for lexicons.
? For NIMM, a rapidly innovating domain
urgently requiring early standardisation,
ISLE work is targeted to develop guidelines
for: the creation of NIMM data resources;
interpretative annotation of NIMM data,
including spoken dialogue in NIMM
contexts; and annotation of discourse
phenomena.
? For evaluation, ISLE is working on: quality
models for machine translation systems; and
maintenance of previous guidelines - in an
ISO based framework (ISO 9126, ISO
14598).
Three Working Groups, and their sub-
groups, carry out the work, according to the
already proven EAGLES methodology, with
experts from both the EU and US, working and
interacting within a strongly co-ordinated
framework. International workshops are used as
a means of achieving consensus and advancing
work. Results will be widely disseminated and
published, after due validation in collaboration
with EU and US HLT R&D projects, National
projects, and industry.
1.3. The Computational Lexicon
Working Group
We concentrate in the following on the
current position of the ISLE Computational
Lexicon Working Group (CLWG).
EAGLES work towards de facto standards
has already allowed the field of Language
Resources to establish broad consensus on key
issues for some well-established areas ? and
will allow similar consensus to be achieved for
other important areas through the ISLE project
? providing thus a key opportunity for further
consolidation and a basis for technological
advance. EAGLES previous results have already
become de facto standards. To mention several
key examples: the LE PAROLE/SIMPLE
resources (morphological/syntactic/semantic
lexicons and corpora for 12 EU languages,
Ruimy et al, 1998, Lenci et al, 1999, Bel et al,
2000) rely on EAGLES results (Sanfilippo, A. et
al., 1996 and 1999), and are now being enlarged
at the national level through many National
Projects; the ELRA Validation Manuals for
Lexicons (Underwood and Navarretta, 1997)
and Corpora (Burnard et al, 1997) are based on
EAGLES guidelines; morpho-syntactic tagging
of corpora in a very large number of EU,
international and national projects ? and for
more than 20 languages ? is conformant to
EAGLES recommendations (Leech and Wilson,
1996).
The first priority of the CLWG in the first
phase of the ISLE project was to do a
comprehensive survey of existing multilingual
lexicons. To this end the European and the
American members decided, among others, i) to
prepare a grid for lexicon description to classify
the content and structure of the surveyed
resources on the basis of a number of agreed
parameters of description, and ii) to provide a
list of cross-lingual lexical phenomena that
could be used to focus the survey. The inventory
(survey) of what exists and is available
(semantic and bilingual/multilingual lexicons,
printed bilingual dictionaries) is now being
completed, and will be made soon available on
the Web. Each participant engaged for surveying
a number of resources. A list of the main
applications that use lexical resources was also
established, to focus the survey and subsequent
recommendations around them. Each summary
of a particular bilingual or multilingual
dictionary includes: i) a description of the
surveyed dictionary structure (on the basis of the
common grid), ii) for one or two examples from
the cross-lingual lexical phenomena, an
explanation of how these examples are handled
by this dictionary.
2 The structure of the prospective
Multilingual ISLE Lexical Entry
The main goal of the CLWG is the definition
of a Multilingual ISLE Lexical Entry
(henceforth MILE). This is the main focus of the
second year of the project, the so called
?recommendation phase?.
2.1 Basic EAGLES principles
We remind here just a few basic
methodological principles derived from and
applied in previous EAGLES phases. They have
proven useful in the process of reaching
consensual de facto standards in a bottom-up
approach and will be at the basis also of ISLE
work.
The MILE is envisaged as a highly modular
and possibly layered structure, with different
levels of recommendations. Such an architecture
has been proven useful in previous EAGLES
work, e.g in the EAGLES morphosyntactic
recommendations (Monachini and Calzolari,
1996), which embody three levels of linguistic
information: obligatory, recommended and
optional (optional splits furthermore into
language independent and language dependent).
This modularity would enhance: the flexibility
of the representation, the easiness of
customisation and integration of existing
resources (developed under different theoretical
frameworks or for different applications), the
usability by different systems which are in need
of different portions of the encoded data, the
compliance with the proposed standards also of
partially instantiated entries.
The MILE recommendations should also be
very granular, in the sense of reaching a
maximal decomposition into the minimal basic
information units that reflect the phenomena we
are dealing with. This principle was previously
recommended and used to allow easier
reusability or mappability into different
theoretical or system approaches (Heid and
McNaught, 1991): small units can be assembled,
in different frameworks, according to different
(theory/application dependent) generalisation
principles. Such basic notions must be
established before considering any system-
specific generalisations, otherwise our work
may be too conditioned by system-specific
approaches. For example, ?synonymy? can be
taken as a basic notion; however, the notion of
?synset? is a generalisation, closely associated
with the WordNet approach. ?Qualia relations?
are another example of a generalisation, whereas
?semantic relation? is a basic notion. Modularity
is also a means to achieve better granularity.
On the other side, past EAGLES experience
has shown it is useful in many cases to accept
underspecification with respect to
recommendations for the representation of some
phenomenon (and hierarchical structure of the
basic notions, attributes, values, etc.), i) to allow
for agreement on a minimal level of specificity
especially in cases where we cannot reach wider
agreement, and/or ii) enable mappability and
comparability of different lexicons, with
different granularity, at the minimal common
level of specificity (or maximal generality). For
example, the work on syntactic
subcategorisation in EAGLES proved that it was
problematic to reach agreement on a few
notions, e.g. it seemed unrealistic to agree on a
set of grammatical functions. This led to an
underspecified recommendation, but
nevertheless one that was useful.
One of the first objectives of the CLWG will
be to discover and list the (maximal) set of
(minimal/more granular) basic notions needed
to describe the multilingual level. This task will
be facilitated by the survey of existing lexicons,
accompanied by the analysis of the requirements
of a few multilingual applications, and by the
parallel analysis of typical multilingual complex
phenomena. Most or part of these basic notions
should be already included in previous EAGLES
recommendations, and, with different
distribution, in the existing and surveyed
lexicons. We have therefore to revisit earlier
linguistic layers (previous EAGLES work,
essentially monolinguistic) to see what we need
to change/add or what we can reuse for the
multilingual layer. The multilingual layer thus
depends on monolingual layers.
2.2 The MILE architecture
The MILE is intended as a meta-entry, acting
as a common representational layer for
multilingual lexical resources. The key-ideas
underlying the design of a meta-entry can be
summarized as follows. Different theoretical
frameworks appear to impose different
requirements on how lexical information should
be represented. One way of tackling the issue of
theoretical compatibility stems from the
observation that existing representational
frameworks mostly differ in the way pieces of
linguistic information are mutually implied,
rather than in the intrinsic nature of this
information. To give a concrete example, almost
all theoretical frameworks claim that lexical
items have a complex semantic organization, but
some of them try to describe it through a
multidimensional internal structure (cf. the
qualia structure in the Generative Lexicon,
Pustejovsky 1995), others by specifying a
network of semantic relations (cf. WordNet,
Miller et al 1990), and others in terms of
argumental frames (cf FrameNet, Baker et al
1998; Lexical Conceptual Structures, Jackendoff
1992; etc.). A way out of this theoretical
variation is to augment the expressive power of
the lexical representation language both
horizontally, i.e. by distributing the linguistic
information over mutually independent "coding
layers", and vertically, by further specifying the
information conveyed by each such layer. This
solution will contribute to solve the issues raised
by theoretical variation by defining a common
level onto which different types of resources
will be mapped without loss of information.
This appears to be a necessary condition to
guarantee an efficient re-use and interchange of
lexical data, often coming from resources
developed according to very different
architectural and theoretical criteria.
With respect to this issue, the MILE is
designed to meet the following desiderata:
? factor out linguistically independent (but
possibly correlated) primitive units of
lexical information;
? make explicit information which is
otherwise only indirectly accessible by NLP
systems;
? rely on lexical analysis which have the
highest degree of inter-theoretical
agreement;
? avoid framework-specific representational
solutions.
All these requirements serve the main purpose
of making the lexical meta-entry open to task-
and system-dependent parameterization.
The MILE is modular along at least three
dimensions:
? modularity in the macrostructure and
general architecture of MILE
? modularity in the microstructure
? modularity in the specific microstructure of
the MILE word sense.
A. Modularity in the macrostructure and
general architecture of the MILE ? The
following modules should be at least envisaged,
referring to the macrostructure of a multilingual
system:
1. Meta-information - versioning of the
lexicon, languages, updates, status, project,
origin, etc. (see e.g. OLIF (Thurmair, 2000),
GENELEX).
2. Possible architecture(s) of bilingual/
multilingual lexicon(s): we must analyse the
interactions of the different modules, and the
general structure in which they are inserted, both
in the interlingua- and transfer-based
approaches, and in possibly hybrid solutions. An
open issue is also the relation between the
source language (SL) and target language (TL)
portions of a lexicon.
B. Modularity in the microstructure of the
MILE ? The following modules should be at
least envisaged, referring to the global
microstructure of MILE:
1. Monolingual linguistic representation -
this includes the morphosyntactic, syntactic, and
semantic information characterizing the MILE
in a certain language. It generally corresponds to
the typology of information contained in
existing lexicons, such as PAROLE-SIMPLE,
(Euro)WordNet (EWN), COMLEX, and
FrameNet. Following the general organizations
of computational lexicons like PAROLE-
SIMPLE, which in turn instantiates the
GENELEX framework (GENELEX, 1994), at
the monolingual level the MILE sorts out the
linguistic information into three layers,
respectively for morphological, syntactic and
semantic dimensions. Typologies of information
to be part of this module include (not an
exhaustive list):
? Phonological layer
? phonemic transcription
? prosodic information
? Morphological layer
? Grammatical category
? Inflectional class
? Modifications of the lemma
? Mass/count, 'pluralia tantum'
? Syntactic layer
? Idiosyncratic behaviour with respect to
specific syntactic rules (passivisation,
middle, etc.)
? Auxiliary
? Attributive vs. predicative function,
gradability
? Subcategorization frames
? Grammatical functions of the positions
? Morphosyntactic and/or lexical features
? Information on control and raising
properties
? Semantic layer
? Characterization of senses through links
to an ontology
? Domain information
? Argument structure, semantic roles,
selectional preferences on the arguments
? Event type
? Link to the syntactic positions
? Basic semantic relations between word
senses (i.e. synonymy, hyponymy,
meronymy)
? Description of word-sense in terms of
more specific, semantic/world-
knowledge relations among word-senses
(such as EWN relations, SIMPLE
Qualia Structure, FrameNet Frame
Elements, etc.)
? Information about regular polisemous
alternation
? Information concerning cross-part of
speech relations (e.g. intelligent -
intelligence; writer - to write)
The expressive power of the semantic layer is of
the utmost importance for the multilingual layer.
A general issue discussed in ISLE concerns
whether consensus has to be pursued at the
generic level of ?type? of information or also at
the level of its ?values? or actual ways of
representation. The answer may be different for
different notions, e.g. try to reach the more
specific level of agreement also on values for
types of meronymy, but not for types of
ontology.
2. Collocational information - This module
includes more or less typical and/or fixed
syntagmatic patterns including the lexical head
defined by the MILE, which can contribute to
characterise its use, or to perform more subtle
and/or domain specific characterisations. It
includes at least:
? Typical collocates
? Support verb construction
? Phraseological or multiwords constructions
? Compounds
? Corpus-driven examples
This module ? not yet dealt with in the
previous EAGLES - is critical in a multilingual
context both to characterise a word-sense in a
more granular way and to make it possible to
perform a number of operations, such as WSD
or translation in a specific context. Here,
synergies with the NSF-XMELLT project on
multi-word expressions are exploited. First
proposals for the representation of support verbs
and noun-noun compounds in multilingual
computational lexicons are laid out, and now
tested on some language pairs.
3. Multilingual apparatus ? This
represents the focal part of the CLWG activities,
which will concentrate its main effort in
proposing a general framework for the
expression of multilingual transfers. Some of the
main issues at stake here are:
? identify a typology of the most common
cases of problematic transfer (actually this
task has been partially performed during the
survey phase of the project);
? identify which conditions must be
expressible and which transformation
actions are necessary, in order to establish
the correct multilingual mappings;
? select which types of information these
conditions must access in the modules (1)
and (2) above;
? identify the various methods of establishing
SL --> TL equivalence
? examine the variability of granularity
needed when translating in different
languages, and the architectural implications
of this.
C. Modularity in the specific
microstructure of the MILE word-sense (word-
sense is the basic unit at the multilingual level) ?
Senses should also have a modular structure (i.e.
the above distinction between modules (B.1.)
and (B.2.) must be intended at word-sense
level):
1. Coarse-grained (general purpose)
characterisation in terms of prototypical
properties, captured by the formal means in
(B.1.) above, which serves to partition the
meaning space in large areas and is sufficient for
some NLP tasks.
2. Fine-grained (domain or text
dependent) characterisation mostly in terms of
collocational/syntagmatic properties (B.2.),
which is especially useful for specific tasks,
such as WSD and translation. Different types of
information may have a sort of different
operational specialisation.
3 Methodological and organisational
issues
As in previous EAGLES, it is considered
helpful to base the recommendations on the
requirements stemming from a few application
systems. The CLWG agreed to focus on two
major broad categories of application: machine
translation (MT) and cross-lingual information
retrieval (CLIR).
As said above, the CLWG has agreed that we
should base any multilingual description on
monolingual descriptions. MILE should
therefore include previous EAGLES
recommendations for other layers. We must
evaluate the usefulness of these layers with
respect to multilingual tasks, focusing in
particular on MT and CLIR tasks. Obviously an
additional module is needed, where
correspondences between languages are defined,
including conditions on syntactic structures
involving lexical entries. The linking module
(transfer) may not be the same for different
applications: it may be simpler for CLIR, which
may be a subset of the one needed for MT. For
CLIR, an ontology or semantic hierarchy is
however required.
We are also adopting an approach that would
lead to a formalisation of the information
contained in traditional bilingual dictionaries,
such as restrictions on translation, collocations
and examples.
The CLWG agreed the following were
appropriate tasks to concentrate on, in order to
discover basic notions for MILE:
1. Analyse information given to the human
user in bilingual/monolingual dictionaries
that allows selection of correct equivalence.
2. Analyse (if these can be obtained)
instructions/guidelines supplied to
lexicographers for writing bilingual entries.
3. Investigate, in corpus concordances, which
are the clues that allow to
disambiguate/decide on proper sense for
translation.
4. Elaborate a typology of transfer
conditions/actions and investigate lexical
requirements.
5. Look at multilingual lexical requirements
for approaches based on interlingual
concepts/ontologies.
6. Rank our typology in terms of scale of
difficulty of disambiguation
3.1. Types of information to be
addressed
Regarding the various types of information to
be addressed, the following "workflow" was
agreed:
1. notion already exists in previous work
(EAGLES, PAROLE/SIMPLE, EWN, etc.):
? evaluate the notion to see if it is
generally adequate
? evaluate its usefulness for multilingual
purposes
2. notion does not exist as recommendation
and is not otherwise used in applications
(e.g. collocation type), or there are notions
from other layers that we have not already
considered:
? decide which method is needed to do
work on it
? prioritise: what is used already in
multilingual lexicons (but not covered in
EAGLES, e.g. covered in OLIF) and
also then look at what needed in near
future
? record what needs further development.
A starting point will be the previous
EAGLES recommendations, as instantiated in
PAROLE/SIMPLE, for which ? as said above -
there is a unique DTD for all the 12 languages
involved. This will be revised and augmented
after work done on various types of information.
ISLE will also implement a lexicographic tool,
with which a sample of lexical entries will be
encoded according to the MILE structure.
Assignments for in-depth analysis of the
information types were done, and work is now
carried out by the various CLWG members.
Results of on-going work will provide: (i.) a list
of types of information that should be encoded
in each module; (ii.) linguistic specifications and
criteria; (iii.) a format for their representation in
multilingual lexicons; (iv.) their respective
weight/importance in a multilingual lexicon
(towards a layered approach to
recommendations).
4 Conclusions
Lexicon construction is a costly enterprise,
and a major goal is to set up general initiatives
to ease and optimise this process. The crescent
needs of lexical data, both of general and of
domain-specific nature, makes lexicon
development an always incremental and
potentially open effort, often to be carried out in
distributed environments and through the joint
work of multiple actors. It is therefore necessary
to facilitate lexicon versioning and authoring,
the fast integration and scalability of the
resources, the fast integration of domain and
general linguistic knowledge, as well as the
integration of the work of human lexicographers
with the information automatically extracted
from corpora and dictionaries. The main purpose
of the ISLE CLWG is to provide a satisfactory
answer to these needs, by establishing a general
infrastructure for lexical resources sharing. Its
backbone is represented by the MILE, a lexical
meta-entry, whose definition is now the focus of
the CLWG activities. The MILE is a modular
architecture for the representation of
multilingual lexical data, and aims at becoming
a common parlance for the representation and
encoding of lexical data.
References
Baker, Collin F., Fillmore, Charles J., and Lowe,
John B. (1998). "The Berkeley FrameNet project";
in Proceedings of the COLING-ACL, Montreal,
Canada.
Bel N., Busa, F., Calzolari, N., Gola, E., Lenci, A.,
Monachini, M., Ogonowski, A., Peters, I., Peters,
W., Ruimy, N., Villegas, M., Zampolli, A. (2000).
SIMPLE: A General Framework for the
Development of Multilingual Lexicons. LREC
Proceedings, Athens.
Burnard, L., Baker, P., McEnery, A. & Wilson, A.
(1997). An analytic framework for the validation
of language corpora. Report of the ELRA Corpus
Validation Group.
Calzolari, N. (1998). An Overview of Written
Language Resources in Europe: a few Reflections,
Facts, and a Vision, in A. Rubio, N. Gallardo, R.
Castro, A. Tejada (eds.), Proceedings of the First
International Conference on Language Resources
and Evaluation, Granada, pp.217-224.
Calzolari, N., Mc Naught, J., Zampolli, A. (1996).
EAGLES Final Report: EAGLES Editors?
Introduction. EAG-EB-EI, Pisa.
EAGLES (1996). Evaluation of Natural Language
Processing Systems. Final Report, Center for
Sprogteknologi, Copenhagen. Also available at
http://issco-
www.unige.ch/projects/ewg96/ewg96.html.
GENELEX Consortium, (1994). Report on the
Semantic Layer, Project EUREKA GENELEX,
Version 2.1.
Gibbon, D., Moore R., Winski, R. (1997). Handbook
of Standards and Resources for Spoken Language
Systems, Mouton de Gruyter, Berlin, New York.
Heid, U., McNaught, J. (1991). EUROTRA-7 Study:
Feasibility and Project Definition Study on the
Reusability of Lexical and Terminological
Resources in Computerised Applications. Final
report.
Jackendoff, R. (1992), Semantic Structures,
Cambridge, MA, MIT Press.
Leech, G., Wilson, A. (1996). Recommendations for
the morphosyntactic annotation of corpora, Eag-
tcwg-mac/r, ILC-CNR, Pisa.
Lenci, A., Busa, F., Ruimy, N., Gola, E., Monachini,
M., Calzolari, N., Zampolli, A. (1999). Linguistic
Specifications. SIMPLE Deliverable D2.1. ILC
and University of Pisa.
Miller G.A, Beckwidth R., Fellbaum C., Gross D.,
and Miller K.J. (1990), "Introduction to WordNet:
An On-line Lexical Database", International
Journal of Lexicography, III, No.4: 235-244.
Monachini, M., Calzolari, N. (1996). Synopsis and
comparison of morphosyntactic phenomena
encoded in lexicons and corpora. A common
proposal and applications to European languages,
Eag-clwg-morphsyn/r, ILC-CNR, Pisa.
Pustejovsky, J. (1995). The Generative Lexicon.
Cambridge, MA, MIT Press.
Ruimy, N., Corazzari, O., Gola, E., Spanu, A.,
Calzolari, N., Zampolli, A. (1998). The European
LE-PAROLE Project: The Italian Syntactic
Lexicon, in Proceedings of the First International
Conference on Language resources and
Evaluation, Granada: 241-248.
Sanfilippo, A. et al (1996). EAGLES
Subcategorization Standards. See
http://www.icl.pi.cnr.it/EAGLES96/syntax/syntax.
html
Sanfilippo, A. et al (1999). EAGLES
Recommendations on Semantic Encoding. See
http://www.ilc.pi.cnr.it/EAGLES96/rep2
Thurmair, G. (2000). OLIF Input Document, June
2000. See http://www.olif.net/main.htm
Underwood, N. & Navarretta, C. (1997). A Draft
Manual for the Validation of Lexica. Final ELRA
Report, Copenhagen.
Zampolli, A. (1998). Introduction, in A. Rubio, N.
Gallardo, R. Castro, A. Tejada (eds.), Proceedings
of the First International Conference on Language
Resources and Evaluation, Granada.
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38?42,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Chasing Hypernyms in Vector Spaces with Entropy 
 
Enrico Santus 
Dept. of Chinese and Bilingual Studies 
The Hong Kong Polytechnic 
University, Hong Kong 
e.santus@connect.polyu.hk 
Alessandro Lenci 
CoLing Lab ? Dept. of  Philology, 
Literature, and Linguistics 
University of Pisa, Italy 
alessandro.lenci@ling.unipi.it 
  
Qin Lu 
Dept. of Computing 
The Hong Kong Polytechnic 
University, Hong Kong 
csluqin@comp.polyu.edu.hk 
Sabine Schulte im Walde 
Inst. for Natural Language Processing 
University of Stuttgart 
Germany 
schulte@ims.uni-stuttgart.de 
 
 
 
Abstract 
In this paper, we introduce SLQS, a new 
entropy-based measure for the unsupervised 
identification of hypernymy and its 
directionality in Distributional Semantic 
Models (DSMs). SLQS is assessed through 
two tasks: (i.) identifying the hypernym in 
hyponym-hypernym pairs, and (ii.) 
discriminating hypernymy among various 
semantic relations. In both tasks, SLQS 
outperforms other state-of-the-art measures. 
1 Introduction 
In recent years, Distributional Semantic Models 
(DSMs) have gained much attention in 
computational linguistics as unsupervised 
methods to build lexical semantic representations 
from corpus-derived co-occurrences encoded as 
distributional vectors (Sahlgren, 2006; Turney 
and Pantel, 2010). DSMs rely on the 
Distributional Hypothesis (Harris, 1954) and 
model lexical semantic similarity as a function of 
distributional similarity, which is most 
commonly measured with the vector cosine 
(Turney and Pantel, 2010). DSMs have achieved 
impressive results in tasks such as synonym 
detection, semantic categorization, etc. (Pad? and 
Lapata, 2007; Baroni and Lenci, 2010). 
One major shortcoming of current DSMs is 
that they are not able to discriminate among 
different types of semantic relations linking 
distributionally similar lexemes. For instance, the 
nearest neighbors of dog in vector spaces 
typically include hypernyms like animal, co-
hyponyms like cat, meronyms like tail, together 
with other words semantically related to dog. 
DSMs tell us how similar these words are to dog, 
but they do not give us a principled way to single 
out the items linked by a specific relation (e.g., 
hypernyms). 
Another related issue is to what extent 
distributional similarity, as currently measured 
by DSMs, is appropriate to model the semantic 
properties of a relation like hypernymy, which is 
crucial for Natural Language Processing. 
Similarity is by definition a symmetric notion (a 
is similar to b if and only if b is similar to a) and 
it can therefore naturally model symmetric 
semantic relations, such as synonymy and co-
hyponymy (Murphy, 2003). It is not clear, 
however, how this notion can also model 
hypernymy, which is asymmetric. In fact, it is 
not enough to say that animal is distributionally 
similar to dog. We must also account for the fact 
that animal is semantically broader than dog: 
every dog is an animal, but not every animal is a 
dog. 
38
In this paper, we introduce SLQS, a new 
entropy-based distributional measure that aims to 
identify hypernyms by providing a distributional 
characterization of their semantic generality. We 
assess it with two tasks: (i.) the identification of 
the broader term in hyponym-hypernym pairs 
(directionality task); (ii.) the discrimination of 
hypernymy among other semantic relations 
(detection task). Given the centrality of 
hypernymy, the relevance of the themes we 
address hardly needs any further motivation. 
Improving the ability of DSMs to identify 
hypernyms is in fact extremely important in tasks 
such as Recognizing Textual Entailment (RTE) 
and ontology learning, as well as to enhance the 
cognitive plausibility of DSMs as general models 
of the semantic lexicon. 
2 Related work 
The problem of identifying asymmetric relations 
like hypernymy has so far been addressed in 
distributional semantics only in a limited way 
(Kotlerman et al., 2010) or treated through semi-
supervised approaches, such as pattern-based 
approaches (Hearst, 1992). The few works that 
have attempted a completely unsupervised 
approach to the identification of hypernymy in 
corpora have mostly relied on some versions of 
the Distributional Inclusion Hypothesis (DIH; 
Weeds and Weir, 2003; Weeds et al., 2004), 
according to which the contexts of a narrow term 
are also shared by the broad term. 
One of the first proposed measures 
formalizing the DIH is WeedsPrec (Weeds and 
Weir, 2003; Weeds et al., 2004), which 
quantifies the weights of the features f of a 
narrow term u that are included into the set of 
features of a broad term v: 
???????????, ?? = ? ????????????? ?????????  
where Fx is the set of features of a term x, and 
wx(f) is the weight of the feature f of the term x. 
Variations of this measure have been introduced 
by Clarke (2009), Kotlerman et al. (2010) and 
Lenci and Benotto (2012). 
In this paper, we adopt a different approach, 
which is not based on DIH, but on the hypothesis 
that hypernyms are semantically more general 
than hyponyms, and therefore tend to occur in 
less informative contexts than hypernyms. 
3 SLQS: A new entropy-based measure 
DIH is grounded on an ?extensional? definition 
of the asymmetric character of hypernymy: since 
the class (i.e., extension) denoted by a hyponym 
is included in the class denoted by the hypernym, 
hyponyms are expected to occur in a subset of 
the contexts of their hypernyms. However, it is 
also possible to provide an ?intensional? 
definition of the same asymmetry. In fact, the 
typical characteristics making up the ?intension? 
(i.e., concept) expressed by a hypernym (e.g., 
move or eat for animal) are semantically more 
general than the characteristics forming the 
?intension? of its hyponyms (e.g., bark or has fur 
for dog). This corresponds to the idea that 
superordinate terms like animal are less 
informative than their hyponyms (Murphy, 2002). 
From a distributional point of view, we can 
therefore expect that the most typical linguistic 
contexts of a hypernym are less informative than 
the most typical linguistic contexts of its 
hyponyms. In fact, contexts such as bark and has 
fur are likely to co-occur with a smaller number 
of words than move and eat. Starting from this 
hypothesis and using entropy as an estimate of 
context informativeness (Shannon, 1948), we 
propose SLQS, which measures the semantic 
generality of a word by the entropy of its 
statistically most prominent contexts. 
For every term wi we identify the N most 
associated contexts c (where N is a parameter 
empirically set to 50)1. The association strength 
has been calculated with Local Mutual 
Information (LMI; Evert, 2005). For each 
selected context c, we define its entropy H(c) as: 
                                                          
1
 N=50 is the result of an optimization of the model 
against the dataset after trying the following 
suboptimal values: 5, 10, 25, 75 and 100. 
39
???? = ??????|?? ? ?????????|???
?
???
 
where p(fi|c) is the probability of the feature fi 
given the context c, obtained through the ratio 
between the frequency of <c, fi> and the total 
frequency of c. The resulting values H(c) are 
then normalized in the range 0-1 by using the 
Min-Max-Scaling (Priddy and Keller, 2005): 
Hn(c). Finally, for each term wi we calculate the 
median entropy Ewi of its N contexts: 
??? = ?????? 	???????? 
???  can be considered as a semantic generality 
index for the term wi: the higher ??? , the more 
semantically general wi is. SLQS is then defined 
as the reciprocal difference between the semantic 
generality ??? and ??? of two terms w1 and w2: 
???????, ??? = 1 ? ?????? 
According to this formula, SLQS<0, if ???>???; 
SLQS?0, if ???????; and SLQS>0, if ???<???. 
SLQS is an asymmetric measure because, by 
definition, SLQS(w1,w2)?SLQS(w2,w1) (except 
when w1 and w2 have exactly the same 
generality). Therefore, if SLQS(w1,w2)>0, w1 is 
semantically less general than w2. 
4 Experiments and evaluation 
4.1 The DSM and the dataset 
For the experiments, we used a standard 
window-based DSM recording co-occurrences 
with the nearest 2 content words to the left and 
right of each target word. Co-occurrences were 
extracted from a combination of the freely 
available ukWaC and WaCkypedia corpora (with 
1.915 billion and 820 million words, respectively) 
and weighted with LMI. 
To assess SLQS we relied on a subset of 
BLESS (Baroni and Lenci, 2011), a freely- 
available dataset that includes 200 distinct 
English concrete nouns as target concepts, 
equally divided between living and non-living 
entities (e.g. BIRD, FRUIT, etc.). For each target 
concept, BLESS contains several relata, 
connected to it through one relation, such as co-
hyponymy (COORD), hypernymy (HYPER), 
meronymy (MERO) or no-relation (RANDOM-N).2 
Since BLESS contains different numbers of 
pairs for every relation, we randomly extracted a 
subset of 1,277 pairs for each relation, where 
1,277 is the maximum number of HYPER-related 
pairs for which vectors existed in our DSM. 
4.2 Task 1: Directionality 
In this experiment we aimed at identifying the 
hypernym in the 1,277 hypernymy-related pairs 
of our dataset. Since the HYPER-related pairs in 
BLESS are in the order hyponym-hypernym (e.g. 
eagle-bird, eagle-animal, etc.), the hypernym in 
a pair (w1,w2) is correctly identified by SLQS, if 
SLQS (w1,w2) > 0. Following Weeds et al. (2004), 
we used word frequency as a baseline model. 
This baseline is grounded on the hypothesis that 
hypernyms are more frequent than hyponyms in 
corpora. Table 1 gives the evaluation results: 
 
SLQS WeedsPrec BASELINE 
POSITIVE 1111 805 844 
NEGATIVE 166 472 433 
TOTAL 1277 1277 1277 
PRECISION 87.00% 63.04% 66.09% 
Table 1. Accuracy for Task 1. 
As it can be seen in Table 1, SLQS scores a 
precision of 87% in identifying the second term 
of the test pairs as the hypernym. This result is 
particularly significant when compared to the 
one obtained by applying WeedsPrec (+23.96%). 
As it was also noticed by Geffet and Dagan 
(2005) with reference to a previous similar 
experiment performed on a different corpus 
(Weeds et al., 2004), the WeedsPrec precision in 
this task is comparable to the na?ve baseline. 
SLQS scores instead a +20.91%. 
                                                          
2
 In these experiments, we only consider the BLESS 
pairs containing a noun relatum. 
40
4.3 Task 2: Detection 
The second experiment aimed at 
HYPER test pairs from those linked by other 
types of relations in BLESS (i.e.
and RANDOM-N). To this purpose, we assume
that hypernymy is characterized by two main 
properties: (i.) the hypernym and the hyponym 
are distributionally similar (in the sense of the 
Distributional Hypothesis), and 
hyponym is semantically less general than the 
hypernym. We measured the first property with 
the vector cosine and the second one with 
After calculating SLQS for all the pairs in our 
datasets, we set to zero all the negative values, 
that is to say those in which 
SLQS ? the first term is semantically more 
general than the second one. Then,
SLQS and vector cosine by the
greater the resulting value, the greater the 
likelihood that we are considering a hypernymy
related pair, in which the first word is 
and the second word is a hypernym.
To evaluate the performance
used Average Precision (AP; Kotlerman et al., 
2010), a method derived from Information 
Retrieval that combines precision, relevance 
ranking and overall recall, returning a value that 
ranges from 0 to 1. AP=1 means that all the 
instances of a relation are in the top of the rank
whereas AP=0 means they are in the bottom
is calculated for the four relations we extracted 
from BLESS. SLQS was also compared with 
WeedsPrec and vector cosine
frequency as baseline. Table 2 shows the results
 HYPER COORD MERO
Baseline 0.40 0.51 
Cosine 0.48 0.46 
WeedsPrec 0.50 0.35 
SLQS * 
Cosine 
0.59 0.27 
Table 2. AP values for T
The AP values show the performance
tested measures on the four 
optimal result would be obtained scoring 
HYPER and 0 for the other relations
discriminating 
, MERO, COORD 
d 
(ii.) the 
SLQS. 
? according to 
 we combined 
ir product. The 
-
a hyponym 
 
 of SLQS, we 
, 
. AP 
, again using 
: 
 RANDOM 
0.38 0.17 
0.31 0.21 
0.39 0.21 
0.35 0.24 
ask 2. 
s of the 
relations. The 
1 for 
.  
The product between SLQS
gets the best performance in identifying 
(+0.09 in comparison to 
discriminating it from COORD
WeedsPrec). It also achieves
discriminating MERO (-0.04
On the other hand, it seems to get 
lower precision in discriminating
(+0.03 in comparison to WeedsPrec
reason is that unrelated pairs might also have a 
fairly high semantic generality difference, 
slightly affecting the measure
Figure 1 gives a graphic depiction of the 
performances. SLQS corresponds to the 
line in comparison to the 
borders, grey fill), the vector c
borders) and the baseline (grey fill).
Figure 1. AP values
5 Conclusions and future work
In this paper, we have proposed 
asymmetric distributional measure of semantic 
generality which is able to identify the 
term in a hypernym-hyponym pair
combined with vector cosine
hypernymy from other types of semantic 
relations. The successful performance of 
the reported experiments
hyponyms and hypernyms 
similar, but hyponyms tend to occur in more 
informative contexts than hypernyms.
shows that an ?intensional? 
hypernymy can be pursued in distributional 
terms. This opens up new 
study of semantic relations 
research, SLQS will also be tested on other 
datasets and languages. 
 and vector cosine 
HYPER 
WeedsPrec) and in 
 (-0.08 than 
 better results in 
 than WeedsPrec). 
a slightly 
 RANDOM-N 
). The likely 
?s performance. 
black 
WeedsPrec (black 
osine (grey 
 
 
 for Task 2. 
 
SLQS, a new 
broader 
 and, when 
, to discriminate 
SLQS in 
 confirms that 
are distributionally 
 SLQS 
characterization of 
possibilities for the 
in DSMs. In further 
41
References  
Baroni, Marco and Lenci, Alessandro. 2010. 
?Distributional Memory: A general framework for 
corpus-based semantics?. Computational 
Linguistics, Vol. 36 (4). 673-721. 
Baroni, Marco and Lenci, Alessandro. 2011. ?How 
we BLESSed distributional semantic 
evaluation?. Proceedings of the EMNLP 2011 
Geometrical Models for Natural Language 
Semantics (GEMS 2011) Workshop. Edinburg, UK. 
1-10. 
Clarke, Daoud. 2009. ?Context-theoretic semantics 
for natural language: An overview?. Proceedings 
of the Workshop on Geometrical Models of Natural 
Language Semantics. Athens, Greece. 112-119. 
Evert, Stefan. 2005. The Statistics of Word 
Cooccurrences. Dissertation, Stuttgart University. 
Geffet, Maayan and Dagan, Idan. 2005. ?The 
Distributional Inclusion Hypotheses and Lexical 
Entailment?. Proceedings of 43rd Annual Meeting 
of the ACL. Michigan, USA. 107-114. 
Harris, Zellig. 1954. ?Distributional structure?. Word, 
Vol. 10 (23). 146-162. 
Hearst, Marti A. 1992. ?Automatic Acquisition of 
Hyponyms from Large Text Corpora?. 
Proceedings of the 14th International Conference 
on Computational Linguistics. Nantes, France. 
539-545. 
Kotlerman, Lili, Dagan, Ido, Szpektor, Idan, and 
Zhitomirsky-Geffet, Maayan. 2010. ?Directional 
Distributional Similarity for Lexical Inference?. 
Natural Language Engineering, Vol. 16 (4). 359-
389. 
Lenci, Alessandro and Benotto, Giulia. 2012. 
?Identifying hypernyms in distributional semantic 
spaces?. SEM 2012 ? The First Joint Conference 
on Lexical and Computational Semantics. Montr?al, 
Canada. Vol. 2. 75-79. 
Murphy, Gregory L.. 2002. The Big Book of Concepts. 
The MIT Press, Cambridge, MA. 
Murphy, M. Lynne. 2003. Lexical meaning. 
Cambridge University Press, Cambridge. 
Pad?, Sebastian and Lapata, Mirella. 2007. 
?Dependency-based Construction of Semantic 
Space Models?. Computational Linguistics, Vol. 
33 (2). 161-199. 
Priddy, Kevin L. and Keller, Paul E. 2005. Artificial 
Neural Networks: An Introduction. SPIE Press -
International Society for Optical Engineering, 
October 2005. 
Sahlgren, Magnus. 2006. The Word-Space Model: 
Using distributional analysis to represent 
syntagmatic and paradigmatic relations between 
words in high-dimensional vector spaces. Ph.D. 
dissertation, Department of Linguistics, Stockholm 
University. 
Shannon, Claude E. 1948. ?A mathematical theory of 
communication?. Bell System Technical Journal, 
Vol. 27. 379-423 and 623-656. 
Turney, Peter D. and Pantel, Patrick. 2010. ?From 
Frequency to Meaning: Vector Space Models of 
Semantics?. Journal of Articial Intelligence 
Research, Vol. 37. 141-188. 
Weeds, Julie and Weir, David. 2003. ?A general 
framework for distributional similarity?. 
Proceedings of the 2003 Conference on Empirical 
Methods in Natural Language Processing. Sapporo, 
Japan. 81-88. 
Weeds, Julie, Weir, David and McCarthy, Diana. 
2004. ?Characterising measures of lexical 
distributional similarity?. Proceedings of COLING 
2004. Geneva, Switzerland.1015-1021. 
 
 
 
 
 
42
Distributional Memory: A General
Framework for Corpus-Based Semantics
Marco Baroni?
University of Trento
Alessandro Lenci??
University of Pisa
Research into corpus-based semantics has focused on the development of ad hoc models that treat
single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting
different kinds of distributional information from the corpus. As an alternative to this ?one task,
one model? approach, the Distributional Memory framework extracts distributional information
once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged
into a third-order tensor. Different matrices are then generated from the tensor, and their rows
and columns constitute natural spaces to deal with different semantic problems. In this way,
the same distributional information can be shared across tasks such as modeling word similarity
judgments, discovering synonyms, concept categorization, predicting selectional preferences of
verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia
structures with patterns or example pairs, predicting the typical properties of concepts, and
classifying verbs into alternation classes. Extensive empirical testing in all these domains shows
that a Distributional Memory implementation performs competitively against task-specific al-
gorithms recently reported in the literature for the same tasks, and against our implementations
of several state-of-the-art methods. The Distributional Memory approach is thus shown to be
tenable despite the constraints imposed by its multi-purpose nature.
1. Introduction
The last two decades have seen a rising wave of interest among computational linguists
and cognitive scientists in corpus-basedmodels of semantic representation (Grefenstette
1994; Lund and Burgess 1996; Landauer and Dumais 1997; Schu?tze 1997; Sahlgren 2006;
Bullinaria and Levy 2007; Griffiths, Steyvers, and Tenenbaum 2007; Pado? and Lapata
2007; Lenci 2008; Turney and Pantel 2010). These models, variously known as vector
spaces, semantic spaces, word spaces, corpus-based semantic models, or, using the term
we will adopt, distributional semantic models (DSMs), all rely on some version of the
distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree
of semantic similarity between two words (or other linguistic units) can be modeled
? Center for Mind/Brain Sciences (CIMeC), University of Trento, C.so Bettini 31, 38068 Rovereto (TN),
Italy. E-mail: marco.baroni@unitn.it.
?? Department of Linguistics T. Bolelli, University of Pisa, Via Santa Maria 36, 56126 Pisa (PI), Italy.
E-mail: alessandro.lenci@ling.unipi.it.
Submission received: 11 January 2010; revised submission received: 15 April 2010; accepted for publication:
1 June 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
as a function of the degree of overlap among their linguistic contexts. Conversely, the
format of distributional representations greatly varies depending on the specific aspects
of meaning they are designed to model.
The most straightforward phenomenon tackled by DSMs is what Turney (2006b)
calls attributional similarity, which encompasses standard taxonomic semantic rela-
tions such as synonymy, co-hyponymy, and hypernymy. Words like dog and puppy,
for example, are attributionally similar in the sense that their meanings share a large
number of attributes: They are animals, they bark, and so on. Attributional similarity
is typically addressed by DSMs based on word collocates (Grefenstette 1994; Lund and
Burgess 1996; Schu?tze 1997; Bullinaria and Levy 2007; Pado? and Lapata 2007). These
collocates are seen as proxies for various attributes of the concepts that the words
denote. Words that share many collocates denote concepts that share many attributes.
Both dog and puppy may occur near owner, leash, and bark, because these words denote
properties that are shared by dogs and puppies. The attributional similarity between
dog and puppy, as approximated by their contextual similarity, will be very high.
DSMs succeed in tasks like synonym detection (Landauer and Dumais 1997) or
concept categorization (Almuhareb and Poesio 2004) because such tasks require a mea-
sure of attributional similarity that favors concepts that share many properties, such
as synonyms and co-hyponyms. However, many other tasks require detecting different
kinds of semantic similarity. Turney (2006b) defines relational similarity as the property
shared by pairs of words (e.g, dog?animal and car?vehicle) linked by similar semantic
relations (e.g., hypernymy), despite the fact that the words in one pair might not be
attributionally similar to those in the other pair (e.g., dog is not attributionally similar to
car, nor is animal to vehicle). Turney generalizes DSMs to tackle relational similarity and
represents pairs of words in the space of the patterns that connect them in the corpus.
Pairs of words that are connected by similar patterns probably hold similar relations,
that is, they are relationally similar. For example, we can hypothesize that dog?tail is
more similar to car?wheel than to dog?animal, because the patterns connecting dog and
tail (of, have, etc.) are more like those of car?wheel than like those of dog?animal (is a, such
as, etc.). Turney uses the relational space to implement tasks such as solving analogies
and harvesting instances of relations. Although they are not explicitly expressed in
these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and
Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and
focus on learning one relation type at a time (e.g., finding parts).
Although semantic similarity, either attributional or relational, has the lion?s share
in DSMs, similarity is not the only aspect of meaning that is addressed by distributional
approaches. For instance, the notion of property plays a key role in cognitive science and
linguistics, which both typically represent concepts as clusters of properties (Jackendoff
1990; Murphy 2002). In this case, the task is not to find out that dog is similar to puppy
or cat, but that it has a tail, it is used for hunting, and so on. Almuhareb (2006), Baroni
and Lenci (2008), and Baroni et al (2010) use the words co-occurring with a noun to
approximate its most prototypical properties and correlate distributionally derived data
with the properties produced by human subjects. Cimiano and Wenderoth (2007) in-
stead focus on that subset of noun properties known in lexical semantics as qualia roles
(Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive
parts of a concept or its function (this is in turn analogous to the problem of relation
extraction). The distributional semantics methodology also extends to more complex
aspects of word meaning, addressing issues such as verb selectional preferences (Erk
2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James
2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture
674
Baroni and Lenci Distributional Memory
a sort of ?topical? relatedness between words: They might find, for example, a relation
between dog and fidelity. Topical relatedness, addressed by DSMs based on document
distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Griffiths,
Steyvers, and Tenenbaum 2007), is not further discussed in this article.
DSMs have found wide applications in computational lexicography, especially for
automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens
2002; Kilgarriff et al 2004; Rapp 2004). Corpus-based semantic models have also at-
tracted the attention of lexical semanticists as a way to provide the notion of synonymy
with a more robust empirical foundation (Geeraerts 2010; Heylen et al 2008). Moreover,
DSMs for attributional and relational similarity are widely used for the semi-automatic
bootstrapping or extension of terminological repositories, computational lexicons (e.g.,
WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Inno-
vative applications of corpus-based semantics are also being explored in linguistics,
for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexical
variation (Peirsman and Speelman 2009), and for the analysis of multiword expressions
(Alishahi and Stevenson 2008).
The wealth and variety of semantic issues that DSMs are able to tackle confirms
the importance of looking at distributional data to explore meaning, as well as the
maturity of this research field. However, if we looked from a distance at the whole field
of DSMs we would see that, besides the general assumption shared by all models that
information about the context of a word is an important key in grasping its meaning, the
elements of difference overcome the commonalities. For instance, DSMs geared towards
attributional similarity represent words in the contexts of other (content) words, thereby
looking very different from models that represent word pairs in terms of patterns
linking them. In turn, both these models differ from those used to explore concept
properties or argument alternations. The typical approach in the field has been a local
one, in which each semantic task (or set of closely related tasks) is treated as a separate
problem, that requires its own corpus-derived model and algorithm, both optimized to
achieve the best performance in a given task, but lacking generality, since they resort
to task-specific distributional representations, often complemented by additional task-
specific resources. As a consequence, the landscape of DSMs looks more like a jigsaw
puzzle in which different parts have been completed and the whole figure starts to
emerge from the fragments, but it is not clear yet how to put everything together and
compose a coherent picture.
We argue that the ?one semantic task, one distributional model? approach repre-
sents a great limit of the current state of the art. From a theoretical perspective, corpus-
based models hold promise as large-scale simulations of how humans acquire and use
conceptual and linguistic information from their environment (Landauer and Dumais
1997). However, existing DSMs lack exactly the multi-purpose nature that is a hallmark
of human semantic competence. The common view in cognitive (neuro)science is that
humans resort to a single semantic memory, a relatively stable long-term knowledge
database, adapting the information stored there to the various tasks at hand (Murphy
2002; Rogers and McClelland 2004). The fact that DSMs need to go back to their
environment (the corpus) to collect ad hoc statistics for each semantic task, and the fact
that different aspects of meaning require highly different distributional representations,
cast many shadows on the plausibility of DSMs as general models of semantic mem-
ory. From a practical perspective, going back to the corpus to train a different model for
each application is inefficient, and it runs the risk of overfitting the model to a specific
task, while losing sight of its adaptivity?a highly desirable feature for any intelligent
system. Think, by contrast, of WordNet (Fellbaum 1998), a single, general purpose
675
Computational Linguistics Volume 36, Number 4
network of semantic information that has been adapted to all sorts of tasks, many of
them certainly not envisaged by the resource creators. We think that it is not by chance
that no comparable resource has emerged from DSM development.
In this article, we want to show that a unified approach is not only a desirable
goal, but it is also a feasible one. With this aim in mind, we introduce Distributional
Memory (DM), a generalized framework for distributional semantics. Differently from
other current proposals that share similar aims, we believe that the lack of generalization
in corpus-based semantics stems from the choice of representing co-occurrence statistics
directly as matrices?geometrical objects that model distributional data in terms of
binary relations between target items (the matrix rows) and their contexts (the matrix
columns). This results in the development of ad hocmodels that lose sight of the fact that
different semantic spaces actually rely on the same kind of underlying distributional
information. DM instead represents corpus-extracted co-occurrences as a third-order
tensor, a ternary geometrical object that models distributional data in terms of word?
link?word tuples. Matrices are then generated from the tensor in order to perform se-
mantic tasks in the spaces they define. Crucially, these on-demand matrices are derived
from the same underlying resource (the tensor) and correspond to different ?views?
of the same data, extracted once and for all from a corpus. DM is tested here on what
we believe to be the most varied array of semantic tasks ever addressed by a single
distributional model. In all cases, we compare the performance of several DM imple-
mentations to state-of-the-art results. While some of the ad hoc models that were devel-
oped to tackle specific tasks do outperform our most successful DM implementation,
the latter is never too far from the top, without any task-specific tuning. We think that
the advantage of having a general model that does not need to be retrained for each new
task outweighs the (often minor) performance advantage of the task-specific models.
The article is structured as follows. After framing our proposal within the general
debate on co-occurrence modeling in distributional semantics (Section 2), we introduce
the DM framework in Section 3 and compare it to other unified approaches in Section 4.
Section 5 pertains to the specific implementations of the DM framework we will test
experimentally. The experiments are reported in Section 6. Section 7 concludes by
summarizing what we have achieved, and discussing the implications of these results
for corpus-based distributional semantics.
2. Modeling Co-occurrence in Distributional Semantics
Corpus-based semantics aims at characterizing the meaning of linguistic expressions in
terms of their distributional properties. The standard view models such properties in
terms of two-way structures, that is, matrices coupling target elements (either single
words or whatever other linguistic constructions we try to capture distributionally)
and contexts. In fact, the formal definition of semantic space provided by Pado? and
Lapata (2007) is built around the notion of a matrix M|B|?|T|, with B the set of basis
elements representing the contexts used to compare the distributional similarity of the
target elements T.
This binary structure is inherently suitable for approaches that represent distribu-
tional data in terms of unstructured co-occurrence relations between an element and
a context. The latter can be either documents (Landauer and Dumais 1997; Griffiths,
Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the
target (Lund and Burgess 1996; Schu?tze 1997; Rapp 2003; Bullinaria and Levy 2007). We
will refer to such models as unstructured DSMs, because they do not use the linguistic
structure of texts to compute co-occurrences, and only record whether the target occurs
676
Baroni and Lenci Distributional Memory
in or close to the context element, without considering the type of this relation. For
instance, an unstructured DSM might derive from a sentence like The teacher eats a red
apple that eat is a feature shared by apple and red, just because they appear in the same
context window, without considering the fact that there is no real linguistic relation
linking eat and red, besides that of linear proximity.
In structured DSMs, co-occurrence statistics are collected instead in the form of
corpus-derived triples: typically, word pairs and the parser-extracted syntactic relation
or lexico-syntactic pattern that links them, under the assumption that the surface con-
nection between two words should cue their semantic relation (Grefenstette 1994; Lin
1998a; Curran and Moens 2002; Almuhareb and Poesio 2004; Turney 2006b; Pado? and
Lapata 2007; Erk and Pado? 2008; Rothenha?usler and Schu?tze 2009). Distributional triples
are also used in computational lexicography to identify the grammatical and colloca-
tional behavior of a word and to define its semantic similarity spaces. For instance,
the Sketch Engine1 builds ?word sketches? consisting of triples extracted from parsed
corpora and formed by two words linked by a grammatical relation (Kilgarriff et al
2004). The number of shared triples is then used to measure the attributional similarity
between word pairs.
Structured models take into account the crucial role played by syntactic structures
in shaping the distributional properties of words. To qualify as context of a target item,
a word must be linked to it by some (interesting) lexico-syntactic relation, which is
also typically used to distinguish the type of this co-occurrence. Given the sentence
The teacher eats a red apple, structured DSMs would not consider eat as a legitimate con-
text for red and would distinguish the object relation connecting eat and apple as a
different type of co-occurrence from the modifier relation linking red and apple. On the
other hand, structured models require more preliminary corpus processing (parsing or
extraction of lexico-syntactic patterns), and tend to be more sparse (because there are
more triples than pairs). What little systematic comparison of the two approaches has
been carried out (Pado? and Lapata 2007; Rothenha?usler and Schu?tze 2009) suggests
that structured models have a slight edge. In our experiments in Section 6.1 herein, the
performance of unstructured and structured models trained on the same corpus is in
general comparable. It seems safe to conclude that structured models are at least not
worse than unstructured models?an important conclusion for us, as DM is built upon
the structured DSM idea.
Structured DSMs extract a much richer array of distributional information from
linguistic input, but they still represent it in the same way as unstructured models.
The corpus-derived ternary data are mapped directly onto a two-way matrix, either
by dropping one element from the tuple (Pado? and Lapata 2007) or, more commonly, by
concatenating two elements. The two words can be concatenated, treating the links as
basis elements, in order to model relational similarity (Pantel and Pennacchiotti 2006;
Turney 2006b). Alternatively, pairs formed by the link and one word are concatenated
as basis elements to measure attributional similarity among the other words, treated
as target elements (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Almuhareb
and Poesio 2004; Rothenha?usler and Schu?tze 2009). In this way, typed DSMs obtain
finer-grained features to compute distributional similarity, but, by couching distribu-
tional information as two-way matrices, they lose the high expressive power of corpus-
derived triples. We believe that falling short of fully exploiting the potential of ternary
1 http://www.sketchengine.co.uk.
677
Computational Linguistics Volume 36, Number 4
distributional structures is the major reason for the lack of unification in corpus-based
semantics.
The debate in DSMs has so far mostly focused on the context choice?for example,
lexical collocates vs. documents (Sahlgren 2006; Turney and Pantel 2010)?or on the
costs and benefits of having structured contexts (Pado? and Lapata 2007; Rothenha?usler
and Schu?tze 2009). Although we see the importance of these issues, we believe that a
real breakthrough in DSMs can only be achieved by overcoming the limits of current
two-way models of distributional data. We propose here the alternative DM approach,
in which the core geometrical structure of a distributional model is a three-way object,
namely a third-order tensor. As in structured DSMs, we adopt word?link?word tuples
as the most suitable way to capture distributional facts. However, we extend and
generalize this assumption, by proposing that, once they are formalized as a three-
way tensor, tuples can become the backbone of a unified model for distributional
semantics. Different semantic spaces are then generated on demand through the inde-
pendently motivated operation of tensor matricization, mapping the third-order tensor
onto two-way matrices. The matricization of the tuple tensor produces both familiar
spaces, similar to those commonly used for attributional or relational similarity, and
other less known distributional spaces, which will yet prove useful for capturing some
interesting semantic phenomena. The crucial fact is that all these different semantic
spaces are now alternative views of the same underlying distributional object. Appar-
ently unrelated semantic tasks can be addressed in terms of the same distributional
memory, harvested only once from the source corpus. Thus, thanks to the tensor-based
representation, distributional data can be turned into a general purpose resource for
semantic modeling. As a further advantage, the third-order tensor formalization of
corpus-based tuples allows distributional information to be represented in a similar
way to other types of knowledge. In linguistics, cognitive science, and AI, semantic and
conceptual knowledge is represented in terms of symbolic structures built around typed
relations between elements, such as synsets, concepts, properties, and so forth. This is
customary in lexical networks like WordNet (Fellbaum 1998), commonsense resources
like ConceptNet (Liu and Singh 2004), and cognitive models of semantic memory
(Rogers andMcClelland 2004). The tensor representation of corpus-based distributional
data promises to build new bridges between existing approaches to semantic represen-
tation that still appear distant in many respects. This may indeed contribute to the on-
going efforts to combine distributional and symbolic approaches to meaning (Clark and
Pulman 2007).
3. The Distributional Memory Framework
We first introduce the notion of a weighted tuple structure, the format in which DM
expects the distributional data extracted from the corpus to be arranged (and that it
shares with traditional structured DSMs). We then show how aweighted tuple structure
can be represented, in linear algebraic terms, as a labeled third-order tensor. Finally,
we derive different semantic vector spaces from the tensor by the operation of labeled
tensor matricization.
3.1 Weighted Tuple Structures
Relations among entities can be represented by ternary tuples, or triples. Let O1 and
O2 be two sets of objects, and R ? O1 ?O2 a set of relations between these objects.
A triple ?o1, r, o2? expresses the fact that o1 is linked to o2 through the relation r. DM
678
Baroni and Lenci Distributional Memory
(like previous structured DSMs) includes tuples of a particular type, namely, weighted
distributional tuples that encode distributional facts in terms of typed co-occurrence
relations among words. Let W1 and W2 be sets of strings representing content words,
and L a set of strings representing syntagmatic co-occurrence links between words in
a text. T ?W1 ? L?W2 is a set of corpus-derived tuples t = ?w1, l,w2?, such that w1
co-occurs with w2 and l represents the type of this co-occurrence relation. For instance,
the tuple ?marine, use, bomb? in the toy example reported in Table 1 encodes the piece
of distributional information that marine co-occurs with bomb in the corpus, and use
specifies the type of the syntagmatic link between these words. Each tuple t has a
weight, a real-valued score vt, assigned by a scoring function ? :W1 ? L?W2 ? R.
A weighted tuple structure consists of the set TW of weighted distributional tuples
tw = ?t, vt? for all t ? T and ?(t) = vt. The ? function encapsulates all the operations
performed to score the tuples, for example, by processing an input corpus with a
dependency parser, counting the occurrences of tuples, and weighting the raw counts
by mutual information. Because our focus is on how tuples, once they are harvested,
should be represented geometrically, we gloss over the important challenges of choos-
ing the appropriateW1, L andW2 string sets, as well as specifying ?.
In this article, we make the further assumption that W1 =W2. This is a natural
assumption when the tuples represent (link-mediated) co-occurrences of word pairs.
Moreover, we enforce an inverse link constraint such that for any link l in L, there is a
k in L such that for each tuple tw = ??wi, l,wj?, vt? in the weighted tuple structure TW ,
the tuple t?1w = ??wj, k,wi?, vt? is also in TW (we call k the inverse link of l). Again, this
seems reasonable in our context: If we extract a tuple ?marine, use, gun? and assign it a
certain score, we might as well add the tuple ?gun, use?1, marine? with the same score.
The two assumptions, combined, lead the matricization process described in Section 3.3
to generate exactly four distinct vector spaces that, as we discuss there, are needed for
the semantic analyses we conduct. See Section 6.6 of Turney (2006b) for a discussion of
similar assumptions. Still, it is worth emphasizing that the general formalism we are
proposing, where corpus-extracted weighted tuple structures are represented as labeled
tensors, does not strictly require these assumptions. For example,W2 could be a larger
set of ?relata? including not only words, but also documents, morphological features,
or even visual features (with appropriate links, such as, for word-document relations,
occurs-at-the-beginning-of ). The inverse link constraint might not be appropriate, for
example, if we use an asymmetric association measure, or if we are only interested in
one direction of certain grammatical relations. We leave the investigation of all these
possibilities to further studies.
Table 1
A toy weighted tuple structure.
word link word weight word link word weight
marine own bomb 40.0 sergeant use gun 51.9
marine use bomb 82.1 sergeant own book 8.0
marine own gun 85.3 sergeant use book 10.1
marine use gun 44.8 teacher own bomb 5.2
marine own book 3.2 teacher use bomb 7.0
marine use book 3.3 teacher own gun 9.3
sergeant own bomb 16.7 teacher use gun 4.7
sergeant use bomb 69.5 teacher own book 48.4
sergeant own gun 73.4 teacher use book 53.6
679
Computational Linguistics Volume 36, Number 4
3.2 Labeled Tensors
DSMs adopting a binary model of distributional information (either unstructured mod-
els or structured models reduced to binary structures) are represented by matrices
containing corpus-derived co-occurrence statistics, with rows and columns labeled by
the target elements and their contexts. In DM,we formalize the weighted tuple structure
as a labeled third-order tensor, from which semantic spaces are then derived through
the operation of labeled matricization. Tensors are multi-way arrays, conventionally
denoted by boldface Euler script letters: X (Turney 2007; Kolda and Bader 2009). The
order (or n-way) of a tensor is the number of indices needed to identify its elements.
Tensors are a generalization of vectors and matrices. The entries in a vector can be
denoted by a single index. Vectors are thus first-order tensors, often indicated by a bold
lowercase letter: v. The i-th element of a vector v is indicated by vi. Matrices are second-
order tensors, and are indicatedwith bold capital letters:A. The entry (i, j) in the i-th row
and j-th column of a matrix A is denoted by aij. An array with three indices is a third-
order (or three-way) tensor. The element (i, j, k) of a third-order tensor X is denoted
by xijk. A convenient way to display third-order tensors is via nested tables such as
Table 2, where the first index is in the header column, the second index in the first
header row, and the third index in the second header row. The entry x321 of the tensor in
the table is 7.0 and the entry x112 is 85.3. An index has dimensionality I if it ranges over
the integers from 1 to I. The dimensionality of a third-order tensor is the product of the
dimensionalities of its indices I ? J ? K. For example, the third-order tensor in Table 2
has dimensionality 3? 2? 3.
If we fix the integer i as the value of the first index of a matrix A and take the
entries corresponding to the full range of values of the other index j, we obtain a row
vector (that we denote ai?). Similarly, by fixing the second index to j, we obtain the
column vector a?j. Generalizing, a fiber is equivalent to rows and columns in higher
order tensors, and it is obtained by fixing the values of all indices but one. A mode-n
fiber is a fiber where only the n-th index has not been fixed. For example, in the tensor
X of Table 2, x?11 = (40.0, 16.7, 5.2) is a mode-1 fiber, x2?3 = (8.0, 10.1) is a mode-2 fiber,
and x32? = (7.0, 4.7, 53.6) is a mode-3 fiber.
A weighted tuple structure can be represented as a third-order tensor whose entries
contain the tuple scores. As for the two-way matrices of classic DSMs, in order to make
tensors linguistically meaningful we need to assign linguistic labels to the elements of
the tensor indices. We define a labeled tensor X? as a tensor such that for each of its
indices there is a one-to-one mapping of the integers from 1 to I (the dimensionality of
the index) to I distinct strings, that we call the labels of the index. We will refer herein to
the string ? uniquely associated to index element i as the label of i, their correspondence
Table 2
A labeled third-order tensor of dimensionality 3? 2? 3 representing the weighted tuple
structure of Table 1.
j=1:own j=2:use j=1:own j=2:use j=1:own j=2:use
k=1:bomb k=2:gun k=3:book
i=1:marine 40.0 82.1 85.3 44.8 3.2 3.3
i=2:sergeant 16.7 69.5 73.4 51.9 8.0 10.1
i=3:teacher 5.2 7.0 9.3 4.7 48.4 53.6
680
Baroni and Lenci Distributional Memory
being indicated by i : ?. A simple way to perform the mapping?the one we apply in the
running example of this section?is by sorting the I items in the string set alhabetically,
and mapping increasing integers from 1 to I to the sorted strings.
A weighted tuple structure TW built from W1, L, and W2 can be represented by a
labeled third-order tensor X? with its three indices labeled by W1, L, and W2, respec-
tively, and such that for each weighted tuple t ? TW = ??w1, l,w2?, vt? there is a tensor
entry (i : w1, j : l, k : w2)= vt. In other terms, a weighted tuple structure corresponds to
a tensor whose indices are labeled with the string sets forming the triples, and whose
entries are the tuple weights. Given the toy weighted tuple structure in Table 1, the
object in Table 2 is the corresponding labeled third-order tensor.
3.3 Labeled Matricization
Matricization rearranges a higher order tensor into a matrix (Kolda 2006; Kolda and
Bader 2009). The simplest case is mode-n matricization, which arranges the mode-n
fibers to be the columns of the resultingDn ?Dj matrix (whereDn is the dimensionality
of the n-th index, Dj is the product of the dimensionalities of the other indices). Mode-n
matricization of a third-order tensor can be intuitively understood as the process of
making vertical, horizontal, or depth-wise slices of a three-way object like the tensor in
Table 2, and arranging these slices sequentially to obtain a matrix (a two-way object).
Matricization unfolds the tensor into a matrix with the n-th index indexing the rows of
the matrix and a column for each pair of elements from the other two tensor indices. For
example, the mode-1 matricization of the tensor in Table 2 results in a matrix with the
entries vertically arranged as they are in the table, but replacing the second and third
indices with a single index ranging from 1 to 6 (cf. matrix A of Table 3). More explicitly,
in mode-n matricization we map each tensor entry (i1, i2, ..., iN ) to matrix entry (in, j),
where j is computed as in Equation (1), adapted from Kolda and Bader (2009).
j = 1+
N
?
k=1
k =n
((ik ? 1)
k?1
?
m=1
m =n
Dm) (1)
For example, if we apply mode-1 matricization to the tensor of dimensionality 3? 2? 3
in Table 2, we obtain the matrix A3?6 in Table 3 (ignore the labels for now). The tensor
entry x3,1,1 is mapped to the matrix cell a3,1; x3,2,3 is mapped to a3,6; and x1,2,2 is mapped
to a1,4. Observe that each column of the matrix is a mode-1 fiber of the tensor: The first
column is the x?11 fiber; the second column is the x?21 fiber, and so on.
Matricization has various mathematically interesting properties and practical appli-
cations in computations involving tensors (Kolda 2006). In DM, matricization is applied
to labeled tensors and it is the fundamental operation for turning the third-order tensor
representing the weighted tuple structure into matrices whose row and column vector
spaces correspond to the linguistic objects we want to study; that is, the outcome of
matricization must be labeled matrices. Therefore, we must define an operation of
labeled mode-n matricization. Recall from earlier discussion that when mode-n matri-
cization is applied, the n-th index becomes the row index of the resultingmatrix, and the
corresponding labels do not need to be updated. The problem is to determine the labels
of the column index of the resulting matrix. We saw that the columns of the matrix pro-
duced by mode-n matricization are the mode-n fibers of the original tensor. We must
681
Computational Linguistics Volume 36, Number 4
Table 3
Labeled mode-1, mode-2, and mode-3 matricizations of the tensor in Table 2.
Amode-1 1:?own, 2:?use, 3:?own, 4:?use, 5:?own, 6:?use,
bomb? bomb? gun? gun? book? book?
1:marine 40.0 82.1 85.3 44.8 3.2 3.3
2:sergeant 16.7 69.5 73.4 51.9 8.0 10.1
3:teacher 5.2 7.0 9.3 4.7 48.4 53.6
Bmode-2 1:?marine, 2:?serg., 3:?teacher, 4:?marine, 5:?serg., 6:?teacher, 7:?marine, 8:?serg., 9:?teach.,
bomb? bomb? bomb? gun? gun? gun? book? book? book?
1:own 40.0 16.7 5.2 85.3 73.4 9.3 3.2 8.0 48.4
2:use 82.1 69.5 7.0 44.8 51.9 4.7 3.3 10.1 53.6
Cmode-3 1:?marine, 2:?marine, 3:?sergeant, 4:?sergeant, 5:?teacher, 6:?teacher,
own? use? own? use? own? use?
1:bomb 40.0 82.1 16.7 69.5 5.2 7.0
2:gun 85.3 44.8 73.4 51.9 9.3 4.7
3:book 3.2 3.3 8.0 10.1 48.4 53.6
therefore assign a proper label to mode-n tensor fibers. A mode-n fiber is obtained by
fixing the values of two indices, and by taking the tensor entries corresponding to the
full range of values of the third index. Thus, the natural choice for labeling a mode-n
fiber is to use the pair formed by the labels of the two index elements that are fixed.
Specifically, each mode-n fiber of a tensor X? is labeled with the binary tuple whose
elements are the labels of the corresponding fixed index elements. For instance, given
the labeled tensor in Table 2, the mode-1 fiber x?11 = (40, 16.7, 5.2) is labeled with the
pair ?own, bomb?, the mode-2 fiber x2?1 = (16.7, 69.5) is labeled with the pair ?sergeant,
bomb?, and the mode-3 fiber x32? = (7.0, 4.7, 53.6) is labeled with the pair ?teacher, use?.
Because mode-n fibers are the columns of the matrices obtained through mode-n
matricization, we define the operation of labeled mode-n matricization that, given
a labeled third-order tensor X?, maps each entry (i1 : ?1, i2 : ?2, i3 : ?3) to the labeled
entry (in : ?n, j : ?j) such that j is obtained according to Equation (1), and ?j is the bi-
nary tuple obtained from the triple ??1, ?2, ?3? by removing ?n. For instance, in mode-1
matricization, the entry (1:marine, 1:own, 2:gun) in the tensor in Table 2 is mapped onto
the entry (1:marine, 3:?own, gun?). Table 3 reports the matrices A, B, and C, respectively,
obtained by applying labeled mode-1, mode-2, and mode-3 matricization to the labeled
tensor in Table 2. The columns of each matrix are labeled with pairs, according to the
definition of labeled matricization we just gave. From now on, when we refer to mode-n
matricization we always assume we are performing labeledmode-nmatricization.
The rows and columns of the three matrices resulting from n-mode matricization
of a third-order tensor are vectors in spaces whose dimensions are the corresponding
column and row elements. Such vectors can be used to perform all standard linear
algebra operations applied in vector-based semantics: Measuring the cosine of the
angle between vectors, applying singular value decomposition (SVD) to the whole
matrix, and so on. Under the assumption thatW1 =W2 and the inverse link constraint
(see Section 3.1), it follows that for each column of the matrix resulting from mode-1
matricization and labeled by ?l,w2?, there will be a column in the matrix resulting
682
Baroni and Lenci Distributional Memory
from mode-3 matricization that is labeled by ?w1, k? (with k being the inverse link of
l and w1 = w2) and that is identical to the former, except possibly for the order of the
dimensions (which is irrelevant to all operations we perform on matrices and vectors,
however). Similarly, for any row w2 in the matrix resulting from mode-3 matricization,
there will be an identical row w1 in the mode-1 matricization. Therefore, given a
weighted tuple structure TW extracted from a corpus and subject to the constraints we
just mentioned, by matricizing the corresponding labeled third-order tensor X ? we
obtain the following four distinct semantic vector spaces:
word by link?word (W1?LW2): vectors are labeled with words w1, and vector
dimensions are labeled with tuples of type ?l,w2?;
word?word by link (W1W2?L): vectors are labeled with tuples of type ?w1,w2?,
and vector dimensions are labeled with links l;
word?link by word (W1L?W2): vectors are labeled with tuples of type ?w1, l?,
and vector dimensions are labeled with words w2;
link by word?word (L?W1W2): vectors are labeled with links l and vector
dimensions are labeled with tuples of type ?w1,w2?.
Words like marine and teacher are represented in the W1?LW2 space by vectors whose
dimensions correspond to features such as ?use, gun? or ?own, book?. In this space,
we can measure the similarity of words to each other, in order to tackle attributional
similarity tasks such as synonym detection or concept categorization. The W1W2?L
vectors represent instead word pairs in a space whose dimensions are links, and it
can be used to measure relational similarity among different pairs. For example, one
could notice that the link vector of ?sergeant, gun? is highly similar to that of ?teacher,
book?. Crucially, as can be seen in Table 3, the corpus-derived scores that populate the
vectors in these two spaces are exactly the same, just arranged in different ways. In DM,
attributional and relational similarity spaces are different views of the same underlying
tuple structure.
The other two distinct spaces generated by tensor matricization look less familiar,
and yet we argue that they allow us to subsume under the same general DM framework
other interesting semantic phenomena. We will show in Section 6.3 how the W1L?W2
space can be used to capture different verb classes based on the argument alternations
they display. For instance, this space can be used to find out that the object slot of kill
is more similar to the subject slot of die than to the subject slot of kill (and, generalizing
from similar observations, that the subject slot of die is a theme rather than an agent). The
L?W1W2 space displays similarities among links. The usefulness of this will of course
depend on what the links are. We will illustrate in Section 6.4 one function of this space,
namely, to perform feature selection, picking links that can then be used to determine a
meaningful subspace of theW1W2?L space.
Direct matricization is just one of the possible uses we can make of the labeled
tensor. In Section 6.5 we illustrate another use of the tensor formalism by performing
smoothing through tensor decomposition. Other possibilities, such as graph-based algo-
rithms operating directly on the graph defined by the tensor (Baroni and Lenci 2009), or
deriving unstructured semantic spaces from the tensor by removing one of the indices,
are left to future work.
Before we move on, it is worth emphasizing that, from a computational point of
view, there is virtually no additional cost in the tensor approach, with respect to tra-
ditional structured DSMs. The labeled tensor is nothing other than a formalization of
683
Computational Linguistics Volume 36, Number 4
distributional data extracted in the word?link?word?score format, which is customary
in many structured DSMs. Labeled matricization can then simply be obtained by con-
catenating two elements in the original triple to build the corresponding matrix?again,
a common step in building a structured DSM. In spite of being cost-free in terms of
implementation, the mathematical formalism of labeled tensors highlights the common
core shared by different views of the semantic space, thereby making distributional
semantics more general.
4. Related Work
As will be clear in the next sections, the ways in which we tackle specific tasks are, by
themselves, mostly not original. The main element of novelty is the fact that methods
originally developed to resort to ad hoc distributional spaces are now adapted to fit into
the unified DM framework. We will point out connections to related research specific to
the various tasks in the sections devoted to describing their reinterpretation in DM.
We omit discussion of our own work that the DM framework is an extension and
generalization of Baroni et al (2010) and Baroni and Lenci (2009). Instead, we briefly
discuss two other studies that explicitly advocate a uniform approach to corpus-based
semantic tasks, and one article that, like us, proposes a tensor-based formalization of
corpus-extracted triples. See Turney and Pantel (2010) for a very recent general survey
of DSMs.
Pado? and Lapata (2007), partly inspired by Lowe (2001), have proposed an interest-
ing general formalization of DSMs. In their approach, a corpus-based semantic model is
characterized by (1) a set of functions to extract statistics from the corpus, (2) construc-
tion of the basis-by-target-elements co-occurrence matrix, and (3) a similarity function
operating on the matrix. Our focus is entirely on the second aspect. A DM, according
to the characterization in Section 3, is a labeled tensor based on a source weighted
tuple structure and coupled with matricization operations. How the tuple structure
was built (corpus extraction methods, association measures, etc.) is not part of the DM
formalization. At the other end, DM provides sets of vectors in different vector spaces,
but it is agnostic about how they are used (measuring similarity via cosines or other
measures, reducing the matrices with SVD, etc.). Of course, much of the interesting
progress in distributional semantics will occur at the two ends of our tensor, with better
tuple extraction and weighting techniques on one side, and better matrix manipulation
and similarity measurement on the other. As long as the former operations result in
data that can be arranged into a weighted tuple structure, and the latter procedures act
on vectors, such innovations fit into the DM framework and can be used to improve
performance on tasks defined on any space derivable from the DM tensor.
Whereas the model proposed by Pado? and Lapata (2007) is designed only to ad-
dress tasks involving the measurement of the attributional similarity between words,
Turney (2008) shares with DM the goal of unifying attributional and relational similarity
under the same distributional model. He observes that tasks that are traditionally solved
with an attributional similarity approach can be recast as relational similarity tasks.
Instead of determining whether two words are, for example, synonymous by looking
at the features they share, we can learn what the typical patterns are that connect syn-
onym pairs when they co-occur (also known as, sometimes called, etc.), and make a de-
cision about a potential synonym pair based on their occurrence in similar contexts.
Given a list of pairs instantiating an arbitrary relation, Turney?s PairClass algorithm
extracts patterns that are correlated with the relation, and can be used to discover new
684
Baroni and Lenci Distributional Memory
pairs instantiating it. Turney tests his system in a variety of tasks (TOEFL synonyms;
SAT analogies; distinguishing synonyms and antonyms; distinguishing pairs that are
semantically similar, associated, or both), obtaining good results across the board.
In the DM approach, we collect one set of statistics from the corpus, and then exploit
different views of the extracted data and different algorithms to tackle different tasks.
Turney, on the contrary, uses a single generic algorithm, but must go back to the corpus
to obtain new training data for each new task. We compare DM with some of Turney?s
results in Section 6 but, independently of performance, we find the DM approach more
appealing. As corpora grow in size and are enriched with further levels of annotation,
extracting ad hoc data from them becomes a very time-consuming operation. Although
we did not carry out any systematic experiments, we observe that the extraction of tuple
counts from (already POS-tagged and parsed) corpora in order to train our sample DM
models took days, whereas even the most time-consuming operations to adapt DM to
a task took on the order of 1 to 2 hours on the same machines (task-specific training is
also needed in PairClass, anyway). Similar considerations apply to space: Compressed,
our source corpora take about 21 GB, our best DM tensor (TypeDM) 1.1 GB (and opti-
mized sparse tensor representations could bring this quantity down drastically, if the
need arises). Perhaps more importantly, extracting features from the corpus requires
a considerable amount of NLP know-how (to pre-process the corpus appropriately, to
navigate a dependency tree, etc.), whereas the DM representation of distributional data
as weighted triples is more akin to other standard knowledge representation formats
based on typed relations, which are familiar to most computer and cognitive scientists.
Thus, a trained DM can become a general-purpose resource and be used by researchers
beyond the realms of the NLP community, whereas applying PairClass requires a good
understanding of various aspects of computational linguistics. This severely limits its
interdisciplinary appeal.
At a more abstract level, DM and PairClass differ in the basic strategy with which
unification in distributional semantics is pursued. Turney?s approach amounts to pick-
ing a task (identifying pairs expressing the same relation) and reinterpreting other tasks
as its particular instances. Thus, attributional and relational similarity are unified by
considering the former as a subtype of the latter. Conversely, DM assumes that each
semantic task may keep its specificity, and unification is achieved by designing a suffi-
ciently general distributional structure, populating a specific instance of the structure,
and generating semantic spaces on demand from the latter. This way, DM is able to
address a wider range of semantic tasks than Turney?s model. For instance, language
is full of productive semantic phenomena, such as the selectional preferences of verbs
with respect to unseen arguments (eating topinambur vs. eating sympathy). Predicting
the plausibility of unseen pairs cannot, by definition, be tackled by the current version
of PairClass, which will have to be expanded to deal with such cases, perhaps adopting
ideas similar to those we present (that are, in turn, inspired by Turney?s own work on
attributional and relational similarity). A first step in this direction, within a framework
similar to Turney?s, was taken by Herdag?delen and Baroni (2009).
Turney (2007) explicitly formalizes the set of corpus-extracted word?link?word
triples as a tensor, and was our primary source of inspiration in formalizing DM in these
terms. The focus of Turney?s article, however, is on dimensionality reduction techniques
applied to tensors, and the application to corpora is only briefly discussed. Moreover,
Turney only derives the W1?LW2 space from the tensor, and does not discuss the pos-
sibility of using the tensor-based formalization to unify different views of semantic data,
which is instead our main point. The higher-order tensor dimensionality reduction
techniques tested on language data by Turney (2007) and Van de Cruys (2009) can be
685
Computational Linguistics Volume 36, Number 4
applied to the DM tensors before matricization. We present a pilot study in this direc-
tion in Section 6.5.
5. Implementing DM
5.1 Extraction of Weighted Tuple Structures from Corpora
In order to make our proposal concrete, we experiment with three different DMmodels,
corresponding to different ways to construct the underlying weighted tuple structure
(Section 3.1). All models are based on the natural idea of extracting word?link?word
tuples from a dependency parse of a corpus, but this is not a requirement for DM: The
links could for example be based on frequent n-grams as in Turney (2006b) and Baroni
et al (2010), or even on very different kinds of relation, such as co-occurring within the
same document.
The current models are trained on the concatenation of (1) the Web-derived ukWaC
corpus,2 about 1.915 billion tokens (here and subsequently, counting only strings
that are entirely made of alphabetic characters); (2) a mid-2009 dump of the English
Wikipedia,3 about 820 million tokens; and (3) the British National Corpus,4 about
95 million tokens. The resulting concatenated corpus was tokenized, POS-tagged, and
lemmatized with the TreeTagger5 and dependency-parsed with the MaltParser.6 It
contains about 2.83 billion tokens. The ukWaC and Wikipedia sections can be freely
downloaded, with full annotation, from the ukWaC corpus site.
For all our models, the label sets W1 =W2 contain 30,693 lemmas (20,410 nouns,
5,026 verbs, and 5,257 adjectives). These terms were selected based on their frequency
in the corpus (they are approximately the top 20,000 most frequent nouns and top 5,000
most frequent verbs and adjectives), augmenting the list with lemmas that we found in
various standard test sets, such as the TOEFL and SAT lists. In all models, the words are
stored in POS-suffixed lemma form. The weighted tuple structures differ for the choice
of links in L and/or for the scoring function ?.
DepDM. Our first DM model relies on the classic intuition that dependency paths are
a good approximation to semantic relations between words (Grefenstette 1994; Curran
and Moens 2002; Pado? and Lapata 2007; Rothenha?usler and Schu?tze 2009). DepDM is
also the model with the least degree of link lexicalization among the three DM instances
we have built (its only lexicalized links are prepositions). LDepDM includes the follow-
ing noun?verb, noun?noun, and adjective?noun links (in order to select more reliable
dependencies and filter out possible parsing errors, dependencies between words with
more than five intervening items were discarded):
sbj intr: subject of a verb that has no direct object: The teacher is singing?
?teacher, sbj intr, sing?; The soldier talked with his sergeant? ?soldier, sbj intr, talk?;
sbj tr: subject of a verb that occurs with a direct object: The soldier is reading a
book? ?soldier, sbj tr, read?;
2 http://wacky.sslmit.unibo.it/.
3 http://en.wikipedia.org/wiki/Wikipedia:Database download.
4 http://www.natcorp.ox.ac.uk.
5 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/.
6 http://w3.msi.vxu.se/?nivre/research/MaltParser.html.
686
Baroni and Lenci Distributional Memory
obj: direct object: The soldier is reading a book? ?book, obj, read?;
iobj: indirect object in a double object construction: The soldier gave the woman
a book? ?woman, iobj, give?;
nmod: noun modifier: good teacher? ?good, nmod, teacher?; school teacher? ?school,
nmod, teacher?;
coord: noun coordination: teachers and soldiers? ?teacher, coord, soldier?;
prd: predicate noun: The soldier became sergeant? ?sergeant, prd, become?;
verb: an underspecified link between a subject noun and a complement noun
of the same verb: The soldier talked with his sergeant? ?soldier, verb, sergeant?;
The soldier is reading a book? ?soldier, verb, book?;
preposition: every preposition linking the noun head of a prepositional phrase
to its noun or verb head (a different link for each preposition): I saw a soldier
with the gun? ?gun, with, soldier?; The soldier talked with his sergeant?
?sergeant, with, talk?.
For each link, we also extract its inverse (this holds for all our DMmodels). For example,
there is a sbj intr?1 link between an intransitive verb and its subject: ?talk, sbj intr?1,
soldier?. The cardinality of LDepDM is 796, including direct and inverse links.
The weights assigned to the tuples by the scoring function ? are given by Local
Mutual Information (LMI) computed on the raw corpus-derived word?link?word co-
occurrence counts. Given the co-occurrence count Oijk of three elements of interest (in
our case, the first word, the link, and the second word), and the corresponding expected
count under independence Eijk, LMI = Oijk log
Oijk
Eijk
. LMI is an approximation to the
log-likelihood ratio measure that has been shown to be a very effective weighting
scheme for sparse frequency counts (Dunning 1993; Pado? and Lapata 2007). The mea-
sure can also be interpreted as the dominant term of average MI or as a heuristic
variant of pointwise MI to avoid its bias towards overestimating the significance of
low frequency events, and it is nearly identical to the Poisson?Stirling measure (Evert
2005). LMI has considerable computational advantages in cases like ours, in which we
measure the association of three elements, because it does not require keeping track
of the full 2? 2? 2 contingency table, which is the case for the log-likelihood ratio.
Following standard practice (Bullinaria and Levy 2007), negative weights (cases where
the observed value is lower than the expected value) are raised to 0. The number of
non-zero tuples in the DepDM tensor is about 110M, including tuples with direct links
and their inverses. DepDM is a 30, 693? 796? 30, 693 tensor with density 0.0149% (the
proportion of non-zero entries in the tensor).
LexDM. The second model is inspired by the idea that the lexical material connect-
ing two words is very informative about their relation (Hearst 1992; Pantel and
Pennacchiotti 2006; Turney 2006b). LLexDM contains complex links, each with the struc-
ture pattern+suffix. The suffix is in turn formed by two substrings separated by a+, each
respectively encoding the following features ofw1 andw2: their POS andmorphological
features (number for nouns, number and tense for verbs); the presence of an article
(further specified with its definiteness value) and of adjectives for nouns; the presence
of adverbs for adjectives; and the presence of adverbs, modals, and auxiliaries for verbs,
together with their diatheses (for passive only). If the adjective (adverb) modifying
w1 or w2 belongs to a list of 10 (250) high frequency adjectives (adverbs), the suffix
687
Computational Linguistics Volume 36, Number 4
string contains the adjective (adverb) itself, otherwise only its POS. For instance, from
the sentence The tall soldier has already shot we extract the tuple ?soldier, sbj intr+n-the-
j+vn-aux-already, shoot?. Its complex link contains the pattern sbj intr and the suffix
n-the-j+vn-aux-already. The suffix substring n-the-j encodes the information that w1 is
a singular noun (n), is definite (the), and has an adjective ( j) that does not belong to
the list of high frequency adjectives. The substring vn-aux-already specifies that w2 is a
past-participle (vn), has an auxiliary (aux), and is modified by already, belonging to the
pre-selected list of high frequency adverbs. The patterns in the LexDM links include:
LDepDM : every DepDM link is a potential pattern of a LexDM link: The soldier has
shot? ?soldier, sbj intr+n-the+vn-aux, shoot?;
verb: if the verb link between a subject noun and a complement noun belongs to
a list of 52 high frequency verbs, the underspecified verb link of DepDM is
replaced by the verb itself: The soldier used a gun? ?soldier, use+n-the+n-a,
gun?; The soldier read the yellow book? ?soldier, verb+n-the+n-the-j, book?;
is: copulative structures with an adjectival predicate: The soldier is tall? ?tall,
is+j+n-the, soldier?;
preposition?link noun?preposition: this schema captures connecting expressions
such as of a number of, in a kind of ; link noun is one of 48 semi-manually
selected nouns such as number, variety, or kind; the arrival of a number of
soldiers? ?soldier, of-number-of+ns+n-the, arrival?;
attribute noun: one of 127 nouns extracted fromWordNet and expressing
attributes of concepts, such as size, color, or height. This pattern connects
adjectives and nouns that occur in the templates (the) attribute noun of
(a|the) NOUN is ADJ (Almuhareb and Poesio 2004) and (a|the) ADJ
attribute noun of NOUN (Veale and Hao 2008): the color of strawberries is red?
?red, color+j+ns, strawberry?; the autumnal color of the forest? ?autumnal,
color+j+n-the, forest?;
as adj as: this pattern links an adjective and a noun that match the template as
ADJ as (a|the) NOUN (Veale and Hao 2008): as sharp as a knife? ?sharp,
as adj as+j+n-a, knife?;
such as: links two nouns occurring in the templates NOUN such as NOUN and
such NOUN as NOUN (Hearst 1992, 1998): animals such as cats?
?animal, such as+ns+ns, cat?; such vehicles as cars? ?vehicle, such as+ns+ns, car?.
LexDM links have a double degree of lexicalization. First, the suffixes encode a wide
array of surface features of the tuple elements. Secondly, the link patterns themselves,
besides including standard syntactic relations (such as direct object or coordination),
extend to lexicalized dependency relations (specific verbs) and lexico-syntactic shallow
templates. The latter include patterns adopted in the literature to extract specific pieces
of semantic knowledge. For instance, NOUN such as NOUN and such NOUN as NOUN
were first proposed by Hearst (1992) as highly reliable patterns for hypernym identifi-
cation, whereas (the) attribute noun of (a|the) NOUN is ADJ and (a|the) ADJ attribute noun
of NOUN were successfully used to identify typical values of concept attributes
(Almuhareb and Poesio 2004; Veale and Hao 2008). Therefore, the LexDM distributional
memory is a repository of partially heterogeneous types of corpus-derived information,
differing in their level of abstractness, which ranges from fairly abstract syntactic rela-
tions to shallow lexicalized patterns. LLexDM contains 3,352,148 links, including inverses.
688
Baroni and Lenci Distributional Memory
The scoring function ? is the same as that in DepDM, and the number of non-
zero tuples is about 355M, including direct and inverse links. LexDM is a 30,693?
3,352,148? 30,693 tensor with density 0.00001%.
TypeDM. This model is based on the idea, motivated and tested by Baroni et al (2010)?
but see also Davidov and Rappoport (2008a, 2008b) for a related method?that what
matters is not so much the frequency of a link, but the variety of surface forms that
express it. For example, if we just look at frequency of co-occurrence (or strength of
association), the triple ? fat, of?1, land? (a figurative expression) is much more common
than the semantically more informative ? fat, of?1, animal?. However, if we count the
different surface realizations of the former pattern in our corpus, we find that there are
only three of them (the fat of the land, the fat of the ADJ land, and the ADJ fat of the land),
whereas ? fat, of?1, animal? has nine distinct realizations (a fat of the animal, the fat of the
animal, fats of animal, fats of the animal, fats of the animals, ADJ fats of the animal, and the fats
of the animal). TypeDM formalizes this intuition by adopting as links the patterns inside
the LexDM links, while the suffixes of these patterns are used to count their number
of distinct surface realizations. We call the model TypeDM because it counts types of
realizations, not tokens. For instance, the two LexDM links of?1+n-a+n-the and of?1+ns-
j+n-the are counted as two occurrences of the same TypeDM link of?1, corresponding to
the pattern in the two original links.
The scoring function ? computes LMI not on the raw word?link?word co-
occurrence counts, but on the number of distinct suffix types displayed by a link when it
co-occurs with the relevant words. For instance, a TypeDM link derived from a LexDM
pattern that occurs with nine different suffix types in the corpus is assigned a frequency
of 9 for the purpose of the computation of LMI. The distinct TypeDM links are 25,336.
The number of non-zero tuples in the TypeDM tensor is about 130M, including direct
and inverse links. TypeDM is a 30, 693? 25, 336? 30, 693 tensor with density 0.0005%.
To sum up, the three DM instance models herein differ in the degree of lexicali-
zation of the link set, and/or in the scoring function. LexDM is a heavily lexicalized
model, contrasting with DepDM, which has a minimum degree of lexicalization, and
consequently the smallest set of links. TypeDM represents a sort of middle level both
for the kind and the number of links. These consist of syntactic and lexicalized patterns,
as in LexDM. The lexical information encoded in the LexDM suffixes, however, is not
used to generate different links, but to implement a different counting scheme as part
of a different scoring function.
A weighted tuple structure (equivalently: a labeled DM tensor) is intended as
a long-term semantic resource that can be used in different projects for different
tasks, analogously to traditional hand-coded resources such as WordNet. Coherent
with this approach, we make our best DM model (TypeDM) publicly available from
http://clic.cimec.unitn.it/dm. The site also contains a set of Perl scripts that per-
form the basic operations on the tensor and its derived vectors we are about to describe.
5.2 Semantic Vector Manipulation
TheDM framework provides, viamatricization, a set of matrices with associated labeled
row and column vectors. These labeled matrices can simply be derived from the tuple
tensor by concatenating two elements in the original triples. Any operation that can
be performed on the resulting matrices and that might help in tackling a semantic
task is fair game. However, in the experiments reported in this article we will work
with a limited number of simple operations that are well-motivated in terms of the
689
Computational Linguistics Volume 36, Number 4
geometric framework we adopt, and suffice to face all the tasks we will deal with (the
decomposition techniques explored in Section 6.5 are briefly introduced there).
Vector length and normalization. The length of a vector v with dimensions v1, v2, . . . , vn is:
||v|| =
?
?i=n
i=1
v2i
A vector is normalized to have length 1 by dividing each dimension by the original
vector length.
Cosine.We measure the similarity of two vectors x and y by the cosine of the angle they
form:
cos(x,y) =
?i=n
i=1 xiyi
||x||||y||
The cosine ranges from?1 for vectors pointing in the same direction to 0 for orthogonal
vectors. Other similarity measures, such as Lin?s measure (Lin 1998b), work better than
the cosine in some tasks (Curran and Moens 2002; Pado? and Lapata 2007). However,
the cosine is the most natural similarity measure in the geometric formalism we are
adopting, and we stick to it as the default approach to measuring similarity.
Vector sum. Two or more vectors are summed in the obvious way, by adding their values
on each dimension. We always normalize the vectors before summing. The resulting
vector points in the same direction as the average of the summed normalized vectors.
We refer to it as the centroid of the vectors.
Projection onto a subspace. It is sometimes useful to measure length or compare vectors by
taking only some of their dimensions into account. For example, one way to find nouns
that are typical objects of the verb to sing is to measure the length of nouns in aW1?LW2
subspace in which only dimensions such as ?obj, sing? have non-0 values. We project
a vector onto a subspace of this kind through multiplication of the vector by a square
diagonal matrix with 1s in the diagonal cells corresponding to the dimensions we want
to preserve and 0s elsewhere. A matrix of this sort performs an orthogonal projection of
the vector it multiplies (Meyer 2000, chapter 5).
6. Semantic Experiments with the DM Spaces
As we saw in Section 3, labeled matricization generates four distinct semantic spaces
from the third-order tensor. For each space, we have selected a set of semantic ex-
periments that we model by applying some combination of the vector manipulation
operations of Section 5.2. The experiments correspond to key semantic tasks in compu-
tational linguistics and/or cognitive science, typically addressed by distinct DSMs so
far. We have also aimed at maximizing the variety of aspects of meaning covered by
the experiments, ranging from synonymy detection to argument structure and concept
properties, and encompassing all the major lexical classes. Both these facts support the
view of DM as a generalized model that is able to overtake state-of-the-art DSMs in
the number and types of semantic issues addressed, while being competitive in each
specific task.
690
Baroni and Lenci Distributional Memory
The choice of the DM semantic space to tackle a particular task is essentially based
on the ?naturalness? with which the task can be modeled in that space. However,
alternatives are conceivable, both with respect to space selection, and to the operations
performed on the space. For instance, Turney (2008) models synonymy detection with
a DSM that closely resembles our W1W2?L space, whereas we tackle this task under
the more standard W1?LW2 view. It is an open question whether there are principled
ways to select the optimal space configuration for a given semantic task. In this article,
we limit ourselves to proving that each space derived through tensor matricization is
semantically interesting in the sense that it provides the proper ground to address some
semantic task.
Feature selection/reweighting and dimensionality reduction have been shown to
improve DSM performance. For instance, the feature bootstrapping method proposed
by Zhitomirsky-Geffet and Dagan (2009) boosts the precision of a DSM in lexical en-
tailment recognition. Even if these methods can be applied to DM as well, we did not
use them in our experiments. The results presented subsequently should be regarded as
a ?baseline? performance that could be enhanced in future work by exploring various
task-specific parameters (we will come back in the conclusion to the role of parameter
tuning in DM). This is consistent with our current aim of focusing on the generality and
adaptivity of DM, rather than on task-specific optimization. As a first, important step
in this latter direction, however, we conclude the empirical evaluation in Section 6.5
by replicating one experiment using tensor-decomposition-based smoothing, a form of
optimization that can only be performed within the tensor-based approach to DSMs.
In order to maximize coverage of the experimental test sets, they are pre-processed
with a mixture of manual and heuristic procedures to assign a POS to the words they
contain, lemmatize, convert some multiword forms to single words, and turn some ad-
verbs into adjectives (our models do not contain multiwords or adverbs). Nevertheless,
some words (or word pairs) are unrecoverable, and in such cases we make a random
guess (in cases where we do not have full coverage of a data set, the reported results are
averages across repeated experiments, to account for the variability in random guesses).
In many of the experiments herein, DM is not only compared to the results avail-
able in the literature, but also to our implementation of state-of-the-art DSMs. These
alternative models have been trained on the same corpus (with the same linguistic pre-
processing) used to build the DM tuple tensors. This way, we aim at achieving a fairer
comparison with alternative approaches in distributional semantics, abstracting away
from the effects induced by differences in the training data.
Most experiments report global (micro-averaged) test set accuracy (alone, or com-
bined with other measures) to assess the performance of the algorithms. The number of
correctly classified items among all test elements can be seen as a binomially distributed
random variable, and we follow the ACL Wiki state-of-the-art site7 in reporting also
Clopper?Pearson binomial 95% confidence intervals around the accuracies (binomial
intervals and other statistical quantities were computed using the R package;8 where no
further references are given, we used the standard R functions for the relevant analysis).
The binomial confidence intervals give a sense of the spread of plausible population
values around the test-set-based point estimates of accuracy. Where appropriate and
interesting, we compare the accuracy of two specific models statistically with an exact
Fisher test on the contingency table of correct and wrong responses given by the two
7 http://aclweb.org/aclwiki/index.php?title=State Of The Art.
8 http://www.r-project.org/.
691
Computational Linguistics Volume 36, Number 4
models. This approach to significance testing is problematic in many respects, the most
important being that we ignore dependencies in correct and wrong counts due to the
fact that the algorithms are evaluated on the same test set (Dietterich 1998). More
appropriate tests, however, would require access to the fully itemized results from the
compared algorithms, whereas in most cases we only know the point estimate reported
in the earlier literature. For similar reasons, we do not make significance claims regard-
ing other performance measures, such as macro-averaged F. Other forms of statistical
analysis of the results are introduced herein when they are used; they are mostly limited
to the models for which we have full access to the results. Note that we are interested in
whether DM performance is overall within state-of-the-art range, and not on making
precise claims about the models it outperforms. In this respect, we think that our
general results are clear even where they are not supported by statistical inference, or
interpretation of the latter is problematic.
6.1 The W1?LW2 Space
The vectors of this space are labeled with words w1 (rows of matrix Amode-1 in Table 3),
and their dimensions are labeled with binary tuples of type ?l,w2? (columns of the same
matrix). The dimensions represent the attributes of words in terms of lexico-syntactic
relations with lexical collocates, such as ?sbj intr, read?, or ?use, gun?. Consistently, all
the semantic tasks that we address with this space involve the measurement of the
attributional similarity between words.
TheW1?LW2 matrix is a structured semantic space similar to those used by Curran
and Moens (2002), Grefenstette (1994), and Lin (1998a), among others. To test if the
use of links detracts from performance on attributional similarity tasks, we trained on
our concatenated corpus two alternative models?Win and DV?whose features only
include lexical collocates of the target. Win is an unstructured DSM that does not rely on
syntactic structure to select the collocates, but just on their linear proximity to the targets
(Lund and Burgess 1996; Schu?tze 1997; Bullinaria and Levy 2007, and many others). Its
matrix is based on co-occurrences of the same 30K words we used for the other models
within a window of maximally five content words before or after the target. DV is an
implementation of the Dependency Vectors approach of Pado? and Lapata (2007). It is a
structured DSM, but dependency paths are used to pick collocates, without being part of
the attributes. The DV model is obtained from the same co-occurrence data as DepDM
(thus, relying on the dependency paths we picked, not the ones originally selected
by Pado? and Lapata for their tests). Frequencies are summed across dependency path
links for word?link?word triples with the same first and second words. Suppose that
soldier and gun occur in the tuples ?soldier, have, gun? (frequency 3) and ?soldier, use, gun?
(frequency 37). In DepDM, this results in two features for soldier: ?have, gun? and ?use,
gun?. In DV, we would derive a single gun feature with frequency 40. As for the DM
models, theWin and DV counts are converted to LMI weights, and negative LMI values
are raised to 0. Win is a 30,693? 30,693 matrix with about 110 million non-zero entries
(density: 11.5%). DV is a 30,693? 30,693 matrix with about 38 million non-zero values
(density: 4%).
6.1.1 Similarity Judgments. Our first challenge comes from the classic data set of
Rubenstein and Goodenough (1965), consisting of 65 noun pairs rated by 51 subjects
on a 0?4 similarity scale. The average rating for each pair is taken as an estimate of the
perceived similarity between the two words (e.g., car?automobile: 3.9, cord?smile: 0.0).
Following the earlier literature, we use Pearson?s r to evaluate how well the cosines
692
Baroni and Lenci Distributional Memory
in the W1?LW2 space between the nouns in each pair correlate with the ratings. The
results (expressed in terms of percentage correlations) are presented in Table 4, which
also reports state-of-the-art performance levels of corpus-based systems from the litera-
ture (the correlation of all systems with the ratings is very significantly above chance,
according to a two-tailed t-test for Pearson correlation coefficients; df = 63, p < 0.0001
for all systems).
One of the DMmodels, namely TypeDM, does very well on this task, outperformed
only by DoubleCheck, an unstructured system that relies onWeb queries (and thus on a
much larger corpus) and for which we report the best result across parameter settings.
We also report the best results from a range of experiments with different models and
parameter settings from Herdag?delen, Erk, and Baroni (2009) (whose corpus is about
half the size of ours) and Pado? and Lapata (2007) (who use a much smaller corpus). For
the latter, we also report the best result they obtain when using cosine as the similarity
measure (cosDV-07). Overall, the TypeDM result is in line with the state of the art, given
the size of the input corpus, and the fact that we did not perform any tuning. Following
Pado?, Pado?, and Erk (2007) we used the approximate test proposed by Raghunathan
(2003) to compare the correlations with the human ratings of sets of models (this is only
possible for themodels we developed, as the test requires computation of correlation co-
efficients across models). The test suggests that the difference in correlation with human
ratings between TypeDM and our second best model, Win, is significant (Q = 4.55, df =
0.23, p< 0.01). On the other hand, there is no significant difference across Win, DepDM,
DV and LexDM (Q = 1.02, df = 1.80, p = 0.55).
6.1.2 Synonym Detection. The previous experiment assessed how the models can simu-
late quantitative similarity ratings. The classic TOEFL synonym detection task focuses
on the high end of the similarity scale, asking the models to make a discrete decision
about which word is the synonym from a set of candidates. The data set, introduced
to computational linguistics by Landauer and Dumais (1997), consists of 80 multiple-
choice questions, each made of a target word (a noun, verb, adjective, or adverb) and
four candidates. For example, given the target levied, the candidates are imposed, believed,
requested, correlated, the first one being the correct choice. Our algorithms pick the
candidate with the highest cosine to the target item as their guess of the right synonym.
Table 5 reports results (percentage accuracies) on the TOEFL set for our models as
well as the best model of Herdag?delen and Baroni (2009) and the corpus-based models
from the ACL Wiki TOEFL state-of-the-art table (we do not include those models from
the Wiki that resort to other knowledge sources, such as WordNet or a thesaurus). The
claims to follow about the relative performance of the models must be interpreted
cautiously, in light of the spread of the confidence intervals: It suffices to note that,
Table 4
Percentage Pearson correlation with the Rubenstein and Goodenough (1965) similarity ratings.
model r model r model r
DoubleCheck1 85 Win 65 DV 57
TypeDM 82 DV-073 62 LexDM 53
SVD-092 80 DepDM 57 cosDV-073 47
Model sources: 1Chen, Lin, and Wei (2006); 2Herdag?delen, Erk, and Baroni (2009); 3Pado? and
Lapata (2007).
693
Computational Linguistics Volume 36, Number 4
Table 5
Percentage accuracy in TOEFL synonym detection with 95% binomial confidence intervals (CI).
model accuracy 95% CI model accuracy 95% CI
LSA-031 92.50 84.39?97.20 DepDM 75.01 64.06?84.01
GLSA2 86.25 76.73?92.93 LexDM 74.37 63.39?83.49
PPMIC3 85.00 75.26?92.00 PMI-IR-018 73.75 62.72?82.96
CWO4 82.55 72.38?90.09 DV-079 73.00 62.72?82.96
PMI-IR-035 81.25 70.97?89.11 Win 69.37 58.07?79.20
BagPack6 80.00 69.56?88.11 Human10 64.50 53.01?74.88
DV 76.87 66.10?85.57 LSA-9710 64.38 52.90?74.80
TypeDM 76.87 66.10?85.57 Random 25.00 15.99?35.94
PairClass7 76.25 65.42?85.05
Model sources: 1Rapp (2003); 2Matveeva et al (2005); 3Bullinaria and Levy (2007); 4Ruiz-Casado,
Alfonseca, and Castells (2005); 5Terra and Clarke (2003); 6Herdag?delen and Baroni (2009); 7Turney
(2008); 8Turney (2001); 9Pado? and Lapata (2007); 10Landauer and Dumais (1997).
according to a Fisher test, the difference between the second-best model, GLSA, and the
twelfthmodel, PMI-IR-01, is not significant at the? = .05 level (p= 0.07). The difference
between the bottom model, LSA-97, and random guessing is, on the other hand, highly
significant (p < .00001).
The best DM model is again TypeDM, which also outperforms Turney?s unified
PairClass approach (supervised, and relying on a much larger corpus), as well as his
Web-statistics based PMI-IR-01 model. TypeDM does better than the best Pado? and
Lapata model (DV-07), and comparably to our DV implementation. Its accuracy is more
than 10% higher than the average human test taker and the classic LSAmodel (LSA-97).
Among the approaches that outperform TypeDM, BagPack is supervised, and CWO
and PMI-IR-03 rely on much larger corpora. This leaves us with three unsupervised
(and unstructured) models from the literature that outperform TypeDM while being
trained on comparable or smaller corpora: LSA-03, GLSA, and PPMIC. In all three
cases, the authors show that parameter tuning is beneficial in attaining the reported
best performance. Further work should investigate how we could improve TypeDM by
exploring various parameter settings (many of which do not require going back to the
corpus: feature selection and reweighting, SVD, etc.).
6.1.3 Noun Categorization. Humans are able to group words into classes or categories
depending on their meaning similarities. Categorization tasks play a prominent role
in cognitive research on concepts and meaning, as a probe into the semantic organiza-
tion of the lexicon and the ability to arrange concepts hierarchically into taxonomies
(Murphy 2002). Research in corpus-based semantics has always been interested in
investigating whether distributional (attributional) similarity could be used to group
words into semantically coherent categories. From the computational point of view, this
is a particularly crucial issue because it concerns the possibility of using distributional
information to assign a semantic class or type to words. Categorization requires (at least
in current settings) a discrete decision, as in the TOEFL task, but it is based on detecting
not only synonyms but also less strictly related words that stand in a coordinate/co-
hyponym relation. We focus here on noun categorization, which we operationalize as
a clustering task. Distributional categorization has been investigated for other POS as
well, most notably verbs (Merlo and Stevenson 2001; Schulte imWalde 2006). However,
694
Baroni and Lenci Distributional Memory
verb classifications are notoriously more controversial than nominal ones, and deeply
interact with argument structure properties. Some experiments on verb classification
will be carried out in theW1L?W2 space in Section 6.3.
Because the task of clustering concepts/words into superordinates has recently
attracted much attention, we have three relevant data sets from the literature available
for our tests. The Almuhareb?Poesio (AP) set includes 402 concepts from WordNet,
balanced in terms of frequency and ambiguity. The concepts must be clustered into
21 classes, each selected from one of the 21 uniqueWordNet beginners, and represented
by between 13 and 21 nouns. Examples include the vehicle class (helicopter,motorcycle. . . ),
the motivation class (ethics, incitement, . . . ), and the social unit class (platoon, branch). See
Almuhareb (2006) for the full set.
The Battig test set introduced by Baroni et al (2010) is based on the expanded
Battig and Montague norms of Van Overschelde, Rawson, and Dunlosky (2004). The
set comprises 83 concepts from 10 common concrete categories (up to 10 concepts per
class), with the concepts selected so that they are rated as highly prototypical of the
class. Class examples include land mammals (dog, elephant. . . ), tools (screwdriver, hammer)
and fruit (orange, plum). See Baroni et al (2010) for the full list.
Finally, the ESSLLI 2008 set was used for one of the Distributional Semantic Work-
shop shared tasks (Baroni, Evert, and Lenci 2008). It is also based on concrete nouns,
but it includes fewer prototypical members of categories (rocket as vehicle or snail as
land animal). The 44 target concepts are organized into a hierarchy of classes of in-
creasing abstraction. There are 6 lower level classes, with maximally 13 concepts per
class (birds, land animals, fruit, greens, tools, vehicles). At a middle level, concepts are
grouped into three classes (animals, vegetables, and artifacts). At the most abstract level,
there is a two-way distinction between living beings and objects. See http://wordspace.
collocations.de for the full set.
We cluster the nouns in each set by computing their similarity matrix based on
pairwise cosines, and feeding it to the widely used CLUTO toolkit (Karypis 2003). We
use CLUTO?s built-in repeated bisections with global optimization method, accepting
all of CLUTO?s default values for this method.
Cluster quality is evaluated by percentage purity, one of the standard clustering
quality measures returned by CLUTO (Zhao and Karypis 2003). If nir is the number of
items from the i-th true (gold standard) class that were assigned to the r-th cluster, n the
total number of items, and k the number of clusters, then
Purity = 1n
k
?
r=1
max
i
(nir)
Expressed in words, for each cluster we count the number of items that belong to the
true class that is most represented in the cluster, and then we sum these counts across
clusters. The resulting sum is divided by the total number of items so that, in the best
case (perfect clusters), purity will be 1 (in percentage terms, 100%). As cluster quality de-
teriorates, purity approaches 0. For the models where we have full access to the results,
we use a heuristic bootstrap procedure to obtain confidence intervals around the puri-
ties (Efron and Tibshirani 1994). We resample with replacement 10K data sets (cluster-
assignment+true-label pairs) of the original size. Empirical 95% confidence intervals are
then computed from the distribution of the purities in the bootstrapped data sets (for
the ESSLLI results, we only perform the procedure for 6-way clustering). The confidence
intervals give a rough idea of how stable purity estimates are across small variations of
695
Computational Linguistics Volume 36, Number 4
the items in the data sets. The Random models for this task are baselines assigning
the concepts randomly to the target clusters, with the constraint that each cluster must
contain at least one concept. Random assignment is repeated 10K times, and we obtain
means and confidence intervals from the distribution of these simulations.
Table 6 reports purity results for the three data sets, comparing our models to
those in the literature. Again, the TypeDM model has an excellent performance. On
the ESSLLI 2008 set, it outperforms the best configuration of the best shared task system
among those that did three-level categorization (Katrenko?s), despite the fact that the
latter uses the full Web as a corpus and manually crafted patterns to improve feature
extraction. TypeDM?s performance is equally impressive on the AP set, where it outper-
forms AttrValue-05, the best unsupervised model by the data set proponents, trained
on the full Web. Interestingly, the DepPath model of Rothenha?usler and Schu?tze (2009),
which is the only one outperforming TypeDM on the AP set, is another structured
model with dependency-based link-mediated features, which would fit well into the
Table 6
Purity in noun clustering with bootstrapped 95% confidence intervals (CI).
Almuhareb & Poesio (AP)
model purity 95% CI model purity 95% CI
DepPath1 79 NA DV 65 61?69
TypeDM 76 72?81 DepDM 62 59?67
AttrValue-052 71 NA LexDM 59 56?65
Win 71 67?76 Random 16 14?17
VSM3 70 67?75
Battig
model purity 95% CI model purity 95% CI
Win 96 91?100 DV-104 79 73?89
TypeDM 94 89?99 LexDM 78 72?88
Strudel4 91 85?98 SVD-104 71 67?83
DepDM 90 84?96 AttrValue4 45 44?61
DV 84 79?93 Random 29 24?34
ESSLLI 2008
model 6-way purity 95% CI 3-way purity 2-way purity avg purity
TypeDM 84 77?95 98 100 94.0
Katrenko5 91 NA 100 80 90.3
DV 75 70?89 93 100 89.3
DepDM 75 68?89 93 100 89.3
LexDM 75 70?89 87 100 87.3
Peirsman5 82 NA 84 86 84.0
Win 75 70?89 86 59 73.3
Shaoul5 41 NA 52 55 49.3
Random 38 32?45 49 57 48.0
Model sources: 1Rothenha?usler and Schu?tze (2009); 2Almuhareb and Poesio (2005); 3Herdag?delen,
Erk, and Baroni (2009); 4Baroni et al (2010); 5ESSLLI 2008 shared task.
696
Baroni and Lenci Distributional Memory
DM framework. TypeDM?s purity is extremely high with the Battig set as well, although
here it is outperformed by the unstructured Win model. Our top two performances are
higher than Strudel, the best model by the proponents of the task. The latter was trained
on about half of the data we used, however (moreover, the confidence intervals of these
models largely overlap, suggesting that their difference is not significant).
6.1.4 Selectional Preferences. Our last pair of data sets for the W1?LW2 space illustrate
how the space can be used not only to measure similarity among words, but also to
work with more abstract notions, such as that of a typical filler of an argument slot of a
verb (such as the typical killer and the typical killee). We think that these are especially
important experiments, because they show how the same matrix that has been used for
tasks that were entirely bound to lexical items can also be used to generalize to struc-
tures that go beyond what is directly observed in the corpus. In particular, we model
here selectional preferences (how plausible a noun is as subject/object of a verb), but
our method is generalizable to many other semantic tasks that pertain to composition
constraints; that is, they require measuring the goodness of fit of a word/concept as
argument filler of another word/concept, including assigning semantic roles, logical
metonymy, coercion (Pustejovsky 1995), and many other challenges.
The selectional preference test sets are based on averages of human judgments
on a seven-point scale about the plausibility of nouns as arguments (either subjects
or objects) of verbs. The McRae data set (McRae, Spivey-Knowlton, and Tanenhaus
1998) consists of 100 noun?verb pairs rated by 36 subjects. The Pado? set (Pado? 2007)
has 211 pairs rated by 20 subjects.
For each verb, we first use the W1?LW2 space to select a set of nouns that are
highly associated with the verb via a subject or an object link. In this space, nouns are
represented as vectors with dimensions that are labeled with ?link, word? tuples, where
the word might be a verb, and the link might stand for, among other things, syntactic
relations such as obj (or, in the LexDMmodel, an expansion thereof, such as obj+the-j). To
find nouns that are highly associated with a verb v when linked by the subject relation,
we project theW1?LW2 vectors onto a subspace where all dimensions are mapped to 0
except the dimensions that are labeled with ?lsbj, v?, where lsbj is a link containing either
the string sbj intr or the string sbj tr, and v is the verb. We then measure the length of the
noun vectors in this subspace, and pick the top n longest ones as prototypical subjects
of the verb. The same operation is performed for the object relation. In our experiments,
we set n to 20, but this is of course a parameter that should be explored.
We normalize and sum the vectors (in the fullW1?LW2 space) of the picked nouns,
to obtain a centroid that represents an abstract ?subject prototype? for the verb (and
analogously for objects). The plausibility of an arbitrary noun as the subject (object) of a
verb is then measured by the cosine of the noun vector to the subject (object) centroid in
W1?LW2 space. Crucially, the algorithm can provide plausibility scores for nouns that
do not co-occur with the target verb in the corpus, by looking at how close they are
to the centroid of nouns that do often co-occur with the verb. The corpus may contain
neither eat topinambur nor eat sympathy, but the topinambur vector will likely be closer to
the prototypical eat object vector than the one of sympathy would be.
It is worth stressing that the whole process relies on a single W1?LW2 matrix: This
space is first used to identify typical subjects (or objects) of a verb via subspacing, then to
construct centroid vectors for the verb subject (object) prototypes, and finally tomeasure
the distance of nouns to these centroids. Our method is essentially the same, save for
implementation and parameter choice details, as the one proposed by Pado?, Pado?,
and Erk (2007), in turn inspired by Erk (2007). However, they treat the identification
697
Computational Linguistics Volume 36, Number 4
of typical argument fillers of a verb as an operation to be carried out using different
resources, whereas we reinterpret it as a different way to use the sameW1?LW2 space in
which we measure plausibility.
Following Pado? and colleagues, we measure performance by the Spearman ? corre-
lation coefficient between the average human ratings and the model predictions, con-
sidering only verb?noun pairs that are present in the model. Table 7 reports percentage
coverage and correlations for the DM models (the task requires the links to extract
typical subjects and objects, so we cannot use DV nor Win), results from Pado?, Pado?,
and Erk (2007) (ParCos is the best among their purely corpus-based systems), and the
performance on the Pado? data set of the supervised system of Herdag?delen and Baroni
(2009). Testing for significance of the correlation coefficients with two-tailed tests based
on a Spearman-coefficient derived t statistic, we find that the Resnik?s model correlation
for the McRae data is not significantly different from 0 (t = 0.29, df = 92, p = 0.39),
ParCos on McRae is significant at ? = .05 (t = 2.134, df = 89, p = 0.018), and all other
models on either data set are significant at ? = .01 and below.
TypeDM emerges as an excellent model to tackle selectional preferences, and as the
overall winner on this task. On the Pado? data set, it is as good as Pado??s (2007) FrameNet
based model, and it is outperformed only by the supervised BagPack approach. On the
McRae data set, all three DM models do very well, and TypeDM is slightly worse than
the other two models. On this data set, the DM models are outperformed by Pado??s
FrameNet model in terms of correlation, but the latter has a much lower coverage,
suggesting that for practical purposes the DM models are a better choice. According to
Raghunathan?s test (see Section 6.1.1), the difference in correlation with human ratings
among the three DM models is not significant on the McRae data, where TypeDM is
below the other models (Q = 0.19, df = 0.67, p = 0.50). On the Pado? data set, on the
other hand, where TypeDM outperforms the other DM models, the same difference is
highly significant (Q = 12.70, df = 1.00, p < 0.001).
As a final remark on the W1?LW2 space, we can notice that DM models perform
very well in tasks involving attributional similarity. The performance of unstructured
DSMs (including Win, our own implementation of this type of model) is also high,
sometimes even better than that of structured DSMs. However, our best DMmodel also
achieves brilliant results in capturing selectional preferences, a task that is not directly
addressable by unstructured DSMs. This fact suggests that the real advantage provided
by structured DSMs?particularly when linguistic structure is suitably exploited, as
Table 7
Correlation with verb?argument plausibility judgments.
McRae Pado?
model coverage ? model coverage ?
Pado?1 56 41 BagPack2 100 60
DepDM 97 32 TypeDM 100 51
LexDM 97 29 Pado?1 97 51
TypeDM 97 28 ParCos1 98 48
ParCos1 91 22 DepDM 100 35
Resnik1 94 3 LexDM 100 34
Resnik1 98 24
Model sources: 1Pado?, Pado?, and Erk (2007); 2Herdag?delen and Baroni (2009).
698
Baroni and Lenci Distributional Memory
with the DM third-order tensor?actually resides in their versatility in addressing a
much larger and various range of semantic tasks. This preliminary conclusion will also
be confirmed by the experiments modeled with the other DM spaces.
6.2 The W1W2?L Space
The vectors of this space are labeled with word pair tuples ?w1,w2? (columns of matrix
Bmode-2 in Table 3) and their dimensions are labeled with links l (rows of the same
matrix). This arrangement of our tensor reproduces the ?relational similarity? space of
Turney (2006b), also implicitly assumed in much relation extraction work, where word
pairs are compared based on the patterns that link them in the corpus, in order to mea-
sure the similarity of their relations (Pantel and Pennacchiotti 2006). The links that in
W1?LW2 space provide a form of shallow typing of lexical features (?use, gun?) associ-
ated with single words (soldier) constitute under theW1W2?L view full features (use) as-
sociated with word pairs (?soldier, gun?). Besides exploiting this view of the tensor to
solve classic relational tasks, we will also show how problems that have not been tradi-
tionally defined in terms of a word-pair-by-link matrix, such as qualia harvesting with
patterns or generating lists of characteristic properties, can be elegantly recast in the
W1W2?L space by measuring the length of ?w1,w2? vectors in a link (sub)space, thus
bringing a wider range of semantic operations under the umbrella of the natural DM
spaces.
TheW1W2?L space represents pairs of words that co-occur in the corpus within the
maximum span determined by the scope of the links connecting them (for our models,
this maximum span is never larger than a single sentence). When words do not co-occur
or only co-occur very rarely (and even in large corpora this will often be the case), attri-
butional similarity can come to the rescue. Given a target pair, we can construct other,
probably similar pairs by replacing one of the words with an attributional neighbor. For
example, given the pair ?automobile, wheel?, we might discover inW1?LW2 space that car
is a close neighbor of automobile. We can then look for the pair ?car, wheel?, and use rela-
tional evidence about this pair as if it pertained to ?automobile, wheel?. This is essentially
the way to deal withW1W2?L data sparseness proposed by Turney (2006b), except that
he relies on independently harvested attributional and relational spaces, whereas we
derive both from the same tensor. More precisely, in theW1W2?L tasks where we know
the set of target pairs in advance (Sections 6.2.1 and 6.2.2), we smooth the DMmodels by
combining in turn one of the words of each target pair with the top 20 nearestW1?LW2
neighbors of the other word, obtaining a total of 41 pairs (including the original). The
centroid of the W1W2?L vectors of these pairs is then taken to represent a target pair
(the smoothed ?automobile, wheel? vector is an average of the ?automobile, wheel?, ?car,
wheel?, ?automobile, circle?, etc., vectors). The nearest neighbors are efficiently searched
in the W1?LW2 matrix by compressing it to 5,000 dimensions via random indexing,
using the parameters suggested by Sahlgren (2005). Smoothing consistently improved
performance, and we only report the relevant results for the smoothed versions of the
models (including our implementation of LRA, to be discussed next).
We reimplemented Turney?s Latent Relational Analysis (LRA) model, training it on
our source corpus (LRA is trained separately for each test set, because it relies on a given
list of word pairs to find the patterns that link them). We chose the parameter values of
Turney?s main model (his ?baseline LRA system?). In short (see Turney?s article for de-
tails), for a given set of target pairs we count all the patterns that connect them, in either
order, in the corpus. Patterns are sequences of one to three words occurring between the
targets, with all, none, or any subset of the elements replaced bywildcards (with the,with
699
Computational Linguistics Volume 36, Number 4
Table 8
Percentage accuracy in solving SAT analogies with 95% binomial confidence intervals (CI).
model accuracy 95% CI model accuracy 95% CI
Human1 57.0 52.0?62.3 TypeDM 42.4 37.4?47.7
LRA-062 56.1 51.0?61.2 LSA7 42.0 37.2?47.4
PERT3 53.3 48.5?58.9 LRA 37.8 32.8?42.8
PairClass4 52.1 46.9?57.3 PMI-IR-062 35.0 30.2?40.1
VSM1 47.1 42.2?52.5 DepDM 31.4 26.6?36.2
BagPack5 44.1 39.0?49.3 LexDM 29.3 24.8?34.3
k-means6 44.0 39.0?49.3 Random 20.0 16.1?24.5
Model sources: 1Turney and Littman (2005); 2Turney (2006b); 3Turney (2006a); 4Turney (2008);
5Herdag?delen and Baroni (2009); 6Bicic?i and Yuret (2006); 7Quesada, Mangalath, and Kintsch
(2004).
*, * the, * *). Only the top 4,000 most frequent patterns are preserved, and a target-pair-
by-pattern matrix is constructed (with 8,000 dimensions, to account for directionality).
Values in the matrix are log- and entropy-transformed using Turney?s formula. Finally,
SVD is applied, reducing the columns to the top 300 latent dimensions (here and sub-
sequently, we use SVDLIBC9 to perform SVD). For simplicity and to make LRA more
directly comparable to the DM models, we applied our attributional-neighbor-based
smoothing technique (the neighbors for target pair expansion are taken from the best
attributional DM model, namely, TypeDM) instead of the more sophisticated one used
by Turney. Thus, our LRA implementation differs from Turney?s original in two aspects:
the smoothing method and the source corpus (Turney uses a corpus of more than
50 billion words). Neither variation pertains to inherent differences between LRA and
DM.Given the appropriate resources, a DMmodel could be trained on Turney?s gigantic
corpus, and smoothed with his technique.
6.2.1 Solving Analogy Problems. The SAT test set introduced by Turney and collaborators
contains 374 multiple-choice questions from the SAT college entrance exam. Each ques-
tion includes one target (ostrich?bird) and five candidate analogies (lion?cat, goose?flock,
ewe?sheep, cub?bear, primate?monkey). The data set is dominated by noun?noun pairs,
but all other combinations are also attested (noun?verb, verb?adjective, verb?verb, etc.)
The task is to choose the candidate pair most analogous to the target (lion?cat in the
previous example). This is essentially the same task as the TOEFL, but applied to word
pairs instead of words. As in the TOEFL, we pick the candidate with the highest cosine
with the target as the right analogy.
Table 8 reports our SAT results together with those of other corpus-based methods
from the ACL Wiki and other systems. TypeDM is again emerging as the best among
our models. To put its performance in context statistically, according to a Fisher test its
accuracy is not significantly different from that of VSM (p = 0.239), whereas it is better
than that of PMI-IR-06 (p= 0.043; even the bottommodel, LexDM, is significantly better
than the random guesser, p = 0.004).
TypeDM is at least as good as LRA when the latter is trained on the same data
and smoothed with our method, suggesting that the excellent performance of Turney?s
version of LRA (LRA-06) is due to the fact that he used a much larger corpus, and/or to
9 http://tedlab.mit.edu/?dr/SVDLIBC/.
700
Baroni and Lenci Distributional Memory
his more sophisticated smoothing technique, and not to the specific way in which LRA
collects corpus-based statistics. All the algorithms with higher accuracy than TypeDM
are based onmuch larger input corpora, except BagPack, which is, however, supervised.
The LSA system of Quesada, Mangalath, and Kintsch (2004), which performs similarly
to TypeDM, is based on a smaller corpus, but it relies on hand-coded ?analogy domains?
that are represented by lists of manually selected characteristic words.
6.2.2 Relation Classification. Just as the SAT is the relational equivalent of the TOEFL task,
the test sets we tackle next are a relational analog to attributional concept clustering,
in that they require grouping pairs of words into classes that instantiate the same
relations. Whereas we cast attributional categorization as an unsupervised clustering
problem (following much of the earlier literature), the common approach to classify-
ing word pairs by relation is supervised, and relies on labeled examples for training. In
this article, we exploit training data in a very simple way, via a nearest centroid method.
In the SEMEVAL task we are about to introduce, where both positive and negative
examples are available for each class, we use the positive examples to construct a
centroid that represents a target class, and negative examples to construct a centroid rep-
resenting items outside the class. We then decide if a test pair belongs to the target class
by measuring its distance from the positive and negative centroids, picking the nearest
one. For example, the Cause?Effect relation has positive training examples such as
cycling?happiness and massage?relief and negative examples such as customer?satisfaction
and exposure?protection. We create a positive centroid by summing theW1W2?L vectors
of the first set of pairs, and a negative centroid by summing the latter. We then mea-
sure the cosine of a test item such as smile?wrinkle with the centroids, and decide
if it instantiates the Cause?Effect relation based on whether it is closer to the positive
or negative centroid. For the other tasks (as well as the transitive alternation task of
Section 6.3), we do not have negative examples, but positive examples for different
classes. We create a centroid for each class, and classify test items based on the centroid
they are nearest to.
Our first test pertains to the seven relations between nominals in Task 4 of
SEMEVAL 2007 (Girju et al 2007): Cause?Effect, Instrument?Agency, Product?
Producer, Origin?Entity, Theme?Tool, Part?Whole, Content?Container. For each rela-
tion, the data set includes 140 training and about 80 test items. Each item consists of a
Web snippet, containing word pairs connected by a certain pattern (e.g., ?* causes *?).
The retrieved snippets are manually classified by the SEMEVAL organizers as positive
or negative instances of a certain relation (see the earlier Cause?Effect examples). About
50% training and test cases are positive instances. In our experiments we do not make
use of the contexts of the target word pairs that are provided with the test set.
The second data set (NS) comes from Nastase and Szpakowicz (2003). It pertains to
the classification of 600 modifier?noun pairs and it is of interest because it proposes a
very fine-grained categorization into 30 semantic classes, such as Cause (cloud?storm),
Purpose (album?picture), Location-At (pain?chest), Location-From (visitor?country), Fre-
quency (superstition?occasional), Time-At (snack?midnight), and so on. The modifiers can
be nouns, adjectives, or adverbs. Because the data set is not split into training and test
data we follow Turney (2006b) and perform leave-one-out cross-validation. The data set
also comes with a coarser five-way classification. Our unreported results on it are com-
parable, in terms of relative performance, to the ones for the 30-way classification.
The last data set (OC) contains 1,443 noun?noun compounds classified by O?
Se?aghdha and Copestake (2009) into 6 relations: Be (celebrity?winner), Have (door?latch),
In (air?disaster), Actor (university?scholarship), Instrument ( freight?train), and About
701
Computational Linguistics Volume 36, Number 4
(bank?panic); see O? Se?aghdha and Copestake (2009) and references there. We use the
same five-way cross-validation splits as the data set proponents.
Table 9 reports performance of models from our experiments and from the literature
on the three supervised relation classification tasks. Following the relevant earlier stud-
ies, for SEMEVAL we report macro-averaged accuracy, whereas for the other two data
sets we report global accuracy (with binomial confidence intervals). All other measures
are macro-averaged. Majority is the performance of a classifier that always guesses the
Table 9
Relation classification performance; all measures macro-averaged, except accuracy in the NS and
OC data sets, where we also report the accuracy 95% confidence intervals (CI).
SEMEVAL 2007
model prec recall F acc model prec recall F acc
TypeDM 71.7 62.5 66.4 70.2 DepDM 61.0 57.3 58.9 61.8
UCD-FC1 66.1 66.7 64.8 66.0 UTH1 56.1 57.1 55.9 58.8
UCB1 62.7 63.0 62.7 65.4 Majority 81.3 42.9 30.8 57.0
LexDM 64.7 61.3 62.5 65.4 ProbMatch 48.5 48.5 48.5 51.7
ILK1 60.5 69.5 63.8 63.5 UC3M1 48.2 40.3 43.1 49.9
LRA 63.7 60.0 61.0 63.1 AllTrue 48.5 100.0 64.8 48.5
UMELB-B1 61.5 55.7 57.8 62.7
Nastase & Szpakowicz (NS)
model prec recall F acc acc 95% CI
LRA-062 41.0 35.9 36.6 39.8 35.9?43.9
VSM-AV3 27.9 26.8 26.5 27.8 24.3?31.6
LRA 23.0 23.1 21.1 25.5 22.1?29.2
VSM-WMTS2 24.0 20.9 20.3 24.7 21.3?28.3
TypeDM 19.5 20.2 13.7 15.4 12.5?18-5
LexDM 7.5 14.1 8.1 12.1 9.7?15.0
DepDM 11.6 14.5 8.1 8.7 6.5?11.2
Majority 0.3 3.3 0.5 8.2 6.1?10.6
ProbMatch 3.3 3.3 3.3 4.7 3.1?6.7
AllTrue 3.3 100 6.4 NA NA
O? Se?aghdha & Copestake (OC)
model prec recall F acc acc 95% CI
OC-Comb4 NA NA 61.6 63.1 60.6?65.6
OC-Rel4 NA NA 49.9 52.1 49.5?54.7
TypeDM 33.8 33.5 31.4 32.1 29.7?34.6
LRA 31.5 30.8 30.7 31.3 28.9?33.8
LexDM 29.9 28.9 28.7 29.7 27.4?32.2
DepDM 28.2 28.2 27.0 27.6 25.3?30.0
Majority 3.6 16.7 5.9 21.3 19.2?23.5
ProbMatch 16.7 16.7 16.7 17.1 15.2?19.2
AllTrue 16.7 100 28.5 NA NA
Model sources: 1SEMEVAL Task 4; 2Turney (2006b); 3Turney and Littman (2005); 4O? Se?aghdha
and Copestake (2009).
702
Baroni and Lenci Distributional Memory
majority class in the test set (in SEMEVAL, for each class, it guesses that all or no items
belong to it depending on whether there are more positive or negative examples in the
test data; in the other tasks, it labels all items with the majority class). AllTrue always
assigns an item to the target class (being inherently binary, it does not provide a well-
defined multi-class global accuracy). ProbMatch randomly guesses classes matching
their distribution in the test data (in SEMEVAL, it matches the proportion of positive
and negative examples within each class).
For SEMEVAL, the table reports the results of those models that took part in the
shared task and, like ours, did not use the organizer-provided WordNet sense labels
nor information about the query used to retrieve the examples. All these models are
outperformed by TypeDM, despite the fact that they exploit the training contexts
and/or specific additional resources: an annotated compound database (UCD-FC),
more sophisticated machine learning algorithms to train the relation classifiers (ILK,
UCD-FC), Web counts (UCB), and so on.
For the NS data set, none of the DM models do well, although TypeDM is once
more the best among them. The DM models are outperformed by other models from
the literature, all trained on much larger corpora, and also by our implementation of
LRA. The difference in global accuracy between LRA and TypeDM is significant (Fisher
test, p = 0.00002). TypeDM?s accuracy is nevertheless well above the best (Majority)
baseline accuracy (p = 0.0001).
The OC results confirm that TypeDM is the best of our models, again (slightly)
outperforming our LRA implementation. Still, our best performance is well below
that of OC-Comb, the absolute best, and OC-Rel, the best purely relational model
of O? Se?aghdha and Copestake (2009) (the difference in global accuracy between the
latter and TypeDM is highly significant, p < 0.000001). O? Se?aghdha and Copestake use
sophisticated kernel-based methods and extensive parameter tuning to achieve these
results. We hope that the TypeDM performance would also improve by improving the
machine learning aspects of the procedure.
As an ad interim summary, we observe that TypeDM achieves competitive results in
semantic tasks involving relational similarity. In particular, in both analogy solving and
two out of three relation classification experiments, TypeDM is at least as good as our
LRA implementation. We now move on to show how this same view of the DM tensor
can be successfully applied to aspects of meaning that are not normally addressed by
relational DSMs.
6.2.3 Qualia Extraction. A popular alternative to the supervised approach to relation
extraction is to pick a set of lexico-syntactic patterns that should capture the relation
of interest and to harvest pairs they connect in text, as famously illustrated by Hearst
(1992) for the hyponymy relation. In the DM approach, instead of going back to the
corpus to harvest the patterns, we exploit the information already available in the
W1W2?L space. We select promising links as our equivalent of patterns and we measure
the length of word pair vectors in the W1W2?L subspace defined by these links. We
illustrate this with the data set of Cimiano andWenderoth (2007), which contains qualia
structures (Pustejovsky 1995) for 30 nominal concepts, both concrete (door) and abstract
(imagination). Cimiano and Wenderoth asked 30 subjects to produce qualia for these
words (each word was rated by at least three subjects), obtaining a total of 1,487 word?
quale pairs, instantiating the four roles postulated by Pustejovsky: Formal (the category
of the object: door?barrier), Constitutive (constitutive parts, materials the object is made
of: food?fat), Agentive (what brings the object about: letter?write), and Telic (the func-
tion of the object: novel?entertain).
703
Computational Linguistics Volume 36, Number 4
We approximate the patterns proposed by Cimiano and Wenderoth by manually
selecting links that are already in our DM models, as reported in Table 10 (here and
subsequently when discussing qualia-harvesting links, we use n and q to indicate the
linear position of the noun and the potential quale with respect to the link). All qualia
roles have links pertaining to noun?noun pairs. The Agentive and Telic patterns also
harvest noun?verb pairs. For LexDM, we pick all links that begin with one of the strings
in Table 10. For the DepDM model, the only attested links are n with q (Constitutive),
n sbj intr q, n sbj tr q (Telic), and q obj n (Agentive). Consequently, DepDM does not
harvest Formal qualia, and is penalized accordingly in the evaluation.
We project all W1W2?L vectors that contain a target noun onto each of the four
subspaces determined by the quale-specific link sets, and we compute their subspace
lengths. Given a target noun n and a potential quale q, the length of the ?n, q? vector
in the subspace characterized by the links that represent role r is our measure of how
good q is as a quale of type r for n (for example, the length of ?book, read? in the subspace
defined by the Telic links is our measure of fitness of read as Telic role of book). We use
length in the subspace associated to the qualia role r to rank all ?n, q? pairs relevant to r.
Following Cimiano and Wenderoth?s evaluation method, for each noun we first
compute, separately for each role, the ranked list precision (with respect to themanually
constructed qualia structure) at 11 equally spaced recall levels from 0% to 100%. We
select the precision, recall, and F values at the recall level that results in the highest
F score (i.e., in the best precision?recall trade-off). We then average across the roles, and
then across target nouns. The task, as framed here, cannot be run with the LRA model,
and, because of its open-ended nature (we do not start from a predefined list of pairs),
we do not smooth the models.
Table 11 reports the performance of our models, as well as the F scores reported by
Cimiano and Wenderoth. For our models, where we have access to the itemized data,
we also report the standard deviation of F across the target nouns.
All the DM models perform well (including DepDM, which is disfavored by the
lack of Formal links), and once more TypeDM emerges as the best among them, with
an F value that is also (slightly) above the best Cimiano and Wenderoth models (that
are based on co-occurrence counts from the whole Web). Despite the large standard
deviations, the difference in F across concepts between TypeDM and the second-best
DM model (DepDM) is highly significant (paired t-test, t = 4.02, df = 29, p < 0.001),
suggesting that the large variance is due to different degrees of difficulty of the concepts,
affecting the models in similar ways.
Table 10
Links approximating the patterns proposed in Cimiano and Wenderoth (2007).
FORMAL CONSTITUTIVE
n as-form-of q, q as-form-of n q as-member-of n, q as-part-of n, nwith q
n as-kind-of q, n as-sort-of q, n be q nwith-lot-of q, nwith-majority-of q
q such as n nwith-number-of q, nwith-sort-of q
nwith-variety-of q
AGENTIVE TELIC
n as-result-of q, q obj n n for-use-as q, n for-use-in q, n sbj tr q
n sbj intr q
704
Baroni and Lenci Distributional Memory
Table 11
Average qualia extraction performance.
model precision recall F F s.d.
TypeDM 26.2 22.7 18.4 8.7
P1 NA NA 17.1 NA
WebP1 NA NA 16.7 NA
LexDM 19.9 23.6 16.2 7.1
WebJac1 NA NA 15.2 NA
DepDM 17.8 16.9 12.8 6.4
Verb-PMI1 NA NA 10.7 NA
Base1 NA NA 7.6 NA
Model source: 1Cimiano and Wenderoth (2007).
6.2.4 Predicting Characteristic Properties. Recently, there has been some interest in the
automated generation of commonsense concept descriptions in terms of intuitively
salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb
2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al 2010). Similar
property lists, collected from subjects in elicitation tasks, are widely used in cognitive
science as surrogates of mental features (Garrard et al 2001; McRae et al 2005; Vinson
and Vigliocco 2008). Large-scale collections of property-based concept descriptions are
also carried out in AI, where they are important for commonsense reasoning (Liu and
Singh 2004).
In the qualia task, given a concept we had to extract properties of certain kinds (cor-
responding to the qualia roles). The property-based description task is less constrained,
because the most salient relations of a nominal concept might be in all sorts of relations
with it (parts, typical behaviors, location, etc.). Still, we couch the task of unconstrained
property extraction as a challenge in theW1W2?L space. The approach is similar to the
method adopted for qualia roles, but now the wholeW1W2?L space is used, instead of
selected subspaces. Given all the ?n,w2? pairs that have the target nominal concept as
first element, we rank them by length in theW1W2?L space. The longest ?n,w2? vectors
in this space should correspond to salient properties of the target concept, as we expect
a concept to often co-occur in texts with its important properties (because in the current
DM implementations links are disjoint across POS, we map properties with different
POS onto the same scale by dividing the length of the vector representing a pair by the
length of the longest vector in the harvested concept?property set that has the same POS
pair). For example, among the longestW1W2?L vectors with car as first itemwe find ?car,
drive?, ?car, park?, and ?car, engine?. The first two pairs are normalized by dividing by the
longest ?noun, verb? vector in the harvested set, the third by dividing by the longest
?noun, noun? vector.
We test this approach in the ESSLLI 2008 Distributional Semantic Workshop un-
constrained property generation challenge (Baroni, Evert, and Lenci 2008). The data set
contains, for each of 44 concrete concepts, 10 properties that are those that were most
frequently produced by subjects in the elicitation experiment of McRae et al (2005) (the
?gold standard lists?). Algorithms must generate lists of 10 properties per concept, and
performance is measured by overlap with the subject-produced properties, that is, by
the cross-concept average proportions of properties in the generated lists that are also
in the corresponding gold standard lists. Smoothing would be very costly (we would
need to smooth all pairs that contain a target concept) and probably counterproductive
705
Computational Linguistics Volume 36, Number 4
(as the most typical properties of a concept should be highly specific to it, rather than
shared with neighbors). Because LRA (at least in a reasonably efficient implementation)
requires a priori specification of the target pairs, it is not well suited to this task.
Table 12 reports the percentage overlap with the gold standard properties (averaged
across the 44 concepts) for our models as well as the only ESSLLI 2008 participant that
tried this task, and for the models of Baroni et al (2010). TypeDM is the best DMmodel,
and it also does quite well compared to the state of the art. The difference between
Strudel, the best model from the earlier literature, and TypeDM is not statistically signif-
icant, according to a paired t-test across the target concepts (t = 1.1, df = 43, p = 0.27).
The difference between TypeDM and DV-10, the second best model from the literature,
is highly significant (t = 2.9, df = 43, p < 0.01). If we consider how difficult this sort
of open-ended task is (see the very low performance of the respectable models at the
bottom of the list), matching on average two out of ten speaker-generated properties, as
TypeDM does, is an impressive feat.
6.3 The W1L?W2 Space
The vectors of this space are labeled with binary tuples of type ?w1, l? (columns of
matrix Cmode-3 in Table 3), and their dimensions are labeled with words w2 (rows of the
samematrix). We illustrate this space in the task of discriminating verbs participating in
different argument alternations. However, other uses of the space can also be foreseen.
For example, the rows of W1L?W2 correspond to the columns of the W1?LW2 space
(given the constraints on the tuple structure we adopted in Section 3.1). We could use
the former space for feature smoothing or selection in the latter space, for example, by
merging the features ofW1?LW2 whose corresponding vectors inW1L?W2 have a cosine
similarity over a given threshold. We leave this possibility to further work.
Among the linguistic objects represented by the W1L?W2 vectors, we find the
syntactic slots of verb frames. For instance, the vector labeled with the tuple ?read,
sbj?1? represents the subject slot of the verb read in terms of the distribution of its
noun fillers, which label the dimensions of the space. We can use theW1L?W2 space to
explore the semantic properties of syntactic frames, and to extract generalizations about
the inner structure of lexico-semantic representations of the sort formal semanticists
have traditionally been interested in. For instance, the high similarity between the
object slot of kill and the subject slot of die might provide a distributional correlate to
the classic cause(subj,die(obj)) analysis of killing by Dowty (1977) and many others.
Measuring the cosine between the vectors of different syntactic slots of the same
verb corresponds to estimating the amount of fillers they share. Measures of ?slot
overlap? have been used by Joanis, Stevenson, and James (2008) as features to classify
verbs on the basis of their argument alternations. Levin and Rappaport-Hovav (2005)
Table 12
Average percentage overlap with subject-generated properties and standard deviation.
model overlap s.d. model overlap s.d. model overlap s.d.
Strudel1 23.9 11.3 LexDM 14.5 12.1 SVD-101 4.1 6.1
TypeDM 19.5 12.4 DV-101 14.1 10.3 Shaoul2 1.8 3.9
DepDM 16.1 12.6 AttrValue1 8.8 9.9
Model sources: 1Baroni et al (2010); 2ESSLLI 2008 shared task.
706
Baroni and Lenci Distributional Memory
define argument alternations as the possibility for verbs to have multiple syntactic
realizations of their semantic argument structure. Alternations involve the expression
of the same semantic argument in two different syntactic slots. We expect that, if a
verb undergoes a particular alternation, then the set of nouns that appear in the two
alternating slots should overlap to a certain degree.
Argument alternations represent a key aspect of the complex constraints that shape
the syntax?semantics interface. Verbs differ with respect to the possible alternations
they can undergo, and this variation is strongly dependent on their semantic proper-
ties (semantic roles, event type, etc.). Levin (1993) has in fact proposed a well-known
classification of verbs based on their range of syntactic alternations. Recognizing the
alternations licensed by a verb is extremely important in capturing its argument struc-
ture properties, and consequently in describing its semantic behavior. We focus here on
a particular class of alternations, namely transitivity alternations, whose verbs allow
both for a transitive NP V NP variant and for an intransitive NP V (PP) variant (Levin
1993). We use the W1L?W2 space to carry out the automatic classification of verbs that
participate in different types of transitivity alternations.
In the causative/inchoative alternation, the object argument (e.g., John broke the vase)
can also be realized as an intransitive subject (e.g., The vase broke). In a first experiment,
we use the W1L?W2 space to discriminate between transitive verbs undergoing the
causative/inchoative alternation (C/I) (e.g., break) and non-alternating ones (e.g.,mince;
cf. John minced the meat vs. *The meat minced). The C/I data set was introduced by
Baroni and Lenci (2009), but not tested in a classification task there. It consists of 232
causative/inchoative verbs and 170 non-alternating transitive verbs from Levin (1993).
In a second experiment, we apply the W1L?W2 space to discriminate verbs that
belong to three different classes, each corresponding to a different type of transitive
alternation. We use the MS data set (Merlo and Stevenson 2001), which includes 19 un-
ergative verbs undergoing the induced action alternation (e.g., race), 19 unaccusative
verbs that undergo the causative/inchoative alternation (e.g., break), and 20 object-drop
verbs participating in the unexpressed object alternation (e.g., play). See Levin (1993)
for details about each of these transitive alternations. The complexity of this task is
due to the fact that the verbs in the three classes have both transitive and intransitive
variants, but with very different semantic roles. For instance, the transitive subject of
unaccusative (The man broke the vase) and unergative verbs (The jockey raced the horse past
the barn) is an agent of causation, whereas the subject of the intransitive variant of un-
accusative verbs has a theme role (i.e., undergoes a change of state: The vase broke), and
the intransitive subject of unergative verbs has instead an agent role (The horse raced past
the barn). Thus, their surface identity notwithstanding, the semantic properties of the
syntactic slots of the verbs in each class are very different. By testing theW1L?W2 space
on such a task we can therefore evaluate its ability to capture non-trivial properties of
the verb?s thematic structure.
We address these tasks by measuring the similarities between theW1L?W2 vectors
of the transitive subject, intransitive subject, and direct object slots of a verb, and using
these inter-slot similarities to classify the verb. For instance, given the definition of
the C/I alternation, we can predict that with alternating verbs the intransitive subject
slot should be similar to the direct object slot (the things that are broken also break),
while this should not hold for non-alternating verbs (mincees are very different from
mincers). For each verb v in a data set, we extract the corresponding W1L?W2 slot
vectors ?v, l? whose links are sbj intr, sbj tr, and obj (for LexDM, we sum the vectors
with links beginning with one of these three patterns). Then, for each v we build a
three-dimensional vector with the cosines between the three slot vectors. These second
707
Computational Linguistics Volume 36, Number 4
order vectors encode the profile of similarity across the slots of a verb, and can be used to
spot verbs that have comparable profiles (e.g., verbs that have a high similarity between
their subj intr and obj slots).
We model both experiments as classification tasks using the nearest centroid
method on the three-dimensional vectors, with leave-one-out cross-validation. We per-
form binary classification of the C/I data set (treating non-alternating verbs as negative
examples), and three-way classification of the MS data. Table 13 reports the results, with
the baselines computed similarly to the ones in Section 6.2.2 (for C/I, Majority is equiv-
alent to AllTrue). The DM performance is also compared with the results of Merlo and
Stevenson (2001) for their classifiers tested with the leave-one-out methodology (macro-
averaged F has been computed on the class-by-class scores reported in that article).
All the DMmodels discriminate the verb classes much more reliably than the base-
lines. The accuracy of DepDM, the worst DM model, is significantly higher than that of
the best baselines, AllTrue in C/I (Fisher test, p= 0.024) andMajority onMS (p= 0.039).
TypeDM is again our best model. Its performance is comparable to the lower range
of the Merlo and Stevenson classifiers (considering the large confidence intervals due to
the small sample size, the accuracy of TypeDM is not significantly below even that of the
top model NoPass; p = 0.43). The TypeDM results were obtained simply by measuring
the verb inter-slot similarities in theW1L?W2 space. Conversely, the classifiers in Merlo
and Stevenson (2001) rely on a much larger range of knowledge-intensive features
selected in an ad hoc fashion for this task (on the other hand, their training corpus
Table 13
Verb classification performance (precision, recall, and F for MS are macro-averaged). Global
accuracy supplemented by 95% binomial confidence intervals (CI).
Causative/Inchoative (C/I)
model prec recall F acc acc 95% CI
LexDM 76.0 69.9 72.8 69.9 65.2?74.3
TypeDM 75.7 68.5 71.9 69.1 64.4?73.6
DepDM 72.8 64.6 68.4 65.7 60.8?70.3
AllTrue 57.7 100 73.2 57.7 52.7?62.6
ProbMatch 57.7 57.7 57.7 51.2 46.2?56.2
Merlo & Stevenson (MS)
model prec recall F acc acc 95% CI
NoPass1 NA NA 71.2 71.2 57.3?81.9
AllFeatures1 NA NA 69.1 69.5 55.5?80.5
NoTrans1 NA NA 63.8 64.4 50.1?76.0
NoCaus1 NA NA 62.6 62.7 48.4?74.5
TypeDM 60.7 61.7 60.8 61.5 47.5?73.7
NoVBN1 NA NA 61.0 61.0 46.6?73.0
NoAnim1 NA NA 59.9 61.0 46.6?73.0
LexDM 55.3 56.7 55.8 56.4 43.2?69.8
DepDM 52.9 55.0 53.2 54.7 41.5?68.3
Majority 11.3 33.3 16.9 33.9 22.5?48.1
ProbMatch 33.3 33.3 33.3 33.3 21.0?46.3
AllTrue 33.3 100 50.0 NA NA
Model source: 1Merlo and Stevenson (2001).
708
Baroni and Lenci Distributional Memory
is not parsed and it is much smaller than ours). Finally, we can notice that in both
experiments the mildly (TypeDM) and heavily (LexDM) lexicalized DM models score
better than their non-lexicalized counterpart (DepDM), although the difference between
the best DM model and DepDM is not significant on either data set (p = 0.23 for the
LexDM/DepDMdifference in C/I; p= 0.57 for the TypeDM/DepDMdifference inMS).
Verb alternations do not typically appear among the standard tasks on which DSMs
are tested. Moreover, they involve non-trivial properties of argument structure. The
good performance of DM in these experiments is therefore particularly significant in
supporting its vocation as a general model for distributional semantics.
6.4 The L?W1W2 Space
The vectors of this space are labeled with links l (rows of matrix Bmode-2 in Table 3)
and their dimensions are labeled with word pair tuples ?w1,w2? (columns of the same
matrix). Links are represented in terms of the word pairs they connect. The L?W1W2
space supports tasks where we are directly interested in the links as an object of
study?for example, characterizing prepositions (Baldwin, Kordoni, and Villavicencio
2009) or measuring the relative similarity of different kinds of verb?noun relations. We
focus here instead on a potentially more common use of L?W1W2 vectors as a ?feature
selection and labeling? space forW1W2?L tasks.
Specifically, we go back to the qualia extraction task of Section 6.2.3. There, we
started with manually identified links. Here, we start with examples of noun?quale
pairs ?n, qr? that instantiate a role r. We project all L?W1W2 vectors in a subspace where
only dimensions corresponding to one of the example pairs are non-zero. We then pick
the most characteristic links in this subspace to represent the target role r, and look for
new pairs ?n, qr? in theW1W2?L subspace defined by these automatically picked links,
instead of the manual ones. Although we stop at this point, the procedure can be seen
as a DM version of popular iterative bootstrapping algorithms such as Espresso (Pantel
and Pennacchiotti 2006): Start with some examples of the target relation, find links that
are typical of these examples, use the links to find new examples, and so on. In DM,
the process does not go back to a corpus to harvest new links and example pairs, but it
iterates between the column and row spaces of a pre-compiled matrix (i.e, the mode-2
matricization in Table 3).
For each of the 30 noun concepts in the Cimiano and Wenderoth gold standard, we
use the noun?quale pairs pertaining to the remaining 29 concepts as training examples
to select a set of 20 links that we then use in the same way as the manually selected links
of Section 6.2.3. Simply picking the longest links in the L?W1W2 subspace defined by the
example ?n, qr? dimensions does not work, because we harvest links that are frequent
in general, rather than characteristic of the qualia roles (noun modification, of, etc.). For
each role r, we construct instead two L?W1W2 subspaces, one positive subspace with the
example pairs ?n, qr? as unique non-zero dimensions, and a negative subspace with non-
zero dimensions corresponding to all ?w1,w2? pairs such that w1 is one of the training
nominal concepts, and w2 is not a quale qr in the example pairs. We then measure the
length of each link in both subspaces. For example, we measure the length of the obj
link in a subspace characterized by ?n, qtelic? example pairs, and the length of obj in a
subspace characterized by ?n, w2? pairs that are probably not Telic examples. We com-
pute the pointwise mutual information (PMI) statistic (Church and Hanks 1990) on
these lengths to find the links that are most typical of the positive subspace corre-
sponding to each qualia role. PMI, with respect to other association measures, finds
more specific links, which is good for our purposes. However, it is also notoriously
709
Computational Linguistics Volume 36, Number 4
prone to over-estimating the importance of rare items (Manning and Schu?tze 1999,
Chapter 5). Thus, before selecting the top 20 links ranked by PMI, we filter out those
links that do not have at least 10 non-zero dimensions in the positive subspace. Many
parameters here should be tuned more systematically (top n links, association measure,
minimum non-zero dimensions), but the current results will nevertheless illustrate our
methodology.
Table 14 reports, for each quale, the TypeDM links that were selected in each of the
30 leave-one-concept-out folds. The links n is q, n in q, and q such as n are a good sketch of
the Formal relation, which essentially subsumes various taxonomic relations. The other
Formal links are less conspicuous. However, note the presence of noun coordination
(n coord q and q coord n), consistently with the common claim that coordinated terms
tend to be related taxonomically (Widdows and Dorow 2002). Constitutive is mostly a
whole?part relation, and the harvested links do a good job at illustrating such a relation.
For the Telic, q by n, q through n, and q via n capture cases in which the quale stands in an
action?instrument relation to the target noun (murder by knife). These links thus encode
the subtype of Telic role that Pustejovsky (1995) calls ?indirect.? The two verb?noun
links (q obj n and n sbj intr q) instead capture ?direct? Telic roles, which are typically
expressed by the theme of a verb (read a book, the book reads well). The least convincing
results are those for the Agentive role, where only q obj n and perhaps q out n are
intuitively plausible canonical links. Interestingly, the manual selections we carried out
in Section 6.2.3 also gave very poor results for the Agentive role, as shown by the fact
that Table 10 reports just one link for such a role. This suggests that the problems with
this qualia role might be due to the number and type of lexicalized links used to build
the DM tensors, rather than to the selection algorithm presented here.
Coming now to the quantitative evaluation of the harvested patterns, the results in
Table 15 (to be compared to Table 11 in Section 6.2.3) are based on W1W2?L subspaces
where the non-zero dimensions correspond to the links that we picked automatically
with the method we just described (different links for each concept, because of the
leave-one-concept-out procedure). TypeDM is the best model in this setting as well.
Its performance is even better than the one (reported in Table 11) obtained with the
manually picked patterns (although the difference is not statistically significant; paired
t-test, t = 0.75, df = 29, p = 0.46), and the automated approach has more room for
improvement via parameter optimization.
We did not get as deeply into L?W1W2 space as we did with the other views, but
our preliminary results on qualia harvesting suggest at least that looking at links as
Table 14
Links selected in all folds of the leave-one-out procedure to extract links typical of each qualia
role.
FORMAL CONSTITUTIVE
n is q, q is n, q become n, n coord q, n have q, n use q, nwith q, nwithout q
q coord n, q have n, n in q, n provide q,
q such as n
AGENTIVE TELIC
q after n, q alongside n, q as n, q before n, q behind n, q by n, q like n, q obj n,
q besides n, q during n, q in n, q obj n, n sbj intr q, q through n, q via n
q out n, q over n, q since n, q unlike n
710
Baroni and Lenci Distributional Memory
Table 15
Average qualia extraction performance with automatically harvested links (compare to Table 11).
model precision recall F F s.d.
TypeDM 24.2 26.7 19.1 7.7
DepDM 18.4 27.0 15.1 4.9
LexDM 22.6 18.1 14.8 7.7
L?W1W2 vectors might be useful for feature selection in W1W2?L or for tasks in which
we are given a set of pairs, and we have to find links that can function as verbal labels
for the relation between the word pairs (Turney 2006a).
6.5 Smoothing by Tensor Decomposition
Dimensionality reduction techniques such as the (truncated) SVD approximate a sparse
co-occurrence matrix with a denser lower-rank matrix of the same size, and they have
been shown to be effective in many semantic tasks, probably because they provide
a beneficial form of smoothing of the dimensions. See Turney and Pantel (2010) for
references and discussion. We can apply SVD (or similar methods) to any of the tensor-
derivedmatrices we used for the tasks herein. An interesting alternative is to smooth the
source tensor directly by a tensor decomposition technique. In this section, we present
(very preliminary) evidence that tensor decomposition can improve performance, and
it is at least as good in this respect as matrix-based SVD. This is the only experiment
in which we operate on the tensor directly, rather than on the matrices derived from it,
paving the way to a more active role for the underlying tensor in the DM approach to
semantics.
The (truncated) Tucker decomposition of a tensor can be seen as a higher-order
generalization of SVD. Given a tensor X of dimensionality I1 ? I2 ? I3, its n-rank Rn
is the rank of the vector space spanned by its mode-n fibers (obviously, for each
mode n of the tensor, Rn ? In). Tucker decomposition approximates the tensorX having
n-ranks R1, . . . ,Rn with X? , a tensor with n-ranks Qn ? Rn for all modes n. Unlike the
case of SVD, there is no analytical procedure to find the best lower-rank approximation
to a tensor, and Tucker decomposition algorithms search for the reduced rank tensor
with the best fit (as measured by least square error) iteratively. Specifically, we use the
memory-efficient MET(1) algorithm of Kolda and Sun (2008) as implemented in the
Matlab Tensor Toolbox.10 Kolda and Bader (2009) provide details on Tucker decompo-
sition, its general properties, as well as applications and alternatives.
SVD is believed to exploit patterns of higher order co-occurrence between the rows
and columns of a matrix (Manning and Schu?tze 1999; Turney and Pantel 2010), making
row elements that co-occur with two synonymic columns more similar than in the
original space. Tucker decomposition applied to the mode-3 tuple tensor could capture
patterns of higher order co-occurrence for each of the modes. For example, it might
capture at the same time similarities between links such as use and hold and w2 elements
such as gun and knife. SVD applied after construction of the W1?LW2 matrix, on the
other hand, would miss the composite nature of columns such as ?use, gun?, ?use, knife?
and ?hold, gun?. Another attractive feature of Tucker decomposition is that it could be
10 http://csmr.ca.sandia.gov/?tgkolda/TensorToolbox/.
711
Computational Linguistics Volume 36, Number 4
Table 16
Purity in Almuhareb?Poesio concept clustering with rank reduction of the APTypeDM tensor;
95% confidence intervals (CI) obtained by bootstrapping.
reduction rank purity 95% CI
Tucker 250?50?500 75 72?80
Tucker 300?50?500 75 71?79
Tucker 300?50?450 74 71?79
SVD 200 74 71?79
SVD 350 74 70?79
Tucker 300?40?500 74 70?78
Tucker 300?60?500 74 70?78
Tucker 350?50?500 73 69?77
Tucker 300?50?550 72 69?77
SVD 250 72 69?77
SVD 150 72 68?77
none ? 402 71 69?77
SVD 300 71 68?76
SVD 100 68 65?73
SVD 50 64 61?70
applied once to smooth the source tensor, whereas with SVD each matricization must
be smoothed separately. However, Tucker decomposition and SVD are computationally
intensive procedures, and, at least with our current computational resources, we are not
able to decompose even the smallest DM tensor (similarly, we cannot apply SVD to a
full matricization). Given the continuous growth in computational power and the fact
that efficient tensor decomposition is a very active area of research (Turney 2007; Kolda
and Sun 2008) full tensor decomposition is nevertheless a realistic near future task.
For the current pilot study, we replicated the AP concept clustering experiment
described in Section 6.1.3. Because for efficiency reasons we must work with just a
portion of the original tensor, we thought that the AP data set, consisting of a relatively
large and balanced collection of nominal concepts, would offer a sensible starting point
to extract the subset. Specifically, we extract from our best tensor TypeDM the values
labeled by tuples ?wAP, l,w2?where wAP is in the AP set, l is one of the 100 most common
links occurring in tuples with a wAP, and w2 is one of the 1,000 most common words
occurring in tuples with a wAP and a l. The resulting (sub-)tensor, APTypeDM, has
dimensionality 402? 100? 1, 000 with 1,318,214 non-zero entries (density: 3%). The
W1?LW2 matricization of APTypeDM results in a 402? 1, 000, 000 matrix with 66,026
non-zero columns and the same number of non-zero entries and density as the tensor.
The possible combinations of target lower n-ranks constitute a large tridimensional
parameter space, and we leave its systematic exploration to further work. Instead, we
pick 300, 50, and 500 as (intuitively reasonable) initial target n-ranks for the threemodes,
and we explore their neighborhood in parameter space by changing one target n-rank at
a time, by a relatively small value (300? 50, 50? 10, and 500? 50, respectively). For the
parameters concerning the reduced tensor fitting process, we accept the default values
of the Tensor Toolbox. For comparison purposes, we also apply SVD to the W1?LW2
matrix derived from APTypeDM. We systematically explore the SVD target lower rank
parameter from 50 to 350 in increments of 50 units.
The results are reported in Table 16. The rank column reports the n-ranks when
reduction is performed on the tensor, and matrix ranks in the other cases. Bootstrapped
confidence intervals are obtained as described in Section 6.1.3. In general, the results
712
Baroni and Lenci Distributional Memory
confirm that smoothing by rank reduction is beneficial to semantic performance, al-
though not spectacularly so, with an improvement of about 4% for the best reduced
model with respect to the raw APTypeDM tensor (consider however also the relatively
wide confidence intervals). As a general trend, tensor-based smoothing (Tucker) does
better than matrix-based smoothing (SVD). As we said, for Tucker we only report re-
sults from a small region of the tridimensional parameter space, whereas the SVD rank
parameter range is explored coarsely but exhaustively. Thus, although other parameter
combinations might lead to dramatic changes in Tucker performance, the best SVD
performance in the table is probably close to the SVD performance upper bound.
The present pilot study suggests an attitude of cautious optimism towards tensor
decomposition as a smoothing technique. At least in the AP task, it helps as compared
to no smoothing at all. The same conclusion is reached by Turney (2007), who uses
essentially the same method (with some differences in implementation) to tackle the
TOEFL task, and obtains more than 10% improvement in accuracy with respect to the
corresponding raw tensor. At least as a trend, tensor decomposition appears to be better
than matrix decomposition, but only marginally so (Turney does not perform this com-
parison). Still, even if the tensor- and matrix-based decompositions turned out to have
comparable effects, tensor-based smoothing is more attractive in the DM framework
because we could perform the decomposition once, and use the smoothed tensor as our
stable underlying DM (modulo, of course, memory problems with computing such a
large tensor decomposition).
Beyond smoothing, tensor decomposition might provide some novel avenues for
distributional semantics, while keeping to the DM program of a single model for many
tasks. Van de Cruys (2009) used tensor decomposition to find commonalities in latent
dimensions across the fiber labels (in the DM formalism, this would amount to finding
commonalities across w1, l, and w2 elements). Another possible use for smoothing
would be to propagate ?link mass? across parts of speech. Our tensors, being based on
POS tagging and dependency parsing, have 0 values for noun-link-noun tuples such as
?city, obj, destruction? and ?city, subj tr, destruction?. In a smoothed tensor, by the influence
of tuples such as ?city, obj, destroy? and ?city, sbj tr, destroy?, these tuples will get some
non-0 weight that, hopefully, will make the object relation between city and destruction
emerge. This is at the moment just a conjecture, but it constitutes an exciting direction
for further work focusing on tensor decomposition within the DM framework.
7. Conclusion
A general framework for distributional semantics should satisfy the following two
requirements: (1) representing corpus-derived data in such a way as to capture aspects
of meaning that have so far been modeled with different, prima facie incompatible data
structures; (2) using this common representation to address a large battery of semantic
experiments, achieving a performance at least comparable to that of state-of-art, task-
specific DSMs. We can now safely claim that DM satisfies both these desiderata, and
thereby represents a genuine step forward in the quest for a general purpose approach
to distributional semantics.
DM addresses point (1) by modeling distributional data as a structure of weighted
tuples that is formalized as a labeled third-order tensor. This is a generalization with
respect to the common approach of many corpus-based semantic models (the structured
DSMs) that rely on distributional information encoded into word?link?word tuples,
associated with weights that are functions of their frequency of co-occurrence in the cor-
pus. Existing structured DSMs still couch this information directly in binary structures,
713
Computational Linguistics Volume 36, Number 4
namely, co-occurrence matrices, thereby giving rise to different semantic spaces and los-
ing sight of the fact that such spaces share the same kind of distributional information.
The third-order tensor formalization of distributional data allows DM to fully exploit
the potential of corpus-derived tuples. The four semantic spaces we analyzed and tested
in Section 6 are generated from the same underlying third-order tensor, by the standard
operation of tensor matricization. This way, we derive a set of semantic spaces that can
be used for measuring attributional similarity (finding synonyms, categorizing concepts
into superordinates, etc.) and relational similarity (finding analogies, grouping concept
pairs into relation classes, etc.). Moreover, the distributional information encoded in the
tensor and unfolded via matricization leads to further arrangements of the data useful
in addressing semantic problems that do not fall straightforwardly into the attributional
or the relational paradigm (grouping verbs by alternations, harvesting patterns that
represent a relation). In some cases, it is obvious how to reformulate a semantic problem
in the new framework. Other tasks can be reframed in terms of our four semantic
spaces using geometric operations such as centroid computations and projection onto
a subspace. This was the case for selectional preferences, pattern- and example-based
relation extraction (illustrated by qualia harvesting), and the task of generating typical
properties of concepts. We consider a further strength of the DM approach that it natu-
rally encourages us to think, as we did in these cases, of ways to tackle apparently
unrelated tasks with the existing resources, rather than devising unrelated approaches
to deal with them.
Regarding point (2), that is, addressing a large battery of semantic experiments with
good performance, in nearly all test sets our best implementation of DM (TypeDM) is
at least as good as other algorithms reported in recently published papers (typically
developed or tuned for the task at hand), often towards (or at) the top of the state-of-
the-art ranking. Where other models outperform TypeDM by a large margin, there are
typically obvious reasons for this: The rivals have been trained on much larger corpora,
or they rely on special knowledge resources, or on sophisticated machine learning
algorithms. Importantly, TypeDM is consistently at least as good (or better than) those
models we reimplemented to be fully comparable to our DMs (i.e., Win, DV, LRA).
Moreover, the best DM implementation does not depend on the semantic space:
TypeDM outperforms (at least in terms of average performance across tasks) the other
two models in all four spaces. This is not surprising (better distributional tuples should
still be better when seen from different views), but it is good to have an empirical con-
firmation of the a priori intuition. The current results suggest that one could, for exam-
ple, compare alternative DMs on a few attributional tasks, and expect the best DM in
these tasks to also be the best in relational tasks and other semantic challenges.
The final experiment of Section 6 briefly explored an interesting aspect of the
tensor-based formalism, namely, the possibility of improving performance on some
tasks by working directly on the tensor (in this case, applying tensor rank reduction
for smoothing purposes) rather than on the matrices derived from it. Besides this pilot
study, we did not carry out any task-specific optimization of TypeDM, which achieves
its very good performance using exactly the same underlying parameter configuration
(e.g., dependency paths, weighting function) across the different spaces and tasks.
Parameter tuning is an important aspect in DSM development, with an often dramatic
impact of parameter variation (Bullinaria and Levy 2007; Erk and Pado? 2009). We
leave the exploration of parameter space in DM for future research. Its importance not-
withstanding, however, we regard this as a rather secondary aspect, if compared with
the good performance of a DM model (even in its current implementation) in the large
and multifarious set of tasks we presented.
714
Baroni and Lenci Distributional Memory
Of course, many issues are still open. It is one thing to claim that the models that
outperform TypeDMdo so because they rely on larger corpora; it is another to show that
TypeDM trained on more data does reach the top of the current heap. The differences
between TypeDM and the other, generally worse-performing DM models remind us
that the idea of a shared distributional memory per se is not enough to obtain good
results, and the extraction of an ideal DM from the corpus certainly demands further
attention. We need to reach a better understanding of which pieces of distributional
information to extract, and whether different semantic tasks require focusing on specific
subsets of distributional data. Another issue we completely ignored but which will be of
fundamental importance in applications is how a DM-based system can deal with out-
of-vocabulary items. Ideally, we would like a seamless way to integrate new terms in
the model incrementally, based on just a few extra data points, but we leave it to further
research to study how this could be accomplished, together with the undoubtedly many
further practical and theoretical problems that will emerge. We will conclude, instead,
by discussing some general advantages that follow from the DM approach of separating
corpus-based model building, the multi-purpose long term distributional memory, and
different views of the memory data to accomplish different semantic tasks, without
resorting to the source corpus again.
First of all, we would like to make a more general point regarding parameter
tuning and task-specific optimization, by going back to the analogy with WordNet as a
semantic multi-purpose resource. If you want to improve performance of a WordNet-
based system, you will probably not wait for its next release, but rather improve the
algorithms that work on the existing WordNet graph. Similarly, in the DM approach we
propose that corpus-based resources for distributional semantics should be relatively
stable, multi-purpose, large-scale databases (in the form of weighted tuple structures),
only occasionally updated (because a better or larger corpus becomes available, a better
parser, etc.). Still, given the same underlying DM and a certain task, much work can be
done to exploit the DM optimally in the task, with no need to go back to corpus-based
resource construction. For example, performance on attributional tasks could be raised
by dimension reweighting techniques such as recently proposed by Zhitomirsky-Geffet
and Dagan (2009). For the problem of data sparseness in the W1W2?L space, we could
treat the tensor as a graph and explore random walks and other graphical approaches
that have been shown to ?scale down? gracefully to capture relations in sparser data
sets (Minkov and Cohen 2007, 2008). As in our simple example of smoothing relational
pairs with attributional neighbors, more complex tasks may be tackled by combining
different views of DM, and/or resorting to different (sub)spaces within the same view,
as in our approach to selectional preferences. One might even foresee an algorithmic
way to mix and match the spaces as most appropriate to a certain task. We propose a
similar split for the role of supervision in DSMs. Construction of the DM tensor from the
corpus is most naturally framed as an unsupervised task, because the model will serve
many different purposes. On the other hand, supervision can be of great help in tuning
the DM data to specific tasks (as we did, in a rather naive way, with the nearest centroid
approach to most non-attributional tasks). A crucial challenge for DSMs is whether
and how corpus-derived vectors can also be used in the construction of meaning for
constituents larger than words. These are the traditional domains of formal semantics,
which is most interested in how the logical representation of a sentence or a discourse
is built compositionally by combining the meanings of its constituents. DSMs have so
far focused on representing lexical meaning, and compositional and logical issues have
either remained out of the picture, or have received still unsatisfactory accounts. A gen-
eral consensus exists on the need to overcome this limitation, and to build new bridges
715
Computational Linguistics Volume 36, Number 4
between corpus-based semantics and symbolic models of meanings (Clark and Pulman
2007; Widdows 2008). Most problems encountered by DSMs in tackling this challenge
are specific instances of more general issues concerning the possibility of representing
symbolic operations with distributed, vector-based data structures (Markman 1999).
Many avenues are currently being explored in corpus-based semantics, and interesting
synergies are emerging with research areas such as neural systems (Smolensky 1990;
Smolensky and Legendre 2006), quantum information (Widdows and Peters 2003; Aerts
and Czachor 2004; Widdows 2004; Van Rijsbergen 2004; Bruza and Cole 2005; Hou
and Song 2009), holographic models of memory (Jones and Mewhort 2007), and so
on. A core problem in dealing with compositionality with DSMs is to account for the
role of syntactic information in determining the way semantic representations are built
from lexical items. For instance, the semantic representation assigned to The dog bites
the man must be different from the one assigned to The man bites the dog, even if they
contain exactly the same lexical items. Although it is still unclear which is the best
way to compose the representation of content words in vector spaces, it is nowadays
widely assumed that structured representations like those adopted by DM are in the
right direction towards a solution to this issue, exactly because they allow distributional
representations to become sensitive to syntactic structures (Erk and Pado? 2008). Compo-
sitionality and similar issues in DSMs lie beyond the scope of this paper. However, there
is nothing in DM that prevents it from interacting with any of the research directions we
have mentioned here. Indeed, we believe that the generalized nature of DM represents
a precondition for distributional semantics to be able to satisfactorily address these
more advanced challenges. A multi-purpose, distributional semantic resource like DM
can allow researchers to focus on the next steps of semantic modeling. These include
compositionality, but also modulating word meaning in context (Erk and Pado? 2008;
Mitchell and Lapata 2008) and finding ways to embed the distributional memory in
complex NLP systems (e.g., for question answering or textual entailment) or even
embodied agents and robots.
DM-style triples predicating a relation between two entities are common currency
in many semantic representation models (e.g., semantic networks) and knowledge-
exchange formalisms such as RDF. This might also pave the way to the integration of
corpus-based information with other knowledge sources. It is hard to see how such
integration could be pursued within generalized systems, such as PairClass (Turney
2008), that require keeping a full corpus around and corpus-processing know-how on
behalf of interested researchers from outside the NLP community (see discussion in
Section 4 above). Similarly, the DM triples might help in fostering the dialogue between
computational linguists and the computational neuro-cognitive community, where it is
common to adopt triple-based representations of knowledge, and to use the same set of
tuples to simulate various aspects of cognition. For a recent extended example of this
approach, see Rogers and McClelland (2004). It would be relatively easy to use a DM
model in lieu of their neural network, and use it to simulate the conceptual processes
they reproduce.
DM, unlike classic DSM models that go directly from the corpus data to solving
specific semantic tasks, introduces a clear distinction between an acquisition phase
(corpus-based tuple extraction and weighting), the declarative structure at the core of
semantic modeling (the distributional memory), and the procedural problem-solving
components (possibly supervised procedures to perform different semantic tasks). This
separation is in line with what is commonly assumed in cognitive science and formal
linguistics, and we hope it will contribute to make corpus-based modeling a core part
of the ongoing study of semantic knowledge in humans and machines.
716
Baroni and Lenci Distributional Memory
Acknowledgments
We thank Abdulrahman Almuhareb,
Philipp Cimiano, George Karypis,
Tamara Kolda, Thomas Landauer,
Mirella Lapata, Ken McRae, Brian Murphy,
Vivi Nastase, Diarmuid O? Se?aghdha,
Sebastian and Ulrike Pado?, Suzanne
Stevenson, Peter Turney, their colleagues,
and the SEMEVAL Task 4 organizers for
data and tools. We thank Gemma Boleda,
Phillipp Cimiano, Katrin Erk, Stefan Evert,
Brian Murphy, Massimo Poesio, Magnus
Sahlgren, Tim Van de Cruys, Peter Turney,
and three anonymous reviewers for a
mixture of advice, clarification, and ideas.
References
Aerts, Diederik and Marek Czachor. 2004.
Quantum aspects of semantic analysis and
symbolic artificial intelligence. Journal of
Physics A: Mathematical and General,
37:123?132.
Alishahi, Afra and Suzanne Stevenson. 2008.
A distributional account of the semantics
of multiword expressions. Italian Journal of
Linguistics, 20(1):157?179.
Almuhareb, Abdulrahman. 2006. Attributes
in Lexical Acquisition. Ph.D. thesis,
University of Essex.
Almuhareb, Abdulrahman and Massimo
Poesio. 2004. Attribute-based and
value-based clustering: An evaluation.
In Proceedings of EMNLP, pages 158?165,
Barcelona.
Almuhareb, Abdulrahman and Massimo
Poesio. 2005. Concept learning and
categorization from the web. In Proceedings
of CogSci, pages 103?108, Stresa.
Baldwin, Timothy, Valia Kordoni, and
Aline Villavicencio. 2009. Prepositions in
applications: A survey and introduction to
the special issue. Computational Linguistics,
35(2):119?149.
Baroni, Marco, Eduard Barbu, Brian Murphy,
and Massimo Poesio. 2010. Strudel: A
distributional semantic model based on
properties and types. Cognitive Science,
34(2):222?254.
Baroni, Marco, Stefan Evert, and
Alessandro Lenci, editors. 2008. Bridging
the Gap between Semantic Theory and
Computational Simulations: Proceedings
of the ESSLLI Workshop on Distributional
Lexical Semantic. FOLLI, Hamburg.
Baroni, Marco and Alessandro Lenci. 2008.
Concepts and properties in word spaces.
Italian Journal of Linguistics, 20(1):55?88.
Baroni, Marco and Alessandro Lenci. 2009.
One distributional memory, many
semantic tasks. In Proceedings of the EACL
GEMS Workshop, pages 1?8, Athens.
Bicic?i, Ergun and Deniz Yuret. 2006.
Clustering word pairs to answer analogy
questions. In Proceedings of the Fifteenth
Turkish Symposium on Artificial Intelligence
and Neural Networks, pages 277?284,
Mug?la.
Bruza, Peter and Richard Cole. 2005.
Quantum logic of semantic space:
An exploratory investigation of context
effects in practical reasoning. In
Sergei Artemov, Howard Barringer,
Arthur d?Avila Garcez, Luis C. Lamb,
and John Woods, editors,We Will Show
Them: Essays in Honour of Dov Gabbay,
volume one. College Publications, London,
pages 339?361.
Buitelaar, Paul, Philipp Cimiano, and
Bernardo Magnini. 2005. Ontology
Learning from Text. IOS Press, Amsterdam.
Bullinaria, John and Joseph Levy. 2007.
Extracting semantic representations
from word co-occurrence statistics: A
computational study. Behavior Research
Methods, 39:510?526.
Chen, Hsin-Hsi, Ming-Shun Lin, and
Yu-Chuan Wei. 2006. Novel association
measures using Web search with double
checking. In Proceedings of COLING-ACL,
pages 1009?1016, Sydney.
Church, Kenneth and Peter Hanks. 1990.
Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Cimiano, Philipp and Johanna Wenderoth.
2007. Automatic acquisition of ranked
qualia structures from the Web. In
Proceedings of ACL, pages 888?895,
Prague.
Clark, Stephen and Stephen Pulman. 2007.
Combining symbolic and distributional
models of meaning. In Proceedings of the
AAAI Spring Symposium on Quantum
Interaction, pages 52?55, Stanford, CA.
Curran, James and Marc Moens. 2002.
Improvements in automatic thesaurus
extraction. In Proceedings of the ACL
Workshop on Unsupervised Lexical
Acquisition, pages 59?66, Philadelphia, PA.
Davidov, Dmitry and Ari Rappoport. 2008a.
Classification of semantic relationships
between nominals using pattern clusters.
In Proceedings of ACL, pages 227?235,
Columbus, OH.
Davidov, Dmitry and Ari Rappoport.
2008b. Unsupervised discovery of
717
Computational Linguistics Volume 36, Number 4
generic relationships using pattern
clusters and its evaluation by
automatically generated SAT analogy
questions. In Proceedings of ACL,
pages 692?700, Columbus, OH.
Dietterich, Thomas. 1998. Approximate
statistical tests for comparing supervised
classification learning algorithms. Neural
Computation, 10(7):1895?1924.
Dowty, David. 1977.Word Meaning and
Montague Grammar. Kluwer, Dordrecht.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Efron, Bradley and Robert Tibshirani. 1994.
An Introduction to the Bootstrap. Chapman
and Hall, Boca Raton, FL.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences.
In Proceedings of ACL, pages 216?223,
Prague.
Erk, Katrin and Sebastian Pado?. 2008. A
structured vector space model for word
meaning in context. In Proceedings of
EMNLP, pages 897?906, Honolulu, HI.
Erk, Katrin and Sebastian Pado?. 2009.
Paraphrase assessment in structured
vector space: Exploring parameters and
datasets. In Proceedings of the EACL
GEMS Workshop, pages 57?65, Athens.
Evert, Stefan. 2005. The Statistics of Word
Cooccurrences. Ph.D. dissertation,
Stuttgart University.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Garrard, Peter, Matthew Lambon Ralph,
John Hodges, and Karalyn Patterson.
2001. Prototypicality, distinctiveness,
and intercorrelation: Analyses of the
semantic attributes of living and nonliving
concepts. Cognitive Neuropsychology,
18(2):25?174.
Geeraerts, Dirk. 2010. Theories of Lexical
Semantics. Oxford University Press,
Oxford.
Girju, Roxana, Adriana Badulescu, and
Dan Moldovan. 2006. Automatic discovery
of part-whole relations. Computational
Linguistics, 32(1):83?135.
Girju, Roxana, Preslav Nakov, Vivi Nastase,
Stan Szpakowicz, Peter Turney, and
Deniz Yuret. 2007. SemEval-2007 task 04:
Classification of semantic relations
between nominals. In Proceedings of
SemEval 2007, pages 13?18, Prague.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer,
Boston, MA.
Griffiths, Tom, Mark Steyvers, and Josh
Tenenbaum. 2007. Topics in semantic
representation. Psychological Review,
114:211?244.
Harris, Zellig. 1954. Distributional structure.
Word, 10(2-3):1456?1162.
Hearst, Marti. 1992. Automatic acquisition
of hyponyms from large text corpora. In
Proceedings of COLING, pages 539?545,
Nantes.
Hearst, Marti. 1998. Automated discovery
of WordNet relations. In Christiane
Fellbaum, editor,WordNet: An Electronic
Lexical Database. MIT Press, Cambridge,
MA, pages 131?151.
Herdag?delen, Amac? and Marco Baroni.
2009. BagPack: A general framework to
represent semantic relations. In Proceedings
of the EACL GEMS Workshop, pages 33?40,
Athens.
Herdag?delen, Amac?, Katrin Erk, and
Marco Baroni. 2009. Measuring semantic
relatedness with vector space models
and random walks. In Proceedings of
TextGraphs-4, pages 50?53, Singapore.
Heylen, Kris, Yves Peirsman, Dirk Geeraerts,
and Dirk Speelman. 2008. Modelling word
similarity: An evaluation of automatic
synonymy extraction algorithms. In
Proceedings of LREC, pages 3243?3249,
Marrakech.
Hou, Yuexian and Dawei Song. 2009.
Characterizing pure high-order
entanglements in lexical semantic
spaces via information geometry.
In Peter Bruza, Donald Sofge, William
Lawless, and C. J. van Rijsbergen, editors,
Quantum Interaction: Third International
Symposium, QI 2009. Springer, Berlin,
pages 237?250.
Jackendoff, Ray. 1990. Semantic Structures.
MIT Press, Cambridge, MA.
Joanis, Eric, Suzanne Stevenson, and David
James. 2008. A general feature space for
automatic verb classification. Natural
Language Engineering, 14(3):337?367.
Jones, Michael and Douglas Mewhort.
2007. Representing word meaning
and order information in a composite
holographic lexicon. Psychological
Review, 114:1?37.
Karypis, George. 2003. CLUTO: A clustering
toolkit. Technical Report 02-017,
University of Minnesota Department
of Computer Science, Minneapolis.
Kilgarriff, Adam, Pavel Rychly, Pavel Smrz,
and David Tugwell. 2004. The Sketch
Engine. In Proceedings of Euralex,
pages 105?116, Lorient.
718
Baroni and Lenci Distributional Memory
Kolda, Tamara. 2006. Multilinear operators
for higher-order decompositions. Technical
Report 2081, SANDIA, Albuquerque, NM.
Kolda, Tamara and Brett Bader. 2009. Tensor
decompositions and applications. SIAM
Review, 51(3):455?500.
Kolda, Tamara and Jimeng Sun. 2008.
Scalable tensor decompositions for
multi-aspect data mining. In Proceedings
of ICDM, pages 94?101, Pisa.
Landauer, Thomas and Susan Dumais. 1997.
A solution to Plato?s problem: The latent
semantic analysis theory of acquisition,
induction, and representation of
knowledge. Psychological Review,
104(2):211?240.
Lenci, Alessandro. 2008. Distributional
approaches in linguistic and cognitive
research. Italian Journal of Linguistics,
20(1):1?31.
Lenci, Alessandro. 2010. The life cycle
of knowledge. In Chu-Ren Huang,
Nicoletta Calzolari, Aldo Gangemi,
Alessandro Lenci, Alessandro Oltramari,
and Laurent Pre?vot, editors, Ontology
and the Lexicon. A Natural Language
Processing Perspective. Cambridge
University Press, Cambridge, UK,
pages 241?257.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago, IL.
Levin, Beth and Malka Rappaport-Hovav.
2005. Argument Realization. Cambridge
University Press, Cambridge, UK.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words. In
Proceedings of COLING-ACL,
pages 768?774, Montreal.
Lin, Dekang. 1998b. An information-theoretic
definition of similarity. In Proceedings of
ICML, pages 296?304, Madison, WI.
Liu, Hugo and Push Singh. 2004.
ConceptNet: A practical commonsense
reasoning toolkit. BT Technology Journal,
pages 211?226.
Lowe, Will. 2001. Towards a theory of
semantic space. In Proceedings of CogSci,
pages 576?581, Edinburgh, UK.
Lund, Kevin and Curt Burgess. 1996.
Producing high-dimensional semantic
spaces from lexical co-occurrence.
Behavior Research Methods, 28:203?208.
Manning, Chris and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language
Processing. MIT Press, Cambridge, MA.
Markman, Arthur B. 1999. Knowledge
Representation. Psychology Press,
New York, NY.
Matveeva, Irina, Gina-Anne Levow,
Ayman Farahat, and Christian Royer. 2005.
Generalized latent semantic analysis for
term representation. In Proceedings of
RANLP, pages 60?68, Borovets.
McRae, Ken, George Cree, Mark Seidenberg,
and Chris McNorgan. 2005. Semantic
feature production norms for a large set
of living and nonliving things. Behavior
Research Methods, 37(4):547?559.
McRae, Ken, Michael Spivey-Knowlton,
and Michael Tanenhaus. 1998. Modeling
the influence of thematic fit (and
other constraints) in on-line sentence
comprehension. Journal of Memory and
Language, 38:283?312.
Merlo, Paola and Suzanne Stevenson. 2001.
Automatic verb classification based on
statistical distributions of argument
structure. Computational Linguistics,
27(3):373?408.
Meyer, Carl. 2000.Matrix Analysis and Applied
Linear Algebra. SIAM, Philadelphia, PA.
Miller, George and Walter Charles. 1991.
Contextual correlates of semantic
similarity. Language and Cognitive
Processes, 6:1?28.
Minkov, Einat and William Cohen. 2007.
Learning to rank typed graph walks:
Local and global approaches. In
Proceedings of WebKDD/SNA-KDD,
pages 1?8, San Jose?, CA.
Minkov, Einat and William Cohen. 2008.
Learning graph walk based similarity
measures for parsed text. In Proceedings
of EMNLP, pages 907?916, Honolulu, HI.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL,
pages 236?244, Columbus, OH.
Murphy, Gregory. 2002. The Big Book of
Concepts. MIT Press, Cambridge, MA.
Nastase, Vivi and Stan Szpakowicz. 2003.
Exploring noun-modifier semantic
relations. In Proceedings of the Fifth
International Workshop on Computational
Semantics, pages 285?301, Tilburg,
The Netherlands.
O? Se?aghdha, Diarmuid and Ann Copestake.
2009. Using lexical and relational
similarity to classify semantic relations.
In Proceedings of EACL, pages 621?629,
Athens.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pado?, Ulrike. 2007. The Integration of
Syntax and Semantic Plausibility in a
719
Computational Linguistics Volume 36, Number 4
Wide-Coverage Model of Sentence Processing.
Ph.D. dissertation, Saarland University,
Saarbru?cken.
Pado?, Ulrike, Sebastian Pado?, and Katrin Erk.
2007. Flexible, corpus-based modelling
of human plausibility judgements. In
Proceedings of EMNLP, pages 400?409,
Prague.
Pantel, Patrick and Marco Pennacchiotti.
2006. Espresso: Leveraging generic
patterns for automatically harvesting
semantic relations. In Proceedings of
COLING-ACL, pages 113?120, Sydney.
Peirsman, Yves and Dirk Speelman. 2009.
Word space models of lexical variation.
In Proceedings of the EACL GEMS
Workshop, pages 9?16, Athens.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Quesada, Jose, Praful Mangalath, and
Walter Kintsch. 2004. Analogy-making
as predication using relational information
and LSA vectors. In Proceedings of CogSci,
page 1623, Chicago, IL.
Raghunathan, Trivellore. 2003. An
approximate test for homogeneity of
correlated correlation coefficients.
Quality & Quantity, 37:99?110.
Rapp, Reinhard. 2003. Word sense discovery
based on sense descriptor dissimilarity.
In Proceedings of the 9th MT Summit,
pages 315?322, New Orleans, LA.
Rapp, Reinhard. 2004. A freely available
automatically generated thesaurus of
related words. In Proceedings of LREC,
pages 395?398, Lisbon.
Rogers, Timothy and James McClelland.
2004. Semantic Cognition: A Parallel
Distributed Processing Approach. MIT Press,
Cambridge, MA.
Rothenha?usler, Klaus and Hinrich Schu?tze.
2009. Unsupervised classification with
dependency based word spaces. In
Proceedings of the EACL GEMS Workshop,
pages 17?24, Athens, Greece.
Rubenstein, Herbert and John Goodenough.
1965. Contextual correlates of synonymy.
Communications of the ACM, 8(10):627?633.
Ruiz-Casado, Maria, Enrique Alfonseca,
and Pablo Castells. 2005. Using
context-window overlapping in synonym
discovery and ontology extension. In
Proceedings of RANLP, pages 1?7, Borovets.
Sagi, Eyal, Stefan Kaufmann, and Brady
Clark. 2009. Semantic density analysis:
Comparing word meaning across time
and phonetic space. In Proceedings of the
EACL GEMS Workshop, pages 104?111,
Athens.
Sahlgren, Magnus. 2005. An introduction to
random indexing. http://www.sics.se/
?mange/papers/RI intro.pdf.
Sahlgren, Magnus. 2006. The Word-Space
Model. Ph.D. dissertation, Stockholm
University.
Schulte im Walde, Sabine. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32:159?194.
Schu?tze, Hinrich. 1997. Ambiguity Resolution
in Natural Language Learning. CSLI
Publications, Stanford, CA.
Smolensky, Paul. 1990. Tensor product
variable binding and the representation
of symbolic structures in connectionist
systems. Artificial Intelligence, 46:159?216.
Smolensky, Paul and Geraldine Legendre.
2006. The Harmonic Mind. From Neural
Computation to Optimality-theoretic
Grammar. MIT Press, Cambridge, MA.
Terra, Egidio and Charles Clarke. 2003.
Frequency estimates for statistical word
similarity measures. In Proceedings of
HLT-NAACL, pages 244?251, Edmonton.
Turney, Peter. 2001. Mining the Web for
synonyms: PMI-IR versus LSA on TOEFL.
In Proceedings of ECML, pages 491?502,
Freiburg.
Turney, Peter. 2006a. Expressing implicit
semantic relations without supervision.
In Proceedings of COLING-ACL,
pages 313?320, Sydney.
Turney, Peter. 2006b. Similarity of semantic
relations. Computational Linguistics,
32(3):379?416.
Turney, Peter. 2007. Empirical evaluation
of four tensor decomposition algorithms.
Technical Report ERB-1152, NRC,
Ottawa.
Turney, Peter. 2008. A uniform approach to
analogies, synonyms, antonyms and
associations. In Proceedings of COLING,
pages 905?912, Manchester.
Turney, Peter and Michael Littman. 2005.
Corpus-based learning of analogies and
semantic relations.Machine Learning,
60(1-3):251?278.
Turney, Peter and Patrick Pantel. 2010. From
frequency to meaning: Vector space
models of semantics. Journal of Artificial
Intelligence Research, 37:141?188.
Van de Cruys, Tim. 2009. A non-negative
tensor factorization model for selectional
preference induction. In Proceedings of the
EACL GEMS Workshop, pages 83?90,
Athens.
Van Overschelde, James, Katherine Rawson,
and John Dunlosky. 2004. Category
720
Baroni and Lenci Distributional Memory
norms: An updated and expanded
version of the Battig and Montague (1969)
norms. Journal of Memory and Language,
50:289?335.
Van Rijsbergen, C. J. 2004. The Geometry of
Information Retrieval. Cambridge
University Press, Cambridge, UK.
Veale, Tony and Yanfen Hao. 2008.
Acquiring naturalistic concept
descriptions from the Web. In
Proceedings of LREC, pages 1121?1124,
Marrakech.
Vinson, David and Gabriella Vigliocco. 2008.
Semantic feature production norms for a
large set of objects and events. Behavior
Research Methods, 40(1):183?190.
Widdows, Dominic. 2004. Geometry and
Meaning. CSLI Publications, Stanford, CA.
Widdows, Dominic. 2008. Semantic vector
products: Some initial investigations.
In Proceedings of the Second AAAI
Symposium on Quantum Interaction,
pages 1?8, Oxford.
Widdows, Dominic and Beate Dorow.
2002. A graph model for unsupervised
lexical acquisition. In Proceedings of
ICCL, pages 1?7, Taipei.
Widdows, Dominic and Stanley Peters.
2003. Word vectors and quantum logic.
In Proceedings of the Eighth Mathematics
of Language Conference, pages 1?14,
Bloomington, IN.
Zarcone, Alessandra and Alessandro Lenci.
2008. Computational models of event type
classification in context. In Proceedings of
LREC, pages 1232?1238, Marrakech.
Zhao, Ying and George Karypis. 2003.
Criterion functions for document
clustering: Experiments and analysis.
Technical Report 01-40, University of
Minnesota Department of Computer
Science, Minneapolis.
Zhitomirsky-Geffet, Maayan and Ido Dagan.
2009. Bootstrapping distributional feature
vector quality. Computational Linguistics,
35(3):435?461.
721

First Joint Conference on Lexical and Computational Semantics (*SEM), pages 75?79,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Identifying hypernyms in distributional semantic spacesAlessandro Lenci
University of Pisa, Dept. of Linguistics
via S. Maria 36
I-56126, Pisa, Italyalessandro.lenci@ling.unipi.it Giulia BenottoUniversity of Pisa, Dept. of Linguisticsvia S. Maria 36I-56126, Pisa, Italymezzanine.g@gmail.comAbstract
In this paper we apply existing directional
similarity measures to identify hypernyms
with a state-of-the-art distributional semantic
model. We also propose a new directional
measure that achieves the best performance in
hypernym identification.1 Introduction and related works
Distributional Semantic Models (DSMs) measure
the semantic similarity between words with proxim-
ity in distributional space. However, semantically
similar words in turn differ for the type of relation
holding between them: e.g., dog is strongly similar
to both animal and cat, but with different types of re-
lations. Current DSMs accounts for these facts only
partially. While they may correctly place both ani-mal and cat among the nearest distributional neigh-
bors of dog, they are not able to characterize the
different semantic properties of these relations, for
instance the fact that hypernymy is an asymmetric
semantic relation, since being a dog entails being an
animal, but not the other way round.
The purpose of this paper is to explore the possi-
bility of identifying hypernyms in DSMs with direc-tional (or asymmetric) similarity measures (Kotler-
man et al, 2010). These measures all rely on some
variation of the Distributional Inclusion Hypothe-sis, according to which if u is a semantically nar-
rower term than v, then a significant number of
salient distributional features of u is included in the
feature vector of v as well. Since hypernymy is
an asymmetric relation and hypernyms are seman-
tically broader terms than their hyponyms, then we
can predict that directional similarity measures are
better suited to identify terms related by the hyper-
nymy relation.
Automatic identification of hypernyms in corpora
is a long-standing research line, but most meth-
ods have adopted semi-supervised, pattern-based ap-
proaches (Hearst, 1992; Pantel and Pennacchiotti,
2006). Fully unsupervised hypernym identification
with DSMs is still a largely open field. Various mod-
els to represent hypernyms in vector spaces have
recently been proposed (Weeds and Weir, 2003;
Weeds et al, 2004; Clarke, 2009), usually grounded
on the Distributional Inclusion Hypothesis (for a dif-
ferent approach based on representing word mean-
ing as ?regions? in vector space, see Erk (2009a;
2009b)). The same hypothesis has been adopted by
Kotlerman et al (2010) to identify (substitutable)
lexical entailments? . Within the context of the Tex-
tual Entailment (TE) paradigm, Zhitomirsky-Geffet
and Dagan (2005; 2009) define (substitutable) lex-ical entailment as a relation holding between two
words, if there are some contexts in which one of
the words can be substituted by the other and the
meaning of the original word can be inferred from
the new one. Its relevance for TE notwithstanding,
this notion of lexical entailment is more general and
looser than hypernymy. In fact, it encompasses sev-
eral standard semantic relations such as synonymy,
hypernymy, metonymy, some cases of meronymy,
etc.
Differently from Kotlerman et al (2010), here we
focus on applying directional, asymmetric similar-
ity measures to identify hypernyms. We assume the
classical definition of a hypernymy, such that Y is
75
an hypernym of X if and only if X is a kind of Y ,
or equivalently every X is a Y .2 Directional similarity measures
In the experiments reported in section 3 we have ap-
plied the following directional similarity measures
(Fx is the set of distributional features of a term x,
wx(f) is the weight of the feature f for x):WeedsPrec (M1) - this is a measure that quantifies
the weighted inclusion of the features of a term u
within the features of a term v (Weeds and Weir,
2003; Weeds et al, 2004; Kotlerman et al, 2010):
WeedsPrec(u, v) =
Pf2Fu\Fv wu(f)Pf2Fu wu(f) (1)cosWeeds (M2) - this measure corresponds to the
geometrical average of WeedsPrec and the symmet-
ric similarity between u and v, measured by their
vectors? cosine:
cosWeeds(u, v) =
q
M1(u, v) ? cos(u, v) (2)
This is actually a variation of the balPrec measure
in Kotlerman et al (2010), the difference being that
cosine is used as a symmetric similarity measure
instead of the LIN measure (Lin, 1998).ClarkeDE (M3) - a close variation of M1,
proposed by Clarke (2009):
ClarkeDE(u, v) =
Pf2Fu\Fv min(wu(f), wv(f))Pf2Fu wu(f)
(3)invCL (M4) - this a new measure that we introduce
and test here for the first time. It takes into account
not only the inclusion of u in v, but also the non-inclusion of v in u, both measured with ClarkeDE:
invCL(u, v) =
q
M3(u, v) ? (1   M3(v, u))
(4)
The intuition behind invCL is that, if v is a seman-
tically broader term of u, then the features of u are
included in the features of v, but crucially the fea-
tures of v are also not included in the features of
u. For instance, if animal is a hypernym of lion,
we can expect i.) that a significant number of thelion-contexts are also animal-contexts, and ii.) that
a significant number of animal-contexts are not lion-
contexts. In fact, being a semantically broader term
of lion, animal should also be found in contexts in
which animals other than lions occur.3 Experiments
The main purpose of the experiments reported below
is to investigate the ability of the directional similar-
ity measures presented in section 2 to identify the
hypernyms of a given target noun, and to discrim-
inate hypernyms from terms related by symmetric
semantic relations, such as coordinate terms.
We have represented lexical items with distribu-
tional feature vectors extracted from the TypeDM
tensor (Baroni and Lenci, 2010). TypeDM is a par-
ticular instantiation of the Distributional Memory
(DM) framework. In DM, distributional facts are
represented as a weighted tuple structure T , a set
of weighted word-link-word tuples hhw1, l, w2i, i,
such that w1 and w2 are content words (e.g. nouns,
verbs, etc.), l is a syntagmatic co-occurrence links
between words in a text (e.g. syntactic dependen-
cies, etc.), and   is a weight estimating the statis-
tical salience of that tuple. The TypeDM word set
contains 30,693 lemmas (20,410 nouns, 5,026 verbs
and 5,257 adjectives). The TypeDM link set con-
tains 25,336 direct and inverse links formed by (par-
tially lexicalized) syntactic dependencies and pat-
terns. The weight   is the Local Mutual Informa-tion (LMI) (Evert, 2005) computed on link type fre-
quency (negative LMI values are raised to 0).3.1 Test set
We have evaluated the directional similarity mea-
sures on a subset of the BLESS data set (Baroni and
Lenci, 2011), consisting of tuples expressing a re-lation between a target concept (henceforth referred
to as concept) and a relatum concept (henceforth re-
ferred to as relatum). BLESS includes 200 distinct
English concrete nouns as target concepts, equally
divided between living and non-living entities, and
grouped into 17 broader classes (e.g., BIRD, FRUIT,
FURNITURE, VEHICLE, etc.).
For each concept noun, BLESS includes several
76
relatum words, linked to the concept by one of 5 se-
mantic relations. Here, we have used the BLESS
subset formed by 14,547 tuples with the relatum
attested in the TypeDM word set, and containing
one of these relations: COORD: the relatum is a
noun that is a co-hyponym (coordinate) of the con-
cept: halligator, coord, lizardi; HYPER: the rela-
tum is a noun that is a hypernym of the concept:
halligator, hyper, animali; MERO: the relatum is
a noun referring to a part/component/organ/member
of the concept, or something that the concept con-
tains or is made of: halligator,mero,mouthi;
RANDOM-N: the relatum is a random noun hold-
ing no semantic relation with the target concept:
halligator, random   n,messagei.
Kotlerman et al (2010) evaluate a set of
directional similarity measure on a data set of
valid and invalid (substitutable) lexical entailments
(Zhitomirsky-Geffet and Dagan, 2009). However,
as we said above, lexical entailment is defined as
an asymmetric relation that covers various types of
classic semantic relations, besides hypernymy . The
choice of BLESS is instead motivated by the fact
that here we focus on the ability of directional simi-
larity measure to identify hypernyms.3.2 Evaluation and results
For each word x in the test set, we represented
x in terms of a set Fx of distributional features
hl, w2i, such that in the TypeDM tensor there is a
tuple hhw1, l, w2i, i, w1 = x. The feature weight
wx(f) is equal to the weight   of the original DM
tuple. Then, we applied the 4 directional simi-
larity measures in section 2 to BLESS, with the
goal of evaluating their ability to discriminate hy-
pernyms from other semantic relations, in particular
co-hyponymy. In fact, differently from hypernyms,
coordinate terms are not related by inclusion. There-
fore, we want to test whether directional similarity
measures are able to assign higher scores to hyper-
nyms, as predicted by the Distributional Inclusion
Hypothesis. We used the Cosine as our baseline,
since it is a symmetric similarity measure and it is
commonly used in DSMs.
We adopt two different evaluation methods. The
first is based on the methodology described in Ba-
roni and Lenci (2011). Given the similarity scores
for a concept with all its relata across all relations
in our test set, we pick the relatum with the high-
est score (nearest neighbour) for each relation. In
this way, for each of the 200 BLESS concepts, we
obtain 4 similarity scores, one per relation. In or-
der to factor out concept-specific effects that might
add to the overall score variance, we transform the
8 similarity scores of each concept onto standard-
ized z scores (mean: 0; s.d: 1) by subtracting from
each their mean, and dividing by their standard devi-
ation. After this transformation, we produce a box-plot summarizing the distribution of scores per rela-
tion across the 200 concepts.
Boxplots for each similarity measure are reported
in Figure 1. They display the median of a distribu-
tion as a thick horizontal line within a box extending
from the first to the third quartile, with whiskers cov-
ering 1.5 of the interquartile range in each direction
from the box, and values outside this extended range
? extreme outliers ? plotted as circles (these are the
default boxplotting option of the R statistical pack-
age). To identify significant differences between re-
lation types, we also performed pairwise compar-
isons with the Tukey Honestly Significant Differ-
ence test, using the standard ? = 0.05 significance
threshold.
In the boxplots we can observe that all measures
(either symmetric or not) are able to discriminate
truly semantically related pairs from unrelated (i.e.
random) ones. Crucially, Cosine shows a strong
tendency to identify coordinates among the near-
est neighbors of target items. This is actually con-
sistent with its being a symmetric similarity mea-
sure. Instead, directional similarity measures signif-
icantly promote hypernyms over coordinates. The
only exception is represented by cosWeeds, which
again places coordinates at the top, though now the
difference with hypernyms is not significant. This
might be due to the cosine component of this mea-
sure, which reduces the effectiveness of the asym-
metric WeedsPrec. The difference between coor-
dinates and hypernyms is slightly bigger in invCL,
and the former appear to be further downgraded than
with the other directional measures. From the box-
plot analysis, we can therefore conclude that simi-
larity measures based on the Distributional Inclusion
Hypothesis do indeed improve hypernym identifica-
tion in context-feature semantic spaces, with respect
to other types of semantic relations, such as COORD.
77
coord hyper mero random-n
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
Cosine
coord hyper mero random-n
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
WeedsPrec
coord hyper mero random-n
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
cosWeeds
coord hyper mero random-n
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
ClarkeDE
coord hyper mero random-n
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
invCL
Figure 1: Distribution of relata similarity scores across concepts (values on ordinate are similarity scores after concept-
by-concept z-normalization).
The second type of evaluation we have performed
is based on Kotlerman et al (2010). The similarity
measures have been evaluated with Average Preci-sion (AP), a method derived from Information Re-
trieval and combining precision, relevance ranking
and overall recall. For each similarity measure, we
computed AP with respect to the 4 BLESS relations.
The best possible score (AP = 1) for a given rela-
tion (e.g., HYPER) corresponds to the ideal case in
which all the relata belonging to that relation have
higher similarity scores than the relata belonging to
the other relations. For every relation, we calculated
the AP for each of the 200 BLESS target concepts.
In Table 1, we report the AP values averaged over
the 200 concepts. On the one hand, these results
confirm the trend illustrated by the boxplots, in par-
ticular the fact that directional similarity measures
clearly outperform Cosine (or cosine-based mea-
sures such as cosWeeds) in identifying hypernyms,
with no significant differences among them. How-
ever, a different picture emerges by comparing the
measure COORD HYPER MERO RANDOM-NCosine 0.79 0.23 0.21 0.30WeedsPrec 0.45 0.40 0.31 0.32cosWeeds 0.69 0.29 0.23 0.30ClarkeDE 0.45 0.39 0.28 0.33invCL 0.38 0.40 0.31 0.34
Table 1: Mean AP values for each semantic relation re-
ported by the different similarity scores.
AP values for HYPER with those for COORD. since
in this case important distinctions among the di-
rectional measures emerge. In fact, even if Weed-sPrec and ClarkeDE increase the AP for HYPER,
still they assign even higher AP values to COORD.
Conversely, invCL is the only measure that assigns
to HYPER the top AP score, higher than COORD too.
The new directional similarity measure we have
proposed in this paper, invCL, thus reveals a higher
ability to set apart hypernyms from other relations,
coordinates terms included. The latter are expected
78
to share a large number of contexts and this is the
reason why they are strongly favored by symmet-
ric similarity measures, such as Cosine. Asymmet-
ric measures like cosWeeds and ClarkeDE also fall
short of distinguishing hypernyms from coordinates
because the condition of feature inclusion they test
is satisfied by coordinate terms as well. If two sets
share a high number of elements, then many ele-
ments of the former are also included in the latter,
and vice versa. Therefore, coordinate terms too are
expected to have high values of feature inclusions.
Conversely, invCL takes into account not only the
inclusion of u into v, but also the amount of v that
is not included in u. Thus, invCL provides a better
distributional correlate to the central property of hy-
pernyms of having a broader semantic content than
their hyponyms.4 Conclusions and ongoing research
The experiments reported in this paper support the
Distributional Inclusion Hypothesis as a viable ap-
proach to model hypernymy in semantic vector
spaces. We have also proposed a new directional
measure that actually outperforms the state-of-the-
art ones. Focusing on the contexts that broader terms
do not share with their narrower terms thus appear
to be an interesting direction to explore to improve
hypernym identification. Our ongoing research in-
cludes testing invCL to recognize lexical entailments
and comparing it with the balAPinc measured pro-
posed by Kotlerman et al (2010) for this task, as
well as designing new distributional methods to dis-
criminate between various other types of semantic
relations.Acknowledgments
We thank the reviewers for their useful and insight-
ful comments on the paper.References
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional Memory: A general framework for corpus-
based semantics. Computational Linguistics, 36(4):
673?721.
Marco Baroni and Alessandro Lenci. 2011. How we
BLESSed distributional semantic evaluation. In Pro-ceedings of the GEMS 2011 Workshop on Geometri-
cal Models of Natural Language Semantics, EMNLP2011, Edinburgh, Scotland, UK: 1?10.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of theEACL 2009Workshop on GEMS: GEometrical Modelsof Natural Language Semantics, Athens, Greece: 112?
119.
Katrin Erk. 2009a. Supporting inferences in semantic
space: representing words as regions. In Proceedingsof the 8th International Conference on ComputationalSemantics, Tilburg, January: 104?115.
Katrin Erk. 2009b. Representing words as regions in
vector space. In Proceedings of the Thirteenth Con-ference on Computational Natural Language Learning(CoNLL), Boulder, Colorado: 57?65.
Stefan Evert. 2005. The Statistics of Word Cooccur-rences. Ph.D. dissertation, Stuttgart University.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING1992, Nantes, France: 539?545.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-gineering, 16(04): 359?389.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the COLING-ACL 1998, Montreal, Canada: 768?774.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In Proceedings of theCOLING-ACL 2006, Sydney, Australia: 113?120.
Idan Szpektor and Ido Dagan. 2008. Learning Entail-
ment Rules for Unary Templates. In Proceedings ofCOLING 2008, Manchester, UK: 849?856.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings ofthe EMNLP 2003, Sapporo, Japan: 81?88.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional sim-
ilarity. In Proceedings of COLING 2004, Geneva,
Switzerland: 1015?1021.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The
distributional inclusion hypotheses and lexical entail-
ment. In Proceedings of ACL 2005, Ann Arbor, MI:
107?114.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Boot-
strapping distributional feature vector quality. Compu-tational Linguistics, 35(3): 435-461.
79
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 58?66,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Composing and Updating Verb Argument Expectations:
A Distributional Semantic Model
Alessandro Lenci
University of Pisa, Department of Linguistics
via S. Maria, 36
56126 Pisa (Italy)
alessandro.lenci@ling.unipi.it
Abstract
The aim of this paper is to present a com-
putational model of the dynamic composition
and update of verb argument expectations us-
ing Distributional Memory, a state-of-the-art
framework for distributional semantics. The
experimental results conducted on psycholin-
guistic data sets show that the model is able
to successfully predict the changes on the pa-
tient argument thematic fit produced by differ-
ent types of verb agents.
1 Introduction
A number of studies using different experimental
paradigms (priming, self-paced reading, etc.) have
shown that verbs (eat) activate expectations about
nouns occurring as their arguments (cheese)(McRae
et al, 1998), and vice versa (McRae et al, 2005).
Nouns also activate expectations about other nouns
occurring as co-arguments in the same event (key ?
door)(Hare et al, 2009). These behavioral effects
support the hypothesis that in the mental lexicon
verbs and their arguments are arranged into a web
of mutual expectations. Verb argument expectations
encoded in lexical representations are exploited by
subjects to determine the plausibility of a noun as an
argument of a particular verb (thematic fit, or selec-
tional preferences in the linguistic literature), which
has been proved to have important effects on human
sentence processing (McRae et al, 1998).
In a recent work, Bicknell et al (2010) bring evi-
dence suggesting a more complex view of the orga-
nization and on-line use of verb argument expecta-
tions. In fact, the expectations about the likely fillers
of a given verb argument (e.g., the patient role) de-
pend on the way another verb argument (e.g., the
agent role) is filled. For instance, if the agent noun is
journalist, the most likely patient for the verb check
is spelling, while if the agent noun is mechanic, the
most likely patient for the same verb is brakes. As
a consequence, thematic fit judgments are also sen-
sitive to the way other roles of the same verb are
filled. Bicknell et al (2010) show that this fact has
clear consequences for sentence processing, and ar-
gue that subjects dynamically compute and update
verb argument expectations and thematic fit during
on-line sentence comprehension, by integrating var-
ious types of knowledge about events and their ar-
guments.
The aim of this paper is to present a computa-
tional model of the dynamic composition and up-
date of verb argument expectations using Distri-
butional Memory (DM)(Baroni and Lenci, 2010),
a state-of-the-art Distributional Semantic Model
(DSM). DSMs (aka vector space models) repre-
sent word meaning with vectors encoding corpus-
based co-occurence statistics, under the assumption
of the so-called Distributional Hypothesis (Miller
and Charles, 1991; Sahlgren, 2006): Words occur-
ring in similar contexts are also semantically sim-
ilar (Landauer and Dumais, 1997; Pado? and Lap-
ata, 2007; Turney and Pantel, 2010). Thematic fit
judgments have already been successfully modeled
with DSMs (Erk et al, 2010), but to the best of our
knowledge the problem of how thematic fit is dy-
namically updated depending on the way other ar-
guments are filled has not been addressed yet. The
core of our proposal is that Distributional Memory
58
can be used to represent the subject?s expectations
about the most likely words co-occurring in given
syntactic role. We will add to the original Distri-
butional Memory framework a model for verb argu-
ment expectation composition called ECU, Expec-
tation Composition and Update. Specifically, we
will show how the expectations of an agent-verb pair
about their patient noun argument can be composi-
tionally derived from the DM representation of the
verb and the DM representation of its agent. ECU
is evaluated on the data set used in Bicknell et al
(2010), and the experimental results show that it is
able to successfully predict the changes on the pa-
tient thematic fit with a verb, depending on different
agent fillers. More generally, we want to argue that
the ECU model proposed here can represent a gen-
eral and viable approach to address compositionality
in distributional semantics.
After reviewing some related work in section 2,
we present Distributional Memory (section 3) and
its use to model verb-argument composition (section
4). Experiments and evaluation are reported in sec-
tion 5.
2 Background and related work
Elman (2009) argues that the information on pre-
ferred fillers of one verb argument depends on what
the filler is of one of the other arguments. For in-
stance, the most likely patient of cut is wood, when
the agent is lumberjack, but it is meat, when the
agent is butcher. This claim finds an empirical con-
firmation in the experiments reported by Bicknell et
al. (2010), in which subjects are presented with sen-
tence pairs like the following ones:
(1) The journalistAG checked the spellingPA of
his latest report. (congruent condition)
(2) The mechanicAG checked the spellingPA of
his latest report. (incongruent condition)
Each pair contains the same verb and patient argu-
ment, while differing for the agent argument. In the
congruent condition, the patient is a preferred argu-
ment of the verb, given the agent, e.g. spelling is
something which is typically checked by a journal-
ist. In the incongruent condition, the patient is not a
preferred argument of the verb, given its agent, e.g.
spelling is not something that is typically checked by
a mechanic, who rather checks brakes, engines, etc.
Thematic fit judgments used to determine congru-
ent and incongruent agent-verb-patient tuples were
collected in an off-line norming study. Bicknell et
al. (2010) report that self-paced reading times were
shorter at the word directly following the patient for
the congruent than the incongruent items. Similar
results were obtained in an event-related brain po-
tential (ERP) experiment, in which an N400 effect
was observed immediately at the patient noun in the
incongruent condition. In eye-tracking experiments,
Kamide et al (2003) also demonstrated that the the-
matic fit of an object depended on the other verb ar-
gument fillers.
The conclusion drawn by Bicknell et al (2010) is
that verb argument expectations and thematic fit are
not simply stored in the lexicon, but are rather dy-
namically updated during sentence comprehension,
by integrating various types of knowledge. In fact,
if the verb expectations about an argument role de-
pend on the nouns filling its other arguments, the
hypothesis that they are compositionally updated
is highly plausible, since, ?it is difficult to envi-
sion how the potentially unbounded number of con-
texts that might be relevant could be anticipated and
stored in the lexicon? (Elman 2009: 21).
Thematic fit judgments have been successfully
modeled in distributional semantics. Erk et al
(2010) propose the Exemplar-Based Model of Se-
lectional Preferences, in turn based on Erk (2007).
The thematic fit of a noun n as an argument of a
verb v is measured with the similarity in a vector
space between n and a set of noun exemplars oc-
curring in the same argument role of v. A related
approach is adopted by Baroni and Lenci (2010),
the main difference being that the thematic fit of n
is measured by comparing its vector with a ?proto-
type? vector obtaining by averaging over the vectors
of the most typical arguments of v. In both cases,
the distributional measure of thematic fit is shown to
be highly correlated with human plausibility judge-
ments. Their success notwithstanding, these mod-
els fall short of accounting for the dynamical and
context-sensitive nature of thematic fit. In the next
section, we will extend the Baroni and Lenci (2010)
approach with a model for verb-argument composi-
tion, which is able to account for the argument inter-
dependency phenomena shown by the experiments
59
in Bicknell et al(2010).
If verb argument expectations are likely to be dy-
namically computed integrating knowledge of the
verb with information about its fillers, modeling the-
matic fit with DSM requires us to address composi-
tional representations. DSMs have mostly addressed
semantic issues related to the representation of the
content of single words. However, growing efforts
have recently been devoted to the problem of how
to build distributional semantic representations of
complex expressions (e.g., phrases, sentences, etc.)
by composing the distributional representations of
their component lexical items (Kintsch, 2001; Clark
and Pulman, 2007; Widdows, 2008; Mitchell and
Lapata, 2010). Different proposals to address com-
positionality in DSM exist, but the most common
approach is to model semantic composition as vector
composition. Mitchell and Lapata (2010) systemat-
ically explore various vector composition functions
(e.g., vector addition, vector product, and other more
sophisticated variants thereof), which are used to
build distributional vector representations for verb-
noun and adjective-noun phrases. The various mod-
els for vector composition are then evaluated in a
phrase similarity task.
Erk and Pado? (2008) address a partially differ-
ent and yet crucial aspect of compositionality, i.e.,
the fact that when words are composed, they tend
to affect each other?s meanings. The meaning of
run in The horse runs is in fact different from its
meaning in The water runs (Kintsch, 2001). Erk
and Pado? (2008) claim that words are associated
with various types of expectations (typical events for
nouns, and typical arguments for verbs)(McRae et
al., 1998; McRae et al, 2005) that influence each
other when words compose, thereby altering their
meaning. They model this context-sensitive com-
positionality by distinguishing the lemma vector of
a word w1 (i.e. its out-of-context representation),
from its vector in the context of another word w2.
The vector-in-context for w1 is obtained by combin-
ing the lemma vector of w1 with the lemma vectors
of the expectations activated byw2. For instance, the
vector-in-context assigned to run in The horse runs
is obtained by combining the lemma vector of run
with the lemma vectors of the most typical verbs in
which horse appears as a subject (e.g. gallop, trot,
etc.). Like in Mitchell and Lapata (2010), various
functions to build vectors in contexts are tested. Erk
and Pado? (2008) evaluate their model for context-
sensitive vector representation to predict verb simi-
larity in context (e.g. slump in the context of shoul-
der is more similar to slouch than to decline) and to
rank paraphrases.
Our model draws close inspiration from Erk and
Pado? (2008), with which it shares the importance of
verb argument expectations. However, differently
from them, we want to model how the combination
of a verb with an argument affects its expectations
about the likely fillers of its other arguments. While
Erk and Pado? (2008) test their model on a standard
word similarity task (i.e. they measure the similar-
ity between the vector-in-context of a verb with the
vector of another ?landmark? verb), we evaluate our
model for compositionality in distributional seman-
tics in a thematic fit task. Indeed, to the best of our
knowledge this is the first time in which the issues
of thematic fit and compositionality in DSMs are ad-
dressed together.
3 Distributional Memory
Distributional Memory (DM) (Baroni and Lenci,
2010) is a framework for distributional semantics
aiming at generalizing over different existing ty-
pologies of semantic spaces. Distributional Memory
represents corpus-extracted distributional facts as a
weighted tuple structure T , a set of weighted word-
link-word tuples ??w1, l, w2?,??, such that w1 and
w2 belong to W , a set of content words (e.g. nouns,
verbs, etc.), and l belongs to L, a set of syntag-
matic co-occurrence links between words in a text
(e.g. syntactic dependencies, lexicalized patterns,
etc.). For instance, the tuple ??book,obj, read?,??
encodes the piece of distributional information that
book co-occurs with read in the corpus, and obj
specifies the type of syntagmatic link between these
words, i.e. direct object. The score ? is some func-
tion of the co-occurrence frequency of the tuple in
a corpus and is used to characterize its statistical
salience.
Distributional Memory belongs to the family of
so-called structured DSMs, which take into account
the crucial role played by syntactic structures in
shaping the distributional properties of words. To
qualify as context of a target item, a word must be
60
linked to it by some (interesting) lexico-syntactic re-
lation, which is also typically used to distinguish the
type of this co-occurrence (Lin, 1998; Pado? and Lap-
ata, 2007). Differently from other structured DSMs,
the tuple structure T is formally represented as a
3-way geometrical object, namely a third order la-
beled tensor. A tensor is a multi-way array (Turney,
2007; Kolda and Bader, 2009), i.e. a generalization
of vectors (first order tensors) and matrices (second
order tensors). Different semantic spaces are then
generated ?on demand? through tensor matriciza-
tion, projecting the tuple tensor onto 2-way matrices,
whose rows and columns represent semantic spaces
to deal with different semantic tasks.
For instance, the space W1?LW2 is formed by
vectors for words and the dimensions represent the
attributes of these words in terms of lexico-syntactic
relations with lexical collocates, such as ?obj, read?,
or ?use, pen?. Consistently, this space is most suit-
able to address tasks involving the measurement of
the ?attributional similarity? between words (Tur-
ney, 2006), such as synonym detection or modeling
selectional preferences. Instead, the space W1W2?L
contains vectors associated with word pairs, whose
dimensions are links between these pairs. This space
is thus suitable to address tasks involving the mea-
surement of so-called ?relational similarity? (Tur-
ney, 2006), such as analogy detection or relation
classification (cf. Baroni and Lenci 2010 for more
details about the Distributional Memory spaces and
tasks). Crucially, these spaces are now alternative
?views? of the same underlying distributional mem-
ory formalized in the tensor. Many semantic tasks
(such as analogical similarity, selectional prefer-
ences, property extraction, synonym detection, etc.),
which are tackled in the literature with different, of-
ten unrelated semantic spaces, are addressed in DM
with the same distributional tensor, harvested once
and for all from the corpus. This is the reason why
Distributional Memory is claimed to be a general
purpose resource for semantic modeling.
Depending on the selection of the sets W and L
and of the scoring function ?, different DM mod-
els can be generated. The Distributional Memory
instantiation chosen for the experiments reported in
this paper is TypeDM, whose links include lexical-
ized dependency paths and lexico-syntactic shallow
patterns, with a scoring function based on pattern
type frequency.1 We have chosen TypeDM, be-
cause it is the best performing DM model across
the various semantic tasks addressed in Baroni and
Lenci (2010). The TypeDM tensor contains about
130M non-zero tuples automatically extracted from
a corpus of about 2.83 billion tokens, obtained by
concatenating the the Web-derived ukWaC corpus
(about 1,915 billion tokens),2 a mid-2009 dump of
the English Wikipedia (about 820 million tokens),3
and the British National Corpus (about 95 million
tokens).4 The resulting concatenated corpus was
tokenized, POS-tagged and lemmatized with the
TreeTagger5 and dependency-parsed with the Malt-
Parser.6
The TypeDM word set (WTypeDM ) contains
30,693 lemmas (20,410 nouns, 5,026 verbs and
5,257 adjectives). These are the top 20,000 most fre-
quent nouns and top 5,000 most frequent verbs and
adjectives in the corpus, augmented with lemmas in
various standard test sets in distributional semantics,
such as the TOEFL and SAT lists. The TypeDM
link set (LTypeDM ) contains 25,336 direct and in-
verse links formed by (partially lexicalized) syntac-
tic dependencies and patterns. This is a sample of
the links in LTypeDM :
? obj: The journalist is checking his article ?
?article, obj, check?
? verb: The journalist is checking his article ?
?journalist, verb, article?
? sbj tr: The journalist is checking his article
? ?journalist, sbj tr, check?
? preposition: I saw a journalist with a pen
? ?pen, with, journalist?
? such as: ?NOUN such as NOUN? and ?such
NOUN as NOUN?: animals such as cats ?
?animal, such as+ns+ns, cat?
1The TypeDM tensor is publicly available at http://
clic.cimec.unitn.it/dm
2http://wacky.sslmit.unibo.it/
3http://en.wikipedia.org/wiki/Wikipedia:
Database_download
4http://www.natcorp.ox.ac.uk
5http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
6http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
61
The first two links above are the most relevant ones
for the purposes of the present paper: obj links a
transitive verb and its direct object, and verb is a
lexically underspecified link between a subject noun
and a complement noun of the same verb.
The scoring function ? is the Local Mutual
Information (LMI) (Evert, 2005) computed on link
type frequency (negative LMI values are raised to 0):
LMI = Oijk log
Oijk
Eijk
(1)
Oijk and Eijk are respectively the observed and ex-
pected frequency of a triple ?wi, lj , wk?.
4 Composing verb argument expectations
In this section, we address the fact that the infor-
mation on preferred fillers of one verb argument de-
pends on the filler of its other arguments by propos-
ing a model for Expectation Composition and Up-
date (ECU), which will then be computationally for-
malized with Distributional Memory.
ECU relies on the hypothesis that nouns and verbs
are linked in a web of mutual expectations. Verbs
are associated with expectations about their likely
arguments, and nouns have expectations about the
events they are involved with and also about other
nouns co-occuring in the same events (cf. section
1). We argue that, when words compose (e.g. a
verb and a noun), their expectations are integrated
and updated. Specifically, we focus here on how the
composition of a verb and its agent argument deter-
mines an update of the verb expectations for its pa-
tient argument. Let EXPA(v) be the expectations
of a verb v about its patient arguments, i.e. a set
of nouns likely to occur as verb patients. For in-
stance, EXPA(check) = mistakes, engines, books,
etc. Let EX(nAG) be the expectations about typi-
cal events performed and typical entities acted upon
by the agent noun. For instance, EX(mechanic) =
mechanics fix cars, mechanics check oil, etc. ECU
is formally defined as follows:
EXPA(?nAG, v?) = f(EX(nAG), EXPA(v)) (2)
f is some function for expectation composition and
update (cf. below). ECU assumes that the result of
semantically composing the verb and its agent ar-
gument is an update of the verb expectations about
its patient argument. EXPA(?nAG, v?) are the up-
dated expectations of v about its patient arguments,
resulting from the composition of its original ex-
pectations with the agent?s expectations. For in-
stance, the result of composing check with the agent
argument mechanic is a new set of expectations
EXPA(?mechanic, check?) formed by objects that
are likely checked by mechanics, such as cars, en-
gines, wheels, etc. These updated expectations are
a function of the typical patients of checking events,
and of the typical patients of events performed by
mechanics.
4.1 Modeling ECU with Distributional
Memory
The tuple structure of the DM tensor is well suited
to represent the web of mutual expectations in which
lexical items are arranged. In fact, given a word w,
the expectations of w, EX(w), can be defined as
the subset of the DM tensor formed by the tuples
??w1, l, w2?,??, such that w = w1 or w = w2. The
tuple score ? determines the statistical salience and
typicality of a particular expectation.
To model ECU with TypeDM, we approximate
the patient semantic role with the syntactic depen-
dency DM link of obj (cf. section 3). The expec-
tations about the typical patient arguments of a verb
v (EXPA(v)) thus correspond to the set of TypeDM
tuples ?ni, obj, v?: e.g, EXPA(check) = ?mistake,
obj, check?, ?engine, obj, check?, etc. We model
EX(nAG) with the set of DM tuples ?nAG, verb,
nj?, which characterize the typical patients (i.e., di-
rect objects) of events performed by the agent noun
nAG: e.g, EX(mechanic) = ?mechanic, verb,
car?, ?mechanic, verb, oil?, ?mechanic, verb,
engine?, etc.
The expectation composition function f of
equation 2 is modeled as a tensor updating function:
f modifies the TypeDM tensor by updating the
scores of the relevant subset of tuples. Following
current compositionality models in distributional
semantics (cf. Mitchell and Lapata 2010), we focus
here on two alternative versions of f :
62
check ?journalist, check? ?mechanic, check?
site article car
page book tyre
website information work
box question price
detail fact vehicle
link report job
list site system
file source bike
record content value
information account problem
Table 1: Original TypeDM expectations for check and
their compositional updates obtained with f = PRODUCT
SUM
For each tuple ??ni, obj, v?, ?i? ? EXPA(v),
??ni, obj, v?, ?u? ? EXPA(?nAG, v?), and
?u =
?
?
?
?i + ?j if ??nAG, verb, nj?, ?j?
? EX(nAG) and ni = nj
?i otherwise
(3)
PRODUCT
For each tuple ??ni, obj, v?, ?i? ? EXPA(v),
??ni, obj, v?, ?u? ? EXPA(?nAG, v?), and
?u =
?
?
?
?i ? ?j if ??nAG, verb, nj?, ?j?
? EX(nAG) and ni = nj
0 otherwise
(4)
The idea underlying both types of tensor updat-
ing functions is that the verb expectations about its
likely patients are modified with the score of the tu-
ples representing objects that are typical patients of
events performed by the agent noun. With SUM,
expectation composition is a linear function of the
score of the tuples in EXPA(v) and in EX(nAG):
EXPA(?nAG, v?) contains all the tuples belonging
to EXPA(v), but their score is added to the score
of the tuples in EX(nAG) sharing the same object
noun. With the PRODUCT function, the updated ex-
pectations only include the tuples inEXPA(v) shar-
ing the same objects with a tuple in EX(nAG). The
score of these tuples is the product of the original
tuple score, while the score of the other tuples in
EXPA(v) is set to 0.
Table 1 reports a sample output of the ap-
plication of ECU to the TypeDM tensor. The
left column contains the objects of the top-
scoring tuples of EXPA(check) in the original
TypeDM tensor (ordered by decreasing values of
?). The central column contains the top-scoring
nouns in EXPA(?journalist, check?), composi-
tionally derived by updating EXPA(check) with
EX(journalist): the nouns are ordered by de-
creasing value of ? modified according to the PROD-
UCT composition function. We can notice that the
updated verb argument expectations include nouns
consistent with what journalists typically check. The
right column instead contains the top-scoring nouns
in EXPA(?mechanic, check?), derived from up-
dating EXPA(check) with EX(mechanic): the
composition function is still the PRODUCT. The dif-
ference with the central column is striking: the top-
scoring nouns in the updated verb argument expec-
tations are now related to what mechanics typically
check.
5 Experiments and evaluation
The ECU model for the compositional update of
verb-argument expectations has been evaluated by
measuring the thematic fit between an agent-verb
pair (?nAG, v?) and a patient noun argument (nPA)
of the same verb. Thematic fit is computed with
the verb expectations in EXPA(?nAG, v?), which in
turned have been obtained by composing EX(nAG)
and EXPA(v) with either of the two functions de-
scribed in section 4. In the following subsections,
we illustrate the data sets used for the experiments,
the procedure to compute the compositional the-
matic fit in TypeDM, and the results of the experi-
ments.
5.1 Data sets
Two data sets of agent-verb-patient triples from
Bicknell et al (2010) have been used to test ECU:
? bicknell.64 - 64 test triples used in the self-
paced reading and ERP experiments in Bicknell
et al (2010);
? bicknell.100 - 100 test triples, a superset of
bicknell.64.
Triples are organized in pairs, each sharing the same
verb, but differing for the agent and patient nouns:
? journalistAG - check - spellingPA
? mechanicAG - check - brakePA
63
Patients in each triple were produced by 47 sub-
jects as the prototypical (congruent) arguments of
the verbs, given a certain agent. The patient noun
in one triple is incongruent for the other triple with
the same verb: e.g., brake is the incongruent patient
for the mechanicAG - check pair. The bicknell.100
dataset contains all the triples produced in the orig-
inal norming study. The bicknell.64 data set is a
subset of the normed triples selected by Bicknell et
al. (2010) after removing test items that were poten-
tially problematic for the behavioral experiments.
5.2 Procedure
The thematic fit of a noun nPA as the patient of
?nAG, v? is measured with the cosine between the
vector of nPA in the TypeDM W1?LW2 space and
the ?prototype? vector in the same space built with
the vectors of the top-k expected objects belong-
ing to EX(nAG, v). This is an extension of the
approach to selectional preferences modeling pre-
sented in Baroni and Lenci (2010) (in turn inspired
to Erk 2007). These are the steps used to com-
pute the compositional thematic fit in the TypeDM
W1?LW2 space:
1. we select a set of k of prototypical patient
nouns nPA for ?nAG, v? (in the reported exper-
iments we set k = 20). The selected nouns
are the ni in the k tuples ??ni, obj, v?, ?u?
? EXPA(?nAG, v?) with the highest score ?.
The patient nouns in the datasets are excluded;
2. the vectors in the W1?LW2 TypeDM space
of the selected nouns are normalized and
summed. The result is a centroid vector rep-
resenting an abstract ?patient prototype vector?
for ?nAG, v?;
3. for each nAG - v - nPA test triple (e.g., journal-
istAG - check - spellingPA), we measure i.) the
cosine between nPA and the ?patient prototype
vector? for the congruent ?nAG, v? pair, (e.g.,
journalistAG - check) and ii.) the cosine be-
tween nPA and the ?patient prototype vector?
for the incongruent ?nAG, v? pair, belonging to
the other triple with the same verb v (e.g., me-
chanicAG - check).
For each test triple, we score a ?hit? if nPA has a
higher thematic fit (i.e., cosine) with the congruent
?nAG, v? pair, than with the incongruent one. For in-
stance, if cosine(?journalist, check?, spelling) >
cosine(?mechanic, check?, spelling), we score a
?hit?, otherwise we score a ?fail?.
5.3 Results
Experiments to model the verb-argument composi-
tional thematic fit have been carried out with the two
ECU functions, SUM and PRODUCT, each tested on
both datasets. Model performance has been evalu-
ated with ?hit? accuracy, i.e. the percentage of ?hits?
scored on each data set. As a baseline, we have sim-
ply adopted the random accuracy. The results of the
ECU models are reported in table 2.
We can notice that when the verb-argument ex-
pectations are compositionally updated with the
PRODUCT function, the model is able to signifi-
cantly outperform the baseline accuracy with both
data sets. Conversely, SUM is never able to go be-
yond the baseline. This is remindful of the results
reported by Erk and Pado? (2008) and Mitchell and
Lapata (2010), in which multiplicative vector com-
position achieves better performance in the (verb in
context or phrase) similarity tasks than (at least sim-
ple) additive functions. In fact, the advantage of the
multiplicative function is that it allows the composi-
tion process to highlight the dimensions shared by
the vectors of the component words, thereby em-
phasizing context effects. Something similar can
be argued to explain the results of the current ex-
periments. With PRODUCT the expectations of
EXPA(?nAG, v?) are a non-linear function of the
expectations about patient nouns shared by v and
nAG. Therefore, the objects that are likely to be
checked by a mechanic depend on the things that are
both typical patients of checking events and typical
patients of actions performed by a mechanic. This
results in a stronger thematic fit in the congruent
condition than in the incongruent one.
We also carried out experiments to investigate
whether the choice of the parameter k (the number
of nouns selected to build the ?prototype patient vec-
tor?) affects the model performance. However, we
obtained no significant difference with respect to the
values reported in table 2.
64
data set ECU function accuracy p-value
bicknell.64 SUM 40.62%
bicknell.64 PRODUCT 84.37% 3.798e-08 ***
bicknell.100 SUM 37.5%
bicknell.100 PRODUCT 73% 4.225e-06 ***
baseline 50%
Table 2: Results of the thematic fit experiments (p-values
computed with a ?2 test).
6 Conclusions and further directions of
research
Psycholinguistic evidence has proved that verb ar-
gument thematic fit is highly context-sensitive. In
fact, subjects? sensitivity to the likelihood of a noun
as a verb argument strongly depends on the nouns
filling other arguments of the same verb. These
data hint at a dynamic process underlying verb ar-
gument expectation and thematic fit computation,
resulting from the compositional integration of the
verb expectations with those activated by its argu-
ments. In this paper, we have presented ECU, a dis-
tributional semantic model for the compositional up-
date of verb argument expectations. ECU has been
applied to Distributional Memory, a state-of-the-art
Distributional Semantic Model, whose core tensor
of corpus-derived tuples is particularly suited to rep-
resent word expectations. ECU has been tested suc-
cesfully in an experiment to measure the thematic
fit between an agent-verb pair (?nAG, v?) and a pa-
tient noun argument (nPA) of the same verb, with
the data set used in the psycholinguistic experiments
reported in Bicknell et al (2010). The good results
we have obtained prove that DSMs can provide in-
teresting computational models of the compositional
update of thematic fit. Of course, other factors be-
sides verb-argument knowledge may also contribute
to the context-sensitive nature of thematic fit. How-
ever, it is worth noticing that one of the hypotheses
advanced by Bicknell et al (2010) to explain their
experimental results is indeed that subjects use their
knowledge of statistical linguistic regularities. This
is exactly the type of knowledge that is represented
in the Distributional Memory tensor structure and is
exploited by ECU.
Starting from the experimental results in Bick-
nell et al (2010) on sentence on-line processing,
in this paper we have addressed the issue of how
the agent of a verb modulates the subjects? expec-
tations about its patients. On the other hand, there is
broad evidence that the meaning of a verb is predom-
inantly modulated by its object. This suggests that
ECU should also be applied to model how the pref-
erences about the agent argument are determined by
the choice of the verb object. We leave this issue for
future research.
Besides being a computational model for thematic
fit, we also claim that the ECU approach has a
more general relevance for the issue of how to ad-
dress compositionality in DSMs. In fact, let us as-
sume that part of the semantic content of a word
consists of expectations about likely co-occurring
words, which in turn can be modeled with subsets
of a distributional tuple tensor. We can therefore
claim that (at least part of) the effect of the semantic
composition of words is to update their expectations
about other co-occurring words, like ECU does. We
have seen here that this hypothesis finds a nice con-
firmation with verb-argument composition. We be-
lieve that an interesting empirical question is to in-
vestigate to what extent this hypothesis can be gen-
eralized to other cases of compositionality.
In the future, we also plan to experiment with
other types of expectation composition functions.
Moreover we will extend the ECU model to tackle
context-sensitive effects in the thematic fit with re-
spect to other types of verb argument relations, be-
sides agent and patient ones. In fact, Matsuki et
al. (submitted) have reported that patient and instru-
ment verb arguments show interdependency effects
similar to the ones between agents and patients that
we have addressed in this paper.
Acknowledgments
I am very grateful to Ken McRae for providing the
data set used in this paper. I also thank Marco Ba-
roni - the co-author of Distributional Memory - and
the two anonymous reviewers for their helpful com-
ments. The usual disclaimers apply.
65
References
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional Memory : A general framework for corpus-
based semantics. Computational Linguistics, 36(4):
673?721.
Klinton Bicknell, Jeffrey L. Elman, Mary Hare,
Ken McRae, and Marta Kutas. 2010. Effects of event
knowledge in processing verbal arguments. Journal of
Memory and Language, 63(4): 489?505.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of the AAAI Spring Symposium on Quan-
tum Interaction: 52?55.
Jeffrey L. Elman. 2009. On the meaning of words and
dinosaur bones: Lexical knowledge without a lexicon.
Cognitive Science, 33(4): 547?582.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of ACL: 216?
223.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 08: 897?906.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4): 723?763.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D. dissertation, Stuttgart University.
Mary Hare, Michael Jones, Caroline Thomson,
Sarah Kelly,and Ken McRae. 2009. Activating
event knowledge. Cognition, 111(2): 151?167.
Yuki Kamide, Gerry T.M. Altmann, and Sarah L. Hay-
wood. 2003. The time-course of prediction in incre-
mental sentence processing: Evidence from anticipa-
tory eye movements. Journal of Memory and Lan-
guage, 49: 133?156.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2): 173?202.
Tamara Kolda and Brett Bader. 2009. Tensor decomposi-
tions and applications. SIAM Review, 51(3): 455?500.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis.
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2): 1?31.
Dekang Lin 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL: 768?
774.
Kazunaga Matsuki, Tracy Chow, Mary Hare, Jeffrey L.
Elman, Christoph Scheepers, and Ken McRae. sub-
mitted for publication. Event-based plausibility im-
mediately influences on-line language comprehension
Ken McRae, Michael J. Spivey-Knowlton, and
Michael K.Tanenhaus. 1998. Modeling the in-
fluence of thematic fit (and other constraints) in
on-line sentence comprehension Journal of Memory
and Language, 38: 283?312.
Ken McRae, Mary Hare, Jeffrey L. Elman, and Todd Fer-
retti. 2005. A basis for generating expectancies for
verbs from nouns Memory & Cognition, 33(7): 1174?
1184.
GeorgeMiller andWalter Charles. 1991. Contextual cor-
relates of semantic similarity Language and Cognitive
Processes, 6: 1?28.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8): 1388?1429.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2): 161?199.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
dissertation, Stockholm University.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3): 379?416.
Peter D. Turney. 2007. Empirical evaluation of four ten-
sor decomposition algorithms. Technical Report ERB-
1152, NRC.
Peter D. Turney, Patrick Pantel. 2010. From frequency to
meaning : Vector space models of semantics. Journal
of Artificial Intelligence Research, 37: 141?188.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the Sec-
ond AAAI Symposium on Quantum Interaction: 1?8.
66
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 1?10,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
How we BLESSed distributional semantic evaluation
Marco Baroni
University of Trento
Trento, Italy
marco.baroni@unitn.it
Alessandro Lenci
University of Pisa
Pisa, Italy
alessandro.lenci@ling.unipi.it
Abstract
We introduce BLESS, a data set specifically
designed for the evaluation of distributional
semantic models. BLESS contains a set of tu-
ples instantiating different, explicitly typed se-
mantic relations, plus a number of controlled
random tuples. It is thus possible to assess the
ability of a model to detect truly related word
pairs, as well as to perform in-depth analy-
ses of the types of semantic relations that a
model favors. We discuss the motivations for
BLESS, describe its construction and struc-
ture, and present examples of its usage in the
evaluation of distributional semantic models.
1 Introduction
In NLP, it is customary to distinguish between in-
trinsic evaluations, testing a system in itself, and
extrinsic evaluations, measuring its performance in
some task or application (Sparck Jones and Galliers,
1996). For instance, the intrinsic evaluation of a de-
pendency parser will measure its accuracy in identi-
fying specific syntactic relations, while its extrinsic
evaluation will focus on the impact of the parser on
tasks such as question answering or machine trans-
lation. Current approaches to the evaluation of Dis-
tributional Semantic Models (DSMs, also known
as semantic spaces, vector-space models, etc.; see
Turney and Pantel (2010) for a survey) are task-
oriented. Model performance is evaluated in ?se-
mantic tasks?, such as detecting synonyms, recog-
nizing analogies, modeling verb selectional prefer-
ences, ranking paraphrases, etc. Measuring the per-
formance of DSMs on such tasks represents an in-
direct test of their ability to capture lexical mean-
ing. The task-oriented benchmarks adopted in dis-
tributional semantics have not specifically been de-
signed to evaluate DSMs. For instance, the widely
used TOEFL synonym detection task was designed
to test the learners? proficiency in English as a sec-
ond language, and not to investigate the structure of
their semantic representations (cf. Section 2).
To gain a real insight into the abilities of DSMs to
address lexical semantics, existing benchmarks must
be complemented with a more intrinsically oriented
approach, to perform direct tests on the specific as-
pects of lexical knowledge captured by the models.
In order to achieve this goal, three conditions must
be met: (i) to single out the particular aspects of
meaning that we want to focus on in the evaluation
of DSMs; (ii) to design a data set that is able to ex-
plicitly and reliably encode the target semantic infor-
mation; (iii) to specify the evaluation criteria of the
system performance on the data set, in order to get
an estimate of the intrinsic ability of DSMs to cope
with the selected semantic aspects. In this paper, we
address these three conditions by presenting BLESS
(Baroni and Lenci Evaluation of Semantic Spaces),
a new data set specifically geared towards the in-
trinsic evaluation of DSMs, downloadable from:
http://clic.cimec.unitn.it/distsem.
2 Distributional semantics benchmarks
There are several benchmarks that have been widely
adopted for the evaluation of DSMs, all of them cap-
turing interesting challenges a DSM should meet.
We briefly review here some commonly used and
representative benchmarks, and discuss why we felt
1
the need to add BLESS to the set. We notice at the
outset of this discussion that we want to carve out a
space for BLESS, and not to detract from the impor-
tance and usefulness of other data sets. We further
remark that we focus on data sets that, like BLESS,
are monolingual English and, while task-oriented,
not aimed at a specific application setting (such as
machine translation or ontology population).
Probably the most commonly used benchmark in
distributional semantics is the TOEFL synonym de-
tection task introduced to computational linguis-
tics by Landauer and Dumais (1997). It consists of
80 multiple-choice questions, each made of a target
word (a noun, verb, adjective or adverb) and 4 re-
sponse words, 1 of them a synonym of the target.
For example, given the target levied, the matched
words are imposed, believed, requested, correlated,
the first one being the correct choice. The task for
a system is then to pick the true synonym among
the responses. The TOEFL task focuses on a single
semantic relation, namely synonymy. Synonymy is
actually not a common semantic relation and one of
the hardest to define, to the point that many lexi-
cal semanticists have concluded that true synonymy
does not exist (Cruse, 1986). Just looking at a few
examples of synonym pairs from the TOEFL set will
illustrate the problem: discrepancy/difference, pro-
lific/productive, percentage/proportion, to market/to
sell, color/hue. Moreover, the criteria adopted to
choose the distractors (probably motivated by the
language proficiency testing purposes of TOEFL)
are not known. By looking at the set, it is hard
to discern a coherent pattern. In certain cases, the
distractors are semantically close to the target word
(volume, sample and profit for percentage), whereas
in other cases they are not (home, trail, and song for
annals). It it thus not clear whether we are asking the
models to distinguish a semantically related word
(the synonym) from random elements, or a more
tightly related word (the synonym, again) from other
related words. The TOEFL task, finally, is based on
a discrete choice (either you get the right word, or
you don?t), with the result that evaluation is ?quan-
tized?, leading to large accuracy gains for small ac-
tual differences (one model that guesses one more
synonym right than another gets 1.25% more points
in percentage accuracy).
The WordSim 353 data set (Finkelstein et al,
2002) is a widely used example of semantic simi-
larity rating set (see also Rubenstein and Goode-
nough (1965) and Miller and Charles (1991)). Sub-
jects were asked to rate a set of 353 word pairs on a
?similarity? scale and average ratings for each pair
were computed. Models are then evaluated in terms
of correlation of their similarity scores with aver-
age ratings across pairs. From the point of view
of assessing the performance of a DSM, the Word-
Sim (and related) similarity ratings are a mixed bag,
in two senses. First, the data set contains a vari-
ety of different semantic relations. In a recent se-
mantic annotation of the WordSim performed by
Agirre et al (2009) we find that, among the 174
pairs with above-median score (and thus presum-
ably related), there is 1 identical pair, 17 synonym
pairs, 28 hyper-/hyponym pairs, 30 coordinate pairs,
6 holo-/meronym pairs and 92 (more than half) pairs
that are ?topically related, but none of the above?.
Second, the scores are a mixture of intuitions about
which of these relations are more semantically tight
and intuitions about more or less connected pairs
within each of the relations. For example, among
the top-rated scores we find synonyms such as jour-
ney/voyage and coordinate concepts (king/queen).
If we look at the relations characterizing pairs
around the median rating, we find both less ?per-
fect? synonyms (monk/brother, that are synonymous
only under an unusual sense of brother) and less
close coordinates (skin/eye), as well as pairs in-
stantiating other, less taxonomically tight relations,
such as many syntagmatically connected items (fam-
ily/planning, disaster/area, bread/butter). Appar-
ently, a single scale is merging intuitions about se-
mantic similarity of specific pairs and semantic sim-
ilarity of different relations.
A perhaps more principled way to evaluate DSMs
that has recently gained some popularity is the con-
cept categorization task, where a DSM has to clus-
ter a set of nouns expressing basic-level concepts
into gold standard categories. A particularly care-
fully constructed example is the Almuhareb-Poesio
(AP) set of 402 concepts introduced in Almuhareb
(2006). Concept categorization sets also include the
Battig (Baroni et al, 2010) and ESSLLI 2008 (Ba-
roni et al, 2008) lists. The AP concepts must be
clustered into 21 classes, each represented by be-
tween 13 and 21 nouns. Examples include the ve-
2
hicle class (helicopter, motorcycle. . . ), the motiva-
tion class (ethics, incitement, . . . ), and the social
unit class (platoon, branch). The concepts are bal-
anced in terms of frequency and ambiguity, so that,
e.g., the tree class contains a common concept such
as pine but also the casuarina tree, as well as the
samba tree, that is not only an ambiguous term, but
one where the non-arboreal sense dominates.
Concept categorization data sets, while interest-
ing to simulate one of the basic aspects of human
cognition, are limited to one kind of semantic re-
lation (discovering coordinates). More importantly,
the quality of the results will depend not only on the
underlying DSMs, but also on the clustering algo-
rithm being used (and on how this interacts with the
overall structure of the DSM), thus making it hard
to interpret the performance of DSMs. The forced
?hard? category choice is also problematic, and ex-
aggerates performance differences between models
especially in the presence of ambiguous terms (a
model that puts samba in the occasion class with
dance and ball might be penalized as much as a
model that puts it in the monetary currency class).
A more general issue with all benchmarks is that
tasks are based on comparing a single quality score
for each considered model (accuracy for TOEFL,
correlation for WordSim, a clustering quality mea-
sure for AP, etc.). This gives little insight into how
and why the models differ. Moreover, there is no
well-established statistical procedure to assess sig-
nificance of differences for most commonly used
measures. Finally, either because the data sets were
not originally intended as standard benchmarks, or
even on purpose, they all are likely to cause coverage
problems even for DSMs trained on very large cor-
pora. Think of the presence of extremely rare nouns
like casuarina in AP, of proper nouns inWordSim (it
is not clear to us that DSMs are adequate semantic
models for referring expressions ? at the very least
they should not be mixed up lightly with common
nouns), or multi-word expressions in other data sets.
3 How we intend to BLESS distributional
semantic evaluation
DSMs measure the distributional similarity between
words, under the assumption that proximity in distri-
butional space models semantic relatedness, includ-
ing, as a special case, semantic similarity (Budanit-
sky and Hirst, 2006). However, semantically related
words in turn differ for the type of relation hold-
ing between them: e.g., dog is strongly related to
both animal and tail, but with different types of re-
lations. Therefore, evaluating the intrinsic ability of
DSMs to represent the semantic space of a word en-
tails both (i) determining to what extent words close
in semantic space are actually semantically related,
and (ii) analyzing, among related words, which type
of semantic relation they tend to instantiate. Two
models can be equally very good in identifying se-
mantically related words, while greatly differing for
the type of related pairs they favor.
The BLESS data set complies with both these
constraints. The set is populated with tuples ex-
pressing a relation between a target concept (hence-
forth referred to as concept) and a relatum concept
(henceforth referred to as relatum). For instance, in
the BLESS tuple coyote-hyper-animal, the concept
coyote is linked to the relatum animal via the hy-
pernymy relation (the relatum is a hypernym of the
concept). BLESS focuses on a coherent set of basic-
level nominal concrete concepts and a small but ex-
plicit set of semantic relations, each instantiated by
multiple relata. Depending on the type of relation,
relata can be nouns, verbs or adjectives. Moreover,
BLESS also contains, for each concept, a number of
random ?relatum? words that are not semantically
related to the concept. Thus, it also allows to evalu-
ate a model in terms of its ability to harvest related
words given a concept (by comparing true and ran-
dom relata), and to identify specific types of relata,
both in terms of semantic relation and part of speech.
A data set intending to represent a gold standard
for evaluation should include tests items that are as
little controversial as possible. The choice of re-
stricting BLESS to concrete concepts is motivated
by the fact that they are by far the most studied ones,
and there is better agreement about the relations that
characterize them (Murphy, 2002; Rogers and Mc-
Clelland, 2004).
As for the types of relation to include, we are
faced with a dilemma. On the one hand, there is
wide evidence that taxonomic relations, the best un-
derstood type, only represent a tiny portion of the
rich spectrum covered by semantic relatedness. On
the other hand, most of these wider semantic rela-
3
tions are also highly controversial, and may easily
lead to questionable classifications. For instance,
concepts are related to events, but often it is not clear
how to distinguish the events expressing a typical
function of nominal concepts (e.g., car and trans-
port), from those events that are also strongly re-
lated to them but without representing their typical
function sensu stricto (e.g., car and fix). As will be
shown in Section 4, the BLESS data set tries to over-
come this dilemma by attempting a difficult com-
promise: Semantic relations are not limited to tax-
onomic types and also include attributes and events
strongly related to a concept, but in these cases we
have resorted to underspecification, rather than com-
mitting ourselves to questionable granular relations.
BLESS strives to capture those differences and
similarities among DSMs that do not depend on
coverage, processing choices or lexical preferences.
BLESS has been constructed using a publicly avail-
able collection of corpora for reference (see Section
4.4 below), which means that anybody can train a
DSM on the same data and be sure to have perfect
coverage (but this is not strictly necessary). For each
concept and relation, we pick a variety of relata (see
next section) in order to abstract away from inciden-
tal gaps of models or different lexical/topical prefer-
ences. For example, the concept robin has 7 hyper-
nyms including the very general and non-technical
animal and bird and the more specific and techni-
cal passerine. A model more geared toward techni-
cal terminology might assign a high similarity score
to the latter, whereas a commonsense-knowledge-
oriented DSM might pick bird. Both models have
captured similarity with a hypernym, and we have
no reason, in general semantic terms, to penalize one
or the other. To maximize coverage, we also make
sure that, for each concept and relation, a reason-
able number of relata are frequently attested in our
reference corpora (see statistics below), we only in-
clude single-word relata and, where appropriate, we
include multiple forms for the same relatum (both
sock and socks as coordinates of scarf ? as discussed
in Section 4.1, we avoided similar ambiguous items
as target concepts).
Currently, distributional models for attributional
similarity and relational similarity (Turney, 2006)
are tested on different data sets, e.g., TOEFL and
SAT respectively (briefly, attributional similarity
pertains to similarity between a pair of concepts in
terms of shared properties, whereas relational sim-
ilarity measures the similarity of the relations in-
stantiated by couples of concept pairs). Conversely,
BLESS is not biased towards any particular type of
semantic similarity and thus allows both families of
models to be evaluated on the same data set. Given
a concept, we can analyze the types of relata that are
selected by a model as more attributionally similar
to the target. Alternatively, given a concept-relatum
pair instantiating a specific semantic relation (e.g.,
hypernymy) we can evaluate a model ability to iden-
tify analogically similar pairs, i.e., others concept-
relatum pairs instantiating the same relation (we do
not illustrate this possibility here).
Finally, by collecting distributions of 200 similar-
ity values for each relation, BLESS allows reliable
statistical testing of the significance of differences
in similarity within a DSM (for example, using the
procedure we present in Section 5 below), as well
as across DSMs (for example, via a linear/ANOVA
model with relations and DSMs as factors ? not il-
lustrated here).
4 Construction
4.1 Concepts
BLESS includes 200 distinct English concrete
nouns as target concepts, equally divided be-
tween living and non-living entities. Concepts
have been grouped into 17 broader classes: AM-
PHIBIAN REPTILE (including amphibians and rep-
tiles: alligator), APPLIANCE (toaster), BIRD
(crow), BUILDING (cottage), CLOTHING (sweater),
CONTAINER (bottle), FRUIT (banana), FURNI-
TURE (chair), GROUND MAMMAL (beaver), IN-
SECT (cockroach), MUSICAL INSTRUMENT (vio-
lin), TOOL (i.e., manipulable tools or devices: ham-
mer), TREE (birch), VEGETABLE (cabbage), VEHI-
CLE (bus), WATER ANIMAL (including fish and sea
mammals: herring), WEAPON (dagger).
All 200 BLESS concepts are single-word nouns
in the singular form (we avoided concepts such as
socks whose surface form might change depending
on lemmatization choices). The major source we
used to select the concepts were the McRae Norms
(McRae et al, 2005), a collection of living and non-
living basic-level concepts described by 725 sub-
4
jects with semantic features, each tagged with its
property type. As further constraints guiding our
selection, we wanted concepts with a reasonably
high frequency (cf. Section 4.4), we avoided am-
biguous or highly polysemous concepts and we bal-
anced inter- and intra-class composition. Classes in-
clude both prototypical and atypical instances (e.g.,
robin and penguin for BIRD), and have a wide spec-
trum of internal variation (e.g., the class VEHICLE
contains wheeled, air and sea vehicles). 175 BLESS
concepts are attested in the McRae Norms, while the
remnants were selected by the authors according to
the above constraints. The average number of con-
cepts per class is 11.76 (median 11; min. 5 AMPHIB-
IAN REPTILE; max. 21 GROUND MAMMAL).
4.2 Relations
For each concept noun, BLESS includes several
relatum words, linked to the concept by one of
the following 5 relations. COORD: the relatum
is a noun that is a co-hyponym (coordinate) of
the concept, i.e., they belong to the same (nar-
rowly or broadly defined) semantic class: alligator-
coord-lizard; HYPER: the relatum is a noun that
is a hypernym of the concept: alligator-hyper-
animal; MERO: the relatum is a noun referring
to a part/component/organ/member of the concept,
or something that the concept contains or is made
of: alligator-mero-mouth; ATTRI: the relatum is
an adjective expressing an attribute of the concept:
alligator-attri-aquatic; EVENT: the relatum is a
verb referring to an action/activity/happening/event
the concept is involved in or is performed by/with
the concept: alligator-event-swim. BLESS also
includes the relations RAN.N, RAN.J and RAN.V,
which relate the target concepts to control tuples
with random noun, adjective and verb relata, respec-
tively.
The BLESS relations cover a wide spectrum of
information useful to describe a target concept and
to qualify the notion of semantic relatedness: taxo-
nomically related entities (hyper and coord), typical
attributes (attri), components (mero), and associated
events (event). However, except for hyper and co-
ord (corresponding to the standard relations of class
inclusion and co-hyponymy respectively), the other
BLESS relations are highly underspecified. For in-
stance, mero corresponds to a very broad notion of
meronymy, including not only parts (dog-tail), but
also the material (table-wood) as well as the mem-
bers (hospital-patient) of the entity the target con-
cept refers to (Winston et al, 1987); event is used to
represent the behaviors of animals (dog-bark), typi-
cal functions of instruments (violin-play), and events
that are simply associated with the target concept
(car-park); attri captures a large range of attributes,
from physical (elephant-big) to evaluative ones (car-
expensive). As we said in section 3, we did not at-
tempt to further specify these relations to avoid any
commitment to controversial ontologies of property
types. Note that we exclude synonymy both because
of the inherent problems in this very notion (Cruse,
1986), and because it is impossible to find convinc-
ing synonyms for 200 concrete concepts.
In BLESS, we have adopted the simplifying as-
sumption that each relation type has relata belonging
to the same part of speech: nouns for hyper, coord
and mero, verbs for event, and adjectives for attri.
Therefore, we abstract away from the fact that the
same semantic relation can be realized with different
parts of speech, e.g., a related event can be expressed
by a verb (transport) or by a noun (transportation).
4.3 Relata
The relata of the non-random relations are English
nouns, verbs and adjectives selected and validated
by both authors using two types of sources: se-
mantic sources (the McRae Norms (McRae et al,
2005), WordNet (Fellbaum, 1998) and ConceptNet
(Liu and Singh, 2004)) and text sources (Wikipedia
and the Web-derived ukWaC corpus, see Section 4.4
below). These resources greatly differ in dimension,
origin and content and therefore provide comple-
mentary views on relata. Their relative contribution
to BLESS also depends on the type of relation and
the target concept. For instance, the rich taxonomic
structure of WordNet has been the main source of in-
formation for many technical hypernyms (e.g. gym-
nosperm, oscine), which instead are missing from
more commonsense-oriented resources such as the
McRae Norms and ConceptNet. Meronyms are
rarer in WordNet, and were collected mainly from
the latter two resources, with many technical terms
(e.g., parts of ships, weapons) harvested from the
Wikipedia entries for the target concepts.
Attributes and events were collected from McRae
5
Norms, ConceptNet and ukWaC. In the McRae
Norms, the number of features per concept is fairly
limited, but they correspond to highly distinctive,
prototypical and cognitively salient properties. Con-
ceptNet instead provides a much wider array of as-
sociated events and attributes that are part of our
commonsense knowledge about the target concepts
(e.g., the events park, steal and break, etc. for car).
ConceptNet relations such as Created by, Used for,
Capable of etc. have been analyzed to identify po-
tential event relata, while the Has property relation
has been inspected to look for attributes. The most
salient adjectival and verbal collocates of the tar-
get nouns in the ukWaC corpus were also used to
identify associated attributes and events. For in-
stance, the target concept elephant is not attested in
the McRae Norms and has few properties in Con-
ceptNet. Thus, many of its related events have been
harvested from ukWaC. They include verbs such as
hunt, kill, etc. which are quite salient and frequent
with respect to elephants, although they can hardly
be defined as prototypical properties of this animal.
As a result of the combined use of such different
types of sources, the BLESS relata are representative
of a wide spectrum of semantic information about
the target concepts: they include domain-specific
terms side by side to commonsense ones, very dis-
tinctive features of a concept (e.g., hoot for owl)
together with attributes and events that are instead
shared by a whole class of concepts (e.g., all animals
have relata such as eat, feed, and live), prototypical
features as well as events and attributes that are sta-
tistically salient for the target, etc.
In many cases, the concept properties contained
in semantic sources are expressed with phrases, e.g.,
lay eggs, eat grass, live in Africa, etc. We decided,
however, to keep only single-word relata in BLESS,
because DSMs are typically populated with single
words, and, when they are not, they differ in the
kinds of multi-word elements they store. There-
fore, phrasal relata have always been reduced to
their head: a verb for properties expressed by a verb
phrase, and a noun for properties expressed by a
noun phrase. For instance, from the property lay
eggs, we derived the event relatum lay.
To extract the random relata, we adopted the fol-
lowing procedure. For each relatum that instantiates
a true relation with the concept, we also randomly
picked from our combined corpus (cf. Section 4.4)
another lemma with the same part of speech, and
frequency within 1 absolute logarithmic unit from
the frequency of the corresponding true relatum.
Since picking a random term does not guarantee
that it will not be related to the concept, we filtered
the extracted list by crowdsourcing, using the Ama-
zon Mechanical Turk via the CrowdFlower interface
(CF).1 We presented CF workers with the list of
about 15K concept+random-term pairs selected with
the procedure we just described, plus a manually
checked validation set (a ?gold set? in CF terminol-
ogy) comprised of 500 concept+true-relatum pairs
and 500 concept+random-term pairs (these elements
are used by CF to determine the reliability of work-
ers, and discard the ratings of unreliable ones), plus a
further set of 1.5K manually checked concept+true-
relatum pairs to make the random-true distribution
less skewed. The workers? task was, for each pair,
to check a YES radio button if they thought there is
a relation between the words, NO otherwise. The
words were annotated with their part of speech, and
workers were instructed to pay attention to this in-
formation when making their choices. Extensive
commented examples of both related pairs and un-
related ones were also provided in the instruction
page. A minimum of 2 CF workers rated each pair,
and, conservatively, we preserved only those items
(about 12K) that were unanimously rated as unre-
lated to their concept by the judges. See Table 1 for
summary statistics about the preserved random sets
(nouns: RAND.N, adjectives: RAN.J, verbs:RAN.V).
4.4 BLESS statistics
For frequency information, we rely on the combi-
nation of the freely available ukWaC and Wackype-
dia corpora (size: 1.915B and 820M tokens, respec-
tively).2 The data set contains 200 concepts that
have a mean corpus frequency of 53K occurrences
(min. 1416 chisel, max. 793K car). The relata of
these concepts (26,554 in total) are distributed as re-
ported in Table 1.
Note that the distributions reflect certain ?natural?
differences between relations (hypernyms tend to be
more frequent words than coordinates, but there are
1http://crowdflower.com/
2http://wacky.sslmit.unibo.it/
6
frequency cardinality
relation min avg max min avg max
COORD 0 37K 1.7M 6 17.1 35
HYPER 31 138K 1.9M 2 6.7 15
MERO 0 133K 2M 2 14.7 53
ATTRI 0 501K 3.7M 4 13.6 27
EVENT 0 517K 5.4M 6 19.1 40
RAN.N 0 92K 2.4M 16 32.9 67
RAN.J 1 472K 4.5M 3 10.9 24
RAN.V 1 508K 7.7M 4 16.3 34
Table 1: Distribution (minimum, mean and maximum) of
the relata of all BLESS concepts: the frequency columns
report summary statistics for corpus counts across relata
instantiating a relation; the cardinality columns report
summary statistics for number of relata instantiating a
relation across the 200 concepts, only considering relata
with corpus frequency ? 100.
more coordinates than hypernyms, etc.). Instead of
trying to artificially control for these differences, we
assess their impact in Section 5 by looking at the
behavior of baselines that exploit the frequency and
cardinality of relations as proxies to semantic simi-
larity (such factors could also be entered as regres-
sors in a linear model).
5 Evaluation
This section illustrates one possible way to use
BLESS to explore and evaluate DSMs. Given the
similarity scores provided by a model for a concept
with all its relata across all relations, we pick the re-
latum with the highest score (nearest neighbour) for
each relation (see discussion in Section 3 above on
why we allow models to pick their favorite from a
set of relata instantiating the same relation). In this
way, for each of the 200 BLESS concepts, we obtain
8 similarity scores, one per relation. In order to fac-
tor out concept-specific effects that might add to the
overall score variance (for example, a frequent con-
cept might have a denser neighborhood than a rarer
one, and consequently the nearest relatum scores of
the former are trivially higher than those of the lat-
ter), we transform the 8 similarity scores of each
concept onto standardized z scores (mean: 0; s.d: 1)
by subtracting from each their mean, and dividing by
their standard deviation. After this transformation,
we produce a boxplot summarizing the distribution
of scores per relation across the 200 concepts (i.e.,
each box of the plot summarizes the distribution of
the 200 standardized scores picked for each rela-
tion). Our boxplots (see examples in Fig. 1 below)
display the median of a distribution as a thick hori-
zontal line within a box extending from the first to
the third quartile, with whiskers covering 1.5 of the
interquartile range in each direction from the box,
and values outside this extended range ? extreme
outliers ? plotted as circles (these are the default
boxplotting option of the R statistical package).3
While the boxplots are extremely informative about
the relation types that are best captured by models,
we expect some degree of overlap among the distri-
butions of different relations, and in such cases we
might want to ask whether a certain model assigns
significantly higher scores to one relation rather than
another (for example, to coordinates rather than ran-
dom nouns). It is difficult to decide a priori which
pairwise statistical comparisons will be interesting.
We thus take a conservative approach in which we
perform all pairwise comparisons using the Tukey
Honestly Significant Difference test, that is simi-
lar to the standard t test, but accounts for the greater
likelihood of Type I errors when multiple compar-
isons are performed (Abdi and Williams, 2010). We
only report the Tukey test results for those com-
parisons that are of interest in the analysis of the
boxplots, using the standard ? = 0.05 significance
threshold.
5.1 Models
Occurrence and co-occurrence statistics for all mod-
els are extracted from the combined ukWaC and
Wackypedia corpora (see Section 4.4 above). We ex-
ploit the automated morphosyntactic annotation of
the corpora by building our DSMs out of lemmas
(instead of inflected words), and relying on part of
speech information.
Baselines. The RelatumFrequency baseline uses
the frequency of occurrence of a relatum as a sur-
rogate of its cosine with the concept. With this ap-
proach, we want to verify that the unequal frequency
distribution across relations (see Table 1 above) is
not trivially sufficient to differentiate relation classes
in a semantically interesting way. For our second
baseline, we assign a random number as cosine sur-
3http://www.r-project.org/
7
rogate to each relatum (to smooth these random val-
ues, we generate them by first sampling, for each
relatum, 10K random variates from a uniform distri-
bution, and then averaging them). If the set of relata
instantiating a certain relation is larger, it is more
likely that it will contain the highest random value.
Thus, this RelationCardinality baseline will favor
relations that tend to have large relata set across con-
cepts, controlling for effects due to different cardi-
nalities across semantic relations (again, see Table 1
above).
DSMs. We choose a few ways to construct DSMs
for illustrative purposes only. All the models contain
vector representations for the same words, namely,
approximately, the top 20K most frequent nouns, 5K
most frequent adjectives and 5K most frequent verbs
in the combined corpora. All the models use Local
Mutual Information (Evert, 2005; Baroni and Lenci,
2010) to weight raw co-occurrence counts (this asso-
ciation measure is obtained by multiplying the raw
count by Pointwise Mutual Information, and it is a
close approximation to the Log-Likelihood Ratio).
Three DSMs are based on counting co-occurrences
with collocates within a window of fixed width,
in the tradition of HAL (Lund and Burgess, 1996)
and many later models. The ContentWindow2
model records sentence-internal co-occurrence with
the nearest 2 content words to the left and right
of each target concept (the same 30K target nouns,
verbs and adjectives are also employed as context
content words). ContentWindow20 is like Con-
tentWindow2, but considers a larger window of 20
words to the left and right of the target. AllWin-
dow2 adopts the same window of ContentWindow2,
but considers all co-occurrences, not only those with
content words. The Document model, finally, is
based on a (Local-Mutual-Information transformed)
word-by-document matrix, recording the distribu-
tion of the 30K target words across the documents in
the concatenated corpus. This DSM is thus akin to
traditional Latent Semantic Analysis (Landauer and
Dumais, 1997), without dimensionality reduction.
The content-window-based models have, by con-
struction, about 30K dimensions. The other models
are much larger, and for practical reasons we only
keep 1 million dimensions (those that account, cu-
mulatively, for the largest proportion of the overall
Local Mutual Information mass).
5.2 Results
The concept-by-concept z-normalized distributions
of cosines of relata instantiating each of our rela-
tions are presented, for each of the example mod-
els, in Fig. 1. The RelatumFrequency baseline
shows a preference for adjectives and verbs in gen-
eral, independently of whether they are meaningful
(attributes, events) or not (random adjectives and
verbs), reflecting the higher frequencies of adjec-
tives and verbs in BLESS (Table 1). The Relation-
Cardinality baseline produces even less interesting
results, with a strong preference for random nouns,
followed by coordinates, events and random verbs
(as predicted by the distribution in Table 1). We can
conclude that the semantically meaningful patterns
produced by the other models cannot be explained
by trivial differences in relatum frequency or rela-
tion cardinality in the BLESS data set.
Moving then to the real DSMs, ContentWindow2
essentially partitions the relations into 3 groups: co-
ordinates are the closest relata, which makes sense
since they are, taxonomically, the most similar en-
tities to target concepts. They are followed by (but
significantly closer to the concept than) events, hy-
pernyms and meronyms (events and hypernyms sig-
nificantly above meronyms). Next come the at-
tributes (significantly lower cosines than all relation
types above). All the meaningful relata are signif-
icantly closer to the concepts than the random re-
lata. Similar patterns can be observed in the Con-
tentWindow20 distribution, however in this case the
events, while still significantly below the coordi-
nates, are significantly above the (statistically in-
distinguishable) hypernym, meronym and attribute
set. Again, all meaningful relata are above the ran-
dom ones. Both content-window-based models pro-
vide reasonable results, with ContentWindow2 be-
ing probably closer to our ?ontological? intuitions.
The high ranking of events is probably explained
by the fact that a nominal concept will often ap-
pear as subject or object of verbs expressing asso-
ciated events (dog barks, fishing tuna), and thus the
corresponding verbs will share even relatively nar-
row context windows with the concept noun. The
AllWindow2 distribution probably reflects the fact
that many contexts picked by this DSM are function
8
?
??
?
?
?
??
?
??
?
?
?
?
?
?
?? ?
?
?
?
??
?
?
?
?
????
??
??
?
?
?
?
?
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
RelatumFrequency
?
?
?
??
??
??
????
????
???
?
?
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2 RelationCardinality
???
?
?
?
?
?
??
?
?
?
?
?
?
???
?
?
?
?? ?
?
???
?
?
?
??
?
?
?
?
???
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
ContentWindow2
?
???
?
??
?
??
?
??
?
?
??
?
?
?
?
?
??
??
?
?
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
ContentWindow20
??
???
?
?
?
?
?
?
?
?
?
??
?
?
?
??
??
?
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
AllWindow2
?
?
?
?
?
?
?
??
?
?
?
?
?
??
?? ?
?
??
?
??
?
?
?
?
?
?
??
?
?
???
COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V
?2
?1
0
1
2
Document
Figure 1: Distribution of relata cosines across concepts (values on ordinate are cosines after concept-by-concept z-
normalization).
words, and thus they capture syntactic, rather than
semantic distributional properties. As a result, ran-
dom nouns are as high (statistically indistinguish-
able from) hypernyms and meronyms. Interestingly,
attributes also belong to this subset of relations ?
probably due to the effect of determiners, quantifiers
and other DP-initial function words, that will often
occur both before nouns and before adjectives. In-
deed, even random adjectives, although significantly
below the other relations we discussed, are signif-
icantly above both random and meaningful verbs
(i.e., events). For the Document model, all mean-
ingful relations are significantly above the random
ones. However, coordinates, while still the nearest
neighbours (significantly closer than all other rela-
tions) are much less distinct than in the window-
based models. Note that we cannot say a priori that
ContentWindow2 is better than Document because
it favors coordinates. However, while they are both
able to sort out true and random relata, the latter
shows a weaker ability to discriminate among differ-
ent types of semantic relations (co-occurring within
a document is indeed a much looser cue to similarity
than specifically co-occurring within a narrow win-
dow). Traditional DSM tests, based on a single qual-
ity measure, would not have given us this broad view
of how models are behaving.
6 Conclusion
We introduced BLESS, the first data set specifically
designed for the intrinsic evaluation of DSMs. The
data set contains tuples instantiating different, ex-
plicitly typed semantic relations, plus a number of
controlled random tuples. Thus, BLESS can be used
to evaluate both the ability of DSMs to discriminate
truly related word pairs, and to perform in-depth
analyses of the types of semantic relata that different
models tend to favor among the nearest neighbors of
a target concept. Even a simple comparison of the
performance of a few DSMs on BLESS - like the
one we have shown here - is able to highlight inter-
esting differences in the semantic spaces produced
by the various models. The success of BLESS will
obviously depend on whether it will become a refer-
ence model for the evaluation of DSMs, something
that can not be foreseen a priori. Whatever its des-
tiny, we believe that the BLESS approach can boost
and innovate evaluation in distributional semantics,
as a key condition to get at a deeper understanding
of its potentialities as a viable model for meaning.
9
References
Herv Abdi and Lynne Williams. 2010. Newman-Keuls
and Tukey test. In N.J. Salkind, D.M. Dougherty, and
B. Frey, editors, Encyclopedia of Research Design.
Sage, Thousand Oaks, CA.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasc?a, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL, pages 19?27, Boulder, CO.
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. Bridging the Gap between Semantic The-
ory and Computational Simulations: Proceedings of
the ESSLLI Workshop on Distributional Lexical Se-
mantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32:13?47.
D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-
versity Press, Cambridge.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems,
20(1):116?131.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
tical commonsense reasoning toolkit. BT Technology
Journal, pages 211?226.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Ken McRae, George Cree, Mark Seidenberg, and Chris
McNorgan. 2005. Semantic feature production norms
for a large set of living and nonliving things. Behavior
Research Methods, 37(4):547?559.
GeorgeMiller andWalter Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
Gregory Murphy. 2002. The Big Book of Concepts. MIT
Press, Cambridge, MA.
Timothy Rogers and James McClelland. 2004. Seman-
tic Cognition: A Parallel Distributed Processing Ap-
proach. MIT Press, Cambridge, MA.
Herbert Rubenstein and John Goodenough. 1965. Con-
textual correlates of synonymy. Communications of
the ACM, 8(10):627?633.
Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-
ing Natural Language Processing Systems: An Analy-
sis and Review. Springer Verlag, Berlin.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Morton E. Winston, Roger Chaffin, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11:417?444.
10
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, page 32,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Unseen features.
Collecting semantic data from congenital blind subjects
Alessandro Lenci?, Marco Baroni?, Giovanna Marotta?
?University of Pisa (Italy)
?University of Trento (Italy)
alessandro.lenci@ling.unipi.it, marco.baroni@unitn.it, gmarotta@ling.unipi.it
Congenital blind subjects are able to learn how to use color terms and other types of vision-related
words in a way that is de facto undistinguishable from sighted people. It has actually been proposed
that language provides a rich source of information that blind subjects can exploit to acquire aspects
of word meaning that are related to visual experience, such as the color of fruits or animals. Despite
this, whether and how sensory deprivation affects the structure of semantic representations is still an
open question. In this talk, we present a new, freely available collection of feature norms produced by
congenital blind subjects and normal sighted people. Subjects were asked to produce semantic features
describing the meaning of concrete and abstract nouns and verbs. Data were collected from Italian
subjects, translated into English, and categorized with respect to their semantic type (e.g. hypernym,
meronym, physical property, etc.). First analyses of the feature norms highlight important differences
between blind and sighted subjects, for instance for the role of color and other visual features in the
produced semantic descriptions. This resource can provide new evidence on the role of perceptual
experience in shaping concepts, as well as on its interplay with information extracted from linguistic
data. The norms will also be used to carry out computational experiments with distributional semantic
models to simulate blind and sighted semantic spaces.
32
Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 36?43,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
The PAIS
`
A Corpus of Italian Web Texts
Verena Lyding
?
verena.lyding@eurac.edu
Egon Stemle
?
egon.stemle@eurac.edu
Claudia Borghetti
?
claudia.borghetti@unibo.it
Marco Brunello
?
marcobrunello84@gmail.com
Sara Castagnoli
?
s.castagnoli@unibo.it
Felice Dell?Orletta
?
felice.dellorletta@ilc.cnr.it
Henrik Dittmann
?
henrik.dittmann@bordet.be
Alessandro Lenci
?
alessandro.lenci@ling.unipi.it
Vito Pirrelli
?
vito.pirrelli@ilc.cnr.it
Abstract
PAIS
`
A is a Creative Commons licensed,
large web corpus of contemporary Italian.
We describe the design, harvesting, and
processing steps involved in its creation.
1 Introduction
This paper provides an overview of the PAIS
`
A cor-
pus of Italian web texts and an introductory de-
scription of the motivation, procedures and facili-
ties for its creation and delivery.
Developed within the PAIS
`
A project, the cor-
pus is intended to meet the objective to help over-
come the technological barriers that still prevent
web users from making use of large quantities of
contemporary Italian texts for language and cul-
tural education, by creating a comprehensive and
easily accessible corpus resource of Italian.
The initial motivation of the initiative stemmed
from the awareness that any static repertoire of
digital data, however carefully designed and de-
veloped, is doomed to fast obsolescence, if con-
tents are not freely available for public usage, con-
tinuously updated and checked for quality, incre-
mentally augmented with new texts and annota-
tion metadata for intelligent indexing and brows-
ing. These requirements brought us to design a
resource that was (1) freely available and freely
re-publishable, (2) comprehensively covering con-
temporary common language and cultural content
and (3) enhanced with a rich set of automatically-
annotated linguistic information to enable ad-
vanced querying and retrieving of data. On top
?
EURAC Research Bolzano/Bozen, IT
?
University of Bologna, IT
?
University of Leeds, UK
?
Institute of Computational Linguistics ?Antonio Zam-
polli? - CNR, IT
?
Institut Jules Bordet, BE
?
University of Pisa, IT
of that, we set out to develop (4) a dedicated in-
terface with a low entry barrier for different target
groups. The end result of this original plan repre-
sents an unprecedented digital language resource
in the Italian scenario.
The main novelty of the PAIS
`
A web corpus is
that it exclusively draws on Creative Commons li-
censed data, provides advanced linguistic annota-
tions with respect to corpora of comparable size
and corpora of web data, and invests in a carefully
designed query interface, targeted at different user
groups. In particular, the integration of richly an-
notated language content with an easily accessible,
user-oriented interface makes PAIS
`
A a unique and
flexible resource for language teaching.
2 Related Work
The world wide web, with its inexhaustible
amount of natural language data, has become an
established source for efficiently building large
corpora (Kilgarriff and Grefenstette, 2003). Tools
are available that make it convenient to bootstrap
corpora from the web based on mere seed term
lists, such as the BootCaT toolkit (Baroni and
Bernardini, 2004). The huge corpora created by
the WaCky project (Baroni et al., 2009) are an ex-
ample of such an approach.
A large number of papers have recently been
published on the harvesting, cleaning and pro-
cessing of web corpora.
1
However, freely avail-
able, large, contemporary, linguistically anno-
tated, easily accessible web corpora are still miss-
ing for many languages; but cf. e.g. (G?en?ereux
et al., 2012) and the Common Crawl Foundations
(CCF) web crawl
2
.
1
cf. the Special Interest Group of the Association for
Computational Linguistics on Web as Corpus (SIGWAC)
http://sigwac.org.uk/
2
CCF produces and maintains a repository of web crawl
data that is openly accessible: http://commoncrawl.
org/
36
3 Corpus Composition
3.1 Corpus design
PAIS
`
A aimed at creating a comprehensive corpus
resource of Italian web texts which adheres to the
criteria laid out in section 1. For these criteria to
be fully met, we had to address a wide variety of
issues covering the entire life-cycle of a digital text
resource, ranging from robust algorithms for web
navigation and harvesting, to adaptive annotation
tools for advanced text indexing and querying and
user-friendly accessing and rendering online inter-
faces customisable for different target groups.
Initially, we targeted a size of 100M tokens, and
planned to automatically annotate the data with
lemma, part-of-speech, structural dependency, and
advanced linguistic information, using and adapt-
ing standard annotation tools (cf. section 4). In-
tegration into a querying environment and a dedi-
cated online interface were planned.
3.2 Licenses
A crucial point when planning to compile a cor-
pus that is free to redistribute without encounter-
ing legal copyright issues is to collect texts that are
in the public domain or at least, have been made
available in a copyleft regime. This is the case
when the author of a certain document decided to
share some rights (copy and/or distribute, adapt
etc.) on her work with the public, in a way that
end users do not need to ask permission to the cre-
ator/owner of the original work. This is possible
by employing licenses other than the traditional
?all right reserved? copyright, i.e. GNU, Creative
Commons etc., which found a wide use especially
on the web. Exploratory studies (Brunello, 2009)
have shown that Creative Commons licenses are
widely employed throughout the web (at least on
the Italian webspace), enough to consider the pos-
sibility to build a large corpus from the web ex-
clusively made of documents released under such
licenses.
In particular, Creative Commons provides five
basic ?baseline rights?: Attribution (BY), Share
Alike (SA), Non Commercial (NC), No Deriva-
tive Works (ND). The licenses themselves are
composed of at least Attribution (which can be
used even alone) plus the other elements, al-
lowing six different combinations:
3
(1) Attribu-
tion (CC BY), (2) Attribution-NonCommercial
3
For detailed descriptions of each license see http://
creativecommons.org/licenses/
(CC BY-NC), (3) Attribution-ShareAlike (CC BY-
SA), (4) Attribution-NoDerivs (CC BY-ND), (5)
Attribution-NonCommercial-ShareAlike (CC BY-
NC-SA), and (6) Attribution-NonCommercial-
NoDerivs (CC BY-NC-ND).
Some combinations are not possible because
certain elements are not compatible, e.g. Share
Alike and No Derivative Works. For our purposes
we decided to discard documents released with the
two licenses containing the No Derivative Works
option, because our corpus is in fact a derivative
work of collected documents.
3.3 The final corpus
The corpus contains approximately 388,000 docu-
ments from 1,067 different websites, for a total of
about 250M tokens. All documents contained in
the PAIS
`
A corpus date back to Sept./Oct. 2010.
The documents come from several web sources
which, at the time of corpus collection, provided
their content under Creative Commons license
(see section 3.2 for details). About 269,000 texts
are from Wikimedia Foundation projects, with
approximately 263,300 pages from Wikipedia,
2380 pages from Wikibooks, 1680 pages from
Wikinews, 740 pages from Wikiversity, 410 pages
from Wikisource, and 390 Wikivoyage pages.
The remaining 119,000 documents come
from guide.supereva.it (ca. 19,000),
italy.indymedia.org (ca. 10,000) and
several blog services from more than another
1,000 different sites (e.g. www.tvblog.it
(9,088 pages), www.motoblog.it (3,300),
www.ecowebnews.it (3,220), and
www.webmasterpoint.org (3,138).
Texts included in PAIS
`
A have an average length
of 683 words, with the longest text
4
counting
66,380 running tokens. A non exhaustive list of
average text lengths by source type is provided in
table 1 by way of illustration.
The corpus has been annotated for lemma, part-
of-speech and dependency information (see sec-
tion 4.2 for details). At the document level, the
corpus contains information on the URL of origin
and a set of descriptive statistics of the text, includ-
ing text length, rate of advanced vocabulary, read-
ability parameters, etc. (see section 4.3). Also,
each document is marked with a unique identifier.
4
The European Constitution from wikisource.org:
http://it.wikisource.org/wiki/Trattato_
che_adotta_una_Costituzione_per_l?Europa
37
Document source Avg text length
PAIS
`
A total 683 words
Wikipedia 693 words
Wikibooks 1844 words
guide.supereva.it 378 words
italy.indymedia.it 1147 words
tvblog.it 1472 words
motoblog.it 421 words
ecowebnews.it 347 words
webmasterpoint.org 332 words
Table 1: Average text length by source
The annotated corpus adheres to the stan-
dard CoNLL column-based format (Buchholz and
Marsi, 2006), is encoded in UTF-8.
4 Corpus Creation
4.1 Collecting and cleaning web data
The web pages for PAIS
`
A were selected in two
ways: part of the corpus collection was made
through CC-focused web crawling, and another
part through a targeted collection of documents
from specific websites.
4.1.1 Seed-term based harvesting
At the time of corpus collection (2010), we used
the BootCaT toolkit mainly because collecting
URLs could be based on the public Yahoo! search
API
5
, including the option to restrict search to CC-
licensed pages (including the possibility to specify
even the particular licenses). Unfortunately, Ya-
hoo! discontinued the free availability of this API,
and BootCaT?s remaining search engines do not
provide this feature.
An earlier version of the corpus was collected
using the tuple list originally employed to build
itWaC
6
. As we noticed that the use of this list, in
combination with the restriction to CC, biased the
final results (i.e. specific websites occurred very
often as top results) , we provided as input 50,000
medium frequent seed terms from a basic Italian
vocabulary list
7
, in order to get a wider distribu-
tion of search queries, and, ultimately, of texts.
As introduced in section 3.2, we restricted the
selection not just to Creative Commons-licensed
5
http://developer.yahoo.com/boss/
6
http://wacky.sslmit.unibo.it/doku.
php?id=seed_words_and_tuples
7
http://ppbm.paravia.it/dib_lemmario.
php
texts, but specifically to those licenses allowing
redistribution: namely, CC BY, CC BY-SA, CC
BY-NC-SA, and CC BY-NC.
Results were downloaded and automatically
cleaned with the KrdWrd system, an environment
for the unified processing of web content (Steger
and Stemle, 2009).
Wrongly CC-tagged pages were eliminated us-
ing a black-list that had been manually populated
following inspection of earlier corpus versions.
4.1.2 Targeted
In September 2009, the Wikimedia Foundation de-
cided to release the content of their wikis under
CC BY-SA
8
, so we decided to download the large
and varied amount of texts made available through
the Italian versions of these websites. This was
done using the Wikipedia Extractor
9
on official
dumps
10
of Wikipedia, Wikinews, Wikisource,
Wikibooks, Wikiversity and Wikivoyage.
4.2 Linguistic annotation and tools
adaptation
The corpus was automatically annotated with
lemma, part-of-speech and dependency infor-
mation, using state-of-the-art annotation tools
for Italian. Part-of-speech tagging was per-
formed with the Part-Of-Speech tagger described
in Dell?Orletta (2009) and dependency-parsed by
the DeSR parser (Attardi et al., 2009), using Mul-
tilayer Perceptron as the learning algorithm. The
systems used the ISST-TANL part-of-speech
11
and dependency tagsets
12
. In particular, the pos-
tagger achieves a performance of 96.34% and
DeSR, trained on the ISST-TANL treebank con-
sisting of articles from newspapers and period-
icals, achieves a performance of 83.38% and
87.71% in terms of LAS (labelled attachment
score) and UAS (unlabelled attachment score) re-
spectively, when tested on texts of the same type.
However, since Gildea (2001), it is widely ac-
knowledged that statistical NLP tools have a drop
of accuracy when tested against corpora differing
from the typology of texts on which they were
trained. This also holds true for PAIS
`
A: it contains
8
Previously under GNU Free Documentation License.
9
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
10
http://dumps.wikimedia.org/
11
http://www.italianlp.it/docs/
ISST-TANL-POStagset.pdf
12
http://www.italianlp.it/docs/
ISST-TANL-DEPtagset.pdf
38
lexical and syntactic structures of non-canonical
languages such as the language of social media,
blogs, forum posts, consumer reviews, etc. As re-
ported in Petrov and McDonald (2012), there are
multiple reasons why parsing the web texts is dif-
ficult: punctuation and capitalization are often in-
consistent, there is a lexical shift due to increased
use of slang and technical jargon, some syntactic
constructions are more frequent in web text than
in newswire, etc.
In order to overcome this problem, two main ty-
pologies of methods and techniques have been de-
veloped: Self-training (McClosky et al., 2006) and
Active Learning (Thompson et al., 1999).
For the specific purpose of the NLP tools adap-
tation to the Italian web texts, we adopted two dif-
ferent strategies for the pos-tagger and the parser.
For what concerns pos-tagging, we used an active
learning approach: given a subset of automatically
pos-tagged sentences of PAIS
`
A, we selected the
ones with the lowest likelihood, where the sen-
tence likelihood was computed as the product of
the probabilities of the assignments of the pos-
tagger for all the tokens. These sentences were
manually revised and added to the training corpus
in order to build a new pos-tagger model incor-
porating some new knowledge from the target do-
main.
For what concerns parsing, we used a self-
training approach to domain adaptation described
in Dell?Orletta et al. (2013), based on ULISSE
(Dell?Orletta et al., 2011). ULISSE is an unsu-
pervised linguistically-driven algorithm to select
reliable parses from a collection of dependency
annotated texts. It assigns to each dependency
tree a score quantifying its reliability based on a
wide range of linguistic features. After collect-
ing statistics about selected features from a cor-
pus of automatically parsed sentences, for each
newly parsed sentence ULISSE computes a reli-
ability score using the previously extracted feature
statistics. From the top of the parses (ranked ac-
cording to their reliability score) different pools of
parses were selected to be used for training. The
new training contains the original training set as
well as the new selected parses which include lex-
ical and syntactic characteristics specific of the tar-
get domain (Italian web texts). The parser trained
on this new training set improves its performance
when tested on the target domain.
We used this domain adaptation approach for
the following three main reasons: a) it is unsuper-
vised (i.e. no need for manually annotated training
data); b) unlike the Active Learning approach used
for pos-tagging, it does not need manual revision
of the automatically parsed samples to be used for
training; c) it was previously tested on Italian texts
with good results (Dell?Orletta et al., 2013).
4.3 Readability analysis of corpus documents
For each corpus document, we calculated several
text statistics indicative of the linguistic complex-
ity, or ?readability? of a text.
The applied measures include, (1) text length in
tokens, that is the number of tokens per text, (2)
sentences per text, that is a sentence count, and (3)
type-token ratio indicated as a percentage value.
In addition, we calculated (4) the advanced vo-
cabulary per text, that is a word count of the text
vocabulary which is not part of the the basic Ital-
ian vocabulary (?vocabolario di base?) for written
texts, as defined by De Mauro (1991)
13
, and (5)
the Gulpease Index (?Indice Gulpease?) (Lucisano
and Piemontese, 1988), which is a measure for the
readability of text that is based on frequency rela-
tions between the number of sentences, words and
letters of a text.
All values are encoded as metadata for the cor-
pus. Via the PAIS
`
A online interface, they can
be employed for filtering documents and building
subcorpora. This facility was implemented with
the principal target group of PAIS
`
A users in mind,
as the selection of language examples according
to their readability level is particularly relevant for
language learning and teaching.
4.4 Attempts at text classification for genre,
topic, and function
Lack of information about the composition of cor-
pora collected from the web using unsupervised
methods is probably one of the major limitations
of current web corpora vis-`a-vis more traditional,
carefully constructed corpora, most notably when
applications to language teaching and learning are
envisaged. This also holds true for PAIS
`
A, es-
13
The advanced vocabulary was calculated on the ba-
sis of a word list consisting of De Mauro?s ?vocabolario
fondamentale? (http://it.wikipedia.org/wiki/
Vocabolario_fondamentale) and ?vocabolario
di alto uso? (http://it.wikipedia.org/wiki/
Vocabolario_di_alto_uso), together with high
frequent function words not contained in those two lists.
39
pecially for the harvested
14
subcorpus that was
downloaded as described in section 4.1. We there-
fore carried out some experiments with the ulti-
mate aim to enrich the corpus with metadata about
text genre, topic and function, using automated
techniques.
In order to gain some insights into the com-
position of PAIS
`
A, we first conducted some man-
ual investigations. Drawing on existing literature
on web genres (e.g. (Santini, 2005; Rehm et al.,
2008; Santini et al., 2010)) and text classification
according to text function and topic (e.g. (Sharoff,
2006)), we developed a tentative three-fold taxon-
omy to be used for text classification. Following
four cycles of sample manual annotation by three
annotators, categories were adjusted in order to
better reflect the nature of PAIS
`
A?s web documents
(cf. (Sharoff, 2010) about differences between do-
mains covered in the BNC and in the web-derived
ukWaC). Details about the taxonomy are provided
in Borghetti et al. (2011). Then, we started to
cross-check whether the devised taxonomy was
indeed appropriate to describe PAIS
`
A?s composi-
tion by comparing its categories with data result-
ing from the application of unsupervised methods
for text classification.
Interesting insights have emerged so far re-
garding the topic category. Following Sharoff
(2010), we used topic modelling based on La-
tent Dirichlet Allocation for the detection of top-
ics: 20 clusters/topics were identified on the ba-
sis of keywords (the number of clusters to re-
trieve is a user-defined parameter) and projected
onto the manually defined taxonomy. This re-
vealed that most of the 20 automatically iden-
tified topics could be reasonably matched to
one of the 8 categories included in the tax-
onomy; exceptions were represented by clus-
ters characterised by proper nouns and gen-
eral language words such bambino/uomo/famiglia
(?child?/?man?/?family?) or credere/sentire/sperare
(?to believe?/?feel?/?hope?), which may in fact be
indicative of genres such as diary or personal com-
ment (e.g. personal blog). Only one of the cate-
gories originally included in the taxonomy ? natu-
ral sciences ? was not represented in the clusters,
which may indicate that there are few texts within
PAIS
`
A belonging to this domain. One of the ma-
14
In fact, even the nature of the targeted texts is not pre-
cisely defined: for instance, Wikipedia articles can actually
encompass a variety of text types such as biographies, intro-
ductions to academic theories etc. (Santini et al., 2010, p. 15)
jor advantages of topic models is that each corpus
document can be associated ? to varying degrees ?
to several topics/clusters: if encoded as metadata,
this information makes it possible not only to fil-
ter texts according to their prevailing domain, but
also to represent the heterogeneous nature of many
web documents.
5 Corpus Access and Usage
5.1 Corpus distribution
The PAIS
`
A corpus is distributed in two ways: it is
made available for download and it can be queried
via its online interface. For both cases, no restric-
tions on its usage apply other than those defined
by the Creative Commons BY-NC-SA license. For
corpus download, both the raw text version and the
annotated corpus in CoNLL format are provided.
The PAIS
`
A corpus together with all project-
related information is accessible via the project
web site at http://www.corpusitaliano.it
5.2 Corpus interface
The creation of a dedicated open online interface
for the PAIS
`
A corpus has been a declared primary
objective of the project.
The interface is aimed at providing a power-
ful, effective and easy-to-employ tool for mak-
ing full use of the resource, without having to go
through downloading, installation or registration
procedures. It is targeted at different user groups,
particularly language learners, teachers, and lin-
guists. As users of PAIS
`
A are expected to show
varying levels of proficiency in terms of language
competence, linguistic knowledge, and concern-
ing the use of online search tools, the interface
has been designed to provide four separate search
components, implementing different query modes.
Initially, the user is directed to a basic keyword
search that adopts a ?Google-style? search box.
Single search terms, as well as multi-word combi-
nations or sequences can be searched by inserting
them in a simple text box.
The second component is an advanced graph-
ical search form. It provides elaborated search
options for querying linguistic annotation layers
and allows for defining distances between search
terms as well as repetitions or optionally occurring
terms. Furthermore, the advanced search supports
regular expressions.
The third component emulates a command-line
search via the powerful CQP query language of
40
the Open Corpus Workbench (Evert and Hardie,
2011). It allows for complex search queries in
CQP syntax that rely on linguistic annotation lay-
ers as well as on metadata information.
Finally, a filter interface is presented in a fourth
component. It serves the purpose of retriev-
ing full-text corpus documents based on keyword
searches as well as text statistics (see section 4.3).
Like the CQP interface, the filter interface is also
supporting the building of temporary subcorpora
for subsequent querying.
By default, search results are displayed as
KWIC (KeyWord In Context) lines, centred
around the search expression. Each search hit can
be expanded to its full sentence view. In addition,
the originating full text document can be accessed
and its source URL is provided.
Based on an interactive visualisation for depen-
dency graphs (Culy et al., 2011) for each search
result a graphical representations of dependency
relations together with the sentence and associated
lemma and part-of-speech information can be gen-
erated (see Figure 1).
Figure 1: Dependency diagram
Targeted at novice language learners of Italian,
a filter for automatically restricting search results
to sentences of limited complexity has been in-
tegrated into each search component. When ac-
tivated, search results are automatically filtered
based on a combination of the complexity mea-
sures introduced in section 4.3.
5.3 Technical details
The PAIS
`
A online interface has been developed in
several layers: in essence, it provides a front-end
to the corpus as indexed in Open Corpus Work-
bench (Evert and Hardie, 2011). This corpus
query engine provides the fundamental search ca-
pabilities through the CQP language. Based on
the CWB/Perl API that is part of the Open Corpus
Workbench package, a web service has been de-
veloped at EURAC which exposes a large part of
the CQP language
15
through a RESTful API.
16
The four types of searches provided by the on-
line interface are developed on top of this web ser-
vice. The user queries are translated into CQP
queries and passed to the web service. In many
cases, such as the free word order queries in the
simple and advanced search forms, more than one
CQP query is necessary to produce the desired
result. Other functionalities implemented in this
layer are the management of subcorpora and the
filtering by complexity. The results returned by
the web service are then formatted and presented
to the user.
The user interface as well as the mechanisms
for translation of queries from the web forms into
CQP have been developed server-side in PHP.
The visualizations are implemented client-side in
JavaScript and jQuery, the dependency graphs
based on the xLDD framework (Culy et al., 2011).
5.4 Extraction of lexico-syntactic information
PAIS
`
A is currently used in the CombiNet project
?Word Combinations in Italian ? Theoretical and
descriptive analysis, computational models, lexi-
cographic layout and creation of a dictionary?.
17
The project goal is to study the combinatory prop-
erties of Italian words by developing advanced
computational linguistics methods for extracting
distributional information from PAIS
`
A.
In particular, CombiNet uses a pattern-based
approach to extract a wide range of multiword
expressions, such as phrasal lexemes, colloca-
tions, and usual combinations. POS n-grams
are automatically extracted from PAIS
`
A, and then
ranked according to different types of associa-
tion measures (e.g., pointwise mutual informa-
tion, log-likelihood ratios, etc.). Extending the
LexIt methodology (Lenci et al., 2012), CombiNet
also extracts distributional profiles from the parsed
layer of PAIS
`
A, including the following types of
information:
1. syntactic slots (subject, complements, modi-
15
To safeguard the system against malicious attacks, secu-
rity measures had to be taken at several of the layers, which
unfortunately also make some of the more advanced CQP fea-
tures inaccessible to the user.
16
Web services based on REST (Representational State
Transfer) principles employ standard concepts such as a URI
and standard HTTP methods to provide an interface to func-
tionalities on a remote host.
17
3-year PRIN(2010/2011)-project, coordination by Raf-
faele Simone ? University of Rome Tre
41
fiers, etc.) and subcategorization frames;
2. lexical sets filling syntactic slots (e.g. proto-
typical subjects of a target verb);
3. semantic classes describing selectional pref-
erences of syntactic slots (e.g. the direct obj.
of mangiare/?to eat? typically selects nouns
referring to food, while its subject selects an-
imate nouns); semantic roles of predicates.
The saliency and typicality of combinatory pat-
terns are weighted by means of different statisti-
cal indexes and the resulting profiles will be used
to define a distributional semantic classification of
Italian verbs, comparable to the one elaborated in
the VerbNet project (Kipper et al., 2008).
6 Evaluation
We performed post-crawl evaluations on the data.
For licensing, we analysed 200,534 pages that
were originally collected for the PAIS
`
A corpus,
and only 1,060 were identified as containing no
CC license link (99.95% with CC mark-up). Then,
from 10,000 randomly selected non-CC-licensed
Italian pages 15 were wrongly identified as CC li-
censed containing CC mark-up (0.15% error). For
language identification we checked the harvested
corpus part with the CLD2 toolkit
18
, and > 99%
of the data was identified as Italian.
The pos-tagger has been adapted to peculiari-
ties of the PAIS
`
A web texts, by manually correct-
ing sample annotation output and re-training the
tagger accordingly. Following the active learning
approach as described in section 4.2 we built a new
pos-tagger model based on 40.000 manually re-
vised tokens. With the new model, we obtained
an improvement in accuracy of 1% on a test-set
of 5000 tokens extracted from PAIS
`
A. Final tag-
ger accuracy reached 96.03%.
7 Conclusion / Future Work
In this paper we showed how a contemporary and
free language resource of Italian with linguistic
annotations can be designed, implemented and de-
veloped from the web and made available for dif-
ferent types of language users.
Future work will focus on enriching the cor-
pus with metadata by means of automatic clas-
sification techniques, so as to make a better as-
sessment of corpus composition. A multi-faceted
18
Compact Language Detection 2, http://code.
google.com/p/cld2/
approach combining linguistic features extracted
from texts (content/function words ratio, sentence
length, word frequency, etc.) and information
extracted from document URLs (e.g., tags like
?wiki?, ?blog?) might be particularly suitable for
genre and function annotation.
Metadata annotation will enable more advanced
applications of the corpus for language teaching
and learning purposes. In this respect, existing
exemplifications of the use of the PAIS
`
A inter-
face for language learning and teaching (Lyding et
al., 2013) could be followed by further pedagogi-
cal proposals as well as empowered by dedicated
teaching guidelines for the exploitation of the cor-
pus and its web interface in the class of Italian as
a second language.
In a more general perspective, we envisage
a tighter integration between acquisition of new
texts, automated text annotation and development
of lexical and language learning resources allow-
ing even non-specialised users to carve out and
develop their own language data. This ambitious
goal points in the direction of a fully-automatised
control of the entire life-cycle of open-access Ital-
ian language resources with a view to address an
increasingly wider range of potential demands.
Acknowledgements
The three years PAIS
`
A project
19
, concluded in
January 2013, received funding from the Italian
Ministry of Education, Universities and Research
(MIUR)
20
, by the FIRB program (Fondo per gli
Investimenti della Ricerca di Base)
21
.
References
G. Attardi, F. Dell?Orletta, M. Simi, and J. Turian.
2009. Accurate dependency parsing with a stacked
multilayer perceptron. In Proc. of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
M. Baroni and S. Bernardini. 2004. Bootcat: Boot-
strapping corpora and terms from the web. In Proc.
of LREC 2004, pages 1313?1316. ELDA.
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed
19
An effort of four Italian research units: University of
Bologna, CNR Pisa, University of Trento and European
Academy of Bolzano/Bozen.
20
http://www.istruzione.it/
21
http://hubmiur.pubblica.istruzione.
it/web/ricerca/firb
42
web-crawled corpora. Journal of LRE, 43(3):209?
226.
C. Borghetti, S. Castagnoli, and M. Brunello. 2011. I
testi del web: una proposta di classificazione sulla
base del corpus pais`a. In M. Cerruti, E. Corino,
and C. Onesti, editors, Formale e informale. La vari-
azione di registro nella comunicazione elettronica.,
pages 147?170. Carocci, Roma.
M. Brunello. 2009. The creation of free linguistic cor-
pora from the web. In I. Alegria, I. Leturia, and
S. Sharoff, editors, Proc. of the Fifth Web as Corpus
Workshop (WAC5), pages 9?16. Elhuyar Fundazioa.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
Tenth Conf. Comput. Nat. Lang. Learn., number
June in CoNLL-X ?06, pages 149?164. Association
for Computational Linguistics.
C. Culy, V. Lyding, and H. Dittmann. 2011. xldd:
Extended linguistic dependency diagrams. In Proc.
of the 15th International Conference on Information
Visualisation IV2011, pages 164?169, London, UK.
T. De Mauro. 1991. Guida all?uso delle parole. Edi-
tori Riuniti, Roma.
F. Dell?Orletta, G. Venturi, and S. Montemagni. 2011.
Ulisse: an unsupervised algorithm for detecting re-
liable dependency parses. In Proc. of CoNLL 2011,
Conferences on Natural Language Learning, Port-
land, Oregon.
F. Dell?Orletta, G. Venturi, and S. Montemagni. 2013.
Unsupervised linguistically-driven reliable depen-
dency parses detection and self-training for adapta-
tion to the biomedical domain. In Proc. of BioNLP
2013, Workshop on Biomedical NLP, Sofia.
F. Dell?Orletta. 2009. Ensemble system for part-of-
speech tagging. In Proceedings of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia.
S. Evert and A. Hardie. 2011. Twenty-first century
corpus workbench: Updating a query architecture
for the new millennium. In Proc. of the Corpus Lin-
guistics 2011, Birmingham, UK.
M. G?en?ereux, I. Hendrickx, and A. Mendes. 2012.
A large portuguese corpus on-line: Cleaning and
preprocessing. In PROPOR, volume 7243 of Lec-
ture Notes in Computer Science, pages 113?120.
Springer.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333?347.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer.
2008. A large-scale classification of english verbs.
Journal of LRE, 42:21?40.
A. Lenci, G. Lapesa, and G. Bonansinga. 2012. Lexit:
A computational resource on italian argument struc-
ture. In N. Calzolari, K. Choukri, T. Declerck,
M. U?gur Do?gan, B. Maegaard, J. Mariani, J. Odijk,
and S. Piperidis, editors, Proc. of LREC 2012, pages
3712?3718, Istanbul, Turkey, May. ELRA.
P. Lucisano and M. E. Piemontese. 1988. Gulpease:
una formula per la predizione della difficolt dei testi
in lingua italiana. Scuola e citt`a, 39(3):110?124.
V. Lyding, C. Borghetti, H. Dittmann, L. Nicolas, and
E. Stemle. 2013. Open corpus interface for italian
language learning. In Proc. of the ICT for Language
Learning Conference, 6th Edition, Florence, Italy.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL 2006, ACL, Sydney.
S. Petrov and R. McDonald. 2012. Overview of the
2012 shared task on parsing the web. In Proc. of
SANCL 2012, First Workshop on Syntactic Analysis
of Non-Canonical Language, Montreal.
G. Rehm, M. Santini, A. Mehler, P. Braslavski,
R. Gleim, A. Stubbe, S. Symonenko, M. Tavosanis,
and V. Vidulin. 2008. Towards a reference corpus of
web genres for the evaluation of genre identification
systems. In Proc. of LREC 2008, pages 351?358,
Marrakech, Morocco.
M. Santini, A. Mehler, and S. Sharoff. 2010. Riding
the Rough Waves of Genre on the Web. Concepts
and Research Questions. In A. Mehler, S. Sharoff,
and M. Santini, editors, Genres on the Web: Compu-
tational Models and Empirical Studies., pages 3?33.
Springer, Dordrecht.
M. Santini. 2005. Genres in formation? an ex-
ploratory study of web pages using cluster analysis.
In Proc. of the 8th Annual Colloquium for the UK
Special Interest Group for Computational Linguis-
tics (CLUK05), Manchester, UK.
S. Sharoff. 2006. Creating General-Purpose Corpora
Using Automated Search Engine Queries. In M. Ba-
roni and S. Bernardini, editors, Wacky! Working
Papers on the Web as Corpus, pages 63?98. Gedit,
Bologna.
S. Sharoff. 2010. Analysing similarities and differ-
ences between corpora. In 7th Language Technolo-
gies Conference, Ljubljana.
J. M. Steger and E. W. Stemle. 2009. KrdWrd ? The
Architecture for Unified Processing of Web Content.
In Proc. Fifth Web as Corpus Work., Donostia-San
Sebastian, Basque Country.
C. A. Thompson, M. E. Califf, and R. J. Mooney. 1999.
Active learning for natural language parsing and in-
formation extraction. In Proc. of ICML99, the Six-
teenth International Conference on Machine Learn-
ing, San Francisco, CA.
43
