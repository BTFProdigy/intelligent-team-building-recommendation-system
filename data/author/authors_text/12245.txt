Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 459?467,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Employing Topic Models for Pattern-based Semantic Class Discovery 
 
 
Huibin Zhang1*     Mingjie Zhu2*     Shuming Shi3     Ji-Rong Wen3 
1Nankai University 
2University of Science and Technology of China 
3Microsoft Research Asia 
{v-huibzh, v-mingjz, shumings, jrwen}@microsoft.com 
 
  
 
Abstract? 
 
A semantic class is a collection of items 
(words or phrases) which have semantically 
peer or sibling relationship. This paper studies 
the employment of topic models to automati-
cally construct semantic classes, taking as the 
source data a collection of raw semantic 
classes (RASCs), which were extracted by ap-
plying predefined patterns to web pages. The 
primary requirement (and challenge) here is 
dealing with multi-membership: An item may 
belong to multiple semantic classes; and we 
need to discover as many as possible the dif-
ferent semantic classes the item belongs to. To 
adopt topic models, we treat RASCs as ?doc-
uments?, items as ?words?, and the final se-
mantic classes as ?topics?. Appropriate 
preprocessing and postprocessing are per-
formed to improve results quality, to reduce 
computation cost, and to tackle the fixed-k 
constraint of a typical topic model. Experi-
ments conducted on 40 million web pages 
show that our approach could yield better re-
sults than alternative approaches. 
1 Introduction 
Semantic class construction (Lin and Pantel, 
2001; Pantel and Lin, 2002; Pasca, 2004; Shinza-
to and Torisawa, 2005; Ohshima et al, 2006) 
tries to discover the peer or sibling relationship 
among terms or phrases by organizing them into 
semantic classes. For example, {red, white, 
black?} is a semantic class consisting of color 
instances. A popular way for semantic class dis-
covery is pattern-based approach, where prede-
fined patterns (Table 1) are applied to a 
                                                   
? This work was performed when the authors were interns at 
Microsoft Research Asia 
collection of web pages or an online web search 
engine to produce some raw semantic classes 
(abbreviated as RASCs, Table 2). RASCs cannot 
be treated as the ultimate semantic classes, be-
cause they are typically noisy and incomplete, as 
shown in Table 2. In addition, the information of 
one real semantic class may be distributed in lots 
of RASCs (R2 and R3 in Table 2). 
 
Type Pattern 
SENT NP {, NP}*{,} (and|or) {other} NP 
TAG <UL>  <LI>item</LI>  ?  <LI>item</LI>  </UL> 
TAG <SELECT> <OPTION>item?<OPTION>item </SELECT> 
* SENT: Sentence structure patterns; TAG: HTML Tag patterns 
Table 1. Sample patterns 
 
R1: {gold, silver, copper, coal, iron, uranium} 
R2: {red, yellow, color, gold, silver, copper} 
R3: {red, green, blue, yellow} 
R4: {HTML, Text, PDF, MS Word, Any file type} 
R5: {Today, Tomorrow, Wednesday, Thursday, Friday, 
Saturday, Sunday} 
R6: {Bush, Iraq, Photos, USA, War} 
Table 2. Sample raw semantic classes (RASCs) 
 
This paper aims to discover high-quality se-
mantic classes from a large collection of noisy 
RASCs. The primary requirement (and chal-
lenge) here is to deal with multi-membership, i.e., 
one item may belong to multiple different seman-
tic classes. For example, the term ?Lincoln? can 
simultaneously represent a person, a place, or a 
car brand name. Multi-membership is more pop-
ular than at a first glance, because quite a lot of 
English common words have also been borrowed 
as company names, places, or product names. 
For a given item (as a query) which belongs to 
multiple semantic classes, we intend to return the 
semantic classes separately, rather than mixing 
all their items together. 
Existing pattern-based approaches only pro-
vide very limited support to multi-membership. 
For example, RASCs with the same labels (or 
hypernyms) are merged in (Pasca, 2004) to gen-
459
erate the ultimate semantic classes. This is prob-
lematic, because RASCs may not have (accurate) 
hypernyms with them. 
In this paper, we propose to use topic models 
to address the problem. In some topic models, a 
document is modeled as a mixture of hidden top-
ics. The words of a document are generated ac-
cording to the word distribution over the topics 
corresponding to the document (see Section 2 for 
details). Given a corpus, the latent topics can be 
obtained by a parameter estimation procedure. 
Topic modeling provides a formal and conve-
nient way of dealing with multi-membership, 
which is our primary motivation of adopting top-
ic models here. To employ topic models, we treat 
RASCs as ?documents?, items as ?words?, and 
the final semantic classes as ?topics?. 
There are, however, several challenges in ap-
plying topic models to our problem. To begin 
with, the computation is intractable for 
processing a large collection of RASCs (our da-
taset for experiments contains 2.7 million unique 
RASCs extracted from 40 million web pages). 
Second, typical topic models require the number 
of topics (k) to be given. But it lacks an easy way 
of acquiring the ideal number of semantic classes 
from the source RASC collection. For the first 
challenge, we choose to apply topic models to 
the RASCs containing an item q, rather than the 
whole RASC collection. In addition, we also per-
form some preprocessing operations in which 
some items are discarded to further improve effi-
ciency. For the second challenge, considering 
that most items only belong to a small number of 
semantic classes, we fix (for all items q) a topic 
number which is slightly larger than the number 
of classes an item could belong to. And then a 
postprocessing operation is performed to merge 
the results of topic models to generate the ulti-
mate semantic classes. 
Experimental results show that, our topic 
model approach is able to generate higher-quality 
semantic classes than popular clustering algo-
rithms (e.g., K-Medoids and DBSCAN). 
We make two contributions in the paper: On 
one hand, we find an effective way of construct-
ing high-quality semantic classes in the pattern-
based category which deals with multi-
membership. On the other hand, we demonstrate, 
for the first time, that topic modeling can be uti-
lized to help mining the peer relationship among 
words. In contrast, the general related relation-
ship between words is extracted in existing topic 
modeling applications. Thus we expand the ap-
plication scope of topic modeling. 
2 Topic Models 
In this section we briefly introduce the two wide-
ly used topic models which are adopted in our 
paper. Both of them model a document as a mix-
ture of hidden topics. The words of every docu-
ment are assumed to be generated via a 
generative probability process. The parameters of 
the model are estimated from a training process 
over a given corpus, by maximizing the likelih-
ood of generating the corpus. Then the model can 
be utilized to inference a new document. 
pLSI: The probabilistic Latent Semantic In-
dexing Model (pLSI) was introduced in Hof-
mann (1999), arose from Latent Semantic 
Indexing (Deerwester et al, 1990). The follow-
ing process illustrates how to generate a docu-
ment d in pLSI: 
1. Pick a topic mixture distribution ?(? |?). 
2. For each word wi in d 
a. Pick a latent topic z with the probabil-
ity ?(?|?) for wi 
b. Generate wi with probability ?(?? |?) 
So with k latent topics, the likelihood of gene-
rating a document d is 
 ?(?) =  ? ?? ? ?(?|?)
??
 (2.1) 
LDA (Blei et al, 2003): In LDA, the topic 
mixture is drawn from a conjugate Dirichlet prior 
that remains the same for all documents (Figure 
1). The generative process for each document in 
the corpus is, 
1. Choose document length N from a Pois-
son distribution Poisson(?). 
2. Choose ?  from a Dirichlet distribution 
with parameter ?. 
3. For each of the N words wi. 
a. Choose a topic z from a Multinomial 
distribution with parameter ?. 
b. Pick a word wi from ? ??  ?,? . 
So the likelihood of generating a document is 
 ?(?) =  ?(?|?)
?
  ?(?|?)? ?? ?,? ??
??
 (2.2) 
 
 
Figure 1. Graphical model representation of LDA, 
from Blei et al (2003) 
 
w? z?
?
N
M
460
3 Our Approach 
The source data of our approach is a collection 
(denoted as CR) of RASCs extracted via applying 
patterns to a large collection of web pages. Given 
an item as an input query, the output of our ap-
proach is one or multiple semantic classes for the 
item. To be applicable in real-world dataset, our 
approach needs to be able to process at least mil-
lions of RASCs. 
3.1 Main Idea 
As reviewed in Section 2, topic modeling pro-
vides a formal and convenient way of grouping 
documents and words to topics. In order to apply 
topic models to our problem, we map RASCs to 
documents, items to words, and treat the output 
topics yielded from topic modeling as our seman-
tic classes (Table 3). The motivation of utilizing 
topic modeling to solve our problem and building 
the above mapping comes from the following 
observations. 
1) In our problem, one item may belong to 
multiple semantic classes; similarly in topic 
modeling, a word can appear in multiple top-
ics. 
2) We observe from our source data that 
some RASCs are comprised of items in mul-
tiple semantic classes. And at the same time, 
one document could be related to multiple 
topics in some topic models (e.g., pLSI and 
LDA). 
 
Topic modeling Semantic class construction 
word item (word or phrase) 
document RASC 
topic semantic class 
Table 3. The mapping from the concepts in topic 
modeling to those in semantic class construction 
 
Due to the above observations, we hope topic 
modeling can be employed to construct semantic 
classes from RASCs, just as it has been used in 
assigning documents and words to topics. 
There are some critical challenges and issues 
which should be properly addressed when topic 
models are adopted here. 
Efficiency: Our RASC collection CR contains 
about 2.7 million unique RASCs and 26 million 
(1 million unique) items. Building topic models 
directly for such a large dataset may be computa-
tionally intractable. To overcome this challenge, 
we choose to apply topic models to the RASCs 
containing a specific item rather than the whole 
RASC collection. Please keep in mind that our 
goal in this paper is to construct the semantic 
classes for an item when the item is given as a 
query. For one item q, we denote CR(q) to be all 
the RASCs in CR containing the item. We believe 
building a topic model over CR(q) is much more 
effective because it contains significantly fewer 
?documents?, ?words?, and ?topics?. To further 
improve efficiency, we also perform preprocess-
ing (refer to Section 3.4 for details) before build-
ing topic models for CR(q), where some low-
frequency items are removed. 
Determine the number of topics: Most topic 
models require the number of topics to be known 
beforehand1. However, it is not an easy task to 
automatically determine the exact number of se-
mantic classes an item q should belong to. Ac-
tually the number may vary for different q. Our 
solution is to set (for all items q) the topic num-
ber to be a fixed value (k=5 in our experiments) 
which is slightly larger than the number of se-
mantic classes most items could belong to. Then 
we perform postprocessing for the k topics to 
produce the final properly semantic classes. 
In summary, our approach contains three 
phases (Figure 2). We build topic models for 
every CR(q), rather than the whole collection CR. 
A preprocessing phase and a postprocessing 
phase are added before and after the topic model-
ing phase to improve efficiency and to overcome 
the fixed-k problem. The details of each phase 
are presented in the following subsections. 
 
 
Figure 2. Main phases of our approach 
 
3.2 Adopting Topic Models 
For an item q, topic modeling is adopted to 
process the RASCs in CR(q) to generate k seman-
tic classes. Here we use LDA as an example to 
                                                   
1 Although there is study of non-parametric Bayesian mod-
els (Li et al, 2007) which need no prior knowledge of topic 
number, the computational complexity seems to exceed our 
efficiency requirement and we shall leave this to future 
work. 
R580 
R1 
R2 
CR 
Item q 
Preprocessing 
?400
?  
?1
? 
?2
? 
T5 
T1 
T2 
C3 
C1 
C2 
Topic  
modeling 
Postprocessing 
T3 
T4 
CR(q) 
461
illustrate the process. The case of other genera-
tive topic models (e.g., pLSI) is very similar. 
According to the assumption of LDA and our 
concept mapping in Table 3, a RASC (?docu-
ment?) is viewed as a mixture of hidden semantic 
classes (?topics?). The generative process for a 
RASC R in the ?corpus? CR(q) is as follows, 
1) Choose a RASC size (i.e., the number of 
items in R): NR ~ Poisson(?). 
2) Choose a k-dimensional vector ??  from a 
Dirichlet distribution with parameter ?. 
3) For each of the NR items an: 
a) Pick a semantic class ??  from a mul-
tinomial distribution with parameter 
?? . 
b) Pick an item an from ?(?? |?? ,?) , 
where the item probabilities are pa-
rameterized by the matrix ?. 
There are three parameters in the model: ? (a 
scalar), ?  (a k-dimensional vector), and ?  (a 
? ? ? matrix where V is the number of distinct 
items in CR(q)). The parameter values can be ob-
tained from a training (or called parameter esti-
mation) process over CR(q), by maximizing the 
likelihood of generating the corpus. Once ?  is 
determined, we are able to compute ?(?|?,?), 
the probability of item a belonging to semantic 
class z. Therefore we can determine the members 
of a semantic class z by selecting those items 
with high ? ? ?,?  values. 
The number of topics k is assumed known and 
fixed in LDA. As has been discussed in Section 
3.1, we set a constant k value for all different 
CR(q). And we rely on the postprocessing phase 
to merge the semantic classes produced by the 
topic model to generate the ultimate semantic 
classes. 
When topic modeling is used in document 
classification, an inference procedure is required 
to determine the topics for a new document. 
Please note that inference is not needed in our 
problem. 
One natural question here is: Considering that 
in most topic modeling applications, the words 
within a resultant topic are typically semantically 
related but may not be in peer relationship, then 
what is the intuition that the resultant topics here 
are semantic classes rather than lists of generally 
related words? The magic lies in the ?docu-
ments? we used in employing topic models. 
Words co-occurred in real documents tend to be 
semantically related; while items co-occurred in 
RASCs tend to be peers. Experimental results 
show that most items in the same output seman-
tic class have peer relationship. 
It might be noteworthy to mention the exchan-
geability or ?bag-of-words? assumption in most 
topic models. Although the order of words in a 
document may be important, standard topic mod-
els neglect the order for simplicity and other rea-
sons2. The order of items in a RASC is clearly 
much weaker than the order of words in an ordi-
nary document. In some sense, topic models are 
more suitable to be used here than in processing 
an ordinary document corpus. 
3.3 Preprocessing and Postprocessing 
Preprocessing is applied to CR(q) before we build 
topic models for it. In this phase, we discard 
from all RASCs the items with frequency (i.e., 
the number of RASCs containing the item) less 
than a threshold h. A RASC itself is discarded 
from CR(q) if it contains less than two items after 
the item-removal operations. We choose to re-
move low-frequency items, because we found 
that low-frequency items are seldom important 
members of any semantic class for q. So the goal 
is to reduce the topic model training time (by 
reducing the training data) without sacrificing 
results quality too much. In the experiments sec-
tion, we compare the approaches with and with-
out preprocessing in terms of results quality and 
efficiency. Interestingly, experimental results 
show that, for some small threshold values, the 
results quality becomes higher after preprocess-
ing is performed. We will give more discussions 
in Section 4. 
In the postprocessing phase, the output seman-
tic classes (?topics?) of topic modeling are 
merged to generate the ultimate semantic classes. 
As indicated in Sections 3.1 and 3.2, we fix the 
number of topics (k=5) for different corpus CR(q) 
in employing topic models. For most items q, 
this is a larger value than the real number of se-
mantic classes the item belongs to. As a result, 
one real semantic class may be divided into mul-
tiple topics. Therefore one core operation in this 
phase is to merge those topics into one semantic 
class. In addition, the items in each semantic 
class need to be properly ordered. Thus main 
operations include, 
1) Merge semantic classes 
2) Sort the items in each semantic class 
Now we illustrate how to perform the opera-
tions. 
Merge semantic classes: The merge process 
is performed by repeatedly calculating the simi-
                                                   
2 There are topic model extensions considering word order 
in documents, such as Griffiths et al (2005). 
462
larity between two semantic classes and merging 
the two ones with the highest similarity until the 
similarity is under a threshold. One simple and 
straightforward similarity measure is the Jaccard 
coefficient, 
 ??? ?1 ,?2 =
 ?1 ? ?2 
 ?1 ? ?2 
 (3.1) 
where ?1 ? ?2  and ?1 ? ?2  are respectively the 
intersection and union of semantic classes C1 and 
C2. This formula might be over-simple, because 
the similarity between two different items is not 
exploited. So we propose the following measure, 
 ??? ?1 ,?2 =
  ???(?, ?)???2???1
 ?1 ?  ?2 
 (3.2) 
where |C| is the number of items in semantic 
class C, and sim(a,b) is the similarity between 
items a and b, which will be discussed shortly. In 
Section 4, we compare the performance of the 
above two formulas by experiments. 
Sort items: We assign an importance score to 
every item in a semantic class and sort them ac-
cording to the importance scores. Intuitively, an 
item should get a high rank if the average simi-
larity between the item and the other items in the 
semantic class is high, and if it has high similari-
ty to the query item q. Thus we calculate the im-
portance of item a in a semantic class C as 
follows, 
 ? ?|? = ? ?sim(a,C)+(1-?) ?sim(a,q) (3.3) 
where ? is a parameter in [0,1], sim(a,q) is the 
similarity between a and the query item q, and 
sim(a,C) is the similarity between a and C, calcu-
lated as, 
 ??? ?,? =
 ???(?, ?)???
 ? 
 (3.4) 
Item similarity calculation: Formulas 3.2, 
3.3, and 3.4 rely on the calculation of the similar-
ity between two items. 
One simple way of estimating item similarity 
is to count the number of RASCs containing both 
of them. We extend such an idea by distinguish-
ing the reliability of different patterns and pu-
nishing term similarity contributions from the 
same site. The resultant similarity formula is, 
 ???(?,?) = log(1 + ?(?(?? ,? ))
??
?=1
)
?
?=1
 (3.5) 
where Ci,j is a RASC containing both a and b, 
P(Ci,j) is the pattern via which the RASC is ex-
tracted, and w(P) is the weight of pattern P. As-
sume all these RASCs belong to m sites with Ci,j 
extracted from a page in site i, and ki being the 
number of RASCs corresponding to site i. To 
determine the weight of every type of pattern, we 
randomly selected 50 RASCs for each pattern 
and labeled their quality. The weight of each 
kind of pattern is then determined by the average 
quality of all labeled RASCs corresponding to it. 
The efficiency of postprocessing is not a prob-
lem, because the time cost of postprocessing is 
much less than that of the topic modeling phase. 
3.4 Discussion 
3.4.1 Efficiency of processing popular items 
Our approach receives a query item q from users 
and returns the semantic classes containing the 
query. The maximal query processing time 
should not be larger than several seconds, be-
cause users would not like to wait more time. 
Although the average query processing time of 
our approach is much shorter than 1 second (see 
Table 4 in Section 4), it takes several minutes to 
process a popular item such as ?Washington?, 
because it is contained in a lot of RASCs. In or-
der to reduce the maximal online processing 
time, our solution is offline processing popular 
items and storing the resultant semantic classes 
on disk. The time cost of offline processing is 
feasible, because we spent about 15 hours on a 4-
core machine to complete the offline processing 
for all the items in our RASC collection. 
3.4.2 Alternative approaches 
One may be able to easily think of other ap-
proaches to address our problem. Here we dis-
cuss some alternative approaches which are 
treated as our baseline in experiments. 
RASC clustering: Given a query item q, run a 
clustering algorithm over CR(q) and merge all 
RASCs in the same cluster as one semantic class. 
Formula 3.1 or 3.2 can be used to compute the 
similarity between RASCs in performing cluster-
ing. We try two clustering algorithms in experi-
ments: K-Medoids and DBSCAN. Please note k-
means cannot be utilized here because coordi-
nates are not available for RASCs. One draw-
back of RASC clustering is that it cannot deal 
with the case of one RASC containing the items 
from multiple semantic classes. 
Item clustering: By Formula 3.5, we are able 
to construct an item graph GI to record the 
neighbors (in terms of similarity) of each item. 
Given a query item q, we first retrieve its neigh-
bors from GI, and then run a clustering algorithm 
over the neighbors. As in the case of RASC clus-
tering, we try two clustering algorithms in expe-
riments: K-Medoids and DBSCAN. The primary 
disadvantage of item clustering is that it cannot 
assign an item (except for the query item q) to 
463
multiple semantic classes. As a result, when we 
input ?gold? as the query, the item ?silver? can 
only be assigned to one semantic class, although 
the term can simultaneously represents a color 
and a chemical element. 
4 Experiments 
4.1 Experimental Setup 
Datasets: By using the Open Directory Project 
(ODP3) URLs as seeds, we crawled about 40 mil-
lion English web pages in a breadth-first way. 
RASCs are extracted via applying a list of sen-
tence structure patterns and HTML tag patterns 
(see Table 1 for some examples). Our RASC col-
lection CR contains about 2.7 million unique 
RASCs and 1 million distinct items. 
Query set and labeling: We have volunteers 
to try Google Sets4, record their queries being 
used, and select overall 55 queries to form our 
query set. For each query, the results of all ap-
proaches are mixed together and labeled by fol-
lowing two steps. In the first step, the standard 
(or ideal) semantic classes (SSCs) for the query 
are manually determined. For example, the ideal 
semantic classes for item ?Georgia? may include 
Countries, and U.S. states. In the second step, 
each item is assigned a label of ?Good?, ?Fair?, 
or ?Bad? with respect to each SSC. For example, 
?silver? is labeled ?Good? with respect to ?col-
ors? and ?chemical elements?. We adopt metric 
MnDCG (Section 4.2) as our evaluation metric. 
Approaches for comparison: We compare 
our approach with the alternative approaches dis-
cussed in Section 3.4.2. 
LDA: Our approach with LDA as the topic 
model. The implementation of LDA is based 
on Blei?s code of variational EM for LDA5. 
pLSI: Our approach with pLSI as the topic 
model. The implementation of pLSI is based 
on Schein, et al (2002). 
KMedoids-RASC: The RASC clustering ap-
proach illustrated in Section 3.4.2, with the 
K-Medoids clustering algorithm utilized. 
DBSCAN-RASC: The RASC clustering ap-
proach with DBSCAN utilized. 
KMedoids-Item: The item clustering ap-
proach with the K-Medoids utilized. 
DBSCAN-Item: The item clustering ap-
proach with the DBSCAN clustering algo-
rithm utilized. 
                                                   
3 http://www.dmoz.org 
4 http://labs.google.com/sets 
5 http://www.cs.princeton.edu/~blei/lda-c/ 
K-Medoids clustering needs to predefine the 
cluster number k. We fix the k value for all dif-
ferent query item q, as has been done for the top-
ic model approach. For fair comparison, the same 
postprocessing is made for all the approaches. 
And the same preprocessing is made for all the 
approaches except for the item clustering ones 
(to which the preprocessing is not applicable). 
4.2 Evaluation Methodology 
Each produced semantic class is an ordered list 
of items. A couple of metrics in the information 
retrieval (IR) community like Precision@10, 
MAP (mean average precision), and nDCG 
(normalized discounted cumulative gain) are 
available for evaluating a single ranked list of 
items per query (Croft et al, 2009). Among the 
metrics, nDCG (Jarvelin and Kekalainen, 2000) 
can handle our three-level judgments (?Good?, 
?Fair?, and ?Bad?, refer to Section 4.1), 
 ????@? =
 ? ? /log(? + 1)??=1
 ?? ? /log(? + 1)??=1
 (4.1) 
where G(i) is the gain value assigned to the i?th 
item, and G*(i) is the gain value assigned to the 
i?th item of an ideal (or perfect) ranking list. 
Here we extend the IR metrics to the evalua-
tion of multiple ordered lists per query. We use 
nDCG as the basic metric and extend it to 
MnDCG. 
Assume labelers have determined m SSCs 
(SSC1~SSCm, refer to Section 4.1) for query q 
and the weight (or importance) of SSCi is wi. As-
sume n semantic classes are generated by an ap-
proach and n1 of them have corresponding SSCs 
(i.e., no appropriate SSC can be found for the 
remaining n-n1 semantic classes). We define the 
MnDCG score of an approach (with respect to 
query q) as, 
 ????? ? =
?1
?
?
 ?? ? ?????(SSC?)
?
i=1
 ??
m
i=1
 (4.2) 
where 
 ????? ???? =  
0                                         ?? ?? = 0
1
??
max
? ?[1, ??]
(???? ?? ,?  )  ?? ?? ? 0
  (4.3) 
In the above formula, nDCG(Gi,j) is the nDCG 
score of semantic class Gi,j; and ki denotes the 
number of semantic classes assigned to SSCi. For 
a list of queries, the MnDCG score of an algo-
rithm is the average of all scores for the queries. 
The metric is designed to properly deal with 
the following cases, 
464
i). One semantic class is wrongly split into 
multiple ones: Punished by dividing ??  in 
Formula 4.3; 
ii). A semantic class is too noisy to be as-
signed to any SSC: Processed by the 
?n1/n? in Formula 4.2; 
iii). Fewer semantic classes (than the number 
of SSCs) are produced: Punished in For-
mula 4.3 by assigning a zero value. 
iv). Wrongly merge multiple semantic 
classes into one: The nDCG score of the 
merged one will be small because it is 
computed with respect to only one single 
SSC. 
The gain values of nDCG for the three relev-
ance levels (?Bad?, ?Fair?, and ?Good?) are re-
spectively -1, 1, and 2 in experiments. 
4.3 Experimental  Results 
4.3.1 Overall performance comparison 
Figure 3 shows the performance comparison be-
tween the approaches listed in Section 4.1, using 
metrics MnDCG@n (n=1?10). Postprocessing 
is performed for all the approaches, where For-
mula 3.2 is adopted to compute the similarity 
between semantic classes. The results show that 
that the topic modeling approaches produce 
higher-quality semantic classes than the other 
approaches. It indicates that the topic mixture 
assumption of topic modeling can handle the 
multi-membership problem very well here. 
Among the alternative approaches, RASC clus-
tering behaves better than item clustering. The 
reason might be that an item cannot belong to 
multiple clusters in the two item clustering ap-
proaches, while RASC clustering allows this. For 
the RASC clustering approaches, although one 
item has the chance to belong to different seman-
tic classes, one RASC can only belong to one 
semantic class. 
 
 
Figure 3. Quality comparison (MnDCG@n) among 
approaches (frequency threshold h = 4 in preprocess-
ing; k = 5 in topic models) 
4.3.2 Preprocessing experiments 
Table 4 shows the average query processing time 
and results quality of the LDA approach, by va-
rying frequency threshold h. Similar results are 
observed for the pLSI approach. In the table, h=1 
means no preprocessing is performed. The aver-
age query processing time is calculated over all 
items in our dataset. As the threshold h increases, 
the processing time decreases as expected, be-
cause the input of topic modeling gets smaller. 
The second column lists the results quality 
(measured by MnDCG@10). Interestingly, we 
get the best results quality when h=4 (i.e., the 
items with frequency less than 4 are discarded). 
The reason may be that most low-frequency 
items are noisy ones. As a result, preprocessing 
can improve both results quality and processing 
efficiency; and h=4 seems a good choice in pre-
processing for our dataset. 
 
h 
Avg. Query Proc. 
Time (seconds) 
Quality 
(MnDCG@10) 
1 0.414 0.281 
2 0.375 0.294 
3 0.320 0.322 
4 0.268 0.331 
5 0.232 0.328 
6 0.210 0.315 
7 0.197 0.315 
8 0.184 0.313 
9 0.173 0.288 
Table 4. Time complexity and quality comparison 
among LDA approaches of different thresholds 
 
4.3.3 Postprocessing experiments 
 
Figure 4. Results quality comparison among topic 
modeling approaches with and without postprocessing 
(metric: MnDCG@10) 
 
The effect of postprocessing is shown in Figure 
4. In the figure, NP means no postprocessing is 
performed. Sim1 and Sim2 respectively mean 
Formula 3.1 and Formula 3.2 are used in post-
processing as the similarity measure between 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 2 3 4 5 6 7 8 9 10
pLSI LDA KMedoids-RASC
DBSCAN-RASC KMedoids-Item DBSCAN-Item
n
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
LDA pLSI
NP
Sim1 
Sim2
465
semantic classes. The same preprocessing (h=4) 
is performed in generating the data. It can be 
seen that postprocessing improves results quality. 
Sim2 achieves more performance improvement 
than Sim1, which demonstrates the effectiveness 
of the similarity measure in Formula 3.2. 
4.3.4 Sample results 
Table 5 shows the semantic classes generated by 
our LDA approach for some sample queries in 
which the bad classes or bad members are hig-
hlighted (to save space, 10 items are listed here, 
and the query itself is omitted in the resultant 
semantic classes).  
 
Query Semantic Classes 
apple 
C1: ibm, microsoft, sony, dell, toshiba,  sam-
sung, panasonic, canon, nec, sharp ? 
C2: peach, strawberry, cherry, orange, bana-
na, lemon, pineapple, raspberry, pear, grape 
? 
gold 
C1: silver, copper, platinum, zinc, lead, iron, 
nickel, tin, aluminum, manganese ? 
C2: silver, red, black, white, blue, purple, 
orange, pink, brown, navy ? 
C3: silver, platinum, earrings, diamonds, 
rings, bracelets, necklaces, pendants, jewelry, 
watches ? 
C4: silver, home, money, business, metal, 
furniture, shoes, gypsum, hematite, fluorite 
?  
lincoln 
C1: ford, mazda, toyota, dodge, nissan, hon-
da, bmw, chrysler, mitsubishi, audi ? 
C2: bristol, manchester, birmingham, leeds, 
london, cardiff, nottingham, newcastle, shef-
field, southampton ? 
C3: jefferson, jackson, washington, madison, 
franklin, sacramento, new york city, monroe, 
Louisville, marion ? 
computer 
science 
C1: chemistry, mathematics, physics, biolo-
gy, psychology, education, history, music, 
business, economics ? 
Table 5. Semantic classes generated by our approach 
for some sample queries (topic model = LDA) 
 
5 Related Work 
Several categories of work are related to ours. 
The first category is about set expansion (i.e., 
retrieving one semantic class given one term or a 
couple of terms). Syntactic context information is 
used (Hindle, 1990; Ruge, 1992; Lin, 1998) to 
compute term similarities, based on which simi-
lar words to a particular word can directly be 
returned. Google sets is an online service which, 
given one to five items, predicts other items in 
the set. Ghahramani and Heller (2005) introduce 
a Bayesian Sets algorithm for set expansion. Set 
expansion is performed by feeding queries to 
web search engines in Wang and Cohen (2007) 
and Kozareva (2008). All of the above work only 
yields one semantic class for a given query. 
Second, there are pattern-based approaches in the 
literature which only do limited integration of 
RASCs (Shinzato and Torisawa, 2004; Shinzato 
and Torisawa, 2005; Pasca, 2004), as discussed 
in the introduction section. In Shi et al (2008), 
an ad-hoc approach was proposed to discover the 
multiple semantic classes for one item. The third 
category is distributional similarity approaches 
which provide multi-membership support (Har-
ris, 1985; Lin  and Pantel, 2001; Pantel and Lin, 
2002). Among them, the CBC algorithm (Pantel 
and Lin, 2002) addresses the multi-membership 
problem. But it relies on term vectors and centro-
ids which are not available in pattern-based ap-
proaches. It is therefore not clear whether it can 
be borrowed to deal with multi-membership here. 
Among the various applications of topic 
modeling, maybe the efforts of using topic model 
for Word Sense Disambiguation (WSD) are most 
relevant to our work. In Cai et al(2007), LDA is 
utilized to capture the global context information 
as the topic features for better performing the 
WSD task. In Boyd-Graber et al (2007), Latent 
Dirichlet with WordNet (LDAWN) is developed 
for simultaneously disambiguating a corpus and 
learning the domains in which to consider each 
word. They do not generate semantic classes. 
6 Conclusions 
We presented an approach that employs topic 
modeling for semantic class construction. Given 
an item q, we first retrieve all RASCs containing 
the item to form a collection CR(q). Then we per-
form some preprocessing to CR(q) and build a 
topic model for it. Finally, the output semantic 
classes of topic modeling are post-processed to 
generate the final semantic classes. For the CR(q) 
which contains a lot of RASCs, we perform of-
fline processing according to the above process 
and store the results on disk, in order to reduce 
the online query processing time. 
We also proposed an evaluation methodology 
for measuring the quality of semantic classes. 
We show by experiments that our topic modeling 
approach outperforms the item clustering and 
RASC clustering approaches. 
 
Acknowledgments 
We wish to acknowledge help from Xiaokang 
Liu for mining RASCs from web pages, Chan-
gliang Wang and Zhongkai Fu for data process.  
  
466
References  
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent dirichlet alocation. J. Mach. Learn. 
Res., 3:993?1022. 
Bruce Croft, Donald Metzler, and Trevor Strohman. 
2009. Search Engines: Information Retrieval in 
Practice. Addison Wesley.  
Jordan Boyd-Graber, David Blei, and Xiaojin 
Zhu.2007. A topic model for word sense disambig-
uation. In Proceedings EMNLP-CoNLL 2007, pag-
es 1024?1033, Prague, Czech Republic, June. 
Association for Computational Linguistics. 
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. 
NUS-ML: Improving word sense disambiguation 
using topic features. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, volume 
4. 
Scott Deerwester, Susan T. Dumais, GeorgeW. Fur-
nas, Thomas K. Landauer, and Richard Harshman. 
1990. Indexing by latent semantic analysis. Journal 
of the American Society for Information Science, 
41:391?407. 
Zoubin Ghahramani and Katherine A. Heller. 2005. 
Bayesian Sets. In Advances in Neural Information 
Processing Systems (NIPS05). 
Thomas L. Griffiths, Mark Steyvers, David M. 
Blei,and Joshua B. Tenenbaum. 2005. Integrating 
topics and syntax. In Advances in Neural Informa-
tion Processing Systems 17, pages 537?544. MIT 
Press 
Zellig Harris. Distributional Structure. The Philoso-
phy of Linguistics. New York: Oxford University 
Press. 1985. 
Donald Hindle. 1990. Noun Classification from Pre-
dicate-Argument Structures. In Proceedings of 
ACL90, pages 268?275.  
Thomas Hofmann. 1999. Probabilistic latent semantic 
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR99, pages 50?57, New York, 
NY, USA. ACM. 
Kalervo Jarvelin, and Jaana Kekalainen. 2000. IR 
Evaluation Methods for Retrieving Highly Rele-
vant Documents. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on 
Research and Development in Information Retriev-
al (SIGIR2000). 
Zornitsa Kozareva, Ellen Riloff and Eduard Hovy. 
2008. Semantic Class Learning from the Web with 
Hyponym Pattern Linkage Graphs, In Proceedings 
of ACL-08. 
Wei Li, David M. Blei, and Andrew McCallum. Non-
parametric Bayes Pachinko Allocation. In Proceed-
ings of Conference on Uncertainty in Artificial In-
telligence (UAI), 2007. 
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of COLING-
ACL98, pages 768-774. 
Dekang Lin and Patrick Pantel. 2001. Induction of 
Semantic Classes from Natural Language Text. In 
Proceedings of SIGKDD01, pages 317-322.  
Hiroaki Ohshima, Satoshi Oyama, and Katsumi Tana-
ka. 2006. Searching coordinate terms with their 
context from the web. In WISE06, pages 40?47. 
Patrick Pantel and Dekang Lin. 2002. Discovering 
Word Senses from Text. In Proceedings of 
SIGKDD02.  
Marius Pasca. 2004. Acquisition of Categorized 
Named Entities for Web Search. In Proc. of 2004 
CIKM.  
Gerda Ruge. 1992. Experiments on Linguistically-
Based Term Associations. In Information 
Processing & Management, 28(3), pages 317-32. 
Andrew I. Schein,  Alexandrin Popescul,  Lyle H. 
Ungar and David M. Pennock. 2002. Methods and 
metrics for cold-start recommendations. In Pro-
ceedings of SIGIR02, pages  253-260. 
Shuming Shi, Xiaokang Liu and Ji-Rong Wen. 2008. 
Pattern-based Semantic Class Discovery with Mul-
ti-Membership Support. In CIKM2008, pages 
1453-1454.  
Keiji Shinzato and Kentaro Torisawa. 2004. Acquir-
ing Hyponymy Relations from Web Documents. In 
HLT/NAACL04, pages 73?80. 
Keiji Shinzato and Kentaro Torisawa. 2005. A Simple 
WWW-based Method for Semantic Word Class 
Acquisition. In RANLP05.  
Richard C. Wang and William W. Cohen. 2007. Lan-
gusage-Independent Set Expansion of Named Enti-
ties Using the Web. In ICDM2007. 
 
467
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Anchor Text Extraction for Academic Search 
 
 
Shuming Shi1     Fei Xing2*     Mingjie Zhu3*     Zaiqing Nie1     Ji-Rong Wen1 
1Microsoft Research Asia 
2Alibaba Group, China 
3University of Science and Technology of China 
{shumings, znie, jrwen}@microsoft.com 
fei_c_xing@yahoo.com; mjzhu@ustc.edu 
 
 
 
Abstract* 
 
Anchor text plays a special important role in 
improving the performance of general Web 
search, due to the fact that it is relatively ob-
jective description for a Web page by poten-
tially a large number of other Web pages. 
Academic Search provides indexing and 
search functionality for academic articles. It 
may be desirable to utilize anchor text in aca-
demic search as well to improve the search re-
sults quality. The main challenge here is that 
no explicit URLs and anchor text is available 
for academic articles. In this paper we define 
and automatically assign a pseudo-URL for 
each academic article. And a machine learning 
approach is adopted to extract pseudo-anchor 
text for academic articles, by exploiting the ci-
tation relationship between them. The ex-
tracted pseudo-anchor text is then indexed and 
involved in the relevance score computation of 
academic articles. Experiments conducted on 
0.9 million research papers show that our ap-
proach is able to dramatically improve search 
performance. 
1 Introduction 
Anchor text is a piece of clickable text that links 
to a target Web page. In general Web search, 
anchor text plays an extremely important role in 
improving the search quality. The main reason 
for this is that anchor text actually aggregates the 
opinion (which is more comprehensive, accurate, 
and objective) of a potentially large number of 
people for a Web page. 
                                                 
* This work was performed when Fei Xing and Mingjie Zhu 
were interns at Microsoft Research Asia. 
In recent years, academic search (Giles et al, 
1998; Lawrence et al, 1999; Nie et al, 2005; 
Chakrabarti et al, 2006) has become an impor-
tant supplement to general web search for re-
trieving research articles. Several academic 
search systems (including Google Scholar?, Cite-
seer?, DBLP?, Libra**, ArnetMiner??, etc.) have 
been deployed. In order to improve the results 
quality of an academic search system, we may 
consider exploiting the techniques which are 
demonstrated to be quite useful and critical in 
general Web search. In this paper, we study the 
possibility of extracting anchor text for research 
papers and using them to improve the search per-
formance of an academic search system. 
 
 
Figure 1. An example of one paper citing other papers 
 
The basic search unit in most academic search 
systems is a research paper. Borrowing the con-
cepts of URL and anchor-text in general Web 
search, we may need to assign a pseudo-URL for 
one research paper as its identifier and to define 
the pseudo-anchor text for it by the contextual 
description when this paper is referenced (or 
mentioned). The pseudo-URL of a research pa-
per could be the combination of its title, authors 
and publication information. Figure-1 shows an 
excerpt where one paper cites a couple of other 
                                                 
? http://scholar.google.com/ 
? http://citeseerx.ist.psu.edu/ 
? http://www.informatik.uni-trier.de/~ley/db/ 
** http://libra.msra.cn/ 
?? http://www.arnetminer.org/ 
10
papers. The grayed text can be treated as the 
pseudo-anchor text of the papers being refe-
renced. Once the pseudo-anchor text of research 
papers is acquired, it can be indexed and utilized 
to help ranking, just as in general web search. 
However it remains a challenging task to cor-
rectly identify and extract these pseudo-URLs 
and pseudo-anchor texts. First, unlike the situa-
tion in general web search where one unique 
URL is assigned to each web page as a natural 
identifier, the information of research papers 
need to be extracted from web pages or PDF files. 
As a result, in constructing pseudo-URLs for 
research papers, we may face the problem of ex-
traction errors, typos, and the case of one re-
search paper having different expressions in dif-
ferent places. Second, in general Web search, 
anchor text is always explicitly specified by 
HTML tags (<a> and </a>). It is however much 
harder to perform anchor text extraction for re-
search papers. For example, human knowledge 
may be required in Figure-1 to accurately identi-
fy the description of every cited paper. 
To address the above challenges, we propose 
an approach for extracting and utilizing pseudo-
anchor text information in academic search to 
improve the search results quality. Our approach 
is composed of three phases. In the first phase, 
each time a paper is cited in another paper, we 
construct a tentative pseudo-URL for the cited 
paper and extract a candidate anchor block for it. 
The tentative pseudo-URL and the candidate 
anchor block are allowed to be inaccurate. In the 
second phase, we merge the tentative pseudo-
URLs that should represent the same paper. All 
candidate anchor blocks belong to the same pa-
per are grouped accordingly in this phase. In the 
third phase, the final pseudo-anchor text of each 
paper is generated from all its candidate blocks, 
by adopting a SVM-based machine learning me-
thodology. We conduct experiments upon a data-
set containing 0.9 million research papers. The 
experimental results show that lots of useful anc-
hor text can be successfully extracted and accu-
mulated using our approach, and the ultimate 
search performance is dramatically improved 
when anchor information is indexed and used for 
paper ranking. 
The remaining part of this paper is organized 
as follows. In Section 2, we describe in detail our 
approach for pseudo-anchor text extraction and 
accumulation. Experimental results are reported 
in Section 3. We discuss related work in Section 
4 and finally conclude the paper in Section 5. 
2 Our Approach 
2.1 Overview 
Before describing our approach in detail, we first 
recall how anchor text is processed in general 
Web search. Assume that there have been a col-
lection of documents being crawled and stored 
on local disk. In the first step, each web page is 
parsed and the out links (or forward links) within 
the page are extracted. Each link is comprised of 
a URL and its corresponding anchor text. In the 
second step, all links are accumulated according 
to their destination URLs (i.e. the anchor texts of 
all links pointed to the same URL are merged). 
Thus, we can get al anchor text corresponding to 
each web page. Figure-2 (a) demonstrates this 
process. 
 
 
Figure 2. The main process of extracting (a) anchor 
text in general web search and (b) pseudo-anchor text 
in academic search 
 
For academic search, we need to extract and 
parse the text content of papers. When a paper A 
mentions another paper B, it either explicitly or 
implicitly displays the key information of B to let 
the users know that it is referencing B instead of 
other papers. Such information can be extracted 
to construct the tentative pseudo-URL of B. The 
pseudo-URLs constructed in this phase are tenta-
tive because different tentative pseudo-URLs 
may be merged to generate the same final pseu-
do-URL. All information related to paper B in 
different papers can be accumulated and treated 
Web pages 
HTML parsing 
Links 
Anchor text 
for pages 
 
Group by link 
destination 
Papers 
Paper parsing 
Tentative pseudo-URLs 
Candidate anchor blocks 
Anchor block accumulation 
Papers with their  
candidate anchor blocks 
Papers with their  
pseudo-anchor text 
Anchor-text learning 
11
as the potential anchor text of B. Our goal is to 
get the anchor text related to each paper. 
Our approach for pseudo-anchor text extrac-
tion is shown in Figure-2 (b). The key process is 
similar to that in general Web search for accumu-
lating and utilizing page anchor text. One prima-
ry difference between Figure-2 (a) and (b) is the 
latter accumulates candidate anchor blocks rather 
than pieces of anchor text. A candidate anchor 
block is a piece of text that contains the descrip-
tion of one paper. The basic idea is: Instead of 
extracting the anchor text for a paper directly (a 
difficult task because of the lack of enough in-
formation), we first construct a candidate anchor 
block to contain the "possible" or "potential" de-
scription of the paper. After we accumulate all 
candidate anchor blocks, we have more informa-
tion to provide a better estimation about which 
pieces of texts are anchor texts. Following this 
idea, our proposed approach adopts a three-phase 
methodology to extract pseudo-anchor text. In 
the first phase, each time a paper B appearing in 
another paper A, a candidate anchor block is ex-
tracted for B. All candidate anchor blocks belong 
to the same paper are grouped in the second 
phase. In the third phase, the final pseudo-anchor 
text of each paper is selected among all candidate 
blocks. 
Extracting tentative pseudo-URLs and can-
didate anchor blocks: When one paper cites 
another paper, a piece of short text (e.g. "[1]" or 
?(xxx et al, 2008)?) is commonly inserted to 
represent the paper to be cited, and the detail in-
formation (key attributes) of it are typically put 
at the end of the document (in the references sec-
tion). We call each paper listed in the references 
section a reference item. The references section 
can be located by searching for the last occur-
rence of term 'reference' or 'references' in larger 
fonts. Then, we adopt a rule-based approach to 
divide the text in the references section into ref-
erence items. Another rule-based approach is 
used to extract paper attributes (title, authors, 
year, etc) from a reference item. We observed 
some errors in our resulting pseudo-URLs caused 
by the quality of HTML files converted from 
PDF format, reference item extraction errors, 
paper attribute extraction errors, and other fac-
tors. We also observed different reference item 
formats for the same paper. The pseudo-URL for 
a paper is defined according to its title, authors, 
publisher, and publication year, because these 
four kinds of information can readily be used to 
identify a paper. 
For each citation of a paper, we treat the sen-
tence containing the reference point (or citation 
point) as one candidate anchor block. When mul-
tiple papers are cited in one sentence, we treat 
the sentence as the candidate anchor block of 
every destination paper. 
Candidate Anchor Block Accumulation: 
This phase is in charge of merging all candidate 
blocks of the same pseudo-URL. As has been 
discussed, tentative pseudo-URLs are often inac-
curate; and different tentative pseudo-URLs may 
correspond to the same paper. The primary chal-
lenge here is perform the task in an efficient way 
and with high accuracy. We will address this 
problem in Subsection 2.2. 
Pseudo-Anchor Generation: In the previous 
phase, all candidate blocks of each paper have 
been accumulated. This phase is to generate the 
final anchor text for each paper from all its can-
didate blocks. Please refer to Subsection 2.3 for 
details. 
2.2 Candidate Anchor Block Accumulation 
via Multiple Feature-String Hashing 
Consider this problem: Given a potentially huge 
number of tentative pseudo-URLs for papers, we 
need to identify and merge the tentative pseudo-
URLs that represent the same paper. This is like 
the problems in the record linkage (Fellegi and 
Sunter, 1969), entity matching, and data integra-
tion which have been extensively studied in da-
tabase, AI, and other areas. In this sub-section, 
we will first show the major challenges and the 
previous similar work on this kind of problem. 
Then a possible approach is described to achieve 
a trade-off between accuracy and efficiency. 
 
 
Figure 3. Two tentative pseudo-URLs representing 
the same paper 
 
2.2.1 Challenges and candidate techniques 
Two issues should be addressed for this problem: 
similarity measurement, and the efficiency of the 
algorithm. On one hand, a proper similarity func-
tion is needed to identify two tentative pseudo-
URLs representing the same paper. Second, the 
12
integration process has to be accomplished effi-
ciently. 
We choose to compute the similarity between 
two papers to be a linear combination of the si-
milarities on the following fields: title, authors, 
venue (conference/journal name), and year. The 
similarity function on each field is carefully de-
signed. For paper title, we adopt a term-level edit 
distance to compute similarity. And for paper 
authors, person name abbreviation is considered. 
The similarity function we adopted is fairly well 
in accuracy (e.g., the similarity between the two 
pseudo-URLs in Figure-3 is high according to 
our function); but it is quite time-consuming to 
compute the similarity for each pair of papers 
(roughly 1012 similarity computation operations 
are needed for 1 million different tentative pseu-
do-URLs). 
Some existing methods are available for de-
creasing the times of similarity calculation opera-
tions. McCallum et al (2000) addresses this high 
dimensional data clustering problem by dividing 
data into overlapping subsets called canopies 
according to a cheap, approximate distance mea-
surement. Then the clustering process is per-
formed by measuring the exact distances only 
between objects from the same canopy. There are 
also other subspace methods (Parsons et al, 2004) 
in data clustering areas, where data are divided 
into subspaces of high dimensional spaces first 
and then processing is done in these subspaces. 
Also there are fast blocking approaches for 
record linkage in Baxter et al (2003). Though 
they may have different names, they hold similar 
ideas of dividing data into subsets to reduce the 
candidate comparison records. The size of data-
set used in the above papers is typically quite 
small (about thousands of data items). For effi-
ciency issue, Broder et al (1997) proposed a 
shingling approach to detect similar Web pages. 
They noticed that it is infeasible to compare 
sketches (which are generated by shingling) of 
all pairs of documents. So they built an inverted 
index that contains a list of shingle values and 
the documents they appearing in. With the in-
verted index, they can effectively generate a list 
of all the pairs of documents that share any shin-
gles, along with the number of shingles they 
have in common. They did experiments on a da-
taset containing 30 million documents. 
By adopting the main ideas of the above tech-
niques to our pseudo-URL matching problem, a 
possible approach can be as follows. 
 
 
Figure 4. The Multiple Feature-String Hashing algo-
rithm for candidate anchor block accumulation 
 
2.2.2 Method adopted 
The method utilized here for candidate anchor 
block accumulation is shown in Figure 4. The 
main idea is to construct a certain number of fea-
ture strings for a tentative pseudo-URL (abbre-
viated as TP-URL) and do hash for the feature 
strings. A feature string of a paper is a small 
piece of text which records a part of the paper?s 
key information, satisfying the following condi-
tions: First, multiple feature strings can typically 
be built from a TP-URL. Second, if two TP-
URLs are different representations of the same 
paper, then the probability that they have at least 
one common feature string is extremely high. We 
can choose the term-level n-grams of paper titles 
(referring to Section 3.4) as feature strings. 
The algorithm maintains an in-memory hash-
table which contains a lot of slots each of which 
is a list of TP-URLs belonging to this slot. For 
each TP-URL, feature strings are generated and 
hashed by a specified hash function. The TP-
URL is then added into some slots according to 
the hash values of its feature strings. Any two 
TP-URLs belonging to the same slot are further 
compared by utilizing our similarity function. If 
their similarity is larger than a threshold, the two 
TP-URLs are treated as being the same and 
therefore their corresponding candidate anchor 
blocks are merged. 
The above algorithm tries to achieve good bal-
ance between accuracy and performance. On one 
hand, compared with the na?ve algorithm of per-
forming one-one comparison between all pairs of 
TP-URLs, the algorithm needs only to compute 
Algorithm Multiple Feature-String Hashing for candidate anchor 
block accumulation 
Input: A list of papers (with their tentative pseudo-URLs 
and candidate anchor blocks) 
Output: Papers with all candidate anchor blocks of the 
same paper aggregated 
 
Initial: An empty hashtable h (each slot of h is a list of pa-
pers) 
For each paper A in the input list { 
For each feature-string of A { 
Lookup by the feature-string in h to get a slot s; 
Add A into s; 
} 
} 
For each slot s with size smaller than a threshold { 
For any two papers A1, A2 in s { 
float fSim = Similarity(A1, A2); 
if(fSim > the specified threshold) { 
Merge A1 and A2; 
} 
} 
} 
13
the similarity for the TP-URLs that share a 
common slot. On the other hand, because of the 
special property of feature strings, most TP-
URLs representing the same paper can be de-
tected and merged. 
The basic idea of dividing data into over-
lapped subsets is inherited from McCallum et al 
(2000), Broder et al (1997), and some subspace 
clustering approaches. Slightly different, we do 
not count the number of common feature strings 
between TP-URLs. Common bins (or inverted 
indices) between data points are calculated in 
McCallum et al (2000) as a ?cheap distance? for 
creating canopies. The number of common Shin-
gles between two Web documents is calculated 
(efficiently via inverted indices), such that Jac-
card similarity could be used to measure the si-
milarity between them. In our case, we simply 
compare any two TP-URLs in the same slot by 
using our similarity function directly. 
The effective and efficiency of this algorithm 
depend on the selection of feature strings. For a 
fixed feature string generation method, the per-
formance of this algorithm is affected by the size 
of each slot, especially the number and size of 
big slots (slots with size larger than a threshold). 
Big slots will be discarded in the algorithm to 
improve performance, just like removing com-
mon Shingles in Broder et al (1997). In Section 
4, we conduct experiments to test the perfor-
mance of the above algorithm with different fea-
ture string functions and different slot size thre-
sholds. 
2.3 Pseudo-Anchor Text Learning 
In this subsection, we address the problem of 
extracting the final pseudo-anchor text for a pa-
per, given all its candidate anchor blocks (see 
Figure 5 for an example). 
2.3.1 Problem definition 
A candidate anchor block is a piece of text with 
one or some reference points (a reference point is 
one occurrence of citation in a paper) specified, 
where a reference point is denoted by a 
<start_pos, end_pos> pair (means start position 
and end position respectively): ref = <start_pos, 
end_pos>. We represent a candidate anchor 
block to be the following format, 
AnchorBlock = (Text, ref1, ref2, ?) 
We define a block set to be a set of candidate 
anchor blocks for a paper, 
BlockSet = {AnchorBlock1, AnchorBlock2, ?} 
Now the problem is: Given a block set con-
taining N elements, extract some text excerpts 
from them as the anchor text of the paper. 
2.3.2 Learn term weights 
We adopt a machine-learning approach to assign, 
for each term in the anchor blocks, a discrete de-
gree of being anchor text. The main reasons for 
taking such an approach is twofold: First, we 
believe that assigning each term a fuzzy degree 
of being anchor text is more appropriate than a 
binary judgment as either an anchor-term or non-
anchor-term. Second, since the importance of a 
term for a ?link? may be determined by many 
factors in paper search, a machine-learning could 
be more flexible and general than the approaches 
that compute term degrees by a specially de-
signed formula. 
 
 
Figure 5. The candidate pseudo-anchor blocks of a 
paper 
 
The features used for learning are listed in Ta-
ble-1. 
We observed that it would be more effective if 
some of the above features are normalized before 
being used for learning. For a term in candidate 
anchor block B, its TF are normalized by the 
BM25 formula (Robertson et al, 1999), 
 
TFL
Bbbk
TFkTFnorm ?????
???
)||)1((
)1(
1
1
 
 
where L is average length of the candidate blocks, 
|B| is the length of B, and k1, b are parameters. 
DF is normalized by the following formula, 
 )1log( DF
NIDF ??
  
where N is the number of elements in the block 
set (i.e. total number of candidate anchor blocks 
for the current paper). 
Features RefPos and Dist are normalized as, 
 
RefPosnorm = RefPos / |B| 
Distnorm = (Dist-RefPos) / |B| 
 
And the feature BlockLen is normalized as, 
14
 BlockLennorm = log(1+BlockLen)  
 
Features Description 
DF 
Document frequency: Number of candidate blocks in 
which the term appears, counted among all candidate 
blocks of all papers. It is used to indicate whether the 
term is a stop word or not. 
BF 
Block frequency: Number of candidate blocks in 
which the term appears, counted among all candidate 
blocks of this paper. 
CTF 
Collection term frequency: Total number of times the 
term appearing in the blocks. For multiple times of 
occurrences in one block, all of them are counted. 
IsInURL 
Specify whether the term appears in the pseudo-URL 
of the paper. 
TF 
Term frequency: Number of times the terms appearing 
in the candidate block. 
Dist 
Directed distance from the nearest reference point to 
the term location 
RefPos 
Position of the nearest reference point in the candidate 
pseudo-anchor block. 
BlockLen Length of the candidate pseudo-anchor block 
Table 1. Features for learning 
 
We set four term importance levels, from 1 
(unrelated terms or stop words) to 4 (words par-
ticipating in describing the main ideas of the pa-
per). 
We choose support vector machine (SVM) for 
learning term weights here, because of its power-
ful classification ability and well generalization 
ability (Burges, 1998). We believe some other 
machine learning techniques should also work 
here. The input of the classifier is a feature vec-
tor of a term and the output is the importance 
level of the term. Given a set of training data 
? ?liii levelfeature 1, ?, a decision function f(x) can be 
acquired after training. Using the decision func-
tion, we can assign an importance level for each 
term automatically. 
 
3 Experiments 
3.1 Experimental Setup 
Our experimental dataset contains 0.9 million 
papers crawled from the web. All the papers are 
processed according to the process in Figure-2 
(b). We randomly select 300 queries from the 
query log of Libra (libra.msra.cn) and retrieve 
the results in our indexing and ranking system 
with/without the pseudo-anchors generated by 
our approach. Then the volunteer researchers and 
students in our group are involved to judge the 
search results. The top 30 results of different 
ranking algorithms for each query are labeled 
and assigned a relevance value from 1 (meaning 
'poor match') to 5 (meaning 'perfect match'). The 
search results quality is measured by NDCG 
(Jarvelin and Kekalainen, 2000). 
3.2 Overall Effect of our Approach 
Figure 6 shows the performance comparison be-
tween the results of two baseline paper ranking 
algorithms and the results of including pseudo-
anchor text in ranking. 
 
0.466
0.426
0.388
0.597
0.619
0.689
0.673 0.672
0.627
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
NDCG@1 NDCG@3 NDCG@10
Base(Without CitationCount)
Base
Pseudo-Anchor Included
 
Figure 6. Comparison between the baseline approach 
and our approach (measure: nDCG) 
 
The ?Base? algorithm considers the title, ab-
stract, full-text and static-rank (which is a func-
tion of the citation count) of a paper. In a bit 
more detail, for each paper, we adopt the BM25 
formula (Robertson et al, 1999) over its title, 
abstract, and full-text respectively. And then the 
resulting score is linearly combined with the stat-
ic-rank to get its final score. The static-rank is 
computed as follows, 
 StaticRank = log(1+CitationCount) (3.1) 
To test the performance of including pseudo-
anchor text in ranking, we compute an anchor 
score for each paper and linearly combine it with 
its baseline score (i.e. the score computed by the 
baseline algorithm). 
We tried two kinds of ways for anchor score 
computation. The first is to merge all pieces of 
anchor excerpts (extracted in the previous section) 
into a larger piece of anchor text, and use BM25 
to compute its relevance score. In another ap-
proach called homogeneous evidence combina-
tion (Shi et al, 2006), a relevance score is com-
puted for each anchor excerpt (still using BM25), 
and all the scores for the excerpts are sorted des-
cending and then combined by the following 
formula, 
 ?
?
?????
m
i
ianchor sicS 1 2))1(1(
1 (3.2) 
where si (i=1, ?, m) are scores for the m anchor 
excerpts, and c is a parameter. The primary idea 
15
here is to let larger scores to have relative greater 
weights. Please refer to Shi et al (2006) for a 
justification of this approach. As we get slightly 
better results with the latter way, we use it as our 
final choice for computing anchor scores. 
From Figure 6, we can see that the overall per-
formance is greatly improved by including pseu-
do-anchor information. Table 2 shows the t-test 
results, where a ?>? indicates that the algorithm 
in the row outperforms that in the column with a 
p-value of 0.05 or less, and a ?>>? means a p-
value of 0.01 or less. 
 
 
 
Base 
Base (without 
CitationCount) 
Our approach > >> 
Base  >> 
Base (without Cita-
tionCount) 
  
Table 2. Statistical significance tests (t-test over 
nDCG@3) 
 
Table 3 shows the performance comparison by 
using some traditional IR measures based on bi-
nary judgments. Since the results of not includ-
ing CitationCount are much worse than the other 
two, we omit it in the table. 
 
Measure 
Approach 
MAP MRR P@1 P@10 
Base (including 
CitationCount) 
0.364 0.727 0.613 0.501 
Our Approach 0.381 0.734 0.625 0.531 
Table 3. Performance compassion using binary judg-
ment measures 
 
3.3 Sample Query Analysis 
Here we analyze some sample queries to get 
some insights about why and how pseudo-anchor 
improves search performance. Figure-7 and Fig-
ure-8 show the top-3 results of two sample que-
ries: {TF-IDF} and {Page Rank}. 
For query "TF-IDF", the top results of the 
baseline approach have keyword "TF-IDF" ap-
peared in the title as well as in other places of the 
papers. Although the returned papers are relevant 
to the query, they are not excellent because typi-
cally users may want to get the first TF-IDF pa-
per or some papers introducing TF-IDF. When 
pseudo-anchor information is involved, some 
excellent results (B1, B2, B3) are generated. The 
main reason for getting the improved results is 
that these papers (or books) are described with 
"TF-IDF" when lots of other papers cite them. 
 
 
Figure 7. Top-3 results for query TF-IDF 
 
 
Figure 8. Top-3 results for query Page Rank 
 
Figure-8 shows another example about how 
pseudo-anchor helps to improve search results 
quality. For query "Page Rank" (note that there is 
a space in between), the results returned by the 
baseline approach are not satisfactory. In the pa-
pers returned by our approach, at least B1 and B2 
are very good results. Although they did not la-
bel themselves "Page Rank", other papers do so 
in citing them. Interestingly, although the result 
B3 is not about the "PageRank" algorithm, it de-
scribes another popular "Page Rank" algorithm 
in addition to PageRank. 
Another interesting observation from the two 
figures is that our approach retrieves older papers 
than the baseline method, because old papers 
tend to have more anchor text (due to more cita-
tions). So our approach may not be suitable for 
retrieve newer papers. To overcome this problem, 
maybe publication year should be considered in 
our ranking functions. 
3.4 Anchor Accumulation Experiments 
We conduct experiments to test the effectiveness 
and efficiency of the multiple-feature-string-
hashing algorithm presented in Section 2.2. The 
duplication detection quality of this algorithm is 
determined by the appropriate selection of fea-
A1. V Safronov, M Parashar, Y Wang et al Optimizing Web servers 
using Page rank prefetching for clustered accesses. Information 
Sciences. 2003. 
A2. AO Mendelzon, D Rafiei. An autonomous page ranking method for 
metasearch engines. WWW, 2002. 
A3. FB Kalhoff. On formally real Division Algebras and Quasifields of 
Rank two. 
(a) Without anchor 
B1. S Brin, L Page. The Anatomy of a Large-Scale Hypertextual Web 
Search Engine. WWW, 1998 
B2. L Page, S Brin, R Motwani, T Winograd. The pagerank citation 
ranking: Bringing order to the web. 1998. 
B3. JM Kleinberg. Authoritative sources in a hyperlinked environment. 
Journal of the ACM, 1999. 
(b) With anchor 
 
A1. K Sugiyama, K Hatano, M Yoshikawa, S Uemura. Refinement of TF-
IDF schemes for web pages using their hyperlinked neighboring pages. 
Hypertext?03 
A2. A Aizawa. An information-theoretic perspective of tf-idf measures. 
IPM?03. 
A3. N Oren. Reexamining tf.idf based information retrieval with Genet-
ic Programming. SAICSIT?02. 
(a) Without anchor 
B1. G Salton, MJ McGill. Introduction to Modern Information Retriev-
al. McGraw-Hill, 1983. 
B2. G Salton and C Buckley. Term weighting approaches in automatic 
text retrieval. IPM?98. 
B3. R Baeza-Yates, B Ribeiro-Neto. Modern Information Retrieval. 
Addison-Wesley, 1999 
(b) With anchor 
 
16
ture strings. When feature strings are fixed, the 
slot size threshold can be used to tune the tra-
deoff between accuracy and performance. 
 
Feature Strings 
Slot Distr. 
Ungram Bigram Trigram 4-gram 
# of Slots 1.4*105 1.2*106 2.8*106 3.4*106 
# of Slots with 
size > 100 
5240 6806 1541 253 
# of Slots with 
size > 1000 
998 363 50 5 
# of Slots with 
size > 10000 
59 11 0 0 
Table 4. Slot distribution with different feature strings 
 
We take all the papers extracted from PDF 
files as input to run the algorithm. Identical TP-
URLs are first eliminated (therefore their candi-
date anchor blocks are merged) by utilizing a 
hash table. This pre-process step results in about 
1.46 million distinct TP-URLs. The number is 
larger than our collection size (0.9 million), be-
cause some cited papers are not in our paper col-
lection. We tested four kinds of feature strings all 
of which are generated from paper title: uni-
grams, bigrams, trigrams, and 4-grams. Table-4 
shows the slot size distribution corresponding to 
each kind of feature strings. The performance 
comparison among different feature strings and 
slot size thresholds is shown in Table 5. It seems 
that bigrams achieve a good trade-off between 
accuracy and performance. 
 
Feature 
Strings 
Slot Size 
Threshold 
Dup. papers 
Detected 
Processing 
Time (sec) 
Unigram 
5000 529,717  119,739.0  
500 327,357 7,552.7  
Bigram 500 528,981 8,229.6  
Trigram 
Infinite 518,564 8,420.4  
500 516,369 2,654.9  
4-gram 500 482,299 1,138.2  
Table 5. Performance comparison between different 
feature strings and slot size thresholds 
 
4 Related Work 
There has been some work which uses anchor 
text or their surrounding text for various Web 
information retrieval tasks. It was known at the 
very beginning era of internet that anchor text 
was useful to Web search (McBryan, 1994). 
Most Web search engines now use anchor text as 
primary and power evidence for improving 
search performance. The idea of using contextual 
text in a certain vicinity of the anchor text was 
proposed in Chakrabarti et al (1998) to automat-
ically compile some lists of authoritative Web 
resources on a range of topics. An anchor win-
dow approach is proposed in Chakrabarti et al
(1998) to extract implicit anchor text. Following 
this work, anchor windows were considered in 
some other tasks (Amitay  et al, 1998; Haveli-
wala et al, 2002; Davison, 2002; Attardi et al, 
1999). Although we are inspired by these ideas, 
our work is different because research papers 
have many different properties from Web pages. 
From the viewpoint of implicit anchor extraction 
techniques, our approach is different from the 
anchor window approach. The anchor window 
approach is somewhat simpler and easy to im-
plement than ours. However, our method is more 
general and flexible. In our approach, the anchor 
text is not necessarily to be in a window. 
Citeseer (Giles et al, 1998; Lawrence  et al, 
1999) has been doing a lot of valuable work on 
citation recognition, reference matching, and pa-
per indexing. It has been displaying contextual 
information for cited papers. This feature has 
been shown to be helpful and useful for re-
searchers. Differently, we are using context de-
scription for improving ranking rather than dis-
play purpose. In addition to Citeseer, some other 
work (McCallum et al, 1999; Nanba and Oku-
mura, 1999; Nanba et al, 2004; Shi et al, 2006) 
is also available for extracting and accumulating 
reference information for research papers. 
5 Conclusions and Future Work 
In this paper, we propose to improve academic 
search by utilizing pseudo-anchor information. 
As pseudo-URL and pseudo-anchor text are not 
as explicit as in general web search, more efforts 
are needed for pseudo-anchor extraction. Our 
machine-learning approach has proven success-
ful in automatically extracting implicit anchor 
text. By using the pseudo-anchors in our academ-
ic search system, we see a significant perfor-
mance improvement over the basic approach. 
 
 
Acknowledgments 
We would like to thank Yunxiao Ma and Pu 
Wang for converting paper full-text from PDF to 
HTML format. Jian Shen has been helping us do 
some reference extraction and matching work. 
Special thanks are given to the researchers and 
students taking part in data labeling. 
 
 
 
 
17
References 
E. Amitay. 1998. Using common hypertext links to 
identify the best phrasal description of target web 
documents. In Proc. of the SIGIR'98 Post Confe-
rence Workshop on Hypertext Information Re-
trieval for the Web, Melbourne, Australia. 
G. Attardi, A. Gulli, and F. Sebastiani. 1999. Theseus: 
categorization by context. In Proceedings of the 8th 
International World Wide Web Conference. 
A. Baxter, P. Christen, T. Churches. 2003. A compar-
ison of fast blocking methods for record linkage. In 
ACM SIGKDD'03 Workshop on Data Cleaning, 
Record Linkage and Object consolidation. Wash-
ington DC. 
A. Broder, S. Glassman, M. Manasse, and G. Zweig. 
1997. Syntactic clustering of the Web. In Proceed-
ings of the Sixth International World Wide Web 
Conference, pp. 391-404. 
C.J.C. Burges. 1998. A tutorial on support vector ma-
chines for pattern recognition. Data Mining and 
Knowledge Discovery, 2, 121-167. 
S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P. 
Raghavan, and S. Rajagopalan. 1998. Automatic 
resource list compilation by analyzing hyperlink 
structure and associated text. In Proceedings of the 
7th International World Wide Web Conference. 
K. Chakrabarti, V. Ganti, J. Han, and D. Xin. 2006. 
Ranking objects based on relationships. In SIG-
MOD ?06: Proceedings of the 2006 ACM SIG-
MOD international conference on Management of 
data, pages 371?382, New York, NY, USA. ACM. 
B. Davison. 2000. Topical locality in the web. In SI-
GIR'00: Proceedings of the 23rd annual interna-
tional ACM SIGIR conference on Research and 
development in information retrieval, pages 272- 
279, New York, NY, USA. ACM. 
I.P. Fellegi, and A.B. Sunter. A Theory for Record 
Linkage, Journal of the American Statistical Asso-
ciation, 64, (1969), 1183-1210. 
C. L. Giles, K. Bollacker, and S. Lawrence. 1998. 
CiteSeer: An automatic citation indexing system. 
In IanWitten, Rob Akscyn, and Frank M. Shipman 
III, editors, Digital Libraries 98 - The Third ACM 
Conference on Digital Libraries, pages 89?98, 
Pittsburgh, PA, June 23?26. ACM Press. 
T.H. Haveliwala, A. Gionis, D. Klein, and P. Indyk. 
2002. Evaluating strategies for similarity search on 
the web. In WWW ?02: Proceedings of the 11th in-
ternational conference on World Wide Web, pages 
432?442, New York, NY, USA. ACM. 
K. Jarvelin, and J. Kekalainen. 2000. IR Evaluation 
Methods for Retrieving Highly Relevant Docu-
ments. In Proceedings of the 23rd Annual Interna-
tional ACM SIGIR Conference on Research and 
Development in Information Retrieval (SI-
GIR2000). 
S. Lawrence, C.L. Giles, and K. Bollacker. 1999. Dig-
ital libraries and Autonomous Citation Indexing. 
IEEE Computer, 32(6):67?71. 
A. McCallum, K. Nigam, J. Rennie, and K. Seymore. 
1999. Building Domain-specific Search Engines 
with Machine Learning Techniques. In Proceed-
ings of the AAAI-99 Spring Symposium on Intelli-
gent Agents in Cyberspace. 
A. McCallum, K. Nigam, and L. Ungar. 2000. Effi-
cient clustering of high-dimensional data sets with 
application to reference matching. In Proc. 6th 
ACM SIGKDD Int. Conf. on Knowledge Discov-
ery and Data Mining. 
O.A. McBryan. 1994. Genvl and wwww: Tools for 
taming the web. In In Proceedings of the First In-
ternational World Wide Web Conference, pages 
79-90. 
H. Nanba, M. Okumura. 1999. Towards Multi-paper 
Summarization Using Reference Information. In 
Proc. of the 16th International Joint Conference on 
Artificial Intelligence, pp.926-931. 
H. Nanba, T. Abekawa, M. Okumura, and S. Saito. 
2004. Bilingual PRESRI: Integration of Multiple 
Research Paper Databases. In Proc. of RIAO 2004, 
195-211. 
L. Parsons, E. Haque, H. Liu. 2004. Subspace cluster-
ing for high dimensional data: a review. SIGKDD 
Explorations 6(1): 90-105. 
S.E. Robertson, S. Walker, and M. Beaulieu. 1999. 
Okapi at TREC-7: automatic ad hoc, filtering, VLC 
and filtering tracks. In Proceedings of TREC?99. 
S. Shi, R. Song, and J-R Wen. 2006. Latent Additivity: 
Combining Homogeneous Evidence. Technique 
report, MSR-TR-2006-110, Microsoft Research, 
August 2006. 
S. Shi, F. Xing, M. Zhu, Z.Nie, and J.-R. Wen. 2006. 
Pseudo-Anchor Extraction for Search Vertical Ob-
jects. In Proc. of the 2006 ACM 15th Conference 
on Information and Knowledge Management. Ar-
lington, USA. 
Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma. 2005. 
Object-level ranking: bringing order to web objects. 
InWWW?05: Proceedings of the 14th international 
conference on World Wide Web, pages 567?574, 
New York, NY, USA. ACM. 
 
18
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 993?1001,
Beijing, August 2010
Corpus-based Semantic Class Mining: 
Distributional vs. Pattern-Based Approaches 
Shuming Shi1    Huibin Zhang2*    Xiaojie Yuan2    Ji-Rong Wen1 
1 Microsoft Research Asia 
2 Nankai University 
{shumings, jrwen}@microsoft.com 
zhanghuibin@126.com; yuanxj@nankai.edu.cn 
 
 
Abstract 
Main approaches to corpus-based seman-
tic class mining include distributional 
similarity (DS) and pattern-based (PB). 
In this paper, we perform an empirical 
comparison of them, based on a publicly 
available dataset containing 500 million 
web pages, using various categories of 
queries. We further propose a frequency-
based rule to select appropriate approach-
es for different types of terms. 
1 Introduction1 
Computing the semantic relationship between 
terms, which has wide applications in natural 
language processing and web search, has been a 
hot topic nowadays. This paper focuses on cor-
pus-based semantic class mining (Lin 1998; Pan-
tel and Lin 2002; Pasca 2004; Shinzato and 
Torisawa, 2005; Ohshima, et al, 2006; Zhang et 
al., 2009), where peer terms (or coordinate terms) 
are discovered from a corpus. 
Existing approaches to semantic class mining 
could roughly be divided into two categories: 
distributional similarity (DS), and pattern-based 
(PB). The first type of work (Hindle, 1990; Lin 
1998; Pantel and Lin 2002) is based on the distri-
butional hypothesis (Harris, 1985), saying that 
terms occurring in analogous (lexical or syntactic) 
contexts tend to be similar. DS approaches basi-
cally exploit second-order co-occurrences to dis-
cover strongly associated concepts. In pattern-
based approaches (Hearst 1992; Pasca 2004; 
Shinzato and Torisawa, 2005; Ohshima, et al, 
2006; Zhang et al, 2009), patterns are applied to 
                                                 
* Work done during an internship at Microsoft 
discover specific relationships between terms, 
from the general first-order co-occurrences. For 
example, ?NP such as NP, NP?, and NP? is a 
popular and high-quality pattern for extracting 
peer terms (and also hyponyms). Besides the nat-
ural language patterns, some HTML tag tree pat-
terns (e.g., the drop down list) are also effective 
in semantic class mining. 
It is worth-noting that the word ?pattern? also 
appears in some DS approaches (Pasca et al, 
2006; Tanev and Magnini, 2006; Pennacchiotti 
and Pantel, 2009), to represent the context of a 
term or a term-pair, e.g., ?(invent, subject-of)? 
for the term ?Edison?, and ?- starring -? for the 
term-pair ?(The Terminal, Tom Hanks)?. Alt-
hough ?patterns? are utilized, we categorize them 
as DS approaches rather than PB, because they 
match the DS framework well. In this paper, PB 
only refers to the approaches that utilize patterns 
to exploit first-order co-occurrences. And the 
patterns in DS approaches are called contexts in 
the following part of this paper. 
Progress has been made and promising results 
have been reported in the past years for both DS 
and PB approaches. However, most previous re-
search work (some exceptions are discussed in 
related work) involves solely one category of ap-
proach. And there is little work studying the 
comparison of their performance for different 
types of terms (we use ?term? to represent a sin-
gle word or a phrase). 
In this paper, we make an empirical study of 
this problem, based on a large-scale, publicly 
available dataset containing 500 million web 
pages. For each approach P, we build a term-
similarity graph G(P), with vertices representing 
terms, and edges being the confidence that the 
two terms are peers. Approaches are compared 
by the quality of their corresponding term graphs. 
993
We measure the quality of a term graph by set 
expansion. Two query sets are adopted: One con-
tains 49 semantic classes of named entities and 
20220 trials (queries), collected by Pantel et al 
(2009) from Wikipedia2; and the other contains 
100 queries of five lexical categories (proper 
nouns, common nouns, verbs, adjectives, and 
adverbs), built in this paper for studying the per-
formance comparison on different term types. 
With the dataset and the query sets, we study the 
comparison of DS and PB. Key observations and 
preliminary conclusions are, 
?   DS vs. PB: DS approaches perform much 
better on common nouns, verbs, adjectives, 
and adverbs; while PB generates higher-
quality semantic classes for proper nouns. 
?   Lexical vs. Html-tag patterns: If only lexi-
cal patterns are adopted in PB, the perfor-
mance drops significantly; while the perfor-
mance only becomes slightly worse with only 
Html-tag patterns being included. 
?   Corpus-size: For proper nouns, PB beats 
DS even based on a much smaller corpus; 
similarly, for other term types, DS performs 
better even with a smaller corpus. 
Given these observations, we further study the 
feasibility of selecting appropriate approaches for 
different term types to obtain better results. A 
simple and effective frequency-based rule is pro-
posed for approach-selection. Our online seman-
tic mining system (NeedleSeek)3 adopts both PB 
and DS to build semantic classes. 
2 Related Work 
Existing efforts for semantic class mining has 
been done upon various types of data sources, 
including text-corpora, search-results, and query 
logs. In corpus-based approaches (Lin 1998; Lin 
and Pantel 2001; Pantel and Lin 2002; Pasca 
2004; Zhang et al, 2009), semantic classes are 
obtained by the offline processing of a corpus 
which can be unstructured (e.g., plain text) or 
semi-structured (e.g., web pages). Search-results-
based approaches (Etzioni et al, 2004; Kozareva 
et al, 2008; Wang and Cohen, 2008) assume that 
multiple terms (or, less often, one term) in a se-
mantic class have been provided as seeds. Other 
terms in the class are retrieved by sending queries 
                                                 
2 http://www.wikipedia.org/ 
3 http://needleseek.msra.cn/ 
(constructed according to the seeds) to a web 
search engine and mining the search results. Que-
ry logs are exploited in (Pasca 2007; Komachi 
and Suzuki, 2008; Yamaguchi 2008) for semantic 
class mining. This paper focuses on corpus-based 
approaches. 
As has been mentioned in the introduction 
part, primarily two types of methodologies are 
adopted: DS and PB. Syntactic context infor-
mation is used in (Hindle, 1990; Ruge, 1992; Lin 
1998; Lin and Pantel, 2001; Pantel and Lin, 2002) 
to compute term similarities. The construction of 
syntactic contexts requires sentences to be parsed 
by a dependency parser, which may be extremely 
time-consuming on large corpora. As an alterna-
tive, lexical context (such as text window) has 
been studied (Pantel et al, 2004; Agirre et al, 
2009; Pantel et al, 2009). In the pattern-based 
category, a lot of work has been done to discover 
term relations by sentence lexical patterns 
(Hearst 1992; Pasca 2004), HTML tag patterns 
(Shinzato and Torisawa, 2005), or both (Shi et al, 
2008; Zhang et al, 2009). In this paper, our focus 
is not one specific methodology, but the compari-
son and combination of them. 
A small amount of existing work is related to 
the comparison or combination of multiple meth-
ods. Pennacchiotti and Pantel (2009) proposed a 
feature combination framework (named ensemble 
semantic) to combine features generated by dif-
ferent extractors (distributional and ?pattern-
based?) from various data sources. As has been 
discussed in the introduction, in our terminology, 
their ?pattern-based? approaches are actually DS 
for term-pairs. In addition, their study is based on 
three semantic classes (actors, athletes, and musi-
cians), all of which are proper nouns. Differently, 
we perform the comparison by classifying terms 
according to their lexical categories, based on 
which additional insights are obtained about the 
pros and cons of each methodology. Pantel et al, 
(2004) proposed, in the scenario of extracting is-
a relations, one pattern-based approach and com-
pared it with a baseline syntactic distributional 
similarity method (called syntactic co-occurrence 
in their paper). Differently, we study the compar-
ison in a different scenario (semantic class min-
ing). In addition, they did not differentiate the 
lexical types of terms in the study. The third dif-
ference is that we proposed a rule for method-
selection while they did not. In (Pasca and Durme, 
994
2008), clusters of distributional similar terms 
were adopted to expand the labeled semantic 
classes acquired from the ?such as | including? 
pattern. Although both patterns and distributional 
similarity were used in their paper, they did not 
do any comparison about their performance. 
Agirre et al (2009) compared DS approaches 
with WordNet-based methods in computing word 
similarity and relatedness; and they also studied 
the combination of them. Differently, the meth-
ods for comparison in our paper are DS and PB. 
3 Similarity Graph Construction 
A key operation in corpus-based semantic class 
mining is to build a term similarity graph, with 
vertices representing terms, and edges being the 
similarity (or distance) between terms. Given the 
graph, a clustering algorithm can be adopted to 
generate the final semantic classes. Now we de-
scribe the state-of-the-art DS and PB approaches 
for computing term similarities. 
3.1 Distributional Similarity 
DS approaches are based on the distributional 
hypothesis (Harris, 1985), which says that terms 
appearing in analogous contexts tend to be simi-
lar. In a DS approach, a term is represented by a 
feature vector, with each feature corresponding to 
a context in which the term appears. The similari-
ty between two terms is computed as the similari-
ty between their corresponding feature vectors. 
Different approaches may have different ways of 
1) defining a context, 2) assigning feature values, 
or 3) measuring the similarity between two fea-
ture vectors. 
 
Contexts 
Text window (window size: 2, 4) 
Syntactic 
Feature value PMI 
Similarity measure Cosine, Jaccard 
Table 1. DS approaches implemented in this paper 
 
Mainly two kinds of contexts have been exten-
sively studied: syntactic context and lexical con-
text. The construction of syntactic contexts relies 
on the syntactic parsing trees of sentences, which 
are typically the output of a syntactic parser. Giv-
en a syntactic tree, a syntactic context of a term w 
can be defined as the parent (or one child) of w in 
the tree together with their relationship (Lin, 
1998; Pantel and Lin, 2002; Pantel et al, 2009). 
For instance, in the syntactic tree of sentence 
?this is an interesting read for anyone studying 
logic?, one context of the word ?logic? can be 
defined as ?study V:obj:N?. In this paper, we 
adopt Minipar (Lin, 1994) to parse sentences and 
to construct syntactic trees. 
One popular lexical context is text window, 
where a context c for a term w in a sentence S is 
defined as a substring of the sentence containing 
but removing w. For example, for sentence 
??w1w2w3ww4w5w6??, a text window context 
(with size 4) of w can be ?w2w3w4w5?. It is typi-
cally time-consuming to construct the syntactic 
trees for a large-scale dataset, even with a light-
weight syntactic parser like Minipar. The con-
struction of lexical contexts is much more effi-
cient because it does not require the syntactic 
dependency between terms. Both contexts are 
studied in this paper. 
After defining contexts for a term w, the next 
step is to construct a feature vector for the term: 
F(w)=(fw1, fw2?, fw,m), where m is the number of 
distinct contexts, and fw,c is the feature value of 
context c with respect to term w. Among all the 
existing approaches, the dominant way of assign-
ing feature values (or context values) is compu-
ting the pointwise mutual information (PMI) be-
tween the feature and the term, 
                
             
             
 (3.1) 
where F(w,c) is the frequency of context c occur-
ring for term w, F(w,*) is the total frequency of 
all contexts for term w, F(*,c) is the frequency of 
context c for all terms, and F(*,*) is the total fre-
quency of all context for all terms. They are cal-
culated as follows respectively, 
 
       ?             
       ?             
       ? ?           
 
     
(3.2) 
where m and n are respectively the distinct num-
bers of contexts and terms. 
Following state-of-the-art, we adopt PMI in 
this paper for context weighting. 
Given the feature vectors of terms, the simi-
larity of any two terms is naturally computed as 
the similarity of their corresponding feature vec-
tors. Cosine similarity and Jaccard similarity 
(weighted) are implemented in our experiments, 
         ?  ?  
?      
??   
 
  ??   
 
 
  (3.3) 
995
          ?  ?  
?           
?     ?     ?           
  (3.4) 
Jaccard similarity is finally used in presenting 
our experimental results (in Section 6), because it 
achieves higher performance. 
3.2 Pattern-based Approaches 
In PB approaches, a list of carefully-designed (or 
automatically learned) patterns is exploited and 
applied to a text collection, with the hypothesis 
that the terms extracted by applying each of the 
patterns to a specific piece of text tend to be simi-
lar. Two categories of patterns have been studied 
in the literature: sentence lexical patterns, and 
HTML tag patterns. Table-2 lists some popular 
patterns utilized in existing semantic class mining 
work (Heast 1992; Pasca 2004; Kozareva et al, 
2008; Zhang et al, 2009). In the table, ?T? means 
a term (a word or a phrase). Exactly the same set 
of patterns is employed in implementing our pat-
tern-based approaches in this paper. 
 
Type Pattern 
Lexical 
T {, T}*{,} (and|or) {other} T 
(such as | including) T (and|,|.) 
T, T, T {,T}* 
Tag 
<ul>  <li> T </li>  ?  <li> T </li>  </ul> 
<ol> <li> T </li> ?  <li> T </li> </ol> 
<select> <option> T ?<option> T </select> 
<table>  <tr> <td> T </td> ? <td> T </td> </tr> ... </table> 
Other Html-tag repeat patterns 
Table 2. Patterns employed in this paper (Lexical: 
sentence lexical patterns; Tag: HTML tag patterns) 
We call the set of terms extracted by applying 
a pattern one time as a raw semantic class 
(RASC). The term similarity graph needs to be 
built by aggregating the information of the ex-
tracted RASCs. 
One basic idea of estimating term similarity is 
to count the number of RASCs containing both of 
them. This idea is extended in the state-of-the-art 
approaches (Zhang et al, 2009) to distinguish the 
reliability of different patterns and to punish term 
similarity contributions from the same domain 
(or site), as follows, 
          ?      ?          
  
   
 
 
   
 (3.5) 
where Ci,j is a RASC containing both term a and 
term b, P(Ci,j) is the pattern via which the RASC 
is extracted, and w(P) is the weight of pattern P. 
The above formula assumes all these RASCs be-
long to m sites (or domains) with Ci,j extracted 
from a page in site i, and ki being the number of 
RASCs corresponding to site i. 
In this paper, we adopt an extension of the 
above formula which considers the frequency of 
a single term, as follows, 
 Sim*(a, b) = Sim(a, b)  ?              (3.6) 
where IDF(a)=log(1+N/N(a)), N is the total num-
ber of RASCs, and N(a) is the number of RASCs 
containing a. In the experiments, we simply set 
the weight of every pattern type to be the same 
value (1.0). 
4 Compare PB and DS 
We compare PB and DS by the quality of the 
term similarity graphs they generated. The quali-
ty of a term graph is measured by set expansion: 
Given a list of seed terms (e.g., S={lent, epipha-
ny}) belonging to a semantic class, our task is to 
find other members of this class, such as advent, 
easter, and christmas. 
In this section, we first describe our set expan-
sion algorithm adopted in our study. Then DS 
and PB are compared in terms of their set-
expansion performance. Finally we discuss ways 
of selecting appropriate approaches for different 
types of seeds to get better expansion results. 
4.1 Set Expansion Algorithm 
Having at hand the similarity graph, set expan-
sion can be implemented by selecting the terms 
most similar to the seeds. So given a query 
Q={s1, s2, ?, sk}, the key is to compute       , 
the similarity between a term t and the seed-set 
Q. Naturally, we define it as the weighted aver-
age similarity between t and every seed in Q, 
        ?             
 
     (4.1) 
where   is the weight of seed   , which can be a 
constant value, or a function of the frequency of 
term    in the corpus. Although Formula 3.6 can 
be adopted directly for calculating Sim(t,si), we 
use the following rank-based formula because it 
generate better expansion results. 
           
 
              
 (4.2) 
where         is the rank of term t among the 
neighbors of   . 
In our experiments, we fix  =1 and  =10. 
996
4.2 Compare DS with PB 
In order to have a comprehensive comparison of 
the two approaches, we intentionally choose 
terms of diverse types and do experiments based 
on various data scales. We classify terms into 5 
types by their lexical categories: proper nouns, 
common nouns, verbs, adjectives, and adverbs. 
The data scales for experiments are from one mil-
lion to 500 million web pages. Please refer to 
sections 5.1 and 5.2 for more details about the 
corpora and seeds used for experiments. 
Experimental results (refer to Section 6) will 
show that, for proper nouns, the ranking of ap-
proaches (in terms of performance) is: 
PB > PB-HtmlTag > DS  PB-Lexical 
While for common nouns, verbs, adjectives, 
and adverbs, we have: 
DS > PB 
Here ?PB-lexical? means only the lexical pat-
terns of Table 2 are adopted. Similarly, ?PB-
HtmlTag? represents the PB approach with only 
Html-tag patterns being utilized. 
Please pay attention that this paper by no 
means covers all PB or DS approaches (although 
we have tried our best to include the most popu-
lar ones). For PB, there are of course other kinds 
of patterns (e.g., patterns based on deeper linguis-
tic analysis). For DS, other types of contexts may 
exist in addition to those listed in Table 1. So in 
interpreting experimental results, making obser-
vations, and drawing preliminary conclusions, we 
only means the patterns in Table 2 for PB and 
Table 1 for DS. It will be an interesting future 
work to include more DS and PB approaches in 
the study. 
In order to understand why PB performs so 
well in dealing with proper nouns while so badly 
for other term categories, we calculated the fre-
quency of each seed term in the extracted RASCs, 
the output of the pattern-matching algorithm. We 
define the normalized frequency of a term to be 
its frequency in the RASCs divided by the fre-
quency in the sentences of the original documents 
(with duplicate sentences merged). Then we de-
fine the mean normalized frequency (MNF) of a 
seed set S, as follows, 
        
?            
   
 (4.3) 
where Fnorm(t) is the normalized frequency of t. 
The MNF values for the five seed sets are 
listed in Table 3, where we can see that proper 
nouns have the largest MNF values, followed by 
common nouns. In other words, the patterns in 
Table 2 capture the relations of more proper 
nouns than other term categories. 
 
Seed Categories Terms MNF 
Proper nouns 40 0.2333 
Common nouns 40 0.0716 
Verbs 40 0.0099 
Adjectives 40 0.0126 
Adverbs 40 0.0053 
Table 3. MNF values of different seed categories 
As mentioned in the introduction, the PB and 
DS approaches we studied capture first-order and 
second-order term co-occurrences respectively. 
Some existing work (e.g., Edmonds, 1997) 
showed that second-order co-occurrence leads to 
better results for detecting synonymy. Consider-
ing that a high proportion of coordinate terms of 
verbs, adjectives, and adverbs are their synonyms 
and antonyms, it is reasonable that DS behaves 
better for these term types because it exploits se-
cond-order co-occurrence. For PB, different from 
the standard way of dealing with first-order co-
occurrences where statistics are performed on all 
pairs of near terms, a subset of co-occurred terms 
are selected in PB by specific patterns. The pat-
terns in Table-2 help detecting coordinate proper 
nouns, because they are frequently occurred to-
gether obeying the patterns in sentences or web 
pages. But it is not the case for other term types. 
It will be interesting to study the performance of 
PB when more pattern types are added. 
4.3 Approach Selection 
Having observed that the two approaches per-
form quite differently on every type of queries 
we investigated, we hope we can improve the 
expansion performance by smartly selecting an 
approach for each query. In this section, we pro-
pose and study several approach-selection meth-
ods, by which we hope to gain some insights 
about the possibility and effectiveness of combin-
ing DS and PB for better set expansion. 
Oracle selection: In order to get an insight 
about the upper bound that we could obtain when 
combing the two methods, we implement an ora-
cle that chooses, for each query, the approach 
that generates better expansion results. 
997
Frequency-based selection: It is shown in 
Table 3 that the mean normalized frequency of 
proper nouns is much larger than other terms. 
Motivated by this observation, we select a set 
expansion methodology for each query as fol-
lows: Select PB if the normalized frequency val-
ues of all terms in the query are larger than 0.1; 
otherwise choose DS. 
We demonstrate, in Section 6.3, the effective-
ness of the above selection methods. 
5 Experimental Setup 
5.1 Dataset and Exp. Environment 
We adopt a public-available dataset in our exper-
iments: ClueWeb094. This is a very large dataset 
collected by Carnegie Mellon University in early 
2009 and has been used by several tracks of the 
Text Retrieval Conference (TREC)5. The whole 
dataset consists of 1.04 billion web pages in ten 
languages while only those in English, about 500 
million pages, are used in our experiments. The 
reason for selecting such a dataset is twofold: 
First, it is a corpus large enough for conducting 
web-scale experiments and getting meaningful 
results. Second, since it is publicly available, it is 
possible for other researchers to reproduce the 
experiments in the paper. 
 
Corpora 
Docs 
(millions) 
Sentences 
(millions) 
Description 
Clue500 500 13,000 All En pages in ClueWeb09 
Clue050  50   1,600 ClueWeb09 category B  
Clue010  10      330 Sampling from Clue050 
Clue001   1       42 Sampling from Clue050 
Table 4. Corpora used in experiments 
To test the impact of corpus size on set expan-
sion performance, four corpora are derived from 
the dataset, as outlined in Table 4. The Clue500 
corpus contains all the 500 million English web 
pages in the dataset; while Clue050 is a subset of 
ClueWeb09 (named category B) containing 50 
million English web pages. The remaining two 
corpora are respectively the 1/5 and 1/50 random 
sampling of web pages from Clue050. 
Documents in the corpora are stored and pro-
cessed in a cluster of 40 four-core machines. 
                                                 
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/  
5 http://trec.nist.gov/  
5.2 Query Sets 
We perform our study using two query sets. 
WikiGold: It was collected by Pantel et al 
(2009) from the ?List of? pages in Wikipedia and 
used as the gold standard in their paper. This gold 
standard consists of 49 entity sets, and 20220 tri-
als (used as queries) of various numbers of seeds. 
Most seeds in the query set are named entities. 
Please refer to Pantel et al (2009) for details of 
the gold standard. 
Mix100: This query set consists of 100 queries 
in five categories: verbs, adjectives, adverbs, 
common nouns, and proper nouns. There are 20 
queries in every category and two seeds in every 
query. The query set was built by the following 
steps: First, 20 terms of each category were ran-
domly selected from a term list (which is con-
structed by part-of-speech tagging the Clue050 
corpus and removing low-frequency terms), and 
were treated as the first seed of the each query. 
Then, we manually added one additional seed for 
each query. The reason for utilizing two seeds 
instead of one is the observation that a large por-
tion of the terms selected in the previous step be-
long to multiple categories. For example, ?color-
ful? is both an adjective and a proper noun (a 
Japanese manga). 
5.3 Results Labeling 
No human labeling efforts are needed for the ex-
pansion results of the WikiGold query set. Every 
returned term is automatically judged to be 
?Good? (otherwise ?Bad?) if it appears in the 
corresponding gold standard entity set. 
For Mix100, the search results of various ap-
proaches are merged and labeled by three human 
labelers. Each labeler assigns each term in the 
search results a label of ?Good?, ?Fair? or ?Bad?. 
The labeling agreement values (measured by per-
centage agreement) between labelers I and II, I 
and III, II and III are respectively 0.82, 0.81, and 
0.81. The ultimate judgment of each result term 
is obtained from the three labelers by majority 
voting. In the case of three labelers giving mutu-
ally different results (i.e., one ?Good?, one ?Fair? 
and one ?Bad?), the ultimate judgment is set to 
?Fair? (the average). 
5.4 Evaluation Metrics 
After removing seeds from the expansion results, 
we adopt the following metrics to evaluate the 
998
results of each query. The evaluation score on a 
query set is the average over all the queries. 
Precision@k: The percentage of relevant 
(good or fair) terms in the top-k expansion results 
(terms labeled as ?Fair? are counted as 0.5) 
Recall@k: The ratio of relevant terms in the 
top-k results to the total number of relevant terms 
R-Precision: Precision@R where R is the total 
number of terms labeled as ?Good? 
Mean average precision (MAP): The average 
of precision values at the positions of all good or 
fair results 
6 Experimental Results 
6.1 Overall Performance Comparison 
Table 5 lists the performance (measured by 
MAP, R-precision, and the precisions at ranks 25, 
50, and 100) of some key approaches on corpus 
Clue050 and query set WikiGold. The results of 
query set Mix100 are shown in Table 6. In the 
results, TWn represents the DS approach with 
text-window of size n as contexts, Syntactic is the 
DS approach with syntactic contexts, PB-Lexical 
means only the lexical patterns of Table 2 are 
adopted, and PB-HtmlTag represents the PB ap-
proach with only Html-tag patterns utilized. 
 
Approach MAP R-Prec P@25 P@50 P@100 
TW2 0.218 0.287 0.359 0.278 0.204 
TW4 0.152 0.210 0.325 0.244 0.173 
Syntactic 0.170 0.247 0.314 0.242 0.178 
PB-Lexical 0.227 0.276 0.352 0.272 0.190 
PB-HtmlTag 0.354 0.417 0.513 0.413 0.311 
PB 0.362 0.424 0.520 0.418 0.314 
Pantel-24M N/A 0.264 0.353 0.298 0.239 
Pantel-120M N/A 0.356 0.377 0.319 0.250 
Pantel-600M N/A 0.404 0.407 0.347 0.278 
Table 5. Performance comparison on the Clue050 cor-
pus (query set: WikiGold) 
It is shown that PB gets much higher evalua-
tion scores than other approaches on the WikiG-
old query set and the proper-nouns category of 
Mix100. While for other seed categories in 
Mix100, TW2 return significantly better results. 
We noticed that most seeds in WikiGold are 
proper nouns. So the experimental results tend to 
indicate that the performance comparison be-
tween state-of-the-art DS and PB approaches de-
pends on the types of terms to be mined, specifi-
cally, DS approaches perform better in mining 
semantic classes of common nouns, verbs, adjec-
tives, and adverbs; while state-of-the-art PB ap-
proaches are more suitable for mining semantic 
classes of proper nouns. The performance of PB 
is low in dealing with other types of terms (espe-
cially adverbs). The performance of PB drops 
significantly if only lexical patterns are used; and 
the HtmlTag-only version of PB performs only 
slightly worse than PB. 
The observations are verified by the precision-
recall graph in Figure 1 on Clue500. The results 
of the syntactic approach on Clue500 are not in-
cluded, because it is too time-consuming to parse 
all the 500 million web pages by a dependency 
parser (even using a high-performance parser like 
Minipar). It took overall about 12,000 CPU-hours 
to parse all the sentences in Clue050 by Minipar. 
 
Query types & 
Approaches 
MAP P@5 P@10 P@20 
Proper 
Nouns 
TW2 0.302 0.835 0.810 0.758 
PB 0.336 0.920 0.838 0.813 
Common 
Nouns 
TW2 0.384 0.735 0.668 0.595 
PB 0.212 0.640 0.548 0.485 
Verbs 
TW2 0.273 0.655 0.543 0.465 
PB 0.176 0.415 0.373 0.305 
Adjectives 
TW2 0.350 0.655 0.563 0.473 
PB 0.120 0.335 0.285 0.234 
Adverbs 
TW2 0.432 0.605 0.505 0.454 
PB 0.043 0.100 0.095 0.089 
Table 6. Performance comparison on different query 
types (Corpus: Clue050; query set: Mix100) 
 
Figure 1. Precision and recall of various approaches 
(query set: WikiGold) 
The methods labeled Pantel-24M etc. (in Table 
5 and Figure 1) are the approaches presented in 
(Pantel et al, 2009) on their corpus (called 
Web04, Web20, and Web100 in the paper) con-
taining respectively 24 million, 120 million, and 
600 million web pages. Please pay attention that 
their results and ours may not be directly compa-
rable, because different corpora and set-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
R
e
c
a
ll
 
Precision 
TW221 (Clue500) PB (Clue500)
PB (Clue010) PB (Clue001)
Pantel-600M Pantel-120M
999
expansion algorithms were used. Their results are 
listed here for reference purpose only. 
6.2 Corpus Size Effect 
Table 7 shows the performance (measured by 
MAP) of two approaches on query set Mix100, 
by varying corpus size. We observed that the per-
formance of TW2 improves rapidly along with 
the growth of corpus size from one million to 50 
million documents. From Clue050 to Clue500, 
the performance is slightly improved. 
 
Query types & 
Approaches 
Clue001 Clue010 Clue050 Clue500 
Proper 
Nouns 
TW2 0.209 0.265 0.302 0.311 
PB 0.355 0.351 0.336 0.327 
Common 
Nouns 
TW2 0.259 0.348 0.384 0.393 
PB 0.200 0.234 0.212 0.205 
Verbs 
TW2 0.224 0.268 0.273 0.278 
PB 0.101 0.134 0.176 0.148 
Adjectives 
TW2 0.309 0.326 0.350 0.353 
PB 0.077 0.158 0.120 0.129 
Adverbs 
TW2 0.413 0.423 0.432 0.437 
PB 0.028 0.058 0.043 0.059 
Table 7. Effect of different corpus size (query set: 
Mix100; metric: MAP) 
For PB, however, the performance change is 
not that simple. For proper nouns, the best per-
formance (in terms of MAP) is got on the two 
small corpora Clue001 and Clue010; and the 
score does not increase when corpus size grows. 
Different observations are made on WikiGold 
(see Figure 1), where the performance improves a 
lot with the data growth from Clue001 to 
Clue010, and then stabilizes (from Clue010 to 
Clue500). For other term types, the MAP scores 
do not grow much after Clue010. To our current 
understanding, the reason may be due to the two-
fold effect of incorporating more data in mining: 
bringing useful information as well as noise. 
Clue001 contains enough information, which is 
fully exploited by the PB approach, for expand-
ing the proper-nouns in Mix100. So the perfor-
mance of PB on Clue001 is excellent. The named 
entities in WikiGold are relatively rare, which 
requires a larger corpus (Clue010) for extracting 
peer terms from. But when the corpus gets larger, 
we may not be able to get more useful infor-
mation to further improve results quality. 
Another interesting observation is that, for 
proper nouns, the performance of PB on Clue001 
is even much better than that of TW2 on corpus 
Clue500. Similarly, for other query types (com-
mon nous, verbs, adjectives, and adverbs), TW2 
easily beats PB even with a much smaller corpus. 
6.3 Approach Selection 
Here we demonstrate the experimental results of 
combining DS and PB with the methods we pro-
posed in Section 4.3. Table 8 shows the combina-
tion of PB and TW2 on corpus Clue050 and que-
ry set Mix100. The overall performance relies on 
the number (or percentage) of queries in each 
category. Two ways of mixing the queries are 
tested: avg(4:1:1:1:1) and avg(1:1:1:1:1), where 
the numbers are the proportion of proper nouns, 
common nouns, verbs, adjectives, and adverbs. 
 
Approach 
Avg (1:1:1:1:1) Avg (4:1:1:1:1) 
P@5 P@10 P@20 P@5 P@10 P@20 
TW2 0.697 0.618 0.548 0.749 0.690 0.627 
PB 0.482 0.428 0.385 0.646 0.581 0.545 
Oracle 0.759 0.663 0.591 0.836 0.759 0.695 
Freq-based 0.721 0.633 0.570 0.799 0.723 0.671 
Table 8. Experiments of combining both approaches 
(Corpus: Clue050; query set: Mix100) 
The expansion performance is improved a lot 
with our frequency-based combination method. 
As expected, oracle selection achieves great per-
formance improvement, which shows the large 
potential of combining DS and PB. Similar re-
sults (omitted due to space limitations) are ob-
served on the other corpora. 
Our online semantic mining system (Needle-
Seek, http://needleseek.msra.cn) adopts both PB 
and DS for semantic class construction. 
7 Conclusion 
We compared two mainstream methods (DS and 
PB) for semantic class mining, based on a dataset 
of 500 million pages and using five term types. 
We showed that PB is clearly adept at extracting 
semantic classes of proper nouns; while DS is 
relatively good at dealing with other types of 
terms. In addition, a small corpus is sufficient for 
each approach to generate better semantic classes 
of its ?favorite? term types than those obtained 
by its counterpart on a much larger corpus. Final-
ly, we tried a frequency-based method of com-
bining them and saw apparent performance im-
provement. 
 
1000
References  
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana 
Kravalova, Marius Pasca, Aitor Soroa. A Study on 
Similarity and Relatedness Using Distributional 
and WordNet-based Approaches. NAACL-HLT 
2009. 
Philip Edmonds. 1997. Choosing the Word most Typ-
ical in Context Using a Lexical Co-Occurrence 
Network. ACL'97, pages 507-509. 
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen 
Soderland, Daniel Weld, and Alexander Yates. 
2004. Web-Scale Information Extraction in 
KnowItAll. WWW?04, pages 100?110, New York. 
Zelig S. Harris. 1985. Distributional Structure. The 
Philosophy of Linguistics. New York: Oxford Uni-
versity Press. 
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING?92, 
Nantes, France. 
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In ACL?90, pages 268?
275, Pittsburg, Pennsylvania, June. 
Mamoru Komachi and Hisami Suzuki. Minimally Su-
pervised Learning of Semantic Knowledge from 
Query Logs. IJCNLP 2008, pages 358?365, 2008. 
Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008. 
Semantic Class Learning from the Web with Hypo-
nym Pattern Linkage Graphs. ACL?08: HLT. 
Dekang Lin. 1994. Principar - an Efficient, Broad-
Coverage, Principle-based Parser. COLING?94, pp. 
482-488. 
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING-ACL?98, pages 
768-774. 
Dekang Lin and Patrick Pantel. 2001. Induction of 
Semantic Classes from Natural Language Text. 
SIGKDD?01, pages 317-322. 
Hiroaki Ohshima, Satoshi Oyama and Katsumi 
Tanaka. 2006. Searching Coordinate Terms with 
their Context from the Web. WISE?06. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. 
EMNLP?09. Singapore. 
Patrick Pantel and Dekang Lin. 2002. Discovering 
Word Senses from Text. SIGKDD'02. 
Patric Pantel, Deepak Ravichandran, and Eduard 
Hovy. 2004. Towards Terascale Knowledge Acqui-
sition. COLING?04, Geneva, Switzerland. 
Marius Pasca. 2004. Acquisition of Categorized 
Named Entities for Web Search. CIKM?04. 
Marius Pasca. 2007. Weakly-Supervised Discovery of 
Named Entities Using Web Search Queries. 
CIKM?07. pp. 683-690. 
Marius Pasca and Benjamin Van Durme. 2008. Weak-
ly-supervised Acquisition of Open-Domain Classes 
and Class Attributes from Web Documents and 
Query Logs. ACL?08. 
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei 
Lifchits, and Alpa Jain. 2006. Organizing and 
Searching the World Wide Web of Facts - Step 
One: The One-Million Fact Extraction Challenge. 
In AAAI?06. 
Marco Pennacchiotti and Patrick Pantel. 2009. Entity 
Extraction via Ensemble Semantics.  EMNLP?09. 
Gerda Ruge. 1992. Experiments on Linguistically-
Based Term Associations. Information Processing 
& Management, 28(3): 317-32. 
Keiji Shinzato and Kentaro Torisawa. 2005. A Simple 
WWW-based Method for Semantic Word Class 
Acquisition. Recent Advances in Natural Language 
Processing (RANLP?05), Borovets, Bulgaria. 
Shuming Shi, Xiaokang Liu, Ji-Rong Wen. 2008. Pat-
tern-based Semantic Class Discovery with Multi-
Membership Support. CIKM?08, Napa Valley, Cali-
fornia, USA. 
Hristo Tanev and Bernardo Magnini. 2006. Weakly 
Supervised Approaches for Ontology Population. 
EACL'2006, Trento, Italy. 
Richard C. Wang and William W. Cohen. 2008. Itera-
tive Set Expansion of Named Entities Using the 
Web. ICDM?08, pages 1091?1096. 
Masashi Yamaguchi, Hiroaki Ohshima, Satoshi Oya-
ma, and Katsumi Tanaka. Unsupervised Discovery 
of Coordinate Terms for Multiple Aspects from 
Search Engine Query Logs. The 2008 
IEEE/WIC/ACM International Conference on Web 
Intelligence and Intelligent Agent Technology. 
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-
Rong Wen. 2009. Employing Topic Models for 
Pattern-based Semantic Class Discovery. ACL?09, 
Singapore. 
 
1001
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1027?1037, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Ensemble Semantics for Large-scale Unsupervised Relation Extraction 
 
 
Bonan Min1* Shuming Shi2 Ralph Grishman1 Chin-Yew Lin2 
  
1New York University 2Microsoft Research Asia 
New York, NY, USA Beijing, China 
{min,grishman}@cs.nyu.edu {shumings,cyl}@microsoft.com 
  
Abstract 
Discovering significant types of relations 
from the web is challenging because of its 
open nature. Unsupervised algorithms are 
developed to extract relations from a cor-
pus without knowing the relations in ad-
vance, but most of them rely on tagging 
arguments of predefined types. Recently, 
a new algorithm was proposed to jointly 
extract relations and their argument se-
mantic classes, taking a set of relation in-
stances extracted by an open IE algorithm 
as input. However, it cannot handle poly-
semy of relation phrases and fails to 
group many similar (?synonymous?) rela-
tion instances because of the sparseness of 
features. In this paper, we present a novel 
unsupervised algorithm that provides a 
more general treatment of the polysemy 
and synonymy problems. The algorithm 
incorporates various knowledge sources 
which we will show to be very effective 
for unsupervised extraction. Moreover, it 
explicitly disambiguates polysemous rela-
tion phrases and groups synonymous 
ones. While maintaining approximately 
the same precision, the algorithm achieves 
significant improvement on recall com-
pared to the previous method. It is also 
very efficient. Experiments on a real-
world dataset show that it can handle 14.7 
million relation instances and extract a 
very large set of relations from the web.  
1 Introduction 
Relation extraction aims at discovering semantic 
relations between entities. It is an important task 
that has many applications in answering factoid 
questions, building knowledge bases and improv-
ing search engine relevance. The web has become 
a massive potential source of such relations. How-
ever, its open nature brings an open-ended set of 
relation types. To extract these relations, a system 
should not assume a fixed set of relation types, nor 
rely on a fixed set of relation argument types.  
The past decade has seen some promising solu-
tions, unsupervised relation extraction (URE) algo-
rithms that extract relations from a corpus without 
knowing the relations in advance. However, most 
algorithms (Hasegawa et al 2004, Shinyama and 
Sekine, 2006, Chen et. al, 2005) rely on tagging 
predefined types of entities as relation arguments, 
and thus are not well-suited for the open domain.  
Recently, Kok and Domingos (2008) proposed 
Semantic Network Extractor (SNE), which gener-
ates argument semantic classes and sets of synon-
ymous relation phrases at the same time, thus 
avoiding the requirement of tagging relation argu-
ments of predefined types. However, SNE has 2 
limitations: 1) Following previous URE algo-
rithms, it only uses features from the set of input 
relation instances for clustering.  Empirically we 
found that it fails to group many relevant relation 
instances. These features, such as the surface forms 
of arguments and lexical sequences in between, are 
very sparse in practice. In contrast, there exist sev-
eral well-known corpus-level semantic resources 
that can be automatically derived from a source 
corpus and are shown to be useful for generating 
the key elements of a relation: its 2 argument se-
mantic classes and a set of synonymous phrases. 
For example, semantic classes can be derived from 
a source corpus with contextual distributional simi-
larity and web table co-occurrences. The ?synony-
my? 1  problem for clustering relation instances 
                                                          
* Work done during an internship at Microsoft Research Asia 
1027
could potentially be better solved by adding these 
resources. 2) SNE assumes that each entity or rela-
tion phrase belongs to exactly one cluster, thus is 
not able to effectively handle polysemy of relation 
phrases2. An example of a polysemous phrase is be 
the currency of  as in 2 triples <Euro, be the cur-
rency of, Germany> and <authorship, be the cur-
rency of, science>. As the target corpus expands 
from mostly news to the open web, polysemy be-
comes more important as input covers a wider 
range of domains. In practice, around 22% (section 
3) of relation phrases are polysemous. Failure to 
handle these cases significantly limits its effective-
ness. 
    To move towards a more general treatment of 
the polysemy and synonymy problems, we present a 
novel algorithm WEBRE for open-domain large-
scale unsupervised relation extraction without pre-
defined relation or argument types. The contribu-
tions are: 
? WEBRE incorporates a wide range of cor-
pus-level semantic resources for improving rela-
tion extraction. The effectiveness of each 
knowledge source and their combination are stud-
ied and compared. To the best of our knowledge, it 
is the first to combine and compare them for unsu-
pervised relation extraction. 
? WEBRE explicitly disambiguates polyse-
mous relation phrases and groups synonymous 
phrases, thus fundamentally it avoids the limitation 
of previous methods. 
? Experiments on the Clueweb09 dataset 
(lemurproject.org/clueweb09.php) show that 
WEBRE is effective and efficient. We present a 
large-scale evaluation and show that WEBRE can 
extract a very large set of high-quality relations. 
Compared to the closest prior work, WEBRE sig-
nificantly improves recall while maintaining the 
same level of precision. WEBRE is efficient. To 
the best of our knowledge, it handles the largest 
triple set to date (7-fold larger than largest previous 
effort). Taking 14.7 million triples as input, a com-
plete run with one CPU core takes about a day.  
 
 
 
                                                                                           
1 We use the term synonymy broadly as defined in Section 3. 
2 A cluster of relation phrases can, however, act as a whole as 
the phrase cluster for 2 different relations in SNE. However, 
this only accounts for 4.8% of the polysemous cases. 
2 Related Work 
Unsupervised relation extraction (URE) algorithms 
(Hasegawa et al 2004; Chen et al 2005; Shinya-
ma and Sekine, 2006) collect pairs of co-occurring 
entities as relation instances, extract features for 
instances and then apply unsupervised clustering 
techniques to find the major relations of a corpus. 
These UREs rely on tagging a predefined set of 
argument types, such as Person, Organization, and 
Location, in advance. Yao et al2011 learns fine-
grained argument classes with generative models, 
but they share the similar requirement of tagging 
coarse-grained argument types. Most UREs use a 
quadratic clustering algorithm such as Hierarchical 
Agglomerate Clustering (Hasegawa et al 2004, 
Shinyama and Sekine, 2006), K-Means (Chen et 
al., 2005), or both (Rosenfeld and Feldman, 2007); 
thus they are not scalable to very large corpora.  
As the target domain shifts to the web, new 
methods are proposed without requiring predefined 
entity types. Resolver (Yates and Etzioni, 2007) 
resolves objects and relation synonyms. Kok and 
Domingos (2008) proposed Semantic Network Ex-
tractor (SNE) to extract concepts and relations. 
Based on second-order Markov logic, SNE used a 
bottom-up agglomerative clustering algorithm to 
jointly cluster relation phrases and argument enti-
ties. However, both Resolver and SNE require 
each entity and relation phrase to belong to exactly 
one cluster. This limits their ability to handle poly-
semous relation phrases. Moreover, SNE only uses 
features in the input set of relation instances for 
clustering, thus it fails to group many relevant in-
stances. Resolver has the same sparseness problem 
but it is not affected as much as SNE because of its 
different goal (synonym resolution).  
As the preprocessing instance-detection step for 
the problem studied in this paper, open IE extracts 
relation instances (in the form of triples) from the 
open domain (Etzioni et al 2004; Banko et al 
2007; Fader et al 2011; Wang et al2011). For 
efficiency, they only use shallow features. Reverb 
(Fader et al 2011) is a state-of-the-art open do-
main extractor that targets verb-centric relations, 
which have been shown in Banko and Etzioni 
(2008) to cover over 70% of open domain rela-
tions. Taking their output as input, algorithms have 
been proposed to resolve objects and relation syn-
onyms (Resolver),  extract semantic networks 
1028
(SNE), and map extracted relations into an existing 
ontology (Soderland and Mandhani, 2007).  
Recent work shows that it is possible to con-
struct semantic classes and sets of similar phrases 
automatically with data-driven approaches. For 
generating semantic classes, previous work applies 
distributional similarity (Pasca, 2007; Pantel et al 
2009), uses a few linguistic patterns (Pasca 2004; 
Sarmento et al 2007), makes use of structure in 
webpages (Wang and Cohen 2007, 2009), or com-
bines all of them (Shi et al 2010). Pennacchiotti 
and Pantel (2009) combines several sources and 
features. To find similar phrases, there are 2 close-
ly related tasks: paraphrase discovery and recog-
nizing textual entailment. Data-driven paraphrase 
discovery methods (Lin and Pantel, 2001; Pasca 
and Dienes, 2005; Wu and Zhou, 2003; Sekine, 
2005) extends the idea of distributional similarity 
to phrases. The Recognizing Textual Entailment 
algorithms (Berant et al2011) can also be used to 
find related phrases since they find pairs of phrases 
in which one entails the other.  
To efficiently cluster high-dimensional datasets, 
canopy clustering (McCallum et al 2000) uses a 
cheap, approximate distance measure to divide da-
ta into smaller subsets, and then cluster each subset 
using an exact distance measure. It has been ap-
plied to reference matching. The second phase of 
WEBRE applies the similar high-level idea of par-
tition-then-cluster for speeding up relation cluster-
ing. We design a graph-based partitioning 
subroutine that uses various types of evidence, 
such as shared hypernyms.  
3 Problem Analysis 
The basic input is a collection of relation instances 
(triples) of the form <ent1, ctx, ent2>. For each tri-
ple, ctx is a relational phrase expressing the rela-
tion between the first argument ent1 and the second 
argument ent2. An example triple is <Obama, win 
in, NY>. The triples can be generated by an open 
IE extractor such as TextRunner or Reverb. Our 
goal is to automatically build a list of relations 
? = {< ent1, ???, ent2 >} ? 3 < ?1,?,?2 >  where P 
is the set of relation phrases, and ?1  and  ?2  are 
two argument classes. Examples of triples and rela-
tions R (as Type B) are shown in Figure 1. 
                                                          
3 This approximately equal sign connects 2 possible represen-
tations of a relation: as a set of triple instances or a triple with 
2 entity classes and a relation phrase class. 
The first problem is the polysemy of relation 
phrases, which means that a relation phrase ctx can 
express different relations in different triples. For 
example, the meaning of be the currency of in the 
following two triples is quite different: <Euro, be 
the currency of, Germany> and <authorship, be 
the currency of, science>. It is more appropriate to 
assign these 2 triples to 2 relations ?a currency is 
the currency of a country? and ?a factor is im-
portant in an area? than to merge them into one. 
Formally, a relation phrase ctx is polysemous if 
there exist 2 different relations < ?1,?,?2 >  and 
< ?1
?
,??,?2
?
> where ??? ? ? ? ??. In the previ-
ous example, be the currency of  is polysemous 
because it appears in 2 different relations.  
Polysemy of relation phrases is not uncommon. 
We generate clusters from a large sample of triples 
with the assistance of a soft clustering algorithm, 
and found that around 22% of relation phrases can 
be put into at least 2 disjoint clusters that represent 
different relations. More importantly, manual in-
spection reveals that some common phrases are 
polysemous. For example, be part of can be put 
into a relation ?a city is located in a country? when 
connecting Cities to Countries, and another rela-
tion ?a company is a subsidiary of a parent com-
pany? when connecting Companies to Companies. 
Failure to handle polysemous relation phrases fun-
damentally limits the effectiveness of an algorithm. 
The WEBRE algorithm described later explicitly 
handles polysemy and synonymy of relation 
phrases in its first and second phase respectively. 
The second problem is the ?synonymy? of rela-
tion instances. We use the term synonymy broadly 
and we say 2 relation instances are synonymous if 
they express the same semantic relation between 
the same pair of semantic classes. For example, 
both <Euro, be the currency used in, Germany> 
and <Dinar, be legal tender in, Iraq> express the 
relation <Currencies, be currency of, Countries>. 
Solving this problem requires grouping synony-
mous relation phrases and identifying argument 
semantic classes for the relation.  
Various knowledge sources can be derived from 
the source corpus for this purpose. In this paper we 
pay special attention to incorporating various se-
mantic resources for relation extraction. We will 
show that these semantic sources can significantly 
improve the coverage of extracted relations and the  
 
1029
Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec-
tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys-
tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the 
green arrows show the resources used in phase 2. 
 
best performance is achieved when various re-
sources are combined together.  
4 Mining Relations from the Web 
We first describe relevant knowledge sources, and 
then introduce the WEBRE algorithm, followed by 
a briefly analysis on its computational complexity.  
4.1 Knowledge Sources 
Entity similarity graph We build two similarity 
graphs for entities: a distributional similarity (DS) 
graph and a pattern-similarity (PS) graph. The DS 
graph is based on the distributional hypothesis 
(Harris, 1985), saying that terms sharing similar 
contexts tend to be similar. We use a text window 
of size 4 as the context of a term, use Pointwise 
Mutual Information (PMI) to weight context fea-
tures, and use Jaccard similarity to measure the 
similarity of term vectors. The PS graph is gener-
ated by adopting both sentence lexical patterns and 
HTML tag patterns (Hearst, 1992; Kozareva et al 
2008; Zhang et al 2009; Shi et al 2010). Two 
terms (T) tend to be semantically similar if they co-
occur in multiple patterns. One example of sen-
tence lexical patterns is (such as | including) 
T{,T}* (and|,|.). HTML tag patterns include tables, 
dropdown boxes, etc. In these two graphs, nodes 
are entities and the edge weights indicate entity 
similarity. In all there are about 29.6 million nodes 
and 1.16 billion edges. 
Hypernymy graph Hypernymy relations are 
very useful for finding semantically similar term 
pairs. For example, we observed that a small city 
in UK and another small city in Germany share 
common hypernyms such as city, location, and 
place. Therefore the similarity between the two 
cities is large according to the hypernymy graph, 
while their similarity in the DS graph and the PS 
graph may be very small. Following existing work 
(Hearst, 1992, Pantel & Ravichandran 2004; Snow 
et al 2005; Talukdar et al 2008; Zhang et al 
2011), we adopt a list of lexical patterns to extract 
hypernyms. The patterns include NP {,} (such as) 
{NP,}* {and|or} NP, NP (is|are|was|were|being) 
(a|an|the) NP, etc. The hypernymy graph is a bi-
partite graph with two types of nodes: entity nodes 
and label (hypernym) nodes. There is an edge (T, 
L) with weight w if L is a hypernym of entity T 
with probability w. There are about 8.2 million 
nodes and 42.4 million edges in the hypernymy 
graph. In this paper, we use the terms hypernym 
and label interchangeably. 
Relation phrase similarity: To generate the pair-
wise similarity graph for relation phrases with re-
gard to the probability of expressing the same 
relation, we apply a variant of the DIRT algorithm 
(Lin and Pantel, 2001). Like DIRT, the paraphrase 
discovery relies on the distributional hypothesis, 
but there are a few differences: 1) we use stemmed 
lexical sequences (relation phrases) instead of de-
pendency paths as phrase candidates because of the 
very large scale of the corpus. 2) We used ordered 
1030
pairs of arguments as features of phrases while 
DIRT uses them as independent features. We em-
pirically tested both feature schemes and found 
that using ordered pairs results in likely para-
phrases but using independent features the result 
contains general inference rules4. 
4.2 WEBRE for Relation Extraction 
WEBRE consists of two phases. In the first 
phase, a set of semantic classes are discovered and 
used as argument classes for each relation phrase. 
This results in a large collection of relations whose 
arguments are pairs of semantic classes and which 
have exactly one relation phrase. We call these 
relations the Type A relations. An example Type A 
relation is <{New York, London?}, be locate in, 
{USA, England, ?}>. During this phase, polyse-
mous relation phrases are disambiguated and 
placed into multiple Type A relations. The second 
phase is an efficient algorithm which groups simi-
lar Type A relations together. This step enriches 
the argument semantic classes and groups synon-
ymous relation phrases to form relations with mul-
tiple expressions, which we called Type B 
relations. Both Type A and Type B relations are 
system outputs since both are valuable resources 
for downstream applications such as QA and Web 
Search. An overview of the algorithm is shown in 
Figure 1. Here we first briefly describe a clustering 
subroutine that is used in both phases, and then 
describe the algorithm in detail. 
To handle polysemy of objects (e.g., entities or 
relations) during the clustering procedure, a key 
building block is an effective Multi-Membership 
Clustering algorithm (MMClustering). For simplic-
ity and effectiveness, we use a variant of Hierar-
chical Agglomerative Clustering (HAC), in which 
we first cluster objects with HAC, and then reas-
sign each object to additional clusters when its 
similarities with these clusters exceed a certain 
threshold5. In the remainder of this paper, we use 
{C} = MMClustering({object}, SimFunc, ?) to rep-
resent running MMClustering over a set of objects, 
                                                          
4 For example, be part of  has ordered argument pairs <A, B> 
and <C, D>, and be not part of has ordered argument pairs 
<A, D> and <B, C>. If arguments are used as independent 
features, these two phrases shared the same set of features {A, 
B, C, D}. However, they are inferential (complement relation-
ship) rather than being similar phrases. 
5 This threshold should be slightly greater than the clustering 
threshold for HAC to avoid generating duplicated clusters. 
with threshold ? to generate a list of clusters {C} of 
the objects, given the pairwise object similarity 
function SimFunc. Our implementation uses HAC 
with average linkage since empirically it performs 
well. 
Discovering Type A Relations The first phase 
of the relation extraction algorithm generates Type 
A relations, which have exactly one relation phrase 
and two argument entity semantic classes. For each 
relation phrase, we apply a clustering algorithm on 
each of its two argument sets to generate argument 
semantic classes. The Phase 1 algorithm processes 
relation phrases one by one. For each relation 
phrase ctx, step 4 clusters the set {ent1} using 
MMClustering to find left-hand-side argument se-
mantic classes {C1}. Then for each cluster C in 
{C1}, it gathers the right-hand-side arguments 
which appeared in some triple whose left hand-
side-side argument is in C, and puts them into 
{ent2?}. Following this, it clusters {ent2?} to find 
right-hand-side argument semantic classes. This 
results in pairs of semantic classes which are ar-
guments of ctx. Each relation phrase can appear in 
multiple non-overlapping Type A relations. For 
example, <Cities, be part of, Countries> and 
<Companies, be part of, Companies> are different 
Type A relations which share the same relation 
phrase be part of. In the pseudo code, SimEntFunc 
is encoded in the entity similarity graphs.  
 
Algorithm Phase 1: Discovering Type A relations 
Input:  set of triples T={<ent1, ctx, ent2>} 
 entity similarity function SimEntFunc 
 Similarity threshold ? 
Output:  list of Type A relations {<C1, ctx, C2>} 
Steps:  
01. For each relation phrase ctx 
02.     {ent1, ctx, ent2} = set of triples sharing ctx 
03.     {ent1} = set of ent1 in {ent1, ctx, ent2} 
04.     {C1} = MMClustering({ent1}, SimEntFunc, ?) 
05.     For each C in { C1} 
06.         {ent2?} = set of ???2 ?. ?.?< ???1, ???, ???2 > ?
 ? ? ???1 ? ?1 
07.         {C2} = MMClustering({ent2?}, SimEntFunc, ?) 
08.         For each C2 in {C2} 
09.             Add <C1, ctx, C2> into {<C1, ctx, C2>} 
10. Return {<C1, ctx, C2>} 
 
    Discovering Type B Relations  The goal of 
phase 2 is to merge similar Type A relations, such 
as <Cities, be locate in, Countries> and <Cities, 
be city of, Countries>, to produce Type B relations, 
which have a set of synonymous relation phrases 
and more complete argument entity classes. The 
challenge for this phase is to cluster a very large 
1031
set of Type A relations, on which it is infeasible to 
run a clustering algorithm that does pairwise all 
pair comparison. Therefore, we designed an evi-
dence-based partition-then-cluster algorithm. 
The basic idea is to heuristically partition the 
large set of Type A relations into small subsets, 
and run clustering algorithms on each subset. It is 
based on the observation that most pairs of Type A 
relations are not similar because of the sparseness 
in the entity class and the relation semantic space. 
If there is little or no evidence showing that two 
Type A relations are similar, they can be put into 
different partitions. Once partitioned, the clustering 
algorithm only has to be run on each much smaller 
subset, thus computation complexity is reduced.  
The 2 types of evidence we used are shared 
members and shared hypernyms of relation argu-
ments. For example, 2 Type A relations 
r1=<Cities, be city of, Countries> and r2=<Cities, 
be locate in, Countries> share a pair of arguments 
<Tokyo, Japan>, and a pair of hypernyms <?city?, 
?country?>. These pieces of evidence give us hints 
that they are likely to be similar. As shown in the 
pseudo code, shared arguments and hypernyms are 
used as independent evidence to reduce sparseness. 
 
Algorithm Phase 2: Discovering Type B relations 
Input:  Set of Type A relations {r}={<C1, ctx, C2>} 
 Relation similarity function SimRelationFunc 
 Map from entities to their hypernyms: Mentity2label 
 Similarity threshold ? 
Edge weight threshold ? 
Variables G(V, E) = weighted graph in which V={r} 
Output:  Set of Type B relations {<C1, P, C2>} 
Steps:  
01. {<ent, {r?}>} = build  inverted index from argument 
ent to set of Type A relations {r?} on {<C1, ctx, C2>}  
02 {<l, {r?}>} = build  inverted index from hypernym l 
of arguments to set of Type A relations {r?} on {<C1, 
ctx, C2>} with map Mentity2label  
03. For each ent in {<ent, {r?}>} 
04.     For each pair of r1 and r2  s.t. ?1 ? {?
?} ? ?2 ? {??}    
05.        weight_edge(<r1, r2>) += weight (ent) 
06. For each l in {<l, {r?}>} 
07.     For each pair of r1 and r2  s.t. ?1 ? {?
?} ? ?2 ? {??}    
08.        weight_edge(<r1, r2>) += weight (l) 
09. For each edge <r1, r2> in G 
10.     If weight_edge(<r1, r2>) < ? 
11.         Remove edge <r1, r2> from G 
12. {CC}= DFS(G) 
13. For each connected component CC in {CC} 
14.     {<C1, ctx, C2>} = vertices in CC 
15. {<C1?, P?, C2?>} = MMClustering({<C1, ctx, C2>},  
  SimRelationFunc, ?) 
16.     Add {<C1?, P?, C2?>} into {<C1, P, C2>} 
17. Return {<C1, P, C2>} 
 
Steps 1 and 2 build an inverted index from evi-
dence to sets of Type A relations. On the graph G 
whose vertices are Type A relations, steps 3 to 8 
set the value of edge weights based on the strength 
of evidence that shows the end-points are related. 
The weight of evidence E is calculated as follows: 
 
??????(?) =
# ?????? ?????? ?? ????? ? ??????? ?? 
max(# ??????? ? ??????? ??)
 
 
The idea behind this weighting scheme is similar 
to that of TF-IDF in that the weight of evidence is 
higher if it appears more frequently and is less am-
biguous (appeared in fewer semantic classes during 
clustering of phase 1). The weighting scheme is 
applied to both shared arguments and labels. 
After collecting evidence, we prune (steps 9 to 
11) the edges with a weight less than a threshold ? 
to remove noise. Then a Depth-First Search (DFS) 
is called on G to find all Connected Components 
CC of the graph. These CCs are the partitions of 
likely-similar Type A relations. We run MMClus-
tering on each CC in {CC} and generate Type B  
relations (step 13 to step 16).  The similarity of 2 
relations (SimRelationFunc) is defined as follows: 
???(< ?1,?,?2 >, < ?1
?,??,?2
? >) 
 
= ?
0,     ?? ???(?,??) <  ?
min????(?1,?1
?), ???(?2,?2
?)? ,   ???? 
  
4.3 Computational Complexity 
WEBRE is very efficient since both phases de-
compose the large-clustering task into much small-
er clustering tasks over partitions. Given n objects 
for clustering, a hierarchical agglomerative cluster-
ing algorithm requires ?(?2)  pairwise compari-
sons. Assuming the clustering task is split into 
subtasks of size ?1, ?2, ?, ??, thus the computa-
tional complexity is reduced to ?(? ??
2?
1 ). Ideally 
each subtask has an equal size of ?/?, so the com-
putational complexity is reduced to O(?2/?) , a 
factor of ? speed up. In practice, the sizes of parti-
tions are not equal. Taking the partition sizes ob-
served in the experiment with 0.2 million Type A 
relations as input, the phase 2 algorithm achieves 
around a 100-fold reduction in pairwise compari-
sons compared to the agglomerative clustering al-
gorithm. The combination of phase 1 and phase 2 
achieves more than a 1000-fold reduction in pair-
wise comparison, compared to running an agglom-
erative clustering algorithm directly on 14.7 
million triples. This reduction of computational 
1032
complexity makes the unsupervised extraction of 
relations on a large dataset a reality. In the experi-
ments with 14.7 million triples as input, phase 1 
finished in 22 hours, and the phase 2 algorithm 
finished in 4 hours with one CPU core. 
Furthermore, both phases can be run in parallel 
in a distributed computing environment because 
data is partitioned. Therefore it is scalable and effi-
cient for clustering a very large number of relation 
instances from a large-scale corpus like the web.  
5 Experiment 
Data preparation We tested WEBRE on re-
sources extracted from the English subset of the 
Clueweb09 Dataset, which contains 503 million 
webpages. For building knowledge resources, all 
webpages are cleaned and then POS tagged and 
chunked with in-house tools. We implemented the 
algorithms described in section 4.1 to generate the 
knowledge sources, including a hypernym graph, 
two entity similarity graphs and a relation phrase 
similarity graph. 
We used Reverb Clueweb09 Extractions 1.1 
(downloaded from reverb.cs.washington.edu) as 
the triple store (relation instances). It is the com-
plete extraction of Reverb over Clueweb09 after 
filtering low confidence and low frequency triples. 
It contains 14.7 million distinct triples with 3.3 
million entities and 1.3 million relation phrases. 
We choose it because 1) it is extracted by a state-
of-the-art open IE extractor from the open-domain, 
and 2) to the best of our knowledge, it contains the 
largest number of distinct triples extracted from the 
open-domain and which is publicly available. 
 
Evaluation setup The evaluations are organized as 
follows: we evaluate Type A relation extraction 
and Type B relation extraction separately, and then 
we compare WEBRE to its closest prior work 
SNE.  Since both phases are essentially clustering 
algorithms, we compare the output clusters with 
human labeled gold standards and report perfor-
mance measures, following most previous work 
such as Kok and Domingos (2008) and Hasegawa 
et al(2004). Three gold standards are created for 
evaluating Type A relations, Type B relations and 
the comparison to SNE, respectively. In the exper-
iments, we set ?=0.6, ?=0.1 and ?=0.02 based on 
trial runs on a small development set of 10k rela-
tion instances. We filtered out the Type A relations 
and Type B relations which only contain 1 or 2 
triples since most of these relations are not differ-
ent from a single relation instance and are not very 
interesting. Overall, 0.2 million Type A relations 
and 84,000 Type B relations are extracted. 
 
Evaluating Type A relations To understand the 
effectiveness of knowledge sources, we run Phase 
1 multiple times taking entity similarity graphs 
(matrices) constructed with resources listed below: 
? TS: Distributional similarity based on the triple 
store. For each triple <ent1, ctx, ent2>, features 
of ent1 are {ctx} and {ctx ent2}; features of ent2 
are {ctx} and {ent1 ctx}. Features are weighted 
with PMI. Cosine is used as similarity measure.  
? LABEL: The similarity between two entities is 
computed according to the percentage of top 
hypernyms they share. 
? SIM: The similarity between two entities is the 
linear combination of their similarity scores in 
the distributional similarity graph and in the 
pattern similarity graph. 
? SIM+LABEL SIM and LABEL are combined. 
Observing that SIM generates high quality but 
overly fine-grained semantic classes, we modify 
the entity clustering procedure to cluster argu-
ment entities based on SIM first, and then fur-
ther clustering the results based on LABEL. 
The outputs of these runs are pooled and mixed 
for labeling. We randomly sampled 60 relation 
phrases. For each phrase, we select the 5 most fre-
quent Type A relations from each run (4?5=206 
Type A relations in all). For each relation phrase, 
we ask a human labeler to label the mixed pool of 
Type A relations that share the phrase: 1) The la-
belers7 are asked to first determine the major se-
mantic relation of each Type A relation, and then 
label the triples as good, fair or bad based on 
whether they express the major relation. 2) The 
labeler also reads all Type A relations and manual-
ly merges the ones that express the same relation. 
These 2 steps are repeated for each phrase. After 
labeling, we create a gold standard GS1, which 
contains roughly 10,000 triples for 60 relation 
phrases. On average, close to 200 triples are manu-
                                                          
6  Here 4 means the 4 methods (the bullet items above) of 
computing similarity. 
7 4 human labelers perform the task. A portion of the judg-
ments were independently dual annotated; inter-annotator 
agreement is 79%. Moreover, each judgment is cross-checked 
by at least one more annotator, further improving quality. 
1033
ally labeled and clustered for each phrase. This 
creates a large data set for evaluation.  
We report micro-average of precision, recall and 
F1 on the 60 relation phrases for each method. Pre-
cision (P) and Recall (R) of a given relation phrase 
is defined as follows. Here ?? and ??
?  represents a 
Type A relation in the algorithm output and GS1, 
respectively. We use t for triples and s(t) to repre-
sent the score of the labeled triple t. s(t) is set to 
1.0, 0.5 or 0 for t labeled as good, fair and bad, 
respectively. 
 
? =
? ? ?(?) ????  ??
? |??|??
, ? =
? ? ?(?) ????  ??
? ? ?(??) ?????
???
?
 
 
The results are in table 1. Overall, LABEL per-
forms 53% better than TS in F-measure, and 
SIM+LABEL performs the best, 8% better than 
LABEL. Applying a simple sign test shows both 
differences are clearly significant (p<0.001). Sur-
prisingly, SIM, which uses the similarity matrix 
extracted from full text, has a F1 of 0.277, which is 
lower than TS. We also tried combining TS and 
LABEL but did not find encouraging performance 
compared to SIM+LABEL. 
 
Algorithm Precision Recall F1 
TS 0.842 (0.886) 0.266 0.388 
LABEL 0.855 (0.870) 0.481 0.596 
SIM 0.755 (0.964) 0.178 0.277 
SIM+LABEL 0.843 (0.872) 0.540 0.643 
 
Table 1. Phase 1 performance (averaged on multiple runs) of 
the 4 methods. The highest performance numbers are in bold. 
(The number in parenthesis is the micro-average when empty-
result relation phrases are not considered for the method). 
 
Among the 4 methods, SIM has the highest preci-
sion (0.964) when relation phrases for which it 
fails to generate any Type A relations are exclud-
ed, but its recall is low. Manual checking shows 
that SIM tends to generate overly fine-grained ar-
gument classes. If fine-grained argument classes or 
extremely high-precision Type A relations are pre-
ferred, SIM is a good choice. LABEL performs 
significantly better than TS, which shows that hy-
pernymy information is very useful for finding ar-
gument semantic classes. However, it has coverage 
problems in that the hypernym finding algorithm 
failed to find any hypernym from the corpus for 
some entities. Following up, we found that 
SIM+LABEL has similar precision and the highest 
recall. This shows that the combination of semantic 
spaces is very helpful. The significant recall im-
provement from TS to SIM+LABEL shows that 
the corpus-based knowledge resources significant-
ly reduce the data sparseness, compared to using 
features extracted from the triple store only. The 
result of the phase 1 algorithm with SIM+LABEL 
is used as input for phase 2. 
 
Evaluating Type B relations The goal is 2-fold: 
1) to evaluate the phase 2 algorithm. This involves 
comparing system output to a gold standard con-
structed by hand, and reporting performance; 2) to 
evaluate the quality of Type B relations. For this, 
we will also report triple-level precision. 
    We construct a gold standard GS28 for evaluat-
ing Type B relations as follows: We randomly 
sampled 178 Type B relations, which contain 1547 
Type A relations and more than 100,000 triples. 
Since the number of triples is very large, it is in-
feasible for labelers to manually cluster triples to 
construct a gold standard. To report precision, we 
asked the labelers to label each Type A relation 
contained in this Type B relation as good, fair or 
bad based on whether it expresses the same rela-
tion. For recall evaluation, we need to know how 
many Type A relations are missing from each Type 
B relation. We provide the full data set of Type A 
relations along with three additional resources: 1) a 
tool which, given a Type A relation, returns a 
ranked list of similar Type A relations based on the 
pairwise relation similarity metric in section 4, 2) 
DIRT paraphrase collection, 3) WordNet (Fell-
baum, 1998) synsets. The labelers are asked to find 
similar phrases by checking phrases which contain 
synonyms of the tokens in the query phrase. Given 
a Type B relation, ideally we expect the labelers to 
find all missing Type A relations using these re-
sources. We report precision (P) and recall (R) as 
follows. Here ??  and ??
?  represent Type B rela-
tions in the algorithm output and GS2, respective-
ly. ??  and ??
?  represent Type A relations. ?(??) 
denotes the score of ??. It is set to 1.0, 0.5 and 0 
for good, fair or bad respectively.  
 
? =
? ? |??|??(??) ???????
? ? |??|  ???????
, ? =
? ? |??|??(??)  ???????
? ? ???
? ???
? ???
???
?
 
 
We also ask the labeler to label at most 50 ran-
domly sampled triples from each Type B relation, 
and calculate triple-level precision as the ratio of 
the sum of scores of triples over the number of  
                                                          
8 3 human labelers performed the task. A portion of the judg-
ments were independently dual annotated; inter-annotator 
agreement is 73%. Similar to labeling Type A relations, each 
judgment is cross-checked by at least one more annotator, 
further improving quality. 
1034
Argument 1 Relation phrase Argument 2 
marijuana, caffeine, nicotine? result in, be risk factor for, be major cause of? insomnia, emphysema, breast cancer,? 
C# 2.0, php5, java, c++, ? allow the use of, also use, introduce the concept of? destructors, interfaces, template,? 
clinton, obama, mccain, ? win, win in, take, be lead in,? ca, dc, fl, nh, pa, va, ga, il, nc,? 
Table 3. Sample Type B relations extracted. 
 
sampled triples. We use ???? to represent the preci-
sion calculated based on labeled triples. Moreover, 
as we are interested in how many phrases are 
found by our algorithm, we also include ???????, 
which is the recall of synonymous phrases. Results 
are shown in Table 2.  
 
Interval P R (???????) F1 ???? count 
[3, 5) 0.913 0.426 (0.026) 0.581 0.872 52149 
[5, 10) 0.834 0.514 (0.074) 0.636 0.863 21981 
[10, 20) 0.854 0.569 (0.066) 0.683 0.883 6277 
[20, 50) 0.899 0.675 (0.406) 0.771 0.894 2630 
[50, +?) 0.922 0.825 (0.594) 0.871 0.929 1089 
Overall 0.897 0.684 (0.324) 0.776 0.898 84126 
Table 2. Performance for Type B relation extraction. The first 
column shows the range of the maximum sizes of Type A 
relations in the Type B relation. The last column shows the 
number of Type B relations that are in this range. The number 
in parenthesis in the third column is the recall of phrases.  
 
The result shows that WEBRE can extract Type B 
relations at high precision (both P and ????). The 
overall recall is 0.684. Table 2 also shows a trend 
that if the maximum number of Type A relation in 
the target Type B relation is larger, the recall is 
better. This shows that the recall of Type B rela-
tions depends on the amount of data available for 
that relation. Some examples of Type B relations 
extracted are shown in Table 3. 
  
Comparison with SNE We compare WEBRE?s 
extracted Type B relations to the relations extract-
ed by its closest prior work SNE9. We found SNE 
is not able to handle the 14.7 million triples in a 
foreseeable amount of time, so we randomly sam-
pled 1 million (1M) triples 10 and test both algo-
rithms on this set. We also filtered out result 
clusters which have only 1 or 2 triples from both 
system outputs. For comparison purposes, we con-
structed a gold standard GS3 as follows: randomly 
select 30 clusters from both system outputs, and 
then find similar clusters from the other system 
output, followed by manually refining the clusters 
                                                          
9 Obtained from alchemy.cs.washington.edu/papers/kok08 
10 We found that SNE?s runtime on 1M triples varies from 
several hours to over a week, depending on the parameters. 
The best performance is achieved with runtime of approxi-
mately 3 days. We also tried SNE with 2M triples, on which 
many runs take several days and show no sign of convergence. 
For fairness, the comparison was done on 1M triples. 
by merging similar ones and splitting non-coherent 
clusters. GS3 contains 742 triples and 135 clusters. 
We report triple-level pairwise precision, recall 
and F1 for both algorithms against GS3, and report 
results in Table 4. We fine-tuned SNE (using grid 
search, internal cross-validation, and coarse-to-fine 
parameter tuning), and report its best performance. 
 
Algorithm Precision Recall F1 
WEBRE 0.848 0.734 0.787 
SNE 0.850 0.080 0.146 
 
Table 4. Pairwise precision/recall/F1 of WEBRE and SNE.  
 
Table 4 shows that WEBRE outperforms SNE 
significantly in pairwise recall while having similar 
precision. There are two reasons. First, WEBRE 
makes use of several corpus-level semantic sources 
extracted from the corpus for clustering entities 
and phrases while SNE uses only features in the 
triple store. These semantic resources significantly 
reduced data sparseness. Examination of the output 
shows that SNE is unable to group many triples 
from the same generally-recognized fine-grained 
relations. For example, SNE placed relation in-
stances <Barbara, grow up in, Santa Fe> and 
<John, be raised mostly in, Santa Barbara> into 2 
different clusters because the arguments and 
phrases do not share features nor could be grouped 
by SNE?s mutual clustering. In contrast, WEBRE 
groups them together. Second, SNE assumes a re-
lation phrase to be in exactly one cluster. For ex-
ample, SNE placed be part of in the phrase cluster 
be city of and failed to place it in another cluster be 
subsidiary of. This limits SNE?s ability to placing 
relation instances with polysemous phrases into 
correct relation clusters. 
It should be emphasized that we use pairwise 
precision and recall in table 4 to be consistent with 
the original SNE paper. Pairwise metrics are much 
more sensitive than instance-level metrics, and pe-
nalize recall exponentially in the worst case11 if an 
algorithm incorrectly splits a coherent cluster; 
therefore the absolute pairwise recall difference 
                                                          
11 Pairwise precision and recall are calculated on all pairs that 
are in the same cluster, thus are very sensitive. For example, if 
an algorithm incorrectly split a cluster of size N to a smaller 
main cluster of size N/2 and some constant-size clusters, pair-
wise recall could drop to as much as ? of its original value. 
1035
should not be interpreted as the same as the in-
stance-level recall reported in previous experi-
ments. On 1 million triples, WEBRE generates 
12179 triple clusters with an average size12 of 13 
while SNE generate 53270 clusters with an aver-
age size 5.1. In consequence, pairwise recall drops 
significantly. Nonetheless, at above 80% pairwise 
precision, it demonstrates that WEBRE can group 
more related triples by adding rich semantics har-
vested from the web and employing a more general 
treatment of polysemous relation phrases.  On 1M 
triples, WEBRE finished in 40 minutes, while the 
run time of SNE varies from 3 hours to a few days. 
6 Conclusion 
We present a fully unsupervised algorithm 
WEBRE for large-scale open-domain relation ex-
traction. WEBRE explicitly handles polysemy rela-
tions and achieves a significant improvement on 
recall by incorporating rich corpus-based semantic 
resources. Experiments on a large data set show 
that it can extract a very large set of high-quality 
relations. 
 
Acknowledgements 
Supported in part by the Intelligence Advanced 
Research Projects Activity (IARPA) via Air Force 
Research Laboratory (AFRL) contract number 
FA8650-10-C-7058. The U.S. Government is au-
thorized to reproduce and distribute reprints for 
Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclu-
sions contained herein are those of the authors and 
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either 
expressed or implied, of IARPA, AFRL, or the 
U.S. Government. 
 
References 
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007. Open 
Information Extraction from the Web. In Proceedings 
of IJCAI 2007. 
                                                          
12 The clusters which have only 1 or 2 triples are removed and 
not counted here for both algorithms. 
Michele Banko and Oren Etzioni. 2008. The Tradeoffs 
Between Open and Traditional Relation Extraction. 
In Proceedings of ACL 2008. 
Jonathan Berant, Ido Dagan and Jacob Goldberger. 
2011. Global Learning of Typed Entailment Rules. In 
Proceedings of ACL 2011. 
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective Information Extraction with Relational Mar-
kov Networks. In Proceedings of ACL 2004. 
Jinxiu Chen, Donghong Ji, Chew Lim Tan, Zhengyu 
Niu. 2005. Unsupervised Feature Selection for Rela-
tion Extraction. In Proceedings of IJCNLP 2005. 
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen 
Soderland, Daniel S. Weld, and Alexander Yates. 
2004. Web-scale information extraction in 
KnowItAll (preliminary results). In Proceedings of 
WWW 2004. 
Oren Etzioni, Michael Cafarella, Doug Downey, 
AnaMaria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An 
Experimental Study. In Artificial Intelligence, 
165(1):91-134. 
Anthony Fader, Stephen Soderland, and Oren Etzioni. 
2011. Identifying Relations for Open Information Ex-
traction. In Proceedings of EMNLP 2011. 
Christiane Fellbaum (Ed.). 1998. WordNet: An Elec-
tronic Lexical Database. Cambridge, MA: MIT Press. 
Zelig S. Harris. 1985. Distributional Structure. The Phi-
losophy of Linguistics. New York: Oxford Uni-
versity Press. 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman . 
2004.Discovering Relations among Named Entities 
from Large Corpora. In Proceedings of ACL 2004. 
Marti A. Hearst. 1992. Automatic  Acquisition of  Hy-
ponyms from Large Text Corpora. In Proceedings of 
COLING 1992. 
Stanley Kok and Pedro Domingos. 2008. Extracting 
Semantic Networks from Text via Relational Cluster-
ing. In Proceedings of ECML 2008. 
Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008. 
Semantic Class Learning from the Web with Hypo-
nym Pattern Linkage Graphs. In Proceedings of ACL 
2008. 
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of 
KDD 2001. 
Andrew McCallum, Kamal Nigam and Lyle Ungar. 
2000. Efficient Clustering of High-Dimensional Data 
Sets with Application to Reference Matching. In Pro-
ceedings of KDD 2000. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In 
Proceedings of EMNLP 2009. 
1036
Patrick Pantel and Dekang Lin. 2002. Discovering word 
senses from text. In Proceedings of KDD2002. 
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. In Proceedings 
of HLT/NAACL-2004. 
Marius Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search, In Proceedings of CIKM 
2004. 
Marius Pasca. 2007. Weakly-supervised discovery of 
named entities using web search queries. In Proceed-
ings of CIKM 2007. 
Marius Pasca and Peter Dienes. 2005. Aligning needles 
in a haystack: Paraphrase acquisition across the Web. 
In Proceedings of IJCNLP 2005. 
Marco Pennacchiotti and Patrick Pantel. 2009. Entity 
Extraction via Ensemble Semantics. In Proceedings 
of EMNLP 2009. 
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In 
Proceedings of CIKM 2007. 
Luis Sarmento, Valentin Jijkoun, Maarten de Rijke and 
Eugenio Oliveira. 2007. ?More like these?: growing 
entity classes from seeds. In Proceedings of CIKM 
2007. 
Satoshi Sekine. 2005. Automatic paraphrase discovery 
based on context and keywords between NE pairs. In 
Proceedings of the International Workshop on Para-
phrasing, 2005. 
Shuming Shi, Huibin Zhang, Xiaojie Yuan, Ji-Rong 
Wen. 2010. Corpus-based Semantic Class Mining: 
Distributional vs. Pattern-Based Approaches. In Pro-
ceedings of COLING 2010. 
Yusuke Shinyama, Satoshi Sekine. 2006. Preemptive 
Information Extraction using Unrestricted Relation 
Discovery, In Proceedings of NAACL 2006. 
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. 
Learning Syntactic Patterns for Automatic Hypernym 
Discovery. In Proceedings of  In NIPS 17, 2005. 
Stephen Soderland and Bhushan Mandhani. 2007. Mov-
ing from Textual Relations to Ontologized Relations. 
In Proceedings of the 2007 AAAI Spring Symposium 
on Machine Reading. 
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, 
Deepak Ravichandran, Rahul Bhagat and Fernando 
Pereira. 2008. Weakly-Supervised Acquisition of La-
beled Class Instances using Graph Random Walks. In 
Proceedings of EMNLP 2008. 
David Vickrey, Oscar Kipersztok and Daphne Koller. 
2010. An Active Learning Approach to Finding Re-
lated Terms. In Proceedings of ACL 2010. 
Vishnu Vyas and Patrick Pantel. 2009. SemiAutomatic 
Entity Set Refinement. In Proceedings of 
NAACL/HLT 2009. 
Vishnu Vyas, Patrick Pantel and Eric Crestan. 2009, 
Helping Editors Choose Better Seed Sets for Entity 
Set Expansion, In Proceedings of CIKM 2009. 
Richard C. Wang and William W. Cohen. 2007. Lan-
guage- Independent Set Expansion of Named Entities 
Using the Web. In Proceedings of ICDM 2007. 
Richard C. Wang and William W. Cohen. 
2009. Automatic Set Instance Extraction using the 
Web. In Proceedings of ACL-IJCNLP 2009. 
Wei Wang, Romaric Besan?on and Olivier Ferret. 2011. 
Filtering and Clustering Relations for Unsupervised 
Information Extraction in Open Domain. In Proceed-
ings of CIKM 2011. 
Fei Wu and Daniel S. Weld. 2010. Open information 
extraction using Wikipedia. In Proceedings of ACL 
2010. 
Hua Wu and Ming Zhou. 2003. Synonymous colloca-
tion extraction using translation information. In Pro-
ceedings of the ACL Workshop on Multiword 
Expressions: Integrating Processing 2003. 
Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew 
McCallum. 2011. Structured Relation Discovery Us-
ing Generative Models. In Proceedings of EMNLP 
2011.  
Alexander Yates and Oren Etzioni. 2007. Unsupervised 
Resolution of Objects and Relations on the Web.  In 
Proceedings of HLT-NAACL 2007.  
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, Chin-
Yew Lin. 2011. Nonlinear Evidence Fusion and 
Propagation for Hyponymy Relation Mining. In Pro-
ceedings of ACL 2011. 
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong 
Wen. 2009. Employing Topic Models for Pattern-
based Semantic Class Discovery. In Proceedings of 
ACL 2009. 
1037
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 799?809,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Template Mining for Semantic Category Understanding
Lei Shi
1,2?
, Shuming Shi
3
, Chin-Yew Lin
3
, Yi-Dong Shen
1
, Yong Rui
3
1
State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of Sciences
2
University of Chinese Academy of Sciences
3
Microsoft Research
{shilei,ydshen}@ios.ac.cn
{shumings,cyl,yongrui}@microsoft.com
Abstract
We propose an unsupervised approach to
constructing templates from a large collec-
tion of semantic category names, and use
the templates as the semantic representa-
tion of categories. The main challenge is
that many terms have multiple meanings,
resulting in a lot of wrong templates. Sta-
tistical data and semantic knowledge are
extracted from a web corpus to improve
template generation. A nonlinear scoring
function is proposed and demonstrated to
be effective. Experiments show that our
approach achieves significantly better re-
sults than baseline methods. As an imme-
diate application, we apply the extracted
templates to the cleaning of a category col-
lection and see promising results (preci-
sion improved from 81% to 89%).
1 Introduction
A semantic category is a collection of items shar-
ing common semantic properties. For example,
all cities in Germany form a semantic category
named ?city in Germany? or ?German city?. In
Wikipedia, the category names of an entity are
manually edited and displayed at the end of the
page for the entity. There have been quite a lot of
approaches (Hearst, 1992; Pantel and Ravichan-
dran, 2004; Van Durme and Pasca, 2008; Zhang et
al., 2011) in the literature to automatically extract-
ing category names and instances (also called is-a
or hypernymy relations) from the web.
Most existing work simply treats a category
name as a text string containing one or multiple
words, without caring about its internal structure.
In this paper, we explore the semantic structure
of category names (or simply called ?categories?).
?
This work was performed when the first author was vis-
iting Microsoft Research Asia.
For example, both ?CEO of General Motors? and
?CEO of Yahoo? have structure ?CEO of [com-
pany]?. We call such a structure a category tem-
plate. Taking a large collection of open-domain
categories as input, we construct a list of category
templates and build a mapping from categories to
templates. Figure 1 shows some example semantic
categories and their corresponding templates.
Templates can be treated as additional features
of semantic categories. The new features can be
exploited to improve some upper-layer applica-
tions like web search and question answering. In
addition, by linking categories to templates, it is
possible (for a computer program) to infer the se-
mantic meaning of the categories. For example in
Figure 1, from the two templates linking to cat-
egory ?symptom of insulin deficiency?, it is rea-
sonable to interpret the category as: ?a symptom
of a medical condition called insulin deficiency
which is about the deficiency of one type of hor-
mone called insulin.? In this way, our knowledge
about a category can go beyond a simple string
and its member entities. An immediate application
of templates is removing invalid category names
from a noisy category collection. Promising re-
sults are observed for this application in our ex-
periments.
An intuitive approach to this task (i.e., extract-
ing templates from a collection of category names)
national holiday of South Africa(instances: Heritage Day, Christmas?)
national holiday of Brazil(instances: Carnival, Christmas?) national holiday of [country]
symptom of cortisol deficiency(instances: low blood sugar?)
symptom of insulin deficiency(instances: nocturia, weight loss?) symptom of [hormone] deficiencysymptom of [medical condition]
school in Denverschool in Houston school in [place]school in [city]
Semantic Categories Category templates
football playerbasketball player [sport] player
Figure 1: Examples of semantic categories and
their corresponding templates.
799
contains two stages: category labeling, and tem-
plate scoring.
Category labeling: Divide a category name
into multiple segments and replace some key seg-
ments with its hypernyms. As an example, as-
sume ?CEO of Delphinus? is divided to three seg-
ments ?CEO + of + Delphinus?; and the last seg-
ment (Delphinus) has hypernyms ?constellation?,
?company?, etc. By replacing this segment with
its hypernyms, we get candidate templates ?CEO
of [constellation]? (a wrong template), ?CEO of
[company]?, and the like.
Template scoring: Compute the score of each
candidate template by aggregating the information
obtained in the first phase.
A major challenge here is that many segments
(like ?Delphinus? in the above example) have mul-
tiple meanings. As a result, wrong hypernyms
may be adopted to generate incorrect candidate
templates (like ?CEO of [constellation]?). In this
paper, we focus on improving the template scor-
ing stage, with the goal of assigning lower scores
to bad templates and larger scores to high-quality
ones.
There have been some research efforts (Third,
2012; Fernandez-Breis et al., 2010; Quesada-
Mart?nez et al., 2012) on exploring the structure of
category names by building patterns. However, we
automatically assign semantic types to the pattern
variables (or called arguments) while they do not.
For example, our template has the form of ?city
in [country]? while their patterns are like ?city in
[X]?. More details are given in the related work
section.
A similar task is query understanding, including
query tagging and query template mining. Query
tagging (Li et al., 2009; Reisinger and Pasca,
2011) corresponds to the category labeling stage
described above. It is different from template gen-
eration because the results are for one query only,
without merging the information of all queries to
generate the final templates. Category template
construction are slightly different from query tem-
plate construction. First, some useful features
such as query click-through is not available in cat-
egory template construction. Second, categories
should be valid natural language phrases, while
queries need not. For example, ?city Germany? is
a query but not a valid category name. We discuss
in more details in the related work section.
Our major contributions are as follows.
1) To the best of our knowledge, this is the first
work of template generation specifically for cate-
gories in unsupervised manner.
2) We extract semantic knowledge and statisti-
cal information from a web corpus for improving
template generation. Significant performance im-
provement is obtained in our experiments.
3) We study the characteristics of the scoring
function from the viewpoint of probabilistic evi-
dence combination and demonstrate that nonlinear
functions are more effective in this task.
4) We employ the output templates to clean
our category collection mined from the web, and
get apparent quality improvement (precision im-
proved from 81% to 89%).
After discussing related work in Section 2, we
define the problem and describe one baseline ap-
proach in Section 3. Then we introduce our ap-
proach in Section 4. Experimental results are re-
ported and analyzed in Section 5. We conclude the
paper in Section 6.
2 Related work
Several kinds of work are related to ours.
Hypernymy relation extraction: Hypernymy
relation extraction is an important task in text min-
ing. There have been a lot of efforts (Hearst, 1992;
Pantel and Ravichandran, 2004; Van Durme and
Pasca, 2008; Zhang et al., 2011) in the literature to
extract hypernymy (or is-a) relations from the web.
Our target here is not hypernymy extraction, but
discovering the semantic structure of hypernyms
(or category names).
Category name exploration: Category name
patterns are explored and built in some ex-
isting research work. Third (2012) pro-
posed to find axiom patterns among category
names on an existing ontology. For ex-
ample, infer axiom pattern ?SubClassOf(AB,
B)? from ?SubClassOf(junior school school)?
and ?SubClassOf(domestic mammal mammal)?.
Fernandez-Breis et al. (2010) and Quesada-
Mart?nez et al. (2012) proposed to find lexical pat-
terns in category names to define axioms (in med-
ical domain). One example pattern mentioned in
their papers is ?[X] binding?. They need man-
ual intervention to determine what X means. The
main difference between the above work and ours
is that we automatically assign semantic types to
the pattern variables (or called arguments) while
they do not.
800
Template mining for IE: Some research work
in information extraction (IE) involves patterns.
Yangarber (2003) and Stevenson and Greenwood
(2005) proposed to learn patterns which were in
the form of [subject, verb, object]. The category
names and learned templates in our work are not
in this form. Another difference between our work
and their work is that, their methods need a super-
vised name classifer to generate the candidate pat-
terns while our approach is unsupervised. Cham-
bers and Jurafsky (2011) leverage templates to de-
scribe an event while the templates in our work are
for understanding category names (a kind of short
text).
Query tagging/labeling: Some research work
in recent years focuses on segmenting web search
queries and assigning semantic tags to key seg-
ments. Li et al. (2009) and Li (2010) employed
CRF (Conditional Random Field) or semi-CRF
models for query tagging. A crowdsourcing-
assisted method was proposed by Han et al. (2013)
for query structure interpretation. These super-
vised or semi-supervised approaches require much
manual annotation effort. Unsupervised meth-
ods were proposed by Sarkas et al. (2010) and
Reisinger and Pasca (2011). As been discussed
in the introduction section, query tagging is only
one of the two stages of template generation. The
tagging results are for one query only, without ag-
gregating the global information of all queries to
generate the final templates.
Query template construction: Some existing
work leveraged query templates or patterns for
query understanding. A semi-supervised random
walk based method was proposed by Agarwal et
al. (2010) to generate a ranked templates list which
are relevant to a domain of interest. A predefined
domain schema and seed information is needed for
this method. Pandey and Punera (2012) proposed
an unsupervised method based on graphical mod-
els to mine query templates. The above methods
are either domain-specific (i.e., generating tem-
plates for a specific domain), or have some degree
of supervision (supervised or semi-supervised).
Cheung and Li (2012) proposed an unsupervised
method to generate query templates by the aid of
knowledge bases. An approach was proposed in
(Szpektor et al., 2011) to improve query recom-
mendation via query templates. Query session in-
formation (which is not available in our task) is
needed in this approach for templates generation.
Li et al. (2013) proposed an clustering algorithm
to group existing query templates by search intents
of users.
Compared to the open-domain unsupervised
methods for query template construction, our ap-
proach improves on two aspects. First, we propose
to incorporate multiple types of semantic knowl-
edge (e.g., term peer similarity and term clusters)
to improve template generation. Second, we pro-
pose a nonlinear template scoring function which
is demonstrated to be more effective.
3 Problem Definition and Analysis
3.1 Problem definition
The goal of this paper is to construct a list of cat-
egory templates from a collection of open-domain
category names.
Input: The input is a collection of category
names, which can either be manually compiled
(like Wikipedia categories) or be automatically ex-
tracted. The categories used in our experiments
were automatically mined from the web, by fol-
lowing existing work (Hearst, 1992, Pantel and
Ravichandran 2004; Snow et al., 2005; Talukdar
et al., 2008; Zhang et al., 2011). Specifically,
we applied Hearst patterns (e.g., ?NP [,] (such
as | including) {NP, }
?
{and|or} NP?) and is-
a patterns (?NP (is|are|was|were|being) (a|an|the)
NP?) to a large corpus containing 3 billion En-
glish web pages. As a result, we obtained a
term?hypernym bi-partite graph containing 40
million terms, 74 million hypernyms (i.e., cate-
gory names), and 321 million edges (e.g., one
example edge is ?Berlin???city in Germany?,
where ?Berlin? is a term and ?city in Germany? is
the corresponding hypernym). Then all the multi-
word hypernyms are used as the input category
collection.
Output: The output is a list of templates, each
having a score indicating how likely it is valid. A
template is a multi-word string with one headword
and at least one argument. For example, in tem-
plate ?national holiday of [country]?, ?holiday? is
the headword, and ?[country]? is the argument.
We only consider one-argument templates in this
paper, and the case of multiple arguments is left as
future work. A template is valid if it is syntacti-
cally and semantically correct. ?CEO of [constel-
lation]? (wrongly generated from ?CEO of Del-
phinus?, ?CEO of Aquila?, etc.) is not valid be-
cause it is semantically unreasonable.
801
3.2 Baseline approach
An intuitive approach to this task contains two
stages: category labeling and template scoring.
Figure 2 shows its workflow with simple exam-
ples.
3.2.1 Phase-1: Category labeling
At this stage, each category name is automatically
segmented and labeled; and some candidate tem-
plate tuples (CTTs) are derived based on the la-
beling results. This can be done in the following
steps.
Category segmentation: Divide each cate-
gory name into multiple segments (e.g., ?holi-
day of South Africa? to ?holiday + of + South
Africa?). Each segment is one word or a phrase
appearing in an entity dictionary. The dictionary
used in this paper is comprised of all Freebase
(www.freebase.com) entities.
Segment to hypernym: Find hypernyms for
every segment (except for the headword and some
trivial segments like prepositions and articles), by
referring to a term?hypernym mapping graph.
Following most existing query labeling work, we
derive the term?hypernym graph from a dump of
Freebase. Below are some examples of Freebase
types (hypernyms),
German city (id: /location/de city)
Italian province (id: /location/it province)
Poem character (id: /book/poem character)
Book (id: /book/book)
To avoid generating too fine-grained templates
like ?mayor of [Germany city]? and ?mayor of
[Italian city]? (semantically ?mayor of [city]?
is more desirable), we discard type modifiers
and map terms to the headwords of Freebase
types. For example, ?Berlin? is mapped to
?city?. In this way, we build our basic version of
term?hypernym mapping which contains 16.13
million terms and 696 hypernyms. Since ?South
Africa? is both a country and a book name in Free-
base, hypernyms ?country?, ?book?, and others
are assigned to the segment ?South Africa? in this
step.
CTT generation: Construct CTTs by choosing
one segment (called the target segment) each time
and replacing the segment with its hypernyms. An
CTT is formed by the candidate template (with
one argument), the target segment (as an argument
value), and the tuple score (indicating tuple qual-
ity). Below are example CTTs obtained after the
last segment of ?holiday + of + South Africa? is
processed,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [book], South Africa, w
2
)
3.2.2 Phase-2: Template scoring
The main objective of this stage is to merge all
the CTTs obtained from the previous stage and to
compute a final score for each template. In this
stage, the CTTs are first grouped by the first ele-
ment (i.e., the template string). For example, tu-
ples for ?holiday of [country]? may include,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [country], Brazil, w
2
)
U
3
: (holiday of [country], Germany, w
3
)
...
Then a scoring function is employed to calcu-
late the template score from the tuple scores. For-
mally, given n tuples
~
U=(U
1
, U
2
..., U
n
) for a tem-
plate, the goal is to find a score fusion function
F (
~
U) which yields large values for high-quality
templates and small (or zero) values for invalid
ones.
Borrowing the idea of TF-IDF from information
retrieval, a reasonable scoring function is,
F (
~
U) =
n
?
i=1
w
i
? IDF (h) (1)
where h is the argument type (i.e., the hypernym
of the argument value) of each tuple. TF means
the ?term frequency? and IDF means the ?inverse
document frequency?. An IDF function assigns
lower scores to common hypernyms (like person
and music track which contain a lot of entities).
Let DF (h) be the number of entities having hy-
pernym h, we test two IDF functions in our exper-
iments,
IDF
1
(h) = log
1 +N
1 +DF (h)
IDF
2
(h) = 1/sqrt(DF (h))
(2)
where N is total number of entities in the entity
dictionary.
The next problem is estimating tuple score w
i
.
Please note that there is no weight or score infor-
mation in the term?hypernym mapping of Free-
base. So we have to set w
i
to be constant in the
baseline,
w
i
= 1 (3)
802
Wikipedia
holiday of Brazil
holiday of South Africa
?
Brazil ? country
Brazil ? book
South Africa ? country
South Africa ? book
?
holiday of [country], Brazil, w 1
holiday of [book], Brazil, w 2
holiday of [country], South Africa, w 3
holiday of [book], South Africa, w 4
?
holiday of [country], S 1
holiday of [book], S 2
?
Phase-1: Category labeling
Phase-2: Template scoring
Phase-1 Phase-2
Input: Category names
Term-hypernym mapping
Output: Category templates
head argument
argument
argument value
tuple score
Candidate template tuples (CTTs)
Figure 2: Problem definition and baseline approach.
4 Approach: Enhancing Template
Scoring
In our approach, we follow the same framework
as in the above baseline approach, and focus on
improving the template scoring phase (i.e., phase-
2).
We try three techniques: First, a better tuple
scorew
i
is calculated in Section 4.1 by performing
statistics on a large corpus. The corpus is a collec-
tion of 3 billion web pages crawled in early 2013
by ourselves. During this paper, we use ?our web
corpus? or ?our corpus? to refer to this corpus.
Second, a nonlinear function is adopted in Sec-
tion 4.2 to replace the baseline tuple fusion func-
tion (Formula 1). Third, we extract term peer sim-
ilarity and term clusters from our corpus and use
them as additional semantic knowledge to refine
template scores.
4.1 Enhancing tuple scoring
Let?s examine the following two template tuples,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [book], South Africa, w
2
)
Intuitively, ?South Africa? is more likely to be
a country than a book when it appears in text. So
for a reasonable tuple scoring formula, we should
have w
1
> w
2
.
The main idea is to automatically calculate
the popularity of a hypernym given a term, by
referring to a large corpus. Then by adding
the popularity information to (the edges of) the
term?hypernym graph of Freebase, we obtain a
weighted term?hypernym graph. The weighted
graph is then employed to enhance the estimation
of w
i
.
For popularity calculation, we apply Hearst
patterns (Hearst, 1992) and is-a patterns (?NP
(is|are|was|were|being) (a|an|the) NP?) to every
sentence of our web corpus. For a (term, hyper-
nym) pair, its popularity F is calculated as the
number of sentences in which the term and the hy-
pernym co-occur and also follow at least one of
the patterns.
For a template tuple U
i
with argument type h
and argument value v, we test two ways of esti-
mating the tuple score w
i
,
w
i
= log (1 + F (v, h)) (4)
w
i
=
F (v, h))
?+
?
h
j
?H
F (v, h
j
)
(5)
where F (v, h) is the popularity of the (v, h) pair
in our corpus, H is the set of all hypernyms for v
in the weighted term?hypernym graph. Parame-
ter ? (=1.0 in our experiments) is introduced for
smoothing purpose. Note that the second formula
is the conditional probability of hypernym h given
term v.
Since it is intuitive to estimate tuple scores with
their frequencies in a corpus, we treat the approach
with the improved w
i
as another baseline (our
strong baseline).
4.2 Enhancing tuple combination function
Now we study the possibility of improving the tu-
ple combination function (Formula 1), by examin-
ing the tuple fusion problem from the viewpoint
of probabilistic evidence combination. We first
demonstrate that the linear function in Formula 1
corresponds to the conditional independence as-
sumption of the tuples. Then we propose to adopt
a series of nonlinear functions for combining tuple
scores.
We define the following events:
T : Template T is a valid template;
T : T is an invalid template;
E
i
: The observation of tuple U
i
.
803
Let?s compute the posterior odds of event T ,
given two tuples U
1
and U
2
. Assuming E
1
and
E
2
are conditionally independent given T or T ,
according to the Bayes rule, we have,
P (T |E
1
, E
2
)
P (T |E
1
, E
2
)
=
P (E
1
, E
2
|T ) ? P (T )
P (E
1
, E
2
|T ) ? P (T )
=
P (E
1
|T )
P (E
1
|T )
?
P (E
2
|T )
P (E
2
|T )
?
P (T )
P (T )
=
P (T |E
1
) ? P (T )
P (T |E
1
) ? P (T )
?
P (T |E
2
) ? P (T )
P (T |E
2
) ? P (T )
?
P (T )
P (T )
(6)
Define the log-odds-gain of T given E as,
G(T |E) = log
P (T |E)
P (T |E)
? log
P (T )
P (T )
(7)
Here G means the gain of the log-odds of T af-
ter E occurs. By combining formulas 6 and 7, we
get
G(T |E
1
, E
2
) = G(T |E
1
) +G(T |E
2
) (8)
It is easy to prove that the above conclusion
holds true when n > 2, i.e.,
G(T |E
1
, ..., E
n
) =
n
?
i=1
G(T |E
i
) (9)
If we treat G(T |E
i
) as the score of template T
when only U
i
is observed, andG(T |E
1
, ..., E
n
) as
the template score after the n tuples are observed,
then the above equation means that the combined
template score should be the sum of w
i
? IDF (h),
which is exactly Formula 1. Please keep in mind
that Equation 9 is based on the assumption that the
tuples are conditional independent. This assump-
tion, however, may not hold in reality. The case
of conditional dependence was studied in (Zhang
et al., 2011), where a group of nonlinear combina-
tion functions were proposed and achieved good
performance in their task of hypernymy extrac-
tion. We choose p-Norm as our nonlinear fusion
functions, as below,
F (
~
U) =
p
?
?
?
?
n
?
i=1
w
p
i
? IDF (h) (p > 1) (10)
where p (=2 in experiments) is a parameter.
Experiments show that the above nonlinear
function performs better than the linear function
of Formula 1. Let?s use an example to show the
intuition. Consider a good template ?city of [coun-
try]? corresponding to CTTs
~
U
A
and a wrong tem-
plate ?city of [book]? having tuples
~
U
B
. Sup-
pose |
~
U
A
| = 200 (including most countries in
the world) and |
~
U
B
| = 1000 (considering that
many place names have already been used as book
names). We observe that each tuple score corre-
sponding to ?city of [country]? is larger than the
tuple score corresponding to ?city of [book]?. For
simplicity, we assume each tuple in
~
U
A
has score
1.0 and each tuple in
~
U
B
has score 0.2. With the
linear and nonlinear (p=2) fusion functions, we
can get,
Linear:
F (
~
U
A
) = 200 ? 1.0 = 200
F (
~
U
B
) = 1000 ? 0.2 = 200
(11)
Nonlinear:
F (
~
U
A
) = 14.1
F (
~
U
B
) = 6.32
(12)
In the above settings the nonlinear function
yields a much higher score for the good template
(than for the invalid template), while the linear one
does not.
4.3 Refinement with term similarity and
term clusters
The above techniques neglect the similarity among
terms, which has a high potential to improve the
template scoring process. Intuitively, for a toy set
{?city in Brazil?, ?city in South Africa?,?city in
China?, ?city in Japan?}, since ?Brazil?, ?South
Africa?, ?China? and ?Japan? are very similar to
each other and they all have a large probability to
be a ?country?, so we have more confidence that
?city in [country]? is a good template. In this sec-
tion, we propose to leverage the term similarity
information to improve the template scoring pro-
cess.
We start with building a large group of small
and overlapped clusters from our web corpus.
4.3.1 Building term clusters
Term clusters are built in three steps.
Mining term peer similarity: Two terms are
peers if they share a common hypernym and they
are semantically correlated. For example, ?dog?
and ?cat? should have a high peer similarity score.
Following existing work (Hearst, 1992; Kozareva
804
et al., 2008; Shi et al., 2010; Agirre et al., 2009;
Pantel et al., 2009), we built a peer similarity graph
containing about 40.5 million nodes and 1.33 bil-
lion edges.
Clustering: For each term, choose its top-30
neighbors from the peer similarity graph and run a
hierarchical clustering algorithm, resulting in one
or multiple clusters. Then we merge highly du-
plicated clusters. The algorithm is similar to the
first part of CBC (Pantel and Lin, 2002), with the
difference that a very high merging threshold is
adopted here in order to generate small and over-
lapped clusters. Please note that one term may be
included in many clusters.
Assigning top hypernyms: Up to two hyper-
nyms are assigned for each term cluster by major-
ity voting of its member terms, with the aid of the
weighted term?hypernym graph of Section 4.1.
To be an eligible hypernym for the cluster, it has
to be the hypernym of at least 70% of terms in the
cluster. The score of each hypernym is the aver-
age of the term?hypernym weights over all the
member terms.
4.3.2 Template score refinement
With term clusters at hand, now we describe the
score refinement procedure for a template T hav-
ing argument type h and supporting tuples
~
U=(U
1
,
U
2
..., U
n
). Denote V = {V
1
, V
2
, ..., V
n
} to be the
set of argument values for the tuples (where V
i
is
the argument value of U
i
).
By computing the intersection of V and every
term cluster, we can get a distribution of the argu-
ment values in the clusters. We find that for a good
template like ?holiday in [country]?, we can often
find at least one cluster (one of the country clus-
ters in this example) which has hypernym h and
also contains many elements in V . However, for
invalid templates like ?holiday of [book]?, every
cluster having hypernym h (=?book? here) only
contains a few elements in V . Inspired by such
an observation, our score refinement algorithm for
template T is as follows,
Step-1. Calculating supporting scores: For
each term cluster C having hypernym h, compute
its supporting score to T as follows:
S(C, T ) = k(C, V ) ? w(C, h) (13)
where k(C, V ) is the number of elements shared
by C and V , and w(C, h) is hypernym score of h
to C (computed in the last step of building clus-
ters).
Step-2. Calculating the final template score:
Let term cluster C
?
has the maximal supporting
score to T , the final template score is computed
as,
S(T ) = F (
~
U) ? S(C
?
, T ) (14)
where F (
~
U) is the template score before refine-
ment.
5 Experiments
5.1 Experimental setup
5.1.1 Methods for comparison
We make a comparison among 10 methods.
SC: The method is proposed in (Cheung and Li,
2012) to construct templates from queries. The
method firstly represents a query as a matrix based
on Freebase data. Then a hierarchical clustering
algorithm is employed to group queries having the
same structure and meaning. Then an intent sum-
marization algorithm is employed to create tem-
plates for each query group.
Base: The linear function in Formula 1 is
adopted to combine the tuple scores. We use
IDF
2
here because it achieves higher precision
than IDF
1
in this setting.
LW: The linear function in Formula 1 is
adopted to combine the tuple scores generated by
Formula 4. IDF
1
is used rather than IDF
2
for
better performance.
LP: The linear function in Formula 1 is adopted
to combine the tuple scores generated by Formula
5. IDF
2
is used rather than IDF
1
for better per-
formance.
NLW: The nonlinear fusion function in For-
mula 10 is used. Other settings are the same as
LW.
NLP: The nonlinear fusion function in Formula
10 is used. Other settings are the same as LP.
LW+C, LP+C, NLW+C, NLP+C: All the set-
tings of LW, LP, NLW, NLP respectively, with the
refinement technology in Section 4.3 applied.
5.1.2 Data sets, annotation and evaluation
metrics
The input category names for experiments are au-
tomatically extracted from a web corpus (Section
3.1). Two test-sets are built for evaluation from the
output templates of various methods.
Subsets: In order to conveniently compare the
performance of different methods, we create 20
sub-collections (called subsets) from the whole in-
put category collection. Each subset contains all
805
the categories having the same headword (e.g.,
?symptom of insulin deficiency? and ?depression
symptom? are in the same subset because they
share the same headword ?symptom?). To choose
the 20 headwords, we first sample 100 at ran-
dom from the set of all headwords; then manu-
ally choose 20 for diversity. The headwords in-
clude symptom, school, food, gem, hero, weapon,
model, etc. We run the 10 methods on these sub-
sets and sort the output templates by their scores.
Top-30 templates from each method on each sub-
set are selected and mixed together for annotation.
Fullset: We run method NLP+C (which has
the best performance according to our subsets
experiments) on the input categories and sort
the output templates by their scores. Then we
split the templates into 9 sections according
to their ranking position. The sections are:
[1?100], (100?1K], (1K?10K], (10K?100K],
(100K,120K], (120K?140K], (140K?160K],
(160K?180K], (180K?200K]. Then 40 templates
are randomly chosen from each section and mixed
together for annotation.
The selected templates (from subsets and the
fullset) are annotated by six annotators, with each
template assigned to two annotators. A template is
assigned a label of ?good?, ?fair?, or ?bad? by an
annotator. The percentage agreement between the
annotators is 80.2%, with kappa 0.624.
For the subset experiments, we adopt
Precision@k (k=10,20,30) to evaluate the
top templates generated by each method. The
scores for ?good?, ?fair?, and ?bad? are 1, 0.5,
and 0. The score of each template is the average
annotation score over two annotators (e.g., if a
template is annotated ?good? by one annotator and
?fair? by another, its score is (1.0+0.5)/2=0.75).
The evaluation score of a method is the average
over the 20 subsets. For the fullset experiments,
we report the precision for each section.
5.2 Experimental results
5.2.1 Results for subsets
The results of each method on the 20 subsets
are presented in Table 1. A few observations
can be made. First, by comparing the per-
formance of baseline-1 (Base) and the methods
adopting term?hypernym weight (LW and LP),
we can see big performance improvement. The
bad performance of baseline-1 is mainly due to
the lack of weight (or frequency) information on
Method P@10 P@20 P@30
Base (baseline-1) 0.359 0.361 0.358
SC (Cheung and Li, 2012) 0.382 0.366 0.371
Weighted LW 0.633 0.582 0.559
(baseline-2) LP 0.771 0.734 0.707
Nonlinear NLW 0.711 0.671 0.638
NLP 0.818 0.791 0.765
LW+C 0.813 0.786 0.754
Term cluster NLW+C 0.854 0.833 0.808
LP+C 0.818 0.788 0.778
NLP+C 0.868 0.839 0.788
Table 1: Performance comparison among the
methods on subset.
term?hypernym edges. The results demonstrate
that edge scores are critical for generating high
quality templates. Manually built semantic re-
sources typically lack such kinds of scores. There-
fore, it is very important to enhance them by de-
riving statistical data from a large corpus. Since
it is relatively easy to have the idea of adopt-
ing a weighted term?hypernym graph, we treat
LW and LP as another (stronger) baseline named
baseline-2.
As the second observation, the results show that
the nonlinear methods (NLP and NLW) achieve
performance improvement over their linear ver-
sions (LW and LP).
Third, let?s examine the methods with template
scores refined by term similarity and term clus-
ters (LW+C, NLW+C, LP+C, NLP+C). It is shown
that the refine-by-cluster technology brings addi-
tional performance gains on all the four settings
(linear and nonlinear, two different ways of calcu-
lating tuple scores). So we can conclude that the
peer similarity and term clusters are quite effective
in improving template generation.
Fourth, the best performance is achieved
when the three techniques (i.e., term?hypernym
weight, nonlinear fusion function, and refine-by-
cluster) are combined together. For instance, by
comparing the P@20 scores of baseline-2 and
NLP+C, we see a performance improvement of
14.3% (from 0.734 to 0.839). Therefore every
technique studied in this paper has its own merit
in template generation.
Finally, by comparing the method SC (Cheung
and Li, 2012) with other methods, we can see that
SC is slightly better than baseline-1, but has much
lower performance than others. The major reason
may be that this method did not employ a weighted
term?hypernym graph or term peer similarity in-
formation in template construction.
806
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@10 NLP > ?? > ?? >
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?? >
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@20 NLP > ?? > ?? > ??
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?? > ??
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@30 NLP > ?? > ?? > ??
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?
Table 2: Paired t-test results on subsets.
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@10 NLW > ?? > ?? > ?
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ?
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@20 NLW > ?? > ?? > ??
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ??
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@30 NLW > ?? > ?? > ??
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ??
Table 3: Paired t-test results on subsets.
Are the performance differences between meth-
ods significant enough for us to say that one is bet-
ter than the other? To answer this question, we run
paired two-tailed t-test on every pair of methods.
We report the t-test values among methods in ta-
bles 2, 3 and 4.
The meaning of the symbols in the tables are,
?: The method on the row and the one on the
column have similar performance.
>: The method on the row outperforms the
method on the column, but the performance dif-
ference is not statistically significant (0.05 ? P <
0.1 in two-tailed t-test).
> ?: The performance difference is statistically
significant (P < 0.05 in two-tailed t-test).
> ??: The performance difference is statisti-
cally highly significant (P < 0.01 in two-tailed
t-test).
P@10 P@20 P@30
LP V.S. LW > ?? > ?? > ??
NLP V.S. NLW > ?? > ?? > ??
LP+C V.S. LW+C ? ? ?
NLP+C V.S. NLW+C ? ? ?
Table 4: Paired t-test results on subsets.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9
Pr
ec
isi
on
Section ID
Figure 3: Precision by section in the fullset.
5.2.2 Fullset results
As described in the Section 5.1.2, for the fullset
experiments, we conduct a section-wise evalua-
tion, selecting 40 templates from each of the 9 sec-
tions of the NLP+C results. The results are shown
in Figure 3. It can be observed that the precision
for each section decreases when the section ID in-
creases. The results indicate the effectiveness of
our approach, since it can rank good templates in
top sections and bad templates in bottom sections.
According to the section-wise precision data, we
are able to determine the template score threshold
for choosing different numbers of top templates in
different applications.
5.2.3 Templates for category collection
cleaning
Since our input category collection is automati-
cally constructed from the web, some wrong or
invalid category names is inevitably contained. In
this subsection, we apply our category templates
to clean the category collection. The basic idea is
that if a category can match a template, it is more
likely to be correct. We compute a new score for
every category name H as follows,
S
new
(H) = log(1 + S(H)) ? S(T
?
) (15)
where S(H) is the existing category score, deter-
mined by its frequency in the corpus. Here S(T
?
)
is the score of template T
?
, the best template (i.e.,
the template with the highest score) for the cate-
gory.
Then we re-rank the categories according to
their new scores to get a re-ranked category list.
We randomly sampled 150 category names from
the top 2 million categories of each list (the old list
and the new list) and asked annotators to judge the
807
quality of the categories. The annotation results
show that, after re-ranking, the precision increases
from 0.81 to 0.89 (i.e., the percent of invalid cate-
gory names decreases from 19% to 11%).
6 Conclusion
In this paper, we studied the problem of build-
ing templates for a large collection of category
names. We tested three techniques (tuple scor-
ing by weighted term?hypernym mapping, non-
linear score fusion, refinement by term clusters)
and found that all of them are very effective and
their combination achieves the best performance.
By employing the output templates to clean our
category collection mined from the web, we get
apparent quality improvement. Future work in-
cludes supporting multi-argument templates, dis-
ambiguating headwords of category names and ap-
plying our approach to general short text template
mining.
Acknowledgments
We would like to thank the annotators for their ef-
forts in annotating the templates. Thanks to the
anonymous reviewers for their helpful comments
and suggestions. This work is supported in part by
China National 973 program 2014CB340301 and
NSFC grant 61379043.
References
Ganesh Agarwal, Govind Kabra, and Kevin Chen-
Chuan Chang. 2010. Towards rich query interpreta-
tion: walking back and forth for mining query tem-
plates. In Proceedings of the 19th international con-
ference on World wide web, pages 1?10. ACM.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 976?986. Association for Computational Lin-
guistics.
Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence
clustering and labeling for unsupervised query intent
discovery. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
pages 383?392. ACM.
Jesualdo Tomas Fernandez-Breis, Luigi Iannone, Ig-
nazio Palmisano, Alan L Rector, and Robert
Stevens. 2010. Enriching the gene ontology via the
dissection of labels using the ontology pre-processor
language. In Knowledge Engineering and Manage-
ment by the Masses, pages 59?73. Springer.
Jun Han, Ju Fan, and Lizhu Zhou. 2013.
Crowdsourcing-assisted query structure interpreta-
tion. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2092?2098. AAAI Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics -
Volume 2, COLING ?92, pages 539?545, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048?1056.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 572?579. ACM.
Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai.
2013. Unsupervised identification of synonymous
query intent templates for attribute intents. In Pro-
ceedings of the 22nd ACM international conference
on Conference on information & knowledge man-
agement, pages 2029?2038. ACM.
Xiao Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1337?1345. Association
for Computational Linguistics.
Sandeep Pandey and Kunal Punera. 2012. Unsuper-
vised extraction of template structure in web search
queries. In Proceedings of the 21st international
conference on World Wide Web, pages 409?418.
ACM.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613?619.
ACM.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In HLT-
NAACL, volume 4, pages 321?328.
808
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 938?947. Association for Com-
putational Linguistics.
Manuel Quesada-Mart?nez, Jesualdo Tom?as
Fern?andez-Breis, and Robert Stevens. 2012.
Enrichment of owl ontologies: a method for defin-
ing axioms from labels. In Proceedings of the First
International Workshop on Capturing and Refining
Knowledge in the Medical Domain (K-MED 2012),
Galway, Ireland, pages 1?10.
Joseph Reisinger and Marius Pasca. 2011. Fine-
grained class label markup of search queries. In
ACL, pages 1200?1209.
Nikos Sarkas, Stelios Paparizos, and Panayiotis
Tsaparas. 2010. Structured annotations of web
queries. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data,
pages 771?782. ACM.
Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class
mining: distributional vs. pattern-based approaches.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 993?1001. As-
sociation for Computational Linguistics.
Mark Stevenson and Mark A Greenwood. 2005. A
semantic approach to ie pattern induction. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 379?386. As-
sociation for Computational Linguistics.
Idan Szpektor, Aristides Gionis, and Yoelle Maarek.
2011. Improving recommendation for long-tail
queries via templates. In Proceedings of the 20th
international conference on World wide web, pages
47?56. ACM.
Allan Third. 2012. Hidden semantics: what can we
learn from the names in an ontology? In Proceed-
ings of the Seventh International Natural Language
Generation Conference, pages 67?75. Association
for Computational Linguistics.
Benjamin Van Durme and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction. In AAAI, volume 8, pages
1243?1248.
Roman Yangarber. 2003. Counter-training in discov-
ery of semantic patterns. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 343?350. Association
for Computational Linguistics.
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and
Chin-Yew Lin. 2011. Nonlinear evidence fusion
and propagation for hyponymy relation mining. In
ACL, volume 11, pages 1159?1168.
809
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Nonlinear Evidence Fusion and Propagation 
for Hyponymy Relation Mining 
 
Fan Zhang2*     Shuming Shi1     Jing Liu2     Shuqi Sun3*     Chin-Yew Lin1 
1Microsoft Research Asia 
2Nankai University, China 
3Harbin Institute of Technology, China 
{shumings, cyl}@microsoft.com 
 
 
 
Abstract 
This paper focuses on mining the hypon-
ymy (or is-a) relation from large-scale, 
open-domain web documents. A nonlinear 
probabilistic model is exploited to model 
the correlation between sentences in the 
aggregation of pattern matching results. 
Based on the model, we design a set of ev-
idence combination and propagation algo-
rithms. These significantly improve the 
result quality of existing approaches.  Ex-
perimental results conducted on 500 mil-
lion web pages and hypernym labels for 
300 terms show over 20% performance 
improvement in terms of P@5, MAP and 
R-Precision. 
1 Introduction1 
An important task in text mining is the automatic 
extraction of entities and their lexical relations; this 
has wide applications in natural language pro-
cessing and web search. This paper focuses on 
mining the hyponymy (or is-a) relation from large-
scale, open-domain web documents. From the 
viewpoint of entity classification, the problem is to 
automatically assign fine-grained class labels to 
terms. 
There have been a number of approaches 
(Hearst 1992; Pantel & Ravichandran 2004; Snow 
et al, 2005; Durme & Pasca, 2008; Talukdar et al, 
2008) to address the problem. These methods typi-
cally exploited manually-designed or automatical-
                                                          
* This work was performed when Fan Zhang and Shuqi Sun 
were interns at Microsoft Research Asia 
ly-learned patterns (e.g., ?NP such as NP?, ?NP 
like NP?, ?NP is a NP?). Although some degree of 
success has been achieved with these efforts, the 
results are still far from perfect, in terms of both 
recall and precision. As will be demonstrated in 
this paper, even by processing a large corpus of 
500 million web pages with the most popular pat-
terns, we are not able to extract correct labels for 
many (especially rare) entities. Even for popular 
terms, incorrect results often appear in their label 
lists. 
The basic philosophy in existing hyponymy ex-
traction approaches (and also many other text-
mining methods) is counting: count the number of 
supporting sentences. Here a supporting sentence 
of a term-label pair is a sentence from which the 
pair can be extracted via an extraction pattern. We 
demonstrate that the specific way of counting has a 
great impact on result quality, and that the state-of-
the-art counting methods are not optimal. Specifi-
cally, we examine the problem from the viewpoint 
of probabilistic evidence combination and find that 
the probabilistic assumption behind simple count-
ing is the statistical independence between the ob-
servations of supporting sentences. By assuming a 
positive correlation between supporting sentence 
observations and adopting properly designed non-
linear combination functions, the results precision 
can be improved. 
It is hard to extract correct labels for rare terms 
from a web corpus due to the data sparseness prob-
lem. To address this issue, we propose an evidence 
propagation algorithm motivated by the observa-
tion that similar terms tend to share common hy-
pernyms. For example, if we already know that 1) 
Helsinki and Tampere are cities, and 2) Porvoo is 
similar to Helsinki and Tampere, then Porvoo is 
1159
very likely also a city. This intuition, however, 
does not mean that the labels of a term can always 
be transferred to its similar terms. For example, 
Mount Vesuvius and Kilimanjaro are volcanoes 
and Lhotse is similar to them, but Lhotse is not a 
volcano. Therefore we should be very conservative 
and careful in hypernym propagation. In our prop-
agation algorithm, we first construct some pseudo 
supporting sentences for a term from the support-
ing sentences of its similar terms. Then we calcu-
late label scores for terms by performing nonlinear 
evidence combination based on the (pseudo and 
real) supporting sentences. Such a nonlinear prop-
agation algorithm is demonstrated to perform bet-
ter than linear propagation. 
Experimental results on a publicly available col-
lection of 500 million web pages with hypernym 
labels annotated for 300 terms show that our non-
linear evidence fusion and propagation significant-
ly improve the precision and coverage of the 
extracted hyponymy data. This is one of the tech-
nologies adopted in our semantic search and min-
ing system NeedleSeek2. 
In the next section, we discuss major related ef-
forts and how they differ from our work. Section 3 
is a brief description of the baseline approach. The 
probabilistic evidence combination model that we 
exploited is introduced in Section 4. Our main ap-
proach is illustrated in Section 5. Section 6 shows 
our experimental settings and results. Finally, Sec-
tion 7 concludes this paper. 
2 Related Work 
Existing efforts for hyponymy relation extraction 
have been conducted upon various types of data 
sources, including plain-text corpora (Hearst 1992; 
Pantel & Ravichandran, 2004; Snow et al, 2005; 
Snow et al, 2006; Banko, et al, 2007; Durme & 
Pasca, 2008; Talukdar et al, 2008), semi-
structured web pages (Cafarella  et al, 2008; Shin-
zato & Torisawa, 2004), web search results (Geraci 
et al, 2006; Kozareva et al, 2008; Wang & Cohen, 
2009), and query logs (Pasca 2010). Our target for 
optimization in this paper is the approaches that 
use lexico-syntactic patterns to extract hyponymy 
relations from plain-text corpora. Our future work 
will study the application of the proposed algo-
rithms on other types of approaches. 
                                                          
2 http://research.microsoft.com/en-us/projects/needleseek/ or 
http://needleseek.msra.cn/  
The probabilistic evidence combination model 
that we exploit here was first proposed in (Shi et 
al., 2009), for combining the page in-link evidence 
in building a nonlinear static-rank computation 
algorithm. We applied it to the hyponymy extrac-
tion problem because the model takes the depend-
ency between supporting sentences into 
consideration and the resultant evidence fusion 
formulas are quite simple. In (Snow et al, 2006), a 
probabilistic model was adopted to combine evi-
dence from heterogeneous relationships to jointly 
optimize the relationships. The independence of 
evidence was assumed in their model. In compari-
son, we show that better results will be obtained if 
the evidence correlation is modeled appropriately. 
Our evidence propagation is basically about us-
ing term similarity information to help instance 
labeling. There have been several approaches 
which improve hyponymy extraction with instance 
clusters built by distributional similarity. In (Pantel 
& Ravichandran, 2004), labels were assigned to 
the committee (i.e., representative members) of a 
semantic class and used as the hypernyms of the 
whole class. Labels generated by their approach 
tend to be rather coarse-grained, excluding the pos-
sibility of a term having its private labels (consid-
ering the case that one meaning of a term is not 
covered by the input semantic classes). In contrast 
to their method, our label scoring and ranking ap-
proach is applied to every single term rather than a 
semantic class. In addition, we also compute label 
scores in a nonlinear way, which improves results 
quality. In Snow et al (2005), a supervised ap-
proach was proposed to improve hypernym classi-
fication using coordinate terms. In comparison, our 
approach is unsupervised. Durme & Pasca (2008) 
cleaned the set of instance-label pairs with a 
TF*IDF like method, by exploiting clusters of se-
mantically related phrases. The core idea is to keep 
a term-label pair (T, L) only if the number of terms 
having the label L in the term T?s cluster is above a 
threshold and if L is not the label of too many clus-
ters (otherwise the pair will be discarded). In con-
trast, we are able to add new (high-quality) labels 
for a term with our evidence propagation method. 
On the other hand, low quality labels get smaller 
score gains via propagation and are ranked lower. 
Label propagation is performed in (Talukdar et 
al., 2008; Talukdar & Pereira, 2010) based on mul-
tiple instance-label graphs. Term similarity infor-
mation was not used in their approach. 
1160
Most existing work tends to utilize small-scale 
or private corpora, whereas the corpus that we used 
is publicly available and much larger than most of 
the existing work. We published our term sets (re-
fer to Section 6.1) and their corresponding user 
judgments so researchers working on similar topics 
can reproduce our results. 
 
Type Pattern 
Hearst-I NPL {,} (such as) {NP,}
* {and|or} NP  
Hearst-II 
NPL {,} (include(s) | including) {NP,}
* 
{and|or} NP 
Hearst-III NPL {,} (e.g.|e.g) {NP,}
* {and|or} NP 
IsA-I NP (is|are|was|were|being) (a|an) NPL 
IsA-II NP (is|are|was|were|being) {the, those} NPL 
IsA-III NP (is|are|was|were|being) {another, any} NPL 
Table 1. Patterns adopted in this paper (NP: named 
phrase representing an entity; NPL: label) 
3 Preliminaries 
The problem addressed in this paper is corpus-
based is-a relation mining: extracting hypernyms 
(as labels) for entities from a large-scale, open-
domain document corpus. The desired output is a 
mapping from terms to their corresponding hyper-
nyms, which can naturally be represented as a 
weighted bipartite graph (term-label graph). Typi-
cally we are only interested in top labels of a term 
in the graph. 
Following existing efforts, we adopt pattern-
matching as a basic way of extracting hyper-
nymy/hyponymy relations. Two types of patterns 
(refer to Table 1) are employed, including the pop-
ular ?Hearst patterns? (Hearst, 1992) and the IsA 
patterns which are exploited less frequently in ex-
isting hyponym mining efforts. One or more term-
label pairs can be extracted if a pattern matches a 
sentence. In the baseline approach, the weight of 
an edge T?L (from term T to hypernym label L) in 
the term-label graph is computed as, 
 w(T?L)      ( )       
   
    ( )
 (3.1) 
where m is the number of times the pair (T, L) is 
extracted from the corpus, DF(L) is the number of 
in-links of L in the graph, N is total number of 
terms in the graph, and IDF means the ?inverse 
document frequency?. 
A term can only keep its top-k neighbors (ac-
cording to the edge weight) in the graph as its final 
labels. 
Our pattern matching algorithm implemented in 
this paper uses part-of-speech (POS) tagging in-
formation, without adopting a parser or a chunker. 
The noun phrase boundaries (for terms and labels) 
are determined by a manually designed POS tag 
list. 
4 Probabilistic Label-Scoring Model 
Here we model the hyponymy extraction problem 
from the probability theory point of view, aiming 
at estimating the score of a term-label pair (i.e., the 
score of a label w.r.t. a term) with probabilistic 
evidence combination. The model was studied in 
(Shi et al, 2009) to combine the page in-link evi-
dence in building a nonlinear static-rank computa-
tion algorithm. 
We represent the score of a term-label pair by 
the probability of the label being a correct hyper-
nym of the term, and define the following events, 
AT,L: Label L is a hypernym of term T (the ab-
breviated form A is used in this paper unless it is 
ambiguous). 
Ei: The observation that (T, L) is extracted from 
a sentence Si via pattern matching (i.e., Si is a sup-
porting sentence of the pair). 
Assuming that we already know m supporting 
sentences (S1~Sm), our problem is to compute 
P(A|E1,E2,..,Em), the posterior probability that L is 
a hypernym of term T, given evidence E1~Em. 
Formally, we need to find a function f to satisfy, 
 P(A|E1,?,Em) = f(P(A), P(A|E1)?, P(A|Em) ) (4.1) 
For simplicity, we first consider the case of 
m=2. The case of m>2 is quite similar. 
We start from the simple case of independent 
supporting sentences. That is, 
  (     )   (  )   (  ) (4.2) 
  (       )   (    )   (    ) (4.3) 
By applying Bayes rule, we get, 
 
 (       )  
 (       )   ( )
 (     )
 
          
 (    )   ( )
 (  )
 
 (    )   ( )
 (  )
 
 
 ( )
 
          
 (    )   (    )
 ( )
 
(4.4) 
Then define 
 (   )     
 (   )
 ( )
     ( (   ))     ( ( )) 
1161
Here G(A|E) represents the log-probability-gain 
of A given E, with the meaning of the gain in the 
log-probability value of A after the evidence E is 
observed (or known). It is a measure of the impact 
of evidence E to the probability of event A. With 
the definition of G(A|E), Formula 4.4 can be trans-
formed to, 
  (       )   (    )   (    ) (4.5) 
Therefore, if E1 and E2 are independent, the log-
probability-gain of A given both pieces of evidence 
will exactly be the sum of the gains of A given eve-
ry single piece of evidence respectively. It is easy 
to prove (by following a similar procedure) that the 
above Formula holds for the case of m>2, as long 
as the pieces of evidence are mutually independent. 
Therefore for a term-label pair with m mutually 
independent supporting sentences, if we set every 
gain G(A|Ei) to be a constant value g, the posterior 
gain score of the pair will be ?         . If the 
value g is the IDF of label L, the posterior gain will 
be, 
 G(AT,L|E1?,Em) ?    ( )
 
         ( ) (4.6) 
This is exactly the Formula 3.1. By this way, we 
provide a probabilistic explanation of scoring the 
candidate labels for a term via simple counting. 
 
 Hearst-I IsA-I 
E1: Hearst-I 
E2: IsA-I 
RA: 
 (      )
 (    ) (    )
  66.87 17.30 24.38 
R: 
 (    )
 (  ) (  )
  5997 1711 802.7 
RA/R 0.011 0.010 0.030 
Table 2. Evidence dependency estimation for intra-
pattern and inter-pattern supporting sentences 
In the above analysis, we assume the statistical 
independence of the supporting sentence observa-
tions, which may not hold in reality. Intuitively, if 
we already know one supporting sentence S1 for a 
term-label pair (T, L), then we have more chance to 
find another supporting sentence than if we do not 
know S1. The reason is that, before we find S1, we 
have to estimate the probability with the chance of 
discovering a supporting sentence for a random 
term-label pair. The probability is quite low be-
cause most term-label pairs do not have hyponymy 
relations. Once we have observed S1, however, the 
chance of (T, L) having a hyponymy relation in-
creases. Therefore the chance of observing another 
supporting sentence becomes larger than before. 
Table 2 shows the rough estimation of 
 (      )
 (    ) (    )
 (denoted as RA), 
 (    )
 (  ) (  )
 (denoted 
as R), and their ratios. The statistics are obtained 
by performing maximal likelihood estimation 
(MLE) upon our corpus and a random selection of 
term-label pairs from our term sets (see Section 
6.1) together with their top labels3. The data veri-
fies our analysis about the correlation between E1 
and E2 (note that R=1 means independent). In addi-
tion, it can be seen that the conditional independ-
ence assumption of Formula 4.3 does not hold 
(because RA>1). It is hence necessary to consider 
the correlation between supporting sentences in the 
model. The estimation of Table 2 also indicates 
that, 
 
 (     )
 (  ) (  )
 
 (       )
 (    ) (    )
 (4.7) 
By following a similar procedure as above, with 
Formulas 4.2 and 4.3 replaced by 4.7, we have, 
  (       )   (    )   (    ) (4.8) 
This formula indicates that when the supporting 
sentences are positively correlated, the posterior 
score of label L w.r.t. term T (given both the sen-
tences) is smaller than the sum of the gains caused 
by one sentence only. In the extreme case that sen-
tence S2 fully depends on E1 (i.e. P(E2|E1)=1), it is 
easy to prove that 
  (       )   (    )  
It is reasonable, since event E2 does not bring in 
more information than E1. 
Formula 4.8 cannot be used directly for compu-
ting the posterior gain. What we really need is a 
function h satisfying 
  (         )   ( (    )    (    )) (4.9) 
and 
  (      )  ?   
 
     (4.10) 
Shi et al (2009) discussed other constraints to h 
and suggested the following nonlinear functions, 
   (      )    (  ? ( 
    )    )  (4.11) 
                                                          
3 RA is estimated from the labels judged as ?Good?; whereas 
the estimation of R is from all judged labels. 
1162
   (      )  ??   
  
   
 
           (p>1) (4.12) 
In the next section, we use the above two h func-
tions as basic building blocks to compute label 
scores for terms. 
5 Our Approach 
Multiple types of patterns (Table 1) can be adopted 
to extract term-label pairs. For two supporting sen-
tences the correlation between them may depend 
on whether they correspond to the same pattern. In 
Section 5.1, our nonlinear evidence fusion formu-
las are constructed by making specific assumptions 
about the correlation between intra-pattern sup-
porting sentences and inter-pattern ones. 
Then in Section 5.2, we introduce our evidence 
propagation technique in which the evidence of a 
(T, L) pair is propagated to the terms similar to T. 
5.1 Nonlinear evidence fusion 
For a term-label pair (T, L), assuming K patterns 
are used for hyponymy extraction and the support-
ing sentences discovered with pattern i are, 
                  (5.1) 
where mi is the number of supporting sentences 
corresponding to pattern i. Also assume the gain 
score of Si,j is xi,j, i.e., xi,j=G(A|Si,j). 
Generally speaking, supporting sentences corre-
sponding to the same pattern typically have a high-
er correlation than the sentences corresponding to 
different patterns. This can be verified by the data 
in Table-2. By ignoring the inter-pattern correla-
tions, we make the following simplified assump-
tion: 
Assumption: Supporting sentences correspond-
ing to the same pattern are correlated, while those 
of different patterns are independent. 
According to this assumption, our label-scoring 
function is, 
      (   )  ? (               )
 
   
 (5.2) 
In the simple case that         ( ) , if the h 
function of Formula 4.12 is adopted, then, 
      (   )  (? ?  
 
 
   
)     ( ) (5.3) 
We use an example to illustrate the above for-
mula. 
Example: For term T and label L1, assume the 
numbers of the supporting sentences corresponding 
to the six pattern types in Table 1 are (4, 4, 4, 4, 4, 
4), which means the number of supporting sen-
tences discovered by each pattern type is 4. Also 
assume the supporting-sentence-count vector of 
label L2 is (25, 0, 0, 0, 0, 0). If we use Formula 5.3 
to compute the scores of L1 and L2, we can have 
the following (ignoring IDF for simplicity), 
Score(L1)   ?    ; Score(L2) ?     
One the other hand, if we simply count the total 
number of supporting sentences, the score of L2 
will be larger. 
The rationale implied in the formula is: For a 
given term T, the labels supported by multiple 
types of patterns tend to be more reliable than 
those supported by a single pattern type, if they 
have the same number of supporting sentences. 
5.2 Evidence propagation 
According to the evidence fusion algorithm de-
scribed above, in order to extract term labels relia-
bly, it is desirable to have many supporting 
sentences of different types. This is a big challenge 
for rare terms, due to their low frequency in sen-
tences (and even lower frequency in supporting 
sentences because not all occurrences can be cov-
ered by patterns). With evidence propagation, we 
aim at discovering more supporting sentences for 
terms (especially rare terms). Evidence propaga-
tion is motivated by the following two observa-
tions: 
(I) Similar entities or coordinate terms tend to 
share some common hypernyms. 
(II) Large term similarity graphs are able to be 
built efficiently with state-of-the-art techniques 
(Agirre et al, 2009; Pantel et al, 2009; Shi et al, 
2010). With the graphs, we can obtain the similari-
ty between two terms without their hypernyms be-
ing available. 
The first observation motivates us to ?borrow? 
the supporting sentences from other terms as auxil-
iary evidence of the term. The second observation 
means that new information is brought with the 
state-of-the-art term similarity graphs (in addition 
to the term-label information discovered with the 
patterns of Table 1). 
1163
Our evidence propagation algorithm contains 
two phases. In phase I, some pseudo supporting 
sentences are constructed for a term from the sup-
porting sentences of its neighbors in the similarity 
graph. Then we calculate the label scores for terms 
based on their (pseudo and real) supporting sen-
tences. 
Phase I: For every supporting sentence S and 
every similar term T1 of the term T, add a pseudo 
supporting sentence S1 for T1, with the gain score, 
  (         )       (    )   (      ) (5.5) 
where         is the propagation factor, and 
   (   ) is the term similarity function taking val-
ues in [0, 1]. The formula reasonably assumes that 
the gain score of the pseudo supporting sentence 
depends on the gain score of the original real sup-
porting sentence, the similarity between the two 
terms, and the propagation factor. 
Phase II: The nonlinear evidence combination 
formulas in the previous subsection are adopted to 
combine the evidence of pseudo supporting sen-
tences. 
Term similarity graphs can be obtained by dis-
tributional similarity or patterns (Agirre et al, 
2009; Pantel et al, 2009; Shi et al, 2010). We call 
the first type of graph DS and the second type PB. 
DS approaches are based on the distributional hy-
pothesis (Harris, 1985), which says that terms ap-
pearing in analogous contexts tend to be similar. In 
a DS approach, a term is represented by a feature 
vector, with each feature corresponding to a con-
text in which the term appears. The similarity be-
tween two terms is computed as the similarity 
between their corresponding feature vectors. In PB 
approaches, a list of carefully-designed (or auto-
matically learned) patterns is exploited and applied 
to a text collection, with the hypothesis that the 
terms extracted by applying each of the patterns to 
a specific piece of text tend to be similar. Two cat-
egories of patterns have been studied in the litera-
ture (Heast 1992; Pasca 2004; Kozareva et al, 
2008; Zhang et al, 2009): sentence lexical patterns, 
and HTML tag patterns. An example of sentence 
lexical patterns is ?T {, T}*{,} (and|or) T?. HTML 
tag patterns include HTML tables, drop-down lists, 
and other tag repeat patterns. In this paper, we 
generate the DS and PB graphs by adopting the 
best-performed methods studied in (Shi et al, 
2010). We will compare, by experiments, the prop-
agation performance of utilizing the two categories 
of graphs, and also investigate the performance of 
utilizing both graphs for evidence propagation. 
6 Experiments 
6.1 Experimental setup 
Corpus We adopt a publicly available dataset in 
our experiments: ClueWeb094. This is a very large 
dataset collected by Carnegie Mellon University in 
early 2009 and has been used by several tracks of 
the Text Retrieval Conference (TREC)5. The whole 
dataset consists of 1.04 billion web pages in ten 
languages while only those in English, about 500 
million pages, are used in our experiments. The 
reason for selecting such a dataset is twofold: First, 
it is a corpus large enough for conducting web-
scale experiments and getting meaningful results. 
Second, since it is publicly available, it is possible 
for other researchers to reproduce the experiments 
in this paper. 
Term sets Approaches are evaluated by using 
two sets of selected terms: Wiki200, and Ext100. 
For every term in the term sets, each approach 
generates a list of hypernym labels, which are 
manually judged by human annotators. Wiki200 is 
constructed by first randomly selecting 400 Wik-
ipedia6 titles as our candidate terms, with the prob-
ability of a title T being selected to be     (  
 ( )), where F(T) is the frequency of T in our data 
corpus. The reason of adopting such a probability 
formula is to balance popular terms and rare ones 
in our term set. Then 200 terms are manually se-
lected from the 400 candidate terms, with the prin-
ciple of maximizing the diversity of terms in terms 
of length (i.e., number of words) and type (person, 
location, organization, software, movie, song, ani-
mal, plant, etc.). Wiki200 is further divided into 
two subsets: Wiki100H and Wiki100L, containing 
respectively the 100 high-frequency and low-
frequency terms. Ext100 is built by first selecting 
200 non-Wikipedia-title terms at random from the 
term-label graph generated by the baseline ap-
proach (Formula 3.1), then manually selecting 100 
terms. 
Some sample terms in the term sets are listed in 
Table 3. 
 
                                                          
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/  
5 http://trec.nist.gov/  
6 http://www.wikipedia.org/  
1164
Term 
Set 
Sample Terms 
Wiki200 
Canon EOS 400D, Disease management, El Sal-
vador, Excellus Blue Cross Blue Shield, F33, 
Glasstron, Indium, Khandala, Kung Fu, Lake 
Greenwood, Le Gris, Liriope, Lionel Barrymore, 
Milk, Mount Alto, Northern Wei, Pink Lady, 
Shawshank, The Dog Island, White flight, World 
War II? 
Ext100 
A2B, Antique gold, GPTEngine, Jinjiang Inn, 
Moyea SWF to Apple TV Converter, Nanny ser-
vice, Outdoor living, Plasmid DNA, Popon, Spam 
detection, Taylor Ho Bynum, Villa Michelle? 
Table 3. Sample terms in our term sets 
 
Annotation For each term in the term set, the 
top-5 results (i.e., hypernym labels) of various 
methods are mixed and judged by human annota-
tors. Each annotator assigns each result item a 
judgment of ?Good?, ?Fair? or ?Bad?. The annota-
tors do not know the method by which a result item 
is generated. Six annotators participated in the la-
beling with a rough speed of 15 minutes per term. 
We also encourage the annotators to add new good 
results which are not discovered by any method. 
The term sets and their corresponding user anno-
tations are available for download at the following 
links (dataset ID=data.queryset.semcat01): 
http://research.microsoft.com/en-us/projects/needleseek/ 
http://needleseek.msra.cn/datasets/ 
Evaluation We adopt the following metrics to 
evaluate the hypernym list of a term generated by 
each method. The evaluation score on a term set is 
the average over all the terms. 
Precision@k: The percentage of relevant (good 
or fair) labels in the top-k results (labels judged as 
?Fair? are counted as 0.5) 
Recall@k: The ratio of relevant labels in the top-
k results to the total number of relevant labels 
R-Precision: Precision@R where R is the total 
number of labels judged as ?Good? 
Mean average precision (MAP): The average of 
precision values at the positions of all good or fair 
results 
Before annotation and evaluation, the hypernym 
list generated by each method for each term is pre-
processed to remove duplicate items. Two hyper-
nyms are called duplicate items if they share the 
same head word (e.g., ?military conflict? and ?con-
flict?). For duplicate hypernyms, only the first (i.e., 
the highest ranked one) in the list is kept. The goal 
with such a preprocessing step is to partially con-
sider results diversity in evaluation and to make a 
more meaningful comparison among different 
methods. Consider two hypernym lists for ?sub-
way?: 
List-1: restaurant; chain restaurant; worldwide chain 
restaurant; franchise; restaurant franchise? 
List-2: restaurant; franchise; transportation; company; 
fast food? 
There are more detailed hypernyms in the first 
list about ?subway? as a restaurant or a franchise; 
while the second list covers a broader range of 
meanings for the term. It is hard to say which is 
better (without considering the upper-layer appli-
cations). With this preprocessing step, we keep our 
focus on short hypernyms rather than detailed ones. 
 
Term Set Method MAP R-Prec P@1 P@5 
Wiki200 
Linear 0.357 0.376 0.783 0.547 
Log 
0.371 
 3.92% 
0.384 
 2.13% 
0.803 
 2.55% 
0.561 
 2.56% 
PNorm 
0.372 
 4.20% 
0.384 
 2.13% 
0.800 
 2.17% 
0.562 
 2.74% 
Wiki100H 
Linear 0.363 0.382 0.805 0.627 
Log 
0.393 
 8.26% 
0.402 
 5.24% 
0.845 
 4.97% 
0.660 
 5.26% 
PNorm 
0.395 
 8.82% 
0.403 
 5.50% 
0.840 
 4.35% 
0.662 
 5.28% 
Table 4. Performance comparison among various 
evidence fusion methods (Term sets: Wiki200 and 
Wiki100H; p=2 for PNorm) 
6.2 Experimental results 
We first compare the evaluation results of different 
evidence fusion methods mentioned in Section 4.1. 
In Table 4, Linear means that Formula 3.1 is used 
to calculate label scores, whereas Log and PNorm 
represent our nonlinear approach with Formulas 
4.11 and 4.12 being utilized. The performance im-
provement numbers shown in the table are based 
on the linear version; and the upward pointing ar-
rows indicate relative percentage improvement 
over the baseline. From the table, we can see that 
the nonlinear methods outperform the linear ones 
on the Wiki200 term set. It is interesting to note 
that the performance improvement is more signifi-
cant on Wiki100H, the set of high frequency terms. 
By examining the labels and supporting sentences 
for the terms in each term set, we find that for 
many low-frequency terms (in Wiki100L), there 
are only a few supporting sentences (corresponding 
1165
to one or two patterns). So the scores computed by 
various fusion algorithms tend to be similar. In 
contrast, more supporting sentences can be discov-
ered for high-frequency terms. Much information 
is contained in the sentences about the hypernyms 
of the high-frequency terms, but the linear function 
of Formula 3.1 fails to make effective use of it. 
The two nonlinear methods achieve better perfor-
mance by appropriately modeling the dependency 
between supporting sentences and computing the 
log-probability gain in a better way. 
The comparison of the linear and nonlinear 
methods on the Ext100 term set is shown in Table 
5. Please note that the terms in Ext100 do not ap-
pear in Wikipedia titles. Thanks to the scale of the 
data corpus we are using, even the baseline ap-
proach achieves reasonably good performance. 
Please note that the terms (refer to Table 3) we are 
using are ?harder? than those adopted for evalua-
tion in many existing papers. Again, the results 
quality is improved with the nonlinear methods, 
although the performance improvement is not big 
due to the reason that most terms in Ext100 are 
rare. Please note that the recall (R@1, R@5) in this 
paper is pseudo-recall, i.e., we treat the number of 
known relevant (Good or Fair) results as the total 
number of relevant ones. 
 
Method MAP R-Prec P@1 P@5 R@1 R@5 
Linear 0.384 0.429 0.665 0.472 0.116 0.385 
Log 
0.395 0.429 0.715 0.472 0.125 0.385 
 2.86%  0%  7.52%  0%  7.76%  0% 
PNorm 
0.390 0.429 0.700 0.472 0.120 0.385 
 1.56%  0%   5.26%  0%  3.45%  0% 
Table 5. Performance comparison among various 
evidence fusion methods (Term set: Ext100; p=2 
for PNorm) 
The parameter p in the PNorm method is related 
to the degree of correlations among supporting 
sentences. The linear method of Formula 3.1 corre-
sponds to the special case of p=1; while p=  rep-
resents the case that other supporting sentences are 
fully correlated to the supporting sentence with the 
maximal log-probability gain. Figure 1 shows that, 
for most of the term sets, the best performance is 
obtained for   [2.0, 4.0]. The reason may be that 
the sentence correlations are better estimated with 
p values in this range. 
 
 
Figure 1. Performance curves of PNorm with dif-
ferent parameter values (Measure: MAP) 
The experimental results of evidence propaga-
tion are shown in Table 6. The methods for com-
parison are, 
Base: The linear function without propagation. 
NL: Nonlinear evidence fusion (PNorm with 
p=2) without propagation. 
LP: Linear propagation, i.e., the linear function 
is used to combine the evidence of pseudo support-
ing sentences. 
NLP: Nonlinear propagation where PNorm 
(p=2) is used to combine the pseudo supporting 
sentences. 
NL+NLP: The nonlinear method is used to 
combine both supporting sentences and pseudo 
supporting sentences. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL 
0.372 0.384 0.800 0.562 0.325 
 4.20%  2.13%  2.17%  2.74%  2.52% 
LP 
0.357 0.376 0.783 0.547 0.317 
 0%  0%  0%  0%  0% 
NLP 
0.396 0.418 0.785 0.605 0.357 
 10.9%  11.2%  0.26%  10.6%  12.6% 
NL+NLP 
0.447 0.461 0.840 0.667 0.404 
 25.2%  22.6%  7.28%  21.9%  27.4% 
Table 6. Evidence propagation results (Term set: 
Wiki200; Similarity graph: PB; Nonlinear formula: 
PNorm) 
In this paper, we generate the DS (distributional 
similarity) and PB (pattern-based) graphs by adopt-
ing the best-performed methods studied in (Shi et 
al., 2010). The performance improvement numbers 
(indicated by the upward pointing arrows) shown 
in tables 6~9 are relative percentage improvement 
1166
over the base approach (i.e., linear function with-
out propagation). The values of parameter   are set 
to maximize the MAP values. 
Several observations can be made from Table 6. 
First, no performance improvement can be ob-
tained with the linear propagation method (LP), 
while the nonlinear propagation algorithm (NLP) 
works quite well in improving both precision and 
recall. The results demonstrate the high correlation 
between pseudo supporting sentences and the great 
potential of using term similarity to improve hy-
pernymy extraction. The second observation is that 
the NL+NLP approach achieves a much larger per-
formance improvement than NL and NLP. Similar 
results (omitted due to space limitation) can be 
observed on the Ext100 term set. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL+NLP 
(PB) 
0.415 0.439 0.830 0.633 0.379 
 16.2%  16.8%  6.00%  15.7%  19.6% 
NL+NLP 
(DS) 
0.456 0.469 0.843 0.673 0.406 
 27.7%  24.7%  7.66%  23.0%  28.1% 
NL+NLP
(PB+DS) 
0.473 0.487 0.860 0.700 0.434 
 32.5%  29.5%  9.83%  28.0%  36.9% 
Table 7. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki200; Nonlin-
ear formula: Log) 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.351 0.370 0.760 0.467 0.317 
NL+NLP 
(PB) 
0.411 0.448 0.770 0.564 0.401 
?17.1% ?21.1% ?1.32% ?20.8% ?26.5% 
NL+NLP 
(DS) 
0.469 0.490 0.815 0.622 0.438 
 33.6%  32.4%  7.24%  33.2%  38.2% 
NL+NLP
(PB+DS) 
0.491 0.513 0.860 0.654 0.479 
 39.9%  38.6%  13.2%  40.0%  51.1% 
Table 8. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki100L) 
Now let us study whether it is possible to com-
bine the PB and DS graphs to obtain better results. 
As shown in Tables 7, 8, and 9 (for term sets 
Wiki200, Wiki100L, and Ext100 respectively, us-
ing the Log formula for fusion and propagation), 
utilizing both graphs really yields additional per-
formance gains. We explain this by the fact that the 
information in the two term similarity graphs tends 
to be complimentary. The performance improve-
ment over Wiki100L is especially remarkable. This 
is reasonable because rare terms do not have ade-
quate information in their supporting sentences due 
to data sparseness. As a result, they benefit the 
most from the pseudo supporting sentences propa-
gated with the similarity graphs. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.384 0.429 0.665 0.472 0.385 
NL+NLP 
(PB) 
0.454 0.479 0.745 0.550 0.456 
 18.3%  11.7%  12.0%  16.5%  18.4% 
NL+NLP 
(DS) 
0.404 0.441 0.720 0.486 0.402 
 5.18%  2.66%  8.27%  2.97%  4.37% 
NL+NLP(P
B+DS) 
0.483 0.518 0.760 0.586 0.492 
 26.0%  20.6%  14.3%  24.2%  27.6% 
Table 9. Combination of PB and DS graphs for 
evidence propagation (Term set: Ext100) 
7 Conclusion 
We demonstrated that the way of aggregating sup-
porting sentences has considerable impact on re-
sults quality of the hyponym extraction task using 
lexico-syntactic patterns, and the widely-used 
counting method is not optimal. We applied a se-
ries of nonlinear evidence fusion formulas to the 
problem and saw noticeable performance im-
provement. The data quality is improved further 
with the combination of nonlinear evidence fusion 
and evidence propagation. We also introduced a 
new evaluation corpus with annotated hypernym 
labels for 300 terms, which were shared with the 
research community. 
Acknowledgments 
We would like to thank Matt Callcut for reading 
through the paper. Thanks to the annotators for 
their efforts in judging the hypernym labels. 
Thanks to Yueguo Chen, Siyu Lei, and the anony-
mous reviewers for their helpful comments and 
suggestions. The first author is partially supported 
by the NSF of China (60903028,61070014), and 
Key Projects in the Tianjin Science and Technolo-
gy Pillar Program. 
 
 
 
 
1167
References  
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas-
ca, and A. Soroa. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-based 
Approaches. In Proc. of NAACL-HLT?2009. 
M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proc. of IJCAI?2007. 
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. 
Zhang. 2008. WebTables: Exploring the Power of 
Tables on the Web. In Proceedings of the 34th Con-
ference on Very Large Data Bases (VLDB?2008), 
pages 538?549, Auckland, New Zealand. 
B. Van Durme and M. Pasca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of 
labeled instances for open-domain information ex-
traction. Twenty-Third AAAI Conference on Artifi-
cial Intelligence. 
F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. 
2006. Cluster Generation and Cluster Labelling for 
Web Snippets: A Fast and Accurate Hierarchical So-
lution. In Proceedings of the 13th Conference on 
String Processing and Information Retrieval 
(SPIRE?2006), pages 25?36, Glasgow, Scotland. 
Z. S. Harris. 1985. Distributional Structure. The Philos-
ophy of Linguistics. New York: Oxford University 
Press. 
M. Hearst. 1992. Automatic Acquisition of Hyponyms 
from Large Text Corpora. In Fourteenth International 
Conference on Computational Linguistics, Nantes, 
France. 
Z. Kozareva, E. Riloff, E.H. Hovy. 2008. Semantic 
Class Learning from the Web with Hyponym Pattern 
Linkage Graphs. In Proc. of ACL'2008. 
P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu and 
V. Vyas. 2009. Web-Scale Distributional Similarity 
and Entity Set Expansion. EMNLP?2009. Singapore. 
P. Pantel and D. Ravichandran. 2004. Automatically 
Labeling Semantic Classes. In Proc. of the 2004 Hu-
man Language Technology Conference (HLT-
NAACL?2004), 321?328. 
M. Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search. In Proc. of CIKM?2004. 
M. Pasca. 2010. The Role of Queries in Ranking La-
beled Instances Extracted from Text. In Proc. of 
COLING?2010, Beijing, China. 
S. Shi, B. Lu, Y. Ma, and J.-R. Wen. 2009. Nonlinear 
Static-Rank Computation. In Proc. of CIKM?2009, 
Kong Kong. 
S. Shi, H. Zhang, X. Yuan, J.-R. Wen. 2010. Corpus-
based Semantic Class Mining: Distributional vs. Pat-
tern-Based Approaches. In Proc. of COLING?2010, 
Beijing, China. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypon-
ymy Relations from Web Documents. In Proc. of the 
2004 Human Language Technology Conference 
(HLT-NAACL?2004). 
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Discov-
ery. In Proceedings of the 19th Conference on Neural 
Information Processing Systems. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic 
Taxonomy Induction from Heterogenous Evidence. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics 
(COLING-ACL-06), 801?808. 
P. P. Talukdar and F. Pereira. 2010. Experiments in 
Graph-based Semi-Supervised Learning Methods for 
Class-Instance Acquisition. In 48th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?2010). 
P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran, 
R. Bhagat, and F. Pereira. 2008. Weakly-Supervised 
Acquisition of Labeled Class Instances using Graph 
Random Walks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP?2008), pages 581?589. 
R.C. Wang. W.W. Cohen. Automatic Set Instance Ex-
traction using the Web. In Proc. of the 47th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP?2009), pages 441?449, Sin-
gapore. 
H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. 2009. Em-
ploying Topic Models for Pattern-based Semantic 
Class Discovery. In Proc. of the 47th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL-IJCNLP?2009), pages 441?449, Singapore. 
 
1168
