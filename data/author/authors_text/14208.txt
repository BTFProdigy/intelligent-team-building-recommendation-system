Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1065?1073,
Beijing, August 2010
Semi-supervised dependency parsing using generalized tri-training
Anders S?gaard and Christian Rish?j
Center for Language Technology
University of Copenhagen
{soegaard|crjensen}@hum.ku.dk
Abstract
Martins et al (2008) presented what to
the best of our knowledge still ranks as
the best overall result on the CONLL-
X Shared Task datasets. The paper
shows how triads of stacked dependency
parsers described in Martins et al (2008)
can label unlabeled data for each other in
a way similar to co-training and produce
end parsers that are significantly better
than any of the stacked input parsers.
We evaluate our system on five datasets
from the CONLL-X Shared Task and ob-
tain 10?20% error reductions, incl. the
best reported results on four of them.
We compare our approach to other semi-
supervised learning algorithms.
1 Introduction
Semi-supervised learning of structured variables
is a difficult problem that has received consid-
erable attention recently, but most results have
been negative (Abney, 2008). This paper uses
stacked learning (Wolpert, 1992) to reduce struc-
tured variables, i.e. dependency graphs, to multi-
nomial variables, i.e. attachment and labeling
decisions, which are easier to manage in semi-
supervised learning scenarios, and which can
later be combined into dependency trees using
parsing algorithms for arc-factored dependency
parsing. Our approach thus combines ensemble-
based methods and semi-supervised learning.
Ensemble-based methods such as stacked
learning are used to reduce the instability of clas-
sifiers, to average out their errors and to com-
bine the strengths of diverse learning algorithms.
Ensemble-based methods have attracted a lot of
attention in dependency parsing recently (Sagae
and Lavie, 2006; Hall et al, 2007; Nivre and
McDonald, 2008; Martins et al, 2008; Fishel
and Nivre, 2009; Surdeanu and Manning, 2010).
Nivre and McDonald (2008) were first to intro-
duce stacking in the context of dependency pars-
ing.
Semi-supervised learning is typically moti-
vated by data sparseness. For many classifi-
cation tasks in natural language processing, la-
beled data can be in short supply but unla-
beled data is more readily available. Semi-
supervised methods exploit unlabeled data in ad-
dition to labeled data to improve performance
on classification tasks. If the predictions of a
learner l on unlabeled data are used to improve
a learner l? in semi-supervised learning, the ro-
bustness of learning will depend on the stabil-
ity of l. Combining ensemble-based and semi-
supervised methods may thus lead to more ro-
bust semi-supervised learning.
Ensemble-based and semi-supervised meth-
ods are some of the areas that receive most at-
tention in machine learning today, but relatively
little attention has been given to combining these
methods (Zhou, 2009). Semi-supervised learn-
ing algorithms can be categorized with respect
to the number of views, i.e. the number of fea-
ture sets, and the number of learners used to in-
form each other (Hady and Schwenker, 2008).
Self-training and expectation maximization are
perhaps the best known semi-supervised learn-
ing algorithms (Abney, 2008). They are both
single-view and single-learner algorithms. Since
there is thus only a single perspective on data,
1065
selecting unlabeled data points with predictions
is a difficult task. There is an imminent danger
that the learner amplifies its previous mistakes,
and while several techniques such as balancing
and throttling have been developed to avoid such
caveats, using single-view and single-learner al-
gorithms often requires both caution and experi-
ence with the modeling task at hand.
Algorithms with multiple views on data are
known to be more robust. This insight led to the
development of co-training (Blum and Mitchell,
1998), a two-view method where views inform
each other, but it also paved the way for the inte-
gration of ensemble-based and semi-supervised
methods, i.e. for methods with multiple learners.
It was mentioned that relatively little work has
been devoted to this topic, but there are notable
exceptions:
Bennett et al (2003) generalized boosting to
semi-supervised learning in a seminal paper,
where the idea of iterative or recursive ensembles
was also introduced. Li and Zhou (2005) intro-
duce tri-training, a form of co-training that trains
an ensemble of three learners on labeled data and
runs them on unlabeled data. If two learners
agree on their labeling of a data point, the data
point is added to the labeled data of the third
learner with the prediction of the first two. Di-
daci and Roli (2006) extend self-training and co-
training to multiple learners. Li and Zhou (2007)
generalize tri-training to larger ensembles of ran-
dom trees. The technique is also known as co-
forests. Hady and Schwenker (2008) general-
ize existing ensemble-based methods for semi-
supervised learning scenarios; in particular they
embed ensembles in a form of co-training that is
shown to maintain the diversity of the ensemble
over time. Milidiu and Duarte (2009) generalize
boosting at start to semi-supervised learning.
This paper applies a generalization of tri-
training to two classification problems, attach-
ment and labeling. The attachment classifier?s
weights are used for arc-factored dependency
parsing, and the labeling classifier?s weights are
then used to label the dependency tree delivered
by the parser.
Semi-supervised dependency parsing has at-
tracted a lot of attention recently (Koo et al,
2008; Wang et al, 2008; Suzuki et al, 2009),
but there has, to the best of our knowledge, been
no previous attempts to apply tri-training or re-
lated combinations of ensemble-based and semi-
supervised methods to any of these tasks, ex-
cept for the work of Sagae and Tsujii (2007)
discussed in Sect. 2.6. However, tri-training
has been applied to Chinese chunking (Chen et
al., 2006), question classification (Nguyen et al,
2008) and POS tagging (S?gaard, 2010).
We compare generalized tri-training to other
semi-supervised learning algorithms, incl. self-
training, the original tri-training algorithm based
on bootstrap samples (Li and Zhou, 2005),
co-forests (Li and Zhou, 2007) and semi-
supervised support vector machines (Sindhwani
and Keerthi, 2006).
Sect. 2 introduces dependency parsing and
stacked learning. Stacked learning is general-
ized to dependency parsing, and previous work is
briefly surveyed. We then describe how stacked
dependency parsers can be further stacked as in-
put for two end classifiers that can be combined
to produce dependency structures. These two
classifiers will learn multinomial variables (at-
tachment and labeling) from a combination of
labeled data and unlabeled data using a gener-
alization of tri-training. Sect. 3 describes our ex-
periments. We describe the data sets, and how
the unlabeled data was prepared. Sect. 4 presents
our results. Sect. 5 presents an error analysis and
discusses the results in light of other results in
the literature, and Sect. 6 concludes the paper.
2 Background and related work
2.1 Dependency parsing
Dependency parsing models a sentence as a tree
where words are vertices and grammatical func-
tions are directed edges (dependencies). Each
word thus has a single incoming edge, except
one called the root of the tree. Dependency pars-
ing is thus a structured prediction problem with
trees as structured variables. Each sentence has
exponentially many possible dependency trees.
Our observed variables are sentences with words
labeled with part-of-speech tags. The task for
1066
each sentence is to find the dependency tree that
maximizes an objective function which in our
case is learned from a combination of labeled
and unlabeled data.
More formally, a dependency tree for a
sentence x = w1, . . . , wn is a tree T =
?{0, 1, . . . , n}, A? with A ? V ? V the set of
dependency arcs. Each vertex corresponds to
a word in the sentence, except 0 which is the
root vertex, i.e. for any i ? n ?i, 0? 6? A.
Since a dependency tree is a tree it is acyclic.
A tree is projective if every vertex has a continu-
ous projection, i.e. if and only if for every arc
?i, j? ? A and node k ? V , if i < k < j
or j < k < i then there is a subset of arcs
{?i, i1?, ?i1, i2?, . . . , ?ik?1, ik?} ? A such that
ik = k.
In this paper we use a maximum spanning tree
algorithm, the so-called Chu-Liu-Edmonds algo-
rithm (CLE) (Edmonds, 1967) to turn the pre-
dictions of our semi-supervised classifiers into a
dependency tree.
2.2 Stacked learning
Stacked generalization, or simply stacking, was
first proposed by Wolpert (1992). Stacking is an
ensemble-based learning method where multiple
weak classifiers are combined in a strong end
classifier. The idea is to train the end classifier
directly on the predictions of the input classifiers.
Say each input classifier ci with 1 ? i ?
n receives an input x and outputs a prediction
ci(x). The end classifier then takes as input
?x, c1(x), . . . , cn(x)? and outputs a final predic-
tion c0(?x, c1(x), . . . , cn(x)?). Training is done
by cross-validation. In sum, stacking is training
a classifier on the output of classifiers.
2.3 Stacked dependency parsing
Stacked learning can be generalized to structured
prediction tasks such as dependency parsing. Ar-
chitectures for stacking dependency parsers typi-
cally only use one input parser, but otherwise the
intuition is the same: the input parser is used to
augment the dependency structures that the end
parser is trained and evaluated on.
Nivre and McDonald (2008) first showed how
the MSTParser (McDonald et al, 2005) and the
MaltParser (Nivre et al, 2007) could be im-
proved by stacking each parser on the predic-
tions of the other. Martins et al (2008) general-
ized their work, considering more combinations
of parsers, and stacking the end parsers on non-
local features from the predictions of the input
parser, e.g. siblings and grand-parents. In this
work we use three stacked dependency parsers
for each language: mst2 (p1), malt/mst2 (p2) and
malt/mst1 (p3).
The notation ?malt/mst2? means that the
second-order MSTParser has been stacked on the
MaltParser. The capital letters refer to feature
configurations. Configuration D stacks a level 1
parser on several (non-local) features of the pre-
dictions of the level 0 parser (along with the in-
put features): the predicted edge, siblings, grand
parents and predicted head of candidate modifier
if predicted edge is 0. Configuration E stacks
a level 1 parser on the features in configuration
D and all the predicted children of the candi-
date head. The chosen parser configurations are
those that performed best in Martins et al (2008)
across the different datasets.
2.4 Stacking stacked dependency parsing
The input features of the input classifiers in
stacked learning x can of course be removed
from the input of the end classifier. It is also
possible to stack stacked classifiers. This leaves
us with four strategies for recursive stacking;
namely to constantly augment the feature set,
with level n classifiers trained on the predictions
of the classifiers at all n? 1 lower levels with or
without the input features x, or simply to train a
level n classifier on the predictions of the level
n? 1 classifiers with or without x.
In this work we stack stacked dependency
parsers by training classifiers on the output of
three stacked dependency parsers and POS tags.
Consequently, we use one of the features from x.
Note that we train classifiers and not parsers on
this new level 2.
The reduction is done the following way: First
we train a classifier on the relative distance from
a word to its head to induce attachments. For
example, we may obtain the following features
from the predictions of our level 1 parsers:
1067
label p1 p2 p3 POS
1 1 -1 1 NNP
0 0 0 0 VBD
In the second row all input parsers, p1?3 in
columnsaa 2?4, agree that the verb is the root of
the sentence. Column 1 tells us that this is cor-
rect. In the first row, two out of three parsers
agree on attaching the noun to the verb, which
again is correct. We train level 2 classifiers on
feature vectors produced this way. Note that or-
acle performance of the ensemble is no upper
bound on the accuracy of a classifier trained on
level 1 predictions this way, since a classifier
may learn the right decision from three wrong
predictions and a POS tag.
Second we train a classifier to predict depen-
dency relations. Our feature vectors are similar
to the ones just described, but now contain de-
pendency label predictions, e.g.:
label p1 p2 p3 POS
SBJ SBJ SBJ SBJ NN
ROOT ROOT ROOT COORD VBN
2.5 Generalized tri-training
Tri-training was originally introduced in Li and
Zhou (2005). The method involves three learners
that inform each other.
Let L denote the labeled data and U the
unlabeled data. Assume that three classifiers
c1, c2, c3 have been trained on L. In the origi-
nal algorithm, the three classifiers are obtained
by applying the same learning algorithm to three
bootstrap samples of the labeled data; but in gen-
eralized algorithms, three different learning al-
gorithms are used. An unlabeled datapoint in
U is labeled for a classifier, say c1, if the other
two classifiers agree on its label, i.e. c2 and c3.
Two classifiers inform the third. If the two clas-
sifiers agree on a labeling, we assume there is a
good chance that they are right. In the original
algorithm, learning stops when the classifiers no
longer change; in generalized tri-training, a fixed
stopping criterion is used. The three classifiers
are combined by voting. Li and Zhou (2005)
show that under certain conditions the increase
in classification noise rate is compensated by the
amount of newly labeled data points.
The most important condition is that the
three classifiers are diverse. If the three clas-
1: for i ? {1..3} do
2: ci ? train classifier (li, L)
3: end for
4: repeat
5: for i ? {1..3} do
6: for x ? U do
7: Li ? ?
8: if cj(x) = ck(x)(j, k 6= i) then
9: Li ? Li ? {(x, cj(x)}
10: end if
11: end for
12: ci ? train classifier(li, L ? Li)
13: end for
14: until stopping criterion is met
15: apply c1
Figure 1: Generalized tri-training.
sifiers are identical, tri-training degenerates to
self-training. As already mentioned, Li and
Zhou (2005) obtain this diversity by training
classifiers on bootstrap samples. In their exper-
iments, they consider classifiers based on deci-
sion trees, BP neural networks and na??ve Bayes
inference.
In this paper we generalize the tri-training al-
gorithm and use three different learning algo-
rithms rather than bootstrap samples to create
diversity: a na??ve Bayes algorithm (no smooth-
ing), random forests (Breiman, 2001) (with 100
unpruned decision trees) and an algorithm that
induces unpruned decision trees. The overall al-
gorithm is sketched in Figure 1 with li a learning
algorithm.
Our weights are those of the random forest
classifier after a fixed number of rounds. The
attachment classifier iterates once over the unla-
beled data, while the dependency relations clas-
sifier uses three iterations. The optimal number
of iterations could of course be estimated on de-
velopment data instead. Given the weights for an
input sentence we use CLE to find its most likely
dependency tree.
2.6 Related work
This paper uses stacking rather than voting to
construct ensembles, but voting has been more
1068
widely used in dependency parsing than stack-
ing. Voting was first introduced in dependency
parsing in Zeman and Zabokrtsky (2005). Sagae
and Lavie (2006) later used weighted voting and
reparsing, i.e. using CLE to find the dependency
tree that reflects the maximum number of votes.
They also showed that binning the vote over
part-of-speech tags led to further improvements.
This set-up was adopted by Hall et al (2007) in
the best performing system in the CONLL 2007
Shared Task. Fishel and Nivre (2009) later ex-
perimented with binning the vote on other fea-
tures with modest improvements.
Semi-supervised dependency parsing has only
recently been explored, and failures have been
more frequent than successes. There are,
however, noteable exceptions such as Koo et
al. (2008), Wang et al (2008), Suzuki et
al. (2009) and Sagae and Gordon (2009).
The semi-supervised methods employed in
these experiments are very different from more
traditional scenarios such as self-training and co-
training. Two approaches (Koo et al, 2008;
Sagae and Gordon, 2009) use clusters obtained
from large amounts of unlabeled data to augment
their labeled data by introducing new features,
and two approaches (Wang et al, 2008; Suzuki et
al., 2009) combine probability distributions ob-
tained from labeled data with probability distri-
butions obtained from unlabeled data.
Successes with self-training and co-training
are rare, and several authors report negative re-
sults, e.g. Spreyer and Kuhn (2009). A note-
able exception in constituent-based parsing is the
work of McClosky et al (2006) who show that
self-training is possible if a reranker is used to
inform the underlying parser.
Sagae and Tsujii (2007) participated in (and
won) the CONLL 2007 Shared Task on do-
main adaptation. They first trained a max-
imum entropy-based transition-based depen-
dency parser on the out-of-domain labeled data
and an SVM-based transition-based dependency
parser on the reversed out-of-domain labeled
data. The two parsers parse the in-domain la-
beled data (reversed, in the case of the SVM-
based parser). Identical analyses are added to the
original training set. The first parser is retrained
and used to parse the test data. In sum, the au-
thors do one round of co-training with the fol-
lowing selection criterion: If the two parsers pro-
duce the same dependency structures for a sen-
tence, the dependency structure is added to the
labeled data. This criterion is also the selection
criterion in tri-training.
3 Experiments
3.1 Data
We use five datasets from the CONLL-X Shared
Task (Buchholz and Marsi, 2006).1 Lemmas and
morphological features (FEATS) are ignored,
since we only add POS and CPOS tags to un-
labeled data. For German and Swedish, we
use 100,000 sentences from the Leipzig Corpora
Collection (Biemann et al, 2007) as unlabeled
data. For Danish, Dutch, and Portuguese we
use 100,000 sentences from the Europarl cor-
pus (Koehn, 2005). The data characteristics are
provided in Figure 2. The unlabeled data were
POS tagged using the freely available SVMTool
(Gimenez and Marquez, 2004) (model 4, left-
right-left).
3.2 Algorithm
Once our data has been prepared, we train the
stacked dependency parsers and use them to la-
bel training data for our classifiers (?4,000 to-
kens), our test data and our unlabeled data. This
gives us three sets of predictions for each of the
three data sets. Using the features described in
Sect. 2.4 we then construct data for training our
two triads of classifiers (for attachment and de-
pendency relations). The entire architecture can
be depicted as in Figure 3.
We first stack three dependency parsers as
described in Martins et al (2008). We then
stack three classifiers on top of these dependency
parsers (and POS tags): a na??ve Bayes classifier,
a random forest, and a decision tree. Finally,
1The CONLL-X Shared Task consists of 12 datasets,
but we did not have consistently tokenized unlabeled data
for Arabic, Chinese, Japanese, Slovene and Turkish. Mar-
tins et al (2008) ignore Czech. Our experiment with the
Spanish dataset crashed unexpectedly. We will post results
on the website as soon as possible.
1069
tokens sents tokens/sents POSs DEPRELs
Danish train 94,386 5,190 18.2 24 52
unl (Europarl) 2,422,144 100,000 24.2 - -
test 5,852 322 18.2 - -
Dutch train 195,069 13,349 14.6 13 26
unl (Europarl) 2,336,176 100,000 23.4 - -
test 5,585 386 14.5 - -
German train 699,610 39,216 17.8 52 46
unl (LCC) 1,763,281 100,000 17.6 - -
test 5,694 357 15.9 - -
Portuguese train 206,678 9,071 22.3 21 55
unl (Europarl) 2,882,967 100,000 28.8 - -
test 5,867 288 22.8 - -
Swedish train 191,467 11,042 17.4 37 56
unl (LCC) 1,727,068 100,000 17.3 - -
test 5,656 389 14.5 - -
Figure 2: Characteristics of the data sets.
tri-training
.
.
.
nb forests tree
stacking
mst2/mst2 malt/mst2 malt/mst1
stacking
mst2 malt mst1
Figure 3: Tri-training stacked classifiers.
we tri-train these three stacked classifiers and for
each test sentence output the weights provided
by the random forest classifier. These weights
are used to find the best possible dependency tree
using CLE.
3.3 Baselines
The best of the stacked input parsers is of course
our natural baseline.
Since we have generalized tri-training, we
also compare generalized tri-training to the orig-
inal tri-training algorithm based on bootstrap
samples. The original tri-training algorithm
is run with the same decomposition and the
same features as our generalized tri-training al-
gorithm. We use the learning algorithm orig-
inally used in Li and Zhou (2005), namely
C4.5. We also compare our results to self-
training (no pool, no growth rate) and co-forests
(Li and Zhou, 2007). Finally, we compare our
results to semi-supervised support vector ma-
chines (S3VMs) (Sindhwani and Keerthi, 2006).
Since S3VMs produce binary classifiers, and
one-vs.-many combination would be very time-
consuming, we train a binary classifier that pro-
duces a probability that any candidate arc is cor-
rect and do greedy head selection. We optimized
the feature set and included a total of seven fea-
tures (head POS, dependent POS, dependent left
neighbor POS, distance+direction, predictions of
the three classifiers).
4 Results
Our results are presented in Figure 4. Labeled
(LAS) and unlabeled attachment scores (UAS)
and labeling accuracy (LA) are defined as usual
and include punctuation signs unless otherwise
noted. Difference (?) in LAS, error reduction
and p-value compare our results to the best input
stacked parser (malt/mst2, excerpt for Swedish).
Generalized tri-training (tri-training-CLE),
i.e. using CLE to find the best well-formed de-
pendency trees given the weights provided by
our tri-trained random forest classifier, leads to
highly significant improvements on all data sets
(p < 0.001) with an average error reduction of
14,9%. The results for the other semi-supervised
learning algorithms are presented in Figure 5.
We only used 10% of the unlabeled data (10k
sentences) in this experiment and only did un-
labeled parsing, but it is quite evident that these
learning strategies seem less promising than gen-
1070
Danish LAS(%) UAS(%) LA(%) EM(%) ? LAS err.red(%) p-value
mst2 84.64 89.11 91.35 24.84
malt/mst2 86.36 90.50 92.09 27.64
malt/mst1 86.11 90.23 91.87 25.78
tri-training-CLE 87.76 92.11 92.87 27.95 1.40 10.26 <0.0001
tri-training-CLE (excl. pnc.) 87.54 92.61 91.68
CONLL-X best (excl. pnc.) 84.79 90.58 89.22
Martins et al (excl. pnc.) 86.79 - -
Dutch
mst2 80.27 84.32 84.96 23.32
malt/mst2 81.00 84.58 85.46 24.35
malt/mst1 80.72 84.17 85.34 26.17
tri-training-CLE 83.42 88.18 87.82 28.00 2.42 12.74 <0.0001
tri-training-CLE (excl. pnc.) 81.73 86.97 86.61
CONLL-X best (excl. pnc.) 79.19 83.57 83.89
Martins et al (excl. pnc.) 81.61 - -
German
mst2 87.32 89.88 93.05 35.85
malt/mst2 88.06 90.53 93.52 40.06
malt/mst1 88.04 90.50 93.48 38.10
tri-training-CLE 90.41 93.22 94.61 43.14 2.35 19.68 <0.0001
tri-training-CLE (excl. pnc.) 90.30 93.49 93.87
CONLL-X best (excl. pnc.) 87.34 90.38 92.11
Martins et al (excl. pnc.) 88.66 - -
Portuguese
mst2 84.83 88.44 92.04 25.69
malt/mst2 85.39 88.80 92.59 28.13
malt/mst1 85.00 88.39 92.23 25.69
tri-training-CLE 88.03 91.89 93.54 29.86 2.64 18.07 <0.0001
tri-training-CLE (excl. pnc.) 89.18 93.69 92.43
CONLL-X best (excl. pnc.) 87.60 91.36 91.54
Martins et al (excl. pnc.) 88.46 - -
Swedish
mst2 81.82 87.36 87.29 27.76
malt/mst2 84.42 89.57 88.68 31.62
malt/mst1 84.74 89.83 89.07 31.11
tri-training-CLE 86.83 92.04 90.65 32.65 2.09 13.70 <0.0001
tri-training-CLE (excl. pnc.) 86.66 92.45 89.58
CONLL-X best (excl. pnc.) 84.58 89.50 87.39
Martins et al (excl. pnc.) 85.16 - -
AV 2.18 14.89
Figure 4: Results on CONLL-X datasets. Scores are including punctuation unless otherwise noted.
? and p-value is difference with respect to best input parser.
UAS malt-mst2 S3VMs self-training orig-tri-training co-forests tri-training tri-training[full]
Danish 90.50 90.47 89.68 89.66 88.79 90.60 92.21
Dutch 84.58 85.34 84.06 83.83 83.97 86.07 88.06
German 90.53 90.15 89.83 89.92 88.47 90.81 93.20
Portuguese 88.80 65.64 87.60 87.62 87.06 89.16 91.87
Swedish 89.83 81.46 89.09 89.20 88.65 90.22 92.24
AV 88.80 82.61 88.05 88.05 87.44 89.37 91.52
Figure 5: Comparison of different semi-supervised learning algorithms (10% of unlabeled data)
using 2-fold CV and no reparsing, UAS including punctuation.
1071
eralized tri-training.
5 Error analysis and discussion
Error reductions are higher with dependencies
to the root node and long distance dependencies
than with local dependencies. The table below
lists the labeled attachment F1-scores for the five
datasets binned on dependency length. The av-
erage error reduction is the same for root depen-
dencies and long distance dependencies (length
>7), but significantly lower for local dependen-
cies. This seems to indicate that large amounts of
data are necessary for the parser to recover long
distance dependencies.
root 1 2 4?7 >7
Da(F1) 98.45 96.21 92.09 88.17 90.93
? err.red 41.34 10.69 13.92 15.75 21.92
Du(F1) 83.65 94.47 88.60 82.40 81.54
? err.red 28.39 16.74 20.72 17.00 31.88
Ge(F1) 97.33 96.47 94.28 92.42 93.94
? err.red 26.65 19.77 17.46 25.25 38.97
Po(F1) 96.23 97.05 95.17 84.80 87.11
? err.red 22.47 19.56 24.86 22.56 26.97
Sw(F1) 96.37 95.67 93.46 88.42 89.57
? err.red 32.85 14.10 15.04 25.97 31.50
AV err.red 30.34 16.17 18.40 21.31 30.25
Our results for Danish, Dutch, German and
Portuguese are to the best of our knowledge the
best reported results in the literature. Zhang and
Chan (2009) obtain a LAS of 87.20 for Swedish
with transition-based parsing based on reinforce-
ment learning. They evaluate their system on
a subset of the CONLL-X datasets and obtain
their (by far) best improvement on the Swedish
dataset. They speculate that ?the reason might
be that [long distance dependencies] are not pop-
ular in Swedish?. Since our parser is particu-
larly good at long distance dependencies, this
may also explain why a supervised parser outper-
forms our system on this dataset. Interestingly,
our unlabeled attachment score is a lot better
than the one reported by Zhang and Chan (2009),
namely 92.45 compared to 91.84.
Generally, our UASs are better than our LASs.
Since we separate attachment and labeling out
in two independent steps, improvements in UAS
and improvements in LA do not necessarily lead
to improvements in LAS. While our average er-
ror reduction in LAS is 14.9%, our average error
reductions in UAS is 23.6%. The average error
reduction in LA is 14.0%. In two-stage depen-
dency parsers or dependency parsers with joint
models, improvements in UAS are typically fol-
lowed by comparable improvements in LAS.
6 Conclusion
This paper showed how the stacked depen-
dency parsers introduced in Martins et al (2008)
can be improved by inference from unlabeled
data. Briefly put, we stack three diverse clas-
sifiers on triads of stacked dependency parsers
and let them label unlabeled data for each
other in a co-training-like architecture. Our
average error reductions in LAS over the best
of our stacked input parsers is 14.9%; in
UAS, it is 23.6%. The code is available at
http://cst.dk/anders/tridep.html.
References
Abney, Steven. 2008. Semi-supervised learning for
computational linguistics. Chapman & Hall.
Bennett, Kristin, Ayhan Demiriz, and Richard
Maclin. 2003. Exploiting unlabeled data in en-
semble methods. In KDD.
Biemann, Chris, G. Heyer, U. Quasthoff, and
M. Richter. 2007. The Leipzig corpora collection.
In Corpus Linguistics.
Blum, Avrim and Tom Mitchell. 1998. Combining
labeled and unlabeled with-co-training. In COLT.
Breiman, Leo. 2001. Random forests. Machine
Learning, 45:5?32.
Buchholz, Sabine and Erwin Marsi. 2006. CONLL-
X shared task on multilingual dependency parsing.
In CONLL.
Chen, Wenliang, Yujie Zhang, and Hitoshi Isahara.
2006. Chinese chunking with tri-training learning.
In Computer processing of oriental languages,
pages 466?473. Springer, Berlin, Germany.
Didaci, Luca and Fabio Roli. 2006. Using co-
training and self-training in semi-supervised mul-
tiple classifier systems. In SSPR& SPR, pages
522?530. Springer, Berlin, Germany.
Edmonds, J. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71:233?240.
1072
Fishel, Mark and Joakim Nivre. 2009. Voting and
stacking in data-driven dependency parsing. In
NODALIDA.
Gimenez, Jesus and Lluis Marquez. 2004. SVM-
Tool: a general POS tagger generator based on
support vector machines. In LREC.
Hady, Mohamed and Friedhelm Schwenker. 2008.
Co-training by committee. International Journal
of Software and Informatics, 2:95?124.
Hall, Johan, Jens Nilsson, Joakim Nivre, Gulsen
Eryigit, Beata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended? In
CONLL.
Koehn, Philipp. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT-Summit.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In ACL.
Li, Ming and Zhi-Hua Zhou. 2005. Tri-training:
exploiting unlabeled data using three classifiers.
IEEE Transactions on Knowledge and Data En-
gineering, 17(11):1529?1541.
Li, Ming and Zhi-Hua Zhou. 2007. Improve
computer-aided diagnosis with machine learning
techniques using undiagnosed samples. IEEE
Transactions on Systems, Man and Cybernetics,
37(6):1088?1098.
Martins, Andre?, Dipanjan Das, Noah Smith, and Eric
Xing. 2008. Stacking dependency parsers. In
EMNLP.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov,
and Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT-
EMNLP.
Milidiu, Ruy and Julio Duarte. 2009. Improv-
ing BAS committee performance with a semi-
supervised approach. In European Symposium on
Artificial Neural Networks.
Nguyen, Tri, Le Nguyen, and Akira Shimazu. 2008.
Using semi-supervised learning for question clas-
sification. Journal of Natural Language Process-
ing, 15:3?21.
Nivre, Joakim and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In ACL-HLT.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser.
Natural Language Engineering, 13(2):95?135.
Sagae, Kenji and Andrew Gordon. 2009. Cluster-
ing words by syntactic similarity improves depen-
dency parsing of predicate-argument structures. In
IWPT.
Sagae, Kenji and Alon Lavie. 2006. Parser combina-
tion by reparsing. In HLT-NAACL.
Sagae, Kenji and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In EMNLP-CONLL.
Sindhwani, Vikas and Sathiya Keerthi. 2006. Large
scale semi-supervised linear SVMs. In ACM SI-
GIR.
S?gaard, Anders. 2010. Simple semi-supervised
training of part-of-speech taggers. In ACL.
Spreyer, Kathrin and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using in-
complete and noisy training data. In CONLL.
Surdeanu, Mihai and Christopher Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In NAACL.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. Semi-supervised convex
training for dependency parsing. In EMNLP.
Wang, Qin, Dekang Lin, and Dale Schuurmans.
2008. Semi-supervised convex training for depen-
dency parsing. In ACL.
Wolpert, David. 1992. Stacked generalization. Neu-
ral Networks, 5:241?259.
Zeman, Daniel and Zdene?k ?Zabokrtsky?. 2005. Im-
proving parsing accuracy by combining diverse
dependency parsers. In IWPT.
Zhang, Lidan and Kwok Chan. 2009. Dependency
parsing with energy-based reinforcement learning.
In IWPT.
Zhou, Zhi-Hua. 2009. When semi-supervised learn-
ing meets ensemble learning. In MCS.
1073
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 29?32,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Shared task system description:
Frustratingly hard compositionality prediction
Anders Johannsen, Hector Martinez Alonso, Christian Rish?j and Anders S?gaard
Center for Language Technology
University of Copenhagen
{ajohannsen|alonso|crjensen|soegaard}@hum.ku.dk
Abstract
We considered a wide range of features for
the DiSCo 2011 shared task about composi-
tionality prediction for word pairs, including
COALS-based endocentricity scores, compo-
sitionality scores based on distributional clus-
ters, statistics about wordnet-induced para-
phrases, hyphenation, and the likelihood of
long translation equivalents in other lan-
guages. Many of the features we considered
correlated significantly with human compo-
sitionality scores, but in support vector re-
gression experiments we obtained the best re-
sults using only COALS-based endocentric-
ity scores. Our system was nevertheless the
best performing system in the shared task, and
average error reductions over a simple base-
line in cross-validation were 13.7% for En-
glish and 50.1% for German.
1 Introduction
The challenge in the DiSCo 2011 shared task is to
estimate and predict the semantic compositionality
of word pairs. Specifically, the data set consists of
adjective-noun, subject-verb and object-verb pairs
in English and German. The organizers also pro-
vided the Wacky corpora for English and German
with lowercased lemmas.1 In addition, we also ex-
perimented with wordnets and using Europarl cor-
pora for the two languages (Koehn, 2005), but none
of the features based on these resources were used
in the final submission.
Semantic compositionality is an ambiguous term
in the linguistics litterature. It may refer to the po-
sition that the meaning of sentences is built from
1http://wacky.sslmit.unibo.it/
the meaning of its parts through very general prin-
ciples of application, as for example in type-logical
grammars. It may also just refer to a typically not
very well defined measure of semantic transparency
of expressions or syntactic constructions, best illus-
trated by examples:
(1) pull the plug
(2) educate people
The verb-object word pair in example (1) is in
the training data rated as much less compositional
than example (2). The intuition is that the mean-
ing of the whole is less related to the meaning of
the parts. The compositionality relation is not de-
fined more precisely, however, and this may in part
explain why compositionality prediction seems frus-
tratingly hard.
2 Features
Many of our features were evaluated with different
amounts of slop. The slop parameter permits non-
exact matches without resorting to language-specific
shallow patterns. The words in the compounds are
allowed to move around in the sentence one position
at a time. The value of the parameter is the maxi-
mum number of steps. Set to zero, it is equivalent
to an exact match. Below are a couple of example
configurations. Note that in order for w1 and w2 to
swap positions, we must have slop > 1 since slop=1
would place them on top of each other.
x x w1 w2 x x (slop=0)
x x w1 x w2 x (slop=1)
x x w1 x x w2 (slop=2)
x x w2 w1 x x (slop=2)
29
2.1 LEFT-ENDOC, RIGHT-ENDOC and
DISTR-DIFF
These features measure the endocentricity of a word
pair w1 w2. The distribution of w1 is likely to be
similar to the distribution of ?w1 w2? if w1 is the
syntactic head of ?w1 w2?. The same is to be ex-
pected for w2, when w2 is the head.
Syntactic endocentricity is related to composi-
tionality, but the implication is one-way only. A
highly compositional compound is endocentric, but
an endocentric compound need not be highly com-
positional. For example, the distribution of ?olive
oil?, which is endocentric and highly compositional,
is very similar to the distribution of ?oil?, the head
word. On the other hand, ?golden age? which is
ranked as highly non-compositional in the training
data, is certainly endocentric. The distribution of
?golden age? is not very different from that of ?age?.
We used COALS (Rohde et al, 2009) to cal-
culate word distributions. The COALS algorithm
builds a word-to-word semantic space from a cor-
pus. We used the implementation by Jurgens and
Stevens (2010), generating the semantic space from
the Wacky corpora for English and German with du-
plicate sentences removed and low-frequency words
substituted by dummy symbols. The word pairs
have been fed to COALS as compounds that have to
be treated as single tokens, and the semantic space
has been generated and reduced using singular value
decompositon. The vectors for w1, w2 and ?w1 w2?
are calculated, and we compute the cosine distance
between the semantic space vectors for the word
pair and its parts, and between the parts themselves,
namely for ?w1 w2? and w1, for ?w1 w2? and w2,
and for w1 and w2, say for ?olive oil? and ?olive?,
for ?olive oil? and ?oil?, and for ?olive? and ?oil?.
LEFT-ENDOC is the cosine distance between the left
word and the compound. RIGHT-ENDOC is the co-
sine distance between the right word and the com-
pound. Finally, DISTR-DIFF is the cosine distance
between the two words, w1 and w2.
2.2 BR-COMP
To accommodate for the weaknesses of syntactic en-
docentricity features, we also tried introducing com-
positionality scores based on hierarchical distribu-
tional clusters that would model semantic composi-
tionality more directly. The scores are referred to
below as BR-COMP (compositionality scores based
on Brown clusters), and the intuition behind these
scores is that a word pair ?w1 w2?, e.g. ?hot dog?, is
non-compositional if w1 and w2 have high colloca-
tional strength, but if w1 is replaced with a different
word w?1 with similar distribution, e.g. ?warm?, then
?w?1 w2? is less collocational. Similarly, if w2 is re-
placed with a different word w?2 with similar distri-
bution, e.g. ?terrier?, then ?w1 w?2? is also much less
collocational than ?w1 w2?.
We first induce a hierarchical clustering of the
words in the Wacky corpora cl : W ? 2W with
W the set of words in our corpora, using publicly
available software.2 Let the collocational strength of
the two words w1 and w2 be G2(w1, w2). We then
compute the average collocational strength of distri-
butional clusters, BR-CS (collocational strength of
Brown clusters):
BR-CS(w1, w2) =
?Nx?cl(w1),x??cl(w2)G
2(x, x?)
N
with N = |cl(w1)| ? |cl(w2)|. We now let
BR-COMP(w1, w2) =
BR-CS(w1,w2)
G2(w1,w2)
.
The Brown clusters were built with C = 1000
and a cut-off frequency of 1000. With these settings
the number of word types per cluster is quite high,
which of course has a detrimental effect on the se-
mantic coherence of the cluster. To counter this we
choose to restrict cl(w) and cl(w?) to include only
the 50 most frequently occurring terms.
2.3 PARAPHR
These features have to do with alternative phrasings
using synonyms from Princeton WordNet 3 and Ger-
maNet4. One word in the compound is held con-
stant while the other is replaced with its synonyms.
The intuition is again that non-compositional com-
pounds are much more frequent than any compound
that results from replacing one of the constituent
words with one of its synonyms. For ?hot dog? we
thus generate ?hot terrier? and ?warm dog?, but not
?warm terrier?. Specifically, PARAPHR?100 means
2http://www.cs.berkeley.edu/?pliang/software/
3http://wordnet.princeton.edu/
4GermaNet Copyright c? 1996, 2008 by University of
Tu?bingen.
30
that at least one of the alternative compounds has
a document count of more than 100 in the cor-
pus. PARAPHRav is the average count for all para-
phrases, PARAPHRsum is the sum of these counts,
and PARAPHRrel is the average count for all para-
phrases over the count of the word pair in question.
2.4 HYPH
The HYPH features were inspired by Bergsma et
al. (2010). It was only used for English. Specif-
ically, we used the relative frequency of hyphen-
ated forms as features. For adjective-noun pairs
we counted the number of hyphenated occurrences,
e.g. ?front-page?, and divided that number by the
number of non-hyphenated occurrences, e.g. ?front
page?. For subject-verb and object-verb pairs, we
add -ing to the verb, e.g. ?information-collecting?,
and divided the number of such forms with non-
hyphenated equivalents, e.g. ?information collect-
ing?.
2.5 TRANS-LEN
The intuition behind our bilingual features is that
non-compositional words typically translate into a
single word or must be paraphrased using multiple
words (circumlocution or periphrasis). TRANS-LEN
is the probability that the phrase?s translation, possi-
bly with intervening articles and markers, is longer
than lmin and shorter than lmax , i.e.:
TRANS-LEN(w1, w2, lmin , lmax ) =
???trans(w1 w2),l1?|? |?l2P (?|w1 w2)
???trans(w1 w2)P (?|w1 w2)
We use English and German Europarl (Koehn,
2005) to train our translation models. In particular,
we use the phrase tables of the Moses PB-SMT sys-
tem5 trained on a lemmatized version of the WMT11
parallel corpora for English and German. Below
TRANS-LEN-n will be the probability of the trans-
lation of a word pair being n or more words. We
also experimented with average translation length as
a feature, but this did not correlate well with seman-
tic compositionality.
5http://statmt.org
feat ?
English German
rel-type = ADJ NN 0.0750 *0.1711
rel-type = V SUBJ 0.0151 **0.2883
rel-type = V OBJ 0.0880 0.0825
LEFT-ENDOC **0.3257 *0.1637
RIGHT-ENDOC **0.3896 0.1379
DISTR-DIFF *0.1885 0.1128
HYPH (5) 0.1367 -
HYPH (5) reversed *0.1829 -
G2 0.1155 0.0535
BR-CS *0.1592 0.0242
BR-COMP 0.0292 0.0024
Count (5) 0.0795 *0.1523
PARAPHR?|w1 w?2| 0.1123 0.1242
PARAPHRrel (5) 0.0906 0.0013
PARAPHRav (1) 0.1080 0.0743
PARAPHRav (5) 0.1313 0.0707
PARAPHRsum (1) 0.0496 0.0225
PARAPHR?100 (1) **0.2434 0.0050
PARAPHR?100 (5) **0.2277 0.0198
TRANS-LEN-1 0.0797 0.0509
TRANS-LEN-2 0.1109 0.0158
TRANS-LEN-3 0.0935 0.0489
TRANS-LEN-5 0.0240 0.0632
Figure 1: Correlations. Coefficients marked with * are
significant (p < 0.05), and coefficients marked with **
are highly significant (p < 0.01). We omit features with
different slop values if they perform significantly worse
than similar features.
3 Correlations
We have introduced five different kinds of features,
four of which are supposed to model semantic com-
positionality directly. For feature selection, we
therefore compute the correlation of features with
compositionality scores and select features that cor-
relate significantly with compositionality. The fea-
tures are then used for regression experiments.
4 Regression experiments
For our regression experiments, we use support vec-
tor regression with a high (7) degree kernel. Other-
wise we use default parameters of publicly available
software.6 In our experiments, however, we were
not able to produce substantially better results than
what can be obtained using only the features LEFT-
ENDOC and RIGHT-ENDOC. In fact, for German
using only LEFT-ENDOC gave slightly better results
than using both. These features are also those that
correlate best with human compositionality scores
according to Figure 1. Consequently, we only use
6http://www.csie.ntu.edu.tw/?cjlin/libsvm/
31
these features in our official runs. Our evaluations
below are cross-validation results on training and de-
velopment data using leave-one-out. We compare
using only LEFT-ENDOC and RIGHT-ENDOC (for
English) with using all significant features that seem
relatively independent. For English, we used LEFT-
ENDOC, RIGHT-ENDOC, DISTR-DIFF, HYPH (5)
reversed, BR-CS, PARAPHR?100 (1). For German,
we used rel-type = ADJ NN, rel-type=V SUBJ and
RIGHT-ENDOC. We only optimized on numeric
scores. The submitted coarse-grained scores were
obtained using average +/- average deviation.7
English German
dev test dev test
BL 18.395 47.123
all sign. indep. 19.22 23.02
L-END+R-END 15.89 16.19 23.51 24.03
err.red (L+R) 0.137 0.501
5 Discussion
Our experiments have shown that the DiSCo 2011
shared task about compositionality prediction was a
tough challenge. This may be because of the fine-
grained compositionality metric or because of in-
consistencies in annotation, but note also that the
syntactically oriented features seem to perform a
lot better than those trying to single out semantic
compositionality from syntactic endocentricity and
collocational strength. For example, LEFT-ENDOC,
RIGHT-ENDOC and BR-CS correlate with compo-
sitionality scores, whereas BR-COMP does not, al-
though it is supposed to model compositionality
more directly. Could it perhaps be that annotations
reflect syntactic endocentricity or distributional sim-
ilarity to a high degree, rather than what is typically
thought of as semantic compositionality?
Consider a couple of examples of adjective-noun
pairs in English in Figure 2 for illustration. These
examples are taken from the training data, but we
have added our subjective judgments about semantic
and syntactic markedness and collocational strength
(peaking at G2 scores). It seems that semantic
markedness is less important for scores than syntac-
7These thresholds were poorly chosen, by the way. Had we
chosen less balanced cut-offs, say 0 and 72, our improved accu-
racy on coarse-grained scores (59.4) would have been compara-
ble to and slightly better than the best submitted coarse-grained
scores (58.5).
sem syn coll score
floppy disk X 61
free kick X 77
happy birthday X X 47
large scale X X 55
old school X X X 37
open source X X 49
real life X 69
small group 91
Figure 2: Subjective judgments about semantic and syn-
tactic markedness and collocational strength.
tic markedness and collocational strength. In partic-
ular, the combination of syntactic markedness and
collocational strength makes annotators rank word
pairs such as happy birthday and open source as
non-compositional, although they seem to be fully
compositional from a semantic perspective. This
may explain why our COALS-features are so predic-
tive of human compositionality scores, and why G2
correlates better with these scores than BR-COMP.
6 Conclusions
In our experiments for the DiSCo 2011 shared task
we have considered a wide range of features and
showed that some of them correlate significantly and
sometimes highly significantly with human compo-
sitionality scores. In our regression experiments,
however, our best results were obtained with only
one or two COALS-based endocentricity features.
We report error reductions of 13.7% for English and
50.1% for German.
References
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz
Kondrak. 2010. Predicting the semantic composition-
ality of prefix verbs. In EMNLP.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space mod-
els. In ACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT-Summit.
Douglas Rohde, Laura Gonnerman, and David Plaut.
2009. An improved model of semantic similarity
based on lexical co-occurrence. In Cognitive Science.
32
