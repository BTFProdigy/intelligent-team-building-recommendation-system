Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 168?173,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Improved Iterative Correction for Distant Spelling Errors
Sergey Gubanov Irina Galinskaya Alexey Baytin
Yandex
16 Leo Tolstoy St., Moscow, 119021 Russia
{esgv,galinskaya,baytin}@yandex-team.ru
Abstract
Noisy channel models, widely used in
modern spellers, cope with typical mis-
spellings, but do not work well with infre-
quent and difficult spelling errors. In this
paper, we have improved the noisy chan-
nel approach by iterative stochastic search
for the best correction. The proposed al-
gorithm allowed us to avoid local minima
problem and improve the F
1
measure by
6.6% on distant spelling errors.
1 Introduction
A speller is an essential part of any program as-
sociated with text input and processing ? e-mail
system, search engine, browser, form editor etc.
To detect and correct spelling errors, the state of
the art spelling correction systems use the noisy
channel approach (Kernighan et al, 1990; Mays
et al, 1991; Brill and Moore, 2000). Its models
are usually trained on large corpora and provide
high effectiveness in correction of typical errors
(most of which consist of 1-2 wrong characters per
word), but does not work well for complex (multi-
character) and infrequent errors.
In this paper, we improved effectiveness of
the noisy channel for the correction of com-
plex errors. In most cases, these are cogni-
tive errors in loan words (folsvagen ? volkswa-
gen), names of drugs (vobemzin ? wobenzym),
names of brands (scatcher? skechers), scientific
terms (heksagidron? hexahedron) and last names
(Shwartzneger ? Schwarzenegger). In all these
cases, the misspelled word contains many errors
and the corresponding error model penalty cannot
be compensated by the LM weight of its proper
form. As a result, either the misspelled word it-
self, or the other (less complicated, more frequent)
misspelling of the same word wins the likelihood
race.
To compensate for this defect of the noisy chan-
nel, the iterative approach (Cucerzan and Brill,
2004) is typically used. The search for the best
variant is repeated several times, what allows cor-
recting rather complex errors, but does not com-
pletely solve the problem of falling into local min-
ima. To overcome this issue we suggest to con-
sider more correction hypotheses. For this pur-
pose we used a method based on the simulated
annealing algorithm. We experimentally demon-
strate that the proposed method outperforms the
baseline noisy channel and iterative spellers.
Many authors employ machine learning to build
rankers that compensate for the drawbacks of the
noisy channel model: (Whitelaw et al, 2009; Gao
et al, 2010). These techniques can be combined
with the proposed method by replacing posterior
probability of single correction in our method with
an estimate obtained via discriminative training
method.
In our work, we focus on isolated word-error
correction (Kukich, 1992), which, in a sense, is a
harder task, than multi-word correction, because
there is no context available for misspelled words.
For experiments we used single-word queries to a
commercial search engine.
2 Baseline speller
2.1 Noisy channel spelling correction
Noisy channel is a probabilistic model that defines
posterior probability P (q
0
|q
1
) of q
0
being the in-
tended word, given the observed word q
1
; for such
model, the optimal decision rule ? is the follow-
ing:
?(q
1
) = argmax
q
0
P (q
0
|q
1
);
P (q
0
|q
1
) ? P
dist
(q
0
? q
1
)P
LM
(q
0
),
(1)
where P
LM
is the source (language) model, and
P
dist
is the error model. Given P (q
0
|q
1
) defined,
to correct the word q
1
we could iterate through
168
all ever-observed words, and choose the one, that
maximizes the posterior probability. However,
the practical considerations demand that we do
not rank the whole list of words, but instead
choose between a limited number of hypotheses
h
1
, ..., h
K
:
1. Given q
1
, generate a set of hypotheses
h
1
, ..., h
K
, such that
K
?
k=1
P (q
0
= h
k
|q
1
) ? 1; (2)
2. Choose the hypothesis h
k
that maximizes
P (q
0
= h
k
|q
1
).
If hypotheses constitute a major part of the poste-
rior probability mass, it is highly unlikely that the
intended word is not among them.
2.2 Baseline speller setup
In baseline speller we use a substring-based error
model P
dist
(q
0
? q
1
) described in (Brill and
Moore, 2000), the error model training method
and the hypotheses generator are similar to (Duan
and Hsu, 2011).
For building language (P
LM
?
) and error (P
dist
?
)
models, we use words collected from the 6-months
query log of a commercial search engine.
Hypotheses generator is based on A* beam
search in a trie of words, and yields K hy-
potheses h
k
, for which the noisy channel scores
P
dist
(h
k
? q
1
)P
LM
(h
k
) are highest possible.
Hypotheses generator has high K-best recall (see
Section 4.2) ? in 91.8% cases the correct hy-
pothesis is found when K = 30, which confirms
the assumption about covering almost all posterior
probability mass (see Equation 2).
3 Improvements for noisy channel
spelling correction
While choosing argmax of the posterior probabil-
ity is an optimal decision rule in theory, in practice
it might not be optimal, due to limitations of the
language and error modeling. For example, vobe-
mzin is corrected to more frequent misspelling
vobenzin (instead of correct form wobenzym) by
the noisy channel, because P
dist
(vobemzin ?
wobenzym) is too low (see Table 1).
There have been attempts (Cucerzan and Brill,
2004) to apply other rules, which would over-
come limitations of language and error models
with compensating changes described further.
c ? logP
dist
? logP
LM
?
vobenzin 2.289 31.75 34.04
wobenzym 12.52 26.02 38.54
Table 1: Noisy-channel scores for two corrections
of vobemzin
3.1 Iterative correction
Iterative spelling correction with E iterations uses
standard noisy-channel to correct the query q re-
peatedly E times. It is motivated by the assump-
tion, that we are more likely to successfully correct
the query if we take several short steps instead of
one big step (Cucerzan and Brill, 2004) .
Iterative correction is hill climbing in the space
of possible corrections: on each iteration we make
a transition to the best point in the neighbourhood,
i.e. to correction, that has maximal posterior prob-
ability P (c|q). As any local search method, itera-
tive correction is prone to local minima, stopping
before reaching the correct word.
3.2 Stochastic iterative correction
A common method of avoiding local minima in
optimization is the simulated annealing algorithm,
key ideas from which can be adapted for spelling
correction task. In this section we propose such an
adaptation. Consider: we do not always transition
deterministically to the next best correction, but
instead transition randomly to a (potentially any)
correction with transition probability being equal
to the posterior P (c
i
|c
i?1
), where c
i?1
is the cor-
rection we transition from, c
i
is the correction we
transition to, and P (?|?) is defined by Equation 1.
Iterative correction then turns into a random walk:
we start at word c
0
= q and stop after E ran-
dom steps at some word c
E
, which becomes our
answer.
To turn random walk into deterministic spelling
correction algorithm, we de-randomize it, using
the following transformation. Described random
walk defines, for each word w, a probability
P (c
E
= w|q) of ending up in w after starting a
walk from the initial query q. With that probability
defined, our correction algorithm is the following:
given query q, pick c = argmax
c
E
P (c
E
|q) as a
correction.
Probability of getting from c
0
= q to some
c
E
= c is a sum, over all possible paths, of prob-
abilities of getting from q to c via specific path
169
q = c
0
? c
1
? ...? c
E?1
? c
E
= c:
P (c
E
|c
0
) =
?
c
1
?W
...
c
E?1
?W
E
?
i=1
P (c
i
|c
i?1
), (3)
P (c
i
|c
i?1
) =
P
dist
(c
i
? c
i?1
)P
LM
(c
i
)
P
observe
(c
i?1
)
, (4)
where W is the set of all possible words, and
P
observe
(w) is the probability of observing w as
a query in the noisy-channel model.
Example: if we start a random walk from vobe-
mzin and make 3 steps, we most probably will end
up in the correct form wobenzym with P = 0.361.
A few of the most probable random walk paths
are shown in Table 2. Note, that despite the fact
that most probable path does not lead to the cor-
rect word, many other paths to wobenzym sum up
to 0.361, which is greater than probability of any
other word. Also note, that the method works only
because multiple misspellings of the same word
are presented in our model; for related research
see (Choudhury et al, 2007).
c
0
? c
1
? c
2
? c
3
P
vobemzin?vobenzin?vobenzin?vobenzin 0.074
vobemzin?vobenzim?wobenzym?wobenzym 0.065
vobemzin?vobenzin?vobenzim?vobenzim 0.052
vobemzin?vobenzim?vobenzim?wobenzym 0.034
vobemzin?wobenzym?wobenzym?wobenzym 0.031
vobemzin?wobenzim?wobenzym?wobenzym 0.028
vobemzin?wobenzyn?wobenzym?wobenzym 0.022
Table 2: Most probable random walk paths start-
ing from c
0
= q = vobemzin (the correct form is
in bold).
Also note, that while Equation 3 uses noisy-
channel posteriors, the method can use an arbitrary
discriminative model, for example the one from
(Gao et al, 2010), and benefit from a more accu-
rate posterior estimate.
3.3 Additional heuristics
This section describes some common heuristic im-
provements, that, where possible, were applied
both to the baseline methods and to the proposed
algorithm.
Basic building block of every mentioned algo-
rithm is one-step noisy-channel correction. Each
basic correction proceeds as described in Sec-
tion 2.1: a small number of hypotheses h
1
, ..., h
K
is generated for the query q, hypotheses are scored,
and scores are recomputed into normalized pos-
terior probabilities (see Equation 5). Posterior
probabilities are then either used to pick the best
correction (in baseline and simple iterative cor-
rection), or are accumulated to later compute the
score defined by Equation 3.
score(h
i
) = P
dist
(h
i
? q)
?
P
LM
(h
i
)
P (h
i
|q) = score(h
i
)
/
K
?
j=1
score(h
j
)
(5)
A standard log-linear weighing trick was ap-
plied to noisy-channel model components, see e.g.
(Whitelaw et al, 2009). ? is the parameter that
controls the trade-off between precision and recall
(see Section 4.2) by emphasizing the importance
of either the high frequency of the correction or its
proximity to the query.
We have also found, that resulting posterior
probabilities emphasize the best hypothesis too
much: best hypothesis gets almost all probability
mass and other hypotheses get none. To compen-
sate for that, posteriors were smoothed by raising
each probability to some power ? < 1 and re-
normalizing them afterward:
P
smooth
(h
i
|q) = P (h
i
|q)
?
/
K
?
j=1
P (h
j
|q)
?
. (6)
In a sense, ? is like temperature parameter in sim-
ulated annealing ? it controls the entropy of the
walk and the final probability distribution. Unlike
in simulated annealing, we fix ? for all iterations
of the algorithm.
Finally, if posterior probability of the best hy-
pothesis was lower than threshold ?, then the orig-
inal query q was used as the spell-checker output.
(Posterior is defined by Equation 6 for the baseline
and simple iterative methods and by Equations 3
and 6 for the proposed method). Parameter ? con-
trols precision/recall trade-off (as well as ? men-
tioned above).
4 Experiments
4.1 Data
To evaluate the proposed algorithm we have col-
lected two datasets. Both datasets were randomly
sampled from single-word user queries from the
1-week query log of a commercial search en-
gine. We annotated them with the help of pro-
fessional analyst. The difference between datasets
170
is that one of them contained only queries with
low search performance: for which the number
of documents retrieved by the search engine was
less than a fixed threshold (we will address it as
the ?hard? dataset), while the other dataset had
no such restrictions (we will call it ?common?).
Dataset statistics are shown in Table 3.
Dataset Queries Misspelled Avg. ? logP
dist
Common 2240 224 (10%) 5.98
Hard 2542 1484 (58%) 9.23
Table 3: Evaluation datasets.
Increased average error model score and er-
ror rate of ?common? dataset compared to ?hard?
shows, that we have indeed managed to collect
hard-to-correct queries in the ?hard? dataset.
4.2 Experimental results
First of all, we evaluated the recall of hypothe-
ses generator using K-best recall ? the number of
correct spelling corrections for misspelled queries
among K hypotheses divided by the total number
of misspelled queries in the test set. Resulting re-
call with K = 30 is 91.8% on ?hard? and 98.6%
on ?common?.
Next, three spelling correction methods were
tested: noisy channel, iterative correction and our
method (stochastic iterative correction).
For evaluation of spelling correction quality, we
use the following metrics:
? Precision: The number of correct spelling
corrections for misspelled words generated
by the system divided by the total number of
corrections generated by the system;
? Recall: The number of correct spelling cor-
rections for misspelled words generated by
the system divided by the total number of
misspelled words in the test set;
For hypotheses generator, K = 30 was fixed: re-
call of 91.8% was considered big enough. Pre-
cision/recall tradeoff parameters ? and ? (they
are applicable to each method, including baseline)
were iterated by the grid (0.2, 0.25, 0.3, ..., 1.5)?
(0, 0.025, 0.05, ..., 1.0), and E (applicable to it-
erative and our method) and ? (just our method)
were iterated by the grid (2, 3, 4, 5, 7, 10) ?
(0.1, 0.15, ...1.0); for each set of parameters, pre-
cision and recall were measured on both datasets.
Pareto frontiers for precision and recall are shown
in Figures 1 and 2.
Figure 1: Precision/recall Pareto frontiers on
?hard? dataset
Figure 2: Precision/recall Pareto frontiers on
?common? dataset
We were not able to reproduce superior perfor-
mance of the iterative method over the noisy chan-
nel, reported by (Cucerzan and Brill, 2004). Sup-
posedly, it is because the iterative method bene-
fits primarily from the sequential application of
split/join operations altering query decomposition
into words; since we are considering only one-
word queries, such decomposition does not matter.
On the ?hard? dataset the performance of the
noisy channel and the iterative methods is infe-
rior to our proposed method, see Figure 1. We
tested all three methods on the ?common? dataset
as well to evaluate if our handling of hard cases
affects the performance of our approach on the
common cases of spelling error. Our method per-
forms well on the common cases as well, as Fig-
ure 2 shows. The performance comparison for
the ?common? dataset shows comparable perfor-
mance for all considered methods.
Noisy channel and iterative methods? frontiers
171
are considerably inferior to the proposed method
on ?hard? dataset, which means that our method
works better. The results on ?common? dataset
show, that the proposed method doesn?t work
worse than baseline.
Next, we optimized parameters for each method
and each dataset separately to achieve the highest
F
1
measure. Results are shown in Tables 4 and 5.
We can see, that, given the proper tuning, our
method can work better on any dataset (but it can-
not achieve the best performance on both datasets
at once). See Tables 4 and 5 for details.
Method ? ? ? E F
1
Noisy channel 0.6 0.1 - - 55.8
Iterative 0.6 0.1 - 2 55.9
Stochastic iterative 0.9 0.2 0.35 3 62.5
Table 4: Best parameters and F
1
on ?hard? dataset
Method ? ? ? E F
1
Noisy channel 0.75 0.225 - - 62.06
Iterative 0.8 0.275 - 2 63.15
Stochastic iterative 1.2 0.4 0.35 3 63.9
Table 5: Best parameters and F
1
on ?common?
dataset
Next, each parameter was separately iterated
(by a coarser grid); initial parameters for each
method were taken from Table 4. Such iteration
serves two purposes: to show the influence of pa-
rameters on algorithm performance, and to show
differences between datasets: in such setup pa-
rameters are virtually tuned using ?hard? dataset
and evaluated using ?common? dataset. Results
are shown in Table 6.
The proposed method is able to successfully
correct distant spelling errors with edit distance of
3 characters (see Table 7).
However, if our method is applied to shorter
and more frequent queries (as opposed to ?hard?
dataset), it tends to suggest frequent words as
false-positive corrections (for example, grid is cor-
rected to creed ? Assassin?s Creed is popular video
game). As can be seen in Table 5, in order to fix
that, algorithm parameters need to be tuned more
towards precision.
5 Conclusion and future work
In this paper we introduced the stochastic itera-
tive correction method for spell check corrections.
Our experimental evaluation showed that the pro-
posed method improved the performance of popu-
F
1
, common F
1
, hard
N.ch. It. Our N.ch. It. Our
? = 0.5 45.3 45.9 37.5 54.9 54.8 50.0
0.6 49.9 50.5 41.5 55.8 55.9 56.6
0.7 50.4 50.4 44.1 54.5 55.1 59.6
0.8 52.7 52.7 46.0 52.6 53.0 61.5
0.9 53.5 53.5 49.3 50.3 50.6 62.5
1.0 55.4 55.0 50.9 47.0 47.3 61.8
1.1 53.7 53.4 52.7 44.3 44.6 60.8
1.2 52.5 52.5 53.7 41.9 42.3 58.8
1.3 52.2 52.6 54.6 39.5 39.9 56.6
1.4 51.4 51.8 55.0 36.8 37.3 53.6
? = 0 41.0 41.5 33.0 52.9 53.1 58.3
0.1 49.9 50.6 35.6 55.8 55.9 59.7
0.15 59.4 59.8 43.2 55.8 55.6 61.6
0.2 60.8 61.3 49.4 51.0 51.0 62.5
0.25 54.0 54.0 54.9 46.3 46.3 61.1
0.3 46.3 46.3 57.3 39.2 39.2 58.4
0.4 25.8 25.8 53.9 22.3 22.3 50.3
E = 2 50.6 53.6 55.9 60.4
3 50.6 49.4 55.9 62.5
4 50.6 46.4 55.9 62.1
5 50.6 46.7 55.9 60.1
? = 0.1 10.1 6.0
0.2 49.4 51.5
0.3 51.4 61.4
0.35 49.4 62.5
0.4 47.5 62.0
0.45 45.8 60.8
0.5 45.2 60.3
Table 6: Per-coordinate iteration of parameters
from Table 4; per-method maximum is shown in
italic, per-dataset in bold
Query Noisy channel Proposed method
akwamarin akvamarin aquamarine
maccartni maccartni mccartney
ariflaim ariflaim oriflame
epika epica replica
grid grid creed
Table 7: Correction examples for the noisy chan-
nel and the proposed method.
lar spelling correction approach ? the noisy chan-
nel model ? in the correction of difficult spelling
errors. We showed how to eliminate the local min-
ima issue of simulated annealing and proposed a
technique to make our algorithm deterministic.
The experiments conducted on the specialized
datasets have shown that our method significantly
improves the performance of the correction of
hard spelling errors (by 6.6% F
1
) while maintain-
ing good performance on common spelling errors.
In continuation of the work we are considering
to expand the method to correct errors in multi-
word queries, extend the method to work with dis-
criminative models, and use a query performance
prediction method, which tells for a query whether
our algorithm needs to be applied.
172
References
Eric Brill and Robert C Moore. 2000. An improved er-
ror model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 286?293.
Association for Computational Linguistics.
Monojit Choudhury, Markose Thomas, Animesh
Mukherjee, Anupam Basu, and Niloy Ganguly.
2007. How difficult is it to develop a perfect spell-
checker? a cross-linguistic analysis through com-
plex network approach. In Proceedings of the sec-
ond workshop on TextGraphs: Graph-based algo-
rithms for natural language processing, pages 81?
88.
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collec-
tive knowledge of web users. In EMNLP, volume 4,
pages 293?300.
Huizhong Duan and Bo-June Paul Hsu. 2011. On-
line spelling correction for query completion. In
Proceedings of the 20th international conference on
World wide web, pages 117?126. ACM.
Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk,
and Xu Sun. 2010. A large scale ranker-based sys-
tem for search query spelling correction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 358?366. Associ-
ation for Computational Linguistics.
Mark D Kernighan, Kenneth W Church, and William A
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
conference on Computational linguistics-Volume 2,
pages 205?210. Association for Computational Lin-
guistics.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys
(CSUR), 24(4):377?439.
Eric Mays, Fred J Damerau, and Robert L Mercer.
1991. Context based spelling correction. Informa-
tion Processing & Management, 27(5):517?522.
Casey Whitelaw, Ben Hutchinson, Grace Y Chung, and
Gerard Ellis. 2009. Using the web for language
independent spellchecking and autocorrection. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 890?899. Association for Com-
putational Linguistics.
173
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 99?103,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Yandex School of Data Analysis machine translation systems for WMT13
Alexey Borisov, Jacob Dlougach, Irina Galinskaya
Yandex School of Data Analysis
16, Leo Tolstoy street, Moscow, Russia
{alborisov,jacob,galinskaya}@yandex-team.ru
Abstract
This paper describes the English-Russian
and Russian-English statistical machine
translation (SMT) systems developed at
Yandex School of Data Analysis for the
shared translation task of the ACL 2013
Eighth Workshop on Statistical Machine
Translation. We adopted phrase-based
SMT approach and evaluated a number
of different techniques, including data fil-
tering, spelling correction, alignment of
lemmatized word forms and translitera-
tion. Altogether they yielded +2.0 and
+1.5 BLEU improvement for ru-en and en-
ru language pairs. We also report on the
experiments that did not have any positive
effect and provide an analysis of the prob-
lems we encountered during the develop-
ment of our systems.
1 Introduction
We participated in the shared translation task of
the ACL 2013 Workshop on Statistical Machine
Translation (WMT13) for ru-en and en-ru lan-
guage pairs. We provide a detailed description of
the experiments carried out for the development of
our systems.
The rest of the paper is organized as follows.
Section 2 describes the tools and data we used.
Our Russian?English and English?Russian se-
tups are discussed in Section 3. In Section 4 we
report on the experiments that did not have any
positive effect despite our expectations. We pro-
vide a thorough analysis of erroneous outputs in
Section 5 and draw conclusions in Section 6.
2 Tools and data
2.1 Tools
We used an open source SMT system Moses
(Koehn et al, 2007) for all our experiments ex-
cluding the one described in Section 4.1 due to its
performance constraints. To overcome the limita-
tion we employed our in-house decoder.
Language models (LM) were created with an
open source IRSTLM toolkit (Federico et al,
2008). We computed 4-gram LMs with modified
Kneser-Ney smoothing (Kneser and Ney, 1995).
We used an open source MGIZA++ tool (Gao
and Vogel, 2008) to compute word alignment.
To obtain part of speech (POS) tags we used
an open source Stanford POS tagger for English
(Toutanova et al, 2003) and an open source suite
of language analyzers, FreeLing 3.0 (Carreras et
al., 2004; Padr? and Stanilovsky, 2012), for Rus-
sian.
We utilized a closed source free for non-
commercial use morphological analyzer, Mystem
(Segalovich, 2003), that used a limited dictionary
to obtain lemmas.
We also made use of the in-house language rec-
ognizer based on (Dunning, 1994) and a spelling
corrector designed on the basis of the work of
Cucerzan and Brill (2004).
We report all results in case-sensitive BLEU
(Papineni et al, 2002) using mt-eval13a script
from Moses distribution.
2.2 Data
Training data
We used News Commentary and News Crawl
monolingual corpora provided by the organizers
of the workshop.
Bilingual training data comprised English-
Russian parallel corpus release by Yandex1, News
Commentary and Common Crawl corpora pro-
vided by the organizers.
We also exploited Wiki Headlines collection of
three parallel corpora provided by CMU2 as a
1https://translate.yandex.ru/corpus
2http://www.statmt.org/wmt13/
wiki-titles.ru-en.tar.gz
99
source of reliable data.
Development set
The newstest2012 test set (Callison-Burch et al,
2012) was divided in the ratio 2:1 into a tuning
set and a test set. The latter is referred to as
newstest2012-test in the rest of the paper.
3 Primary setups
3.1 Baseline
We built the baseline systems according to the in-
structions available at the Moses website3.
3.2 Preprocessing
The first thing we noticed was that some sentences
marked as Russian appeared to be sentences in
other languages (most commonly English). We
applied a language recognizer for both monolin-
gual and bilingual corpora. Results are given in
Table 1.
Corpus Filtered out (%)
Bilingual 3.39
Monolingual (English) 0.41
Monolingual (Russian) 0.58
Table 1: Results of the language recognizer: per-
centage of filtered out sentences.
The next thing we came across was the pres-
ence of a lot of spelling errors in our training data,
so we applied a spelling corrector. Statistics are
presented in Table 2.
Corpus Modified (%)
Bilingual (English) 0.79
Bilingual (Russian) 1.45
Monolingual (English) 0.61
Monolingual (Russian) 0.52
Table 2: Results of the spelling corrector: percent-
age of modified sentences.
3.3 Alignment of lemmatized word forms
Russian is a language with rich morphology. The
diversity of word forms results in data sparse-
ness that makes translation of rare words dif-
ficult. In some cases inflections do not con-
tain any additional information and are used
3http://www.statmt.org/moses/?n=moses.
baseline
only to make an agreement between two words.
E.g. ADJ + NOUN: ?????? ?? ???? (beau-
tiful harp), ?????? ?? ??????? (beautiful pi-
ano), ?????? ?? ????? (beautiful grand piano).
These inflections reflect the gender of the noun
words, that has no equivalent in English.
In this particular case we can drop the inflec-
tions, but for other categories they can still be use-
ful for translation, because the information they
contain appears in function words in English. On
the other hand, most of Russian morphology is
useless for word alignment.
We applied a morphological analyzer Mystem
(Segalovich, 2003) to the Russian text and con-
verted each word to its dictionary form. Next
we computed word alignment between the origi-
nal English text and the lemmatized Russian text.
All the other steps were executed according to the
standard procedure with the original texts.
3.4 Phrase score adjustment
Sometimes phrases occur one or two times in the
training corpus. In this case the corresponding
phrase translation probability would be overesti-
mated. We used Good-Turing technique described
in (Gale, 1994) to decrease it to some more realis-
tic value.
3.5 Decoding
Minimum Bayes-Risk (MBR)
MBR decoding (Kumar and Byrne, 2004) aims
to minimize the expected loss of translation er-
rors. As it is not possible to explore the space of
all possible translations, we approximated it with
the 1,000 most probable translations. A minus
smoothed BLEU score (Lin and Och, 2004) was
used for the loss function.
Reordering constrains
We forbade reordering over punctuation and trans-
lated quoted phrases independently.
3.6 Handling unknown words
The news texts contained a lot of proper names
that did not appear in the training data. E.g. al-
most 25% of our translations contained unknown
words. Dropping the unknown words would lead
to better BLEU scores, but it might had caused
bad effect on human judgement. To leave them
in Cyrillic was not an option, so we exploited two
approaches: incorporating reliable data from Wiki
Headlines and transliteration.
100
newstest2012-test newstest2013
Russian?English
Baseline 28.96 21.82
+ Preprocessing 29.59 22.28
+ Alignment of lemmatized word forms 29.97 22.61
+ Good-Turing 30.31 22.87
+ MBR 30.45 23.21
+ Reordering constraints 30.54 23.33
+ Wiki Headlines 30.68 23.46
+ Transliteration 30.93 23.73
English?Russian
Baseline 21.96 16.24
+ Preprocessing 22.48 16.76
+ Good-Turing 22.84 17.13
+ MBR and Reordering constraints 23.27 17.45
+ Wiki Headlines and Transliteration 23.54 17.80
Table 3: Experimental results in case-sensitive BLEU for Russian?English and English?Russian tasks.
Wiki Headlines
We replaced the names occurring in the text with
their translations, based on the information in
"guessed-names" corpus from Wiki Headlines.
As has been mentioned in Section 3.3, Russian
is a morphologically rich language. This often
makes it hard to find exactly the same phrases,
so we applied lemmatization of Russian language
both for the input text and the Russian side of the
reference corpus.
Russian?English transliteration
We gained considerable improvement from incor-
porating Wiki Headlines, but still 17% of transla-
tions contained Cyrillic symbols.
We applied a transliteration algorithm based on
(Knight and Graehl, 1998). This technique yielded
us a significant improvement, but introduced a lot
of errors. E.g. ?????? ???? (James Bond) was
converted to Dzhejms Bond.
English?Russian transliteration
In Russian, it is a common practice to leave some
foreign words in Latin. E.g. the names of compa-
nies: Apple, Google, Microsoft look inadmissible
when either translated directly or transliterated.
Taking this into account, we applied the
same transliteration algorithm (Knight and Graehl,
1998), but replaced an unknown word with its
transliteration only if we found a sufficient num-
ber of occurrences of its transliterated form in the
monolingual corpus. We used five for such num-
ber.
3.7 Experimental results
We summarized the gains from the de-
scribed techniques for Russian?English and
English?Russian tasks on Table 3.
4 What did not work
4.1 Translation in two stages
Frequently machine translations contain errors
that can be easily corrected by human post-editors.
Since human aided machine translation is cost-
efficient, we decided to address this problem to the
computer.
We propose to translate sentences in two stages.
At the first stage a SMT system is used to trans-
late the input text into a preliminary form (in target
language). At the next stage the preliminary form
is translated again with an auxiliary SMT system
trained on the translated and the target sides of the
parallel corpus.
We encountered a technical challenge, when we
had to build a SMT system for the second stage.
A training corpus with one side generated with
the first stage SMT system was not possible to be
acquired with Moses due to its performance con-
straints. Thereupon we utilized our in-house SMT
decoder and managed to translate 2M sentences in
time.
We applied this technique both for ru-en and en-
ru language pairs. Approximately 20% of the sen-
101
tences had changed, but the BLEU score remained
the same.
4.2 Factored model
We tried to build a factored model for ru-en lan-
guage pair with POS tags produced by Stanford
POS tagger (Toutanova et al, 2003).
Unfortunately, we did not gain any improve-
ments from it.
5 Analysis
We carefully examined the erroneous outputs of
our system and compared it with the outputs of
the other systems participating in ru-en and en-ru
tasks, and with the commercial systems available
online (Bing, Google, Yandex).
5.1 Transliteration
Russian?English
The standard transliteration procedure is not in-
vertible. This means that a Latin word being trans-
fered into Cyrillic and then transliterated back
to Latin produces an artificial word form. E.g.
?????? ?????????? / Havard Halvarsen was
correctly transliterated by only four out of 23
systems, including ours. Twelve systems either
dropped one of the words or left it in Cyrillic.
We provide a list of typical mistakes in order of
their frequency: Khavard Khalvarsen, Khavard
Khal?varsen, Xavard Xaljvarsen. Another exam-
ple: ???? ?????? (Miss Wyatt) ? Miss Uayett
(all the systems failed).
The next issue is the presence of non-null in-
flections that most certainly would result in wrong
translation by any straight-forward algorithm. E.g.
??????????? ? (Heidelberg)? Heidelberga.
English?Russian
In Russian, most words of foreign origin are writ-
ten phonetically. Thereby, in order to obtain the
best quality we should transliterate the transcrip-
tion, not the word itself. E.g. the French derived
name Elsie Monereau [?elsi mon@?r@V] being trans-
lated by letters would result in ???? ????????
while the transliteration of the transcription would
result in the correct form ???? ?????.
5.2 Grammars
English and Russian make use of different gram-
mars. When the difference in their sentence struc-
ture becomes fundamental the phrase-based ap-
proach might get inapplicable.
Word order
Both Russian and English are classified as subject-
verb-object (SOV) languages, but Russian has
rather flexible word order compared to English
and might frequently appear in other forms. This
often results in wrong structure of the translated
sentence. A common mistake made by our sys-
tem and reproduced by the major online services:
?? ?????????? ? ??????? (rules have not been
changed either) ? have not changed and the
rules.
Constructions
? there is / there are is a non-local construc-
tion that has no equivalent in Russian. In
most cases it can not be produced from the
Russian text. E.g. ?? ????? ????? ??????-
?? (there is a matryoshka doll on the table)
? on the table is a matryoshka.
? multiple negatives in Russian are grammati-
cally correct ways to express negation (a sin-
gle negative is sometimes incorrect) while
they are undesirable in standard English. E.g.
??? ????? ??????? ?? ??? (nobody has
ever been there) being translated word by
word would result in there nobody never not
was.
5.3 Idioms
Idiomatic expressions are hard to discover and
dangerous to translate literary. E.g. a Russian
idiom ???? ?? ???? (let come what may) be-
ing translated word by word would result in was
not was. Neither of the commercial systems we
checked managed to collect sufficient statistic to
translate this very popular expression.
6 Conclusion
We have described the primary systems developed
by the team of Yandex School of Data Analysis for
WMT13 shared translation task.
We have reported on the experiments and
demonstrated considerable improvements over the
respective baseline. Among the most notable tech-
niques are data filtering, spelling correction, align-
ment of lemmatized word forms and translitera-
tion. We have analyzed the drawbacks of our sys-
tems and shared the ideas for further research.
102
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT12), pages 10?51, Montr?al, Canada, June.
Association for Computational Linguistics.
Xavier Carreras, Isaac Chao, Llu?s Padr?, and Muntsa
Padr?. 2004. FreeLing: An open-source suite of
language analyzers. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC).
Silviu Cucerzan and Eric Brill. 2004. Spelling cor-
rection as an iterative process that exploits the col-
lective knowledge of web users. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 293?300.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical report, Computing Research Lab
(CRL), New Mexico State University, Las Cruces,
NM, USA.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit
for handling large scale language models. In Pro-
ceedings of 9th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 1618?1621.
William Gale. 1994. Good-Turing smoothing with-
out tears. Journal of Quantitative Linguistics (JQL),
2:217?237.
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 49?57.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), vol-
ume 1, pages 181?184.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-burch, Richard Zens, Rwth Aachen,
Alexandra Constantin, Marcello Federico, Nicola
Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen,
Christine Moran, and Ondr?ej Bojar. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 177?180.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), pages 163?171.
Chin-Yew Lin and Franz Josef Och. 2004. OR-
ANGE: a method for evaluating automatic evalua-
tion metrics for machine translation. In Proceed-
ings of the 20th international conference on Com-
putational Linguistics (COLING), Stroudsburg, PA,
USA. Association for Computational Linguistics.
Llu?s Padr? and Evgeny Stanilovsky. 2012. FreeLing
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC), Istanbul, Turkey, May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Processings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318.
Ilya Segalovich. 2003. A fast morphological algorithm
with unknown word guessing induced by a dictio-
nary for a web search engine. In Hamid R. Arab-
nia and Elena B. Kozerenko, editors, Proceedings of
the International Conference on Machine Learning;
Models, Technologies and Applications (MLMTA),
pages 273?280, Las Vegas, NV, USA, June. CSREA
Press.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Human Language Technol-
ogy Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 252?259.
103
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 66?70,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Yandex School of Data Analysis
Russian-English Machine Translation System for WMT14
Alexey Borisov and Irina Galinskaya
Yandex School of Data Analysis
16, Leo Tolstoy street, Moscow, Russia
{alborisov, galinskaya}@yandex-team.ru
Abstract
This paper describes the Yandex School
of Data Analysis Russian-English system
submitted to the ACL 2014 Ninth Work-
shop on Statistical Machine Translation
shared translation task. We start with the
system that we developed last year and in-
vestigate a few methods that were success-
ful at the previous translation task includ-
ing unpruned language model, operation
sequence model and the new reparameter-
ization of IBM Model 2. Next we propose
a {simple yet practical} algorithm to trans-
form Russian sentence into a more easily
translatable form before decoding. The al-
gorithm is based on the linguistic intuition
of native Russian speakers, also fluent in
English.
1 Introduction
The annual shared translation task organized
within the ACL Workshop on Statistical Machine
Translation (WMT) aims to evaluate the state of
the art in machine translation for a variety of lan-
guages. We participate in the Russian to English
translation direction.
The rest of the paper is organized as follows.
Our baseline system as well as the experiments
concerning the methods already discussed in lit-
erature are described in Section 2. In Section 3 we
present an algorithm we use to transform the Rus-
sian sentence before translation. In Section 4 we
discuss the results and conclude.
2 Initial System Development
We use all the Russian-English parallel data avail-
able in the constraint track and the Common Crawl
English monolingual corpus.
2.1 Baseline
We use the phrase-based Moses statistical ma-
chine translation system (Koehn et al., 2007) with
mostly default settings and a few changes (Borisov
et al., 2013) made in the following steps.
Data Preprocessing includes filtering out non
Russian-English sentence pairs and correction of
spelling errors.
Phrase Table Smoothing uses Good-Turing
scheme (Foster et al., 2006).
Consensus Decoding selects the translation
with minimum Bayes risk (Kumar and Byrne,
2004).
Handling of UnknownWords comprises incor-
poration of proper names from Wiki Headlines
parallel data provided by CMU
1
and translitera-
tion. We improve the transliteration algorithm in
Section 2.4.
Note that unlike last year we do not use word
alignments computed for the lemmatized word
forms.
2.2 Language Model
We use 5-gram unpruned language model with
modified Kneser-Ney discount estimated with
KenLM toolkit (Heafield et al., 2013).
2.3 Word alignment
Word alignments are generated using the
fast_align tool (Dyer et al., 2013), which is much
faster than IBM Model 4 from MGIZA++ (Gao
and Vogel, 2008) and outperforms the latter in
terms of BLEU. Results are given in Table 1.
2.4 Transliteration
We employ machine transliteration to generate ad-
ditional translation options for out-of-vocabulary
1
http://www.statmt.org/wmt14/
wiki-titles.tgz
66
MGIZA++ fast_align
Run Time 22 h 14 m 2h 49 m
Perplexity
? ru?en 97.00 90.37
? en?ru 209.36 216.71
BLEU
? WMT13 25.27 25.49
? WMT14 31.76 31.92
Table 1: Comparison of word alignment tools:
MGIZA++ vs. fast_align. fast_align runs ten
times as fast and outperforms the IBM Model 4
from MGIZA++ in terms of BLEU scores.
words. The transformation model we use is a
transfeme based model (Duan and Hsu, 2011),
which is analogous to translation model in phrase-
based machine translation. Transformation units,
or transfemes, are trained with Moses using the
default settings. Decoding is very similar to beam
search. We build a trie from the words in English
monolingual corpus, and search in it, based on the
transformation model.
2.5 Operation Sequence Model
The Operation Sequence N-gram Model (OSM)
(Durrani et al., 2011) integrates reordering opera-
tions and lexical translations into a heterogeneous
sequence of minimal translation units (MTUs) and
learns a Markov model over it. Reordering deci-
sions influence lexical selections and vice versa
thus improving the translation model. We use
OSM as a feature function in phrase-based SMT.
Please, refer to (Durrani et al., 2013) for imple-
mentation details.
3 Morphological Transformations
Russian is a fusional synthetic language, mean-
ing that the relations between words are redundant
and encoded inside the words. Adjectives alter
their form to reflect the gender, case, number and
in some cases, animacy of the nouns, resulting in
dozens of different word forms matching a single
English word. An example is given in Table 2.
Verbs in Russian are typically constructed from
the morphemes corresponding to functional words
in English (to, shall, will, was, were, has, have,
had, been, etc.). This Russian phenomenon leads
to two problems: data sparsity and high number of
one-to-many alignments, which both may result in
translation quality degradation.
Number
SG PL
Case Gender
NOM MASC ??????
NOM FEM ?????? ??????
NOM NEUT ??????
GEN MASC ???????
GEN FEM ?????? ??????
GEN NEUT ???????
DAT MASC ???????
DAT FEM ?????? ??????
DAT NEUT ???????
ACC MASC, AN ???????
ACC MASC, INAN ?????? ??????
ACC FEM ??????
ACC NEUT ??????
INS MASC ??????
INS FEM ?????? ??????
INS FEM ??????
INS NEUT ??????
ABL MASC ??????
ABL FEM ?????? ??????
ABL NEUT ??????
Table 2: Russian word forms corresponding to the
English word "summer" (adj.).
Hereafter, we propose an algorithm to transform
the original Russian sentence into a more easily
translatable form. The algorithm is based on the
linguistic intuition of native Russian speakers, also
fluent in English.
3.1 Approach
Based on the output from Russian morphological
analyzer we rewrite the input sentence based on
the following principles:
1. the original sentence is restorable
(by a Russian native speaker)
2. redundant information is omitted
3. word alignment is less ambiguous
3.2 Algorithm
The algorithm consists of two steps.
On the first step we employ in-house Rus-
sian morphological analyzer similar to Mys-
tem (Segalovich, 2003) to convert each word
(WORD) into a tuple containing its canonical form
(LEMMA), part of speech tag (POS) and a set
67
Category Abbr. Values
Animacy ANIM AN, INAN
Aspect ASP IMPERF, PERF
Case CASE NOM, GEN, DAT, ACC, INS, ABL
Comparison Type COMP COMP, SURP
Gender GEND MASC, FEM, NEUT
Mood MOOD IND, IMP, COND, SBJV
Number NUM SG, PL
Participle Type PART ACT, PASS
Person PERS PERS1, PERS2, PERS3
Tense TNS PRES, NPST, PST
Table 3: Morphological Categories
of other grammemes associated with the word
(GRAMMEMES). The tuple is later referred to as
LPG. If the canonical form or part of speech are
ambiguous, we set LEMMA to WORD; POS to
"undefined"; and GRAMMEMES to ?. Gram-
memes are grouped into grammatical categories
listed in Table 3.
WORD ?? LEMMA + POS + GRAMMEMES
On the second step, the LPGs are converted into
tokens that, we hope, will better match English
structure. Some grammemes result in separate to-
kens, others stay with the lemma, and the rest get
dropped. The full set of morphological transfor-
mations we use is given in Table 4.
An example of applying the algorithm to a Rus-
sian sentence is given in Figure 1.
3.3 Results
The translation has been improved in several
ways:
Incorrect Use of Tenses happens quite often in
statistical machine translation, which is especially
vexing in simple cases such as asks instead of
asked, explains instead of explain along with more
difficult ones e.g. has increased instead of would
increase. The proposed algorithm achieves con-
siderable improvement, since it explicitly models
tenses and all its relevant properties.
Missing Articles is a common problem of
most Russian-English translation systems, be-
cause there are no articles in Russian. Our model
creates an auxiliary token for each noun, which re-
flects its case and motivates an article.
Use of Simple Vocabulary is not desirable
when the source text is a vocabulary-flourished
?????? ???? 
??????.adj+?   ins  ????.n+sg 
on a summer day 
??????, adj, 
{inan, dat|ins, ?, male|neut, sg|pl} 
????, ?noun, 
{inan, ins, male, sg} 
Figure 1: An illustration of the proposed algorithm
to transform Russian sentence ?????? ???? (let-
nim dnem), meaning on a summer day, into a more
easily translatable form. First, for each word we
extract its canonical form, part of speech tag and a
set of associated morphological properties (gram-
memes). Then we apply hand-crafted rules (Ta-
ble 4) to transform them into separate tokens.
one. News are full of academic, bookish, inkhorn,
and other rare words. Phrase Table smoothing
methods discount the translation probabilities for
rare phrase pairs, preventing them from appearing
in English translation, while many of these rare
phrase pairs are correct. The good thing is that the
phrase pairs containing the transformed Russian
words may not be rare themselves, and thereby are
not discounted so heavily. A more effective use of
English vocabulary has been observed on WMT13
test dataset (see Table 5).
We have demonstrated the improvements on a
qualitative level. The quantitative results are sum-
marized in Table 6 (baseline ? without morpholog-
ical transformations; proposed ? with morpholog-
ical transformations).
68
LPG? tokens
LEMMA, adj,
{ANIM, CASE, COMP, GEND, NUM}
?
LEMMA.adj+COMP
LEMMA, noun,
{ANIM, CASE, GEND, NUM}
?
CASE LEMMA.n+NUM
LEMMA, verb (ger), {ASP, TNS}
?
LEMMA.vg+ASP+TNS
LEMMA, verb (inf), {ASP}
?
LEMMA.vi+ASP
LEMMA, verb (part), {PART, ASP, TNS}
?
LEMMA.vp+PART+ASP+TNS
LEMMA, verb (?),
{PART, ASP, MOOD, TENSE,
NUM, PERS}
?
1. TNS={PRES} | TNS={NPST} & ASP={IMPERF}
a. PERS3 ? PERS & SG ? NUM
LEMMA.v+pres+MOOD+PERS+NUM
b. otherwise
LEMMA.v+pres+MOOD
2. TNS={PST}
ASP LEMMA.v+pst+MOOD
3. TNS={NPST} & ASP={IMPERF}
fut LEMMA.v+MOOD
4. if ambiguous
LEMMA.v+PART+ASP+MOOD
+TNS+NUM+PERS
LEMMA, OTHER, GRAMMEMES
?
LEMMA.POS+GRAMMEMES
Table 4: A set of rules we use to transform
the LPGs (LEMMA, POS, GRAMMEMES), ex-
tracted on the first step, into individual tokens.
4 Discussion and Conclusion
We described the Yandex School of Data Anal-
ysis Russian-English system submitted to the
ACL 2014 Ninth Workshop on Statistical Machine
Translation shared translation task. The main con-
tribution of this work is an algorithm to transform
the Russian sentence into a more easily translat-
Input Translation
??????????? (a) differences
(raznoglasiya) (b) disputes
?????????????? (a) promoter
(propagandistom) (b) propagandist
??????????????? (a) mainly
(preimuschestvenno) (b) predominantly
Table 5: Morphological Transformations lead to
more effective use of English vocabulary. Trans-
lations marked with "a" were produced using the
baseline system; with "b" also use Morphological
Transformations.
Baseline Proposed
Distinct Words 899,992 564,354
OOV Words
? WMT13 829 590
? WMT14 884 660
Perplexity
? ru?en 90.37 99.81
? en?ru 216.71 128.15
BLEU
? WMT13 25.49 25.63
? WMT14 31.92 32.56
Table 6: Results of Morphological Transforma-
tions. We improved the statistical characteristics
of our models by reducing the number of distinct
words by 37% and managed to translate 25% of
previously untranslated words. BLEU scores were
improved by 0.14 and 0.64 points for WMT13 and
WMT14 test sets respectively.
able form before decoding. Significant improve-
ments in human satisfaction and BLEU scores
have been demonstrated from applying this algo-
rithm.
One limitation of the proposed algorithm is that
it does not take into account the relations between
words sharing the same root. E.g. the word ?????-
??? (aistinyh) meaning stork (adj.) is handled in-
dependently from the word ???? (aist) meaning
stork (n.). Our system as well as the major online
services (Bing, Google, Yandex) transliterated this
word, but the word aistinyh does not make much
sense to a non-Russian reader. It might be worth-
while to study this problem in more detail.
Another direction for future work is to apply
the proposed algorithm in reverse direction. We
suggest the following two-step procedure. English
69
sentence is first translated into Russian
?
(Russian
after applying the morphological transformations),
and at the next step it is translated again with an
auxiliary SMT system trained on the (Russian*,
Russian) parallel corpus created from the Russian
monolingual corpus.
References
Alexey Borisov, Jacob Dlougach, and Irina Galinskaya.
2013. Yandex school of data analysis machine trans-
lation systems for wmt13. In Proceedings of the
Eighth Workshop on Statistical Machine Translation
(WMT), pages 97?101. Association for Computa-
tional Linguistics.
Huizhong Duan and Bo-June Paul Hsu. 2011. On-
line spelling correction for query completion. In
Proceedings of the 20th international conference on
World Wide Web (WWW), pages 117?126. ACM.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1045?1054. Association
for Computational Linguistics.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation (WMT), pages 112?119. Associa-
tion for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), pages 644?648.
Association for Computational Linguistics.
George Foster, Roland Kuhn, and John Howard John-
son. 2006. Phrasetable smoothing for statistical ma-
chine translation. In Proceedings of the 44th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 53?61. Association for Com-
putational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 49?57. As-
sociation for Computational Linguistics.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
690?696, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-burch, Richard Zens, Rwth Aachen,
Alexandra Constantin, Marcello Federico, Nicola
Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen,
Christine Moran, and Ond?rej Bojar. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 177?180. Association for Compu-
tational Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 163?171. Association for
Computational Linguistics.
Ilya Segalovich. 2003. A fast morphological algorithm
with unknown word guessing induced by a dictio-
nary for a web search engine. In Hamid R. Arab-
nia and Elena B. Kozerenko, editors, Proceedings of
the International Conference on Machine Learning;
Models, Technologies and Applications (MLMTA),
pages 273?280, Las Vegas, NV, USA, June. CSREA
Press.
70
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 43?50,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Applying HMEANT to English-Russian Translations
Alexander Chuchunkov Alexander Tarelkin
{madfriend,newtover,galinskaya}@yandex-team.ru
Yandex LLC
Leo Tolstoy st. 16, Moscow, Russia
Irina Galinskaya
Abstract
In this paper we report the results of
first experiments with HMEANT (a semi-
automatic evaluation metric that assesses
translation utility by matching semantic
role fillers) on the Russian language. We
developed a web-based annotation inter-
face and with its help evaluated practica-
bility of this metric in the MT research
and development process. We studied reli-
ability, language independence, labor cost
and discriminatory power of HMEANT
by evaluating English-Russian translation
of several MT systems. Role labeling
and alignment were done by two groups
of annotators - with linguistic background
and without it. Experimental results were
not univocal and changed from very high
inter-annotator agreement in role labeling
to much lower values at role alignment
stage, good correlation of HMEANT with
human ranking at the system level sig-
nificantly decreased at the sentence level.
Analysis of experimental results and anno-
tators? feedback suggests that HMEANT
annotation guidelines need some adapta-
tion for Russian.
1 Introduction
Measuring translation quality is one of the most
important tasks in MT, its history began long ago
but most of the currently used approaches and
metrics have been developed during the last two
decades. BLEU (Papineni et al., 2002), NIST
(Doddington, 2002) and METEOR (Banerjee and
Lavie, 2005)metric require reference translation
to compare it with MT output in fully automatic
mode, which resulted in a dramatical speed-up for
MT research and development. These metrics cor-
relate with manual MT evaluation and provide re-
liable evaluation for many languages and for dif-
ferent types of MT systems.
However, the major problem of popular MT
evaluation metrics is that they aim to capture lexi-
cal similarity of MT output and reference transla-
tion (fluency), but fail to evaluate the semantics of
translation according to the semantics of reference
(adequacy) (Lo and Wu, 2011a). An alternative
approach that is worth mentioning is the one pro-
posed by Snover et al. (2006), known as HTER,
which measures the quality of machine translation
in terms of post-editing. This method was proved
to correlate well with human adequacy judgments,
though it was not designed for a task of gisting.
Moreover, HTER is not widely used in machine
translation evaluation because of its high labor in-
tensity.
A family of metrics called MEANT was pro-
posed in 2011 (Lo and Wu, 2011a), which ap-
proaches MT evaluation differently: it measures
how much of an event structure of reference does
machine translation preserve, utilizing shallow se-
mantic parsing (MEANT metric) or human anno-
tation (HMEANT) as a gold standard.
We applied HMEANT to a new language ?
Russian ? and evaluated the usefulness of met-
ric. The practicability for the Russian language
was studied with respect to the following criteria
provided by Birch et al. (2013):
Reliability ? measured as inter-annotator agree-
ment for individual stages of evaluation task.
Discriminatory Power ? the correlation of
rankings of four MT systems (by manual evalu-
ation, BLEU and HMEANT) measured on a sen-
tence and test set levels.
Language Independence ? we collected the
problems with the original method and guidelines
and compared these problems to those reported by
Bojar and Wu (2012) and Birch et al. (2013).
Efficiency ? we studied the labor cost of anno-
tation task, i. e. average time required to evaluate
43
translations with HMEANT. Besides, we tested
the statement that semantic role labeling (SRL)
does not require experienced annotators (in our
case, with linguistic background).
Although the problems of HMEANT were out-
lined before (by Bojar and Wu (2012) and Birch
et al. (2013)) and several improvements were pro-
posed, we decided to step back and conduct ex-
periments with HMEANT in its original form. No
changes to the metric, except for the annotation
interface enhancements, were made.
This paper has the following structure. Sec-
tion 2 reports the previous experiments with
HMEANT; section 3 summarizes the methods be-
hind HMEANT; section 4 ? the settings for our
own experiments; sections 5 and 6 are dedicated
to results and discussion.
2 Related Work
Since the beginning of the machine translation era
the idea of semantics-driven approach for transla-
tion wandered around in the MT researchers com-
munity (Weaver, 1955). Recent works by Lo and
Wu (2011a) claim that this approach is still per-
spective. These works state that in order for ma-
chine translation to be useful, it should convey the
shallow semantic structure of the reference trans-
lation.
2.1 MEANT for Chinese-English
Translations
The original paper on MEANT (Lo and Wu,
2011a) proposes the semi-automatic metric, which
evaluates machine translations utilizing annotated
event structure of a sentence both in reference and
machine translation. The basic assumption be-
hind the metric can be stated as follows: trans-
lation shall be considered "good" if it preserves
shallow semantic (predicate-argument) structure
of reference. This structure is described in the pa-
per on shallow semantic parsing (Pradhan et al.,
2004): basically, we approach the evaluation by
asking simple questions about events in the sen-
tence: "Who did what to whom, when, where, why
and how?". These structures are annotated and
aligned between two translations. The authors of
MEANT reported results of several experiments,
which utilized both human annotation and seman-
tic role labeling (as a gold standard) and automatic
shallow semantic parsing. Experiments show that
HMEANT correlates with human adequacy judg-
ments (for three MT systems) at the value of 0.43
(Kendall tau, sentence level), which is very close
to the correlation of HTER (BLEU has only 0.20).
Also inter-annotator agreement was reported for
two stages of annotation: role identification (se-
lecting the word span) and role classification (la-
beling the word span with role). For the former,
IAA ranged from 0.72 to 0.93 (which can be in-
terpreted as a good agreement) and for the latter,
from 0.69 to 0.88 (still quite good, but should be
put in doubt). IAA for the alignment stage was not
reported.
2.2 HMEANT for Czech-English
Translations
MEANT and HMEANT metrics were adopted
for an experiment on evaluation of Czech-English
and English-Czech translations by Bojar and
Wu (2012). These experiments were based on
a human-evaluated set of 40 translations from
WMT12
1
, which were submitted by 13 systems;
each system was evaluated by exactly one anno-
tator, plus an extra annotator for reference trans-
lations. This setting implied that inter-annotator
agreement could not be examined. HMEANT cor-
relation with human assessments was reported as
0.28, which is significantly lower than the value
obtained by Lo and Wu (2011a).
2.3 HMEANT for German-English
Translations
Birch et al. (2013) examined HMEANT thor-
oughly with respect to four criteria, which address
the usefulness of a task-based metric: reliability,
efficiency, discriminatory power and language in-
dependence. The authors conducted an experi-
ment to evaluate three MT systems: rule-based,
phrase-based and syntax-based on a set of 214 sen-
tences (142 German and 72 English). IAA was
broken down into the different stages of annotation
and alignment. The experimental results showed
that whilst the IAA for HMEANT is satisfying at
the first stages of the annotation, the compound-
ing effect of disagreement at each stage (up to
the alignment stage) greatly reduced the effective
overall IAA ? to 0.44 on role alignment for Ger-
man, and, only slightly better, 0.59 for English.
HMEANT successfully distinguished three types
of systems, however, this result could not be con-
sidered reliable as IAA is not very high (and rank
1
http://statmt.org/wmt12
44
correlation was not reported). The efficiency of
HMEANT was stated as reasonably good; how-
ever, it was not compared to the labor cost of (for
example) HTER. Finally, the language indepen-
dence of the metric was implied by the fact that
original guidelines can be applied both to English
and German translations.
3 Methods
3.1 Evaluation with HMEANT
The underlying annotation cycle of HMEANT
consists of two stages: semantic role labeling
(SRL) and alignment. During the SRL stage, each
annotator is asked to mark all the frames (a pred-
icate and associated roles) in reference translation
and hypothesis translation. To annotate a frame,
one has to mark the frame head ? predicate (which
is a verb, but not a modal verb) and its argu-
ments, role fillers, which are linked to that pred-
icate. These role fillers are given a role from the
inventory of 11 roles (Lo and Wu, 2011a). The
role inventory is presented in Table 1, where each
role corresponds to a specific question about the
whole frame.
Who? What? Whom?
Agent Patient Benefactive
When? Where? Why?
Temporal Locative Purpose
How?
Manner, Degree, Negation, Modal, Other
Table 1. The role inventory.
On the second stage, the annotators are asked
to align the elements of frames from reference
and hypothesis translations. The annotators link
both actions and roles, and these alignments can
be matched as ?Correct? or ?Partially Correct? de-
pending on how well the meaning was preserved.
We have used the original minimalistic guidelines
for the SRL and alignment provided by Lo and Wu
(2011a) in English with a small set of Russian ex-
amples.
3.2 Calculating HMEANT
After the annotation, HMEANT score of the
hypothesis translation can be calculated as the
F-score from the counts of matches of predicates
and their role fillers (Lo and Wu, 2011a). Pred-
icates (and roles) without matches are not ac-
counted, but they result in the lower value overall.
We have used the uniform model of HMEANT,
which is defined as follows.
#F
i
? number of correct role fillers for predicate
i in machine translation;
#F
i
(partial) ? number of partially correct role
fillers for predicate i in MT;
#MT
i
, #REF
i
? total number of role fillers in
MT or reference for predicate i;
N
mt
, N
ref
? total number of predicates in MT or
reference;
w ? weight of the partial match (0.5 in the uniform
model).
P =
?
matched i
#F
i
#MT
i
R =
?
matched i
#F
i
#REF
i
P
part
=
?
matched i
#F
i
(partial)
#MT
i
R
part
=
?
matched i
#F
i
(partial)
#REF
i
P
total
=
P + w ? P
part
N
mt
R
total
=
R+ w ?R
part
N
ref
HMEANT =
2 ? P
total
?R
total
P
total
+R
total
3.3 Inter-Annotator Agreement
Like Lo and Wu (2011a) and Birch et al. (2013)
we studied inter-annotator agreement (IAA). It is
defined as an F1-measure, for which we consider
one of the annotators as a gold standard:
IAA =
2 ? P ?R
P +R
Where precision (P ) is the number of labels (roles,
predicates or alignments) that match between an-
notators divided by the total number of labels by
annotator 1; recall (R) is the number of matching
labels divided by the total number of labels by an-
notator 2. Following Birch et al. (2013), we con-
sider only exact word span matches. Also we have
adopted the individual stages of the annotation
procedure that are described in (Birch et al. 2013):
role identification (selecting the word span), role
classification (marking the word span with a role),
action identification (marking the word span as a
predicate), role alignment (linking roles between
translations) and action alignment (linking frame
heads). Calculating IAA for each stage separately
45
helped to isolate the disagreements and to see,
which stages resulted in a low agreement value
overall. To look at the most common role dis-
agreements we also created the pairwise agree-
ment matrix, every cell (i, j) of which is the num-
ber of times the role i was confused with the role
j by any pair of annotators.
3.4 Kendall?s Tau Rank Correlation With
Human Judgments
For the set of translations used in our experiments,
we had a number of relative human judgments (the
set was taken from WMT13
2
). We used the rank
aggregation method described in (Callison-Burch
et al., 2012) to build up one ranking from these
judgments. This method is called Expected Win
Score (EWS) and for MT system S
i
from the set
{S
j
} it is defined the following way:
score(S
i
) =
1
|{S
j
}|
?
j,j 6=i
win(S
i
, S
j
)
win(S
i
, S
j
) + win(S
j
, S
i
)
Where win(S
i
, S
j
) is the number of times system
i was given a rank higher than system j. This
method of aggregation was used to obtain the com-
parisons of systems, which outputs were never
presented together to assessors during the evalu-
ation procedure at WMT13.
After we had obtained the ranking of systems
by human judgments, we compared this ranking
to the ranking by HMEANT values of machine
translations. To do that, we used Kendall?s tau
(Kendall, 1938) rank correlation coefficient and
reported the results as Lo and Wu (2011a) and Bo-
jar (Bojar and Wu, 2012).
4 Experimental Setup
4.1 Test Set
For our experiments we used the set of translations
from WMT13. We tested HMEANT on a set of
four best MT systems (Bojar et al., 2013) for the
English-Russian language pair (Table 2).
From the set of direct English-Russian transla-
tions (500 sentences) we picked those which al-
lowed to build a ranking for the four systems (94
sentences); then out of these we randomly picked
50 and split them into 6 tasks of 25 so that each
of the 50 sentences was present in exactly three
tasks. Each task consisted of 25 reference transla-
tions and 100 hypothesis translations.
2
http://statmt.org/wmt13
System EWS (WMT)
PROMT 0.4949
Online-G 0.475
Online-B 0.3898
CMU-Primary 0.3612
Table 2. The top four MT systems for the en-ru
translation task at WMT13. The scores were
calculated for the subset of translations which we
used in experiments.
4.2 Annotation Interface
As far as we know there is no publically available
interface for HMEANT annotation. Thus, first
of all, having the prototype (Lo and Wu, 2011b)
and taking into account comments and sugges-
tions of Bojar and Wu (2012) (e.g., ability to go
back within the phases of annotation), we created
a web-based interface for role labeling and align-
ment. This interface allows to annotate a set of
references with one machine translation at a time
(Figure 1) and to align actions and roles. We also
provided a timer which allowed to measure the
time required to label the predicates and roles.
4.3 Annotators
We asked to participate two groups of annota-
tors: 6 researchers with linguistic background (lin-
guists) and 4 developers without it. Every annota-
tor did exactly one task; each of the 50 sentences
was annotated by three linguists and at least two
developers.
5 Results
As a result of the experiment, 638 frames were
annotated in reference translations (overall) and
2 016 frames in machine translations. More de-
tailed annotation statistics are presented in Table
3. A closer look indicates that the ratio of aligned
frames and roles in references was larger than in
any of machine translations.
5.1 Manual Ranking
After the test set was annotated, we compared
manual ranking and ranking by HMEANT; on the
system level, these rankings were similar; how-
ever, on the sentence level, there was no correla-
tion between rankings at all. Thus we decided to
take a closer look at the manual assessments. For
the selected 4 systems most of the pairwise com-
46
Figure 1. The screenshot of SRL interface. The tables under the sentences contain the information
about frames (the active frame has a red border and is highlighted in the sentence, inactive frames (not
shown) are semi-transparent).
Source # Frames # Roles Aligned frames, % Aligned roles, %
Reference 638 1 671 86.21 % 74.15 %
PROMT 609 1 511 79.97 % 67.57 %
Online-G 499 1 318 77.96 % 66.46 %
Online-B 469 1 257 78.04 % 68.42 %
CMU-Primary 439 1 169 75.17 % 66.30 %
Table 3. Annotation statistics.
parisons were obtained in a transitive way, i. e.
using comparisons with other systems. Further-
more, we encountered a number of useless rank-
ings, where all the outputs were given the same
rank. After all, for many sentences the ranking
of systems was based on a few pairwise compar-
isons provided by one or two annotators. These
rankings seemed to be not very reliable, thus we
decided to rank four machine translations for each
of the 50 sentences manually to make sure that the
ranking has a strong ground. We asked 6 linguists
to do that task. The average pairwise rank correla-
tion (between assessors) reached 0.77, making the
overall ranking reliable; we aggregated 6 rankings
for each sentence using EWS.
5.2 Correlation with Manual Assessments
To look at HMEANT on a system level, we com-
pared rankings produced during manual assess-
ment and HMEANT annotation tasks. Those rank-
ings were then aggregated with EWS (Table 4).
It should be noticed that HMEANT allowed to
rank systems correctly. This fact indicates that
HMEANT has a good discriminatory power on the
level of systems, which is a decent argument for
System Manual HMEANT BLEU
PROMT 0.532 0.443 0.126
Online-G 0.395 0.390 0.146
Online-B 0.306 0.374 0.147
CMU-Primary 0.267 0.292 0.136
Table 4. EWS over manual assessments, EWS
over HMEANT and BLEU scores for MT
systems.
the usage of this metric. Also it is worth to note
that ranking by HMEANT matched the ranking by
the number of frames and roles (Table 3).
On a sentence level, we studied the rank corre-
lation of ranking by manual assessments and by
HMEANT values for each of the annotators. The
manual ranking was aggregated by EWS from the
manual evaluation task (see Section 5.1). Results
are reported in Table 5.
We see that resulting correlation values are sig-
nificantly lower than those reported by Lo and Wu
(2011a) ? our rank correlation values did not reach
0.43 on average across all the annotators (and even
0.28 as reported by Bojar and Wu (2012)).
47
Annotator ?
Linguist 1 0.0973
Linguist 2 0.3845
Linguist 3 0.1157
Linguist 4 -0.0302
Linguist 5 0.1547
Linguist 6 0.1468
Developer 1 0.1794
Developer 2 0.2411
Developer 3 0.1279
Developer 4 0.1726
Table 5. The rank correlation coefficients for
HMEANT and human judgments. Reliable
results (with p-value >0.05) are in bold.
5.3 Inter-Annotator Agreement
Following Lo and Wu (2011a) and Birch et al.
(2013) we report the IAA for the individual stages
of annotation and alignment. These results are
shown in Table 6.
Stage
Linguists Developers
Max Avg Max Avg
REF, id 0.959 0.803 0.778 0.582
MT, id 0.956 0.795 0.667 0.501
REF, class 0.862 0.715 0.574 0.466
MT, class 0.881 0.721 0.525 0.434
REF, actions 0.979 0.821 0.917 0.650
MT, actions 0.971 0.839 0.700 0.577
Actions ? align 0.908 0.737 0.429 0.332
Roles ? align 0.709 0.523 0.378 0.266
Table 6. The inter-annotator agreement for the
individual stages of annotation and alignment
procedures. Id, class, align stand for
identification, classification and alignment
respectively.
The results are not very different from those re-
ported in the papers mentioned above, except for
even lower agreement for developers. The fact
that the results could be reproduced on a new lan-
guage seems very promising, however, the lack of
training for the annotators without linguistic back-
ground resulted in lower inter-annotator agree-
ment.
Also we studied the most common role dis-
agreements for each pair of annotators (either lin-
guists or developers). As it can be deduced from
the IAA values, the agreement on all roles is lower
for linguists, however, both groups of annotators
share the roles on which the agreement is best of
all: Predicate, Agent, Locative, Negation, Tempo-
ral. Most common disagreements are presented in
Table 7.
Role A Role B %, L %, D
Whom What 18.0 15.2
Whom Who 13.7 23.1
Why None 17.0 22.3
How (manner) What 10.5 -
How (manner) How (degree) - 19.0
How (modal) Action 18.1 16.3
Table 7. Most common role disagreements. Last
columns (L for linguists, D for developers) stand
for the ratio of times Role A was confused with
Role B across all the label types (roles, predicate,
none).
These disagreements can be explained by the
fact that some annotators looked ?deeper? in the
sentence semantics, whereas other annotators only
tried to capture the shallow structure as fast as pos-
sible. This fact explains, for example, disagree-
ment on the Whom role ? for some sentences, e. g.
?mogli by ubedit~ politiqeskih liderov?
(?could persuade the political leaders?) it requires
some time to correctly mark politiqeskih lid-
erov (political leaders) as an answer to Whom,
not What. The disagreement on the Purpose (a lot
of times it was annotated only by one expert) is ex-
plained by the fact that there were no clear instruc-
tions on how to mark clauses. As for the Action
and Modal, this disagreement is based on the re-
quirement that Action should consist of one word
only; this requirement raised questions about com-
plex verbs, e.g. ?zakonqil delat~? (?stopped
doing?). It is ambiguous how to annotate these
verbs: some annotators decided to mark it as
Modal+Action, some ? as Action+What. Proba-
bly, the correct way to mark it should be just as
Action.
5.4 Efficiency
Additionnaly, we conducted an efficiency experi-
ment in the group of linguists. We measured the
average time required to annotate a predicate (in
reference or machine translation) and a role. Re-
sults are presented in Table 8.
48
Annotator
REF MT
Role Action Role Action
Linguist 1 14 26 11 36
Linguist 2 10 12 8 12
Linguist 3 13 14 8 23
Linguist 4 16 15 9 15
Linguist 5 13 20 11 24
Linguist 6 17 35 9 32
Table 8. Average times (in seconds) required to
annotate actions and roles.
These results look very promising; using the
numbers in Table 3, we get the average time re-
quired to annotate a sentence: 1.5 ? 2 minutes for a
reference (and even up to 4 minutes for slower lin-
guists) and 1.5 ? 2.5 minutes for a machine trans-
lation. Also for a group of ?slower? linguists (1, 5,
6) inter-annotator agreement was lower (-0.05 on
average) than between ?faster? linguists (2, 3, 4)
for all stages of annotation and alignment. Aver-
age time to annotate an action is similar for the ref-
erence and MT outputs, but it takes more time to
annotate roles in references than in machine trans-
lations.
6 Discussion
6.1 Problems with HMEANT
As we can see, HMEANT is an acceptably reliable
and efficient metric. However, we have met some
obstacles and problems with original instructions
during the experiments with Russian translations.
We believe that these obstacles are the main causes
of low inter-annotator agreement at the last stages
of annotation procedure and low correlation of
rankings.
Frame head (predicate) is required. This re-
quirement does not allow frames without predicate
at all, e.g. ?On mo