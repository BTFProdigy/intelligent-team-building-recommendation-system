A Comparative Study of Language Models for
Book and Author Recognition
O?zlem Uzuner and Boris Katz
MIT,Computer Science and Artificial Intelligence Laboratory,
Cambridge, MA 02139
{ozlem, boris}@csail.mit.edu
Abstract. Linguistic information can help improve evaluation of simi-
larity between documents; however, the kind of linguistic information to
be used depends on the task. In this paper, we show that distributions
of syntactic structures capture the way works are written and accurately
identify individual books more than 76% of the time. In comparison,
baseline features, e.g., tfidf-weighted keywords, function words, etc., give
an accuracy of at most 66%. However, testing the same features on au-
thorship attribution shows that distributions of syntactic structures are
less successful than function words on this task; syntactic structures vary
even among the works of the same author whereas features such as func-
tion words are distributed more similarly among the works of an author
and can more effectively capture authorship.
1 Introduction
Expression is an abstract concept that we define as ?the way people convey
particular content?. Copyrights protect an author?s expression of content where
content refers to the information contained in a work and expression refers to
the linguistic choices of authors in presenting this content. Therefore, capturing
expression is important for copyright infringement detection.
In this paper, we evaluate syntactic elements of expression in two contexts:
book recognition for copyright infringement detection and authorship attribu-
tion. Our first goal is to enable identification of individual books from their
expression of content, even when they share content, and even when they are
written by the same person. For this purpose, we use a corpus that includes
translations of the same original work into English by different people. For the
purposes of this study, we refer to the translations as books and an original work
itself as a title.
Given the syntactic elements of expression, our second goal is to test them on
authorship attribution, where the objective is to identify all works by a particu-
lar author. Our syntactic elements of expression capture differences in the way
people express content and could be useful for authorship attribution. However,
the experiments we present here indicate that syntactic elements of expression
are more successful at identifying expression in individual books while function
words are more successful at identifying authors.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 969?980, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
970 O?. Uzuner and B. Katz
2 Related Work
In text classification literature, similarity of works has been evaluated, for ex-
ample, in terms of genre, e.g., novels vs. poems, in terms of the style of au-
thors, e.g., Austen?s novels vs. Kipling?s novels, and in terms of topic, e.g., sto-
ries about earthquakes vs. stories about volcanoes. In this paper, we compare
several different language models in two different classification tasks: book recog-
nition based on similarity of expression, and authorship attribution. Authorship
attribution has been studied in the literature; however, evaluation of similar-
ity of expression, e.g., Verne?s 20000 Leagues vs. Flaubert?s Madame Bovary,
is a novel task that we endeavor to address as a first step towards copyright
infringement detection.
We define expression as ?the linguistic choices of authors in presenting con-
tent?: content of works and the linguistic choices made while presenting it to-
gether constitute expression. Therefore, capturing expression requires measuring
similarity of works in terms of both of these components.
To classify documents based on their content, most approaches focus on key-
words. Keywords contain information regarding the ideas and facts presented
in documents and, despite being ambiguous in many contexts, have been heav-
ily exploited to represent content. In addition to keywords, subject?verb and
verb?object relationships [12], noun phrases [12,13], synonym sets of words from
WordNet [12], semantic classes of verbs [12] from Levin?s studies [21], and proper
nouns have all been used to capture content.
Linguistic choices of authors have been studied in stylometry for authorship
attribution. Brinegar [7], Glover [9] and Mendenhall [22], among others, used
distribution of word lengths to identify authors, e.g., Glover and Hirst studied
distributions of two- and three-letter words [9]. Thisted et al [33] and Holmes [14]
studied the idea of richness of vocabulary and the rate at which new words are
introduced to the text. Many others experimented with distributions of sentence
lengths [9,18,24,30,31,32,38,40], sequences of letters [17,20], and syntactic classes
(part of speech) of words [9,20,19].
Mosteller and Wallace [25] studied the distributions of function words to
identify the authors of 12 unattributed Federalist papers. Using a subset of the
function words from Mosteller and Wallace?s work, Peng [26] showed that verbs
(used as function words, e.g., be, been, was, had) are important for differentiating
between authors. Koppel et al [19] studied the ?stability? of function words and
showed that the features that are most useful for capturing the style of authors
are ?unstable?, i.e., they can be replaced without changing the meaning of the
text. Koppel et al?s measure of stability identified function words, tensed verbs,
and some part-of-speech tag trigrams as unstable.
Syntactically more-informed studies of the writings of authors came from
diMarco and Wilkinson [39] who treated style as a means for achieving par-
ticular communicative goals and used parsed text to study the syntactic ele-
ments associated with each goal, e.g., clarity vs. obscurity. Adapting elements
from Halliday and Hasan [10,11], diMarco et al studied the use of cohesive el-
ements of text, e.g., anaphora and ellipsis, and disconnective elements of text,
A Comparative Study of Language Models for Book and Author Recognition 971
e.g., parenthetical constructions, as well as the patterns in the use of relative
clauses, noun embeddings, and hypotaxis (marked by subordinating conjunc-
tions) when authors write with different communicative goals.
Expression is related to both content and style. However, it is important to
differentiate expression from style. Style refers to the linguistic elements that,
independently of content, persist over the works of an author and has been widely
studied in authorship attribution. Expression involves the linguistic elements
that relate to how an author phrases particular content and can be used to
identify potential copyright infringement.
3 Syntactic Elements of Expression
We hypothesize that, given particular content, authors choose from a set of
semantically equivalent syntactic constructs to create their own expression of
it. As a result, different authors may choose to express the same content in
different ways. In this paper, we capture the differences in expression of authors
by studying [34,35,36]:
? sentence-initial and -final phrase structures that capture the shift in focus
and emphasis of a sentence due to reordered material,
? semantic classes and argument structures of verbs such as those used in
START for question answering [16] and those presented by Levin [21],
? syntactic classes of embedding verbs, i.e., verbs that take clausal arguments,
such as those studied by Alexander and Kunz [1] and those used in START
for parsing and generation [15], and
? linguistic complexity of sentences, measured both in terms of depths of
phrases and in terms of depths of clauses, examples of which are shown
in Table 1.
Table 1. Sample sentences broken down into their clauses and the depth of the top-
level subject (the number on the left) and predicate (the number on the right)
Sentence Depth of
Clauses
[I]a [would not think that [this]b [was possible]b]a 0, 2
[I]a [have found [it]b [difficult to say that [I]c [like it]c]b] a. 2, 2
[That [she]b [would give such a violent reaction]b]a [was unexpected]a. 1, 1
[For [her]b [to see this note]b]a [is impossible]a. 1, 1
[Wearing the blue shirt]a [was a good idea]a. 1, 1
[It]a [is not known whether [he]b [actually libelled the queen]b]a. 0, 2
[He]a [was shown that [the plan]b [was impractical]b ]a. 0, 2
[They]a [believed [him]b [to be their only hope]b]a. 0, 2
[I]a [suggest [he]b [go alone]b]a. 0, 2
[I]a [waited for [John]b [to come]b]a. 0, 2
972 O?. Uzuner and B. Katz
We extracted all of these features from part-of-speech tagged text [5] and
studied their distributions in different works. We also studied their correlations
with each other, e.g., semantic verb classes and the syntactic structure of the
alternation [21] in which they occur. The details of the relevant computations
are discussed by Uzuner [34].
3.1 Validation
We validated the syntactic elements of expression using the chi-square (and/or
likelihood ratio) test of independence. More specifically, for each of sentence-
initial and -final phrase structures, and semantic and syntactic verb classes, we
tested the null hypothesis that these features are used similarly by all authors
and that the differences observed in different books are due to chance. We per-
formed chi-square tests in three different settings: on different translations of
the same title (similar content but different expression), on different books by
different authors (different content and different expression), and on disjoint sets
of chapters from the same book (similar content and similar expression).
For almost all of the identified features, we were able to reject the null hy-
pothesis when comparing books that contain different expression, indicating that
regardless of content, these features can capture expression. For all of the fea-
tures, we were unable to reject the null hypothesis when we compared chapters
from the same book, indicating a certain consistency in the distributions of these
features throughout a work.
4 Evaluation
We used the syntactic elements of expression, i.e., sentence-initial and sentence-
final phrase structures, semantic and syntactic classes of verbs, and measures of
linguistic complexity [34,35,36], for book recognition and for authorship attribu-
tion.
4.1 Baseline Features
To evaluate the syntactic elements of expression, we compared the performance
of these features to baseline features that capture content and baseline features
that capture the way works are written. Our baseline features that capture con-
tent included tfidf-weighted keywords [27,28] excluding proper nouns, because
for copyright infringement purposes, proper nouns can easily be changed without
changing the content or expression of the documents and a classifier based on
proper nouns would fail to recognize otherwise identical works. Baseline features
that focus on the way people write included function words [25,26], distributions
of word lengths [22,40], distributions of sentence lengths [14], and a basic set
of linguistic features, extracted from tokenized, part-of-speech tagged, and/or
syntactically parsed text. This basic set of linguistic features included the num-
ber of words and the number of sentences in the document; type?token ratio;
A Comparative Study of Language Models for Book and Author Recognition 973
average and standard deviation of the lengths of words (in characters) and of
the lengths of sentences (in words) in the document; frequencies of declarative
sentences, interrogatives, imperatives, and fragmental sentences; frequencies of
active voice sentences, be-passives, and get-passives; frequencies of ?s-genitives,
of-genitives, and of phrases that lack genitives; frequency of overt negations; and
frequency of uncertainty markers [9,34].
4.2 Classification Experiments
We compared the syntactic elements of expression with the baseline features
in two separate experiments: recognizing books even when some of them are
derived from the same title (different translations) and recognizing authors. For
these experiments, we split books into chapters, created balanced sets of relevant
classes, and used boosted [29] decision trees [41] to classify chapters into books
and authors. We tuned parameters on the training set: we determined that the
performance of classifiers stabilized at around 200 rounds of boosting and we
eliminated from each feature set the features with zero information gain [8,37].
Recognizing Books: Copyrights protect original expression of content for a
limited time period. After the copyright period of a work, its derivatives by
different people are eligible for their own copyright and need to be recognized
from their unique expression of content. Our experiment on book recognition
focused on and addressed this scenario.
Data: For this experiment, we used a corpus that included 49 books derived from
45 titles ; for 3 of the titles, the corpus included multiple books (3 books for the
title Madame Bovary, 2 books for 20000 Leagues, and 2 books for The Kreutzer
Sonata). The remaining titles included works from J. Austen, F. Dostoyevski,
C. Dickens, A. Doyle, G. Eliot, G. Flaubert, T. Hardy, I. Turgenev, V. Hugo,
W. Irving, J. London, W. M. Thackeray, L. Tolstoy, M. Twain, and J. Verne.
We obtained 40?50 chapters from each book (including each of the books that
are derived from the same title), and used 60% of the chapters from each book
for training and the remaining 40% for testing.
Results: The results of this evaluation showed that the syntactic elements of
expression accurately recognized books 76% of the time; they recognized each
of the paraphrased books 89% of the time (see right column in Table 2). In
either case, the syntactic elements of expression significantly outperformed all
individual baseline features (see Table 2).
The syntactic elements of expression contain no semantic information; they
recognize books from the way they are written. The fact that these features
can differentiate between translations of the same title implies that translators
add their own expression to works, even when their books are derived from
the same title, and that the expressive elements chosen by each translator help
differentiate between books derived from the same title.
Despite recognizing books more accurately than each of the individual base-
line features, syntactic elements of expression on their own are less effective
974 O?. Uzuner and B. Katz
Table 2. Classification results on the test set for recognizing books from their expres-
sion of content even when some books contain similar content
Feature Set Accuracy on complete Accuracy on
corpus paraphrases only
Syntactic elements of expression 76% 89%
Tfidf-weighted keywords 66% 88%
Function words 61% 81%
Baseline linguistic 42% 53%
Dist. of word length 29% 72%
Dist. of sentence length 13% 14%
than the combined baseline features in recognizing books; the combined baseline
features give an accuracy of 88% on recognizing books (compare this to 76%
accuracy by the syntactic elements of expression alone). But the performance of
the combined baseline features is further improved by the addition of syntactic
elements of expression (see Table 3). This improvement is statistically significant
at ? = 0.05.
Table 3. Classification results of combined feature sets on the test set for book recog-
nition even when some books contain similar content
Feature Set Accuracy on complete Accuracy on
corpus paraphrases only
All baseline features +
syntactic elements of expression 92% 98%
All baseline features 88% 97%
Ranking the combined features based on information gain for recognizingbooks
shows that the syntactic elements of expression indeed play a significant role in rec-
ognizing books accurately; of the top tenmost useful features identifiedby informa-
tion gain, seven are syntactic elements of expression (see rows in italics in Table 4).
In the absence of syntactic elements of expression, the top ten most useful
features identified by information gain from the complete set of baseline features
reveal that the keywords ?captain? and ?sister? are identified as highly discrim-
inative features. Similarly, the function words ?she?, ?her?, and ??ll? are highly
discriminative (see Table 5). Part of the predictive power of these features is due
to the distinct contents of most of the books in this corpus; we expect that as
the corpus grows, these words will lose predictive power.
Recognizing Authors: In Section 2, we described the difference between style
and expression. These concepts, though different, both relate to the way people
write. Then, an interesting question to answer is: Can the same set of features
help recognize both books (from their unique expression) and authors (from their
unique style)?
A Comparative Study of Language Models for Book and Author Recognition 975
Table 4. Top ten features identified by information gain for recognizing books even
when some books share content. Features which are syntactic elements of expression
are in italics; baseline features are in roman.
Features
Std. dev. of the depths of the top-level left branches (measured in phrase depth)
Std. dev. of the depths of the top-level right branches (measured in phrase depth)
Std. dev. of the depths of the deepest prepositional phrases of sentences
(measured in phrase depth)
% of words that are one character long
Average word length
% of sentences that contain unembedded verbs
% of sentences that contain an unembedded verb with noun phrase object (0-V-NP)
Frequency of the word ?the? (normalized by chapter length)
Avg. depth of the subordinating clauses at the beginning of sentences
(measured in phrase depth)
% of sentences that contain equal numbers of clauses in left and right branch
Type-token ratio
Table 5. Top ten baseline features identified by information gain that recognize books
even when some books share content
Features
% words that are one character long
Average word length
Frequency of the word ?the? (normalized by chapter length)
Type-token ratio
Frequency of the word ?captain? (tfidf-weighted)
Probability of Negations
Frequency of the word ?sister? (tfidf-weighted)
Frequency of the word ?she? (normalized by chapter length)
Frequency of the word ?her? (normalized by chapter length)
Frequency of the word ??ll? (normalized by chapter length)
Data: In order to answer this question, we experimented with a corpus of books
that were written by native speakers of English. This corpus included works from
eight authors: three titles by W. Irving, four titles by G. Eliot, five titles by J.
Austen, six titles by each of C. Dickens and T. Hardy, eight titles by M. Twain,
and nine titles by each of J. London and W. M. Thackeray.
Results: To evaluate the different sets of features on recognizing authors from
their style, we trained models on a subset of the titles by each of these authors
and tested on a different subset of titles by the same authors. We repeated this
experiment five times so that several different sets of titles were trained and tested
on. At each iteration, we used 150 chapters from each of the authors for training
and 40 chapters from each of the authors for testing.
976 O?. Uzuner and B. Katz
Table 6. Results for authorship attribution. Classifier is trained on 150 chapters from
each author and tested on 40 chapters from each author. The chapters in the training
and test sets come from different titles.
Feature Set AccuracyAccuracyAccuracyAccuracyAccuracy
Run 1 Run 2 Run 3 Run 4 Run 5
Function words 86% 89% 87% 90% 81%
Syntactic elements of expression 64% 63% 64% 55% 62%
Distribution of word length 33% 37% 44% 53% 35%
Baseline linguistic 39% 39% 41% 48% 28%
Distribution of sentence length 33% 41% 31% 41% 25%
Table 7. Average classification results on authorship attribution
Feature Set Avg. Accuracy
Function words 87%
Syntactic elements of expression 62%
Distribution of word length 40%
Baseline linguistic 39%
Distribution of sentence length 34%
The results in Table 7 show that function words capture the style of authors
better than any of the other features; syntactic elements of expression are not as
effective as function words in capturing the style of authors. This finding is consis-
tent with our intuition: we selected the syntactic elements of expression for their
ability to differentiate between individual works, even when some titles are written
by the same author and even when some books were derived from the same title.
Recognizing the style of an author requires focus on the elements that are similar in
the works written by the same author, instead of focus on elements that differenti-
ate these works. However, the syntactic elements of expression are not completely
devoid of any style information: they recognize authors accurately 62%of the time.
In comparison, the function words recognize authors accurately 87% of the time.
Top ten most predictive function words identified by information gain for author-
ship attribution are: the, not, of, she, very, be, her, ?s, and, and it.
Combining the baseline features together does not improve the performance of
function words on authorship attribution: function words give an accuracy of 87%
by themselves whereas the combined baseline features give an accuracy of 86%.1
Adding the syntactic elements of expression to the combination of baseline features
hurts performance (see Table 8).
We believe that the size of the corpus is an important factor in this conclu-
sion. More specifically, we expect that as more authors are added to the corpus,
the contribution of syntactic elements of expression to authorship attribution will
increase. To test this hypothesis, we repeated our experiments with up to thir-
teen authors. We observed that the syntactic elements of expression improved the
1 This difference is not statistically significant.
A Comparative Study of Language Models for Book and Author Recognition 977
Table8.Average classification results of combined feature sets on authorship attribution
Feature Set Average Accuracy for 8 Authors
All baseline features +
syntactic elements of expression 81%
All baseline features 86%
Function words 87%
Syntactic elements
of expression 62%
Table 9. Average classification results of combined feature sets on authorship attribu-
tion. For these experiments, the original corpus was supplemented with works from W.
Ainsworth, L. M. Alcott, T. Arthur, M. Braddon, and H. James.
Feature Set Average Accuracy for 8-13 Authors
8 9 10 11 12 13
All baseline features +
syntactic elements of expression 81% 88% 88.4% 87.6% 88% 88%
All baseline features 86% 86% 87.8% 86.6% 86% 86.8%
Function words 87% 86.4% 85.4% 85.2% 84.8% 82.6%
Syntactic elements
of expression 62% 65.6% 68.2% 67.4% 66% 64.4%
performance of the baseline features: as we added more authors to the corpus, the
performance of function words degraded, the performance of syntactic elements of
expression improved, and the performance of the combined feature set remained
fairly consistent at around 88% (see Table 9).
4.3 Conclusion
In this paper, we compared several different language models on two classifica-
tion tasks: book recognition and authorship attribution. In particular, we evalu-
ated syntactic elements of expression consisting of sentence-initial and -final phrase
structures, semantic and syntactic categories of verbs, and linguistic complexity
measures, on recognizing books (even when they are derived from the same title)
and on recognizing authors. Through experiments on a corpus of novels, we have
shown that syntactic elements of expression outperform all individual baseline fea-
tures in recognizing books and when combined with the baseline features, they im-
prove recognition of books.
In our authorship attribution experiments, we have shown that the syntactic
elements of expression are not as useful as function words in recognizing the style
978 O?. Uzuner and B. Katz
of authors. This finding highlights the need for a task-dependent approach to en-
gineering feature sets for text classification. In our experiments, feature sets that
have been engineered for studying expression and the language models based on
these feature sets outperform all others in identifying expression. Similarly, feature
sets that have been engineered for studying style and the language models based
on these feature sets outperform syntactic elements of expression in authorship
attribution.
References
1. D. Alexander and W. J. Kunz. SomeClasses of Verbs inEnglish. Linguistics Research
Project. Indiana University, 1964.
2. J. C. Baker. A Test of Authorship Based on the Rate at which New Words Enter an
Author?s Text. Journal of the Association for Literary and Linguistic Computing,
3(1), 36?39, 1988.
3. D. Biber. A Typology of English Texts. Language, 27, 3?43, 1989.
4. D. Biber, S. Conrad, and R. Reppen. Corpus Linguistics: Investigating Language
Structure and Use. Cambridge University Press, 1998.
5. E. Brill. A Simple Rule-Based Part of Speech Tagger. Proceedings of the 3rd Con-
ference on Applied Natural Language Processing, 1992.
6. M. Diab, J. Schuster, and P. Bock. A Preliminary Statistical Investigation into the
Impact of an N-GramAnalysisApproach based onWordSyntacticCategories toward
Text Author Classification. In Proceedings of Sixth International Conference on
Artificial Intelligence Applications, 1998.
7. C. S. Brinegar. Mark Twain and the QuintusCurtius Snodgrass Letters: A Statistical
Test of Authorship. Journal of the American Statistical Association, 58, 85?96, 1963.
8. G. Forman. An Extensive Empirical Study of Feature Selection Metrics for Text
Classification. Journal of Machine Learning Research, 3, 1289?1305, 2003.
9. A. Glover and G. Hirst. Detecting stylistic inconsistencies in collaborative writing. In
Sharples, Mike and van derGeest, Thea (eds.), The new writing environment: Writers
at work in a world of technology. London: Springer-Verlag, 1996.
10. M. Halliday and R. Hasan. Cohesion in English. London: Longman, 1976.
11. M. Halliday. An introduction to functional grammar. London; Baltimore, Md., USA
: Edward Arnold, 1985.
12. V. Hatzivassiloglou, J. Klavans, and E. Eskin. Detecting Similarity by Applying
Learning over Indicators. 37th Annual Meeting of the ACL, 1999.
13. V. Hatzivassiloglou, J. Klavans, M. Holcombe, R. Barzilay, M.Y. Kan, and
K.R. McKeown. SimFinder: A Flexible Clustering Tool for Summarization.
NAACL?01 Automatic Summarization Workshop, 2001.
14. D. I. Holmes. Authorship Attribution. Computers and the Humanities, 28, 87?106.
Kluwer Academic Publishers, Netherlands, 1994.
15. B. Katz. Using English for Indexing and Retrieving. Artificial Intelligence at MIT:
Expanding Frontiers. P. H. Winston and S. A. Shellard, eds. MIT Press. Cambridge,
MA., 1990.
16. B. Katz andB. Levin. ExploitingLexical Regularities in Designing Natural Language
Systems. In Proceedings of the 12th International Conference on Computational Lin-
guistics, COLING ?88, 1988.
17. D. Khmelev and F. Tweedie. Using Markov Chains for Identification of Writers.
Literary and Linguistic Computing, 16(4), 299?307, 2001.
A Comparative Study of Language Models for Book and Author Recognition 979
18. G. Kjetsaa. The Authorship of the Quiet Don. ISBN 0391029487. International Spe-
cialized Book Service Inc., 1984.
19. M. Koppel, N. Akiva, and I. Dagan. A Corpus-Independent Feature Set for Style-
Based Text Categorization. Proceedings of IJCAI?03 Workshop on Computational
Approaches to Style Analysis and Synthesis, 2003.
20. O. V. Kukushkina, A. A. Polikarpov, and D. V. Khemelev. Using Literal and Gram-
matical Statistics for Authorship Attribution. Published in Problemy Peredachi In-
formatsii,37(2), April-June 2000, 96?108. Translated in ?Problems of Information
Transmission?, 172?184.
21. B. Levin. English Verb Classes and Alternations. A Preliminary Investigation. ISBN
0-226-47533-6. University of Chicago Press. Chicago, 1993.
22. T. C. Mendenhall. Characteristic Curves of Composition. Science, 11, 237?249,
1887.
23. G. A. Miller, E. B. Newman, and E. A. Friedman.: Length-Frequency Statistics for
Written English. Information and Control,1(4), 370?389, 1958.
24. A. Q. Morton. The Authorship of Greek Prose. Journal of the Royal Statistical
Society (A), 128, 169?233, 1965.
25. F. Mosteller and D. L. Wallace. Inference in an authorship Problem. Journal of the
American Statistical Association, 58(302), 275?309, 1963.
26. R. D. Peng and H. Hengartner. Quantitative Analysis of Literary Styles. The Amer-
ican Statistician, 56(3), 175?185, 2002.
27. G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval.
Information Processing and Management, 24(5), 513?523, 1998.
28. G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing.
Communications of the ACM, 18(11), 613?620, 1975.
29. R. E. Schapire. The Boosting Approach to Machine Learning. In MSRI Workshop
on Nonlinear Estimation and Classification, 2002.
30. H. S. Sichel. On a Distribution Representing Sentence-Length in Written Prose.
Journal of the Royal Statistical Society (A), 137, 25?34, 1974.
31. M. W. A. Smith. Recent Experience and New Developments of Methods for the
Determination of Authorship. Association for Literary and Linguistic Computing
Bulletin, 11, 73?82, 1983.
32. D. R. Tallentire. An Appraisal of Methods and Models in Computational Stylistics,
with Particular Reference to Author Attribution. PhD Thesis. University of Cam-
bridge, 1972.
33. R. Thisted and B. Efron. Did Shakespeare Write a Newly-discovered Poem?
Biometrika, 74, 445?455, 1987.
34. O?. Uzuner. Identifying Expression Fingerprints using Linguistic Information. Ph.D.
Dissertation. Massachusetts Institute of Technology, 2005.
35. O?. Uzuner and B. Katz. Capturing Expression Using Linguistic Information. In
Proceedings of the 20th National Conference on Artificial Intelligence (AAAI-05),
2005.
36. O?. Uzuner, B. Katz and Thade Nahnsen. Using Syntactic Information to Identify
Plagiarism. In Proceedings of the Association for Computational Linguistics Work-
shop on Educational Applications (ACL 2005), 2005.
37. Y. Yang and J. O. Pedersen. A Comparative Study on Feature Selection in Text Cat-
egorization. In Proceedings of ICML-97, 14th International Conference on Machine
Learning. 412?420, 1997.
980 O?. Uzuner and B. Katz
38. G. U. Yule. On Sentence-Length as a Statistical Characteristic of Style in Prose,
with Application to Two Cases of Disputed Authorship. Biometrika, 30, 363?390,
1938.
39. J. Wilkinson and C. diMarco. Automated Multi-purpose Text Processing. In Pro-
ceedings of IEEE Fifth Annual Dual-Use Technologies and Applications Conference,
1995.
40. C. B. Williams. Mendenhall?s Studies of Word-Length Distribution in the Works of
Shakespeare and Bacon. Biometrika, 62(1), 207?212, 1975.
41. I. H. Witten and E. Frank. Data Mining: Practical machine Learning Tools with
Java Implementations. Morgan Kaufmann, San Francisco, 2000.
Lexical Chains and Sliding Locality Windows in Content-based 
Text Similarity Detection 
Thade Nahnsen, ?zlem Uzuner, Boris Katz 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Cambridge, MA 02139 
{tnahnsen,ozlem,boris}@csail.mit.edu 
 
 
Abstract 
We present a system to determine 
content similarity of documents. 
Our goal is to identify pairs of book 
chapters that are translations of the 
same original chapter.  Achieving 
this goal requires identification of 
not only the different topics in the 
documents but also of the particular 
flow of these topics.   
Our approach to content 
similarity evaluation employs n-
grams of lexical chains and 
measures similarity using the 
cosine of vectors of n-grams of 
lexical chains, vectors of tf*idf-
weighted keywords, and vectors of 
unweighted lexical chains 
(unigrams of lexical chains).  Our 
results show that n-grams of 
unordered lexical chains of length 
four or more are particularly useful 
for the recognition of content 
similarity. 
1   Introduction 
This paper addresses the problem of determining 
content similarity between chapters of literary 
novels.  We aim to determine content similarity 
even when book chapters contain more than one 
topic by resolving exact content matches rather 
than finding similarities in dominant topics.   
Our solution to this problem relies on lexical 
chains extracted from WordNet [6]. 
2   Related Work 
Lexical Chains (LC) represent lexical items 
which are conceptually related to each other, for 
example, through hyponymy or synonymy 
relations.  Such conceptual relations have 
previously been used in evaluating cohesion, 
e.g., by Halliday and Hasan [2, 3].    Barzilay 
and Elhadad [1] used lexical chains for text 
summarization; they identified important 
sentences in a document by retrieving strong 
chains.  Silber and McCoy [7] extended the 
work of Barzilay and Elhadad; they developed 
an algorithm that is linear in time and space for 
efficient identification of lexical chains in large 
documents.  In this algorithm, Silber and McCoy 
first created a text representation in the form of 
metachains, i.e., chains that capture all possible 
lexical chains in the document.  After creating 
the metachains, they used a scoring algorithm to 
identify the lexical chains that are most relevant 
to the document, eliminated unnecessary 
overhead information from the metachains, and 
selected the lexical chains representing the 
document.  Our method for building lexical 
chains follows this algorithm. 
N-gram based language models, i.e., models 
that divide text into n-word (or n-character) 
strings, are frequently used in natural language 
processing.  In plagiarism detection, the overlap 
of n-grams between two documents has been 
used to determine whether one document 
plagiarizes another [4].   In general, n-grams 
capture local relations.  In our case, they capture 
local relations between lexical chains and 
between concepts represented by these chains.   
Three main streams of research in content 
similarity detection are: 1) shallow, statistical 
analysis of documents, 2) analysis of rhetorical 
relations in texts [5], and 3) deep syntactic 
150
analysis [8]. Shallow methods do not include 
much linguistic information and provide a very 
rough model of content while approaches that 
use syntactic analysis generally require 
significant computation. Our approach strikes a 
compromise between these two extremes: it uses 
the linguistic knowledge provided in WordNet 
as a way of making use of low-cost linguistic 
information for building lexical chains that can 
help detect content similarity.   
3   Lexical Chains in Content Similarity 
Detection 
3.1   Corpus 
The experiments in this paper were performed 
on a corpus consisting of chapters from 
translations of four books (Table 1) that cover a 
variety of topics.  Many of the chapters from 
each book deal with similar topics; therefore, 
fine-grained content analysis is required to 
identify chapters that are derived from the same 
original chapter. 
 
# 
translati
ons 
Title # 
chapters 
2 20,000 Leagues under the Sea 47 
3 Madame Bovary 35 
2 The Kreutzer Sonata 28 
2 War and Peace 365 
Table 1: Corpus 
3.2   Computing Lexical Chains 
Our approach to calculating lexical chains uses 
nouns, verbs, and adjectives present in 
WordNetV2.0.  We first extract such words 
from each chapter in the corpus and represent 
each chapter as a set of these word instances {I1, 
?, In}.  Each instance of each of these words 
has a set of possible interpretations, IN, in 
WordNet.  These interpretations are either the 
synsets or the hypernyms of the instances.  
Given these interpretations, we apply a slightly 
modified version of the algorithm by Silber and 
McCoy [7] to automatically disambiguate 
nouns, verbs, and adjectives, i.e., to select the 
correct interpretation, for each instance.  Silber 
and McCoy?s algorithm computes all of the 
scored metachains for all senses of each word in 
the document and attributes the word to the 
metachain to which it contributes the most.   
During this process, the algorithm computes the 
contribution of a word to a given chain by 
considering 1) the semantic relations between 
the synsets of the words that are members of the 
same metachain, and 2) the distance between 
their respective instances in the discourse.  Our 
approach uses these two parameters, with minor 
modifications.  Silber and McCoy measure 
distance in terms of paragraphs on prose text; 
we measure distance in terms of sentences in 
order to handle both dialogue and prose text.  
 
 
Figure 1: Intermediate representation after 
eliminating words that are not nouns, verbs, or 
adjectives and after identifying lexical chains 
(represented by WordNet synset IDs).  Note that 
{kitchen, bathroom} are represented by the same 
synset ID which corresponds to the synset ID of 
their common hypernym ?room?.  {kitchen, 
bathroom} is a lexical chain.  Ties are broken in 
favor of hypernyms. 
 
Following Silber and McCoy, we allow 
different types of conceptual relations to 
contribute differently to each lexical chain, i.e., 
the contribution of each word to a lexical chain 
is dependent on its semantic relation to the chain 
(see Table 2).  After scoring, concepts that are 
dominant in the text segment are identified and 
each word is represented by only the WordNet 
ID of the synset (or the hypernym/hyponym set) 
that best fits its local context.  Figure 1 gives an 
example of the resulting intermediate 
representation, corresponding to the 
interpretation, S, found for each word instance, 
I, that can be used to represent each chapter, C, 
where C = {S1, ?, Sm}.   
 
Lexical 
semantic 
relation 
Distance <= 
6 sentences 
Distance > 
6 sentences 
Same word 1 0 
Hyponym 0.5 0 
Hypernym 0.5 0 
Sibling 0.2 0 
Table 2: Contribution to lexical chains 
Original document (underlined words are represented 
with lexical chains): 
The furniture in the kitchen seems beautiful, but the bathroom 
seems untidy. 
 
Intermediate representation (lexical chains): 
03281101   03951013   02071636   00218842   03951013  
02071636   02336718    
 
151
3.3 Determining the Locality Window 
After computing the lexical chains, we created a 
representation for text by substituting the correct 
lexical chain for each noun, verb, and adjective 
in each document. We omitted the remaining 
parts of speech from the documents (see Figure 
1 for sample intermediate representation). We 
obtained ordered and unordered n-grams of 
lexical chains from this representation. 
Ordered n-grams consist of n consecutive 
lexical chains extracted from text. These ordered 
n-grams preserve the original order of the lexical 
chains in the text. Corresponding unordered n-
grams disregard this order. The resulting text 
representation is T = {gram1, gram2, ?, gramn}, 
where grami = [lc1, ?,  lcn], where lci ? {I1, ?, 
Ik} (the chains that represent Chapter C). The 
elements in grami may be sorted or unsorted, 
depending on the selected method.  N-grams are 
extracted from text using sliding locality 
windows and provide what we call ?attribute 
vectors?. The attribute vector for ordered n-
grams has the form C = {(e1, ?, en), (e2, ?, 
en+1), ?, (em-n, ?, em)} where (e1, ?, en) is an 
ordered n-gram and em is the last lexical chain in 
the chapter.  For unordered n-grams, the 
attribute vector has the form C = {sort[(e1, ?, 
en)], sort[(e2, ?, en+1)], ?, sort[(em-n, ?, em)]} 
where sort[?] indicates alphabetical sorting of 
chains (rather than the actual order in which the 
chains appear in the text).  
We evaluated similarity between pairs of 
book chapters using the cosine of the attribute 
vectors of n-grams of lexical chains (sliding 
locality windows of width n).  We varied the 
width of the sliding locality windows from two 
to five elements.  
4   Evaluation 
We used cosine similarity as the distance metric, 
computed the cosine of the angle between the 
vectors of pairs of documents in the corpus, and 
ranked the pairs based on this score.  We 
identified the top n most similar pairs (also 
referred to as ?selection level of n?) and 
considered them to be similar in content. 
We calculated similarity between pairs of 
documents in several different ways, evaluated 
these approaches with the standard information 
retrieval measures, i.e., precision, recall, and f-
measure, and compared our results with two 
baselines. The first baseline measured the 
similarity of documents with tf*idf-weighted 
keywords; the second used the cosine of 
unweighted lexical chains (unigrams of lexical 
chains). 
The corpus of parallel translations provides 
data that can be used as ground truth for content 
similarity; corresponding chapters from different 
translations of the same original title are 
considered similar in content, i.e., chapter 1 of 
translation 1 of Madame Bovary is similar in 
content to chapter 1 of translation 2 of Madame 
Bovary. 
Figure 2 shows the f-measure of different 
methods for measuring similarity between pairs 
of chapters using ordered lexical chains, 
unordered lexical chains, and baselines.  These 
graphs present the results when the top 100?
1,600 most similar pairs in the corpus are 
considered similar in content and the rest are 
considered dissimilar (selection level of 100?
1,600).  The total number of chapter pairs is 
approximately 1,000,000. Of these, 1,080 (475 
unique chapters with 2 or 3 translations each) 
are considered similar for evaluation purposes. 
The results indicate that four similarity 
measures gave the best performance.  These 
were tri-grams, quadri-grams, penta-grams, and 
hexa-grams of unordered lexical chains.  The 
peak f-measure at the selection level of 1,100 
chapter pairs was 0.981. Chi squared tests 
performed on the f-measures (when the top 
1,100 pairs were considered similar) were 
significant at p = 0.001. 
Closer analysis of the graphs in Figure 2 
shows that, at the optimal selection level, n-
grams of ordered lexical chains of length greater 
than four significantly outperformed the baseline 
at p = 0.001 while n-grams of ordered lexical 
chains of length less than or equal to four are 
significantly outperformed by the baseline at the 
same p. A similar observation cannot be made 
for the n-grams of unordered lexical chains; for 
these n-grams, the performance degradation 
appears at n = 7, i.e., the corresponding curves 
have a steeper negative incline than the baseline.   
After the cut-off point of 1,100 chapter pairs, 
the performance of all algorithms declines. This 
is due to the evaluation method we have chosen: 
although the cut-off for similarity judgement can 
be increased, the number of chapters that are in 
fact similar does not change and at high cut-off 
values many dissimilar pairs are considered 
similar, leading to degradation in performance. 
152
Figures 2a and 2b show that some of the 
lexical chain representations do not outperform 
the tf*idf-weighted baseline.  A comparison of 
Figures 2a and 2b shows that, for n < 5, n-grams 
of ordered lexical chains perform worse than n-
grams of unordered lexical chains. This 
indicates that between different translations of 
the same book the order of chains changes 
significantly, but that the chains within 
contiguous regions (locality windows) of the 
texts remain similar.   
Interestingly, ordered n-grams of length 3 to 5 
perform significantly better than unordered n-
grams of the same length. This implies that, 
during translation, the order of the content 
words does not change enormously for three to 
five lexical chain elements.  Allowing flexible 
order for the lexical chains (i.e., unordered 
lexical chains) in these n-grams therefore hurts 
performance by allowing many false positives.  
However, for longer n-grams to be successful, 
the order of the lexical chains has to be flexible. 
Figure 2: F-Measure
. 
F-M e as u r e  vs . C h ap te r s  Se le cte d  (Un o r d e r e d  N-Gr am s )
0
0,1
0 ,2
0 ,3
0 ,4
0 ,5
0 ,6
0 ,7
0 ,8
0 ,9
1
100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600
C h ap te r s  Se le cte d
F
-M
ea
su
re
u2gram/LC u3gram/LC u4gram/LC u5gram/LC u6gram/LC
u7gram/LC tf *id f c os ine
 
(a) F-Measure: Unordered n-grams vs. the baselines 
F- M e a s u r e  v s . C h a p t e r s  S e le c t e d  ( O r d e r e d  N-G r a m s )
0
0 ,1
0 ,2
0 ,3
0 ,4
0 ,5
0 ,6
0 ,7
0 ,8
0 ,9
1
100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600
C h a p te r s  S e le c t e d
F
-M
ea
su
re
tf * id f c o s in e 4g ram/LC 5g ram/LC 6g ram/LC
7g ram/LC 2g ram/LC 3g ram/LC
 
(b) F-Measure: Ordered n-grams vs. the baselines 
   
  ngram/LC ? unordered n-grams of lexical chains are used in the attribute vector 
  ungram/LC ? ordered n-grams of lexical chains are used in the attribute vector  
  tf*idf ? tf*idf weighted words are used in the attribute vector 
  cosine ? the standard information retrieval measure; words are used in the attribute vector 
153
5   Future Work 
Currently, our similarity measures do not 
employ any weighting scheme for n-grams, i.e., 
every n-gram is given the same weight.  For 
example, the n-gram ?be it as it has been? in 
lexical chain form corresponds to synsets for the 
words be, have and be.  The trigram of these 
lexical chains does not convey significant 
meaning.  On the other hand, the n-gram ?the 
lawyer signed the heritage? is converted into the 
trigram of lexical chains of lawyer, sign, and 
heritage.  This trigram is more meaningful than 
the trigram be have be, but in our scheme both 
trigrams will get the same weight.  As a result, 
two documents that share the trigram be have be 
will look as similar as two documents that share 
lawyer sign heritage. This problem can be 
addressed in two possible ways: using a ?stop 
word? list to filter such expressions completely 
or giving different weights to n-grams based on 
the number of their occurrences in the corpus.   
6   Conclusion 
We have presented a system that extends 
previous work on lexical chains to content 
similarity detection.   This system employs 
lexical chains and sliding locality windows, and 
evaluates similarity using the cosine of n-grams 
of lexical chains and tf*idf weighted keywords.  
The results indicate that lexical chains are 
effective for detecting content similarity 
between pairs of chapters corresponding to the 
same original in a corpus of parallel translations.  
References 
1. Barzilay, R., Elhadad, M. 1999. Using lexical 
chains for text summarization. In: Inderjeet 
Mani and Mark T. Maybury, eds., Advances 
in AutomaticText Summarization, pp. 111?
121. Cambridge/MA, London/England: MIT 
Press. 
2. Halliday, M. and Hasan, R. 1976. Cohesion in 
English. Longman, London. 
3. Halliday, M. and Hasan, R. 1989. Language, 
context, and text. Oxford University Press, 
Oxford, UK. 
4. Lyon, C., Malcolm, J. and Dickerson, B. 
2001. Detecting Short Passages of Similar 
Text in Large Document Collections, In 
Proceedings of the 2001 Conference on 
Empirical Methods in Natural Language 
Processing, pp.118-125. 
5. Marcu, D. 1997. The Rhetorical Parsing, 
Summarization, and Generation of Natural 
Language Texts (Ph.D. dissertation). Univ. of 
Toronto. 
6. Miller, G., Beckwith, R., Felbaum, C., Gross, 
D., and Miller, K. 1990. Introduction to 
WordNet: An online lexical database. J. 
Lexicography, 3(4), pp. 235-244. 
7. Silber, G. and McCoy, K. 2002. Efficiently 
computed lexical chains as an intermediate 
representation for automatic text 
summarization. Computational Linguistics, 
28(4). 
8. Uzuner, O., Davis, R., Katz, B. 2004. Using 
Empirical Methods for Evaluating Expression 
and Content Similarity. In: Proceedings of the 
37th Hawaiian International Conference on 
System Sciences (HICSS-37).  IEEE 
Computer Society. 
 
154
Answering Definition Questions Using Multiple Knowledge Sources
Wesley Hildebrandt, Boris Katz, and Jimmy Lin
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street, Cambridge, MA 02139
{wes,boris,jimmylin}@csail.mit.edu
Abstract
Definition questions represent a largely unex-
plored area of question answering?they are
different from factoid questions in that the
goal is to return as many relevant ?nuggets?
of information about a concept as possible.
We describe a multi-strategy approach to an-
swering such questions using a database con-
structed offline with surface patterns, a Web-
based dictionary, and an off-the-shelf docu-
ment retriever. Results are presented from
component-level evaluation and from an end-
to-end evaluation of our implemented system
at the TREC 2003 Question Answering Track.
1 Introduction
To date, research in question answering has concentrated
on factoid questions such as ?Who was Abraham Lincoln
married to?? The standard strategy for answering these
questions using a textual corpus involves a combination
of information retrieval and named-entity extraction tech-
nology; see (Voorhees, 2002) for an overview. Factoid
questions, however, represent but one facet of question
answering, whose broader goal is to provide humans with
intuitive information access using natural language.
In contrast to factoid questions, the objective for ?defi-
nition? questions is to produce as many useful ?nuggets?
of information as possible. For example, the answer to
?Who is Aaron Copland?? might include the following:
American composer
wrote ballets and symphonies
born in Brooklyn, New York, in 1900
son of a Jewish immigrant
American communist
civil rights advocate
Until recently, definition questions remained a largely
unexplored area of question answering. Standard factoid
question answering technology, designed to extract sin-
gle answers, cannot be directly applied to this task. The
solution to this interesting research challenge will draw
from related fields such as information extraction, multi-
document summarization, and answer fusion.
In this paper, we present an approach to answering
definition questions that combines knowledge from three
sources. We present results from our own component
analysis and the TREC 2003 Question Answering Track.
2 Answering Definition Questions
Our first step in answering a definition question is to ex-
tract the concept for which information is being sought?
called the target term, or simply, the target. Once the tar-
get term has been found, three techniques are employed
to retrieve relevant nuggets: lookup in a database created
from the AQUAINT corpus1, lookup in a Web dictionary
followed by answer projection, and lookup directly in the
AQUAINT corpus with an IR engine. Answers from the
three different sources are then merged to produce the fi-
nal system output. The following subsections describe
each of these modules in greater detail.
2.1 Target Extraction
We have developed a simple pattern-based parser to ex-
tract the target term using regular expressions. If the nat-
ural language question does not fit any of our patterns,
the parser heuristically extracts the last sequence of capi-
talized words in the question as the target.
Our simple target extractor was tested on all definition
questions from the TREC-9 and TREC-10 QA Track test-
sets and performed with one hundred percent accuracy on
those questions. However, there were several instances
where the target term was not correctly extracted from
1official corpus used for the TREC QA Track, available from
the Linguistic Data Consortium
Name Pattern Bindings
Copular1 (e1 is) NP1 be NP2 [NP1 = t, NP2 = n]
Become2 (e1 beca) NP1 become NP2 [NP1 = t, NP2 = n]
Verb3 (e1 verb): NP1 v NP2 [where v ? biography-verb; NP1 = t, NP2 = n]
Appositive4 (e1/2 appo) NP1, NP2 [NP1 = t ? n, NP2 = t ? n]
Occupation5 (e2 occu) NP1 NP2 [where head(NP1) ? occupation; NP1 = n, NP2 = t]
Parenthesis6 (e1 pare) NP1 (NP2) [NP1 = t, NP2 = n]
Also-known-as7 (e1/2 aka) NP1, (also) known as NP2 [NP1 = t ? n, NP2 = t ? n]
Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NP2 = t]
Or9 (e1 or) NP1, or NP2 [NP1 = t, NP2 = n]
Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NP2 = t]
Relative clause11 (e1 wdt) NP (which|that) VP [NP = t, VP = n]
1In order to filter out spurious nuggets (e.g., progressive tense), our system discards nuggets that do not begin with a determiner.
2The verb become, like be, often yields good nuggets that define a target.
3By statistically analyzing a corpus of biographies of famous people, we compiled a list of verbs commonly used to describe people
and their accomplishments, such as write, invent, and make.
4Either NP1 or NP2 can be the target; thus, we index both NPs as the target term.
5NPs preceding proper nouns provide information such as occupation or affiliation. To boost precision, our system discards nuggets
that do not contain an occupation (e.g., actor, spokesman, leader). We mined this list from WordNet and the Web.
6Parenthetical expressions usually contain interesting nuggets; for persons, they often include a lifespan or job description.
7Either NP1 or NP2 can be the target; thus, we index both NPs as the target term.
8This and the previous pattern frequently identify hyponymy relations; typically, NP1 is the hypernym of NP2.
9This pattern often identifies the discourse function of elaboration.
10This pattern typically identifies an exemplification relationship, where NP2 is an instance of NP1.
11Relative clauses often provide useful nuggets.
Table 1: Description of the surface patterns used in constructing our database. (t is short for target, n for nugget)
the definition questions in TREC 2003, which made it
difficult for downstream modules to find relevant nuggets
(see Section 3.2 for a discussion).
2.2 Database Lookup
The use of surface patterns for answer extraction has
proven to be an effective strategy for factoid question
answering (Soubbotin and Soubbotin, 2001; Brill et al,
2001; Hermjakob et al, 2002). Typically, surface patterns
are applied to a candidate set of documents returned by
a document or passage retriever. Although this strategy
often suffers from low recall, it is generally not a prob-
lem for factoid questions, where only a single instance
of the answer is required. Definition questions, however,
require a system to find as many relevant nuggets as pos-
sible, making recall very important.
To boost recall, we employed an alternative strategy:
by applying the set of surface patterns offline, we were
able to ?precompile? from the AQUAINT corpus a list
of nuggets about every entity mentioned within it. In
essence, we have automatically constructed an immense
relational database containing nuggets distilled from ev-
ery article in the corpus. The task of answering defini-
tion questions then becomes a simple lookup for the rel-
evant term. This approach is similar in spirit to the work
reported by Fleischman et al (2003) and Mann (2002),
except that our system benefits from a greater variety of
patterns and answers a broader range of questions.
Our surface patterns operated both at the word and
part-of-speech level. Rudimentary chunking, such as
marking the boundaries of noun phrases, was performed
by grouping words based on their part-of-speech tags. In
total, we applied eleven surface patterns over the entire
corpus?these are detailed in Table 1, with examples in
Table 2.
Typically, surface patterns identify nuggets on the or-
der of a few words. In answering definition questions,
however, we decided to return responses that include ad-
ditional context?there is evidence that contextual in-
formation results in higher-quality answers (Lin et al,
2003). To accomplish this, all nuggets were expanded
around their center point to encompass one hundred char-
acters. We found that this technique enhances the read-
ability of the responses, because many nuggets seem odd
and out of place without context.
The results of applying our surface patterns to the en-
tire AQUAINT corpus?the target, pattern type, nugget,
and source sentence?are stored in a relational database.
To answer a definition question, the target is used to query
for all relevant nuggets in the database.
2.3 Dictionary Lookup
Another component of our system for answering
definition questions utilizes an existing Web-based
dictionary?dictionary definitions often supply knowl-
edge that can be directly exploited. Previous factoid ques-
Copular A fractal is a pattern that is irregular, but self-similar at all size scales
Become Althea Gibson became the first black tennis player to win a Wimbledon singles title
Verb Francis Scott Key wrote ?The Star-Spangled Banner?
Appositive The Aga Khan, Spiritual Leader of the Ismaili Muslims
Occupation steel magnate Andrew Carnegie
Parenthesis Alice Rivlin (director of the Office of Management and Budget)
Also-known-as special proteins, known as enzymes // amitriptyline, also known as Elavil
Also-called amino acid called phenylalanine
Or caldera, or cauldron-like cavity on the summit
Like prominent human rights leaders like Desmond Tutu
Relative clause Solar cells which currently produce less than one percent of global power supplies
Table 2: Example nuggets for each pattern. (target term in bold, textual landmark in italics, and nugget underlined)
tion answering systems have already demonstrated the
value of semistructured resources on the Web (Lin and
Katz, 2003); we believe that some of these resources can
be similarly employed to answer definition questions.
The setup of the TREC evaluations requires every an-
swer to be paired with a supporting document; therefore,
a system cannot simply return the dictionary definition
of a term as its response. To address this issue, we de-
veloped answer projection techniques to ?map? dictio-
nary definitions back onto AQUAINT documents. Simi-
lar techniques have been employed for factoid questions,
for example, in (Brill et al, 2001).
We have constructed a wrapper around the Merriam-
Webster online dictionary. To answer a question using
this technique, keywords from the target term?s dictio-
nary definition and the target itself are used as the query
to Lucene, a freely-available open-source IR engine. Our
system retrieves the top one hundred documents returned
by Lucene and tokenizes them into individual sentences,
discarding candidate sentences that do not contain the tar-
get term. The remaining sentences are scored by their
keyword overlap with the dictionary definition, weighted
by the inverse document frequency of each keyword. All
sentences with a non-zero score are retained and short-
ened to one hundred characters centered around the target
term, if necessary.
The following are two examples of results from our
dictionary lookup component:
What is the vagus nerve?
Dictionary definition: either of the 10th pair of
cranial nerves that arise from the medulla and
supply chiefly the viscera especially with auto-
nomic sensory and motor fibers
Projected answer: The vagus nerve is some-
times called the 10th cranial nerve. It runs from
the brain . . .
What is feng shui?
Dictionary definition: a Chinese geomantic
practice in which a structure or site is chosen
or configured so as to harmonize with the spir-
itual forces that inhabit it
Projected answer: In case you?ve missed the
feng shui bandwagon, it is, according to Web-
ster?s, ?a Chinese geomantic practice . . .
This strategy was inspired by query expansion
techniques often employed in document retrieval?
essentially, the dictionary definition of a term is used as
the source of expansion terms. Creative use of Web-
based resources combined with proven information re-
trieval techniques enables this component to provide high
quality responses to definition questions.
2.4 Document Lookup
If no answers are found by the previous two techniques,
as a last resort our system employs traditional document
retrieval to extract relevant nuggets. The target term is
used as a Lucene query to gather a set of one hundred can-
didate documents. These documents are tokenized into
individual sentences, and all sentences containing the tar-
get term are retained as responses (ranked by the Lucene-
generated score of the document from which they came).
These sentences are also shortened if necessary.
2.5 Answer Merging
The answer merging component of our system is re-
sponsible for integrating results from all three sources:
database lookup, dictionary lookup, and document
lookup. As previously mentioned, responses extracted
using document lookup are used only if the other two
methods returned no answers.
Redundancy presents a major challenge for integrating
knowledge from multiple sources. This problem is espe-
cially severe for nuggets stored in our database. Since
we precompiled knowledge about every entity instance
in the entire AQUAINT corpus, common nuggets are of-
ten repeated. In order to deal with this problem, we ap-
plied a simple heuristic to remove duplicate information:
if two responses share more than sixty percent of their
keywords, one of them is randomly discarded.
After duplicate removal, all responses are ordered by
the expected accuracy of the technique used to extract
the nugget. To determine this expected accuracy, we per-
formed a fine-grained evaluation for each surface pattern
as well as the dictionary lookup strategy; we discuss these
results further in Section 3.1.
Finally, the answer merging component decides how
many responses to return. Given n total responses, we
calculate the final number of responses to return as:
n if n ? 10
n +
?
n? 10 if n > 10
Having described the architecture of our system, we
proceed to present evaluation results.
3 Evaluation
In this section we present two separate evaluations of
our system. The first is a component analysis of our
database and dictionary techniques, and the second in-
volves our participation in the TREC 2003 Question An-
swering Track.
3.1 Component Evaluation
We evaluated the performance of each individual surface
pattern and the dictionary lookup technique on 160 def-
inition questions selected from the TREC-9 and TREC-
10 QA Track testsets. Since we primarily generated our
patterns by directly analyzing the corpus, these questions
can be considered a blind testset. The performance of our
surface patterns and our dictionary lookup technique is
shown in Table 3.
Overall, database lookup retrieved approximately eight
nuggets per question at an accuracy nearing 40%; dictio-
nary lookup retrieved about 1.5 nuggets per question at
an accuracy of 45%. Obviously, recall of our techniques
is extremely hard to measure directly; instead, we use the
prevalence of each pattern as a poor substitute. As shown
in Table 3, some patterns occur frequently (e.g., e1 is and
e1 appo), but others are relatively rare, such as the rela-
tive clause pattern, which yielded only six nuggets for the
entire testset.
These results represent a baseline for the performance
of each technique. Our focus was not on perfecting each
individual pattern and the dictionary matching algorithm,
but on building a complete working system. We will dis-
cuss future improvements and refinements in Section 5.
3.2 TREC 2003 Results
Our system for answering definition questions was in-
dependently and formally evaluated at the TREC 2003
Question Answering Track. For the first time, TREC
evaluated definition questions in addition to factoid and
list questions. Although our entry handled all three types
Pattern accuracy nuggets
e2 also 85.71 7
e2 aka 80.00 5
e2 occu 69.35 62
e1 or 67.74 31
e1 wdt 66.67 6
e2 like 64.60 113
e2 appo 60.00 20
e1 aka 50.00 2
e1 is 35.37 246
e1 pare 34.91 106
e1 appo 30.40 579
e1 verb 26.09 92
e1 beca 25.00 8
average 38.37 98.2
total 1277
dictionary 45.23 241
Table 3: Performance of each surface pattern and the dic-
tionary lookup technique for all 160 test questions.
Group Run F-measure
MITCSAIL03a 0.309
MIT MITCSAIL03b 0.282
MITCSAIL03c 0.282
best 0.555
Overall baseline IR 0.493
median 0.192
Table 4: Official TREC 2003 results.
of questions, we only report the results of the definition
questions here; see (Katz et al, 2003) for description of
the other components.
Overall, our system performed well, ranking eighth out
of twenty-five groups that participated (Voorhees, 2003).
Our official results for the definition sub-task are shown
in Table 4, along with overall statistics for all groups. The
formula used to calculate the F-measure is given in Fig-
ure 1. The ? value of five indicates that recall is consid-
ered five times more important than precision, an arbi-
trary value set for the purposes of the evaluation.
Nugget precision is computed based on a length al-
lowance of one hundred non-whitespace characters per
relevant response, because a pilot study demonstrated that
it was impossible for assessors to consistently enumer-
ate the total set of ?concepts? contained in a system re-
sponse (Voorhees, 2003). The assessors? nugget list (i.e.,
the ground truth) was created by considering the union
of all responses returned by all participants. All rele-
vant nuggets are divided into ?vital? and ?non-vital? cat-
egories, where vital nuggets are items of information that
Let r # of vital nuggets returned in a response
a # of non-vital nuggets returned in a response
R total # of vital nuggets in the assessors? list
l # of non-whitespace characters in the entire
answer string
Then
recall (R) = r/R
allowance (?) = 100? (r + a)
precision (P) =
{
1 if l < ?
1? l??l otherwise
Finally, the F (? = 5) = (?
2 + 1)? P ?R
?2 ? P +R
Figure 1: Official definition of F-measure.
must be in a definition for it to be considered ?good?.
Non-vital nuggets may also provide relevant information,
but a ?good? definition does not need to include them.
Nugget recall is thus only a function of vital nuggets.
The best run, with an F-measure of 0.555, was submit-
ted by BBN (Xu et al, 2003). The system used many of
the same techniques we described here, with one impor-
tant exception?they did not precompile nuggets into a
database. In their own error analysis, they cited recall as
a major cause of bad performance; this is an issue specif-
ically addressed by our approach.
Interestingly, Xu et al also reported an IR baseline
which essentially retrieved the top 1000 sentences in the
corpus that mentioned the target term (subjected to sim-
ple heuristics to remove redundant answers). This base-
line technique achieved an F-measure of 0.493, which
beat all other runs (expect for BBN?s own runs). Because
the F-measure heavily favored recall over precision, sim-
ple IR techniques worked extremely well. This issue is
discussed in Section 4.1.
To identify areas for improvement, we analyzed the
questions on which we did poorly and found that many
of the errors can be traced back to problems with target
extraction. If the target term is not correctly identified,
then all subsequent modules have little chance of provid-
ing relevant nuggets. For eight questions, our system did
not identify the correct target. The presence of stopwords
and special characters in names was not anticipated:
What is Bausch & Lomb?
Who is Vlad the Impaler?
Who is Akbar the Great?
Our naive pattern-based parser extracted Lomb, Im-
paler, and Great as the target terms for the above ques-
tions. Fortunately, because Lomb and Impaler were
rare terms, our system did manage to return relevant
nuggets. However, since Great is a very common word,
our nuggets for Akbar the Great were meaningless.
The system?s inability to parse certain names is related
to our simple assumption that the final consecutive se-
quence of capitalized words in a question is likely to be
the target. This simply turned out to be an incorrect as-
sumption, as seen in the following questions:
Who was Abraham in the Old Testament?
What is ETA in Spain?
What is Friends of the Earth?
Our parser extracted Old Testament, Spain, and Earth
as the targets for these questions, which directly resulted
in the system?s failure to return relevant nuggets.
Our target extractor also had difficulty with apposi-
tion. Given the question ?What is the medical condition
shingles??, the extractor incorrectly identified the entire
phrase medical condition shingles as the target term. Fi-
nally, our policy of ignoring articles before the target term
caused problems with the question ?What is the Hague??
Since we extracted Hague as the target term, we returned
answers about a British politician as well as the city in
Holland. Our experiences show that while target extrac-
tion seems relatively straightforward, there are instances
where a deeper linguistic understanding is necessary.
Overall, our database and dictionary lookup techniques
worked well. For six questions (out of fifty), however,
neither technique found any nuggets, and therefore our
system resorted to document lookup.
4 Evaluation Reconsidered
This section takes a closer look at the setup of the defini-
tion question evaluation at TREC 2003. In particular, we
examine three issues: the scoring metric, error inherent in
the evaluation process, and variations in judgments.
4.1 The Scoring Metric
As defined, nugget recall is only a function of the nuggets
considered ?vital?. This, however, leads to a counter-
intuitive situation where a system that returned every
non-vital nugget but no vital nuggets would receive a
score of zero. This certainly does not reflect the informa-
tion needs of a real user?even in the absence of ?vital?
information, related knowledge might still be useful to a
user. One solution might be to assign a relative weight to
distinguish vital and non-vital nuggets.
The distinction between vital and non-vital nuggets
is itself somewhat arbitrary. Consider some relevant
nuggets for the question ?What is Bausch & Lomb??:
world?s largest eye care company
about 12000 employees
in 50 countries
Run Total Relevant Recall
Nuggets Returned
official 407 118 28.99%
fixed 407 120 29.48%
Table 5: Nugget recall, disregarding the distinction be-
tween vital and non-vital nuggets.
Figure 2: F-measure as a function of ?.
approx. $1.8 billion annual revenue
based in Rochester, New York
According to the official assessment, the first four
nuggets are vital and the fifth is not. This means that
the location of Bausch & Lomb?s headquarters is consid-
ered less important than employee count and revenue. We
disagree and also believe that ?based in Rochester, New
York? is more important than ?in 50 countries?. Since
it appears that the difference between vital and non-vital
cannot be easily operationalized, there is little hope for
systems to learn and exploit this distinction.
As a reference, we decided to reevaluate our sys-
tem, ignoring the distinction between vital and non-vital
nuggets. The overall nugget recall is reported in Table 5.
We also report the nugget recall of our system after fixing
our target extractor to handle the variety of target terms
in the testset (the ?fixed? run). Unfortunately, our perfor-
mance for the fixed run did not significantly increase be-
cause the problem associated with unanticipated targets
extended beyond the target extractor. Since our surface
patterns did not handle these special entities, the database
did not contain relevant entries for those targets.
Another important issue in the evaluation concerns the
value of ?, the relative importance between precision
and recall in calculating the F-measure. The top entry
achieved an F-measure of 0.555, but the response length
averaged 2059 non-whitespace characters per question.
In contrast, our run with an F-measure of 0.309 averaged
only 620 non-whitespace characters per answer (only two
other runs in the top ten had average response lengths
lower than ours; the lowest was 338). Figure 2 shows F-
measure of our system, the top run, and the IR baseline
plotted against the value of ?. As can be seen, if precision
and recall are considered equally important (i.e., ? = 1),
the difference in performance between our system and
that of the top system is virtually indistinguishable (and
our system performs significantly better than the IR base-
line). At the level of ? = 5, it is obvious that standard IR
technology works very well. The advantages of surface
patterns, linguistic processing, answer fusion, and other
techniques become more obvious if the F-measure is not
as heavily biased towards recall.
What is the proper value of ?? As this was the first for-
mal evaluation of definition questions, the value was set
arbitrarily. However, we believe that there is no ?correct?
value of ?. Instead, the relative importance of precision
and recall varies dramatically from application to applica-
tion, depending on the user information need. A college
student writing a term paper, for example, would most
likely value recall highly, whereas the opposite would be
true for a user asking questions on a PDA. We believe that
these tradeoffs are worthy of further research.
4.2 Evaluation Error
In the TREC 2003 evaluation, we submitted three iden-
tical runs, but nevertheless received different scores for
each of the runs. This situation can be viewed as a probe
into the error margin of the evaluation?assessors are hu-
man and naturally make mistakes, and to ensure the qual-
ity of the evaluation we need to quantify this variation.
Voorhees? analysis (2003) revealed that scores for pairs of
identical runs differed by as much as 0.043 in F-measure.
For the three identical runs we submitted, there was
one nugget missed in our first run that was found in the
other two runs, ten nuggets from six questions missed
for our second run that were found in the other runs, and
ten nuggets from five questions missed in our third run.
There were also nine nuggets from seven questions that
were missed for all three runs, even though they were
clearly present in our answers.
Together over our three runs, there were 48 nuggets
from 13 questions that were clearly present in our re-
sponses but were not consistently recognized by the as-
sessors. The question affected most by these discrepan-
cies was ?Who is Alger Hiss??, for which we received an
F-measure of 0.671 in our first run, while for the second
and third runs we received a score of zero.
If the 48 missed nuggets had been recognized by the
assessors, our F-measure would be 0.327, 0.045 higher
than the score we actually received for runs b and c. This
single-point investigation is not meant to contest the rel-
ative rankings of submitted runs, but simply to demon-
strate the magnitude of the human error currently present
in the evaluation of definition questions (presumably, all
groups suffered equally from these variations).
4.3 Variations in Judgment
The answers to definition questions were judged by hu-
mans, and humans naturally have differing opinions as to
the quality of a response. These differences of opinion are
not mistakes (unlike the issues discussed in the previous
section), but legitimate variations in what assessors con-
sider to be acceptable. These variations are compounded
by the small size of the testset?only fifty questions. In
a post-evaluation analysis, Voorhees (2003) determined
that a score difference of at least 0.1 in F-measure is re-
quired in order for two evaluation results to be consid-
ered statistically different (at 95% confidence). A range
of ?0.1 around our F-measure of 0.309 could either push
our results up to fifth place or down to eleventh place.
A major source of variation is whether or not a pas-
sage matches a particular nugget in the assessor?s list (the
ground truth). Obviously, the assessors are not merely
doing a string comparison, but are instead performing a
?semantic match? of the relevant concepts involved. The
following passages were rejected as matches to the asses-
sors? nuggets:
Who is Al Sharpton?
Nugget: Harlem civil rights leader
Our answer: New York civil rights activist
Who is Ari Fleischer?
Nugget: Elizabeth Dole?s Press Secretary
Our answer: Ari Fleischer, spokesman for . . .
Elizabeth Dole
What is the medical condition shingles?
Nugget: tropical [sic] capsaicin relieves pain of
shingles
Our answer: Epilepsy drug relieves pain from
. . . shingles
Consider the nugget for Al Sharpton: although an ?ac-
tivist? may not be a ?leader?, and someone from New
York may not necessarily be from Harlem, one might ar-
gue that the two nuggets are ?close enough? to warrant a
semantic match. The same situation is true of the other
two questions. The important point here is that different
assessors may judge these nuggets differently, contribut-
ing to detectable variations in score.
Another important issue is the composition of the as-
sessors? nugget list, which serves as ?ground truth?. To
insure proper assessment, each nugget should ideally rep-
resent an ?atomic? concept?which in many cases, it
does not. Again consider the nugget for Al Sharpton; ?a
Harlem civil rights leader? includes the concepts that he
was an important civil rights figure and that he did much
of his work in Harlem. It is entirely conceivable that a
response would provide one fact but not the other. How
then should this situation be scored? As another example,
one of the nuggets for Alexander Pope is ?English poet?,
which is clearly two separate facts.
Another desirable characteristic of the assessor?s
nugget list is uniqueness?nuggets should be unique, not
only in their text but also in their meaning. In the TREC
2003 testset, three questions had exact duplicate nuggets.
Furthermore, there were also several questions for which
multiple nuggets are nearly synonymous (or are implied
by other nuggets), such as the following:
What is TB?
highly infectious lung disease
contagious respiratory disease
common communicable disease
Who is Allen Iverson?
professional basketball player
philadelphia 76 er
What is El Shaddai?
catholic charismatic group
christian organization
catholic sect
religious group
Because the nuggets overlap greatly with each other
in the concepts they denote, consistent and reproducible
evaluation results are difficult.
Another desirable property of the ground truth is com-
pleteness, or coverage of the nuggets?which we also
found to be lacking. There were many relevant items of
information returned by our runs that did not make it onto
the assessors? nugget list (even as non-vital nuggets). For
the question ?Who is Alberto Tomba??, the fact that he
is Italian was not judged to be relevant. For ?What are
fractals??, the ground truth does not contain the idea that
they can be described by simple formulas, which is one
of their most important characteristics. Some more ex-
amples are shown below:
Aga Khan is the founder and principal share-
holder of the Nation Media Group.
The vagus nerve is the sometimes known as the
10th cranial nerve.
Alexander Hamilton was an author, a general,
and a founding father.
Andrew Carnegie established a library system
in Canada.
Angela Davis taught at UC Berkeley.
This coverage issue also points to a deeper method-
ological problem with evaluating definition questions by
pooling the results of all participants. Vital nuggets may
be excluded simply because no system returned them.
Unfortunately, there is no easy way to quantify this phe-
nomenon.
Clearly, evaluating answers to definition questions is
a challenging task. Nevertheless, consistent, repeatable,
and meaningful scoring guidelines are critical to driving
the development of the field. We believe that lessons
learned from our analysis can lead to a more refined eval-
uation in the coming years.
5 Future Work
The results of our work highlight several areas for future
improvement. As mentioned earlier, target extraction is a
key, non-trivial capability critical to the success of a sys-
tem. Similarly, database lookup works only if the relevant
target terms are identified and indexed while preprocess-
ing the corpus. Both of these issues point to the need for a
more robust named-entity extractor, capable of handling
specialized names (e.g., ?Bausch & Lomb?, ?Destiny?s
Child?, ?Akbar the Great?). At the same time, the named-
entity extractor must not be confused by sentences such
as ?Raytheon & Boeing are defense contractors? or ?She
gave John the Honda for Christmas?.
Another area for improvement is the accuracy of the
surface patterns. In general, our patterns only used lo-
cal information; we expect that expanding the context on
which these patterns operate will reduce the number of
false matches. As an example, consider our e1 is pattern;
in one test, over 60% of irrelevant nuggets were cases
where the target is the object of a preposition and not
the subject of the copular verb immediately following it.
For example, this pattern matched the question ?What is
mold?? to the sentence ?tools you need to look for mold
are . . .?. If we endow our patterns with better linguis-
tic notions of constituency, we can dramatically improve
their precision. Another direction we are pursuing is the
use of machine learning techniques to learn predictors
of good nuggets, much like the work of Fleischman et
al. (2003). Separating ?good? from ?bad? nuggets fits
very naturally into a binary classification task.
6 Conclusion
In this paper, we have described a novel set of strategies
for answering definition questions from multiple sources:
a database of nuggets precompiled offline using surface
patterns, a Web-based electronic dictionary, and doc-
uments retrieved using traditional information retrieval
technology. We have also demonstrated how answers
derived using multiple strategies can be smoothly inte-
grated to produce a final set of answers. In addition, our
analyses have shown the difficulty of evaluating defini-
tion questions and inability of present metrics to accu-
rately capture the information needs of real-world users.
We believe that our research makes significant contribu-
tions toward the understanding of definition questions, a
largely unexplored area of question answering.
7 Acknowledgement
This work was supported in part by the ARDA?s Ad-
vanced Question Answering for Intelligence (AQUAINT)
Program.
References
Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,
and Andrew Ng. 2001. Data-intensive question an-
swering. In Proceedings of the Tenth Text REtrieval
Conference (TREC 2001).
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online ques-
tion answering: Answering questions before they are
asked. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL
2003).
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and Web exploitation for question answering.
In Proceedings of the Eleventh Text REtrieval Confer-
ence (TREC 2002).
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003. In-
tegrating Web-based and corpus-based techniques for
question answering. In Proceedings of the Twelfth Text
REtrieval Conference (TREC 2003).
Jimmy Lin and Boris Katz. 2003. Question answering
from the Web using knowledge annotation and knowl-
edge mining techniques. In Proceedings of the Twelfth
International Conference on Information and Knowl-
edge Management (CIKM 2003).
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,
David Huynh, Boris Katz, and David R. Karger. 2003.
What makes a good answer? The role of context in
question answering. In Proceedings of the Ninth IFIP
TC13 International Conference on Human-Computer
Interaction (INTERACT 2003).
Gideon Mann. 2002. Fine-grained proper noun ontolo-
gies for question answering. In Proceedings of the Se-
maNet?02 Workshop at COLING 2002 on Building and
Using Semantic Networks.
Martin M. Soubbotin and Sergei M. Soubbotin. 2001.
Patterns of potential answer expressions as clues to the
right answers. In Proceedings of the Tenth Text RE-
trieval Conference (TREC 2001).
Ellen M. Voorhees. 2002. Overview of the TREC
2002 question answering track. In Proceedings of the
Eleventh Text REtrieval Conference (TREC 2002).
Ellen M. Voorhees. 2003. Overview of the TREC
2003 question answering track. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
Jinxi Xu, Ana Licuanan, and Ralph Weischedel. 2003.
TREC2003 QA at BBN: Answering definitional ques-
tions. In Proceedings of the Twelfth Text REtrieval
Conference (TREC 2003).
REXTOR: A System for Generating Relations 
from Natural Language 
Boris Katz and J immy Lin 
MIT Artificial Intelligence Laboratory 
200 Technology Square 
Cambridge, MA 02139 USA 
{boris, j immylin}@ai.mit, edu 
Abst ract  
This paper argues that a finite-state 
language model with a ternary expres- 
sion representation is currently the most 
practical and suitable bridge between 
natural language processing and infor- 
mation retrieval. Despite the theoreti- 
cal computational inadequacies of finite- 
state grammars, they are very cost ef- 
fective (in time and space requirements) 
and adequate for practical purposes. 
The ternary expressions that we use 
are not only linguistically-motivated, but
also amenable to rapid large-scale index- 
ing. REXTOR (Relations EXtracTOR) is 
an implementation f this model; in one 
uniform framework, the system provides 
two separate grammars for extracting 
arbitrary patterns of text and building 
ternary expressions from them. These 
content representational structures serve 
as the input to our ternary expressions 
indexer. This approach to natural lan- 
guage information retrieval promises to 
significantly raise the performance of 
current systems. 
1 In t roduct ion  
Traditional information retrieval (IR) has been 
built on the "bag-of-words" assumption, which 
equates the weighted component keywords of a 
document with its semantic ontent. Obviously, 
a document is much more than the sum of its in- 
dividual keywords. Although keywords may offer 
some indication of "meaning," they alone cannot 
capture the richness and expressiveness of natu- 
ral language. Consider the following sets of sen- 
tences/phrases that have similar word content, 
but (dramatically) different meanings: 1 
IExamples taken from (Loper, 2000) 
(1) The big man ate the dog. 
(1') The big dog ate the man. 
(2) The meaning of life 
(2') A me~ningfll\] life 
(3) The bank of the river 
(3') The bank near the river 
Due to the inability of keywords to capture the 
"meaning" of documents, a traditional informa- 
tion retrieval system (i.e., one using the bag-of- 
words paradigm) will suffer from poor precision in 
response to a user query accurately and precisely 
formulated in natural anguage. 
The application of natural language process- 
ing (NLP) techniques to information retrieval 
promises to generate representational structures 
that better capture the semantic ontent of docu- 
ments. In particular, syntactic analysis can high- 
light the relationships between various terms and 
phrases in a sentence, which will allow us to distin- 
guish between the example pairs given above and 
answer queries with higher precision than tradi- 
tional IR systems. 
However, a syntactically-informed representa- 
tional structure faces the problem of Hnguistic 
variations, the phenomenon in which similar se- 
mantic content may be expressed in different sur- 
face forms. Consider the following sets of sen- 
tences that express the same meaning using dif- 
ferent constructions: 
(4) What is Bill Gates' net worth? 
(4')What is the net worth of Bill Gates? 
(5) John gave the book to Mary. 
(5') John gave Mary the book. 
(5") Mary was given the book by John. 
(6) The president surprised the country with 
his actions. 
(6') The president's actions surprised the 
country. 
(7) Over 22 million people live in Taiwan. 
(7') The population of Taiwan is 22 million. 
An effective linguistically-motivated informa- 
tion retrieval system must not only handle rel- 
67 
atively simple syntactic variations (e.g., (4) and 
(5)), alternate realization of verb arguments (e.g., 
(6) and (6')), but also more complicated semantic 
variations (e.g., (7) and (7')). This can be ac- 
complished by linguistic normalization, a process 
by which linguistic variants that contain the same 
semantic ontent are mapped onto the same rep- 
resentational structure. 
The precision of information retrieval systems 
can be dramatically improved if they index not 
only single terms, but normelized representational 
structures derived from language. However, the 
optimal structure of this representation a d the 
efficient generation ofthese structures remains an 
open research problem. 
This paper argues that, for the purposes of 
information retrieval systems, the most suitable 
representational structure of document content is 
ternary expressions (compared to, for example, 
keywords, trees or case frames). Ternary (three- 
place) expressions may be thought of as typed 
binary relations (e.g., subject-relation-object) or 
two-place predicates (e.g., transitive verbs like 
'hit'); they are linguistically-motivated an  effi- 
cient to index. Also, for information retrieval, a
finite-state grammar isthe most practical and cost 
effective method by which to extract these ternary 
expressions from documents. Combined together, 
a finite-state language model and ternary expres- 
sion representation provide a convenient and pow- 
erful framework for integrating natural anguage 
processing with information retrieval. 
REXTOR (Relations EXtracTOR) is a docu- 
ment content analysis ystem designed to unify 
and generalize many previous natural anguage 
information retrieval techniques into one single 
framework. The system provides two separate 
grammars: one for extracting arbitrary entities 
from documents, and the other for building re- 
lations from the extracted items. REXTOI~ also 
provides a playground and testbed for future ex- 
perimentation in linguistically-motivated indexing 
schemes. 
2 Mot ivat ion  
We believe that, for humans, natural language is
the best mechanism for information access. It is 
intuitive, easy to use, rapidly deployable, and re- 
quires no specialized training, 
The REXTOR System builds on the experi- 
ence of START (SynTactic Analysis using Re- 
versible Transformations), a natural language sys- 
tem available for question answering on the World 
Wide Web. 2 Since December, 1993, when it first 
2hZZp :/\[~ww. ai .mit. edu/projects/infolab 
came online, START has engaged in millions of ex- 
changes with hundreds of thousands of people all 
over the world, supplying users with knowledge 
regarding eography, weather, movies, and many 
many other areas. Despite the successes of START 
in serving actual users, its domain of knowledge is
relatively small and expanding its knowledge base 
is a time-consuming task. The goal of REXTOR is 
to overcome this bottleneck and to provide a gen- 
eral framework for natural-language information 
retrieval. REXTOR not only draws its inspiration 
from START (in providing question answering ca- 
pabilities), but also borrows a simplified form of 
its representational structures (Katz, 1980; Katz, 
1990). 
The START System (Katz, 1990; Katz, 1997) 
analyzes English text and builds a knowledge base 
from information found in the text. The knowl- 
edge is expressed in the form of embedded ternary 
expressions (T-expressions) - -  subject-relation- 
object triples where the subject and object can 
themselves be ternary expressions. For exam- 
ple, "The population of Zimbabwe is 11,044,147" 
would be represented astwo ternary expressions: 
\[POPULATION-1 IS 11044147\] 
\[POPULATION-1 RELATED-T0 ZIMBABWE\] 
Experience from START has shown that a robust 
full-text natural language question-answering sys- 
tem cannot be realistically expected any time 
soon. Numerous problems uch as intersenten- 
tial reference, paraphrasing, summarization, com- 
mon sense implication, and many more, will take 
a long time to solve satisfactorily. In order to by- 
pass intractable complexities of language, START 
uses computer-analyzable natural language anno- 
tations, which consist of simplified English sen- 
tences and phrases, to describe various informa- 
tion segments (which may be text, images, or even 
video and other multimedia content). These nat- 
ural language annotations serve as metadata and 
inform START regarding the type of questions that 
a particular information segment is capable of an- 
swering (Katz, 1997). By performing retrieval on 
natural anguage annotations, the system is able 
to provide knowledge that it may not be able to 
analyze itself (either language that is too com- 
plex or non-textual segments). Because these an- 
notations must be manually generated, expand- 
ing START'S knowledge base is relatively time- 
intensive. 
REXTOR attempts to eliminate the need for hu- 
man involvement during content analysis, and also 
aims to serve as the foundation of a natural lan- 
guage information retrieval system. Ultimately, 
68 
we hope that REXTOR will serve as a stepping 
stone towards a comprehensive system capable of 
providing users with "just the right information" 
to queries posed in natural language. 
3 P rev ious  Work  
The concept of indexing more than simple key- 
words is not new; the idea of indexing (parts 
of) phrases, for example, is more than a decade 
old (Fagan, 1987). Arampatzis (1998) introduced 
the phrase retrieval hypothesis, which asserted 
that phrases are a better indication of document 
content han keywords. Several researchers have 
also explored ifferent techniques oflinguistic nor- 
realization for information retrieval (Strzalkowski 
et al, 1996; Zhai et al, 1996; Arampatzis et 
al., 2000). The performance improvements were 
neither negligible nor dramatic, but despite the 
lack of any significant breakthroughs, the au- 
thors affirmed the potential value of linguistically- 
motivated indexing schemes and the advantages 
they offer over traditional IR. 
Previous research in linguistically motivated 
information retrieval concentrated primarily on 
noun phrases and their attached prepositional 
phrases. Techniques that involve head/modifier 
relations have been tried, e.g., indexing adjec- 
tive/noun and noun/right adjunct pairs (which 
normalizes variants such as "information re- 
trievai" and "retrieval of information"). How- 
ever, there has been little experimentation with 
other types of linguistic relations, e.g., apposi- 
tives, predicate nominatives (i.e., the i s -a  rela- 
tion), predicate adjectives (i.e., the has-property 
relation), etc. Furthermore, indexing of word 
pairs and phrases in many previous ystems was 
accomplished by converting those representations 
into lexical items and atomic terms, indexed in 
the same manner as single words. The treat- 
ment of these representational structures using a 
restrictive bag-of-words paradigm limits the type 
of queries that may be formulated. For example, 
treating adjective/noun pairs (\[adj., noun\]) as lex- 
ical atoms renders it impossible to find the equiv- 
alent of "all big things," corresponding to the pair 
\[big, *\]. 
The extraction of these relations from docu- 
ments has been relatively inefficient and unsys- 
tematic. One approach is to first parse the 
document using a full-text parser, and then ex- 
tract interesting relations from the resulting parse 
tree (Fagan, 1987; Grishman and Sterling, 1993; 
Loper, 2000). This approach isslow and inefficient 
because full-text parsing is very time-intensive. 
Due to current limitations of computational tech- 
nology, only a small fraction of the information 
gathered by a full parser can be efficiently indexed. 
For the most part, relations that can be effec- 
tively utilized for information retrieval purposes 
only occupy a few nodes of a (possibly dense) 
parse tree; thus, most of the knowledge gathered 
by the parser is thrown away. Also, extracting 
non-linguistic relations from parse trees is very 
difficult; many interesting relations (from an IR 
point of view) have no linguistic foundation, e.g., 
adjacent word pairs. The other approach to ex- 
tracting relations from text is to build simple fil- 
ters for every new relation. This approach is un- 
systematic, and does not allow for rapid addition 
of new relations to a system. 
The REXTOR System utilizes an integrated 
model to systematically extract arbitrary textual 
patterns and relations (ternary expressions) from 
documents. The concept of coupling structure- 
building actions with parsing originated with aug- 
mented transition etworks (ATNs)(Thorne t al., 
1968; Woods, 1970). Similarly, PLNLP (Heidorn, 
1972; Jensen et al, 1993) is a programming lan- 
guage for writing phrase structure rules that in- 
clude specific conditions under which the rule can 
be applied. These rules may also be augmented 
by structure-building actions that are to be taken 
when the rule is applied. However, these sys- 
tems that attempt full-text parsing are less effi- 
cient for information retrieval applications due to 
the long time necessary to generate full linguistic 
parse trees. REXTOR was designed with a simple 
language model and an equally simple, yet expres- 
sive, representation f "meaning." 
4 Br idg ing  Natura l  Language and  
In fo rmat ion  Ret r ieva l  
In order to bridge the gap between atural an- 
guage and information retrieval, natural language 
text must be distilled into a representational 
structure that is amenable to fast, large-scale in- 
dexing. We argue that a finite-state model of nat- 
ural language with ternary expressions is currently 
the most suitable combination for this task. 
4.1 Finite-State Language Model  
Despite its limitations, a finite-state grammar 
seems to provide the best natural language model 
for information retrieval purposes. One of the 
most notable computational inadequacies of the 
finite-state model is the absence of a pushdown 
mechanism to suspend the processing of a con- 
stituent at a given level while using the same 
grammar to process an embedded constituent 
(Woods, 1970). Due to this inadequacy, certain 
69 
English constructions, such as center embedding, 
cannot be described by any finite-state gram- 
mar (Chomsky, 1959a; Chomsky, 1959b). How- 
ever, Church (1980) demonstrated that the finite- 
state language model is adequate to describe a 
performance model of language (i.e., constrained 
by memory, attention, and other realistic limi- 
tations) that approximates competence (i.e., lan- 
guage ability under optimal conditions without re- 
source constraints). Many phenomena that can- 
not be handled by fiuite-state grammars are awk- 
ward from a psycholinguistic point of view, and 
hence rarely seen. More recently, Pereira and 
Wright (1991) developed formal methods of ap- 
proximating context-free grammars with finite- 
state grammars, s Thus, for practical purposes, 
computationally simple finite-state grammars can 
be utilized to adequately model natural language. 
Empirically, the effectiveness of the finite- 
state language model has been demonstrated in 
the Message Understanding Conferences (MUCs), 
which evaluated information extraction (IE) sys- 
terns on a variety of domain-specific tasks. The 
conferences have shown that superficial parsing 
using finite-state grammars performs better than 
deep parsing using context-free grammars (at least 
under the current constraints oftechnology). The 
NYU team switched over from a system that per- 
formed full parsing (PROTEUS) in MUC-5 (Gr- 
ishman and Sterling, 1993) to a regular expres- 
sion matching parser in MUC-6 (Grishman, 1995). 
Full parsing was slow and error-prone, and the 
process of building a full syntactic analysis in- 
volved relatively unconstrained search which con- 
sumed large amounts of both time and space. The 
longer debug-cycles that resulted from this trans- 
lated into fewer iterations with which to tune the 
system within a given amount of time. Further- 
more, the complexity of a full context-free gram- 
mar contributed to maintenance problems; com- 
plex interactions within the grammar prevented 
rapid updating of the system to handle new con- 
structions. 
Finite-state grammars have been used to ex- 
tract entities such as proper nouns, names, lo- 
cations, etc., with relatively high precision. To 
a lesser extent, these grammars have proven to 
be effective in identifying syntactic onstructions 
such as noun phrases and verb phrases. FASTUS 
(Hobbs et al, 1996), the most notable of these sys- 
tems, is modeled after cascaded, nondeterrninistic 
finite-state automata. The finite-state transduc- 
ers are "cascaded" in that they are arranged in 
SHowever, these approximations overgenerate, al-
though in predictable, systematic ways. 
series; each one maps the output structures from 
the previous transducer into structures that com- 
prise the input to the next transducer. 
There are many similarities between informa- 
tion extraction and building effective representa- 
tional structures for information retrieval. Both 
tasks involve identifying entities (e.g., phrases) 
and the relationships between those entities. 
Thus, the application of proven information ex- 
traction techniques (i.e., finite-state technology) 
to information retrieval offers promise in raising 
the performance of IR systems. 
4.2 Ternary Expressions 
Ternary (three-place) expressions currently ap- 
pear to be the most suitable representational 
structure for meaning extracted from text. They 
may be intuitively viewed as subject-relation- 
object riples, and can easily express many types 
of relations, e.g., subject-verb-object relations, 
possession relations, etc. From a syntactic point of 
view, ternary expressions may be viewed as typed 
binary relations. Given the binary branching hy- 
pothesis of linguistic theory, ternary expressions 
are theoretically capable of expressing any arbi- 
trary tree - -  thus, ternary expressions are com- 
patible with linguistic theory. From a semantic 
point of view, ternary expressions may be viewed 
as two-place predicates, and can be manipulated 
using predicate logic. Finally, ternary expressions 
are highly amenable to rapid large-scale indexing, 
which is a necessary prerequisite of information re- 
trieval systems. Although other representational 
structures (e.g., trees or case frames) may be bet- 
ter adapted for some purposes, they are much 
more difficult to index and retrieve efficiently due 
to their size and complexity. 
In fact, indexing linguistic tree structures has 
been attempted (Smeaton et al, 1994), with very 
disappointing results: precision actually decreased 
due to the inability to handle variations in tree 
structure (i.e., the same semantic content could 
be expressed using different syntactic structures), 
and to the poor quality of the full-text natural lan- 
guage parser, which was also rather slow. Despite 
recent advances, full-text natural language parsers 
are still relatively error-prone; indexing incorrect 
parse trees is a source of performance degrada- 
tion. Furthermore, matching trees and sub-trees is 
a computationally intensive task, especially since 
full linguistic parse trees may be relatively deep. 
Relations are easier to match because they are 
typically much simpler than parse trees. For ex- 
ample, the tree 
\[\[shiny happy people \] \[of \[Wonderland\]\]\] 
70 
may be "flattened" into three relations: 
< shiny describes people > 
< happy describes people > 
< people related-to Wonderland > 
Indexing chse frames has also been attempted 
(Croft and Lewis, 1987; Loper, 2000), but with 
limited success. Full semantic analysis is still 
an open research problem, especially in the gen- 
eral domain. Since full semantic analysis can- 
not be performed without full-text parsing, case 
frame analysis inherits the unreliability of current 
parsers. Furthermore, semantic analysis requires 
extensive knowledge in the lexicon, which is ex- 
tremely time-intensive to construct. Finally, due 
to the complex structure of case frames, they are 
more difficult o store and index than ternary ex- 
pressions. 
Since ternary expressions are merely three-place 
relations, they may be indexed and retrieved much 
in the same way as rows within the table of a rela- 
tional database; 4 hence, well-known optimizations 
for databases may be applied for extremely high 
performance. 
Previous linguistically-motivated indexing 
schemes may easily be reformulated using ternary 
expressions. For example, indexing adjacent 
word pairs consists of indexing adjacent words 
with the adjacent relation. In fact, all pairs 
(e.g., adjective-noun, head-modifier) can be 
reformulated as ternary expressions by assigning 
a type to the pair. This finer granulaxity allows 
the capture of more intricate relations between 
words in a document. 
5 The  REXTOR System 
Using its finite-state language model, the REXTOR 
System generates a set of ternary expressions 
that correspond to content of a part-of-speech- 
tagged input document. Currently, the Brill Tag- 
ger (Brill, 1992) (with minor postprocessing) is 
used for the part-of-speech (POS) tagging. The 
relations construction process consists of two dis- 
tinct processes, each guided by its own externally 
specified grammar file. Extraction rules are ap- 
plied to match arbitrary patterns of text, based 
either on one of thirty-nine POS tags or on exact 
words. Whenever an item is extracted, a corre- 
sponding relation rule is triggered, which handles 
the actual generation of the ternary expressions 
(relations). 
4In fact, our first implementation f a ternary ex- 
pressions indexer used a SQL database. 
5.1 Ext rac t ion  Ru les  
Extraction rules are used to extract arbitrary pat- 
terns of text according to a grammar specification. 
The REXTOR grammar is written as regular ex- 
pression rules, which are computationally equiv- 
alent to finite-state automata, s Writing gram- 
mar rules in this fashion allows for perspicuity, 
the property whereby permitted types of construc- 
tions are readily apparent from the rules. Such 
a human-readable formulation simplifies mainte- 
nance of the grammar. 
The extraction stage of the REXTOR System 
performs a no-lookahead left-to-right scan of ev- 
ery input sentence, identifies the longest match- 
ing pattern (from any grammar rule), reduces the 
input sequence based on the matched rule, and 
continues with the next unmatched word. If a 
word cannot be included in any grammar rule, it 
is skipped. 
An extraction rule takes the following form: 
En?ityType := template; 
The rule can be read as Ent?tyType is defined 
as template. A successful match of the pattern in 
template signifies a successfully extracted entity. 
The template consists of a series of legal tokens, 
which are shown in Table 1. In addition, token 
modifiers (also in Table 1) can alter the meaning 
of the immediately preceding token. Tokens sur- 
rounded by curly braces ({}) are saved as bound 
variables, which can be later utilized to build re- 
lations (ternary expressions). These variables are 
referenced numerically starting at zero (e.g., the 
0th bound variable). 
5.2 Relat ion Rules 
A relation rule is triggered by the successful ex- 
traction of a particular entity (Ent?tyType). The 
relations grammar directs the construction ofthe 
actual ternary expression. A relation rule takes 
the following form: 
EntityType :=> <atoml atom2 acorn3>; 
The EntityType is the trigger for the relation, 
i.e., the rule is applied whenever a string of that 
type is extracted. The right hand side of the re- 
lation rule is the ternary expression to be gener- 
ated, which is a triple composed of three atoms. 
Valid atoms are shown in Table 2. They are either 
string literals or they manipulate the bound vari- 
ables saved from the extraction process in some 
manner. 
5For an algorithm converting regular expressions 
to nondeterministic finite-state automata, please refer 
to (Aho et al, 1988), Chapter 3. 
71 
POS 
POS \[string\] 
Enl;il~yType 
(tokeno I tokenx I . .  ? ) 
Descr ipt ion 
This matches any word tagged as the part-of-speech POS. 
This matches a specific word (string) of a specific part-of-speech (POS). 
This matches any extracted string of type EntityType. 
This expression matches any one of the alternative tokens given within 
the parentheses. Matches are attempted in the order in which they are 
written, e.g., the first token is tried first. 
Token Mod i f ie r  Descr ipt ion 
This modifier matches zero or more occurrences of the previous token. 
This modifier matches zero or one occurrence of the previous token. 
This modifier matches one or more occurrences of the previous token. 
Table 1: Valid tokens and token modif iers for ext ract ion  rules. 
Mod i f ie r  
In \ ]  
{~ 
\[Q ,Ent i tyTypel  \[3\] . . . .  
( alternativex l alternative21 ? ? ? ) 
' str ing' 
Descr ipt ion 
Evaluates to the nth bound variable of the trigger EntityType, 
interpreted as a string. 
Evaluates to the nth bound variable of the trigger EntityType, 
interpreted as a list of strings. The extraction rule token inside 
the bound variable is stripped of its outermost * or +, and the 
bound variable is broken into a list according to this pattern. For 
example, {JJX*} is interpreted as a list of JJX, or adjectives. 
This expression extracts a bound variable nested inside other 
bound variables. The ith bound variable of trigger FaxtityType 
is extracted; if this item is of type FEntityTypel, then the j th  
bound variable is extracted (the expression returns fa l se  if the 
entity types do not match); each comma separated unit is inter- 
preted in this manner, up to an arbitrary depth. 
This compound expression evaluates to the disjunction of an 
arbitrary number of valid atoms (as defined in this table). Each 
alternative is evaluated in a left to right order; the disjunction 
evaluates to the first alternative that returns a non-empty string. 
A literal string. 
Table 2: Valid atoms for the relat ion rules. 
Ezt ract ion  Rules:  
Relat ion Rules: 
NounGroup := (Pi~.PZIDT)? {JJX*} {(IflfPXINI~IXIINlfPSII~INS)+}; 
PrepositionalPhrase : = IN {NounGroup} ; 
ComplexNounGroup := {NounGroup} {PrepositionalPhrase}; 
NounGroup :=> <{0} 'describes ~ \[I\]>; 
ComplexNounGroup : ffi> 
< \[0\] ,Nou~Group \[1\] 
re la ted- to  ~ 
\[I\], PrepositionalPhrase \[0\] ,NounGroup \[1\] >; 
Figure 1: F.xample of re lat ion and extract ion rules. (PRPZ is the part-of-speech tag for possessive pronouns, 
DT for determiners, JJX for adjectives, J JR for comparative adjectives, JJS for superlative adjectives, NNX for 
singular or mass nouns, NNS for plural nouns, NNPX for singular proper nouns, NNPS for plural proper nouns, IN 
for prepositions.) 
72 
5.3 Examples  
A few extraction and relation rules are given 
in Figure 1. The first extraction rule defines 
a NounGroup as a sequence consisting of: an 
optional possessive pronoun or determiner, any 
number of adjectives, one or more nouns (of 
any type). Also, the sequence of adjectives is 
saved as the 0th bound variable, and the se- 
quence of nouns is saved as the 1st bound vari- 
able. The rules for PrepositionalPhrase and 
ComplexNounGroup can be interpreted similarly. 
Consider the following noun phrase: 
the big, bad wolf of the dark forest 
REXTOR recognizes two NounGroups in the 
above phrase: the big, bad wolf and the dark for- 
est. The corresponding relation rule triggers, and 
generates the following relations: 
< (big, bad) describes wolf > 
< (dark) describes forest > 
Note that the first bound variable in NounGroup 
is interpreted as a list; thus, the above two re- 
lations expand into three distinct relations when 
completely enumerated: 
< big describes wolf > 
< bad describes wolf > 
< dark describes forest > 
The ability to interpret bound variables as a list 
of strings allows for easy manipulation of repeated 
structure, like textual lists or enumerations. 
In addition, the entire noun phrase the big, bad 
wolf of the dark/forest will be recognized as a 
ComplexNounGroup. This will result in the fop 
lowing relation: 
< wolf related-to forest > 
The relation rule associated with 
ComplexNounGroup involves extracting nested 
bound variables. The first atom evaluates to 
the lth bound variable (a NounGroup) inside 
the 0th bound variable inside the trigger item 
ComplexNounGroup. The third atom is similarly 
evaluated. 
6 D iscuss ion  
Informal analysis of documents using REXTOR re- 
veals that it can potentially serve as an effective 
framework for extracting "meaning" from docu- 
ments. In particular, the system is capable of 
identifying the following types of linguistic con- 
structions and generating relations from them: 
? S imple sentences can be extracted by 
noting a simple NounGroup VerbGroup 
NounGroup pattern. From this, subject-verl>- 
object (SVO) relations can be derived. 
? Predicative nominat ives can be recog- 
nized by identifying the "be" verb and the 
NounGroup directly following it. These con- 
structions may be useful in establishing onto- 
logical hierarchies, i.e., is-a trees. 
? Predicative adjectives can be recognized 
by the "be" verb and a succession of one or 
more adjectives (or adjectival phrase). They 
may provide addition information regarding 
the attributes of entities, e.g., has-property. 
? Appositives are characteristically offset by 
commas and usually contain a single noun 
phrase; thus, they can be recognized rela- 
tively easily. Common in prose, appositives 
offer a wealth of additional information re- 
garding various entities, e.g., location of sites, 
age or position of people, etc. 
? Prepositional phrases are relatively easy 
to extract, and may supply valuable relations 
that increase the precision of information re- 
trieval systems. Ternary expressions allow 
for a better representation of prepositional 
phrases (compared to pairs) because they al- 
low the preposition to more specifically de- 
termine the type of relation (thus, examples 
like "boat by the water" and "boat under 
the water," which have completely different 
meanings, may be indexed separately and dis- 
tinctly). 
However, the prepositional phrase attach- 
ment problem (in the general-domain case) 
is still an open research topic, and thus poses 
some problems to content analysis. Regard- 
less, for the purposes of information retrieval, 
it may be acceptable to err on the side of over- 
generation in considering attachment, i.e., 
enumerate all possible relations. This will 
no doubt generate a large number of (pos- 
sibly incorrect) relations, and more research 
is required to determine effective methods of 
controlling this explosion. 
? Relative clauses of some types can be iden- 
tiffed by a finite-state language model. They 
may supply additional useful SVO relations 
for indexing purposes. 
We believe that future breakthroughs in natu- 
ral language information retrieval will occur in the 
generation of meaningful relations. Although the 
finite-state language model of REXTOR is powerful 
73 
enough to extract many linguistically interesting 
constructions, the approach is not fundamentally 
new. What differentiates our system from pre- 
vious work such as FASTUS (Hobbs et al, 1996) 
is that REXTOR not only provides a mechanism 
for extraction, but also introduces the paradigm 
of ternary expressions to capture document con- 
tent for information retrieval. The relations view 
of natural language documents i highly amenable 
to integration with information retrieval systems. 
Through a relations representation, REXTOR is 
able to distinguish the subtle differences in mean- 
ing between the pairs of sentences and phrases 
given in the introduction: 
(1) The man ate the dog. 
< man is-subject-of eat > 
< dog is-object-of eat > 
(I') The dog ate the man. 
< man is-object-of eat > 
< dog is-subject-of eat > 
(2) The meaning of life 
< meaning possessive-relation life > 
(2') A meaningful life 
< meaningful describes life > 
(3) The bank of the river 
< bank possessive-relation river > 
(3') The bank near the river 
< bank near-relation river > 
The ability to extract subject-verb-object re- 
lations, e.g., (1) and (1'), allows an IR system 
to distinguish between two very different state- 
ments. Similarly, REXTOR can differentiate be- 
tween prepositional phrases (2) and adjectival 
modification (2'). Although the system does not 
have any notion of semantics (e.g., word sense), 
syntax may offer crucial clues to meaning in cases 
such as (3) and (3'). 
Similarly, REXTOR is capable of performing lin- 
guistic normalization at the syntactic and mor- 
phological levels. Consider these sets of examples 
originally presented in the introduction: 
(4) What is Bill Gates' net worth? 
(4') What is the net worth of Bill Gates? 
< "net worth" related-to "Bill Gates" > 
(5) John gave the book to Mary. 
(5') John gave Mary the book. 
(5") Mary was given the book by John. 
< John is-subject-of give > 
< book is-direct-object-of give > 
< Mary is-indirect-object-of give > 
(6) The president surprised the country with 
his actions. 
< president is-subject-of surprise > 
< country is-object-of surprise > 
< surprise with actions > 
(6') The president's actions surprised his 
country. 
< actions re la ted- to  president > 
< actions is-subject-of surprise > 
< country is-object-of surprise > 
(7) Over 22 million people live in Waiwan. 
< "22 million" is-quantity-of people > 
< people is-subject-of live > 
< live in Taiwan > 
(7') The population of Taiwan is 22 million. 
< population is "22 million" > 
< population related-to Taiwan > 
With relations, different surface forms of ex- 
pressing the "possession relation" may be nor- 
malized into the same structure, e.g., (4) and 
(4'). Similarly, alternative surface realization of 
the same verb-headed relation can be recognized 
and equated with each other by writing different 
extraction rules that generate the same relations, 
e.g., (5), (5'), and (5"). The process of normal- 
ization will hopefully lead to greater ecall in in- 
formation retrieval systems. Note that (6) and 
(6') demonstrate a limitation of REXTOR, namely 
its inability to deal with alternative r alizations of 
verb arguments. Also, the system does not have 
any notion of semantics, and thus is unable to 
equate two sentences that have the same meaning, 
e.g., (7) and (7'). Although it is certainly possible 
to manually encode such semantic knowledge as 
extraction and relation rules, this solution is far 
from elegant. 
A potential solution to this semantic variations 
problem is to borrow the solution employed by 
START. A ternary expression representation of
natural language mimics its syntactic organiza- 
tion, and hence sentences that differ in surface 
form but are close in meaning will not map into 
the same structure. In order to solve this problem, 
START deploys "S-rules" (Katz and Levin, 1988), 
which are reversible syntactic/semantic transfor- 
mational rules that render explicit he relationship 
between alternate realizations of the same mean- 
ing. For example, a buy expression is semantically 
equivalent to a sell expression, except he subject 
and indirect objects are exchanged. Because many 
verbs can undergo the same alternations, they can 
in fact be grouped into verb classes, and hence 
governed by the same S-rules. Thus, S-rules can 
be viewed as metarules applied over ternary ex- 
pressions. A similar technique for handling both 
syntactic and semantic variations can be found in 
(Grishman, 1995; Jacquemin et al, 1997). Both 
utilize metarules (e.g., for passive/active transfor- 
mation) applied over textual patterns in order to 
generate and handle variations. 
74 
Below we present a concrete example of how 
REXTOR could potentially improve the perfor- 
mance of existing keyword search engines dramat- 
ically. We indexed an electronic version of the 
Worldbook Encyclopedia at the sentence level us- 
ing the following two techniques: 
1. A simple inverted keyword index. All stop- 
words are thrown out, and all content words 
are stemmed. Retrieval was performed by  
matching content words in the query with 
content words in the encyclopedia articles. 
2. A ternary expressions index using the rela- 
tions generated by REXTOR. The grammar 
was written to extract possessive relations, 
description relations (adjective-noun modifi- 
cation), prepositional relations, subject-verb 
relations, and verb-object relations. Re- 
trieval was performed by matching ternary 
expressions from the query (extracted using a 
separate grammar) with ternary expressions 
extracted from the encyclopedia articles. 
The following shows the results of the keyword 
search engine: 
Question: What do frogs eat? 
Answer:  
(R1) Adult frogs eat mainly insects and 
other small animals, including earthworms, 
minnows, and spiders. 
(R2) Bow'fms eat mainly other fish, frogs, 
and crayfish. 
(R3) Most cobras eat many kinds of ani- 
mals, such as frogs, fishes, birds, and various 
small mammals. 
(R4) One group of South American frogs 
feeds mainly on other frogs. 
(RS) Cranes eat a variety of foods, including 
frogs, fishes, birds, and various small mam- 
mals. 
(R6) Frogs eat many other animals, includ- 
ing spiders, flies, and worms. 
(R7) ... 
After removing stopwords from the query, our 
simple keyword search engine returned 33 results 
that contain the keywords frog and eat. How- 
ever, only (R1), (R4), and (R6) correctly answer 
the user query; the other results answer the ques- 
tion "What eats frogs?" or otherwise coinciden- 
tally contain those two terms. (Apparently, our 
poor frog has more predators than prey.) A bag- 
of-words approach fundamentally cannot differen- 
tiate between a query in which the frog is in the 
subject position and a query in which the frog is in 
the object position. However, by parsing subject- 
verb-object relations using REXTOR, a ternary ex- 
pressions indexer can effectively filter out irrele- 
vant results, returning the three correct responses. 
While indexing relations may potentially ower re- 
call, due to unanticipated constructions, it has a 
tremendous potential in increasing precision. 
Furthermore, consider the following queries, in 
which REXTOR would outperform traditional key- 
word engines: 
(8) How many South Koreans were recently 
allowed to visit their North Korean rela- 
tives? 
(9) Where did John see Mary? 
(10) Regarding what issue did the president 
of Russia criticize China? 
(11) Are electronics the biggest export from 
Japan to the United States? 
A traditional search engine using the bag-of- 
words approach would suffer from poor precision 
when faced with the above queries. Many verbs 
take arguments of the same semantic type, and in 
most of these sentences, reordering the verb argu- 
ments drastically alters their meaning. For exam- 
ple, a keyword search engine would not be able to 
distinguish between a question regarding South 
Koreans visiting North Korea and North Kore- 
ans visiting South Korea (8) because both queries 
have the same keyword content. Similarly, the 
keyword approach would be unable to determine 
who did the seeing (9), or who did the criticiz- 
ing (I0). Modification relations also pose difllcul- 
ties to the bag-of-words paradigm, e.g., was it the 
North Korean or South Korean relatives (8)? Was 
it the president of Russia or the president of China 
(10)? Furthermore, there are some constructions 
whose meaning critically depends on relations be- 
tween the entities, e.g., (11), because "from X to 
Y" and "from Y to X" usually differ in meaning. 
The current version of REXTOR is merely apro- 
totype; thus, we have made minimal attempts 
to optimize its processing speed. On a Pentium 
Ill 933 MHz Linux system with 512 megabytes 
of RAM,  s analyzing a sentence in the Worldbook 
Encyclopedia required 0.0378 seconds on average. 
This translates into a content analysis rate of 
roughly 340 words a second, or approximately 11.4 
megabytes of text per hour. Although the system 
composed of REXTOR and the ternary expressions 
indexer is slower than the simple keyword indexer, 
we believe that the potential to dramatically in- 
crease precision offsets the longer processing time. 
SHowever, REXTOR is not a memory-intensive sys- 
tem; RAM utilization during trial runs was rather low. 
75 
This paper presents only the first stage of an 
linguistically-motivated information retrieval sys- 
tem. Although we have presented the results of 
a preliminary investigation i to the effectiveness 
of this approach, we cannot draw any conclu- 
sions until more comprehensive t sts have been 
conducted. However, many prior techniques used 
in natural language information retrieval (e.g., 
head/modifier pairs) can be expressed within the 
ItEXTOR framework, and furthermore the system 
provides a playground for experimenting with new 
techniques. Thus, we believe that our approach 
shows great promise in moving towards higher 
performance information retrieval systems. 
7 Conc lus ion  
This paper presented a scheme for integrating nat- 
ural language processing and information retrieval 
by adopting a finite-state model of language and 
a ternary expression representation f document 
content. We provided justification for our lan- 
guage model and representational structures in 
both linguistic and empirical terms. ItEXTOR is 
an implementation of our ideas - -  it not only in- 
tegrates many previous natural anguage indexing 
techniques, but also provides a sufficiently gen- 
eral framework for much future experimentation. 
Although we have not yet conducted comprehen- 
sive tests, the extraction of "meaning" from doc- 
uments using ItEXTOR promises to better fulfill 
users' information eeds. 
8 Acknowledgments  
We would like to thank Sue Felshin for her insight- 
ful comments in reviewing drafts of this paper. 
Re ferences  
Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. 
1988. Compilers- Principles, Techniques, and 
Tools. Addison-Wesley. 
Avi Arampatzis, Th.P. van der Weide, C.H.A. 
Koster, and P. van Bommel. 1998. Phrase- 
based information retrieval. Information Pro- 
cessing and Management, 34(6):693-707, De- 
cember. 
Avi Arampatzis, Th.P. van der Weide, C.H.A. 
Koster, and P. van Bommel. 2000. An 
evaluation of linguistically-motivated in exing 
schemes. In Proceedings o.f BCS-IRSG 2000 
Colloquium on IR Research. 
Eric Brill. 1992. A simple rule-based part of 
speech tagger. In Proceedings of the Third Con- 
.ference on Applied Natural Language Process- 
ing. 
Noam Chomsky. 1959a. A note on phrase 
structure grammars. Information and Control, 
2:393-395. 
Noam Chomsky. 1959b. On certain formal prop- 
erties of grammars. Information and Control, 
2:137-167. 
Kenneth W. Church. 1980. On memory limita- 
tions in natural anguage processing. Technical 
Iteport Tit-245, MIT Laboratory for Computer 
Science. 
Bruce Croft and David D. Lewis. 1987. An ap- 
proach to natural anguage processing for doc- 
ument retrieval. In Proceedings of the lOth An- 
nual International ACM SIGIR Conference on 
Research and Development in Information Re- 
trieval (SIGIR-87). 
Joel L. Fagan. 1987. Experiments in Auto- 
matic Phrase Indexing .for Document Retrieval: 
A Comparisons of Syntactic and Non-Syntactic 
Methods. Ph.D. thesis, Cornell University. 
Ralph Grishman and John Sterling. 1993. New 
York University: Description of the PROTEUS 
system as used for MUC-5. In Proceedings of the 
5th Message Understanding Conference (MUG- 
5). 
Ralph Grishman. 1995. The NYU system for 
MUC-6 or where's the syntax. In Proceedings 
of the 6th Message Understanding Conference 
(MUC-6). 
George E. Heidorn. 1972, Natural lan- 
guage inputs to a simulation programming sys- 
tems. Technical Iteport NPS-55HD72101A, 
Naval Postgraduate School. 
Jerry It. Hobbs, Douglas Appelt, John Bear, 
David Israel, Megalmi Kameyama, Mark Stickel, 
and Mabry Tyson. 1996. FASTUS: A cascaded 
finite-state transducer for extracting informa- 
tion from natural-language text. In Roche and 
Schabes, editors, Finite State Devices .for Nat- 
ural Language Processing. MIT Press. 
Christian Jacquemin, Judith L. Klavans, and Eve- 
lyne Tzoukermann. 1997. Expansion of multi- 
word terms for indexing and retrieval using 
morphology and syntax. In Proceedings of the 
35th Annual Meeting of the Association for 
Computational Linguistics (A CL '97). 
Karen Jensen, George E. Heidorn, and Stephen D. 
Richardson, editors. 1993. Natural Language 
Processing: The PLNLP Approach. Kluwer 
Academic Publishers. 
76 
Boris Katz and Beth Levin. 1988. Exploiting lex- 
ical regularities in designing natural language 
systems. In Proceedings of the l~h Interna- 
tional Conference on Computational Linguistics 
(COLING '88). 
Boris Katz. 1980. A three-step rocedure for lan- 
guage generation. Technical Report 599, MIT 
Artificial Intelligence Laboratory. 
Boris Katz. 1990. Using English for indexing and 
retrieving. In P.H. Winston and S.A. Shellard, 
editors, Artificial Intelligence at MIT: Expand- 
ing Frontiers, volume 1. MIT Press. 
Boris Katz. 1997. Annotating the World Wide 
Web using natural anguage. In Proceedings of 
the 5th RIA O Conference on Computer Assisted 
Information Searching on the Internet (RIAO 
'97). 
Edward Loper. 2000. Applying semantic rela- 
tion extraction to information retrieval. Mas- 
ter's thesis, Massachusetts Institute of Technol- 
ogy. 
Fernando Pereira and Rebecca Wright. 1991. 
Finite-state approximation of phrase structure 
grammars. In Proceedings of the 29th Meeting 
of the ACL. 
Alan F. Smeaton, Ruairi O'Donnell, and Fergus 
Kelledy. 1994. Indexing structures derived 
from syntax in TREC-3: System description. 
In Proceedings of the 3rd Text REtrieval Con- 
ference (TREC-3). 
Tomek Strzalkowski, Louise Guthrie, Jussi Karl- 
gren, Jim Leistensnider, Fang Lin, Jose Perez- 
Carballo, Troy Straszheim, Jin Wang, and Jon 
Wilding. 1996. Natural language information 
retrieval: TREC-5 report. In Proceedings of the 
5th Text REtrieval Conference (TREC-5). 
J. Thorne, P. Bratley, and H. Dewar. 1968. The 
syntactic analysis of English by machine. In 
Donald Michie, editor, Machine Intelligence 3. 
Edinburgh University Press. 
William A. Woods. 1970. Transition network 
grammars for natural anguage analysis. Com- 
munications of the ACM, 13(10). 
Chengxiang Zhai, Xiang Tong, Natasa Milic- 
Frayling, and David A. Evans. 1996. Evalu- 
ation of syntactic phrase indexing - CLARIT 
NLP track report. In Proceedings of the 5th 
Text REtrieval Conference (TREC-5). 
77 
Gathering Knowledge for a Question Answering System from
Heterogeneous Information Sources
Boris Katz and Jimmy Lin and Sue Felshin
MIT Articial Intelligence Laboratory
200 Technology Square
Cambridge, MA 02139
fboris, jimmylin, sfelshing@ai.mit.edu
Abstract
Although vast amounts of information
are available electronically today, no ef-
fective information access mechanism ex-
ists to provide humans with convenient
information access. A general, open-
domain question answering system is a
solution to this problem. We propose an
architecture for a collaborative question
answering system that contains four pri-
mary components: an annotations sys-
tem for storing knowledge, a ternary ex-
pression representation of language, a
transformational rule system for han-
dling some complexities of language, and
a collaborative mechanism by which or-
dinary users can contribute new knowl-
edge by teaching the system new infor-
mation. We have developed a initial pro-
totype, called Webnotator, with which to
test these ideas.
1 Introduction
A tremendous amount of heterogenous informa-
tion exists in electronic format (the most promi-
nent example being the World Wide Web), but the
potential of this large body of knowledge remains
unrealized due to the lack of an eective informa-
tion access method. Because natural language is
the most convenient and most intuitive method
of accessing this information, people should be
able to access information using a system capa-
ble of understanding and answering natural lan-
guage questions|in short, a system that com-
bines human-level understanding with the infal-
lible memory of a computer.
Natural language processing has had its suc-
cesses and failures over the past decades; while the
successes are signicant, computers will not soon
be able to fully process and understand language.
In addition to the traditional di?culties associ-
ated with syntactic analysis, there remains many
other problems to be solved, e.g., semantic inter-
pretation, ambiguity resolution, discourse model-
ing, inferencing, common sense, etc. Furthermore,
not all information on the Web is textual|some
is sound, pictures, video, etc. While natural lan-
guage processing is advanced enough to under-
stand typical interactive questions about knowl-
edge (interactive questions are typically fairly sim-
ple in structure), it cannot understand the knowl-
edge itself. For the time being, therefore, the
only way for computers to access their own knowl-
edge is for humans to tell the computers what the
knowledge means in a language that the comput-
ers can understand|but still in a language that
humans can produce. A good way to accomplish
this is with the use of natural language annota-
tions, sentences which are simple enough for a
computer to analyze, yet which are in natural hu-
man language. Once knowledge is so annotated,
and indexed in a knowledge repository, a question
answering system can retrieve it.
The Start (SynTactic Analysis using Re-
versible Transformations) Natural Language Sys-
tem (Katz, 1990; Katz, 1997) is an example of a
question answering system that uses natural lan-
guage annotations. Start is a natural language
question answering system that has been available
to users on the World Wide Web
1
since Decem-
ber, 1993. During this time, it has engaged in
millions of exchanges with hundreds of thousands
of people all over the world, supplying users with
knowledge regarding geography, weather, movies,
corporations, and many many other areas. De-
spite the success of Start in serving real users,
its domain of expertise is relatively small and ex-
panding its knowledge base is a time-consuming
task that requires trained individuals.
We believe that the popularity of the Web may
oer a solution to this knowledge acquisition prob-
lem by providing collaborative mechanisms on a
scale that has not existed before. We can poten-
tially leverage millions of users on the World Wide
1
http://www.ai.mit.edu/projects/infolab
Web to construct and annotate a knowledge base
for question answering. In fact, we had proposed
a distributed mechanism for gathering knowledge
from the World Wide Web in 1997 (Katz, 1997),
but only recently have we attempted to implement
this idea.
An advantage of natural language annotations
is that it paves a smooth path of transition as nat-
ural language processing technology improves. As
natural language analysis techniques advance, the
annotations may become more and more complex.
Eventually, a textual information segment could
be its own annotation; someday, through other
technologies such as speech and image recognition,
etc., annotations could even be automatically con-
structed for non-textual information.
A further advantage is that natural language
annotations can be processed via techniques that
only partially understand them|via IR engines,
or less-than-ideal natural language systems|yet
they retain their more complex content and can be
reanalyzed at a later date by more sophisticated
systems.
2 Overview
We propose a collaborative question answering ar-
chitecture composed of the four following compo-
nents:
1. Natural Language Annotation is a tech-
nique of describing the content of informa-
tion segments in machine parsable natural
language sentences and phrases.
2. Ternary Expressions are subject-relation-
object triples that are expressive enough
to represent natural language, and also
amenable to rapid, large-scale indexing.
3. Transformational Rules handle the prob-
lem of linguistic variation (the phenomenon
in which sentences with dierent surface
structures share the same semantic content)
by explicitly equating representational struc-
tures (derived from dierent surface forms)
that have approximately the same meaning.
4. Collaborative Knowledge Gathering is a
technique by which the World Wide Web may
be viewed not only as a knowledge resource,
but also a human resource. The knowledge
base of a question answering system could be
constructed by enlisting the help of millions
of ordinary users all over the Web.
3 Annotations
Natural language annotations are machine-
parsable sentences or phrases that describe the
content of various information segments. They de-
scribe the questions that a particular segment of
information is capable of answering. For example,
the following paragraph about polar bears:
Most polar bears live along the northern
coasts of Canada, Greenland, and Russia,
and on islands of the Arctic Ocean. . .
may be annotated with one or more of the follow-
ing:
Polar bears live in the Arctic.
Where do polar bears live?
habitat of polar bears
A question answering system would parse these
annotations and store the parsed structures with
pointers back to the original information segment
that they described. To answer a question, the
user query would be compared against the anno-
tations stored in the knowledge base. Because this
match occurs at the level of ternary expressions,
structural relations and transformation (to be dis-
cussed in Section 5) can equate queries and anno-
tations even if their surface forms were dierent.
Furthermore, linguistically sophisticated machin-
ery such as synonymy/hyponymy, ontologies, can
be brought to bear on the matching process. If a
match were found, the segment corresponding to
the annotation would be returned to the user as
the answer.
The annotation mechanism we have outlined
serves as a good basis for constructing a question
answering system because annotating information
segments with natural language is simple and intu-
itive. The only requirement is that annotations be
machine parsable, and thus the sophistication of
annotations depends on the parser itself. As natu-
ral language understanding technology improves,
we can use more and more sophisticated annota-
tions.
In addition, annotations can be written to de-
scribe any type of information, e.g., text, im-
ages, sound clips, videos, and even multimedia.
This allows integration of heterogenous informa-
tion sources into a single framework.
Due to the vast size of the World Wide Web,
trying to catalog all knowledge on the World Wide
Web is a daunting task. Instead, focusing on
meta-knowledge is a more promising approach to
building a knowledge base that spans more than a
tiny fraction of the Web. Consider that reference
librarians at large libraries obviously don't know
all the knowledge stored in the reference books,
but they are nevertheless helpful in nding infor-
mation, precisely because they have a lot of knowl-
edge about the knowledge. Natural language anno-
tations can assist in creating a smart \reference
librarian" for the World Wide Web.
4 Representing Natural Language
A good representational structure for natural lan-
guage is ternary expressions.
2
They may be in-
tuitively viewed as subject-relation-object triples,
and can express most types of syntactic relations
between various entities within a sentence. We be-
lieve that the expressiveness of ternary relations
is adequate for capturing the information need of
users and the meaning of annotations. For ex-
ample, \What is the population of Zimbabwe?"
would be represented as two ternary expressions:
[what is population]
[population of Zimbabwe]
Ternary expressions can capture many rela-
tionships between entities within a sentence.
Such a representational structure is better than
a keyword-based scheme which equates a doc-
ument's keyword statistics with its semantic
content. Consider the following sets of sen-
tences/phrases that have similar word content,
but (dramatically) dierent meanings:
3
(1) The bird ate the young snake.
(1
0
) The snake ate the young bird.
(2) The meaning of life
(2
0
) A meaningful life
(3) The bank of the river
(3
0
) The bank near the river
Ternary expressions abstract away the linear or-
der of words in a sentence into a structure that is
closer to meaning, and therefore a relations-based
information access system will produce much more
precise results.
We have conducted some initial information re-
trieval experiments comparing a keyword-based
approach with one that performs matching based
on relations
4
. Using Minipar (Lin, 1999), we
parsed the entire contents of the Worldbook En-
cyclopedia and extracted salient relations from
it (e.g., subject-verb-object, possessives, prepo-
sitional phrase, etc.) We found that precision
2
See (Katz, 1990; Katz, 1997) for details about
such representation in Start.
3
Examples taken from (Loper, 2000)
4
to be published
for relations-based retrieval was much higher than
for keyword-based retrieval. In one test, retrieval
based on relations returned the database's three
correct entries:
Question: What do frogs eat?
Answer:
(R1) Adult frogs eat mainly insects and
other small animals, including earthworms,
minnows, and spiders.
(R4) One group of South American frogs
feeds mainly on other frogs.
(R6) Frogs eat many other animals, includ-
ing spiders, ies, and worms.
compared to 33 results containing the keywords
frog and eat which were returned by the keyword-
based system|the additional results all answer a
dierent question (\What eats frogs?") or other-
wise coincidentally contain those two terms.
Question: What do frogs eat?
Answer:
. . .
(R7) Adult frogs eat mainly insects and
other small animals, including earthworms,
minnows, and spiders.
(R8) Bowns eat mainly other sh, frogs,
and craysh.
(R9) Most cobras eat many kinds of animals,
such as frogs, shes, birds, and various small
mammals.
(R10) One group of South American frogs
feeds mainly on other frogs.
(R11) Cranes eat a variety of foods, includ-
ing frogs, shes, birds, and various small
mammals.
(R12) Frogs eat many other animals, includ-
ing spiders, ies, and worms.
(R13) . . .
Another advantage of ternary expressions is
that it becomes easier to write explicit transfor-
mational rules that encode specic linguistic vari-
ations. These rules are capable of equating struc-
tures derived from dierent sentences with the
same meaning (to be discussed in detail later).
In addition to being adequately expressive for
our purposes, ternary expressions are also highly
amenable to rapid large-scale indexing and re-
trieval. This is an important quality because
a large question answering system could poten-
tially contain answers to millions of questions.
Thus, compactness of representation and e?-
ciency of retrieval become an important consid-
eration. Ternary expressions may be indexed and
retrieved e?ciently because they may be viewed
using a relational model of data and manipulated
using relational databases.
5 Handling Linguistic Variation
Linguistic variation is the phenomenon in which
the same meaning can be expressed in a variety
of dierent ways. Consider these questions, which
ask for exactly the same item of information:
(4) What is the capital of Taiwan?
(5) What's the capital city of Taiwan?
(6) What is Taiwan's capital?
Linguistic variations can occur at all levels of
language; the examples above demonstrate lexical,
morphological and syntactic variations. Linguistic
variations may sometimes be quite complicated,
as in the following example, which demonstrates
verb argument alternation.
5
(7) Whose declaration of guilt shocked the
country?
(8) Who shocked the country with his dec-
laration of guilt?
Transformational rules provide a mechanism to
explicitly equate alternate realizations of the same
meaning at the level of ternary expressions.
As an example, Figure 1 shows a sample trans-
formational rule for (7) and (8).
6
Thus, through
application of this rule, question (7) can be
equated with question (8).
[n
1
shock n
2
] [n
3
shock n
2
]
[shock with n
3
] $
[n
3
related-to n
1
] [n
3
related-to n
1
]
where n 2 Nouns where n 2 Nouns
Figure 1: Sample Transformational Rule
Transformational rules may be generalized by
associating arbitrary conditions with them; e.g.,
verb 2 shock, surprise, excite : : :
A general observation about English verbs is
that they divide into \classes," where verbs in
the same class undergo the same alternations.
For example, the verbs `shock', `surprise', `excite',
etc., participate in the alternation shown in Sen-
tence (7) and (8) not by coincidence, but because
5
Beth Levin (Levin, 1993) oers an excellent treat-
ment on English verb classes and verb argument al-
ternations.
6
This rule is bidirectional in the sense that each
side of the rule implies the other side. The rule is
actually used in only one direction, so that we canon-
icalize the representation.
they share certain semantic qualities. Although
the transformational rule required to handle this
alternation is very specic (in that it applies to a
very specic pattern of ternary expression struc-
ture), the rule can nevertheless be generalized over
all verbs in the same class by associating with the
rule conditions that must be met for the rule to
re, i.e., verb 2 emotional-reaction-verbs; see Fig-
ure 2.
[n
1
v
1
n
2
] [n
3
v
1
n
2
]
[v
1
with n
3
] $
[n
3
related-to n
1
] [n
3
related-to n
1
]
where n 2 Nouns and v 2 emotional-reaction-verbs
Figure 2: Sample Transformational Rule
Note that transformational rules can also en-
code semantic knowledge and even elements of
common sense. For example, a rule can be written
that equates a selling action with a buying action
(with verb arguments in dierent positions). Or
as another example, rules can even encode impli-
catures, e.g., A murdered B implies that B is dead.
Transformational rules can apply at the syntac-
tic, semantic, or even pragmatic levels, and oer
a convenient, powerful, and expressive framework
for handling linguistic variations.
In order for a question answering system to be
successful and have adequate linguistic coverage,
it must have a large number of these rules. A lexi-
con which classied verbs by argument alternation
patterns would be a good start, but this is another
resource lacking in the world today. Rules gener-
ally may be quite complex, and it would be di?-
cult to gather such knowledge from average Web
users with little linguistic background. Requesting
that users describe segments with multiple anno-
tations (each representing a dierent phrasing of
the description), might serve as a preliminary so-
lution to the linguistic variation problem. Another
possible solution will involve learning transforma-
tional rules from a corpus. The di?culty in cre-
ating transformational rules is a serious problem
and unless and until this problem is solved, an
NL-based QA system would have to be restricted
to a limited domain where a small number of ex-
perts could provide enough transformational rule
coverage, or would require a large commitment of
resources to attain su?cient coverage.
6 Collaboration on the Web
A critical component of a successful natural lan-
guage question answering system is the knowledge
base itself. Although the annotation mechanism
simplies the task of building a knowledge base,
the accumulation of knowledge is nevertheless a
time consuming and labor intensive task. How-
ever, due to the simplicity of natural language an-
notations (i.e., describing knowledge in everyday
English), ordinary users with no technical skills
may contribute to a knowledge base. Thus, by
providing a general framework in which people on
the World Wide Web can enter additional knowl-
edge, we can engage millions of potential users all
over the world to collaboratively construct a ques-
tion answering system. We can distribute the ef-
fort of building a knowledge base across many or-
dinary users by allowing them to teach the system
new knowledge.
The idea of using the Internet as a tool for
collaboration across geographically distributed re-
gions is not a new idea. The Open Source move-
ment rst demonstrated the eectiveness and sus-
tainability of programming computer systems in
a distributed manner. Made possible in part by
the World Wide Web, the Open Source move-
ment promotes software development by nurtur-
ing a community of individual contributors work-
ing on freely distributed source code. Under this
development model, software reliability and qual-
ity is ensured through independent peer review by
a large number of programmers. Successful Open
Source projects include Linux, a popular Unix-like
operating system; Apache, the most popular Web
server in the World; SendMail, an utility on vir-
tually every Unix machine; and dmoz, the Open
Directory Project, whose goal is to produce the
most comprehensive directory of the Web by rely-
ing on volunteer editors.
7
Another example of Web-based collaboration
is the Open Mind Initiative (Stork, 1999; Stork,
2000), which is a recent eort to organize ordi-
nary users on the World Wide Web (netizens) to
assist in developing intelligent software. Based on
the observation that many tasks such as speech
recognition and character recognition require vast
quantities of training data, the initiative attempts
to provide a collaborate framework for collecting
data from the World Wide Web. The three pri-
mary contributors within such a framework are
domain experts, who provide fundamental algo-
rithms, tool/infrastructure developers, who de-
velop the framework for capturing data, and non-
expert netizens, who supply the raw training data.
Open Mind Commonsense
8
is an attempt at
constructing a large common sense database by
7
http://www.dmoz.org
8
http://openmind.media.mit.edu
collecting assertions from users all over the Web.
9
Other projects have demonstrated the viabil-
ity of Web-enabled collaborative problem-solving
by harnessing the computational power of idle
processors connected to the Web.
10
The SETI
(Search for Extraterrestrial Intelligence) Institute
was founded after Nasa canceled its High Resolu-
tion Microwave Survey project. The institute or-
ganizes thousands of individuals who donate their
idle processor cycles to search small segments of
radio telescope logs for signs of extraterrestrial
intelligence.
11
Other similar projects that orga-
nize the usage of idle processor time on personal
computers include the Internet Mersenne Prime
Search,
12
and the RC5 Challenge.
13
Recent technical, social, and economic devel-
opments have made the abovementioned models
of collaboration possible. Furthermore, numerous
successful projects have already demonstrated the
eectiveness of these collaborative models. Thus,
it is time to capitalize on these emerging trends
to create the rst collaborative question answer-
ing system on the World Wide Web.
Even with the components such as those de-
scribed above, there still remains a major hurdle
in jumpstarting the construction of a collaborative
question answering system. We are faced with a
classic chicken-and-egg problem: in order to at-
tract users to contribute knowledge, the system
must serve a real information need (i.e., actually
provide users with answers). However, in order
to serve user information needs, the system needs
knowledge, which must be contributed by users.
In the initial stages of building a question an-
swering system, the knowledge base will be too
sparse to be useful. Furthermore, the system may
be very brittle, and might not retrieve the correct
information segment, even if it did exist within
the knowledge base (e.g., due to a missing trans-
formational rule).
It may be possible to address this dilemma with
an incremental approach. The system can rst
be restricted to a very limited domain (e.g., \an-
imals" or \geography"). Users' expectations will
be carefully managed so that they realize the sys-
tem is highly experimental and has a very lim-
ited range of knowledge. In eect, the users will
9
A non-collaborative approach to building a com-
mon sense knowledge base is taken by Lenat whose
Cyc project (Lenat, 1995) is an attempt to build a
common sense knowledge base through a small team
of dedicated and highly trained specialists.
10
http://www.distributed.org
11
http://setiathome.ssl.berkeley.edu
12
http://www.mersenne.org
13
http://www.distributed.org/rc5/
be populating a domain-specic knowledge base.
Over time, the system will be able to answer more
and more questions in that domain, and hence be-
gin to oer interesting answers to real users. After
this, a critical mass will form so that users are not
only teaching the system new knowledge, but also
receiving high quality answers to their questions.
At that point, a decision can be made to increase
the domain coverage of the system.
In order to initialize this process, we can boot-
strap o the curiosity and altruism of individual
users. As an example, the Openmind Common
Sense project has accumulated over 280 thousand
items of information by over six thousand users
based on a data collection model that does not
supply the user with any useful service. The
dream of building \smart" systems has always
been a fascination in our culture (e.g., HAL from
2001: A Space Odyssey); we believe that this will
serve to attract rst-time users.
7 Evolving the System
While the collaborative information gathering
task proceeds, we are then faced with the prob-
lem of maintaining the system and ensuring that
it will provide users with useful information. Two
immediate issues arise: quality control and lin-
guistic variation.
How can we insure the quality of the contributed
material? In general, any system that solicits in-
formation from the World Wide Web faces a prob-
lem of quality control and moderation. Although
most Web users are well-meaning, a small frac-
tion of Web users may have malicious intentions.
Therefore, some ltering mechanisms must be im-
plemented to exclude inappropriate content (e.g.,
pornography or commercial advertisement) from
being inserted into the knowledge base. More
troublesome is the possibility of well-meant but
incorrect information which is probably more com-
mon and denitely harder to detect.
How can we handle linguistic variation? There
are often dierent ways of asking the same ques-
tion; the annotation of a particular segment might
not match the user query, and hence the correct
answer may not be returned as a result. Transfor-
mational rules may be a solution to the problem,
but writing and compiling these rules remain a
di?cult problem.
We propose a variety of solutions for the main-
tenance of a collaborative question answering sys-
tem, depending on the level of human intervention
and supervision.
At one end of the spectrum, an unsupervised
approach to quality control can be implemented
through a distributed system of moderation with
dierent trust levels. The scheme essentially calls
for self-management of the knowledge repository
by the users themselves (i.e., the users with high
trust levels). Dierent trust levels will allow users
various levels of access to the knowledge base, e.g.,
the ability to modify or delete information seg-
ments and their associated annotations or to mod-
ify other users' trust levels. To initiate the process,
only a small group of core editors is required.
In such an unsupervised system, the problem of
linguistic variation could be addressed by prompt-
ing users to give multiple annotations, each de-
scribing the information content of a particular
segment in a dierent way. With a su?ciently
large user base, wide coverage might still be
achieved in the absence of broad-coverage trans-
formational rules.
At the other end of the spectrum, a large or-
ganization may commit signicant amounts of re-
sources to maintaining a supervised collaborative
knowledge base. For example, an organization
may be willing to commit resources to preserve
its organizational memory in the form of an \in-
telligent FAQ" supported by natural language an-
notations. Computers can be eectively utilized
to augment the memory of an organization (Allen,
1977), and have been successfully deployed in real-
world environments with relative success (Acker-
man, 1998).
If an organization were willing to commit signi-
cant resources to a collaborative knowledge reposi-
tory, then transformational rules can be written by
experts with linguistic background. Such experts
could constantly review the annotations entered
by ordinary users and formulate transformational
rules to capture generalizations.
Supervised use of natural language annotation
falls short of the grandiose goal of accessing the
entire World Wide Web, but is the practical and
useful way to apply NL annotation until the trans-
formational rule problem can be solved for unlim-
ited domains.
8 Initial Prototype
Webnotator is a prototype test-bed to evaluate
the practicality of NL-based annotation and re-
trieval through Web-based collaboration. It pro-
vides e?cient facilities for retrieving answers al-
ready stored within the knowledge base and a scal-
able framework for ordinary users to contribute
knowledge.
The system analyzes natural language annota-
tions to produce ternary expressions by postpro-
cessing the results of Minipar (Lin, 1993; Lin,
1994), a fast and robust functional dependency
parser that is freely available for non-commercial
purposes. The quality of the representational
structures depends ultimately on the quality of
whatever parser Webnotator is made to access. In
the current implementation, ternary expressions
are not embedded, elements of ternary expres-
sions are not indexed, and coreference is not de-
tected. Words are stemmed to their root form and
morphological information is discarded. The sys-
tem also implements a version of transformational
rules described above as a simple forward-chaining
rule-based system.
Using a relational database, Webnotator imple-
ments a knowledge base that stores ternary ex-
pressions derived from annotations and their asso-
ciated information segments. Ternary expressions
t neatly into a relational model of data, and thus
manipulation of the knowledge (including answer-
ing queries and inserting new knowledge) can be
formulated as SQL queries. This vastly simplies
development eorts while maintaining robustness
and performance.
Webnotator provides an interface through
which users may teach the system new knowl-
edge by supplying new information segments and
adding new annotations. Essentially, the user en-
ters, in a CGI form, an information segment and
annotations that describe the knowledge. Since
the segment of information can contain any valid
HTML, images, tables, and even multimedia con-
tent may be included. Alternatively, the user may
simply provide a URL to annotate, and Webnota-
tor will automatically create a link to the URL in
its knowledge base.
Currently, Webnotator is a prototype that has
been released to a small community of developers
and testers within the MIT Articial Intelligence
Laboratory. We plan on releasing the system to
the general public in the near future. By col-
lecting knowledge from the general public and by
varying the representations and transformations
applied by Webnotator, it should be possible to
discover which features are most important for
a natural-language-based annotation system and
whether the state of the art is indeed su?ciently
advanced to make such a system practical and ef-
fective.
9 Related Work
A variety of research has been conducted on bet-
ter information access methods on the World
Wide Web (e.g., the \Semantic Web" (Berners-
Lee, 1999)). However, most of these approaches
have concentrated on methods of annotating exist-
ing web pages with metadata such as XML/RDF
(Resource Description Framework) (Staab et al,
2000), extensions to HTML (Luke et al, 1997;
Hein et al, 1999; Staab et al, 2000), special-
ized descriptions (W. Dalitz and Lugger, 1997),
or even conceptual graphs (Martin and Eklund,
1999).
The common thread among previous work is the
embedding of metadata directly into Web docu-
ments, which are then gathered via crawling or
spidering. This approach only works if the target
community of the system is well-dened; adop-
tion of various metadata techniques are presently
limited, and thus it would be pointless to crawl
the entire web to search for metadata. A model
in which distributed metadata are gathered by a
spider will not work with a constantly changing
community that is ill-dened. In principle, there
is no reason why our natural language annotations
cannot be embedded into Web documents also; the
issue is strictly a practical concern.
Another common theme in previous work is
the organization of knowledge in accordance with
some pre-established ontology. This presents sev-
eral challenges for building a general system for
gathering knowledge. Ontologies are often ei-
ther too specic to be of general use (e.g., Ri-
boWeb's ontology for ribosome data (Altmann et
al., 1999)), or too weak to provide much structure
(e.g., Yahoo). Since the ontology is static and
must be agreed upon prior to any knowledge base
development, it may be too constricting and too
inconvenient for the expression of new or unantic-
ipated concepts. Although systems do allow for
arbitrary extension of the ontology (Hein et al,
1999; Staab et al, 2000), such extensions defeat
the purpose of a structure-imposing ontology. Our
proposed alternative to a ontological hierarchy is
to take advantage of the expressiveness of natu-
ral language, and use linguistic devices to relate
concepts. The combination of lexical resources
(e.g., synonyms and meronyms in WordNet) and
transformational rules provide a natural, extensi-
ble way to relate and structure dierent concepts.
A compelling argument for natural language an-
notations is their expressiveness and compactness.
Martin and Eklund (Martin and Eklund, 1999) ar-
gue against an XML-based system of metadata be-
cause XML was primarily intended to be machine
readable, not human readable. In their paper,
they started with an English phrase, and then pro-
ceeded to demonstrate the encoding of that sen-
tence in various formalisms. A constraint graph
encoding was simpler than a KIF (Knowledge In-
terchange Format) encoding, which was in turn
shorter than a RDF format. Of course, this begs
the question: why not just annotate the document
with the original English phrase? Current NLP
technology can handle a large variety of English
sentences and phrases, which may serve as the
annotations directly. Such is system is not only
simpler, more intuitive, but also more compact.
10 Conclusion
Recent social, technical, and economic develop-
ments have made possible a new paradigm of soft-
ware development and problem solving through
loosely-organized collaboration of individuals on
the World Wide Web. Many successful prece-
dents have already proven the viability of this ap-
proach. By leveraging this trend with existing an-
notation and natural language technology, we can
provide a exible framework for a question an-
swering system that grows and \evolves" as each
user contributes to the knowledge base, with only
minimal outside supervision. Testing will reveal
whether such a system can help users realize some
of the untapped potential of the World Wide Web
and other sources of digital information as a vast
repository of human knowledge.
References
Mark S. Ackerman. 1998. Augmenting organi-
zational memory: A eld study of answer gar-
den. ACM Transactions on Information Sys-
tems, 16(3):203{224, July.
Thomas Allen. 1977. Managing the Flow of Tech-
nology. MIT Press.
R. Altmann, M. Bada, X. Chai, M. W. Car-
illo, R. Chen, and N. Abernethy. 1999. Ri-
boWeb: An ontology-based system for collabo-
rative molecular biology. IEEE Intelligent Sys-
tems, 14(5):68{76.
T. Berners-Lee. 1999. Weaving the Web. Harper,
New York.
Je Hein, James Hendler, and Sean Luke. 1999.
SHOE: A knowledge representation language
for internet applications. Technical Report
CS-TR-4078, Institute of Advanced Computer
Studies, University of Maryland, College Park.
Boris Katz. 1990. Using English for indexing and
retrieving. In P.H. Winston and S.A. Shellard,
editors, Articial Intelligence at MIT: Expand-
ing Frontiers, volume 1. MIT Press.
Boris Katz. 1997. Annotating the World Wide
Web using natural language. In Proceedings of
the 5th RIAO Conference on Computer Assisted
Information Searching on the Internet (RIAO
'97).
Doug Lenat. 1995. CYC: A large-scale investment
in knowledge infrastructure. Communications
of the ACM, 38(11):33{38.
Beth Levin. 1993. English Verb Classes and Al-
ternations: A Preliminary Investigation. Uni-
versity of Chicago Press.
Dekang Lin. 1993. Principled-based parsing with-
out overgeneration. In Proceedings of the 31th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL'93).
Dekang Lin. 1994. PRINCIPAR|An e?cient,
broad-coverage, principle-based parser. In Pro-
ceedings of the 15th International Conference on
Computational Linguistics (COLING '94).
Dekang Lin. 1999. Minipar|a minimalist parser.
In Maryland Linguistics Colloquium, University
of Maryland, College Park, March 12,.
Edward Loper. 2000. Applying semantic relation
extraction to information retrieval. Master's
thesis, Massachusetts Institute of Technology.
S. Luke, L. Spector, D. Rager, and J. Hendler.
1997. Ontology-based web agents. In Proceed-
ings of the First International Conference on
Autonomous Agents.
Philippe Martin and Peter Eklund. 1999. Em-
bedding knowledge in web documents. In Pro-
ceedings of the Eighth International World Wide
Web Conference.
S. Staab, J. Angele, S. Decker, M. Erd-
mann, A. Hotho, A. Maedche, H.-P. Schnurr,
R. Studer, and Y. Sure. 2000. Semantic com-
munity web portals. In Proceedings of the Ninth
International World Wide Web Conference.
David G. Stork. 1999. Character and document
research in the open mind initiative. In Pro-
ceedings of the Fifth International Conference
on Document Analysis and Recognition.
David G. Stork. 2000. Open data collection for
training intelligent software in the open mind
initiative. In Proceedings of the Engineering In-
telligent Systems Symposium (EIS '2000).
M. Grotschel W. Dalitz and J. Lugger. 1997.
Information services for mathematics and the
internet. In A. Sydow, editor, Proceedings of
the 15th IMACS World Congress on Scientic
Computation: Modelling and Applied Mathe-
matics. Wissenschaft und Technik Verlag.
 	
	
Extracting Structural Paraphrases from Aligned Monolingual Corpora
Ali Ibrahim Boris Katz Jimmy Lin
MIT Artificial Intelligence Laboratory
200 Technology Square
Cambridge, MA 02139
{aibrahim,boris,jimmylin}@ai.mit.edu
Abstract
We present an approach for automatically
learning paraphrases from aligned mono-
lingual corpora. Our algorithm works
by generalizing the syntactic paths be-
tween corresponding anchors in aligned
sentence pairs. Compared to previous
work, structural paraphrases generated by
our algorithm tend to be much longer
on average, and are capable of captur-
ing long-distance dependencies. In ad-
dition to a standalone evaluation of our
paraphrases, we also describe a question
answering application currently under de-
velopment that could immensely bene-
fit from automatically-learned structural
paraphrases.
1 Introduction
The richness of human language allows people to
express the same idea in many different ways; they
may use different words to refer to the same entity or
employ different phrases to describe the same con-
cept. Acquisition of paraphrases, or alternative ways
to convey the same information, is critical to many
natural language applications. For example, an ef-
fective question answering system must be equipped
to handle these variations, because it should be able
to respond to differently phrased natural language
questions.
While there are many resources that help sys-
tems deal with single-word synonyms, e.g., Word-
Net, there are few resources for multiple-word or
domain-specific paraphrases. Because manually
collecting paraphrases is time-consuming and im-
practical for large-scale applications, attention has
recently focused on techniques for automatically ac-
quiring paraphrases.
We present an unsupervised method for acquir-
ing structural paraphrases, or fragments of syntactic
trees that are roughly semantically equivalent, from
aligned monolingual corpora. The structural para-
phrases produced by our algorithm are similar to the
S-rules advocated by Katz and Levin for question
answering (1988), except that our paraphrases are
automatically generated. Because there is disagree-
ment regarding the exact definition of paraphrases
(Dras, 1999), we employ that operating definition
that structural paraphrases are roughly interchange-
able within the specific configuration of syntactic
structures that they specify.
Our approach is a synthesis of techniques devel-
oped by Barzilay and McKeown (2001) and Lin
and Pantel (2001), designed to overcome the limi-
tations of both. In addition to the evaluation of para-
phrases generated by our method, we also describe
a novel information retrieval system under develop-
ment that is designed to take advantage of structural
paraphrases.
2 Previous Work
There has been a rich body of research on automati-
cally deriving paraphrases, including equating mor-
phological and syntactic variants of technical terms
(Jacquemin et al, 1997), and identifying equiva-
lent adjective-noun phrases (Lapata, 2001). Unfor-
tunately, both are limited in types of paraphrases that
they can extract. Other researchers have explored
distributional clustering of similar words (Pereira et
al., 1993; Lin, 1998), but it is unclear to what extent
such techniques produce paraphrases.1
Most relevant to this paper is the work of Barzi-
lay and McKeown and the work of Lin and Pan-
tel. Barzilay and McKeown (2001) extracted
both single- and multiple-word paraphrases from a
sentence-aligned corpus for use in multi-document
summarization. They constructed an aligned corpus
from multiple translations of foreign novels. From
this, they co-trained a classifier that decided whether
or not two phrases were paraphrases of each other
based on their surrounding context. Barzilay and
McKeown collected 9483 paraphrases with an av-
erage precision of 85.5%. However, 70.8% of the
paraphrases were single words. In addition, the
paraphrases were required to be contiguous.
Lin and Pantel (2001) used a general text corpus
to extract what they called inference rules, which
we can take to be paraphrases. In their algorithm,
rules are represented as dependency tree paths be-
tween two words. The words at the ends of a path
are considered to be features of that path. For each
path, they recorded the different features (words)
that were associated with the path and their respec-
tive frequencies. Lin and Pantel calculated the sim-
ilarity of two paths by looking at the similarity of
their features. This method allowed them to extract
inference rules of moderate length from general cor-
pora. However, the technique is computationally
expensive, and furthermore can give misleading re-
sults, i.e., paths having the opposite meaning often
share similar features.
3 Approach
Our approach, like Barzilay and McKeown?s, is built
on the application of sentence-alignment techniques
used in machine translation to generate paraphrases.
The insight is simple: if we have pairs of sentences
with the same semantic content, then the difference
in lexical content can be attributed to variations in
the surface form. By generalizing these differences
we can automatically derive paraphrases. Barzilay
and McKeown perform this learning process by only
1For example, ?dog? and ?cat? are recognized to be similar,
but they are obviously not paraphrases of one another.
considering the local context of words and their fre-
quencies; as a result, paraphrases must be contigu-
ous, and in the majority of cases, are only one word
long. We believe that disregarding the rich syntactic
structure of language is an oversimplification, and
that structural paraphrases offer several distinct ad-
vantages over lexical paraphrases. Long distance re-
lations can be captured by syntactic trees, so that
words in the paraphrases do not need to be contigu-
ous. Use of syntactic trees also buffers against mor-
phological variants (e.g., different inflections) and
some syntactic variants (e.g., active vs. passive).
Finally, because paraphrases are context-dependent,
we believe that syntactic structures can encapsulate
a richer context than lexical phrases.
Based on aligned monolingual corpora, our tech-
nique for extracting paraphrases builds on Lin and
Pantel?s insight of using dependency paths (derived
from parsing) as the fundamental unit of learning
and using parts of those paths as features. Based
on the hypothesis that paths between identical words
in aligned sentences are semantically equivalent, we
can extract paraphrases by scoring the path fre-
quency and context. Our approach addresses the
limitations of both Barzilay and McKeown?s and
Lin and Pantel?s work: using syntactic structures al-
lows us to generate structural paraphrases, and using
aligned corpora renders the process more computa-
tionally tractable. The following sections describe
our approach in greater detail.
3.1 Corpus Alignment
Multiple English translations of foreign novels, e.g.,
Twenty Thousand Leagues Under the Sea by Jules
Verne, were used for extraction of paraphrases.
Although translations by different authors differ
slightly in their literary interpretation of the origi-
nal text, it was usually possible to find correspond-
ing sentences that have the same semantic content.
Sentence alignment was performed using the Gale
and Church algorithm (1991) with the following cost
function:
cost of substitution = 1? ncw
anw
ncw: number of common words
anw: average number of words in two strings
Here is a sample from two different translations
of Twenty Thousand Leagues Under the Sea:
Ned Land tried the soil with his feet, as if to take
possession of it.
Ned Land tested the soil with his foot, as if he were
laying claim to it.
To test the accuracy of our alignment, we man-
ually aligned 454 sentences from two different ver-
sions of Chapter 21 from Twenty Thousand Leagues
Under the Sea and compared the results of our au-
tomatic alignment algorithm against the manually
generated ?gold standard.? We obtained a precision
of 0.93 and recall of 0.88, which is comparable to
the numbers (P.94/R.85) reported by Barzilay and
McKeown, who used a different cost function for the
alignment process.
3.2 Parsing and Postprocessing
The sentence pairs produced by the alignment al-
gorithm are then parsed by the Link Parser (Sleator
and Temperly, 1993), a dependency-based parser de-
veloped at CMU. The resulting parse structures are
post-processed to render the links more consistent:
Because the Link Parser does not directly identify
the subject of a passive sentence, our postprocessor
takes the object of the by-phrase as the subject by
default. For our purposes, auxiliary verbs are ig-
nored; the postprocessor connects verbs directly to
their subjects, discarding links through any auxiliary
verbs. In addition, subjects and objects within rela-
tive clauses are appropriately modified so that the
linkages remained consistent with subject and object
linkages in the matrix clause. For sentences involv-
ing verbs that have particles, the Link Parser con-
nects the object of the verb directly to the verb it-
self, attaching the particle separately. Our postpro-
cessor modifies the link structure so that the object
is connected to the particle in order to form a contin-
uous path. Predicate adjectives are converted into an
adjective-noun modification link instead of a com-
plete verb-argument structure. Also, common nouns
denoting places and people are marked by consult-
ing WordNet.
3.3 Paraphrase Extraction
The paraphrase extraction process starts by finding
anchors within the aligned sentence pairs. In our ap-
proach, only nouns and pronouns serve as possible
anchors. The anchor words from the sentence pairs
are brought into alignment and scored by a simple
set of ordered heuristics:
? Exact string matches denote correspondence.
? Noun and matching pronoun (same gender and
number) denote correspondence. Such a match
penalizes the score by 50%.
? Unique semantic class (e.g., places and people)
denotes correspondence. Such a match penal-
izes the score by 50%.
? Unique part of speech (i.e., the only noun
pair in the sentences) denotes correspondence.
Such a match penalizes the score by 50%.
? Otherwise, attempt to find correspondence by
finding longest common substrings. Such a
match penalizes the score by 50%.
? If a word occurs more than once in the aligned
sentence pairs, all possible combinations are
considered, but the score for such a correspond-
ing anchor pair is further penalized by 50%.
For each pair of anchors, a breadth-first search is
used to find the shortest path between the anchor
words. The search algorithm explicitly rejects paths
that contain conjunctions and punctuation. If valid
paths are found between anchor pairs in both of the
aligned sentences, the resulting paths are considered
candidate paraphrases, with a default score of one
(subjected to penalties imposed by imperfect anchor
matching).
Scores of candidate paraphrases take into account
two factors: the frequency of anchors with respect
to a particular candidate paraphrase and the variety
of different anchors from which the paraphrase was
produced. The initial default score of any paraphrase
is one (assuming perfect anchor matches), but for
each additional occurrence the score is incremented
by 1
2
n
, where n is the number of times the current
set of anchors has been seen. Therefore, the effect of
seeing new sets of anchors has a big initial impact on
the score, but the additional increase in score is sub-
jected to diminishing returns as more occurrences of
the same anchor are encountered.
count
aligned sentences 27479
parsed aligned sentences 25292
anchor pairs 43974
paraphrases 5925
unique paraphrases 5502
gathered paraphrases (score ? 1.0) 2886
Table 1: Summary of the paraphrase generation pro-
cess
Figure 1: Distribution of paraphrase length
4 Results
Using the approach described in previous sections,
we were able to extract nearly six thousand different
paraphrases (see Table 1) from our corpus, which
consisted of two translations of 20,000 Leagues Un-
der the Sea, two translations of The Kreutzer Sonata,
and three translations of Madame Bouvary.
Our corpus was essentially the same as the one
used by Barzilay and McKeown, with the exception
of some short fairy tale translations that we found to
be unsuitable. Due to the length of sentences (some
translations were noted for their paragraph-length
sentences), the Link Parser was unable to produce
a parse for approximately eight percent of the sen-
tences. Although the Link Parser is capable of pro-
ducing partial linkages, accuracy deteriorated signif-
icantly as the length of the input string increased.
The distribution of paraphrase length is shown in
Figure 1. The length of paraphrases is measured by
the number of words that it contains (discounting the
anchors on both sides).
To evaluate the accuracy of our results, 130
Evaluator Precision
Evaluator 1 36.2%
Evaluator 2 40.0%
Evaluator 3 44.6%
Average 41.2%
Table 2: Summary of judgments by human evalua-
tors for 130 unique paraphrases
unique paraphrases were randomly chosen to be
assessed by human judges. The human assessors
were specifically asked whether they thought the
paraphrases were roughly interchangeable with each
other, given the context of the genre. We believe that
the genre constraint was important because some
paraphrases captured literary or archaic uses of par-
ticular words that were not generally useful. This
should not be viewed as a shortcoming of our ap-
proach, but rather an artifact of our corpus. In ad-
dition, sample sentences containing the structural
paraphrases were presented as context to the judges;
structural paraphrases are difficult to comprehend
without this information.
A summary of the judgments provided by human
evaluators is shown in Table 2. The average preci-
sion of our approach stands at just over forty per-
cent; the average length of the paraphrases learned
was 3.26 words long. Our results also show that
judging structural paraphrases is a difficult task and
inter-assessor agreement is rather low. All of the
evaluators agreed on the judgments (either positive
or negative) only 75.4% of the time. The average
correlation constant of the judgments is only 0.66.
The highest scoring paraphrase was the equiva-
lence of the possessive morpheme ?s with the prepo-
sition of. We found it encouraging that our algo-
rithm was able to induce this structural paraphrase,
complete with co-indexed anchors on the ends of the
paths, i.e., A?s B ?? B of A. Some other interest-
ing examples include:2
A
1
?
?? liked O?? A
2
??
A
1
?
?? fond OF?? of J?? A
2
Example: The clerk liked Monsieur Bovary. ??
2Brief description of link labels: S: subject to verb; O: object
to verb; OF: certain verbs to of; K: verbs to particles; MV: verbs
to certain modifying phrases. See Link Parser documentation
for full descriptions.
Score Threshold Avg. Precision Avg. Length Count
? 1.0 40.2% 3.24 130
? 1.25 46.0% 2.88 58
? 1.5 47.8% 2.22 23
? 1.75 38.9% 1.67 12
Table 3: Breakdown of our evaluation results
The clerk was fond of Monsieur Bovary.
A
1
s
?? rush K?? over MV?? to J?? A
2
??
A
1
s
?? run
MV
?? to
J
?? A
2
Example: And he rushed over to his son, who had
just jumped into a heap of lime to whiten his shoes.
?? And he ran to his son, who had just precipi-
tated himself into a heap of lime in order to whiten
his boots.
A
1
s
?? put K?? on O?? A
2
??
A
1
s
?? wear
O
?? A
2
Example: That is why he puts on his best waistcoat
and risks spoiling it in the rain. ?? That?s why he
wears his new waistcoat, even in the rain!
A
1
?
?? fit MV?? to I?? give O?? A
2
??
A
1
?
?? appropriate MV?? to I?? supply O?? A
2
Example: He thought fit, after the first few mouth-
fuls, to give some details as to the catastrophe. ??
After the first few mouthfuls he considered it appro-
priate to supply a few details concerning the catas-
trophe.
A more detailed breakdown of the evaluation re-
sults can be seen in Table 3. Increasing the thresh-
old for generating paraphrases tends to increase their
precision, up to a certain point. In general, the high-
est ranking structural paraphrases consisted of sin-
gle word paraphrases of prepositions, e.g., at ??
in. Our algorithm noticed that different prepositions
were often interchangeable, which is something that
our human assessors disagreed widely on. Beyond a
certain threshold, the accuracy of our approach ac-
tually decreases.
5 Discussion
An obvious first observation about our algorithm is
the dependence on parse quality; bad parses lead to
many bogus paraphrases. Although the parse results
from the Link Parser are far from perfect, it is un-
clear whether other purely statistical parsers would
fare any better, since they are generally trained on
corpora containing a totally different genre of text.
However, future work will most likely include a
comparison of different parsers.
Examination of our results show that a better no-
tion of constituency would increase the accuracy
of our results. Our algorithm occasionally gener-
ates non-sensical paraphrases that cross constituent
boundaries, for example, including the verb of a
subordinate clause with elements from the matrix
clause. Other problems arise because our current al-
gorithm has no notion of verb phrases; it often gen-
erates near misses such as fail?? succeed, neglect-
ing to include not as part of the paraphrase.
However, there are problems inherent in para-
phrase generation that simple knowledge of con-
stituency alone cannot solve. Consider the following
two sentences:
John made out gold at the bottom of the well.
John discovered gold near the bottom of the well.
Which structural paraphrases should we be able to
extract?
made out X at Y?? discovered X near Y
made out X?? discovered X
at X?? near X
Arguably, all three paraphrases are valid, although
opinions vary more regarding the last paraphrase.
What is the optimal level of structure for para-
phrases? Obviously, this represents a tradeoff be-
tween specificity and accuracy, but the ability of
structural paraphrases to capture long-distance re-
lationships across large numbers of lexical items
complicates the problem. Due to the sparseness of
our data, our algorithm cannot make a good deci-
sion on what constituents to generalize as variables;
naturally, greater amounts of data would alleviate
this problem. This current inability to decide on a
good ?scope? for paraphrasing was a primary rea-
son why we were unable to perform a strict eval-
uation of recall. Our initial attempts at generating
a gold standard for estimating recall failed because
human judges could not agree on the boundaries of
paraphrases.
The accuracy of our structural paraphrases is
highly dependent on the corpus size. As can be seen
from the numbers in Table 1, paraphrases are rather
sparse?nearly 93% of them are unique. Without
adequate statistical evidence, validating candidate
paraphrases can be very difficult. Although our data
spareness problem can be alleviated simply by gath-
ering a larger corpus, the type of parallel text our
algorithm requires is rather hard to obtain, i.e., there
are only so many translations of so many foreign
novels. Furthermore, since our paraphrases are ar-
guably genre-specific, different applications may re-
quire different training corpora. Similar to the work
of Barzilay and Lee (2003), who have applied para-
phrase generation techniques to comparable corpora
consisting of different newspaper articles about the
same event, we are currently attempting to solve the
data sparseness problem by extending our approach
to non-parallel corpora.
We believe that generating paraphrases at the
structural level holds several key advantages over
lexical paraphrases, from the capturing of long-
distance relationships to the more accurate modeling
of context. The paraphrases generated by our ap-
proach could prove to be useful in any natural lan-
guage application where understanding of linguis-
tic variations is important. In particular, we are at-
tempting to apply our results to improve the perfor-
mance of question answering system, which we will
describe in the following section.
6 Paraphrases and Question Answering
The ultimate goal of our work on paraphrases is
to enable the development of high-precision ques-
tion answering system (cf. (Katz and Levin, 1988;
Soubbotin and Soubbotin, 2001; Hermjakob et al,
2002)). We believe that a knowledge base of para-
phrases is the key to overcoming challenges pre-
sented by the expressiveness of natural languages.
Because the same semantic content can be expressed
in many different ways, a question answering sys-
tem must be able to cope with a variety of alternative
phrasings. In particular, an answer stated in a form
that differs from the form of the question presents
significant problems:
When did Colorado become a state?
(1a) Colorado became a state in 1876.
(1b) Colorado was admitted to the Union in 1876.
Who killed Abraham Lincoln?
(2a) John Wilkes Booth killed Abraham Lincoln.
(2b) John Wilkes Booth ended Abraham Lincoln?s
life with a bullet.
In the above examples, question answering sys-
tems have little difficulty extracting answers if the
answers are stated in a form directly derived from
the question, e.g., (1a) and (2a); simple keyword
matching techniques with primitive named-entity
detection technology will suffice. However, ques-
tion answering systems will have a much harder time
extracting answers from sentences where they are
not obviously stated, e.g., (1b) and (2b). To re-
late question to answers in those examples, a system
would need access to rules like the following:
X became a state in Y??
X was admitted to the Union in Y
X killed Y?? X ended Y?s life
We believe that such rules are best formulated at
the syntactic level: structural paraphrases represent
a good level of generality and provide much more
accurate results than keyword-based approaches.
The simplest approach to overcoming the ?para-
phrase problem? in question answering is via key-
word query expansion when searching for candidate
answers:
(AND X became state)??
(AND X admitted Union)
(AND X killed)??
(AND X ended life)
The major drawback of such techniques is over-
generation of bogus answer candidates. For ex-
ample, it is a well-known result that query expan-
sion based on synonymy, hyponymy, etc. may actu-
ally degrade performance if done in an uncontrolled
manner (Voorhees, 1994). Typically, keyword-
based query expansion techniques sacrifice signifi-
cant amounts of precision for little (if any) increase
in recall.
The problems associated with keyword query ex-
pansion techniques stem from the fundamental de-
ficiencies of ?bag-of-words? approaches; in short,
they simply cannot accurately model the semantic
content of text, as illustrated by the following pairs
of sentences and phrases that have the same word
content, but dramatically different meaning:
(3a) The bird ate the snake.
(3b) The snake ate the bird.
(4a) the largest planet?s volcanoes
(4b) the planet?s largest volcanoes
(5a) the house by the river
(5b) the river by the house
(6a) The Germans defeated the French.
(6b) The Germans were defeated by the French.
The above examples are nearly indistinguishable
in terms of lexical content, yet their meanings are
vastly different. Naturally, because one text frag-
ment might be an appropriate answer to a question
while the other fragment may not be, a question an-
swering system seeking to achieve high precision
must provide mechanisms for differentiating the se-
mantic content of the pairs.
While paraphrase techniques at the keyword-level
vastly overgenerate, paraphrase techniques at the
phrase-level undergenerate, that is, they are often
too specific. Although paraphrase rules can eas-
ily be formulated at the string-level, e.g., using
regular expression matching and substitution tech-
niques (Soubbotin and Soubbotin, 2001; Hermjakob
et al, 2002), such a treatment fails to capture im-
portant linguistic generalizations. For example, the
addition of an adverb typically does not alter the va-
lidity of a paraphrase; thus, a phrase-level rule ?X
killed Y? ?? ?X ended Y?s life? would not be able
to match an answer like ?John Wilkes Booth sud-
denly ended Abraham Lincoln?s life with a bullet?.
String-level paraphrases are also unable to handle
syntactic phenomenona like passivization, which are
easily captured at the syntactic level.
We believe that answering questions at level of
syntactic relations, that is, matching parsed rep-
resentations of questions with parsed representa-
tions of candidates, addresses the issues presented
above. Syntactic relations, basically simplified ver-
sions of dependency structures derived from the
Link Parser, can capture significant portions of the
meaning present in text documents, while providing
a flexible foundation on which to build machinery
for paraphrases.
Our position is that question answering should be
performed at the level of ?key relations? in addition
to keywords. We have begun to experiment with re-
lations indexing and matching techniques described
above using an electronic encyclopedia as the test
corpus. We identified a particular set of linguistic
phenomena where relation-based indexing can dra-
matically boost the precision of a question answer-
ing system (Katz and Lin, 2003). As an example,
consider a sample output from a baseline keyword-
based IR system:
What do frogs eat?
(R1) Alligators eat many kinds of small animals
that live in or near the water, including fish, snakes,
frogs, turtles, small mammals, and birds.
(R2) Some bats catch fish with their claws, and a
few species eat lizards, rodents, birds, and frogs.
(R3) Bowfins eat mainly other fish, frogs, and cray-
fish.
(R4) Adult frogs eat mainly insects and other small
animals, including earthworms, minnows, and spi-
ders.
. . .
(R32) Kookaburras eat caterpillars, fish, frogs, in-
sects, small mammals, snakes, worms, and even
small birds.
Of the 32 sentences returned, only (R4) correctly
answers the user query; the other results answer a
different question??What eats frogs?? A bag-of-
words approach fundamentally cannot differentiate
between a query in which the frog is in the subject
position and a query in which the frog is in the object
position. Compare this to the results produced by
our relations matcher:
What do frogs eat?
(R4) Adult frogs eat mainly insects and other small
animals, including earthworms, minnows, and spi-
ders.
By examining subject-verb-object relations, our
system can filter out irrelevant results and return
only the correct responses.
We are currently working on combining this
relations-indexing technology with the automatic
paraphrase generation technology described earlier.
For example, our approach would be capable of au-
tomatically learning a paraphrase like X eat Y?? Y
is a prey of X; a large collection of such paraphrases
would go a long way in overcoming the brittleness
associated with a relations-based indexing scheme.
7 Contributions
We have presented a method for automatically learn-
ing structural paraphrases from aligned monolingual
corpora that overcomes the limitation of previous
approaches. In addition, we have sketched how this
technology can be applied to enhance the perfor-
mance of a question answering system based on in-
dexing relations. Although we have not completed
a task-based evaluation, we believe that the ability
to handle variations in language is key to building
better question answering systems.
8 Acknowledgements
This research was funded by DARPA under contract
number F30602-00-1-0545 and administered by the
Air Force Research Laboratory.
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL-2001).
Mark Dras. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text. Ph.D. thesis, Macquarie
University, Australia.
William A. Gale and Kenneth Ward Church. 1991. A
program for aligning sentences in bilingual corpora.
In Proceedings of the 29th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-1991).
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource andWeb exploitation for question answering.
In Proceedings of the Eleventh Text REtrieval Confer-
ence (TREC 2002).
Christian Jacquemin, Judith L. Klavans, and Evelyne
Tzoukermann. 1997. Expansion of multi-word terms
for indexing and retrieval using morphology and syn-
tax. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics (ACL-
1997).
Boris Katz and Beth Levin. 1988. Exploiting lexical
regularities in designing natural language systems. In
Proceedings of the 12th International Conference on
Computational Linguistics (COLING-1988).
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering. In
Proceedings of the EACL-2003 Workshop on Natural
Language Processing for Question Answering.
Maria Lapata. 2001. A corpus-based account of reg-
ular polysemy: The case of context-sensitive adjec-
tives. In Proceedings of the Second Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-2001).
Dekang Lin and Patrick Pantel. 2001. DIRT?discovery
of inference rules from text. In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Dekang Lin. 1998. Extracting collocations from text cor-
pora. In Proceedings of the First Workshop on Com-
putational Terminology.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 30th AnnualMeeting of the Association
for Computational Linguistics (ACL-1991).
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the Third
International Workshop on Parsing Technology.
Martin M. Soubbotin and Sergei M. Soubbotin. 2001.
Patterns of potential answer expressions as clues to the
right answers. In Proceedings of the Tenth Text RE-
trieval Conference (TREC 2001).
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the
17th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR-1994).
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 37?44, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Using Syntactic Information to Identify Plagiarism
O?zlem Uzuner, Boris Katz, and Thade Nahnsen
Massachusetts Institute of Technology
Computer Science and Articial Intelligence Laboratory
Cambridge, MA 02139
ozlem,boris,tnahnsen@csail.mit.edu
Abstract
Using keyword overlaps to identify pla-
giarism can result in many false negatives
and positives: substitution of synonyms
for each other reduces the similarity be-
tween works, making it difficult to rec-
ognize plagiarism; overlap in ambiguous
keywords can falsely inflate the similar-
ity of works that are in fact different in
content. Plagiarism detection based on
verbatim similarity of works can be ren-
dered ineffective when works are para-
phrased even in superficial and immate-
rial ways. Considering linguistic informa-
tion related to creative aspects of writing
can improve identification of plagiarism
by adding a crucial dimension to evalu-
ation of similarity: documents that share
linguistic elements in addition to content
are more likely to be copied from each
other. In this paper, we present a set of
low-level syntactic structures that capture
creative aspects of writing and show that
information about linguistic similarities
of works improves recognition of plagia-
rism (over tfidf-weighted keywords alone)
when combined with similarity measure-
ments based on tfidf-weighted keywords.
1 Introduction
To plagiarize is ?to steal and pass off (the ideas
or words of another) as one?s own; [to] use (an-
other?s production) without crediting the source; [or]
to commit literary theft [by] presenting as new and
original an idea or product derived from an exist-
ing source?.1 Plagiarism is frequently encountered
in academic settings. According to turnitin.com, a
2001 survey of 4500 high school students revealed
that ?15% [of students] had submitted a paper ob-
tained in large part from a term paper mill or web-
site?. Increased rate of plagiarism hurts quality of
education received by students; facilitating recog-
nition of plagiarism can help teachers control this
damage.
To facilitate recognition of plagiarism, in the re-
cent years many commercial and academic prod-
ucts have been developed. Most of these approaches
identify verbatim plagiarism2 and can fail when
works are paraphrased. To recognize plagiarism
in paraphrased works, we need to capture similar-
ities that go beyond keywords and verbatim over-
laps. Two works that exhibit similarity both in their
conceptual content (as indicated by keywords) and
in their expression of this content should be consid-
ered more similar than two works that are similar
only in content. In this context, content refers to
the story or the information; expression refers to the
linguistic choices of authors used in presenting the
content, i.e., creative elements of writing, such as
whether authors tend toward passive or active voice,
whether they prefer complex sentences with embed-
ded clauses or simple sentences with independent
clauses, as well as combinations of such choices.
Linguistic information can be a source of power
for measuring similarity between works based on
1www.webster.com
2www.turnitin.com
37
their expression of content. In this paper, we use lin-
guistic information related to the creative aspects of
writing to improve recognition of paraphrased doc-
uments as a first step towards plagiarism detection.
To identify a set of features that relate to the linguis-
tic choices of authors, we rely on different syntactic
expressions of the same content. After identifying
the relevant features (which we call syntactic ele-
ments of expression), we rely on patterns in the use
of these features to recognize paraphrases of works.
In the absence of real-life plagiarism data, in this
paper, we use a corpus of parallel translations of
novels as surrogate for plagiarism data. Transla-
tions of titles, i.e., original works, into English by
different people provide us with books that are para-
phrases of the same content. We use these para-
phrases to automatically identify:
1. Titles even when they are paraphrased, and
2. Pairs of book chapters that are paraphrases of
each other.
Our first experiment shows that syntactic elements
of expression outperform all baselines in recogniz-
ing titles even when they are paraphrased, provid-
ing a way of recognizing copies of works based on
the similarities in their expression of content. Our
second experiment shows that similarity measure-
ments based on the combination of tfidf-weighted
keywords and syntactic elements of expression out-
perform the weighted keywords in recognizing pairs
of book chapters that are paraphrases of each other.
2 Related Work
We define expression as ?the linguistic choices of
authors in presenting a particular content? (Uzuner,
2005; Uzuner and Katz, 2005). Linguistic similarity
between works has been studied in the text classifi-
cation literature for identifying the style of an author.
However, it is important to differentiate expression
from style. Style refers to the linguistic elements
that, independently of content, persist over the works
of an author and has been widely studied in author-
ship attribution. Expression involves the linguistic
elements that relate to how an author phrases par-
ticular content and can be used to identify potential
copyright infringement or plagiarism. Similarities
in the expression of similar content in two differ-
ent works signal potential copying. We hypothesize
that syntax plays a role in capturing expression of
content. Our approach to recognizing paraphrased
works is based on phrase structure of sentences in
general, and structure of verb phrases in particular.
Most approaches to similarity detection use com-
putationally cheap but linguistically less informed
features (Peng and Hengartner, 2002; Sichel, 1974;
Williams, 1975) such as keywords, function words,
word lengths, and sentence lengths; approaches that
include deeper linguistic information, such as syn-
tactic information, usually incur significant compu-
tational costs (Uzuner et al, 2004). Our approach
identifies useful linguistic information without in-
curring the computational cost of full text pars-
ing; it uses context-free grammars to perform high-
level syntactic analysis of part-of-speech tagged
text (Brill, 1992). It turns out that such a level of
analysis is sufficient to capture syntactic informa-
tion related to creative aspects of writing; this in
turn helps improve recognition of paraphrased doc-
uments. The results presented here show that ex-
traction of useful linguistic information for text clas-
sification purposes does not have to be computa-
tionally prohibitively expensive, and that despite the
tradeoff between the accuracy of features and com-
putational efficiency, we can extract linguistically-
informed features without full parsing.
3 Identifying Creative Aspects of Writing
In this paper, we first identify linguistic elements
of expression and then study patterns in the use of
these elements to recognize a work even when it is
paraphrased. Translated literary works provide ex-
amples of linguistic elements that differ in expres-
sion but convey similar content. These works pro-
vide insight into the linguistic elements that capture
expression. For example, consider the following se-
mantically equivalent excerpts from three different
translations of Madame Bovary by Gustave Flaubert.
Excerpt 1: ?Now Emma would often take it into
her head to write him during the day. Through her
window she would signal to Justin, and he would
whip off his apron and fly to la huchette. And when
Rodolphe arrived in response to her summons, it
was to hear that she was miserable, that her husband
was odious, that her life was a torment.? (Trans-
lated by Unknown1.)
38
Excerpt 2: ?Often, even in the middle of the day,
Emma suddenly wrote to him, then from the win-
dow made a sign to Justin, who, taking his apron
off, quickly ran to la huchette. Rodolphe would
come; she had sent for him to tell him that she was
bored, that her husband was odious, her life fright-
ful.? (Translated by Aveling.)
Excerpt 3: ?Often, in the middle of the day, Emma
would take up a pen and write to him. Then she
would beckon across to Justin, who would off with
his apron in an instant and fly away with the letter
to la huchette. And Rodolphe would come. She
wanted to tell him that life was a burden to her, that
she could not endure her husband and that things
were unbearable.? (Translated by Unknown2.)
Inspired by syntactic differences displayed in
such parallel translations, we identified a novel set
of syntactic features that relate to how people con-
vey content.
3.1 Syntactic Elements of Expression
We hypothesize that given particular content, au-
thors choose from a set of semantically equivalent
syntactic constructs to express this content. To para-
phrase a work without changing content, people
try to interchange semantically equivalent syntactic
constructs; patterns in the use of various syntactic
constructs can be sufficient to indicate copying.
Our observations of the particular expressive
choices of authors in a corpus of parallel translations
led us to define syntactic elements of expression in
terms of sentence-initial and -final phrase structures,
semantic classes and argument structures of verb
phrases, and syntactic classes of verb phrases.
3.1.1 Sentence-initial and -final phrase
structures
The order of phrases in a sentence can shift the
emphasis of a sentence, can attract attention to par-
ticular pieces of information and can be used as an
expressive tool.
1 (a) Martha can finally put some money in the bank.
(b) Martha can put some money in the bank, finally.
(c) Finally, Martha can put some money in the bank.
2 (a) Martha put some money in the bank on Friday.
(b) On Friday, Martha put some money in the bank.
(c) Some money is what Martha put in the bank on Fri-
day.
(d) In the bank is where Martha put some money on
Friday.
The result of such expressive changes affect the
distributions of various phrase types in sentence-
initial and -final positions; studying these distribu-
tions can help us capture some elements of expres-
sion. Despite its inability to detect the structural
changes that do not affect the sentence-initial and
-final phrase types, this approach captures some of
the phrase-level expressive differences between se-
mantically equivalent content; it also captures dif-
ferent sentential structures, including question con-
structs, imperatives, and coordinating and subordi-
nating conjuncts.
3.1.2 Semantic Classes of Verbs
Levin (1993) observed that verbs that exhibit sim-
ilar syntactic behavior are also related semantically.
Based on this observation, she sorted 3024 verbs
into 49 high-level semantic classes. Verbs of ?send-
ing and carrying?, such as convey, deliver,
move, roll, bring, carry, shuttle, and
wire, for example, are collected under this seman-
tic class and can be further broken down into five
semantically coherent lower-level classes which in-
clude ?drive verbs?, ?carry verbs?, ?bring and take
verbs?, ?slide verbs?, and ?send verbs?. Each of
these lower-level classes represents a group of verbs
that have similarities both in semantics and in syn-
tactic behavior, i.e., they can grammatically un-
dergo similar syntactic alternations. For example,
?send verbs? can be seen in the following alterna-
tions (Levin, 1993):
1. Base Form
? Nora sent the book to Peter.
? NP + V + NP + PP.
2. Dative Alternation
? Nora sent Peter the book.
? NP + V + NP + NP.
Semantics of verbs in general, and Levin?s verb
classes in particular, have previously been used for
evaluating content and genre similarity (Hatzivas-
siloglou et al, 1999). In addition, similar seman-
tic classes of verbs were used in natural language
processing applications: START was the first nat-
ural language question answering system to use
such verb classes (Katz and Levin, 1988). We use
39
Levin?s semantic verb classes to describe the ex-
pression of an author in a particular work. We as-
sume that semantically similar verbs are often used
in semantically similar syntactic alternations; we
describe part of an author?s expression in a par-
ticular work in terms of the semantic classes of
verbs she uses and the particular argument struc-
tures, e.g., NP + V + NP + PP, she prefers for them.
As many verbs belong to multiple semantic classes,
to capture the dominant semantic verb classes in
each document we credit all semantic classes of all
observed verbs. We extract the argument structures
from part of speech tagged text, using context-free
grammars (Uzuner, 2005).
3.1.3 Syntactic Classes of Verbs
Levin?s verb classes include exclusively ?non-
embedding verbs?, i.e., verbs that do not take
clausal arguments, and need to be supplemented by
classes of ?embedding verbs? that do take such argu-
ments. Alexander and Kunz (1964) identified syn-
tactic classes of embedding verbs, collected a com-
prehensive set of verbs for each class, and described
the identified verb classes with formulae written in
terms of phrasal and clausal elements, such as verb
phrase heads (Vh), participial phrases (Partcp.), in-
finitive phrases (Inf.), indicative clauses (IS), and
subjunctives (Subjunct.). We used 29 of the more
frequent embedding verb classes and identified their
distributions in different works. Examples of these
verb classes are shown in Table 1. Further examples
can be found in (Uzuner, 2005; Uzuner and Katz,
2005).
Syntactic Formula Example
NP + Vh + NP + from The belt kept him from dying.
+ Partcp.
NP + Vh + that + IS He admitted that he was guilty.
NP + Vh + that I request that she go alone.
+ Subjunct.
NP + Vh + to + Inf. My father wanted to travel.
NP + Vh + wh + IS He asked if they were alone.
NP + pass. + Partcp. He was seen stealing.
Table 1: Sample syntactic formulae and examples of
embedding verb classes.
We study the syntax of embedding verbs by iden-
tifying their syntactic class and the structure of
their observed embedded arguments. After identi-
fying syntactic and semantic characteristics of verb
phrases, we combine these features to create fur-
ther elements of expression, e.g., syntactic classes
of embedding verbs and the classes of semantic non-
embedding verbs they co-occur with.
4 Evaluation
We tested sentence-initial and -final phrase struc-
tures, semantic and syntactic classes of verbs, and
structure of verb arguments, i.e., syntactic elements
of expression, in paraphrase recognition and in pla-
giarism detection in two ways:
? Recognizing titles even when they are para-
phrased, and
? Recognizing pairs of book chapters that are
paraphrases of each other.
For our experiments, we split books into chapters,
extracted all relevant features from each chapter, and
normalized them by the length of the chapter.
4.1 Recognizing Titles
Frequently, people paraphrase parts of rather than
complete works. For example, they may paraphrase
chapters or paragraphs from a work rather than the
whole work. We tested the effectiveness of our
features on recognizing paraphrased components of
works by focusing on chapter-level excerpts (smaller
components than chapters have very sparse vectors
given our sentence-level features and will be the
foci of future research) and using boosted decision
trees (Witten and Frank, 2000).
Our goal was to recognize chapters from the ti-
tles in our corpus even when some titles were para-
phrased into multiple books; in this context, titles
are original works and paraphrased books are trans-
lations of these titles. For this, we assumed the ex-
istence of one legitimate book from each title. We
used this book to train a model that captured the syn-
tactic elements of expression used in this title. We
used the remaining paraphrases of the title (i.e., the
remaining books paraphrasing the title) as the test
set?these paraphrases are considered to be plagia-
rized copies and should be identified as such given
the model for the title.
40
4.1.1 Data
Real life plagiarism data is difficult to obtain.
However, English translations of foreign titles ex-
ist and can be obtained relatively easily. Titles that
have been translated on different occasions by dif-
ferent translators and that have multiple translations
provide us with examples of books that paraphrase
the same content and serve as our surrogate for pla-
giarism data.
To evaluate syntactic elements of expression on
recognizing paraphrased chapters from titles, we
compared the performance of these features with
tfidf-weighted keywords on a 45-way classifica-
tion task. The corpus used for this experiment
included 49 books from 45 titles. Of the 45 ti-
tles, 3 were paraphrased into a total of 7 books
(3 books paraphrased the title Madame Bovary, 2
books paraphrased 20000 Leagues, and 2 books
paraphrased The Kreutzer Sonata). The remaining
titles included works from J. Austen (1775-1817),
C. Dickens (1812-1870), F. Dostoyevski (1821-
1881), A. Doyle (1859-1887), G. Eliot (1819-
1880), G. Flaubert (1821-1880), T. Hardy (1840-
1928), V. Hugo (1802-1885), W. Irving (1789-
1859), J. London (1876-1916), W. M. Thack-
eray (1811-1863), L. Tolstoy (1828-1910), I. Tur-
genev (1818-1883), M. Twain (1835-1910), and
J. Verne (1828-1905).
4.1.2 Baseline Features
The task described in this section focuses on rec-
ognizing paraphrases of works based on the way
they are written. Given the focus of authorship attri-
bution literature on ?the way people write?, to eval-
uate the syntactic elements of expression on recog-
nizing paraphrased chapters of a work, we compared
these features against features frequently used in au-
thorship attribution as well as features used in con-
tent recognition.
Tfidf-weighted Keywords: Keywords, i.e., con-
tent words, are frequently used in content-based text
classification and constitute one of our baselines.
Function Words: In studies of authorship at-
tribution, many researchers have taken advantage
of the differences in the way authors use function
words (Mosteller and Wallace, 1963; Peng and Hen-
gartner, 2002). In our studies, we used a set of 506
function words (Uzuner, 2005).
Distributions of Word Lengths and Sentence
Lengths: Distributions of word lengths and sen-
tence lengths have been used in the literature for
authorship attribution (Mendenhall, 1887; Williams,
1975; Holmes, 1994). We include these features
in our sets of baselines along with information
about means and standard deviations of sentence
lengths (Holmes, 1994).
Baseline Linguistic Features: Sets of surface,
syntactic, and semantic features have been found to
be useful for authorship attribution and have been
adopted here as baseline features. These features
included: the number of words and the number of
sentences in the document; type?token ratio; aver-
age and standard deviation of the lengths of words
(in characters) and of the lengths of sentences (in
words) in the document; frequencies of declara-
tive sentences, interrogatives, imperatives, and frag-
mental sentences; frequencies of active voice sen-
tences, be-passives and get-passives; frequencies of
?s-genitives, of-genitives and of phrases that lack
genitives; frequency of overt negations, e.g., ?not?,
?no?, etc.; and frequency of uncertainty markers,
e.g., ?could?, ?possibly?, etc.
4.1.3 Experiment
To recognize chapters from the titles in our corpus
even when some titles were paraphrased into mul-
tiple books, we randomly selected 40?50 chapters
from each title. We used 60% of the selected chap-
ters from each title for training and the remaining
40% for testing. For paraphrased titles, we selected
training chapters from one of the paraphrases and
testing chapters from the remaining paraphrases. We
repeated this experiment three times; at each round,
a different paraphrase was chosen for training and
the rest were used for testing.
Our results show that, on average, syntactic ele-
ments of expression accurately recognized compo-
nents of titles 73% of the time and significantly out-
performed all baselines3 (see middle column in Ta-
ble 2).4
3The tfidf-weighted keywords used in this experiment do not
include proper nouns. These words are unique to each title and
can be easily replaced without changing content or expression
in order to trick a plagiarism detection system that would rely
on proper nouns.
4For the corpora used in this paper, a difference of 4% or
more is statistically significant with ? = 0.05.
41
Feature Set Avg. Avg.
accuracy accuracy
(complete (para-
corpus) phrases)
only
Syntactic elements of expression 73% 95%
Function words 53% 34%
Tfidf-weighted keywords 47% 38%
Baseline linguistic 40% 67%
Dist. of word length 18% 54%
Dist. of sentence length 12% 17%
Table 2: Classification results (on the test set) for
recognizing titles in the corpus even when some ti-
tles are paraphrased (middle column) and classifi-
cation results only on the paraphrased titles (right
column). In either case, random chance would rec-
ognize a paraphrased title 2% of the time.
The right column in Table 2 shows that the syntac-
tic elements of expression accurately recognized on
average 95% of the chapters taken from paraphrased
titles. This finding implies that some of our elements
of expression are common to books that are derived
from the same title. This commonality could be due
to the similarity of their content or due to the under-
lying expression of the original author.
4.2 Recognizing Pairs of Paraphrased
Chapters
Experiments in Section 4.1 show that we can use
syntactic elements of expression to recognize titles
and their components based on the way they are
written even when some works are paraphrased. In
this section, our goal is to identify pairs of chapters
that paraphrase the same content, i.e., chapter 1 of
translation 1 of Madame Bovary and chapter 1 of
translation 2 of Madame Bovary. For this evalua-
tion, we used a similar approach to that presented by
Nahnsen et al (2005).
4.2.1 Data
Our data for this experiment included 47 chap-
ters from each of two translations of 20000 Leagues
under the Sea (Verne), 35 chapters from each of 3
translations of Madame Bovary (Flaubert), 28 chap-
ters from each of two translations of The Kreutzer
Sonata (Tolstoy), and 365 chapters from each of 2
translations of War and Peace (Tolstoy). Pairing
up the chapters from these titles provided us with
more than 1,000,000 chapter pairs, of which approx-
imately 1080 were paraphrases of each other.5
4.2.2 Experiment
For experiments on finding pairwise matches, we
used similarity of vectors of tfidf-weighted key-
words;6 and the multiplicative combination of the
similarity of vectors of tfidf-weighed keywords of
works with the similarity of vectors of syntactic ele-
ments of expression of these works. We used cosine
to evaluate the similarity of the vectors of works. We
omitted the remaining baseline features from this
experiment?they are features that are common to
majority of the chapters from each book, they do
not relate to the task of finding pairs of chapters that
could be paraphrases of each other.
We ranked all chapter pairs in the corpus based
on their similarity. From this ranked list, we iden-
tified the top n most similar pairs and predicted that
they are paraphrases of each other. We evaluated our
methods with precision, recall, and f-measure.7
Figure 1: Precision.
Figures 1, 2, and 3 show that syntactic elements
of expression improve the performance of tfidf-
weighted keywords in recognizing pairs of para-
phrased chapters significantly in terms of precision,
recall, and f-measure for all n; in all of these figures,
the blue line marked syn tdf represents the per-
formance of tfidf-weighted keywords enhanced with
5Note that this number double-counts the paraphrased pairs;
however, this fact is immaterial for our discussion.
6In this experiment, proper nouns are included in the
weighted keywords.
7The ground truth marks only the same chapter from two
different translations of the same title as similar, i.e., chapter x
of translation 1 of Madame Bovary and chapter y of translation
2 of Madame Bovary are similar only when x = y.
42
Figure 2: Recall.
syntactic elements of expression. More specifically,
the peak f-measure for tfidf-weighted keywords is
approximately 0.77 without contribution from syn-
tactic elements of expression. Adding information
about similarity of syntactic features to cosine sim-
ilarity of tfidf-weighted keywords boosts peak f-
measure value to approximately 0.82.8 Although
the f-measure of both representations degrade when
n > 1100, this degradation is an artifact of the eval-
uation metric: the corpus includes only 1080 similar
pairs, at n > 1100, recall is very close to 1, and
therefore increasing n hurts overall performance.
Figure 3: F-measure.
5 Conclusion
Plagiarism is a problem at all levels of education.
Increased availability of digital versions of works
makes it easier to plagiarize others? work and the
large volumes of information available on the web
makes it difficult to identify cases of plagiarism.
8The difference is statistically significant at ? = 0.05.
To identify plagiarism even when works are para-
phrased, we propose studying the use of particular
syntactic constructs as well as keywords in docu-
ments.
This paper shows that syntactic information can
help recognize works based on the way they are
written. Syntactic elements of expression that fo-
cus on the changes in the phrase structure of works
help identify paraphrased components of a title. The
same features help improve identification of pairs
of chapters that are paraphrases of each other, de-
spite the content these chapters share with the rest
of the chapters taken from the same title. The re-
sults presented in this paper are based on experi-
ments that use translated novels as surrogate for pla-
giarism data. Our future work will extend our study
to real life plagiarism data.
6 Acknowledgements
The authors would like to thank Sue Felshin for her
insightful comments. This work is supported in part
by the Advanced Research and Development Activ-
ity as part of the AQUAINT research program.
References
D. Alexander and W. J. Kunz. 1964. Some classes of
verbs in English. In Linguistics Research Project. In-
diana University, June.
E. Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the 3rd Conference on Applied
Natural Language Processing.
V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999. De-
tecting similarity by applying learning over indicators.
In Proceedings of the 37th Annual Meeting of the ACL.
D. I. Holmes. 1994. Authorship attribution. Computers
and the Humanities, 28.
B. Katz and B. Levin. 1988. Exploiting lexical reg-
ularities in designing natural language systems. In
Proceedings of the 12th Int?l Conference on Compu-
tational Linguistics (COLING ?88).
B. Levin. 1993. English Verb Classes and Alternations.
A Preliminary Investigation. University of Chicago
Press.
T. C. Mendenhall. 1887. Characteristic curves of com-
position. Science, 11.
43
F. Mosteller and D. L. Wallace. 1963. Inference in an au-
thorship problem. Journal of the American Statistical
Association, 58(302).
T. Nahnsen, O?. Uzuner, and B. Katz. 2005. Lexical
chains and sliding locality windows in content-based
text similarity detection. CSAIL Memo, AIM-2005-
017.
R. D. Peng and H. Hengartner. 2002. Quantitative analy-
sis of literary styles. The American Statistician, 56(3).
H. S. Sichel. 1974. On a distribution representing
sentence-length in written prose. Journal of the Royal
Statistical Society (A), 137.
O?. Uzuner and B. Katz. 2005. Capturing expression us-
ing linguistic information. In Proceedings of the 20th
National Conference on Artificial Intelligence (AAAI-
05).
O?. Uzuner, R. Davis, and B. Katz. 2004. Using em-
pirical methods for evaluating expression and content
similarity. In Proceedings of the 37th Hawaiian Inter-
national Conference on System Sciences (HICSS-37).
IEEE Computer Society.
O?. Uzuner. 2005. Identifying Expression Fingerprints
Using Linguistic Information. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
C. B. Williams. 1975. Mendenhall?s studies of word-
length distribution in the works of Shakespeare and
Bacon. Biometrika, 62(1).
I. H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools with Java Implementations.
Morgan Kaufmann, San Francisco.
44
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 21?29,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Reconstructing Native Language Typology from Foreign Language Usage
Yevgeni Berzak
CSAIL MIT
berzak@mit.edu
Roi Reichart
Technion IIT
roiri@ie.technion.ac.il
Boris Katz
CSAIL MIT
boris@mit.edu
Abstract
Linguists and psychologists have long
been studying cross-linguistic transfer, the
influence of native language properties on
linguistic performance in a foreign lan-
guage. In this work we provide empirical
evidence for this process in the form of a
strong correlation between language simi-
larities derived from structural features in
English as Second Language (ESL) texts
and equivalent similarities obtained from
the typological features of the native lan-
guages. We leverage this finding to re-
cover native language typological similar-
ity structure directly from ESL text, and
perform prediction of typological features
in an unsupervised fashion with respect to
the target languages. Our method achieves
72.2% accuracy on the typology predic-
tion task, a result that is highly competi-
tive with equivalent methods that rely on
typological resources.
1 Introduction
Cross-linguistic transfer can be broadly described
as the application of linguistic structure of a
speaker?s native language in the context of a
new, foreign language. Transfer effects may be
expressed on various levels of linguistic perfor-
mance, including pronunciation, word order, lex-
ical borrowing and others (Jarvis and Pavlenko,
2007). Such traces are prevalent in non-native
English, and in some cases are even cele-
brated in anecdotal hybrid dialect names such as
?Frenglish? and ?Denglish?.
Although cross-linguistic transfer was exten-
sively studied in Psychology, Second Language
Acquisition (SLA) and Linguistics, the conditions
under which it occurs, its linguistic characteristics
as well as its scope remain largely under debate
(Jarvis and Pavlenko, 2007; Gass and Selinker,
1992; Odlin, 1989).
In NLP, the topic of linguistic transfer was
mainly addressed in relation to the Native Lan-
guage Identification (NLI) task, which requires to
predict the native language of an ESL text?s au-
thor. The overall high performance on this classi-
fication task is considered to be a central piece of
evidence for the existence of cross-linguistic trans-
fer (Jarvis and Crossley, 2012). While the success
on the NLI task confirms the ability to extract na-
tive language signal from second language text, it
offers little insight into the linguistic mechanisms
that play a role in this process.
In this work, we examine the hypothesis that
cross-linguistic structure transfer is governed by
the typological properties of the native language.
We provide empirical evidence for this hypothe-
sis by showing that language similarities derived
from structural patterns of ESL usage are strongly
correlated with similarities obtained directly from
the typological features of the native languages.
This correlation has broad implications on the
ability to perform inference from native language
structure to second language performance and vice
versa. In particular, it paves the way for a novel
and powerful framework for comparing native
languages through second language performance.
This framework overcomes many of the inher-
ent difficulties of direct comparison between lan-
guages, and the lack of sufficient typological doc-
umentation for the vast majority of the world?s lan-
guages.
Further on, we utilize this transfer enabled
framework for the task of reconstructing typolog-
ical features. Automated prediction of language
typology is extremely valuable for both linguistic
studies and NLP applications which rely on such
information (Naseem et al., 2012; T?ackstr?om et
al., 2013). Furthermore, this task provides an ob-
jective external testbed for the quality of our native
21
language similarity estimates derived from ESL
texts.
Treating native language similarities obtained
from ESL as an approximation for typological
similarities, we use them to predict typological
features without relying on typological annotation
for the target languages. Our ESL based method
yields 71.4% ? 72.2% accuracy on the typology re-
construction task, as compared to 69.1% ? 74.2%
achieved by typology based methods which de-
pend on pre-existing typological resources for the
target languages.
To summarize, this paper offers two main con-
tributions. First, we provide an empirical result
that validates the systematic existence of linguistic
transfer, tying the typological characteristics of the
native language with the structural patterns of for-
eign language usage. Secondly, we show that ESL
based similarities can be directly used for predic-
tion of native language typology. As opposed to
previous approaches, our method achieves strong
results without access to any a-priori knowledge
about the target language typology.
The remainder of the paper is structured as fol-
lows. Section 2 surveys the literature and positions
our study in relation to previous research on cross-
linguistic transfer and language typology. Section
3 describes the ESL corpus and the database of
typological features. In section 4, we delineate
our method for deriving native language similar-
ities and hierarchical similarity trees from struc-
tural features in ESL. In section 5 we use typolog-
ical features to construct another set of language
similarity estimates and trees, which serve as a
benchmark for the typological validity of the ESL
based similarities. Section 6 provides a correla-
tion analysis between the ESL based and typology
based similarities. Finally, in section 7 we report
our results on typology reconstruction, a task that
also provides an evaluation framework for the sim-
ilarity structures derived in sections 4 and 5.
2 Related Work
Our work integrates two areas of research, cross-
linguistic transfer and linguistic typology.
2.1 Cross-linguistic Transfer
The study of cross-linguistic transfer has thus far
evolved in two complementary strands, the lin-
guistic comparative approach, and the computa-
tional detection based approach. While the com-
parative approach focuses on case study based
qualitative analysis of native language influence
on second language performance, the detection
based approach revolves mainly around the NLI
task.
Following the work of Koppel et al. (2005), NLI
has been gaining increasing interest in NLP, cul-
minating in a recent shared task with 29 partici-
pating systems (Tetreault et al., 2013). Much of
the NLI efforts thus far have been focused on ex-
ploring various feature sets for optimizing classifi-
cation performance. While many of these features
are linguistically motivated, some of the discrimi-
native power of these approaches stems from cul-
tural and domain artifacts. For example, our pre-
liminary experiments with a typical NLI feature
set, show that the strongest features for predicting
Chinese are strings such as China and in China.
Similar features dominate the weights of other lan-
guages as well. Such content features boost clas-
sification performance, but are hardly relevant for
modeling linguistic phenomena, thus weakening
the argument that NLI classification performance
is indicative of cross-linguistic transfer.
Our work incorporates an NLI component, but
departs from the performance optimization orien-
tation towards leveraging computational analysis
for better understanding of the relations between
native language typology and ESL usage. In par-
ticular, our choice of NLI features is driven by
their relevance to linguistic typology rather than
their contribution to classification performance. In
this sense, our work aims to take a first step to-
wards closing the gap between the detection and
comparative approaches to cross-linguistic trans-
fer.
2.2 Language Typology
The second area of research, language typology,
deals with the documentation and comparative
study of language structures (Song, 2011). Much
of the descriptive work in the field is summa-
rized in the World Atlas of Language Structures
(WALS)
1
(Dryer and Haspelmath, 2013) in the
form of structural features. We use the WALS fea-
tures as our source of typological information.
Several previous studies have used WALS fea-
tures for hierarchical clustering of languages and
typological feature prediction. Most notably, Teh
et al. (2007) and subsequently Daum?e III (2009)
1
http://wals.info/
22
predicted typological features from language trees
constructed with a Bayesian hierarchical cluster-
ing model. In Georgi et al. (2010) additional clus-
tering approaches were compared using the same
features and evaluation method. In addition to the
feature prediction task, these studies also evalu-
ated their clustering results by comparing them to
genetic language clusters.
Our approach differs from this line of work
in several aspects. First, similarly to our WALS
based baselines, the clustering methods presented
in these studies are affected by the sparsity of
available typological data. Furthermore, these
methods rely on existing typological documen-
tation for the target languages. Both issues are
obviated in our English based framework which
does not depend on any typological information
to construct the native language similarity struc-
tures, and does not require any knowledge about
the target languages except from the ESL essays of
a sample of their speakers. Finally, we do not com-
pare our clustering results to genetic groupings,
as to our knowledge, there is no firm theoretical
ground for expecting typologically based cluster-
ing to reproduce language phylogenies. The em-
pirical results in Georgi et al. (2010), which show
that typology based clustering differs substantially
from genetic groupings, support this assumption.
3 Datasets
3.1 Cambridge FCE
We use the Cambridge First Certificate in English
(FCE) dataset (Yannakoudakis et al., 2011) as our
source of ESL data. This corpus is a subset of
the Cambridge Learner Corpus (CLC)
2
. It con-
tains English essays written by upper-intermediate
level learners of English for the FCE examination.
The essay authors represent 16 native lan-
guages. We discarded Dutch and Swedish speak-
ers due to the small number of documents avail-
able for these languages (16 documents in total).
The remaining documents are associated with the
following 14 native languages: Catalan, Chinese,
French, German, Greek, Italian, Japanese, Korean,
Polish, Portuguese, Russian, Spanish, Thai and
Turkish. Overall, our corpus comprises 1228 doc-
uments, corresponding to an average of 87.7 doc-
uments per native language.
2
http://www.cambridge.org/gb/elt/
catalogue/subject/custom/item3646603
3.2 World Atlas of Language Structures
We collect typological information for the FCE
native languages from WALS. Currently, the
database contains information about 2,679 of
the world?s 7,105 documented living languages
(Lewis, 2014). The typological feature list has 188
features, 175 of which are present in our dataset.
The features are associated with 9 linguistic cat-
egories: Phonology, Morphology, Nominal Cate-
gories, Nominal Syntax, Verbal Categories, Word
Order, Simple Clauses, Complex Sentences and
Lexicon. Table 1 presents several examples for
WALS features and their range of values.
One of the challenging characteristics of WALS
is its low coverage, stemming from lack of avail-
able linguistic documentation. It was previously
estimated that about 84% of the language-feature
pairs in WALS are unknown (Daum?e III, 2009;
Georgi et al., 2010). Even well studied languages,
like the ones used in our work, are lacking values
for many features. For example, only 32 of the
WALS features have known values for all the 14
languages of the FCE corpus. Despite the preva-
lence of this issue, it is important to bear in mind
that some features do not apply to all languages by
definition. For instance, feature 81B Languages
with two Dominant Orders of Subject, Object, and
Verb is relevant only to 189 languages (and has
documented values for 67 of them).
We perform basic preprocessing, discarding 5
features that have values for only one language.
Further on, we omit 19 features belonging to the
category Phonology as comparable phonological
features are challenging to extract from the ESL
textual data. After this filtering, we remain with
151 features, 114.1 features with a known value
per language, 10.6 languages with a known value
per feature and 2.5 distinct values per feature.
Following previous work, we binarize all the
WALS features, expressing each feature in terms
of k binary features, where k is the number of
values the original feature can take. Note that
beyond the well known issues with feature bi-
narization, this strategy is not optimal for some
of the features. For example, the feature 111A
Non-periphrastic Causative Constructions whose
possible values are presented in table 1 would
have been better encoded with two binary features
rather than four. The question of optimal encoding
for the WALS feature set requires expert analysis
and will be addressed in future research.
23
ID Type Feature Name Values
26A Morphology Prefixing vs. Suffixing in Little affixation, Strongly suffixing, Weakly
Inflectional Morphology suffixing, Equal prefixing and suffixing,
Weakly prefixing, Strong prefixing.
30A Nominal Number of Genders None, Two, Three, Four, Five or more.
Categories
83A Word Order Order of Object and Verb OV, VO, No dominant order.
111A Simple Clauses Non-periphrastic Causative Neither, Morphological but no compound,
Constructions Compound but no morphological, Both.
Table 1: Examples of WALS features. As illustrated in the table examples, WALS features can take
different types of values and may be challenging to encode.
4 Inferring Language Similarities from
ESL
Our first goal is to derive a notion of similarity be-
tween languages with respect to their native speak-
ers? distinctive structural usage patterns of ESL. A
simple way to obtain such similarities is to train
a probabilistic NLI model on ESL texts, and in-
terpret the uncertainty of this classifier in distin-
guishing between a pair of native languages as a
measure of their similarity.
4.1 NLI Model
The log-linear NLI model is defined as follows:
p(y|x; ?) =
exp(? ? f(x, y))
?
y
?
?Y
exp(? ? f(x, y
?
))
(1)
where y is the native language, x is the observed
English document and ? are the model parame-
ters. The parameters are learned by maximizing
the L2 regularized log-likelihood of the training
data D = {(x
1
, y
1
), ..., (x
n
, y
n
)}.
L(?) =
n
?
i=1
log p(y
i
|x
i
; ?)? ????
2
(2)
The model is trained using gradient ascent with L-
BFGS-B (Byrd et al., 1995). We use 70% of the
FCE data for training and the remaining 30% for
development and testing.
As our objective is to relate native language and
target language structures, we seek to control for
biases related to the content of the essays. As pre-
viously mentioned, such biases may arise from the
essay prompts as well as from various cultural fac-
tors. We therefore define the model using only un-
lexicalized morpho-syntactic features, which cap-
ture structural properties of English usage.
Our feature set, summarized in table 2, contains
features which are strongly related to many of the
structural features in WALS. In particular, we use
features derived from labeled dependency parses.
These features encode properties such as the types
of dependency relations, ordering and distance be-
tween the head and the dependent. Additional
syntactic information is obtained using POS n-
grams. Finally, we consider derivational and in-
flectional morphological affixation. The annota-
tions required for our syntactic features are ob-
tained from the Stanford POS tagger (Toutanova
et al., 2003) and the Stanford parser (de Marneffe
et al., 2006). The morphological features are ex-
tracted heuristically.
4.2 ESL Based Native Language Similarity
Estimates
Given a document x and its author?s native lan-
guage y, the conditional probability p(y
?
|x; ?) can
be viewed as a measure of confusion between lan-
guages y and y
?
, arising from their similarity with
respect to the document features. Under this in-
terpretation, we derive a language similarity ma-
trix S
?
ESL
whose entries are obtained by averaging
these conditional probabilities on the training set
documents with the true label y, which we denote
as D
y
= {(x
i
, y) ? D}.
S
?
ESL
y,y
?
=
?
?
?
1
|D
y
|
?
(x,y)?D
y
p(y
?
|x; ?) if y
?
6= y
1 otherwise
(3)
For each pair of languages y and y
?
, the matrix
S
?
ESL
contains an entry S
?
ESL
y,y
?
which captures
the average probability of mistaking y for y
?
, and
an entry S
?
ESL
y
?
,y
, which represents the opposite
24
Feature Type Examples
Unlexicalized labeled dependencies Relation = prep Head = VBN Dependent = IN
Ordering of head and dependent Ordering = right Head = NNS Dependent = JJ
Distance between head and dependent Distance = 2 Head = VBG Dependent = PRP
POS sequence between head and dependent Relation = det POS-between = JJ
POS n-grams (up to 4-grams) POS bigram = NN VBZ
Inflectional morphology Suffix = ing
Derivational morphology Suffix = ity
Table 2: Examples of syntactic and morphological features of the NLI model. The feature values are set
to the number of occurrences of the feature in the document. The syntactic features are derived from the
output of the Stanford parser. A comprehensive description of the Stanford parser dependency annotation
scheme can be found in the Stanford dependencies manual (de Marneffe and Manning, 2008).
confusion. We average the two confusion scores to
receive the matrix of pairwise language similarity
estimates S
ESL
.
S
ESL
y,y
?
= S
ESL
y
?
,y
=
1
2
(S
?
ESL
y,y
?
+ S
?
ESL
y
?
,y
)
(4)
Note that comparable similarity estimates can
be obtained from the confusion matrix of the clas-
sifier, which records the number of misclassifica-
tions corresponding to each pair of class labels.
The advantage of our probabilistic setup over this
method is its robustness with respect to the actual
classification performance of the model.
4.3 Language Similarity Tree
A particularly informative way of representing
language similarities is in the form of hierarchi-
cal trees. This representation is easier to inspect
than a similarity matrix, and as such, it can be
more instrumental in supporting linguistic inquiry
on language relatedness. Additionally, as we show
in section 7, hierarchical similarity trees can out-
perform raw similarities when used for typology
reconstruction.
We perform hierarchical clustering using the
Ward algorithm (Ward Jr, 1963). Ward is a
bottom-up clustering algorithm. Starting with a
separate cluster for each language, it successively
merges clusters and returns the tree of cluster
merges. The objective of the Ward algorithm is
to minimize the total within-cluster variance. To
this end, at each step it merges the cluster pair
that yields the minimum increase in the overall
within-cluster variance. The initial distance ma-
trix required for the clustering algorithm is de-
fined as 1 ? S
ESL
. We use the Scipy implemen-
tation
3
of Ward, in which the distance between a
newly formed cluster a ? b and another cluster c
is computed with the Lance-Williams distance up-
date formula (Lance and Williams, 1967).
5 WALS Based Language Similarities
In order to determine the extent to which ESL
based language similarities reflect the typological
similarity between the native languages, we com-
pare them to similarities obtained directly from the
typological features in WALS.
The WALS based similarity estimates between
languages y and y
?
are computed by measuring the
cosine similarity between the binarized typologi-
cal feature vectors.
S
WALS
y,y
?
=
v
y
? v
y
?
?v
y
??v
y
?
?
(5)
As mentioned in section 3.2, many of the WALS
features do not have values for all the FCE lan-
guages. To address this issue, we experiment with
two different strategies for choosing the WALS
features to be used for language similarity compu-
tations. The first approach, called shared-all, takes
into account only the 32 features that have known
values in all the 14 languages of our dataset. In
the second approach, called shared-pairwise, the
similarity estimate for a pair of languages is deter-
mined based on the features shared between these
two languages.
As in the ESL setup, we use the two matrices
of similarity estimates to construct WALS based
hierarchical similarity trees. Analogously to the
ESL case, a WALS based tree is generated by the
3
http://docs.scipy.org/.../scipy.
cluster.hierarchy.linkage.html
25
Figure 1: shared-pairwise WALS based versus
ESL based language similarity scores. Each point
represents a language pair, with the vertical axis
corresponding to the ESL based similarity and
the horizontal axis standing for the typological
shared-pairwise WALS based similarity. The
scores correlate strongly with a Pearson?s coeffi-
cient of 0.59 for the shared-pairwise construction
and 0.50 for the shared-all feature-set.
Ward algorithm with the input distance matrix 1?
S
WALS
.
6 Comparison Results
After independently deriving native language sim-
ilarity matrices from ESL texts and from typo-
logical features in WALS, we compare them to
one another. Figure 1 presents a scatter plot
of the language similarities obtained using ESL
data, against the equivalent WALS based similar-
ities. The scores are strongly correlated, with a
Pearson Correlation Coefficient of 0.59 using the
shared-pairwise WALS distances and 0.50 using
the shared-all WALS distances.
This correlation provides appealing evidence
for the hypothesis that distinctive structural pat-
terns of English usage arise via cross-linguistic
transfer, and to a large extent reflect the typologi-
cal similarities between the respective native lan-
guages. The practical consequence of this result is
the ability to use one of these similarity structures
to approximate the other. Here, we use the ESL
based similarities as a proxy for the typological
similarities between languages, allowing us to re-
construct typological information without relying
on a-priori knowledge about the target language
typology.
In figure 2 we present, for illustration purposes,
the hierarchical similarity trees obtained with the
Ward algorithm based on WALS and ESL similar-
ities. The trees bear strong resemblances to one
other. For example, at the top level of the hier-
archy, the Indo-European languages are discerned
from the non Indo-European languages. Further
down, within the Indo-European cluster, the Ro-
mance languages are separated from other Indo-
European subgroups. Further points of similarity
can be observed at the bottom of the hierarchy,
where the pairs Russian and Polish, Japanese and
Korean, and Chinese and Thai merge in both trees.
In the next section we evaluate the quality of
these trees, as well as the similarity matrices used
for constructing them with respect to their ability
to support accurate nearest neighbors based recon-
struction of native language typology.
7 Typology Prediction
Although pairwise language similarities derived
from structural features in ESL texts are highly
correlated with similarities obtained directly from
native language typology, evaluating the absolute
quality of such similarity matrices and trees is
challenging.
We therefore turn to typology prediction based
evaluation, in which we assess the quality of
the induced language similarity estimates by their
ability to support accurate prediction of unseen ty-
pological features. In this evaluation mode we
project unknown WALS features to a target lan-
guage from the languages that are closest to it in
the similarity structure. The underlying assump-
tion of this setup is that better similarity structures
will lead to better accuracies in the feature predic-
tion task.
Typological feature prediction not only pro-
vides an objective measure for the quality of the
similarity structures, but also has an intrinsic value
as a stand-alone task. The ability to infer typolog-
ical structure automatically can be used to create
linguistic databases for low-resource languages,
and is valuable to NLP applications that exploit
such resources, most notably multilingual parsing
(Naseem et al., 2012; T?ackstr?om et al., 2013).
Prediction of typological features for a target
language using the language similarity matrix is
performed by taking a majority vote for the value
of each feature among the K nearest languages of
the target language. In case none of the K nearest
languages have a value for a feature, or given a tie
26
(a) Hierarchical clustering using WALS based shared-
pairwise distances.
(b) Hierarchical clustering using ESL based distances.
Figure 2: Language Similarity Trees. Both trees
are constructed with the Ward agglomerative hi-
erarchical clustering algorithm. Tree (a) uses the
WALS based shared-pairwise language distances.
Tree (b) uses the ESL derived distances.
between several values, we iteratively expand the
group of nearest languages until neither of these
cases applies.
To predict features using a hierarchical cluster
tree, we set the value of each target language fea-
ture to its majority value among the members of
the parent cluster of the target language, excluding
the target language itself. For example, using the
tree in figure 2(a), the feature values for the target
language French will be obtained by taking ma-
jority votes between Portuguese, Italian and Span-
ish. Similarly to the matrix based prediction, miss-
ing values and ties are handled by backing-off to a
larger set of languages, in this case by proceeding
to subsequent levels of the cluster hierarchy. For
the French example in figure 2(a), the first fall-
back option will be the Romance cluster.
Following the evaluation setups in Daum?e III
(2009) and Georgi et al. (2010), we evaluate the
WALS based similarity estimates and trees by con-
structing them using 90% of the WALS features.
We report the average accuracy over 100 random
folds of the data. In the shared-all regime, we pro-
vide predictions not only for the remaining 10%
of features shared by all languages, but also for all
the other features that have values in the target lan-
guage and are not used for the tree construction.
Importantly, as opposed to the WALS based
prediction, our ESL based method does not re-
quire any typological features for inferring lan-
guage similarities and constructing the similarity
tree. In particular, no typological information is
required for the target languages. Typological fea-
tures are needed only for the neighbors of the tar-
get language, from which the features are pro-
jected. This difference is a key advantage of our
approach over the WALS based methods, which
presuppose substantial typological documentation
for all the languages involved.
Table 3 summarizes the feature reconstruction
results. The ESL approach is highly competitive
with the WALS based results, yielding comparable
accuracies for the shared-all prediction, and lag-
ging only 1.7% ? 3.4% behind the shared-pairwise
construction. Also note that for both WALS based
and ESL based predictions, the highest results are
achieved using the hierarchical tree predictions,
confirming the suitability of this representation for
accurately capturing language similarity structure.
Figure 3 presents the performance of the
strongest WALS based typological feature com-
pletion method, WALS shared-pairwise tree, as a
function of the percentage of features used for ob-
taining the language similarity estimates. The fig-
ure also presents the strongest result of the ESL
method, using the ESL tree, which does not re-
quire any such typological training data for ob-
taining the language similarities. As can be seen,
the WALS based approach would require access to
almost 40% of the currently documented WALS
features to match the performance of the ESL
method.
The competitive performance of our ESL
method on the typology prediction task underlines
27
Method NN 3NN Tree
WALS shared-all 71.6 71.4 69.1
WALS shared-pairwise 73.1 74.1 74.2
ESL 71.4 70.7 72.2
Table 3: Typology reconstruction results. Three
types of predictions are compared, nearest neigh-
bor (NN), 3 nearest neighbors (3NN) and near-
est tree neighbors (Tree). WALS shared-all are
WALS based predictions, where only the 32 fea-
tures that have known values in all 14 languages
are used for computing language similarities. In
the WALS shared-pairwise predictions the lan-
guage similarities are computed using the WALS
features shared by each language pair. ESL re-
sults are obtained by projection of WALS features
from the closest languages according to the ESL
language similarities.
its ability to extract strong typologically driven
signal, while being robust to the partial nature of
existing typological annotation which hinders the
performance of the baselines. Given the small
amount of ESL data at hand, these results are
highly encouraging with regard to the prospects
of our approach to support typological inference,
even in the absence of any typological documen-
tation for the target languages.
8 Conclusion and Outlook
We present a novel framework for utilizing cross-
linguistic transfer to infer language similarities
from morpho-syntactic features of ESL text. Trad-
ing laborious expert annotation of typological fea-
tures for a modest amount of ESL texts, we
are able to reproduce language similarities that
strongly correlate with the equivalent typology
based similarities, and perform competitively on
a typology reconstruction task.
Our study leaves multiple questions for future
research. For example, while the current work ex-
amines structure transfer, additional investigation
is required to better understand lexical and phono-
logical transfer effects.
Furthermore, we currently focuse on native lan-
guage typology, and assume English as the foreign
language. This limits our ability to study the con-
straints imposed on cross-linguistic transfer by the
foreign language. An intriguing research direction
would be to explore other foreign languages and
compare the outcomes to our results on English.
Figure 3: Comparison of the typological fea-
ture completion performance obtained using the
WALS tree with shared-pairwise similarities and
the ESL tree based typological feature comple-
tion performance. The dotted line represents the
WALS based prediction accuracy, while the hor-
izontal line is the ESL based accuracy. The
horizontal axis corresponds to the percentage of
WALS features used for constructing the WALS
based language similarity estimates.
Finally, we plan to formulate explicit models
for the relations between specific typological fea-
tures and ESL usage patterns, and extend our ty-
pology induction mechanisms to support NLP ap-
plications in the domain of multilingual process-
ing.
Acknowledgments
We would like to thank Yoong Keok Lee, Jesse
Harris and the anonymous reviewers for valuable
comments on this paper. This material is based
upon work supported by the Center for Brains,
Minds, and Machines (CBMM), funded by NSF
STC award CCF-1231216, and by Google Faculty
Research Award.
References
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A limited memory algorithm for
bound constrained optimization. SIAM Journal on
Scientific Computing, 16(5):1190?1208.
Hal Daum?e III. 2009. Non-parametric Bayesian areal
linguistics. In Proceedings of human language tech-
nologies: The 2009 annual conference of the north
american chapter of the association for computa-
tional linguistics, pages 593?601. Association for
Computational Linguistics.
28
Marie-Catherine de Marneffe and Christopher D Man-
ning. 2008. Stanford typed dependencies manual.
URL http://nlp. stanford. edu/software/dependencies
manual. pdf.
Marie-Catherine de Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.
Susan M Gass and Larry Selinker. 1992. Language
Transfer in Language Learning: Revised edition,
volume 5. John Benjamins Publishing.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, pages 385?393. Association for
Computational Linguistics.
Scott Jarvis and Scott A Crossley. 2012. Approaching
language transfer through text classification: Explo-
rations in the detection-based approach, volume 64.
Multilingual Matters.
Scott Jarvis and Aneta Pavlenko. 2007. Crosslinguis-
tic influence in language and cognition. Routledge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author?s native language by
mining a text for errors. In Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 624?
628. ACM.
Godfrey N Lance and William Thomas Williams.
1967. A general theory of classificatory sorting
strategies ii. clustering systems. The computer jour-
nal, 10(3):271?277.
M. Paul Lewis. 2014. Ethnologue: Languages of the
world. www.ethnologue.com.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629?637. Asso-
ciation for Computational Linguistics.
Terence Odlin. 1989. Language transfer: Cross-
linguistic influence in language learning. Cam-
bridge University Press.
J.J. Song. 2011. The Oxford Handbook of Linguistic
Typology. Oxford Handbooks in Linguistics. OUP
Oxford.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
transfer parsers. Proceedings of NAACL-HLT.
Yee Whye Teh, Hal Daum?e III, and Daniel M Roy.
2007. Bayesian agglomerative clustering with co-
alescents. In NIPS.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. NAACL/HLT 2013, page 48.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Joe H Ward Jr. 1963. Hierarchical grouping to opti-
mize an objective function. Journal of the American
statistical association, 58(301):236?244.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In ACL, pages 180?189.
29
