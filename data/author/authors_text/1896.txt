Proceedings of the 43rd Annual Meeting of the ACL, pages 541?548,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Machine Translation Using Probabilistic 
Synchronous Dependency Insertion Grammars 
 
Yuan Ding Martha Palmer 
Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA 19104, USA 
{yding, mpalmer}@linc.cis.upenn.edu 
 
Abstract 
Syntax-based statistical machine transla-
tion (MT) aims at applying statistical 
models to structured data. In this paper, 
we present a syntax-based statistical ma-
chine translation system based on a prob-
abilistic synchronous dependency 
insertion grammar. Synchronous depend-
ency insertion grammars are a version of 
synchronous grammars defined on de-
pendency trees. We first introduce our 
approach to inducing such a grammar 
from parallel corpora. Second, we de-
scribe the graphical model for the ma-
chine translation task, which can also be 
viewed as a stochastic tree-to-tree trans-
ducer. We introduce a polynomial time 
decoding algorithm for the model. We 
evaluate the outputs of our MT system us-
ing the NIST and Bleu automatic MT 
evaluation software. The result shows that 
our system outperforms the baseline sys-
tem based on the IBM models in both 
translation speed and quality. 
1 Introduction 
Statistical approaches to machine translation, pio-
neered by (Brown et al, 1993), achieved impres-
sive performance by leveraging large amounts of 
parallel corpora. Such approaches, which are es-
sentially stochastic string-to-string transducers, do 
not explicitly model natural language syntax or 
semantics. In reality, pure statistical systems some-
times suffer from ungrammatical outputs, which 
are understandable at the phrasal level but some-
times hard to comprehend as a coherent sentence. 
In recent years, syntax-based statistical machine 
translation, which aims at applying statistical mod-
els to structural data, has begun to emerge. With 
the research advances in natural language parsing, 
especially the broad-coverage parsers trained from 
treebanks, for example (Collins, 1999), the utiliza-
tion of structural analysis of different languages 
has been made possible. Ideally, by combining the 
natural language syntax and machine learning 
methods, a broad-coverage and linguistically well-
motivated statistical MT system can be constructed. 
However, structural divergences between lan-
guages (Dorr, 1994)?which are due to either sys-
tematic differences between languages or loose 
translations in real corpora?pose a major chal-
lenge to syntax-based statistical MT. As a result, 
the syntax based MT systems have to transduce 
between non-isomorphic tree structures. 
(Wu, 1997) introduced a polynomial-time solu-
tion for the alignment problem based on synchro-
nous binary trees. (Alshawi et al, 2000) represents 
each production in parallel dependency trees as a 
finite-state transducer.  Both approaches learn the 
tree representations directly from parallel sen-
tences, and do not make allowances for non-
isomorphic structures.  (Yamada and Knight, 2001, 
2002) modeled translation as a sequence of tree 
operations transforming a syntactic tree into a 
string of the target language.  
When researchers try to use syntax trees in both 
languages, the problem of non-isomorphism must 
be addressed. In theory, stochastic tree transducers 
and some versions of synchronous grammars pro-
vide solutions for the non-isomorphic tree based 
transduction problem and hence possible solutions 
for MT. Synchronous Tree Adjoining Grammars, 
proposed by (Shieber and Schabes, 1990), were 
introduced primarily for semantics but were later 
also proposed for translation. Eisner (2003) pro-
posed viewing the MT problem as a probabilistic 
synchronous tree substitution grammar parsing 
541
problem. Melamed (2003, 2004) formalized the 
MT problem as synchronous parsing based on 
multitext grammars. Graehl and Knight (2004) de-
fined training and decoding algorithms for both 
generalized tree-to-tree and tree-to-string transduc-
ers. All these approaches, though different in for-
malism, model the two languages using tree-based 
transduction rules or a synchronous grammar, pos-
sibly probabilistic, and using multi-lemma elemen-
tary structures as atomic units. The machine 
translation is done either as a stochastic tree-to-tree 
transduction or a synchronous parsing process. 
However, few of the above mentioned formal-
isms have large scale implementations. And to the 
best of our knowledge, the advantages of syntax 
based statistical MT systems over pure statistical 
MT systems have yet to be empirically verified. 
We believe difficulties in inducing a synchro-
nous grammar or a set of tree transduction rules 
from large scale parallel corpora are caused by:  
1. The abilities of synchronous grammars and 
tree transducers to handle non-isomorphism 
are limited. At some level, a synchronous 
derivation process must exist between the 
source and target language sentences.  
2. The training and/or induction of a synchro-
nous grammar or a set of transduction rules 
are usually computationally expensive if all 
the possible operations and elementary struc-
tures are allowed. The exhaustive search for 
all the possible sub-sentential structures in a 
syntax tree of a sentence is NP-complete. 
3. The problem is aggravated by the non-perfect 
training corpora. Loose translations are less of 
a problem for string based approaches than for 
approaches that require syntactic analysis. 
Hajic et al (2002) limited non-isomorphism by 
n-to-m matching of nodes in the two trees.  How-
ever, even after extending this model by allowing 
cloning operations on subtrees, Gildea (2003) 
found that parallel trees over-constrained the 
alignment problem, and achieved better results 
with a tree-to-string model than with a tree-to-tree 
model using two trees. In a different approach, 
Hwa et al (2002) aligned the parallel sentences 
using phrase based statistical MT models and then 
projected the alignments back to the parse trees. 
This motivated us to look for a more efficient 
and effective way to induce a synchronous gram-
mar from parallel corpora and to build an MT sys-
tem that performs competitively with the pure 
statistical MT systems. We chose to build the syn-
chronous grammar on the parallel dependency 
structures of the sentences. The synchronous 
grammar is induced by hierarchical tree partition-
ing operations. The rest of this paper describes the 
system details as follows: Sections 2 and 3 de-
scribe the motivation behind the usage of depend-
ency structures and how a version of synchronous  
dependency grammar is learned. This grammar is 
used as the primary translation knowledge source 
for our system. Section 4 defines the tree-to-tree 
transducer and the graphical model for the stochas-
tic tree-to-tree transduction process and introduces 
a polynomial time decoding algorithm for the 
transducer.  We evaluate our system in section 5 
with the NIST/Bleu automatic MT evaluation 
software and the results are discussed in Section 6. 
2 The Synchronous Grammar 
2.1 Why Dependency Structures? 
According to Fox (2002), dependency representa-
tions have the best inter-lingual phrasal cohesion 
properties. The percentage for head crossings is 
12.62% and that of modifier crossings is 9.22%. 
Furthermore, a grammar based on dependency 
structures has the advantage of being simple in 
formalism yet having CFG equivalent formal gen-
erative capacity (Ding and Palmer, 2004b). 
Dependency structures are inherently lexical-
ized as each node is one word. In comparison, 
phrasal structures (treebank style trees) have two 
node types: terminals store the lexical items and 
non-terminals store word order and phrasal scopes. 
2.2 Synchronous Dependency Insertion Grammars 
Ding and Palmer (2004b) described one version of 
synchronous grammar: Synchronous Dependency 
Insertion Grammars. A Dependency Insertion 
Grammars (DIG) is a generative grammar formal-
ism that captures word order phenomena within the 
dependency representation. In the scenario of two 
languages, the two sentences in the source and tar-
get languages can be modeled as being generated 
from a synchronous derivation process. 
A synchronous derivation process for the two 
syntactic structures of both languages suggests the 
level of cross-lingual isomorphism between the 
two trees (e.g. Synchronous Tree Adjoining 
Grammars (Shieber and Schabes, 1990)). 
542
Apart from other details, a DIG can be viewed 
as a tree substitution grammar defined on depend-
ency trees (as opposed to phrasal structure trees). 
The basic units of the grammar are elementary 
trees (ET), which are sub-sentential dependency 
structures containing one or more lexical items. 
The synchronous version, SDIG, assumes that the 
isomorphism of the two syntactic structures is at 
the ET level, rather than at the word level, hence 
allowing non-isomorphic tree to tree mapping. 
We illustrate how the SDIG works using the 
following pseudo-translation example: 
y [Source] The girl kissed her kitty cat. 
y [Target] The girl gave a kiss to her cat. 
 
Figure 1.
An example
 
Figure 2. 
Tree-to-tree 
transduction
Almost any tree-transduction operations de-
fined on a single node will fail to generate the tar-
get sentence from the source sentence without 
using insertion/deletion operations. However, if we 
view each dependency tree as an assembly of indi-
visible sub-sentential elementary trees (ETs), we 
can find a proper way to transduce the input tree to 
the output tree. An ET is a single ?symbol? in a 
transducer?s language. As shown in Figure 2, each 
circle stands for an ET and thick arrows denote the 
transduction of each ET as a single symbol. 
3 Inducing a Synchronous Dependency 
Insertion Grammar 
As the start to our syntax-based SMT system, the 
SDIG must be learned from the parallel corpora.  
3.1 Cross-lingual Dependency Inconsistencies 
One straightforward way to induce a generative 
grammar is using EM style estimation on the gen-
erative process. Different versions of such training 
algorithms can be found in (Hajic et al, 2002; Eis-
ner 2003; Gildea 2003; Graehl and Knight 2004). 
However, a synchronous derivation process 
cannot handle two types of cross-language map-
pings: crossing-dependencies (parent-descendent 
switch) and broken dependencies (descendent ap-
pears elsewhere), which are illustrated below: 
 
Figure 3. Cross-lingual dependency consistencies 
In the above graph, the two sides are English 
and the foreign dependency trees. Each node in a 
tree stands for a lemma in a dependency tree. The 
arrows denote aligned nodes and those resulting 
inconsistent dependencies are marked with a ?*?.  
Fox (2002) collected the statistics mainly on 
French and English data: in dependency represen-
tations, the percentage of head crossings per 
chance (case [b] in the graph) is 12.62%.  
Using the statistics on cross-lingual dependency 
consistencies from a small word to word aligned 
Chinese-English parallel corpus1, we found that the 
percentage of crossing-dependencies (case [b]) 
between Chinese and English is 4.7% while that of 
broken dependencies (case [c]) is 59.3%. 
The large number of broken dependencies pre-
sents a major challenge for grammar induction 
based on a top-down style EM learning process. 
Such broken and crossing dependencies can be 
modeled by SDIG if they appear inside a pair of 
elementary trees. However, if they appear between 
the elementary trees, they are not compatible with 
the isomorphism assumption on which SDIG is 
based. Nevertheless, the hope is that the fact that 
the training corpus contains a significant percent-
age of dependency inconsistencies does not mean 
that during decoding the target language sentence 
cannot be written in a dependency consistent way. 
3.2 Grammar Induction by Synchronous  
Hierarchical Tree Partitioning 
(Ding and Palmer, 2004a) gave a polynomial time 
solution for learning parallel sub-sentential de-
                                                           
1  Total 826 sentence pairs, 9957 Chinese words, 12660 Eng-
lish words. Data made available by the courtesy of Microsoft 
Research, Asia and IBM T.J. Watson Research. 
543
pendency structures from non-isomorphic depend-
ency trees. Our approach, while similar to (Ding 
and Palmer, 2004a) in that we also iteratively parti-
tion the parallel dependency trees based on a heu-
ristic function, departs (Ding and Palmer, 2004a) 
in three ways: (1) we base the hierarchical tree par-
titioning operations on the categories of the de-
pendency trees; (2) the statistics of the resultant 
tree pairs from the partitioning operation are col-
lected at each iteration rather than at the end of the 
algorithm; (3) we do not re-train the word to word 
probabilities at each iteration. Our grammar induc-
tion algorithm is sketched below: 
Step 0. View each tree as a ?bag of words? and train a 
statistical translation model on all the tree pairs to 
acquire word-to-word translation probabilities. In 
our implementation, the IBM Model 1 (Brown et 
al., 1993) is used. 
Step 1. Let i  denote the current iteration and let 
[ ]C CategorySequence i=  be the current syntac-
tic category set. 
For each tree pair in the corpus, do { 
a) For the tentative synchronous partitioning opera-
tion, use a heuristic function to select the BEST word 
pair * *( , )i je f , where both * *,i je f  are NOT ?chosen?,  
*( )iCategory e C?  and *( )jCategory f C? . 
b) If * *( , )i je f  is found in (a), mark * *,i je f  as ?cho-
sen? and go back to (a), else go to (c). 
c) Execute the synchronous tree partitioning opera-
tion on all the ?chosen? word pairs on the tree pair. 
Hence, several new tree pairs are created. Replace the 
old tree pair with the new tree pairs together with the 
rest of the old tree pair. 
d) Collect the statistics for all the new tree pairs as 
elementary tree pairs. } 
Step 2. 1i i= + . Go to Step 1 for the next iteration. 
At each iteration, one specific set of categories 
of nodes is handled. The category sequence we 
used in the grammar induction is:  
1. Top-NP: the noun phrases that do not have 
another noun phrase as parent or ancestor. 
2. NP: all the noun phrases 
3. VP, IP, S, SBAR:  verb phrases equivalents. 
4. PP, ADJP, ADVP, JJ, RB: all the modifiers 
5. CD: all the numbers. 
We first process top NP chunks because they are 
the most stable between languages. Interestingly, 
NPs are also used as anchor points to learn mono-
lingual paraphrases (Ibrahim et al, 2003). The 
phrasal structure categories can be extracted from 
automatic parsers using methods in (Xia, 2001). 
An illustration is given below (Chinese in pin-
yin form). The placement of the dependency arcs 
reflects the relative word order between a parent 
node and all its immediate children. The collected 
ETs are put into square boxes and the partitioning 
operations taken are marked with dotted arrows. 
y [English]   I have been in Canada since 1947. 
y [Chinese]  Wo 1947 nian yilai  yizhi   zhu  zai  jianada. 
y [Glossary]  I   1947 year since always live in  Canada 
[ ITERATION 1 & 2 ] Partition at word pair  
(?I? and ?wo?) (?Canada? and ?janada?) 
 
[ ITERATION 3 ] (?been? and ?zhu?) are chosen but no 
partition operation is taken because they are roots. 
[ ITERATION 4 ] Partition at word pair  
(?since? and ?yilai?) (?in? and ?zai?) 
 
[ ITERATION 5 ] Partition at ?1947? and ?1947? 
 
[ FINALLY ] Total of 6 resultant ET pairs (figure omitted) 
Figure 4. An Example 
3.3 Heuristics 
Similar to (Ding and Palmer, 2004a), we also use a 
heuristic function in Step 1(a) of the algorithm to 
rank all the word pairs for the tentative tree parti-
544
tioning operation. The heuristic function is based 
on a set of heuristics, most of which are similar to 
those in (Ding and Palmer, 2004a).  
For a word pair ( , )i je f for the tentative parti-
tioning operation, we briefly describe the heuristics: 
y Inside-outside probabilities: We borrow the 
idea from PCFG parsing. This is the probabil-
ity of an English subtree (inside) generating a 
foreign subtree and the probability of the Eng-
lish residual tree (outside) generating a for-
eign residual tree. Here both probabilities are 
based on a ?bag of words? model. 
y Inside-outside penalties: here the probabilities 
of the inside English subtree generating the 
outside foreign residual tree and outside Eng-
lish residual tree generating the inside English 
subtree are used as penalty terms. 
y Entropy: the entropy of the word to word 
translation probability of the English word ie . 
y Part-of-Speech mapping template: whether the 
POS tags of the two words are in the ?highly 
likely to match? POS tag pairs. 
y Word translation probability: P( | )j if e . 
y Rank: the rank of the word to word probabil-
ity of jf  in as a translation of ie  among all 
the foreign words in the current tree. 
The above heuristics are a set of real valued 
numbers. We use a Maximum Entropy model to 
interpolate the heuristics in a log-linear fashion, 
which is different from the error minimization 
training in (Ding and Palmer, 2004a).  ( )0 1P | ( , ), ( , )... ( , )
1 exp ( , )
i j i j n i j
k k i j s
k
y h e f h e f h e f
h e f
Z
? ?? ?= +? ?? ??
  (1) 
where (0,1)y =  as labeled in the training data 
whether the two words are mapped with each other. 
The MaxEnt model is trained using the same 
word level aligned parallel corpus as the one in 
Section 3.1. Although the training corpus isn?t 
large, the fact that we only have a handful of pa-
rameters to fit eased the problem.  
3.4 A Scaled-down SDIG 
It is worth noting that the set of derived parallel 
dependency Elementary Trees is not a full-fledged 
SDIG yet. Many features in the SDIG formalism 
such as arguments, head percolation, etc. are not 
yet filled. We nevertheless use this derived gram-
mar as a Mini-SDIG, assuming the unfilled fea-
tures as empty by default. A full-fledged SDIG 
remains a goal for future research. 
4 The Machine Translation System 
4.1 System Architecture 
As discussed before (see Figure 1 and 2), the archi-
tecture of our syntax based statistical MT system is 
illustrated in Figure 5. Note that this is a non-
deterministic process. The input sentence is first 
parsed using an automatic parser and a dependency 
tree is derived. The rest of the pipeline can be 
viewed as a stochastic tree transducer. The MT 
decoding starts first by decomposing the input de-
pendency tree in to elementary trees. Several dif-
ferent results of the decomposition are possible. 
Each decomposition is indeed a derivation process 
on the foreign side of SDIG. Then the elementary 
trees go through a transfer phase and target ETs are 
combined together into the output. 
 
Figure 5. System architecture 
4.2 The Graphical Model 
The stochastic tree-to-tree transducer we propose 
models MT as a probabilistic optimization process. 
Let f  be the input sentence (foreign language), 
and e  be the output sentence (English). We have 
P( | ) P( )P( | )
P( )
f e ee f
f
= , and the best translation is: 
 * arg max P( | )P( )
e
e f e e=    (2) 
P( | )f e  and P( )e  are also known as the ?trans-
lation model? (TM) and the ?language model? 
(LM). Assuming the decomposition of the foreign 
tree is given, our approach, which is based on ETs, 
uses the graphical model shown in Figure 6. 
In the model, the left side is the input depend-
ency tree (foreign language) and the right side is 
the output dependency tree (English). Each circle 
stands for an ET. The solid lines denote the syntac-
tical dependencies while the dashed arrows denote 
the statistical dependencies. 
545
 Figure 6 
The graphical 
model 
Let T( )x be the dependency tree constructed 
from sentence x . A tree-decomposition function  
D( )t  is defined on a dependency tree t , and out-
puts a certain ET derivation tree of  t , which is 
generated by decomposing t  into ETs. Given t , 
there could be multiple decompositions. Condi-
tioned on decomposition D , we can rewrite (2) as: 
* arg max P( , | )P( )
arg max P( | , )P( | )P( )
e D
e D
e f e D D
f e D e D D
=
=
?
?  (3) 
By definition, the ET derivation trees of the in-
put and output trees should be isomorphic: 
D(T( )) D(T( ))f e? . Let Tran( )u  be a set of possi-
ble translations for the ET u . We have: 
D(T( )), D(T( )), Tran( )
P( | , ) P(T( ) | P(T( ), )
P( | )
u f v e v u
f e D f e D
u v
? ? ?
=
= ?           (4) 
For any ET v  in a given ET derivation tree d , 
let Root( )d  be the root ET of d , and let 
Parent( )v  denote the parent ET of  v . We have: 
( )( )
D(T( )), Root(D(T( ))
P( | ) P(T( ) | )
P Root D(T( )
P( | Parent( ))
v e v e
e D e D
e
v v
? ?
=
= ?
? ??? ?? ??
 (5) 
where, letting root( )v  denote the root word of v , 
( ) ( )( )P | Parent( ) P root( ) | root Parent( )v v v v=  (6) 
The prior probability of tree decomposition is 
defined as: ( )
D(T( ))
P D(T( )) P( )
u f
f u
?
= ?   (7) 
Figure 7 
 Comparing to 
the HMM 
An analogy between our model and a Hidden 
Markov Model (Figure 7) may be helpful. In Eq. 
(4), P( | )u v  is analogous to the emission probably 
P( | )i io s  in an HMM. In Eq. (5), P( | Parent( ))v v  is 
analogous to the transition probability 1P( | )i is s ?  in 
an HMM. While HMM is defined on a sequence 
our model is defined on the derivation tree of ETs. 
4.3 Other Factors 
y Augmenting parallel ET pairs 
In reality, the learned parallel ETs are unlikely to 
cover all the structures that we may encounter in 
decoding. As a unified approach, we augment the 
SDIG by adding all the possible word pairs ( , )j if e   
as a parallel ET pair and using the IBM Model 1 
(Brown et al, 1993) word to word translation 
probability as the ET translation probability. 
y Smoothing the ET translation probabilities. 
The LM probabilities P( | Parent( ))v v  are simply 
estimated using the relative frequencies. In order to 
handle possible noise from the ET pair learning 
process, the ET translation probabilities P ( | )emp u v  
estimated by relative frequencies are smoothed 
using a word level model. For each ET pair ( , )u v , 
we interpolate the empirical probability with the 
?bag of words? probability and then re-normalize: 
size( )
1 1P( | ) P ( , ) P( | )
size( )
ij
emp j iv
e vf u
u v u v f e
Z u ??
= ? ??  (8) 
4.4 Polynomial Time Decoding 
For efficiency reasons, we use maximum approxi-
mation for (3). Instead of summing over all the 
possible decompositions, we only search for the 
best decomposition as follows: 
,
*, * arg max P( | , )P( | )P( )
e D
e D f e D e D D=  (9) 
So bringing equations (4) to (9) together, the 
best translation would maximize: 
( )P( | ) P Root( ) P( | Parent( )) P( )u v e v v u? ?? ? ?? ?? ?? ? ? (10) 
Observing the similarity between our model 
and a HMM, our dynamic programming decoding 
algorithm is in spirit similar to the Viterbi algo-
rithm except that instead of being sequential the 
decoding is done on trees in a top down fashion. 
As to the relative orders of the ETs, we cur-
rently choose not to reorder the children ETs given 
the parent ET because: (1) the permutation of the 
ETs is computationally expensive (2) it is possible 
that we can resort to simple linguistic treatments 
on the output dependency tree to order the ETs. 
Currently, all the ETs are attached to each other 
546
at their root nodes. 
In our implementation, the different decomposi-
tions of the input dependency tree are stored in a 
shared forest structure, utilizing the dynamic pro-
gramming property of the tree structures explicitly. 
Suppose the input sentence has n  words and 
the shared forest representation has m  nodes. 
Suppose for each word, there are maximally k  
different ETs containing it, we have knm ? . Let 
b  be the max breadth factor in the packed forest, it 
can be shown that the decoder visits at most mb  
nodes during execution. Hence, we have: 
)()( kbnOdecodingT ?             (11) 
which is linear to the input size. Combined with a 
polynomial time parsing algorithm, the whole 
decoding process is polynomial time. 
5 Evaluation  
We implemented the above approach for a Chi-
nese-English machine translation system. We used 
an automatic syntactic parser (Bikel, 2002) to pro-
duce the parallel parse trees. The parser was 
trained using the Penn English/Chinese Treebanks. 
We then used the algorithm in (Xia 2001) to con-
vert the phrasal structure trees to dependency trees 
to acquire the parallel dependency trees. The statis-
tics of the datasets we used are shown as follows: 
Dataset Xinhua FBIS NIST 
Sentence# 56263 45212 206 
Chinese word# 1456495 1185297 27.4 average
English word# 1490498 1611932 37.7 average
Usage training training testing 
Figure 8. Evaluation data details 
 The training set consists of Xinhua newswire 
data from LDC and the FBIS data (mostly news), 
both filtered to ensure parallel sentence pair quality. 
We used the development test data from the 2001 
NIST MT evaluation workshop as our test data for 
the MT system performance. In the testing data, 
each input Chinese sentence has 4 English transla-
tions as references. Our MT system was evaluated 
using the n-gram based Bleu (Papineni et al, 2002) 
and NIST machine translation evaluation software. 
We used the NIST software package ?mteval? ver-
sion 11a, configured as case-insensitive. 
In comparison, we deployed the GIZA++ MT 
modeling tool kit, which is an implementation of 
the IBM Models 1 to 4 (Brown et al, 1993; Al-
Onaizan et al, 1999; Och and Ney, 2003). The 
IBM models were trained on the same training data 
as our system. We used the ISI Rewrite decoder 
(Germann et al 2001) to decode the IBM models. 
The results are shown in Figure 9. The score 
types ?I? and ?C? stand for individual and cumula-
tive n-gram scores. The final NIST and Bleu scores 
are marked with bold fonts.  
Systems Score Type 1-gram 2-gram 3-gram 4-gram
NIST 2.562 0.412 0.051 0.008I Bleu 0.714 0.267 0.099 0.040
NIST 2.562 2.974 3.025 3.034
IBM 
Model 4 C Bleu 0.470 0.287 0.175 0.109
NIST 5.130 0.763 0.082 0.013I Bleu 0.688 0.224 0.075 0.029
NIST 5.130 5.892 5.978 5.987
SDIG
C Bleu 0.674 0.384 0.221 0.132
Figure 9. Evaluation Results. 
The evaluation results show that the NIST score 
achieved a 97.3% increase, while the Bleu score 
increased by 21.1%. 
In terms of decoding speed, the Rewrite de-
coder took 8102 seconds to decode the test sen-
tences on a Xeon 1.2GHz 2GB memory machine. 
On the same machine, the SDIG decoder took 3 
seconds to decode, excluding the parsing time. The 
recent advances in parsing have achieved parsers 
with 3( )O n  time complexity without the grammar 
constant (McDonald et al, 2005). It can be ex-
pected that the total decoding time for SDIG can 
be as short as 0.1 second per sentence. 
Neither of the two systems has any specific 
translation components, which are usually present 
in real world systems (E.g. components that trans-
late numbers, dates, names, etc.) It is reasonable to 
expect that the performance of SDIG can be further 
improved with such specific optimizations. 
6 Discussions 
We noticed that the SDIG system outputs tend to 
be longer than those of the IBM Model 4 system, 
and are closer to human translations in length. 
Translation Type Human SDIG IBM-4
Avg. Sent. Len. 37.7 33.6 24.2 
Figure 10. Average Sentence Word Count 
This partly explains why the IBM Model 4 system 
has slightly higher individual n-gram precision 
scores (while the SDIG system outputs are still 
better in terms of absolute matches).  
547
The relative orders between the parent and child 
ETs in the output tree is currently kept the same as 
the orders in the input tree. Admittedly, we bene-
fited from the fact that both Chinese and English 
are SVO languages, and that many of orderings 
between the arguments and adjuncts can be kept 
the same. However, we did notice that this simple 
?ostrich? treatment caused outputs such as ?foreign 
financial institutions the president of?. 
While statistical modeling of children reorder-
ing is one possible remedy for this problem, we 
believe simple linguistic treatment is another, as 
the output of the SDIG system is an English 
dependency tree rather than a string of words. 
7 Conclusions and Future Work 
In this paper we presented a syntax-based statisti-
cal MT system based on a Synchronous Depend-
ency Insertion Grammar and a non-isomorphic 
stochastic tree-to-tree transducer. A graphical 
model for the transducer is defined and a polyno-
mial time decoding algorithm is introduced. The 
results of our current implementation were evalu-
ated using the NIST and Bleu automatic MT 
evaluation software. The evaluation shows that the 
SDIG system outperforms an IBM Model 4 based 
system in both speed and quality. 
Future work includes a full-fledged version of 
SDIG and a more sophisticated MT pipeline with 
possibly a tri-gram language model for decoding. 
References  
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, 
I. D. Melamed, F. Och, D. Purdy, N. A. Smith, and D. 
Yarowsky. 1999. Statistical machine translation. 
Technical report, CLSP, Johns Hopkins University.  
H. Alshawi, S. Bangalore, S. Douglas. 2000. Learning 
dependency translation models as collections of finite 
state head transducers. Comp. Linguistics, 26(1):45-60. 
Daniel M. Bikel. 2002. Design of a multi-lingual, paral-
lel-processing statistical parsing engine. In HLT 2002. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2): 263-311. 
Michael John Collins. 1999. Head-driven Statistical 
Models for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania, Philadelphia. 
Ding and Palmer. 2004a. Automatic Learning of Paral-
lel Dependency Treelet Pairs. In First International 
Joint Conference on NLP (IJCNLP-04). 
Ding and Palmer. 2004b. Synchronous Dependency 
Insertion Grammars: A Grammar Formalism for Syn-
tax Based Statistical MT. Workshop on Recent Ad-
vances in Dependency Grammars, COLING-04. 
Bonnie J. Dorr. 1994. Machine translation divergences: 
A formal description and proposed solution. Compu-
tational Linguistics, 20(4): 597-633. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In ACL-03. (compan-
ion volume), Sapporo, July. 
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP-02. 
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel 
Marcu, and Kenji Yamada. 2001. Fast Decoding and 
Optimal Decoding for Machine Translation. ACL-01. 
Daniel Gildea. 2003. Loosely tree based alignment for 
machine translation. ACL-03, Japan. 
Jonathan Graehl and Kevin Knight. 2004. Training Tree 
Transducers. In NAACL/HLT-2004 
Jan Hajic, et al 2002. Natural language generation in 
the context of machine translation. Summer workshop 
final report, Center for Language and Speech Process-
ing, Johns Hopkins University, Baltimore.  
Rebecca Hwa, Philip S. Resnik, Amy Weinberg, and 
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. ACL-02 
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolin-
gual Corpora. In Proceedings of the Second 
International Workshop on Paraphrasing (IWP 2003) 
Dan Melamed. 2004. Statistical Machine Translation by 
Parsing. In ACL-04, Barcelona, Spain. 
Dan Melamed. 2003. Multitext Grammars and Synchro-
nous Parsers, In NAACL/HLT-2003. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
BLEU: a method for automatic evaluation of machine 
translation. ACL-02, Philadelphia, USA. 
Ryan McDonald, Koby Crammer and Fernando Pereira. 
2005. Online Large-Margin Training of Dependency 
Parsers. ACL-05. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19?51. 
S. M. Shieber and Y. Schabes. 1990. Synchronous Tree-
Adjoining Grammars, Proceedings of the 13th 
COLING, pp. 253-258, August 1990. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):3-403. 
Fei Xia. 2001. Automatic grammar generation from two 
different perspectives. PhD thesis, U. of Pennsylvania. 
Kenji Yamada and Kevin Knight. 2001. A syntax based 
statistical translation model. ACL-01, France. 
Kenji Yamada and Kevin Knight. 2002. A decoder for 
syntax-based statistical MT. ACL-02, Philadelphia. 
548
 Synchronous Dependency Insertion Grammars 
A Grammar Formalism for Syntax Based Statistical MT 
Yuan Ding     and     Martha Palmer 
Department of Computer and Information Science 
University of Pennsylvania 
Philadelphia, PA 19104, USA 
{yding, mpalmer}@linc.cis.upenn.edu 
 
Abstract 
This paper introduces a grammar formalism 
specifically designed for syntax-based sta-
tistical machine translation. The synchro-
nous grammar formalism we propose in 
this paper takes into consideration the per-
vasive structure divergence between lan-
guages, which many other synchronous 
grammars are unable to model. A Depend-
ency Insertion Grammars (DIG) is a gen-
erative grammar formalism that captures 
word order phenomena within the depend-
ency representation. Synchronous Depend-
ency Insertion Grammars (SDIG) is the 
synchronous version of DIG which aims at 
capturing structural divergences across the 
languages. While both DIG and SDIG have 
comparatively simpler mathematical forms, 
we prove that DIG nevertheless has a gen-
eration capacity weakly equivalent to that 
of CFG. By making a comparison to TAG 
and Synchronous TAG, we show how such 
formalisms are linguistically motivated. We 
then introduce a probabilistic extension of 
SDIG. We finally evaluated our current im-
plementation of a simplified version of 
SDIG for syntax based statistical machine 
translation. 
1 Introduction 
Dependency grammars have a long history and 
have played an important role in machine translation 
(MT). The early use of dependency structures in ma-
chine translation tasks mainly fall into the category 
of transfer based MT, where the dependency struc-
ture of the source language is first analyzed, then 
transferred to the target language by using a set of 
transduction rules or a transfer lexicon, and finally 
the linear form of the target language sentence is 
generated. 
While the above approach seems to be plausible, 
the transfer process demands intense human effort in 
creating a working transduction rule set or a transfer 
lexicon, which largely limits the performance and 
application domain of the resultant machine transla-
tion system. 
In the early 1990s, (Brown et. al. 1993) intro-
duced the idea of statistical machine translation, 
where the word to word translation probabilities and 
sentence reordering probabilities are estimated from 
a large set of parallel sentence pairs. By having the 
advantage of leveraging large parallel corpora, the 
statistical MT approach outperforms the traditional 
transfer based approaches in tasks for which ade-
quate parallel corpora is available (Och, 2003). 
However, a major criticism of this approach is that it 
is void of any internal representation for syntax or 
semantics. 
In recent years, hybrid approaches, which aim at 
applying statistical learning to structured data, began 
to emerge. Syntax based statistical MT approaches 
began with (Wu 1997), who introduced a polyno-
mial-time solution for the alignment problem based 
on synchronous binary trees. (Alshawi et al, 2000) 
extended the tree-based approach by representing 
each production in parallel dependency trees as a 
finite-state transducer. (Yamada and Knight, 2001, 
2002) model translation as a sequence of operations 
transforming a syntactic tree in one language into 
the string of the second language. 
The syntax based statistical approaches have 
been faced with the major problem of pervasive 
structural divergence between languages, due to both 
systematic differences between languages (Dorr, 
1994) and the vagaries of loose translations in real 
corpora. While we would like to use syntactic in-
formation in both languages, the problem of non-
isomorphism grows when trees in both languages are 
required to match.  
To allow the syntax based machine translation 
approaches to work as a generative process, certain 
isomorphism assumptions have to be made. Hence a 
reasonable question to ask is: to what extent should 
the grammar formalism, which we choose to repre-
sent syntactic language transfer, assume isomor-
phism between the structures of the two languages? 
(Hajic et al, 2002) allows for limited non-
isomorphism in that n-to-m matching of nodes in the 
two trees is permitted.  However, even after extend-
ing this model by allowing cloning operations on 
subtrees, (Gildea, 2003) found that parallel trees 
over-constrained the alignment problem, and 
achieved better results with a tree-to-string model 
 using one input tree than with a tree-to-tree model 
using two. 
At the same time, grammar theoreticians have 
proposed various generative synchronous grammar 
formalisms for MT, such as Synchronous Context 
Free Grammars (S-CFG) (Wu, 1997) or Synchro-
nous Tree Adjoining Grammars (S-TAG) (Shieber 
and Schabes, 1990). Mathematically, generative 
synchronous grammars share many good properties 
similar to their monolingual counterparts such as 
CFG or TAG (Joshi and Schabes, 1992). If such a 
synchronous grammar could be learnt from parallel 
corpora, the MT task would become a mathemati-
cally clean generative process. 
However, the problem of inducing a synchronous 
grammar from empirical data was never solved. For 
example, Synchronous TAGs, proposed by (Shieber 
and Schabes, 1990), which were introduced primar-
ily for semantics but were later also proposed for 
translation.  From a formal perspective, Syn-TAGs 
characterize the correspondences between languages 
by a set of synchronous elementary tree pairs. While 
examples show that this formalism does capture cer-
tain cross language structural divergences, there is 
not, to our knowledge, any successful statistical 
learning method to learn such a grammar from em-
pirical data. We believe that this is due to the limited 
ability of Synchronous TAG to model structure di-
vergences. This observation will be discussed later 
in Section 5. 
We studied the problem of learning synchronous 
syntactic sub-structures (parallel dependency treelets) 
from unaligned parallel corpora in (Ding and Palmer, 
2004). At the same time, we would like to formalize 
a synchronous grammar for syntax based statistical 
MT. The necessity of a well-defined formalism and 
certain limitations of the current existing formalisms, 
motivate us to design a new synchronous grammar 
formalism which will have the following properties: 
1. Linguistically motivated: it should be able to 
capture most language phenomena, e.g. compli-
cated word orders such as ?wh? movement. 
2. Without the unrealistic word-to-word isomor-
phism assumption: it should be able to capture 
structural variations between the languages. 
3. Mathematically rigorous: it should have a well 
defined formalism and a proven generation ca-
pacity, preferably context free or mildly context 
sensitive. 
4. Generative: it should be ?generative? in a 
mathematical sense. This property is essential 
for the grammar to be used in statistical MT. 
Each production rule should have its own prob-
ability, which will allow us to decompose the 
overall translation probability. 
5. Simple: it should have a minimal number of 
different structures and operations so that it will 
be learnable from the empirical data. 
In the following sections of this paper, we intro-
duce a grammar formalism that satisfies the above 
properties: Synchronous Dependency Insertion 
Grammar (SDIG). Section 2 gives an informal look 
at the desired capabilities of a monolingual version 
Dependency Insertion Grammar (DIG) by address-
ing the problems with previous dependency gram-
mars. Section 3 gives the formal definition of the 
DIG and shows that it is weakly equivalent to Con-
text Free Grammar (CFG). Section 4 shows how 
DIG is linguistically motivated by making a com-
parison between DIG and Tree Adjoining Grammar 
(TAG). Section 5 specifies the Synchronous DIG 
and Section 6 gives the probabilistic extension of 
SDIG. 
2 Issues with Dependency Grammars 
2.1 Dependency Grammars and Statistical MT 
According to (Fox, 2002), dependency represen-
tations have the best phrasal cohesion properties 
across languages. The percentage of head crossings 
per chance is 12.62% and that of modifier crossings 
per chance is 9.22%. Observing this fact, it is rea-
sonable to propose a formalism that handles lan-
guage transfer based on dependency structures. 
What is more, if a formalism based on depend-
ency structures is made possible, it will have the 
nice property of being simple, as expressed in the 
following table: 
 CFG TAG DG 
Node# 2n 2n n 
Lexicalized? NO YES YES 
Node types 2 2 1* 
Operation types 1 2 1* 
(*: will be shown later in this paper) 
Figure 1. 
The simplicity of a grammar is very important for 
statistical modeling, i.e. when it is being learned 
from the corpora and when it is being used in ma-
chine translation decoding, we don?t need to condi-
tion the probabilities on two different node types or 
operations. 
At the same time, dependency grammars are in-
herently lexicalized in that each node is one word. 
Statistical parsers (Collins 1999) showed perform-
ance improvement by using bilexical probabilities, 
i.e. probabilities of word pair occurrences. This is 
what dependency grammars model explicitly. 
 2.2 A Generative Grammar? 
Why do we want the grammar for statistical MT 
to be generative? First of all, generative models have 
long been studied in the machine learning commu-
nity, which will provide us with mathematically rig-
orous algorithms for training and decoding. Second, 
CFG, the most popular formalism in describing 
natural language phenomena, is generative. Certain 
ideas and algorithms can be borrowed from CFG if 
we make the formalism generative. 
While there has been much previous work in 
formalizing dependency grammars and in its appli-
cation to the parsing task, until recently (Joshi and 
Rambow, 2003), little attention has been given to the 
issue of making the proposed dependency grammar 
generative. And in machine translation tasks, al-
though using dependency structures is an old idea, 
little effort has been made to propose a formal 
grammar which views the composition and decom-
position of dependency trees as a generative process 
from a formal perspective. 
There are two reasons for this fact: (1) The 
?pure? dependency trees do not have nonterminals. 
The standard solution to this problem was intro-
duced as early as (Gaifman 1965), where he pro-
posed adding syntactic categories to each node on 
the dependency tree. (2) However, there is a deeper 
problem with dependency grammar formalisms, as 
observed by (Rambow and Joshi 1997). In the de-
pendency representation, it is hard to handle com-
plex word order phenomena without resorting to 
global word order rules, which makes the grammar 
no longer generative. This will be explored in the 
next subsection (2.3).  
2.3 Non-projectivity 
Non-projectivity has long been a major obstacle 
for anyone who wants to formalize dependency 
grammar. When we draw projection lines from the 
nodes in the dependency trees to a linear representa-
tion of the sentence, if we cannot do so without hav-
ing one or more projection lines going across at least 
one of the arcs of the dependency tree, we say the 
dependency tree is non-projective. 
A typical example for non-projectivity is ?wh? 
movement, which is illustrated below.  
 
Figure 2. 
 
Our solution for this problem is given in section 
4 and in the next section we will first give the formal 
definition of the monolingual Dependency Insertion 
Grammar. 
3 The DIG Formalism 
3.1 Elementary Trees 
Formally, the Dependency Insertion Grammar is 
defined as a six tuple ),,,,,( RSBALC . C  is a set 
of syntactic categories and L  is a set of lexical 
items. A  is a set of Type-A trees and B  is a set of 
Type-B trees (defined later). S  is a set of the start-
ing categories of the sentences. R  is a set of word 
order rules local to each node of the trees. 
Each node in the DIG has three fields:  
A Node consists of: 
1. One lexical item 
2. One corresponding category 
3. One local word order rule.  
We define two types of elementary trees in DIG: 
Type-A trees and Type-B trees. Both types of trees 
have one or more nodes. One of the nodes in an 
elementary tree is designated as the head of the ele-
mentary tree. 
Type-A trees are also called ?root lexicalized 
trees?. They roughly correspond to the ?  trees in 
TAG. Type-A trees have the following properties: 
Properties of a Type-A elementary tree: 
1. The root is lexicalized. 
2. The root is designated as the head of the 
tree 
3. Any lexicalized node can take a set of 
unlexicalized nodes as its arguments. 
4. The local word order rule specifies the 
relative order between the current node 
and all its immediate children, including 
the unlexicalized arguments. 
Here is an example of a Type-A elementary tree 
for the verb ?like?. Note that the head node is 
marked with (@).  
Please note that the placement of the dependency 
arcs reflects the relative order between the parent 
and all its immediate children. 
Figure 3 
Type-B trees are also called ?root unlexicalized 
trees?. They roughly correspond to ?  trees in TAG 
and have the following properties: 
 Properties of a Type-B elementary tree: 
1. The root is the ONLY unlexicalized node 
2. One of the lexicalized nodes is desig-
nated as the head of the tree 
3. Similar to Type-A trees, each node also 
have a word order rule that specifies the 
relative order between the current node 
and all its immediate children. 
Here is and example of a Type-B elementary tree for 
the adverb ?really? 
 
Figure 4 
3.2 The Unification Operation 
We define only one type of operation: unification 
for any DIG derivation: 
Unification Operation: 
When an unlexicalized node and a head 
node have the same categories, they can 
be merged into one node. 
This specifies that an unlexicalized node cannot 
be unified with a non-head node, which guarantees 
limited complexity when a unification operation 
takes place.  
After unification,  
1. If the resulting tree is a Type-A tree, its root 
becomes the new root; 
2. If the resulting tree is a Type-B tree, the root 
node involved in the unification operation be-
comes the new root. 
Here is one example for the unification operation 
which adjoins the adverb ?really? to the verb ?like?: 
 
Figure 5 
Note that for the above unification operation the 
dependency tree on the right hand side is just one of 
the possible resultant dependency trees. The strings 
generated by the set of possible resultant depend-
ency trees should all be viewed as the language 
)(DIGL  generated by the DIG grammar. 
Also note that the definition of DIG is preserved 
through the unification operation, as we have: 
1. (Type-A) (unify) (Type A)  =  (Type-A) 
2. (Type-A) (unify) (Type B)  =  (Type-A) 
3. (Type-B) (unify) (Type B)  =  (Type-B) 
3.3 Comparison to Other Approaches 
There are two major differences between our de-
pendency grammar formalism and that of (Joshi and 
Rambow, 2003): 
1. We only define one unification operation, 
whereas (Joshi and Rambow, 2003) defined two 
operations: substitution and adjunction. 
2. We introduce the concept of ?heads? in the DIG 
so that the derivation complexity is significantly 
smaller. 
3.4 Proof of Weak Equivalence between DIG 
and CFG 
We prove the weak equivalence between DIG and 
CFG by first showing that the language that a DIG 
generates is a subset of one that a CFG generates, 
i.e. )()( CFGLDIGL ? . And then we show the 
opposite is also true: )()( DIGLCFGL ? . 
3.4.1 )()( CFGLDIGL ?  
The proof is given constructively. First, for each 
Type-A tree, we ?insert? a ?waiting for Type-B tree? 
argument at each possible slot underneath it with the 
category B. This process is shown below: 
 
Figure 6 
Then we ?flatten? the Type-A tree to its linear 
form according to the local word order rule, which 
decides the relative ordering between the parent and 
all its children at each of the nodes. And we get: 
}.{}{
}.{}{}.{}.{ 100
Hnji
HHH
CBNTwCNTw
CBNTwCNTwCBNTCANT
LL
L?  
y nww L0 is the strings of lexical items 
y }.{ HCANT  is the nonterminal created for  
this Type-A tree, and HC is the category of the 
head (root). 
y }{ jCNT  is the nonterminal for each category 
y }.{ HCBNT  is the nonterminal for each ?Type-
B site? 
Similarly, for each Type-B tree we can create 
?Type-B site? under its head node. So we have: 
nHiHR wCBNTwCBNTwCRBNT }.{}.{}.{ 0 LL?  
Then we create the production to take arguments: 
}.{}{ CANTCNT ?  
And the production rules to take Type-B trees: 
}.{}.{}.{ CBNTCRBNTCBNT ?  
}.{}.{}.{ CRBNTCBNTCBNT ?  
 Hence, a DIG can be converted to a CFG. 
3.4.2 )()( DIGLCFGL ?  
It is known that a context free grammar can be con-
verted to Greibach Normal Form, where each pro-
duction will have the form: 
*aVA ? , where V  is the set of nonterminals 
We simply construct a corresponding Type-A 
dependency tree as follows: 
 
Figure 7
 
4 Compare DIG to TAG 
A Tree Adjoining Grammars is defined as a five 
tuple ),,,,( SAINT? , where ?  is a set of terminals, 
NT  is a set of nonterminals, I  is a finite set of fi-
nite initial trees (?  trees), A  is a finite set of auxil-
iary trees ( ?  trees), and S  is a set of starting 
symbols. The TAG formalism defines two opera-
tions, substitution and adjunction. 
A TAG derives a phrase-structure tree, called the 
?derived tree? and at the same time, in each step of 
the derivation process, two elementary trees are 
connected through either the substitution or adjunc-
tion operation. Hence, we have a ?derivation tree? 
which represents the syntactic and/or logical relation 
between the elementary trees. Since each elementary 
tree of TAG has exactly one lexical node, we can 
view the derivation tree as a ?Deep Syntactic Repre-
sentation? (DSynR). This representation closely re-
sembles the dependency structure of the sentence. 
Here we show how DIG models different opera-
tions of TAG and hence handles word order phe-
nomena gracefully.  
We categorize the TAG operations into three dif-
ferent types: substitution, non-predicative adjunction 
and predicative adjunction. 
z Substitution 
We model the TAG substitution operation by 
having the embedded tree replaces the non-terminal 
that is in accordance with its root. An example for 
this type is the substitution of NP. 
 
Figure 8a Substitution in TAG 
 
Figure 8b Substitution through DIG unification 
z Non-predicative Adjunction 
In TAG, this type of operation includes all ad-
junctions when the embedded tree does not contain a 
predicate, i.e. the root of the embedded tree is not an 
S. For example, the trees for adverbs are with root 
VP and are adjoined to non-terminal VPs in the ma-
trix tree. 
 
Figure 9a Non-predicative Adjunction in TAG 
Like[V]@
[N]John[N]really[adv]@
[V] Like[V]@
[N]John[N] really[adv]  
Figure 9b Non-predicative Adjunction through DIG 
unification 
z Predicative Adjunction 
This type of operation adjoins an embedded tree 
which contains a predicate, i.e. with a root S, to the 
matrix tree. A typical example is the sentence: Who 
does John think Mary likes?  
This example is non-projective and has ?wh? 
movement. In the TAG sense, the tree for ?does 
John think? is adjoined to the matrix tree for ?Who 
Mary likes?. This category of operation has some 
interesting properties. The dependency relation of 
the embedded tree and the matrix tree is inverted. 
This means that if tree T1 is adjoined to T2, in non-
predicative adjunction, T1 depends on T2, but in 
predicative adjunction, T2 depends on T1. In the 
above example, the tree with ?like? depends on the 
tree with ?think?. 
 
Figure 10a ?Wh? movement through TAG  
(predicative) adjunction operation 
 Our solution is quite simple: when we are con-
structing the grammar, we invert the arc that points 
to a predicative clause. Despite the fact that the re-
sulting dependency trees have certain arcs inverted, 
we will still be able to use localized word order rules 
and derive the desired sentence with the simple uni-
fication operation. As shown below: 
 
Figure 10b ?Wh? movement through unification 
Since TAG is mildly context sensitive, and we 
have shown in Section 3 that DIG is context free, we 
are not claiming the two grammars are weakly or 
strongly equivalent. Also, please note DIG does not 
handle all the non-projectivity issues due to its CFG 
equivalent generation capacity. 
5 Synchronous DIG 
5.1 Definition 
(Wu, 1997) introduced synchronous binary trees 
and (Shieber, 1990) introduced synchronous tree 
adjoining grammars, both of which view the transla-
tion process as a synchronous derivation process of 
parallel trees. Similarly, with our DIG formalism, 
we can construct a Synchronous DIG by synchroniz-
ing both structures and operations in both languages 
and ensuring synchronous derivations. 
Properties of SDIG: 
1. The roots of both trees of the source and 
target languages are aligned, and have the 
same category 
2. All the unlexicalized nodes of both trees 
are aligned and have the same category. 
3. The two heads of both trees are aligned 
and have the same category. 
Synchronous Unification Operation: 
By the above properties of SDIG, we can 
show that unification operations are synchro-
nized in both languages. Hence we can have 
synchronous unification operations. 
5.2 Isomorphism Assumption 
So how is SDIG different from other synchro-
nous grammar formalisms?  
As we know, a synchronous grammar derives 
both source and target languages through a series of 
synchronous derivation steps. For any tree-based 
synchronous grammar, the synchronous derivation 
would create two derivation trees for both languages 
which have isomorphic structure. Thus a synchro-
nous grammar assumes certain isomorphism be-
tween the two languages which we refer to as the 
?isomorphism assumption?. 
Now we examine the isomorphism assumptions 
in S-CFG and S-TAG: 
y For S-CFG, the substitutions for all the non-
terminals need to be synchronous. Hence the 
isomorphism assumption for S-CFG is isomor-
phic phrasal structure. 
y For S-TAG, all the substitution and adjunction 
operations need to be synchronous, and the 
derivation trees of both languages are isomor-
phic. The derivation tree for TAG is roughly 
equivalent to a dependency tree. Hence the 
isomorphism assumption for S-TAG is an iso-
morphic dependency structure. 
As shown by real translation tasks, both of those 
assumptions would fail due to structural divergences 
between languages. 
On the other hand SDIG does NOT assume word 
level isomorphism or isomorphic dependency trees. 
Since in the SDIG sense, the parallel dependency 
trees are in fact the ?derived? form rather than the 
?derivation? form. In other words, SDIG assumes 
the isomorphism lies deeper than the dependency 
structure. It is ?the derivation tree of DIG? that is 
isomorphic. 
The following ?pseudo-translation? example il-
lustrates how SDIG captures structural divergence 
between the languages. Suppose we want to translate: 
y [Source] The girl kissed her kitty cat. 
y [Target] The girl gave a kiss to her cat. 
 
 
Figure 11 
Note that both S-CFG and S-TAG won?t be able 
to handle such structural divergence. However, 
when we view each of the two sentences as derived 
from three elementary trees in DIG, we can have a 
synchronous derivation, as shown below: 
 6 The Probabilistic Extension to SDIG and 
Statistical MT 
The major reason to construct an SDIG is to have 
a generative model for syntax based statistical MT. 
By relying on the assumption that the derivation tree 
of DIG represents the probability dependency graph, 
we can build a graphical model which captures the 
following two statistical dependencies: 
1. Probabilities of Elementary Tree unification (in 
the target language) 
2. Probabilities of Elementary Tree transfer (be-
tween languages), i.e. the probability of two 
elementary trees being paired 
ET-f3
ET-f1
ET-f2
ET-f4
ET-e3
ET-e1
ET-e2
ET-e4  
Figure 12 
The above graph shows two isomorphic deriva-
tion trees for two languages. ET stands for elemen-
tary trees and dotted arcs denote the conditional 
dependence assumptions). Under the above model, 
the best translation is: )()|(maxarg* ePefPe
e
= ; 
And ?=
i
ii eETfETPefP ))(|)(()|( ; also we 
have ( )?=
i
ii eETParenteETPeP ))((|)()( . 
Hence, we can have PSDIG (probabilistic syn-
chronous Dependency Insertion Grammar). Given 
the dynamic programming property of the above 
graphical model, an efficient polynomial time 
Viterbi decoding algorithm can be constructed. 
7 Current Implementation 
To test our idea, we implemented the above syn-
chronous grammar formalism in a Chinese-English 
machine translation system. The actual implementa-
tion of the synchronous grammar used in the system 
is a scaled-down version of the SDIG introduced 
above, where all the word categories are treated as 
one. The reason for this simplification is that word 
category mappings across languages are not straight-
forward. Defining the word categories so that they 
can be consistent between the languages is a major 
goal for our future research. 
The uni-category version of the SDIG is induced 
using the algorithm in (Ding and Palmer, 2004), 
which is a statistical approach to extracting parallel 
dependency structures from large scale parallel cor-
pora. An example is given in Figure 12. We can 
construct the parallel dependency trees as shown in 
Figure 13a. The expected output of the above ap-
proach is shown in Figure 13b. (e) stands for an 
empty node trace. 
y [English]  I have been here since 1947. 
y [Chinese] Wo 1947  nian   yilai   yizhi     zhu   zai  zheli. 
              I             year   since  always  live   in     here 
 
Figure 
13a.  
Input 
 
Figure 13b. Output 
(5 parallel elementary tree pairs) 
We build a decoder for the model in Section 6 for 
our machine translation system. The decoder is 
based on a polynomial time decoding algorithm for 
fast non-isomorphic tree-to-tree transduction (Un-
published by the time of this paper). 
We use an automatic syntactic parser (Collins, 
1999; Bikel, 2002) to produce the parallel unaligned 
syntactic structures. The parser was trained using the 
Penn English/Chinese Treebanks. We then used the 
algorithm in (Xia 2001) to convert the phrasal struc-
ture trees into dependency trees. 
The following table shows the statistics of the 
datasets we used. (Genre, number of sentence pairs, 
number of Chinese/English words, type and usage). 
Dataset Xinhua FBIS NIST 
Genre News News News 
Sent# 56263 21003 206 
Chn W# 1456495 522953 26.3 average 
Eng W# 1490498 658478 32.5 average 
Type unaligned unaligned multi-reference
Usage training training testing 
Figure 14 
The training set consists of Xinhua newswire 
data from LDC and the FBIS data. We filtered both 
datasets to ensure parallel sentence pair quality. We 
used the development test data from the 2001 NIST 
MT evaluation workshop as our test data for the MT 
system performance. In the testing data, each input 
Chinese sentence has 4 English translations as refer-
ences, so that the result of the MT system can be 
evaluated using Bleu and NIST machine translation 
evaluation software. 
  1-gram 2-gram 3-gram 4-gram
NIST: 4.3753 4.9773 5.0579 5.0791
BLEU: 0.5926 0.3417 0.2060 0.1353
Figure 15 
The above table shows the cumulative Bleu and 
NIST n-gram scores for our current implementation; 
with the final Bleu score 0.1353 with average input 
sentence length of 26.3 words.  
In comparison, in (Yamada and Knight, 2002), 
which was a phrasal structure based statistical MT 
system for Chinese to English translation, the Bleu 
score reported for short sentences (less than 14 
words) is 0.099 to 0.102.  
Please note that the Bleu/NIST scorers, while 
based on n-gram matching, do not model syntax dur-
ing evaluation, which means a direct comparison 
between a syntax based MT system and a string 
based statistical MT system using the above scorer 
would favor the string based systems. 
We believe that our results can be improved us-
ing a more sophisticated machine translation pipe-
line which has separate components that handle 
specific language phenomena such as named entities. 
Larger training corpora can also be helpful. 
8 Conclusion 
Finally, let us review whether the proposed SDIG 
formalism has achieved the goals we setup in Sec-
tion 1 of this paper for a grammar formalism for Sta-
tistical MT applications: 
1. Linguistically motivated: DIG captures word-
order phenomena within the CFG domain. 
2. SDIG dropped the unrealistic word-to-word 
isomorphism assumption and is able to capture 
structural divergences. 
3. DIG is weakly equivalent to CFG. 
4. DIG and SDIG are generative grammars. 
5. They have both simple formalisms, only one 
type of node, and one type of operation. 
9 Future Work 
We observe from our testing results that the cur-
rent simplified uni-category version of SDIG suffers 
from various grammatical errors, both in grammar 
induction and decoding, therefore our future work 
should focus on word category consistency between 
the languages so that a full-fledged version of SDIG 
can be used.  
10 Acknowledgements 
Our thanks go Aravind Joshi, Owen Rambow, 
Dekai Wu and all the anonymous reviewers of the 
previous versions of the paper, who gave us invalu-
able advices, suggestions and feedbacks.  
 
References 
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computational 
Linguistics, 26(1): 45-60. 
Daniel M. Bikel. 2002. Design of a multi-lingual, paral-
lel-processing statistical parsing engine. In Proceedings 
of HLT 2002. 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della 
Pietra, and Robert L. Mercer. 1993. The mathematics of 
statistical machine translation: parameter estimation. 
Computational Linguistics, 19(2): 263-311. 
Michael John Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia. 
Yuan Ding and Martha Palmer. 2004. Automatic Learn-
ing of Parallel Dependency Treelet Pairs, in Proceed-
ings of The First International Joint Conference on 
Natural Language Processing (IJCNLP-04). 
Bonnie J. Dorr. 1994. Machine translation divergences: A 
formal description and proposed solution. Computa-
tional Linguistics, 20(4): 597-633. 
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP-02, pages 
304-311 
Daniel Gildea. 2003. Loosely tree based alignment for 
machine translation. In Proceedings of ACL-03 
Jan Hajic, et al 2002. Natural language generation in the 
context of machine translation. Summer workshop final 
report, Center for Language and Speech Processing, 
Johns Hopkins University, Baltimore.  
Aravind Joshi and Owen Rambow. 2003. A formalism of 
dependency grammar based on Tree Adjoining Gram-
mar. In Proceedings of the first international confer-
ence on meaning text theory (MTT 2003), June 2003. 
Aravind K. Joshi and Yves Schabes. Tree-adjoining 
grammars and lexicalized grammars. In Maurice Nivat 
and Andreas Podelski, editors, Tree Automata and Lan-
guages. Elsevier Science, 1992. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
ACL-03), pages 160-167. 
Owen Rambow and Aravind Joshi. 1997. A formal look 
at dependency grammars and phrase structures. In Leo 
Wanner, editor, Recent Trends in Meaning-Text Theory, 
pages 167-190.  
S. M. Shieber and Y. Schabes. 1990. Synchronous Tree-
Adjoining Grammars, Proceedings of the 13th COLING, 
pp. 253-258, August 1990. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):3-403. 
Fei Xia. 2001. Automatic grammar generation from two 
different perspectives. Ph.D. thesis, University of Penn-
sylvania, Philadelphia. 
Kenji Yamada and Kevin Knight. 2001. A syntax based 
statistical translation model. In Proceedings of ACL-01 
Kenji Yamada and Kevin Knight. 2002. A decoder for 
syntax-based statistical MT. In Proceedings of ACL-02 
