Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 88?95,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
 Clustering Technique in Multi-Document Personal Name Disambigu-
ation 
 
 
Chen Chen 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China 
chenchen@pku.edu.cn 
Hu Junfeng 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China
hujf@pku.edu.cn 
Wang Houfeng 
Key Laboratory of Computa-
tional Linguistics (Peking 
University), 
Ministry of Education, China
wanghf@pku.edu.cn 
 
  
Abstract 
 
Focusing on multi-document personal name 
disambiguation, this paper develops an agglo-
merative clustering approach to resolving this 
problem. We start from an analysis of point-
wise mutual information between feature and 
the ambiguous name, which brings about a 
novel weight computing method for feature in 
clustering. Then a trade-off measure between 
within-cluster compactness and among-cluster 
separation is proposed for stopping clustering. 
After that, we apply a labeling method to find 
representative feature for each cluster.  Finally, 
experiments are conducted on word-based 
clustering in Chinese dataset and the result 
shows a good effect. 
1 Introduction 
Multi-document named entity co-reference reso-
lution is the process of determining whether an 
identical name occurring in different texts refers 
to the same entity in the real world. With the rap-
id development of multi-document applications 
like multi-document summarization and informa-
tion fusion, there is an increasing need for multi-
document named entity co-reference resolution. 
This paper focuses on multi-document personal 
name disambiguation, which seeks to determine 
if the same name from different documents refers 
to the same person. 
This paper develops an agglomerative cluster-
ing approach to resolving multi-document per-
sonal name disambiguation. In order to represent 
texts better, a novel weight computing method 
for clustering features is presented. It is based on 
the pointwise mutual information between the 
ambiguous name and features. This paper also 
develops a trade-off point based cluster-stopping 
measure and a labeling algorithm for each clus-
ters. Finally, experiments are conducted on 
word-based clustering in Chinese dataset. The 
dataset contains eleven different personal names 
with varying-sized datasets, and has 1669 texts in 
all. 
The rest of this paper is organized as follows: 
in Section 2 we review the related work; Section 
3 describes the framework; section 4 introduces 
our methodologies including feature weight 
computing with pointwise mutual information, 
cluster-stopping measure based on trade-off 
point, and cluster labeling algorithm. These are 
the main contribution of this paper; Section 5 
discusses our experimental result. Finally, the 
conclusion and suggestions for further extension 
of the work are given in Section 6. 
2 Related Work 
Due to the varying ambiguity of personal names 
in a corpus, existing approaches typically cast it 
as an unsupervised clustering problem based on 
vector space model. The main difference among 
these approaches lies in the features, which are 
used to create a similarity space. Bagga & Bald-
win (1998) first performed within-document co-
reference resolution, and then explored features 
in local context. Mann & Yarowsky (2003) ex-
tracted local biographical information as features. 
Al-Kamha and Embley (2004) clustered search 
results with feature set including attributes, links 
and page similarities. Chen and Martin (2007) 
explored the use of a range of syntactic and se-
mantic features in unsupervised clustering of 
documents. Song (2007) learned the PLSA and 
LDA model as feature sets. Ono et al (2008) 
used mixture features including co-occurrences 
88
of named entities, key compound words, and top-
ic information. Previous works usually focus on 
feature identification and feature selection. The 
method to assign appropriate weight to each fea-
ture has not been discussed widely.  
A major challenge in clustering analysis is de-
termining the number of ?clusters?. Therefore, 
clustering based approaches to this problem still 
require estimating the number of clusters. In Hie-
rarchy clustering, it equates to determine the 
stopping step of clustering. The measure to find 
the ?knee? in the criterion function curve is a 
well known cluster-stopping measure. Pedersen 
and Kulkarni had studied this problem (Pedersen 
and Kulkarni, 2006). They developed cluster-
stopping measures named PK1, PK2, PK3, and 
presented the Adapted Gap Statistics.  
After estimating the number of ?clusters?, we 
obtain the clustering result. In order to label the 
?clusters?, the method that finding representative 
features for each ?cluster? is needed. For example, 
the captain John Smith can be labeled as captain. 
Pedersen and Kulkarni (2006) selected the top N 
non-stopping word features from texts grouped 
in a cluster as label. 
3 Framework 
On the assumption of ?one person per document? 
(i.e. all mentions of an ambiguous personal name 
in one document refer to the same personal enti-
ty), the task of disambiguating personal name in 
text set intends to partition the set into subsets, 
where each subset refer to one particular entity. 
Suppose the set of texts containing the ambi-
guous name is denoted by D= {d1,d2,?,dn}, and  
di (0<i<n+1) stands for one text. The entities 
with the ambiguous name are denoted by a set 
E= {e1,e2,?,em}, where the number of entities ?m? 
is unknown. The ambiguous name in each text di 
indicates only one entity ek. The aim of the work 
is to map an ambiguous name appearing in each 
text to an entity. Therefore, those texts indicating 
the same entity need to be clustered together. 
In determining whether a personal name refers 
to a specific entity, the personal information, so-
cial network information and related topics play 
important roles,  all of which are expressed by 
words in texts,. Extracting words as features, this 
paper applies an agglomerative clustering ap-
proach to resolving name co-reference. The 
framework of our approach consists of the fol-
lowing seven main steps: 
 
Step 1: Pre-process each text with Chinese 
word segmentation tool; 
Step 2: Extract words as features from the 
set of texts D;. 
Step 3: Represent texts d1,?,dn by features 
vectors; 
Step 4: Calculate similarity between texts; 
Step 5: Cluster the set D step by step until 
only one cluster exists;  
Step 6: Estimate the number of entities in 
accordance with cluster-stopping 
measure; 
Step 7: Assign each cluster a discriminating 
label. 
 
This paper focuses on the Step 4, Step 6 and 
Step 7, i.e., feature weight computing method, 
clustering stopping measure and cluster labeling 
method. They will be described in the next sec-
tion in detail.  
Step1 and Step3 are simple, and there is no 
further description here. In Step 2, we use co-
occurrence words of the ambiguous name in 
texts as features. In the process of agglomerative 
clustering (see Step 5), each text is viewed as one 
cluster at first, and the most similar two clusters 
are merged together as a new cluster at each 
round. After replacing the former two clusters 
with the new one, we use average linked method 
to update similarity between clusters. 
4 Methodology  
4.1 Feature weight  
Each text is represented as a feature vector, and 
each item of the vector represents the weight 
value for corresponding feature in the text. Since 
our approach is completely unsupervised we 
cannot use supervised methods to select 
significant features. Since the weight of feature 
will be adjusted well instead of feature selection, 
all words in set D are used as feature in our 
approach. 
The problem of computing feature weight is 
involved in both text clustering and text classifi-
cation. By comparing the supervised text classi-
fication and unsupervised text clustering, we find 
that the former one has a better performance ow-
ing to the selection of features and the computing 
method of feature weight. Firstly, in the applica-
tion of supervised text classification, features can 
be selected by many methods, such as, Mutual 
Information (MI) and Expected Cross Entropy 
(ECE) feature selection methods. Secondly, 
model training methods, such as SVM model, are 
generally adopted by programs when to find the 
89
optimal feature weight. There is no training data 
for unsupervised tasks, so above-mentioned me-
thods are unsuitable for text clustering. 
In addition, we find that the text clustering for 
personal name disambiguation is different from 
common text clustering. System can easily judge 
whether a text contains the ambiguous personal 
name or not. Thus the whole collection of texts 
can be easily divided into two classes: texts  with 
or without the name. As a result, we can easily 
calculate the pointwise mutual information 
between feature words and the personal name. 
To a certain extent, it represents the correlative 
degree between feature words and the underlying 
entity corresponding to the personal name. 
For these reasons, our feature weight 
computing method calculates the pointwise 
mutual information between personal name and 
feature word. And the value of pointwise mutual 
information will be used to expresse feature 
word?s weight by combining the feature?s tf (the 
abbreviation for term-frequency) in text and idf 
(the abbreviation for inverse document frequency) 
in dataset. The formula of feature weight compu-
ting proposed in this paper is as below, and it is 
need both texts containing and not containing the 
ambiguous personal name to form dataset D. For 
each tk in di that contains name, its mi_weight is 
computed as follow: 
))(||log()),MI(1log(
))),(log(1(),,_weight(mi
kk
ikik
tdfDnamet
dttfdnamet
?+?
+=
 
(1) 
And 
)()(
||),(
||/)()(
||/),(
)()(
),(
),MI(
2
k
k
k
k
k
k
k
tdfnamedf
Dtnamedf
Dtdfnamedf
Dtnamedf
tpnamep
tnamep
namet
?
?=
?=
?=
     (2) 
 Where tk is a feature; name is the ambiguous 
name; di is the ith text in dataset; tf(tk,di) 
represents term frequency of feature tk in text di; 
df(tk), df(name) is the number of the texts con-
taining tk or name in dataset D respectively; 
df(tk,name) is the number of texts containing both 
tk and name; |D| is the number of all the texts.  
Formula (2) can be comprehended as: if word 
tk occurs much more times in texts containing the 
ambiguous name than in texts not containing the 
name, it must have some information about the 
name. 
 A widely used approach for computing feature 
weight is tf*idf scheme as formula (3) (Salton 
and Buckley. 1998), which only uses the texts 
containing the ambiguous name. We denote it by 
old_weight . For each tk in di containing name, 
the old_weight is computed as follow: 
)),()(log(
))),(log(1(
),,(old_weight
nametdfnamedf
dttf
dnamet
k
ik
ik
?
+=               (3) 
The first term on the right side is tf, and the 
second term is idf. If the idf scheme is computed 
in the whole dataset D for reducing noise, the 
weight computing formula can be expressed as 
follow, and is denoted by imp_weight: 
))(|D|log())),(log(1(
),_weight(imp
kik
ik
tdfdttf
dt
?+= (4) 
Before clustering, the similarity between texts 
is computed by cosine value of the angle 
between vectors (such as dx, dy in formula (5)):     
yx
yx
yx dd
dd
d,d ?
?=)cos(                               (5) 
Each item of the vector (i.e. dx, dy) represents 
the weight value for corresponding feature in the 
text. 
4.2 Cluster-stopping measure 
The process of clustering will produce n cluster 
results, one for each step. Independent of 
clustering algorithm, the cluster stopping meas-
ure should choose the cluster results which can 
represent the structure of data. 
A fundamental and difficult problem in cluster 
analysis is to measure the structure of clustering 
result. The geometric structure is a representative 
method. It defines that a ?good? clustering re-
sults should make data points from one cluster 
?compact?, while data points from different clus-
ter are ?separate? as far as possible. The indica-
tors should quantify the ?compactness? and ?se-
paration? for clusters, and combine both.  In the 
study of cluster stopping measures by Pedersen 
and Kulkarni (2006), the criterion functions de-
fines text similarity based on cosine value of the 
angle between vectors. Their cluster-stopping 
measures focused on finding the ?knee? of crite-
rion function.  
Our cluster-stopping measure is also based on 
the geometric structure of dataset. The measure 
aims to find the trade-off point between within-
cluster compactness and among-cluster 
separation. Both the within-cluster compactness 
(Internal critical function) and among-cluster 
90
separation (External critical function) are defined 
by Euclidean distance. The hybrid critical 
function (Hybrid critical function) combines 
internal and external criterion functions. 
Suppose that the given dataset contains N ref-
erences, which are denoted as: d1,d2,?,dN; the 
data have been repeatedly clustered into k clus-
ters, where k=N,?,1; and clusters are denoted as 
Cr, r=1,?k; and the number of references in 
each cluster is nr, so nr=|Cr|. We introduce Incrf 
(Internal critical function), Excrf (External 
critical function) and Hycrf (Hybrid critical 
function) to measure it as follows. 
 
??
= ?
?=
k
i
k
1
)Incrf(
iyx Cd,d
2
yx dd                  (6) 
? ? ?
= ?= ??
?=
k
i
k
ijj jinn
k
1 ,1
1
)Excrf(
jyix Cd,Cd
2
yx dd
            (7) 
))Excrf()(Incrf(
M
1
)Hycrf( kkk +?=          (8) 
Where M=Incrf(1)=Excrf(N) 
 
 
 
Figure 1 Hycrf vs. t (N-k) 
 
Chen proved the existence of the minimum 
value between (0,1) in Hycrf(k) (see Chen et al 
2008). The Hycrf value in a typical Hycrf(t) 
curve is shown as Figure 1, where t=N-k. 
Function Hycrf based on Incrf and Excrf is 
used as the Hybrid criterion function. The Hycrf 
curve will rise sharply after the minimum, indi-
cating that the cluster of several optimal parti-
tions? subsets will lead to drastic drop in cluster 
quality. Thus cluster partition can be determined. 
Using the attributes of the Hycrf(k) curve, we put 
forward a new cluster-stopping measure named 
trade-off point based cluster-stopping measure 
(TO_CSM). 
)1Hycrf(
)Hycrf(
)1Hycrf(
1
)TO_CSM( +?+= k
k
k
k  
(9) 
 
Trade-off point based cluster-stopping meas-
ure (TO_CSM) selects the k value which max-
imizes TO_CSM(k), and indicates the number of 
cluster. The first term on the right side of formu-
la (9) is used to minimize the value of Hycrf(k), 
and the second one is used to find the ?knee? ris-
ing sharply. 
4.3 Labeling 
Once the clusters are created, we label each 
entity to represent the underlying entity with 
some important information. A label is 
represented as a list of feature words, which 
summarize the information about cluster?s 
underlying entity. 
The algorithm is outlined as follows: after 
clustering N references into m clusters, for each 
cluster Ck in {C1, C2, ?, Cm}, we calculate the 
score of each feature for Ck and choose features 
as the label of Ck whose scores rank top N. In 
particular, the score caculated in this paper is 
different from Pedersen and Kulkarni?s (2006). 
We combine pointwise mutual information 
computing method with term frequency in cluster 
to compute the score.  
The formula of feature scoring for labeling is 
shown as follows: 
 
))),(log(1(
),(MI),MI(),Score( name
ik
ikkik
Cttf
CtnametCt
+?
?=
 
(10) 
The calculation of MI(tk,name) is shown as 
formula (2) in subsection 4.1. tf(tk,Ci) represents 
the total occurrence frequency of feature tk in 
cluster Ci . The MIname(tk,Ci) is computed as for-
mula (11): 
)()(
||),(
||/)()(
||/),(
)()(
),(
)C,(MI
2
ik
ik
ik
ik
ik
ik
ikname
Cdftdf
DCtdf
DCdftdf
DCtdf
Cptp
Ctp
t
?
?=
?=
?=
 
 (11) 
In formula (10), the weight of stopping words 
can be reduced by the first item. The second item 
can increase the weight of words with high dis-
tinguishing ability for a certain ambiguous name. 
The third item of formula (10) gives higher 
scores to features whose frequency are higher.  
0
0.5
1
1.5
1 8 15 22 29 36 43 50 57 64 71 78 85 92 99 10
6
11
3
12
0
Hycrf(t)
91
5 Experiment  
5.1 Data 
The dataset is from WWW, and contains 1,669 
texts with eleven real ambiguous personal names. 
Such raw texts containing ambiguous names are 
collected via search engine1, and most of them 
are news. The eleven person-names are, "??? 
Liu-Yi-si ?Lewis?", "??? Liu-Shu-zhen ", "?
? Li-Qiang", "?? Li-Na", "??? Li-Gui-
ying", "??? Mi-xie-er ?Michelle?", "?? 
Ma-Li ?Mary?", "???  Yue-han-xun ?John-
son?", "?? Wang-Tao", "?? Wang-Gang", "
??? Chen-Zhi-qiang". Names like ?Michelle?, 
?Johnson? are transliterated from English to Chi-
nese, while names like ?Liu ?Shu-zhen?, ?Chen-
Zhi-qiang? are original Chinese personal names. 
Some of these names only have a few persons, 
while others have more persons.  
Table 1 shows our data set. ?#text? presents 
the number of texts with the personal name. 
?#per? presents the number of entities with the 
personal name in text dataset. ?#max? presents 
the maximum of texts for an entity with the per-
sonal name, and ?#min? presents the minimum. 
 
 #text #per #max #min
Lewis 120 6 25 10 
Liu-Shu-zhen 149 15 28 3 
Li-Qiang 122 7 25 9 
Li-Na 149 5 39 21 
Li-Gui-ying 150 7 30 10 
Michelle 144 7 25 12 
Mary 127 7 35 10 
Johnson 279 19 26 1 
Wang-Gang 125 18 26 1 
Wang-Tao 182 10 38 5 
Chen-Zhi-qiang 122 4 52 13 
 
Table 1 Statistics of the test dataset 
 
We first convert all the downloaded docu-
ments into plain text format to facilitate the test 
process, and pre-process them by using the seg-
mentation toolkit ICTCLAS2. 
In testing and evaluating, we adopt B-Cubed 
definition for Precision, Recall and F-Measure 
as indicators (Bagga, Amit and Baldwin. 1998). 
F-Measure is the harmonic mean of Precision 
and Recall. 
The definitions are presented as below: 
                                                 
1 April.2008 
2 http://ictclas.org/ 
? ?= Dd dprecisionNprecision 1              (12) 
? ?= Dd drecallNrecall 1                           (13) 
recallprecision
recallprecision
measureF +
??=? 2      (14) 
where precisiond is the precision for a text d. 
Suppose the text d is in subset A, precisiond is 
the percentage of texts in A which indicates the 
same entity as d. Recalld is the recall ratio for a 
text d. Recalld is the ratio of number of texts 
which indicates the same entity as d in A to that 
in corpus D. n = | D |, D refers to a collection of 
texts containing a particular name (such as Wang 
Tao, e.g. a set of 200 texts, n = 200). Subset A is 
a set formed after clustering (text included in 
class), and d refers to a certain text that contain-
ing "Wang Tao". 
5.2 Result 
All the 1669 texts in the dataset are employed 
during experiment. Each personal name disam-
biguation process only clusters the texts contain-
ing the ambiguous name. After pre-processing, in 
order to verify the mi_weight method for feature 
weight computing, all the words in texts are used 
as features.   
Using formula (1), (3) and (4) as feature 
weight computing formula, we can get the evalu-
ation of cluster result shown as table 2. In this 
step, cluster-stopping measure is not used. In-
stead, the highest F-measure during clustering is 
highlighted to represent the efficiency of the fea-
ture weight computing method.  
Further more, we carry out the experiment on 
the trade-off point based cluster-stopping 
measure, and compare its cluster result with 
highest F-measure and cluster result determined 
by cluster-stopping measure PK3 proposed by 
Pedersen and Kulkarni?s. Based on the 
experiment in Table 2, a structure tree is 
constructed in the clustering process. Cluster-
stopping measures are used to determine where 
to stop cutting the dendrogram. As shown in 
Table 3, the TO-CMS method predicts the 
optimal results of four names in eleven, while 
PK3 method predicts the optimal result of one 
name, which are marked in a bold type. 
 
92
 
 old_weight imp_weight mi_weight 
#pre #rec #F #pre #rec #F #pre #rec #F 
Lewis 0.9488 0.8668. 0.9059 1 1 1 1 1 1 
Liu-Shu-zhen 0.8004 0.7381 0.7680 0.8409 0.8004 0.8201 0.9217 0.7940 0.8531
Li-Qiang 0.8057 0.6886 0.7426 0.9412 0.7968 0.8630 0.8962 0.8208 0.8569
Li-Na 0.9487 0.7719 0.8512 0.9870 0.8865 0.9340 0.9870 0.9870 0.9870
Li-Gui-ying 0.8871 0.9124 0.8996 0.9879 0.8938 0.9385 0.9778 0.8813 0.9271
Michelle 0.9769 0.7205 0.8293 0.9549 0.8146 0.8792 0.9672 0.9498 0.9584
Mary 0.9520 0.6828 0.7953 1 0.9290 0.9632 1 0.9001 0.9474
Johnson 0.9620 0.8120 0.8807 0.9573 0.8083 0.8765 0.9593 0.8595 0.9067
Wang-Gang 0.8130 0.8171 0.8150 0.7804 0.9326 0.8498 0.8143 0.9185 0.8633
Wang-Tao 1 0.9323 0.9650 0.9573 0.9485 0.9529 0.9897 0.9768 0.9832
Chen-Zhi-qiang 0.9732 0.8401 0.9017 0.9891 0.9403 0.9641 0.9891 0.9564 0.9725
Average 0.9153 0.7916 0.8504 0.9451 0.8864 0.9128 0.9548 0.9131 0.9323
 
Table 2 comparison of feature weight computing method (highest F-measure)
 
 Optimal TO-CMS PK3 
#pre #rec #F #pre #rec #F #pre #rec #F 
Lewis 1 1 1 1 1 1 0.8575 1 0.9233
Liu-Shuzhen 0.9217 0.7940 0.8531 0.8466 0.8433 0.8450 0.5451 0.9503 0.6928
Li-Qiang 0.8962 0.8208 0.8569 0.8962 0.8208 0.8569 0.7897 0.9335 0.8556
Li-Na 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9016 0.9424
Li-Gui-ying 0.9778 0.8813 0.9271 0.9778 0.8813 0.9271 0.8750 0.9427 0.9076
Michelle 0.9672 0.9498 0.9584 0.9482 0.9498 0.9490 0.9672 0.9498 0.9584
Mary 1 0.9001 0.9474 0.8545 0.9410 0.8957 0.8698 0.9410 0.9040
Johnson 0.9593 0.8595 0.9067 0.9524 0.8648 0.9066 0.2423 0.9802 0.3885
Wang-Gang 0.8143 0.9185 0.8633 0.9255 0.7102 0.8036 0.5198 0.9550 0.6732
Wang-Tao 0.9897 0.9768 0.9832 0.8594 0.9767 0.9144 0.9700 0.9768 0.9734
Chen-Zhi-qiang 0.9891 0.9564 0.9725 0.8498 1 0.9188 0.8499 1 0.9188
Average 0.9548 0.9131 0.9323 0.9179 0.9068 0.9095 0.7703 0.9574 0.8307
 
Table 3 comparison of cluster-stopping measures? performance
name Entity Created Labels 
Lewis Person-1 ???(Babbitt),???????(Sinclair Lewis),?????(Arrow smith),?
??(Literature Prize),???(Dresser),????(Howells),?????
(Swedish Academy),???????(Sherwood Anderson),???????
(Elmer  Gan Hartley),??(street),??(award),????????(American 
Literature and Arts Association) 
Person-2 ????(Bank of America),????(Bank of America),??(bank),???
(investors),???(credit card),??(Bank of China),??(Citibank),??
(mergers and acquisitions),??(Construction Bank),???(executive officer),
???(banking),??(stock),?????(Ken Lewis) 
Person-3 ??(Single),???(Liana),??(album),???(Liana),???????(Liana 
Lewis),???(Liana),??(airborne),??(sales),???(Music Awards),??
????(Maria Kelly),?(List),??(debut)? 
Person-4 ??????(Carl Lewis),??(long jump),??(Carl),???(Owens),??
(track and field),???(Burrell),?????(the U.S. Olympic Committee),?
?(sprint),???(Taylors),?????(Belgrade),??????(Verde Exxon),
???(Exxon) 
93
Person-5 ??(Tyson),??(King of Boxer),??(knock down),???(heavyweight),?
?(Don King),??(boxing),??(belt),??(Boxing),?(fist),??(bout),??
(Ring),WBC 
Person-6 ???(Daniel),?????(Day Lewis),??(Blood),?????????(Daniel 
Day Lewis),??(There Will Be Blood),??(left crus),??(movie king),??
?????(New York Film Critics Circles),???(the Gold Oscar statues),?
??(Best Actor in a Leading Role),???(Oscar),????(There Will Be 
Blood) 
 
Table 4  Labels for ?Lewis? clusters 
 
On the basis of text clustering result that 
obtained from the Trade-off based cluster-
stopping measure experiment in Table 3, we try 
our labelling method mentioned in subsection 4.3. 
For each cluster, we choose 12 words with 
highest score as its label. The experiment result 
demonstrates that the created label is able to 
represent the category. Take name ???? Liu-
Yi-si ?Lewis?? for example, the labeling result 
shown as Table 4.  
 
5.3 Discussion  
From the test result in table 2, we find that our 
feature weight computing method can improve 
the Chinese personal name clustering disambigu-
ation performance effectively. For each personal 
name in test dataset, the performance is im-
proved obviously. The average value of optimal 
F-measures for eleven names rises from 85.04% 
to 91.28% by using the whole dataset D for cal-
culated idf, and rises from 91.28% to 93.23% by 
using mi_weight. Therefore, in the application of 
Chinese text clustering with constraints, we can 
compute pointwise mutual information between 
constraints and feature, and it can be merged 
with feature weight value to improve the cluster-
ing performance.  
We can see from table 3 that trade-off point 
based cluster-stopping measure (TO_CSM) per-
forms much better than PK3. According to the 
experimental results, PK3 measure is not that 
robust. The optimal number of clusters can be 
determined for certain data. However, we found 
that it did not apply to all cases. For example, it 
obtains the optimal estimation result for data 
?Michelle?, as for ?Liu Shuzhen?, ?Wang Gang? 
and ?Johnson?, the results are extremely bad. 
The better result is achieved by using TO_CSM 
measure, and the selected results are closer to the 
optimal value. The PK3 measure uses the mean 
and the standard deviation to deduce, and its 
processes are more complicated than TO_CSM?s.  
Our cluster labeling method computes the fea-
tures? score with formula (10). From the labeling 
results sample shown in Table 4, we can see that 
all of the labels are representative. Most of them 
are person and organizations? name, and the rest 
are key compound words. Therefore, when the 
clustering performance is good, the quality of 
cluster labels created by our method is also good. 
6 Future Work 
This paper developed a clustering algorithm of 
multi-document personal name disambiguation, 
and put forward a novel feature weight compu-
ting method for vector space model. This method 
computes weight with the pointwise mutual in-
formation between the personal name and feature. 
We also study a hybrid criterion function based 
on trade-off point and put forward the trade-off 
point cluster-stopping measure. At last, we expe-
riment on our score computing method for clus-
ter labeling.  
Unsupervised personal name disambiguation 
techniques can be extended to address the prob-
lem of unsupervised Entity Resolution and unsu-
pervised word sense discrimination. We will at-
tempt to apply the feature weight computing me-
thod to these fields. 
One of the main directions of our future work 
will be how to improve the performance of per-
sonal name disambiguation. Computing weight 
based on a window around names may be helpful. 
Moreover, word-based text features haven?t 
solved two difficult problems of natural language 
problems: Synonym and Polysemy, which se-
riously affect the precision and efficiency of 
clustering algorithms. Text representation based 
on concept and topic may solve the problem.  
 
Acknowledgments 
This research is supported by National Natural 
Science Foundation of Chinese (No.60675035) 
and Beijing Natural Science Foundation 
(No.4072012) 
94
References  
Al-Kamha. R. and D. W. Embley. 2004. Grouping 
search-engine returned citations for person-name 
queries. In Proceedings of WIDM?04, 96-103, 
Washington, DC, USA. 
Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space 
model. In Proceedings of 17th International Con-
ference on Computational Linguistics, 79?85. 
Bagga, Amit and B. Baldwin. 1998. Algorithms for 
scoring co-reference chains. In Proceedings of the 
First International Conference on Language Re-
sources and Evaluation Workshop on Linguistic 
co-reference. 
Chen Ying and James Martin. 2007. Towards Robust 
Unsupervised Personal Name Disambiguation, 
EMNLP 2007. 
Chen Lifei, Jiang Qingshan, and Wang Shengrui. 
2008. A Hierarchical Method for Determining the 
Number of Clusters. Journal of Software, 19(1). [in 
Chinese] 
Chung Heong Gooi and James Allan. 2004. Cross-
document co-reference on a large scale corpus. In S. 
Dumais, D. Marcu, and S. Roukos, editors, HLT-
NAACL 2004: Main Proceedings, 9?16, Boston, 
Massachusetts, USA, May 2 - May 7 2004. Asso-
ciation for Computational Linguistics. 
Gao Huixian. Applied Multivariate Statistical Analy-
sis. Peking Univ. Press. 2004. 
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information 
Processing and Management, 
Kulkarni Anagha and Ted Pedersen. 2006. How Many 
Different ?John Smiths?, and Who are They? In 
Proceedings of the Student Abstract and Poster 
Session of the 21st National Conference on Artifi-
cial Intelligence, Boston, Massachusetts. 
Mann G. and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In W. Daelemans and 
M. Osborne, editors, Proceedings of CoNLL-2003, 
33?40, Edmonton, Canada. 
Niu Cheng, Wei Li, and Rohini K. Srihari. 2004.  
Weakly Supervised Learning for Cross-document 
Person Name Disambiguation Supported by Infor-
mation Extraction. In Proceedings of ACL 2004. 
Ono. Shingo, Issei Sato, Minoru Yoshida, and Hiroshi 
Nakagawa2. 2008. Person Name Disambiguation 
in Web Pages Using Social Network, Compound 
Words and Latent Topics. T. Washio et al (Eds.): 
PAKDD 2008, LNAI 5012, 260?271. 
Song Yang, Jian Huang, Isaac G. Councill, Jia Li, and 
C. Lee Giles. 2007. Efficient Topic-based Unsu-
pervised Name Disambiguation. JCDL?07, June 
18?23, 2007, Vancouver, British Columbia, Cana-
da. 
Ted Pedersen and Kulkarni Anagha. 2006. Automatic 
Cluster Stopping with Criterion Functions and the 
Gap Statistic. In Proceedings of the Demonstration 
Session of the Human Language Technology Con-
ference and the Sixth Annual Meeting of the North 
American Chapter of the Association for Computa-
tional Linguistic, New York City, NY. 
 
95
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1360?1365,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Chinese Zero Pronoun Resolution: Some Recent Advances
Chen Chen and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{yzcchen,vince}@hlt.utdallas.edu
Abstract
We extend Zhao and Ng's (2007) Chinese
anaphoric zero pronoun resolver by (1) using
a richer set of features and (2) exploiting the
coreference links between zero pronouns dur-
ing resolution. Results on OntoNotes show
that our approach significantly outperforms
two state-of-the-art anaphoric zero pronoun re-
solvers. To our knowledge, this is the first
work to report results obtained by an end-to-
end Chinese zero pronoun resolver.
1 Introduction
A zero pronoun (ZP) is a gap in a sentence that is
found when a phonetically null form is used to refer
to a real-world entity. An anaphoric zero pronoun
(AZP) is a ZP that corefers with one or more preced-
ing noun phrases (NPs) in the associated text. Un-
like overt pronouns, ZPs lack grammatical attributes
that are useful for overt pronoun resolution such as
number and gender. This makes ZP resolution more
challenging than overt pronoun resolution.
We aim to improve the state of the art in Chinese
AZP resolution by proposing two extensions. First,
while previous approaches to this task have primarily
focused on employing positional and syntactic fea-
tures (e.g., Zhao and Ng (2007) [Z&N], Kong and
Zhou (2010) [K&Z]), we exploit a richer set of fea-
tures for capturing the context of an AZP and its
candidate antecedents. Second, to alleviate the diffi-
culty of resolving an AZP to an antecedent far away
from it, we break down the process into smaller, in-
termediate steps, where we allow coreference links
between AZPs to be established.
We apply our two extensions to a state-of-the-art
Chinese AZP resolver proposed by Z&N and eval-
uate the resulting resolver on the OntoNotes cor-
pus. Experimental results show that this resolver sig-
nificantly outperforms both Z&N's resolver and an-
other state-of-the-art resolver proposed by K&Z. It
is worth noting that while previous work on Chinese
ZP resolution has reported results obtained via gold
information (e.g., using gold AZPs and extracting
candidate antecedents and other features from gold
syntactic parse trees), this is the first work to report
the results of an end-to-end Chinese ZP resolver.
The rest of this paper is organized as follows. Sec-
tion 2 describes the two baselineAZP resolvers. Sec-
tions 3 and 4 discuss our two extensions. We present
our evaluation results in Section 5 and our conclu-
sions in Section 6.
2 Baseline AZP Resolution Systems
An AZP resolution algorithm takes as input a set
of AZPs produced by an AZP identification system.
Below we first describe the AZP identifier we em-
ploy, followed by our two baseline AZP resolvers.
2.1 Anaphoric Zero Pronoun Identification
We employ two steps to identifyAZPs. In the extrac-
tion step, we heuristically extract candidate ZPs. In
the classification step, we train a classifier to distin-
guish AZPs from non-AZPs.
To implement the extraction step, we use Z&N's
and K&Z's observation: ZPs can only occur before a
VP node in a syntactic parse tree. However, accord-
ing to K&Z, ZPs do not need to be extracted from
every VP: if a VP node occurs in a coordinate struc-
ture or is modified by an adverbial node, then only its
parent VP node needs to be considered. We extract
ZPs from all VPs that satisfy the above constraints.
1360
Syntactic
features
(13)
whether z is the first gap in an IP clause; whether z is the first gap in a subject-less IP clause, and if
so, POS(w1); whether POS(w1) is NT; whether t1 is a verb that appears in a NP or VP; whether Pl is
a NP node; whether Pr is a VP node; the phrasal label of the parent of the node containing POS(t1);
whether V has a NP, VP or CP ancestor; whether C is a VP node; whether there is a VP node whose
parent is an IP node in the path from t1 to C.
Lexical
features
(13)
the words surrounding z and/or their POS tags, including w1, w?1, POS(w1), POS(w?1)+POS(w1),
POS(w1)+POS(w2), POS(w?2)+POS(w?1), POS(w1)+POS(w2)+POS(w3), POS(w?1)+w1, and
w?1+POS(w1); whether w1 is a transitive verb, an intransitive verb or a preposition; whether w?1 is
a transitive verb without an object.
Other fea-
tures (6)
whether z is the first gap in a sentence; whether z is in the headline of the text; the type of the clause in
which z appears; the grammatical role of z; whether w?1 is a punctuation; whether w?1 is a comma.
Table 1: Features for AZP identification. z is a zero pronoun. V is the VP node following z. wi is the ith word to the
right of z (if i is positive) or the ith word to the left of z (if i is negative). C is lowest common ancestor of w?1 and
w1. Pl and Pr are the child nodes of C that are the ancestors of w?1 and w1 respectively.
Features
between a
and z (4)
the sentence distance between a and z; the segment distance between a and z, where segments are
separated by punctuations; whether a is the closest NP to z; whether a and z are siblings in the
associated parse tree.
Features
on a (12)
whether a has an ancestor NP, and if so, whether this NP is a descendent of a's lowest ancestor IP;
whether a has an ancestor VP, and if so, whether this VP is a descendent of a's lowest ancestor IP;
whether a has an ancestor CP; the grammatical role of a; the clause type in which a appears; whether
a is an adverbial NP, a temporal NP, a pronoun or a named entity; whether a is in the headline of the
text.
Features
on z (10)
whether V has an ancestor NP, and if so, whether this NP node is a descendent of V's lowest ancestor
IP; whether V has an ancestor VP, and if so, whether this VP is a descendent of V's lowest ancestor IP;
whether V has an ancestor CP; the grammatical role of z; the type of the clause in which V appears;
whether z is the first or last ZP of the sentence; whether z is in the headline of the text.
Table 2: Features for AZP resolution in the Zhao and Ng (2007) baseline system. z is a zero pronoun. a is a candidate
antecedent of z. V is the VP node following z in the parse tree.
To implement the classification step, we train a
classifier using SVMlight (Joachims, 1999) to distin-
guishAZPs from non-AZPs. We employ 32 features,
13 of which were proposed by Z&N and 19 of which
were proposed by Yang and Xue (2010). A brief de-
scription of these features can be found in Table 1.
2.2 Two Baseline AZP Resolvers
The Zhao and Ng (2007) [Z&N] baseline. In
our implementation of the Z&N baseline, we use
SVMlight to train amention-pairmodel for determin-
ing whether an AZP z and a candidate antecedent
of z are coreferent. We consider all NPs preced-
ing z that do not have the same head as its parent
NP in the parse tree to be z's candidate antecedents.
We use Soon et al's (2001) method to create train-
ing instances: we create a positive instance between
an AZP, z, and its closest overt antecedent, and we
create a negative instance between z and each of the
intervening candidates. Each instance is represented
by the 26 features employed by Z&N. A brief de-
scription of these features can be found in Table 2.
During testing, we adopt the closest-first resolution
strategy, resolving an AZP to the closest candidate
antecedent that is classified as coreferent with it.1
The Kong and Zhou (2010) [K&Z] baseline.
K&Z employ a tree kernel-based approach to AZP
resolution. Like Z&N, K&Z (1) train a mention-
pair model for determining whether an AZP z and
a candidate antecedent of z are coreferent, (2) use
Soon et al's method to create training instances, and
(3) resolve an AZP to its closest coreferent can-
didate antecedent. Unlike Z&N, however, K&Z
use the SVMlight?TK learning algorithm (Moschitti,
1When resolving a goldAZP z, if none of the preceding can-
didate antecedents is classified as coreferent with it, we resolve
it to the candidate that has the highest coreference likelihood
with it. Here, we employ the signed distance from the SVM
hyperplane to measure the coreference likelihood.
1361
2006) to train their model, employing a parse sub-
tree known as a dynamic expansion tree (Zhou et al,
2008) as a structured feature to represent an instance.
3 Extension 1: Novel Features
We propose three kinds of features to better capture
the context of an AZP, as described below.
Antecedent compatibility. AZPs are omitted sub-
jects that precede VP nodes in a sentence's parse
tree. From the VP node, we can extract its head verb
(Predz) and the head of its object NP (Obj), if any.
Note that Predz and Obj contain important contex-
tual information for an AZP.
Next, observe that if a NP is coreferent with an
AZP, it should be able to fill the AZP's gap and be
compatible with the gap's context. Consider the fol-
lowing example:
E1: ?????????????? ?pro???
?????????????
(They are trying that service. That means ?pro?
hope that our visitors can try it when they come in
September.)
The head of the VP following ?pro? is ??
(hope). There are two candidate antecedents, ??
(They) and ???? (that service). If we try us-
ing them to fill this AZP's gap, we know based on
selectional preferences that ???? (They hope)
makes more sense than?????? (that service
hope). We supply the AZP resolver with the fol-
lowing information to help it make these decisions.
First, we find the head word of each candidate an-
tecedent, Headc. Then we form two strings, Headc
+ Predz and Headc + Predz + Obj (if the object
of the VP is present). Finally, we employ them as bi-
nary lexical features, setting their feature values to 1
if and only if they can be extracted from the instance
under consideration. The training data can be used
to determine which of these features are useful.2
Narrative event chains. A narrative event chain is
a partially ordered set of events related by a common
protagonist (Chambers and Jurafsky, 2008). For ex-
ample, we can infer from the chain "borrow-s invest-
s spend-s lend-s" that a person who borrows (pre-
2We tried to apply Kehler et al's (2004) and Yang et
al.'s (2005) methods to learn Chinese selectional preferences
from unlabeled data, but without success.
sumably money) can invest it, spend it, or lend it to
other people.3 Consider the following example:
E2: ???????pro???????????
??????
(The country gives our department money, but all
?pro? provides is exactly what we worked for.)
In E2, ?pro? is coreferent with ?? (The coun-
try), and the presence of the narrative event chain?
??? (gives?provides) suggests that the subjects
of the two events are likely to be coreferent.
However, given the unavailability of induced or
hand-crafted narrative chains in Chinese4, we make
the simplifying assumption that two verbs form a
lexical chain if they are lexically identical.5 We
create two features to exploit narrative event chains
for a candidate NP, c, if it serves as a subject or
object. Specifically, let the verb governing c be
Predc. The first feature, which encodes whether
narrative chains are present, has three possible val-
ues: 0 if Predc and Predz are not the same; 1 if
Predc and Predz are the same and c is a subject;
and 2 if Predc and Predz are the same and c is an
object. The second feature is a binary lexical fea-
ture, Predc+Predz+Subject/Object; its value is
1 if and only if Predc, Predz , and Subject/Object
can be found in the associated instance, where
Subject/Object denotes the grammatical role of c.
Final punctuation hint. We observe that the punc-
tuation (Punc) at the end of a sentence where an
AZP occurs also provides contextual information,
especially in conversation documents. In conversa-
tions, if a sentence containing an AZP ends with a
3"-s" denotes the fact that the protagonist serves as the gram-
matical subject in these events.
4We tried to construct narrative chains for Chinese using
both learning-based and dictionary-based methods. Specifi-
cally, we induced narrative chains using Chambers and Juraf-
sky's (2008) method, but were not successful owing to the lack
of an accurate Chinese coreference resolver. In addition, we
constructed narrative chains using both lexically identical verbs
and the synonyms obtained from a WordNet-like Chinese re-
source called Tongyicicilin, but they did not help improve reso-
lution performance.
5Experiments on the training data show that if an AZP and
a candidate antecedent are subjects of (different occurrences of)
the same verb, then the probability that the candidate antecedent
is coreferent with the AZP is 0.703. This result suggests that our
assumption, though somewhat simplistic, is useful as far as AZP
resolution is concerned.
1362
A:?????????
(A: How is her life now? )
B: ?pro1???????????????
(B: ?pro1? attitude toward life is plain and simple.)
A:??
(A: Yes.)
A: ?pro2???????????
(A: ?pro2? is living in Beijing or the USA?)
B: ?pro3?????
(B: ?pro3? is living in the USA.)
Figure 1: An illustrative example.
question mark, the mention this AZP refers to is less
likely to be the speaker himself6, as illustrated in the
following example:
E3: ?? ?pro????
(Are ?pro? cold in the winter?)
Here, ?pro? refers to the person the speaker talks
with. To capture this information, we create a binary
lexical feature, Headc+Punc, whose value is 1 if
and only if Headc and Punc appear in the instance
under consideration.
4 Extension 2: Zero Pronoun Links
4.1 Motivation
Like an overt pronoun, a ZP whose closest overt
antecedent is far away from it is harder to resolve
than one that has a nearby overt antecedent. How-
ever, a corpus study of our training data reveals that
only 55.2% of the AZPs appear in the same sentence
as their closest overt antecedent, and 22.7% of the
AZPs appear two or more sentences away from their
closest overt antecedent.
Fortunately, we found that some of the difficult-
to-resolve AZPs (i.e., AZPs whose closest overt an-
tecedents are far away from them) are coreferential
with nearby ZPs. Figure 1, which consists of a set of
sentences from a conversation, illustrates this phe-
nomenon. There are three AZPs (denoted by ?proi?,
where 1 ? i ? 3), all of which refer to the overt
pronoun ? (She) in the first sentence. In this ex-
ample, it is fairly easy to resolve ?pro1? correctly,
6One may wonder whether we can similarly identify con-
straints on the antecedents of a ZP from clause conjunctions.
Our preliminary analysis suggests that the answer is no.
Training Test
Documents 1,391 172
Sentences 36,487 6,083
Words 756,063 110,034
ZPs 23,065 3,658
AZPs 12,111 1,713
Table 3: Statistics on the training and test sets.
since its antecedent is the subject of previous sen-
tence. However, ?pro3? and its closest overt an-
tecedent? (She) are four sentences apart. Together
with the fact that there are many intervening candi-
date antecedents, it is not easy for a resolver to cor-
rectly resolve ?pro3?.
To facilitate the resolution of ?pro3? and difficult-
to-resolve AZPs in general, we propose the follow-
ing idea. We allow an AZP resolver to (1) establish
coreferent links between two consecutive ZPs (i.e.,
?pro1???pro2? and ?pro2???pro3? in our exam-
ple), which are presumably easy to establish because
the two AZPs involved are close to each other; and
then (2) treat them as bridges and infer that ?pro3?'s
overt antecedent is? (She).
4.2 Modified Resolution Algorithm
We implement the aforementioned idea by modify-
ing the AZP resolver as follows. Whenwe resolve an
AZP z during testing, we augment the set of candi-
date antecedents for z with the set of AZPs preceding
z. Since we have only specified how to compute fea-
tures for instances composed of an AZP and an overt
candidate antecedent thus far (see Section 2.2), the
question, then, is: how can we compute features for
instances composed of two AZPs?
To answer this question, we first note that the
AZPs in a test text are resolved in a left-to-right man-
ner. Hence, by the time we resolve an AZP z, all the
AZPs preceding z have been resolved. Hence, when
we create a test instance i between z and one of the
preceding AZPs (say y), we create i as if the gap y
was filled with the smallest tree embedding the NP
to which y was resolved.
By allowing coreference links between (presum-
ably nearby) ZPs to be established, we can reason
over the resulting coreference links, treating them as
bridges that can help us find an overt antecedent that
is far away from an AZP.
1363
Gold AZP System AZP System AZP
Gold Parse Tree Gold Parse Tree System Parse Tree
System Variation R P F R P F R P F
K&Z Baseline System 38.0 38.0 38.0 17.7 22.4 19.8 10.6 13.6 11.9
Z&N Baseline System 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4
Z&N Baseline + Contextual Features 46.2 46.2 46.2 25.2 27.5 26.3 14.4 16.1 15.2
Z&N Baseline + Zero Pronoun Links 42.7 42.7 42.7 22.5 24.6 23.5 13.2 14.8 13.9
Full System 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7
Table 4: Resolution results on the test set.
5 Evaluation
5.1 Experimental Setup
Dataset. For evaluation, we employ the portion of
theOntoNotes 4.0 corpus that was used in the official
CoNLL-2012 shared task. The shared task dataset is
composed of a training set, a development set, and
a test set. Since only the training set and the de-
velopment set are annotated with ZPs, we use the
training set for classifier training and reserve the de-
velopment set for testing purposes. Statistics on the
datasets are shown in Table 3. In these datasets, a ZP
is marked as ?pro?. We consider a ZP anaphoric if
it is coreferential with a preceding ZP or overt NP.
Evaluation measures. We express the results of
both AZP identification and AZP resolution in terms
of recall (R), precision (P) and F-score (F).
5.2 Results and Discussion
The three major columns of Table 4 show the re-
sults obtained in three settings, which differ in
terms of whether gold/system AZPs and manu-
ally/automatically constructed parse trees are used to
extract candidate antecedents and features.
In the first setting, the resolvers are provided with
gold AZPs and gold parse trees. Results are shown in
column 1. As we can see, the Z&N baseline signifi-
cantly outperforms the K&Z baseline by 3.5% in F-
score.7 Adding the contextual features, the ZP links,
and both extensions to Z&N increase its F-score sig-
nificantly by 4.7%, 1.2% and 6.2%, respectively.
In the next two settings, the resolvers operate on
the system AZPs provided by the AZP identification
component. When gold parse trees are employed,
the recall, precision and F-score of AZP identifica-
tion are 50.6%, 55.1% and 52.8% respectively. Col-
umn 2 shows the results of the resolvers obtained
7All significance tests are paired t-tests, with p < 0.05.
when these automatically identified AZPs are used.
As we can see, Z&N again significantly outperforms
K&Z by 3.5% in F-score. Adding the contextual fea-
tures, the ZP links, and both extensions to Z&N in-
crease its F-score by 3.0%, 0.2% and 3.1%, respec-
tively. The system with contextual features and the
full system both yield results that are significantly
better than those of the Z&N baseline. A closer ex-
amination of the results reveals why the ZP links are
not effective in improving performance: when em-
ploying systemAZPs, many erroneous ZP linkswere
introduced to the system.
Column 3 shows the results of the resolvers when
we employ system AZPs and the automatically gen-
erated parse trees provided by the CoNLL-2012
shared task organizers to compute candidate an-
tecedents and features. Hence, these are end-to-end
ZP resolution results. To our knowledge, these are
the first reported results on end-to-end Chinese ZP
resolution. Using automatic parse trees, the perfor-
mance on AZP identification drops to 30.8% (R),
34.4% (P) and 32.5% (F). In this setting, Z&N still
outperforms K&Z significantly, though by a smaller
margin when compared to the previous settings. In-
corporating the contextual features, the ZP links, and
both extensions increase the F-score by 1.8%, 0.5%
and 2.3%, respectively. The system with contextual
features and the full system both yield results that are
significantly better than those of the Z&N baseline.
6 Conclusions
We proposed two extensions to a state-of-the-
art Chinese AZP resolver proposed by Zhao and
Ng (2007). Experimental results on the OntoNotes
dataset showed that the resulting resolver signifi-
cantly improved both Zhao and Ng's and Kong and
Zhou's (2010) resolvers, regardless of whether gold
or system AZPs and syntactic parse trees are used.
1364
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the views or official poli-
cies, either expressed or implied, of NSF.
References
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 787--797.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and Alexan-
der Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning, pages 44--56. MIT Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. Competitive self-trained pronoun
interpretation. In Proceedings of HLT-NAACL 2004:
Short Papers, pages 33--36.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882--891.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language processing. In Proceedings
of the 11th Conference of the European Chapter of the
Association for Computational Linguistics, pages 113-
-120.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521--544.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese Treebank.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1382--
1390.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 165--172.
Shanheng Zhao and Hwee Tou Ng. 2007. Identification
and resolution of Chinese zero pronouns: A machine
learning approach. In Proceedings of the 2007 Joint
Conference on Empirical Methods on Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 541--550.
GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008.
Context-sensitive convolution tree kernel for pronoun
resolution. In Proceedings of the 3rd International
Joint Conference on Natural Language Processing,
pages 25--31.
1365
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763?774,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Chinese Zero Pronoun Resolution: An Unsupervised Probabilistic
Model Rivaling Supervised Resolvers
Chen Chen and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{yzcchen,vince}@hlt.utdallas.edu
Abstract
State-of-the-art Chinese zero pronoun res-
olution systems are supervised, thus re-
lying on training data containing manu-
ally resolved zero pronouns. To elimi-
nate the reliance on annotated data, we
present a generative model for unsuper-
vised Chinese zero pronoun resolution.
At the core of our model is a novel hy-
pothesis: a probabilistic pronoun resolver
trained on overt pronouns in an unsuper-
vised manner can be used to resolve zero
pronouns. Experiments demonstrate that
our unsupervised model rivals its state-of-
the-art supervised counterparts in perfor-
mance when resolving the Chinese zero
pronouns in the OntoNotes corpus.
1 Introduction
A zero pronoun (ZP) is a gap in a sentence that
is found when a phonetically null form is used to
refer to a real-world entity. An anaphoric zero
pronoun (AZP) is a ZP that corefers with one or
more preceding noun phrases (NPs) in the asso-
ciated text. Below is an example taken from the
Chinese TreeBank (CTB), where the ZP (denoted
as *pro*) refers to??? (Russia).
[???] ???????????????
*pro*?????????????
([Russia] is a consistent supporter of Milo?evi?,
*pro* has proposed to mediate the political crisis.)
As we can see, ZPs lack grammatical attributes
that are useful for overt pronoun resolution such as
number and gender. This makes ZP resolution
more challenging than overt pronoun resolution.
Automatic ZP resolution is typically composed
of two steps. The first step, AZP identification, in-
volves extracting ZPs that are anaphoric. The sec-
ond step, AZP resolution, aims to identify an an-
tecedent of an AZP. State-of-the-art ZP resolvers
have tackled both of these steps in a supervised
manner, training a classifier for AZP identification
and another one for AZP resolution (e.g., Zhao and
Ng (2007), Chen and Ng (2013)).
In this paper, we focus on the second task, AZP
resolution, designing a model that assumes as in-
put the AZPs in a document and resolves each of
them. Note that the task of AZP resolution alone is
by no means easy: even when gold-standard AZPs
are given, state-of-the-art supervised resolvers can
only achieve an F-score of 47.7% for resolving
Chinese AZPs (Chen and Ng, 2013). For the sake
of completeness, we will evaluate our AZP resolu-
tion model using both gold-standard AZPs as well
as AZPs automatically identified by a rule-based
approach that we propose in this paper.
Our contribution lies in the proposal of the first
unsupervised probabilistic model for AZP resolu-
tion that rivals its supervised counterparts in per-
formance when evaluated on the Chinese portion
of the OntoNotes 5.0 corpus. Its main advan-
tage is that it does not require training data with
manually resolved AZPs. This, together with the
fact that its underlying generative process is not
language-dependent, enables it to be applied to
languages where such annotated data is not read-
ily available. At its core is a novel hypothesis:
we can apply a probabilistic pronoun resolution
model trained on overt pronouns in an unsuper-
vised manner to resolve zero pronouns. Moti-
vated by Cherry and Bergsma's (2005) and Char-
niak and Elsner's (2009) work on unsupervised
English pronoun resolution, we train our unsu-
pervised resolver on Chinese overt pronouns us-
ing the Expectation-Maximization (EM) algorithm
(Dempster et al., 1977).
2 Related Work
Chinese ZP resolution. Early approaches to
Chinese ZP resolution are rule-based. Con-
verse (2006) applied Hobbs' algorithm (Hobbs,
763
1978) to resolve the ZPs in the CTB documents.
Yeh and Chen (2007) hand-engineered a set of
rules for ZP resolution based on Centering The-
ory (Grosz et al., 1995).
In contrast, virtually all recent approaches to
this task are based on supervised learning. Zhao
and Ng (2007) are the first to employ a supervised
learning approach to Chinese ZP resolution. They
trained an AZP resolver by employing syntactic
and positional features in combination with a de-
cision tree learner. Unlike Zhao and Ng, Kong
and Zhou (2010) employed context-sensitive con-
volution tree kernels (Zhou et al., 2008) in their
resolver to model syntactic information. More re-
cently, we extended Zhao and Ng's feature set with
novel features that encode the context surrounding
a ZP and its candidate antecedents, and exploited
the coreference links between ZPs as bridges to
find textually distant antecedents for ZPs (Chen
and Ng, 2013).
ZP resolution for other languages. There have
been rule-based and supervised machine learning
approaches for resolving ZPs in other languages.
For example, to resolve ZPs in Spanish texts,
Ferr?ndez and Peral (2000) proposed a set of hand-
crafted rules that encode preferences for candidate
antecedents. In addition, supervised approaches
have been extensively employed to resolve ZPs
in Korean (e.g., Han (2006)), Japanese (e.g., Seki
et al. (2002), Isozaki and Hirao (2003), Iida et
al. (2006; 2007), Imamura et al. (2009), Iida and
Poesio (2011), Sasano and Kurohashi (2011)), and
Italian (e.g., Iida and Poesio (2011)).
3 Chinese Overt Pronouns
Since our approach relies heavily on Chinese
overt pronouns, in this section we introduce them
by describing their four grammatical attributes,
namely Number, Gender, Person and Ani-
macy. Number has two values, singular and
plural. Gender has three values, neuter, mascu-
line and feminine. Person has three values, first,
second and third. Finally, Animacy has two val-
ues, animate and inanimate.
We exploit ten personal pronouns that have
well-defined grammatical attribute values, namely
? (singular you),? (I),? (he),? (she),? (it),
?? (plural you), ?? (we), ?? (masculine
they),?? (feminine they), and?? (impersonal
they). As can be seen in Table 1, each of them can
be uniquely identified using these four attributes.
Pronouns Number Gender Person Animacy
? (I) singular neuter first animate
? (you) singular neuter second animate
? (he) singular masculine third animate
? (she) singular feminine third animate
? (it) singular neuter third inanimate
?? (you) plural neuter second animate
?? (we) plural neuter first animate
?? (they) plural masculine third animate
?? (they) plural feminine third animate
?? (they) plural neuter third inanimate
Table 1: Attribute values of Chinese overt pronouns.
4 The Generative Model
4.1 Notation
Let p be an overt pronoun in PR, the set of the
10 overt pronouns described in Section 3. C, the
set of candidate antecedents of p, contains all and
only those maximal or modifier NPs that precede
p in the associated text and are at most two sen-
tences away from it.1 k is the context surround-
ing p as well as every candidate antecedent c in
C; k
c
is the context surrounding p and candidate
antecedent c; and l is a binary variable indicat-
ing whether c is the correct antecedent of p. The
set A = {Num,Gen, Per,Ani} has four ele-
ments, which correspond to Number, Gender,
Person and Animacy respectively. a is an at-
tribute in A. Finally, p
a
and c
a
are the attribute
values of p and c with respect to a respectively.
4.2 Training
Our model estimates P (p, k, c, l), the probability
of seeing (1) the overt pronoun p; (2) the context
k surrounding p and its candidate antecedents; (3)
a candidate antecedent c of p; and (4) whether c is
the correct antecedent of p. Since we estimate this
probability from a raw, unannotated corpus, we are
effectively treating p, k, and c as observed data and
l as hidden data.
Owing to the presence of hidden data, we es-
timate the model parameters using the EM algo-
rithm. Specifically, we use EM to iteratively es-
timate the model parameters from data in which
each overt pronoun is labeled with the probability
it corefers with each of its candidate antecedents
and apply the resultingmodel to re-label each overt
pronoun with the probability it corefers with each
of its candidate antecedents. Below we describe
1Only 8% of the overt pronouns in our corpus, the Chi-
nese portion of the OntoNotes 5.0 corpus, do not have any
antecedent in the preceding two sentences.
764
the details of the E-step and the M-step.
4.2.1 E-Step
The goal of the E-step is to compute
P (l=1|p, k, c), the probability that a candi-
date antecedent c is the correct antecedent of p
given context k. Assuming that exactly one of the
p's candidate antecedents is its correct antecedent,
we can rewrite P (l=1|p, k, c) as follows:
P (l=1|p, k, c) =
P (p, k, c, l=1)
?
c
?
?C
P (p, k, c
?
, l=1)
(1)
Applying Chain Rule, we can rewrite
P (p, k, c, l=1) as follows:
P (p, k, c, l=1) = P (p|k, c, l=1) ? P (l=1|k, c)
? P (c|k) ? P (k)
(2)
Next, given l = 1 (i.e., c is the antecedent of p),
we assume that we can generate p from c without
looking at the context.2 Then we represent p using
its grammatical attributes A. We further assume
that p's value with respect to attribute a ? A is
independent of the value of each of its remaining
attributes given the antecedent's value with respect
to a. So we can rewrite P (p|k, c, l=1) as follows:
P (p|k, c, l=1) ? P (p|c, l=1)
? P (p
Num
, p
Gen
, p
Per
, p
Ani
|c, l=1)
?
?
a?A
P (p
a
|c
a
, l=1)
(3)
Moreover, we assume that (1) given p and c's
context, the probability of c being the antecedent
of p is not affected by the context of the other can-
didate antecedents; and (2) k
c
is sufficient for de-
termining whether c is the antecedent of p. So,
P (l=1|k, c) ? P (l=1|k
c
, c) ? P (l=1|k
c
) (4)
Furthermore, we assume that given context k,
each candidate antecedent of p is generated with
equal probability. In other words,
P (c|k) ? P (c
?
|k) ? c, c
?
? C (5)
Given Equations (2), (3), (4) and (5), we can
rewrite P (l=1|p, k, c) as:
P (l=1|p, k, c) =
P (p, k, c, l=1)
?
c
?
?C
P (p, k, c
?
, l=1)
?
?
a?A
P (p
a
|c
a
, l=1) ? P (l=1|k
c
)
?
c
?
?C
?
a?A
P (p
a
|c
?
a
, l=1) ? P (l=1|k
c
?
)
(6)
2This assumption is reasonable because it is fairly easy to
determine which pronoun can be used to refer to a given NP.
As we can see from Equation (6), our model has
two groups of parameters, namely P (p
a
|c
a
, l=1)
and P (l=1|k
c
). Since we have four grammatical
attributes, P (p
a
|c
a
, l=1) contains four sets of pa-
rameters, with one set per attribute. Using Equa-
tion (6) and the current parameter estimates, we
can compute P (l=1|p, k, c).
Two points deserve mention before we describe
the M-step. First, we estimate P (l=1|p, k, c) from
all and only those overt pronouns p ? PR that
are surface or deep subjects in their correspond-
ing sentences. This condition is motivated by our
observation that 99.56% of the ZPs in our evalu-
ation corpus (i.e., OntoNotes 5.0) are surface or
deep subjects. In other words, we impose this con-
dition so that we can focus our efforts on learn-
ing a model for resolving overt pronouns that are
subjects. This is by no means a limitation of our
model: if we were given a corpus in which many
ZPs occur as grammatical objects, we could sim-
ilarly train another model on overt objects. Sec-
ond, since in the E-step we attempt to probabilisti-
cally label every overt pronoun p that satisfies the
condition above, our model is effectively making
the simplifying assumption that every overt pro-
noun is anaphoric. This is clearly an overly sim-
plistic assumption. One way to relax this assump-
tion, which we leave as future work, is to first iden-
tify those pronouns that are anaphoric and then use
EM to estimate the joint probability only from the
anaphoric pronouns.
4.2.2 M-Step
Given P (l=1|p, k, c), the goal of the M-step is to
(re)estimate the model parameters, P (p
a
|c
a
, l=1)
and P (l=1|k
c
), using maximum likelihood esti-
mation. Specifically, P (p
a
|c
a
, l=1) is estimated
as follows:
P (p
a
|c
a
, l=1) =
Count(p
a
, c
a
, l=1) + ?
Count(c
a
, l=1) + ? ? |a|
(7)
where Count(c
a
, l=1) is the expected number of
times c has attribute value c
a
when it is the an-
tecedent of p; |a| is the number of possible values
of attribute a; ? is the Laplace smoothing param-
eter, which we set to 1; and Count(p
a
, c
a
, l=1)
is the expected number of times p has attribute
value p
a
when its antecedent c has attribute value
c
a
. Given attribute values p?
a
and c?
a
, we compute
765
Count(p
?
a
, c
?
a
, l=1) as follows:
Count(p
?
a
, c
?
a
, l=1) =
?
p,c:p
a
=p
?
a
,c
a
=c
?
a
P (l=1|p, k, c)
(8)
Similarly, P (l=1|k
c
) is estimated as follows:
P (l=1|k
c
) =
Count(k
c
, l=1) + ?
Count(k
c
) + ? ? 2
(9)
where Count(k
c
) is the number of times k
c
ap-
pears in the training data, and Count(k
c
, l=1) is
the expected number of times k
c
is the context sur-
rounding a pronoun and its antecedent c. Given
context k?
c
, we compute Count(k?
c
, l=1) as fol-
lows:
Count(k
?
c
, l=1) =
?
k:k
c
=k
?
c
P (l=1|p, k, c) (10)
To start the induction process, we initialize all
parameters with uniform values. Specifically,
P (p
a
|c
a
, l=1) is set to 1
|a|
, and P (l=1|k
c
) is set
to 0.5. Then we iteratively run the E-step and the
M-step until convergence.
There are two important questions we have not
addressed. First, how can we compute the four at-
tribute values of a candidate antecedent (i.e., c
a
for each attribute a), which we need to estimate
P (p
a
|c
a
, l=1)? Second, what features should we
use to represent context k
c
, which we need to esti-
mate P (l=1|k
c
)? We defer the discussion of these
questions to Sections 5 and 6.
4.3 Inference
After training, we can apply the resulting model to
resolve AZPs. Given an AZP z, we determine its
antecedent as follows:
(c?, p?) = argmax
c?C, p?PR
P (l=1|p, k, c) (11)
where PR is our set of 10 Chinese overt pronouns
and C is the set of candidate antecedents of z. In
other words, we apply Formula (11) to eachAZP z,
searching for the candidate antecedent c and overt
pronoun p that maximize P (l=1|p, k, c) when p is
used to fill the ZP gap left behind by z. The c that
results in the maximum probability value over all
overt pronouns in PR is chosen as the antecedent
of z. In essence, since the model is trained on
overt pronouns but is applied to ZPs, we have to
exhaustively fill the ZP's gap under consideration
with each of the 10 overt pronouns in PR during
inference.
Although we can now apply our generative
model to resolve AZPs, the resolution procedure
can be improved further. The improvement is
motivated by a problem we observed previously
(Chen and Ng, 2013): an AZP and its closest an-
tecedent can sometimes be far away from each
other, thus making it difficult to correctly resolve
the AZP. To address this problem, we employ the
following resolution procedure in our experiments.
Given a test document, we process its AZPs in a
left-to-right manner. As soon as we resolve an
AZP to a preceding NP c, we fill the correspond-
ing AZP's gap with c. Hence, when we process
an AZP z, all of its preceding AZPs in the associ-
ated text have been resolved, with their gaps filled
by the NPs they are resolved to. To resolve z, we
create test instances between z and its candidate
antecedents in the same way as described before.
The only difference is that the set of candidate an-
tecedents of z may now include those NPs that are
used to fill the gaps of the AZPs resolved so far. In
other words, this incremental resolution procedure
may increase the number of candidate antecedents
of each AZP z. Some of these additional candidate
antecedents are closer to z than the original candi-
date antecedents, thereby facilitating the resolution
of z. If the model resolves z to the additional can-
didate antecedent that fills the gap left behind by,
say, AZP z?, we postprocess the output by resolv-
ing z to the NP that z? is resolved to.3
5 Attributes of Candidate Antecedents
In this section, we describe how we determine
the four grammatical attribute values (Number,
Gender, Person and Animacy) of a candidate
antecedent c, as they are used to represent c when
estimating P (p
a
|c
a
, l=1) for each attribute a.
5.1 ANIMACY
We determine the Animacy of a candidate an-
tecedent c heuristically. Specifically, we first
check the NP type of c. If c is a pronoun, we look
up its Animacy in Table 1. If c is a named en-
tity, there are two cases to consider: if c is a per-
son4, we label it as animate; otherwise, we label it
as inanimate. If c is a common noun, we look up
the Animacy of its head noun in an automatically
3This postprocessing step is needed because the additional
candidate antecedents are only gap fillers.
4A detailed description of our named entity recognizer can
be found in Chen and Ng (2014).
766
constructed word list WL. If the head noun is not
in WL, we set its Animacy to unknown.
Our method for constructing WL is motivated
by an observation of measure words in Chinese:
some of them only modify inanimate nouns while
others only modify animate nouns. For example,
the nouns modified by the measure word? are al-
ways inanimate, as in??? (one piece of paper).
On the other hand, the nouns modified by the mea-
sure word? are always animate, as in????
(one worker).
Given this observation, we first define two lists,
M
ani
andM
inani
. M
ani
is a list of measure words
that can only modify animate nouns. M
inani
is a
list of measure words that can only modify inan-
imate nouns.5 There exists a special measure
word in Chinese, ?, which can be used to mod-
ify most of the common nouns regardless of their
Animacy. As a result, we remove ? from both
lists. After constructing M
ani
and M
inani
, we (1)
parse the Chinese Gigaword corpus (Parker et al.,
2009), which contains 4,370,600 documents, using
an efficient dependency parser, ctbparser6 (Qian et
al., 2010), and then (2) collect all pairs of words
(m,n), where m is a measure word, n is a com-
mon noun, and there is a NMOD dependency re-
lation between m and n. Finally, we determine
the Animacy of a given common noun n as fol-
lows. First, we retrieve all of the pairs contain-
ing n. Then, we sum over all occurrences of m
in M
ani
(call the sum C
ani
), as well as all occur-
rences of m in M
inani
(call the sum C
inani
). If
C
ani
> C
inani
, we label this common noun as an-
imate; otherwise, we label it as inanimate.
Table 2 shows the learned values of
P (p
Ani
|c
Ani
, l=1). These results are consis-
tent with our intuition: an animate (inanimate)
pronoun is more likely to be generated from
an animate (inanimate) antecedent than from an
inanimate (animate) antecedent. Note that animate
pronouns are more likely to be generated than
inanimate pronouns regardless of the antecedent's
Animacy. This can be attributed to the fact that
94.6% of the pronouns in our corpus are animate.
5.2 GENDER
We determine the Gender of a candidate an-
tecedent c as follows. If c is a pronoun, we look up
its Gender in Table 1. Otherwise, we determine
5We create these two lists with the help of this page:
http://chinesenotes.com/ref_measure_words.htm
6http://code.google.com/p/ctbparser/
`
`
`
`
`
`
`
`
`
`
Antecedent
Pronoun animate inanimate
animate 0.999 0.001
inanimate 0.858 0.142
unknown 0.945 0.055
Table 2: Learned values of P (p
Ani
|c
Ani
, l=1).
its Gender based on its Animacy. Specifically,
if c is inanimate, we set its Gender to neuter.
Otherwise, we determine its gender by looking up
a gender word list constructed by Bergsma and
Lin's (2006) approach. If the word is not in the
list, we set its Gender to masculine by default.
Next, we describe how the aforementioned gen-
der word list is constructed. Following Bergsma
and Lin (2006), we define a dependency path as the
sequence of non-terminal nodes and dependency
labels between two potentially coreferent entities
in a dependency parse tree. From the parsed Chi-
nese Gigaword corpus, we first collect every de-
pendency path that connects two pronouns. For
each path P collected, we compute CL(P ), the
coreference likelihood of P , as follows:
CL(P ) =
N
I
(P )
N
I
(P ) + N
D
(P )
(12)
where N
I
(P ) is the number of times P connects
two identical pronouns, andN
D
(P ) is the number
of times it connects two different pronouns. As-
suming that two identical pronouns in a sentence
are coreferent (Bergsma and Lin, 2006), we can
see that the larger a path's CL value is, the more
likely it is that the two NPs it connects are corefer-
ent. To ensure that we have dependency paths that
are strongly indicative of coreference relations, we
consider a dependency path P a coreferent path if
and only if CL(P ) > 0.8.
Given these coreferent paths, we can compute
theGender of a nounn as follows. First, we com-
pute (1) N
M
(n), the number of coreferent paths
connecting n with a masculine pronoun; and (2)
N
F
(n), the number of coreferent paths connect-
ing n with a feminine pronoun. Then, if N
F
(n) >
N
M
(n), we set n's gender to feminine; otherwise,
we set it to masculine.
Table 3 shows the learned values of
P (p
Gen
|c
Gen
, l=1). These results are con-
sistent with our intuition: a pronoun is a lot more
likely to be generated from an antecedent with the
same Gender than one with a different Gender.
767
``
`
`
`
`
`
`
`
`
Antecedent
Pronoun neuter feminine masculine
neuter 0.864 0.018 0.117
feminine 0.065 0.930 0.005
masculine 0.130 0.041 0.828
Table 3: Learned values of P (p
Gen
|c
Gen
, l=1).
`
`
`
`
`
`
`
`
`
`
Antecedent
Pronoun singular plural
singular 0.861 0.139
plural 0.26 0.74
Table 4: Learned values of P (p
Num
|c
Num
, l=1).
5.3 NUMBER
When computing the Number of a candidate an-
tecedent in English, Charniak and Elsner (2009)
rely on part-of-speech information. For example,
NN and NNP denote singular nouns, whereas NNS
and NNPS denote plural nouns. However, Chi-
nese part-of-speech tags do not provide such in-
formation. Hence, we need a different method for
finding theNumber of a candidate antecedent c in
Chinese. If c is a pronoun, we look up itsNumber
in Table 1. If c is a named entity, its Number is
singular. If c is a common noun, we infer itsNum-
ber from its string: if the string ends with? or is
modified by a quantity word (e.g.,??,??), c
is plural; otherwise, c is singular.
Table 4 shows the learned values of
P (p
Num
|c
Num
, l=1). These results are con-
sistent with our intuition: a pronoun is more likely
to be generated from an antecedent with the same
Number than one with a different Number.
5.4 PERSON
Finally, we compute the Person of a candi-
date antecedent c. Similar to Charniak and El-
sner (2009), we set ? (I) and ?? (we) to first
person, ? (singular you) and ?? (plural you)
to second person, and everything else to third
person. We estimate two sets of probabilities
P (p
Per
|c
Per
, l=1), one where p and c are from the
same speaker, and the other where they are from
different speakers.7 This is based on our observa-
tion thatP (p
Per
|c
Per
, l=1) could be very different
in these two cases.
7We employ a simple heuristic to identify the speaker of
NPs occurring in direct speech: we assume that the speaker
is the subject of the speech's reporting verb. So for example,
we identify Jack as the speaker of This book in the sentence
"This book is good," Jack said.
`
`
`
`
`
`
`
`
`
`
Antecedent
Pronoun first second third
first 0.856 0.119 0.025
second 0.219 0.766 0.016
third 0.289 0.077 0.634
Table 5: Learned values of P (p
Per
|c
Per
, l=1)
(same speaker).
`
`
`
`
`
`
`
`
`
`
Antecedent
Pronoun first second third
first 0.417 0.525 0.057
second 0.75 0.23 0.02
third 0.437 0.229 0.334
Table 6: Learned values of P (p
Per
|c
Per
, l=1)
(different speakers).
Tables 5 and 6 show the learned values of these
two sets of probabilities. These results are consis-
tent with our intuition. In the same-speaker case, a
pronoun is a lot more likely to be generated from
an antecedent with the same speaker than one with
a different speaker. In the different-speaker case,
a first (second) person pronoun is most likely to be
generated from a second (first) person pronoun.
6 Context Features
To fully specify our model, we need to describe
how to represent k
c
, which is needed to compute
P (l=1|k
c
). Recall that k
c
encodes the context sur-
rounding candidate antecedent c and the associated
pronoun p. As described below, we represent k
c
using eight features, some of which are motivated
by previous work on supervised AZP resolution
(e.g., Zhao and Ng (2007), Chen and Ng (2013)).
Note that (1) all but feature 1 are computed based
on syntactic parse trees, and (2) features 2, 3, 6,
and 8 are ternary-valued features.
1. the sentence distance between c and p;
2. whether the node spanning c has an ancestor
NP node; if so, whether this NP node is a de-
scendant of c's lowest ancestor IP node;
3. whether the node spanning c has an ancestor
VP node; if so, whether this VP node is a de-
scendant of c's lowest ancestor IP node;
4. whether vp has an ancestor NP node, where
vp is the VP node spanning the VP that fol-
lows p;
5. whether vp has an ancestor VP node;
768
Training Test
Documents 1,391 172
Sentences 36,487 6,083
Words 756,063 110,034
Overt Subject Pronouns 13,418 ?
AZPs ? 1,713
Table 7: Statistics on the training and test sets.
6. whether p is the first word of a sentence; if
not, whether p is the first word of an IP clause;
7. whether c is a subject whose governing verb
is lexically identical to the verb governing p;
8. whether c is the closest candidate antecedent
with subject grammatical role and is seman-
tically compatible with p's governing verb; if
not, whether c is the first semantically com-
patible candidate antecedent8.
Our approach to determine semantic compatibil-
ity (in feature 8) resembles Kehler et al.'s (2004)
and Yang et al.'s (2005) methods for computing se-
lectional preferences. Specifically, for each verb
and each noun that serves as a subject in Chinese
Gigaword, we compute their mutual information
(MI). Now, given a pronoun p and a candidate an-
tecedent c in the training/test corpus, we retrieve
the MI value of c and p's governing verb. We then
consider them semantically compatible if and only
if their MI value is greater than zero.
7 Evaluation
7.1 Experimental Setup
Datasets. We employ the Chinese portion of the
OntoNotes 5.0 corpus that was used in the official
CoNLL-2012 shared task (Pradhan et al., 2012).
In the CoNLL-2012 data, the training set and de-
velopment set contain ZP coreference annotations,
but the test set does not. Therefore, we train our
models on the training set and perform evaluation
on the development set. Statistics on the datasets
are shown in Table 7. The documents in these
datasets come from six sources, namely Broadcast
News (BN), Newswire (NW), Broadcast Conver-
sation (BC), Telephone Conversation (TC), Web
Blog (WB) and Magazine (MZ).
8 We sort the candidate antecedents of p as follows. We
first consider the subject candidate antecedents in the same
sentence as p from right to left, then the other candidate an-
tecedents in the same sentence from right to left. Next, we
consider the candidate antecedents in the previous sentence,
also preferring candidates that are subjects, but in left-to-right
order. Finally, we consider the candidate antecedents two
sentences back, following the subject-first, left-to-right order.
Evaluation measures. We express the results of
ZP resolution in terms of recall (R), precision (P)
and F-score (F).
Evaluation settings. Following Chen and Ng
(2013), we evaluate our model in three settings.
In Setting 1, we assume the availability of gold
syntactic parse trees and gold AZPs. In Setting 2,
we employ gold syntactic parse trees and system
(i.e., automatically identified) AZPs. Finally, in
Setting 3, we employ system syntactic parse trees
and system AZPs. The gold and system syntactic
parse trees, as well as the gold AZPs, are obtained
from the CoNLL-2012 shared task dataset, while
the system AZPs are identified by the rule-based
approach described in the Appendix.9 Since our
AZP identification approach does not rely on any
labeled data, we are effectively evaluating an end-
to-end unsupervised AZP resolver in Setting 3.
7.2 Results
Baseline systems. We employ seven resolvers
as baseline systems. To gauge the difficulty of
the task, we employ four simple rule-based re-
solvers, which resolve an AZP z to (1) the can-
didate antecedent closest to z (Baseline 1); (2) the
subject NP closest to z (Baseline 2); (3) the clos-
est candidate antecedent that is semantically com-
patible with z (Baseline 3); and (4) the first can-
didate antecedent that is semantically compatible
with z, where the candidate antecedents are vis-
ited according to the order described in Footnote 8
(Baseline 4). These four baselines allow us to
study the role of (1) recency, (2) salience, (3) re-
cency combined with semantic compatibility, and
(4) salience combined with semantic compatibil-
ity in AZP resolution respectively. The remaining
three baselines are state-of-the-art supervised AZP
resolvers, which include our own resolver (Chen
and Ng, 2013) as well as our re-implementations
of Zhao and Ng's (2007) resolver and Kong and
Zhou's (2010) resolver.
The test set results of these seven baseline re-
solvers when evaluated under the three afore-
mentioned evaluation settings are shown in Ta-
ble 8. The system AZPs employed by the rule-
based resolvers are obtained using our rule-based
9One may wonder why we do not train a supervised sys-
tem for identifying AZPs and instead experiment with a rule-
based AZP identification system. The reason is that employ-
ing labeled data defeats the whole purpose of having an unsu-
pervised AZP resolution model: if annotated data is available
for training an AZP identification system, the same data can
be used to train an AZP resolution system.
769
Setting 1: Setting 2: Setting 3:
Gold Parses, Gold Parses, System Parses,
Gold AZPs System AZPs System AZPs
Baseline R P F R P F R P F
Selecting closest candidate antecedent 25.0 25.2 25.1 18.3 10.8 13.6 10.3 6.7 8.1
Selecting closest subject 42.0 43.6 42.8 31.8 19.2 23.9 18.0 11.9 14.4
Selecting closest semantically compatible candidate antecedent 28.5 28.8 28.7 20.5 12.2 15.3 11.7 7.6 9.2
Selecting first semantically compatible candidate antecedent 45.2 45.7 45.5 33.6 20.0 25.1 18.9 12.3 14.9
Zhao and Ng (2007) 41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2 13.4
Kong and Zhou (2010) 44.9 44.9 44.9 33.0 19.3 24.4 18.7 11.9 14.5
Chen and Ng (2013) 47.7 47.7 47.7 25.3 27.6 26.4 14.9 16.7 15.7
Table 8: AZP resolution results of the baseline systems on the test set.
Setting 1: Gold Parses, Gold AZPs Setting 2: Gold Parses, System AZPs Setting 3: System Parses, System AZPs
Best Baseline Our Model Best Baseline Our Model Best Baseline Our Model
Source R P F R P F R P F R P F R P F R P F
Overall 47.7 47.7 47.7 47.5 47.9 47.7 25.3 27.6 26.4 35.4 21.0 26.4 14.9 16.7 15.7 19.9 12.9 15.7
NW 38.1 38.1 38.1 41.7 41.7 41.7 15.5 21.7 18.1 29.8 24.8 27.0 6.0 12.2 8.0 11.9 13.0 12.4
MZ 34.6 34.6 34.6 34.0 34.2 34.1 18.5 19.6 19.0 24.1 14.5 18.1 6.2 9.4 7.5 6.2 5.2 5.7
WB 46.1 46.1 46.1 47.9 47.9 47.9 21.8 22.0 21.8 37.3 18.7 24.9 8.5 11.4 9.7 19.0 11.3 14.2
BN 47.2 47.2 47.2 52.8 52.8 52.8 21.8 33.2 26.3 31.5 28.1 29.7 14.6 26.3 18.8 18.2 19.5 18.8
BC 52.7 52.7 52.7 49.8 50.3 50.0 23.3 30.7 26.5 38.0 21.0 27.0 12.7 16.2 14.3 20.6 12.4 15.5
TC 51.2 51.2 51.2 45.2 46.7 46.0 43.1 28.2 34.1 42.4 20.3 27.4 33.2 17.1 22.5 32.2 13.3 18.8
Table 9: AZP resolution results of the best baseline and our unsupervised model on the test set.
AZP identification system. On the other hand,
since our supervised resolvers are meant to be re-
implementations of existing resolvers, we follow
previous work and let them employ a supervised
AZP identification system. In particular, we em-
ploy the one described in Chen and Ng (2013).
Several observations can be made about these
results. First, among the rule-based resolvers,
Baseline 4 achieves the best performance, outper-
forming Baselines 1, 2, and 3 by 12.9%, 1.5%,
and 10.8% in F-score respectively when averaged
over the three evaluation settings. From their
relative performance, which remains the same in
the three settings, we can conclude that as far as
AZP resolution is concerned, (1) salience plays a
greater role than recency; and (2) semantic com-
patibility is useful. Second, among the super-
vised baselines, our supervised resolver (Chen and
Ng, 2013) achieves the best performance, outper-
forming Zhao and Ng's resolver and Kong and
Zhou's resolver by 3.9% and 2.0% in F-score re-
spectively when averaged over the three evalua-
tion settings. Finally, comparing the rule-based
resolvers and the learning-based resolvers, we can
see that the best rule-based baseline (Baseline 4)
performs even better than Zhao and Ng's resolver
and Kong and Zhou's resolver.
In the rest of this subsection, we will compare
our unsupervised model against the best baseline,
Chen and Ng's (2013) supervised resolver.
Our model. Results of the best baseline and our
model on the entire test set and each of the six
sources are shown in Table 9. As we can see, our
model achieves the same overall F-score as the best
baseline under all three settings, despite the fact
that it is unsupervised. In fact, our model even out-
performs the best baseline on NW, WB and BN in
Setting 1, NW, WB, BN and BC in Setting 2, and
NW, WB and BC in Setting 3.
It is worth mentioning that while the two re-
solvers achieved the same overall performance,
their outputs differ a lot from each other. Specifi-
cally, the twomodels only agree on the antecedents
of 55% of the AZPs in Setting 1.10
7.3 Ablation Experiments
Impact of P (p
a
|c
a
, l=1) and P (l=1|k
c
). Re-
call that our model is composed of five probability
terms,P (p
a
|c
a
, l=1) for each of the four grammat-
ical attributes and P (l=1|k
c
), the context proba-
bility. To investigate the contribution of context
and each attribute to overall performance, we con-
duct ablation experiments. Specifically, in each
ablation experiment, we remove exactly one prob-
ability term from the model and retrain it.
10Note that it is difficult to directly compare the outputs
produced under Settings 2 and 3: the AZPs identified by the
best baseline are quite different from those identified by our
rule-based system, as can be inferred from the AZP identifi-
cation results in Table 12.
770
Setting 1 Setting 3
System R P F R P F
Full model 47.5 47.9 47.7 19.9 12.9 15.7
? Number 47.5 47.9 47.7 19.7 12.8 15.5
? Gender 44.5 45.0 44.7 19.2 12.5 15.1
? Person 45.2 45.6 45.4 19.1 12.4 15.1
? Animacy 45.1 45.5 45.3 19.1 12.4 15.1
? Context Features 32.9 33.1 33.0 15.2 9.8 11.9
Table 10: Probability term ablation results.
Ablation results under Settings 1 and 3 are
shown in Table 10. As we can see, under Set-
ting 1, afterNumber is ablated, performance does
not drop. We attribute this to the fact that al-
most all candidate antecedents are singular. On the
other hand, when we ablate any of the remaining
three attributes, performance drops significantly
by 2.3?3.0% in overall F-score.11 Similar trends
can be observed with respect to Setting 3: after
Number is ablated, performance only decreases
by 0.2%, while ablating any of the other three at-
tributes results in a drop of 0.6%.
Results after ablating context are shown in the
last row of Table 10. As we can see, the F-score
drops significantly by 14.7% and 3.8% under Set-
tings 1 and 3 respectively. These results illustrate
the importance of context features in our model.
Context feature ablation. Recall that we em-
ployed eight context features to encode the rela-
tionship between a pronoun and a candidate an-
tecedent. To determine the relative contribution
of these eight features to overall performance,
we conduct ablation experiments under Settings 1
and 3. In these ablation experiments, all four gram-
matical attributes are retained in the model.
Ablation results are shown in rows 2?9 of Ta-
ble 11. To facilitate comparison, the F-score of the
model in which all eight context features are used
is shown in row 1. As we can see, feature 8 (the
rule-based feature) is the most useful feature: its
removal causes the F-scores of our resolver to drop
significantly by 6.4% under Setting 1 and 1.5% un-
der Setting 3.
7.4 Error Analysis
To gain additional insights into our full model, we
examine its major sources of error below. To focus
on errors attributable to AZP resolution, we ana-
lyze our full model under Setting 1.
Specifically, we randomly select 100 AZPs that
our model incorrectly resolves under Setting 1.
11All significance tests are paired t-tests, with p < 0.05.
Setting 1 Setting 3
System R P F R P F
Full model 47.5 47.9 47.7 19.9 12.9 15.7
? Feature 1 46.1 46.5 46.3 19.4 12.6 15.3
? Feature 2 46.5 46.9 46.7 19.4 12.6 15.3
? Feature 3 45.3 45.7 45.5 19.1 12.4 15.1
? Feature 4 47.4 47.8 47.6 20.1 13.0 15.8
? Feature 5 47.4 47.8 47.6 19.7 12.8 15.5
? Feature 6 47.1 47.5 47.3 19.6 12.7 15.4
? Feature 7 47.1 47.5 47.3 20.1 13.0 15.8
? Feature 8 41.2 41.6 41.4 18.0 11.8 14.2
Table 11: Context feature ablation results.
We found that 17 errors are attributable to dis-
course disfluency, lack of background knowledge
and subject detection, while the remaining 83 er-
rors can be divided into three types:
Failure to recognize the topics of a document.
Our model incorrectly resolves 32 AZPs that are
coreferent with NPs corresponding to the topics of
the associated documents. Consider the following
example:
[???]????????????????
????*pro*?????????????
([Bali Town] is located in the Northwest of Taipei
Basin. Its administrative area is affiliated with
Taipei County, *pro* is one of Taipei County's 29
towns and cities.)12
The model incorrectly resolves the AZP *pro*
to??? (Its administrative area). The reason is
that the correct antecedent, ??? (Bali Town),
is far from *pro*: there are five candidate an-
tecedents between *pro* and??? (Bali Town).
Note, however, that it is easy for a human to re-
solve *pro* to ??? (Bali Town) because the
whole passage is discussing??? (Bali Town).
Hence, to correctly handle such cases, one may
construct a topic model over the passage and as-
sign each candidate antecedent a prior probability
so that the resulting system favors the selection of
candidates representing the topics as antecedents.
Errors in computing semantic compatibility.
This type of error contributes to 28 of the incor-
rectly resolved AZPs. When computing seman-
tic compatibility in our model, we only consider
the mutual information between a candidate an-
tecedent and the pronoun's governing verb, but in
some cases, additional context needs to be taken
into account. Consider the following example:
12The pronoun Its in the phrase Its administrative area is
inserted into the English translation for the sake of grammat-
icality and correct understanding of the sentence. The corre-
sponding Chinese phrase does not contain any pronoun.
771
[???????]???? [24??????
????]?*pro*??????????
([Marines] killed about [24 unarmed Iraqis], *pro*
include women and six children.)
There are two candidate antecedents in this ex-
ample,??????? (Marines) and 24???
??????? (24 unarmed Iraqis), which we
denote as c
1
and c
2
respectively. The correct an-
tecedent of *pro* is c
2
, while our model wrongly
resolves *pro* to c
1
. Note that both c
1
and c
2
are
compatible with the AZP's governing verb ??
(include). However, if the object of the govern-
ing verb, i.e., ??????? (women and six
children), were also considered, the model could
determine that c
1
is not compatible with the object
while c
2
is, and then correctly resolve *pro* to c
2
.
Failure to recognize and exploit semantically
similar sentences. This type of error contributes
to 23 wrongly resolved AZPs. Recall that an AZP
is omitted for brevity, so the sentence it appears in
often expresses similar meaning to an earlier sen-
tence. However, our model fails to handle such
cases. Consider the following example:
[?????????]?????????.....
*pro*???????
([The command and the onrush of troops] lost con-
nection with each other. ... *pro* cannot connect
with each other.)
The above example shows two sentences that
are separated by some other sentences. The AZP
under consideration is in the last sentence, while
the first sentence contains the correct antecedent
????????? (the command and the on-
rush troops), denoted as c
1
. Our model fails to re-
solve *pro* to c
1
, because there are many com-
peting candidate antecedents between c
1
and AZP.
However, if our model were aware of the similarity
between the constructions appearing after c
1
and
*pro*, i.e., ???????? (lost connection
with each other) and?????? (cannot con-
nect with each other), then it might be able to cor-
rectly resolve the AZP.
8 Conclusion
We proposed an unsupervised model for Chinese
zero pronoun resolution, investigating the novel
hypothesis that an unsupervised probabilistic re-
solver trained on overt pronouns can be applied to
resolve ZPs. To our knowledge, this is the first un-
supervised probabilistic model for this task. Ex-
periments on the OntoNotes 5.0 corpus showed
that our unsupervised model rivaled its state-of-
the-art supervised counterparts in performance.
Acknowledgments
We thank the three anonymous reviewers for their
detailed comments. This work was supported in
part by NSF Grants IIS-1147644 and IIS-1219142.
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 33--40.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 148--156.
Chen Chen and Vincent Ng. 2013. Chinese zero pro-
noun resolution: Some recent advances. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1360--1365.
Chen Chen and Vincent Ng. 2014. SinoCorefer-
encer: An end-to-end Chinese event coreference
resolver. In Proceedings of the 9th International
Conference on Language Resources and Evaluation,
pages 4532--4538.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution.
In Proceedings of the Ninth Conference on Natural
Language Learning, pages 88--95.
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, University of Pennsyl-
vania.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39:1--38.
Antonio Ferr?ndez and Jes?s Peral. 2000. A compu-
tational approach to zero-pronouns in Spanish. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 166--172.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203--226.
Na-Rae Han. 2006. Korean Zero Pronouns: Analysis
and Resolution. Ph.D. thesis, University of Pennsyl-
vania.
Jerry Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311--338.
772
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 804--813.
Ryu Iida, Kentaro Inui, andYujiMatsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 625--632.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing, 6(4).
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85--88.
Hideki Isozaki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the 2003 Con-
ference on Empirical Methods in Natural Language
Processing, pages 184--191.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Proceedings of 2004 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 289--296.
Fang Kong and GuoDong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference
on EmpiricalMethods in Natural Language Process-
ing, pages 882--891.
Robert Parker, David Graff, Ke Chen, Junbo Kong, and
Kazuaki Maeda. 2009. Chinese Gigaword fourth
edition. Linguistic Data Consortium. Philadelphia,
PA.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings of
2012 Joint Conference on EmpiricalMethods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning: Shared Task, pages 1--40.
Xian Qian, Qi Zhang, Xuangjing Huang, and Lide Wu.
2010. 2d trie for fast parsing. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 904--912.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of the 5th International Joint Confer-
ence on Natural Language Processing, pages 758--
766.
Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and res-
olution. In Proceedings of the 19th International
Conference on Computational Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 165--172.
Ching-Long Yeh and Yi-Chun Chen. 2007. Zero
anaphora resolution in Chinese with shallow pars-
ing. Journal of Chinese Language and Computing,
17(1):41--56.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of Chinese zero pronouns: A ma-
chine learning approach. In Proceedings of the 2007
Joint Conference on Empirical Methods on Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 541--550.
GuoDong Zhou, Fang Kong, and Qiaoming Zhu. 2008.
Context-sensitive convolution tree kernel for pro-
noun resolution. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 25--31.
Appendix: Automatic AZP Identification
Our automatic AZP identification system employs
an ordered set of rules. The first rule is a positive
rule that aims to extract as many candidate AZPs
as possible. It is followed by seven negative rules
that aim to improve precision by filtering out er-
roneous candidate AZPs. Below we first describe
the rules and then evaluate this rule-based system.
Rule 1. Add candidate AZP z if it occurs before
the leftmost word spanned by a VP node vp.
Rule 2. Remove z if its associated vp is in a coor-
dinate structure or modified by an adverbial node.
Rule 3. Remove z if the parent of its associated
vp node is not an IP node.
Rule 4. Remove z if its associated vp has a NP
or QP node as an ancestor.
Rule 5. Remove z if one of the left sibling nodes
of vp is NP, QP, IP or ICP.
Rule 6. Remove z if (1) z does not begin a sen-
tence, (2) the highest node whose spanning word
sequence ends with the left non-comma neighbor
word of z is either NP, QP or IP, and (3) the parent
of this node is VP.
773
Gold Parses System Parses
Systems R P F R P F
Rule-based 72.4 42.3 53.4 42.3 26.8 32.8
Supervised 50.6 55.1 52.8 30.8 34.4 32.5
Table 12: AZP identification results on the test set.
Rule 7. Remove z if vp's lowest IP ancestor has
(1) a VP node as its parent and (2) a VV node as
its left sibling.
Rule 8. Remove z if it begins a document.
To gauge the performance of our rule-based
AZP identification system, we compare it with our
supervised AZP identification system (Chen and
Ng, 2013). Results of the two systems on our test
set are shown in Table 12. As we can see, the F-
scores achieved by the rule-based system is com-
parable to those of the supervised system.
774
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831?843,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Relieving the Computational Bottleneck: Joint Inference for
Event Extraction with High-Dimensional Features
Deepak Venugopal and Chen Chen and Vibhav Gogate and Vincent Ng
Department of Computer Science and Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
dxv021000@utdallas.edu, {yzcchen,vgogate,vince}@hlt.utdallas.edu
Abstract
Several state-of-the-art event extraction sys-
tems employ models based on Support Vec-
tor Machines (SVMs) in a pipeline architec-
ture, which fails to exploit the joint depen-
dencies that typically exist among events
and arguments. While there have been at-
tempts to overcome this limitation using
Markov Logic Networks (MLNs), it re-
mains challenging to perform joint infer-
ence in MLNs when the model encodes
many high-dimensional sophisticated fea-
tures such as those essential for event ex-
traction. In this paper, we propose a new
model for event extraction that combines
the power of MLNs and SVMs, dwarfing
their limitations. The key idea is to reli-
ably learn and process high-dimensional
features using SVMs; encode the output
of SVMs as low-dimensional, soft formu-
las in MLNs; and use the superior joint in-
ferencing power of MLNs to enforce joint
consistency constraints over the soft for-
mulas. We evaluate our approach for the
task of extracting biomedical events on
the BioNLP 2013, 2011 and 2009 Genia
shared task datasets. Our approach yields
the best F1 score to date on the BioNLP?13
(53.61) and BioNLP?11 (58.07) datasets
and the second-best F1 score to date on the
BioNLP?09 dataset (58.16).
1 Introduction
Event extraction is the task of extracting and la-
beling all instances in a text document that corre-
spond to a pre-defined event type. This task is quite
challenging for a multitude of reasons: events are
often nested, recursive and have several arguments;
there is no clear distinction between arguments and
events; etc. For instance, consider the BioNLP Ge-
nia event extraction shared task (N?edellec et al.,
2013). In this task, participants are asked to extract
instances of a pre-defined set of biomedical events
from text. An event is identified by a keyword
called the trigger and can have an arbitrary number
of arguments that correspond to pre-defined argu-
ment types. The task is complicated by the fact
that an event may serve as an argument of another
event (nested events). An example of the task is
shown in Figure 1. As we can see, event E13 takes
as arguments two events, E14 and E12, which in
turn has E11 as one of its arguments.
A standard method that has been frequently em-
ployed to perform this shared task uses a pipeline
architecture with three steps: (1) detect if a token
is a trigger and assign a trigger type label to it; (2)
for every detected trigger, determine all its argu-
ments and assign types to each detected argument;
and (3) combine the extracted triggers and argu-
ments to obtain events. Though adopted by the
top-performing systems such as the highest scoring
system on the BioNLP?13 Genia shared task (Kim
et al., 2013), this approach is problematic for at
least two reasons. First, as is typical in pipeline
architectures, errors may propagate from one stage
to the next. Second, since each event/argument is
identified and assigned a type independently of the
others, it fails to capture the relationship between
a trigger and its neighboring triggers, an argument
and its neighboring arguments, etc.
More recently, researchers have investigated
joint inference techniques for event extraction us-
ing Markov Logic Networks (MLNs) (e.g., Poon
and Domingos (2007), Poon and Vanderwende
(2010), Riedel and McCallum (2011a)), a statis-
tical relational model that enables us to model the
dependencies between different instances of a data
sample. However, it is extremely challenging to
make joint inference using MLNs work well in
practice (Poon and Domingos, 2007). One reason
is that it is generally difficult to model sophisti-
cated linguistic features using MLNs. The diffi-
831
. . . demonstrated that HOIL-1L interacting protein (HOIP), a ubiquitin ligase that can catalyze the assembly of linear
polyubiquitin chains, is recruited to DC40 in a TRAF2-dependent manner following engagement of CD40 . . .
(a) Sentence fragment
ID Event Type Trigger Arguments
E11 Binding recruited Theme={HOIL-1L interacting protein,CD40}
E12 Regulation dependent Theme=E11, Cause=TRAF2
E13 +ve Regulation following Theme=E12, Cause=E14
E14 Binding engagement Theme=CD40
(b) Events
Figure 1: Example of event extraction in the BioNLP Genia task. The table in (b) shows all the events
extracted from sentence (a). Note that successful extraction of E13 depends on E12 and E14.
culty stems from the fact that some of these fea-
tures are extremely high dimensional (e.g., Chen
and Ng (2012), Huang and Riloff (2012b), Li et al.
(2012), Li et al. (2013b), Li et al. (2013c)), and to
reliably learn weights of formulas that encode such
features, one would require an enormous number
of data samples. Moreover, even the complexity of
approximate inference on such models is quite high,
often prohibitively so. For example, a trigram can
be encoded as an MLN formula, Word(w
1
, p?1)?
Word(w
2
, p) ? Word(w
3
, p + 1)? Type(p, T ).
For any given position (p), this formula has W
3
groundings, where W is the number of possible
words, making it too large for learning/inference.
Therefore, current MLN-based systems tend to in-
clude a highly simplified model ignoring powerful
linguistic features. This is problematic because
such features are essential for event extraction.
Our contributions in this paper are two-fold.
First, we propose a novel model for biomedical
event extraction based on MLNs that addresses the
aforementioned limitations by leveraging the power
of Support Vector Machines (SVMs) (Vapnik,
1995; Joachims, 1999) to handle high-dimensional
features. Specifically, we (1) learn SVM models us-
ing rich linguistic features for trigger and argument
detection and type labeling; (2) design an MLN
composed of soft formulas (each of which encodes
a soft constraint whose associated weight indicates
how important it is to satisfy the constraint) and
hard formulas (constraints that always need to be
satisfied, thus having a weight of ?) to capture
the relational dependencies between triggers and
arguments; and (3) encode the SVM output as prior
knowledge in the MLN in the form of soft formulas,
whose weights are computed using the confidence
values generated by the SVMs. This formulation
naturally allows SVMs and MLNs to complement
each other?s strengths and weaknesses: learning
in a large and sparse feature space is much easier
with SVMs than with MLNs, whereas modeling
relational dependencies is much easier with MLNs
than with SVMs.
Our second contribution concerns making infer-
ence with this MLN feasible. Recall that inference
involves detecting and assigning the type label to
all the triggers and arguments. We show that exist-
ing Maximum-a-posteriori (MAP) inference meth-
ods, even the most advanced approximate ones
(e.g., Selman et al. (1996), Marinescu and Dechter
(2009), Sontag and Globerson (2011) ), are infea-
sible on our proposed MLN because of their high
memory cost. Consequently, we identify decompo-
sitions of the MLN into disconnected components
and solve each independently, thereby drastically
reducing the memory requirements.
We evaluate our approach on the BioNLP 2009,
2011 and 2013 Genia shared task datasets. On
the BioNLP?13 dataset, our model significantly
outperforms state-of-the-art pipeline approaches
and achieves the best F1 score to date. On the
BioNLP?11 and BioNLP?09 datasets, our scores
are slightly better and slightly worse respectively
than the best reported results. However, they
are significantly better than state-of-the-art MLN-
based systems.
2 Background
2.1 Related Work
As a core task in information extraction, event ex-
traction has received significant attention in the nat-
ural language processing (NLP) community. The
development and evaluation of large-scale learning-
based event extraction systems was propelled in
part by the availability of annotated corpora pro-
duced as part of the Message Understanding Con-
ferences (MUCs), the Automatic Content Extrac-
tion (ACE) evaluations, and the BioNLP shared
832
tasks on event extraction. Previous work on event
extraction can be broadly divided into two cate-
gories, one focusing on the development of fea-
tures (henceforth feature-based approaches) and
the other focusing on the development of models
(henceforth model-based approaches).
Feature-based approaches. Early work on
feature-based approaches has primarily focused
on designing local sentence-level features such as
token and syntactic features (Grishman et al., 2005;
Ahn, 2006). Later, it was realized that local features
were insufficient to reliably and accurately perform
event extraction in complex domains and therefore
several researchers proposed using high-level fea-
tures. For instance, Ji and Grishman (2008) used
global information from related documents; Gupta
and Ji (2009) extracted implicit time information;
Patwardhan and Riloff (2009) used broader sen-
tential context; Liao and Grishman (2010; 2011)
leveraged document-level cross-event information
and topic-based features; and Huang and Riloff
(2012b) explored discourse properties.
Model-based approaches. The model-based ap-
proaches developed to date have focused on mod-
eling global properties and seldom use rich, high-
dimensional features. To capture global event struc-
ture properties, McClosky et al. (2011a) proposed
a dependency parsing model. To extract event ar-
guments, Li et al. (2013b) proposed an Integer
Linear Programming (ILP) model to encode the
relationship between event mentions. To overcome
the error propagation problem associated with the
pipeline architecture, several joint models have
been proposed, including those that are based on
MLNs (e.g., Poon and Domingos (2007), Riedel et
al. (2009), Poon and Vanderwende (2010)), struc-
tured perceptrons (e.g., Li et al. (2013c)), and dual
decomposition with minimal domain adaptation
(e.g., Riedel and McCallum (2011a; 2011b)).
In light of the high annotation cost required by
supervised learning-based event extraction systems,
several semi-supervised, unsupervised, and rule-
based systems have been proposed. For instance,
Huang and Riloff (2012a) proposed a bootstrap-
ping method to extract event arguments using only
a small amount of annotated data; Lu and Roth
(2012) developed a novel unsupervised sequence
labeling model; Bui et al. (2013) implemented a
rule-based approach to extract biomedical events;
and Ritter et al. (2012) used unsupervised learning
to extract events from Twitter data.
Our work extends prior work by developing a
rich framework that leverages sophisticated feature-
based approaches as well as joint inference using
MLNs. This combination gives us the best of both
worlds because on one hand, it is challenging to
model sophisticated linguistic features using MLNs
while on the other hand, feature-based approaches
employing sophisticated high-dimensional features
suffer from error propagation as the model is gen-
erally not rich enough for joint inference.
2.2 The Genia Event Extraction Task
The BioNLP Shared Task (BioNLP-ST) series
(Kim et al. (2009), Kim et al. (2011a) and N?edellec
et al. (2013)) is designed to tackle the problem of
extracting structured information from the biomedi-
cal literature. The Genia Event Extraction task is ar-
guably the most important of all the tasks proposed
in BioNLP-ST and is also the only task organized
in all three events in the series.
The 2009 edition of the Genia task (Kim et
al., 2009) was conducted on the Genia event
corpus (Kim et al., 2008), which only contains
abstracts of the articles that represent domain
knowledge around NF?B proteins. The 2011 edi-
tion (Kim et al., 2011b) augmented the dataset to
include full text articles, resulting in two collec-
tions, the abstract collection and the full text col-
lection. The 2013 edition (Kim et al., 2013) further
augmented the dataset with recent full text articles
but removed the abstract collection entirely.
The targeted event types have also changed
slightly over the years. Both the 2009 and 2011
editions are concerned with nine fine-grained event
sub-types that can be categorized into three main
types, namely simple, binding and regulation
events. These three main event types can be dis-
tinguished by the kinds of arguments they take. A
simple event can take exactly one protein as its
Theme argument. A binding event can take one
or more proteins as its Theme arguments, and is
therefore slightly more difficult to extract than a
simple event. A regulation event takes exactly one
protein or event as its Theme argument and option-
ally one protein or event as its Cause argument. If
a regulation event takes another event as its Theme
or Cause argument, it will lead to a nested event.
Regulation events are considered the most difficult-
to-extract among the three event types owing in part
to the presence of an optional Cause argument and
their recursive structure. The 2013 edition intro-
833
duced a new event type, protein-mod, and its three
sub-types. Theoretically, a protein-mod event takes
exactly one protein as its Theme argument and
optionally one protein or event as its Cause argu-
ment. In practice, however, it rarely occurs: there
are only six protein-mod events having Cause ar-
guments in the training data for the 2013 edition.
Consequently, our model makes the simplifying
assumption that a protein-mod event can only take
one Theme argument, meaning that we are effec-
tively processing protein-mod events in the same
way as simple events.
2.3 Markov Logic Networks
Statistical relational learning (SRL) (Getoor and
Taskar, 2007) is an emerging field that seeks to
unify logic and probability, and since most NLP
techniques are grounded either in logic or proba-
bility or both, NLP serves as an ideal application
domain for SRL. In this paper, we will employ a
popular SRL approach called Markov logic net-
works (MLNs) (Domingos and Lowd, 2009). At a
high level, an MLN is a set of weighted first-order
logic formulas (f
i
, w
i
), where w
i
is the weight
associated with formula f
i
. Given a set of con-
stants that model objects in the domain, it defines a
Markov network or a log-linear model (Koller and
Friedman, 2009) in which we have one node per
ground first-order atom and a propositional feature
corresponding to each grounding of each first-order
formula. The weight of the feature is the weight of
the corresponding first-order formula.
Formally, the probability of a world ?, which
represents an assignment of values to all ground
atoms in the Markov network, is given by:
Pr(?) =
1
Z
exp
(
?
i
w
i
N(f
i
, ?)
)
where N(f
i
, ?) is the number of groundings of f
i
that evaluate to True in ? and Z is a normalization
constant called the partition function.
The key inference tasks over MLNs are com-
puting the partition function (Z) and the most-
probable explanation given evidence (the MAP
task). Most queries, including those required by
event extraction, can be reduced to these inference
tasks. Formally, the partition function and the MAP
tasks are given by:
Z =
?
?
exp
(
?
i
w
i
N(f
i
, ?)
)
(1)
arg max
?
P (?) = arg max
?
?
i
w
i
N(f
i
, ?) (2)
3 Pipeline Model
We implement a pipeline event extraction system
using SVMs. This pipeline model serves two im-
portant functions: (1) providing a baseline for eval-
uation and (2) producing prior knowledge for the
joint model.
Our pipeline model consists of two steps: trig-
ger labeling and argument labeling. In the trigger
labeling step, we determine whether a candidate
trigger is a true trigger and label each true trigger
with its trigger type. Then, in the argument label-
ing step, we identify the arguments for each true
trigger discovered in the trigger labeling step and
assign a role to each argument.
We recast each of the two steps as a classification
task and employ SVM
multiclass
(Tsochantaridis
et al., 2004) to train the two classifiers. We describe
each step in detail below.
3.1 Trigger Labeling
A preliminary study of the BioNLP?13 training
data suggests that 98.7% of the true triggers? head
words
1
are either verbs, nouns or adjectives. There-
fore, we consider only those words whose part-of-
speech tags belong to the above three categories
as candidate triggers. To train the trigger classifier,
we create one training instance for each candidate
trigger in the training data. If the candidate trigger
is not a trigger, the class label of the corresponding
instance is None; otherwise, the label is the type
of the trigger. Thus, the number of class labels
equals the number of trigger types plus one. Each
training instance is represented by the features de-
scribed in Table 1(a). These features closely mirror
those used in state-of-the-art trigger labeling sys-
tems such as Miwa et al. (2010b) and Bj?orne and
Salakoski (2013).
After training, we apply the resulting trigger clas-
sifier to classify the test instances, which are cre-
ated in the same way as the training instances. If a
test instance is predicted as None by the classifier,
the corresponding candidate trigger is labeled as
a non-trigger; otherwise, the corresponding candi-
date trigger is posited as a true trigger whose type
is the class value assigned by the classifier.
1
Head words are found using Collins? (1999) rules.
834
(a) Features for trigger labeling
Token features The basic token features (see Table 1(c)) computed from (1) the candidate trigger word and (2) the
surrounding tokens in a window of two; character bigrams and trigrams of the candidate trigger word;
word n-grams (n=1,2,3) of the candidate trigger word and its context words in a window of three; whether
the candidate trigger word contains a digit; whether the candidate trigger word contains an upper case
letter; whether the candidate trigger word contains a symbol.
Dependency
features
The basic dependency path features (see Table 1(c)) computed using the shortest paths from the candidate
trigger to (1) the nearest protein word, (2) the nearest protein word to its left, and (3) the nearest protein
word to its right.
Other
features
The distances from the candidate trigger word to (1) the nearest protein word, (2) the nearest protein
word to its left, and (3) the nearest protein word to its right; the number of protein words in the sentence.
(b) Features for argument labeling
Token features Word n-grams (n=1,2,3) of (1) the candidate trigger word and its context in a window of three and (2) the
candidate argument word and its context in a window of three; the basic token features (see Table 1(c))
computed from (1) the candidate trigger word and (2) the candidate argument word; the trigger type of
the candidate trigger word.
Dependency
features
The basic dependency features (see Table 1(c)) computed using the shortest path from the candidate
trigger word to the candidate argument word.
Other
features
The distance between the candidate trigger word and the candidate argument word; the number of
proteins between the candidate trigger word and the candidate argument word; the concatenation of the
candidate trigger word and the candidate argument word; the concatenation of the candidate trigger type
and the candidate argument word.
(c) Basic token and dependency features
Basic token fea-
tures
Six features are computed given a token t, including: (a) the lexical string of t, (b) the lemma of t, (c) the
stem of t obtained using the Porter stemmer (Porter, 1980), (d) the part-of-speech tag of t, (e) whether t
appears as a true trigger in the training data, and (f) whether t is a protein name.
Basic
dependency
features
Six features are computed given a dependency path p, including: (a) the vertex walk in p, (b) the edge
walk in p, (c) the n-grams (n=2,3,4) of the (stemmed) words associated with the vertices in p, (d) the
n-grams (n=2,3,4) of the part-of-speech tags of the words associated with the vertices in p, (e) the
n-grams (n=2,3,4) of the dependency types associated with the edges in p, and (f) the length of p.
Table 1: Features for trigger labeling and argument labeling.
3.2 Argument Labeling
The argument classifier is trained as follows. Each
training instance corresponds to a candidate trigger
and one of its candidate arguments.
2
A candidate
argument for a candidate trigger ct is either a pro-
tein or a candidate trigger that appears in the same
sentence as ct. If ct is not a true trigger, the label of
the associated instance is set toNone. On the other
hand, if ct is a true trigger, we check whether the
candidate argument in the associated instance is in-
deed one of ct?s arguments. If so, the class label of
the instance is the argument?s role; otherwise, the
class label is None. The features used for repre-
senting each training instance, which are modeled
after those used in Miwa et al. (2010b) and Bj?orne
and Salakoski (2013), are shown in Table 1(b).
After training, we can apply the resulting clas-
sifier to classify the test instances, which are cre-
ated in the same way as the training instances. If
a test instance is assigned the class None by the
classifier, the corresponding candidate argument is
classified as not an argument of the trigger. Other-
2
Following the definition of the GENIA event extraction
task, the protein names are provided as part of the input.
wise, the candidate argument is a true argument of
the trigger whose role is the class value assigned
by the classifier.
4 Joint Model
In this section, we describe our Markov logic model
that encodes the relational dependencies in the
shared task and uses the output of the pipeline
model as prior knowledge (soft evidence). We be-
gin by describing the structure of our Markov logic
model, and then describe the parameter learning
and inference algorithms for it.
4.1 MLN Structure
Figure 2 shows our proposed MLN for BioNLP
event extraction, which we refer to as BioMLN.
The MLN contains six predicates.
The query predicates in Figure 2(a) are those
whose assignments are not given during infer-
ence and thus need to be predicted. Predicate
TriggerType(sid,tid,ttype!) is true when the
token located in sentence sid at position tid has
type ttype. ?
ttype
, which denotes the set of con-
stants (or objects) that the logical variable ttype
835
TriggerType(sid,tid,ttype!)
ArgumentRole(sid,aid,tid,arole!)
(a) Query
Simple(sid,tid)
Regulation(sid,tid)
(b) Hidden
Word(sid,tid,word)
DepType(sid,aid,tid,dtype)
(c) Evidence
1. ?t TriggerType(i,j,t).
2. ?a ArgumentRole(i,k,j,a).
3. ?TriggerType(i,j,None) ? ?k ArgumentRole(i,k,j,Theme).
4. Simple(i,j) ?? ?k ArgumentRole(i,k,j,Cause).
5. TriggerType(i,j,None) ? ArgumentRole(i,k,j,None).
6. ?ArgumentRole(i,k,j,None) ??TriggerType(i,k,None) ? Regulation(i,j).
7. Simple(i,j)? TriggerType(i,j,Simple1) ? . . .? TriggerType(i,j,Binding).
8. Regulation(i,j) ? TriggerType(i,j,Reg) ? TriggerType(i,j,PosReg)
? TriggerType(i,j,NegReg).
9. Word(i,j,+w) ? TriggerType(i,j,+t) ? DepType(i,k,j,+d) ? ArgumentRole(i,k,j,+a)
(d) Joint Formulas
Figure 2: The BioMLN structure.
can be instantiated to, includes all possible trigger
types in the dataset plus None (which indicates
that the token is not a trigger). The ?!? symbol mod-
els commonsense knowledge that only one of the
types in the domain ?
ttype
of ttype is true for every
unique combination of sid and tid. Similarly, pred-
icate ArgumentRole(sid,aid,tid,arole!) as-
serts that a token in sentence sid at position aid
plays exactly one argument role, denoted by arole,
with respect to the token at position tid. ?
arole
includes the two argument types, namely, Theme
and Cause plus the additional None that indicates
that the token is not an argument.
The hidden predicates in Figure 2(b) are ?clus-
ters? of trigger types. Predicate Simple(sid,tid)
is true when the token in sentence sid at posi-
tion tid corresponds to one of the Simple event
trigger types (BioNLP?13 has 9 simple events,
BioNLP?09/?11 have 5) or a binding event trig-
ger type. Similarly, Regulation(sid,tid) asserts
that the token in sentence sid at position tid corre-
sponds to any of the three regulation event trigger
types.
The evidence predicates in Figure 2(c) are those
that are always assumed to be known during in-
ference. We define two evidence predicates based
on dependency structures. Word(sid,tid,word) is
true when the word in sentence sid at position tid
is equal to word. DepType(sid,aid,tid,dtype)
asserts that dtype is the dependency type in the de-
pendency parse tree that connects the token at posi-
tion tid to the token at position aid in sentence sid.
If the word at tid and the word at aid are directly
connected in the dependency tree, then dtype is the
label of dependency edge with direction; otherwise
dtype is None.
The MLN formulas, expressing commonsense,
prior knowledge in the domain (Poon and Van-
derwende, 2010; Riedel and McCallum, 2011a),
are shown in Fig. 2(d). All formulas, except For-
mula (9), are hard formulas, meaning that they have
infinite weights. Note that during weight learning,
we only learn the weights of soft formulas.
Formulas (1) and (2) along with the ?!? con-
straint in the predicate definition ensure that the
token types are mutually exclusive and exhaustive.
Formula (3) asserts that every trigger should have
an argument of type Theme, since a Theme argu-
ment is mandatory for any event. Formula (4) mod-
els the constraint that a Simple orBinding trigger
has no arguments of type Cause since only regu-
lation events have a Cause. Formula (5) asserts
that non-triggers have no arguments and vice-versa.
Formula (6) models the constraint that if a token
is both an argument of t and a trigger by itself,
then t must belong to one of the three regulation
trigger types. This formula captures the recursive
relationship between triggers. Formulas (7) and
(8) connect the hidden predicates with the query
predicates. Formula (9) is a soft formula encoding
836
the relationship between triggers and arguments in
a dependency parse tree. It joins a word and the
dependency type label that connects the word token
to the argument token in the dependency parse tree
with the trigger types and argument types of the
two tokens. The ?+? symbol indicates that each
grounding of Formula (9) may have a different
weight.
4.2 Weight Learning
We can learn BioMLN from data either discrimina-
tively or generatively. Since discriminative learning
is much faster than generative learning, we use the
former. In discriminative training, we maximize
the conditional log-likelihood (CLL) of the query
and the hidden variables given an assignment to
the evidence variables. In principle, we can use the
standard gradient descent algorithm for maximiz-
ing the CLL. In each iteration of gradient descent,
we update the weights using the following equation
(cf. Singla and Domingos (2005) and Domingos
and Lowd (2009)):
w
t+1
j
= w
t
j
? ?(E
w
(n
j
)? n
j
) (3)
where w
t
j
represents the weight of the j
th
formula
in the t
th
iteration, n
j
is the number of groundings
in which the j
th
formula is satisfied in the training
data, E
w
(n
j
) is the expected number of ground-
ings in which the j
th
formula is satisfied given the
current weight vector w, and ? is the learning rate.
As such, the update rule given in Equation (3)
is likely to yield poor accuracy because the num-
ber of training examples of some types (e.g.,
None) far outnumber other types. To rectify this
ill-conditioning problem (Singla and Domingos,
2005; Lowd and Domingos, 2007), we divide the
gradient with the number of true groundings in
the data, namely, we compute the gradient using
(E
w
(n
j
)?n
j
)
n
j
.
Another key issue with using Equation (3) is that
computing E
w
(n
j
) requires performing inference
over the MLN. This step is intractable, #P-complete
in the worst case. To circumvent this problem and
for fast, scalable training, we instead propose to
use the voted perceptron algorithm (Collins, 2002;
Singla and Domingos, 2005). This algorithm ap-
proximates E
w
(n
j
) by counting the number of
satisfied groundings of each formula in the MAP
assignment. Computing the MAP assignment is
much easier (although still NP-hard in the worst
case) than computing E
w
(n
j
), and as a result the
voted perceptron algorithm is more scalable than
the standard gradient descent algorithm. In addi-
tion, it converges much faster.
4.3 Testing
In the testing phase, we combine BioMLN with the
output of the pipeline model (see Section 3) to ob-
tain a new MLN, which we refer to as BioMLN
+
.
For every candidate trigger, the SVM trigger clas-
sifier outputs a vector of signed confidence val-
ues (which is proportional to the distance from
the separating hyperplane) of dimension ?
ttype
with one entry for each trigger type. Similarly,
for every candidate argument, the SVM argu-
ment classifier outputs a vector of signed confi-
dence values of dimension ?
arole
with one en-
try for each argument role. In BioMLN
+
, we
model the SVM output as soft evidence, using
two soft unit clauses, TriggerType(i,+j,+t) and
ArgumentRole(i,+k,+j,+a). We use the con-
fidence values to determine the weights of these
clauses. Intuitively, higher (smaller) the confidence,
higher (smaller) the weight.
Specifically, the weights of the soft unit clauses
are set as follows. If the SVM trigger classifier
determines that the trigger in sentence i at po-
sition j belongs to type t with confidence C
i,j
,
then we attach a weight of
C
i,j
?n
i
to the clause
TriggerType(i,j,t). Here, n
i
denotes the num-
ber of trigger candidates in sentence i. Similarly,
if the SVM argument classifier determines that the
token at position k in sentence i belongs to the ar-
gument role a with respect to the token at position
j, with confidence C
?
i,k,j
, then we attach a weight
of
C
?
i,k,j
?
?
n
i
j=1
m
ij
to the clause ArgumentRole(i, k,
j,a). Here, m
ij
denotes the number of argument
candidates for the j
th
trigger candidate in sentence
i. ? and ? act as scale parameters for the confi-
dence values ensuring that the weights don?t get
too large (or too small).
4.4 Inference
As we need to perform MAP inference, both at
training time and at test time, in this subsection we
will describe how to do it efficiently by exploiting
unique properties of our proposed BioMLN.
Naively, we can perform MAP inference by
grounding BioMLN to a Markov network and
then reducing the Markov network by removing
from it all (grounded propositional) formulas that
are inconsistent with the evidence. On the re-
837
duced Markov network, we can then compute the
MAP solution using standard MAP solvers such as
MaxWalkSAT (a state-of-the-art local search based
MAP solver) (Selman et al., 1996) and Gurobi
3
(a
state-of-the-art, parallelized ILP solver).
The problem with the above approach is that
grounding the MLN is infeasible in practice; even
the reduced Markov network is just too large. For
example, assuming a total of |?
sid
| sentences and
a maximum of N tokens in a sentence, Formula (3)
alone has O(|?
sid
|N
3
) groundings. Concretely, at
training time, assuming 1000 sentences with 10
tokens per sentence, Formula (3) itself yields one
million groundings. Clearly, this approach is not
scalable. It turns out, however, that the (ground)
Markov network can be decomposed into several
disconnected components, each of which can be
solved independently. This greatly reduces the
memory requirement of the inference step. Specif-
ically, for every grounding of sid, we get a set of
nodes in the Markov network that are disconnected
from the rest of the Markov network and therefore
independent of the rest of the network. Formally,
Proposition 1. For any world ? of the BioMLN,
P
M
(?) = P
M
i
(?
i
)P
M\M
i
(? \ ?
i
) (4)
where ?
i
is the world ? projected on the ground-
ings of sentence i andM
i
is BioMLN grounded
only using sentence i.
Using Equation (4), it is easy to see that the MLN
M can be decomposed into |?
sid
| disjoint MLNs,
{M
k
}
|?
sid
|
k=1
. The MAP assignment toM can be
computed using,
|?
sid
|
?
i=1
(
arg max
?
i
P
M
i
(?
i
)
)
. This
result ensures that to approximate the expected
counts E
w
(n
j
), it is sufficient to keep exactly one
sentence?s groundings in memory. Specifically,
E
w
(n
j
) can be written as
?
|?
sid
|
k=1
E
w
(n
k
j
), where
E
w
(n
k
j
) indicates the expected number of satisfied
groundings of the j
th
formula in the k
th
sentence.
Since the MAP computation is decomposable, we
can estimate E
w
(n
k
j
) using MAP inference on just
the k
th
sentence.
5 Evaluation
5.1 Experimental Setup
We evaluate our system on the BioNLP?13 (Kim
et al., 2013), ?11 (Kim et al., 2011a) and ?09 (Kim
3
http://www.gurobi.com/
Dataset #Papers #Abstracts #TT #Events
BioNLP?13 (10,10,14) (0,0,0) 13 (2817,3199,3348)
BioNLP?11 (5,5,4) (800,150,260) 9 (10310,4690,5301)
BioNLP?09 (0,0,0) (800,150,260) 9 (8597,1809,3182)
Table 2: Statistics on the BioNLP datasets, which
consist of annotated papers/abstracts from PubMed.
(x, y, z): x in training, y in development and z
in test. #TT indicates the total number of trigger
types. The total number of argument types is 2.
et al., 2009) Genia datasets for the main event ex-
traction shared task. Note that this task is the most
important one for Genia and therefore has the most
active participation. Statistics on the datasets are
shown in Table 2. All our evaluations use the on-
line tool provided by the shared task organizers.
We report scores obtained using the approximate
span, recursive evaluation.
To generate features, we employ the support-
ing resources provided by the organizers. Specif-
ically, sentence split and tokenization are done
using the GENIA tools, while part-of-speech in-
formation is provided by the BLLIP parser that
uses the self-trained biomedical model (McClosky,
2010). Also, we create dependency features from
the parse trees provided by two dependency parsers,
the Enju parser (Miyao and Tsujii, 2008) and the
aforementioned BLLIP parser that uses the self-
trained biomedical model, which results in two sets
of dependency features.
For MAP inference, we use Gurobi, a par-
allelized ILP solver. After inference, a post-
processing step is required to generate biomedi-
cal events from the extracted triggers and argu-
ments. Specifically, for binding events, we em-
ploy a learning-based method similar to Bj?orne and
Salakoski (2011), while for the other events, we
employ a rule-based approach similar to Bj?orne et
al. (2009). Both the SVM baseline system and the
combined MLN+SVM system employ the same
post-processing strategy.
During weight learning, in order to combat the
problem of different initializations yielding radi-
cally different parameter estimates, we start at sev-
eral different initialization points and average the
weights obtained after 100 iterations of gradient
descent. However, we noticed that if we simply
choose random initialization points, the variance of
the weights was quite high and some initialization
points were much worse than others. To counter
this, we use the following method to systematically
838
System Rec. Prec. F1
Our System 48.95 59.24 53.61
EVEX (Hakala et al., 2013) 45.44 58.03 50.97
TEES-2.1 (Bj?orne and Salakoski, 2013) 46.17 56.32 50.74
BIOSEM (Bui et al., 2013) 42.47 62.83 50.68
NCBI (Liu et al., 2013) 40.53 61.72 48.93
DLUTNLP (Li et al., 2013a) 40.81 57.00 47.56
Table 3: Recall (Rec.), Precision (Prec.) and F1
score on the BioNLP?13 test data.
initialize the weights. Let n
i
be the number of sat-
isfied groundings of formula f
i
in the training data
and m
i
be the total number of possible groundings
of f
i
. We use a threshold ? to determine whether
we wish to make the initial weight positive or neg-
ative. If
n
i
m
i
? ?, then we choose the initial weight
uniformly at random from the range [?0.1, 0]. Oth-
erwise, we chose it from the range [0, 0.1]. These
steps ensure that the weights generated from dif-
ferent initialization points have smaller variance.
Also, in the testing phase, we set the scale parame-
ters for the soft evidence as ? = ? = max
c?C
|c|, where
C is the set of SVM confidence values.
5.2 Results on the BioNLP?13 Dataset
Among the three datasets, the BioNLP?13 dataset
is most ?realistic? one because it is the only one
that contains full papers and no abstracts. As a re-
sult, it is also the most challenging dataset among
the three. Table 3 shows the results of our system
along with the results of other top systems pub-
lished in the official evaluation of BioNLP?13. Our
system achieves the best F1-score (an improvement
of 2.64 points over the top-performing system) and
has a much higher recall (mainly because our sys-
tem detects more regulation events which outnum-
ber other event types in the dataset) and a slightly
higher precision than the winning system. Of the
top five teams, NCBI is the only other joint infer-
ence system, which adopts joint pattern matching
to predict triggers and arguments at the same time.
These results illustrate the challenge in using joint
inference effectively. NCBI performed much worse
than the SVM-based pipeline systems, EVEX and
TEES2.1. It was also worse than BIOSEM, a rule-
based system that uses considerable domain exper-
tise. Nevertheless, it was better than DLUTNLP,
another SVM-based system.
Figure 3 compares our baseline pipeline model
with our combined model. We can clearly see that
the combined model has a significantly better F1
score than the pipeline model on most event types.
System Rec. Prec. F1
Our System 53.42 63.61 58.07
Miwa12 (Miwa et al., 2012) 53.35 63.48 57.98
Riedel11 (Riedel et al., 2011) ? ? 56
UTurku (Bj?orne and Salakoski, 2011) 49.56 57.65 53.30
MSR-NLP (Quirk et al., 2011) 48.64 54.71 51.50
Table 4: Results on the BioNLP?11 test data.
The regulation events are considered the most com-
plex events to detect because they have a recursive
structure. At the same time, this structure yields a
large number of joint dependencies. The advantage
of using a rich model such as MLNs can be clearly
seen in this case; the combined model yields a 10
point and 6 point increase in F1-score on the test
data and development data respectively compared
to the pipeline model.
5.3 Results on the BioNLP?11 Dataset
Table 4 shows the results on the BioNLP?11 dataset.
We can see that our system is marginally better than
Miwa12, which is a pipeline-based system. It is
also more than two points better than Riedel11,
a state-of-the-art structured prediction-based joint
inference system. Reidel11 incorporates the Stan-
ford predictions (McClosky et al., 2011b) as fea-
tures in the model. On the two hardest, most
complex tasks, detecting regulation events (which
have recursive structures and more joint dependen-
cies than other event types) and detecting bind-
ing events (which may have multiple arguments),
our system performs better than both Miwa12 and
Riedel11.
4
Specifically, our system?s F1 score for
regulation events is 46.84, while those of Miwa12
and Riedel11 are 45.46 and 44.94 respectively. Our
system?s F1 score for the binding event is 58.79,
while those of Miwa12 and Riedel11 are 56.64 and
48.49 respectively. These results clearly demon-
strate the effectiveness of enforcing joint dependen-
cies along with high-dimensional features.
5.4 Results on the BioNLP?09 Dataset
Table 5 shows the results on the BioNLP?09 dataset.
Our system has a marginally lower score (by 0.11
points) than Miwa12, which is the best performing
system on this dataset. Specifically, our system
achieves a higher recall but a lower precision than
Miwa12. However, note that Miwa12 used co-
reference features while we are able to achieve
4
Detailed results are not shown for any of these three
datasets due to space limitations.
839
SVM MLN+SVM
Type Rec. Prec. F1 Rec. Prec. F1
Simple 64.47 87.89 74.38 73.11 78.99 75.94
Protein-Mod 66.49 79.87 72.57 72.25 69.70 70.95
Binding 39.04 50.00 43.84 48.05 43.84 45.85
Regulation 23.51 56.21 33.15 36.47 50.86 42.48
Overall 37.90 67.88 48.64 48.95 59.24 53.61
(a) Test
SVM MLN+SVM
Type Rec. Prec. F1 Rec. Prec. F1
Simple 55.79 81.63 66.28 63.21 75.10 68.64
Protein-Mod 64.47 87.89 74.38 71.14 85.63 77.72
Binding 31.90 48.77 38.57 47.99 50.00 48.97
Regulation 20.13 52.46 29.10 28.57 43.41 34.46
Overall 34.42 66.14 45.28 43.50 57.45 49.51
(b) Development
Figure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP?13
test and development data.
System Rec. Prec. F1
Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27
Our System 53.96 63.08 58.16
Riedel11 (Riedel et al., 2011) ? ? 57.4
Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28
Bjorne (Bj?orne et al., 2009) 46.73 58.48 51.95
PoonMLN (Poon&Vanderwende,2010) 43.7 58.6 50.0
RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4
Table 5: Results on the BioNLP?09 test data. ???
indicates that the corresponding values are not
known.
similar accuracy without the use of co-reference
data. The F1 score of Miwa10, which does not
use co-reference features, is nearly 2 points lower
than that of our system. Our system also has a
higher F1 score than Reidel11, which is the best
joint inference-based system for this task.
On the regulation events, our system (47.55) out-
performs both Miwa12 (45.99) and Riedel11 (46.9),
while on the binding event, our system (59.88) is
marginally worse than Miwa12 (59.91) and signifi-
cantly better than Riedel11 (52.6). As mentioned
earlier, these are the hardest events to extract. Also,
existing MLN-based joint inference systems such
as RiedelMLN and PoonMLN do not achieve state-
of-the-art results because they do not leverage com-
plex, high-dimensional features.
6 Summary and Future Work
Markov logic networks (MLNs) are a powerful
representation that can compactly encode rich rela-
tional structures and ambiguities (uncertainty). As
a result, they are an ideal representation for com-
plex NLP tasks that require joint inference, such
as event extraction. Unfortunately, the superior
representational power greatly complicates infer-
ence and learning over MLN models. Even the
most advanced methods for inference and learning
in MLNs (Gogate and Domingos, 2011) are un-
able to handle complex, high-dimensional features,
and therefore existing MLN systems primarily use
low-dimensional features. This limitation severely
affects the accuracy of MLN-based NLP systems,
and as a result, in some cases their performance
is inferior to pipeline methods that do not employ
joint inference.
In this paper, we presented a general approach
for exploiting the power of high-dimensional lin-
guistic features in MLNs. Our approach involves
reliably processing and learning high-dimensional
features using SVMs and encoding their output as
low-dimensional features in MLNs. We showed
that we could achieve scalable learning and in-
ference in our proposed MLN model by exploit-
ing decomposition. Our results on the BioNLP
shared tasks from ?13, ?11, and ?09 clearly show
that our proposed combination is extremely effec-
tive, achieving the best or second best score on all
three datasets.
In future work, we plan to (1) improve our joint
model by incorporating co-reference information
and developing model ensembles; (2) transfer the
results of this investigation to other complex NLP
tasks that can potentially benefit from joint infer-
ence; and (3) develop scalable inference and learn-
ing algorithms (Ahmadi et al., 2013).
Acknowledgments
This work was supported in part by the AFRL un-
der contract number FA8750-14-C-0021, by the
ARO MURI grant W911NF-08-1-0242, and by the
DARPA Probabilistic Programming for Advanced-
Machine Learning Program under AFRL prime
contract number FA8750-14-C-0005. Any opin-
ions, findings, conclusions, or recommendations
expressed in this paper are those of the authors
and do not necessarily reflect the views or official
policies, either expressed or implied, of DARPA,
AFRL, ARO or the US government.
840
References
Babak Ahmadi, Kristian Kersting, Martin Mladenov,
and Sriraam Natarajan. 2013. Exploiting symme-
tries for scaling loopy belief propagation and rela-
tional training. Machine Learning, 92(1):91?132.
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning About Time and Events, pages 1?8.
Jari Bj?orne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
the BioNLP Shared Task 2011 Workshop, pages 183?
191.
Jari Bj?orne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the bionlp
2013 shared task. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 16?25.
Jari Bj?orne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18.
Quoc-Chinh Bui, David Campos, Erik van Mulligen,
and Jan Kors. 2013. A fast rule-based approach
for biomedical event extraction. In Proceedings of
the BioNLP Shared Task 2013 Workshop, pages 104?
108.
Chen Chen and Vincent Ng. 2012. Joint modeling for
Chinese event extraction with rich linguistic features.
In Proceedings of the 24th International Conference
on Computational Linguistics, pages 529?544.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, PA.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan & Claypool, San Rafael, CA.
Lise Getoor and Ben Taskar, editors. 2007. Introduc-
tion to Statistical Relational Learning. MIT Press.
Vibhav Gogate and Pedro Domingos. 2011. Proba-
bilistic theorem proving. In Proceedings of the 27th
Conference on Uncertainty in Artificial Intelligence,
pages 256?265.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. NYU?s English ACE 2005 system description.
In Proceedings of the ACE 2005 Evaluation Work-
shop. Washington.
Prashant Gupta and Heng Ji. 2009. Predicting un-
known time arguments based on cross-event prop-
agation. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 369?372.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,
Yves Van de Peer, and Filip Ginter. 2013. EVEX
in ST?13: Application of a large-scale text mining
resource to event extraction and network construc-
tion. In Proceedings of the BioNLP Shared Task
2013 Workshop, pages 26?34.
Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped
training of event extraction classifiers. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 286?295.
Ruihong Huang and Ellen Riloff. 2012b. Modeling
textual cohesion for event extraction. In Proceed-
ings of the 26th AAAI Conference on Artificial Intel-
ligence.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 254?262.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press, Cambridge,
MA, USA.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedi-
cal events from literature. BMC bioinformatics,
9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011a.
Overview of BioNLP shared task 2011. In Pro-
ceedings of the BioNLP Shared Task 2011 Workshop,
pages 1?6.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of Genia event
task in BioNLP shared task 2011. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
7?15.
Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori.
2013. The Genia event extraction shared task, 2013
edition - overview. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 8?15.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.
841
Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Libin
Hou. 2012. Employing compositional semantics
and discourse consistency in Chinese event extrac-
tion. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1006?1016.
Lishuang Li, Yiwen Wang, and Degen Huang. 2013a.
Improving feature-based biomedical event extrac-
tion system by integrating argument information. In
Proceedings of the BioNLP Shared Task 2013 Work-
shop, pages 109?115.
Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013b.
Argument inference from relevant event mentions in
Chinese argument extraction. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 1477?1487.
Qi Li, Heng Ji, and Liang Huang. 2013c. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 73?82.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 789?797.
Shasha Liao and Ralph Grishman. 2011. Acquiring
topic features to improve event extraction: in pre-
selected and balanced collections. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing 2011, pages 9?16.
Haibin Liu, Karin Verspoor, Donald C. Comeau, An-
drew MacKinlay, and W John Wilbur. 2013. Gen-
eralizing an approximate subgraph matching-based
system to extract events in molecular biology and
cancer genetics. In Proceedings of the BioNLP
Shared Task 2013 Workshop, pages 76?85.
Daniel Lowd and Pedro Domingos. 2007. Efficient
weight learning for markov logic networks. In
Proceedings of the 11th European Conference on
Principles and Practice of Knowledge Discovery in
Databases, pages 200?211.
Wei Lu and Dan Roth. 2012. Automatic event extrac-
tion with structured preference modeling. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 835?844.
Radu Marinescu and Rina Dechter. 2009. AND/OR
branch-and-bound search for combinatorial opti-
mization in graphical models. Artificial Intelligence,
173(16-17):1457?1491.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational
Linguistics: Human Language Technologies, pages
1626?1635.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011b. Event extraction as dependency
parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Ph.D. thesis, Brown University,
Providence, RI.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. Evaluating dependency rep-
resentation for event extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 779?787.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010b. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology,
8(01):131?146.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou.
2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference
resolution. Bioinformatics, 28(13):1759?1765.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Claire N?edellec, Robert Bossy, Jin-Dong Kim, Jung-
Jae Kim, Tomoko Ohta, Sampo Pyysalo, and Pierre
Zweigenbaum. 2013. Overview of BioNLP shared
task 2013. In Proceedings of the BioNLP Shared
Task 2013 Workshop, pages 1?7.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?160.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings
of the 22nd National Conference on Artificial Intelli-
gence, pages 913?918.
Hoifung Poon and Lucy Vanderwende. 2010. Joint
inference for knowledge extraction from biomedi-
cal literature. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 813?821.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130?137.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP entry in
BioNLP shared task 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 155?
163.
842
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1?12.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Workshop,
pages 46?50.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A Markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41?49.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Andrew McCallum, and Christopher D. Manning.
2011. Model combination for event extraction in
bionlp 2011. In Proceedings of the BioNLP Shared
Task 2011 Workshop, pages 51?55.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from Twit-
ter. In Proceedings of the 18th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 1104?1112.
Bart Selman, Henry Kautz, and Bram Cohen. 1996.
Local Search Strategies for Satisfiability Testing. In
D. S. Johnson and M. A. Trick, editors, Cliques,
Coloring, and Satisfiability: Second DIMACS Im-
plementation Challenge, pages 521?532. American
Mathematical Society, Washington, DC.
Parag Singla and Pedro Domingos. 2005. Discrimina-
tive training of Markov logic networks. In Proceed-
ings of the 20th National Conference on Artificial
Intelligence, pages 868?873.
David Sontag and Amir Globerson. 2011. Introduction
to Dual Decomposition for Inference. Optimization
for Machine Learning.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the 21st Interna-
tional Conference on Machine Learning, pages 104?
112.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York, NY.
843
A Pipeline Approach to Chinese Personal Name
Disambiguation
Yang Song, Zhengyan He, Chen Chen, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
{ysong, hezhengyan, chenchen, wanghf}@pku.edu.cn
Abstract
In this paper, we describe our sys-
tem for Chinese personal name dis-
ambiguation task in the first CIPS-
SIGHAN joint conference on Chinese
Language Processing(CLP2010). We
use a pipeline approach, in which pre-
processing, unrelated documents dis-
carding, Chinese personal name exten-
sion and document clustering are per-
formed separately. Chinese personal
name extension is the most important
part of the system. It uses two addi-
tional dictionaries to extract full per-
sonal names in Chinese text. And then
document clustering is performed un-
der different personal names. Exper-
imental results show that our system
can achieve good performances.
1 Introduction
Personal name search is one of the most im-
portant tasks for search engines. When a per-
sonal name query is given to a search engine,
a list of related documents will be shown. But
not all of the returned documents refer to the
same person whom users want to find. For ex-
ample, the query name ?jordan? is submitted
to a search engine, we can get a lot of doc-
uments containing ?jordan?. Some of them
may refer to the computer scientist, others
perhaps refer to the basketball player. For
English, there have been three Web People
Search (WePS1) evaluation campaigns on per-
sonal name disambiguation. But for Chinese,
1http://nlp.uned.es/weps/
this is the first time. It encounters more chal-
lenge for Chinese personal name disambigua-
tion. There are no word boundary in Chinese
text, so it becomes difficult to recognize the
full personal names from Chinese text. For ex-
ample, a query name ???? is given, but the
full personal name from some documents may
be an extension of ????, like ????? or
?????, and so on. Meanwhile, ???? can
also be a common Chinese word. So we need
to discard those documents which are not ref-
ered to any person related to the given query
name.
To solve the above-mentioned problem, we
explore a pipeline approach to Chinese per-
sonal name disambiguation. The overview of
our system is illustrated in Figure 1. We split
this task into four parts: preprocessing, unre-
lated documents discarding, Chinese personal
name extension and document clustering. In
preprocessing and unrelated documents dis-
carding, we use word segmentation and part-
of-speech tagging tools to process the given
dataset and documents are discarded when
the given query name is not tagged as a per-
sonal name or part of a personal name. After
that we perform personal name extension in
the documents for a given query name. When
the query name has only two characters. We
extend it to the left or right for one character.
For example, we can extend ???? to ???
?? or ?????. The purpose of extending
the query name is to obtain the full personal
name. In this way, we can get a lot of full per-
sonal names for a given query name from the
documents. And then document clustering
Figure 1: Overview of the System
is performed under different personal names.
HAC (Hierarchical Agglomerative Clustering)
is selected here. We represent documents with
bag of words and solve the problem in vector
space model, nouns, verbs, bigrams of nouns
or verbs and named entities are selected as
features. The feature weight value takes 0 or
1. In HAC, we use group-average link method
as the distance measure and consine similar-
ity as the similarity computing measure. The
stopping criteria is dependent on a threshold
which is obtained from training data. Our sys-
tem produces pretty good results in the final
evaluation.
The remainder of this paper is organized as
follows. Section 2 introduces related work.
Section 3 gives a detailed description about
our pipeline approach. It includes preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering. Section 4 presents the experimental
results. The conclusions are given in Section
5.
2 Related Work
Several important studies have tried to
solve the task introduced in the previous sec-
tion. Most of them treated it as an cluster-
ing problem. Bagga & Baldwin (1998) first
selected tokens from local context as features
to perform intra-document coreference resolu-
tion. Mann & Yarowsky (2003) extracted lo-
cal biographical information as features. Niu
et al (2004) used relation extraction results
in addition to local context features and get a
perfect results. Al-Kamha and Embley (2004)
clustered search results with feature set in-
cluding attributes, links and page similarities.
In recent years, this problem has attracted
a great deal of attention from many research
institutes. Ying Chen et al (2009) used a
Web 1T 5-gram corpus released by Google
to extract additional features for clustering.
Masaki Ikeda et al (2009) proposed a two-
stage clustering algorithm to improve the low
recall values. In the first stage, some reliable
features (like named entities) are used to con-
nect documents about the same person. Af-
ter that, the connected documents (document
cluster) are used as a source from which new
features (compound keyword features) are ex-
tracted. These new features are used in the
second stage to make additional connections
between documents. Their approach is to im-
prove clusters step by step, where each step
refines clusters conservatively. Han & Zhao
(2009) presented a system named CASIANED
to disambiguate personal names based on pro-
fessional categorization. They first catego-
rize different personal name appearances into
a real world professional taxonomy, and then
the personal name appearances are clustered
into a single cluster. Chen Chen et al (2009)
explored a novel feature weight computing
method in clustering. It is based on the point-
wise mutual information between the ambigu-
ous name and features. In their paper, they
also develop a trade-off point based cluster
stopping criteria which find the trade-off point
between intra-cluster compactness and inter-
cluster separation.
Our approach is based on Chinese per-
sonal name extension. We recognize the full
personal names in Chinese text and perform
document clustering under different personal
names.
3 Methodology
In this section, we will explain preprocess-
ing, unrelated documents discarding, Chinese
personal name extension and document clus-
tering in order.
3.1 Preprocessing
We use ltp-service2 to process the given Chi-
nese personal name disambiguation dataset
(a detailed introduction to it will be given
in section 4). Training data in the dataset
contains 32 query names. There are 100-300
documents under every query name. All the
documents are collected from Xinhua News
Agency. They contain the exact same string
as query names. Ltp-service is a web ser-
vice interface for LTP3(Language Technology
Platform). LTP has integrated many Chinese
processing modules, including word segmen-
tation, part-of-speech tagging, named entity
recognition, word sense disambiguation, and
so on. Jun Lang et al (2006) give a detailed
introduction to LTP. Here we only use LTP
to generate word segmentation, part-of-speech
tagging and named entity recognition results
for the given dataset.
3.2 Unrelated documents discarding
Under every query name, there are 100-300
documents. But not all of them are really re-
lated. For example, ???? is a query name in
training data. In corresponding documents,
some are refered to real personal names like
???? or ?????. But others may be a sub-
string of an expression such as ??????
??. These documents are needed to be fil-
tered out. We use the preprocessing tool LTP
to slove this problem. LTP can do word seg-
mentation and part-of-speech tagging for us.
For each document under a given query name,
if the query name in the document is tagged as
a personal name or part of some extended per-
sonal name, the document will be marked as
undiscarded, otherwise the document will be
discarded. Generally speaking, for the query
name containing three characters, we don?t
need to discard any of the corresponding doc-
uments. But in practice, we find that for some
query names, LTP always gives the invariable
2http://code.google.com/p/ltp-service/
3http://ir.hit.edu.cn/ltp/
part-of-speech. For example, no matter what
the context of ???? is, it is always tagged
as a geographic name. So we use another pre-
processing tool ICTCLAS4. Only when both
of them mark one document as discarded, we
discard the corresponding document.
3.3 Chinese personal name extension
After discarding unrelated documents, we
need to recognize the full Chinese personal
names. We hypothesize that the full Chinese
personal name has not more than three char-
acters (We don?t consider the compound sur-
names here). So the query names containing
only two Chinese characters are considered to
extend. In our approach, we use two Chinese
personal names dictionaries. One is a sur-
name dictionary containing 423 one-character
entries. We use it to do left extend for the
query name. For example, the query name
is ???? and its left character in a docu-
ment is ???, we will extend it to full per-
sonal name ?????. The other is a non-
ending Chinese character dictionary contain-
ing 64 characters which could not occur at the
end of personal names. It is constructed by a
personal title dictionary. We use every title?s
first character and some other special charac-
ters (such as numbers or punctuations) to con-
stuct the dictionary. Some manual work has
also been done to filter a few incorrect charac-
ters. Several examples of the two dictionaries
are shown in Table 1.
Through the analysis of Xinhua News arti-
cles, we also find that nearly half of the docu-
ments under given query name actually refer
to the reporters. And they often appear in
the first or last brackets in the body of cor-
responding document. For example, ?(??
????????)? is a sentence containing
query name ????. We use some simple but
efficient rules to get full personal names for
this case.
3.4 Document clustering
For every query name, we can get a list of
full peronal names. For example, when the
4http://ictclas.org/
Table 1: Several Examples of the two Dictionaries
Dictionaries Examples
Surnames ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?...
Non-ending Chinese characters ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?...
query name is ????, we can get the per-
sonal names like ?????, ?????, ???
??, ?????. And then document clustering
is performed under different personal names.
3.4.1 Features
We use bag of words to represent docu-
ments. Some representative words need to be
chosen as features. LTP can give us POS tag-
ging and NER results. We select all the nouns,
verbs and named entities which appear in the
same paragraph with given query name as fea-
tures. Meanwhile, the bigrams of nouns or
verbs are also selected. We take 0 or 1 for
feature weight value. 0 represents that the
feature doesn?t appear in corresponding para-
graphs, and 1 represents just the opposite. We
find that this weighting scheme is more effec-
tive than TFIDF.
3.4.2 Clustering
All features are represented in vector space
model. Every document is modeled as a ver-
tex in the vector space. So every document
can be seen as a feature vector. Before cluster-
ing, the similarity between documents is com-
puted by cosine value of the angle between
feature vectors. We use HAC to do document
clustering. It is a bottom-up algorithm which
treats each document as a singleton cluster at
the outset and then successively merges (or
agglomerates) pairs of clusters until all clus-
ters have been merged into a single cluster
that contains all documents. From our ex-
perience, single link and group-average link
method seem to work better than complete
link one. We use group-average link method
in the final submission. The stopping criteria
is a difficult problem for clustering. Here we
use a threshold for terminating condition. So
it is not necessary to determine the number
of clusters beforehand. We select a threshold
which produces the best performance in train-
ing data.
4 Experimental Results
The dataset for Chinese personal name dis-
ambiguation task contains training data and
testing data. The training data contains
32 query names. Every query name folder
contains 100-300 news articles. Given the
query name, all the documents are retrived
by character-based matching from a collection
of Xinhua news documents in a time span of
fourteen years. The testing data contains 25
query names. Two threshold values as termi-
nating conditions are obtained from training
data. They are 0.4 and 0.5. For evaluation,
we use P-IP score and B-cubed score (Bagga
and Baldwin, 1998). Table 2 & Table 3 show
the official evaluation results.
Table 2: Official Results for P-IP score
Threshold P-IP
P IP F score
0.4 88.32 94.9 91.15
0.5 91.3 91.77 91.18
Table 3: Official Results for B-Cubed score
Threshold B-Cubed
Precision Recall F score
0.4 83.68 92.23 86.94
0.5 87.87 87.49 86.84
Besides the formal evaluation, the organizer
also provide a diagnosis test designed to ex-
plore the relationship between Chinese word
segmentation and personal name disambigua-
tion. That means the query names in the
documents are segmented correctly by manual
work. Table 4 & Table 5 show the diagnosis
results.
Table 4: Diagnosis Results for P-IP score
Threshold P-IP
P IP F score
0.4 89.01 95.83 91.96
0.5 91.85 92.68 91.96
Table 5: Diagnosis Results for B-Cubed score
Threshold B-Cubed
Precision Recall F score
0.4 84.53 93.42 87.96
0.5 88.59 88.59 87.8
The official results show that our method
performs pretty good. The diagnosis results
show that correct word segmentation can im-
prove the evaluation results. But the improve-
ment is rather limited. That is mainly because
Chinese personal name extension is done well
in our approach. So the diagnosis results don?t
gain much profit from query names? correct
segmentation.
5 Conclusions
We describe our framework in this paper.
First, we use LTP to do preprocessing for orig-
inal dataset which comes from Xinhua news
articles. LTP can produce good results for
Chinese text processing. And then we use
two additional dictionaries(one is Chinese sur-
name dictionary, the other is Non-ending Chi-
nese character dictionary) to do Chinese per-
sonal name extension. After that we perform
document clustering under different personal
names. Official evaluation results show that
our method can achieve good performances.
In the future, we will attempt to use other
features to represent corresponding persons in
the documents. We will also investigate auto-
matic terminating condition.
6 Acknowledgments
This research is supported by National
Natural Science Foundation of Chinese
(No.60973053) and Research Fund for the
Doctoral Program of Higher Education of
China (No.20090001110047).
References
J. Artiles, J. Gonzalo, and S. Sekine. 2009. WePS
2 evaluation campaign: overview of the web peo-
ple search clustering task. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Bagga and B. Baldwin. 1998. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of 17th Interna-
tional Conference on Computational Linguis-
tics, 79?85.
Mann G. and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In Proceedings
of CoNLL-2003, 33?40, Edmonton, Canada.
C. Niu, W. Li, and R. K. Srihari. 2004. Weakly
Supervised Learning for Cross-document Person
Name Disambiguation Supported by Informa-
tion Extraction. In Proceedings of ACL 2004.
Al-Kamha. R. and D. W. Embley. 2004. Group-
ing search-engine returned citations for person-
name queries. In Proceedings of WIDM 2004,
96-103, Washington, DC, USA.
Ying Chen, Sophia Yat Mei Lee, and Chu-Ren
Huang. 2009. PolyUHK:A Robust Information
Extraction System for Web Personal Names.
In 2nd Web People Search Evaluation Work-
shop(WePS 2009), 18th WWW Conference.
Masaki Ikeda, Shingo Ono, Issei Sato, Minoru
Yoshida, and Hiroshi Nakagawa. 2009. Person
Name Disambiguation on the Web by Two-Stage
Clustering. In 2nd Web People Search Evalua-
tion Workshop(WePS 2009), 18th WWW Con-
ference.
Xianpei Han and Jun Zhao. 2009. CASIANED:
Web Personal Name Disambiguation Based on
Professional Categorization. In 2nd Web People
Search Evaluation Workshop(WePS 2009), 18th
WWW Conference.
Chen Chen, Junfeng Hu, and Houfeng Wang.
2009. Clustering technique in multi-document
personal name disambiguation. In Proceed-
ings of the ACL-IJCNLP 2009 Student Research
Workshop, pages 88?95.
Jun Lang, Ting Liu, Huipeng Zhang and Sheng Li.
2006. LTP: Language Technology Platform. In
Proceedings of SWCL 2006.
Bagga, Amit and B. Baldwin. 1998. Algorithms
for scoring co-reference chains. In Proceedings
of the First International Conference on Lan-
guage Resources and Evaluation Workshop on
Linguistic co-reference.
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 56?63,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Combining the Best of Two Worlds:
A Hybrid Approach to Multilingual Coreference Resolution
Chen Chen and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{yzcchen,vince}@hlt.utdallas.edu
Abstract
We describe our system for the CoNLL-2012
shared task, which seeks to model corefer-
ence in OntoNotes for English, Chinese, and
Arabic. We adopt a hybrid approach to
coreference resolution, which combines the
strengths of rule-based methods and learning-
based methods. Our official combined score
over all three languages is 56.35. In particu-
lar, our score on the Chinese test set is the best
among the participating teams.
1 Introduction
TheCoNLL-2012 shared task extends last year's task
on coreference resolution from a monolingual to a
multilingual setting (Pradhan et al, 2012). Unlike
the SemEval-2010 shared task on Coreference Reso-
lution inMultiple Languages (Recasens et al, 2010),
which focuses on coreference resolution in European
languages, the CoNLL shared task is arguably more
challenging: it focuses on three languages that come
from very different language families, namely En-
glish, Chinese, and Arabic.
We designed a system for resolving references in
all three languages. Specifically, we participated
in four tracks: the closed track for all three lan-
guages, and the open track for Chinese. In compari-
son to last year's participating systems, our resolver
has two distinguishing characteristics. First, unlike
last year's resolvers, which adopted either a rule-
based method or a learning-based method, we adopt
a hybrid approach to coreference resolution, attempt-
ing to combine the strengths of both methods. Sec-
ond, while last year's resolvers did not exploit genre-
specific information, we optimize our system's pa-
rameters with respect to each genre.
Our decision to adopt a hybrid approach is mo-
tivated by the observation that rule-based meth-
ods and learning-based methods each have their
unique strengths. As shown by the Stanford coref-
erence resolver (Lee et al, 2011), the winner of
last year's shared task, many coreference relations in
OntoNotes can be identified using a fairly small set
of simple hand-crafted rules. On the other hand, our
prior work on machine learning for coreference res-
olution suggests that coreference-annotated data can
be profitably exploited to (1) induce lexical features
(Rahman and Ng, 2011a, 2011b) and (2) optimize
system parameters with respect to the desired coref-
erence evaluation measure (Ng, 2004, 2009).
Our system employs a fairly standard architecture,
performing mention detection prior to coreference
resolution. As we will see, however, the parameters
of these two components are optimized jointly with
respect to the desired evaluation measure.
In the rest of this paper, we describe the men-
tion detection component (Section 2) and the coref-
erence resolution component (Section 3), show how
their parameters are jointly optimized (Section 4),
and present evaluation results on the development set
and the official test set (Section 5).
2 Mention Detection
To build a mention detector that strikes a relatively
good balance between precision and recall, we em-
ploy a two-step approach. First, in the extrac-
tion step, we identify named entities (NEs) and em-
ploy language-specific heuristics to extract mentions
56
from syntactic parse trees, aiming to increase our up-
per bound on recall as much as possible. Then, in
the pruning step, we aim to improve precision by
employing both language-specific heuristic pruning
and language-independent learning-based pruning.
Section 2.1 describes the language-specific heuris-
tics for extraction and pruning, and Section 2.2 de-
scribes our learning-based pruning method.
2.1 Heuristic Extraction and Pruning
English. During extraction, we create a candidate
mention from a contiguous text span s if (1) s is a
PRP or an NP in a syntactic parse tree; or (2) s cor-
responds to a NE that is not a PERCENT, MONEY,
QUANTITY or CARDINAL. During pruning, we
remove a candidate mentionmk if (1)mk is embed-
ded within a larger mentionmj such thatmj andmk
have the same head, where the head of a mention is
detected using Collins's (1999) rules; (2) mk has a
quantifier or a partitive modifier; or (3) mk is a sin-
gular common NP, with the exception that we retain
mentions related to time (e.g., "today").
Chinese. Similar to English mention extraction,
we create Chinese mentions from all NP and QP
nodes in syntactic parse trees. During pruning, we
remove a candidate mentionmk if (1)mk is embed-
ded within a larger mentionmj such thatmj andmk
have the same head, except if mj and mk appear
in a newswire document since, unlike other docu-
ment annotations, Chinese newswire document an-
notations do consider such pairs coreferent; (2) mk
is a NE that is a PERCENT, MONEY, QUANTITY
and CARDINAL; or (3) mk is an interrogative pro-
noun such as "?? [what]", "?? [where]".
Arabic. We employ as candidate mentions all the
NPs extracted from syntactic parse trees, removing
those that are PERCENT, MONEY, QUANTITY or
CARDINAL.
2.2 Learning-Based Pruning
While the heuristic pruning method identifies can-
didate mentions, it cannot determine which candi-
date mentions are likely to be coreferent. To improve
pruning (and hence the precision of mention detec-
tion), we employ learning-based pruning, where we
employ the training data to identify and subsequently
discard those candidate mentions that are not likely
to be coreferent with other mentions.
Language Recall Precision F-Score
English 88.59 40.56 55.64
Chinese 85.74 42.52 56.85
Arabic 81.49 21.29 33.76
Table 1: Mention detection results on the development set
obtained prior to coreference resolution.
Specifically, for each mention mk in the test set
that survives heuristic pruning, we compute its men-
tion coreference probability, which indicates the
likelihood that the head noun of mk is coreferent
with another mention. If this probability does not
exceed a certain threshold tC , we will remove mk
from the list of candidate mentions. Section 4 dis-
cusses how tC is jointly learned with the parameters
of the coreference resolution component to optimize
the coreference evaluation measure.
We estimate the mention coreference probability
ofmk from the training data. Specifically, since only
non-singleton mentions are annotated in OntoNotes,
we can compute this probability as the number of
times mk 's head noun is annotated (as a gold men-
tion) divided by the total number of timesmk 's head
noun appears. If mk 's head noun does not appear in
the training set, we set its coreference probability to
1, meaning that we let it pass through the filter. In
other words, we try to be conservative and do not
filter any mention for which we cannot compute the
coreference probability.
Table 1 shows the mention detection results of the
three languages on the development set after heuris-
tic extraction and pruning but prior to learning-based
pruning and coreference resolution.
3 Coreference Resolution
Like the mention detection component, our corefer-
ence resolution component employs heuristics and
machine learning. More specifically, we employ
Stanford's multi-pass sieve approach (Lee et al,
2011) for heuristic coreference resolution, but since
most of these sieves are unlexicalized, we seek to im-
prove the multi-pass sieve approach by incorporat-
ing lexical information using machine learning tech-
niques. As we will see below, while different sieves
are employed for different languages, the way we in-
corporate lexical information into the sieve approach
is the same for all languages.
57
3.1 The Multi-Pass Sieve Approach
A sieve is composed of one or more heuristic rules.
Each rule extracts a coreference relation between
two mentions based on one or more conditions. For
example, one rule in Stanford's discourse processing
sieve posits two mentions as coreferent if two con-
ditions are satisfied: (1) they are both pronouns; and
(2) they are produced by the same speaker.
Sieves are ordered by their precision, with the
most precise sieve appearing first. To resolve a set
of mentions in a document, the resolver makes mul-
tiple passes over them: in the i-th pass, it attempts
to use only the rules in the i-th sieve to find an an-
tecedent for each mention mk. Specifically, when
searching for an antecedent formk, its candidate an-
tecedents are visited in an order determined by their
positions in the associated parse tree (Haghighi and
Klein, 2009). The partial clustering of the mentions
created in the i-th pass is then passed to the i+1-th
pass. Hence, later passes can exploit the informa-
tion computed by previous passes, but a coreference
link established earlier cannot be overridden later.
3.2 The Sieves
3.2.1 Sieves for English
Our sieves for English are modeled after those em-
ployed by the Stanford resolver (Lee et al, 2011),
which is composed of 12 sieves.1 Since we partic-
ipated in the closed track, we re-implemented the
10 sieves that do not exploit external knowledge
sources. These 10 sieves are listed under the "En-
glish" column in Table 2. Specifically, we leave out
the Alias sieve and the Lexical Chain sieve, which
compute semantic similarity using information ex-
tracted from WordNet, Wikipedia, and Freebase.
3.2.2 Sieves for Chinese
Recall that for Chinese we participated in both the
closed track and the open track. The sieves we em-
ploy for both tracks are the same, except that we use
NE information to improve some of the sieves in the
system for the open track.2 To obtain automatic NE
annotations, we employ a NE model that we trained
on the gold NE annotations in the training data.
1Table 1 of Lee et al's (2011) paper listed 13 sieves, but one
of them was used for mention detection.
2Note that the use of NEs puts a Chinese resolver in the open
track.
English Chinese
Discourse Processing Chinese Head Match
Exact String Match Discourse Processing
Relaxed String Match Exacth String Match
Precise Constructs Precise Constructs
Strict Head Match A?C Strict Head Match A?C
Proper Head Match Proper Head Match
Relaxed Head Match Pronouns
Pronouns --
Table 2: Sieves for English and Chinese (listed in the or-
der in which they are applied).
The Chinese resolver is composed of 9 sieves,
as shown under the "Chinese" column of Table 2.
These sieves are implemented in essentially the same
way as their English counterparts except for a few
of them, which are modified in order to account for
some characteristics specific to Chinese or the Chi-
nese coreference annotations. As described in de-
tail below, we introduce a new sieve, the Chinese
Head Match sieve, and modify two existing sieves,
the Precise Constructs sieve, and the Pronoun sieve.
1. Chinese Head Match sieve: Recall from Sec-
tion 2 that the Chinese newswire articles were
coreference-annotated in such away that amen-
tion and its embedding mention can be coref-
erent if they have the same head. To iden-
tify these coreference relations, we employ the
Same Head sieve, which posits two mentions
mj and mk as coreferent if they have the same
head and mk is embedded within mj . There is
an exception to this rule, however: if mj is a
coordinated NP composed of two or more base
NPs, and mk is just one of these base NPs, the
two mentions will not be considered coreferent
(e.g., ??????? [Charles and Diana]
and??? [Diana]).
2. Precise Constructs sieve: Recall from Lee
et al (2011) that the Precise Constructs sieve
posits two mentions as coreferent based on in-
formation such as whether one is an acronym of
the other and whether they form an appositive
or copular construction. We incorporate addi-
tional rules to this sieve to handle specific cases
of abbreviations in Chinese: (a) Abbreviation
of foreign person names, e.g., ??????
? [Saddam Hussein] and ??? [Saddam].
(b) Abbreviation of Chinese person names, e.g.,
58
??? [Chen President] and ?????
[Chen Shui-bian President]. (c) Abbreviation
of country names, e.g, ?? [Do country] and
???? [Dominica].
3. Pronouns sieve: The Pronouns sieve resolves
pronouns by exploiting grammatical informa-
tion such as the gender and number of a men-
tion. While such grammatical information is
provided to the participants for English, the
same is not true for Chinese.
To obtain such grammatical information for
Chinese, we employ a simple method, which
consists of three steps.
First, we employ simple heuristics to extract
grammatical information from those Chinese
NPs for which such information can be easily
inferred. For example, we can heuristically de-
termine that the gender, number and animacy
for ? [she] is {Female, Single and Animate};
and for?? [they] is {Unknown, Plural, Inani-
mate}. In addition, we can determine the gram-
matical attributes of a mention by its named
entity information. For example, a PERSON
can be assigned the grammatical attributes {Un-
known, Single, Animate}.
Next, we bootstrap from these mentions with
heuristically determined grammatical attribute
values. This is done based on the observation
that all mentions in the same coreference chain
should agree in gender, number, and animacy.
Specifically, given a training text, if one of the
mentions in a coreference chain is heuristically
labeled with grammatical information, we au-
tomatically annotate all the remaining mentions
with the same grammatical attribute values.
Finally, we automatically create six word lists,
containing (1) animate words, (2) inanimate
words, (3) male words, (4) female words, (5)
singular words, and (6) plural words. Specif-
ically, we populate these word lists with the
grammatically annotated mentions from the
previous step, where each element of a word
list is composed of the head of a mention and a
count indicating the number of times the men-
tion is annotated with the corresponding gram-
matical attribute value.
We can then apply these word lists to determine
the grammatical attribute values of mentions in
a test text. Due to the small size of these word
lists, and with the goal of improving precision,
we consider two mentions to be grammatically
incompatible if for one of these three attributes,
onemention has anUnknown value whereas the
other has a known value.
As seen in Table 2, our Chinese resolver does
not have the Relaxed String Match sieve, unlike its
English counterpart. Recall that this sieve marks
two mentions as coreferent if the strings after drop-
ping the text following their head words are identical
(e.g.,MichaelWolf, andMichaelWolf, a contributing
editor for "New York"). Since person names in Chi-
nese are almost always composed of a single word
and that heads are seldom followed by other words
in Chinese, we believe that Relaxed HeadMatch will
not help identify Chinese coreference relations. As
noted before, cases of Chinese person name abbrevi-
ation will be handled by the Precise Constructs sieve.
3.2.3 Sieves for Arabic
We only employ one sieve for Arabic, the exact
match sieve. While we experimented with additional
sieves such as the Head Match sieve and the Pro-
nouns sieve, we ended up not employing them be-
cause they do not yield better results.
3.3 Incorporating Lexical Information
Asmentioned before, we improve the sieve approach
by incorporating lexical information.
To exploit lexical information, we first compute
lexical probabilities. Specifically, for each pair of
mentions mj and mk in a test text, we first com-
pute two probabilities: (1) the string-pair probability
(SP-Prob), which is the probability that the strings
of the two mentions, sj and sk, are coreferent; and
(2) the head-pair probability (HP-Prob), which is the
probability that the head nouns of the two mentions,
hj and hk, are coreferent. For better probability esti-
mation, we preprocess the training data and the two
mentions by (1) downcasing (but not stemming) each
English word, and (2) replacing each Arabic word w
by a string formed by concatenating w with its lem-
matized form, its Buckwalter form, and its vocalized
Buckwalter form. Note that SP-Prob(mj ,mk) (HP-
59
Prob(mj ,mk)) is undefined if one or both of sj (hj)
and sk (hk) do not appear in the training set.
Next, we exploit these lexical probabilities to im-
prove the resolution of mj and mk by presenting
two extensions to the sieve approach. The first ex-
tension aims to improve the precision of the sieve
approach. Specifically, before applying any sieve,
we check whether SP-Prob(mj ,mk) ? tSPL or HP-
Prob(mj ,mk)? tHPL for some thresholds tSPL and
tHPL. If so, our resolver will bypass all of the
sieves and simply posit mj and mk as not corefer-
ent. In essence, we use the lexical probabilities to
improve precision, specifically by positing twomen-
tions as not coreferent if there is "sufficient" infor-
mation in the training data for us to make this de-
cision. Note that if one of the lexical probabilities
(say SP-Prob(mj ,mk)) is undefined, we only check
whether the condition on the other probability (in this
case HP(mj ,mk) ? tHPL) is satisfied. If both of
them are undefined, this pair of mentions will sur-
vive this filter and be processed by the sieve pipeline.
The second extension, on the other hand, aims to
improve recall. Specifically, we create a new sieve,
the Lexical Pair sieve, which we add to the end of
the sieve pipeline and which posits two mentionsmj
and mk as coreferent if SP-Prob(mj ,mk) ? tSPU
or HP-Prob(mj ,mk) ? tHPU . In essence, we use
the lexical probabilities to improve recall, specifi-
cally by positing two mentions as coreferent if there
is "sufficient" information in the training data for
us to make this decision. Similar to the first ex-
tension, if one of the lexical probabilities (say SP-
Prob(mj ,mk)) is undefined, we only check whether
the condition on the other probability (in this case
HP(mj ,mk) ? tHPU ) is satisfied. If both of them
are undefined, the Lexical Pair sieve will not process
this pair of mentions.
The four thresholds, tSPL, tHPL, tSPU , and
tHPU , will be tuned to optimize coreference perfor-
mance on the development set.
4 Parameter Estimation
As discussed before, we learn the system parameters
to optimize coreference performance (which, for the
shared task, is Uavg, the unweighted average of the
three commonly-used evaluation measures, MUC,
B3, and CEAFe) on the development set. Our sys-
tem has two sets of tunable parameters. So far, we
have seen one set of parameters, namely the five lex-
ical probability thresholds, tC , tSPL, tHPL, tSPU ,
and tHPU . The second set of parameters contains the
rule relaxation parameters. Recall that each rule in
a sieve may be composed of one or more conditions.
We associate with condition i a parameter ?i, which
is a binary value that controls whether condition i
should be removed or not. In particular, if ?i=0, con-
dition iwill be dropped from the corresponding rule.
The motivation behind having the rule relaxation pa-
rameters should be clear: they allow us to optimize
the hand-crafted rules using machine learning. This
section presents two algorithms for tuning these two
sets of parameters on the development set.
Before discussing the parameter estimation algo-
rithms, recall from the introduction that one of the
distinguishing features of our approach is that we
build genre-specific resolvers. In other words, for
each genre of each language, we (1) learn the lexi-
cal probabilities from the corresponding training set;
(2) obtain optimal parameter values ?1 and ?2 for
the development set using parameter estimation al-
gorithms 1 and 2 respectively; and (3) among?1 and
?2, take the one that yields better performance on
the development set to be the final set of parameter
estimates for the resolver.
Parameter estimation algorithm 1. This algo-
rithm learns the two sets of parameters in a sequential
fashion. Specifically, it first tunes the lexical proba-
bility thresholds, assuming that all the rule relaxation
parameters are set to one. To tune the five probabil-
ity thresholds, we try all possible combinations of
the five probability thresholds and select the combi-
nation that yields the best performance on the devel-
opment set. To ensure computational tractability, we
allow each threshold to have the following possible
values. For tC , the possible values are?0.1, 0, 0.05,
0.1, . . ., 0.3; for tSPL and tHPL, the possible values
are ?0.1, 0, 0.05, 0.15, . . ., 0.45; and for tSPU and
tHPU , the possible values are 0.55, 0.65, . . ., 0.95,
1.0 and 1.1. Note that the two threshold values?0.1
and 1.1 render a probability threshold useless. For
example, if tC = ?0.1, that means all mentions will
survive learning-based pruning in the mention detec-
tion component. As another example, if tSPU and
tHPU are both 1.1, it means that the String Pair sieve
60
will be useless because it will not posit any pair of
mentions as coreferent.
Given the optimal set of probability thresholds, we
tune the rule relaxation parameters. To do so, we ap-
ply the backward elimination feature selection algo-
rithm, viewing each condition as a feature that can be
removed from the "feature set". Specifically, all the
parameters are initially set to one, meaning that all
the conditions are initially present. In each iteration
of backward elimination, we identify the condition
whose removal yields the highest score on the de-
velopment set and remove it from the feature set. We
repeat this process until all conditions are removed,
and identify the subset of the conditions that yields
the best score on the development set.
Parameter estimation algorithm 2. In this algo-
rithm, we estimate the two sets of parameters in an
interleaved, iterative fashion, where in each itera-
tion, we optimize exactly one parameter from one
of the two sets. More specifically, (1) in iteration
2n, we optimize the (n mod 5)-th lexical probabil-
ity threshold while keeping the remaining parame-
ters constant; and (2) in iteration 2n+1, we optimize
the (n mod m)-th rule relaxation parameter while
keeping the remaining parameters constant, where
n = 1, 2, . . ., and m is the number of rule relax-
ation parameters. When optimizing a parameter in a
given iteration, the algorithm selects the value that,
when used in combination with the current values of
the remaining parameters, optimizes theUavg value
on the development set. We begin the algorithm by
initializing all the rule relaxation parameters to one;
tC , tSPL and tHPL to ?0.1; and tSPU and tHPU
to 1.1. This parameter initialization is equivalent to
the configuration where we employ all and only the
hand-crafted rules as sieves and do not apply learn-
ing to perform any sort of optimization at all.
5 Results and Discussion
The results of our Full coreference resolver on the
development set with optimal parameter values are
shown in Table 3. As we can see, both the men-
tion detection results and the coreference results (ob-
tained via MUC, B3, and CEAFe) are expressed in
terms of recall (R), precision (P), and F-measure (F).
In addition, to better understand the role played by
the two sets of system parameters, we performed ab-
lation experiments, showing for each language-track
combination the results obtained without tuning (1)
the rule relaxation parameters (? ?i's); (2) the proba-
bility thresholds (? tj 's); and (3) any of these param-
eters (? ?i's & tj). Note that (1) we do not have any
rule relaxation parameters for the Arabic resolver
owing to its simplicity; and (2) for comparison pur-
poses, we show the results of the Stanford resolver
for English in the row labeled "Lee et al (2011)".
A few points regarding the results in Table 3 de-
serve mention. First, these mention detection re-
sults are different from those shown in Table 1: here,
the scores are computed over the mentions that ap-
pear in the non-singleton clusters in the coreference
partitions produced by a resolver. Second, our re-
implementation of the Stanford resolver is as good
as the original one. Third, parameter tuning is com-
paratively less effective for Chinese, presumably be-
cause we spent more time on engineering the sieves
for Chinese than for the other languages. Fourth,
our score on Arabic is the lowest among the three
languages, primarily because Arabic is highly inflec-
tional and we have little linguistic knowledge of the
language to design effective sieves. Finally, these
results and our official test set results (Table 4), as
well as our supplementary evaluation results on the
test set obtained using gold mention boundaries (Ta-
ble 5) and gold mentions (Table 6), exhibit similar
performance trends.
Table 7 shows the optimal parameter values ob-
tained for the Full resolver on the development set.
Since there are multiple genres for English and Chi-
nese, we show in the table the probability thresholds
averaged over all the genres and the corresponding
standard deviation values. For the rule relaxation
parameters, among the 36 conditions in the English
sieves and the 61 conditions in the Chinese sieves,
we show the number of conditions being removed
(when averaged over all the genres) and the corre-
sponding standard deviation values. Overall, differ-
ent conditions were removed for different genres.
To get a better sense of the usefulness of
the probability thresholds, we show in Tables 8
and 9 some development set examples of cor-
rectly and incorrectly identified/pruned mentions
and coreferent/non-coreferent pairs for English and
Chinese, respectively. Note that no Chinese exam-
ples for tC are shown, since its tuned value cor-
61
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 74.8 75.6 75.2 65.6 67.3 66.4 69.1 74.7 71.8 49.8 47.9 48.8 62.3
? ?i 's 75.2 73.4 74.3 64.6 65.8 65.2 68.5 74.1 71.2 48.8 47.6 48.2 61.5
? tj 's 76.4 73.0 74.7 65.1 65.3 65.2 68.6 73.8 71.1 48.6 48.3 48.4 61.6
? ?i 's & tj 's 75.2 72.8 74.0 64.2 64.8 64.5 68.0 73.4 70.6 47.8 47.1 47.5 60.8
Lee et al (2011) 74.1 72.5 73.3 64.3 64.9 64.6 68.2 73.1 70.6 47.0 46.3 46.7 60.6
Chinese Closed Full 72.2 72.7 72.4 62.4 65.8 64.1 70.8 77.7 74.1 52.3 48.9 50.5 62.9
? ?i 's 71.3 72.8 71.9 61.8 66.7 64.2 70.2 78.2 74.0 52.2 47.6 49.9 62.6
? tj 's 72.7 71.1 71.9 62.3 64.8 63.5 70.7 77.1 73.8 51.2 48.8 50.0 62.4
? ?i 's & tj 's 71.7 71.4 71.5 61.5 65.1 63.3 70.0 77.6 73.6 51.3 47.9 49.5 62.1
Chinese Open Full 73.1 72.6 72.9 63.5 67.2 65.3 71.6 78.2 74.8 52.5 48.9 50.7 63.6
? ?i 's 72.5 73.1 72.8 63.2 67.0 65.1 71.3 78.1 74.5 52.4 48.7 50.4 63.3
? tj 's 72.8 72.5 72.7 63.5 66.5 65.0 71.4 77.8 74.5 51.9 48.9 50.4 63.3
? ?i 's & tj 's 72.4 72.5 72.4 63.0 66.3 64.6 71.0 77.8 74.3 51.7 48.5 50.1 63.0
Arabic Closed Full 56.6 64.5 60.3 40.4 42.8 41.6 58.9 62.7 60.7 40.4 37.8 39.1 47.1
? tj 's 52.0 64.3 57.5 33.1 40.2 36.3 53.4 67.9 59.8 41.9 34.2 37.6 44.6
Table 3: Results on the development set with optimal parameter values.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 75.1 72.6 73.8 63.5 64.0 63.7 66.6 71.5 69.0 46.7 46.2 46.4 59.7
Chinese Closed Full 71.1 72.1 71.6 59.9 64.7 62.2 69.7 77.8 73.6 53.4 48.7 51.0 62.2
Chinese Closed Full 71.5 73.5 72.4 62.5 67.1 64.7 71.2 78.4 74.6 53.6 49.1 51.3 63.5
Arabic Closed Full 56.2 64.0 59.8 38.1 40.0 39.0 60.6 62.5 61.5 41.9 39.8 40.8 47.1
Table 4: Official results on the test set.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 74.8 75.7 75.2 63.3 66.8 65.0 65.4 73.6 69.2 48.8 44.9 46.8 60.3
Chinese Closed Full 82.0 79.0 80.5 70.8 72.1 71.4 74.4 79.9 77.0 58.0 56.4 57.2 68.6
Chinese Open Full 82.4 80.1 81.2 73.5 74.3 73.9 76.3 80.5 78.3 58.2 57.3 57.8 70.0
Arabic Closed Full 57.2 62.6 59.8 38.7 39.2 39.0 61.5 61.8 61.7 41.6 40.9 41.2 47.3
Table 5: Supplementary results on the test set obtained using gold mention boundaries and predicted parse trees.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 80.8 100 89.4 72.3 89.4 79.9 64.6 85.9 73.8 76.3 46.4 57.7 70.5
Chinese Closed Full 84.7 100 91.7 76.6 92.4 83.8 73.0 91.4 81.2 83.6 57.9 68.4 77.8
Chinese Open Full 84.8 100 91.8 78.1 93.2 85.0 75.0 91.6 82.5 84.0 59.2 69.4 79.0
Arabic Closed Full 58.3 100 73.7 41.7 63.2 50.3 50.0 75.3 60.1 64.6 36.2 46.4 52.3
Table 6: Supplementary results on the test set obtained using gold mentions and predicted parse trees.
tC tHPL tSPL tHPU tSPU Rule Relaxation
Language Track Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev.
English Closed ?0.06 0.11 ?0.04 0.08 ?0.06 0.12 0.90 0.23 0.60 0.05 6.13 1.55
Chinese Closed ?0.10 0.00 ?0.08 0.06 0.00 0.95 1.01 0.22 0.88 0.27 4.67 1.63
Chinese Open ?0.10 0.00 ?0.08 0.06 ?0.05 0.05 1.01 0.22 0.88 0.27 5.83 1.94
Arabic Closed 0.05 0.00 0.00 0.00 ?0.10 0.00 1.10 0.00 0.15 0.00 0.00 0.00
Table 7: Optimal parameter values.
responds to the case where no mentions should be
pruned.
6 Conclusion
We presented a multilingual coreference resolver de-
signed for the CoNLL-2012 shared task. We adopted
62
Parameter Correct Incorrect
tC no problem; the same that; that idea
tHPL (people,that); (both of you,that) (ours,they); (both of you,us)
tSPL (first,first); (the previous year,its) (China,its); (Taiwan,its)
tHPU (The movie's,the film); (Firestone,the company's) (himself,he); (My,I)
tSPU (Barak,the Israeli Prime Minister); (she,the woman); (Taiwan,the island)
(Kostunica,the new Yugoslav President)
Table 8: Examples of correctly & incorrectly identified/pruned English mentions and coreferent/non-coreferent pairs.
Parameter Correct Incorrect
tC --- ---
tHPL (????,??); (????,?) (?????,??); (??,?)
tSPL (??,??); (???,???) (??,??); (??,?)
tHPU (??,????); (??,???) (???,??); (??,?)
tSPU (??,????); (??,??); (??,?) ; (????,??)
Table 9: Examples of correctly & incorrectly identified/pruned Chinese mentions and coreferent/non-coreferent pairs.
a hybrid approach to coreference resolution, which
combined the advantages of rule-based methods and
learning-based methods. Specifically, we proposed
two extensions to Stanford's multi-pass sieve ap-
proach, which involved the incorporation of lexical
information using machine learning and the acqui-
sition of genre-specific resolvers. Experimental re-
sults demonstrated the effectiveness of these exten-
sions, whether or not they were applied in isolation
or in combination.
In future work, we plan to explore other ways
to combine rule-based methods and learning-based
methods for coreference resolution, as well as im-
prove the performance of our resolver on Arabic.
Acknowledgments
We thank the two anonymous reviewers for their
comments on the paper. This work was supported in
part by NSF Grants IIS-0812261 and IIS-1147644.
References
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 1152-
-1161.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford's multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28--34.
Vincent Ng. 2004. Learning noun phrase anaphoricity
to improve conference resolution: Issues in represen-
tation and optimization. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics, pages 151--158.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings of
the 2009 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 575--583.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes, In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning.
Altaf Rahman and Vincent Ng. 2011a. Coreference reso-
lution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814--824.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. Journal of Artificial Intelligence Re-
search, 40:469--521.
Marta Recasens, Llu?s M?rquez, Emili Sapena,
M. Ant?nia Mart?, Mariona Taul?, V?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1--8.
63
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 49?58,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Comma Placement in Chinese Text for Better Readability using
Linguistic Features and Gaze Information
Tadayoshi Hara1 Chen Chen2? Yoshinobu Kano3,1 Akiko Aizawa1
1National Institute of Informatics, Japan 2The University of Tokyo, Japan
3PRESTO, Japan Science and Technology Agency
{harasan, kano, aizawa}@nii.ac.jp
Abstract
Comma placements in Chinese text are
relatively arbitrary although there are
some syntactic guidelines for them. In this
research, we attempt to improve the read-
ability of text by optimizing comma place-
ments through integration of linguistic fea-
tures of text and gaze features of readers.
We design a comma predictor for gen-
eral Chinese text based on conditional ran-
dom field models with linguistic features.
After that, we build a rule-based filter for
categorizing commas in text according to
their contribution to readability based on
the analysis of gazes of people reading text
with and without commas.
The experimental results show that our
predictor reproduces the comma distribu-
tion in the Penn Chinese Treebank with
78.41 in F1-score and commas chosen by
our filter smoothen certain gaze behaviors.
1 Introduction
Chinese is an ideographic language, with no natu-
ral apparent word boundaries, little morphology,
and no case markers. Moreover, most Chinese
sentences are quite long. These features make it
especially difficult for Chinese learners to identify
composition of a word or a clause in a sentence.
Punctuation marks, especially commas, are al-
lowed to be placed relatively arbitrarily to serve as
important segmentation cues (Yue, 2006) for pro-
viding syntactic and prosodic boundaries in text;
commas indicate not only phrase or clause bound-
aries but also sentence segmentations, and they
capture some of the major aspects of a writer?s
prosodic intent (Chafe, 1988). The combination
of both aspects promotes cognition when reading
text (Ren and Yang, 2010; Walker et al, 2001).
?The Japan Research Institute, Ltd. (from April, 2013)
Linguistic FeaturesCRF modelCRF model-based Comma Predictor
Gaze FeaturesHuman AnnotationRule-based Comma Filter Text with/without Commas
Parse Tree
Treebank
Comma Distribution for Readability
Comma Distribution in General Text
Input (Comma-less) Text
+
+
Figure 1: Our approach
However, although there are guidelines and re-
search on the syntactic aspects of comma place-
ment, prosodic aspects have not been explored,
since they are more related with cognition. It is
as yet unclear how comma placement should be
optimized for reading, and it has thus far been up
to the writer (Huang and Chen, 2011).
In this research, we attempt to optimize comma
placements by integrating the linguistic features of
text and the gaze features of readers. Figure 1 il-
lustrates our approach. First, we design a comma
predictor for general Chinese text based on con-
ditional random field (CRF) models with various
linguistic features. Second, we build a rule-based
filter for classifying commas in text into ones fa-
cilitating or obstructing readability, by comparing
the gaze features of persons reading text with and
without commas. These two steps are connected
by applying our rule-based filter to commas pre-
dicted by our comma predictor. The experimental
results for each step validate our approach.
Related work is described in Section 2. The
functions of Chinese commas are described in
Section 3. Our CRF model-based comma predic-
tor is examined in Section 4, and our rule-based
comma filter is constructed and examined in Sec-
tion 5 and 6. Section 7 contains a summary and
outlines future directions of this research.
49
[Case 1] When a pause between a subject and a predicate is needed. (? (,) means the original or comparative position of the comma in Chinese text.)
e.g. ????????????????????????(The stars we can see (,)? are mostly fixed stars that are far away from the earth.)
[Case 2] When a pause between an inner predicate and an object of a sentence is needed.
e.g. ?????????????????????(We should see that (,) science needs a person to devote all his/her life to it.)
[Case 3] When a pause after an inner (adverbial, prepositional, etc.) modifier of a sentence is needed.
e.g. ?????????????(He is no stranger (,) to this city.) (The order of the modifier and the main clause is opposite in the English translation.)
[Case 4] When a pause between clauses in a complex sentence is needed, besides the use of semicolon (?).
e.g. ??????????????????????(It is said that there are more than 100 Suzhou traditional gardens, (,) no more than 10 of which I
have been to.)
[Case 5] When a pause between phrases of the same syntactic type is needed.
e.g. ??????????????? (The students prefer young (,) and energetic teachers.)
Table 1: Five main usages of commas in Chinese text
(a) Screenshot of a material
Display PC Monitor SubjectEye TrackerHost PC Monitor
(b) Scene of the experiment (c) Window around a gaze point
Figure 3: Settings for eye-tracking experiments
WS Word surface
POS POS tag
DIP Depth of a word in the parse tree
STAG Syntactic tag
OIC Order of the clause in a sentence that a word belongs to
WL Word length
LOD Length of fragment with specific depth in a parsing tree
Table 2: Features used in our CRF model
2 Related Work
Previous work on Chinese punctuation prediction
mostly focuses on sentence segmentation in au-
tomatic speech recognition (Shriberg et al, 2000;
Huang and Zweig, 2002; Peitz et al, 2011).
Jin et al (2002) classified commas for sentence
segmentation and succeeded in improving pars-
ing performance. Lu and Ng (2010) proposed
an approach built on a dynamic CRF for predict-
ing punctuations, sentence boundaries, and sen-
tence types of speech utterances without prosodic
cues. Zhang et al (2006) suggested that a cascade
CRF-based approach can deal with ancient Chi-
nese prose punctuation better than a single CRF.
Guo et al (2010) implemented a three-tier max-
imum entropy model incorporating linguistically
motivated features for generating commonly used
Chinese punctuation marks in unpunctuated sen-
tences output by a surface realizer.
(a)
WS|POS|STAG|DIP|OIC|WL|LOD|IOB-tag
(b)
Figure 2: Example of a parse tree (a) and its cor-
responding training data (b) with the features
3 Functions of Chinese Commas
There are five main uses of commas in Chinese
text, as shown in Table 1. Cases 1 to 4 are from
ZDIC.NET (2005), and Case 5 obviously exists in
Chinese text. The first three serve the function of
emphasis, while the latter two indicate coordinat-
ing or subordinating clauses or phrases.
In Cases 1 and 2, a comma is inserted as a
kind of pause between a short subject and a long
predicate, or between a short remainder predicate,
such as?? (see/know),??/?? (indicate),?
50
Feature F1 (P/R) A
WS 59.32 (72.67/50.12) 95.45
POS 32.51 (69.06/21.26) 94.08
DIP 34.14 (68.65/22.72) 94.13
STAG 22.44 (64.00/13.60) 93.67
OIC 9.27 (66.56/ 4.98) 93.42
WL 10.70 (75.24/ 5.76) 93.52
LOD 35.32 (59.20/25.17) 93.81
WS+POS 63.75 (79.93/53.01) 96.03
WS +DIP 70.06 (83.27/60.47) 96.61
WS +STAG 57.42 (81.94/44.19) 95.67
WS +OIC 60.35 (77.98/49.22) 95.73
WS +WL 60.90 (76.39/50.63) 95.71
WS +LOD 70.85 (78.87/64.31) 96.53
WS+POS+DIP 73.41 (84.62/64.82) 96.93
WS+POS+DIP+STAG 74.58 (83.66/67.27) 97.01
WS+POS+DIP +OIC 76.87 (84.29/70.65) 97.23
WS+POS+DIP +WL 70.18 (83.33/60.62) 96.63
WS+POS+DIP +LOD 76.61 (82.61/71.43) 97.16
WS+POS+DIP+STAG+OIC 76.62 (84.48/70.09) 97.21
WS+POS+DIP+STAG +WL 74.12 (84.00/66.33) 96.98
WS+POS+DIP+STAG +LOD 77.64 (85.11/71.38) 97.33
WS+POS+DIP +OIC+WL 75.43 (84.76/67.95) 97.11
WS+POS+DIP +OIC +LOD 78.23 (84.23/73.03) 97.36
WS+POS+DIP +WL+LOD 74.01 (85.80/65.06) 97.02
WS+POS+DIP+STAG+OIC+WL 77.25 (83.97/71.53) 97.26
WS+POS+DIP+STAG+OIC +LOD 77.31 (86.36/69.97) 97.33
WS+POS+DIP+STAG +WL+LOD 76.55 (85.24/69.46) 97.23
WS+POS+DIP +OIC+WL+LOD 77.60 (84.30/71.89) 97.30
WS+POS+DIP+STAG+OIC+WL+LOD 78.41 (83.97/73.54) 97.36
F1: F1-Score, P: precision (%), R: recall (%), A: accuracy (%)
Table 3: Performance of the comma predictor
(A) #Characters,
Article (B) #Punctuations, (C) / (A) (C) / (B) Subjects
ID (C) #Commas
6 692 49 28 4.04% 57.14% L, T, C
7 335 30 15 4.48% 50.00% L, T, C
10 346 18 7 2.02% 38.89% L, T, C, Z
12 221 18 7 3.17% 38.89% L, T, C
14 572 33 14 2.45% 42.42% L, T, C
18 471 36 13 2.76% 36.11% C, Z
79 655 53 28 4.27% 52.83% Z
82 471 30 13 2.76% 43.33% Z
121 629 41 19 3.02% 46.34% Z
294 608 50 24 3.95% 48.00% Z
401 567 43 21 3.70% 48.84% L, T, C
406 558 39 18 3.23% 46.15% Z
413 552 52 22 3.99% 42.31% T, C, Z
423 580 49 26 4.48% 53.06% L, C, Z
438 674 46 28 4.15% 60.87% Z
Average 528.73 39.13 18.87 3.57% 48.22% -
Table 4: Materials assigned to each subject
? (find) etc., and following long clause-style ob-
jects. English commas, on the other hand, sel-
dom have such usages (Zeng, 2006). In Cases 3
and 4, commas instead of conjunctions sometimes
connect two clauses in a relation of either coordi-
nation or subordination. English commas, on the
other hand, are only required between independent
clauses connected by conjunctions (Zeng, 2006).
Liu et al (2010) proved that Chinese commas
can change the syntactic structures of sentences
by playing lexical or syntactic roles. Ren and
Yang (2010) claimed that inserting commas as
clause boundaries shortens the fixation time in
post-comma regions. Meanwhile, in computa-
tional linguistics, Xue and Yang (2011) showed
Figure 4: Obtained eye-movement trace map
0
100,000
200,000
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10
ith commas Without commas
Tota
l vie
win
g 
time
 
(sec
.)
0
1 0
2 0
Figure 5: Total viewing time
that Chinese sentence segmentation can be viewed
as detecting loosely coordinated clauses separated
by commas.
4 CRF Model-based Comma Predictor
We first predict comma placements in existing
text. The prediction is formalized as a task to an-
notate each word in a word sequence with an IOB-
style tag such as I-Comma (following a comma),
B-Comma (preceding a comma) or O (neither I-
Comma nor B-Comma). We utilize a CRF model
for this sequential labeling (Lafferty et al, 2001).
4.1 CRF Model for Comma Prediction
A conditional probability assigned to a label se-
quence Y for a particular sequence of words X in
a first-order linear-chain CRF is given by:
P?(Y |X) =
exp(
?n
w
?k
i ?ifi(Yw?1, Yw, X,w))
Z0(X)
where w is a word position in X , fi is a binary
function describing a feature for Yw?1, Yw, X , and
w, ?i is a weight for that feature, and Z0 is a nor-
malization factor over all possible label sequences.
The weight ?i for each fi is learned on training
data. For fi, the linguistic features shown in Ta-
ble 2 are derived from a syntactic parse of a sen-
tence1. The first three were used initially; the rest
were added after we got feedback from construc-
tion of our rule-based filters (see Section 5). Fig-
ure 2 shows an example of a parsing tree and its
corresponding training data.
1Some other features or tag formats which worked well in
the previous research, such as bi-/tri-gram, a preceding word
(L-1) or its POS (POS-1), and IO-style tag (Leaman and Gon-
zalez, 2008) were also examined, but they did not work that
well, probably because of the difference in task settings.
51
0
1,000
2,000
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10Fixa
tion
 
time
 
/
com
ma
 
(sec
.)
0.0
1.0
2.0 ith commas Without commas
Figure 6: Fixation time per comma
0.01.0
2.03.0
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10#reg
res
sion
s /
com
ma 0
ith co mas Without commas
12
3
Figure 7: Number of regressions per comma
4.2 Experimental Settings
The Penn Chinese Treebank (CTB) 7.0 (Nai-
wen Xue and Palmer, 2005) consists of 2,448
articles in five genres. It contains 1,196,329
words, and all sentences are annotated with parse
trees. We selected four genres for written Chi-
nese (newswire, news magazine, broadcast news
and newsgroups/weblogs) from this corpus as our
dataset. These were randomly divided into train-
ing (90%) and test data (10%). We also corrected
errors in tagging and inconsistencies in the dataset,
mainly by solving problems around strange char-
acters tagged as PU (punctuation). The commas
and characters after this preprocessing numbered
63,571 and 1,533,928 in the training data and
4,116 and 111,172 in the test data.
MALLET (McCallum, 2002) and its applica-
tion ABNER (Settles, 2005) were used to train the
CRF model. We evaluated the results in terms
of precision (P = tp/(tp + fp)), recall (R =
tp/(tp+fn)), F1-score (F1 = 2PR/(P+R)), and
accuracy (A = (tp + tn)/(tp + tn + fp + fn)),
where tp, tn, fp and fn are respectively the num-
ber of true positives, true negatives, false positives
and false negatives, based on whether the model
and the corpus provided commas at each location.
4.3 Performance of the CRF Model
Table 3 shows the performance of our CRF
model2. We can see that WS contributed much
more to the performance than other features, prob-
ably because a word surface itself has a lot of
information on both prosodic and syntactic func-
tions. Combining WS with other features greatly
improved performance, and as a result, with all
2Precision, recall, F1-score, and accuracy with WS + POS
+ DIP + L-1 + POS-1 were 82.96%, 65.04%, 72.91 and
96.84%, respectively (lower than those with WS+POS+DIP).
4080
120160
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10Sac
cad
e le
ngth
(1) / 
com
ma
ith co mas Without commas
4080
120160
(pixel)
Figure 8: Saccade length (1) per comma
30
60
90
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Commaith co mas Without commas
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z1030Sac
cad
e le
ngth
(2) / 
com
ma 60
90(pixel)
Figure 9: Saccade length (2) per comma
features (WS + POS + STAG + DIP + OIC + LOD
+ WL), precision, recall, F1-score and accuracy
were 83.97%, 73.54%, 78.41 and 97.36%.
We also found that a large number of false pos-
itives seemed helpful according to native speakers
(see the description of the subjects in Section 5 and
6). Although these commas do not appear in the
CTB text, they might smoothen the reading expe-
rience. We constructed a rule-based filter in order
to pick out such commas.
5 Rule-based Comma Filter
We constructed a rule-based comma filter for clas-
sifying commas in text into ones facilitating (pos-
itive) or obstructing (negative) the reading process
as follows:
[Step 1]: Collect gaze data from persons reading
text with or without commas (Section 5.1).
[Step 2]: Compare gaze features around commas
to find those features that reflect the effect of
comma placement. (Section 5.2).
[Step 3]: Annotate commas with categories based
on the obtained features (Section 5.3), and devise
rules to explain the annotation (Section 5.4).
5.1 Collecting Human Eye-movement Data
Eye-movements during reading contain rich infor-
mation on how the document is being read, what
the reader is interested in, where difficulties hap-
pen, etc. The movements are characterized by fix-
ations (short periods of steadiness), saccades (fast
movements), and regressions (backward saccades)
(Rayner, 1998). In order to analyze the effect of
commas on reading through the features, we col-
lected gaze data from subjects reading text in the
following settings.
[Subjects and Materials] Four native Man-
52
Categories Effect on readability Outward manifestation
Positive (?) Can improve readability. Presence would cause GF+.
Semi-positive (?) Might be necessary for readability, but the importance is not as obvious as a positive comma. Absence might cause GF-.
Semi-negative (2) Might be negative, but its severity is not as obvious as a negative comma. Absence might cause GF+.
Negative (?) Thought to reduce a document?s readability. Presence would cause GF-.
GF+/GF-: values of eye-tracking features that represent good/poor readability
Table 5: Comma categories
Subject Positive (?) Semi-positive (?) Semi-negative (2) Negative (?) Adjustment formula
L ?FT?>800 500<?FT??800 -100<?FT??500 ?FT?<-100 ?FT? = ?FT ? ?RT ? 200
C ?FT?>900 600<?FT??900 -200<?FT??600 ?FT?<-200 ?FT? = ?FT ? ?RT ? 275
T ?FT?>600 300<?FT??600 -300<?FT??300 ?FT?<-300 ?FT? = ?FT ? ?RT ? 250
Z ?FT?>650 350<?FT??650 -250<?FT??350 ?FT?<-250 ?FT? = ?FT ? ?RT ? 250
?FT = [ fixation time (without commas) [ms]]? [ fixation time (with commas) [ms]]
?RT = [ #regressions (without commas) ]? [ #regressions (with commas) ]
Table 6: Estimation formula for judging the contribution of commas to readability
ID ? ? 2 ?
6 13 6 4 5
7 8 6 1 0
10 5 0 1 1
12 1 4 2 0
14 4 4 5 1
18 5 1 4 3
79 11 4 9 4
82 5 6 2 0
ID ? ? 2 ?
121 11 2 6 0
294 9 9 4 1
401 10 7 2 2
406 5 6 5 2
413 8 5 6 3
423 11 4 7 4
438 6 16 6 0
Total 112 80 64 26
Table 7: Categories of annotated commas
darin Chinese speakers (graduate students and re-
searchers) read 15 newswire articles selected from
CTB 7.0 (included in the test data in Section 4.2).
Table 4 and Figure 3(a) show the materials as-
signed to each subject and a screenshot of one ma-
terial. Each article was presented in 12-15 points
of bold-faced Fang-Song font occupying 13?13,
14?15, 15?16 or 16?16 pixels along with a line
spacing of 5-10 pixels3.
[Apparatus] Figure 3(b) shows a scene of the
experiment. An EyeLink 1000 eye tracker (SR
Research Ltd., Toronto, Canada) with a desktop
mount monitored the movements of a right eye at
1,000 Hz. The subject?s head was supported at the
chin and forehead. The distance between the eyes
and the monitor was around 55 cm, and each Chi-
nese character subtended a visual angle 1?. Text
was presented on a 19? monitor at a resolution
of 800?600 pixels, with the brightness adjusted
to a comfortable level. The displayed article was
masked except for the area around a gaze point
(see Figure 3(c)) in order to confirm that the gaze
point was correctly detected and make the subject
concentrate on the area (adjusted for him/her).
[Procedure] Each article was presented twice
(once with/once without commas) to each subject.
3These values, as well as the screen position of the article,
were adjusted for each subject.
The one without commas was presented first4 (not
necessarily in a row). We did not give any compre-
hension test after reading; we just asked the sub-
jects to read carefully and silently at their normal
or lower speed, in order to minimize the effect of
the first reading on the second. The subjects were
informed of the presence or absence of commas
beforehand. The apparatus was calibrated before
the experiment and between trials. The experi-
ment lasted around two hours for each subject.
[Alignment of eye-tracking data to text] Figure 4
shows an example of the obtained eye-movement
trace map, where circles and lines respectively
mean fixation points and saccades, and color depth
shows their duration. The alignment of the data to
the text is a critical task, and although automatic
approaches have been proposed (Mart??nez-Go?mez
et al, 2012a; Mart??nez-Go?mez et al, 2012b), they
do not seem robust enough for our purpose. Ac-
cordingly, we here just compared the entire layout
of the gaze point distribution and that of the actual
text, and adjusted them to have relatively coherent
positions on the x-axis; i.e., the beginning and end
of the gaze point sequence in a line were made as
close as possible to those of the line in the text.
5.2 Analysis of Eye-movement Data
The gaze data were analyzed by focusing on re-
gions around each comma or where each one
should be (three characters left and right to the
comma5).
4If we had used the reversed order, the subject would have
knowledge about original comma distribution, and this would
cause abnormally quick reading of the text without commas.
With the order we set, conflicts between false segmentations
(made in first reading) and correct ones might bother the sub-
ject, which is trade-off (though minor) in the second reading.
5When a comma appeared at the beginning of a line, two
characters to the left and right of the comma and one charac-
53
1. If L Seg and R Seg are both very long, a comma must be put between them.
2. If two ? appear serially, one is necessary whereas the other might be optional or judged negative, but it still depends on the lengths of the siblings.
3. If two neighboring commas appear very close to each other, one of them is judged as negative whereas judgment on the other one is reserved.
4. If several (more than 2) ?s appear continually, one or more ?s might be reserved in consideration of the global condition.
5. A comma is always needed after a long sentence or clause without any syntactically significant punctuation with the function of segmentation.
6. If a ? appears near a ?, it might be judged as negative with a high probability. However, the judgment process is always from the bottom up, which
means ? ? 2? ???. For example, if a 2 appears near a ?, we judge 2 first (to be positive or negative), then judge the ? in the condition
with or without the comma of 2.
Table 8: General rules for reference
Figure 5, 6 and 7 respectively show the total
viewing time, fixation time (duration for all fix-
ations and saccades in a target region) per comma,
and number of regressions per comma6 for each
trial. We can see a general trend wherein the for-
mer two were shorter and the latter was smaller for
the articles with commas than without. The diver-
sity of the subjects was also observed in Figure 6.
Figure 8 and 9 show the saccade length per
comma for different measures. The former (lat-
ter) figure considers a saccade in which at least
one edge (both edges) was in the region. We can-
not see any global trend, probably because of the
difference in global layout of materials brought by
the presence or absence of commas.
5.3 Categorization of Commas
Using the features shown to be effective to repre-
sent the effect of comma placement, we analyzed
the statistics for each comma in order to manu-
ally construct an estimation formula for judging
the contribution of each comma to readability. The
contribution was classified into four categories
(Table 5), and the formula is described in Table 67.
The adjustment formula was based on our obser-
vation that the number of regressions could only
be regarded as an aid. For example, for subject
C, if ?FT=200ms and ?RT =?2, ?FT?=?350,
and therefore, the comma is annotated as negative.
All parameters were decided empirically and man-
ually checked twice (self-judgment and feedback
from the subjects).
On the basis of this estimation formula, all arti-
cles in Table 4 were manually annotated. Table 7
shows the distribution of the assigned categories8.
ter to the left and right of the final character of the last line
were analyzed.
6Calculated by counting the instances where the x-
position of [a fixation / end point of a saccade ] was ahead
of [the former fixation / its start point]. Although the counts
of these two types were almost the same, by counting both of
them, we expected to cover any possible regression.
7One or two features are used to judge the category of a
comma. We will explore more features in the future.
8In the case of severe contradictions, the annotators dis-
cussed them and resolved them by voting.
5.4 Implementation of Rule-based Filter
The annotated commas were classified into Cases
1 to 5 in Table 1, based on the types of left and
right segment conjuncts (L Seg and R Seg, which
were obtained from the parse trees in CTB). For
each of the five cases, the reason for the assign-
ment of a category (?, ?, 2 or ?) to each
comma was explained by a manually constructed
rule which utilized information about L Seg and
R Seg. The rules were constructed so that they
would cover as many instances as possible. Ta-
ble 8 shows the general rules utilized as a refer-
ence, and Table 9 shows the finally obtained rules.
The rightmost column in this table shows the num-
ber of commas matching each rule. These rules
were then implemented as a filter for classifying
commas in a given text.
For several rules (?10, 28, 210, 211 and
212), there were only single instances. In addi-
tion, although our rules were built carefully, a few
exceptions to the detailed threshold were found.
Collecting and investigating more gaze data would
help to make our rules more sophisticated.
6 Performance of the Rule-based Filter
We assumed that our comma predictor provides a
CTB text with the same distribution as the origi-
nal one in CTB (see Figure 1). Accordingly, we
examined the quality of the comma categorization
by our rule-based filter through gaze experiments.
6.1 Experimental Settings
Another five native Mandarin Chinese speakers
were invited as test subjects. The CTB articles as-
signed to the subjects are listed in Table 10. These
articles were selected from the test data in Sec-
tion 4.2 in such a way that 520<#characters<700,
#commas>17, #commas/#punctuations>38%,
and #commas/#characters>3.1%, since we
needed articles of appropriate length with a fair
number of commas. After that, we manually
chose articles that seemed to attract the subjects?
interest from those that satisfied the conditions.
54
Case 1: L Subject + R Predicate #commas
?6 L IP-SBJ + R VP (length both<14 (In Seg Len)) 2
?7 L IP-SBJ/NP-SBJ (Org Len>13, Ttl Len>15) 7
?6 L NP-SBJ/IP-SBJ (<14) + R VP (?25) 2
Case 2: L Predicate + R Object #commas
?9 Long frontings (Modifier/Subject, >7) + short L predicate (VV/VRD/VSB? ? ? , ?3) + Longer R object (IP-OBJ, >28) 6
?8 Short frontings (<5) + short L predicate (<3) + moderate-length R object (IP-SBJ, <20) 4
26 Short frontings (<6) + short L predicate (?3) + long R object (IP-SBJ, >23) 9
Case 3: L Modifier #commas
?3 Short frequently used L modifier (2-3,??,??, etc.) + moderate-length/long R SPO (?w18p10) 13
?7 Short L (PP/LCP)-TMP (5, 6) + long R NP (?10) 4
?10 Long L CP-CND (e.g.,??, >18) + moderate-length R Seg (SPO, IP, etc. <18) 1
?1 Long L modifier (PP(-XXX, P+Long NP/IP), IP-ADV, ?17) 6
?4 Moderate-length/short L modifier (PP(-XXX, P+IP, There is IP inside, >6<15, cf. 26 (NP)) 9
?9 Long L (PP/LCP)-TMP (Ttl Len?10), short R Seg (NP/ADVP, <3) 4
?10 Short L (LCP/PP)-LOC (<8) 2
22 Long L LOC (or there is LCP inside PP, >10) 5
23 Very short frequently used L ADVP/ADV (2) 8
25 Short L (PP/LCP/NP)-TMP (4;5-6, when R Seg is short (<10)) 12
24 Moderate-length PP(-XXX, P+NP, >8 ?13) + R Seg (SPO, IP, VO, MSPO, etc.) 6
28 Short L IP-CND (<8) 1
211 Long L PP-DIR (>20) + short R VO (?10) 1
?2 Very short L (QP/NP/LCP)-TMP (?3) 8
?5 Short frequently used L modifier (as in ?3, ?3) + short/moderate-length R Seg (SPO etc., <c20w9) 1
Case 4: L c + R c #commas
?2 L c & R c are both long (In Seg Len?15; or one>13, the other near 20) 39
?8 L c is the summary of R c 2
?2 Moderate-length L c + R c (both ?10?15; or one?17, the other?12) 25
?3 Moderate-length clause (>10), but connected with familiar CC or ADVP 6
?5 Three or more consecutive moderate-length clauses (all<15, and at least one ?10) 12
?7 Very short L c + R c (both <5), something like slogan) 1
Case 5: L p + R p #commas
?1 Short coordinate modifiers (Both side <5) 4
?4 Short L p+R p (both<c15w5, and at least one <10), but pre-L p (e.g., SBJ) is too long (>18) 2
?5 Between two moderate-length/long phrases (both ?15; or L p?17, R p=10-14; Or L p=10-14, R p>20) 39
?11 Long pre-L p (SBJ /ADV, etc. >16) + short L p (?5) + long R p (?18) 2
(?3 Moderate-length phrase (>10), but connected with familiar CC or ADVP) (6)
?6 Three or more consecutive short/moderate-length phrases (both<15, at least one<8) 5
21 Between short phrases (both ?c13w5), and pre-L p (SBJ/ADV, etc.) is short/moderate-length (<11) 13
27 Coordinate VPs, and L VP is a moderate-length VP (PP-MNR VP) 4
29 Phrasal coordination between a long (?18) and a short (<10) phrase 3
210 Moderate-length coordinate VPs (>10<15), and R VP has the structure like VP (MSP VP) 1
212 Between two short/moderate-length NP phrases (both ?15, e.g., L NP-TPC+R NP-SBJ) 1
?1 Moderate-length/short phrase ((i) c:one>10<18, The other >5?10, w:one?5, the other>5?10; (ii) c:both?10<15, 13
w:both>5?7), and pre-L p (SBJ/ADV, etc.) is short (?5)
? L x/R x: the left/right segment of a target comma which is x.
(x can be ?p? (phrase) / ?c? (clause), syntactic tags (with function tags) such as ?VP? and ?IP-SBJ?, or general functions such as ?Subject? and ?Predicate?.)
? Org Len: the number of characters in a segment (including other commas or punctuation inside).
? In Seg Len/Ttl Len: the number of characters between the comma and nearest punctuation (inside a long/outside a short target segment).
? SPO: subject + predicate + object, belonging to the outermost sentence. The length is defined in the similar way as In Seg Len.
? MSPO: modifier + subject + predicate + object. The length is defined in the similar way as In Seg Len.
? -XX or -XXX: arbitrary type of possible functional tag (or without any functional tag) connected with the former syntactic tag.
? ?ciwj: #characters?i and #words?j.
? In some cases (in Case 3, 4 and 5), the length is calculated after negative (or judged negative) commas are eliminated.
? The rules related with TMP are applied faster than ones related with LCP (in Case 3).
? ?3 appears in both Case 4 (clause) and Case 5 (phrase). The number of commas is given by the sum of those in both cases.
Table 9: Entire classification of rules based on traditional comma categories
(A) #Characters,
Article (B) #Punctuations, (C) / (A) (C) / (B) Subjects
ID (C) #Commas
6 692 49 28 4.04% 57.14% L, S, H
11 672 48 21 3.13% 43.75% L, S, F
15 674 67 26 3.86% 38.81% L, S, H
16 547 43 22 4.02% 51.16% L, S, F
56 524 43 18 3.44% 41.86% L, H, M
73 595 46 28 4.71% 60.87% S, H, F, M
79 655 53 28 4.27% 52.83% H, F, M
99 671 55 24 3.58% 43.64% F, M
Average 628.75 50.50 24.38 3.88% 48.27% -
Table 10: Materials assigned to each subject
Our rule-based filter was applied to the commas
of each article9, and the commas were classified
9Instances of incoherence among the applied rules were
0
40,000
80,000
120,000
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)Positive distribution Negative distribution
040
8012
Tot
al v
iew
ing
tim
e (se
c.)
Figure 10: Total viewing time for two distributions
into two distributions: a positive one (positive +
semi-positive commas) and a negative one (nega-
tive + semi-negative commas). Two types of ma-
terials were thus generated by leaving the commas
in one distribution and removing the others.
manually checked and corrected.
55
20
40
60
80
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
FFT
(10
0) Positive distribution Negative distribution
Figure 11: EMFFT for two distributions
46
810
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
FT
(80
0) Positive distribution Negative distribution
Figure 12: EMFT for two distributions
The apparatus and procedure were almost the
same as those in Section 5.1, whereas, on the ba-
sis of the feedback from the previous experiments,
the font size, number of characters in a line, and
line spacing were fixed to single optimized values,
respectively, 14-point Fang-Song font occupying
15?16 pixels, 33 characters and 7 pixels.
6.2 Evaluation Metrics
We examined whether our positive/negative distri-
butions really facilitated/obstructed the subjects?
reading process by using the following metrics:
TT, EMFFT = FFTFT
10
, EMFT = FTCN?TT
11
,
EMRT = RT2?CN
12
, EMSLO = SLO2?TT ,
where TT, FT, RT and CN are total viewing time,
fixation time, number of regressions, and num-
ber of commas respectively, as described in Sec-
tion 5.2. FFT and SLO are additionally introduced
metrics respectively for the ?total duration for all
first-pass fixations in a target region that exclude
any regressions? and for the ?length of saccades
from inside a target region to the outside?13. All of
the areas around commas appearing in the original
article were considered target areas for the metrics.
The other settings were the same as in Section 5.
6.3 Contribution of Categorized Commas
Figure 10, 11, 12, 13 and 14 respectively show TT,
EMFFT , EMFT , EMRT and EMSLO for two types
of comma distributions in each trial.
10Ratio to the total fixation time in the target areas (FT).
11Normalized by the total viewing time (TT).
12Two types of RT count (see Section 5.2) were averaged.
13Respectively to reflect ?the early-stage processing of the
region? and ?the information processed for a fixation and a
decision of the next fixation point? (Hirotani et al, 2006).
0
5
10
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
RT
(1
0) Positive distribution Nega ive distribution
Figure 13: EMRT for two distributions
0
5
10
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)Positive distribution Negative distribution
EM
SLO
(10
0)
Figure 14: EMSLO for two distributions
For TT, we cannot see any general trend, mainly
because this time, the reading order of the text
was random, which spread out the second reading
effect evenly between the two distributions. For
EMFFT , we cannot reach a conclusion either. In
contrast, in more than half of the trials, EMFFT
was larger for positive distributions, which would
imply that the positive commas helped to prevent
the reader?s gaze from revisiting the target regions.
For most trials, except for subject S whose cal-
ibration was poor and reading process was poor
in M56, EMFT and EMRT decreased and EMSLO
increased for positive distributions, which implies
that the positive commas smoothed the reading
process around the target regions.
7 Conclusion
We proposed an approach for modeling comma
placement in Chinese text for smoothing reading.
In our approach, commas are added to the text on
the basis of a CRF model-based comma predic-
tor trained on the treebank, and a rule-based filter
then classifies the commas into ones facilitating or
obstructing reading. The experimental results on
each part of this approach were encouraging.
In our future work, we would like see how com-
mas affect reading by using much more material,
and thereby refine our framework in order to bring
a better reading experience to readers.
Acknowledgments
This research was partially supported by Kakenhi,
MEXT Japan [23650076] and JST PRESTO.
56
References
Wallace Chafe. 1988. Punctuation and the prosody of
written language. Written Communication, 5:396?
426.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2010. A linguistically inspired statistical model
for Chinese punctuation generation. ACM Trans-
actions on Asian Language Information Processing,
9(2):6:1?6:27, June.
Masako Hirotani, Lyn Frazier, and Keith Rayner. 2006.
Punctuation and intonation effects on clause and
sentence wrap-up: Evidence from eye movements.
Journal of Memory and Language, 54(3):425?443.
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Pause and
stop labeling for Chinese sentence boundary detec-
tion. In Proceedings of Recent Advances in Natural
Language Processing, pages 146?153.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proceedings of the International Conference on
Spoken Language Processing, pages 917?920.
Mei xun Jin, Mi-Young Kim, Dongil Kim, and Jong-
Hyeok Lee. 2002. Segmentation of Chinese
long sentences using commas. In Proceedings of
the Third SIGHAN Workshop on Chinese Language
Processing, pages 1?8.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survery of advances in biomed-
ical named entity recognition. In Pacific Symposium
on Biocomputing (PSB?08), pages 652?663.
Baolin Liu, Zhongning Wang, and Zhixing Jin. 2010.
The effects of punctuations in Chinese sentence
comprehension: An erp study. Journal of Neurolin-
guistics, 23(1):66?68.
Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random fields.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
?10), pages 177?186.
Pascual Mart??nez-Go?mez, Chen Chen, Tadayoshi Hara,
Yoshinobu Kano, and Akiko Aizawa. 2012a. Image
registration for text-gaze alignment. In Proceedings
of the 2012 ACM international conference on Intel-
ligent User Interfaces (IUI ?12), pages 257?260.
Pascual Mart??nez-Go?mez, Tadayoshi Hara, Chen
Chen, Kyohei Tomita, Yoshinobu Kano, and Akiko
Aizawa. 2012b. Synthesizing image representa-
tions of linguistic and topological features for pre-
dicting areas of attention. In Patricia Anthony, Mit-
suru Ishizuka, and Dickson Lukose, editors, PRICAI
2012: Trends in Artificial Intelligence, pages 312?
323. Springer.
Andrew Kachites McCallum. 2002. MALLET: A ma-
chine learning for language toolkit.
Fu-dong Chiou Naiwen Xue, Fei Xia and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Stephan Peitz, Markus Freitag, Arne Mauser, and Her-
mann Ney. 2011. Modeling punctuation prediction
as machine translation. In Proceedings of Interna-
tional Workshop on Spoken Language Translation,
pages 238?245.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124(3):372?422.
Gui-Qin Ren and Yufang Yang. 2010. Syntac-
tic boundaries and comma placement during silent
reading of Chinese text: evidence from eye move-
ments. Journal of Research in Reading, 33(2):168?
177.
Burr Settles. 2005. ABNER: an open source tool
for automatically tagging genes, proteins, and other
entity names in text. Bioinformatics, 21(14):3191?
3192.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-
Tu?r, and Go?khan Tu?r. 2000. Prosody-based au-
tomatic segmentation of speech into sentences and
topics. Speech Communication, 32(1-2):127?154.
Judy Perkins Walker, Kirk Fongemie, and Tracy
Daigle. 2001. Prosodic facilitation in the resolu-
tion of syntactic ambiguities in subjects with left
and right hemisphere damage. Brain and Language,
78(2):169?196.
Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classification. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics:shortpapers,
pages 631?635.
Ming Yue. 2006. Discursive usage of six Chinese
punctuation marks. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages
43?48.
ZDIC.NET. 2005. Commonly used Chinese punctua-
tion usage short list. Long Wiki, Retrieved Dec 10,
2012, from http://www.zdic.net/appendix/f3.htm.
(in Chinese).
X. Y. Zeng. 2006. The comparison and the use
of English and Chinese comma. College English,
3(2):62?65. (in Chinese).
57
Kaixu Zhang, Yunqing Xia, and Hang Yu. 2006.
CRF-based approach to sentence segmentation and
punctuation for ancient Chinese prose. Jour-
nal of Tsinghua Univ (Science and Technology),
49(10):1733?1736. (in Chinese).
58
