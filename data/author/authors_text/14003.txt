Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 311?314,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JHU System Combination Scheme for WMT 2010
Sushant Narsale
Johns Hopkins University
Baltimore, USA.
sushant@jhu.edu
Abstract
This paper describes the JHU system
combination scheme that was used in
the WMT 2010 submission. The in-
cremental alignment scheme of (Karakos
et.al, 2008) was used for confusion net-
work generation. The system order
in the alignment of each sentence was
learned using SVMs, following the work
of (Karakos et.al, 2010). Additionally,
web-scale n-grams from the Google cor-
pus were used to build language models
that improved the quality of the combi-
nation output. Experiments in Spanish-
English, French-English, German-English
and Czech-English language pairs were
conducted, and the results show approxi-
mately 1 BLEU point and 2 TER points
improvement over the best individual sys-
tem.
1 Introduction
System Combination refers to the method of com-
bining output of multiple MT systems, to pro-
duce a output better than each individual system.
Currently, there are several approaches to ma-
chine translation which can be classified as phrase-
based, hierarchical, syntax-based (Hildebrand and
Vogel, 2008) which are equally good in their trans-
lation quality even though the underlying frame-
works are completely different. The motivation
behind System Combination arises from this di-
versity in the state-of-art MT systems, which sug-
gests that systems with different paradigms make
different errors, and can be made better by com-
bining their strengths.
One approach of combining translations is
based on representing translations by confusion
network and then aligning these confusion net-
works using string alignment algorithms (Rosti
et.al, 2009), (Karakos and Khudanpur, 2008).
Another approach generates features for every
translation to train algorithms for ranking systems
based on their quality and the top ranking output
is considered to be a candidate translation, (Hilde-
brand and Vogel, 2008) is an example of ranking
based combination. We use ideas from ranking
based approaches to learn order in which systems
should be aligned in a confusion network based
approach.
Our approach is based on incremental align-
ment of confusion networks (Karakos et.al, 2008),
wherein each system output is represented by a
confusion network. The confusion networks are
then aligned in a pre-defined order to generate a
combination output. This paper contributes two
enhancements to (Karakos et.al, 2008). First,
use of Support Vector Machines to learn order in
which the system outputs should be aligned. Sec-
ond, we explore use of Google n-grams for build-
ing dynamic language model and interpolate the
resulting language model with a large static lan-
guage model for rescoring of system combination
outputs.
The rest of the paper is organized as follows:
Section 2 illustrates the idea and pipeline of the
baseline combination system; Section 3 gives de-
tails of SVM ranking for learning system order
for combination; Section 4 explains use of Google
n-gram based language models; Results are dis-
cussed in Section 5; Concluding remarks are given
in Section 6;
2 Baseline System Combination
This section summarizes the algorithm for base-
line combination. The baseline combination
pipeline includes three stages:
1. Representing translations by confusion net-
works.
311
2. Generating between system confusion net-
works.
3. Rescoring the final confusion network.
Confusion networks are compressed form of
lattices with a constraint that all paths should pass
through all nodes. Each system output is repre-
sented by an equivalent confusion network. The
per-system confusion networks are aligned one at
a time. The order in which systems are aligned
is usually decided by evaluation of system?s per-
formance. Two alternatives for deciding the sys-
tem order are discussed in Section 3. Inversion-
Transduction Grammar (Wu, 1997) is used for
alignments and the cost function for aligning two
confusion networks is
cost(b1, b2) =
1
|b1||b2|
?
w?b1
?
v?b2
c(v)c(w)1(w 6= v)
where b1 and b2 are two different bins, |b1| and |b2|
is the number of tokens in b1 and b2 respectively,
c(v) and c(w) are the number of words of token
v and token w. which are in b1 and b2 separately.
The idea of this cost is to compute the probability
that a word from bin b1 is not equal to a word from
bin b2.
cost(b1, b2) = Prob(v 6= w, v ? b1, w ? b2)
The final confusion network is rescored with a
5-gram language model with Kneser-Ney smooth-
ing. To generate the final output, we need to find
the best (minimum-cost) path through the rescored
confusion network. In the best path every bin in
the network contributes only one word to the out-
put.
Ordering the systems for incremental combina-
tion and use of different language models were the
two components of the pipeline that were experi-
mented with for WMT?2010 shared task. The fol-
lowing sections describe these variations in detail.
3 Learning to Order Systems for
Combination
Determining the order in which systems are
aligned is critical step in our system combination
process. The first few aligned translations/systems
determine the word ordering in the final output and
have a significant influence on the final transla-
tion quality. For the baseline combination the sys-
tems are aligned in the increasing order of (TER-
BLEU) scores. TER and BLEU (Papineni et.al,
2002) scores are calculated over all the sentences
in the training set. This approach to ordering of
systems is static and results in a global order for
all the source segments. An alternative approach
is to learn local order of systems for every source
sentence using a SVM ranker.
3.1 SVM Rank Method
This section describes an approach to order sys-
tems for alignment using SVMs (Karakos et.al,
2010). For each system output a number of fea-
tures are generated, the features fall broadly under
the following three categories:
N-gram Agreements
These features capture the percentage of hypoth-
esis for a source sentence that contain same n-
grams as the candidate translation under consid-
eration. The n-gram matching is position indepen-
dent because phrases often appear in different or-
ders in sentences with same meaning and correct
grammar. The scores for each n-gram are summed
and normalized by sentence length. N-grams of
length 1 ? ? ? 5 are used as five features.
Length Feature
The ratio of length of the translation to the source
sentence is a good indication of quality of the
translation, for a lengthy source sentence a short
translation is most likely to be bad. Here, the ra-
tio of source sentence length to length of the target
sentence is calculated.
Language Model Features
Language models for target language are used to
calculate perplexity of a given translation. The
lower the perplexity the better is the translation
quality. We use two different language models:
(i) a large static 5-gram language model and (ii)a
dynamic language model generated from all the
translations of the same source segment. The
perplexity values are normalized by sentence
length.
Translations in training set are ranked based
on (TER-BLEU) scores. An SVM ranker is then
trained on this set. The SVM ranker (Joachims,
2002) returns a score for each translation, based
on its signed distance from the separating hyper-
plane. This value is used in the combination pro-
cess to weight the contribution of systems to the
final confusion network scores.
312
Table 1: Results for all Language pairs on development set
es-en fr-en cz-en de-en
Combination BLEU TER BLEU TER BLEU TER BLEU TER
BEST SYSTEM 29.27 52.38 26.74 56.88 21.56 58.24 26.53 56.87
BASELINE 28.57 51.61 27.65 55.20 21.01 58.79 26.80 54.54
SVM 28.68 51.99 27.53 55.35 21.56 58.24 26.85 54.9
SVM+NGRAM 29.92 50.92 27.86 55.06 21.80 57.78 27.24 54.86
4 Language Models
In the system combination process, the final con-
fusion networks are rescored with language mod-
els. Language models are widely used to en-
sure a fluent output translation. I explored use of
two language models. The first language model
was trained on the English side of French-English
corpus, UN corpus and English Gigaword cor-
pus made available by WMT. The second lan-
guage model used counts generated from Google
n-grams. It was trained by generating all 1-gram
to 5-grams in the system outputs for a source
segment and then using the N-gram search en-
gine (Lin et.al, 2010) built over Google n-grams
to get the corresponding n-gram counts. The n-
gram counts were used to train a 5-gram language
model with Kneser-Ney smoothing. SRILM
toolkit (Stockle, 2002) was used for training the
language models.
The baseline combinations were rescored only
with the static language model. I always did a
weighted interpolation of the two language mod-
els when using n-gram based language model.
5 Results
Results for four language pairs: Spanish-English,
French-English, Czech-English and German-
English are presented. The training data for
WMT?10 was divided into development and test
set, consisting of 208 and 247 segments respec-
tively. Table 1 shows TER and BLEU scores
on the TEST set for all the four language pairs
in the following settings: (i) Baseline corre-
sponds to procedure described in section 2, (ii)
SVM corresponds to using SVM ranker for learn-
ing order of systems as described in section 3.1
(iii)SVM+N-Grams corresponds to the use of a
SVM ranker along with weighted interpolation of
n-gram language model and the large static lan-
guage model. The ranking SVM was trained us-
ing SVM-light (Joachims, 2002) with a RBF ker-
nel. Two-fold cross-validation was done to pre-
vent over-fitting on development data. All the
scores are with lower-cased outputs, a tri-gram
language model was used to true-case the output
before the final submission. 1-best output from
only the primary systems were used for combina-
tion. The number of systems used for combination
in each language pair are: 6 for Czech-English,
8 in Spanish-English, 14 in French-English and
16 in German-English. The best results for base-
line combination were obtained with 3 systems
for Czech-English, 6 systems for German-English,
3 systems for Spanish-English and 9 systems for
French-English.
From the results, we conclude that for all lan-
guage pairs the combinations with SVM and n-
gram language models show gain over all the other
settings in both TER and BLEU evaluations. How-
ever, use of SVM with only one large language
model shows performance degradation on three
out of four language pairs. Size of training data
(208 segments) could be one reason for the degra-
dation and this issue needs further investigation.
For the final submission, the settings that per-
formed the best on (TER?BLEU)2 scale were cho-
sen.
6 Conclusion
The system combination task gave us an opportu-
nity to evaluate enhancements added to the JHU
system combination pipeline. Experimental re-
sults show that web-scale language models can be
used to improve translation quality, this further un-
derlines the usefulness of web-scale resources like
Google n-grams. Further investigation is needed
to completely understand the reasons for incon-
sistency in the magnitude of gain across different
language pairs. Specifically the impact of training
data on SVMs for ranking in system combination
scenario needs to be analysed.
313
Acknowledgments
This work was partially supported by the DARPA
GALE program Grant No HR0022-06-2-0001. I
would like to thank all the participants of WMT
2010 for their system outputs. I would also like
to thank Prof. Damianos Karakos for his guidance
and support. Many thanks go to the Center for
Language and Speech Processing at Johns Hop-
kins University for availability of their computer
clusters.
References
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of Machine Translation Systems via
Hypothesis Selection from Combined N-Best Lists.
In MT at work: Proceedings of the Eight Conference
of Association of Machine Translation in the Amer-
icas, pages 254-261, Waikiki, Hawaii, October. As-
sociation for Machine Translations in the Americas.
Almut Silja Hildebrand and Stephan Vogel. 2009.
CMU System Combination for WMT?09. Proceed-
ings of Fourth Workshop on Statistical Machine
Translation,Athen,Greece, March 2009.
Andreas Stockle. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings International Con-
ference for Spoken Language Processing, Denver,
Colarado, September.
Antti-Veikko I. Rosti and Necip Fazil Ayan and Bing
Xiang and Spyros Matsoukas and Richard Schwartz
and Bonnie J. Dorr 2007. Combining Outputs from
Multiple Machine Translation Systems. In Proceed-
ings of the Third Workshop on Statistical Machine
Transaltion, pages 183-186, Colombus, Ohio, June.
Association for Computational Linguistics.
Damianos Karakos and Sanjeev Khudanpur 2008. Se-
quential System Combination for Machine Transla-
tion of Speech. In Proceedings of IEEE SLT-08, De-
cember 2008.
Damianos Karakos and Jason Smith and Sanjeev Khu-
danpur 2010. Hypothesis Ranking and Two-pass
Approaches for Machine Translation System Com-
bination. In Proceedings of ICASSP-2010, Dallas,
Texas, March 14-19 2010.
Damianos Karakos and Jason Eisner and Sanjeev Khu-
danpur and Markus Dreyer. 2008. Machine Trans-
lation system combination using ITG-based align-
ments. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 81-84, Colombus, Ohio, June. Associa-
tion for Computational Linguistics.
Dekang Lin and Kenneth Church and Heng Ji and
Satoshi Sekine and David Yarowsky and Shane
Bergsma and Kailash Patil and Emily Pitler Rachel
Lathbury and Vikram Rao and Kapil Dalwani and
Sushant Narsale 2010. New Tools for Web-Scale
N-grams. In the Proceedings of LREC, 2010.
D. Wu 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora.
Computational Linguistics, vol.23,no.3,pp.377-403,
September 1997.
Kishore Papineni and Salim Roukos and Todd Ward
and Wei-Jing Zhu. 2002. BLEU: A method for
automatic evaluation of machine translation. In
Proceedings of 40th Annual Meeting of Associa-
tion for Computational Linguistics, pages 311-318.
Philadelphia, PA, July.
Thorsten Joachims 2002. Optimizing Search Engines
using Clickthrough Data. In Proceedings of ACM
Conference on Knowledge Discovery and Data Min-
ing(KDD), 2002.
314
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 163?170,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Combining Quality Prediction and System Selection for Improved
Automatic Translation Output
Radu Soricut
SDL Language Weaver
6060 Center Drive, Suite 150
Los Angeles, CA 90045
rsoricut@sdl.com
Sushant Narsale?
Google Inc
1600 Amphitheatre Parkway
Mountain View, CA 94043
snarsale@google.com
Abstract
This paper presents techniques for reference-
free, automatic prediction of Machine Trans-
lation output quality at both sentence- and
document-level. In addition to helping with
document-level quality estimation, sentence-
level predictions are used for system selection,
improving the quality of the output transla-
tions. We present three system selection tech-
niques and perform evaluations that quantify
the gains across multiple domains and lan-
guage pairs.
1 Introduction
Aside from improving the performance of core-
translation models, there additionally exist two
orthogonal approaches via which fully-automatic
translations can achieve increased acceptance and
better integration in real-world use cases. These two
approaches are: improved translation accuracy via
system combination (Rosti et al, 2008; Karakos et
al., 2008; Hildebrand and Vogel, 2008), and auto-
matic quality-estimation techniques used as an ad-
ditional layer on top of MT systems, which present
the user only with translations that are predicted as
being accurate (Soricut and Echihabi, 2010; Specia,
2011).
In this paper, we describe new contributions to
both these approaches. First, we present a novel
and superior technique for performing quality esti-
mation at document level. We achieve this by chang-
?Research was completed before the author started in his
current role at Google Inc. The opinions stated are his own and
not of Google Inc.
ing the granularity of the prediction mechanism
from document-level (Soricut and Echihabi, 2010)
to sentence-level, and predicting BLEU scores via
directly modeling the sufficient statistics for BLEU
computation. A document-level score is then recre-
ated based on the predicted sentence-level sufficient
statistics. A second contribution is related to system
combination (or, to be more precise, system selec-
tion). This is an intended side-effect of the granular-
ity change: since the sentence-level statistics allow
us to make quality predictions at sentence level, we
can use these predictions to perform system com-
bination by selecting among various sentence-level
translations produced by different MT systems. That
is, instead of presenting the user with a document
with sentences translated entirely by a single system,
we can present documents for which, say, 60% of
the sentences were translated by system A, and 40%
were translated by system B. We contribute a novel
set of features and several techniques for choos-
ing between competing machine translation outputs.
The evaluation results show better output quality,
across multiple domains and language pairs.
2 Related Work
Several approaches to reference-free automatic MT
quality assessment have been proposed, using classi-
fication (Kulesza and Shieber, 2004), regression (Al-
brecht and Hwa, 2007), and ranking (Ye et al, 2007;
Duh, 2008). The focus of these approaches is on sys-
tem performance evaluation, as they use a constant
test set and measure various MT systems against it.
In contrast, we are interested in evaluating the
quality of the translations themselves, while treat-
163
ing the MT components as constants. In this re-
spect, the goal is more related to the area of con-
fidence estimation for MT (Blatz et al, 2004). Con-
fidence estimation is usually concerned with iden-
tifying words/phrases for which one can be confi-
dent in the quality of the translation. A sentence-
level approach to quality estimation is taken on the
classification-based work of Gamon et al (2005) and
regression-based work of Specia et al (2009).
Our approach to quality estimation focuses on
both sentence-level and document-level estimation.
We improve on the quality estimation technique that
is proposed for document-level estimation in (Sori-
cut and Echihabi, 2010). Furthermore, we exploit
the availability of multiple translation hypotheses to
perform system combination. Our system combina-
tion methods are based on generic Machine Learn-
ing techniques, applied on 1-best output strings. In
contrast, most of the approaches to MT system com-
bination combine N-best lists from multiple MT sys-
tems via confusion network decoding (Karakos et
al., 2008; Rosti et al, 2008). The closest system
combination approach to our work is (Hildebrand
and Vogel, 2008), where an ensemble of hypothe-
ses is generated by combining N-best lists from all
the participating systems, and a log-linear model is
trained to select the best translation from all the pos-
sible candidates.
In our work, we show that it is possible to gain
significant translation quality by taking advantage
of only two participating systems. This makes the
system-combination proposition much more palat-
able in real production deployment scenarios for
Machine Translation, as opposed to pure research
scenarios as the ones used in the previous NIST and
DARPA/GALE MT efforts (Olive et al, 2011). As
our evaluations show, the two participating systems
can be at very similar performance levels, and yet a
system-selection procedure using Machine Learning
techniques can achieve significant translation im-
provements in quality. In addition, in a scenario
where quality estimation needs to happen as a re-
quirement for MT integration in large applications,
having two translation systems producing transla-
tions for the same inputs is part of the deployment
set-up (Soricut and Echihabi, 2010). The improve-
ment in overall translation quality comes in these
cases at near-zero cost.
3 Sentence-level Quality Predictions
The requirement for document-level quality esti-
mation comes from the need to present a fully-
automated translation solution, in which translated
documents are either good enough to be directly
published (or otherwise must undergo, say, a human-
driven post-processing pipeline). In the proposal of
Soricut and Echihabi (2010), regression models pre-
dict BLEU-like scores for each document, based on
document-level features.
However, even if the predicted value is at
document-level, the actual feature computation and
model prediction does not necessarily need to hap-
pen at document-level. It is one of the goals of this
work to determine if the models of prediction work
better at a coarser granularity (such as document
level) or finer granularity (such as sentence-level).
We describe here a mechanism for predicting
BLEU scores at sentence level, and then combin-
ing these scores into document-level scores. To
make explicit our prediction mechanism, we present
here in detail the formula for computing BLEU
scores (Papineni et al, 2002). First, n-gram preci-
sion scores Pn are computed as follows:
Pn =
?
C?Candidates
?
n-gram?C Countclip(n-gram)
?
C?Candidates
?
n-gram?C Count(n-gram)
(1)
where Countclip(n-gram) is the maximum number
of n-grams co-occurring in a candidate translation
and a reference translation, and Count(n-gram) is
the number of n-grams in the candidate translation.
To prevent very short translations that try to max-
imize their precision scores, BLEU adds a brevity
penalty, BP, to the formula:
BP =
{
1 if |c| > |r|
e(1?|r|/|c|) if |c| ? |r|
(2)
where |c| is the length of the candidate translation
and |r| is the length of the reference translation. The
BLEU formula is then written as follows:
BLEU = BP ? exp(
N?
n=1
wn log pn) (3)
where the weighting factors wn are set to 1/N , for
all 1 ? n ? 4.
164
3.1 The learning method
The results we report in this section are ob-
tained using the freely-available Weka engine. 1
For both sentence-level and document-level qual-
ity prediction, we report all the results using
Weka implementation of M5P regression trees
(weka.classifiers.trees.M5P).
We use the components of the BLEU score (Equa-
tions 1 and 2) to train fine-granularity M5P models
using our set of features (Section 3.2), for a total of
five individual regression-tree models (four for the
sentence-level precision scores Pn, 1 ? n ? 4 fac-
tors, and one for the BP factor). The numbers pro-
duced individually by our models are then combined
using the BLEU equation 3 into a sentence-level
BLEU score. The sentence-level predicted BLEU
scores play an important role in our system combi-
nation mechanism (see Section 4).
At the same time, we sum up the sufficient
statistics for the sentence-level precision scores Pn
(Equation 1) over all the sentences in a document,
thus obtaining document-level precision scores. A
document-level BP score (Equation 2) is similarly
obtained by summing over all sentences. Finally,
we plug the predicted document-level Pn and BP
scores in the BLEU formula (Equation 3) and arrive
at a document-level predicted BLEU score.
3.2 The features
Most of the features we use in this work are not
internal features of the MT system, but rather de-
rived starting from input/output strings. Therefore,
they can be applied for a large variety of MT ap-
proaches, from statistical-based to rule-based ap-
proaches. The features we use can be divided
into text-based, language-model?based, pseudo-
reference?based, example-based, and training-data?
based feature types (these latter features assume that
the engine is statistical and one has access to the
training data). These feature types can be computed
both on the source-side (MT input) and on the target-
side (MT output).
Text-based features
These features compute the length of the input in
terms of (tokenized) number of words. The source-
1Weka software at http://www.cs.waikato.ac.nz/ml/weka/.
side text feature is computed on the input string,
while the target-side text feature is computed to the
output translation string. These two features are use-
ful in modeling the relationship between the number
of words in the input and output and the expected
BLEU score for these sizes.
Language-model?based features
These features are among the ones that were first
proposed as possible differentiators between good
and bad translations (Gamon et al, 2005). They are
a measure of how likely a collection of strings is un-
der a language model trained on monolingual data
(either on the source or target side).
The language-model?based feature values we use
here are computed as perplexity numbers using a 5-
gram language model trained on the MT training
set. This can be achieved, for instance, by using
the publicly-available SRILM toolkit 2. These two
features are useful in modeling the relationship be-
tween the likelihood of a string (or set of strings)
under an n-gram language model and the expected
BLEU score for that input/output pair.
Pseudo-reference?based features
Previous work has shown that, in the ab-
sence of human-produced references, automatically-
produced ones are still helpful in differentiating be-
tween good and bad translations (Albrecht and Hwa,
2008). When computed on the target side, this
type of features requires one (or possibly more)
secondary MT system(s), used to generate transla-
tions starting from the same input. These pseudo-
references are useful in gauging translation conver-
gence, using BLEU scores as feature values. In in-
tuitive terms, their usefulness can be summarized as
follows: ?if system X produced a translation A and
system Y produced a translation B starting from the
same input, and A and B are similar, then A is prob-
ably a good translation?.
An important property here is that systems X and
Y need to be as different as possible from each other.
This property ensures that a convergence on sim-
ilar translations is not just an artifact of the sys-
tems sharing the same translation model/resources,
but a true indication that the translations converge.
The secondary systems we use in this work are
2Available at www-speech.sri.com/projects/srilm.
165
still phrase-based, but equipped with linguistically-
oriented modules similar with the ones proposed
in (Collins et al, 2005; Xu et al, 2009). Our exper-
iments indicate that this single feature is one of the
most powerful ones in terms of its predictive power.
Example-based features
For example-based features, we use a develop-
ment set of parallel sentences, for which we pro-
duce translations and compute sentence-level BLEU
scores. We set aside the top BLEU scoring sen-
tences and bottom BLEU scoring sentences. These
sets are used as positive examples (with better-than-
average BLEU) and negative examples (with worse-
than-average BLEU), respectively. We define a
positive-example?based feature function as a geo-
metric mean of 1-to-4?gram precision scores (i.e.,
the BLEU equation 3 with the BP term set to 1) be-
tween a string (on either source or target side) and
the positive examples used as references. That is,
we compute precision scores against all the positive
examples at the same time, similar with how mul-
tiple references are used to increase the precision
of the BLEU metric. (The negative-example?based
features are defined in an analogous way.) The set of
positive and negative examples is a fixed set that is
used in the same manner both at training-time (to
compute the example-based feature values for the
training examples) and at test-time (to compute the
example-based feature values for the test examples).
The intuition behind these features can be sum-
marized as follows: ?if system X translated A
well/poorly, and A and B are similar, then system X
probably translates B well/poorly?. The total num-
ber of features on this type is 4 (2 for positive ex-
amples against source/target strings, 2 for negative
examples against source/target strings).
Training-data?based features
If the system for which we make the predictions is
trained on a parallel corpus, the data in this corpus
can be exploited towards assessing translation qual-
ity (Specia et al, 2009; Soricut and Echihabi, 2010;
Specia, 2011). In our context, the documents that
make up this corpus can be used in a fashion simi-
lar with the positive examples. One type of training-
data?based features operates by computing the num-
ber of out-of-vocabulary (OOV) tokens with respect
to the training data (on source side).
A more powerful type of training-data?based fea-
tures operates by computing a geometric mean of 1-
to-4?gram precision score between a string (source
or target side) and the training-data strings used as
references. Intuitively, these features assess the cov-
erage of the candidate strings with respect to the
training data: ?if the n-grams of input string A are
well covered by the source-side of the training data,
then the translation of A is probably good? (on the
source side); ?if the n-grams in the output translation
B are well covered by the target-side of the parallel
training data, then B is probably a good translation?
(on the target side). The total number of features on
this type is 3 (1 for the OOV counts, and 2 for the
source/target-side n-gram coverage).
Given the described 12 feature functions, the
training for our five M5P prediction models is done
using the feature-function values at sentence-level,
and associating these values with reference labels
that are automatically-produced from parallel-text
using the sufficient-statistics of the BLEU score
(Equations 1 and 2).
3.3 Metrics for Quality Prediction
Performance
The metrics we use here are designed to answer the
following question: how well can we automatically
separate better translations from worse translations
(in the absence of human-produced references)?
A first metric we use is Ranking Accuracy (rAcc),
see (Gunawardana and Shani, 2009; Soricut and
Echihabi, 2010). In the general case, it measures
how well N elements are assigned into n quantiles
as a result of a ranking procedure. The formula is:
rAcc[n] = Avgni=1
TPi
N
n
=
1
N
?
n?
i=1
TPi
where TPi (True-Positivei) is the number of
correctly-assigned documents in quantile i. Intu-
itively, this formula is an average of the ratio of ele-
ments correctly assigned in each quantile. For sim-
plicity, we present here results using only 2 quan-
tiles (n = 2), which effectively makes the rAcc[2]
metric equivalent with binary classification accuracy
when the two sets are required to have equal size.
That is, we measure the accuracy of placing the 50%
166
Training BLEU Ranking rAcc[2] DeltaAvg[2]
Size Sys1 Sys2 Test Size Doc Sent Doc Sent
WMT09 Hungarian-English 26 Mw 26.9 26.9 510 Kw 88% 89% +8.3 +8.4
Travel English-French 30 Mw 32.3 34.6 282 Kw 77% 80% +9.1 +10.1
Travel English-German 44 Mw 40.6 43.4 186 Kw 74% 79% +9.8 +11.7
HiTech English-French 0.4 Mw 44.1 44.7 69 Kw 75% 77% +4.4 +6.0
HiTech English-Korean 16 Mw 37.4 36.1 80 Kw 78% 79% +9.3 +10.0
Table 1: MT system performance and ranking performance using BLEU prediction at Doc- and Sent-level.
best-translated documents (as measured by BLEU
against human reference) in the top 50% of ranked
documents. Note that a random assignment gives a
performance lower bound of 50% accuracy.
A second metric we use here is the DeltaAvg met-
ric (Callison-Burch et al, 2012). The goal of the
DeltaAvg metric is to measure how valuable a pro-
posed ranking (hypothesis) is from the perspective
of an extrinsic metric associated with the test en-
tries (in our case, the BLEU scores). The follow-
ing notations are used: for a given entry sentence s,
V (s) represents the function that associates an ex-
trinsic value to that entry; we extend this notation
to a set S, with V (S) representing the average of
all V (s), s ? S. Intuitively, V (S) is a quantitative
measure of the ?quality? of the set S, as induced by
the extrinsic values associated with the entries in S.
For a set of ranked entries S and a parameter n, we
denote by S1 the first quantile of set S (the highest-
ranked entries), S2 the second quantile, and so on,
for n quantiles of equal sizes.3 We also use the no-
tation Si,j =
?j
k=i Sk. Using these notations, the
metric is defined as:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1
? V (S) (4)
When the valuation function V is clear from the con-
text, we write DeltaAvg[n] for DeltaAvgV [n]. The
parameter n represents the number of quantiles we
want to split the set S into. For simplicity, we con-
sider there only the case for n = 2, which gives
DeltaAvg[2] = V (S1) ? V (S). This measures the
difference between the quality of the top quantile
(top half) S1 and the overall quality (represented by
3If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
V (S)). For the results presented here, the valuation
function V is taken to be the BLEU function (Equa-
tion 3).
3.4 Experimental Results
We measure the impact in ranking accuracy using a
variety of European and Asian language pairs, using
parallel data from various domains. One domain we
use is the publicly available WMT09 data (Koehn
and Haddow, 2009), a combination of European par-
liament and news data. Another domain, called
Travel, consists of user-generated reviews and de-
scriptions; and a third domain, called HiTech, con-
sists of parallel data from customer support for the
high-tech industry. Using these parallel data sets,
we train statistical phrase-based MT system similar
to (Och and Ney, 2004) as primary systems (Sys1).
As secondary systems (Sys2) we use phrase-based
systems equipped with linguistically-oriented mod-
ules similar with the ones proposed in (Collins et
al., 2005; Xu et al, 2009). Table 1 lists the size of
the parallel training data on which the MT systems
were trained in the first column, and BLEU scores
for the primary and secondary systems on held-out
1000-sentence test sets in the next two columns.
The training material for the regression-tree mod-
els consists of 1000-document held-out sets. (For
parallel data for which we do not have document
boundaries, we simply simulate document bound-
aries after every 10 consecutive sentences.) Simi-
larly, the Ranking test sets we use consist of 1000-
document held-out sets (see column 4 in Table 1 for
size). In the last four columns of Table 1, we show
the results for ranking the translations produced by
the primary MT system (Sys1). We measure the
ranking performance for the two granularity cases.
The one labeled as ?Doc? is an implementation of
167
the work described in (Soricut and Echihabi, 2010),
where the BLEU prediction is done using document-
level feature values and models. The one labeled
as ?Sent? is the novel one proposed in this paper,
where the BLEU prediction is done using sentence-
level feature values and models, which are then ag-
gregated into document-level BLEU scores.
Both rAcc[2] and DeltaAvg[2] numbers support
the choice of making document-level BLEU pre-
diction at a finer, sentence-based granularity level.
For Travel English-French, for instance, the accu-
racy of the ranking improves from 77% to 80%. To
put some intuition behind these numbers, it means
that 4 out of every 5 sentences that the ranker places
in the top 50% do belong there. At the same time,
the DeltaAvg[2] numbers for Travel English-French
indicate that the translation quality of the top 50%
of the 1000 Ranking Test documents exceeds by
10.1 BLEU points the overall quality of the trans-
lations (up from 9.1 BLEU points for the document-
level prediction). This large gap in the BLEU score
of the top 50% ranked sentences and the overall-
corpus BLEU indicates that these top-ranked trans-
lations are indeed of much better quality (closer to
the human-produced references). The same large
numbers are measured on the WMT09 data for
Hungarian-English. This is a set for which it is hard
to obtain significant improvements via core-model
translation improvements. Our quality-estimation
method allows one to automatically identify the top
50% of the sentences with 89% accuracy. This set of
top 50% sentences also has an overall BLEU score
of 35.3, which is better by +8.4 BLEU-points com-
pared to the overall BLEU score of 26.9 (we only
show the base overall BLEU score and the BLEU-
point gain in Table 2 to avoid displaying redundant
information).
4 System Combination at Sentence Level
Since we produce two translations for every input
sentence for the purpose of quality estimation, we
exploit the availability of these competing hypothe-
ses in order to choose the best one. In this section
we describe three system combination schemes that
choose between the output of the primary and sec-
ondary MT systems.
4.1 System Combination using Regression
This combination scheme makes use of the
regression-based sentence-level BLEU prediction
mechanism described in Section 3. It requires that
we also train and use an additional BLEU predic-
tion mechanism for which the secondary MT sys-
tem is now considered primary, and vice-versa. As a
consequence, we can predict a sentence-level BLEU
score for each of the two competing hypotheses. We
then simply choose the hypothesis with the highest
predicted BLEU score.
4.2 System Combination using Ranking
This approach is based on ranking the candidate
translations and then selecting the highest-ranked
translation as the final output. To this end we use
SVM-rank (Joachims, 1999), a ranking algorithm
built on SVM. We use SVM-rank with a linear ker-
nel and the same feature set as the regression-based
method (we make the observation here that only the
target-based features have discriminative power in
this context).
4.3 System Combination using Classification
In this approach, we model the problem of select-
ing the best output from the two candidate transla-
tions into a binary classification problem. We use the
same feature set as before for each candidate transla-
tion (again, only the target-based features have dis-
criminative power in this context).
The final feature vectors are obtained by subtract-
ing the values of the primary-system feature vec-
tor from the values of the secondary-system feature
vector. The binary classifier is trained to predict
?0? if the primary-system is better, and ?1? if the
secondary-system is better.
4.4 Experimental Results
In Table 2, we summarize the results for the three
system combination techniques discussed before
across our domains (WMT09, Travel, and Hi-Tech).
To get an upper bound on the performance of these
system combination techniques, we also compute
an oracle function which selects the translation
with highest BLEU score computed against human-
produced references.
The results in Table 2 indicate that the BLEU
improvements obtained by our system combina-
168
BLEU Oracle Regression Rank Classify
Sys1 Sys2
WMT09 Hungarian-English 26.9 26.9 30.7(+3.8) 29.0(+2.1) 29.0(+2.1) 28.9(+2.0)
Travel English-French 32.3 34.6 38.7(+3.9) 36.2(+1.6) 36.0(+1.4) 35.7(+1.1)
Travel English-German 40.6 43.4 47.2(+3.8) 44.5(+1.1) 44.0(+0.6) 44.9(+1.5)
HiTech English-French 44.1 44.7 49.8(+5.1) 46.1(+1.4) 46.3(+1.7) 45.3(+0.6)
HiTech English-Korean 37.4 36.1 42.2(+4.8) 39.4(+2.0) 39.1(+1.7) 38.8(+1.4)
Table 2: BLEU scores for the proposed system combination techniques across domains and language pairs.
Travel Eng-Fra Hi-Tech Eng-Fra
Sys1 Sys2 KL Sys1 Sy2 KL
BLEU score 32.3 34.6 - 44.1 44.7 -
Oracle distr. 34.9% 65.1% 0.00 34.5% 65.5% 0.00
Regression distr. 31.2% 68.9% 0.68 32.3% 67.7% 0.11
Rank distr. 43.4% 56.6% 1.92 47.0% 53.0% 3.31
Classify distr. 47.4% 52.7% 3.78 63.9% 36.1% 17.88
Table 3: Distribution of sentences selected from the participating system for Eng-Fra, across domains (Travel and
Hi-Tech).
tion techniques are significant. For instance,
both the Regression-based system combination and
the Ranking-based system combination achieve a
BLEU score of 29.0 on the WMT09 Hungarian-
English test set, an increase of +2.1 BLEU points.
In the case of Travel English-French, an increase of
+1.6 BLEU points is obtained by the Regression-
based system combination, in spite of the fact that
one of the systems is measured to be 2.3 BLEU
points lower in translation accuracy. Increases in the
range of +1.5-2.0 BLEU points are obtained across
all the experimental conditions that we tried: three
different domains, various language pairs (both in
and out of English), and various training data sizes
(from 0.4Mw to 40Mw).
Since our system-combination methods chose one
system translation over another system translation,
we can also measure the distribution of choices
made between the two participating systems. These
bimodal distributions can help us gauge the perfor-
mance of various methods, when compared against
the BLEU Oracle distribution.
In Table 3, we report the percentages of sentences
selected from each system in the oracle combina-
tion and each of the described system combination
methods. We also report the Kullback-Liebler di-
vergence (KL) between the BLEU Oracle distribu-
tion and the distribution induced by each of the sys-
tem combination methods. The results indicate that,
for both English-French cases that we considered
(in the Travel and HiTech domains), the choice dis-
tribution of the Regression-based system combina-
tion method is much closer to the oracle distribution
(KL of 0.68 and 0.11, respectively), compared to the
other two methods. Note that this does not neces-
sarily correlate with the evaluation based on over-
all BLEU score of the system-combination meth-
ods (Table 2). For instance, for HiTech English-
French the best BLEU improvement is obtained by
the Rank-based method with +1.7 BLEU points, but
the KL divergence score of 3.31 is higher than the
one for the Regression-based method (KL score of
0.11). Nevertheless, the choice distributions are an
important factor in judging the performance of a
given system selection method.
5 Conclusions
Document-level quality estimation is an important
component for building fully-automated translation
solutions where the translated documents are di-
rectly published, without the need for human inter-
vention. Such approaches are the only possible solu-
169
tion to mitigate the imperfection of current MT tech-
nology and the need to translate large volumes of
data on a continuous basis.
We show in this paper that sentence-level predic-
tions, when aggregated to document-level predic-
tions, outperform previously-proposed document-
level quality estimation algorithms. In addition to
that, these finer-granularity, sentence-level predic-
tions can be used as part of a system selection
scheme. The three alternative system selection tech-
niques we describe here are intuitive, computation-
ally cheap, and bring significant BLEU gains across
multiple domains and language pairs. The finding
that the regression-based system selection technique
performs as well (or sometimes better) compared to
the discriminative methods fits well with the overall
theme of using two systems for both improved qual-
ity estimation and improved MT performance.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level MT evaluation with pseudo references.
In Proceedings of ACL.
Joshua Albrecht and Rebecca Hwa. 2008. The role of
pseudo references in MT evaluation. In Proceedings
of ACL.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Gouette, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of COLING.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In Proceedings of the ACL
Third Workshop on Statistical Machine Translation.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without reference
translations: Beyond language modeling. In Proceed-
ings of EAMT.
Asela Gunawardana and Guy Shani. 2009. A sur-
vey of accuracy evaluation metrics of recommenda-
tion tasks. Journal of Machine Learning Research,
10:2935?2962.
Almut Silja Hildebrand and Stephan Vogel. 2008. Com-
bination of machine translation systems via hypothesis
selection. In Proceedings of AMTA.
T. Joachims. 1999. Making large-Scale SVM Learning
Practical. M.I.T. Press.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In Pro-
ceedings of ACL.
Philipp Koehn and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared task
with reordering and speed improvements to Moses.
In Proceedings of EACL Workshop on Statistical Ma-
chine Translation.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Joseph Olive, Caitlin Christianson, and John McCary, ed-
itors. 2011. Handbook of Natural Language Pro-
cessing and Machine Translation: DARPA Global Au-
tonomous Language Exploitation. Springer.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of Third Workshop on Statistical Machine
Translation.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations via
ranking. In Proceedings of ACL.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Mar-
cho Turchi, and Nello Cristianini. 2009. Estimating
the sentence-level quality of machine translation. In
Proceedings of EAMT.
Lucia Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proceed-
ings of EAMT.
Peng Xu, Jaeho Kang, Michael Ringaard, and Franz Och.
2009. Using a dependency parser to improve SMT
for Subject-Object-Verb languages. In Proceedings of
ACL.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence level machine translation evaluation as a rank-
ing. In Proceedings of the ACL Second Workshop on
Statistical Machine Translation.
170
