Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 533?542,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Word Games for Semantic Data Collection
David Vickrey Aaron Bronzan William Choi Aman Kumar
Jason Turner-Maier Arthur Wang Daphne Koller
Stanford University
Stanford, CA 94305-9010
{dvickrey,abronzan,aman,arthurex,koller}@cs.stanford.edu
{wchoi25,jasonptm}@stanford.edu
Abstract
Obtaining labeled data is a significant obstacle
for many NLP tasks. Recently, online games
have been proposed as a new way of obtain-
ing labeled data; games attract users by be-
ing fun to play. In this paper, we consider the
application of this idea to collecting seman-
tic relations between words, such as hyper-
nym/hyponym relationships. We built three
online games, inspired by the real-life games
of ScattergoriesTM and TabooTM. As of June
2008, players have entered nearly 800,000
data instances, in two categories. The first
type of data consists of category/answer pairs
(?Types of vehicle?,?car?), while the second
is essentially free association data (?subma-
rine?,?underwater?). We analyze both types
of data in detail and discuss potential uses of
the data. We show that we can extract from
our data set a significant number of new hy-
pernym/hyponym pairs not already found in
WordNet.
1 Introduction
One of the main difficulties in natural language pro-
cessing is the lack of labeled data. Typically, obtain-
ing labeled data requires hiring human annotators.
Recently, building online games has been suggested
an alternative to hiring annotators. For example, von
Ahn and Dabbish (2004) built the ESP Game1, an
online game in which players tag images with words
that describe them. It is well known that there are
large numbers of web users who will play online
games. If a game is fun, there is a good chance that
sufficiently many online users will play.
We have several objectives in this paper. The
first is to discuss design decisions in building word
games for collecting data, and the effects of these
decisions. The second is to describe the word games
1www.gwap.com/gwap/gamesPreview/espgame
that we implemented and the kinds of data they are
designed to collect. As of June 2008, our games
have been online for nearly a year, and have col-
lected nearly 800,000 data instances. The third goal
is to analyze the resulting data and demonstrate that
the data collected from our games is potentially use-
ful in linguistic applications. As an example appli-
cation, we show that the data we have collected can
be used to augment WordNet (Fellbaum, 1998) with
a significant number of new hypernyms.
2 General Design Guidelines
Our primary goal is to produce a large amount of
clean, useful data. Each of these three objectives
(?large?, ?clean?, and ?useful?) has important im-
plications for the design of our games.
First, in order to collect large amounts of data,
the game must be attractive to users. If the game
is not fun, people will not play it. This requirement
is perhaps the most significant factor to take into ac-
count when designing a game. For one thing, it tends
to discourage extremely complicated labeling tasks,
since these are more likely to be viewed as work. It
would certainly be a challenge (although not neces-
sarily impossible) to design a game that yields la-
beled parse data, for example.
In this paper, we assume that if people play a
game in real life, there is a good chance they will
play it online as well. To this end, we built on-
line versions of two popular ?real-world? games:
ScattergoriesTM and TabooTM. Not only are these
games fun, but there is also a preexisting demand
for online versions of these games, driving search
traffic to our site. We will go into more detail about
these games in the next section.
An important characteristic of these games is that
they involve more than one player. Interacting with
another player increases the sense of fun. Another
important feature these games share is that they are
533
timed. Timing has several advantages. First, tim-
ing helps make the games feel more ?game-like?, by
adding a sense of urgency. Without timing, it risks
feeling more like a labeling task than a game.
The next requirement is that the data be clean.
First, the players must be capable of producing high-
quality annotations. Second, the game should en-
courage users to enter relevant data. We award
points as a motivating factor, but this can lead play-
ers to enter irrelevant data, or collude with other
players, in order to get a higher score. In particu-
lar, collusion is more likely when players can freely
communicate. An excellent technique for producing
good data, used effectively in the ESP game, is to
require the players to match on their inputs. Requir-
ing players to match their partner?s hidden answers
discourages off-topic answers and makes it quite dif-
ficult to collude (requiring outside communication).
We use this technique in all of our games.
Finally, the data must be useful. Ideally, it would
be directly applicable to an NLP task. This require-
ment can come into conflict with the other goals.
There are certainly many kinds of data that would
be useful for NLP tasks (such as labeled parses), but
designing a game to collect this data that people will
play and that produces clean data is difficult.
In this paper, we focus on a particular kind of lin-
guistic data: semantic relationships between pairs of
words and/or phrases. We do this for several rea-
sons. First, this kind of data is relatively simple,
leading to fun games which produce relatively clean
data. Second, the real-world games we chose to
emulate naturally produce this kind of data. Third,
there are a number of recent works which focus on
extracting these kinds of relationships, e.g. (Snow
et al, 2006; Nakov & Hearst, 2008). Our work
presents an interesting new way of extracting this
type of data. Finally, at least one of these kinds of
relationships, the hypernym, or ?X is a Y? relation,
has proven to be useful for a variety of NLP tasks.
3 Description of Our Games
We now describe our three games in detail.
3.1 Categorilla
Categorilla, inspired by ScattergoriesTM, asks play-
ers to supply words or phrases which fit specific cat-
egories, such as ?Things that fly? or ?Types of fish?.
In addition, each game has a specific letter which all
answers must begin with. Thus, if the current game
has letter ?b?, reasonable answers would be ?bird?
and ?barracuda?, respectively. In each game, a ran-
domly matched pair of players are given the same
10 categories; they receive points when they match
with the other player for a particular category. Play-
ers are allowed to type as may answers for a given
category as they wish (until a match is made for that
category). After a match is made, the players get
to see what word they matched on for that category.
Each answer is supposed to fit into a specific cate-
gory, so the data is automatically structured.
Our system contains 8 types of categories, many
of which were designed to correspond to linguistic
resources used in NLP applications. Table 1 de-
scribes the category types.
The purpose of the first three types of categories is
to extract hypernym/hyponym pairs like those found
in WordNet (e.g., ?food? is a hypernym of ?pizza?).
In fact, the categories were automatically generated
from WordNet, as follows. First, we assigned counts
Cs to each synset s in WordNet using the Sem-
Cor2 labeled data set of word senses. Let desc(s)
be the set of descendants of s in the hypernym hi-
erarchy. Then for each pair of synsets s, d, where
d ? desc(s), we computed a conditional distribu-
tion P (d|s) = CdP
d??desc(s) Cd?
, the probability that
we choose node d from among the descendants of
s. Finally, we computed the entropy of each node s
in WordNet,
?
d?desc(s) P (d|s)logP (d|s). Synsets
with many different descendants occurring in Sem-
Cor will have higher entropies. Each node with a
sufficiently high entropy was chosen as a category.
We then turned each synset into a category by tak-
ing the first word in that synset and plugging it into
one of several set phrases. For nouns, we tried two
variants (?Types of food? and ?Foods?). Depend-
ing on the noun, either of these may be more natu-
ral (consider ?Cities? vs. ?Types of city?). ?Types
of food? tends to produce more adjectival answers
than ?Foods?. We tried only one variation for verbs
(?Methods of paying?). This phrasing is not per-
fect; in particular, it encourages non-verb answers
like ?credit card?.
The second group of categories tries to capture se-
lectional preferences of verbs ? for example, ?ba-
2Available at www.cs.unt.edu/ rada/downloads.html
534
Name # Description Example Good Answer
NHyp 269 Members of a class of nouns ?Vehicles? ?car?
NType 269 Members of a class of nouns ?Types of vehicle? ?car?
VHyp 70 Members of a class of verbs ?Methods of cutting? ?trimming?
VS 1380 Subjects of a verb ?Things that eat? ?cats?
VO 909 Direct objects of a verb ?Things that are abandoned? ?family?
VPP 77 Preposition arguments of a verb ?Things that are accused of? ?crime?
Adj 219 Things described by an adjective ?Things that are recycled? ?cans?
O 105 Other; mostly ?Things found at/in ...? ?Things found in a school? ?teachers?
Table 1: Summary of category types. # indicates the number of categories of that type.
nana? makes sense as the object of ?eat? but not as
the subject. Our goal with these categories was to
produce data useful for automatically labeling se-
mantic roles (Gildea & Jurafsky, 2002), where selec-
tional preferences play an important role. We tried
three different types of categories, corresponding to
subjects, objects, and prepositional objects. Exam-
ples are ?Things that eat?, ?Things that are eaten?,
and ?Things that are eaten with?, to which good
answers would be ?animals?, ?food?, and ?forks?.
These categories were automatically generated us-
ing the labeled parses in Penn Treebank (Marcus
et al, 1993) and the labeled semantic roles of Prop-
Bank (Kingsbury et al, 2002). To generate the
object categories, for example, for each verb we
then counted the number of times a core argument
(ARG0-ARG5) appeared as the direct object of that
verb (according to the gold-standard parses), and
used all verbs with count at least 5. This guaran-
teed that all generated categories were grammati-
cally correct and captured information about core
arguments for that verb. Most of the prepositional
object categories proved to be quite confusing (e.g.,
?Things that are acted as?), so we manually removed
all but the most clear. Not surprisingly, the use of
the Wall Street Journal had a noticeable effect on the
types of categories extracted; they have a definite fi-
nancial bias.
The third group of categories only has one
type, which consists of adjective categories such as
?Things that are large?. While we did not have any
specific task in mind for this category type, having a
database of attributes/noun pairs seems potentially
useful for various NLP tasks. To generate these
categories, we simply took the most common ad-
jectives in the SemCor data set. Again, the result-
ing set of adjectives reflect the corpus; for example,
?Things that are green? was not generated as a cate-
gory, while ?Things that are corporate? was.
The final group of categories were hand-written.
This group was added to make sure that a sufficient
number of ?fun? categories were included, since
some of the category types, particularly the verb
categories, are somewhat confusing and difficult.
Most of the hand-written categories are of the form
?Things found at/in X?, where X is a location, such
as ?Japan? or ?the ocean?.
The starting letter requirement also has important
consequences for data collection. It was designed
to increase the variety of obtained data; without this
restriction, players might produce a smaller set of
?obvious? answers. As we will see in the results,
this restriction did indeed lead to a great diversity of
answers, but at a severe cost to data quality.
3.2 Categodzilla
Categodzilla is a slightly modified version of Cat-
egorilla, with the starting letter constraint relaxed.
The combination of difficult categories and rare let-
ters often leads to bad answers in Categorilla. To in-
crease data quality, in Categodzilla for each category
there are three boxes. In the first box you can type
any word you want. Answers in the second box must
start with a given ?easy? letter such as ?c?. Answers
in the third box must start with a given ?hard? letter,
such as ?k?. The boxes much be matched in order;
guesses typed in the first box which match either of
the other two boxes are automatically propagated.
3.3 Free Association
Free Association, inspired by TabooTM, simply asks
players to type words related to a given ?seed? word.
Players are not allowed to type any of several words
on a ?taboo? list, specific to the current seed word.
535
As soon as a match is achieved, players move on to
a new seed word.
The seed words came from two sources. The first
was the most common words in SemCor. The sec-
ond was the Google unigram data, which lists the
most common words on the web. In both cases, we
filtered out stop words (including all prepositions).
Unlike Categorilla, we found that nearly all col-
lected Free Association data was of good quality,
due to the considerably easier nature of the task. Of
course, we do lose the structure present in Catego-
rilla. As the name suggests, the collected data is es-
sentially free word association pairs. We analyze the
data in depth to see what kinds of relations we got.
4 Existing Word Games
Two notable word games already exist for collecting
linguistic data. The first is the Open Mind Common
Sense system3 (Chklovski, 2003). The second is
Verbosity4 (von Ahn et al, 2006). Both these games
are designed to extract common sense facts, and thus
have a different focus than our games.
5 Bots
There may not always be enough players available
online to match a human player with another human
player. Therefore, one important part of designing
an online game is building a bot which can func-
tion in the place of a player. The bots for all of our
games are similar. Each has a simple random model
which determines how long to wait between guesses.
The bot?s guesses are drawn from past guesses made
by human players for that category/seed word (plus
starting letter in the case of Categorilla). Just as with
a human player, as soon as one of the bot?s guesses
matches one of the player?s, a match is made.
If there are no past guesses, the bot instead makes
?imaginary? guesses. For example, in Categorilla,
we make the (obviously false) assumption that for
every category and every starting letter there are ex-
actly 20 possible answers, and that both the player?s
guesses and the bot?s imaginary guesses are drawn
from those 20 answers. Then, given the number
of guesses made by the player and the number of
imaginary guesses made by the bot, the probabil-
ity of a match can be computed (assuming that all
3http://commons.media.mit.edu/en
4www.gwap.com/gwap/gamesPreview/verbosity
Grla Gdza Free
Game Length 3min 3min 2min
Games Played 19656 2999 15660
Human-Human Games 428 45 401
Categories 3298 3298 9488
Guesses Collected 391804 78653 307963
Guesses/Categories 119 24 32
Unique Guesses 340433 56142 221874
Guesses: All/Unique 1.15 1.40 1.39
Guesses/Games 19.9 26.2 19.7
Guesses per minute 6.6 8.7 9.9
Table 2: Statistics for Categorilla, Categodzilla, and Free
Association.
guesses are made independently). Once this proba-
bility passes a certain threshold, randomly generated
for each category at the start of each game, the bot
matches one of the player?s guesses, chosen at ran-
dom. The Free Association bot works similarly.
For Free Association, the bot rarely has to resort
to generating these imaginary guesses. In Catego-
rilla, due to the starting letter requirement, the bot
has to make imaginary guesses much more often.
Imaginary guessing can encourage poor behavior on
the part of players, since they see that matches can
occur for obviously bad answers. They may also re-
alize that they are playing against a bot.
An additional complication for Categorilla and
Categodzilla is that the bot has to decide which cat-
egories to make guesses for, and in what order. Our
current guessing model takes into account past diffi-
culty of the category and the current guessing of the
human player to determine where to guess next.
6 Users and Usage
Table 2 shows statistics of each of the games, as
of late June 2008. While we have collected nearly
800,000 data instances, nearly all of the games were
between a human and the bot. Over the course of
a year, our site received between 40 and 100 vis-
its a day; this was not enough to make it likely for
human-human games to occur. The fact that we still
collected this amount of data suggests that our bot is
a satisfactory substitue for a human teammate. We
have anecdotally found that most players do not re-
alize they are playing against a bot. While most of
the data comes from games between a human and a
bot, our data set consists only of input by the human
players.
536
110
100
1000
1 2 3-5 6-1
0
11-
20
21-
50
51-
100
101
-
200
201
-
500
501
-
100
0
100
1-2
000
Games Played
Nu
mb
er
 
of 
Us
er
s
Categorilla
Categodzilla
Free Association
Figure 1: Users are grouped by number of games played.
Note that this graph is on a double-log scale.
Our main tool for attracting traffic to our site was
Google. First, we obtained $1 a day in AdWords,
which pays for between 7 to 10 clicks on our ad
a day. Second, our site is in the top 10 results for
many relevant searches, such as ?free online scatter-
gories?.
Categorilla was the most popular of the games,
with about 25% more games played than Free As-
sociation. Taking the longer length of Categorilla
games into account (see Table 2), this corresponds
to almost 90% more play time. This is despite the
fact that Free Association is the first game listed on
our home page. We hypothesize that this is because
ScattergoriesTM is a more popular game in real life,
and so many people come to our site specifically
looking for an online ScattergoriesTM game. Cat-
egodzilla has been played signficantly less; it has
been available for less time and is listed third on the
site. Even for Categodzilla, the least played game,
we have collected on average 24 guesses per cate-
gory.
Several of our design decisions for the games
were based on trying to increase the diversity of an-
swers. Categorilla has the highest answer diversity.
For a given category, each answer occurred on aver-
age only 1.15 times. In general, this average should
increase with the amount of collected data. How-
ever, Categodzilla and Free Association have col-
lected significantly fewer answers per category than
Categorilla, but still have a higher average, around
1.4. The high answer diversity of Categorilla is a
direct result of the initial letter constraint. For all
three games, the majority of category/answer pairs
occurred only once.
Figure 1 shows the distribution over users of the
0
0.02
0.04
0.06
0.08
0.1
0.12
a b c d e f g h i j k l m n o p q r s t u v w x y z *
Fra
ctio
n o
f An
sw
ers
Categorilla
Categodzilla
Free Assocation
Figure 2: Fraction of answers with given initial letter. *
denotes everything nonalphabetical.
number of games played. Not surprisingly, it follows
the standard Zipfian curve; there are a large number
of users who have played only a few games, and a
few users who have played a lot of games. The mid-
dle of the curve is quite thick; for both Categorilla
and Free Association there are more than 100 play-
ers who have played between 21 and 50 games.
Figure 2 shows the distribution of initial letters
of collected answers for each game. Categorilla
is nearly flat over all letters besides ?q?, ?x?, and
?z? which are never chosen as the inital letter con-
straint. This means players make a similar number
of guesses even for difficult initial letters. In con-
trast, the distribution of initial letters for Free Asso-
ciation data reflects the relatively frequency of initial
letters in English. Even though Categodzilla does
have letter constraints in the 2nd and 3rd columns,
its statistics over initial letter are very similar to Free
Association.
7 Categorilla and Categodzilla Data
In our analyses, we take ALL guesses made at any
time, whether or not they actually produced a match.
This greatly increases the amount of usable data, but
also increases the amount of noise in the data.
The biggest question about the data collected
from Categorilla and Categodzilla is the quality of
the data. Many categories can be difficult or some-
what confusing, and the initial letter constraint fur-
ther increases the difficulty.
To evaluate the quality of the data, we asked
three volunteer labelers to label 1000 total cate-
gory/answer pairs. Each labeler labeled every pair
with one of three labels, ?y?, ?n?, or ?k?. ?y? means
that the answer fit the category. ?n? means that it
537
Annotator y k n
#1 72 13 115
#2 77 27 96
#3 88 42 70
Majority 76 29 95
Table 3: Comparison of annotators
Data Set y k n
Control 30 14 156
Categorilla 76 29 95
Categodzilla 144 23 33
Table 4: Overall answer accuracy
does not fit. ?k? means that it ?kind of? fits. This was
mostly left up to the labelers; the only suggestion
was that one use of ?k? could be if the category was
?Things that eat? and the answer was ?sandwich.?
Here, the answer is clearly related to the category,
but doesn?t actually fit.
The inter-annotator agreement was reasonable,
with a Fleiss? kappa score of .49. The main differ-
ence between annotators was how permissive they
were; the percentage of answers labeled ?n? ranged
from 58% for the first annotator to 35% for the third.
The labeled pairs were divided into 5 subgroups of
200 pairs each (described below); Table 3 shows the
number of each label for the Categorilla-Random
subset. We aggregated the different annotations by
taking a majority vote; if all three answers were dif-
ferent, the item was labeled ?k?. Table 3 also shows
the statistics of the majority vote on the same subset.
Overall Data Quality. We compared results
for three random subsets of answers, Control-
Random, Categorilla-Random, and Categodzilla-
Random. Categorilla-Random was built by select-
ing 200 random category/answer pairs from the Cat-
egorilla data. Note that category/answer pairs that
occurred more than once were more likely to be se-
lected. Categodzilla-Random was built similarly.
Control-Random was built by randomly selecting
two sets of 200 category/answer pairs each (includ-
ing data from both Categorilla and Categodzilla),
and then combining the categories from the first set
with the answers from the second to generate a set
of random category/answer pairs.
Table 4 shows results for these three subsets. The
chance for a control answer to be labeled ?y? was
15%. Categorilla produces data that is significantly
Category Results -- Categorilla
0
5
10
15
20
25
NHyp NType VHyp VS VO VPP Adj O
Category Type
n
k
y
Figure 3: Categorilla accuracy by category type
better than control, with 38% of answers labeled ?y?.
Categodzilla, which is more relaxed about initial let-
ter restrictions, is significantly better than Catego-
rilla, with 72% of answers labeled ?y?. This relax-
ation has an enormous impact on the quality of the
data. Note however that these statistics are not ad-
justed for accuracy of individual players; it may be
that only more accurate players play Categodzilla.
Effect of Category Type on Data Quality.
Within each type of category (see Table 1), cer-
tain categories appear much more often than oth-
ers due to the way categories are selected (at least
two ?easy? categories are guaranteed every game).
To adjust for this, we built a subset of 200 cat-
egory/answer pairs by selecting 25 different cate-
gories randomly from each type of category. We
then selected an answer at random from among the
answers submitted for that category. In addition, we
built a control set using the same 200 categories but
instead using answers selected at random from the
entire Categorilla data set. Results for Categorilla
data are shown in Figure 3; we omit the correspond-
ing graph for control for lack of space. For most
categories, the Categorilla data is significantly bet-
ter than the control. The hand-written category type,
O, has the best data quality, which is not surpris-
ing because these categories allow the most possible
answers, and thus are easiest of think of answers for.
These categories also have the highest number of ?y?
labels for the control. Next best are the hypernym
categories, NType. NType is much higher than the
other noun hypernym category NHyp because the
?Type of? phrasing is generally more natural and al-
lows for adjectival answers. The VPP category type,
which tries to extract prepositional objects, contains
538
Data Set Letters Size y k n
Control Easy 127 .14 .08 .78
Control Hard 72 .15 .06 .79
Categorilla Easy 106 .45 .14 .41
Categorilla Hard 94 .30 .15 .55
Table 5: Accuracy of easy letters vs. hard letters. Size is
the number of answers for that row.
the most number of ?k? annotations; this is because
players often put answers that are subjects or ob-
jects of the verb, such as ?pizza? for ?Things that
are eaten with?. The adjective category type, Adj,
has the lowest increase over the control; this is likely
due to the nature of the extracted adjectives.
Effect of Initial Letter on Data Quality. In
general, we would expect common initial letters to
yield better data since there are more possible an-
swers to choose from. We did not have enough la-
beled data to do letter by letter statistics. Instead, we
broke the letters into two groups, based on the em-
pirical difficulty of obtaining matches when given
that initial letter. The easy letters were ?abcfhlmn-
prst?, while the hard letters were ?degijkouvwy?. Ta-
ble 5 shows the results on Categorilla-Random and
Control-Random on these two subsets. First, note
that the results on Control-Random are the same for
hard letters and easy letters. This means that words
starting with common letters are not more likely to
fit in a category. For both hard letters and easy let-
ters, the accuracy is considerably better on the Cat-
egorilla data. However, the increase in the number
of ?y? labels for easy letters is twice that for hard
letters. The quality of data for hard letters is consid-
erably worse than that for easy letters.
8 Free Association Data
In contrast to Categorilla and even Categodzilla, we
found that the Free Association data was quite clean.
However, it is also not structured; we simply get
pairs of related words. Thus, the essential question
for this game is what kind of data we get.
To analyze the types of relationships between
words, the authors labeled 500 randomly extracted
unique pairs with a rich set of word-word relations,
described in Table 6. This set of relations was de-
signed to capture the observed relationships encoun-
tered in the Free Association data. Unlike our Cat-
egorilla labeled set, pairs that occurred more than
once were NOT more likely to be selected than pairs
that occurred once (i.e., the category/answer pairs
were aggregated prior to sampling). Sampling in this
way led to more diversity in the pairs extracted.
To label each pair, the authors found a sequence
of relationships which connected the two words. In
many cases, this was a single link. For example,
?dragon? and ?wing? are connected by a single link,
?wing? IS PART OF ?dragon?. In others, multiple
links were required. For the seed word ?dispute? and
answer ?arbitrator?, we can connect using two links:
?dispute? IS OBJECT OF ?resolve?, ?arbitrator? IS
SUBJECT OF ?resolve?. There were two other pos-
sible ways to label a pair. First, they might be totally
unrelated (i.e., a bad answer). Second, they might
be related, but not connectable using our set of basic
relations. For example, ?echo? is clearly related to
?valley?, but in a complicated way.
The quality of the data is considerably higher than
Categorilla and Categodzilla; under 10% of words
are unrelated. Slightly over 20% of the pairs are la-
beled Misc, i.e., the words are related but in a com-
plicated way. 3% of the pairs can be linked with a
chain of two simple relations. The remaining 67%
of all pairs were linked with a single simple relation.
The category Desc deserves some discussion.
This category included both simple adjective de-
scriptions, such as ?creek? and ?noisy?, and also
qualifiers, such as ?epidemic? and ?typhoid?, where
one word specifies what kind of thing the other is.
The distinction between Desc and Phrase was sim-
ply based on to what extent the combination of the
two words was a set phrase (such as ?east? and ?Ger-
many?).
Schulte im Walde et al (2008) address very sim-
ilar issues to those discussed in this section. They
built a free association data set containing about
200,000 German word pairs using a combination of
online and offline volunteers (but not a game). They
then analyze the resulting associations by comparing
the resulting pairs to a large-scale lexical resource,
GermaNet (the German counterpart of WordNet).
Our data analysis was by hand, making it compar-
atively small scale but more detailed. It would be
interesting to compare the data sets to see whether
the use of a game affects the resulting data.
9 Filtering Bad Data
In this section, we consider a simple heuristic for
filtering bad data: only retaining answers that were
539
Name # Description Example
Misc 103 Words related, but in a complicated way ?echo?, ?valley?
Desc 76 One of the words describes the other ?cards?, ?business?
None 47 Words are not related ?congress?,?store?
Syn 46 The words are synonyms ?downturn?, ?dip?
Obj 33 One word is the object of the other ?exhale?,?emission?
Hyp 30 One word is an example of the other ?cabinet?,?furniture?
?Syn 29 The words are ?approximate? synonyms ?maverick?,?outcast?
Cousin 21 The words share a common hypernym (is-a) relation ?meter?,?foot?
Has 18 One word ?has? the other ?supermarket?,?carrots?
2-Chain 15 Words are linked by a chain of two simple relations ?arbitrator?,?dispute?
Phrase 13 Words make a phrase; similar to Desc ?East?, ?Germany?
Part 11 One is a part of the other ?dragon?,?wings?
At 10 One is found at the other ?harbor?, ?lake?
Subj 8 One is the subject of the other ?actor?, ?pretend?
Form 7 One is a form of the other ?revere?,?reverence?
Def 7 One defines the other ?blind?,?unable to see?
Opp 7 The two are opposites ?positive?,?negative?
Sound 6 The two words sound similar ?boutique?,?antique?
Sub 5 One is a subword of the other ?outlet?, ?out?
Unit 2 One is a unit of the other ?reel?,?film?
Made 2 One is made of the other ?knee?,?bone?
Table 6: Relation types for 500 hand-labeled examples. # indicates the number of pairs with that label.
guessed some minimum number of times. Note that
in this section all answers were stemmed in order to
combine counts across plurals and verb tenses.
For the Categorilla data, filtering out cate-
gory/answer pairs that only occurred once from
Categorilla-Random left a total of 64 answers (from
an original 200), of which 36 were labeled ?y? and 8
were labeled ?k?. The fraction of ?y? labels in the
reduced set is 56%, up from 38% in the original
set. This gain in quality comes at the cost of losing
slightly over two-thirds of the data.
For Categodzilla-Random, a similar filter left 88
(out of 200), with 79 labeled ?y? and 7 labeled ?k?.
For the hand-labeled Free Association data, apply-
ing this filter yielded a total of 123 pairs (out of an
original 500), with only 2 having no relation5. In
these two games, this filter eliminates nearly all bad
data while keeping a reasonable fraction of the data.
Clearly, this filter is less effective for Catego-
rilla than the other two games. One of the main
reasons for this is that the letter constraints cause
5The higher fraction of lost pairs for Free Association is pri-
marily due to the method of sampling pairs for evaluation, as
discussed in Section 8.
people to try to fit words starting with that letter
into all categories that they even vaguely relate to,
rather than thinking of words that really fit that cat-
egory. Examples include {?Art supplies?,?jacket?},
{?Things found in Chicago?,?king?} and {?Things
that are African?,?yak?}. Of course, we can further
increase the quality of the data by making the fil-
ter more restrictive, at the cost of losing more data.
For example, removing answers occuring fewer than
5 times from Categorilla-Random leaves only 8 an-
swers (out of 200), 7 labeled ?y? and 1 labeled ?n?.
There are other ways we could filter the data. For
example, suppose we are given an outside database
of pairs of words which are known to be semanti-
cally related. We could apply the following heuris-
tic: if an answer to a particular category is similar to
many other answers for that category, then that an-
swer is likely to be a good one. Preliminary experi-
ments using distributional similarity of words as the
similarity metric suggest that this heuristic captures
complimentary information to the guess frequency
heuristic. We leave as future work a full integration
of the two heuristics into a single improved filter.
540
Classified Type # Example
Real hypernyms 96 ?equipment?,?racquet?
Compound hypernyms 32 ?arrangement?,?flower?
Adjectives 25 ?building?,?old?
Sort-of hypernyms 14 ?vegetable?,?salad?
Not hypernyms 33 ?profession?,?money?
Table 7: Breakdown of potential hypernym pairs
10 Using the Data
Categorilla and Categodzilla produce structured data
which is already in a usable or nearly usable form.
For example, the NHyp and NType categories pro-
duce lists of hypernyms, which could be used to aug-
ment WordNet. We looked at this particular applica-
tion in some detail.
First, in order to remove noisy data, we used
only Categodzilla data and removed answers which
occurred only once. We took all category/answer
pairs where the category was of type either NHyp or
NType, and where the answer was a noun. This re-
sulted in 1604 potential hypernym/hyponym pairs.
Of these, 733 (or 46%) were already in WordNet.
The remaining 871 were not found in WordNet. We
then hand-labeled a random subset of 200 of the 871
to determine how many of them were real hyper-
nym/hyponym pairs. The results are shown in Ta-
ble 7. Counting compound hyponyms, nearly two-
thirds of the pairs are real hypernym/hyponym pairs.
These new pairs could directly augment WordNet.
For example, for the word ?crime?, WordNet has
as hyponyms ?burglary? and ?fraud?. However,
it doesn?t have ?arson?, ?homicide?, or ?murder?,
which are among the 871 new pairs. WordNet lists
?wedding? as being an ?event?, but not ?birthday?.
The verb subject, object, and prepositional object
categories were designed to collect data about the
selectional preferences of verbs. These categories
turned out to be problematic for several reasons.
First, statistics about selectional preferences of verbs
are not too difficult to extract from the web (although
in some cases they might be somewhat noisy). Thus,
the motivation for extracting this data using a game
is not as apparent. Second, providing arguments of
verbs out of the context of a sentence may be too dif-
ficult. For example, for the category ?Things that are
accumulated?, there a couple of obvious answers,
such as ?wealth? or ?money?, but beyond these it
becomes more difficult. In the context of an actual
document, quite a lot of things can accumulate, but
outside of that context it is difficult to think of them.
One solution to this problem would be to provide
context. For example, the category ?Things that ac-
cumulate in your body? is both easier to think of
answers for and probably collects more useful data.
However, automatically creating categories with the
right level of specificity is not a trivial task; our ini-
tial experiments suggested that it is easy to gener-
ate too much context, creating an uninteresting cat-
egory.
The Free Association game produces a lot of very
clean data, but does not classify the relationships be-
tween the words. While a web of relationships might
be useful by itself, classifying the pairs by relation
type would clearly be valuable. Snow et al (2006)
and Nakov and Hearst (2008), among others, look at
using a large amount of unlabeled data to classify
relations between words. One issue with extract-
ing new relations from text, for example meronyms
(part-of relationships), is that they tend to occur
fairly rarely. Thus, it is very easy to get a large num-
ber of spurious pairs. Using our data as a set of can-
didate pairs for relation extraction could greatly re-
duce the resulting noise. We believe that application
of existing techniques to the data from the Free As-
sociation game could lead to a clean, classified set of
word-word relations, but leave this as future work.
11 Discussion and Future Work
One way to extend Categorilla and Categodzilla
would be to add additional types of categories. For
example, a meronym category type (e.g. ?Parts of a
car?) would work well. Further developing the verb
categories (e.g., ?Things that accumulate in your
body?) is another challenging but interesting direc-
tion; these categories would produce phrase-word
relationships rather than word-word relationships.
Probably the most interesting direction for future
work is trying to increase the complexity of the data
collected from a game. There are two significant dif-
ficulties: keeping the game fun, and making sure the
collected data is not too noisy. One interesting ques-
tion for future research is whether different game ar-
chitectures might be better suited to certain kinds
of data. For example, a ?telephone? style game,
where players relay a phrase or sentence through
some noisy channel, might be an interesting way to
obtain paraphrase data.
541
References
Chklovski, T. (2003). Using analogy to acquire com-
monsense knowledge from human contributors.
Thesis.
Fellbaum, C. (Ed.). (1998). Wordnet: An electronic
lexical database. MIT Press.
Gildea, D., & Jurafsky, D. (2002). Automatic label-
ing of semantic roles. Computational Linguistics.
Kingsbury, P., Palmer, M., & Marcus, M. (2002).
Adding semantic annotation to the penn treebank.
Proceedings of the Human Language Technology
Conference (HLT?02).
Marcus, M., Marcinkiewicz, M., & Santorini, B.
(1993). Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguis-
tics.
Nakov, P., & Hearst, M. (2008). Solving relational
similarity problems using the web as a corpus.
Proceedings of ACL.
Schulte imWalde, S., Melinger, A., Roth, M., &We-
ber, A. (2008). An empirical characterisation of
response types in german association norms. To
appear, Research on Language and Computation.
Snow, R., Jurafsky, D., & Ng, A. (2006). Semantic
taxonomy induction from heterogenous evidence.
Proceedings of COLING/ACL.
von Ahn, L., & Dabbish, L. (2004). Labeling images
with a computer game. ACM CHI.
von Ahn, L., Kedia, M., & Blum, M. (2006). Ver-
bosity: a game for collecting common-sense facts.
Proceedings of the SIGCHI conference on Human
Factors in computing systems.
542
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 771?778, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Word-Sense Disambiguation for Machine Translation
David Vickrey Luke Biewald Marc Teyssier Daphne Koller
Department of Computer Science
Stanford University
Stanford, CA 94305-9010
{dvickrey,lukeb,teyssier,koller}@cs.stanford.edu
Abstract
In word sense disambiguation, a system attempts to
determine the sense of a word from contextual fea-
tures. Major barriers to building a high-performing
word sense disambiguation system include the dif-
ficulty of labeling data for this task and of pre-
dicting fine-grained sense distinctions. These is-
sues stem partly from the fact that the task is be-
ing treated in isolation from possible uses of au-
tomatically disambiguated data. In this paper, we
consider the related task of word translation, where
we wish to determine the correct translation of a
word from context. We can use parallel language
corpora as a large supply of partially labeled data
for this task. We present algorithms for solving the
word translation problem and demonstrate a signif-
icant improvement over a baseline system. We then
show that the word-translation system can be used
to improve performance on a simplified machine-
translation task and can effectively and accurately
prune the set of candidate translations for a word.
1 Introduction
The problem of distinguishing between multiple
possible senses of a word is an important subtask in
many NLP applications. However, despite its con-
ceptual simplicity, and its obvious formulation as a
standard classification problem, achieving high lev-
els of performance on this task has been a remark-
ably elusive goal.
In its standard formulation, the disambiguation
task is specified via an ontology defining the dif-
ferent senses of ambiguous words. In the Sense-
val competition, for example, WordNet (Fellbaum,
1998) is used to define this ontology. However, on-
tologies such as WordNet are not ideally suited to
the task of word-sense disambiguation. In many
cases, WordNet is overly ?specific?, defining senses
which are very similar and hard to distinguish. For
example, there are seven definitions of ?respect?
as a noun (including closely related senses such as
?an attitude of admiration or esteem? and ?a feel-
ing of friendship and esteem?); there are even more
when the verb definitions are included as well. Such
closely related senses pose a challenge both for auto-
matic disambiguation and hand labeling. Moreover,
the use of a very fine-grained set of senses, most of
which are quite rare in practice, makes it very diffi-
cult to obtain sufficient amounts of training data.
These issues are clearly reflected in the perfor-
mance of current word-sense disambiguation sys-
tems. When given a large amount of training data
for a particular word with reasonably clear sense
distinctions, existing systems perform fairly well.
However, for the ?all-words? task, where all am-
biguous words from a test corpus must be disam-
biguated, it has so far proved difficult to perform sig-
nificantly better than the baseline heuristic of choos-
ing the most common sense for each word.1
In this paper, we address a different formulation
of the word-sense disambiguation task. Rather than
considering this task on its own, we consider a task
of disambiguating words for the purpose of some
larger goal. Perhaps the most direct and compelling
application of a word-sense disambiguator is to ma-
chine translation. If we knew the correct seman-
tic meaning of each word in the source language,
we could more accurately determine the appropriate
words in the target language. Importantly, for this
application, subtle shades of meaning will often be
irrelevant in choosing the most appropriate words in
the target language, as closely related senses of a
single word in one language are often encoded by a
single word in another. In the context of this larger
goal, we can focus only on sense distinctions that a
human would consider when choosing the transla-
tion of a word in the source language.
We therefore consider the task of word-sense dis-
ambiguation for the purpose of machine translation.
Rather than predicting the sense of a particular word
a, we predict the possible translations of a into the
1See, for example, results of Senseval-3, available at
http://www.senseval.org/senseval3
771
target language. We both train and evaluate the sys-
tem on this task. This formulation of the word-sense
disambiguation task, which we refer to as word
translation, has multiple advantages. First, a very
large amount of ?partially-labeled? data is available
for this task in the form of bilingual corpora (which
exist for a wide range of languages). Second, the
?labeling? of these corpora (that is, translation from
one language to another), is a task at which humans
are quite proficient and which does not generally re-
quire the labeler (translator) to make difficult dis-
tinctions between fine shades of meaning.
In the remainder of this paper, we first discuss
how training data for this task can be acquired au-
tomatically from bilingual corpora. We apply a
standard learning algorithm for word-sense disam-
biguation to the word translation task, with several
modifications which proved useful for this task.We
present the results of our algorithm on word trans-
lation, showing that it significantly improves perfor-
mance on this task. We also consider two simple
methods for incorporating word translation into ma-
chine translation. First, we can use the output of
our model to help a translation model choose better
words; since general translation is a very noisy pro-
cess, we present results on a simplified translation
task. Second, we show that the output of our model
can be used to prune candidate word sets for trans-
lation; this could be used to significantly speed up
current translation systems.
2 Machine Translation
In machine translation, we wish to translate a sen-
tence s in our source language into t in our target
language. The standard approach to statistical ma-
chine translation uses the source-channel model ,
argmaxtP (t|s) = argmaxtP (t)P (s|t),
where P (t) is the language model for the target lan-
guage, and P (s|t) is an alignment model from the
target language to the source language. Together
they define a generative model for the source/target
pair (s, t): first t is generated according to the lan-
guage model P (t); then s is generated from t ac-
cording to P (s|t).2
Typically, strong independence assumptions are
then made about the distribution P (s|t). For ex-
ample, in the IBM Models (Brown et al, 1993),
each word ti independently generates 0, 1, or more
2Note that we refer to t as the target sentence, even though in
the source-channel model, t is the source sentence which goes
through the channel model P (s|t) to produce the observed sen-
tence s.
words in the source language. Thus, the words gen-
erated by ti are independent of the words generated
by tj for each j 6= i. This means that correla-
tions between words in the source sentence are not
captured by P (s|t), and so the context we will use
in our word translation models to predict ti given
si is not available to a system making these inde-
pendence assumptions. In this type of system, se-
mantic and syntactic relationships between words
are only modeled in the target language; most or
all of the semantic and syntactic information con-
tained in the source sentence is ignored. The lan-
guage model P (t) does introduce some context-
dependencies, but the standard n-gram model used
in machine translation is too weak to provide a rea-
sonable solution to the strong independence assump-
tions made by the alignment model.
3 Task Formulation
We define the word translation task as finding, for
an individual word a in the source language S , the
correct translation, either a word or phrase, in the
target language T . Clearly, there are cases where
a is part of a multi-word phrase that needs to be
translated as a unit. Our approach could be extended
by preprocessing the data in S to find phrases, and
then executing the entire algorithm treating phrases
as atomic units. We do not explore this extension in
this paper, instead focusing on the word-to-phrase
translation problem.
As we discussed, a key advantage of the word
translation vs. word sense disambiguation is the
availability of large amounts of training data. This
data is in the form of bilingual corpora, such as
the European Parliament proceedings3 . Such doc-
uments provide many training instances, where a
word in one language is translated into another.
However, the data is only partially labeled in that
we are not given a word-to-word alignment between
the two languages, and thus we do not know what
every word in the source language S translates to in
the target language T . While sentence-to-sentence
alignment is a fairly easy task, word-to-word align-
ment is considerably more difficult. To obtain word-
to-word alignments, we used GIZA++4, an imple-
mentation of the IBM Models (specifically, we used
the output of IBM Model 4). We did not perform
stemming on either language, so as to preserve suf-
fix information for our word translation system and
the machine translation language model.
Let DS be the set of sentences in the source lan-
3Available at http://www.isi.edu/ koehn/
4Available at http://www.isi.edu/ och/GIZA++.html
772
French (frequency) Translation
monte?e(51) going up
le`ve(10), lever(17) standing up
hausse(58), augmenter(37), increase(number)
augmentation(150)
interviens(53) to rise to speak
naissance(21), source(10) to be created, arise
souleve?(10) raising an issue
Table 1: Aligned translations for ?rise? occurring at
least 10 times in the corpus
guage and DT the set of target language sentences.
The alignment algorithm can be run in either di-
rection. When run in the S ? T direction, the al-
gorithm aligns each word in t to at most one word
in s. Consider some source sentence s that contains
the word a, and let Ua,s?t = b1, . . . , bk be the set
of words that align to a in the aligned sentence t. In
general, we can consider Ua = {Ua,s?t}s?Da to be
the candidate set of translations for a in T , where
Da is the set of source language sentences contain-
ing a. However, this definition is quite noisy: a word
bi might have been aligned with a arbitrarily; or, bi
might be a word that itself corresponds to a multi-
word translation in S . Thus, we also align the sen-
tences in the T ? S direction, and require that each
bi in the phrase aligns either with a or with nothing.
As this process is still fairly noisy, we only consider
a word or phrase b ? Ua to be a candidate translation
for a if it occurs some minimum number of times in
the data.
For example, Table 1 shows a possible candidate
set for the English word ?rise?, with French as the
target language. Note that this set can contain not
only target words corresponding to different mean-
ings of ?rise? (the rows in the table) but also words
which correspond to different grammatical forms in
the target language corresponding to different parts
of speech, verb tenses, etc. So, disambiguation in
this case is both over senses and grammatical forms.
The final result of our processing of the corpus is,
for each source word a, a set of target words/phrases
Ua; and a set of sentences Da where, in each sen-
tence, a is aligned to some b ? Ua. For any sen-
tence s ? Da, aligned to some target sentence t,
let ua,s ? Ua be the word or phrase in t aligned
with a. We can now treat this set of sentences as
a fully-labeled corpus, which can be split into a set
used for learning the word-translation model and a
test set used for evaluating its performance.
We note, however, that there is a limitation to us-
ing accuracy on the test set for evaluating the perfor-
mance of the algorithm. A source word a in a given
context may have two equally good, interchangeable
translations into the target language. Our evaluation
metric only rewards the algorithm for selecting the
target word/phrase that happened to be used in the
actual translation. Thus, accuracies measured us-
ing this metric may be artificially low. This is a
common problem with evaluating machine transla-
tion systems.
Another issue is that we take as ground truth the
alignments produced by GIZA++. This has two im-
plications: first, our training data may be noisy since
some alignments may be incorrect; and second, our
test data may not be completely accurate. As men-
tioned above, we only consider possible translations
which occur some minimum number of times; this
removes many of the mistakes made by GIZA++.
Even if the test set is not 100% reliable, though, im-
provement over baseline performance is indicative
of the potential of a method.
4 Word Translation Algorithms
The word translation task and the word-sense dis-
ambiguation task have the same form: each word a
is associated with a set of possible labels Ua; given
a sentence s containing word a, we must determine
which of the possible labels in Ua to assign to a in
the context s. The only difference in the two tasks is
the set Ua: for word translation it is the set of pos-
sible translations of a, while for word sense disam-
biguation it is the set of possible senses of a in some
ontology. Thus, we may use any word sense disam-
biguation algorithm as a word translation algorithm
by appropriately defining the senses (assuming that
the WSD algorithm does not assume that a particular
ontology is used to choose the senses).
Our main focus in this paper is to show that ma-
chine learning techniques are effective for the word
translation task, and to demonstrate that we can use
the output of our word translation system to im-
prove performance on two machine-translation re-
lated tasks. We will therefore restrict our atten-
tion to a relatively simple model, logistic regres-
sion (Minka, 2000). There are several motivations
for using this discriminative, probabilistic model.
First, it is known both theoretically and empirically
(e.g., (Ng and Jordan, 2002)) that discriminative
models achieve higher accuracies than generative
models if enough data is available. For the tradi-
tional word-sense disambiguation task, data must be
hand-labeled, and is therefore often too scarce to al-
low for discriminative training. In our setting, how-
ever, training data is acquired automatically from
bilingual corpora, which are widely available and
quite large. Thus, discriminative training is a viable
option for the word translation problem. A second
773
consideration is that, to effectively incorporate our
system into a statistical machine translation system,
we would like to produce not just a single prediction,
but a list of confidence-rated possibilities. The op-
timization procedure of logistic regression attempts
to produce a distribution over possible translations
which accurately represents the confidence of the
model for each translation. By contrast, a classical
Naive Bayes model often assigns very low proba-
bilities to all but the most likely translation. Other
word-sense disambiguation models may not produce
confidence measures at all.
Features. Our word translation model for a word
a in a sentence s = w1, . . . , wk is based on features
constructed from the word and its context within the
sentence. Our basic logistic regression model uses
the following features, which correspond to the fea-
ture space for a standard Naive Bayes model:
? the part of speech of a (generated using the
Brill tagger)5;
? a binary ?occurs? variable for each word which
is 1 if that word is in a fixed context centered
at a (cr words to the right and cl words to the
left), and 0 otherwise.
We also consider an extension to this model, where
instead of the fixed context features above, we use:
? for each direction d ? {l, r} and each possi-
ble context size cd ? {1, ..., Cd}, an ?occurs?
variable for each word.
This is a true generalization of the previous con-
text features, since it contains features for all pos-
sible context sizes, not just one particular fixed size.
This feature set is equivalent to having one feature
for each word in each context position, except that
it will have a different prior over parameters under
standard L2 regularization. This feature set alows
our model to distinguish between very local (often
syntactic) features and somewhat longer range fea-
tures whose exact position is not as important.
Let ?a,s be the set of features for word a to be
translated, with sentence context s (the description
of the model does not depend on the particular fea-
ture set selected).
Model. The logistic regression model encodes the
conditional distribution (P (ua,s = b | a, s) : b ?
Ua). Such a model is parameterized by a set of vec-
tors ?ab , one for each word a and each possible target
b ? Ua, where each vector contains a weight ?ab,j for
each feature ?a,sj . We can now define our conditional
distribution:
5Available at http://www.cs.jhu.edu/ brill/
P?a(b | a, s) =
1
Za,s
e?ab?a,s
with partition function Za,s =
?
b??Ua exp(?ab??a,s).
Training. We train the logistic regression model to
maximize the conditional likelihood of the observed
labels given the features in our training set. Thus,
our goal in training the model for a is to maximize
?
s?Da
P?a(ua,s | a, s).
We maximize this objective by maximizing its log-
arithm (the log-conditional-likelihood) using conju-
gate gradient ascent (Shewchuk, 1994).
One important consideration when training using
maximum likelihood is regularization of the param-
eters. In the case of logistic regression, the most
common type of regularization is L2 regularization;
we then maximize
?
b,j
exp
(
?
(?ab,j)2
2?2
)
?
s?Da
P?a(ua,s | a, s).
This penalizes the likelihood for the distance of each
parameter ?ab,j from 0; it corresponds to a Gaussian
prior on each parameter with variance ?2.
5 Word Translation Results
For our word translation experiments we used the
European Parliament proceedings corpus, which
contains approximately 27 million words in each of
English and French (as well as a number of other
languages). We tested on a set of 1859 ambigu-
ous words ? specifically, all ambiguous words con-
tained in the first document of the corpus. For each
of these words, we found all instances of the word in
the corpus and split these instances into training and
test sets.
We tested four different models. The first, Base-
line, always chooses the most common translation
for the word; the second, Baseline with Part of
Speech, uses tagger-generated parts of speech to
choose the most common translation for the ob-
served word/part-of-speech pair. The third model,
Simple Logistic, is the logistic regression model
with the simpler feature set, a context window of a
fixed size. We selected the window size by eval-
uating accuracy for a variety of window sizes on
20 of the 1859 ambiguous words using a random
train-test split. The window size which performed
best on average extended one word to the left and
774
Model Macro Micro
Baseline 0.511 0.526
Baseline with Part of Speech 0.519 0.532
Simple Logistic 0.581 0.605
Logistic 0.596 0.620
Table 2: Average Word Translation Accuracy
two words to the right (larger windows generally re-
sulted in overfitting). The fourth model, Logistic, is
the logistic regression model with overlapping con-
text windows; the maximum window size for this
model was four words to the left and four words to
the right. We selected the standard deviation ?2 for
the logistic models by trying different values on the
same small subset of the ambiguous words. For the
Simple Logistic model, the best value was ?2 = 1;
for the Logistic model, it was 0.35.
Table 2 shows results of these four models. The
first column is macro-averaged over the 1859 words,
that is, the accuracy for each word counts equally
towards the average. The second column shows the
micro-averaged accuracy, where each test example
counts equally. We will focus on the micro-averaged
results, since they correspond to overall accuracy.
The less accurate of our two models, Simple Lo-
gistic, improves around 8% over the simple baseline
and 7% over the part-of-speech baseline on aver-
age. Our more complex logistic model, which is able
to handle larger context sizes without significantly
overfitting, improves accuracy by another 1.5%.
There was a great deal of variance from word
to word in the performance of our models relative
to baseline. For a few words, we achieved very
large increases in accuracy. For instance, the noun
?agenda? showed a 31.2% increase over both base-
lines. Similarly, the word ?rise? (either a noun
or a verb) had part-of-speech baseline accuracy of
27.9%. Our model increased the accuracy to 57.0%.
It is worth repeating that accuracies on this task
are artificially low since in many cases a single word
can be translated to many different words with the
same meaning. At the same time, accuracies are ar-
tificially inflated by the fact that we only consider
examples where we can find an aligned word in
the French corpus, so translations where a word is
dropped or translated as part of a compound word
are not counted.
One disadvantage of the EuroParl corpus is that it
is not ?balanced? in terms of semantic content. It is
not clear how this affects our results.
6 Blank-Filling Task
One of the most difficult parts of machine translation
is decoding ? finding the most likely translation ac-
cording to some probability model. The difficulty
arises from the enormous number of possible trans-
lated sentences. Existing decoders generally use ei-
ther highly pruned search or greedy heuristic search.
In either case, the quality of a translation can vary
greatly from sentence to sentence. This variation
is much higher than the improvement in ?seman-
tic? accuracy our model is attempting to achieve.
Moreover, currently available decoders do not pro-
vide a natural way to incorporate the results of a
word translation system. For example, Carpuat and
Wu (2005) obtain negative results for two methods
of incorporating the output of a word-sense disam-
biguation system into a machine translation system.
Thus, we instead used our word translation model
for a simplified translation problem. We prepared a
dataset as follows: for each occurrence of an am-
biguous words in an English sentence in the first
document of the Europarl corpus, we tried to de-
termine what the correct translation for that word
was in the corresponding French sentence. If we
found one and exactly one possible translation for
that word in the French sentence, we replaced that
word with a ?blank?, and linked the English word
to that blank. The final result was a set of 655 sen-
tences with a total of 3018 blanks.
For example, the following English-French sen-
tence pair contains the two ambiguous words ad-
dress and issue and one possible translation for each,
examiner and question:
? Therefore, the commission should address the
issue once and for all.
? Par conse?quent, la commission devra enfin ex-
aminer cette question particulie`re.
We replace the translations of the ambiguous words
with blanks; we would like a decoder to replace the
blanks with the correct translations:
? Par conse?quent, la commission devra enfin [ad-
dress] cette [issue] particulie`re.
An advantage of this task is that, for a given distri-
bution P (t|s), we can easily write a decoder which
exhaustively searches the entire solution space for
the best answer (provided that there are not too many
blanks and that P (t|s) is sufficiently ?local? with re-
spect to t). Thus, we can be sure that it is the prob-
ability model, and not the decoder, which is deter-
mining the quality of the output. Also, we have re-
moved most or all syntactic variability from the task,
775
Model ?lm ?ga ?da ?wt Acc
Language Model only 1 0 0 0 0.749
Source-Channel 1 1 0 0 0.821
LM + GA + DA 1 0.6? 0.6? 0 0.833
LM + GA + DA + WT 1 0.6? 0? 1.2? 0.846
Table 3: Blank-filling results. Weights marked with
* have been optimized.
allowing us to better gauge whether we are choosing
semantically correct translations.
Let (ai, bi) be the pairs of words corresponding to
the blanks in sentence t. Then the alignment model
decomposes as a product of terms over these pairs,
e.g. P (s|t) ? ?(ai,bi) P (ai|bi). Analogously, we
extend the word translation model as Pwt(t|s) ?
?
(ai,bi) Pwt(bi|s, ai).
The source-channel model can be used directly
to solve the blank filling task; the language model
makes use of the French words surrounding each
blank, while the alignment model guesses the ap-
propriate translation based on the aligned English
word. As we have mentioned, this model does not
take full advantage of the context in the English sen-
tence. Thus, we hope that incorporating the word
translation model into the decoder will improve per-
formance on this task.
Conversely, simply using the word translation
model alone for the blank-filling task would not take
advantage of the available French context. There
are four probability distributions we might consider
using: the language model Plm(t); the ?genera-
tive? alignment model Pga(s|t), which we calcu-
late using the training samples from the previous
section; the analogous ?discriminative? alignment
model Pda(t|s), which corresponds to the Base-
line system we compared to on the word translation
task; and our overlapping context logistic model,
Pwt(t|s), which also goes in the ?discriminative? di-
rection, but uses the context features in the source
language for determining the distribution over each
word?s possible translations.
We combine these models by simply taking a log-
linear combination:
log P (t|s) ? ?lm logPlm(t) + ?ga log Pga(s|t)
+ ?da logPda(t|s) + ?wt logPwt(t|s).
The case of ?lm = ?ga = 1 and ?da = ?wt = 0 re-
duces to the source-channel model; other settings in-
corporate discriminative models to varying degrees.
We evaluated this combined translation model on
the blank-filling task for various settings of the mix-
ture coefficients ?. For our language model we used
0 0.5 1 1.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Generative Coefficient
W
or
d 
Tr
an
sla
tio
n 
Co
ef
fic
ie
nt
0.77
0.79
0.81
0.83
0.83
0.84
0.84
0.845
0.8
45
Figure 1: Accuracy on blank-filling task with ?lm = 1 and
?disc = 0 as a function of ?gen and ?wt.
the CMU-Cambridge toolkit.6 The word translation
model for each ambiguous word was trained on all
documents except the first.
Table 3 shows results for several sets of weights.
A * denotes entries which we optimized (see be-
low); other entries were fixed. For example, the third
model was obtained by fixing the coefficient of the
language model to 1 and the word-translation to 0,
and optimizing the weights for the generative and
discriminative alignment models.
The language model alone is able to achieve rea-
sonable results; adding the alignment models im-
proves performance further. By adding the word-
translation model, we are able to improve perfor-
mance by approximately 2.5% over the source-
channel model, a relative error reduction of 14%,
and 1.3% over the optimized model using the
language model and generative and discriminative
alignment models, a relative error reduction of 7.8%.
We chose optimal coefficients for the combined
probability models by exhaustively trying all possi-
ble settings of the weights, at a resolution of 0.1,
evaluating accuracy for each one on the test set. Fig-
ure 1 shows the performance on the blank-filling
task as a function of the weights of the generative
alignment model and the word-translation model
(the optimum value of the discriminative alignment
model P (t|s) is always 0 when we include the
word-translation model). As we can see, the per-
formance of this model is robust with respect to
the exact value of the coefficients. The ?obvious?
setting of 1.0 for the generative model and 1.0 for
the word translation model performs nearly as well
6Available at http://mi.eng.cam.ac.uk/ prc14/toolkit.html.
776
as the optimized setting. In the optimal region,
the word-translation model receives twice as much
weight as the generative alignment model, indicat-
ing that word-translation model is more informative
than the generative alignment model. Incorporating
the discriminative alignment model into the source-
channel model also improves performance, but not
nearly as much as using the word-translation model.
An alternate way to optimize weights over trans-
lation features is described in Och and Ney (2002).
They consider a number of translation features, in-
cluding the language model and generative and dis-
criminative alignment models.
7 Search Space Pruning
As we have mentioned, one of the main difficulties
in translation is that there are an enormous number
of possible translations to consider. Decoding al-
gorithms must therefore use some kind of search-
space pruning in order to be efficient. A key part
of pruning the search space is deciding on the set
of words to consider in possible translations (Ger-
mann et al, 2001). One standard method is to con-
sider only target words which have high probabil-
ity according to the discriminative alignment model.
But we have already shown that the word translation
model achieves much better performance on word
translation than this baseline model; thus, we would
expect the word translation model to improve accu-
racy when used to pick sets of candidate translations.
Given a probability distribution over possible
translations of a word, P (b|a, s), there are several
ways to choose a reduced set of possible transla-
tions. Two commonly used methods are to only
consider the top n scoring words from this distribu-
tion (best-n); and to only consider words b such that
P (b|a, s) is above some fixed threshold (cut-off ).
We use the same data set as for the blank-filling
task. We evaluate the accuracy of a pruning strategy
by evaluating whether the correct translation is in
the candidate set selected by the pruning strategy.
To compare results for different pruning strategies,
we plot performance as a function of average size
of the candidate translation set. Figure 2 shows the
accuracy vs. average candidate set size for the word-
translation model, discriminative alignment model,
and generative alignment model.
The generative alignment model has the worst
performance of the three. This is not surprising as it
does not take into account the prior probability of the
target word P (b). More interestingly, we see that the
word-translation model outperforms the discrimina-
tive translation model by a significant amount. For
0 2 4 6 8 10 12
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Average number of possible translations
Ac
cu
ra
cy
Figure 2: Accuracy of best-n strategy (dotted lines) and cut-
off strategy (solid lines). o = generative alignment, + = discrim-
inative alignment, * = word translation.
instance, to achieve 95% recall (that is, for 95% of
the ambiguous words, we retain the correct transla-
tion), we only need candidate sets of average size 4.2
for the cut-off strategy using the word-translation
model, whereas for the same strategy on the discrim-
inative alignment model we require an average set
size of 6.7 words.
As the size of the solution space grows exponen-
tially with the size of the candidate sets, the word-
translation model could potentially greatly reduce
the search space while maintaining good accuracy.
It would be interesting to use similar techniques to
learn null fertility (i.e., when a word a has no trans-
lation in the target sentence t).
8 Related Work
Berger et al (1996) apply maximum entropy meth-
ods (equivalent to logistic regression) to, among
other tasks, the word-translation task. However, no
quantitative results are presented. In this paper we
demonstrate that the method can improve perfor-
mance on a large data set and show how it might
be used to improve machine translation.
Diab and Resnik (2002) suggest using large bilin-
gual corpora to improve performance on word sense
disambiguation. The main idea is that knowing a
French word may help determine the meaning of the
corresponding English word. They apply this intu-
ition to the Senseval word disambiguation task by
running off-the-shelf translators to produce transla-
tions which they then use for disambiguation.
Ng et al (2003) address word sense disambigua-
tion by manually annotating WordNet senses with
their translation in the target language (Chinese),
and then automatically extracting labeled examples
for word sense disambiguation by applying the IBM
777
Models to a bilingual corpus. They achieve compa-
rable results to training on hand-labeled examples.
Koehn and Knight (2003) focus on the task of
noun-phrase translation. They improve performance
on the noun-phrase translation task, and show that
they can use this to improve full translations. A key
difference is that, in predicting noun-phrase trans-
lations, they do not consider the context of nouns.
They present results which indicate that humans can
accurately translate noun phrases without looking
at the surrounding context. However, as we have
demonstrated in this paper, context can be very use-
ful for a (sub-human-level) machine translator.
A similar argument applies to phrase-based trans-
lation methods (e.g., Koehn et al (2003)). While
phrase-based systems do take into account context
within phrases, they are not able to use context
across phrase boundaries. This is especially impor-
tant when ambiguous words do not occur as part of
a phrase ? verbs in particular often appear alone.
9 Conclusions
In this paper, we focus on the word-translation prob-
lem. By viewing word-sense disambiguation in the
context of a larger task, we were able to obtain large
amounts of training data and directly evaluate the
usefulness of our system for a real-world task. Our
results improve over a baseline which is difficult to
outperform in the word sense disambiguation task.
The word translation model could be improved in
a variety of ways, drawing upon the large body of
work on word-sense disambiguation. In particular,
there are many types of context features which could
be used to improve word translation performance,
but which are not available to standard machine-
translation systems. Also, the model could be ex-
tended to handle phrases.
To evaluate word translation in the context of a
machine translation task, we introduce the novel
blank-filling task, which decouples the impact of
word translation from a variety of other factors, such
as syntactic correctness. For this task, increased
word-translation accuracy leads to improved ma-
chine translation. We also show that the word trans-
lation model is effective at choosing sets of candi-
date translations, suggesting that a word translation
component would be immediately useful to current
machine translations systems.
There are several ways in which the results of
word translation could be integrated into a full trans-
lation system. Most naturally, the word translation
model can be used directly to modify the score of
different translations. Alternatively, a decoder can
produce several candidate translations, which can be
reranked using the word translation model. Unfortu-
nately, we were unable to try these approaches, due
to the lack of an appropriate publicly available de-
coder. Carpuat and Wu (2005) recently observed
that simpler integration approaches, such as forcing
the machine translation system to use the word trans-
lation model?s first choice, do not improve transla-
tion results. Together, these results suggest that one
should incorporate the results of word translation in
a ?soft? way, allowing the word translation, align-
ment, and language models to work together to pro-
duce coherent translations. Given an appropriate de-
coder, trying such a unified approach is straightfor-
ward, and would provide insight about the value of
word translation.
References
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1).
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statisti-
cal machine translation. Computational Linguistics,
19(2).
M. Carpuat and D. Wu. 2005. Word sense disambigua-
tion vs. statistical machine translation. Proc. ACL.
M. Diab and P. Resnik. 2002. An unsupervised method
for word sense tagging using parallel corpora. Proc.
ACL.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. Proc. ACL.
P. Koehn and K. Knight. 2003. Feature-rich statistical
translation of noun phrases. Proc. ACL.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. HLT/NAACL.
T. Minka. 2000. Algorithms for
maximum-likelihood logistic regression.
http://lib.stat.cmu.edu/ minka/papers/logreg.html.
A. Ng and M. Jordan. 2002. On discriminative vs. gen-
erative classifiers: A comparison of logistic regression
and naive bayes. Proc. NIPS.
H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting
parallel texts for word sense disambiguation: An em-
pirical study. Proc. ACL.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. Proc. ACL.
J. Shewchuk. 1994. An introduction to the conjugate gra-
dient method without the agonizing pain. http://www-
2.cs.cmu.edu/ jrs/jrspapers.html.
778
Proceedings of ACL-08: HLT, pages 344?352,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Sentence Simplification for Semantic Role Labeling
David Vickrey and Daphne Koller
Stanford University
Stanford, CA 94305-9010
{dvickrey,koller}@cs.stanford.edu
Abstract
Parse-tree paths are commonly used to incor-
porate information from syntactic parses into
NLP systems. These systems typically treat
the paths as atomic (or nearly atomic) features;
these features are quite sparse due to the im-
mense variety of syntactic expression. In this
paper, we propose a general method for learn-
ing how to iteratively simplify a sentence, thus
decomposing complicated syntax into small,
easy-to-process pieces. Our method applies
a series of hand-written transformation rules
corresponding to basic syntactic patterns ?
for example, one rule ?depassivizes? a sen-
tence. The model is parameterized by learned
weights specifying preferences for some rules
over others. After applying all possible trans-
formations to a sentence, we are left with a
set of candidate simplified sentences. We ap-
ply our simplification system to semantic role
labeling (SRL). As we do not have labeled ex-
amples of correct simplifications, we use la-
beled training data for the SRL task to jointly
learn both the weights of the simplification
model and of an SRL model, treating the sim-
plification as a hidden variable. By extracting
and labeling simplified sentences, this com-
bined simplification/SRL system better gener-
alizes across syntactic variation. It achieves
a statistically significant 1.2% F1 measure in-
crease over a strong baseline on the Conll-
2005 SRL task, attaining near-state-of-the-art
performance.
1 Introduction
In semantic role labeling (SRL), given a sentence
containing a target verb, we want to label the se-
mantic arguments, or roles, of that verb. For the
verb ?eat?, a correct labeling of ?Tom ate a salad?
is {ARG0(Eater)=?Tom?, ARG1(Food)=?salad?}.
Current semantic role labeling systems rely pri-
marily on syntactic features in order to identify and
SNP VP
VP
NP PP
Tom wants S
a
to
eatVPNP NPsalad
croutonswith
Tom: NP S(NP) VPVP VPS TNP1croutons: VPPP(with)Tsalad: NP1 VP T
Figure 1: Parse with path features for verb ?eat?.
classify roles. Features derived from a syntactic
parse of the sentence have proven particularly useful
(Gildea & Jurafsky, 2002). For example, the syntac-
tic subject of ?give? is nearly always the Giver. Path
features allow systems to capture both general pat-
terns, e.g., that the ARG0 of a sentence tends to be
the subject of the sentence, and specific usage, e.g.,
that the ARG2 of ?give? is often a post-verbal prepo-
sitional phrase headed by ?to?. An example sentence
with extracted path features is shown in Figure 1.
A major problem with this approach is that the
path from an argument to the verb can be quite
complicated. In the sentence ?He expected to re-
ceive a prize for winning,? the path from ?win? to its
ARG0, ?he?, involves the verbs ?expect? and ?re-
ceive? and the preposition ?for.? The corresponding
path through the parse tree likely occurs a relatively
small number of times (or not at all) in the training
corpus. If the test set contained exactly the same
sentence but with ?expected? replaced by ?did not
expect? we would extract a different parse path fea-
ture; therefore, as far as the classifier is concerned,
the syntax of the two sentences is totally unrelated.
In this paper we learn a mapping from full, com-
plicated sentences to simplified sentences. For ex-
ample, given a correct parse, our system simplifies
the above sentence with target verb ?win? to ?He
won.? Our method combines hand-written syntac-
tic simplification rules with machine learning, which
344
determines which rules to prefer. We then use the
output of the simplification system as input to a SRL
system that is trained to label simplified sentences.
Compared to previous SRL models, our model
has several qualitative advantages. First, we be-
lieve that the simplification process, which repre-
sents the syntax as a set of local syntactic transfor-
mations, is more linguistically satisfying than using
the entire parse path as an atomic feature. Improving
the simplification process mainly involves adding
more linguistic knowledge in the form of simplifi-
cation rules. Second, labeling simple sentences is
much easier than labeling raw sentences and allows
us to generalize more effectively across sentences
with differing syntax. This is particularly important
for verbs with few labeled training instances; using
training examples as efficiently as possible can lead
to considerable gains in performance. Third, our
model is very effective at sharing information across
verbs, since most of our simplification rules apply
equally well regardless of the target verb.
A major difficulty in learning to simplify sen-
tences is that we do not have labeled data for this
task. To address this problem, we simultaneously
train our simplification system and the SRL system.
We treat the correct simplification as a hidden vari-
able, using labeled SRL data to guide us towards
?more useful? simplifications. Specifically, we train
our model discriminatively to predict the correct role
labeling assignment given an input sentence, treat-
ing the simplification as a hidden variable.
Applying our combined simplification/SRL
model to the Conll 2005 task, we show a significant
improvement over a strong baseline model. Our
model does best on verbs with little training data and
on instances with paths that are rare or have never
been seen before, matching our intuitions about the
strengths of the model. Our model outperforms all
but the best few Conll 2005 systems, each of which
uses multiple different automatically-generated
parses (which would likely improve our model).
2 Sentence Simplification
Wewill begin with an example before describing our
model in detail. Figure 2 shows a series of transfor-
mations applied to the sentence ?I was not given a
chance to eat,? along with the interpretation of each
transformation. Here, the target verb is ?eat.?
I was not given a chance to eat.
Someone gave me a chance to eat.
I had a chance to eat.
I ate.
depassivizegive -> havechance to X
I was given a chance to eat.remove not
Figure 2: Example
simplification
Sam?s chance to eat has passed.
Sam has a chance to eat.
Sam ate.chance to X
possessive
Figure 3: Shared simplifica-
tion structure
There are several important things to note. First,
many of the steps do lose some semantic informa-
tion; clearly, having a chance to eat is not the same
as eating. However, since we are interested only in
labeling the core arguments of the verb (which in
this case is simply the Eater, ?I?), it is not important
to maintain this information. Second, there is more
than one way to choose a set of rules which lead
to the desired final sentence ?I ate.? For example,
we could have chosen to include a rule which went
directly from the second step to the fourth. In gen-
eral, the rules were designed to allow as much reuse
of rules as possible. Figure 3 shows the simplifica-
tion of ?Sam?s chance to eat has passed? (again with
target verb ?eat?); by simplifying both of these sen-
tences as ?X had a chance to Y?, we are able to use
the same final rule in both cases.
Of course, there may be more than one way to
simplify a sentence for a given rule set; this ambigu-
ity is handled by learning which rules to prefer.
In this paper, we use simplification to mean some-
thing which is closer to canonicalization that sum-
marization. Thus, given an input sentence, our goal
is not to produce a single shortened sentence which
contains as much information from the original sen-
tence as possible. Rather, the goal is, for each
verb in the sentence, to produce a ?simple? sentence
which is in a particular canonical form (described
below) relative to that verb.
3 Transformation Rules
A transformation rule takes as input a parse tree and
produces as output a different, changed parse tree.
Since our goal is to produce a simplified version of
the sentence, the rules are designed to bring all sen-
tences toward the same common format.
A rule (see left side of Figure 4) consists of two
345
NP-7[Someone] VB-5 NPVP-4give chanceNP-2I
S-1S-1
NP-2 VP-3
VB*-6VBN-5be VP-4
TransformedRule
Replace 3 with 4Create new node 7 ? [Someone]Substitute 7 for 2Add 2 after 5Set category of 5 to VB
S
NP VP
VBD
VBN NPwas
VP
given chance
I
Original
Figure 4: Rule for depassivizing a sentence
parts. The first is a ?tree regular expression? which
is most simply viewed as a tree fragment with op-
tional constraints at each node. The rule assigns
numbers to each node which are referred to in the
second part of the rule. Formally, a rule node X
matches a parse-tree node A if: (1) All constraints of
node X (e.g., constituent category, head word, etc.)
are satisfied by node A. (2) For each child node Y
of X, there is a child B of A that matches Y; two
children of X cannot be matched to the same child
B. There are no other requirements. A can have
other children besides those matched, and leaves of
the rule pattern can match to internal nodes of the
parse (corresponding to entire phrases in the origi-
nal sentence). For example, the same rule is used to
simplify both ?I had a chance to eat,? and ?I had a
chance to eat a sandwich,? (into ?I ate,? and ?I ate
a sandwich,?). The insertion of the phrase ?a sand-
wich? does not prevent the rule from matching.
The second part of the rule is a series of simple
steps that are applied to the matched nodes. For ex-
ample, one type of simple step applied to the pair of
nodes (X,Y) removes X from its current parent and
adds it as the final child of Y. Figure 4 shows the
depassivizing rule and the result of applying it to the
sentence ?I was given a chance.? The transformation
steps are applied sequentially from top to bottom.
Note that any nodes not matched are unaffected by
the transformation; they remain where they are rel-
ative to their parents. For example, ?chance? is not
matched by the rule, and thus remains as a child of
the VP headed by ?give.?
There are two significant pieces of ?machinery? in
our current rule set. The first is the idea of a floating
node, used for locating an argument within a subor-
dinate clause. For example, in the phrases ?The cat
that ate the mouse?, ?The seed that the mouse ate?,
and ?The person we gave the gift to?, the modified
nouns (?cat?, ?seed?, and ?person?, respectively) all
SimplifiedOriginal#Rule Category
I ate the food.Float(The food) I 
ate.5Floating nodes
He slept.I said he slept.4Sentence extraction
Food is tasty.Salt makes food tasty.8?Make? rewrites
The total includes tax.Including tax, the total?7Verb acting as PP/NP
John has a 
chance to eat.John?s chance to eat?7Possessive
I will eat.Will I eat?7Questions I will eat.Nor will I eat.7Inverted sentences
Float(The food) I 
ate.The food I ate ?8Modified nouns
I eat.I have a chance to 
eat.7Verb RC (Noun)
I eat.I am likely to eat.6Verb RC (ADJP/ADVP) I eat.I want to eat.17Verb Raising/Control (basic)
I eat.I must eat.14Verb Collapsing/Rewriting
I ate.I ate and slept.8Conjunctions John is a lawyer.John, a lawyer, ?20Misc Collapsing/Rewriting
A car hit me.I was hit by a car.5Passive
I slept Thursday.Thursday, I slept.24Sentence normalization
Table 1: Rule categories with sample simplifications.
Target verbs are underlined.
should be placed in different positions in the subor-
dinate clauses (subject, direct object, and object of
?to?) to produce the phrases ?The cat ate the mouse,?
?The mouse ate the seed?, and ?We gave the gift to
the person.? We handle these phrases by placing a
floating node in the subordinate clause which points
to the argument; other rules try to place the floating
node into each possible position in the sentence.
The second construct is a system for keeping track
of whether a sentence has a subject, and if so, what
it is. A subset of our rule set normalizes the input
sentence by moving modifiers after the verb, leaving
either a single phrase (the subject) or nothing before
the verb. For example, the sentence ?Before leaving,
I ate a sandwich,? is rewritten as ?I ate a sandwich
before leaving.? In many cases, keeping track of the
presence or absence of a subject greatly reduces the
set of possible simplifications.
Altogether, we currently have 154 (mostly unlex-
icalized) rules. Our general approach was to write
very conservative rules, i.e., avoid making rules
with low precision, as these can quickly lead to a
large blowup in the number of generated simple sen-
tences. Table 1 shows a summary of our rule-set,
grouped by type. Note that each row lists only one
possible sentence and simplification rule from that
346
S-1
NP or S VP
VB*
eat
#children(S-1) = 2
S-1
VP
VB*
eat
#children(S-1) = 1
Figure 5: Simple sentence constraints for ?eat?
category; many of the categories handle a variety of
syntax patterns. The two examples without target
verbs are helper transformations; in more complex
sentences, they can enable further simplifications.
Another thing to note is that we use the terms Rais-
ing/Control (RC) very loosely to mean situations
where the subject of the target verb is displaced, ap-
pearing as the subject of another verb (see table).
Our rule set was developed by analyzing perfor-
mance and coverage on the PropBank WSJ training
set; neither the development set nor (of course) the
test set were used during rule creation.
4 Simple Sentence Production
We now describe how to take a set of rules and pro-
duce a set of candidate simple sentences. At a high
level, the algorithm is very simple. We maintain a
set of derived parses S which is initialized to con-
tain only the original, untransformed parse. One it-
eration of the algorithm consists of applying every
possible matching transformation rule to every parse
in S, and adding all resulting parses to S. With care-
fully designed rules, repeated iterations are guaran-
teed to converge; that is, we eventually arrive at a set
S? such that if we apply an iteration of rule applica-
tion to S?, no new parses will be added. Note that
we simplify the whole sentence without respect to a
particular verb. Thus, this process only needs to be
done once per sentence (not once per verb).
To label arguments of a particular target verb, we
remove any parse from our set which does not match
one of the two templates in Figure 5 (for verb ?eat?).
These select simple sentences that have all non-
subject modifiers moved to the predicate and ?eat?
as the main verb. Note that the constraint VB* indi-
cates any terminal verb category (e.g., VBN, VBD,
etc.) A parse that matches one of these templates
is called a valid simple sentence; this is exactly
the canonicalized version of the sentence which our
simplification rules are designed to produce.
This procedure is quite expensive; we have to
copy the entire parse tree at each step, and in gen-
eral, this procedure could generate an exponential
number of transformed parses. The first issue can be
solved, and the second alleviated, using a dynamic-
programming data structure similar to the one used
to store parse forests (as in a chart parser). This data
structure is not essential for exposition; we delay
discussion until Section 7.
5 Labeling Simple Sentences
For a particular sentence/target verb pair s, v, the
output from the previous section is a set Ssv =
{tsvi }i of valid simple sentences. Although labeling
a simple sentence is easier than labeling the original
sentence, there are still many choices to be made.
There is one key assumption that greatly reduces the
search space: in a simple sentence, only the subject
(if present) and direct modifiers of the target verb
can be arguments of that verb.
On the training set, we now extract a set of role
patterns Gv = {gvj }j for each verb v. For exam-
ple, a common role pattern for ?give? is that of ?I
gave him a sandwich?. We represent this pattern
as ggive1 = {ARG0 = Subject NP, ARG1 =
Postverb NP2, ARG2 = Postverb NP1}. Note
that this is one atomic pattern; thus, we are keep-
ing track not just of occurrences of particular roles
in particular places in the simple sentence, but also
how those roles co-occur with other roles.
For a particular simple sentence tsvi , we apply
all extracted role patterns gvj to t
sv
i , obtaining a set
of possible role labelings. We call a simple sen-
tence/role labeling pair a simple labeling and denote
the set of candidate simple labelings Csv = {csvk }k.
Note that a given pair tsvi , g
v
j may generate more
than one simple labeling, if there is more than one
way to assign the elements of gvj to constituents in
tsvi . Also, for a sentence s there may be several
simple labelings that lead to the same role labeling.
In particular, there may be several simple labelings
which assign the correct labels to all constituents;
we denote this set Ksv ? Csv.
6 Probabilistic Model
We now define our probabilistic model. Given a
(possibly large) set of candidate simple labelings
Csv, we need to select a correct one. We assign
a score to each candidate based on its features:
347
Rule = DepassivizePattern = {ARG0 = Subj NP, ARG1 = PV NP2, ARG2 = PV NP1}Role = ARG0, Head Word = JohnRole = ARG1, Head Word = sandwichRole = ARG2, Head Word = IRole = ARG0, Category = NPRole = ARG1, Category = NPRole = ARG2, Category = NPRole = ARG0, Position = Subject NPRole = ARG1, Position = Postverb NP2Role = ARG2, Position = Postverb NP1
Figure 6: Features for ?John gave me a sandwich.?
which rules were used to obtain the simple sentence,
which role pattern was used, and features about the
assignment of constituents to roles. A log-linear
model then assigns probability to each simple label-
ing equal to the normalized exponential of the score.
The first type of feature is which rules were used
to obtain the simple sentence. These features are in-
dicator functions for each possible rule. Thus, we do
not currently learn anything about interactions be-
tween different rules. The second type of feature is
an indicator function of the role pattern used to gen-
erate the labeling. This allows us to learn that ?give?
has a preference for the labeling {ARG0 = Subject
NP, ARG1 = Postverb NP2, ARG2 = Postverb NP1}.
Our final features are analogous to those used in se-
mantic role labeling, but greatly simplified due to
our use of simple sentences: head word of the con-
stituent; category (i.e., constituent label); and posi-
tion in the simple sentence. Each of these features
is combined with the role assignment, so that each
feature indicates a preference for a particular role
assignment (i.e., for ?give?, head word ?sandwich?
tends to be ARG1). For each feature, we have a
verb-specific and a verb-independent version, allow-
ing sharing across verbs while still permitting dif-
ferent verbs to learn different preferences. The set
of extracted features for the sentence ?I was given
a sandwich by John? with simplification ?John gave
me a sandwich? is shown in Figure 6. We omit verb-
specific features to save space . Note that we ?stem?
all pronouns (including possessive pronouns).
For each candidate simple labeling csvk we extract
a vector of features f svk as described above. We now
define the probability of a simple labeling csvk with
respect to a weight vector w P (csvk ) =
ew
T fsvk
P
k? e
wT fsv
k?
.
Our goal is to maximize the total probability as-
signed to any correct simple labeling; therefore, for
each sentence/verb pair (s, v), we want to increase
?
csvk ?K
sv P (csvk ). This expression treats the simple
labeling (consisting of a simple sentence and a role
assignment) as a hidden variable that is summed out.
Taking the log, summing across all sentence/verb
pairs, and adding L2 regularization on the weights,
we have our final objective F (w):
?
s,v
?
?log
?
csvk ?K
sv ew
T fsvk
?
csv
k?
?Csv e
wT fsv
k?
?
??
wTw
2?2
We train our model by optimizing the objective
using standard methods, specifically BFGS. Due to
the summation over the hidden variable representing
the choice of simple sentence (not observed in the
training data), our objective is not convex. Thus,
we are not guaranteed to find a global optimum; in
practice we have gotten good results using the de-
fault initialization of setting all weights to 0.
Consider the derivative of the likelihood compo-
nent with respect to a single weight wl:
?
csvk ?K
sv
f svk (l)
P (csvk )?
csv
k?
?Ksv
P (csvk? )
?
?
csvk ?C
sv
f svk (l)P (c
sv
k )
where f svk (l) denotes the l
th component of f svk .
This formula is positive when the expected value of
the lth feature is higher on the set of correct simple
labelings Ksv than on the set of all simple labelings
Csv. Thus, the optimization procedure will tend to
be self-reinforcing, increasing the score of correct
simple labelings which already have a high score.
7 Simplification Data Structure
Our representation of the set of possible simplifi-
cations of a sentence addresses two computational
bottlenecks. The first is the need to repeatedly copy
large chunks of the sentence. For example, if we are
depassivizing a sentence, we can avoid copying the
subject and object of the original sentence by simply
referring back to them in the depassivized version.
At worst, we only need to add one node for each
numbered node in the transformation rule. The sec-
ond issue is the possible exponential blowup of the
number of generated sentences. Consider ?I want
to eat and I want to drink and I want to play and
. . . ? Each subsentence can be simplified, yielding
two possibilities for each subsentence. The number
of simplifications of the entire sentence is then ex-
ponential in the length of the sentence. However,
348
ROOT
S
NP([Someone])
VP VBD(gave)
S
NP(chance)
VP
VBD(was)
NP(I)
VBN(given)
VP
Figure 7: Data structure after applying the depassivize
rule to ?I was given (a) chance.? Circular nodes are OR-
nodes, rectangular nodes are AND-nodes.
we can store these simplifications compactly as a set
of independent decisions, ?I {want to eat OR eat}
and I {want to drink OR drink} and . . . ?
Both issues can be addressed by representing the
set of simplifications using an AND-OR tree, a gen-
eral data structure also used to store parse forests
such as those produced by a chart parser. In our case,
the AND nodes are similar to constituent nodes in a
parse tree ? each has a category (e.g. NP) and (if it
is a leaf) a word (e.g. ?chance?), but instead of hav-
ing a list of child constituents, it instead has a list of
child OR nodes. Each OR node has one or more con-
stituent children that correspond to the different op-
tions at this point in the tree. Figure 7 shows the re-
sulting AND-OR tree after applying the depassivize
rule to the original parse of ?I was given a chance.?
Because this AND-OR tree represents only two dif-
ferent parses, the original parse and the depassivized
version, only one OR node in the tree has more than
one child ? the root node, which has two choices,
one for each parse. However, the AND nodes imme-
diately above ?I? and ?chance? each have more than
one OR-node parent, since they are shared by the
original and depassivized parses1. To extract a parse
from this data structure, we apply the following re-
cursive algorithm: starting at the root OR node, each
time we reach an OR node, we choose and recurse
on exactly one of its children; each time we reach
an AND node, we recurse on all of its children. In
Figure 7, we have only one choice: if we go left at
the root, we generate the original parse; otherwise,
we generate the depassivized version.
Unfortunately, it is difficult to find the optimal
AND-OR tree. We use a greedy but smart proce-
1In this particular example, both of these nodes are leaves,
but in general shared nodes can be entire tree fragments
dure to try to produce a small tree. We omit details
for lack of space. Using our rule set, the compact
representation is usually (but not always) small.
For our compact representation to be useful, we
need to be able to optimize our objective without ex-
panding all possible simple sentences. A relatively
straight-forward extension of the inside-outside al-
gorithm for chart-parses allows us to learn and per-
form inference in our compact representation (a sim-
ilar algorithm is presented in (Geman & Johnson,
2002)). We omit details for lack of space.
8 Experiments
We evaluated our system using the setup of the Conll
2005 semantic role labeling task.2 Thus, we trained
on Sections 2-21 of PropBank and used Section 24
as development data. Our test data includes both the
selected portion of Section 23 of PropBank, plus the
extra data on the Brown corpus. We used the Char-
niak parses provided by the Conll distribution.
We compared to a strong Baseline SRL system
that learns a logistic regression model using the fea-
tures of Pradhan et al (2005). It has two stages.
The first filters out nodes that are unlikely to be ar-
guments. The second stage labels each remaining
node either as a particular role (e.g. ?ARGO?) or as a
non-argument. Note that the baseline feature set in-
cludes a feature corresponding to the subcategoriza-
tion of the verb (specifically, the sequence of nonter-
minals which are children of the predicate?s parent
node). Thus, Baseline does have access to some-
thing similar to our model?s role pattern feature, al-
though the Baseline subcategorization feature only
includes post-verbal modifiers and is generally much
noisier because it operates on the original sentence.
Our Transforms model takes as input the Char-
niak parses supplied by the Conll release, and labels
every node with Core arguments (ARG0-ARG5).
Our rule set does not currently handle either ref-
erent arguments (such as ?who? in ?The man who
ate . . . ?) or non-core arguments (such as ARGM-
TMP). For these arguments, we simply filled in us-
ing our baseline system (specifically, any non-core
argument which did not overlap an argument pre-
dicted by our model was added to the labeling).
Also, on some sentences, our system did not gen-
erate any predictions because no valid simple sen-
2http://www.lsi.upc.es/ srlconll/home.html
349
Model Dev Test Test Test
WSJ Brown WSJ+Br
Baseline 74.7 76.9 64.7 75.3
Transforms 75.6 77.4 66.8 76.0
Combined 76.0 78.0 66.4 76.5
Punyakanok 77.35 79.44 67.75 77.92
Table 2: F1 Measure using Charniak parses
tences were produced by the simplification system .
Again, we used the baseline to fill in predictions (for
all arguments) for these sentences.
Baseline and Transforms were regularized using
a Gaussian prior; for both models, ?2 = 1.0 gave
the best results on the development set.
For generating role predictions from our model,
we have two reasonable options: use the labeling
given by the single highest scoring simple labeling;
or compute the distribution over predictions for each
node by summing over all simple labelings. The lat-
ter method worked slightly better, particularly when
combined with the baseline model as described be-
low, so all reported results use this method.
We also evaluated a hybrid model that combines
the Baseline with our simplification model. For a
given sentence/verb pair (s, v), we find the set of
constituents N sv that made it past the first (filtering)
stage of Baseline. For each candidate simple sen-
tence/labeling pair csvk = (t
sv
i , g
v
j ) proposed by our
model, we check to see which of the constituents
in N sv are already present in our simple sentence
tsvi . Any constituents that are not present are then as-
signed a probability distribution over possible roles
according to Baseline. Thus, we fall back Base-
line whenever the current simple sentence does not
have an ?opinion? about the role of a particular con-
stituent. The Combined model is thus able to cor-
rectly label sentences when the simplification pro-
cess drops some of the arguments (generally due to
unusual syntax). Each of the two components was
trained separately and combined only at testing time.
Table 2 shows results of these three systems on
the Conll-2005 task, plus the top-performing system
(Punyakanok et al, 2005) for reference. Baseline al-
ready achieves good performance on this task, plac-
ing at about 75th percentile among evaluated sys-
tems. Our Transforms model outperforms Baseline
on all sets. Finally, our Combined model improves
over Transforms on all but the test Brown corpus,
Model Test WSJ
Baseline 87.6
Transforms 88.2
Combined 88.5
Table 3: F1 Measure using gold parses
achieving a statistically significant increase over the
Baseline system (according to confidence intervals
calculated for the Conll-2005 results).
The Combined model still does not achieve the
performance levels of the top several systems. How-
ever, these systems all use information from multi-
ple parses, allowing them to fix many errors caused
by incorrect parses. We return to this issue in Sec-
tion 10. Indeed, as shown in Table 3, performance
on gold standard parses is (as expected) much bet-
ter than on automatically generated parses, for all
systems. Importantly, the Combined model again
achieves a significant improvement over Baseline.
We expect that by labeling simple sentences, our
model will generalize well even on verbs with a
small number of training examples. Figure 8 shows
F1 measure on theWSJ test set as a function of train-
ing set size. Indeed, both the Transformsmodel and
the Combined model significantly outperform the
Baseline model when there are fewer than 20 train-
ing examples for the verb. While theBaselinemodel
has higher accuracy than the Transforms model for
verbs with a very large number of training examples,
theCombinedmodel is at or above both of the other
models in all but the rightmost bucket, suggesting
that it gets the best of both worlds.
We also found, as expected, that our model im-
proved on sentences with very long parse paths. For
example, in the sentence ?Big investment banks re-
fused to step up to the plate to support the beleagured
floor traders by buying blocks of stock, traders say,? the
parse path from ?buy? to its ARG0, ?Big investment
banks,? is quite long. The Transforms model cor-
rectly labels the arguments of ?buy?, while theBase-
line system misses the ARG0.
To understand the importance of different types of
rules, we performed an ablation analysis. For each
major rule category in Figure 1, we deleted those
rules from the rule set, retrained, and evaluated us-
ing the Combined model. To avoid parse-related
issues, we trained and evaluated on gold-standard
parses. Most important were rules relating to (ba-
350
F1 vs. Verb Training Examples 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0-4 5-9 10-
19
20-
49
50-
99
100
-
199
200
-
499
500
-
999
100
0-1
999
200
0-4
999 500
0+
Training Examples
F1 
Me
as
ur
e
Combined
Transforms
Baseline
Figure 8: F1 Measure on the WSJ test set as a function of
training set size. Each bucket on the X-axis corresponds
to a group of verbs for which the number of training ex-
amples fell into the appropriate range; the value is the
average performance for verbs in that bucket.
sic) verb raising/control, ?make? rewrites, modified
nouns, and passive constructions. Each of these rule
categories when removed lowered the F1 score by
approximately .4%. In constrast, removing rules
for non-basic control, possessives, and inverted sen-
tences caused a negligible reduction in performance.
This may be because the relevant syntactic structures
occur rarely; because Baseline does well on those
constructs; or because the simplification model has
trouble learning when to apply these rules.
9 Related Work
One area of current research which has similarities
with this work is on Lexical Functional Grammars
(LFGs). Both approaches attempt to abstract away
from the surface level syntax of the sentence (e.g.,
the XLE system3). The most obvious difference be-
tween the approaches is that we use SRL data to train
our system, avoiding the need to have labeled data
specific to our simplification scheme.
There have been a number of works which model
verb subcategorization. Approaches include incor-
porating a subcategorization feature (Gildea & Ju-
rafsky, 2002; Xue & Palmer, 2004), such as the one
used in our baseline; and building a model which
jointly classifies all arguments of a verb (Toutanova
et al, 2005). Our method differs from past work in
that it extracts its role pattern feature from the sim-
plified sentence. As a result, the feature is less noisy
3http://www2.parc.com/isl/groups/nltt/xle/
and generalizes better across syntactic variation than
a feature extracted from the original sentence.
Another group of related work focuses on summa-
rizing sentences through a series of deletions (Jing,
2000; Dorr et al, 2003; Galley & McKeown, 2007).
In particular, the latter two works iteratively simplify
the sentence by deleting a phrase at a time. We differ
from these works in several important ways. First,
our transformation language is not context-free; it
can reorder constituents and then apply transforma-
tion rules to the reordered sentence. Second, we are
focusing on a somewhat different task; these works
are interested in obtaining a single summary of each
sentence which maintains all ?essential? informa-
tion, while in our work we produce a simplification
that may lose semantic content, but aims to contain
all arguments of a verb. Finally, training our model
on SRL data allows us to avoid the relative scarcity
of parallel simplification corpora and the issue of de-
termining what is ?essential? in a sentence.
Another area of related work in the semantic role
labeling literature is that on tree kernels (Moschitti,
2004; Zhang et al, 2007). Like our method, tree ker-
nels decompose the parse path into smaller pieces
for classification. Our model can generalize better
across verbs because it first simplifies, then classifies
based on the simplified sentence. Also, through it-
erative simplifications we can discover structure that
is not immediately apparent in the original parse.
10 Future Work
There are a number of improvements that could be
made to the current simplification system, includ-
ing augmenting the rule set to handle more con-
structions and doing further sentence normaliza-
tions, e.g., identifying whether a direct object exists.
Another interesting extension involves incorporating
parser uncertainty into the model; in particular, our
simplification system is capable of seamlessly ac-
cepting a parse forest as input.
There are a variety of other tasks for which sen-
tence simplification might be useful, including sum-
marization, information retrieval, information ex-
traction, machine translation and semantic entail-
ment. In each area, we could either use the sim-
plification system as learned on SRL data, or retrain
the simplification model to maximize performance
on the particular task.
351
References
Dorr, B., Zajic, D., & Schwartz, R. (2003). Hedge:
A parse-and-trim approach to headline genera-
tion. Proceedings of the HLT-NAACL Text Sum-
marization Workshop and Document Understand-
ing Conference.
Galley, M., & McKeown, K. (2007). Lexicalized
markov grammars for sentence compression. Pro-
ceedings of NAACL-HLT.
Geman, S., & Johnson, M. (2002). Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. Proceedings of ACL.
Gildea, D., & Jurafsky, D. (2002). Automatic label-
ing of semantic roles. Computational Linguistics.
Jing, H. (2000). Sentence reduction for automatic
text summarization. Proceedings of Applied NLP.
Moschitti, A. (2004). A study on convolution ker-
nels for shallow semantic parsing. Proceedings of
ACL.
Pradhan, S., Hacioglu, K., Krugler, V., Ward, W.,
Martin, J. H., & Jurafsky, D. (2005). Support vec-
tor learning for semantic argument classification.
Machine Learning, 60, 11?39.
Punyakanok, V., Koomen, P., Roth, D., & Yih, W.
(2005). Generalized inference with multiple se-
mantic role labeling systems. Proceedings of
CoNLL.
Toutanova, K., Haghighi, A., &Manning, C. (2005).
Joint learning improves semantic role labeling.
Proceedings of ACL, 589?596.
Xue, N., & Palmer, M. (2004). Calibrating fea-
tures for semantic role labeling. Proceedings of
EMNLP.
Zhang, M., Che, W., Aw, A., Tan, C. L., Zhou, G.,
Liu, T., & Li, S. (2007). A grammar-driven convo-
lution tree kernel for semantic role classification.
Proceedings of ACL.
352
Proceedings of the ACL 2010 Conference Short Papers, pages 371?376,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
An Active Learning Approach to Finding Related Terms
David Vickrey
Stanford University
dvickrey@cs.stanford.edu
Oscar Kipersztok
Boeing Research & Technology
oscar.kipersztok
@boeing.com
Daphne Koller
Stanford Univeristy
koller@cs.stanford.edu
Abstract
We present a novel system that helps non-
experts find sets of similar words. The
user begins by specifying one or more seed
words. The system then iteratively sug-
gests a series of candidate words, which
the user can either accept or reject. Cur-
rent techniques for this task typically boot-
strap a classifier based on a fixed seed
set. In contrast, our system involves
the user throughout the labeling process,
using active learning to intelligently ex-
plore the space of similar words. In
particular, our system can take advan-
tage of negative examples provided by the
user. Our system combines multiple pre-
existing sources of similarity data (a stan-
dard thesaurus, WordNet, contextual sim-
ilarity), enabling it to capture many types
of similarity groups (?synonyms of crash,?
?types of car,? etc.). We evaluate on a
hand-labeled evaluation set; our system
improves over a strong baseline by 36%.
1 Introduction
Set expansion is a well-studied NLP problem
where a machine-learning algorithm is given a
fixed set of seed words and asked to find additional
members of the implied set. For example, given
the seed set {?elephant,? ?horse,? ?bat?}, the al-
gorithm is expected to return other mammals. Past
work, e.g. (Roark & Charniak, 1998; Ghahramani
& Heller, 2005; Wang & Cohen, 2007; Pantel
et al, 2009), generally focuses on semi-automatic
acquisition of the remaining members of the set by
mining large amounts of unlabeled data.
State-of-the-art set expansion systems work
well for well-defined sets of nouns, e.g. ?US Pres-
idents,? particularly when given a large seed set.
Set expansions is more difficult with fewer seed
words and for other kinds of sets. The seed words
may have multiple senses and the user may have in
mind a variety of attributes that the answer must
match. For example, suppose the seed word is
?jaguar?. First, there is sense ambiguity; we could
be referring to either a ?large cat? or a ?car.? Be-
yond this, we might have in mind various more (or
less) specific groups: ?Mexican animals,? ?preda-
tors,? ?luxury cars,? ?British cars,? etc.
We propose a system which addresses sev-
eral shortcomings of many set expansion systems.
First, these systems can be difficult to use. As ex-
plored by Vyas et al (2009), non-expert users
produce seed sets that lead to poor quality expan-
sions, for a variety of reasons including ambiguity
and lack of coverage. Even for expert users, con-
structing seed sets can be a laborious and time-
consuming process. Second, most set expansion
systems do not use negative examples, which can
be very useful for weeding out other bad answers.
Third, many set expansion systems concentrate on
noun classes such as ?US Presidents? and are not
effective or do not apply to other kinds of sets.
Our system works as follows. The user initially
thinks of at least one seed word belonging to the
desired set. One at a time, the system presents can-
didate words to the user and asks whether the can-
didate fits the concept. The user?s answer is fed
back into the system, which takes into account this
new information and presents a new candidate to
the user. This continues until the user is satisfied
with the compiled list of ?Yes? answers. Our sys-
tem uses both positive and negative examples to
guide the search, allowing it to recover from ini-
tially poor seed words. By using multiple sources
of similarity data, our system captures a variety of
kinds of similarity. Our system replaces the poten-
tially difficult problem of thinking of many seed
words with the easier task of answering yes/no
questions. The downside is a possibly increased
amount of user interaction (although standard set
expansion requires a non-trivial amount of user in-
teraction to build the seed set).
There are many practical uses for such a sys-
tem. Building a better, more comprehensive the-
saurus/gazetteer is one obvious application. An-
other application is in high-precision query expan-
sion, where a human manually builds a list of ex-
371
pansion terms. Suppose we are looking for pages
discussing ?public safety.? Then synonyms (or
near-synonyms) of ?safety? would be useful (e.g.
?security?) but also non-synonyms such as ?pre-
cautions? or ?prevention? are also likely to return
good results. In this case, the concept we are inter-
ested in is ?Words which imply that safety is being
discussed.? Another interesting direction not pur-
sued in this paper is using our system as part of
a more-traditional set expansion system to build
seed sets more quickly.
2 Set Expansion
As input, we are provided with a small set of seed
words s. The desired output is a target set of
words G, consisting of all words that fit the de-
sired concept. A particular seed set s can belong
to many possible goal sets G, so additional infor-
mation may be required to do well.
Previous work tries to do as much as possible
using only s. Typically s is assumed to contain at
least 2 words and often many more. Pantel et al
(2009) discusses the issue of seed set size in detail,
concluding that 5-20 seed words are often required
for good performance.
There are several problems with the fixed seed
set approach. It is not always easy to think of
even a single additional seed word (e.g., the user is
trying to find ?German automakers? and can only
think of ?Volkswagen?). Even if the user can think
of additional seed words, time and effort might be
saved by using active learning to find good sug-
gestions. Also, as Vyas et al (2009) show, non-
expert users often produce poor-quality seed sets.
3 Active Learning System
Any system for this task relies on information
about similarity between words. Our system takes
as input a rectangular matrix M . Each column
corresponds to a particular word. Each row cor-
responds to a unique dimension of similarity; the
jth entry in row i mij is a number between 0 and
1 indicating the degree to which wj belongs to the
ith similarity group. Possible similarity dimen-
sions include ?How similar is word wj to the verb
jump?? ?Is wj a type of cat?? and ?Are the words
which appear in the context of wj similar to those
that appear in the context of boat?? Each row ri
of M is labeled with a word li. This may follow
intuitively from the similarity axis (e.g., ?jump,?
?cat,? and ?boat?, respectively), or it can be gen-
erated automatically (e.g. the word wj with the
highest membership mij).
Let ? be a vector of weights, one per row, which
correspond to how well each row aligns with the
goal set G. Thus, ?i should be large and positive if
row i has large entries for positive but not negative
examples; and it should be large and negative if
row i has large entries for negative but not positive
examples. Suppose that we have already chosen
an appropriate weight vector ?. We wish to rank
all possible words (i.e., the columns of M ) so that
the most promising word gets the highest score.
A natural way to generate a score zj for column
j is to take the dot product of ? with column j,
zj =
?
i ?imij . This rewards word wj for having
high membership in rows with positive ?, and low
membership in rows with negative ?.
Our system uses a ?batch? approach to active
learning. At iteration i, it chooses a new ? based
on all data labeled so far (for the 1st iteration,
this data consists of the seed set s). It then
chooses the column (word) with the highest score
(among words not yet labeled) as the candidate
wordwi. The user answers ?Yes? or ?No,? indicat-
ing whether or not wi belongs to G. wi is added
to the positive set p or the negative set n based
on the user?s answer. Thus, we have a labeled data
set that grows from iteration to iteration as the user
labels each candidate word. Unlike set expansion,
this procedure generates (and uses) both positive
and negative examples.
We explore two options for choosing ?. Recall
that each row i is associated with a label li. The
first method is to set ?i = 1 if li ? p (that is, the
set of positively labeled words includes label li),
?i = ?1 if li ? n, and ?i = 0 otherwise. We
refer to this method as ?Untrained?, although it is
still adaptive ? it takes into account the labeled
examples the user has provided so far.
The second method uses a standard machine
learning algorithm, logistic regression. As be-
fore, the final ranking over words is based on the
score zj . However, zj is passed through the lo-
gistic function to produce a score between 0 and
1, z?j =
1
1+e?zj
. We can interpret this score
as the probability that wj is a positive example,
P?(Y |wj). This leads to the objective function
L(?) = log(
?
wj?p
P?(Y |wj)
?
wj?n
(1?P?(Y |wj))).
This objective is convex and can be optimized us-
ing standard methods such as L-BFGS (Liu & No-
cedal, 1989). Following standard practice we add
an L2 regularization term ? ?
T ?
2?2 to the objective.
This method does not use the row labels li.
372
Data Word Similar words
Moby arrive accomplish, achieve, achieve success, advance, appear, approach, arrive at, arrive in, attain,...
WordNet factory (plant,-1.9);(arsenal,-2.8);(mill,-2.9);(sweatshop,-4.1);(refinery,-4.2);(winery,-4.5);...
DistSim watch (jewerly,.137),(wristwatch,.115),(shoe,0.09),(appliance,0.09),(household appliance,0.089),...
Table 1: Examples of unprocessed similarity entries from each data source.
4 Data Sources
We consider three similarity data sources: the
Moby thesaurus1, WordNet (Fellbaum, 1998), and
distributional similarity based on a large corpus
of text (Lin, 1998). Table 1 shows similarity lists
from each. These sources capture different kinds
of similarity information, which increases the rep-
resentational power of our system. For all sources,
the similarity of a word with itself is set to 1.0.
It is worth noting that our system is not strictly
limited to choosing from pre-existing groups. For
example, if we have a list of luxury items, and an-
other list of cars, our system can learn weights so
that it prefers items in the intersection, luxury cars.
Moby thesaurus consists of a list of word-
based thesaurus entries. Each word wi has a list of
similar words simij . Moby has a total of about 2.5
million related word pairs. Unlike some other the-
sauri (such as WordNet and thesaurus.com), en-
tries are not broken down by word sense.
In the raw format, the similarity relation is not
symmetric; for example, there are many words
that occur only in similarity lists but do not have
their own entries. We augmented the thesaurus to
make it symmetric: if ?dog? is in the similarity en-
try for ?cat,? we add ?cat? to the similarity entry
for ?dog? (creating an entry for ?dog? if it does not
exist yet). We then have a row i for every similar-
ity entry in the augmented thesaurus; mij is 1 if
wj appears in the similarity list of wi, and 0 other-
wise. The label li of row i is simply word wi. Un-
like some other thesauri (including WordNet and
thesaurus.com), the entries are not broken down
by word sense or part of speech. For polysemic
words, there will be a mix of the words similar to
each sense and part of speech.
WordNet is a well-known dictionary/thesaurus/
ontology often used in NLP applications. It con-
sists of a large number of synsets; a synset is a set
of one or more similar word senses. The synsets
are then connected with hypernym/hyponym links,
which represent IS-A relationships. We focused
on measuring similarity in WordNet using the hy-
pernym hierarchy.2. There are many methods for
1Available at icon.shef.ac.uk/Moby/.
2A useful similarity metric we did not explore in this pa-
per is similarity between WordNet dictionary definitions
converting this hierarchy into a similarity score;
we chose to use the Jiang-Conrath distance (Jiang
& Conrath, 1997) because it tends to be more ro-
bust to the exact structure of WordNet. The num-
ber of types of similarity in WordNet tends to be
less than that captured by Moby, because synsets
in WordNet are (usually) only allowed to have a
single parent. For example, ?murder? is classified
as a type of killing, but not as a type of crime.
The Jiang-Conrath distance gives scores for
pairs of word senses, not pairs of words. We han-
dle this by adding one row for every word sense
with the right part of speech (rather than for ev-
ery word); each row measures the similarity of ev-
ery word to a particular word sense. The label of
each row is the (undisambiguated) word; multiple
rows can have the same label. For the columns, we
do need to collapse the word senses into words;
for each word, we take a maximum across all of
its senses. For example, to determine how similar
(the only sense of) ?factory? is to the word ?plant,?
we compute the similarity of ?factory? to the ?in-
dustrial plant? sense of ?plant? and to the ?living
thing? sense of ?plant? and take the higher of the
two (in this case, the former).
The Jiang-Conrath distance is a number be-
tween?? and 0. By examination, we determined
that scores below ?12.0 indicate virtually no sim-
ilarity. We cut off scores below this point and
linearly mapped each score x to the range 0 to
1, yielding a final similarity of min(0,x+12)12 . This
greatly sparsified the similarity matrix M .
Distributional similarity. We used Dekang
Lin?s dependency-based thesaurus, available at
www.cs.ualberta.ca/?lindek/downloads.htm.
This resource groups words based on the words
they co-occur with in normal text. The words
most similar to ?cat? are ?dog,? ?animal,? and
?monkey,? presumably because they all ?eat,?
?walk,? etc. Like Moby, similarity entries are not
divided by word sense; usually, only the dominant
sense of each word is represented. This type of
similarity is considerably different from the other
two types, tending to focus less on minor details
and more on broad patterns.
Each similarity entry corresponds to a single
373
wordwi and is a list of scored similar words simij .
The scores vary between 0 and 1, but usually the
highest-scored word in a similarity list gets a score
of no more than 0.3. To calibrate these scores
with the previous two types, we divided all scores
by the score of the highest-scored word in that
list. Since each row is normalized individually,
the similarity matrix M is not symmetric. Also,
there are separate similarity lists for each of nouns,
verbs, and modifiers; we only used the lists match-
ing the seed word?s part of speech.
5 Experimental Setup
Given a seed set s and a complete target set G, it is
easy to evaluate our system; we say ?Yes? to any-
thing in G, ?No? to everything else, and see how
many of the candidate words are in G. However,
building a complete gold-standard G is in practice
prohibitively difficult; instead, we are only capa-
ble of saying whether or not a word belongs to G
when presented with that word.
To evaluate a particular active learning algo-
rithm, we can just run the algorithm manually, and
see how many candidate words we say ?Yes? to
(note that this will not give us an accurate estimate
of the recall of our algorithm). Evaluating several
different algorithms for the same s and G is more
difficult. We could run each algorithm separately,
but there are several problems with this approach.
First, we might unconsciously (or consciously)
bias the results in favor of our preferred algo-
rithms. Second, it would be fairly difficult to be
consistent across multiple runs. Third, it would be
inefficient, since we would label the same words
multiple times for different algorithms.
We solved this problem by building a labeling
system which runs all algorithms that we wish to
test in parallel. At each step, we pick a random al-
gorithm and either present its current candidate to
the user or, if that candidate has already been la-
beled, we supply that algorithm with the given an-
swer. We do NOT ever give an algorithm a labeled
training example unless it actually asks for it ? this
guarantees that the combined system is equivalent
to running each algorithm separately. This pro-
cedure has the property that the user cannot tell
which algorithms presented which words.
To evaluate the relative contribution of active
learning, we consider a version of our system
where active learning is disabled. Instead of re-
training the system every iteration, we train it once
on the seed set s and keep the weight vector ? fixed
from iteration to iteration.
We evaluated our algorithms along three axes.
First, the method for choosing ?: Untrained and
Logistic (U and L). Second, the data sources used:
each source separately (M for Moby, W for Word-
Net, D for distributional similarity), and all three
in combination (MWD). Third, whether active
learning is used (+/-). Thus, logistic regression us-
ing Moby and no active learning is L(M,-). For lo-
gistic regression, we set the regularization penalty
?2 to 1, based on qualitative analysis during devel-
opment (before seeing the test data).
We also compared the performance of our
algorithms to the popular online thesaurus
http://thesaurus.com. The entries in this
thesaurus are similar to Moby, except that each
word may have multiple sense-disambiguated en-
tries. For each seed word w, we downloaded the
page for w and extracted a set of synonyms en-
tries for that word. To compare fairly with our al-
gorithms, we propose a word-by-word method for
exploring the thesaurus, intended to model a user
scanning the thesaurus. This method checks the
first 3 words from each entry; if none of these are
labeled ?Yes,? it moves on to the next entry. We
omit details for lack of space.
6 Experimental Results
We designed a test set containing different types
of similarity. Table 2 shows each category, with
examples of specific similarity queries. For each
type, we tested on five different queries. For each
query, the first author built the seed set by writ-
ing down the first three words that came to mind.
For most queries this was easy. However, for the
similarity type Hard Synonyms, coming up with
more than one seed word was considerably more
difficult. To build seed sets for these queries, we
ran our evaluation system using a single seed word
and took the first two positive candidates; this en-
sured that we were not biasing our seed set in favor
of a particular algorithm or data set.
For each query, we ran our evaluation system
until each algorithm had suggested 25 candidate
words, for a total of 625 labeled words per algo-
rithm. We measured performance using mean av-
erage precision (MAP), which corresponds to area
under the precision-recall curve. It gives an over-
all assessment across different stopping points.
Table 3 shows results for an informative sub-
set of the tested algorithms. There are many con-
clusions we can draw. Thesaurus.Com performs
poorly overall; our best system, L(MWD,+),
outscores it by 164%. The next group of al-
374
Category Name Example Similarity Queries
Simple Groups (SG) car brands, countries, mammals, crimes
Complex Groups (CG) luxury car brands, sub-Saharan countries
Synonyms (Syn) syn of {scandal, helicopter, arrogant, slay}
Hard Synonyms (HS) syn of {(stock-market) crash, (legal) maneuver}
Meronym/Material (M) parts of a car, things made of wood
Table 2: Categories and examples
Algorithm MAP
Thesaurus.Com .122
U(M,-) .176
U(W,-) .182
U(D,-) .211
L(D,-) .236
L(D,+) .288
U(MWD,-) .233
U(MWD,+) .271
L(MWD,-) .286
L(MWD,+) .322
Table 3: Comparison of algorithms
SG CG Syn HS M
Thesaurus.Com .041 .060 .275 .173 .060
L(D,+) .377 .344 .211 .329 .177
L(M,-) .102 .118 .393 .279 .119
U(W,+) .097 .136 .296 .277 .165
U(MWD,+) .194 .153 .438 .357 .213
L(MWD,-) .344 .207 .360 .345 .173
L(MWD,+) .366 .335 .379 .372 .158
Table 4: Results by category
gorithms, U(*,-), add together the similarity en-
tries of the seed words for a particular similarity
source. The best of these uses distributional simi-
larity; L(MWD,+) outscores it by 53%. Combin-
ing all similarity types, U(MWD,-) improves by
10% over U(D,-). L(MWD,+) improves over the
best single-source, L(D,+), by a similar margin.
Using logistic regression instead of the un-
trained weights significantly improves perfor-
mance. For example, L(MWD,+) outscores
U(MWD,+) by 19%. Using active learning also
significantly improves performance: L(MWD,+)
outscores L(MWD,-) by 13%. This shows that
active learning is useful even when a reasonable
amount of initial information is available (three
seed words for each test case). The gains from
logistic regression and active learning are cumula-
tive; L(MWD,+) outscores U(MWD,-) by 38%.
Finally, our best system, L(MWD,+) improves
over L(D,-), the best system using a single data
source and no active learning, by 36%. We con-
sider L(D,-) to be a strong baseline; this compari-
son demonstrates the usefulness of the main con-
tributions of this paper, the use of multiple data
sources and active learning. L(D,-) is still fairly
sophisticated, since it combines information from
the similarity entries for different words.
Table 4 shows the breakdown of results by cat-
egory. For this chart, we chose the best set-
ting for each similarity type. Broadly speaking,
the thesauri work reasonably well for synonyms,
but poorly for groups. Meronyms were difficult
across the board. Neither logistic regression nor
active learning always improved performance, but
L(MWD,+) performs near the top for every cate-
gory. The complex groups category is particularly
interesting, because achieving high performance
on this category required using both logistic re-
gression and active learning. This makes sense
since negative evidence is particularly important
for this category.
7 Discussion and Related Work
The biggest difference between our system and
previous work is the use of active learning, espe-
cially in allowing the use of negative examples.
Most previous set expansion systems use boot-
strapping from a small set of positive examples.
Recently, the use of negative examples for set ex-
pansion was proposed by Vyas and Pantel (2009),
although in a different way. First, set expansion is
run as normal using a fixed seed set. Then, human
annotators label a small number of negative exam-
ples from the returned results, which are used to
weed out other bad answers. Our method incorpo-
rates negative examples at an earlier stage. Also,
we use a logistic regression model to robustly in-
corporate negative information, rather than deter-
ministically ruling out words and features.
Our system is limited by our data sources. Sup-
pose we want actors who appeared in Star Wars. If
we only know that Harrison Ford andMark Hamill
are actors, we have little to go on. There has
been a large amount of work on other sources of
word-similarity. Hughes and Ramage (2007) use
random walks over WordNet, incorporating infor-
mation such as meronymy and dictionary glosses.
Snow et al (2006) extract hypernyms from free
text. Wang and Cohen (2007) exploit web-page
structure, while Pasca and Durme (2008) exam-
ine query logs. We expect that adding these types
of data would significantly improve our system.
375
References
Fellbaum, C. (Ed.). (1998). Wordnet: An elec-
tronic lexical database. MIT Press.
Ghahramani, Z., & Heller, K. (2005). Bayesian
sets. Advances in Neural Information Process-
ing Systems (NIPS).
Hughes, T., & Ramage, D. (2007). Lexical se-
mantic relatedness with random graph walks.
EMNLP-CoNLL.
Jiang, J., & Conrath, D. (1997). Semantic similar-
ity based on corpus statistics and lexical taxon-
omy. Proceedings of International Conference
on Research in Computational Linguistics.
Lin, D. (1998). An information-theoretic defini-
tion of similarity. Proceedings of ICML.
Liu, D. C., & Nocedal, J. (1989). On the lim-
ited memory method for large scale optimiza-
tion. Mathematical Programming B.
Pantel, P., Crestan, E., Borkovsky, A., Popescu,
A., & Vyas, V. (2009). Web-scale distributional
similarity and entity set expansion. EMNLP.
Pasca, M., & Durme, B. V. (2008). Weakly-
supervised acquisition of open-domain classes
and class attributes from web documents and
query logs. ACL.
Roark, B., & Charniak, E. (1998). Noun-phrase
co-occurrence statistics for semiautomatic se-
mantic lexicon construction. ACL-COLING.
Snow, R., Jurafsky, D., & Ng, A. (2006). Seman-
tic taxonomy induction from heterogenous evi-
dence. ACL.
Vyas, V., & Pantel, P. (2009). Semi-automatic en-
tity set refinement. NAACL/HLT.
Vyas, V., Pantel, P., & Crestan, E. (2009). Helping
editors choose better seed sets for entity expan-
sion. CIKM.
Wang, R., & Cohen, W. (2007). Language-
independent set expansion of named entities us-
ing the web. Seventh IEEE International Con-
ference on Data Mining.
376
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 268?272
Manchester, August 2008
Applying Sentence Simplification to the CoNLL-2008 Shared Task
David Vickrey and Daphne Koller
Stanford University
Stanford, CA 94305-9010
{dvickrey,koller}@cs.stanford.edu
Abstract
Our submission to the CoNLL-2008
shared task (Surdeanu et al, 2008) focused
on applying a novel method for semantic
role labeling to the shared task. Our system
first simplifies each sentence to be labeled
using a set of hand-constructed rules; the
weights of the system are trained on se-
mantic role labeling data to generate sim-
plifications which are as useful as possible
for semantic role labeling. Our system is
only a semantic role labeling system, and
thus did not receive a score for Syntactic
Dependencies (or, by extension, a score for
the complete problem). Unlike most sys-
tems in the shared task, our system took
constituency parses as input. On the sub-
task of semantic dependencies, our system
obtained an F1 score of 76.17, the high-
est in the open task. In this paper we give
a high-level overview of the sentence sim-
plification system, and discuss and analyze
the modifications to this system required
for the CoNLL-2008 shared task.
1 Sentence Simplification
The main technical interest of our method is a sen-
tence simplification system. This system is de-
scribed in depth in (Vickrey and Koller, 2008); for
lack of space, we omit many details, including a
discussion of related work, from this paper.
Current semantic role labeling systems rely pri-
marily on syntactic features in order to identify
and classify roles. Features derived from a syntac-
tic parse of the sentence have proven particularly
useful (Gildea and Jurafsky, 2002). For example,
the syntactic subject of ?eat? is nearly always the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
SNP VP
VP
NP PP
Tom wants S
a
to
eatVPNP NPsalad
croutonswith
Tom: NP S(NP) VPVP VPS TNP1croutons: VPPP(with)Tsalad: NP1 VP T
Figure 1: Constituency parse with path features for verb
?eat?. I was not given a chance to eat.
Someone gave me a chance to eat.
I had a chance to eat.
I ate.
depassivizegive -> havechance to X
I was given a chance to eat.remove not
Figure 2: Example simplification
ARG0. An example sentence with extracted path
features is shown in Figure 1.
A major problem with this approach is that the
path from a phrase to the verb can be quite com-
plicated. In the sentence ?He expected to receive a prize
for winning,? the path from ?win? to its ARG0, ?he?,
involves the verbs ?expect? and ?receive? and the
preposition ?for.? The corresponding path through
the parse tree likely occurs a small number of times
(or not at all) in the training corpus. If the test set
contained exactly the same sentence but with ?ex-
pected? replaced by ?did not expect? we would ex-
tract a different parse path feature; therefore, as far
as the classifier is concerned, the syntax of the two
sentences is totally unrelated.
The idea of our method is to learn a mapping
from full, complicated sentences to simplified sen-
268
NP-7[Someone] VB-5 NPVP-4give chanceNP-2I
S-1S-1
NP-2 VP-3
VB*-6VBN-5be VP-4
TransformedRule
Replace 3 with 4Create new node 7 ? [Someone]Substitute 7 for 2Add 2 after 5Set category of 5 to VB
S
NP VP
VBD
VBN NPwas
VP
given chance
I
Original
Figure 3: Rule for depassivizing a sentence
tences. Figure 2 shows an example of a series
of simplifications applied to the sentence ?I was
not given a chance to eat.? Our method com-
bines hand-written syntactic simplification rules
with machine learning, which determines which
rules to prefer. We then use the output of the sim-
plification system as input to an SRL system that
is trained to label simplified sentences.
There are several reasons to simplify sentences
before doing semantic role labeling. First, label-
ing simplified sentences is much easier than label-
ing raw sentences. Second, by mapping all sen-
tences to a common, canonical form, we can gener-
alize more effectively across sentences with differ-
ing syntax. Third, our model is effective at sharing
information across verbs, since most of our simpli-
fication rules apply equally well regardless of the
target verb. These latter two benefits are particu-
larly important for verbs with few labeled training
instances; using training examples efficiently can
lead to considerable gains in performance.
Note that unlike most participants in the
CoNLL-2008 Shared Task (Surdeanu et al, 2008),
our model took as input constituency parses as
generated by the Charniak parser (specifically, we
used the parses provided with the CoNLL-2005
shared task distribution). This was the only labeled
data used that was not available in the closed task.
1.1 Transformation Rules
At the center of our sentence simplification system
is a hand-written set of transformation rules. A
transformation rule takes as input a parse tree and
produces as output a different, changed parse tree.
Since our goal is to produce a simplified version
of the sentence, the rules are designed to bring all
sentences toward the same common format.
A rule (see left side of Figure 3) consists of two
parts. The first is a ?tree regular expression?, a tree
fragment with optional constraints at each node.
The rule assigns numbers to each node which are
referred to in the second part of the rule. Formally,
a rule node X matches a parse-tree node A if: (1)
SimplifiedOriginal#Rule Category
I ate the food.Float(The food) I 
ate.5Floating nodes
He slept.I said he slept.4Sentence extraction
Food is tasty.Salt makes food tasty.8?Make? rewrites
The total includes tax.Including tax, the total?7Verb acting as PP/NP
John has a 
chance to eat.John?s chance to eat?7Possessive
I will eat.Will I eat?7Questions I will eat.Nor will I eat.7Inverted sentences
Float(The food) I 
ate.The food I ate ?8Modified nouns
I eat.I have a chance to 
eat.7Verb RC (Noun)
I eat.I am likely to eat.6Verb RC (ADJP/ADVP) I eat.I want to eat.17Verb Raising/Control (basic)
I eat.I must eat.14Verb Collapsing/Rewriting
I ate.I ate and slept.8Conjunctions John is a lawyer.John, a lawyer, ?20Misc Collapsing/Rewriting
A car hit me.I was hit by a car.5Passive
I slept Thursday.Thursday, I slept.24Sentence normalization
Table 1: Rule categories with sample simplifica-
tions. Target verbs are underlined.
All constraints of node X (e.g., constituent cate-
gory, head word, etc.) are satisfied by node A.
(2) For each child node Y of X, there is a child
B of A that matches Y; two children of X cannot
be matched to the same child B. There are no other
requirements. A can have other children besides
those matched, and leaves of the rule pattern can
match to internal nodes of the parse (correspond-
ing to entire phrases in the original sentence). For
example, the same rule is used to simplify both ?I
had a chance to eat,? and ?I had a chance to eat a
sandwich,? (into ?I ate,? and ?I ate a sandwich,?).
The second part of the rule is a series of simple
steps that are applied to the matched nodes. For ex-
ample, one type of simple step applied to the pair
of nodes (X,Y) removes X from its current parent
and adds it as the final child of Y. Figure 3 shows
the depassivizing rule and the result of applying it
to the sentence ?I was given a chance.? The trans-
formation steps are applied sequentially from top
to bottom. Any nodes not matched are unaffected
by the transformation; they remain where they are
relative to their parents. For example, ?chance?
is not matched by the rule, and thus remains as a
child of the VP headed by ?give.?
1.2 Rule Set
Altogether, we currently have 154 (mostly unlex-
icalized) rules. Table 1 shows a summary of our
rule-set, grouped by type. Note that each row lists
269
only one possible sentence and simplification rule
from that category; many of the categories handle a
variety of syntax patterns. Our rule set was devel-
oped by analyzing performance and coverage on
the PropBank WSJ training set; neither the devel-
opment set nor (of course) the test set were used
during rule creation. Please refer to (Vickrey and
Koller, 2008) for more details about the rule set.
In the context of the CoNLL-2008 Shared Task,
the rule set might be viewed as consisting of out-
side information. Since we only submitted a sys-
tem to the open task, this was not an issue.
1.3 Generating Simple Sentences
We now describe how to produce all possible sim-
plified sentences for a given input sentence. We
maintain a set of derived parses S which is initial-
ized to contain only the original, untransformed
parse. One iteration of the algorithm consists of
applying every possible matching transformation
rule to every parse in S, and adding all resulting
parses to S. With carefully designed rules, re-
peated iterations are guaranteed to converge; that
is, we eventually arrive at a set
?
S such that if we
apply an iteration of rule application to
?
S, no new
parses are added. Note that we simplify the whole
sentence without respect to a particular verb.
We then find all parses in
?
S that have ?eat? as
the main verb. We call such a parse a valid simple
sentence; this is exactly the canonicalized version
of the sentence which our simplification rules are
designed to produce.
1.4 Labeling Simple Sentences
For a particular sentence/target verb pair s, v, the
output from the previous section is a set S
sv
=
{t
sv
i
}
i
of valid simple sentences. From the train-
ing set, we now extract a set of role patterns
G
v
= {g
v
j
}
j
for each verb v. For example, a
common role pattern for ?give? is that of ?I gave
him a sandwich?. We represent this pattern as
g
give
1
= {ARG0 = Subject NP, ARG1 =
Postverb NP2, ARG2 = Postverb NP1}.
For each simple sentence t
sv
i
? S
sv
, we ap-
ply all extracted role patterns g
v
j
to t
sv
i
, obtaining
a set of possible role labelings. We call a sim-
ple sentence/role labeling pair a simple labeling
and denote the set of candidate simple labelings
C
sv
= {c
sv
k
}
k
.
1.5 Probabilistic Model
Given a (possibly large) set of candidate simple la-
belings C
sv
, we need to select a correct one. We
Rule = DepassivizePattern = {ARG0 = Subj NP, ARG1 = PV NP2, ARG2 = PV NP1}Role = ARG0, Head Word = JohnRole = ARG1, Head Word = sandwichRole = ARG2, Head Word = IRole = ARG0, Category = NPRole = ARG1, Category = NPRole = ARG2, Category = NPRole = ARG0, Position = Subject NPRole = ARG1, Position = Postverb NP2Role = ARG2, Position = Postverb NP1
Figure 4: Features for ?John gave me a sandwich.?
assign a score to each candidate based on its fea-
tures: which rules were used to obtain the simple
sentence, which role pattern was used, and fea-
tures about the assignment of constituents to roles.
The set of extracted features for the sentence ?I
was given a sandwich by John? with simplification
?John gave me a sandwich? is shown in Figure 4.
We now define a log-linear model which as-
signs a probability to each candidate simple label-
ing based on its score. Specifically, the probability
of a simple labeling c
sv
k
with respect to a weight
vector w is P (c
sv
k
) =
e
w
T
f
sv
k
?
k
?
e
w
T
f
sv
k
?
.
Unfortunately, we do not have labeled examples
of correct simplifications. To get around this, we
treat the correct simplification as a hidden variable.
Thus, we say that the probability of a particular
semantic role labeling is
?
c
sv
k
?K
sv
P (c
sv
k
). This
leads to our final objective,
?
s,v
?
?
log
?
c
sv
k
?K
sv
e
w
T
f
sv
k
?
c
sv
k
?
?C
sv
e
w
T
f
sv
k
?
?
?
?
w
T
w
2?
2
.
We train our model by optimizing the objective
using standard methods, specifically BFGS. Due
to the summation over the hidden variable repre-
senting the choice of simplification (not observed
in the training data), our objective is not convex.
Thus, we are not guaranteed to find a global opti-
mum; in practice we have gotten good results using
the initialization of setting all weights to 0.
2 Baseline Model
In addition to our simplification system, we also
built a high-performing logistic regression model
for semantic role labeling, which we refer to as
Baseline. This model uses a slightly modified ver-
sion of the features used in (Pradhan et al, 2005).
This model was also trained on the PropBank train-
ing set, using Charniak constituency parses.
270
Both our simplification model andBaseline pro-
duce labeled constituencies. Since we were re-
quired to produce semantic dependency relations,
we simply labeled the head word of each con-
stituent with the role chosen by the model.
3 Labeling Nouns
The 2008 shared task requires systems to label the
arguments of both nouns and verbs. However, our
sentence simplification system was built to handle
only verbs. While in principle the model can nat-
urally be extended to label nouns by the addition
of further syntactic simplification rules, we were
not able to complete this extension in time for the
contest deadline. Instead, we trained our Baseline
model to label the arguments of nouns as well as
verbs. The features of this model are the same as
those used to label verbs, and were not extended to
handle special features of nouns.
4 Identifying Predicates
Another important subtask was to identify the
predicates to be labeled. In the labeled training
corpus, nouns with no labeled arguments are gen-
erally skipped (i.e., not labeled as predicates at all).
Thus, we made a strong simplifying assumption: if
a predicate (either noun or verb) is labeled by our
system as having no arguments, we should not la-
bel it as being a predicate. On the development
set, out of a total of 6390 labeled predicates, only
23 had no labeled arguments; thus, this assumption
appears to be quite reasonable.
Our system architecture was as follows. First,
we modified the training (and test) set by mark-
ing as a potential predicate every word that was ei-
ther: a) a verb that wasn?t ?do?, ?be?, or ?have? or
b) a noun found in the nombank index. Then, we
trained our system on all potential predicates (not
just predicates that were actually labeled). Finally,
after applying our classifier to the test data, we re-
moved any predicate with no labeled arguments.
5 Sense-Tagging Predicates
We tried three simple heuristics for sense-labeling
the predicates. All of them were applied at the end
of our pipeline, and thus did not interact with the
argument labeling decisions.
The simplest heuristic labeled every predicate as
sense 1. A slightly more intelligent heuristic la-
beled every predicate with its most common sense
in the training set. Finally, we extended this heuris-
tic to label each verb with its most common sense
for the subcategorization (i.e., set of roles) actu-
ally produced by the labeling system. Thus, if one
sense was intransitive while the other was transi-
tive, we would be able to distinguish between them
(assuming that our labeling system produced the
correct set of arguments). For this third heuris-
tic, we ignored all but the core arguments (ARG0 -
ARG5). The final heuristic was the most effective,
as discussed in the results section.
6 Results
The first stage of Baseline, which tries to filter out
constituents which are obviously not arguments,
took about three hours and approximately 4Gb of
memory to train
1
. The second stage, which per-
forms the final classification of arguments, took
about four hours and 3Gb of memory to train.
The sentence simplification system, which we
will refer to as Simplification, works in two steps.
First, it generates the set of all possible simplifi-
cations for each sentence. This step took a rela-
tively small amount of memory, under 1Gb, but
took around 24 hours to complete. The set of sim-
plifications is stored in a compact form; the total
storage required for all simplifications of all sen-
tences was roughly 4 times the (compressed) size
of the Charniak input parses. The second step,
which trains the model using the possible simplifi-
cations, took around 12 hours and 3Gb of memory.
We only submitted results for the semantic de-
pendencies portion of the competition. The sys-
tem we used was the Combined system described
in (Vickrey and Koller, 2008), which combines
the simplification procedure with the Baseline
model. The Combined model was augmented
with the modifications described above. Our sys-
tem achieved an official F1 score on the SRL
subtask of 76.17, the highest in the open task.
Our results are not strictly comparable to those
in the closed task, due to the use of the Char-
niak parser trained on Penn Treebank constituency
parses. However, a comparison still provides in-
sight into the relative strength of our system; our
score would place us tied for fourth in the closed
challenge for semantic dependencies.
We will now discuss the relative contributions
of various components of our system. All results
in this section are for TestWSJ + TestBrown.
Our Combined model provides the same ben-
efit over Baseline as described in (Vickrey and
1
All runs were done on a dual core 2.66Mhz Xeon ma-
chine with 4Gb of RAM
271
Koller, 2008) for labeling the arguments of verbs
2
.
When applied to just verb predicates, the Com-
binedmodel provides a statistically significant im-
provement of 1.2 points of F1 score over Base-
line. However, since the CoNLL-2008 shared task
adds both labeling of noun dependencies and pred-
icate identification and sense tagging, the gain due
to better labeling arguments of verbs is reduced.
The Baseline model achieves an F1 score of 75.31
on the semantic dependencies task, .86 F1 points
lower than the Combined system.
Note that while most of this gain is directly due
to better verb argument labeling, better verb ar-
gument labeling also indirectly slightly improves
predicate identification and sense-tagging since we
use the predicted arguments for both of of these
subtasks. We do in fact see a small increase for
labeling and sense-tagging predicates, from 80.72
F1 for the Baseline to 80.81 F1 for Combined.
As mentioned, we use Baseline to label the ar-
guments of nouns. Noun argument labeling ap-
pears to be more difficult than verb argument la-
beling, or at least requires some modification of
the features. Baseline obtains an F1 score of 75.64
for verbs, but only 68.19 F1 for nouns.
On the subtask of predicate identification, Com-
bined achieved an F1 of 90.65. It performed bet-
ter on verbs than nouns. For predicates with part
of speech VB*
3
it scored 95.43 F1; for predicates
with part of speech NN*, it scored 85.97 F1. Verbs
without arguments are often labeled in the gold
data, so the verb score could perhaps be improved
by retaining verb predicates without arguments.
As described above, we tried three heuristics for
sense-labeling predicates. Our final system used
the third heuristic, which chose the most com-
mon sense for the set of labeled arguments pro-
duced by the system. Combined obtained an F1
score of 80.81 on the combined predicate identifi-
cation/classification task, with a score of 82.58 for
verbs and 79.28 for nouns. The decrease in per-
formance by adding classification is much larger
for verbs than nouns; verb sense classification is
apparently significantly more difficult than noun
sense classification (at least for verbal nouns).
Table 2 compares the results of the Combined
system using each of the three heuristics. Going
2
Note that the scoring metrics are different between the
CoNLL-2005 and CoNLL-2008 shared tasks. The CoNLL-
2005 required the constituent boundaries to be labeled cor-
rectly, while the CoNLL-2008 only requires identifying the
head word of each argument.
3
This category includes some nouns, e.g. gerunds.
Overall Predicate ID/Class
Heuristic Score All Verbs Nouns
Always 1 75.69 79.29 81.26 77.58
Most common 76.02 80.33 81.73 79.21
Best for subcat 76.17 80.81 82.58 79.28
Table 2: Relative performance of sense-labeling heuristics
from the simplest heuristic to the third heuristic
gained 1.52 points of F1 score on the subtask of
predicate identification/classification, and an im-
provement of .48 F1 score for the overall seman-
tic dependency score. Another interesting thing to
note is that all of improvement for noun predicates
came from choosing the most common sense in-
stead of always choosing sense 1. On the other
hand, using subcategorization information is quite
important for sense-tagging verbs.
7 Discussion and Future Work
The CoNLL-08 task introduces two new sub-
tasks for labeling semantic dependencies: predi-
cate identification and predicate classification. Our
experimental results show that both are non-trivial
and suggest that there is room for additional im-
provement on these subtasks.
We are particularly interested in two extensions
to our simplification model related to the 2008
shared task. The first is extending our simplifica-
tion model to handle the arguments of nouns. As
discussed above, there is a large amount of room
for improvement for argument labeling of nouns.
The second is incorporating uncertainty from the
parser into our model. Specifically, we would like
to extract a complete parse forest from the Char-
niak parser and use it as input to our model. This
would allow our simplification model to jointly
reason about the correct parse, possible simplifica-
tions of those parses, and semantic role labelings
of the resulting simplified sentences.
References
Gildea, D. and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics.
Pradhan, S., K. Hacioglu, V. Krugler, W. Ward, J. H.
Martin, and D. Jurafsky. 2005. Support vector learn-
ing for semantic argument classification. Machine
Learning, 60(1-3):11?39.
Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez,
and J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
In Proceedings of CoNLL.
Vickrey, D. and D. Koller. 2008. Sentence simplifica-
tion for semantic role labeling. Proceedings of ACL.
272
